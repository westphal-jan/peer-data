{"id": "1506.01245", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "3-Jun-2015", "title": "A density compensation-based path computing model for measuring semantic similarity", "abstract": "The shortest path between two concepts in a taxonomic ontology is commonly used to represent the semantic distance between concepts in the edge-based semantic similarity measures. In the past, the edge counting is considered to be the default method for the path computation, which is simple, intuitive and has low computational complexity. However, a large lexical taxonomy of such as WordNet has the irregular densities of links between concepts due to its broad domain but. The edge counting-based path computation is powerless for this non-uniformity problem. In this paper, we advocate that the path computation is able to be separated from the edge-based similarity measures and form various general computing models. Therefore, in order to solve the problem of non-uniformity of concept density in a large taxonomic ontology, we propose a new path computing model based on the compensation of local area density of concepts, which is equal to the number of direct hyponyms of the subsumers of concepts in their shortest path. This path model considers the local area density of concepts as an extension of the edge-based path and converts the local area density divided by their depth into the compensation for edge-based path with an adjustable parameter, which idea has been proven to be consistent with the information theory. This model is a general path computing model and can be applied in various edge-based similarity algorithms. The experiment results show that the proposed path model improves the average correlation between edge-based measures with human judgments on Miller and Charles benchmark from less than 0.8 to more than 0.85, and has a big advantage in efficiency than information content (IC) computation in a dynamic ontology, thereby successfully solving the non-uniformity problem of taxonomic ontology.", "histories": [["v1", "Wed, 3 Jun 2015 13:53:05 GMT  (938kb)", "http://arxiv.org/abs/1506.01245v1", "17 pages,11 figures"]], "COMMENTS": "17 pages,11 figures", "reviews": [], "SUBJECTS": "cs.AI", "authors": ["xinhua zhu", "fei li", "hongchao chen", "qi peng"], "accepted": false, "id": "1506.01245"}, "pdf": {"name": "1506.01245.pdf", "metadata": {"source": "META", "title": "A density compensation-based path computing model for measuring semantic similarity", "authors": ["Xinhua Zhu", "Fei Li", "Hongchao Chen", "Qi Peng"], "emails": [], "sections": [{"heading": null, "text": "In the past, edge counting was considered a standard method for path calculation, which was simple, intuitive, and low in computational complexity. However, in this paper we advocate that path calculation, due to its wide range, exhibits the irregular densities of connections between concepts and forms various general computational models. Therefore, in order to solve the problem of inhomogeneity of concept density in a large taxonomic ontology, we propose a new path calculation model based on the compensation of the local area density of concepts."}, {"heading": "1 Introduction", "text": "This year it is as far as never before in the history of the Federal Republic of Germany."}, {"heading": "2 Related approaches", "text": "The semantic dictionaries WordNet [13], VerbNet [23], FrameNet [24], and MindNet [25] can be used as taxonomic ontology for similarity measurements, and most of the popular similarity approaches are implemented and evaluated using WordNet as the underlying reference ontology, since their clear concept hierarchy and abundant vocabulary are used. Here, we present some popular similarity approaches in connection with our study. 2.1 However, the shortest path between concepts has a close relationship to their similarity. In fact, the shortest concept paths and concept similarities are the different forms of the same characteristics between a pair of concepts in which a corresponding relationship can be established. Rada et al [11] used the shortest path linking two concepts to measure their similarity. They defined the similarity between concepts 1 and c2 as Eq."}, {"heading": "2.3.1 Similarity measures based on IC", "text": "Resnik [12] was the first person to combine ontology and corpus, he noted that the similarity of the concept depends on the amount of information shared between them, and proposed a method based on information contents.The similarity formula is as follows: Equation (12): Re 1 2 1 2 (,) (,) ssim c IC LCS c (12) Jiang and Conrath [22] proposed a distance-based method for measuring semantic similarity between concepts.The length of the taxonomic linkages is quantified as the difference between the IC of a concept and its subsumer.In calculating the semantic distance between two concepts, they use the sum of the IC of each individual concept to subtract the IC of their LCS. The distance formula is like Equation (13): 1 2 1 2 2 2 2 2 2 (,) (2) silics IC c IC c IC LCS c (13) Lepus 17 [of the required equality (equality) (equality) (equality (2)."}, {"heading": "2.3.2 IC calculation methods", "text": "There are two basic IC calculation models: Corpora-based IC Calculation and Intrinsic IC Calculation. The corpora-based IC Calculation method was first proposed by Resnik [12], which requires a large corpus to calculate the probability of a concept, and was mainly used in the early stages. Intrinsic IC Calculation method was first proposed by Seco [27], which is only related to the hierarchical structure of a taxonomic ontology. Resnik [12] was the first to propose the IC Calculation method, and used the probability of the concept c in a given environment. The IC value is called Equ. (15) In the above equation (15) the p (c) is calculated as an equation. (16): () w Word count c w p cN (16) is the term hypo."}, {"heading": "3. Path computing model", "text": "As content of information, we can decouple from information theory, and we think that the path of surveying can also be separated from the boundary of surveying."}, {"heading": "3.3 Comparison", "text": "To compare the behavior of the two path models mentioned above in edge-based similarity measurements, we selected the six popular edge-based algorithms in this section to use the two path models to measure similarity for word pairs in MC30 and RG65 [32] datasets, and then calculated the Pearson correlation coefficients between their measurements and human judgments. Table 1 summarizes the results of their correlation coefficients. Edgecounting-based Rada et al. [11] Eq. (1) 0.6379 0.7374 Leacock et al. [19] Eq. (2) 0.7977 0.8518Wu et al. [16] Eq. (4) 0.7464 0.7854Liu _ 1 et al. [5) Eq. (5) 0.8018 0.858018 0.8446 Liu _ 2 et al. (86838 et al. [4] Eq. (6) 0.770.8405et al. Li _ 2 et al."}, {"heading": "4.1 Proposed model", "text": "Our proposed model derives from such a widespread phenomenon on similarity scales: In Fig.1, Suppose the number of subsumers of the concept c1 or cl in their shortest path, including their least common subsumers (LCS) and Si, is represented by a function of the LocalSubsumers (c1, c2) when the number of direct hyposomes of the LocalSubsumers (c1, c2) increases, the information content of the LCS (c1, c2) decreases, and the information content of the concepts c1 and c2 are unchanged, so that the similarity between c1 and c2 will decrease."}, {"heading": "5. Evaluation", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "5.1 Performance", "text": "In this study, we evaluated the performance of the proposed path calculation method from two aspects: First, we compared it with the edge-counting-based path-calculation model by using the same edge-based similarity algorithms combined with different path models to measure the same dataset from which we observe our model's ability to improve the measurement accuracy of edge-based similarity algorithms. (Then, we compared the edge-based similarity algorithms with our current excellent similarity algorithms, including IC-based approaches and hybrid approaches, to evaluate whether the edge-based similarity algorithms combined with our path model have reached a level of excellence.) To ensure fairness, we used the famous Miller & Charles [29] and Rubenstein & Goodenough [32] benchmark, which has become a de facto standard for evaluating the performance of similarity and taking many related work as a test bed."}, {"heading": "5.2 Efficiency", "text": "Efficiency is an important indicator to assess the usefulness of a methodology. IC-based similarity measurements require counting the number of all hyponyms of concepts in the taxonomy, the average time complexity is O (Max _ Nodes) in theory (in WordNet3.0, the Max _ Nodes is equal to 82115); while edge-based similarity measurements need only search the branch where the concepts lie, its average time complexity is O (Max _ Depth) in theory (in WordNet3.0, the Max _ Depth is equal to 19). To verify the large differences between edge-based measurements combined with our path model and IC-based measurements in time complexity, we chose the Wu & Palmer Method (edge-based) and the Lin Method (IC-based) the method (IC-based on comparing efficiency)."}, {"heading": "6. Discussion", "text": "From the above experimental results, we can draw several conclusions. First, the results from Table 4 to Table 7 show that our orbit model can significantly improve the measurement accuracy of various edge-based similarity algorithms, including path-based and path-depth-based algorithms. Combined with our orbit model, there are five edge-based algorithms to achieve the correlation of more than 0.85 and the best correlation reaches 0.87 on MC30, which is the widely recognized, repeatable highest level [10,35] of computer-based similarity measurements on MC30 datasets and fairly close to the average correlation (0.9015) between individual subjects of human replication reported in Resnik's replication [12] by Miller and Charles Experiment. Through the analysis, we found that a good result is mainly due to the ability of our orbit model to effectively reduce the effects of high local area density on similarity measurement, for example, for the Wortpaare & Wald, etc."}, {"heading": "7. Conclusions", "text": "Edge is an important component of the hierarchical structure of a taxonomic ontology.Edge-based similarity measurement has the advantages of being simple, intuitive and easy to understand, and it has better performance in some specific applications of computational intelligence with severely restricted taxonomies. But edge-based similarity measurements encounter problems of inconsistency in a major taxonomic ontology such as WordNet. Although this problem can be better corrected by IC-based measurement, but the IC calculation in a dynamic ontology requires a lot of computing time, this is difficult to endure in some real-time applications such as QA systems and the recovery of web information.The proposed model broke this impasse; it can significantly improve the measurement accuracy of various edge-based similarity algorithms in order to capture edge-based similarity algorithms with less computational overhead. Experiments show that our model broke this impasse; it can significantly improve the measurement accuracy of various edge-based algorithms of similarity with less computational time."}], "references": [], "referenceMentions": [], "year": 2015, "abstractText": "The shortest path between two concepts in a taxonomic ontology is commonly used to represent the semantic distance between concepts in the edge-based semantic similarity measures. In the past, the edge counting is considered to be the default method for the path computation, which is simple, intuitive and has low computational complexity. However, a large lexical taxonomy of such as WordNet has the irregular densities of links between concepts due to its broad domain but. The edge counting-based path computation is powerless for this non-uniformity problem. In this paper, we advocate that the path computation is able to be separated from the edge-based similarity measures and form various general computing models. Therefore, in order to solve the problem of non-uniformity of concept density in a large taxonomic ontology, we propose a new path computing model based on the compensation of local area density of concepts, which is equal to the number of direct hyponyms of the subsumers of concepts in their shortest path. This path model considers the local area density of concepts as an extension of the edge-based path and converts the local area density divided by their depth into the compensation for edge-based path with an adjustable parameter, which idea has been proven to be consistent with the information theory. This model is a general path computing model and can be applied in various edge-based similarity algorithms. The experiment results show that the proposed path model improves the average correlation between edge-based measures with human judgments on Miller and Charles benchmark from less than 0.8 to more than 0.85, and has a big advantage in efficiency than information content (IC) computation in a dynamic ontology, thereby successfully solving the non-uniformity problem of taxonomic ontology.", "creator": "Microsoft\u00ae Word 2010"}}}