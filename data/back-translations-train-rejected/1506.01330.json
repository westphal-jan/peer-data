{"id": "1506.01330", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "3-Jun-2015", "title": "Unsupervised Feature Analysis with Class Margin Optimization", "abstract": "Unsupervised feature selection has been always attracting research attention in the communities of machine learning and data mining for decades. In this paper, we propose an unsupervised feature selection method seeking a feature coefficient matrix to select the most distinctive features. Specifically, our proposed algorithm integrates the Maximum Margin Criterion with a sparsity-based model into a joint framework, where the class margin and feature correlation are taken into account at the same time. To maximize the total data separability while preserving minimized within-class scatter simultaneously, we propose to embed Kmeans into the framework generating pseudo class label information in a scenario of unsupervised feature selection. Meanwhile, a sparsity-based model, ` 2 ,p-norm, is imposed to the regularization term to effectively discover the sparse structures of the feature coefficient matrix. In this way, noisy and irrelevant features are removed by ruling out those features whose corresponding coefficients are zeros. To alleviate the local optimum problem that is caused by random initializations of K-means, a convergence guaranteed algorithm with an updating strategy for the clustering indicator matrix, is proposed to iteractively chase the optimal solution. Performance evaluation is extensively conducted over six benchmark data sets. From plenty of experimental results, it is demonstrated that our method has superior performance against all other compared approaches.", "histories": [["v1", "Wed, 3 Jun 2015 17:49:52 GMT  (1172kb)", "http://arxiv.org/abs/1506.01330v1", "Accepted by European Conference on Machine Learning and Principles and Practice of Knowledge Discovery in Databases, ECML/PKDD 2015"]], "COMMENTS": "Accepted by European Conference on Machine Learning and Principles and Practice of Knowledge Discovery in Databases, ECML/PKDD 2015", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["sen wang", "feiping nie", "xiaojun chang", "lina yao", "xue li", "quan z sheng"], "accepted": false, "id": "1506.01330"}, "pdf": {"name": "1506.01330.pdf", "metadata": {"source": "CRF", "title": "Unsupervised Feature Analysis with Class Margin Optimization", "authors": ["Sen Wang", "Feiping Nie", "Xiaojun Chang", "Lina Yao", "Xue Li", "Quan Z. Sheng"], "emails": ["sen.wang@uq.edu.au", "feiping.nie@gmail.com", "cxj273@gmail.com", "lina@cs.adelaide.edu.au", "xueli@itee.uq.edu.au", "michael.sheng@adelaide.edu.au"], "sections": [{"heading": null, "text": "ar Xiv: 150 6.01 330v 1 [cs.L GKeywords: unattended feature selection, maximum margin criterion, sparse structure learning, embedded K-means clustering: sen.wang @ uq.edu.au, feiping.nie @ gmail.com, cxj273 @ gmail.com \u2020 lina @ cs.adelaide.edu.au, xueli @ itee.uq.edu.au \u00a7 michael.sheng @ adelaide.edu.au"}, {"heading": "1 Introduction", "text": "In recent years, the data has more than often been represented by high-quality characteristics in a number of research areas, such as data mining [29], image processing [25], etc. To answer this question, we need to consider two points: First, the number of characteristics selected should be smaller than the number of characteristics performed in each area. Second, the number of selected characteristics must differ in the way subsequent learning tasks gain in terms of efficiency. Second, the selected characteristics should be stronger than the original ones. Many previous work has shown that removing these characteristics can improve discriminatory power in most cases."}, {"heading": "2 Notations and Definitions", "text": "To give a better understanding of the proposed method, notations and definitions used in this essay are summarized in this section. Matrices and vectors are written in uppercase bold letters and in lowercase bold letters. In the face of a data set designated as X = [x1,.., xn] - Rd \u00b7 n, where n is the number of training data and d is the dimension of the characteristics, the mean of the data is represented as x. The characteristic coefficient matrix, W \u0441Rd \u00b7 d \u2032, more linear combines data characteristics as W TX, d \u2032 is the characteristic dimension following the linear transformation. In the face of a cluster centrix matrix for the transformed data is G = [g1,..., gc] - Rd \u2032 \u00b7 \u00b7 c, its cluster indicator of the transformed xi series is called ui."}, {"heading": "3 Proposed Method", "text": "We also present our proposed method for unmonitored data collection in a geometrically maximized way, to solve this problem is an intuitive way to find a linear transformation matrix that can project the data into a new space where the original data is more divisible. - PCA is the most popular approach to analyzing the separability of characteristics. - PCA aims to find instructions where transformed data has maximum variances. - In other words, PCA aims to maximize the separability of data. - Without losing the generality, we assume that the data has zero mean, i.e. - Recall is the definition of total scatter of data, PCA is equivalent to total scatter of data."}, {"heading": "4 Optimization", "text": "In this section we present our solution for the objective function in (6). Since the function in (2), p-norm is used to exploit economical structures, the objective function cannot be solved in a closed form. (7) The objective function in (6) is equivalent to: max W, UTr (W TStW) \u2212 xTr TX, whose diagonal entries are defined as: Dii = 12 p TW = I (8) The objective function in (6) is equivalent to: max W, UTr (W TStW) \u2212 xXXTr, whose diagonal entries are defined as: Dii = 12 p TW = I (8) We propose to optimize the objective function in two steps in each iteration as follows: (1) Fix W, G and optimize U: When W is fixed."}, {"heading": "5 Experiments", "text": "In this section, experimental results are presented together with related analyses. We compare our method with seven approaches across six benchmark datasets. In addition, we conduct experiments to evaluate performance fluctuations in various aspects, including the effects of various selected attribute numbers, validation of attribute correlation analyses, and parameter sensitivity analyses. Finally, the convergence demonstration is shown."}, {"heading": "5.1 Experiment Setup", "text": "In the experiments, we compared our method with seven approaches as follows: - All Features: All original variables are preserved as the baseline in the experiments. - Max Variance: Features are ranked in the variance scale of each feature in a descending order. - Spectral Feature Selection (MCFS) [1]: This unsupervised approach selects those features who make the multi-cluster structure of the data preserved best. Features are selected with spectral regression with the 2-norm regularization.- Robust Unsupervised Feature Selection (RUFS) [20]: RUFS jointly performs robust label learning and robust feature learning."}, {"heading": "5.2 Experimental Results", "text": "In order to compare the performance of our proposed algorithm with others, we perform the test five times and report the average performance results (ACC and NMI) with standard deviations in Tables 2 and 3. It is observed that our proposed method consistently performs better than all other comparing approaches across all data sets. It is also worth noting that our method is superior to those state-of-the-art counterparts based on a Laplacian chart (SPEC, RUFS, NDFS, LapScore). We investigate how the number of selected features can affect performance by conducting an experiment whose results are shown in Figure 1. From the figure, performance variations are used in relation to the number of selected features using the proposed algorithm across three datasets, including COIL20, MNIST and USPS, which affect results. We only take ACC as a measurement. Some observations can be used as a measurement: 1) If the number of data selected is relative to each 500, for example, the number of features is small."}, {"heading": "5.3 Studies on Parameter Sensitivity and Convergence", "text": "In order to investigate the sensitivity of the parameters, we conduct an experiment to investigate how they influence performance. First, we fix \u03b2 = 10 \u2212 1 and conduct the power variations under different combinations of \u03b1s and ps in Figure 3. Second, \u03b1 is set to 10 \u2212 1. The power variation results in relation to different \u03b2s and ps in Figure 4. Both \u03b1 and \u03b2 vary in a range of [10 \u2212 3, 10 \u2212 1, 101, 101]. While p changes are set to [0.5, 1.0, 1.5], we only take ACC as a measurement. To confirm that our algorithm will monotonously increase the objective functional value in (6), we conduct an experiment to demonstrate this fact. In this case, all parameters (\u03b2, \u03b2 and the results in the objective S number) that we generally consider effective can be converted."}, {"heading": "6 Conclusion", "text": "In this paper, an approach to the uncontrolled selection of characteristics was proposed by applying the maximum margin criterion and the savings-based model. Specifically, the proposed method seeks to maximize overall dispersion on the one hand, and on the other, to simultaneously minimize dispersion within the class. As there is no label information in an unattended scenario, K-mean clustering is embedded in the framework together. Advantages can be summarized as follows: First, pseudo-labels generated by K-mean clustering may be advantageous to maximize class margins in each iteration step. Second, pseudo-labels can guide the parity-based model to exploit sparse structures of the characteristic coefficient matrix. Therefore, noisy and uncorrelated characteristics can be removed. Since the objective function for all variables is non-convex, we have an algorithm with guaranteed convergence-fast results suggested by a local property improvement."}], "references": [{"title": "Unsupervised feature selection for multi-cluster data", "author": ["D. Cai", "C. Zhang", "X. He"], "venue": "ACM International Conference on Knowledge Discovery and Data Mining (SIGKDD). pp. 333\u2013342. ACM", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2010}, {"title": "Compound rank-k projections for bilinear analysis", "author": ["X. Chang", "F. Nie", "S. Wang", "Y. Yang"], "venue": "IEEE Trans. Neural Netw. Learning Syst.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2015}, {"title": "A convex formulation for semi-supervised multi-label feature selection", "author": ["X. Chang", "F. Nie", "Y. Yang", "H. Huang"], "venue": "AAAI Conference on Artificial Intelligence (AAAI)", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2014}, {"title": "Semi-supervised feature analysis for multimedia annotation by mining label correlation", "author": ["X. Chang", "H. Shen", "S. Wang", "J. Liu", "X. Li"], "venue": "Advances in Knowledge Discovery and Data Mining, pp. 74\u201385. Springer", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2014}, {"title": "Semantic concept discovery for large-scale zero-shot event detection", "author": ["X. Chang", "Y. Yang", "A.G. Hauptmann", "E.P. Xing", "Y. Yu"], "venue": "IJCAI", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2015}, {"title": "Multiple graph unsupervised feature selection", "author": ["X. Du", "Y. Yan", "P. Pan", "G. Long", "L. Zhao"], "venue": "Signal Processing", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2014}, {"title": "Pattern classification", "author": ["R.O. Duda", "P.E. Hart", "D.G. Stork"], "venue": "John Wiley & Sons", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2012}, {"title": "From few to many: Illumination cone models for face recognition under variable lighting and pose", "author": ["A.S. Georghiades", "P.N. Belhumeur", "D. Kriegman"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI) 23(6), 643\u2013 660", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2001}, {"title": "Semisupervised feature selection via spline regression for video semantic recognition", "author": ["Y. Han", "Y. Yang", "Y. Yan", "Z. Ma", "N. Sebe", "X. Zhou"], "venue": null, "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2015}, {"title": "Laplacian score for feature selection", "author": ["X. He", "D. Cai", "P. Niyogi"], "venue": "Advances in Neural Information Processing Systems (NIPS). pp. 507\u2013514", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2005}, {"title": "Joint embedding learning and sparse regression: A framework for unsupervised feature selection", "author": ["C. Hou", "F. Nie", "X. Li", "D. Yi", "Y. Wu"], "venue": "IEEE T. Cybernetics 44(6), 793\u2013804", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2014}, {"title": "A database for handwritten text recognition research", "author": ["J.J. Hull"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI) 16(5), 550\u2013554", "citeRegEx": "12", "shortCiteRegEx": null, "year": 1994}, {"title": "A practical approach to feature selection", "author": ["K. Kira", "L.A. Rendell"], "venue": "International Workshop on Machine Learning. pp. 249\u2013256", "citeRegEx": "13", "shortCiteRegEx": null, "year": 1992}, {"title": "Estimating attributes: analysis and extensions of relief", "author": ["I. Kononenko"], "venue": "Machine Learning: ECML-94. pp. 171\u2013182. Springer", "citeRegEx": "14", "shortCiteRegEx": null, "year": 1994}, {"title": "Gradient-based learning applied to document recognition", "author": ["Y. LeCun", "L. Bottou", "Y. Bengio", "P. Haffner"], "venue": "Proceedings of the IEEE 86(11), 2278\u20132324", "citeRegEx": "15", "shortCiteRegEx": null, "year": 1998}, {"title": "Unsupervised feature selection using nonnegative spectral analysis", "author": ["Z. Li", "Y. Yang", "J. Liu", "X. Zhou", "H. Lu"], "venue": "AAAI Conference on Artificial Intelligence (AAAI). pp. 1026\u20131032", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2012}, {"title": "Variable selection for clustering with gaussian mixture models", "author": ["C. Maugis", "G. Celeux", "M.L. Martin-Magniette"], "venue": "Biometrics 65(3), 701\u2013709", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2009}, {"title": "Columbia object image library (coil-20)", "author": ["S.A. Nene", "S.K. Nayar", "H Murase"], "venue": "Tech. rep., Technical Report CUCS-005-96", "citeRegEx": "18", "shortCiteRegEx": null, "year": 1996}, {"title": "Efficient and robust feature selection via joint l2, 1-norms minimization", "author": ["F. Nie", "H. Huang", "X. Cai", "C.H.Q. Ding"], "venue": "NIPS", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2010}, {"title": "Robust unsupervised feature selection", "author": ["M. Qian", "C. Zhai"], "venue": "International Joint Conference on Artificial Intelligence (IJCAI). pp. 1621\u20131627. AAAI Press", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2013}, {"title": "Theoretical comparison between the gini index and information gain criteria", "author": ["L.E. Raileanu", "K. Stoffel"], "venue": "Annals of Mathematics and Artificial Intelligence 41(1), 77\u201393", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2004}, {"title": "Parameterisation of a stochastic model for human face identification", "author": ["F.S. Samaria", "A.C. Harter"], "venue": "IEEE Workshop on Applications of Computer Vision. pp. 138\u2013142. IEEE", "citeRegEx": "22", "shortCiteRegEx": null, "year": 1994}, {"title": "Multi-task support vector machines for feature selection with shared knowledge discovery", "author": ["Sen Wang", "Xiaojun Chang", "X.L.Q.Z.S.W.C."], "venue": "Signal Processing", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2014}, {"title": "Cluster ensembles\u2014a knowledge reuse framework for combining multiple partitions", "author": ["A. Strehl", "J. Ghosh"], "venue": "Journal of Machine Learning Research (JMLR) 3, 583\u2013617", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2003}, {"title": "Deep learning face representation from predicting 10,000 classes", "author": ["Y. Sun", "X. Wang", "X. Tang"], "venue": "IEEE Conference on Computer Vision and Pattern Recognition (CVPR). pp. 1891\u20131898. IEEE", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2014}, {"title": "An unsupervised feature selection algorithm based on ant colony optimization", "author": ["S. Tabakhi", "P. Moradi", "F. Akhlaghian"], "venue": "Engineering Applications of Artificial Intelligence 32, 112\u2013123", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2014}, {"title": "Unsupervised feature selection via unified trace ratio formulation and k-means clustering (TRACK)", "author": ["D. Wang", "F. Nie", "H. Huang"], "venue": "ECML/PKDD", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2014}, {"title": "Embedded unsupervised feature selection", "author": ["S. Wang", "J. Tang", "H. Liu"], "venue": "AAAI Conference on Artificial Intelligence (AAAI)", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2015}, {"title": "Data mining with big data", "author": ["X. Wu", "X. Zhu", "G.Q. Wu", "W. Ding"], "venue": "IEEE Transactions on Knowledge and Data Engineering (TKDE) 26(1), 97\u2013107", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2014}, {"title": "Discriminative semi-supervised feature selection via manifold regularization", "author": ["Z. Xu", "I. King", "M.T. Lyu", "R. Jin"], "venue": "IEEE Transactions on Neural Networks 21(7), 1033\u20131047", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2010}, {"title": "Feature selection for multimedia analysis by sharing information among multiple tasks", "author": ["Y. Yang", "Z. Ma", "A.G. Hauptmann", "N. Sebe"], "venue": "IEEE Transactions on Multimedia 15(3), 661\u2013669", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2013}, {"title": "l2, 1-norm regularized discriminative feature selection for unsupervised learning", "author": ["Y. Yang", "H.T. Shen", "Z. Ma", "Z. Huang", "X. Zhou"], "venue": "International Joint Conference on Artificial Intelligence (IJCAI). vol. 22, p. 1589. Citeseer", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2011}, {"title": "Harmonizing hierarchical manifolds for multimedia document semantics understanding and cross-media retrieval", "author": ["Y. Yang", "Y. Zhuang", "F. Wu", "Y. Pan"], "venue": "IEEE Transactions on Multimedia 10(3), 437\u2013446", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2008}, {"title": "Spectral feature selection for supervised and unsupervised learning", "author": ["Z. Zhao", "H. Liu"], "venue": "International Conference on Machine Learning. pp. 1151\u20131157. ACM", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2007}, {"title": "Self-taught dimensionality reduction on the high-dimensional small-sized data", "author": ["X. Zhu", "Z. Huang", "Y. Yang", "H.T. Shen", "C. Xu", "J. Luo"], "venue": "Pattern Recognition 46(1), 215\u2013229", "citeRegEx": "35", "shortCiteRegEx": null, "year": 2013}], "referenceMentions": [{"referenceID": 28, "context": "Over the past few years, data are more than often represented by high-dimensional features in a number of research fields, such as data mining [29], computer vision [25], etc.", "startOffset": 143, "endOffset": 147}, {"referenceID": 24, "context": "Over the past few years, data are more than often represented by high-dimensional features in a number of research fields, such as data mining [29], computer vision [25], etc.", "startOffset": 165, "endOffset": 169}, {"referenceID": 34, "context": "Due to a lower dimensional representation, the subsequent learning tasks with no doubt can gain benefit in terms of efficiency [35].", "startOffset": 127, "endOffset": 131}, {"referenceID": 6, "context": "Representative supervised feature selection algorithms include Fisher score [7], Relief[13] and its extension, ReliefF [14], information gain [21], etc [23,31].", "startOffset": 76, "endOffset": 79}, {"referenceID": 12, "context": "Representative supervised feature selection algorithms include Fisher score [7], Relief[13] and its extension, ReliefF [14], information gain [21], etc [23,31].", "startOffset": 87, "endOffset": 91}, {"referenceID": 13, "context": "Representative supervised feature selection algorithms include Fisher score [7], Relief[13] and its extension, ReliefF [14], information gain [21], etc [23,31].", "startOffset": 119, "endOffset": 123}, {"referenceID": 20, "context": "Representative supervised feature selection algorithms include Fisher score [7], Relief[13] and its extension, ReliefF [14], information gain [21], etc [23,31].", "startOffset": 142, "endOffset": 146}, {"referenceID": 22, "context": "Representative supervised feature selection algorithms include Fisher score [7], Relief[13] and its extension, ReliefF [14], information gain [21], etc [23,31].", "startOffset": 152, "endOffset": 159}, {"referenceID": 30, "context": "Representative supervised feature selection algorithms include Fisher score [7], Relief[13] and its extension, ReliefF [14], information gain [21], etc [23,31].", "startOffset": 152, "endOffset": 159}, {"referenceID": 29, "context": "In the real world, class information is quite limited, resulting in the development of semi-supervised feature selection methods [30,9,3,4], in which both labeled and unlabeled data are utilized.", "startOffset": 129, "endOffset": 139}, {"referenceID": 8, "context": "In the real world, class information is quite limited, resulting in the development of semi-supervised feature selection methods [30,9,3,4], in which both labeled and unlabeled data are utilized.", "startOffset": 129, "endOffset": 139}, {"referenceID": 2, "context": "In the real world, class information is quite limited, resulting in the development of semi-supervised feature selection methods [30,9,3,4], in which both labeled and unlabeled data are utilized.", "startOffset": 129, "endOffset": 139}, {"referenceID": 3, "context": "In the real world, class information is quite limited, resulting in the development of semi-supervised feature selection methods [30,9,3,4], in which both labeled and unlabeled data are utilized.", "startOffset": 129, "endOffset": 139}, {"referenceID": 9, "context": "[10] assume that data from the same class are often close to each other and use the locality preserving power of data, also termed as Laplacian Score, to evaluate importance degrees of features.", "startOffset": 0, "endOffset": 4}, {"referenceID": 33, "context": "In [34], a unified framework has been proposed for both supervised and unsupervised feature selection schemes using a spectral graph.", "startOffset": 3, "endOffset": 7}, {"referenceID": 25, "context": "[26] have proposed an unsupervised feature selection method to select the optimal feature subset in an iterative algorithm, which is based on ant colony optimization.", "startOffset": 0, "endOffset": 4}, {"referenceID": 16, "context": "In [17], for instance, the authors have developed a model that selects relevant features using two backward stepwise selection algorithms without prior knowledges of features.", "startOffset": 3, "endOffset": 7}, {"referenceID": 0, "context": "Because clustering algorithms are able to provide pseudo labels which can reflect the intrinsic information of data, some works [1,16,20,28] incorporate different clustering algorithms in objective functions to select features.", "startOffset": 128, "endOffset": 140}, {"referenceID": 15, "context": "Because clustering algorithms are able to provide pseudo labels which can reflect the intrinsic information of data, some works [1,16,20,28] incorporate different clustering algorithms in objective functions to select features.", "startOffset": 128, "endOffset": 140}, {"referenceID": 19, "context": "Because clustering algorithms are able to provide pseudo labels which can reflect the intrinsic information of data, some works [1,16,20,28] incorporate different clustering algorithms in objective functions to select features.", "startOffset": 128, "endOffset": 140}, {"referenceID": 27, "context": "Because clustering algorithms are able to provide pseudo labels which can reflect the intrinsic information of data, some works [1,16,20,28] incorporate different clustering algorithms in objective functions to select features.", "startOffset": 128, "endOffset": 140}, {"referenceID": 9, "context": "Most of the existing unsupervised feature selection methods [10,34,20,27,16,11] rely on a graph, e.", "startOffset": 60, "endOffset": 79}, {"referenceID": 33, "context": "Most of the existing unsupervised feature selection methods [10,34,20,27,16,11] rely on a graph, e.", "startOffset": 60, "endOffset": 79}, {"referenceID": 19, "context": "Most of the existing unsupervised feature selection methods [10,34,20,27,16,11] rely on a graph, e.", "startOffset": 60, "endOffset": 79}, {"referenceID": 26, "context": "Most of the existing unsupervised feature selection methods [10,34,20,27,16,11] rely on a graph, e.", "startOffset": 60, "endOffset": 79}, {"referenceID": 15, "context": "Most of the existing unsupervised feature selection methods [10,34,20,27,16,11] rely on a graph, e.", "startOffset": 60, "endOffset": 79}, {"referenceID": 10, "context": "Most of the existing unsupervised feature selection methods [10,34,20,27,16,11] rely on a graph, e.", "startOffset": 60, "endOffset": 79}, {"referenceID": 9, "context": "Meanwhile, some traditional feature selection algorithms [10,7] neglect correlations among features.", "startOffset": 57, "endOffset": 63}, {"referenceID": 6, "context": "Meanwhile, some traditional feature selection algorithms [10,7] neglect correlations among features.", "startOffset": 57, "endOffset": 63}, {"referenceID": 31, "context": "Recently, exploiting feature correlations has attracted much research attention [32,20,5,6,19,33].", "startOffset": 80, "endOffset": 97}, {"referenceID": 19, "context": "Recently, exploiting feature correlations has attracted much research attention [32,20,5,6,19,33].", "startOffset": 80, "endOffset": 97}, {"referenceID": 4, "context": "Recently, exploiting feature correlations has attracted much research attention [32,20,5,6,19,33].", "startOffset": 80, "endOffset": 97}, {"referenceID": 5, "context": "Recently, exploiting feature correlations has attracted much research attention [32,20,5,6,19,33].", "startOffset": 80, "endOffset": 97}, {"referenceID": 18, "context": "Recently, exploiting feature correlations has attracted much research attention [32,20,5,6,19,33].", "startOffset": 80, "endOffset": 97}, {"referenceID": 32, "context": "Recently, exploiting feature correlations has attracted much research attention [32,20,5,6,19,33].", "startOffset": 80, "endOffset": 97}, {"referenceID": 3, "context": "Inspired by recent feature selection works using sparsity-based model on the regularization term [4], on the other hand, the proposed algorithm learns sparse structural information of the coefficient matrix, with the goal of reducing noisy and irrelevant features by removing those features whose coefficients are zeros.", "startOffset": 97, "endOffset": 100}, {"referenceID": 1, "context": "However, LDA and its variants require class information to construct between-class and within-class scatter matrices [2], which is not suitable for unsupervised feature selection.", "startOffset": 117, "endOffset": 120}, {"referenceID": 33, "context": "\u2013 Spectral Feature Selection (SPEC) [34]: This method employs a unified framework to select features one by one based on spectral graph theory.", "startOffset": 36, "endOffset": 40}, {"referenceID": 0, "context": "\u2013 Multi-Cluster Feature Selection (MCFS) [1]: This unsupervised approach selects those features who make the multi-cluster structure of the data preserved best.", "startOffset": 41, "endOffset": 44}, {"referenceID": 19, "context": "\u2013 Robust Unsupervised Feature Selection (RUFS) [20]: RUFS jointly performs robust label learning and robust feature learning.", "startOffset": 47, "endOffset": 51}, {"referenceID": 15, "context": "\u2013 Nonnegative Discriminative Feature Selection (NDFS) [16]: NDFS exploits local discriminative information and feature correlations simultaneously.", "startOffset": 54, "endOffset": 58}, {"referenceID": 9, "context": "\u2013 Laplacian Score (LapScore) [10]: This method learns and selects distinctive features by evaluating their powers of locality preserving, which is also called Laplacian Score.", "startOffset": 29, "endOffset": 33}, {"referenceID": 23, "context": "According to the definition in [24], NMI is defined as:", "startOffset": 31, "endOffset": 35}, {"referenceID": 17, "context": "The performance evaluations are performed over six benchmark datasets as follows: \u2013 COIL20 [18]: It contains 1,440 gray-scale images of 20 objects (72 images per object) under various poses.", "startOffset": 91, "endOffset": 95}, {"referenceID": 14, "context": "\u2013 MNIST [15]: It is a large-scale dataset of handwritten digits, which has been widely used as a test bed in data mining.", "startOffset": 8, "endOffset": 12}, {"referenceID": 21, "context": "\u2013 ORL [22]: This data set which is used as a benchmark for face recognition, consists of 40 different subjects with 10 images each.", "startOffset": 6, "endOffset": 10}, {"referenceID": 11, "context": "\u2013 USPS [12]: This dataset collects 9,298 images of handwritten digits (0-9) from envelops by the U.", "startOffset": 7, "endOffset": 11}, {"referenceID": 7, "context": "\u2013 YaleB [8]: It consists of 2,414 frontal face images of 38 subjects.", "startOffset": 8, "endOffset": 11}, {"referenceID": 9, "context": "\u03b2 varies in a range of [0, 10, 10, 10, 1, 10, 10, 10].", "startOffset": 23, "endOffset": 53}, {"referenceID": 9, "context": "\u03b2 varies in a range of [0, 10, 10, 10, 1, 10, 10, 10].", "startOffset": 23, "endOffset": 53}, {"referenceID": 9, "context": "\u03b2 varies in a range of [0, 10, 10, 10, 1, 10, 10, 10].", "startOffset": 23, "endOffset": 53}, {"referenceID": 0, "context": "\u03b2 varies in a range of [0, 10, 10, 10, 1, 10, 10, 10].", "startOffset": 23, "endOffset": 53}, {"referenceID": 9, "context": "\u03b2 varies in a range of [0, 10, 10, 10, 1, 10, 10, 10].", "startOffset": 23, "endOffset": 53}, {"referenceID": 9, "context": "\u03b2 varies in a range of [0, 10, 10, 10, 1, 10, 10, 10].", "startOffset": 23, "endOffset": 53}, {"referenceID": 9, "context": "\u03b2 varies in a range of [0, 10, 10, 10, 1, 10, 10, 10].", "startOffset": 23, "endOffset": 53}, {"referenceID": 9, "context": "Both \u03b1 and \u03b2 vary in a range of [10, 10, 10, 10].", "startOffset": 32, "endOffset": 48}, {"referenceID": 9, "context": "Both \u03b1 and \u03b2 vary in a range of [10, 10, 10, 10].", "startOffset": 32, "endOffset": 48}, {"referenceID": 9, "context": "Both \u03b1 and \u03b2 vary in a range of [10, 10, 10, 10].", "startOffset": 32, "endOffset": 48}, {"referenceID": 9, "context": "Both \u03b1 and \u03b2 vary in a range of [10, 10, 10, 10].", "startOffset": 32, "endOffset": 48}], "year": 2015, "abstractText": "Unsupervised feature selection has been always attracting research attention in the communities of machine learning and data mining for decades. In this paper, we propose an unsupervised feature selection method seeking a feature coefficient matrix to select the most distinctive features. Specifically, our proposed algorithm integrates the Maximum Margin Criterion with a sparsity-based model into a joint framework, where the class margin and feature correlation are taken into account at the same time. To maximize the total data separability while preserving minimized within-class scatter simultaneously, we propose to embed Kmeans into the framework generating pseudo class label information in a scenario of unsupervised feature selection. Meanwhile, a sparsity-based model, l2,p-norm, is imposed to the regularization term to effectively discover the sparse structures of the feature coefficient matrix. In this way, noisy and irrelevant features are removed by ruling out those features whose corresponding coefficients are zeros. To alleviate the local optimum problem that is caused by random initializations of K-means, a convergence guaranteed algorithm with an updating strategy for the clustering indicator matrix, is proposed to iteratively chase the optimal solution. Performance evaluation is extensively conducted over six benchmark data sets. From plenty of experimental results, it is demonstrated that our method has superior performance against all other compared approaches.", "creator": "LaTeX with hyperref package"}}}