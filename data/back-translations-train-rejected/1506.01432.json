{"id": "1506.01432", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "3-Jun-2015", "title": "Encoding Markov Logic Networks in Possibilistic Logic", "abstract": "Markov logic uses weighted formulas to compactly encode a probability distribution over possible worlds. Despite the use of logical formulas, Markov logic networks (MLNs) can be difficult to interpret, due to the often counter-intuitive meaning of their weights. To address this issue, we propose a method to construct a possibilistic logic theory that exactly captures what can be derived from a given MLN using maximum a posteriori (MAP) inference. Unfortunately, the size of this theory is exponential in general. We therefore also propose two methods which can derive compact theories that still capture MAP inference, but only for specific types of evidence. These theories can be used, among others, to make explicit the hidden assumptions underlying an MLN or to explain the predictions it makes.", "histories": [["v1", "Wed, 3 Jun 2015 23:20:28 GMT  (26kb)", "https://arxiv.org/abs/1506.01432v1", null], ["v2", "Mon, 8 Jun 2015 19:58:03 GMT  (27kb)", "http://arxiv.org/abs/1506.01432v2", "Extended version of a paper appearing in UAI 2015"]], "reviews": [], "SUBJECTS": "cs.AI", "authors": ["ondrej kuzelka", "jesse davis", "steven schockaert"], "accepted": false, "id": "1506.01432"}, "pdf": {"name": "1506.01432.pdf", "metadata": {"source": "CRF", "title": null, "authors": [], "emails": [], "sections": [{"heading": null, "text": "ar Xiv: 150 6.01 432v 2 [cs.A I] 8 Jun 2Markov logic uses weighted formulas to compactly encode a probability distribution across possible worlds. Despite the use of logical formulas, Markov logic networks (MLNs) can be difficult to interpret due to the often counterintuitive importance of their weighting. To solve this problem, we propose a method of constructing a theory of possibility logic that accurately captures what can be derived from a given MLN, using maximum a posteriori (MAP) conclusions. Unfortunately, the magnitude of this theory is generally exponential. We therefore also propose two methods that can derive compact theories that still capture MAP conclusions, but only for certain types of evidence. These theories can be used, among other things, to explicitly explain the hidden assumptions underlying an MLN or the predictions it makes."}, {"heading": "1 INTRODUCTION", "text": "In fact, it is the case that most of them are able to move into another world, in which they are able to move, in which they are able to move, in which they move, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live in which they live, in which they live, in which they live in which they live, in which they live, in which they live in which they live, in which they live in which they live, in which they live, in which they live in which they live, in which they live, in which they live, in which they live in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they, in which they live, in which they, in which they"}, {"heading": "2 BACKGROUND", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "2.1 MARKOV LOGIC", "text": "A Markov logic network (MLN) [22] is a series of pairs (F, wF), where F is a first-order formula of logic and wF is a real number that intuitively reflects a penalty applied to possible worlds (i.e. logical interpretations) that violate F. In examples, we will also use the notation wF: F to mark the formula (F, wF). An MLN serves as a template for constructing a propositional Markov network. In particular, given a set of constants C, an MLN M induces the following probability distribution to possible worlds: pM (F) = 1Z exp \u2211 (F, wF)."}, {"heading": "2.2 POSSIBILISTIC LOGIC", "text": "A possibility distribution in a universe is a representation of the entire universe on a deterrent machine with access to an intuitive level. (A) We assume that a certain variable X (A) is always available. (A) We assume that all universes are finite. (A) We assume that a certain variable X (A) is possible. (A) We assume that in a state of complete ignorance we have such a possibility. (A) = 1 and (x) = 0 for x 6 = x0. The theory of possibility [27, 12] is based on the possibility measurement and the dual necessity measurement N, which is induced by a possibility distribution. (A) = max. (A) N (A) = 1 \u2212 (A) The possibility theory J2 contains those decision problems that can be solved in a polynomial distribution machine."}, {"heading": "3 ENCODING GROUND NETWORKS", "text": "In this section, we assume that M is a basis for MLN in which all weights are absolutely positive. (This can always be guaranteed for MLNs by replacing formulas (Alpha, Alpha, Alpha, Alpha, Alpha, Alpha, Alpha, Alpha, Alpha, Alpha, Alpha, Alpha, Alpha, Alpha, Alpha, Alpha, Alpha, Alpha, Alpha, Alpha,..., Fn, Alpha, Alpha, Alpha, Alpha, Alpha, Alpha, Alpha, Alpha, Alpha, Alpha, Alpha, Alpha, Alpha, Alpha, Alpha, Alpha, Alpha, Fn, etc.) In particular, we are the classical formulas that appear in the MLN. The following transformation constructs a possible logical theory that in a sense is equivalent to a given MLN. It is inspired by the probability stranslation of possible logic from MLN, which follows [1.]"}, {"heading": "3.1 SELECTIVELY AVOIDING DROWNING", "text": "In many applications, we are only interested in certain types of proofs to ensure that the resulting certainty prevents them when they are contained in most K-words, or in proofs that contain only positive dictionaries. In such cases, we can often derive a more compact possible theory that is as follows: Let E be the set of proofs we want to consider, with each email being a set of proofs. (M, E) We write SE for the set of all the minimum subsets {F1,..., Fl} of M-E = {F | F-M-E-evidence we want to consider < pen (M, E) s.t.pen (M, E-E).pen (5) The following transformation constructs a possible logical theory that prevents MAP conclusions for proofs in E. The basic intuition is that we want the formulas in M-E to ensure that certainty resulting from it is available."}, {"heading": "3.2 MAP INFERENCE AS DEFAULT REASONING", "text": "A large number of approaches in possible logic are based on this idea."}, {"heading": "4 ENCODING NON-GROUND NETWORKS", "text": "In cases where there is only one type, we do not explicitly write it."}, {"heading": "5 ILLUSTRATIVE EXAMPLES", "text": "The first example is a variation of a classical problem for non-monotonous reasons. (\u00ac) (\u00ac) (\u00ac) (\u00ac) (\u00ac) (\u00ac) (\u00ac) (\u00ac) (\u00ac) (\u00ac) (\u00ac) (\u00ac) (\u00ac) (\u00ac) (\u00ac) (\u00ac) (\u00ac) (\u00ac) (\u00ac) (\u00ac) (\u00ac) (\u00ac) (\u00ac) (\u00ac) (\u00ac) (\u00ac) (\") (\") (\") (\") (\") (\") () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () ()) () () () () () () () ()) () () ()) () () () ()) () () ()) () () () () () () () () () () ()) () () () ()) () () ()) () () () () ()) () () () () () ()) () () () () () () () () () ()) () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () (() () () () () () () () () () (() () () (() () () (() () () ((() () () () () () () () () () () () () () (("}, {"heading": "6 RELATED WORK", "text": "One line of related work focuses on extracting a comprehensible model from another learned model that is difficult or impossible to interpret. A pioneering work in this area is the TREPAN [5] algorithm. Faced with a trained neural network and a dataset, TREPAN learns a decision tree to mimic the predictions of the neural network. In addition to producing interpretable results, it has been shown that this algorithm learns accurate models that faithfully mimic the predictions of the neural network. Recent research focuses on converging complex ensemble classifiers with a single model. For example, Popovic et al. [21] proposed a method for learning a single decision tree that mimics the predictions of a random forest. While to the best of knowledge this is the first paper to examine the relationship between Markov logic and possible logic, the link between the possibility theory and probability theory is comprehensively examined."}, {"heading": "7 CONCLUSIONS", "text": "This work focused on how a Markov logic network M can be coded in possible logic. We started from the observation that it is always possible to construct a theory of possible logic equivalent to M, in the sense that the probability distribution induced by M is isomorphic for the possibility distribution induced by M. Consequently, applying the theory of possible logic leads to the same conclusions as applying the MAP inference to M. Although the size of M is exponentially in the number of formulas in M, we have shown how more compact theories can be obtained in cases where we can impose limitations on the types of evidence that must be taken into account (e.g. small sets of words). Our main motivation was the use of possible logic as a way to explicitly make the assumptions coded in a given MLN."}, {"heading": "Acknowledgements", "text": "We thank the anonymous reviewers for their helpful comments. This work was supported by a grant from the Leverhulme Trust (RPG-2014-164). JD is partially supported by the research fund KU Leuven (OT / 11 / 051), the Marie Curie Career Integration Grant of the EU (# 294068) and FWO-Vlaanderen (G.0356.12)."}, {"heading": "A PROOFS", "text": "The proof of proposition 1 is that formula E is a formula which is not contained in formula X. In the definition of X, this means that formula E must apply to any other formula. Indeed, for formula Y, for formula E, for formula E, for formula E, for formula E, for formula E, for formula E, for formula Y, for formula E, for formula E, for formula E, for formula Y, for formula E, for formula E, for formula E, for formula E, for formula E, for formula E, for formula Y, for formula E, for formula E, for formula E, for formula E, for formula E, for formula Y, for formula E, for formula E, for formula E, for formula E, for formula E, for formula E, for formula E, for formula E, for formula E, for formula E, for formula E, for formula E, for formula E, for formula E, for formula E, for formula E, for formula E, for formula E, for formula Y, for formula E, for formula E, for formula E, for formula E, for formula E, for formula E, for formula E, for formula E, for formula E, for formula E, for formula E, for formula E, for formula E, for formula E. \""}, {"heading": "B ADDITIONAL EXAMPLE", "text": "In this section, we illustrate Transformation 4 on a larger MLN trained for predicting categories of computer science papers 5, which is shown in Table 1. This MLN contains rules for predicting categories of papers from categories of other papers that refer to it (Rules 2-3) or from categories of papers written by the same author (Rule 3). In addition, it contains rules, the previous probabilities of individual probabilities, and a hard rule that stipulates that each work has at most one category (for the sake of simplicity). We applied Transformation 4 to this MLN, which led to a possible logic theory with 200 rules (Rule 3). Table 2 shows a subset of rules in the resulting theory that are interesting for the visualization of some properties of the MLN and that are not immediately obvious from the MLN itself. Note that apart from the formulas of Form (14), which represent formulas of theory, we present, in the implication form, to simplify the interpretation."}], "references": [], "referenceMentions": [], "year": 2015, "abstractText": "Markov logic uses weighted formulas to com-<lb>pactly encode a probability distribution over pos-<lb>sible worlds. Despite the use of logical formu-<lb>las, Markov logic networks (MLNs) can be diffi-<lb>cult to interpret, due to the often counter-intuitive<lb>meaning of their weights. To address this issue,<lb>we propose a method to construct a possibilis-<lb>tic logic theory that exactly captures what can<lb>be derived from a given MLN using maximum<lb>a posteriori (MAP) inference. Unfortunately, the<lb>size of this theory is exponential in general. We<lb>therefore also propose two methods which can<lb>derive compact theories that still capture MAP<lb>inference, but only for specific types of evidence.<lb>These theories can be used, among others, to<lb>make explicit the hidden assumptions underlying<lb>an MLN or to explain the predictions it makes.", "creator": "LaTeX with hyperref package"}}}