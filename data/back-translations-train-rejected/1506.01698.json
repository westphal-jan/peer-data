{"id": "1506.01698", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "4-Jun-2015", "title": "The Long-Short Story of Movie Description", "abstract": "Generating descriptions for videos has many applications including assisting blind people and human-robot interaction. The recent advances in image captioning as well as the release of large-scale movie description datasets such as MPII Movie Description allow to study this task in more depth. Many of the proposed methods for image captioning rely on pre-trained object classifier CNNs and Long-Short Term Memory recurrent networks (LSTMs) for generating descriptions. While image description focuses on objects, we argue that it is important to distinguish verbs, objects, and places in the challenging setting of movie description. In this work we show how to learn robust visual classifiers from the weak annotations of the sentence descriptions. Based on these visual classifiers we learn how to generate a description using an LSTM. We explore different design choices to build and train the LSTM and achieve the best performance to date on the challenging MPII-MD dataset. We compare and analyze our approach and prior work along various dimensions to better understand the key challenges of the movie description task.", "histories": [["v1", "Thu, 4 Jun 2015 19:45:36 GMT  (1347kb,D)", "http://arxiv.org/abs/1506.01698v1", null]], "reviews": [], "SUBJECTS": "cs.CV cs.CL", "authors": ["anna rohrbach", "marcus rohrbach", "bernt schiele"], "accepted": false, "id": "1506.01698"}, "pdf": {"name": "1506.01698.pdf", "metadata": {"source": "CRF", "title": "The Long-Short Story of Movie Description", "authors": ["Anna Rohrbach", "Marcus Rohrbach", "Bernt Schiele"], "emails": [], "sections": [{"heading": "1 Introduction", "text": "Many of the proposed methods are based on Long-Short Term Memory Networks (LSTMs) [13]. In the meantime, two large film description datasets have been proposed, MPII Movie Description (MPIIMD) [28] and Montreal Video Annotation Dataset (M-VAD) [31]. Both are based on movies with accompanying text descriptions and allow studying the problem of generating movie descriptions for visually impaired people. Work on these datasets [28, 33, 39] shows that they do indeed pose a challenge in terms of visual recognition and automatic description, resulting in significantly lower performance than simpler video datasets (e.g. MSVD [2]), but lacking a detailed analysis of the difficulties. In this paper, we will focus on the performance of existing film description methods."}, {"heading": "2 Related Work", "text": "Most work has been proposed [6, 8, 16, 17, 23, 35, 37], many of which rely on Recurrent Neural Networks (RNNs) and in particular Long-Short Term Memory Networks (LSTMs), and new datasets have also been published, Flickr30k [40] and MS COCO Captions [3], which additionally introduce a standardized captioning setup. There are also attempts to analyze the performance of newer methods, e.g. [5] comparing them in terms of novelty of the generated descriptions, and also proposing a nearest baseline that improves current methods. In the past, video descriptions have been approached in semirealistic environments."}, {"heading": "3 Approach", "text": "In this section, we present our two-step approach to video description: the first step performs visual recognition, while the second step generates text descriptions; for visual recognition, we suggest using visual classifiers trained according to the semantics and \"visuality\" of the labels; and for speech generation, we rely on an LSTM network that has been successfully used for image and video description [6, 33]; and we discuss various design options for building and training the LSTM. An overview of our approach can be found in Figure 1."}, {"heading": "3.1 Visual Labels for Robust Visual Classifiers", "text": "For training, we rely on a parallel corpus of videos and weak sentence comments. As in [28], we analyze sentences to obtain a set of labels (single words or short phrases, for example, look up) to train our visual classifiers. However, unlike [28], we do not want to keep all of these initial labels because they are loud, but only select visual labels that are actually highly recognizable. Let's avoid parser failure. Not all sentences can be successfully analyzed, because, for example, some sentences are incomplete or grammatically wrong. In order not to lose the potential labels in these sentences, we adjust our initial labels to sentences that the parser does not process. Our labels correspond to different semantic groups. In this work, we look at three main groups: the objects (actions), objects, and places that are typical of visual labels."}, {"heading": "3.2 LSTM for Sentence Generation", "text": "We rely on the basic LSTM architecture proposed in [6] for the video description. As shown in Figures 1 and 2 (a), an LSTM generates a word at each step and receives the visual classifiers (input-vis) as well as the previously generated word (input-lang) as input. To handle natural words, we encode each word with a one-hot vector according to its index in a dictionary and a low-dimensional embedding. The embedding is learned jointly during the training of the LSTM. [6] compares three variants: (a) an encoder decoder architecture, (b) a decoder architecture with visual max predictions, and (c) a decoder architecture with visual probability predictions. In this work, we rely on the variant (c) that works best because it can rely on the richest visual input. We analyze the following aspects for this architecture: We compare a layered architecture with an architecture-1."}, {"heading": "4 Evaluation", "text": "In this section, we will first analyze our approach to the MPII-MD dataset [28] and examine various design options. Then, we will compare our best system with previous work."}, {"heading": "4.1 Analysis of our approach", "text": "This year is the highest in the history of the country."}, {"heading": "4.2 Comparison to related work", "text": "We compare the best methods of S2VT with those of S2K. We report on all possible measures we have taken to get the situation under control."}, {"heading": "5 Analysis", "text": "Despite recent advances in the field of video descriptions, including our proposed approach, the performance of video descriptions on the film description data sets (MPII-MD [28] and M-VAD [31]) remains relatively low. In this section, we will take a closer look at three methods, the best SMT of [28], S2VT [33] and ours, to understand where these methods succeed and where they fail. Below, we evaluate all three methods on the MPII-MD test set."}, {"heading": "5.1 Difficulty versus performance", "text": "As the first study, we propose to sort the reference sentences (from the test sentence) by difficulty, defining the difficulty in several ways. Sentence length and Word frequency. Two of the simplest sets of difficulty measures are their length and average frequency of words. When sorting the data by difficulty (increasing the sentence length or decreasing the average word frequency), we find that all three methods have the same tendency to achieve lower METEOR values as the difficulty increases (Figures 4a and 4b). For the word frequency, the correlation is stronger than the other two, most noteworthy than the difficulty. Textual and visual nearest tendencies (figures) we look for the nearest reference sentence (in terms of METEOR values). We use the best preserved reference sentences to sort the reference sentences by difficulty."}, {"heading": "5.2 Semantic analysis", "text": "We analyze the test sentences more closely with respect to different verbs. For this, we rely on WordNet themes (high entries in WordNet ontology, e.g. \"movement,\" \"perception,\" \"competition,\" \"emotion\"), which are defined for most synsets in WordNet [10]. We get the sensory information from the semantic parser of [28], so the senses could be loud. We show the 5 most common verbs for each topic in Table 6. We select sentences with a single verb, group them by verb theme and calculate an average METEOR rating for each topic, see Figure 6, that our method is best for all topics except \"communication.\" The most common verbs in this topic are \"look up\" and \"nod,\" which are also frequently produced in the datasets and in the sentences."}, {"heading": "6 Conclusion", "text": "We propose an approach to automatic film description that trains visual classifiers and uses the classification values as input for LSTM. To deal with the weak sentence comments, we rely on three main components. Firstly, we distinguish three semantic groups of names (verbs, objects and places); secondly, we train them in a discriminatory way by removing potentially noisy negatives; and thirdly, we select only a small number of the most reliable classifiers. In generating sentences, we show the benefits of exploring different LSTM architectures and learning configurations. As a result, we achieve the highest performance on the MPII-MD dataset as demonstrated by all automatic assessment measures and comprehensive human ratings. We analyze the challenges in the film description task based on our and two previous works. Factors contributing to higher performance include: the presence of frequent words, sentence length and simplicity, and the presence of \"visual\" (\"nod\")."}], "references": [{"title": "Video in sentences out", "author": ["A. Barbu", "A. Bridge", "Z. Burchill", "D. Coroian", "S. Dickinson", "S. Fidler", "A. Michaux", "S. Mussman", "S. Narayanaswamy", "D. Salvi", "L. Schmidt", "J. Shangguan", "J.M. Siskind", "J. Waggoner", "S. Wang", "J. Wei", "Y. Yin", "Z. Zhang"], "venue": "Proceedings of the conference on Uncertainty in Artificial Intelligence (UAI)", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2012}, {"title": "Collecting highly parallel data for paraphrase evaluation", "author": ["D. Chen", "W. Dolan"], "venue": "Proceedings of the Annual Meeting of the Association for Computational Linguistics (ACL)", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2011}, {"title": "Microsoft coco captions: Data collection and evaluation server", "author": ["X. Chen", "H. Fang", "T. Lin", "R. Vedantam", "S. Gupta", "P. Dollr", "C.L. Zitnick"], "venue": "arXiv:1504.00325", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2015}, {"title": "Thousand frames in just a few words: Lingual description of videos through latent topics and sparse object stitching", "author": ["P. Das", "C. Xu", "R. Doell", "J. Corso"], "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2013}, {"title": "Language models for image captioning: The quirks and what works", "author": ["J. Devlin", "H. Cheng", "H. Fang", "S. Gupta", "L. Deng", "X. He", "G. Zweig", "M. Mitchell"], "venue": "arXiv:1505.01809", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2015}, {"title": "Long-term recurrent convolutional networks for visual recognition and description", "author": ["J. Donahue", "L.A. Hendricks", "S. Guadarrama", "M. Rohrbach", "S. Venugopalan", "K. Saenko", "T. Darrell"], "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2015}, {"title": "Image description using visual dependency representations", "author": ["D. Elliott", "F. Keller"], "venue": "EMNLP. pp. 1292\u20131302", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2013}, {"title": "From captions to visual concepts and back", "author": ["H. Fang", "S. Gupta", "F.N. Iandola", "R. Srivastava", "L. Deng", "P. Doll\u00e1r", "J. Gao", "X. He", "M. Mitchell", "J.C. Platt", "C.L. Zitnick", "G. Zweig"], "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2015}, {"title": "Every picture tells a story: Generating sentences from images", "author": ["A. Farhadi", "M. Hejrati", "M. Sadeghi", "P. Young", "C. Rashtchian", "J. Hockenmaier", "D. Forsyth"], "venue": "Proceedings of the European Conference on Computer Vision (ECCV)", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2010}, {"title": "WordNet: An Electronical Lexical Database", "author": ["C. Fellbaum"], "venue": "The MIT Press", "citeRegEx": "10", "shortCiteRegEx": null, "year": 1998}, {"title": "Youtube2text: Recognizing and describing arbitrary activities using semantic hierarchies and zero-shoot recognition", "author": ["S. Guadarrama", "N. Krishnamoorthy", "G. Malkarnenkar", "S. Venugopalan", "R. Mooney", "T. Darrell", "K. Saenko"], "venue": "Proceedings of the IEEE International Conference on Computer Vision (ICCV)", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2013}, {"title": "Improving neural networks by preventing co-adaptation of feature detectors", "author": ["G.E. Hinton", "N. Srivastava", "A. Krizhevsky", "I. Sutskever", "R.R. Salakhutdinov"], "venue": "arXiv:1207.0580", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2012}, {"title": "Long short-term memory", "author": ["S. Hochreiter", "J. Schmidhuber"], "venue": "Neural Computation", "citeRegEx": "13", "shortCiteRegEx": null, "year": 1997}, {"title": "LSDA: Large scale detection through adaptation", "author": ["J. Hoffman", "S. Guadarrama", "E. Tzeng", "J. Donahue", "R. Girshick", "T. Darrell", "K. Saenko"], "venue": "Advances in Neural Information Processing Systems (NIPS)", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2014}, {"title": "Caffe: Convolutional architecture for fast feature embedding", "author": ["Y. Jia", "E. Shelhamer", "J. Donahue", "S. Karayev", "J. Long", "R. Girshick", "S. Guadarrama", "T. Darrell"], "venue": "arXiv:1408.5093", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2014}, {"title": "Deep visual-semantic alignments for generating image descriptions", "author": ["A. Karpathy", "L. Fei-Fei"], "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2015}, {"title": "Unifying visual-semantic embeddings with multimodal neural language models", "author": ["R. Kiros", "R. Salakhutdinov", "R.S. Zemel"], "venue": "Transactions of the Association for Computational Linguistics (TACL)", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2015}, {"title": "Natural language description of human activities from video images based on concept hierarchy of actions", "author": ["A. Kojima", "T. Tamura", "K. Fukunaga"], "venue": "International Journal of Computer Vision (IJCV)", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2002}, {"title": "Baby talk: Understanding and generating simple image descriptions", "author": ["G. Kulkarni", "V. Premraj", "S. Dhar", "S. Li", "Y. Choi", "A.C. Berg", "T.L. Berg"], "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2011}, {"title": "Treetalk: Composition and compression of trees for image descriptions", "author": ["P. Kuznetsova", "V. Ordonez", "T.L. Berg", "U.C. Hill", "Y. Choi"], "venue": "Transactions of the Association for Computational Linguistics (TACL)", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2014}, {"title": "Meteor universal: Language specific translation evaluation for any target language", "author": ["M.D.A. Lavie"], "venue": "ACL 2014 p. 376", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2014}, {"title": "Rouge: A package for automatic evaluation of summaries", "author": ["C.Y. Lin"], "venue": "Text Summarization Branches Out: Proceedings of the ACL-04 Workshop. pp. 74\u201381", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2004}, {"title": "Deep captioning with multimodal recurrent neural networks (m-rnn)", "author": ["J. Mao", "W. Xu", "Y. Yang", "J. Wang", "A.L. Yuille"], "venue": "arXiv:1412.6632", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2014}, {"title": "Midge: Generating image descriptions from computer vision detections", "author": ["M. Mitchell", "J. Dodge", "A. Goyal", "K. Yamaguchi", "K. Stratos", "X. Han", "A. Mensch", "A.C. Berg", "T.L. Berg", "H.D. III"], "venue": "Proceedings of the Conference of the European Chapter of the Association for Computational Linguistics (EACL)", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2012}, {"title": "Jointly modeling embedding and translation to bridge video and language", "author": ["Y. Pan", "T. Mei", "T. Yao", "H. Li", "Y. Rui"], "venue": "arXiv:1505.01861", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2015}, {"title": "BLEU: a method for automatic evaluation of machine translation", "author": ["K. Papineni", "S. Roukos", "T. Ward", "W.J. Zhu"], "venue": "Proceedings of the Annual Meeting of the Association for Computational Linguistics (ACL)", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2002}, {"title": "Coherent multi-sentence video description with variable level of detail", "author": ["A. Rohrbach", "M. Rohrbach", "W. Qiu", "A. Friedrich", "M. Pinkal", "B. Schiele"], "venue": "Proceedings of the German Confeence on Pattern Recognition (GCPR)", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2014}, {"title": "A dataset for movie description", "author": ["A. Rohrbach", "M. Rohrbach", "N. Tandon", "B. Schiele"], "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2015}, {"title": "Translating video content to natural language descriptions", "author": ["M. Rohrbach", "W. Qiu", "I. Titov", "S. Thater", "M. Pinkal", "B. Schiele"], "venue": "Proceedings of the IEEE International Conference on Computer Vision (ICCV)", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2013}, {"title": "Integrating language and vision to generate natural language descriptions of videos in the wild", "author": ["J. Thomason", "S. Venugopalan", "S. Guadarrama", "K. Saenko", "R.J. Mooney"], "venue": "Proceedings of the International Conference on Computational Linguistics (COLING)", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2014}, {"title": "Using descriptive video services to create a large data source for video annotation research", "author": ["A. Torabi", "C. Pal", "H. Larochelle", "A. Courville"], "venue": "arXiv:1503.01070v1", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2015}, {"title": "Cider: Consensus-based image description evaluation", "author": ["R. Vedantam", "C.L. Zitnick", "D. Parikh"], "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2015}, {"title": "Sequence to sequence \u2013 video to text", "author": ["S. Venugopalan", "M. Rohrbach", "J. Donahue", "R. Mooney", "T. Darrell", "K. Saenko"], "venue": "arXiv:1505.00487", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2015}, {"title": "Translating videos to natural language using deep recurrent neural networks", "author": ["S. Venugopalan", "H. Xu", "J. Donahue", "M. Rohrbach", "R. Mooney", "K. Saenko"], "venue": "Proceedings of the Conference of the North American Chapter of the Association for Computational Linguistics (NAACL)", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2015}, {"title": "Show and tell: A neural image caption generator", "author": ["O. Vinyals", "A. Toshev", "S. Bengio", "D. Erhan"], "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)", "citeRegEx": "35", "shortCiteRegEx": null, "year": 2015}, {"title": "Action recognition with improved trajectories", "author": ["H. Wang", "C. Schmid"], "venue": "Proceedings of the IEEE International Conference on Computer Vision (ICCV)", "citeRegEx": "36", "shortCiteRegEx": null, "year": 2013}, {"title": "Show, attend and tell: Neural image caption generation with visual attention", "author": ["K. Xu", "J. Ba", "R. Kiros", "A. Courville", "R. Salakhutdinov", "R. Zemel", "Y. Bengio"], "venue": "arXiv:1502.03044", "citeRegEx": "37", "shortCiteRegEx": null, "year": 2015}, {"title": "Jointly modeling deep video and compositional text to bridge vision and language in a unified framework", "author": ["R. Xu", "C. Xiong", "W. Chen", "J.J. Corso"], "venue": "Proceedings of the Conference on Artificial Intelligence (AAAI)", "citeRegEx": "38", "shortCiteRegEx": null, "year": 2015}, {"title": "Describing videos by exploiting temporal structure", "author": ["L. Yao", "A. Torabi", "K. Cho", "N. Ballas", "C. Pal", "H. Larochelle", "A. Courville"], "venue": "arXiv:1502.08029v4", "citeRegEx": "39", "shortCiteRegEx": null, "year": 2015}, {"title": "From image descriptions to visual denotations: New similarity metrics for semantic inference over event descriptions", "author": ["P. Young", "A. Lai", "M. Hodosh", "J. Hockenmaier"], "venue": "Transactions of the Association for Computational Linguistics (TACL) 2, 67\u201378", "citeRegEx": "40", "shortCiteRegEx": null, "year": 2014}, {"title": "Learning Deep Features for Scene Recognition using Places Database", "author": ["B. Zhou", "A. Lapedriza", "J. Xiao", "A. Torralba", "A. Oliva"], "venue": "Advances in Neural Information Processing Systems (NIPS)", "citeRegEx": "41", "shortCiteRegEx": null, "year": 2014}], "referenceMentions": [{"referenceID": 27, "context": "The recent advances in image captioning as well as the release of large-scale movie description datasets such as MPII-MD [28] allow to study this task in more depth.", "startOffset": 121, "endOffset": 125}, {"referenceID": 5, "context": "Multiple works have successfully addressed the image captioning problem [6, 16, 17, 35].", "startOffset": 72, "endOffset": 87}, {"referenceID": 15, "context": "Multiple works have successfully addressed the image captioning problem [6, 16, 17, 35].", "startOffset": 72, "endOffset": 87}, {"referenceID": 16, "context": "Multiple works have successfully addressed the image captioning problem [6, 16, 17, 35].", "startOffset": 72, "endOffset": 87}, {"referenceID": 34, "context": "Multiple works have successfully addressed the image captioning problem [6, 16, 17, 35].", "startOffset": 72, "endOffset": 87}, {"referenceID": 12, "context": "Many of the proposed methods rely on Long-Short Term Memory networks (LSTMs) [13].", "startOffset": 77, "endOffset": 81}, {"referenceID": 27, "context": "In the meanwhile, two large-scale movie description datasets have been proposed, namely MPII Movie Description (MPIIMD) [28] and Montreal Video Annotation Dataset (M-VAD) [31].", "startOffset": 120, "endOffset": 124}, {"referenceID": 30, "context": "In the meanwhile, two large-scale movie description datasets have been proposed, namely MPII Movie Description (MPIIMD) [28] and Montreal Video Annotation Dataset (M-VAD) [31].", "startOffset": 171, "endOffset": 175}, {"referenceID": 27, "context": "Works addressing these datasets [28, 33, 39] show that they are indeed challenging in terms of visual recognition and automatic description.", "startOffset": 32, "endOffset": 44}, {"referenceID": 32, "context": "Works addressing these datasets [28, 33, 39] show that they are indeed challenging in terms of visual recognition and automatic description.", "startOffset": 32, "endOffset": 44}, {"referenceID": 38, "context": "Works addressing these datasets [28, 33, 39] show that they are indeed challenging in terms of visual recognition and automatic description.", "startOffset": 32, "endOffset": 44}, {"referenceID": 1, "context": "MSVD [2]), but a detailed analysis of the difficulties is missing.", "startOffset": 5, "endOffset": 8}, {"referenceID": 8, "context": "Automatic image description has been studied in the past [9, 19, 20, 24], however it regained attention just recently.", "startOffset": 57, "endOffset": 72}, {"referenceID": 18, "context": "Automatic image description has been studied in the past [9, 19, 20, 24], however it regained attention just recently.", "startOffset": 57, "endOffset": 72}, {"referenceID": 19, "context": "Automatic image description has been studied in the past [9, 19, 20, 24], however it regained attention just recently.", "startOffset": 57, "endOffset": 72}, {"referenceID": 23, "context": "Automatic image description has been studied in the past [9, 19, 20, 24], however it regained attention just recently.", "startOffset": 57, "endOffset": 72}, {"referenceID": 5, "context": "Multiple works have been proposed [6, 8, 16, 17, 23, 35, 37].", "startOffset": 34, "endOffset": 60}, {"referenceID": 7, "context": "Multiple works have been proposed [6, 8, 16, 17, 23, 35, 37].", "startOffset": 34, "endOffset": 60}, {"referenceID": 15, "context": "Multiple works have been proposed [6, 8, 16, 17, 23, 35, 37].", "startOffset": 34, "endOffset": 60}, {"referenceID": 16, "context": "Multiple works have been proposed [6, 8, 16, 17, 23, 35, 37].", "startOffset": 34, "endOffset": 60}, {"referenceID": 22, "context": "Multiple works have been proposed [6, 8, 16, 17, 23, 35, 37].", "startOffset": 34, "endOffset": 60}, {"referenceID": 34, "context": "Multiple works have been proposed [6, 8, 16, 17, 23, 35, 37].", "startOffset": 34, "endOffset": 60}, {"referenceID": 36, "context": "Multiple works have been proposed [6, 8, 16, 17, 23, 35, 37].", "startOffset": 34, "endOffset": 60}, {"referenceID": 39, "context": "Also new datasets have been released, Flickr30k [40] and MS COCO Captions [3], where [3] additionally presents a standardized setup for image captioning evaluation.", "startOffset": 48, "endOffset": 52}, {"referenceID": 2, "context": "Also new datasets have been released, Flickr30k [40] and MS COCO Captions [3], where [3] additionally presents a standardized setup for image captioning evaluation.", "startOffset": 74, "endOffset": 77}, {"referenceID": 2, "context": "Also new datasets have been released, Flickr30k [40] and MS COCO Captions [3], where [3] additionally presents a standardized setup for image captioning evaluation.", "startOffset": 85, "endOffset": 88}, {"referenceID": 4, "context": "[5] compares them with respect to the novelty of generated descriptions and additionally proposes a nearest neighbor baseline that improves over recent methods.", "startOffset": 0, "endOffset": 3}, {"referenceID": 0, "context": "In the past video description has been addressed in semirealistic settings [1, 18], on a small scale [4, 11, 30] or in constrained scenarios like cooking [27, 29].", "startOffset": 75, "endOffset": 82}, {"referenceID": 17, "context": "In the past video description has been addressed in semirealistic settings [1, 18], on a small scale [4, 11, 30] or in constrained scenarios like cooking [27, 29].", "startOffset": 75, "endOffset": 82}, {"referenceID": 3, "context": "In the past video description has been addressed in semirealistic settings [1, 18], on a small scale [4, 11, 30] or in constrained scenarios like cooking [27, 29].", "startOffset": 101, "endOffset": 112}, {"referenceID": 10, "context": "In the past video description has been addressed in semirealistic settings [1, 18], on a small scale [4, 11, 30] or in constrained scenarios like cooking [27, 29].", "startOffset": 101, "endOffset": 112}, {"referenceID": 29, "context": "In the past video description has been addressed in semirealistic settings [1, 18], on a small scale [4, 11, 30] or in constrained scenarios like cooking [27, 29].", "startOffset": 101, "endOffset": 112}, {"referenceID": 26, "context": "In the past video description has been addressed in semirealistic settings [1, 18], on a small scale [4, 11, 30] or in constrained scenarios like cooking [27, 29].", "startOffset": 154, "endOffset": 162}, {"referenceID": 28, "context": "In the past video description has been addressed in semirealistic settings [1, 18], on a small scale [4, 11, 30] or in constrained scenarios like cooking [27, 29].", "startOffset": 154, "endOffset": 162}, {"referenceID": 26, "context": "[27]) study the task of describing a short clip with a single sentence.", "startOffset": 0, "endOffset": 4}, {"referenceID": 5, "context": "[6] first proposed to describe videos using an LSTM, relying on precomputed CRF scores from [27].", "startOffset": 0, "endOffset": 3}, {"referenceID": 26, "context": "[6] first proposed to describe videos using an LSTM, relying on precomputed CRF scores from [27].", "startOffset": 92, "endOffset": 96}, {"referenceID": 33, "context": "[34] extended this work to extract CNN features from frames which are max-pooled over time.", "startOffset": 0, "endOffset": 4}, {"referenceID": 24, "context": "[25] proposes a framework that consists of a 2-D and/or 3-D CNN and the LSTM is trained jointly with a visual-semantic embedding to ensure better coherence between video and text.", "startOffset": 0, "endOffset": 4}, {"referenceID": 37, "context": "[38] jointly addresses the language generation and video/language retrieval tasks by learning a joint embedding model for a deep video model and compositional semantic language model.", "startOffset": 0, "endOffset": 4}, {"referenceID": 27, "context": "Recently two large-scale movie description datasets have been proposed, MPII Movie Description (MPII-MD) [28] and Montreal Video Annotation Dataset (M-VAD) [31].", "startOffset": 105, "endOffset": 109}, {"referenceID": 30, "context": "Recently two large-scale movie description datasets have been proposed, MPII Movie Description (MPII-MD) [28] and Montreal Video Annotation Dataset (M-VAD) [31].", "startOffset": 156, "endOffset": 160}, {"referenceID": 26, "context": "TACoS Multi-Level [27], thus one has to rely on the weak annotations of the sentence descriptions.", "startOffset": 18, "endOffset": 22}, {"referenceID": 38, "context": "To handle this challenging scenario [39] proposes an attention based model which selects the most relevant temporal segments in a video and incorporates 3-D CNN and generates a sentence using an LSTM.", "startOffset": 36, "endOffset": 40}, {"referenceID": 32, "context": "[33] proposes an encoder-decoder framework, where a single LSTM encodes the input video frame by frame and decodes it into a sentence, outperforming [39].", "startOffset": 0, "endOffset": 4}, {"referenceID": 38, "context": "[33] proposes an encoder-decoder framework, where a single LSTM encodes the input video frame by frame and decodes it into a sentence, outperforming [39].", "startOffset": 149, "endOffset": 153}, {"referenceID": 5, "context": "Our approach for sentence generation is most similar to [6] and we also rely on their LSTM implementation based on Caffe [15].", "startOffset": 56, "endOffset": 59}, {"referenceID": 14, "context": "Our approach for sentence generation is most similar to [6] and we also rely on their LSTM implementation based on Caffe [15].", "startOffset": 121, "endOffset": 125}, {"referenceID": 27, "context": "To extract labels from sentences we rely on the semantic parser of [28], however we treat the labels differently to handle the weak supervision (see Section 3.", "startOffset": 67, "endOffset": 71}, {"referenceID": 27, "context": "We show that this improves over [28] and [33].", "startOffset": 32, "endOffset": 36}, {"referenceID": 32, "context": "We show that this improves over [28] and [33].", "startOffset": 41, "endOffset": 45}, {"referenceID": 35, "context": "We first train the visual classifiers for verbs, objects and places, using different visual features: DT (dense trajectories [36]), LSDA (large scale object detector [14]) and PLACES (Places-CNN [41]).", "startOffset": 125, "endOffset": 129}, {"referenceID": 13, "context": "We first train the visual classifiers for verbs, objects and places, using different visual features: DT (dense trajectories [36]), LSDA (large scale object detector [14]) and PLACES (Places-CNN [41]).", "startOffset": 166, "endOffset": 170}, {"referenceID": 40, "context": "We first train the visual classifiers for verbs, objects and places, using different visual features: DT (dense trajectories [36]), LSDA (large scale object detector [14]) and PLACES (Places-CNN [41]).", "startOffset": 195, "endOffset": 199}, {"referenceID": 5, "context": "For the language generation we rely on a LSTM network which has been successfully used for image and video description [6, 33].", "startOffset": 119, "endOffset": 126}, {"referenceID": 32, "context": "For the language generation we rely on a LSTM network which has been successfully used for image and video description [6, 33].", "startOffset": 119, "endOffset": 126}, {"referenceID": 27, "context": "As in [28] we parse the sentences to obtain a set of labels (single words or short phrases, e.", "startOffset": 6, "endOffset": 10}, {"referenceID": 27, "context": "However, in contrast to [28] we do not want to keep all of these initial labels as they are noisy, but select only visual ones which actually can be robustly recognized.", "startOffset": 24, "endOffset": 28}, {"referenceID": 35, "context": "(DT) [36] for verbs, large scale object detector responses (LSDA) [14] for objects and scene classification scores (PLACES) [41] for places.", "startOffset": 5, "endOffset": 9}, {"referenceID": 13, "context": "(DT) [36] for verbs, large scale object detector responses (LSDA) [14] for objects and scene classification scores (PLACES) [41] for places.", "startOffset": 66, "endOffset": 70}, {"referenceID": 40, "context": "(DT) [36] for verbs, large scale object detector responses (LSDA) [14] for objects and scene classification scores (PLACES) [41] for places.", "startOffset": 124, "endOffset": 128}, {"referenceID": 27, "context": "Now, how do we select visual labels for our semantic groups? In order to find the verbs among the labels we rely on the semantic parser of [28].", "startOffset": 139, "endOffset": 143}, {"referenceID": 40, "context": "Next, we look up the list of \u201cplaces\u201d used in [41] and search for corresponding words among our labels.", "startOffset": 46, "endOffset": 50}, {"referenceID": 13, "context": "We look up the object classes used in [14] and search for these \u201cobjects\u201d, as well as their base forms (e.", "startOffset": 38, "endOffset": 42}, {"referenceID": 5, "context": "We rely on the basic LSTM architecture proposed in [6] for video description.", "startOffset": 51, "endOffset": 54}, {"referenceID": 5, "context": "[6] compares three variants: (a) an encoder-decoder architecture, (b) a decoder architecture with visual max predictions, and (c) a decoder architecture with visual probabilistic predictions.", "startOffset": 0, "endOffset": 3}, {"referenceID": 5, "context": "In the 2-layer architecture, the output of the first layer is used as input for the second layer (Figure 2b) and was used by [6] for video description.", "startOffset": 125, "endOffset": 128}, {"referenceID": 5, "context": "we also compare to a 2-layer factored architecture [6], where the first layer only gets the language as input and the second gets the output of the first layer as well as the visual input.", "startOffset": 51, "endOffset": 54}, {"referenceID": 11, "context": "Dropout placement: To learn a more robust network which is less likely to overfit we rely on a dropout [12].", "startOffset": 103, "endOffset": 107}, {"referenceID": 14, "context": "The polynomial learning strategy has been shown to give good results faster without tweaking step size for GoogleNet implemented by Sergio Guadarrama in Caffe [15].", "startOffset": 159, "endOffset": 163}, {"referenceID": 27, "context": "In this section we first analyze our approach on the MPII-MD [28] dataset and explore different design choices.", "startOffset": 61, "endOffset": 65}, {"referenceID": 27, "context": "We build on the labels discovered by our semantic parser [28] and additionally match these labels to sentences which the parser failed to process.", "startOffset": 57, "endOffset": 61}, {"referenceID": 27, "context": "We use the visual features (DT, LSDA, PLACES) provided with the MPII-MD dataset [28].", "startOffset": 80, "endOffset": 84}, {"referenceID": 20, "context": "We evaluate our method on the validation set (4,930 clips) using the METEOR [21] score, which, according to [7, 32], supersedes other popular measures, such as BLEU [26], ROUGE [22], in terms of agreement with human judgments.", "startOffset": 76, "endOffset": 80}, {"referenceID": 6, "context": "We evaluate our method on the validation set (4,930 clips) using the METEOR [21] score, which, according to [7, 32], supersedes other popular measures, such as BLEU [26], ROUGE [22], in terms of agreement with human judgments.", "startOffset": 108, "endOffset": 115}, {"referenceID": 31, "context": "We evaluate our method on the validation set (4,930 clips) using the METEOR [21] score, which, according to [7, 32], supersedes other popular measures, such as BLEU [26], ROUGE [22], in terms of agreement with human judgments.", "startOffset": 108, "endOffset": 115}, {"referenceID": 25, "context": "We evaluate our method on the validation set (4,930 clips) using the METEOR [21] score, which, according to [7, 32], supersedes other popular measures, such as BLEU [26], ROUGE [22], in terms of agreement with human judgments.", "startOffset": 165, "endOffset": 169}, {"referenceID": 21, "context": "We evaluate our method on the validation set (4,930 clips) using the METEOR [21] score, which, according to [7, 32], supersedes other popular measures, such as BLEU [26], ROUGE [22], in terms of agreement with human judgments.", "startOffset": 177, "endOffset": 181}, {"referenceID": 31, "context": "The authors of CIDEr [32] showed that METEOR also outperforms CIDEr when the number of references is small and in the case of MPII-MD we have typically only a single reference.", "startOffset": 21, "endOffset": 25}, {"referenceID": 27, "context": "We compare the best method of [28], the recently proposed method S2VT [33] and our proposed \u201cVisual Labels\u201d-LSTM on the test set of the MPII-MD dataset (6,578 clips).", "startOffset": 30, "endOffset": 34}, {"referenceID": 32, "context": "We compare the best method of [28], the recently proposed method S2VT [33] and our proposed \u201cVisual Labels\u201d-LSTM on the test set of the MPII-MD dataset (6,578 clips).", "startOffset": 70, "endOffset": 74}, {"referenceID": 31, "context": "We report all popular automatic evaluation measures, CIDEr [32], BLEU [26], ROUGE [22] and METEOR [21], computed using the evaluation code of [3].", "startOffset": 59, "endOffset": 63}, {"referenceID": 25, "context": "We report all popular automatic evaluation measures, CIDEr [32], BLEU [26], ROUGE [22] and METEOR [21], computed using the evaluation code of [3].", "startOffset": 70, "endOffset": 74}, {"referenceID": 21, "context": "We report all popular automatic evaluation measures, CIDEr [32], BLEU [26], ROUGE [22] and METEOR [21], computed using the evaluation code of [3].", "startOffset": 82, "endOffset": 86}, {"referenceID": 20, "context": "We report all popular automatic evaluation measures, CIDEr [32], BLEU [26], ROUGE [22] and METEOR [21], computed using the evaluation code of [3].", "startOffset": 98, "endOffset": 102}, {"referenceID": 2, "context": "We report all popular automatic evaluation measures, CIDEr [32], BLEU [26], ROUGE [22] and METEOR [21], computed using the evaluation code of [3].", "startOffset": 142, "endOffset": 145}, {"referenceID": 27, "context": "We also perform a human evaluation, by randomly selecting 1300 video snippets and asking AMT turkers to rank three systems (the best SMT of [28], S2VT [33] and ours) with respect to Correctness, Grammar and Relevance, similar to [28].", "startOffset": 140, "endOffset": 144}, {"referenceID": 32, "context": "We also perform a human evaluation, by randomly selecting 1300 video snippets and asking AMT turkers to rank three systems (the best SMT of [28], S2VT [33] and ours) with respect to Correctness, Grammar and Relevance, similar to [28].", "startOffset": 151, "endOffset": 155}, {"referenceID": 27, "context": "We also perform a human evaluation, by randomly selecting 1300 video snippets and asking AMT turkers to rank three systems (the best SMT of [28], S2VT [33] and ours) with respect to Correctness, Grammar and Relevance, similar to [28].", "startOffset": 229, "endOffset": 233}, {"referenceID": 27, "context": "While we rely on identical features and similar labels as [28], we significantly improve the performance in all automatic measures, specifically by 1.", "startOffset": 58, "endOffset": 62}, {"referenceID": 27, "context": "Best SMT of [28] 8.", "startOffset": 12, "endOffset": 16}, {"referenceID": 32, "context": "08 S2VT [33] 9.", "startOffset": 8, "endOffset": 12}, {"referenceID": 32, "context": "Moreover, we improve over the recent approach of [33], which also uses LSTM to generate video descriptions.", "startOffset": 49, "endOffset": 53}, {"referenceID": 27, "context": "An interesting characteristic is the output vocabulary size, which is 94 for [28], 86 for [33] and 605 for our method, while the test set contains 6422 unique words.", "startOffset": 77, "endOffset": 81}, {"referenceID": 32, "context": "An interesting characteristic is the output vocabulary size, which is 94 for [28], 86 for [33] and 605 for our method, while the test set contains 6422 unique words.", "startOffset": 90, "endOffset": 94}, {"referenceID": 27, "context": "Despite the recent advances in the video description domain, including our proposed approach, the video description performance on the movie description datasets (MPII-MD [28] and M-VAD [31]) remains relatively low.", "startOffset": 171, "endOffset": 175}, {"referenceID": 30, "context": "Despite the recent advances in the video description domain, including our proposed approach, the video description performance on the movie description datasets (MPII-MD [28] and M-VAD [31]) remains relatively low.", "startOffset": 186, "endOffset": 190}, {"referenceID": 27, "context": "In this section we want to take a closer look at three methods, best SMT of [28], S2VT [33] and ours, in order to understand where these methods succeed and where they fail.", "startOffset": 76, "endOffset": 80}, {"referenceID": 32, "context": "In this section we want to take a closer look at three methods, best SMT of [28], S2VT [33] and ours, in order to understand where these methods succeed and where they fail.", "startOffset": 87, "endOffset": 91}, {"referenceID": 27, "context": "SMT [28] Someone is a man, someone is a man.", "startOffset": 4, "endOffset": 8}, {"referenceID": 32, "context": "S2VT [33] Someone looks at him, someone turns to someone.", "startOffset": 5, "endOffset": 9}, {"referenceID": 27, "context": "SMT [28] The car is a water of the water.", "startOffset": 4, "endOffset": 8}, {"referenceID": 32, "context": "S2VT [33] On the door, opens the door opens.", "startOffset": 5, "endOffset": 9}, {"referenceID": 27, "context": "SMT [28] Someone is down the door, someone is a back of the door, and someone is a door.", "startOffset": 4, "endOffset": 8}, {"referenceID": 32, "context": "S2VT [33] Someone shakes his head and looks at someone.", "startOffset": 5, "endOffset": 9}, {"referenceID": 27, "context": "<\u2212short Sentences long\u2212> M E T E O R Best SMT of [28] S2VT [33] Our", "startOffset": 49, "endOffset": 53}, {"referenceID": 32, "context": "<\u2212short Sentences long\u2212> M E T E O R Best SMT of [28] S2VT [33] Our", "startOffset": 59, "endOffset": 63}, {"referenceID": 27, "context": "<\u2212short Sentences long\u2212> M E T E O R Best SMT of [28] S2VT [33] Our", "startOffset": 49, "endOffset": 53}, {"referenceID": 32, "context": "<\u2212short Sentences long\u2212> M E T E O R Best SMT of [28] S2VT [33] Our", "startOffset": 59, "endOffset": 63}, {"referenceID": 27, "context": "<\u2212easy Sentences difficult\u2212> M E T E O R Best SMT of [28] S2VT [33] Our", "startOffset": 53, "endOffset": 57}, {"referenceID": 32, "context": "<\u2212easy Sentences difficult\u2212> M E T E O R Best SMT of [28] S2VT [33] Our", "startOffset": 63, "endOffset": 67}, {"referenceID": 27, "context": "<\u2212easy Sentences difficult\u2212> M E T E O R Best SMT of [28] S2VT [33] Our", "startOffset": 53, "endOffset": 57}, {"referenceID": 32, "context": "<\u2212easy Sentences difficult\u2212> M E T E O R Best SMT of [28] S2VT [33] Our", "startOffset": 63, "endOffset": 67}, {"referenceID": 27, "context": "w e a th e r (7 ) M E T E O R Best SMT of [28] S2VT [33] Our", "startOffset": 42, "endOffset": 46}, {"referenceID": 32, "context": "w e a th e r (7 ) M E T E O R Best SMT of [28] S2VT [33] Our", "startOffset": 52, "endOffset": 56}, {"referenceID": 9, "context": "\u201cmotion\u201d, \u201cperception\u201d, \u201ccompetition\u201d, \u201cemotion\u201d), defined for most synsets in WordNet [10].", "startOffset": 87, "endOffset": 91}, {"referenceID": 27, "context": "We obtain the sense information from the semantic parser of [28], thus senses might be noisy.", "startOffset": 60, "endOffset": 64}, {"referenceID": 27, "context": "We find that our method is best for all topics except \u201ccommunication\u201d, where [28] wins.", "startOffset": 77, "endOffset": 81}, {"referenceID": 27, "context": "The most frequent verbs in this topic are \u201clook up\u201d and \u201cnod\u201d, which are also frequent in the dataset and in the sentences produced by [28].", "startOffset": 135, "endOffset": 139}], "year": 2015, "abstractText": "Generating descriptions for videos has many applications including assisting blind people and human-robot interaction. The recent advances in image captioning as well as the release of large-scale movie description datasets such as MPII-MD [28] allow to study this task in more depth. Many of the proposed methods for image captioning rely on pre-trained object classifier CNNs and Long-Short Term Memory recurrent networks (LSTMs) for generating descriptions. While image description focuses on objects, we argue that it is important to distinguish verbs, objects, and places in the challenging setting of movie description. In this work we show how to learn robust visual classifiers from the weak annotations of the sentence descriptions. Based on these visual classifiers we learn how to generate a description using an LSTM. We explore different design choices to build and train the LSTM and achieve the best performance to date on the challenging MPII-MD dataset. We compare and analyze our approach and prior work along various dimensions to better understand the key challenges of the movie description task.", "creator": "TeX"}}}