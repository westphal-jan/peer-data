{"id": "1506.02075", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "5-Jun-2015", "title": "Large-scale Simple Question Answering with Memory Networks", "abstract": "Training large-scale question answering systems is complicated because training sources usually cover a small portion of the range of possible questions. This paper studies the impact of multitask and transfer learning for simple question answering; a setting for which the reasoning required to answer is quite easy, as long as one can retrieve the correct evidence given a question, which can be difficult in large-scale conditions. To this end, we introduce a new dataset of 100k questions that we use in conjunction with existing benchmarks. We conduct our study within the framework of Memory Networks (Weston et al., 2015) because this perspective allows us to eventually scale up to more complex reasoning, and show that Memory Networks can be successfully trained to achieve excellent performance.", "histories": [["v1", "Fri, 5 Jun 2015 21:48:39 GMT  (31kb)", "http://arxiv.org/abs/1506.02075v1", null]], "reviews": [], "SUBJECTS": "cs.LG cs.CL", "authors": ["antoine bordes", "nicolas usunier", "sumit chopra", "jason weston"], "accepted": false, "id": "1506.02075"}, "pdf": {"name": "1506.02075.pdf", "metadata": {"source": "CRF", "title": null, "authors": [], "emails": ["abordes@fb.com", "usunier@fb.com", "jase}@fb.com"], "sections": [{"heading": null, "text": "ar Xiv: 150 6.02 075v 1 [cs.L G] 5J un2 015"}, {"heading": "1 Introduction", "text": "This year is the highest in the history of the country."}, {"heading": "2 Simple Question Answering", "text": "Knowledge databases contain facts expressed as triples (subject, relationship, object), where subject and object are units, and relationship describes the nature of the (directed) connection between these units. The simple question we are addressing here is to find the answer to questions that can be reformulated as queries of form (subject, relationship,?), asking for all objects that are related to subject by relationship. For example, the question What do Jamaicans speak? could be rephrased as a freebase query (Jamaica, spoken language,?). In other words, retrieving a single fact from a KB is sufficient to answer correctly. The term simple QA refers to the simplicity of the reasoning process required to answer questions, as it is a single fact. However, this does not mean that the QA problem per se is simple, as querying this single supporting fact can be very difficult, as it requires the search for alternatives to express the number of KB's natural problems in addition to a multiplicity of languages."}, {"heading": "2.1 Knowledge Bases", "text": "We use KB Freebase1 as the foundation of our QA system, our source of facts and answers; all freebase units and relationships are typed in and the lexicon of types and relationships is closed; Freebase data is collectively collected and curated to ensure a high reliability of facts; each unit has an internal identifier and a set of strings that are normally used to refer to that unit in the text, so-called aliases; we look at two excerpts from Freebase whose statistics are in Table 2. FB2M, used in (Bordes et al., 2014a), contains about 2M units and 5k relationships; FB5M is much larger, with about 5M units and more than 7.5k relationships; we also use KB Reverb as a secondary source of facts to examine how good a model that has been trained to answer questions with freebase facts."}, {"heading": "2.2 The SimpleQuestions dataset", "text": "Existing resources for QA such as WebQuestions (Berant et al., 2013) are rather small (a few thousand questions away) and therefore do not provide a very thorough coverage of the multitude of questions that could be answered with a KB like Freebase, even in the context of simple QA. Therefore, in this paper we maintain a new dataset of much larger scale for the task of simple QA called SimpleQuestions.2 This dataset consists of a total of 108,442 questions written in natural language by human English-speaking annotators, each paired with a corresponding FB2M fact that provides the answer and explains it. We shuffle these questions randomly, using 70% of them as a training set, 10% as a validation set (10845), and the remaining 20% as a test set. Examples of questions and facts are given in Table 1.We collected SimpleQuestions in two phases."}, {"heading": "3 Memory Networks for Simple QA", "text": "A storage network consists of a memory (an indexed array of objects) and a neural network that is trained to query it based on some input (usually questions). It consists of four components: Input Map (I), Generalization (G), Output Map (O), and Response (R), which we will discuss in more detail below. But first, we describe the MemNs workflow used to build a model for simple QA. This is done in three steps: 1. Freebase storage: This first phase analyzes Freebase (either FB2M or FB5M depending on the setting) and stores it in memory. It uses the input module to pre-process the data. 2. Training: This second phase trains the MemNN to answer the question, using input, output, and response modules, with the training mainly using the parameters of the embedding model at the core of the output module."}, {"heading": "3.1 Input module", "text": "In fact, it is in such a way that the greater of them will be able to be in a position to be in a position to be in a position to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to fight, to fight, to fight, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to fight, to move, to move, to move, to fight, to move, to fight, to move, to move, to move, to fight, to move, to move, to move, to fight, to move, to move, to move, to move, to move, to fight, to move, to move, to move, to move, to move, to move, to fight, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to"}, {"heading": "3.2 Generalization module", "text": "This module is responsible for adding new elements to memory. In our case, the memory has a multigraph structure in which each node is a freebase entity and labeled slurs are freebase relations in the multigraph: after pre-processing, all freebase facts are stored using that structure. We also consider the case in which new facts are made available to MemNNN with a different structure (i.e. new types of relationships) by using freebase entities. In this case, we use the generalization module to link reverb facts to the freebase-based memory structure in order to make them usable and searchable through the MemNN. To associate the subject and object of a reverb fact with freebase entities, we use precalculated entity links (Lin et al., 2012). If such links do not yield results for an entity, we look for freebase entries that match at least one of the aliases."}, {"heading": "3.3 Output module", "text": "In our case of simple QA, this module provides only one supporting fact. To avoid all stored facts being evaluated, we first perform an approximate association of words of the question with aliases of freebase entities and select a few matching entities. All facts that have one of these entities as subject are evaluated in a second step. We first create all possible n-grams from the question by removing those that contain a question pronoun or 1-gram of the question belonging to a list of stopwords. We keep only the n-grams, which are an alias of one entity, and then discard all n-grams that are a sub-sequence of another n-gram."}, {"heading": "3.4 Response module", "text": "In storage networks, the response module processes the result of the output module to calculate the desired response. In our case, it returns the set of objects of the selected supporting fact."}, {"heading": "4 Training", "text": "This section describes how we trained the scoring function of the output module using a multi-task training process on four different data sources. First, in addition to the new SimpleQuestions dataset described in Section 2, we also used WebQuestions, a benchmark for QA introduced in (Berant et al., 2013): Questions are labeled with response strings of aliases of freebase units, and many questions expect multiple answers. Table 3 describes the statistics of both datasets. We also train automatic questions generated from the KB, i.e., depending on the setting FB2M or FB5M, which are crucial for embedding the units that do not appear in WebQuestions or SimpleQuestions. Statistics on FB2M or FB5M are given in Table 2; we generated one training question per fact after the same process as in (Bordes et al., 2014a). After previous work on FB2M or FB5M are given in Table 2, we generated one training question per fact after the same process as in (Bordes et al., 2014a)."}, {"heading": "4.1 Multitask training", "text": "As in previous work on embedding models and storage networks (Bordes et al., 2014q q; Bordes et al., 2014b; Weston et al., 2015), embedding is trained with a ranking criterion. In QA datasets, the goal is that in the embedding space a supporting fact of the question is more similar than any other non-supporting fact. In the case of paraphrase datasets, a question of one of their paraphrases should be more similar than another question.Multitasking learning of the embedding matrices WV and WS is performed by alternating stochastic descent (SGD) via the loss function on the different datasets. In QA datasets with a question-supporting fact pair (q, y) and a non-supporting fact pair between QA datasets, we are taking a step towards minimizing the loss functionality QA (q, y, y)."}, {"heading": "4.2 Distant supervision", "text": "Unlike SimpleQuestions or the synthetic QA data generated from Freebase, WebQuestions only provides answer strings for questions: the supporting facts are not known. To generate the supervision, we use the fact generation algorithm of Section 3.3. For each candidate, the aliases of his objects are compared with the supplied answer strings. The fact (s) that can generate the maximum number of response strings from the aliases of their objects are then retained. If multiple facts are determined for the same question, the aliases of his objects are considered as monitoring facts with the minimum number of objects. The latter selection avoids favoring irrelevant relationships that would only sustain because they refer to many objects but would not be specific enough. If no answer strings could be found from the objects of the original candidates, the question will be discarded from the training set. Future work should highlight the process of weakly supervised training from Membaal schools, which it was recently introduced to investigate Natar (the fact)."}, {"heading": "4.3 Generating negative examples", "text": "As in (Bordes et al., 2014a; Bordes et al., 2014b), learning is done with gradient descent, so that negative examples (non-supporting facts or non-paraphrases) are generated according to a random policy during the training. To paraphrase, we use two strategies for a couple (q, q \u2032). The standard policy for obtaining a non-supporting fact is to corrupt the fact of the answer by exchanging its subject, its relationship, or its object (s) with that of another factor randomly selected from the KB. In this policy, the element of the fact of being corrupt is randomly chosen, with a low probability (0.3) that more than one element of the fact of the answer is corrupted."}, {"heading": "5 Related Work", "text": "The first approaches to open domain QA were search engine-based systems, where keywords extracted from the question are sent to a search engine and the answer is extracted from the top results (Yahya et al., 2012; Unger et al., 2012). This method was adapted to KB-based QA (Yahya et al., 2012; Unger et al., 2012) and achieved competitive results in terms of semantic parsing and embedding-based approaches (Cai and Yates, 2013; Berant et al., 2013; Kwiatkowski et al., 2013; Berant and Liang, 2014; Fader et al., 2014), performing a functional analysis of the sentence that can be interpreted as KB query. Although these approaches are difficult to learn due to the complexity of their conclusions, their advantage lies in providing a deep interpretation of the question. Some of these approaches require little to no question-answer pairs (Fader et al, the QS-based rules, 2014)."}, {"heading": "6 Experiments", "text": "This section provides a comprehensive evaluation of our MemNNs implementation versus state-of-the-art QA methods, as well as an empirical study of the impact of using multiple training sources on predictive performance."}, {"heading": "6.1 Evaluation and baselines", "text": "Table 3 describes the dimensions of the WebQuestions, SimpleQuestions, and Reverb test sets that we used for the evaluation. On WebQuestions, we evaluate using this benchmark (Berant et al., 2013; Yao and Van Durme, 2014; Berant and Liang et al., 2014; Bordes et al., 2014a; Yang et al., 2014) with respect to the F1 score defined in (Berant and Liang, 2014), which is the average of the F1 score of the predicted answers across all test questions. Since no previous result has been published on SimpleQuestions, we only compare different versions of MemNs. SimpleQuestions questions are labeled with their entire freebase fact, so we evaluate for path-level accuracy, where a prediction is correct if the subject and relationship are labeled correctly."}, {"heading": "6.2 Experimental setup", "text": "The hyperparameters were selected to maximize the F1 score of the validation set for WebQuestions, regardless of the test dataset; the embedding dimension and learning rate were selected under {64, 128, 256} and {1, 0.1,..., 1.0e \u2212 4}, respectively, and the margin was set to 0.1. For each configuration of hyperparameters, the validation set's F1score was calculated regularly during learning to perform an early stop. We tested additional configurations for our algorithm. Firstly, in the candidate configuration as negatives (negative facts are sampled from the candidate set, see Section 4), abbreviated as CANDS AS NEGS, the experimental protocol is the same as in the standard configuration, but the embedding is initialized with the best configuration of the default configuration."}, {"heading": "6.3 Results", "text": "This year it has come to the point that it will only be a matter of time before it will happen, until it does."}, {"heading": "7 Conclusion", "text": "Our results show that MemNNs, when properly trained, are capable of handling natural language and a very large amount of memory (millions of entries), thus achieving state-of-the-art technology in the popular benchmark WebQuestions. We would like to emphasize that many of our results, especially those relating to KB formatting, concern not only MemNNs, but potentially any QA system. In this paper, the new SimpleQuestions dataset was also presented, which, with 100,000 examples, is an order of magnitude larger than WebQuestions: We hope that it will promote interesting new research in the field of QA, simple or not."}], "references": [{"title": "Semantic parsing via paraphrasing", "author": ["Berant", "Liang2014] Jonathan Berant", "Percy Liang"], "venue": "In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (ACL\u201914),", "citeRegEx": "Berant et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Berant et al\\.", "year": 2014}, {"title": "Semantic parsing on Freebase from question-answer pairs", "author": ["Roy Frostig", "Percy Liang."], "venue": "Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing (EMNLP\u201913), Seattle, USA.", "citeRegEx": "Frostig and Liang.,? 2013", "shortCiteRegEx": "Frostig and Liang.", "year": 2013}, {"title": "Question answering with subgraph embeddings", "author": ["Sumit Chopra", "Jason Weston"], "venue": "In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP),", "citeRegEx": "Bordes et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Bordes et al\\.", "year": 2014}, {"title": "Open question answering with weakly supervised embedding models", "author": ["Jason Weston", "Nicolas Usunier"], "venue": "In Proceedings of the 7th European Conference on Machine Learning and Principles and Prac-", "citeRegEx": "Bordes et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Bordes et al\\.", "year": 2014}, {"title": "Large-scale semantic parsing via schema matching and lexicon extension", "author": ["Cai", "Yates2013] Qingqing Cai", "Alexander Yates"], "venue": "In Proceedings of the 51st Annual Meeting of the Association", "citeRegEx": "Cai et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Cai et al\\.", "year": 2013}, {"title": "Adaptive subgradient methods for online learning and stochastic optimization", "author": ["Duchi et al.2011] John Duchi", "Elad Hazan", "Yoram Singer"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "Duchi et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Duchi et al\\.", "year": 2011}, {"title": "Web question answering: Is more always better", "author": ["Dumais et al.2002] Susan Dumais", "Michele Banko", "Eric Brill", "Jimmy Lin", "Andrew Ng"], "venue": "In Proceedings of the 25th annual international ACM SIGIR conference on Research and development in in-", "citeRegEx": "Dumais et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Dumais et al\\.", "year": 2002}, {"title": "Identifying relations for open information extraction", "author": ["Fader et al.2011] Anthony Fader", "Stephen Soderland", "Oren Etzioni"], "venue": "In Proceedings of the Conference of Empirical Methods in Natural Language Processing (EMNLP\u201911),", "citeRegEx": "Fader et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Fader et al\\.", "year": 2011}, {"title": "Paraphrase-driven learning for open question answering", "author": ["Fader et al.2013] Anthony Fader", "Luke Zettlemoyer", "Oren Etzioni"], "venue": "In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics", "citeRegEx": "Fader et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Fader et al\\.", "year": 2013}, {"title": "Open question answering over curated and extracted knowledge bases", "author": ["Fader et al.2014] Anthony Fader", "Luke Zettlemoyer", "Oren Etzioni"], "venue": "In Proceedings of 20th SIGKDD Conference on Knowledge Discovery and Data Mining (KDD\u201914),", "citeRegEx": "Fader et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Fader et al\\.", "year": 2014}, {"title": "Scaling semantic parsers with on-the-fly ontology matching", "author": ["Eunsol Choi", "Yoav Artzi", "Luke Zettlemoyer"], "venue": "In Proceedings of the 2013 Conference", "citeRegEx": "Kwiatkowski et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Kwiatkowski et al\\.", "year": 2013}, {"title": "Entity linking at web scale. In Proceedings of the Joint Workshop on Automatic Knowledge Base Construction and Web-scale Knowledge Extraction (AKBC-WEKEX\u201912), Montreal, Canada", "author": ["Lin et al.2012] Thomas Lin", "Mausam", "Oren Etzioni"], "venue": null, "citeRegEx": "Lin et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Lin et al\\.", "year": 2012}, {"title": "Hogwild!: A lock-free approach to parallelizing stochastic gradient descent", "author": ["Recht et al.2011] Benjamin Recht", "Christopher R\u00e9", "Stephen J Wright", "Feng Niu"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "Recht et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Recht et al\\.", "year": 2011}, {"title": "Large-scale semantic parsing without question-answer pairs. Transactions of the Association for Computational Linguistics, 2:377\u2013392", "author": ["Reddy et al.2014] Siva Reddy", "Mirella Lapata", "Mark Steedman"], "venue": null, "citeRegEx": "Reddy et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Reddy et al\\.", "year": 2014}, {"title": "Weakly supervised memory networks. arXiv preprint arXiv:1503.08895", "author": ["Arthur Szlam", "Jason Weston", "Rob Fergus"], "venue": null, "citeRegEx": "Sukhbaatar et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Sukhbaatar et al\\.", "year": 2015}, {"title": "Template-based question answering over RDF data", "author": ["Lorenz B\u00fchmann", "Jens Lehmann", "Axel-Cyrille Ngonga Ngomo", "Daniel Gerber", "Philipp Cimiano"], "venue": "In Proceedings of the 21st international confer-", "citeRegEx": "Unger et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Unger et al\\.", "year": 2012}, {"title": "Overview of the trec-9 question answering track", "author": ["Voorhees", "Tice2000] Ellen M Voorhees", "DM Tice"], "venue": null, "citeRegEx": "Voorhees et al\\.,? \\Q2000\\E", "shortCiteRegEx": "Voorhees et al\\.", "year": 2000}, {"title": "Large scale image annotation: learning to rank with joint word-image embeddings", "author": ["Weston et al.2010] Jason Weston", "Samy Bengio", "Nicolas Usunier"], "venue": "Machine learning,", "citeRegEx": "Weston et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Weston et al\\.", "year": 2010}, {"title": "Natural language questions for the web of data", "author": ["Yahya et al.2012] Mohamed Yahya", "Klaus Berberich", "Shady Elbassuoni", "Maya Ramanath", "Volker Tresp", "Gerhard Weikum"], "venue": "In Proceedings of the Conference on Empirical Methods in Natural Lan-", "citeRegEx": "Yahya et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Yahya et al\\.", "year": 2012}, {"title": "Joint relational embeddings for knowledge-based question answering", "author": ["Yang et al.2014] Min-Chul Yang", "Nan Duan", "Ming Zhou", "Hae-Chang Rim"], "venue": "In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing", "citeRegEx": "Yang et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Yang et al\\.", "year": 2014}, {"title": "Information extraction over", "author": ["Yao", "Van Durme2014] Xuchen Yao", "Benjamin Van Durme"], "venue": null, "citeRegEx": "Yao et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Yao et al\\.", "year": 2014}], "referenceMentions": [{"referenceID": 6, "context": "(Voorhees and Tice, 2000; Dumais et al., 2002)), recent progress has been made with the release of large Knowledge Bases (KBs) such as Freebase, which contain consolidated knowledge stored as atomic facts, and extracted from different sources, such as free text, tables in webpages or collab-", "startOffset": 0, "endOffset": 46}, {"referenceID": 14, "context": "Second, in sections 3 and 4, we present an embedding-based QA system developed under the framework of Memory Networks (MemNNs) (Weston et al., 2015; Sukhbaatar et al., 2015).", "startOffset": 127, "endOffset": 173}, {"referenceID": 7, "context": "We used the full extraction from (Fader et al., 2011), which contains 2M entities and 600k relationships.", "startOffset": 33, "endOffset": 53}, {"referenceID": 11, "context": "To link the subject and the object of a Reverb fact to Freebase entities, we use precomputed entity links (Lin et al., 2012).", "startOffset": 106, "endOffset": 124}, {"referenceID": 8, "context": "Following previous work such as (Fader et al., 2013), we also use the indirect supervision signal of pairs of question paraphrases.", "startOffset": 32, "endOffset": 52}, {"referenceID": 9, "context": "We used a subset of the large set of paraphrases extracted from WIKIANSWERS and introduced in (Fader et al., 2014).", "startOffset": 94, "endOffset": 114}, {"referenceID": 17, "context": "We use the WARP loss (Weston et al., 2010) to speed up training, and Adagrad (Duchi et al.", "startOffset": 21, "endOffset": 42}, {"referenceID": 5, "context": ", 2010) to speed up training, and Adagrad (Duchi et al., 2011) as SGD algorithm multi-threaded with HogWild! (Recht et al.", "startOffset": 42, "endOffset": 62}, {"referenceID": 12, "context": ", 2011) as SGD algorithm multi-threaded with HogWild! (Recht et al., 2011).", "startOffset": 54, "endOffset": 74}, {"referenceID": 14, "context": "Future work should investigate the process of weak supervised training of MemNNs recently introduced in (Sukhbaatar et al., 2015) that allows to train them without any supervision coming from the supporting facts.", "startOffset": 104, "endOffset": 129}, {"referenceID": 18, "context": "The first approaches to open-domain QA were search engine-based systems, where keywords extracted from the question are sent to a search engine, and the answer is extracted from the top results (Yahya et al., 2012; Unger et al., 2012).", "startOffset": 194, "endOffset": 234}, {"referenceID": 15, "context": "The first approaches to open-domain QA were search engine-based systems, where keywords extracted from the question are sent to a search engine, and the answer is extracted from the top results (Yahya et al., 2012; Unger et al., 2012).", "startOffset": 194, "endOffset": 234}, {"referenceID": 18, "context": "This method has been adapted to KB-based QA (Yahya et al., 2012; Unger et al., 2012), and obtained competitive results with respect to semantic parsing and embedding-based approaches.", "startOffset": 44, "endOffset": 84}, {"referenceID": 15, "context": "This method has been adapted to KB-based QA (Yahya et al., 2012; Unger et al., 2012), and obtained competitive results with respect to semantic parsing and embedding-based approaches.", "startOffset": 44, "endOffset": 84}, {"referenceID": 10, "context": "Semantic parsing approaches (Cai and Yates, 2013; Berant et al., 2013; Kwiatkowski et al., 2013; Berant and Liang, 2014; Fader et al., 2014) perform a functional parse of the sentence that can be interpreted as a KB query.", "startOffset": 28, "endOffset": 140}, {"referenceID": 9, "context": "Semantic parsing approaches (Cai and Yates, 2013; Berant et al., 2013; Kwiatkowski et al., 2013; Berant and Liang, 2014; Fader et al., 2014) perform a functional parse of the sentence that can be interpreted as a KB query.", "startOffset": 28, "endOffset": 140}, {"referenceID": 8, "context": "Some of these approaches require little to no question-answer pairs (Fader et al., 2013; Reddy et al., 2014), relying on simple rules to tranform the semantic interpretation to a KB query.", "startOffset": 68, "endOffset": 108}, {"referenceID": 13, "context": "Some of these approaches require little to no question-answer pairs (Fader et al., 2013; Reddy et al., 2014), relying on simple rules to tranform the semantic interpretation to a KB query.", "startOffset": 68, "endOffset": 108}, {"referenceID": 19, "context": "The approach of (Yang et al., 2014) uses a different representation of questions, in which recognized entities are replaced by an entity token, and a different training data using entity mentions from WIKIPEDIA.", "startOffset": 16, "endOffset": 35}, {"referenceID": 19, "context": "(Berant et al., 2013; Yao and Van Durme, 2014; Berant and Liang, 2014; Bordes et al., 2014a; Yang et al., 2014) in terms of F1-score as defined in (Berant and Liang, 2014), which is the average,", "startOffset": 0, "endOffset": 111}, {"referenceID": 8, "context": "same name and introduced in (Fader et al., 2013) is used for evaluation only.", "startOffset": 28, "endOffset": 48}, {"referenceID": 8, "context": "We compare our approach to the original system (Fader et al., 2013), to (Bordes et al.", "startOffset": 47, "endOffset": 67}, {"referenceID": 9, "context": "3 n/a n/a (Fader et al., 2014) n/a n/a 54 (Bordes et al.", "startOffset": 10, "endOffset": 30}, {"referenceID": 19, "context": "9 n/a n/a (Yang et al., 2014) 41.", "startOffset": 10, "endOffset": 29}, {"referenceID": 19, "context": "The best published competing approach (Yang et al., 2014) has an F1-score of 41.", "startOffset": 38, "endOffset": 57}], "year": 2015, "abstractText": "Training large-scale question answering systems is complicated because training sources usually cover a small portion of the range of possible questions. This paper studies the impact of multitask and transfer learning for simple question answering; a setting for which the reasoning required to answer is quite easy, as long as one can retrieve the correct evidence given a question, which can be difficult in large-scale conditions. To this end, we introduce a new dataset of 100k questions that we use in conjunction with existing benchmarks. We conduct our study within the framework of Memory Networks (Weston et al., 2015) because this perspective allows us to eventually scale up to more complex reasoning, and show that Memory Networks can be successfully trained to achieve excellent performance.", "creator": "LaTeX with hyperref package"}}}