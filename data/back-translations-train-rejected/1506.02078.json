{"id": "1506.02078", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "5-Jun-2015", "title": "Visualizing and Understanding Recurrent Networks", "abstract": "Recurrent Neural Networks (RNNs), and specifically a variant with Long Short-Term Memory (LSTM), are enjoying renewed interest as a result of successful applications in a wide range of machine learning problems that involve sequential data. However, while LSTMs provide exceptional results in practice, the source of their performance and their limitations remain rather poorly understood. Using character-level language models as an interpretable testbed, we aim to bridge this gap by providing a comprehensive analysis of their representations, predictions and error types. In particular, our experiments reveal the existence of interpretable cells that keep track of long-range dependencies such as line lengths, quotes and brackets. Moreover, an extensive analysis with finite horizon n-gram models suggest that these dependencies are actively discovered and utilized by the networks. Finally, we provide detailed error analysis that suggests areas for further study.", "histories": [["v1", "Fri, 5 Jun 2015 22:33:04 GMT  (4031kb,D)", "http://arxiv.org/abs/1506.02078v1", null], ["v2", "Tue, 17 Nov 2015 02:42:24 GMT  (2671kb,D)", "http://arxiv.org/abs/1506.02078v2", "changing style, adding references, minor changes to text"]], "reviews": [], "SUBJECTS": "cs.LG cs.CL cs.NE", "authors": ["andrej karpathy", "justin johnson", "li fei-fei"], "accepted": false, "id": "1506.02078"}, "pdf": {"name": "1506.02078.pdf", "metadata": {"source": "CRF", "title": "Visualizing and Understanding Recurrent Networks", "authors": ["Andrej Karpathy", "Justin Johnson", "Li Fei-Fei"], "emails": ["karpathy@cs.stanford.edu", "jcjohns@cs.stanford.edu", "feifeili@cs.stanford.edu"], "sections": [{"heading": "1 Introduction", "text": "Recurring neural networks, in particular a variant with long short-term memory (LSTM) (14), have recently proven to be an effective model in a variety of applications that include sequential data, including speech modeling (22), handwriting recognition and generation (9), machine translation (28; 1), speech recognition (10), video analysis (7) and captions (30; 19).However, the source of their impressive performance remains unclear, raising concerns about interpretability and limiting our ability to design better architectures. Some recent ablation studies have analyzed the impact on performance by removing or modifying various gates and connections (12; 5).While this analysis illuminates the performance-critical parts of the architecture, it is still limited to measuring the Perplexity Alone test set. A commonly cited benefit of the LSTM architecture is that it can store and retrieve information over long periods of time."}, {"heading": "2 Related Work", "text": "Recurrent Networks. Recurrent Neural Networks (RNNs) have a long history of applications in various sequence learning tasks (31; 26; 25). Despite their early successes, the difficulty of training encouraged both authors to do the same. Among the most successful variants are the Long Short Term Memory Networks (14), which can store and retrieve information over long periods of time with explicit gating mechanisms and a built-in permanent error carousel. Among the most successful are the Long Short Term Memory Networks (14), which can store and retrieve information over long periods of time with explicit gating mechanisms and a built-in permanent error carousel. In recent years, there has been renewed interest in further improving the basic architecture by modifying the form of functionality as seen with gated pop recurrent units (4), with content-based soft attention mechanisms focused on our architecture (1), as well as on external stacks (18), or on a GRP architecture."}, {"heading": "3 Experimental Setup", "text": "We first describe three variants of a deep recurrent network (RNN, LSTM, and the GRU models), then explain how they are used in sequence learning, and finally describe the optimization."}, {"heading": "3.1 Recurrent Neural Network Models", "text": "The simplest beginnings of the reurrent network (RNN) has a reurrence of reecurrence. (hlt \u2212) We are dealing with a more reactive way in which we find the more reactive vectors in the more reactive way in which they find the more reactive vectors and vectors in the more reactive time in the more reactive time in the reactive time in the more reactive time in the reactive time in the reactive time in the reactive time in the more reactive time in the more reactive way in the reactive time in the reactive time in the more reactive way in which all input vectors up to this time in the reactive time in the more reactive form of recurrence (hlt \u2212 1 t). (hlt \u2212 1 t) \u2192 varies from model to model and we describe this neverted form of the reactive network (RNN)."}, {"heading": "3.2 Character-level Language Modeling", "text": "We use character-level language modeling as an interpretable test bed for sequence learning. In this setting, input into the network consists of a sequence of characters, and the network is trained to predict the next character in the sequence with a Softmax classifier at each time step. Specifically, assuming a fixed vocabulary of K characters, we encode all characters with K-dimensional 1-of-K vectors {xt}, t = 1,..., T, and enter them into the recursive network to obtain a sequence of D-dimensional hidden vectors at the last level of the network {hLt}, t = 1,..., T. To get predictions for the next character in the sequence, we project this top activation layer onto a sequence of vectors {yt}, with yt = Wyh L t and Wy being a [K \u00b7 D] parameter matrix. These vectors are interpreted so that the next sequence will hold (uninterpreted) the next probability."}, {"heading": "3.3 Optimization", "text": "After previous work (28) we initialize all parameters uniformly in the range [\u2212 0.08, 0.08]. These settings work robustly with all our models. The network is unrolled in 100 time steps. We train each model for 50 epochs and decrease the learning rate after 10 epochs by multiplying by a factor of 0.95 after each subsequent epoch. We use early termination based on validation performance and individually tick the number of dropouts for each model. Our supplementary material contains further details for efficient implementation and further experiments with other initialization areas and updates."}, {"heading": "4 Experiments", "text": "Datasets. Two datasets previously used in the context of character-level language models are the Penn Treebank dataset (20) and the Hutter Prize 100MB of the Wikipedia dataset (16). However, both datasets contain a mixture of common language and special markup. Our goal is not to compete with previous work, but to study recurring networks in a controlled environment and at both ends in terms of structural grades. Therefore, we opted for Leo Tolstoy's novel War and Peace (WP), which consists of 3,258,246 characters of almost exclusively English text with minimal markup, and at the other end of the spectrum for the source code of the Linux Kernel (LK). We randomly shuffled all header and source files and linked them into a single file to form the 6,206,996 character long dataset."}, {"heading": "4.1 Comparing Recurrent Networks", "text": "First, we train several recurring network models to support further analysis and compare their performance in a controlled environment. Specifically, we train models in cross-product type (LSTM / RNN / GRU), in number of layers (1 / 2 / 3), number of parameters (4 settings), and in both datasets (WP / KL).The 4 parameter sizes were selected in units of single-layer LSTMs with 64, 128, 256, and 512 cells. Using our character vocabulary sizes, this results in approximately 50K, 130K, 400K, and 1.3 million parameters, respectively. Sizes of the hidden layers in the other models have always been chosen to be as close as possible to the nearest prototype size. Test results are shown in Figure 1. Our consistent finding is that a depth of at least two, 400K, and 1.3 million parameters is advantageous. However, our results are mixed between two and three layers."}, {"heading": "4.2 Internal Mechanisms of an LSTM", "text": "Interpretable, far-reaching LSTM cells. LSTMs can, in principle, use their memory cells to remember far-reaching information and track various attributes of the text in which they are located. For example, it is a simple exercise to write down cell weights that would allow the cell to keep track of whether it is within a quoted string. However, to our knowledge, the existence of such cells has never been experimentally demonstrated using real-world data. In particular, it could be argued that even if LSTM is principally able to use these operations, practical optimization challenges (i.e. the dynamics of the SGD or unaffected gradients due to truncated backward propagation over time) could prevent them from discovering these solutions. In this experiment, we verify that several interpretable cells actually exist in these networks. We show several examples in Figure 2, in which we prevent the shortest backward propagation of signals."}, {"heading": "4.3 Understanding Long-Range Interactions", "text": "In fact, most of them will be able to move to another world, in which they are able, in which they are able to integrate, and in which they are able to change the world."}, {"heading": "4.4 Error Analysis: Breaking Down the Failure Cases", "text": "This year, it has reached the point where it will be able to retaliate."}, {"heading": "5 Conclusion", "text": "In particular, qualitative visualization experiments, cell activation statistics, and in-depth comparisons with finite horizontal n-gram models show that these networks learn powerful, far-reaching interactions. We have also conducted a detailed error analysis that highlights the limitations of relapsing networks and allows us to propose specific areas for further study. In particular, n-gram errors can be significantly reduced by scaling the models, and rare words could be addressed with larger datasets. However, further architectural innovations may be needed to eliminate the remaining errors."}, {"heading": "Supplementary Material", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "Exact Model Sizes", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "Training Details", "text": "In this case, it is a pure process, and it is an experiment."}, {"heading": "Activation Distributions", "text": "During our analysis, we noticed a distinct peak of tanh c (Figure 7, below) at \u00b1 0.76, which is exactly tanh (1). This is caused by the fact that the largest amount that the LSTM can add or subtract to a cell is 1, since g is the output of tanh. This raises concerns because the hidden state h is calculated as h = o tanh (c). Thus, if a cell is randomly zero because it was reset in the previous iteration, the LSTM does not have the capacity to saturate the cell completely in a single time step. We investigated a simple solution to this problem by changing the update to clt = f clt \u2212 1 + \u03b1i g, where \u03b1 is a constant. We observed a significant decrease in performance for \u03b1 < 0.25. With \u03b1 = 2, the LSTM can saturate the cell in one step. In this case, we observed a consistently faster conversion (e.g., by 2,900 vs. all architectures), however, remaining at a final loss of 700."}, {"heading": "Weights", "text": "For an LSTM trained on the Linux kernel dataset, the characters with the strongest influence turn out to be special characters (\\;} ('{v.w + gb +), while the characters with the weakest influence contain rare Unicode symbols, followed by' $789\u0445 5. This suggests that the LSTMs, to some extent, ignore numbers. We wanted to gain a more holistic understanding of the influence of each character on network activation by embedding the lines of the LSTM weight matrix, which connects the letters to the gates, with t-SNE (29) (Figure 8)."}, {"heading": "Cell state representation", "text": "In Figure 9, we use t-SNE to visualize the (gated) cell state tanh (c) of an LSTM, giving us insight into the type of information it encodes. At the bottom left, we see a cluster of cell states that occur immediately after a space, and at the top right, we see a cluster of cell states in which a space will emerge, showing that the LSTM uses its cell state both to record recently seen signs and to hypothesize signs that might appear next."}], "references": [{"title": "Neural machine translation by jointly learning to align and translate", "author": ["D. Bahdanau", "K. Cho", "Y. Bengio"], "venue": "arXiv preprint arXiv:1409.0473,", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2014}, {"title": "Learning long-term dependencies with gradient descent is difficult", "author": ["Y. Bengio", "P. Simard", "P. Frasconi"], "venue": "Neural Networks, IEEE Transactions on, 5(2):157\u2013166,", "citeRegEx": "2", "shortCiteRegEx": null, "year": 1994}, {"title": "An empirical study of smoothing techniques for language modeling", "author": ["S.F. Chen", "J. Goodman"], "venue": "Computer Speech & Language, 13(4):359\u2013393,", "citeRegEx": "3", "shortCiteRegEx": null, "year": 1999}, {"title": "On the properties of neural machine translation: Encoder-decoder approaches", "author": ["K. Cho", "B. van Merri\u00ebnboer", "D. Bahdanau", "Y. Bengio"], "venue": "arXiv preprint arXiv:1409.1259,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2014}, {"title": "Empirical evaluation of gated recurrent neural networks on sequence modeling", "author": ["J. Chung", "C. Gulcehre", "K. Cho", "Y. Bengio"], "venue": "arXiv preprint arXiv:1412.3555,", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2014}, {"title": "Rmsprop and equilibrated adaptive learning rates for non-convex optimization", "author": ["Y.N. Dauphin", "H. de Vries", "J. Chung", "Y. Bengio"], "venue": "arXiv preprint arXiv:1502.04390,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2015}, {"title": "Long-term recurrent convolutional networks for visual recognition and description", "author": ["J. Donahue", "L.A. Hendricks", "S. Guadarrama", "M. Rohrbach", "S. Venugopalan", "K. Saenko", "T. Darrell"], "venue": "CVPR,", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2015}, {"title": "Adaptive subgradient methods for online learning and stochastic optimization", "author": ["J. Duchi", "E. Hazan", "Y. Singer"], "venue": "The Journal of Machine Learning Research, 12:2121\u20132159,", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2011}, {"title": "Generating sequences with recurrent neural networks", "author": ["A. Graves"], "venue": "arXiv preprint arXiv:1308.0850,", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2013}, {"title": "Speech recognition with deep recurrent neural networks", "author": ["A. Graves", "A.-R. Mohamed", "G. Hinton"], "venue": "Acoustics, Speech and Signal Processing (ICASSP), 2013 IEEE International Conference on, pages 6645\u20136649. IEEE,", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2013}, {"title": "Neural turing machines", "author": ["A. Graves", "G. Wayne", "I. Danihelka"], "venue": "arXiv preprint arXiv:1410.5401,", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2014}, {"title": "LSTM: A search space odyssey", "author": ["K. Greff", "R.K. Srivastava", "J. Koutn\u0131\u0301k", "B.R. Steunebrink", "J. Schmidhuber"], "venue": "CoRR, abs/1503.04069,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2015}, {"title": "Scalable modified Kneser-Ney language model estimation", "author": ["K. Heafield", "I. Pouzyrevsky", "J.H. Clark", "P. Koehn"], "venue": "Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 690\u2013696, Sofia, Bulgaria, August", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2013}, {"title": "Long short-term memory", "author": ["S. Hochreiter", "J. Schmidhuber"], "venue": "Neural computation, 9(8):1735\u20131780,", "citeRegEx": "14", "shortCiteRegEx": null, "year": 1997}, {"title": "Spoken language processing: A guide to theory, algorithm, and system development", "author": ["X. Huang", "A. Acero", "H.-W. Hon", "R. Foreword By-Reddy"], "venue": "Prentice Hall PTR,", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2001}, {"title": "The human knowledge compression contest", "author": ["M. Hutter"], "venue": null, "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2012}, {"title": "A dynamic language model for speech recognition", "author": ["F. Jelinek", "B. Merialdo", "S. Roukos", "M. Strauss"], "venue": "HLT, volume 91, pages 293\u2013295,", "citeRegEx": "17", "shortCiteRegEx": null, "year": 1991}, {"title": "Inferring algorithmic patterns with stack-augmented recurrent nets", "author": ["A. Joulin", "T. Mikolov"], "venue": "CoRR, abs/1503.01007,", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2015}, {"title": "Deep visual-semantic alignments for generating image descriptions", "author": ["A. Karpathy", "L. Fei-Fei"], "venue": "CVPR,", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2015}, {"title": "Building a large annotated corpus of english: The penn treebank", "author": ["M.P. Marcus", "M.A. Marcinkiewicz", "B. Santorini"], "venue": "Computational linguistics, 19(2):313\u2013330,", "citeRegEx": "20", "shortCiteRegEx": null, "year": 1993}, {"title": "Statistical language models based on neural networks", "author": ["T. Mikolov"], "venue": "Presentation at Google, Mountain View, 2nd April,", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2012}, {"title": "Recurrent neural network based language model", "author": ["T. Mikolov", "M. Karafi\u00e1t", "L. Burget", "J. Cernock\u1ef3", "S. Khudanpur"], "venue": "INTERSPEECH 2010, 11th Annual Conference of the International Speech Communication Association, Makuhari, Chiba, Japan, September 26-30, 2010, pages 1045\u20131048,", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2010}, {"title": "How to construct deep recurrent neural networks", "author": ["R. Pascanu", "\u00c7. G\u00fcl\u00e7ehre", "K. Cho", "Y. Bengio"], "venue": "CoRR, abs/1312.6026,", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2013}, {"title": "On the difficulty of training recurrent neural networks", "author": ["R. Pascanu", "T. Mikolov", "Y. Bengio"], "venue": "arXiv preprint arXiv:1211.5063,", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2012}, {"title": "Learning internal representations by error propagation", "author": ["D.E. Rumelhart", "G.E. Hinton", "R.J. Williams"], "venue": "Technical report, DTIC Document,", "citeRegEx": "25", "shortCiteRegEx": null, "year": 1985}, {"title": "Deep learning in neural networks: An overview", "author": ["J. Schmidhuber"], "venue": "Neural Networks, 61:85\u2013117,", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2015}, {"title": "Generating text with recurrent neural networks", "author": ["I. Sutskever", "J. Martens", "G.E. Hinton"], "venue": "Proceedings of the 28th International Conference on Machine Learning (ICML-11), pages 1017\u20131024,", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2011}, {"title": "Sequence to sequence learning with neural networks", "author": ["I. Sutskever", "O. Vinyals", "Q.V. Le"], "venue": "Advances in Neural Information Processing Systems, pages 3104\u20133112,", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2014}, {"title": "Visualizing data using t-sne", "author": ["L. Van der Maaten", "G. Hinton"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2008}, {"title": "Show and tell: A neural image caption generator", "author": ["O. Vinyals", "A. Toshev", "S. Bengio", "D. Erhan"], "venue": "CVPR,", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2015}, {"title": "Generalization of backpropagation with application to a recurrent gas market model", "author": ["P.J. Werbos"], "venue": "Neural Networks, 1(4):339\u2013356,", "citeRegEx": "31", "shortCiteRegEx": null, "year": 1988}, {"title": "Memory networks", "author": ["J. Weston", "S. Chopra", "A. Bordes"], "venue": "CoRR, abs/1410.3916,", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2014}], "referenceMentions": [], "year": 2015, "abstractText": "Recurrent Neural Networks (RNNs), and specifically a variant with Long ShortTerm Memory (LSTM), are enjoying renewed interest as a result of successful applications in a wide range of machine learning problems that involve sequential data. However, while LSTMs provide exceptional results in practice, the source of their performance and their limitations remain rather poorly understood. Using character-level language models as an interpretable testbed, we aim to bridge this gap by providing a comprehensive analysis of their representations, predictions and error types. In particular, our experiments reveal the existence of interpretable cells that keep track of long-range dependencies such as line lengths, quotes and brackets. Moreover, an extensive analysis with finite horizon n-gram models suggest that these dependencies are actively discovered and utilized by the networks. Finally, we provide detailed error analysis that suggests areas for further study.", "creator": "LaTeX with hyperref package"}}}