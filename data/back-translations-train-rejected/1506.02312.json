{"id": "1506.02312", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "7-Jun-2015", "title": "A Framework for Constrained and Adaptive Behavior-Based Agents", "abstract": "Behavior Trees are commonly used to model agents for robotics and games, where constrained behaviors must be designed by human experts in order to guarantee that these agents will execute a specific chain of actions given a specific set of perceptions. In such application areas, learning is a desirable feature to provide agents with the ability to adapt and improve interactions with humans and environment, but often discarded due to its unreliability. In this paper, we propose a framework that uses Reinforcement Learning nodes as part of Behavior Trees to address the problem of adding learning capabilities in constrained agents. We show how this framework relates to Options in Hierarchical Reinforcement Learning, ensuring convergence of nested learning nodes, and we empirically show that the learning nodes do not affect the execution of other nodes in the tree.", "histories": [["v1", "Sun, 7 Jun 2015 20:52:31 GMT  (305kb,D)", "http://arxiv.org/abs/1506.02312v1", "2015; 15 pages"]], "COMMENTS": "2015; 15 pages", "reviews": [], "SUBJECTS": "cs.AI cs.LG cs.RO cs.SY", "authors": ["renato de pontes pereira", "paulo martins engel"], "accepted": false, "id": "1506.02312"}, "pdf": {"name": "1506.02312.pdf", "metadata": {"source": "CRF", "title": "A Framework for Constrained and Adaptive Behavior-Based Agents", "authors": ["Renato de Pontes Pereira", "Paulo Martins Engel"], "emails": ["rppereira@inf.ufrgs.br", "engel@inf.ufrgs.br"], "sections": [{"heading": null, "text": "Keywords: Behavior Trees, Reinforcement Learning, Hierarchical Reinforcement Learning, Agent Modeling, Robotics, Games"}, {"heading": "1 Introduction", "text": "This year, it has reached the point where it will be able to put itself at the top without being able to put itself at the top."}, {"heading": "2 Behavior Trees", "text": "This year, more than ever before in the history of a country in which it is a country in which it is a country in which it is a country in which it is a country, a country in which it is a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a"}, {"heading": "3 Reinforcement Learning", "text": "We address the problem of adding learning capacity to an agent acting without expert supervision (i.e., input-output pairs are never presented in real-time applications (i.e., it must be learned continuously) and preferably online (i.e., the model is updated upon arrival of a new piece of information).This problem can be formulated as the Markov Decision Process (i.e., it is the common basis for the Reinforcement Learning Task).In an MDP problem, a learning agent interacts with an environment with discrete time steps t = 1, 2, 3, 3,... In each step of time, the agent observes the system state st-S and acts in the branch where As is a finite rather than empty set of permissible actions for a state.As a result of the agent action, the system generates a rt + 1 reward corresponding to an expected instant reward function."}, {"heading": "3.1 Hierarchical Reinforcement Learning", "text": "While most classic affirmation processes are based on the configuration described by the MDPs, the hierarchical Reinforcement Learning (HRL) models are developed on the basis of Semi-Markov decision-making processes (SMDPs). Due to the generalization of SMDPs, these hierarchical models are able to explore the temporal aspects of the tasks and reduce the effects of the curse of dimensionality by dividing the task space into several sub-problems. In an SMDP, the learning agent considers the time during the transition between states. This means that an agent, when observing a state, performs an action that takes time to move into a new state by taking two steps to rewrite the probability to P (s) and the expected immediate reward function R (s, a), there is now the amount of the discounted reward that is expected to accumulate into s over the waiting time. Equations for the optimal value function and the optimal value function (Q) become the optimal value function (Q)."}, {"heading": "4 Learning Framework", "text": "We propose a framework for modeling agents who follow behaviors strictly as modeled by a human expert and are still able to learn from experiences. This framework allows an expert to manually define what behaviors an agent will have, to determine when and in what order the behaviors will be executed, and to determine where learning can be applied. Behavioral rules are used as the basic modeling tool for our framework, due to their advantages as described in Section 2. In summary, BTs are compact, easy to understand, maintain, and reuse, they have expressive power and can be easily paralleled. We also adopt Reinforcement Learning to support our modeling tool by providing the ability to learn with the agent experiences. RL allows agents to optimize actions in specific situations and be able to adapt to changes in the environment configuration and dynamics."}, {"heading": "5 Experimental Validation", "text": "In this section, we present two simulated fire control scenarios to empirically validate the proposed framework. The following experiments use discrete state and action models of Q-Learning in the learning node, but continuous versions of Q-Learning can be used. We also used the Behavior3 library and Behavior Trees [13] to model the behavioral trees. All experiments and custom codes are available online. Both experiments perform 30 experiments with 400 iterations each. At the beginning of each experiment, the experiment is reset. All diagrams show the average results of the 30 experiments. The environment is divided into infinite spaces. In each room, the agent can perform three possible actions: save the victim, use the fire extinguisher X, and change the locker room. Each given room has a 50% chance of having a victim in the rubble; if there is a victim, the agent must first extinguish the fire and the agent is extinguished before the proposed framework."}, {"heading": "5.1 Scenario 1", "text": "In this scenario, all actions are performed immediately. Figure 3 shows the behavior tree that models the learning agent. Note that after the root child, there are three branches: the first with the highest priority represents the behavior saving victim; the second branch represents the utility extinguisher X; and the last branch with the lowest priority represents the modification space. We use a learning action node for the behavior of the utility extinguisher X. This node must learn which fire extinguisher can be used for each fire type; it is configured to get the state s = < fire type >, the fire type = {1, 2, 3} and the actions a = {A, B, C}; this node also receives the reward of + 10 if it could put out the fire and \u2212 10 otherwise. Table 1 shows the ratio between the correct activations of the three main activities over the total expected activations (i.e. accuracy for 3)."}, {"heading": "5.2 Scenario 2", "text": "In this scenario, we add a greater complexity to the task. \u2212 The actions of the agent save the victim and use the fire extinguisher X, the completion of which now takes time, depending on the intensity of the fire. Each given fire has an intensity of the fire intensity, which is randomly selected for each room. The fire intensity indicates how many ticks the agent needs to complete the actions (i.e., if the fire intensity is 0, all actions are immediate; if it is 1, the action takes 1 tick to complete; etc.) The fire intensity decreases by 1 tick if the right fire extinguisher is used. Note that the change room is always immediate and the use of the wrong fire extinguisher leaves the agent out of space. \u2212 Figure 5 shows the behavioral tree that models the learning material for this scenario. Now, he uses 2 learning nodes similar to the one used in the first scenario, and the learning node is a learning action with the state of the second scenario}, a learning action {and a learning action with the learning action = {1}. < 1 ="}, {"heading": "6 Related Work", "text": "Behavior Trees were created as an alternative to Hierarchical Finite State Machines (HFSM) and similar methods aimed at providing more flexible controllers for non-playable characters (NPCs) in video games, a method that quickly gained acceptance in the gaming industry and was recently applied to robotics [3] [4], where BTs were given a formal and standard definition."}, {"heading": "7 Conclusion", "text": "We have proposed a framework for using reinforcement learning in behavior-based agents that offers adaptability to physical or virtual agents while respecting the constraints modeled by the expert. On the basis of behavioral trees, we proposed the creation of a new type of composite and action node, called learning nodes, in which we embed a Q-learning algorithm to perform local learning without affecting the functioning of other nodes. We show that this framework is related to hierarchical reinforcement learning, which is a specialization of the options framework, ensuring convergence of nested learning nodes that can be interrupted before the task is completed, and enabling the use of intra-option learning for more complex models.We also empirically validate our framework using experiments in simulated fire protection scenarios. The experiments show how the expert knowledge can be used to validate continuous learning decisions without incorporating behavioral modes."}, {"heading": "Acknowledgment", "text": "The authors thank Edigleison Carvalho and Thiago Rodrigues for their valuable contribution to this work, which is supported by CNPq, a Brazilian agency for scientific and technological development."}], "references": [{"title": "Recent advances in hierarchical reinforcement learning", "author": ["A.G. Barto", "S. Mahadevan"], "venue": "Discrete Event Dynamic Systems 13(4), 341\u2013379", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2003}, {"title": "Adaptive fault tolerant execution of multi-robot missions using behavior trees", "author": ["M. Colledanchise", "A. Marzinotto", "D.V. Dimarogonas", "P. \u00d6gren"], "venue": "arXiv preprint arXiv:1502.02960", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2015}, {"title": "Performance analysis of stochastic behavior trees", "author": ["M. Colledanchise", "A. Marzinotto", "P. Ogren"], "venue": "Robotics and Automation (ICRA), 2014 IEEE International Conference on. pp. 3265\u20133272. IEEE", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2014}, {"title": "How behavior trees modularize robustness and safety in hybrid systems", "author": ["M. Colledanchise", "P. Ogren"], "venue": "Intelligent Robots and Systems (IROS 2014), 2014 IEEE/RSJ International Conference on. pp. 1482\u20131488. IEEE", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2014}, {"title": "Hierarchical reinforcement learning with the maxq value function decomposition", "author": ["T.G. Dietterich"], "venue": "J. Artif. Intell. Res.(JAIR) 13, 227\u2013303", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2000}, {"title": "Query-enabled behavior trees", "author": ["G. Fl\u00f3rez-Puga", "M.A. G\u00f3mez-Mart\u0301\u0131n", "P.P. G\u00f3mez-Mart\u0301\u0131n", "B. D\u0131\u0301az-Agudo", "P.A. Gonz\u00e1lez-Calero"], "venue": "Computational Intelligence and AI in Games, IEEE Transactions on 1(4), 298\u2013308", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2009}, {"title": "proceeding: Handling complexity in the halo 2 ai (March 2005), http://www.gamasutra.com/view/feature/130663/gdc_2005_ proceeding_handling_.php, last access", "author": ["D. Isla"], "venue": "Gdc", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2005}, {"title": "Emotional behavior trees", "author": ["A. Johansson", "P. Dell\u2019Acqua"], "venue": "Computational Intelligence and Games (CIG), 2012 IEEE Conference on. pp. 355\u2013362. IEEE", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2012}, {"title": "Towards a unified behavior trees framework for robot control", "author": ["A. Marzinotto", "M. Colledanchise", "C. Smith", "P. Ogren"], "venue": "Robotics and Automation (ICRA), 2014 IEEE International Conference on. pp. 5420\u20135427. IEEE", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2014}, {"title": "Increasing modularity of uav control systems using computer game behavior trees", "author": ["P. Ogren"], "venue": "AIAA Guidance, Navigation and Control Conference, Minneapolis, MN", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2012}, {"title": "Extending case-based planning with behavior trees", "author": ["R. Palma", "P.A. Gonz\u00e1lez-Calero", "M.A. G\u00f3mez-Mart\u0301\u0131n", "P.P. G\u00f3mez-Mart\u0301\u0131n"], "venue": "FLAIRS Conference", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2011}, {"title": "Reinforcement learning with hierarchies of machines", "author": ["R. Parr", "S. Russell"], "venue": "Advances in neural information processing systems pp. 1043\u20131049", "citeRegEx": "12", "shortCiteRegEx": null, "year": 1998}, {"title": "http://behavior3.com/, last access on 201503-05", "author": ["Pereira", "R.d.P"], "venue": null, "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2014}, {"title": "Reinforcement learning: An introduction", "author": ["R.S. Sutton", "A.G. Barto"], "venue": "MIT Press, Cambridge, MA, USA, 1st edn.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 1998}, {"title": "Between mdps and semi-mdps: A framework for temporal abstraction in reinforcement learning", "author": ["R.S. Sutton", "D. Precup", "S. Singh"], "venue": "Artificial intelligence 112(1), 181\u2013211", "citeRegEx": "15", "shortCiteRegEx": null, "year": 1999}], "referenceMentions": [{"referenceID": 6, "context": "This tool was created in the game industry [7] with fast adoption by the game development community.", "startOffset": 43, "endOffset": 46}, {"referenceID": 9, "context": "Since 2012, there are some efforts to apply BTs on robotics, including modeling and controlling of UAVs [10], fault tolerance in hybrid and multi-robot systems [4][2] and an attempt to unify notation and formalize Behavior Trees as controller, proving the relation to CHDSs (Controlled Hybrid Dynamical Systems) [9].", "startOffset": 104, "endOffset": 108}, {"referenceID": 3, "context": "Since 2012, there are some efforts to apply BTs on robotics, including modeling and controlling of UAVs [10], fault tolerance in hybrid and multi-robot systems [4][2] and an attempt to unify notation and formalize Behavior Trees as controller, proving the relation to CHDSs (Controlled Hybrid Dynamical Systems) [9].", "startOffset": 160, "endOffset": 163}, {"referenceID": 1, "context": "Since 2012, there are some efforts to apply BTs on robotics, including modeling and controlling of UAVs [10], fault tolerance in hybrid and multi-robot systems [4][2] and an attempt to unify notation and formalize Behavior Trees as controller, proving the relation to CHDSs (Controlled Hybrid Dynamical Systems) [9].", "startOffset": 163, "endOffset": 166}, {"referenceID": 8, "context": "Since 2012, there are some efforts to apply BTs on robotics, including modeling and controlling of UAVs [10], fault tolerance in hybrid and multi-robot systems [4][2] and an attempt to unify notation and formalize Behavior Trees as controller, proving the relation to CHDSs (Controlled Hybrid Dynamical Systems) [9].", "startOffset": 312, "endOffset": 315}, {"referenceID": 8, "context": "There can be other types depending on the application of the model, but these are generic and necessary nodes to build a complete controller [9].", "startOffset": 141, "endOffset": 144}, {"referenceID": 0, "context": "The goal of the agent is to learn a policy \u03c0 : S \u00d7 A \u2192 [0, 1], where A = \u22c3 s\u2208S As (the set of all actions), that maximizes the expected discounted future reward from each state s [15]:", "startOffset": 55, "endOffset": 61}, {"referenceID": 14, "context": "The goal of the agent is to learn a policy \u03c0 : S \u00d7 A \u2192 [0, 1], where A = \u22c3 s\u2208S As (the set of all actions), that maximizes the expected discounted future reward from each state s [15]:", "startOffset": 179, "endOffset": 183}, {"referenceID": 0, "context": "where \u03c0(s, a) is the probability of a policy \u03c0 choosing an action a in the state s, and \u03b3 \u2208 [0, 1] is a discount-rate parameter.", "startOffset": 92, "endOffset": 98}, {"referenceID": 13, "context": "Unfortunately, the only solutions to Equations 2 and 4 are the Equations 1 and 3 [14], respectively, which cannot be computed without knowing R(s, a) and P (s\u2032|s, a).", "startOffset": 81, "endOffset": 85}, {"referenceID": 13, "context": "With this update process, Qk converges to Q \u2217 with probability 1 if at the limit, all admissible state-action pairs are updated infinitely often, and \u03b1k decays with time, regardless of the policy being followed [14][1].", "startOffset": 211, "endOffset": 215}, {"referenceID": 0, "context": "With this update process, Qk converges to Q \u2217 with probability 1 if at the limit, all admissible state-action pairs are updated infinitely often, and \u03b1k decays with time, regardless of the policy being followed [14][1].", "startOffset": 215, "endOffset": 218}, {"referenceID": 0, "context": "on Semi Markov Decision Processes (SMDPs) [1].", "startOffset": 42, "endOffset": 45}, {"referenceID": 0, "context": "Now, the joint probability is rewritten to P (s\u2032, \u03c4 |s, a) and the expected immediate reward function R(s, a) now gives the amount of discounted reward expected to accumulate over the waiting time in s given a [1].", "startOffset": 210, "endOffset": 213}, {"referenceID": 0, "context": "An option can be defined as 3-tuple \u3008I, \u03bc, \u03b2\u3009, consisting of an input set I \u2286 S, a semi-Markov policy \u03bc : S \u00d7O \u2192 [0, 1] (where O = \u22c3 s\u2208S Os), and a termination condition \u03b2 : S \u2192 [0, 1].", "startOffset": 113, "endOffset": 119}, {"referenceID": 0, "context": "An option can be defined as 3-tuple \u3008I, \u03bc, \u03b2\u3009, consisting of an input set I \u2286 S, a semi-Markov policy \u03bc : S \u00d7O \u2192 [0, 1] (where O = \u22c3 s\u2208S Os), and a termination condition \u03b2 : S \u2192 [0, 1].", "startOffset": 178, "endOffset": 184}, {"referenceID": 0, "context": ", it can choose the next option based on the entire history h of states, actions, and rewards since the option was initiated [1].", "startOffset": 125, "endOffset": 128}, {"referenceID": 0, "context": "2 Here, we assume \u03c4 to be discrete, but it can be extended to continuous without much impact [1]", "startOffset": 93, "endOffset": 96}, {"referenceID": 13, "context": "As result from Theorem 1, we can also exploit the features provided from the Options framework [14], such as: both learning nodes can be trained using Q-Learning; guarantee of convergence for nested nodes with the same conditions to a single Q-Learning model; due to the division of the space, the nodes can converge faster than a single learning model; nodes can be interrupted by prioritized behaviors without problem; intra-option learning can be used to speed up global convergence among the tree.", "startOffset": 95, "endOffset": 99}, {"referenceID": 12, "context": "We also used the Behavior3 library and editor [13] for modeling the Behavior Trees.", "startOffset": 46, "endOffset": 50}, {"referenceID": 6, "context": "Behavior Trees were created as alternative to Hierarchical Finite State Machines (HFSMs) and similar methods, aiming to provide more flexible controller for Non-Playable Characters (NPCs) in video games [7].", "startOffset": 203, "endOffset": 206}, {"referenceID": 9, "context": "The method had a quick acceptance in the game industry and, recently, it has been applied to robotics [10] [9] [3] [4] [2], where BTs received a more formal and standard definition.", "startOffset": 102, "endOffset": 106}, {"referenceID": 8, "context": "The method had a quick acceptance in the game industry and, recently, it has been applied to robotics [10] [9] [3] [4] [2], where BTs received a more formal and standard definition.", "startOffset": 107, "endOffset": 110}, {"referenceID": 2, "context": "The method had a quick acceptance in the game industry and, recently, it has been applied to robotics [10] [9] [3] [4] [2], where BTs received a more formal and standard definition.", "startOffset": 111, "endOffset": 114}, {"referenceID": 3, "context": "The method had a quick acceptance in the game industry and, recently, it has been applied to robotics [10] [9] [3] [4] [2], where BTs received a more formal and standard definition.", "startOffset": 115, "endOffset": 118}, {"referenceID": 1, "context": "The method had a quick acceptance in the game industry and, recently, it has been applied to robotics [10] [9] [3] [4] [2], where BTs received a more formal and standard definition.", "startOffset": 119, "endOffset": 122}, {"referenceID": 14, "context": "We show that our framework has a close relation to the Options framework [15], but it has also similarities to other models of Hierarchical Reinforcement Learning, such as the Hierarchies of Abstract Machines [12] and the MAXQ model [5].", "startOffset": 73, "endOffset": 77}, {"referenceID": 11, "context": "We show that our framework has a close relation to the Options framework [15], but it has also similarities to other models of Hierarchical Reinforcement Learning, such as the Hierarchies of Abstract Machines [12] and the MAXQ model [5].", "startOffset": 209, "endOffset": 213}, {"referenceID": 4, "context": "We show that our framework has a close relation to the Options framework [15], but it has also similarities to other models of Hierarchical Reinforcement Learning, such as the Hierarchies of Abstract Machines [12] and the MAXQ model [5].", "startOffset": 233, "endOffset": 236}], "year": 2015, "abstractText": "Behavior Trees are commonly used to model agents for robotics and games, where constrained behaviors must be designed by human experts in order to guarantee that these agents will execute a specific chain of actions given a specific set of perceptions. In such application areas, learning is a desirable feature to provide agents with the ability to adapt and improve interactions with humans and environment, but often discarded due to its unreliability. In this paper, we propose a framework that uses Reinforcement Learning nodes as part of Behavior Trees to address the problem of adding learning capabilities in constrained agents. We show how this framework relates to Options in Hierarchical Reinforcement Learning, ensuring convergence of nested learning nodes, and we empirically show that the learning nodes do not affect the execution of other nodes in the tree.", "creator": "LaTeX with hyperref package"}}}