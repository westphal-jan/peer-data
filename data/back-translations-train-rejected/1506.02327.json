{"id": "1506.02327", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "7-Jun-2015", "title": "A Multi-layered Acoustic Tokenizing Deep Neural Network (MAT-DNN) for Unsupervised Discovery of Linguistic Units and Generation of High Quality Features", "abstract": "This paper summarizes the work done by the authors for the Zero Resource Speech Challenge organized in the technical program of Interspeech 2015. The goal of the challenge is to discover linguistic units directly from unlabeled speech data. The Multi-layered Acoustic Tokenizer (MAT) proposed in this work automatically discovers multiple sets of acoustic tokens from the given corpus. Each acoustic token set is specified by a set of hyperparameters that describe the model configuration. These sets of acoustic tokens carry different characteristics of the given corpus and the language behind thus can be mutually reinforced. The multiple sets of token labels are then used as the targets of a Multi-target DNN (MDNN) trained on low-level acoustic features. Bottleneck features extracted from the MDNN are used as feedback for the MAT and the MDNN itself. We call this iterative system the Multi-layered Acoustic Tokenizing Deep Neural Network (MAT-DNN) which generates high quality features for track 1 of the challenge and acoustic tokens for track 2 of the challenge.", "histories": [["v1", "Sun, 7 Jun 2015 23:52:54 GMT  (272kb,D)", "http://arxiv.org/abs/1506.02327v1", "submitted to Interspeech 2015"]], "COMMENTS": "submitted to Interspeech 2015", "reviews": [], "SUBJECTS": "cs.CL cs.LG cs.NE", "authors": ["cheng-tao chung", "cheng-yu tsai", "hsiang-hung lu", "yuan-ming liou", "yen-chen wu", "yen-ju lu", "hung-yi lee", "lin-shan lee"], "accepted": false, "id": "1506.02327"}, "pdf": {"name": "1506.02327.pdf", "metadata": {"source": "CRF", "title": "A Multi-layered Acoustic Tokenizing Deep Neural Network (MAT-DNN) for Unsupervised Discovery of Linguistic Units and Generation of High Quality Features", "authors": ["Cheng-Tao Chung", "Cheng-Yu Tsai", "Hsiang-Hung Lu", "Yuan-ming Liou", "Yen-Chen Wu", "Yen-Ju Lu", "Hung-yi Lee", "Lin-shan Lee"], "emails": ["f01921031@ntu.edu.tw,", "r02942067@ntu.edu.tw,", "r03942039@ntu.edu.tw,", "qxesqxes@gmail.com,", "r03942044@ntu.edu.tw,", "r03942063@ntu.edu.tw,", "tlkagkb93901106@gmail.com,", "lslee@gate.sinica.edu.tw"], "sections": [{"heading": "1. Introduction", "text": "The process is not yet fully understood, and it is difficult to reproduce it through the current Automatic Speech Recognition (ASR), in which an entire language has to be learned from the ground up. [2] The aim of this challenge is to find linguistic units directly from the language without learning the language. [3] The challenge is to learn the language."}, {"heading": "2. Proposed Approach", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "2.1. Overview of the proposed framework", "text": "The framework of the approach is illustrated in Fig.1 on the left. In the left part, the multi-layer acoustic tokenizers (MAT) are specified by two hyperparameters describing HMM configurations. A set of acoustic tokens is obtained for each configuration by iterative optimization of the token models and token labels on the given acoustic corpus. Several pairs of hyperparameters were selected to generate multi-layer token labels for the given corpus, which are used as training targets of the Multi-target Deep Neural Network (MDNN) on the right part of the acoustic corpus. The MDNN on the right learns its parameters based on the multi-layer token labels for the given corpus, which are used as training targets of the Multi-target Deep Neural Network (MDNN)."}, {"heading": "2.2. Multi-layered Acoustic Tokenizer", "text": "The goal of this step is to obtain several sets of acoustic signs, each defined by a few hyperparameters that capture complementary aspects of the corpus. There is no knowledge of the corpus, so the process here is completely unattended."}, {"heading": "2.2.1. Unsupervised Token Discovery for Each layer of MAT", "text": "With the help of unattended HMMs, it is easy to find acoustic characters from the corpus for a selected pair of hyperparameters that determine the HMM configuration (number of states per model and number of different models) [11, 12, 13, 14, 15]. This can be achieved by first finding an initial set of assumed characters for all characters in corpus X as in (1) [14]. Subsequently, in each iteration t the HMM parameters can be trained with the label \u03c9t \u2212 1 obtained in the previous iteration, and the new label set \u03c9t can be obtained by decrypting tokens with the obtained parameters as in (3)."}, {"heading": "2.2.2. Granularity Space of Multi-layered Acoustic Token Sets", "text": "The process described above can be performed with different HMM configurations, each of which is characterized by two hyperparameters: the number of states m in each acoustic sign HMM and the total number of unique acoustic signs n during initialization. Transcribing a signal decoded with these signs can be considered a temporal segmentation of the signal so that the total number n of distinct acoustic signs represents the phonetic granularity, resulting in a two-dimensional representation of the acoustic sign configurations in terms of temporal and phonetic granularities as shown in Fig.2. Each point in this two-dimensional space corresponds to the phonetic granularity. Although the acoustic sign configurations in Fig.2 have a two-dimensional granularity in terms of temporal and phonetic granularities, we can form two different acoustic sign configurations in two levels."}, {"heading": "2.3. Mutual Reinforcement of Multi-layered Tokens", "text": "Since all layers obtained in the above MAT are learned unattended, they are not precise, but we have many layers, each of which corresponds to a different pair of hyperparameters \u044b = (m, n) so that they can mutually reinforce each other. This is explained here and shown in Fig.3, including merging symbol boundaries and re-initializing LDAbased symbols as shown in Fig.3 (a)."}, {"heading": "2.3.1. Token Boundary Fusion", "text": "Fig.3 (b) shows the token boundary when a portion of an expression is segmented into acoustic tokens at different levels with different pairs of hyperparameters, usually = (m, n). We define a boundary function bm, n (j) for the possible boundary between each pair of two adjacent frames within the utterance, where j is the time index of such possible boundaries. At each level bm, n (j) = 1 if boundary j is a token boundary and 0 otherwise. All these boundary functions bm, n (j) for all different levels are then weighted and averaged to produce a common boundary function B (j). Weights take into account the fact that smaller m or shorter HMMs generate more boundaries. The peaks of B (j) are then selected on the basis of the second derivative and some filter and threshold processes, resulting in the new segmentation of the utterance than below in Fig.3 (b)."}, {"heading": "2.3.2. LDA-based Token Label Re-initialization", "text": "As shown in Fig.3 (c), each new segment obtained above usually consists of a sequence of acoustic characters at each level, based on the characters defined at that level. We now consider all characters at all different levels to be different words, so we have a vocabulary of words (tokens) at all different levels, i.e. there are ni-words at the i level, and there are MN layers altogether. Thus, a new segment is regarded here as a document (pouch) consisting of words (tokens) collected from all different layers. Latent dirichlet allocation [16] (LDA) is preformed for theme modeling, and then each document (new segment) is labeled with the most likely topic. Since in LDA a topic is characterized by a word distribution (token distribution), here a token distribution (token distribution across different layers) is also defined as a certain acoustic property or a specific acoustic symbol (a specific token symbol) as the number of the differentiator (1)."}, {"heading": "2.4. The Multi-target DNN (MDNN)", "text": "As shown in the right part of Fig.1, the sequence of token markings from one level (with a pair of hyperparameters \u043d = (m, n) is a valid target for supervised frame-by-frame training, albeit unattended. In the initial work, we do not use the HMM states as a target here, but simply take the token label as a training target. As shown in Fig.1, there are multi-layer token markings with different hyperparameter pairs flood = (m, n) for each expression, so together we look at all multi-layered token markings by learning the parameters for a single DNN with a uniformly weighted cross-entropy target at the output level. As a result, the bottleneck function (BNF) extracted from this DNN automatically merges all knowledge about the body and the underlying language with the different groups of acoustic tokens."}, {"heading": "2.5. The Iterative Learning Framework for MAT-DNN", "text": "Once the BNFs are extracted from the MDNN in iteration 1, they can be used as input of the MAT to the left of Fig.1 (c), which replaces the initial acoustic characteristics. MAT then generates updated sets of multi-layered token labels, and these updated sets of multi-layered token labels can be used as an updated MDNN training target. MDNN input characteristics can also be updated by concatenating the initial acoustic characteristics with the newly extracted BNFs as tandem characteristics. This process can be repeated for multiple iterations until satisfactory results are obtained. The tandem function used as input of the MDNN can be further supplemented by concatenating unattended characteristics that are included as tandem characteristics in other systems such as the Deep Boltzmann Machine [17] (DBM), which are included in the next iteration of recursive characteristics of the network."}, {"heading": "3. Experimental Setup", "text": "The general framework of the MAT-DNN enabled several flexible configurations. However, in this work we train the MATDNN in the following way: We are committed to the 39 dimensions in which the MFK coefficients are located (MFK), namely with the initial acoustics for the MFK and MFK. We refuel the MFK before and after (MFK)."}, {"heading": "3.1. Track 1", "text": "The two official corporas are the Buckeye Corpus [27] and NCHLT Xitsonga Speech corpus [28] in English and Tsonga respectively. They are used in the evaluation based on the ABX discrimination test [4] including over and within speaker tests. The end result is in error rate, meaning that the lower of the MFCC characteristics in Table 1.Rows (1) and (11) are the official baseline MFCC characteristics and official topline provided by the challenge organizers. Row (2) is our baseline of the MFCC characteristics, the initial acoustic characteristics used to train all systems in this work. Row (3) is for the DBM posteriorgrams of the series (2), which is used as a strong unguarded baseline of the MFCC characteristics."}, {"heading": "3.2. Track 2", "text": "In fact, in the USA, in the USA, in the USA, in Europe, in the USA, in Europe, in the USA, in the USA, in the USA, in Europe, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA in the USA, in the USA, in the USA, in the USA, in the USA, in the USA in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA,"}, {"heading": "4. Conclusion", "text": "This paper summarizes the preparatory work for the Zero Resource Speech Challenge in Interspeech 2015. We propose a MATDNN to generate multi-layered token sets and merge the different skills in different token sets into the bottleneck features. We present the full results of all evaluations we have tested by the deadline, in the hope that these results will serve as good references for future studies."}, {"heading": "5. References", "text": "In fact, the tenor said, it is a matter of being able to play by the rules."}], "references": [{"title": "Deep neural networks for acoustic modeling in speech recognition: The shared views of four research groups", "author": ["G. Hinton", "L. Deng", "D. Yu", "G.E. Dahl", "A.-r. Mohamed", "N. Jaitly", "A. Senior", "V. Vanhoucke", "P. Nguyen", "T.N. Sainath"], "venue": "Signal Processing Magazine, IEEE, vol. 29, no. 6, pp. 82\u201397, 2012.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2012}, {"title": "A nonparametric bayesian approach to acoustic model discovery", "author": ["C.-y. Lee", "J. Glass"], "venue": "Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics: Long Papers-Volume 1. Association for Computational Linguistics, 2012, pp. 40\u201349.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2012}, {"title": "Unsupervised training of an hmm-based self-organizing unit recognizer with applications to topic classification and keyword discovery", "author": ["M.-h. Siu", "H. Gish", "A. Chan", "W. Belfield", "S. Lowe"], "venue": "Computer Speech & Language, vol. 28, no. 1, pp. 210\u2013223, 2014.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2014}, {"title": "Evaluating speech features with the minimalpair abx task: Analysis of the classical mfc/plp pipeline", "author": ["T. Schatz", "V. Peddinti", "F. Bach", "A. Jansen", "H. Hermansky", "E. Dupoux"], "venue": "INTERSPEECH 2013: 14th Annual Conference of the International Speech Communication Association, 2013, pp. 1\u20135.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2013}, {"title": "Bridging the gap between speech technology and natural language processing: an evaluation toolbox for term discovery systems", "author": ["B. Ludusan", "M. Versteegh", "A. Jansen", "G. Gravier", "X.-N. Cao", "M. Johnson", "E. Dupoux"], "venue": "Language Resources and Evaluation Conference, 2014.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2014}, {"title": "Unsupervised spoken term detection with spoken queries by multi-level acoustic patterns with varying model granularity", "author": ["C.-T. Chung", "C.-a. Chan", "L.-s. Lee"], "venue": "Acoustics, Speech and Signal Processing (ICASSP), 2014 IEEE International Conference on. IEEE, 2014.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2014}, {"title": "Performance analysis for latticebased speech indexing approaches using words and subword units", "author": ["Y.-c. Pan", "L.-s. Lee"], "venue": "Audio, Speech, and Language Processing, IEEE Transactions on, vol. 18, no. 6, pp. 1562\u20131574, 2010.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2010}, {"title": "Enhancing automatically discovered multi-level acoustic patterns considering context consistency with applications in spoken term detection", "author": ["C.-T. Chung", "W.-N. Hsu", "C.-Y. Lee", "L.-S. Lee"], "venue": "Acoustics, Speech and Signal Processing (ICASSP), 2015 IEEE International Conference on. IEEE, 2015.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2015}, {"title": "Investigating the learning effect of multilingual bottle-neck features for asr", "author": ["N.T. Vu", "J. Weiner", "T. Schultz"], "venue": "Fifteenth Annual Conference of the International Speech Communication Association, 2014.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2014}, {"title": "The language-independent bottleneck features", "author": ["K. Vesely", "M. Karafi\u00e1t", "F. Grezl", "M. Janda", "E. Egorova"], "venue": "Spoken Language Technology Workshop (SLT), 2012 IEEE. IEEE, 2012, pp. 336\u2013341.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2012}, {"title": "Towards unsupervised training of speaker independent acoustic models.", "author": ["A. Jansen", "K. Church"], "venue": "in INTERSPEECH,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2011}, {"title": "Unsupervised training of an hmm-based speech recognizer for topic classification.", "author": ["H. Gish", "M.-h. Siu", "A. Chan", "W. Belfield"], "venue": "INTERSPEECH,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2009}, {"title": "Improved topic classification and keyword discovery using an hmmbased speech recognizer trained without supervision.", "author": ["M.-H. Siu", "H. Gish", "A. Chan", "W. Belfield"], "venue": "TERSPEECH,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2010}, {"title": "Unsupervised discovery of linguistic structure including two-level acoustic patterns using three cascaded stages of iterative optimization", "author": ["C.-T. Chung", "C.-a. Chan", "L.-s. Lee"], "venue": "Acoustics, Speech and Signal Processing (ICASSP), 2013 IEEE International Conference on. IEEE, 2013, pp. 8081\u2013 8085.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2013}, {"title": "Unsupervised models for morpheme segmentation and morphology learning", "author": ["M. Creutz", "K. Lagus"], "venue": "ACM Transactions on Speech and Language Processing (TSLP), vol. 4, no. 1, p. 3, 2007.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2007}, {"title": "Latent dirichlet allocation", "author": ["D.M. Blei", "A.Y. Ng", "M.I. Jordan"], "venue": "the Journal of machine Learning research, vol. 3, pp. 993\u20131022, 2003.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2003}, {"title": "Deep boltzmann machines", "author": ["R. Salakhutdinov", "G.E. Hinton"], "venue": "International Conference on Artificial Intelligence and Statistics, 2009, pp. 448\u2013455.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2009}, {"title": "Long short-term memory", "author": ["S. Hochreiter", "J. Schmidhuber"], "venue": "Neural computation, vol. 9, no. 8, pp. 1735\u20131780, 1997.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 1997}, {"title": "I-vector based speaker recognition on short utterances", "author": ["A. Kanagasundaram", "R. Vogt", "D.B. Dean", "S. Sridharan", "M.W. Mason"], "venue": "Proceedings of the 12th Annual Conference of the International Speech Communication Association. International Speech Communication Association (ISCA), 2011, pp. 2341\u20132344.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2011}, {"title": "zrst", "author": ["C.-T. Chung"], "venue": "https://github.com/C2Tao/zrst, 2014.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2014}, {"title": "The HTK book", "author": ["S. Young", "G. Evermann", "M. Gales", "T. Hain", "D. Kershaw", "X. Liu", "G. Moore", "J. Odell", "D. Ollason", "D. Povey"], "venue": "Entropic Cambridge Research Laboratory Cambridge,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 1997}, {"title": "Srilm-an extensible language modeling toolkit.", "author": ["A. Stolcke"], "venue": "in INTERSPEECH,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2002}, {"title": "MALLET: A Machine Learning for Language Toolkit", "author": ["A.K. McCallum"], "venue": "2002.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2002}, {"title": "The kaldi speech recognition toolkit", "author": ["D. Povey", "A. Ghoshal", "G. Boulianne", "L. Burget", "O. Glembek", "N. Goel", "M. Hannemann", "P. Motlicek", "Y. Qian", "P. Schwarz", "J. Silovsky", "G. Stemmer", "K. Vesely"], "venue": "IEEE 2011 Workshop on Automatic Speech Recognition and Understanding. IEEE Signal Processing Society, Dec. 2011, iEEE Catalog No.: CFP11SRW- USB.", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2011}, {"title": "libdnn", "author": ["P.-W. Chou"], "venue": "https://github.com/botonchou/libdnn, 2014.", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2014}, {"title": "Caffe: Convolutional architecture for fast feature embedding", "author": ["Y. Jia", "E. Shelhamer", "J. Donahue", "S. Karayev", "J. Long", "R. Girshick", "S. Guadarrama", "T. Darrell"], "venue": "arXiv preprint arXiv:1408.5093, 2014.", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2014}, {"title": "Buckeye corpus of conversational speech (2nd release)", "author": ["M.A. Pitt", "L. Dilley", "K. Johnson", "S. Kiesling", "W. Raymond", "E. Hume", "E. Fosler-Lussier"], "venue": "Columbus, OH: Department of Psychology, Ohio State University, 2007.", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2007}, {"title": "A smartphonebased asr data collection tool for under-resourced languages", "author": ["N.J. De Vries", "M.H. Davel", "J. Badenhorst", "W.D. Basson", "F. De Wet", "E. Barnard", "A. De Waal"], "venue": "Speech communication, vol. 56, pp. 119\u2013131, 2014.", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2014}, {"title": "Efficient spoken term discovery using randomized algorithms", "author": ["A. Jansen", "B. Van Durme"], "venue": "Automatic Speech Recognition and Understanding (ASRU), 2011 IEEE Workshop on. IEEE, 2011, pp. 401\u2013406.", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2011}], "referenceMentions": [{"referenceID": 0, "context": "The process is not yet completely understood, and is difficult to be reproduced by current automatic speech recognition (ASR) technologies where the dominant paradigm is supervised learning with large human-annotated data sets[1].", "startOffset": 226, "endOffset": 229}, {"referenceID": 1, "context": "The idea behind the Zero Resource Speech Challenge is to inspire the development of speech recognition under the extreme situation where a whole language has to be learned from scratch[2, 3].", "startOffset": 184, "endOffset": 190}, {"referenceID": 2, "context": "The idea behind the Zero Resource Speech Challenge is to inspire the development of speech recognition under the extreme situation where a whole language has to be learned from scratch[2, 3].", "startOffset": 184, "endOffset": 190}, {"referenceID": 3, "context": "The performance of the feature is evaluated using the ABX discriminability [4] on within and across-speaker phone pairs.", "startOffset": 75, "endOffset": 78}, {"referenceID": 4, "context": "The intervals in which each word unit appears in the corpus is then evaluated on parsing, clustering and matching quality [5].", "startOffset": 122, "endOffset": 125}, {"referenceID": 5, "context": "Each layer carries complementary knowledge about the corpus and the language behind[6].", "startOffset": 83, "endOffset": 86}, {"referenceID": 6, "context": "Since it is well known that speech signals have multi-level structures including at least phonemes and words which are helpful in analysing or decoding speech [7], these sets of acoustic tokens can be further mutually reinforced[8].", "startOffset": 159, "endOffset": 162}, {"referenceID": 7, "context": "Since it is well known that speech signals have multi-level structures including at least phonemes and words which are helpful in analysing or decoding speech [7], these sets of acoustic tokens can be further mutually reinforced[8].", "startOffset": 228, "endOffset": 231}, {"referenceID": 8, "context": "The multi-layered token labels generated by the MAT are then used as the training targets of a Multi-target Deep Neural Network[9] (MDNN) to learn the framewise bottleneck features[10] (BNFs).", "startOffset": 127, "endOffset": 130}, {"referenceID": 9, "context": "The multi-layered token labels generated by the MAT are then used as the training targets of a Multi-target Deep Neural Network[9] (MDNN) to learn the framewise bottleneck features[10] (BNFs).", "startOffset": 180, "endOffset": 184}, {"referenceID": 10, "context": "Using unsupervised HMMs, it is straight forward to discover acoustic tokens from the corpus for a chosen hyperparameter pair \u03c8 that determines the HMM configuration (number of states per model and number of distinct models) [11, 12, 13, 14, 15].", "startOffset": 224, "endOffset": 244}, {"referenceID": 11, "context": "Using unsupervised HMMs, it is straight forward to discover acoustic tokens from the corpus for a chosen hyperparameter pair \u03c8 that determines the HMM configuration (number of states per model and number of distinct models) [11, 12, 13, 14, 15].", "startOffset": 224, "endOffset": 244}, {"referenceID": 12, "context": "Using unsupervised HMMs, it is straight forward to discover acoustic tokens from the corpus for a chosen hyperparameter pair \u03c8 that determines the HMM configuration (number of states per model and number of distinct models) [11, 12, 13, 14, 15].", "startOffset": 224, "endOffset": 244}, {"referenceID": 13, "context": "Using unsupervised HMMs, it is straight forward to discover acoustic tokens from the corpus for a chosen hyperparameter pair \u03c8 that determines the HMM configuration (number of states per model and number of distinct models) [11, 12, 13, 14, 15].", "startOffset": 224, "endOffset": 244}, {"referenceID": 14, "context": "Using unsupervised HMMs, it is straight forward to discover acoustic tokens from the corpus for a chosen hyperparameter pair \u03c8 that determines the HMM configuration (number of states per model and number of distinct models) [11, 12, 13, 14, 15].", "startOffset": 224, "endOffset": 244}, {"referenceID": 13, "context": "This can be achieved by first finding an initial label set \u03c90 based on a set of assumed tokens for all features in the corpus X as in (1) [14].", "startOffset": 138, "endOffset": 142}, {"referenceID": 15, "context": "Latent Dirichlet Allocation[16] (LDA) is preformed for topic modeling, and then each document (new segment) is labeled with the most probable topic.", "startOffset": 27, "endOffset": 31}, {"referenceID": 16, "context": "The tandem feature used as the input of the MDNN can be further augmented by concatenating unsupervised features obtained in other systems such as the Deep Boltzmann Machine[17] (DBM) posteriorgrams, LongShort Term Memory Recurrent Neural Network[18] (LSTM-RNN) autoencoder bottleneck features, and i-vectors[19] trained on MFCC.", "startOffset": 173, "endOffset": 177}, {"referenceID": 17, "context": "The tandem feature used as the input of the MDNN can be further augmented by concatenating unsupervised features obtained in other systems such as the Deep Boltzmann Machine[17] (DBM) posteriorgrams, LongShort Term Memory Recurrent Neural Network[18] (LSTM-RNN) autoencoder bottleneck features, and i-vectors[19] trained on MFCC.", "startOffset": 246, "endOffset": 250}, {"referenceID": 18, "context": "The tandem feature used as the input of the MDNN can be further augmented by concatenating unsupervised features obtained in other systems such as the Deep Boltzmann Machine[17] (DBM) posteriorgrams, LongShort Term Memory Recurrent Neural Network[18] (LSTM-RNN) autoencoder bottleneck features, and i-vectors[19] trained on MFCC.", "startOffset": 308, "endOffset": 312}, {"referenceID": 19, "context": "The MAT is trained using the zrst[20], a python wrapper for the HTK toolkit[21], srilm[22] that we developed for training unsupervised HMMs with varying model granularity.", "startOffset": 33, "endOffset": 37}, {"referenceID": 20, "context": "The MAT is trained using the zrst[20], a python wrapper for the HTK toolkit[21], srilm[22] that we developed for training unsupervised HMMs with varying model granularity.", "startOffset": 75, "endOffset": 79}, {"referenceID": 21, "context": "The MAT is trained using the zrst[20], a python wrapper for the HTK toolkit[21], srilm[22] that we developed for training unsupervised HMMs with varying model granularity.", "startOffset": 86, "endOffset": 90}, {"referenceID": 22, "context": "The LDA tool we used in the Mutual Reinforcement is done with MALLET[23].", "startOffset": 68, "endOffset": 72}, {"referenceID": 20, "context": "The MFCC were extracted using the HTK toolkit[21].", "startOffset": 45, "endOffset": 49}, {"referenceID": 23, "context": "The i-vectors were extracted using Kaldi[24].", "startOffset": 40, "endOffset": 44}, {"referenceID": 24, "context": "The DBM posteriorgram is extracted using libdnn[25].", "startOffset": 47, "endOffset": 51}, {"referenceID": 25, "context": "The MDNN was trained using Caffe[26].", "startOffset": 32, "endOffset": 36}, {"referenceID": 26, "context": "The two official corpora are the Buckeye corpus [27] and NCHLT Xitsonga Speech corpus [28] in English and Tsonga respectively.", "startOffset": 48, "endOffset": 52}, {"referenceID": 27, "context": "The two official corpora are the Buckeye corpus [27] and NCHLT Xitsonga Speech corpus [28] in English and Tsonga respectively.", "startOffset": 86, "endOffset": 90}, {"referenceID": 3, "context": "They are used in the evaluation based on the ABX discriminability test [4] including across and within speaker tests.", "startOffset": 71, "endOffset": 74}, {"referenceID": 4, "context": "The evaluation tool for track 2 provided by the challenge organizers[5] gives five main metrics plus two more scores: NED and coverage.", "startOffset": 68, "endOffset": 71}, {"referenceID": 28, "context": "4, and compared them with the JHU baseline[29] in Table 2 including Precision (P), Recall (R) and F-scores (F).", "startOffset": 42, "endOffset": 46}], "year": 2015, "abstractText": "This paper summarizes the work done by the authors for the Zero Resource Speech Challenge organized in the technical program of Interspeech 2015. The goal of the challenge is to discover linguistic units directly from unlabeled speech data. The Multi-layered Acoustic Tokenizer (MAT) proposed in this work automatically discovers multiple sets of acoustic tokens from the given corpus. Each acoustic token set is specified by a set of hyperparameters that describe the model configuration. These sets of acoustic tokens carry different characteristics of the given corpus and the language behind thus can be mutually reinforced. The multiple sets of token labels are then used as the targets of a Multi-target DNN (MDNN) trained on low-level acoustic features. Bottleneck features extracted from the MDNN are used as feedback for the MAT and the MDNN itself. We call this iterative system the Multi-layered Acoustic Tokenizing Deep Neural Network (MAT-DNN) which generates high quality features for track 1 of the challenge and acoustic tokens for track 2 of the challenge.", "creator": "LaTeX with hyperref package"}}}