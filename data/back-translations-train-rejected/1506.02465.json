{"id": "1506.02465", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "8-Jun-2015", "title": "ASlib: A Benchmark Library for Algorithm Selection", "abstract": "The task of algorithm selection involves choosing an algorithm from a set of algorithms on a per-instance basis in order to exploit the varying performance of algorithms over a set of instances. The algorithm selection problem is attracting increasing attention from researchers and practitioners in AI. Years of fruitful applications in a number of domains have resulted in a large amount of data, but the community lacks a standard format or repository for this data. This situation makes it difficult to share and compare different approaches effectively, as is done in other, more established fields. It also unnecessarily hinders new researchers who want to work in this area. To address this problem, we introduce a standardized format for representing algorithm selection scenarios and a repository that contains a growing number of data sets from the literature. Our format has been designed to be able to express a wide variety of different scenarios. Demonstrating the breadth and power of our platform, we describe a set of example experiments that build and evaluate algorithm selection models through a common interface. The results display the potential of algorithm selection to achieve significant performance improvements across a broad range of problems and algorithms.", "histories": [["v1", "Mon, 8 Jun 2015 12:35:04 GMT  (2356kb,D)", "http://arxiv.org/abs/1506.02465v1", null], ["v2", "Wed, 24 Feb 2016 14:38:52 GMT  (1600kb,D)", "http://arxiv.org/abs/1506.02465v2", null], ["v3", "Wed, 6 Apr 2016 13:20:23 GMT  (2168kb,D)", "http://arxiv.org/abs/1506.02465v3", "Accepted to be published in Artificial Intelligence Journal"]], "reviews": [], "SUBJECTS": "cs.AI cs.LG", "authors": ["bernd bischl", "pascal kerschke", "lars kotthoff", "marius lindauer", "yuri malitsky", "alexandre frechette", "holger hoos", "frank hutter", "kevin leyton-brown", "kevin tierney", "joaquin vanschoren"], "accepted": false, "id": "1506.02465"}, "pdf": {"name": "1506.02465.pdf", "metadata": {"source": "CRF", "title": "ASlib: A Benchmark Library for Algorithm Selection", "authors": ["Bernd Bischl", "Pascal Kerschke", "Lars Kotthoff", "Marius Lindauer", "Yuri Malitsky", "Alexandre Fr\u00e9chette", "Holger Hoos", "Frank Hutter", "Kevin Leyton-Brown", "Kevin Tierney", "Joaquin Vanschoren"], "emails": ["bernd.bischl@stat.uni-muenchen.de", "kerschke@uni-muenster.de", "larsko@cs.ubc.ca", "lindauer@cs.uni-freiburg.de", "yuri.malitsky@gmail.com", "afrechet@cs.ubc.ca", "hoos@cs.ubc.ca", "fh@cs.uni-freiburg.de", "kevinlb@cs.ubc.ca", "tierney@dsor.de", "j.vanschoren@tue.nl"], "sections": [{"heading": null, "text": "The task of algorithm selection is to select an algorithm from a set of algorithms per instance in order to exploit the different performance of algorithms across a set of instances. Increasingly, the problem of algorithm selection is attracting the attention of researchers and practitioners in the field of artificial intelligence. Years of fruitful applications in a number of areas have resulted in a large amount of data, but the community lacks a standard format or repository for that data. This situation makes it difficult to effectively share and compare different approaches, as is the case in other, more established areas. It also unnecessarily hinders new researchers who want to work in this area. To address this problem, we are introducing a standardized format for presenting algorithm selection scenarios and a repository that contains a growing number of data sets from the literature. Our format is designed to be able to express a wide variety of different scenarios."}, {"heading": "1. Introduction", "text": "In fact, most of them are able to survive on their own."}, {"heading": "2. Background", "text": "Rice [76] was the first to formalize the idea of selecting between different algorithms on a per-instance basis. While he described the problem simply as algorithm selection, we prefer the more precise term per-instance algorithm selection to avoid confusion with the (simpler) task of selecting one of several predefined algorithms to optimize performance on a given set or distribution of instances. Definition 1 (per-instance algorithm selection problem) Given a number of problem instances and a distribution D over I, a space of algorithms A and a performance metric m: I \u00d7 A \u2192 R, the per-instance algorithm selection problem is to find a mapping 3 s: I \u2192 A that optimizes egg-Dm (i, s (i), which is achieved by executing the selected algorithm (i), for example, in expectation that an algorithm selection model in instances-per-instance algorithm will solve the many problems associated with it in practice."}, {"heading": "2.1. What to select and when", "text": "The main drawback of this approach is that there is no way to mitigate a poor selection - there is no way to recover if the system chooses an algorithm that performs poorly on the problem. Alternatively, we can aim for a schedule that sets an order and time budget according to which we all or a subset of algorithms in the3In practice, mapping is often implemented by using so-called instance characteristics, i.e. numerical characterizations of instances i-I. These instance characteristics are then assigned to an algorithm that uses machine learning techniques. However, the calculation of instance characteristics incurs additional costs that must be taken into account in performance measurement."}, {"heading": "2.2. How to select", "text": "If only a single algorithm is to be executed, we can train a classification model that makes exactly this prediction, which makes algorithm selection quite simple conceptually - only a single machine learning model needs to be trained and executed to determine which algorithm is to be selected (see e.g. [27, 32, 63]). There are alternatives to using a classification model to select a single algorithm to run on a specific instance, such as using regression models to predict the performance of each algorithm in the portfolio. This regression approach was chosen by early versions of SATzilla [68, 104] as well as several other systems [65, 77, 80]. Other approaches include using cluster techniques to partition problem instances in feature space and make decisions for each partition separately [48, 85] hierarchical models that make a series of decisions [37, 102] and low-cost Vector-13 to choose the best machine for each one]."}, {"heading": "2.3. Selection enablers", "text": "In this context, it should be noted that the solution to problems that have arisen in the past is not a problem, but a problem that has arisen in the past."}, {"heading": "2.4. Algorithm Selection vs. Algorithm Configuration", "text": "One problem closely related to the selection of algorithms is the following algorithm configuration problem: faced with a parameterized algorithm A, a number of problem cases I, and a performance metric m, you will find a parameter setting of A that optimizes the parameter. While the algorithm selection is based on finite (usually small) algorithms, the algorithm configuration works on the combinatorial space of an algorithm's parameter settings. General algorithm configuration methods such as ParamILS [43], GGA [2], I / F races [9], and SMAC [41] have resulted in significant performance improvements (sometimes orders of magnitude) of state-of-the-art algorithms for multiple benchmarks, including SAT-based formal verification [38], mixed integer programming [40], AI planning [95], and the combined selection and hyperparameter optimization of algorithms [92]."}, {"heading": "3. Summary of Format Specification", "text": "In fact, the definition of the algorithm selection problem in Section 1 is tailored to the following generic approach to algorithm selection (Figure 1), in which an algorithm must be applied online to a new problem instance. A vector of instance selection in Section 1 is calculated. Feature calculation can be performed in several stages, each of which produces a group of (one or more) attributes. Further stages may depend on the results of previous stages."}, {"heading": "4. Algorithm Selection Scenarios Provided in ASlib Release 1.0.1", "text": "The algorithm selection scenarios presented in Table 2 in the release version 1.0.1 of our library have been compiled to cover a wide range of problem areas, algorithms, features and problem cases. Our scenarios include both problems that have been extensively investigated in the context of algorithm selection techniques (such as SAT and CSP) and newer ones (such as the container premarshaling problem), all of which come from publications that report performance improvements through algorithm selection and consist of ofalgorithms where the Virtually Best Solver (VBS) 6 is significantly better than the single best algorithm. 7 Therefore, these are problems where it makes sense to seek performance improvements through algorithm selection. All scenarios are available on our online platform (http: / / www.aslib.net /). The scenarios we offer here focus on satisfaction issues, but we encourage readers to submit new scenarios that we briefly describe."}, {"heading": "4.1. SAT: propositional satisfiability", "text": "The Propositional Satifiability Problem (SAT) is a classic NP complete problem, which consists in determining the existence of a mapping of values to variables of a Boolean formula so that the formula is true. It is extensively studied, with many applications including formal verification [72], planning [18], planning [50] and chart coloring [96]. Our SAT data is mainly derived from different iterations of the SAT contest, 8 divided into three tracks: industrial (INDU), craft (HAND) and random (RAND). SAT scenarios are characterized by a high degree of maturity and diversity in terms of their solvers, properties and instances. Each SAT scenario includes a highly diverse set of solvers, many of which have been developed for several years. In addition, the SAT scenarios are the best-studied features."}, {"heading": "4.2. QBF-2011: Quantified Boolean Formula solver evaluation 2010", "text": "A quantified Boolean formula (QBF) is a formula in propositional logic with universal or existential quantifiers for each variable in the formula. A QBF solver finds a set of variable assignments that makes the formula true or proves that such a set cannot exist. This is a PSPACE-complete problem for which solvers have a wide range of features. Our QBF-2011 dataset comes from the QBF Solver Evaluation 20109 and consists of instances of the main, small hard, 2QBF and random tracks. The instance features and solvers are taken from the AQME system and described in more detail by Pulina et al. [74] Although the QBF scenario comprises only five algorithms, this set is highly versatile. Our QBF solvers and instances come from a competitive environment used to evaluate the performance of solvers, similar to the domain SAT just described."}, {"heading": "4.3. MAXSAT12-PMS", "text": "MaxSAT is the optimization version of the previously introduced SAT problem and aims to find a variable allocation that maximizes the number of satisfied clauses. Presentation of the MaxSAT problem can be used to effectively encode a number of real problems, such as FPGA routing [101] and the installation of software packages [4], as it allows to think about both optimization and feasibility. This particular scenario focuses on the partial MaxSAT (PMS) problem [8]. This scenario consists of a collection of random, created and industrial instances from the MaxSAT Evaluation 2012 [5], making it particularly diverse compared to the other scenarios. Techniques used to solve the different instances in this scenario tend to differ significantly from each other, resulting in a significant performance gap between the best single solver and the best virtual solver."}, {"heading": "4.4. CSP-2010: Lazy learning in constraint solving", "text": "Constraint programming [88] is about finding solutions to constraint satisfaction problems - a task that is NP-complete. Learning related to constraint solving is a technique that detects previously unknown constraints implied by the problem specification during the search and then uses them to speed up the solving process.9http: / / www.qbflib.org / index _ eval.phpThe scenario contains only two approaches: one that implies lazy learning [27, 29] and one that does not. [28] The dataset has a strong bias toward non-learning solvers, so the baseline (the best algorithm) is already very good."}, {"heading": "4.5. PROTEUS-2014", "text": "The PROTEUS scenario resulting from [37] includes, in addition to competing SAT solvers that need to solve (convert) XCSP instances, a highly diverse mix of known CSP solvers in SAT (see, for example, [57, 89, 90]) that are available in our format as separate algorithms. In fact, this scenario is the only one in which the solvers are tested with different \"views\" of the same problem. Furthermore, the features of this scenario are also unique in that they include both the SAT and CSP functions for a given instance, potentially providing additional information on the selection approach that would not normally be available for solving CSPs. An algorithm selection system here has a very high degree of flexibility and can choose to perform only a portion of the possible conversions, thereby reducing the set of solvers and features, resulting in the overhead of conversion and feature calculations."}, {"heading": "4.6. ASP-POTASSCO: Answer Set Programming", "text": "Answer Set Programming (ASP, [6, 24]) is a form of declarative programming rooted in knowledge representation, non-monotonous reasoning, and the solution of constraints. Unlike many other areas where constraints are solved (e.g. the satisfaction problem), ASP offers a rich but simple declarative modeling language, in which problems can be expressed up to \u2206 p3 (disjunctive optimization problems). ASP has proven to be efficiently applicable to many real-world applications, e.g. product configuration [83], decision support for NASA shuttle controllers [67], synthesis of multiprocessor systems [46], and team building in industry [31]. Unlike the other scenarios, the algorithms in the ASP scenario were automatically constructed by an adapted version of Hydra [103], i.e. the set of algorithms consists of complex configurations of the bracket [26]."}, {"heading": "4.7. PREMARSHALLING-ASTAR-2013: Container pre-marshalling", "text": "The Container Priority Problem (CPMP) is a NP-hard container stacking problem from the literature of container terminals [84]. We constructed an algorithm selection scenario from two current A * and IDA * approaches to solve the CPMP problem presented in [94] using examples from the literature, described in detail in [93]. The pre-shunting scenario differs from other scenarios in particular due to its highly homogeneous algorithms. Furthermore, all algorithms are parameterization risks of a single, heuristically breaking symmetry, using either A * or IDA * search techniques, which are in sharp contrast to the variety of solutions available in most other datasets. Furthermore, the features provided are new and not as well tested as in the other scenarios, which may be more similar to the characteristics created by domain experts in their first attempt to model a problem, and finally, the scenario presents a true-to-life scenario, where a large-scale literature can be selected."}, {"heading": "5. Automated Exploratory Data Analysis", "text": "\u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf"}, {"heading": "6. Basic Algorithm Selection Experiments", "text": "In this section, we present exploratory benchmark experiments that give an indication of the diversity of our benchmarks. First, we evaluate the performance of the basic algorithm selectors on our scenarios. Then, we conduct a subset of selection studies to identify the important algorithms and instance characteristics in each of the scenarios. We do not claim that the presented experimental settings are exhaustive or that we achieve state-of-the-art algorithm selection performance; rather, we provide basic results that can be achieved through standard machine learning approaches to the core technology of an algorithm selection system - the selector itself. These results, and our framework in general, allow us to examine which algorithm selection approaches work well for which of our scenarios."}, {"heading": "6.1. Data preprocessing", "text": "We removed constantly evaluated (and therefore irrelevant) features and averaged missing feature values over all non-missing values of the feature. We normalized the range of each feature to the interval [\u2212 1, 1]. While this is unnecessary for some machine learning approaches (e.g. decision trees), it is often helpful or mandatory for others (e.g. SVMs or clustering). Missing performance values were calculated using the timeout value for the dataset. For each problem instance, we calculated the cost of the function calculation based on the cost for the function groups stated in the data. If the problem instance was resolved during the function calculation, we considered the cost of the features only up to the one that solved it. In addition, we set the runtime for all algorithms solved during the function calculation to zero."}, {"heading": "6.2. Experimental setup", "text": "We consider three fundamentally different approaches to algorithm selection, which have been extensively investigated in the literature (see Section 2.2): The specific machine learning algorithms we used for our experiments are in Table 4. To provide a good starting point, they contain representatives of the three large approaches mentioned above. We matched the hyperparameters of ksvm and randomForest (classification and regression) with the listed parameter ranges, setting the (maximum) number of clusters to 30 after several experiments; the exact number of clusters was dynamically determined by XMeans."}, {"heading": "6.3. Evaluation", "text": "Each of the algorithm selection models was evaluated using three different metrics: the fraction of all cases resolved within the timeout; the punished average runtime with a penalty factor of 10 (PAR10: this means that runtimes are averaged at ten times the time budget); and the average misclassification penalty (which, for one particular example, is the difference between the performance of the selected algorithm and the performance of the best algorithm).The performance of each algorithm selection model was compared with the virtual best solver (VBS) and the only best solver. Note that the misclassification penalty for VBS is by definition zero. The only best solver is the (actual) solver with the overall best performance on the dataset. Specifically, we consider the solver with the best PAR10 value over all problem cases in a scenario.14"}, {"heading": "6.4. Experimental results", "text": "Figure 5 presents a summary of our experimental results. In most cases, the approaches to algorithm selection performed better than the individual best solution approaches. We expected this because all our data sets came from publications that endorsed algorithm selection systems. Nevertheless, there were significant differences between the scenarios. While most of them almost all approaches to algorithm selection exceeded the individual best algorithm, there are some scenarios that seemed to be much more difficult for algorithm selection. In particular, in the SAT11-INDU scenario, three approaches were unable to achieve performance improvement and all other approaches (with the exception of the random regression forests) improved only slightly. Random regression forests highlighted SAT11 SAT11 SAT11 SAT11 SAT11 SAT11 SAT11 SAT11 SAT11 SAT11 SAT12 SAT11 SAT11 SAT11 SAT11 SAT11 SAT11 SAT11 SAT11 S11 SAT11 SAT11 S11 SAT11 S11 SAT11 S11 SAT11 S11 SAT11 S11 SAT11 S11 S11 SAT11 S11 S11 SAT11 S11 SAT11 S11 S11 SAT11 S11 S11 SAT11 S11 S11 S11 S11 S11 SAT11 S11 S11 SAT11 S11 S11 S11 S11 S11 S11 S11 S11 S11 S11 S11 S11 S11 S11 S11 S11 S11 S11 S11 S11 S11 S11 S11 S11 S11 S11 S11 S11 S11 S11 S11 S11 S11 S11 S11 S11 S11 S11 S11 S11 S11 S11 S11 S11 S11 S11 S11 S11 S11 S11 S11 S11 S11 S11 S11 S11 S11 S11 S11 S11 S11 S11 S11 S11 S11 S11 S11 S11 S11 S11 S11 S11 S11 S11 S11 S11 S11 S11 S11 S11 S11 S11 S11 S11 S11 S11 S11 S11 S11 S11 S11 S11 S11 S11 S11 S11 S11 S11 S11 S11 S11 S11 S11 S11 S"}, {"heading": "6.5. Algorithm and Feature Subset Selection", "text": "In order to gain further insight into our algorithm selection scenarios, we turned to Advance Planning [52] to determine whether smaller subsets still provide comparable performance. We performed Advance Planning independently of algorithms and attributes for each scenario. The process begins with the empty set and inserts the algorithm or attribute into the sentence that most improves random regression (PAR10). Selection is terminated if the score does not improve by at least 1. In all other aspects, the experimental setup was the same as described above."}, {"heading": "7. Conclusion", "text": "We have introduced ASlib, a benchmark library for algorithm selection, a fast-growing field of research with significant implications for various subgroups of artificial intelligence. Release version 1.0.1 of the library includes 12 algorithm selection scenarios across six different areas, with a focus on satisfaction issues (but not limited to). We discussed the format of new algorithm selection scenarios and showed examples of automated exploratory data analysis running for each new scenario submitted on our online platform http: / / aslib.net /. Finally, exploratory experiments with various simple types of algorithm selection systems on our 12 algorithm selection scenarios have shown that even simple algorithm selection systems can dramatically outperform the individual best solver, confirming that random forest models perform best overall. We have achieved performance improvements over the best single entities in all datasets, often reducing the average runtime of algorithms by a factor of 3.2 in the case of best and facilitating by a factor of 3.2 in the case of best."}, {"heading": "Acknowledgements", "text": "We thank the creators of the algorithms and instance distributions used in our various algorithm selection scenarios, and the performance of algorithm selection systems depends crucially on the ingenuity and tireless dedication of domain experts who continue to invent new solutions."}], "references": [{"title": "MaxSAT by Improved Instance-Specific Algorithm Configuration", "author": ["C. Ans\u00f3tegui", "Y. Malitsky", "M. Sellmann"], "venue": "in: Proceedings of the Twenty- Eighth National Conference on Artificial Intelligence", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2014}, {"title": "A gender-based genetic algorithm for the automatic configuration of algorithms", "author": ["C. Ans\u00f3tegui", "M. Sellmann", "K. Tierney"], "venue": "in: Proceedings of the Fifteenth International Conference on Principles and Practice of Constraint Programming", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2009}, {"title": "Continuous search in constraint programming", "author": ["A. Arbelaez", "Y. Hamadi", "M. Sebag"], "venue": "in: Proceedings of the Twenty-Second IEEE International Conference on Tools with Artificial Intelligence,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2010}, {"title": "Solving linux upgradeability problems using boolean optimization", "author": ["J. Argelich", "D.L. Berre", "I. Lynce", "J. Marques-Silva", "P. Rapicault"], "venue": "in: Proceedings of the International Workshop on Logics for Component Configuration,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2010}, {"title": "Knowledge Representation, Reasoning and Declarative Problem Solving", "author": ["C. Baral"], "venue": null, "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2003}, {"title": "Yet another local search solver and Lingeling and friends entering the SAT competition", "author": ["A. Biere"], "venue": "in: Proceedings of SAT Competition 2014: Solver and Benchmark Descriptions,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2014}, {"title": "Empirical Methods for the Analysis of Optimization Algorithms. Springer. chapter F-race and iterated F-race: An overview", "author": ["M. Birattari", "Z. Yuan", "P. Balaprakash", "T. St\u00fctzle"], "venue": null, "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2010}, {"title": "Algorithm Selection Format Specification", "author": ["B. Bischl", "L. Kotthoff", "M. Lindauer", "Y. Malitsky", "A. Frech\u00e9tte", "H. Hoos", "F. Hutter", "P. Kerschke", "K. Leyton-Brown", "J. Vanschoren"], "venue": null, "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2014}, {"title": "2015a. BatchJobs and BatchExperiments: Abstraction mechanisms for using R in batch environments", "author": ["B. Bischl", "M. Lang", "O. Mersmann", "J. Rahnenf\u00fchrer", "C. Weihs"], "venue": "Journal of Statistical Software", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2015}, {"title": "Algorithm selection based on exploratory landscape analysis and cost-sensitive learning", "author": ["B. Bischl", "O. Mersmann", "H. Trautmann", "M. Preuss"], "venue": "in: Proceedings of the Fourteenth Annual Conference on Genetic and Evolutionary Computation,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2012}, {"title": "Resampling methods for meta-model validation with recommendations for evolutionary computation", "author": ["B. Bischl", "O. Mersmann", "H. Trautmann", "C. Weihs"], "venue": "Evolutionary Computation", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2012}, {"title": "Metalearning: Applications to Data Mining. 1 ed", "author": ["P. Brazdil", "C. Giraud-Carrier", "C. Soares", "R. Vilalta"], "venue": null, "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2008}, {"title": "The max k-armed bandit: A new model of exploration applied to search heuristic selection", "author": ["V.A. Cicirello", "S.F. Smith"], "venue": "in: Proceedings of the Twentieth National Conference on Artificial Intelligence,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2005}, {"title": "Maximizing the benefits of parallel search using machine learning", "author": ["D.J. Cook", "R.C. Varnell"], "venue": "in: Proceedings of the Fourteenth National Conference on Artificial Intelligence,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 1997}, {"title": "Experimental results on the application of satisfiability algorithms to scheduling problems", "author": ["J.M. Crawford", "A.B. Baker"], "venue": "Proceedings of the Twelfth National Conference on Artificial Intelligence,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 1994}, {"title": "Self-Adapting linear algebra algorithms and software", "author": ["J. Demmel", "J. Dongarra", "V. Eijkhout", "E. Fuentes", "A. Petitet", "R. Vuduc", "R.C. Whaley", "K. Yelick"], "venue": "Proceedings of the IEEE", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2005}, {"title": "Efficient benchmarking of hyperparameter optimizers via surrogates", "author": ["K. Eggensperger", "F. Hutter", "H.H. Hoos", "K. Leyton-Brown"], "venue": "in: Proceedings of the Twenty-Ninth AAAI Conference on Artificial Intelligence (AAAI)", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2015}, {"title": "Improved features for runtime prediction of domain-independent planners", "author": ["C. Fawcett", "M. Vallati", "F. Hutter", "J. Hoffmann", "H. Hoos", "K. Leyton-Brown"], "venue": "in: Proceedings of the International Conference on Automated Planning and Scheduling", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2014}, {"title": "Learning restart strategies", "author": ["M. Gagliolo", "J. Schmidhuber"], "venue": "in: Proceedings of the Twentieth International Joint Conference on Artificial Intelligence (IJCAI),", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2007}, {"title": "Adaptive online time allocation to search algorithms", "author": ["M. Gagliolo", "V. Zhumatiy", "J. Schmidhuber"], "venue": "in: Proceedings of European Conference on Machine Learning,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2004}, {"title": "Answer Set Solving in Practice", "author": ["M. Gebser", "R. Kaminski", "B. Kaufmann", "T. Schaub"], "venue": "Synthesis Lectures on Artificial Intelligence and Machine Learning,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2012}, {"title": "A portfolio solver for answer set programming: preliminary report", "author": ["M. Gebser", "R. Kaminski", "B. Kaufmann", "T. Schaub", "M.T. Schneider", "S. Ziller"], "venue": "in: Eleventh International Conference on Logic Programming and Nonmonotonic Reasoning,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2011}, {"title": "Multi-threaded ASP solving with clasp", "author": ["M. Gebser", "B. Kaufmann", "T. Schaub"], "venue": "Theory and Practice of Logic Programming", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2012}, {"title": "Learning when to use lazy learning in constraint solving", "author": ["I. Gent", "C. Jefferson", "L. Kotthoff", "I. Miguel", "N. Moore", "P. Nightingale", "K. Petrie"], "venue": "in: Proceedings of the Nineteenth European Conference on Artificial Intelligence,", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2010}, {"title": "MINION: A fast, scalable, constraint solver", "author": ["I.P. Gent", "C.A. Jefferson", "I. Miguel"], "venue": "in: Proceedings of the European Conference on Artificial Intelligence,", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2006}, {"title": "Lazy explanations for constraint propagators", "author": ["I.P. Gent", "I. Miguel", "N.C.A. Moore", "2010b"], "venue": "in: Proceedings of the Twelfth International Symposium on Practical Aspects of Declarative Languages,", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2010}, {"title": "An ASP-based system for team-building in the Gioia-Tauro seaport", "author": ["G. Grasso", "S. Iiritano", "N. Leone", "V. Lio", "F. Ricca", "F. Scalise"], "venue": "in: Proceedings of the Twelfth International Symposium on Practical Aspects of Declarative Languages,", "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2010}, {"title": "Learning techniques for automatic algorithm portfolio selection", "author": ["A. Guerri", "M. Milano"], "venue": "in: Proceedings of the Sixteenth Eureopean Conference on Artificial Intelligence,", "citeRegEx": "32", "shortCiteRegEx": "32", "year": 2004}, {"title": "The WEKA data mining software: An update", "author": ["M. Hall", "E. Frank", "G. Holmes", "B. Pfahringer", "P. Reutemann", "I.H. Witten"], "venue": "SIGKDD Explorations 11,", "citeRegEx": "33", "shortCiteRegEx": "33", "year": 2009}, {"title": "Fast downward stone soup: A baseline for building planner portfolios", "author": ["M. Helmert", "G. R\u00f6ger", "E. Karpas"], "venue": "in: Proceedings of the Workshop on Planning and Learning at the Twenty-First International Conference on Automated Planning and Scheduling,", "citeRegEx": "34", "shortCiteRegEx": "34", "year": 2011}, {"title": "claspfolio 2: Advances in algorithm selection for answer set programming", "author": ["H. Hoos", "M. Lindauer", "T. Schaub"], "venue": "Theory and Practice of Logic", "citeRegEx": "35", "shortCiteRegEx": "35", "year": 2014}, {"title": "aspeed: Solver scheduling via answer set programming. Theory and Practice of Logic Programming", "author": ["H.H. Hoos", "R. Kaminski", "M. Lindauer", "T. Schaub"], "venue": null, "citeRegEx": "36", "shortCiteRegEx": "36", "year": 2014}, {"title": "Proteus: A hierarchical portfolio of solvers and transformations", "author": ["B. Hurley", "L. Kotthoff", "Y. Malitsky", "B. O\u2019Sullivan"], "venue": "in: Proceedings of the Eleventh International Conference on Integration of AI and OR Techniques in Constraint Programming for Combinatorial Optimization Problems,", "citeRegEx": "37", "shortCiteRegEx": "37", "year": 2014}, {"title": "Boosting Verification by Automatic Tuning of Decision Procedures, in: Formal Methods in Computer Aided Design, IEEE Computer Society", "author": ["F. Hutter", "D. Babi\u0107", "H.H. Hoos", "A.J. Hu"], "venue": null, "citeRegEx": "38", "shortCiteRegEx": "38", "year": 2007}, {"title": "Performance prediction and automated tuning of randomized and parametric algorithms", "author": ["F. Hutter", "Y. Hamadi", "H.H. Hoos", "K. Leyton-Brown"], "venue": "in: Proceedings of the Twelfth International Conference on Principles and Practice of Constraint Programming,", "citeRegEx": "39", "shortCiteRegEx": "39", "year": 2006}, {"title": "Automated configuration of mixed integer programming solvers, in: Integration of AI and OR Techniques in Constraint Programming for Combinatorial Optimization Problems", "author": ["F. Hutter", "H.H. Hoos", "K. Leyton-Brown"], "venue": null, "citeRegEx": "40", "shortCiteRegEx": "40", "year": 2010}, {"title": "Sequential model-based optimization for general algorithm configuration", "author": ["F. Hutter", "H.H. Hoos", "K. Leyton-Brown"], "venue": "in: Proceedings of the International Conference on Learning and Intelligent Optimization,", "citeRegEx": "41", "shortCiteRegEx": "41", "year": 2011}, {"title": "Identifying key algorithm parameters and instance features using forward selection, in: LION", "author": ["F. Hutter", "H.H. Hoos", "K. Leyton-Brown"], "venue": null, "citeRegEx": "42", "shortCiteRegEx": "42", "year": 2013}, {"title": "ParamILS: an automatic algorithm configuration framework", "author": ["F. Hutter", "H.H. Hoos", "K. Leyton-Brown", "T. St\u00fctzle"], "venue": "Journal of Artificial Intelligence Research (JAIR)", "citeRegEx": "43", "shortCiteRegEx": "43", "year": 2009}, {"title": "Aclib: a benchmark library for algorithm configuration", "author": ["F. Hutter", "M.L.I. nez", "C. Fawcett", "M. Lindauer", "H. Hoos", "K. Leyton- Brown", "T. St\u00fctzle"], "venue": "in: Proceedings of the International Conference on Learning and Intelligent Optimization,", "citeRegEx": "44", "shortCiteRegEx": "44", "year": 2014}, {"title": "Algorithm runtime prediction: Methods & evaluation", "author": ["F. Hutter", "L. Xu", "H.H. Hoos", "K. Leyton-Brown"], "venue": "Artificial Intelligence", "citeRegEx": "45", "shortCiteRegEx": "45", "year": 2014}, {"title": "Answer set vs integer linear programming for automatic synthesis of multiprocessor systems from real-time parallel programs", "author": ["H. Ishebabi", "P. Mahr", "C. Bobda", "M. Gebser", "T. Schaub"], "venue": "Journal of Reconfigurable Computing", "citeRegEx": "46", "shortCiteRegEx": "46", "year": 2009}, {"title": "Algorithm selection and scheduling", "author": ["S. Kadioglu", "Y. Malitsky", "A. Sabharwal", "H. Samulowitz", "M. Sellmann"], "venue": "in: Proceedings of the International Conference on Principles and Practice of Constraint Programming. Springer. volume 6876 of Lecture Notes in Computer Science,", "citeRegEx": "47", "shortCiteRegEx": "47", "year": 2011}, {"title": "ISAC Instance- Specific Algorithm Configuration", "author": ["S. Kadioglu", "Y. Malitsky", "M. Sellmann", "K. Tierney"], "venue": "in: Proceedings of Nineteenth European Conference on Artificial Intelligence,", "citeRegEx": "48", "shortCiteRegEx": "48", "year": 2010}, {"title": "kernlab \u2013 an S4 package for kernel methods in R", "author": ["A. Karatzoglou", "A. Smola", "K. Hornik", "A. Zeileis"], "venue": "Journal of Statistical Software", "citeRegEx": "49", "shortCiteRegEx": "49", "year": 2004}, {"title": "Unifying SAT-based and graph-based planning", "author": ["H. Kautz", "B. Selman"], "venue": "in: Proceedings of the Sixteenth International Joint Conference on Artifical Intelligence,", "citeRegEx": "50", "shortCiteRegEx": "50", "year": 1999}, {"title": "Cell mapping techniques for exploratory landscape analysis, in: Proceedings of the EVOLVE 2014: A Bridge between Probability, Set Oriented Numerics, and Evolutionary Computation", "author": ["P. Kerschke", "M. Preuss", "C. Hern\u00e1ndez", "O. Sch\u00fctze", "J.Q. Sun", "C. Grimme", "G. Rudolph", "B. Bischl", "H. Trautmann"], "venue": null, "citeRegEx": "51", "shortCiteRegEx": "51", "year": 2014}, {"title": "Wrappers for feature subset selection", "author": ["R. Kohavi", "G.H. John"], "venue": "Artificial Intelligence", "citeRegEx": "52", "shortCiteRegEx": "52", "year": 1997}, {"title": "LLAMA: Leveraging Learning to Automatically Manage Algorithms", "author": ["L. Kotthoff"], "venue": "Technical Report arXiv:1306.1031. arXiv. http://arxiv.org/", "citeRegEx": "53", "shortCiteRegEx": "53", "year": 2013}, {"title": "Algorithm selection for combinatorial search problems: A survey", "author": ["L. Kotthoff"], "venue": "AI Magazine", "citeRegEx": "54", "shortCiteRegEx": "54", "year": 2014}, {"title": "Algorithm selection using reinforcement learning", "author": ["M. Lagoudakis", "M. Littman"], "venue": "in: Proceedings of the Seventeenth International Conference on Machine Learning,", "citeRegEx": "55", "shortCiteRegEx": "55", "year": 2000}, {"title": "Learning to select branching rules in the DPLL procedure for satisfiability", "author": ["M. Lagoudakis", "M. Littman"], "venue": "in: Proceedings of the International Conference on Satisfiability,", "citeRegEx": "56", "shortCiteRegEx": "56", "year": 2001}, {"title": "CSP2SAT4J: A simple CSP to SAT translator", "author": ["D. Le Berre", "I. Lynce"], "venue": "Proceedings of the Second International CSP Solver", "citeRegEx": "57", "shortCiteRegEx": "57", "year": 2008}, {"title": "Selecting classification algorithms with active testing, in: Machine Learning and Data Mining in Pattern Recognition", "author": ["R. Leite", "P. Brazdil", "J. Vanschoren"], "venue": null, "citeRegEx": "58", "shortCiteRegEx": "58", "year": 2012}, {"title": "A portfolio approach to algorithm selection", "author": ["K. Leyton-Brown", "E. Nudelman", "G. Andrew", "J. McFadden", "Y. Shoham"], "venue": "in: Proceedings of the Eighteenth International Joint Conference on Artificial Intelligence,", "citeRegEx": "59", "shortCiteRegEx": "59", "year": 2003}, {"title": "Classification and regression by randomForest", "author": ["A. Liaw", "M. Wiener"], "venue": "R News", "citeRegEx": "60", "shortCiteRegEx": "60", "year": 2002}, {"title": "Evolving instance specific algorithm configuration", "author": ["Y. Malitsky", "D. Mehta", "B. O\u2019Sullivan"], "venue": "in: The Sixth Annual Symposium on Combinatorial Search", "citeRegEx": "61", "shortCiteRegEx": "61", "year": 2013}, {"title": "A portfolio approach to enumerating minimal correction subsets for satisfiability problems", "author": ["Y. Malitsky", "B. O\u2019Sullivan", "A. Previti", "J. Marques-Silva"], "venue": "in: Proceedings of the Eleventh International Conference on Integration of Artificical Intelligence and Operations Research Techniques in Constraint Programming", "citeRegEx": "62", "shortCiteRegEx": "62", "year": 2014}, {"title": "Nonmodel-based algorithm portfolios for SAT", "author": ["Y. Malitsky", "A. Sabharwal", "H. Samulowitz", "M. Sellmann"], "venue": "in: Proceedings of the Fourteenth International Conference on Theory and Applications of Satisfiability Testing,", "citeRegEx": "63", "shortCiteRegEx": "63", "year": 2011}, {"title": "Algorithm portfolios based on cost-sensitive hierarchical clustering", "author": ["Y. Malitsky", "A. Sabharwal", "H. Samulowitz", "M. Sellmann"], "venue": "in: Proceedings of the Twenty-Third International Joint Conference on Artificial Intelligence", "citeRegEx": "64", "shortCiteRegEx": "64", "year": 2013}, {"title": "A novel feature-based approach to characterize algorithm performance for the traveling salesperson problem", "author": ["O. Mersmann", "B. Bischl", "H. Trautmann", "M. Wagner", "J. Bossek", "F. Neumann"], "venue": null, "citeRegEx": "65", "shortCiteRegEx": "65", "year": 2013}, {"title": "Instance-based selection of policies for SAT solvers", "author": ["M. Nikoli\u0107", "F. Mari\u0107", "P. Jani\u010di\u0107"], "venue": "in: Proceedings of the Twelfth International Conference on Theory and Applications of Satisfiability Testing,", "citeRegEx": "66", "shortCiteRegEx": "66", "year": 2009}, {"title": "An A-prolog decision support system for the space shuttle", "author": ["M. Nogueira", "M. Balduccini", "M. Gelfond", "R. Watson", "M. Barry"], "venue": "in: Proceedings of the Third International Symposium on Practical Aspects of Declarative Languages,", "citeRegEx": "67", "shortCiteRegEx": "67", "year": 2001}, {"title": "Understanding random SAT: beyond the Clauses-to-Variables ratio, in: Principles and Practice of Constraint Programming CP", "author": ["E. Nudelman", "K. Leyton-Brown", "H.H. Hoos", "A. Devkar", "Y. Shoham"], "venue": null, "citeRegEx": "69", "shortCiteRegEx": "69", "year": 2004}, {"title": "Using case-based reasoning in an algorithm portfolio for constraint solving", "author": ["E. O\u2019Mahony", "E. Hebrard", "A. Holland", "C. Nugent", "B. O\u2019Sullivan"], "venue": "in: Proceedings of the Nineteenth Irish Conference on Artificial Intelligence and Cognitive Science", "citeRegEx": "70", "shortCiteRegEx": "70", "year": 2008}, {"title": "Meta-learning by landmarking various learning algorithms", "author": ["B. Pfahringer", "H. Bensusan", "C. Giraud-Carrier"], "venue": "Proceedings of the Seventeenth International Conference on Machine", "citeRegEx": "71", "shortCiteRegEx": "71", "year": 2000}, {"title": "A survey of recent advances in SAT-based formal verification", "author": ["M.R. Prasad", "A. Biere", "A. Gupta"], "venue": "International Journal on Software Tools for Technology Transfer", "citeRegEx": "72", "shortCiteRegEx": "72", "year": 2005}, {"title": "A multi-engine solver for quantified boolean formulas", "author": ["L. Pulina", "A. Tacchella"], "venue": "in: Proceedings of the Thirteenth International Conference on Principles and Practice of Constraint Programming,", "citeRegEx": "73", "shortCiteRegEx": "73", "year": 2007}, {"title": "A self-adaptive multi-engine solver for quantified boolean formulas. Constraints", "author": ["L. Pulina", "A. Tacchella"], "venue": null, "citeRegEx": "74", "shortCiteRegEx": "74", "year": 2009}, {"title": "The algorithm selection problem", "author": ["J.R. Rice"], "venue": "Advances in Computers", "citeRegEx": "76", "shortCiteRegEx": "76", "year": 1976}, {"title": "Learned models of performance for many planners", "author": ["M. Roberts", "A.E. Howe"], "venue": "in: Proceedings of the Workshop on AI Planning and Learning at the Seventeenth International Conference on Automated Planning and Scheduling", "citeRegEx": "77", "shortCiteRegEx": "77", "year": 2007}, {"title": "Learning to solve QBF", "author": ["H. Samulowitz", "R. Memisevic"], "venue": "in: Proceedings of the Twenty-Second National Conference on Artificial Intelligence,", "citeRegEx": "78", "shortCiteRegEx": "78", "year": 2007}, {"title": "A survey of intelligent assistants for data analysis", "author": ["F. Serban", "J. Vanschoren", "J.U. Kietz", "A. Bernstein"], "venue": "ACM Comput. Surv", "citeRegEx": "79", "shortCiteRegEx": "79", "year": 2013}, {"title": "Latent class models for algorithm portfolio methods", "author": ["B. Silverthorn", "R. Miikkulainen"], "venue": "in: Proceedings of the Twenty-Fourth AAAI Conference on Artificial Intelligence,", "citeRegEx": "80", "shortCiteRegEx": "80", "year": 2010}, {"title": "Cross-disciplinary perspectives on meta-learning for algorithm selection", "author": ["K.A. Smith-Miles"], "venue": "ACM Computing Surveys", "citeRegEx": "81", "shortCiteRegEx": "81", "year": 2008}, {"title": "Towards objective measures of algorithm performance across instance space", "author": ["K.A. Smith-Miles", "D. Baatar", "B.J. Wreford", "R. Lewis"], "venue": "Computers & Operations Research", "citeRegEx": "82", "shortCiteRegEx": "82", "year": 2014}, {"title": "Developing a declarative rule language for applications in product configuration", "author": ["T. Soininen", "I. Niemel\u00e4"], "venue": "in: Proceedings of the First International Workshop on Practical Aspects of Declarative Languages,", "citeRegEx": "83", "shortCiteRegEx": "83", "year": 1999}, {"title": "Operations research at container terminals: a literature update", "author": ["R. Stahlbock", "S. Vo\u00df"], "venue": "OR Spectrum", "citeRegEx": "84", "shortCiteRegEx": "84", "year": 2008}, {"title": "Heuristics for dynamically adapting propagation in constraint satisfaction problems", "author": ["K. Stergiou"], "venue": "AI Communications", "citeRegEx": "85", "shortCiteRegEx": "85", "year": 2009}, {"title": "Combining multiple heuristics online", "author": ["M.J. Streeter", "D. Golovin", "S.F. Smith"], "venue": "in: Proceedings of the Twenty-Second National Conference on Artificial Intelligence,", "citeRegEx": "86", "shortCiteRegEx": "86", "year": 2007}, {"title": "Restart schedules for ensembles of problem instances", "author": ["M.J. Streeter", "D. Golovin", "S.F. Smith"], "venue": "in: Proceedings of the Twenty-Second National Conference on Artificial Intelligence,", "citeRegEx": "87", "shortCiteRegEx": "87", "year": 2007}, {"title": "The MiniZinc challenge 2008-2013", "author": ["P.J. Stuckey", "T. Feydy", "A. Schutt", "G. Tack", "J. Fischer"], "venue": "AI Magazine", "citeRegEx": "88", "shortCiteRegEx": "88", "year": 2014}, {"title": "System description of a SATbased CSP solver sugar", "author": ["N. Tamura", "T. Tanjo", "M. Banbara"], "venue": "Proceedings of the Third International CSP Solver", "citeRegEx": "89", "shortCiteRegEx": "89", "year": 2008}, {"title": "Azucar: a SAT-based CSP solver using compact order encoding, in: Theory and Applications of Satisfiability Testing \u2013 SAT", "author": ["T. Tanjo", "N. Tamura", "M. Banbara"], "venue": null, "citeRegEx": "90", "shortCiteRegEx": "90", "year": 2012}, {"title": "rpart: Recursive Partitioning and Regression Trees. URL: http://CRAN.R-project.org/package= rpart. r package version 4.1-8", "author": ["T. Therneau", "B. Atkinson", "B. Ripley"], "venue": null, "citeRegEx": "91", "shortCiteRegEx": "91", "year": 2014}, {"title": "Auto-WEKA: Combined selection and hyperparameter optimization of classification algorithms", "author": ["C. Thornton", "F. Hutter", "H. Hoos", "K. Leyton-Brown"], "venue": "in: Proceedings of the Nineteenth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining,", "citeRegEx": "92", "shortCiteRegEx": "92", "year": 2013}, {"title": "An algorithm selection benchmark of the container pre-marshalling", "author": ["K. Tierney", "Y. Malitsky"], "venue": "Proceedings of the Ninth International Conference Learning and Intelligent Optimisation LION (To Appear),", "citeRegEx": "93", "shortCiteRegEx": "93", "year": 2015}, {"title": "Solving the Pre-Marshalling Problem to Optimality with A* and IDA", "author": ["K. Tierney", "D. Pacino", "S. Vo\u00df"], "venue": "Technical Report Working Paper #1401", "citeRegEx": "94", "shortCiteRegEx": "94", "year": 2014}, {"title": "Automatic generation of efficient domain-optimized planners from generic parametrized planners, in: International Symposium on Combinatorial Search (SoCS)", "author": ["M. Vallati", "C. Fawcett", "A. Gerevini", "H.H. Hoos", "A. Saetti"], "venue": null, "citeRegEx": "95", "shortCiteRegEx": "95", "year": 2013}, {"title": "Another look at graph coloring via propositional satisfiability", "author": ["A. Van Gelder"], "venue": "Discrete Applied Mathematics", "citeRegEx": "96", "shortCiteRegEx": "96", "year": 2008}, {"title": "Understanding Machine Learning Performance with Experiment Databases", "author": ["J. Vanschoren"], "venue": "Ph.D. thesis. University of Leuven", "citeRegEx": "97", "shortCiteRegEx": "97", "year": 2010}, {"title": "Experiment databases. A new way to share, organize and learn from experiments", "author": ["J. Vanschoren", "H. Blockeel", "B. Pfahringer", "G. Holmes"], "venue": "Machine Learning", "citeRegEx": "98", "shortCiteRegEx": "98", "year": 2012}, {"title": "OpenML: Networked science in machine learning", "author": ["J. Vanschoren", "J.N. van Rijn", "B. Bischl", "L. Torgo"], "venue": "SIGKDD Explorations", "citeRegEx": "99", "shortCiteRegEx": "99", "year": 2013}, {"title": "Hierarchical grouping to optimize an objective function", "author": ["J. Ward"], "venue": "Journal of the American Statistical Association", "citeRegEx": "100", "shortCiteRegEx": "100", "year": 1963}, {"title": "Sub-SAT: A formulation for relaxed boolean satisfiability with applications in routing", "author": ["H. Xu", "R. Rutenbar", "K. Sakallah"], "venue": "in: IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems,", "citeRegEx": "101", "shortCiteRegEx": "101", "year": 2003}, {"title": "Hierarchical hardness models for SAT", "author": ["L. Xu", "H.H. Hoos", "K. Leyton-Brown"], "venue": "in: Proceedings of the Thirteenth International Conference on Principles and Practice of Constraint Programming,", "citeRegEx": "102", "shortCiteRegEx": "102", "year": 2007}, {"title": "Hydra: Automatically configuring algorithms for Portfolio-Based selection", "author": ["L. Xu", "H.H. Hoos", "K. Leyton-Brown"], "venue": "in: Proceedings of the Twenty-Fourth AAAI Conference on Artificial Intelligence,", "citeRegEx": "103", "shortCiteRegEx": "103", "year": 2010}, {"title": "SATzilla: portfolio-based algorithm selection for SAT", "author": ["L. Xu", "F. Hutter", "H.H. Hoos", "K. Leyton-Brown"], "venue": "Journal of Artificial Intelligence Research", "citeRegEx": "104", "shortCiteRegEx": "104", "year": 2008}, {"title": "Hydra-MIP: automated algorithm configuration and selection for mixed integer programming, in: Proceedings of the RCRA Workshop on Experimental Evaluation of Algorithms for Solving Problems with Combinatorial Explosion at the Twenty-Second International", "author": ["L. Xu", "F. Hutter", "H.H. Hoos", "K. Leyton-Brown"], "venue": "Joint Conference on Artificial Intelligence", "citeRegEx": "105", "shortCiteRegEx": "105", "year": 2011}, {"title": "Evaluating component solver contributions to Portfolio-Based algorithm selectors", "author": ["L. Xu", "F. Hutter", "H.H. Hoos", "K. Leyton-Brown"], "venue": "in: Proceedings of the Fifteenth International Conference on Theory and Applications of Satisfiability Testing,", "citeRegEx": "106", "shortCiteRegEx": "106", "year": 2012}, {"title": "Satzilla2012: Improved algorithm selection based on cost-sensitive classification models", "author": ["L. Xu", "F. Hutter", "J. Shen", "H.H. Hoos", "K. Leyton-Brown"], "venue": "Proceedings of SAT Challenge 2012: Solver and Benchmark Descriptions ,", "citeRegEx": "107", "shortCiteRegEx": "107", "year": 2012}], "referenceMentions": [{"referenceID": 99, "context": "In particular, this fact has been observed to hold across a wide variety of AI applications, including propositional satisfiability (SAT) [106], constraint satisfaction (CSP) [70], AI planning [34], and supervised machine learning [92, 98].", "startOffset": 138, "endOffset": 143}, {"referenceID": 64, "context": "In particular, this fact has been observed to hold across a wide variety of AI applications, including propositional satisfiability (SAT) [106], constraint satisfaction (CSP) [70], AI planning [34], and supervised machine learning [92, 98].", "startOffset": 175, "endOffset": 179}, {"referenceID": 29, "context": "In particular, this fact has been observed to hold across a wide variety of AI applications, including propositional satisfiability (SAT) [106], constraint satisfaction (CSP) [70], AI planning [34], and supervised machine learning [92, 98].", "startOffset": 193, "endOffset": 197}, {"referenceID": 85, "context": "In particular, this fact has been observed to hold across a wide variety of AI applications, including propositional satisfiability (SAT) [106], constraint satisfaction (CSP) [70], AI planning [34], and supervised machine learning [92, 98].", "startOffset": 231, "endOffset": 239}, {"referenceID": 91, "context": "In particular, this fact has been observed to hold across a wide variety of AI applications, including propositional satisfiability (SAT) [106], constraint satisfaction (CSP) [70], AI planning [34], and supervised machine learning [92, 98].", "startOffset": 231, "endOffset": 239}, {"referenceID": 69, "context": "An alternative is to accept that no single algorithm will offer the best performance on all instances, and instead aim to identify a portfolio of complementary algorithms and a strategy for choosing between them [76].", "startOffset": 212, "endOffset": 216}, {"referenceID": 5, "context": "The best of the 35 submitted solvers, Lingeling ayv [7], solved 77% of the 300 instances.", "startOffset": 52, "endOffset": 55}, {"referenceID": 69, "context": "Research on this algorithm selection problem [76] has demonstrated the practical feasibility of using machine learning for this task.", "startOffset": 45, "endOffset": 49}, {"referenceID": 97, "context": "The area began to attract considerable attention when methods based on algorithm selection began to outperform standalone solvers in SAT competitions [104].", "startOffset": 150, "endOffset": 155}, {"referenceID": 64, "context": "Algorithm selectors have since come to dominate the state of the art on many other problems, including CSP [70], AI planning [34], Max-SAT [61], QBF [74], and ASP [25].", "startOffset": 107, "endOffset": 111}, {"referenceID": 29, "context": "Algorithm selectors have since come to dominate the state of the art on many other problems, including CSP [70], AI planning [34], Max-SAT [61], QBF [74], and ASP [25].", "startOffset": 125, "endOffset": 129}, {"referenceID": 56, "context": "Algorithm selectors have since come to dominate the state of the art on many other problems, including CSP [70], AI planning [34], Max-SAT [61], QBF [74], and ASP [25].", "startOffset": 139, "endOffset": 143}, {"referenceID": 68, "context": "Algorithm selectors have since come to dominate the state of the art on many other problems, including CSP [70], AI planning [34], Max-SAT [61], QBF [74], and ASP [25].", "startOffset": 149, "endOffset": 153}, {"referenceID": 21, "context": "Algorithm selectors have since come to dominate the state of the art on many other problems, including CSP [70], AI planning [34], Max-SAT [61], QBF [74], and ASP [25].", "startOffset": 163, "endOffset": 167}, {"referenceID": 69, "context": "Background Rice [76] was the first to formalize the idea of selecting among different algorithms on a per-instance basis.", "startOffset": 16, "endOffset": 20}, {"referenceID": 97, "context": "This is used in the SATzilla [68, 104], ArgoSmArT [66], SALSA [19] and Eureka [17] systems, to name but a few examples.", "startOffset": 29, "endOffset": 38}, {"referenceID": 61, "context": "This is used in the SATzilla [68, 104], ArgoSmArT [66], SALSA [19] and Eureka [17] systems, to name but a few examples.", "startOffset": 50, "endOffset": 54}, {"referenceID": 15, "context": "This is used in the SATzilla [68, 104], ArgoSmArT [66], SALSA [19] and Eureka [17] systems, to name but a few examples.", "startOffset": 62, "endOffset": 66}, {"referenceID": 13, "context": "This is used in the SATzilla [68, 104], ArgoSmArT [66], SALSA [19] and Eureka [17] systems, to name but a few examples.", "startOffset": 78, "endOffset": 82}, {"referenceID": 31, "context": ", [36, 47, 70, 74]).", "startOffset": 2, "endOffset": 18}, {"referenceID": 42, "context": ", [36, 47, 70, 74]).", "startOffset": 2, "endOffset": 18}, {"referenceID": 64, "context": ", [36, 47, 70, 74]).", "startOffset": 2, "endOffset": 18}, {"referenceID": 68, "context": ", [36, 47, 70, 74]).", "startOffset": 2, "endOffset": 18}, {"referenceID": 12, "context": ", [16, 22, 30, 87]).", "startOffset": 2, "endOffset": 18}, {"referenceID": 18, "context": ", [16, 22, 30, 87]).", "startOffset": 2, "endOffset": 18}, {"referenceID": 80, "context": ", [16, 22, 30, 87]).", "startOffset": 2, "endOffset": 18}, {"referenceID": 19, "context": "Such methods monitor the execution of the chosen algorithm(s) and take remedial action if performance deviates from what is expected [23, 58, 62], or perform selection repeatedly for subproblems of the given instance [3, 55, 56, 78].", "startOffset": 133, "endOffset": 145}, {"referenceID": 53, "context": "Such methods monitor the execution of the chosen algorithm(s) and take remedial action if performance deviates from what is expected [23, 58, 62], or perform selection repeatedly for subproblems of the given instance [3, 55, 56, 78].", "startOffset": 133, "endOffset": 145}, {"referenceID": 57, "context": "Such methods monitor the execution of the chosen algorithm(s) and take remedial action if performance deviates from what is expected [23, 58, 62], or perform selection repeatedly for subproblems of the given instance [3, 55, 56, 78].", "startOffset": 133, "endOffset": 145}, {"referenceID": 2, "context": "Such methods monitor the execution of the chosen algorithm(s) and take remedial action if performance deviates from what is expected [23, 58, 62], or perform selection repeatedly for subproblems of the given instance [3, 55, 56, 78].", "startOffset": 217, "endOffset": 232}, {"referenceID": 50, "context": "Such methods monitor the execution of the chosen algorithm(s) and take remedial action if performance deviates from what is expected [23, 58, 62], or perform selection repeatedly for subproblems of the given instance [3, 55, 56, 78].", "startOffset": 217, "endOffset": 232}, {"referenceID": 51, "context": "Such methods monitor the execution of the chosen algorithm(s) and take remedial action if performance deviates from what is expected [23, 58, 62], or perform selection repeatedly for subproblems of the given instance [3, 55, 56, 78].", "startOffset": 217, "endOffset": 232}, {"referenceID": 71, "context": "Such methods monitor the execution of the chosen algorithm(s) and take remedial action if performance deviates from what is expected [23, 58, 62], or perform selection repeatedly for subproblems of the given instance [3, 55, 56, 78].", "startOffset": 217, "endOffset": 232}, {"referenceID": 23, "context": ", [27, 32, 63]).", "startOffset": 2, "endOffset": 14}, {"referenceID": 27, "context": ", [27, 32, 63]).", "startOffset": 2, "endOffset": 14}, {"referenceID": 58, "context": ", [27, 32, 63]).", "startOffset": 2, "endOffset": 14}, {"referenceID": 97, "context": "This regression approach was adopted by early versions of SATzilla [68, 104], as well as by several other systems [65, 77, 80].", "startOffset": 67, "endOffset": 76}, {"referenceID": 60, "context": "This regression approach was adopted by early versions of SATzilla [68, 104], as well as by several other systems [65, 77, 80].", "startOffset": 114, "endOffset": 126}, {"referenceID": 70, "context": "This regression approach was adopted by early versions of SATzilla [68, 104], as well as by several other systems [65, 77, 80].", "startOffset": 114, "endOffset": 126}, {"referenceID": 73, "context": "This regression approach was adopted by early versions of SATzilla [68, 104], as well as by several other systems [65, 77, 80].", "startOffset": 114, "endOffset": 126}, {"referenceID": 43, "context": "Other approaches include the use of clustering techniques to partition problem instances in feature space and make decisions for each partition separately [48, 85], hierarchical models that make a series of decisions [37, 102], and costsensitive support vector machines [13].", "startOffset": 155, "endOffset": 163}, {"referenceID": 78, "context": "Other approaches include the use of clustering techniques to partition problem instances in feature space and make decisions for each partition separately [48, 85], hierarchical models that make a series of decisions [37, 102], and costsensitive support vector machines [13].", "startOffset": 155, "endOffset": 163}, {"referenceID": 32, "context": "Other approaches include the use of clustering techniques to partition problem instances in feature space and make decisions for each partition separately [48, 85], hierarchical models that make a series of decisions [37, 102], and costsensitive support vector machines [13].", "startOffset": 217, "endOffset": 226}, {"referenceID": 95, "context": "Other approaches include the use of clustering techniques to partition problem instances in feature space and make decisions for each partition separately [48, 85], hierarchical models that make a series of decisions [37, 102], and costsensitive support vector machines [13].", "startOffset": 217, "endOffset": 226}, {"referenceID": 9, "context": "Other approaches include the use of clustering techniques to partition problem instances in feature space and make decisions for each partition separately [48, 85], hierarchical models that make a series of decisions [37, 102], and costsensitive support vector machines [13].", "startOffset": 270, "endOffset": 274}, {"referenceID": 98, "context": "The current version of SATzilla [105] uses cost-sensitive decision forests to determine the best algorithm for each pair of algorithms and selects the overall best by aggregating these \u201cvotes\u201d.", "startOffset": 32, "endOffset": 37}, {"referenceID": 19, "context": "Some approaches use only past performance of the algorithms in the portfolio as a basis for selecting the one(s) to be run on a given problem instance [23, 80, 86].", "startOffset": 151, "endOffset": 163}, {"referenceID": 73, "context": "Some approaches use only past performance of the algorithms in the portfolio as a basis for selecting the one(s) to be run on a given problem instance [23, 80, 86].", "startOffset": 151, "endOffset": 163}, {"referenceID": 79, "context": "Some approaches use only past performance of the algorithms in the portfolio as a basis for selecting the one(s) to be run on a given problem instance [23, 80, 86].", "startOffset": 151, "endOffset": 163}, {"referenceID": 23, "context": "Deeper analysis can involve properties of graph representations derived from the input instance (such as the constraint graph [27, 59]) or properties of encodings into different problems (such as SAT features for SAT-encoded planning problems [21]).", "startOffset": 126, "endOffset": 134}, {"referenceID": 54, "context": "Deeper analysis can involve properties of graph representations derived from the input instance (such as the constraint graph [27, 59]) or properties of encodings into different problems (such as SAT features for SAT-encoded planning problems [21]).", "startOffset": 126, "endOffset": 134}, {"referenceID": 17, "context": "Deeper analysis can involve properties of graph representations derived from the input instance (such as the constraint graph [27, 59]) or properties of encodings into different problems (such as SAT features for SAT-encoded planning problems [21]).", "startOffset": 243, "endOffset": 247}, {"referenceID": 40, "context": ", [45, 69, 70, 73, 104]).", "startOffset": 2, "endOffset": 23}, {"referenceID": 63, "context": ", [45, 69, 70, 73, 104]).", "startOffset": 2, "endOffset": 23}, {"referenceID": 64, "context": ", [45, 69, 70, 73, 104]).", "startOffset": 2, "endOffset": 23}, {"referenceID": 67, "context": ", [45, 69, 70, 73, 104]).", "startOffset": 2, "endOffset": 23}, {"referenceID": 97, "context": ", [45, 69, 70, 73, 104]).", "startOffset": 2, "endOffset": 23}, {"referenceID": 9, "context": "For continuous blackbox optimization, algorithm selection can be performed based on Exploratory Landscape Analysis [13, 51, 65].", "startOffset": 115, "endOffset": 127}, {"referenceID": 46, "context": "For continuous blackbox optimization, algorithm selection can be performed based on Exploratory Landscape Analysis [13, 51, 65].", "startOffset": 115, "endOffset": 127}, {"referenceID": 60, "context": "For continuous blackbox optimization, algorithm selection can be performed based on Exploratory Landscape Analysis [13, 51, 65].", "startOffset": 115, "endOffset": 127}, {"referenceID": 11, "context": "g, [15]), these features are known as meta-features.", "startOffset": 3, "endOffset": 7}, {"referenceID": 65, "context": ", variable entropy), landmarkers (measurements of the performance of fast algorithms [71]), sampling landmarkers (similar to probing features) and model-based meta-features [97].", "startOffset": 85, "endOffset": 89}, {"referenceID": 90, "context": ", variable entropy), landmarkers (measurements of the performance of fast algorithms [71]), sampling landmarkers (similar to probing features) and model-based meta-features [97].", "startOffset": 173, "endOffset": 177}, {"referenceID": 92, "context": "These meta-features, and the past performance measurements of many machine learning algorithms, are available from the online machine learning platform OpenML [99].", "startOffset": 159, "endOffset": 163}, {"referenceID": 38, "context": "General algorithm configuration methods, such as ParamILS [43], GGA [2], I/F-Race [9], and SMAC [41], have yielded substantial performance improvements (sometimes orders of magnitude", "startOffset": 58, "endOffset": 62}, {"referenceID": 1, "context": "General algorithm configuration methods, such as ParamILS [43], GGA [2], I/F-Race [9], and SMAC [41], have yielded substantial performance improvements (sometimes orders of magnitude", "startOffset": 68, "endOffset": 71}, {"referenceID": 6, "context": "General algorithm configuration methods, such as ParamILS [43], GGA [2], I/F-Race [9], and SMAC [41], have yielded substantial performance improvements (sometimes orders of magnitude", "startOffset": 82, "endOffset": 85}, {"referenceID": 36, "context": "General algorithm configuration methods, such as ParamILS [43], GGA [2], I/F-Race [9], and SMAC [41], have yielded substantial performance improvements (sometimes orders of magnitude", "startOffset": 96, "endOffset": 100}, {"referenceID": 33, "context": "speedups) of state-of-the-art algorithms for several benchmarks, including SATbased formal verification [38], mixed integer programming [40], AI planning [95], and the combined selection and hyperparameter optimization of machine learning algorithms [92].", "startOffset": 104, "endOffset": 108}, {"referenceID": 35, "context": "speedups) of state-of-the-art algorithms for several benchmarks, including SATbased formal verification [38], mixed integer programming [40], AI planning [95], and the combined selection and hyperparameter optimization of machine learning algorithms [92].", "startOffset": 136, "endOffset": 140}, {"referenceID": 88, "context": "speedups) of state-of-the-art algorithms for several benchmarks, including SATbased formal verification [38], mixed integer programming [40], AI planning [95], and the combined selection and hyperparameter optimization of machine learning algorithms [92].", "startOffset": 154, "endOffset": 158}, {"referenceID": 85, "context": "speedups) of state-of-the-art algorithms for several benchmarks, including SATbased formal verification [38], mixed integer programming [40], AI planning [95], and the combined selection and hyperparameter optimization of machine learning algorithms [92].", "startOffset": 250, "endOffset": 254}, {"referenceID": 0, "context": "Consequently, several possibilities exist for combining algorithm configuration and selection [1, 39, 48, 61, 103, 105].", "startOffset": 94, "endOffset": 119}, {"referenceID": 34, "context": "Consequently, several possibilities exist for combining algorithm configuration and selection [1, 39, 48, 61, 103, 105].", "startOffset": 94, "endOffset": 119}, {"referenceID": 43, "context": "Consequently, several possibilities exist for combining algorithm configuration and selection [1, 39, 48, 61, 103, 105].", "startOffset": 94, "endOffset": 119}, {"referenceID": 56, "context": "Consequently, several possibilities exist for combining algorithm configuration and selection [1, 39, 48, 61, 103, 105].", "startOffset": 94, "endOffset": 119}, {"referenceID": 96, "context": "Consequently, several possibilities exist for combining algorithm configuration and selection [1, 39, 48, 61, 103, 105].", "startOffset": 94, "endOffset": 119}, {"referenceID": 98, "context": "Consequently, several possibilities exist for combining algorithm configuration and selection [1, 39, 48, 61, 103, 105].", "startOffset": 94, "endOffset": 119}, {"referenceID": 39, "context": "The algorithm configuration counterpart of ASlib is AClib [44] (http://aclib.", "startOffset": 58, "endOffset": 62}, {"referenceID": 49, "context": "A full coverage of the wide literature on algorithm selection is beyond the scope of this article, but we refer the interested reader to recent survey articles on the topic [54, 79, 81].", "startOffset": 173, "endOffset": 185}, {"referenceID": 72, "context": "A full coverage of the wide literature on algorithm selection is beyond the scope of this article, but we refer the interested reader to recent survey articles on the topic [54, 79, 81].", "startOffset": 173, "endOffset": 185}, {"referenceID": 74, "context": "A full coverage of the wide literature on algorithm selection is beyond the scope of this article, but we refer the interested reader to recent survey articles on the topic [54, 79, 81].", "startOffset": 173, "endOffset": 185}, {"referenceID": 16, "context": "One way of mitigating it is offered by fast-to-evaluate surrogate algorithm configuration benchmarks [20].", "startOffset": 101, "endOffset": 105}, {"referenceID": 7, "context": "Table 1 introduces the basic structure of our format definition; the complete specification with all details can be found in an accompanying technical report [10] and on our online platform.", "startOffset": 158, "endOffset": 162}, {"referenceID": 97, "context": "scenario #I #A #F #Fg Costs Literature SAT11-HAND 296 15 115 10 X [104] SAT11-INDU 300 18 115 10 X [104] SAT11-RAND 600 9 115 10 X [104] SAT12-ALL 1614 31 115 10 X [107] SAT12-HAND 767 31 115 10 X [107] SAT12-INDU 1167 31 115 10 X [107] SAT12-RAND 1362 31 115 10 X [107] QBF-2011 1368 5 46 1 \u00d7 [74] MAXSAT12-PMS 876 6 37 1 X [61] CSP-2010 2024 2 17 1 \u00d7 [27] PROTEUS-2014 4021 22 198 4 X [37] ASP-POTASSCO 1294 11 138 5 X [35] PREMARSHALLING-ASTAR-2013 527 4 16 1 \u00d7 [93] Table 2: Overview of algorithm selection scenarios in the ASLib with the number of instances #I, number of algorithms #A, number of features #F , number of feature processing groups #Fg and availability of feature costs.", "startOffset": 66, "endOffset": 71}, {"referenceID": 97, "context": "scenario #I #A #F #Fg Costs Literature SAT11-HAND 296 15 115 10 X [104] SAT11-INDU 300 18 115 10 X [104] SAT11-RAND 600 9 115 10 X [104] SAT12-ALL 1614 31 115 10 X [107] SAT12-HAND 767 31 115 10 X [107] SAT12-INDU 1167 31 115 10 X [107] SAT12-RAND 1362 31 115 10 X [107] QBF-2011 1368 5 46 1 \u00d7 [74] MAXSAT12-PMS 876 6 37 1 X [61] CSP-2010 2024 2 17 1 \u00d7 [27] PROTEUS-2014 4021 22 198 4 X [37] ASP-POTASSCO 1294 11 138 5 X [35] PREMARSHALLING-ASTAR-2013 527 4 16 1 \u00d7 [93] Table 2: Overview of algorithm selection scenarios in the ASLib with the number of instances #I, number of algorithms #A, number of features #F , number of feature processing groups #Fg and availability of feature costs.", "startOffset": 99, "endOffset": 104}, {"referenceID": 97, "context": "scenario #I #A #F #Fg Costs Literature SAT11-HAND 296 15 115 10 X [104] SAT11-INDU 300 18 115 10 X [104] SAT11-RAND 600 9 115 10 X [104] SAT12-ALL 1614 31 115 10 X [107] SAT12-HAND 767 31 115 10 X [107] SAT12-INDU 1167 31 115 10 X [107] SAT12-RAND 1362 31 115 10 X [107] QBF-2011 1368 5 46 1 \u00d7 [74] MAXSAT12-PMS 876 6 37 1 X [61] CSP-2010 2024 2 17 1 \u00d7 [27] PROTEUS-2014 4021 22 198 4 X [37] ASP-POTASSCO 1294 11 138 5 X [35] PREMARSHALLING-ASTAR-2013 527 4 16 1 \u00d7 [93] Table 2: Overview of algorithm selection scenarios in the ASLib with the number of instances #I, number of algorithms #A, number of features #F , number of feature processing groups #Fg and availability of feature costs.", "startOffset": 131, "endOffset": 136}, {"referenceID": 100, "context": "scenario #I #A #F #Fg Costs Literature SAT11-HAND 296 15 115 10 X [104] SAT11-INDU 300 18 115 10 X [104] SAT11-RAND 600 9 115 10 X [104] SAT12-ALL 1614 31 115 10 X [107] SAT12-HAND 767 31 115 10 X [107] SAT12-INDU 1167 31 115 10 X [107] SAT12-RAND 1362 31 115 10 X [107] QBF-2011 1368 5 46 1 \u00d7 [74] MAXSAT12-PMS 876 6 37 1 X [61] CSP-2010 2024 2 17 1 \u00d7 [27] PROTEUS-2014 4021 22 198 4 X [37] ASP-POTASSCO 1294 11 138 5 X [35] PREMARSHALLING-ASTAR-2013 527 4 16 1 \u00d7 [93] Table 2: Overview of algorithm selection scenarios in the ASLib with the number of instances #I, number of algorithms #A, number of features #F , number of feature processing groups #Fg and availability of feature costs.", "startOffset": 164, "endOffset": 169}, {"referenceID": 100, "context": "scenario #I #A #F #Fg Costs Literature SAT11-HAND 296 15 115 10 X [104] SAT11-INDU 300 18 115 10 X [104] SAT11-RAND 600 9 115 10 X [104] SAT12-ALL 1614 31 115 10 X [107] SAT12-HAND 767 31 115 10 X [107] SAT12-INDU 1167 31 115 10 X [107] SAT12-RAND 1362 31 115 10 X [107] QBF-2011 1368 5 46 1 \u00d7 [74] MAXSAT12-PMS 876 6 37 1 X [61] CSP-2010 2024 2 17 1 \u00d7 [27] PROTEUS-2014 4021 22 198 4 X [37] ASP-POTASSCO 1294 11 138 5 X [35] PREMARSHALLING-ASTAR-2013 527 4 16 1 \u00d7 [93] Table 2: Overview of algorithm selection scenarios in the ASLib with the number of instances #I, number of algorithms #A, number of features #F , number of feature processing groups #Fg and availability of feature costs.", "startOffset": 197, "endOffset": 202}, {"referenceID": 100, "context": "scenario #I #A #F #Fg Costs Literature SAT11-HAND 296 15 115 10 X [104] SAT11-INDU 300 18 115 10 X [104] SAT11-RAND 600 9 115 10 X [104] SAT12-ALL 1614 31 115 10 X [107] SAT12-HAND 767 31 115 10 X [107] SAT12-INDU 1167 31 115 10 X [107] SAT12-RAND 1362 31 115 10 X [107] QBF-2011 1368 5 46 1 \u00d7 [74] MAXSAT12-PMS 876 6 37 1 X [61] CSP-2010 2024 2 17 1 \u00d7 [27] PROTEUS-2014 4021 22 198 4 X [37] ASP-POTASSCO 1294 11 138 5 X [35] PREMARSHALLING-ASTAR-2013 527 4 16 1 \u00d7 [93] Table 2: Overview of algorithm selection scenarios in the ASLib with the number of instances #I, number of algorithms #A, number of features #F , number of feature processing groups #Fg and availability of feature costs.", "startOffset": 231, "endOffset": 236}, {"referenceID": 100, "context": "scenario #I #A #F #Fg Costs Literature SAT11-HAND 296 15 115 10 X [104] SAT11-INDU 300 18 115 10 X [104] SAT11-RAND 600 9 115 10 X [104] SAT12-ALL 1614 31 115 10 X [107] SAT12-HAND 767 31 115 10 X [107] SAT12-INDU 1167 31 115 10 X [107] SAT12-RAND 1362 31 115 10 X [107] QBF-2011 1368 5 46 1 \u00d7 [74] MAXSAT12-PMS 876 6 37 1 X [61] CSP-2010 2024 2 17 1 \u00d7 [27] PROTEUS-2014 4021 22 198 4 X [37] ASP-POTASSCO 1294 11 138 5 X [35] PREMARSHALLING-ASTAR-2013 527 4 16 1 \u00d7 [93] Table 2: Overview of algorithm selection scenarios in the ASLib with the number of instances #I, number of algorithms #A, number of features #F , number of feature processing groups #Fg and availability of feature costs.", "startOffset": 265, "endOffset": 270}, {"referenceID": 68, "context": "scenario #I #A #F #Fg Costs Literature SAT11-HAND 296 15 115 10 X [104] SAT11-INDU 300 18 115 10 X [104] SAT11-RAND 600 9 115 10 X [104] SAT12-ALL 1614 31 115 10 X [107] SAT12-HAND 767 31 115 10 X [107] SAT12-INDU 1167 31 115 10 X [107] SAT12-RAND 1362 31 115 10 X [107] QBF-2011 1368 5 46 1 \u00d7 [74] MAXSAT12-PMS 876 6 37 1 X [61] CSP-2010 2024 2 17 1 \u00d7 [27] PROTEUS-2014 4021 22 198 4 X [37] ASP-POTASSCO 1294 11 138 5 X [35] PREMARSHALLING-ASTAR-2013 527 4 16 1 \u00d7 [93] Table 2: Overview of algorithm selection scenarios in the ASLib with the number of instances #I, number of algorithms #A, number of features #F , number of feature processing groups #Fg and availability of feature costs.", "startOffset": 294, "endOffset": 298}, {"referenceID": 56, "context": "scenario #I #A #F #Fg Costs Literature SAT11-HAND 296 15 115 10 X [104] SAT11-INDU 300 18 115 10 X [104] SAT11-RAND 600 9 115 10 X [104] SAT12-ALL 1614 31 115 10 X [107] SAT12-HAND 767 31 115 10 X [107] SAT12-INDU 1167 31 115 10 X [107] SAT12-RAND 1362 31 115 10 X [107] QBF-2011 1368 5 46 1 \u00d7 [74] MAXSAT12-PMS 876 6 37 1 X [61] CSP-2010 2024 2 17 1 \u00d7 [27] PROTEUS-2014 4021 22 198 4 X [37] ASP-POTASSCO 1294 11 138 5 X [35] PREMARSHALLING-ASTAR-2013 527 4 16 1 \u00d7 [93] Table 2: Overview of algorithm selection scenarios in the ASLib with the number of instances #I, number of algorithms #A, number of features #F , number of feature processing groups #Fg and availability of feature costs.", "startOffset": 325, "endOffset": 329}, {"referenceID": 23, "context": "scenario #I #A #F #Fg Costs Literature SAT11-HAND 296 15 115 10 X [104] SAT11-INDU 300 18 115 10 X [104] SAT11-RAND 600 9 115 10 X [104] SAT12-ALL 1614 31 115 10 X [107] SAT12-HAND 767 31 115 10 X [107] SAT12-INDU 1167 31 115 10 X [107] SAT12-RAND 1362 31 115 10 X [107] QBF-2011 1368 5 46 1 \u00d7 [74] MAXSAT12-PMS 876 6 37 1 X [61] CSP-2010 2024 2 17 1 \u00d7 [27] PROTEUS-2014 4021 22 198 4 X [37] ASP-POTASSCO 1294 11 138 5 X [35] PREMARSHALLING-ASTAR-2013 527 4 16 1 \u00d7 [93] Table 2: Overview of algorithm selection scenarios in the ASLib with the number of instances #I, number of algorithms #A, number of features #F , number of feature processing groups #Fg and availability of feature costs.", "startOffset": 353, "endOffset": 357}, {"referenceID": 32, "context": "scenario #I #A #F #Fg Costs Literature SAT11-HAND 296 15 115 10 X [104] SAT11-INDU 300 18 115 10 X [104] SAT11-RAND 600 9 115 10 X [104] SAT12-ALL 1614 31 115 10 X [107] SAT12-HAND 767 31 115 10 X [107] SAT12-INDU 1167 31 115 10 X [107] SAT12-RAND 1362 31 115 10 X [107] QBF-2011 1368 5 46 1 \u00d7 [74] MAXSAT12-PMS 876 6 37 1 X [61] CSP-2010 2024 2 17 1 \u00d7 [27] PROTEUS-2014 4021 22 198 4 X [37] ASP-POTASSCO 1294 11 138 5 X [35] PREMARSHALLING-ASTAR-2013 527 4 16 1 \u00d7 [93] Table 2: Overview of algorithm selection scenarios in the ASLib with the number of instances #I, number of algorithms #A, number of features #F , number of feature processing groups #Fg and availability of feature costs.", "startOffset": 387, "endOffset": 391}, {"referenceID": 30, "context": "scenario #I #A #F #Fg Costs Literature SAT11-HAND 296 15 115 10 X [104] SAT11-INDU 300 18 115 10 X [104] SAT11-RAND 600 9 115 10 X [104] SAT12-ALL 1614 31 115 10 X [107] SAT12-HAND 767 31 115 10 X [107] SAT12-INDU 1167 31 115 10 X [107] SAT12-RAND 1362 31 115 10 X [107] QBF-2011 1368 5 46 1 \u00d7 [74] MAXSAT12-PMS 876 6 37 1 X [61] CSP-2010 2024 2 17 1 \u00d7 [27] PROTEUS-2014 4021 22 198 4 X [37] ASP-POTASSCO 1294 11 138 5 X [35] PREMARSHALLING-ASTAR-2013 527 4 16 1 \u00d7 [93] Table 2: Overview of algorithm selection scenarios in the ASLib with the number of instances #I, number of algorithms #A, number of features #F , number of feature processing groups #Fg and availability of feature costs.", "startOffset": 421, "endOffset": 425}, {"referenceID": 86, "context": "scenario #I #A #F #Fg Costs Literature SAT11-HAND 296 15 115 10 X [104] SAT11-INDU 300 18 115 10 X [104] SAT11-RAND 600 9 115 10 X [104] SAT12-ALL 1614 31 115 10 X [107] SAT12-HAND 767 31 115 10 X [107] SAT12-INDU 1167 31 115 10 X [107] SAT12-RAND 1362 31 115 10 X [107] QBF-2011 1368 5 46 1 \u00d7 [74] MAXSAT12-PMS 876 6 37 1 X [61] CSP-2010 2024 2 17 1 \u00d7 [27] PROTEUS-2014 4021 22 198 4 X [37] ASP-POTASSCO 1294 11 138 5 X [35] PREMARSHALLING-ASTAR-2013 527 4 16 1 \u00d7 [93] Table 2: Overview of algorithm selection scenarios in the ASLib with the number of instances #I, number of algorithms #A, number of features #F , number of feature processing groups #Fg and availability of feature costs.", "startOffset": 465, "endOffset": 469}, {"referenceID": 66, "context": "It is widely studied, with many applications including formal verification [72], scheduling [18], planning [50] and graph coloring [96].", "startOffset": 75, "endOffset": 79}, {"referenceID": 14, "context": "It is widely studied, with many applications including formal verification [72], scheduling [18], planning [50] and graph coloring [96].", "startOffset": 92, "endOffset": 96}, {"referenceID": 45, "context": "It is widely studied, with many applications including formal verification [72], scheduling [18], planning [50] and graph coloring [96].", "startOffset": 107, "endOffset": 111}, {"referenceID": 89, "context": "It is widely studied, with many applications including formal verification [72], scheduling [18], planning [50] and graph coloring [96].", "startOffset": 131, "endOffset": 135}, {"referenceID": 68, "context": "[74].", "startOffset": 0, "endOffset": 4}, {"referenceID": 94, "context": "The MaxSAT problem representation can be used to effectively encode a number of real-world problems, such as FPGA routing [101], and software package installation [4], among others, as it permits reasoning about both optimality and feasibility.", "startOffset": 122, "endOffset": 127}, {"referenceID": 3, "context": "The MaxSAT problem representation can be used to effectively encode a number of real-world problems, such as FPGA routing [101], and software package installation [4], among others, as it permits reasoning about both optimality and feasibility.", "startOffset": 163, "endOffset": 166}, {"referenceID": 81, "context": "CSP-2010: Lazy learning in constraint solving Constraint programming [88] is concerned with finding solutions to constraint satisfaction problems\u2014a task that is NP-complete.", "startOffset": 69, "endOffset": 73}, {"referenceID": 23, "context": "The scenario contains only two solvers: one that employs lazy learning [27, 29] and one that does not [28].", "startOffset": 71, "endOffset": 79}, {"referenceID": 25, "context": "The scenario contains only two solvers: one that employs lazy learning [27, 29] and one that does not [28].", "startOffset": 71, "endOffset": 79}, {"referenceID": 24, "context": "The scenario contains only two solvers: one that employs lazy learning [27, 29] and one that does not [28].", "startOffset": 102, "endOffset": 106}, {"referenceID": 32, "context": "PROTEUS-2014 The PROTEUS scenario, stemming from [37], includes an extremely diverse mix of well-known CSP solvers alongside competition-winning SAT solvers that have to solve (converted) XCSP instances.", "startOffset": 49, "endOffset": 53}, {"referenceID": 52, "context": ", [57, 89, 90]), which in our format are provided as separate algorithms.", "startOffset": 2, "endOffset": 14}, {"referenceID": 82, "context": ", [57, 89, 90]), which in our format are provided as separate algorithms.", "startOffset": 2, "endOffset": 14}, {"referenceID": 83, "context": ", [57, 89, 90]), which in our format are provided as separate algorithms.", "startOffset": 2, "endOffset": 14}, {"referenceID": 4, "context": "ASP-POTASSCO: Answer Set Programming Answer Set Programming (ASP, [6, 24]) is a form of declarative programming with roots in knowledge representation, non-monotonic reasoning and constraint solving.", "startOffset": 66, "endOffset": 73}, {"referenceID": 20, "context": "ASP-POTASSCO: Answer Set Programming Answer Set Programming (ASP, [6, 24]) is a form of declarative programming with roots in knowledge representation, non-monotonic reasoning and constraint solving.", "startOffset": 66, "endOffset": 73}, {"referenceID": 76, "context": ", product configuration [83], decision support for NASA shuttle controllers [67], synthesis of multiprocessor systems [46] and industrial team building [31].", "startOffset": 24, "endOffset": 28}, {"referenceID": 62, "context": ", product configuration [83], decision support for NASA shuttle controllers [67], synthesis of multiprocessor systems [46] and industrial team building [31].", "startOffset": 76, "endOffset": 80}, {"referenceID": 41, "context": ", product configuration [83], decision support for NASA shuttle controllers [67], synthesis of multiprocessor systems [46] and industrial team building [31].", "startOffset": 118, "endOffset": 122}, {"referenceID": 26, "context": ", product configuration [83], decision support for NASA shuttle controllers [67], synthesis of multiprocessor systems [46] and industrial team building [31].", "startOffset": 152, "endOffset": 156}, {"referenceID": 96, "context": "In contrast to the other scenarios, the algorithms in the ASP scenario were automatically constructed by an adapted version of Hydra [103], i.", "startOffset": 133, "endOffset": 138}, {"referenceID": 22, "context": ", the set of algorithms consists of complementary configurations of the solver clasp [26].", "startOffset": 85, "endOffset": 89}, {"referenceID": 32, "context": "html as described in [37].", "startOffset": 21, "endOffset": 25}, {"referenceID": 21, "context": "static and probing features organized into feature groups; they were previously used in the algorithm selector claspfolio [25, 35].", "startOffset": 122, "endOffset": 130}, {"referenceID": 30, "context": "static and probing features organized into feature groups; they were previously used in the algorithm selector claspfolio [25, 35].", "startOffset": 122, "endOffset": 130}, {"referenceID": 77, "context": "PREMARSHALLING-ASTAR-2013: Container pre-marshalling The container pre-marshalling problem (CPMP) is an NP-hard container stacking problem from the container terminals literature [84].", "startOffset": 179, "endOffset": 183}, {"referenceID": 87, "context": "We constructed an algorithm selection scenario from two recent A* and IDA* approaches for solving the CPMP presented in [94], using instances from the literature.", "startOffset": 120, "endOffset": 124}, {"referenceID": 86, "context": "The scenario is described in detail in [93].", "startOffset": 39, "endOffset": 43}, {"referenceID": 99, "context": "Because detecting correlation in algorithm performance is also of interest when analyzing the strengths and weaknesses of a given portfolio-based solver [106], we also present a clustered correlation matrix, cf.", "startOffset": 153, "endOffset": 158}, {"referenceID": 93, "context": "The algorithms are also clustered according to these values (using Ward\u2019s method [100]) and then sorted, such that similar algorithms appear together in blocks.", "startOffset": 81, "endOffset": 86}, {"referenceID": 75, "context": "For the future, we plan to extend our data analysis by additional techniques, such as further measures of algorithm performance [82].", "startOffset": 128, "endOffset": 132}, {"referenceID": 59, "context": "In order to reach the performance of current state-of-the-art algorithm selection systems [64, 107], we would have to include various extensions, such as cost-sensitive classification and complementary techniques such as pre-solving.", "startOffset": 90, "endOffset": 99}, {"referenceID": 100, "context": "In order to reach the performance of current state-of-the-art algorithm selection systems [64, 107], we would have to include various extensions, such as cost-sensitive classification and complementary techniques such as pre-solving.", "startOffset": 90, "endOffset": 99}, {"referenceID": 48, "context": "We use the LLAMA toolkit [53], version 0.", "startOffset": 25, "endOffset": 29}, {"referenceID": 8, "context": "We parallelize all of our benchmark experiments through the BatchExperiments [11] R package.", "startOffset": 77, "endOffset": 81}, {"referenceID": 97, "context": "[104].", "startOffset": 0, "endOffset": 5}, {"referenceID": 44, "context": "Technical Name Algorithm and parameter ranges reference classification ksvm support vector machine [49] C \u2208 [2\u221212, 2], \u03b3 \u2208 [2\u221212, 2] randomForest random forest [60] ntree \u2208 [10, 200], mtry \u2208 [1, 30] rpart recursive partitioning tree, CART [91] regression lm linear regression [75] randomForest random forest [60] ntree \u2208 [10, 200], mtry \u2208 [1, 30] rpart recursive partitioning tree, CART [91] clustering XMeans extended k-means clustering [33] Table 4: Machine learning algorithms and their parameter ranges used for our experiments.", "startOffset": 99, "endOffset": 103}, {"referenceID": 55, "context": "Technical Name Algorithm and parameter ranges reference classification ksvm support vector machine [49] C \u2208 [2\u221212, 2], \u03b3 \u2208 [2\u221212, 2] randomForest random forest [60] ntree \u2208 [10, 200], mtry \u2208 [1, 30] rpart recursive partitioning tree, CART [91] regression lm linear regression [75] randomForest random forest [60] ntree \u2208 [10, 200], mtry \u2208 [1, 30] rpart recursive partitioning tree, CART [91] clustering XMeans extended k-means clustering [33] Table 4: Machine learning algorithms and their parameter ranges used for our experiments.", "startOffset": 160, "endOffset": 164}, {"referenceID": 7, "context": "Technical Name Algorithm and parameter ranges reference classification ksvm support vector machine [49] C \u2208 [2\u221212, 2], \u03b3 \u2208 [2\u221212, 2] randomForest random forest [60] ntree \u2208 [10, 200], mtry \u2208 [1, 30] rpart recursive partitioning tree, CART [91] regression lm linear regression [75] randomForest random forest [60] ntree \u2208 [10, 200], mtry \u2208 [1, 30] rpart recursive partitioning tree, CART [91] clustering XMeans extended k-means clustering [33] Table 4: Machine learning algorithms and their parameter ranges used for our experiments.", "startOffset": 173, "endOffset": 182}, {"referenceID": 0, "context": "Technical Name Algorithm and parameter ranges reference classification ksvm support vector machine [49] C \u2208 [2\u221212, 2], \u03b3 \u2208 [2\u221212, 2] randomForest random forest [60] ntree \u2208 [10, 200], mtry \u2208 [1, 30] rpart recursive partitioning tree, CART [91] regression lm linear regression [75] randomForest random forest [60] ntree \u2208 [10, 200], mtry \u2208 [1, 30] rpart recursive partitioning tree, CART [91] clustering XMeans extended k-means clustering [33] Table 4: Machine learning algorithms and their parameter ranges used for our experiments.", "startOffset": 191, "endOffset": 198}, {"referenceID": 84, "context": "Technical Name Algorithm and parameter ranges reference classification ksvm support vector machine [49] C \u2208 [2\u221212, 2], \u03b3 \u2208 [2\u221212, 2] randomForest random forest [60] ntree \u2208 [10, 200], mtry \u2208 [1, 30] rpart recursive partitioning tree, CART [91] regression lm linear regression [75] randomForest random forest [60] ntree \u2208 [10, 200], mtry \u2208 [1, 30] rpart recursive partitioning tree, CART [91] clustering XMeans extended k-means clustering [33] Table 4: Machine learning algorithms and their parameter ranges used for our experiments.", "startOffset": 239, "endOffset": 243}, {"referenceID": 55, "context": "Technical Name Algorithm and parameter ranges reference classification ksvm support vector machine [49] C \u2208 [2\u221212, 2], \u03b3 \u2208 [2\u221212, 2] randomForest random forest [60] ntree \u2208 [10, 200], mtry \u2208 [1, 30] rpart recursive partitioning tree, CART [91] regression lm linear regression [75] randomForest random forest [60] ntree \u2208 [10, 200], mtry \u2208 [1, 30] rpart recursive partitioning tree, CART [91] clustering XMeans extended k-means clustering [33] Table 4: Machine learning algorithms and their parameter ranges used for our experiments.", "startOffset": 308, "endOffset": 312}, {"referenceID": 7, "context": "Technical Name Algorithm and parameter ranges reference classification ksvm support vector machine [49] C \u2208 [2\u221212, 2], \u03b3 \u2208 [2\u221212, 2] randomForest random forest [60] ntree \u2208 [10, 200], mtry \u2208 [1, 30] rpart recursive partitioning tree, CART [91] regression lm linear regression [75] randomForest random forest [60] ntree \u2208 [10, 200], mtry \u2208 [1, 30] rpart recursive partitioning tree, CART [91] clustering XMeans extended k-means clustering [33] Table 4: Machine learning algorithms and their parameter ranges used for our experiments.", "startOffset": 321, "endOffset": 330}, {"referenceID": 0, "context": "Technical Name Algorithm and parameter ranges reference classification ksvm support vector machine [49] C \u2208 [2\u221212, 2], \u03b3 \u2208 [2\u221212, 2] randomForest random forest [60] ntree \u2208 [10, 200], mtry \u2208 [1, 30] rpart recursive partitioning tree, CART [91] regression lm linear regression [75] randomForest random forest [60] ntree \u2208 [10, 200], mtry \u2208 [1, 30] rpart recursive partitioning tree, CART [91] clustering XMeans extended k-means clustering [33] Table 4: Machine learning algorithms and their parameter ranges used for our experiments.", "startOffset": 339, "endOffset": 346}, {"referenceID": 84, "context": "Technical Name Algorithm and parameter ranges reference classification ksvm support vector machine [49] C \u2208 [2\u221212, 2], \u03b3 \u2208 [2\u221212, 2] randomForest random forest [60] ntree \u2208 [10, 200], mtry \u2208 [1, 30] rpart recursive partitioning tree, CART [91] regression lm linear regression [75] randomForest random forest [60] ntree \u2208 [10, 200], mtry \u2208 [1, 30] rpart recursive partitioning tree, CART [91] clustering XMeans extended k-means clustering [33] Table 4: Machine learning algorithms and their parameter ranges used for our experiments.", "startOffset": 387, "endOffset": 391}, {"referenceID": 28, "context": "Technical Name Algorithm and parameter ranges reference classification ksvm support vector machine [49] C \u2208 [2\u221212, 2], \u03b3 \u2208 [2\u221212, 2] randomForest random forest [60] ntree \u2208 [10, 200], mtry \u2208 [1, 30] rpart recursive partitioning tree, CART [91] regression lm linear regression [75] randomForest random forest [60] ntree \u2208 [10, 200], mtry \u2208 [1, 30] rpart recursive partitioning tree, CART [91] clustering XMeans extended k-means clustering [33] Table 4: Machine learning algorithms and their parameter ranges used for our experiments.", "startOffset": 438, "endOffset": 442}, {"referenceID": 40, "context": "This is in line with recent results showing the strong performance of this model for algorithm runtime prediction [45].", "startOffset": 114, "endOffset": 118}, {"referenceID": 99, "context": "[106] reported somewhat better results for the three SAT11 datasets than the one achieved here with our off-theshelf methods (which is to be expected since their latest SATzilla version used a cost-sensitive approach and pre-solving schedules).", "startOffset": 0, "endOffset": 5}, {"referenceID": 47, "context": "Algorithm and Feature Subset Selection To provide further insights into our algorithm selection scenarios, we applied forward selection [52] to the algorithms and features to determine whether smaller subsets still achieve comparable performance.", "startOffset": 136, "endOffset": 140}, {"referenceID": 10, "context": "We note that the selection results use normal resampling and not the nested version, which may result in overconfident performance estimates for the selected subsets [14].", "startOffset": 166, "endOffset": 170}], "year": 2017, "abstractText": "The task of algorithm selection involves choosing an algorithm from a set of algorithms on a per-instance basis in order to exploit the varying performance of algorithms over a set of instances. The algorithm selection problem is attracting increasing attention from researchers and practitioners in AI. Years of fruitful applications in a number of domains have resulted in a large amount of data, but the community lacks a standard format or repository for this data. This situation makes it difficult to share and compare different approaches effectively, as is done in other, more established fields. It also unnecessarily hinders new researchers who want to work in this area. To address this problem, we introduce a standardized format for representing algorithm selection scenarios and a repository that contains a growing number of data sets from the literature. Our format has been designed to be able to express a wide variety of different scenarios. Demonstrating the breadth and power of our platform, we describe a set of example experiments that build and evaluate algorithm selection models through a common interface. The results display the potential of algorithm selection to achieve significant performance improvements across a broad range of problems and algorithms.", "creator": "LaTeX with hyperref package"}}}