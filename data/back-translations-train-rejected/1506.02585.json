{"id": "1506.02585", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "8-Jun-2015", "title": "Optimal Sparse Kernel Learning for Hyperspectral Anomaly Detection", "abstract": "In this paper, a novel framework of sparse kernel learning for Support Vector Data Description (SVDD) based anomaly detection is presented. In this work, optimal sparse feature selection for anomaly detection is first modeled as a Mixed Integer Programming (MIP) problem. Due to the prohibitively high computational complexity of the MIP, it is relaxed into a Quadratically Constrained Linear Programming (QCLP) problem. The QCLP problem can then be practically solved by using an iterative optimization method, in which multiple subsets of features are iteratively found as opposed to a single subset. The QCLP-based iterative optimization problem is solved in a finite space called the \\emph{Empirical Kernel Feature Space} (EKFS) instead of in the input space or \\emph{Reproducing Kernel Hilbert Space} (RKHS). This is possible because of the fact that the geometrical properties of the EKFS and the corresponding RKHS remain the same. Now, an explicit nonlinear exploitation of the data in a finite EKFS is achievable, which results in optimal feature ranking. Experimental results based on a hyperspectral image show that the proposed method can provide improved performance over the current state-of-the-art techniques.", "histories": [["v1", "Mon, 8 Jun 2015 16:51:40 GMT  (523kb,D)", "http://arxiv.org/abs/1506.02585v1", "4 pages, 1 figure, 5th workshop on Hyperspectral image and signal processing: evolution in remote sensing"]], "COMMENTS": "4 pages, 1 figure, 5th workshop on Hyperspectral image and signal processing: evolution in remote sensing", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["zhimin peng", "prudhvi gurram", "heesung kwon", "wotao yin"], "accepted": false, "id": "1506.02585"}, "pdf": {"name": "1506.02585.pdf", "metadata": {"source": "CRF", "title": "OPTIMAL SPARSE KERNEL LEARNING FOR HYPERSPECTRAL ANOMALY DETECTION", "authors": ["Prudhvi Gurram", "Heesung Kwon", "Zhimin Peng", "Wotao Yin"], "emails": [], "sections": [{"heading": null, "text": "Index terms - economical kernel learning, optimal feature selection, empirical kernel feature space, empirical kernel map"}, {"heading": "1. INTRODUCTION", "text": "This year, it is so far that it is only a matter of time before it is ready, until it is ready."}, {"heading": "2. OPTIMAL SPARSE KERNEL LEARNING", "text": "In this section, we present an optimal, frugal learning for anomalous detection (OSKLAD) = problem that SVDD uses as a basic building block. Inspired by the approach of property selection for kernel-based classification [1], OSKLAD addresses the problem of optimal property selection for SVDD-based anomaly detection. OSKLAD's basic formulation is to minimize the radius of the surrounding hypersphere while allowing outliers, except that OSKLAD uses only a subset of properties. Thus, the model is described as a mixed integer programming problem: min d min R, an R2 + C, an R2 + N that can be used in the surrounding hypersphere, while outlier (x) \u2212 a subset of features that are actually used. The model is described as a mixed integer-programming problem."}, {"heading": "3. OPTIMAL FEATURE SELECTION: FINDING", "text": "In order to update d, the features that maximum violate the last constraint in Eq.3 must be determined. Since the objective of Eq.3 is to maximize t, and it is limited upwards according to the constraint by S (\u03b1, d), the features that maximum violate this constraint will minimize S (\u03b1, d). It is necessary to solve the following optimization problem: min d S (\u03b1, d) is subject to M \u2211 i = 1 di = Bdi \u01090, 1}. (7) In this section, we describe the method for finding these characteristic vectors for both the linear and the non-linear core."}, {"heading": "3.1. Linear Kernel", "text": "When a linear nucleus is used, because k (xi, xj) = < xi, xj >, we have S (\u03b1, d) = \u2211 M j = 1 djcj, where cj = \u2211 N i = 1 \u03b1ix 2 ij + (\u2211 N i = 1 \u03b1ixij) 2. S (\u03b1, d) has a linear function of d. Once we have optimal support vectors, the global solution of d can easily be achieved by sorting the CJ's in ascending order and setting the first B-corresponding elements in d, dj to 1 and the rest to 0. Once the optimal feature subset for a nucleus is selected, optimal \u03b1 and \u00b5 are updated by solving Equation 6. These two steps are repeated until the algorithm converges."}, {"heading": "3.2. Non-linear Kernel", "text": "When a Gaussian RBF kernel is used, S (\u03b1, d) is not a linear function of d. < We cannot optimally solve the problem in Eq. 7 due to the large number of combinations of characteristics that need to be taken into account. Thus, the data of infinite RKHS characteristics are transformed into another space called empirical attribute space (EKFS) with finite dimensionality using empirical core map (EKM), which allows us to optimally select subsets of characteristics while maintaining the nonlinear correlations between the characteristics. For a given set of training points {xi} ni = 1, the map defined by kernel n is defined: Rn \u2192 Rn, where x 7 \u2192 k (x1, x),."}, {"heading": "4. SIMULATION RESULTS", "text": "In this section, the performance of OSKLAD is evaluated on a hyperspectral digital imaging experiment (HYDICE) image, which contains 30 small painted channels in the background. We chose a small patch (69 pixels x 10 pixels) as background data set, which is used to obtain the radius R and the center of the hypersphere. The distance of each test pixel in the image to the center of the hypersphere is determined. If the distance is greater than R, the pixel is considered an anomaly, otherwise it is a background pixel. In our experiments, the performance of SVDD, SKAD [8] 1 and OSKLAD is compared with both linear and Gaussian RBF cores. For SVDD and SKAD, both linear and GBF kernels are used in the input area."}, {"heading": "5. CONCLUSIONS", "text": "In the proposed work, the QCLP problem is optimally solved in a new finite space called the Empirical Kernel Feature Space (EKFS) instead of the RKHS in order to achieve an optimal selection of kernel-based features for detecting anomalies using SVDD. Experimental results show that by optimally selecting features, significant improvements can be achieved in detecting hyperspectral anomalies in the EKFS and not in the original input space."}, {"heading": "6. REFERENCES", "text": "[1] M. Tan, L. Wang, and I. W. Tsang, \"Learning sparse SVM for feature selection on very high dimensional datasets,\" 1Sparse kernel-based anomaly detection (SKAD) was developed by two of the current authors. [3] R. Hettich and K. O. Kortanek, \"Semi-infinite programming: Theory, methods, and applications,\" SIAM Review, vol. 35, no., pp. 380-429, September 1993. [4] R. Hettich and K. O. Kortanek, \"Semi-infinite programming: Theory, methods, and applications,\" SIAM Review, vol. 35, no., pp. 380-429, September 1993. [4] A. Rakotomamonjy, F. R. Bach, S. Canu, and Y. Grandvalet, \"Simplemkl,\"."}], "references": [{"title": "Learning sparse SVM for feature selection on very high dimensional datasets", "author": ["M. Tan", "L. Wang", "I.W. Tsang"], "venue": "1Sparse kernel-based anomaly detection (SKAD) has been developed by two of the current authors  (a) SVDD \u2013 linear kernel (b) SVDD \u2013 RBF kernel (c) SKAD \u2013 linear kernel (d) SKAD \u2013 RBF kernel (e) OSKLAD \u2013 linear kernel (f) OSKLAD \u2013 EKFS Fig. 1. Anomaly detection results of the HYDICE image using SVDD, SKAD and OSKLAD in ICML, Haifa, Israel, June 2010, pp. 1047\u20131054.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2010}, {"title": "Learning with Kernels", "author": ["B. Sch\u00f6lkopf", "A.J. Smola"], "venue": "The MIT Press, Massachusetts,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2002}, {"title": "Semi-infinite programming: Theory, methods, and applications", "author": ["R. Hettich", "K.O. Kortanek"], "venue": "SIAM Review, vol. 35, no. 3, pp. 380\u2013429, Sept. 1993.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 1993}, {"title": "Simplemkl", "author": ["A. Rakotomamonjy", "F.R. Bach", "S. Canu", "Y. Grandvalet"], "venue": "J. Machine Learning Research, vol. 9, pp. 2491\u20132521, 2008.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2008}, {"title": "The cutting plane method for solving convex problems", "author": ["J.E. Kelly"], "venue": "J. Soc. Indust. Appl. Math., vol. 8, no. 4, Dec. 1960.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 1960}, {"title": "Training SVM with indefinite kernels", "author": ["J. Chen", "J. Ye"], "venue": "ICML, Helsinki, June 2008, pp. 136\u2013143.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2008}, {"title": "Optimal sparse kernel learning for anomaly detection", "author": ["P. Gurram", "Z. Peng", "H. Kwon", "W. Yin"], "venue": "Pattern Recognition, under review.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 0}, {"title": "Sparse kernel-based hyperspectral anomaly detection", "author": ["P. Gurram", "H. Kwon", "T. Han"], "venue": "vol. 9, no. 5, pp. 943\u2013 947, Sept. 2012.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2012}, {"title": "A support vector method for anomaly detection in hyperspectral imagery", "author": ["Amit Banerjee", "Philippe Burlina", "Chris Diehl"], "venue": "Geoscience and Remote Sensing, IEEE Transactions on, vol. 44, no. 8, pp. 2282\u20132291, 2006.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2006}], "referenceMentions": [{"referenceID": 0, "context": "The proposed OSKLAD optimally extends the feature selection technique used for the kernel-based learning approaches [1] into SVDD-based anomaly detection by fully optimizing the feature selection method for nonlinear kernels in a newly defined finite space called the EKFS [2].", "startOffset": 116, "endOffset": 119}, {"referenceID": 1, "context": "The proposed OSKLAD optimally extends the feature selection technique used for the kernel-based learning approaches [1] into SVDD-based anomaly detection by fully optimizing the feature selection method for nonlinear kernels in a newly defined finite space called the EKFS [2].", "startOffset": 273, "endOffset": 276}, {"referenceID": 2, "context": "However, the MIP problem is NP-hard, and so the MIP model is relaxed into a Quadratically Constrained Linear Programming (QCLP) problem [3] by converting the objective function of the MIP problem into lower bounded quadratic inequality constraints.", "startOffset": 136, "endOffset": 139}, {"referenceID": 3, "context": "To address this issue, a cutting plane method based on the restricted master problem coupled with Multiple Kernel Leaning (MKL) [4] is iteratively used.", "startOffset": 128, "endOffset": 131}, {"referenceID": 0, "context": "Inspired by the feature selection approach for the kernel-based classification [1], the OSKLAD addresses the problem of the optimal feature selection for the SVDDbased anomaly detection.", "startOffset": 79, "endOffset": 82}, {"referenceID": 4, "context": "This optimization problem is called the restricted master problem, which is closely related to the cutting plane algorithm described in [5].", "startOffset": 136, "endOffset": 139}, {"referenceID": 5, "context": "The restricted master problem consists of two steps [6]: 1) (t, \u03b1) are optimized based on a previously found restricted subset I of features, which maximally violates the constraints; and 2) a new vector d of the most violated features is obtained based on newly optimized (t, \u03b1) in step 1 and added to the restricted subset I = I \u22c3 d.", "startOffset": 52, "endOffset": 55}, {"referenceID": 6, "context": "These two steps are iterated until convergence [7].", "startOffset": 47, "endOffset": 50}, {"referenceID": 1, "context": ", k (xn, x)) (8) is called the EKM with respect to {xi}i=1 [2].", "startOffset": 59, "endOffset": 62}, {"referenceID": 1, "context": "After analyzing certain conditions using this equality as shown in [2], the dot product \u3008\u00b7, \u00b7\u3009n can be converted to a canonical dot product by merely whitening the EKFS and using the new basis functions as features.", "startOffset": 67, "endOffset": 70}, {"referenceID": 7, "context": "In our experiments, the performance of SVDD, SKAD [8]1 and OSKLAD with both linear and Gaussian RBF kernels are compared with one another.", "startOffset": 50, "endOffset": 53}, {"referenceID": 8, "context": "The kernel bandwidth parameter is determined by implementing the minimax technique on randomly selected 10 regions of the image to represent the background as done in [9].", "startOffset": 167, "endOffset": 170}], "year": 2015, "abstractText": "In this paper, a novel framework of sparse kernel learning for Support Vector Data Description (SVDD) based anomaly detection is presented. In this work, optimal sparse feature selection for anomaly detection is first modeled as a Mixed Integer Programming (MIP) problem. Due to the prohibitively high computational complexity of the MIP, it is relaxed into a Quadratically Constrained Linear Programming (QCLP) problem. The QCLP problem can then be practically solved by using an iterative optimization method, in which multiple subsets of features are iteratively found as opposed to a single subset. The QCLP-based iterative optimization problem is solved in a finite space called the Empirical Kernel Feature Space (EKFS) instead of in the input space or Reproducing Kernel Hilbert Space (RKHS). This is possible because of the fact that the geometrical properties of the EKFS and the corresponding RKHS remain the same. Now, an explicit nonlinear exploitation of the data in a finite EKFS is achievable, which results in optimal feature ranking. Experimental results based on a hyperspectral image show that the proposed method can provide improved performance over the current state-of-the-art techniques.", "creator": "LaTeX with hyperref package"}}}