{"id": "1506.02649", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "8-Jun-2015", "title": "Faster SGD Using Sketched Conditioning", "abstract": "We propose a novel method for speeding up stochastic optimization algorithms via sketching methods, which recently became a powerful tool for accelerating algorithms for numerical linear algebra. We revisit the method of conditioning for accelerating first-order methods and suggest the use of sketching methods for constructing a cheap conditioner that attains a significant speedup with respect to the Stochastic Gradient Descent (SGD) algorithm. While our theoretical guarantees assume convexity, we discuss the applicability of our method to deep neural networks, and experimentally demonstrate its merits.", "histories": [["v1", "Mon, 8 Jun 2015 15:08:37 GMT  (32kb)", "http://arxiv.org/abs/1506.02649v1", null]], "reviews": [], "SUBJECTS": "cs.NA cs.LG", "authors": ["alon gonen", "shai shalev-shwartz"], "accepted": false, "id": "1506.02649"}, "pdf": {"name": "1506.02649.pdf", "metadata": {"source": "CRF", "title": "Faster SGD Using Sketched Conditioning", "authors": ["Alon Gonen", "Shai Shalev-Shwartz"], "emails": [], "sections": [{"heading": null, "text": "ar Xiv: 150 6.02 649v 1 [cs.N A] 8J un"}, {"heading": "1 Introduction", "text": "In fact it is in such a way that the WWs are able to limit themselves to the real world, namely in the manner in which they move in the real world, in which they move in the real world and in the real world, in which they live in the real world and in the real world, in which they live in the real world and in the real world, in which they live in the real world and in the real world, in which they live in the real world and in the real world, in which they live in the real world and in the real world, in which they live in the real world and in the real world, in which they live in the real world and in the real world, in which they live in the real world and in which they live in the real world, and in which they live in the world, and in which they live in the real world, and in which they live in the real world, and in which they live in the real world, and in which they live in the real world, and in which they live in the real world, and in which they live in the real world, and in which they live in which they live in the real world, and in which they live in which they live in the real world, and in which they live in which they live in the real world, and in which they live in which they live in the real world, and in which they live in which they live in the real world, and in which they live in which they live in the real world, and in which they live in which they live in the real world, and in which they live in which they live in the real world, and in which they live in which they live in the real world, and in which they live in which they live in the real world, and in which they live in the real world, and in which they live in which they live in the real world, and in which they live in the real world, and in which they live in which they live in the real world, and in which they live in the real world, and in which they live in the real world, and in which they live in the real world, and in which they live in which they live in the real world, and in which they live in the real world, and in which they live in the real world, and in which they live in the real world, and in which they live in which they live in the real"}, {"heading": "1.1 Related work", "text": "In fact, it is a matter of a way in which it is a question of whether and in what form and in what way people in the various countries of the world have behaved in a way as they do. (...) In fact, it is a matter of how they have behaved in a way as they do. (...) It is as if it is a matter of a way as they do it. (...) It is as if it is a matter of a way as it does it. \"(...)\" It is as if it is a matter of a way as it does it. \"(...) It is as if it is a matter of a way. (...) It is as if it is a matter of a way.\" (...) It is a matter of a way. (...) It is a matter of a way. (...) And it is a matter. (...)"}, {"heading": "2 Conditioning and Sketched Conditioning", "text": "As already mentioned, the algorithms we are considering start with an initial matrix W1 and update the matrix according to Equation (3) for each iteration. The following problem provides an upper limit for the expected sub-optimality of each algorithm of this form.Lemma 1. Attach a positive definitive matrix A-Rn \u00b7 n. Let W-Ra be the minimizer of the equation (1), let the updating rule specified in Equation (3) be such that it is to be considered using the conditioner A and designate C = 1 x x x m i = 1. Suppose that for each i, x yi is convex and \u03c1-Lipschitz. Then, we apply the updating rule specified in Equation (3) to Ra-Ra-R-Ra-Ra-Ra-Ra-Ra-Ra-R-R-Ra-Ra-R-Ra-Ra-Ra-Ra-Ra-Ra-R-R-Ra-Ra-Ra-Ra-Ra-Ra-Ra-Ra-Ra-Ra-Ra-Ra-Ra-Ra-Ra-Ra-Ra-Ra-Ra-Ra-Ra-Ra-Ra-Ra-Ra-Ra-Ra-Ra-Ra-Ra-Ra-Ra-Ra-Ra-Ra-Ra-Ra-Ra-Ra-Ra-Ra-Ra-Ra-Ra-Ra-Ra-Ra-Ra-Ra-Ra-Ra-Ra-Ra-Ra-Ra-Ra-Ra-Ra-Ra-Ra-Ra-Ra-Ra-Ra-Ra-Ra-Ra-Ra-Ra-Ra-Ra-Ra-Ra-Ra-Ra-Ra-Ra-Ra-Ra-Ra-Ra-Ra-Ra-Ra-Ra-Ra-Ra-A-Ra-A-A-"}, {"heading": "2.1 Sketched Conditioning", "text": "Let us consider k < n and assume that rank (C) \u2265 k. Let us consider the following family of conditioners: A = {A = QBQ + a (I \u2212 QQ): Q Rn \u00b7 k, Q \u2212 I, B = QBQ + a (I \u2212 QQ), a > 0} (4) Before proceeding, let us show that the conditioners in A are actually unambiguously positive, and give a formula for their reverse. Lemma 2. Let A = QBQ + a (I \u2212 QQ). The most attractive feature of these conditioners is that we can calculate A \u2212 1 = QB \u2212 1Q + a \u2212 1 (I \u2212 QQ) and therefore the time complexity of calculating the update in Equation (3) and the identity conditioner."}, {"heading": "2.2 Low-rank conditioning via exact low-rank approximation", "text": "Perhaps the simplest approach to defining Q and B is by taking the leading eigenvectors of C. Remember that for each k \u2264 n the best rank-k approximation of C is given by Ck = UkDkU k, where Uk-Rn \u00b7 k from the first k columns of U and Dk is the first k \u00b7 k sub-matrix of D. Denote C = Q CQ, and consider the condition factor A determined from Equation (4) by specifying Q = Uk, B = C-1 / 2 and one as in Theorem 2. Let Q = Uk, B = C-1 / 2 and a as in Theorem 2 \u2212 \u2212 and consider the condition factor given in Equation (4)."}, {"heading": "2.3 Low-rank conditioning via sketching", "text": "In this section we describe a faster technique for calculating a random number of points in the plane, which we represent in a random combination of these points. As we can see, z corresponds to the strongest direction of the data, whose coordinates N (0, 1) i.d are random variables and whose vector z = X.d is a random combination of these points."}, {"heading": "3 Experiments with Deep Learning", "text": "While our theoretical guarantees were derived on the basis of convex algorithms, we can apply conditioning techniques to the profound learning problems, as we outline below. (D) We have the ability to analyze and analyze the individual layers. (D) The most popular layer functions with weights are the affine layer (a.k.a.). This layer performs the transformation y = Wx + b, where a network focuses on optimization w.r.t. The most popular layer functions with weights are the affine layer (a.k.ov \"fully connected\" layer). This layer performs the transformation y = Wx + b, W \".Rp, and b\" Rp. \"The network is usually based on variants of stochastic gradients, where the gradients of the objective w.r.t. W\" We have the calculation based on baking propagation, and has the shape where we apply them."}, {"heading": "Acknowledgments", "text": "This work is supported by the Intel Collaborative Research Institute for Computational Intelligence (ICRI-CI)."}, {"heading": "A Proofs Omitted from The Text", "text": "W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W"}], "references": [{"title": "Natural gradient works efficiently in learning", "author": ["Shun-Ichi Amari"], "venue": "Neural computation,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 1998}, {"title": "Adaptive method of realizing natural gradient learning for multilayer perceptrons", "author": ["Shun-Ichi Amari", "Hyeyoung Park", "Kenji Fukumizu"], "venue": "Neural Computation,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2000}, {"title": "Improving the convergence of back-propagation learning with second order methods", "author": ["Sue Becker", "Yann Le Cun"], "venue": "In Proceedings of the 1988 connectionist models summer school,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 1988}, {"title": "Practical recommendations for gradient-based training of deep architectures", "author": ["Yoshua Bengio"], "venue": "In Neural Networks: Tricks of the Trade,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2012}, {"title": "Sgd-qn: Careful quasinewton stochastic gradient descent", "author": ["Antoine Bordes", "L\u00e9on Bottou", "Patrick Gallinari"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2009}, {"title": "Adaptive subgradient methods for online learning and stochastic optimization", "author": ["John Duchi", "Elad Hazan", "Yoram Singer"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2011}, {"title": "Gradient-based learning applied to document recognition", "author": ["Yann LeCun", "L\u00e9on Bottou", "Yoshua Bengio", "Patrick Haffner"], "venue": "Proceedings of the IEEE,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 1998}, {"title": "The mnist database of handwritten digits", "author": ["Yann LeCun", "Corinna Cortes"], "venue": null, "citeRegEx": "8", "shortCiteRegEx": "8", "year": 1998}, {"title": "Deep learning via hessian-free optimization", "author": ["James Martens"], "venue": "In Proceedings of the 27th International Conference on Machine Learning", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2010}, {"title": "Exact calculation of the product of the hessian matrix of feedforward network error functions and a vector in 0 (n) time", "author": ["Martin F M\u00f8ller"], "venue": "DAIMI Report Series,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 1993}, {"title": "Introductory lectures on convex optimization, volume 87", "author": ["Yurii Nesterov"], "venue": "Springer Science & Business Media,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2004}, {"title": "Reading digits in natural images with unsupervised feature learning", "author": ["Yuval Netzer", "Tao Wang", "Adam Coates", "Alessandro Bissacco", "Bo Wu", "Andrew Y Ng"], "venue": "In NIPS workshop on deep learning and unsupervised feature learning,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2011}, {"title": "Revisiting natural gradient for deep networks", "author": ["Razvan Pascanu", "Yoshua Bengio"], "venue": "arXiv preprint arXiv:1301.3584,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2013}, {"title": "Fast exact multiplication by the hessian", "author": ["Barak A Pearlmutter"], "venue": "Neural computation,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 1994}, {"title": "Topmoumoute online natural gradient algorithm", "author": ["Nicolas L Roux", "Pierre-Antoine Manzagol", "Yoshua Bengio"], "venue": "In Advances in neural information processing systems,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2008}, {"title": "Partial bfgs update and efficient step-length calculation for three-layer neural networks", "author": ["Kazumi Saito", "Ryohei Nakano"], "venue": "Neural Computation,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 1997}, {"title": "Improved approximation algorithms for large matrices via random projections", "author": ["Tamas Sarlos"], "venue": "In Foundations of Computer Science,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2006}, {"title": "A stochastic quasi-newton method for online convex optimization", "author": ["Nicol Schraudolph", "Jin Yu", "Simon G\u00fcnter"], "venue": null, "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2007}, {"title": "Fast curvature matrix-vector products for second-order gradient descent", "author": ["Nicol N Schraudolph"], "venue": "Neural computation,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2002}, {"title": "On the importance of initialization and momentum in deep learning", "author": ["I. Sutskever", "J. Martens", "G. Dahl", "G. Hinton"], "venue": "In ICML,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2013}, {"title": "Krylov subspace descent for deep learning", "author": ["Oriol Vinyals", "Daniel Povey"], "venue": "arXiv preprint arXiv:1111.4259,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2011}, {"title": "Backpropagation: Past and future", "author": ["Paul J Werbos"], "venue": "In Neural Networks,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 1988}], "referenceMentions": [{"referenceID": 13, "context": "For example, the R{\u00b7} operator technique [14, 22, 10, 9].", "startOffset": 41, "endOffset": 56}, {"referenceID": 21, "context": "For example, the R{\u00b7} operator technique [14, 22, 10, 9].", "startOffset": 41, "endOffset": 56}, {"referenceID": 9, "context": "For example, the R{\u00b7} operator technique [14, 22, 10, 9].", "startOffset": 41, "endOffset": 56}, {"referenceID": 8, "context": "For example, the R{\u00b7} operator technique [14, 22, 10, 9].", "startOffset": 41, "endOffset": 56}, {"referenceID": 2, "context": "An obvious way to decrease the storage and computational cost is to only consider the diagonal elements of the Hessian (see [3]).", "startOffset": 124, "endOffset": 127}, {"referenceID": 17, "context": "Schraudolph [18] proposed an adaptation of the L-BFGS approach to the online setting, in which at each iteration, the estimation of the inverse of the Hessian is computed based on only the last few noisy gradients.", "startOffset": 12, "endOffset": 16}, {"referenceID": 4, "context": "In [5], the two aforementioned approaches are combined to yield the SGD-QN algorithm.", "startOffset": 3, "endOffset": 6}, {"referenceID": 15, "context": "There are various other approximations, see for example [16, 4, 21, 15].", "startOffset": 56, "endOffset": 71}, {"referenceID": 3, "context": "There are various other approximations, see for example [16, 4, 21, 15].", "startOffset": 56, "endOffset": 71}, {"referenceID": 20, "context": "There are various other approximations, see for example [16, 4, 21, 15].", "startOffset": 56, "endOffset": 71}, {"referenceID": 14, "context": "There are various other approximations, see for example [16, 4, 21, 15].", "startOffset": 56, "endOffset": 71}, {"referenceID": 18, "context": "To tackle the second problem, several methods [19, 9, 21, 13] rely on different variants of the Gauss-Newton approximation of the Hessian.", "startOffset": 46, "endOffset": 61}, {"referenceID": 8, "context": "To tackle the second problem, several methods [19, 9, 21, 13] rely on different variants of the Gauss-Newton approximation of the Hessian.", "startOffset": 46, "endOffset": 61}, {"referenceID": 20, "context": "To tackle the second problem, several methods [19, 9, 21, 13] rely on different variants of the Gauss-Newton approximation of the Hessian.", "startOffset": 46, "endOffset": 61}, {"referenceID": 12, "context": "To tackle the second problem, several methods [19, 9, 21, 13] rely on different variants of the Gauss-Newton approximation of the Hessian.", "startOffset": 46, "endOffset": 61}, {"referenceID": 0, "context": "A somewhat related approach is Amari\u2019s natural gradient descent [1, 2].", "startOffset": 64, "endOffset": 70}, {"referenceID": 1, "context": "A somewhat related approach is Amari\u2019s natural gradient descent [1, 2].", "startOffset": 64, "endOffset": 70}, {"referenceID": 12, "context": "See the discussion in [13].", "startOffset": 22, "endOffset": 26}, {"referenceID": 10, "context": "A general treatment of this approach is described in [11][Section 1.", "startOffset": 53, "endOffset": 57}, {"referenceID": 5, "context": "Maybe the most relevant approach is the Adagrad algorithm [6], which was originally proposed for the online learning setting but can be easily adapted to the stochastic optimization setting.", "startOffset": 58, "endOffset": 61}, {"referenceID": 16, "context": "The above intuition is formalized by the following result, which follows from [17] by setting \u01eb = 11.", "startOffset": 78, "endOffset": 82}, {"referenceID": 19, "context": "9, as described in [20].", "startOffset": 19, "endOffset": 23}, {"referenceID": 7, "context": "We conducted experiments with the MNIST dataset [8] and with the Street View House Numbers (SVHN) dataset [12].", "startOffset": 48, "endOffset": 51}, {"referenceID": 11, "context": "We conducted experiments with the MNIST dataset [8] and with the Street View House Numbers (SVHN) dataset [12].", "startOffset": 106, "endOffset": 110}, {"referenceID": 6, "context": "MNIST: We used a variant of the LeNet architecture [7].", "startOffset": 51, "endOffset": 54}], "year": 2015, "abstractText": "We propose a novel method for speeding up stochastic optimization algorithms via sketching methods, which recently became a powerful tool for accelerating algorithms for numerical linear algebra. We revisit the method of conditioning for accelerating first-order methods and suggest the use of sketching methods for constructing a cheap conditioner that attains a significant speedup with respect to the Stochastic Gradient Descent (SGD) algorithm. While our theoretical guarantees assume convexity, we discuss the applicability of our method to deep neural networks, and experimentally demonstrate its merits.", "creator": "LaTeX with hyperref package"}}}