{"id": "1506.02750", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "9-Jun-2015", "title": "Self Organizing Maps Whose Topologies Can Be Learned With Adaptive Binary Search Trees Using Conditional Rotations", "abstract": "Numerous variants of Self-Organizing Maps (SOMs) have been proposed in the literature, including those which also possess an underlying structure, and in some cases, this structure itself can be defined by the user Although the concepts of growing the SOM and updating it have been studied, the whole issue of using a self-organizing Adaptive Data Structure (ADS) to further enhance the properties of the underlying SOM, has been unexplored. In an earlier work, we impose an arbitrary, user-defined, tree-like topology onto the codebooks, which consequently enforced a neighborhood phenomenon and the so-called tree-based Bubble of Activity. In this paper, we consider how the underlying tree itself can be rendered dynamic and adaptively transformed. To do this, we present methods by which a SOM with an underlying Binary Search Tree (BST) structure can be adaptively re-structured using Conditional Rotations (CONROT). These rotations on the nodes of the tree are local, can be done in constant time, and performed so as to decrease the Weighted Path Length (WPL) of the entire tree. In doing this, we introduce the pioneering concept referred to as Neural Promotion, where neurons gain prominence in the Neural Network (NN) as their significance increases. We are not aware of any research which deals with the issue of Neural Promotion. The advantages of such a scheme is that the user need not be aware of any of the topological peculiarities of the stochastic data distribution. Rather, the algorithm, referred to as the TTOSOM with Conditional Rotations (TTOCONROT), converges in such a manner that the neurons are ultimately placed in the input space so as to represent its stochastic distribution, and additionally, the neighborhood properties of the neurons suit the best BST that represents the data. These properties have been confirmed by our experimental results on a variety of data sets.", "histories": [["v1", "Tue, 9 Jun 2015 02:29:57 GMT  (913kb)", "http://arxiv.org/abs/1506.02750v1", null]], "reviews": [], "SUBJECTS": "cs.NE cs.AI", "authors": ["c\\'esar a astudillo", "b john oommen"], "accepted": false, "id": "1506.02750"}, "pdf": {"name": "1506.02750.pdf", "metadata": {"source": "CRF", "title": "Self Organizing Maps Whose Topologies Can Be Learned With Adaptive Binary Search Trees Using Conditional Rotations", "authors": ["C\u00e9sar A. Astudillo", "John Oommen"], "emails": ["castudillo@utalca.cl", "oommen@scs.carleton.ca"], "sections": [{"heading": null, "text": "In fact, this structure itself has not yet been explored by the user. In an earlier paper, we put an arbitrary, user-defined, tree-like topology on the codebooks, which consequently forced a neighborhood phenomenon and the so-called tree-based distribution bubble of activity (BoA). In this paper, we look at how the underlying tree itself can be dynamically and adaptively transformed. To do this, we present methods by which a SOM can be adapted with an underlying Binary Search Tree (BST) structure. We look at how the underlying tree itself can be dynamically and adapted. To do this, we present methods by which a SOM can be adapted with an underlying Binary Search Tree (BST) structure."}, {"heading": "1 Introduction", "text": "This year, it is only a matter of time before there is an agreement, until there is an agreement."}, {"heading": "1.1 Motivations", "text": "This year it is so far that it will only be a matter of time before it is so far, until it is so far."}, {"heading": "1.2 Contributions of the Paper", "text": "The contributions of the paper can be summarized as follows: 1. We present an integration of the areas SOM and ADS. We respectfully present this as pioneering work. 2. The neurons of SOM are linked to each other by an underlying tree-based DS and are subject to the laws of the TTOSOM paradigm and at the same time to the restructuring adaptation provided by CONROT.3. 3. The definition of the distance between the neurons is based on the tree structure and not in the property space. This also applies to the BoA, whereby the migrations differ from the state of the art. 4. The adaptive character of TTOCONROT is unique because the adaptation is perceived in two forms: The migration of the codebook vectors in the feature space is a consequence of the SOM updating rule and the rearrangement of the neurons within the tree as a result of the rotations."}, {"heading": "1.3 Organization of the Paper", "text": "The rest of the paper is structured as follows: In the next section, the relevant literature3 will be examined, covering both the field of SOMs, including their tree-based instances, and the respective field of conditional rotation BSTs. Then, in Section 2, we will provide an in-depth explanation of the TTOCONROT philosophy, which is our primary contribution. In the following section, the capabilities of the approach will be presented through a series of experiments, and finally, Section 5 closes the essay paragraph.3 For space reasons, the literature review has been substantially condensed. However, as there is no review of the field of tree-based SOMs reported in the literature, we are currently preparing an essay summarizing the field."}, {"heading": "2 Literature Review", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "2.1 The SOM", "text": "One of the most important families of ANNs used to address clustering problems is the well-known SOM [36]. Typically, SOM is trained using (in) supervised learning to create a neural representation in a space whose dimension is usually smaller than the one in which the training samples are located. Furthermore, the neurons attempt to maintain the topological properties of the entrance space. SOM focuses on all the information contained in a set of n input samples belonging to ddimensional space, say X = {x1, x2,. xn}, the use of a much smaller set of neurons, C = {c1, c2, c2,.., cm}, each of which is represented as a vector. Each of them contains a weight vector = [w1, w2, w2]."}, {"heading": "2.2 Tree-Based SOMs", "text": "This year it has come to the point that it has never come as far as this year."}, {"heading": "3 Merging ADS and TTOSOM", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "3.1 Adaptive Data Structures (ADSs) for BSTs", "text": "This year, it is closer than ever before in the history of the country."}, {"heading": "3.2 The TTOSOM with Conditional Rotations (TTOCONROT)", "text": "This year, it is time for the EU Member States to set out to find a solution that paves the way to the European Union, to pave the way to the European Union."}, {"heading": "3.2.1 Neural Distance", "text": "This year, more than ever before in the history of a country in which it is a country in which it is a country in which it is not a country in which it is not a country, but a country in which it is a country, a country, a country, a country, a country, a country, a city, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a city, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a"}, {"heading": "3.2.2 The Bubble of Activity", "text": "One concept that is closely linked to the neural node is what is called the \"bubble of activity\" (BoA), that is, the subset of nodes within an environment in which the individual nodes are located. (B) This concept applies to all SOM-like nominations, especially TTOSOM. We will now think about how this bubble can be modified in the context of rotations. (D) The concept of the bubble involves looking at a quantity, the so-called radius, which determines how large the BoA is, and which therefore has a direct impact on the number of nodes that can be formally defined. (D) The concept of the bubble involves considering a quantity, the so-called radius, which establishes how the BoA is established, how large the BoA is, and which has a direct impact on the number of nodes."}, {"heading": "3.2.3 Enforcing the BST Property", "text": "This year, it is at an all-time high in the history of the European Union."}, {"heading": "3.2.4 Initialization", "text": "In the case of the BST-based TTOSOM, initialization takes place in two main steps, defining the initial value of each neuron and the connections between them. Initialization of the code book vectors is carried out in the same way as in the basic TTOSOM. The neurons can take on a starting value at will, for example, by placing them on randomly selected input samples. On the other hand, a significant improvement with regard to the basic TTOSOM lies in the way the neurons are connected to each other. TTOSOM's basic definition uses connections that remain static over time. The beauty of such an arrangement is that it reflects the perspective of the user at the time of the description of the topology and can maintain this configuration until the algorithm achieves convergence."}, {"heading": "3.2.5 The Required Local Information", "text": "In our proposed approach, the SOM code books correspond to the nodes of a BST. Apart from the information on the code books themselves in the attribute space, each neuron requires the maintenance of additional fields to achieve the adaptation. Furthermore, each node inherits the properties of a BST node and therefore contains a pointer to the left and right children and (to facilitate implementation) a pointer to the parent node. Furthermore, each node contains a label that is able to uniquely identify the neuron when it is in the \"company\" of other neurons. This identification index represents the lexicographic key used to sort the nodes of the tree and remains static over time. Figure 6 shows all the fields contained in the aneuron of a BST-based SOM."}, {"heading": "3.2.6 The Neural State", "text": "The various states that a neuron can assume during its lifetime are illustrated in Figure 7. First, when creating the node, it is assigned a unique identifier, and the remaining data fields are populated with their initial values. Here, the codebook vector assumes an initial value in the attribute space, and the pointers are configured to adequately link the neuron to the other neurons in the tree in a BST configuration. Next, the neuron enters a main loop during the most significant part of the algorithm in which training is carried out. This training phase involves customizing the code books and can also trigger optional modules that affect the neuron. Once the BMU is identified, the neuron could assume the \"restructured\" state, meaning that a restructuring technique, such as the CONROT algorithm, is applied. Alternatively, the neuron might be ready to accept the capping part, i.e., in the mapping process at present."}, {"heading": "3.2.7 The Training Step of the TTOCONROT", "text": "The training module of the TTOCONROT is responsible for the determination of the BMU, the execution of restructuring, the calculation of the BoA and the migration of neurons within the BoA. Basically, the CONROT algorithm must be integrated into the sequence of steps responsible for the training phase of the TTOSOM algorithm 4. Line 1 performs the first task of the algorithm, which includes the determination of the BMU. Next, line 2 calls up the CONROT procedure. The reason for following this sequence of steps is that the parameters required to perform the conditional rotation, as indicated in [16], contain the \"key\" of the determined element, which corresponds in the present context to the identity of the BMU. At this stage of the algorithm, the BMU may be rotated or not depending on the optimization criterion getation (1)."}, {"heading": "3.2.8 Alternative Restructuring Techniques", "text": "Although we have explained the benefits of the CONROT algorithm, the architecture we propose allows for the inclusion of alternative restructuring modules outside the CONROT algorithm. Potential candidates that can be used for the customization are those mentioned in Section 3.1, which include, among others, the splay and the MT algorithms."}, {"heading": "4 Experimental Results", "text": "In order to illustrate the capabilities of our method, the experiments reported in this paper focus primarily on low-dimensional attribute spaces. This will help the reader in the geometric visualization of the results obtained by us. However, it is important to note that the algorithm is also able to solve problems in higher dimensions, although a graphical representation of the results will not be as graphic. We know that TTOSOM is able to infer the distribution and structure of the data according to the results obtained in [8]. In this context, however, we are interested in investigating the effects of the application of neural rotation as part of the training process. In order to make the results comparable, the experiments in this section use the same timetable for the learning rate and the radius, i.e., no particular refinement of the parameters for certain data sets has been made. Furthermore, the parameters follow a rather \"slow\" decrease in the so-called decay parameters, which allows us to understand the problems as we recommend an increase in practical application speed."}, {"heading": "4.1 TTOCONROT\u2019s Structure Learning Capabilities", "text": "We will describe the performance of TTOCONROT with datasets in 1, 2 and 3 dimensions as well as experiments in the multidimensional range and highlight the specific advantages of the algorithm for different scenarios."}, {"heading": "4.1.1 One Dimensional Objects", "text": "Since our entire learning paradigm assumes that the data have a tree-shaped model, our first attempt was to see how philosophy is relevant to a unidimensional object (i.e., a curve) that actually has a \"linear\" topology. Thus, we tested the strength of the TTOCONROT as a primer to derive the properties of data sets generated from linear functions in the plane. Figure 8 shows various snapshots of how the TTOCONROT learns the data generated from a curve. Random initialization was used by uniform drawing points from the unit square. Note that the original data sets are not in the curve. Our goal here was to show how our algorithm could learn the structure of the data using arbitrary (initial and \"unrealistic\") values for the code book vectors. Figures 8b and 8c represent the middle phase of the training process, the simplicity of the reasons that the edges are left out."}, {"heading": "4.1.2 Two Dimensional Data Points", "text": "This year is the highest in the history of the country."}, {"heading": "4.1.3 Three Dimensional Data Points", "text": "We explain the results when applying the algorithm with and without CONROT. We choose three-dimensional objects for this purpose. The experiments use the data generated by the contour of the unit sphere. They also include a one-dimensional chain of 31 neurons. In addition, we learn the unit sphere without performing a conditional rotation. The figure shows the state of the neurons before the first iteration. The figure represents the case when the basic TTO algorithm (without CONROT) is learned."}, {"heading": "4.1.4 Multidimensional Data Points", "text": "The well-known iris dataset was chosen to show the power of our system in a scenario where the dimensionality is increased. This dataset indicates the measurements (in centimeters) of the variables that are the length of the room, the width of the room and the width of the petal. The species are the Iris Setosa, Versicolor and Virginica. In this set of experiments, the iris dataset was learned under three different configurations, with a fixed timetable for the learning rate and the radius, but with a unique tree configuration. The results of the experiments are presented in Figure 14 and include a complete binary tree of depth 3, 4 and 5."}, {"heading": "4.2 Skeletonization", "text": "In general, the main objective of skeletonization is to create a simpler representation of the shape of an object. [44] The authors of [44] refer to skeletonization at the plane as the process by which a 2-dimensional form is transformed into a 1-dimensional one, similar to a \"stick figure.\" Skeletonization applications are diverse, including the fields of computer vision and pattern recognition. As explained in [8], traditional methods of skeletonization are based on the connectivity of data points, and if this is not the case, more complex methods are required, which include SOM variants, to achieve skeletonization. [8, 19, 53] We point out that TTOSOM [8] is the only one that uses a tree-based structure. TTOSOM assumes that the shape of the object is not a priori known."}, {"heading": "4.3 Theoretical Analysis", "text": "In fact, most people who are able to move are able to move, to move and to move, to move, to move, to move, to move, to move, to move, to move and to move, to move, to move, to move, to move and to move, to move, to move, to move, to move, to move, to move and to move, to move and to move, to move, to move and to move, to move, to move and to move, to move, to move and to move."}, {"heading": "5 Conclusions and Discussions", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "5.1 Concluding Remarks", "text": "In this paper, we have proposed a novel integration between the areas of Adaptive Data Structures (ADS) and Self-Organizing Maps (SOMs). In particular, we have shown how a tree-based SOM can be adaptively transformed by using an underlying binary search tree (BST) structure and subsequently restructured using conditional rotations. These rotations at the nodes of the tree are local, can be performed in constant time and thus reduce the weighted path length (WPL) of the entire tree. One of the main advantages of the algorithm is that the user does not need to have a priori knowledge of the topology of the input data set. Instead, our proposed method, namely the conditional rotation TTOSOM (TTOCONROT), can represent the topological properties of the stochastic distribution and at the same time the state of the tree."}, {"heading": "5.2 Discussions and Future Work", "text": "As explained in Section 4.3, the work related to the measurement of topology conservation of the SOM, including the proof of its convergence in the one-dimensional case, was carried out only for the traditional SOM. Questions about how a tree-like topology should be measured, and how to define order in topologies that are not net-based, are unanswered. Therefore, we believe that even the tools for formal analysis of the TTOCONROT are not yet available. Although our main goal was to obtain a more accurate representation of stochastic distribution, our results also indicate that the neural network (NN) is capable of receiving stimuli for which the concept of order on tree-like structures has not yet been resolved."}], "references": [{"title": "An algorithm for the organization of information", "author": ["M. Adelson-Velskii", "M.E. Landis"], "venue": "Sov. Math. DokL, 3:1259\u20131262,", "citeRegEx": "1", "shortCiteRegEx": null, "year": 1962}, {"title": "Identification and classification of microaneurysms for early detection of diabetic retinopathy", "author": ["M.U. Akram", "S. Khalid", "S.A. Khan"], "venue": "Pattern Recognition, 46(1):107\u2013116,", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2013}, {"title": "Dynamic self-organizing maps with controlled growth for knowledge discovery", "author": ["D. Alahakoon", "S.K. Halgamuge", "B. Srinivasan"], "venue": "IEEE Transactions on Neural Networks, 11(3):601\u2013614,", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2000}, {"title": "Self-organizing binary search trees", "author": ["B. Allen", "I. Munro"], "venue": "J. ACM, 25(4):526\u2013535,", "citeRegEx": "4", "shortCiteRegEx": null, "year": 1978}, {"title": "D\u0131\u0301az Mart\u0301\u0131n. Topology preservation in SOM", "author": ["F.E. Arsuaga Uriarte"], "venue": "International Journal of Applied Mathematics and Computer Sciences,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2005}, {"title": "Self Organizing Maps Constrained by Data Structures", "author": ["C.A. Astudillo"], "venue": "PhD thesis, Carleton University,", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2011}, {"title": "On using adaptive binary search trees to enhance self organizing maps", "author": ["C.A. Astudillo", "B.J. Oommen"], "venue": "A. Nicholson and X. Li, editors, 22nd Australasian Joint Conference on Artificial Intelligence (AI 2009), pages 199\u2013209,", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2009}, {"title": "Imposing tree-based topologies onto self organizing maps", "author": ["C.A. Astudillo", "B.J. Oommen"], "venue": "Information Sciences, 181(18):3798\u20133815,", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2011}, {"title": "On achieving semi-supervised pattern recognition by utilizing tree-based SOMs", "author": ["C.A. Astudillo", "B.J. Oommen"], "venue": "Pattern Recognition, 46(1):293 \u2013 304,", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2013}, {"title": "Neural maps and topographic vector quantization", "author": ["H.U. Bauer", "M. Herrmann", "T. Villmann"], "venue": "Neural Networks, 12(4-5):659 \u2013 676,", "citeRegEx": "10", "shortCiteRegEx": null, "year": 1999}, {"title": "Quantifying the neighborhood preservation of self-organizing feature maps", "author": ["H.U. Bauer", "K.R. Pawelzik"], "venue": "Neural Networks, 3(4):570\u2013579, July", "citeRegEx": "11", "shortCiteRegEx": null, "year": 1992}, {"title": "Heuristics that dynamically organize data structures", "author": ["J.R. Bitner"], "venue": "SIAM J. Comput., 8:82\u2013110,", "citeRegEx": "12", "shortCiteRegEx": null, "year": 1979}, {"title": "Visualizing high-dimensional structure with the incremental grid growing neural network", "author": ["J. Blackmore"], "venue": "Master\u2019s thesis, University of Texas at Austin,", "citeRegEx": "13", "shortCiteRegEx": null, "year": 1995}, {"title": "On the ordering conditions for self-organizing maps", "author": ["M. Budinich"], "venue": "Neural Computation, 7(2):284\u2013289,", "citeRegEx": "14", "shortCiteRegEx": null, "year": 1995}, {"title": "The art of adaptive pattern recognition by a self-organizing neural network", "author": ["G.A. Carpenter", "S. Grossberg"], "venue": "Computer, 21(3):77\u201388,", "citeRegEx": "15", "shortCiteRegEx": null, "year": 1988}, {"title": "Adaptive structuring of binary search trees using conditional rotations", "author": ["R.P. Cheetham", "B.J. Oommen", "D.T.H. Ng"], "venue": "IEEE Trans. on Knowl. and Data Eng., 5(4):695\u2013704,", "citeRegEx": "16", "shortCiteRegEx": null, "year": 1993}, {"title": "On the mathematical treatment of self organization: extension of some classical results", "author": ["P.L. Conti", "L. De Giovanni"], "venue": "Artificial Neural Networks - ICANN 1991, International Conference, volume 2, pages 1089\u20131812,", "citeRegEx": "17", "shortCiteRegEx": null, "year": 1991}, {"title": "Introduction to Algorithms, Second Edition", "author": ["T.H. Cormen", "C.E. Leiserson", "R.L. Rivest", "C. Stein"], "venue": "McGraw-Hill Science/Engineering/Math, July", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2001}, {"title": "Skeletal shape extraction from dot patterns by selforganization", "author": ["A. Datta", "S.M. Parui", "B.B. Chaudhuri"], "venue": "Pattern Recognition, 1996., Proceedings of the 13th International Conference on, 4:80\u201384 vol.4, Aug", "citeRegEx": "19", "shortCiteRegEx": null, "year": 1996}, {"title": "Content-based image collection summarization and comparison using self-organizing maps", "author": ["D. Deng"], "venue": "Pattern Recognition, 40(2):718 \u2013 727,", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2007}, {"title": "The growing hierarchical self-organizing map", "author": ["M. Dittenbach", "D. Merkl", "A. Rauber"], "venue": "Neural Networks, 2000. IJCNN 2000, Proceedings of the IEEE-INNS-ENNS International Joint Conference on, volume 6, pages 15\u201319 vol.6,", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2000}, {"title": "Phylogenetic reconstruction using an unsupervised growing neural network that adopts the topology of a phylogenetic tree", "author": ["J. Dopazo", "J.M. Carazo"], "venue": "Journal of Molecular Evolution, 44(2):226\u2013233, February", "citeRegEx": "22", "shortCiteRegEx": null, "year": 1997}, {"title": "Pattern Classification (2nd Edition)", "author": ["R. Duda", "P.E. Hart", "D.G. Stork"], "venue": "Wiley-Interscience,", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2000}, {"title": "Growing Cell Structures \u2013 a self-organizing network for unsupervised and supervised learning", "author": ["B. Fritzke"], "venue": "Neural Networks, 7(9):1441\u20131460,", "citeRegEx": "24", "shortCiteRegEx": null, "year": 1994}, {"title": "Growing Grid - a self-organizing network with constant neighborhood range and adaptation strength", "author": ["B. Fritzke"], "venue": "Neural Processing Letters, 2(5):9\u201313,", "citeRegEx": "25", "shortCiteRegEx": null, "year": 1995}, {"title": "A growing neural gas network learns topologies", "author": ["B. Fritzke"], "venue": "G. Tesauro, D. S. Touretzky, and T. K. Leen, editors, Advances in Neural Information Processing Systems 7, pages 625\u2013632, Cambridge MA,", "citeRegEx": "26", "shortCiteRegEx": null, "year": 1995}, {"title": "Self-organizing trees and forests: A powerful tool in pattern clustering and recognition", "author": ["L. Guan"], "venue": "Image Analysis and Recognition, Third International Conference, ICIAR 2006, P\u00f3voa de Varzim, Portugal, September 18-20, 2006, Proceedings, Part I, pages I: 1\u201314,", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2006}, {"title": "Neural Networks and Learning Machines", "author": ["S. Haykin"], "venue": "Prentice Hall, 3rd edition edition,", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2008}, {"title": "Ordering of self-organizing maps in multi-dimensional cases", "author": ["G. Huang", "H.A. Babri", "H. Li"], "venue": "Neural Computation, 10:19\u201324,", "citeRegEx": "29", "shortCiteRegEx": null, "year": 1998}, {"title": "Real-time multiple people tracking using competitive condensation", "author": ["H.-G. Kang", "D. Kim"], "venue": "Pattern Recognition, 38(7):1045 \u2013 1058,", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2005}, {"title": "Handbook of Data Structures and Applications, chapter 31: Persistent Data Structures, pages 31.1 \u2013 31.26", "author": ["H. Kaplan"], "venue": "Chapman and Hall/CRC,", "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2004}, {"title": "Bibliography of self-organizing map (SOM) papers: 1981\u20131997", "author": ["S. Kaski", "J. Kangas", "T. Kohonen"], "venue": "Neural Computing Surveys, 1:102\u2013350,", "citeRegEx": "32", "shortCiteRegEx": null, "year": 1998}, {"title": "Human eye sclera detection and tracking using a modified timeadaptive self-organizing map", "author": ["M.H. Khosravi", "R. Safabakhsh"], "venue": "Pattern Recognition, 41(8):2571\u20132593,", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2008}, {"title": "Topology preservation in self-organizing maps", "author": ["K. Kiviluoto"], "venue": "P. IEEE Neural Networks Council, editor, Proceedings of International Conference on Neural Networks, ICNN\u201996, volume 1, pages 294\u2013299, New Jersey, USA,", "citeRegEx": "34", "shortCiteRegEx": null, "year": 1996}, {"title": "The art of computer programming, volume 3: (2nd ed.) sorting and searching", "author": ["D.E. Knuth"], "venue": null, "citeRegEx": "35", "shortCiteRegEx": "35", "year": 1998}, {"title": "Self-Organizing Maps", "author": ["T. Kohonen"], "venue": "Springer-Verlag New York, Inc., Secaucus, NJ, USA,", "citeRegEx": "36", "shortCiteRegEx": null, "year": 1995}, {"title": "Self-organizing hierarchical feature maps", "author": ["P. Koikkalainen", "E. Oja"], "venue": "IJCNN International Joint Conference on Neural Networks, 2:279\u2013284, June", "citeRegEx": "37", "shortCiteRegEx": null, "year": 1990}, {"title": "Efficient maintenance of binary search trees", "author": ["T.W.H. Lai"], "venue": "PhD thesis, University of Waterloo, Waterloo, Ont., Canada,", "citeRegEx": "38", "shortCiteRegEx": null, "year": 1990}, {"title": "No titlea synthesised word approach to word retrieval in handwritten documents", "author": ["Y. Liang", "M.C. Fairhurst", "R.M. Guest"], "venue": "Pattern Recognition, 45(12):4225\u20134236,", "citeRegEx": "39", "shortCiteRegEx": null, "year": 2012}, {"title": "A \u201cneural-gas\u201d network learns topologies", "author": ["M. Martinetz", "K.J. Schulten"], "venue": "in Proceedings of International Conference on Articial Neural Networks, volume I, pages 397\u2013402, North-Holland, Amsterdam,", "citeRegEx": "40", "shortCiteRegEx": null, "year": 1991}, {"title": "Dynamic binary search", "author": ["K. Mehlhorn"], "venue": "SIAM Journal on Computing, 8(2):175\u2013198,", "citeRegEx": "41", "shortCiteRegEx": null, "year": 1979}, {"title": "Adaptive hierarchical incremental grid growing: An architecture for high-dimensional data visualization", "author": ["D. Merkl", "S. Hui-He", "M. Dittenbach", "A. Rauber"], "venue": "In Proceedings of the 4th Workshop on Self-Organizing Maps, Advances in Self-Organizing Maps, pages 293\u2013298,", "citeRegEx": "42", "shortCiteRegEx": null, "year": 2003}, {"title": "Script recognition with hierarchical feature maps", "author": ["R. Miikkulainen"], "venue": "Connection Science, 2(1&2):83\u2013101,", "citeRegEx": "43", "shortCiteRegEx": null, "year": 1990}, {"title": "Hierarchic voronoi skeletons", "author": ["R.L. Ogniewicz", "O. K\u00fcbler"], "venue": "Pattern Recognition, 28(3):343\u2013359,", "citeRegEx": "44", "shortCiteRegEx": null, "year": 1995}, {"title": "Bibliography of self-organizing map (SOM) papers: 1998-2001 addendum", "author": ["M. Oja", "S. Kaski", "T. Kohonen"], "venue": "Neural Computing Surveys, 3:1\u2013156,", "citeRegEx": "45", "shortCiteRegEx": null, "year": 2003}, {"title": "The Evolving Tree \u2014 a novel self-organizing network for data analysis", "author": ["J. Pakkanen", "J. Iivarinen", "E. Oja"], "venue": "Neural Processing Letters, 20(3):199\u2013211, December", "citeRegEx": "46", "shortCiteRegEx": null, "year": 2004}, {"title": "Sur une courbe, qui remplit toute une aire plane", "author": ["G. Peano"], "venue": "Mathematische Annalen, 36(1):157\u2013160,", "citeRegEx": "47", "shortCiteRegEx": null, "year": 1890}, {"title": "Bibliography of self-organizing map (SOM) papers: 2002- 2005 addendum", "author": ["M. P\u00f6ll\u00e4", "T. Honkela", "T. Kohonen"], "venue": "Technical Report TKK-ICS-R23, Helsinki University of Technology, Department of Information and Computer Science, Espoo, Finland, December", "citeRegEx": "48", "shortCiteRegEx": null, "year": 2009}, {"title": "Survey and comparison of quality measures for self-organizing maps", "author": ["G. P\u00f6lzlbauer"], "venue": "J\u00e1n Parali\u010d, Georg P\u00f6lzlbauer, and Andreas Rauber, editors, Proceedings of the Fifth Workshop on Data Analysis (WDA\u201904), pages 67\u201382, Sliezsky dom, Vysok\u00e9 Tatry, Slovakia, June 24\u201327", "citeRegEx": "49", "shortCiteRegEx": null, "year": 2004}, {"title": "The Growing Hierarchical Self-Organizing Map: exploratory analysis of high-dimensional data", "author": ["A. Rauber", "D. Merkl", "M. Dittenbach"], "venue": "IEEE Transactions on Neural Networks, 13(6):1331\u20131341,", "citeRegEx": "50", "shortCiteRegEx": null, "year": 2002}, {"title": "Neural networks: a systematic introduction", "author": ["R. Rojas"], "venue": "Springer-Verlag New York, Inc., New York, NY, USA,", "citeRegEx": "51", "shortCiteRegEx": null, "year": 1996}, {"title": "Treesom: Cluster analysis in the self-organizing map", "author": ["E.V. Samsonova", "J.N. Kok", "A.P. IJzerman"], "venue": "Neural Networks, 19(6\u20137):935 \u2013 949,", "citeRegEx": "52", "shortCiteRegEx": null, "year": 2006}, {"title": "Self-Organizing Maps for the skeletonization of sparse shapes", "author": ["R. Singh", "V. Cherkassky", "N. Papanikolopoulos"], "venue": "Neural Networks, IEEE Transactions on, 11(1):241\u2013248, Jan", "citeRegEx": "53", "shortCiteRegEx": null, "year": 2000}, {"title": "Self-adjusting binary search trees", "author": ["D.D. Sleator", "R.E. Tarjan"], "venue": "J. ACM, 32(3):652\u2013686,", "citeRegEx": "54", "shortCiteRegEx": null, "year": 1985}, {"title": "Neighborhood preservation in nonlinear projection methods: An experimental study", "author": ["J. Venna", "S. Kaski"], "venue": "Georg Dorffner, Horst Bischof, and Kurt Hornik, editors, ICANN, volume 2130 of Lecture Notes in Computer Science, pages 485\u2013491. Springer,", "citeRegEx": "55", "shortCiteRegEx": null, "year": 2001}, {"title": "Unsupervised segmentation using a selforganizing map and a noise model estimation in sonar imagery", "author": ["K.C. Yao", "M. Mignotte", "C. Collet", "P. Galerne", "G. Burel"], "venue": "Pattern Recognition, 33(9):1575 \u2013 1584,", "citeRegEx": "56", "shortCiteRegEx": null, "year": 2000}], "referenceMentions": [{"referenceID": 35, "context": "Kohonen, in his book [36], mentions that it is possible to distinguish between two basic types of neighborhood functions.", "startOffset": 21, "endOffset": 25}, {"referenceID": 23, "context": "Examples of strategies that use the neural distance to determine the BoA are the Growing Cell Structures (GCS) [24], the Growing Grid (GG) [25], the Incremental Grid Growing (IGG) [13], the Growing SOM (GSOM) [3], the Tree-Structured SOM (TSSOM) [37], the Hierarchical Feature Map (HFM) [43], the Growing Hierarchical SOM (GHSOM) [50], the SelfOrganizing Tree Algorithm (SOTA) [22], the Evolving Tree (ET) [46], the Tree-based Topology Oriented SOM (TTOSOM) [8], among others.", "startOffset": 111, "endOffset": 115}, {"referenceID": 24, "context": "Examples of strategies that use the neural distance to determine the BoA are the Growing Cell Structures (GCS) [24], the Growing Grid (GG) [25], the Incremental Grid Growing (IGG) [13], the Growing SOM (GSOM) [3], the Tree-Structured SOM (TSSOM) [37], the Hierarchical Feature Map (HFM) [43], the Growing Hierarchical SOM (GHSOM) [50], the SelfOrganizing Tree Algorithm (SOTA) [22], the Evolving Tree (ET) [46], the Tree-based Topology Oriented SOM (TTOSOM) [8], among others.", "startOffset": 139, "endOffset": 143}, {"referenceID": 12, "context": "Examples of strategies that use the neural distance to determine the BoA are the Growing Cell Structures (GCS) [24], the Growing Grid (GG) [25], the Incremental Grid Growing (IGG) [13], the Growing SOM (GSOM) [3], the Tree-Structured SOM (TSSOM) [37], the Hierarchical Feature Map (HFM) [43], the Growing Hierarchical SOM (GHSOM) [50], the SelfOrganizing Tree Algorithm (SOTA) [22], the Evolving Tree (ET) [46], the Tree-based Topology Oriented SOM (TTOSOM) [8], among others.", "startOffset": 180, "endOffset": 184}, {"referenceID": 2, "context": "Examples of strategies that use the neural distance to determine the BoA are the Growing Cell Structures (GCS) [24], the Growing Grid (GG) [25], the Incremental Grid Growing (IGG) [13], the Growing SOM (GSOM) [3], the Tree-Structured SOM (TSSOM) [37], the Hierarchical Feature Map (HFM) [43], the Growing Hierarchical SOM (GHSOM) [50], the SelfOrganizing Tree Algorithm (SOTA) [22], the Evolving Tree (ET) [46], the Tree-based Topology Oriented SOM (TTOSOM) [8], among others.", "startOffset": 209, "endOffset": 212}, {"referenceID": 36, "context": "Examples of strategies that use the neural distance to determine the BoA are the Growing Cell Structures (GCS) [24], the Growing Grid (GG) [25], the Incremental Grid Growing (IGG) [13], the Growing SOM (GSOM) [3], the Tree-Structured SOM (TSSOM) [37], the Hierarchical Feature Map (HFM) [43], the Growing Hierarchical SOM (GHSOM) [50], the SelfOrganizing Tree Algorithm (SOTA) [22], the Evolving Tree (ET) [46], the Tree-based Topology Oriented SOM (TTOSOM) [8], among others.", "startOffset": 246, "endOffset": 250}, {"referenceID": 42, "context": "Examples of strategies that use the neural distance to determine the BoA are the Growing Cell Structures (GCS) [24], the Growing Grid (GG) [25], the Incremental Grid Growing (IGG) [13], the Growing SOM (GSOM) [3], the Tree-Structured SOM (TSSOM) [37], the Hierarchical Feature Map (HFM) [43], the Growing Hierarchical SOM (GHSOM) [50], the SelfOrganizing Tree Algorithm (SOTA) [22], the Evolving Tree (ET) [46], the Tree-based Topology Oriented SOM (TTOSOM) [8], among others.", "startOffset": 287, "endOffset": 291}, {"referenceID": 49, "context": "Examples of strategies that use the neural distance to determine the BoA are the Growing Cell Structures (GCS) [24], the Growing Grid (GG) [25], the Incremental Grid Growing (IGG) [13], the Growing SOM (GSOM) [3], the Tree-Structured SOM (TSSOM) [37], the Hierarchical Feature Map (HFM) [43], the Growing Hierarchical SOM (GHSOM) [50], the SelfOrganizing Tree Algorithm (SOTA) [22], the Evolving Tree (ET) [46], the Tree-based Topology Oriented SOM (TTOSOM) [8], among others.", "startOffset": 330, "endOffset": 334}, {"referenceID": 21, "context": "Examples of strategies that use the neural distance to determine the BoA are the Growing Cell Structures (GCS) [24], the Growing Grid (GG) [25], the Incremental Grid Growing (IGG) [13], the Growing SOM (GSOM) [3], the Tree-Structured SOM (TSSOM) [37], the Hierarchical Feature Map (HFM) [43], the Growing Hierarchical SOM (GHSOM) [50], the SelfOrganizing Tree Algorithm (SOTA) [22], the Evolving Tree (ET) [46], the Tree-based Topology Oriented SOM (TTOSOM) [8], among others.", "startOffset": 377, "endOffset": 381}, {"referenceID": 45, "context": "Examples of strategies that use the neural distance to determine the BoA are the Growing Cell Structures (GCS) [24], the Growing Grid (GG) [25], the Incremental Grid Growing (IGG) [13], the Growing SOM (GSOM) [3], the Tree-Structured SOM (TSSOM) [37], the Hierarchical Feature Map (HFM) [43], the Growing Hierarchical SOM (GHSOM) [50], the SelfOrganizing Tree Algorithm (SOTA) [22], the Evolving Tree (ET) [46], the Tree-based Topology Oriented SOM (TTOSOM) [8], among others.", "startOffset": 406, "endOffset": 410}, {"referenceID": 7, "context": "Examples of strategies that use the neural distance to determine the BoA are the Growing Cell Structures (GCS) [24], the Growing Grid (GG) [25], the Incremental Grid Growing (IGG) [13], the Growing SOM (GSOM) [3], the Tree-Structured SOM (TSSOM) [37], the Hierarchical Feature Map (HFM) [43], the Growing Hierarchical SOM (GHSOM) [50], the SelfOrganizing Tree Algorithm (SOTA) [22], the Evolving Tree (ET) [46], the Tree-based Topology Oriented SOM (TTOSOM) [8], among others.", "startOffset": 458, "endOffset": 461}, {"referenceID": 36, "context": ", it constitutes an instance of hard Competitive Learning (CL), as in the case of the Tree-Structured VQ (TSVQ) [37] and the Self-Organizing Tree Map (SOTM) [27].", "startOffset": 112, "endOffset": 116}, {"referenceID": 26, "context": ", it constitutes an instance of hard Competitive Learning (CL), as in the case of the Tree-Structured VQ (TSVQ) [37] and the Self-Organizing Tree Map (SOTM) [27].", "startOffset": 157, "endOffset": 161}, {"referenceID": 39, "context": "An example of a SOM variant that uses such a ranking is the Neural Gas (NG) [40].", "startOffset": 76, "endOffset": 80}, {"referenceID": 45, "context": "According to the authors of [46], the SOM-based variants included in the literature attempt to tackle two main goals: They either try to design a more flexible topology, which is usually useful to analyze large datasets, or to reduce the the most time-consuming task required by the SOM, namely, the search for the BMU when the input set has a complex nature.", "startOffset": 28, "endOffset": 32}, {"referenceID": 12, "context": "Regardless of the fact that numerous variants of the SOM has been devised, few of them possess the ability of modifying the underlying topology [13, 21, 22, 26, 27, 42, 46, 52].", "startOffset": 144, "endOffset": 176}, {"referenceID": 20, "context": "Regardless of the fact that numerous variants of the SOM has been devised, few of them possess the ability of modifying the underlying topology [13, 21, 22, 26, 27, 42, 46, 52].", "startOffset": 144, "endOffset": 176}, {"referenceID": 21, "context": "Regardless of the fact that numerous variants of the SOM has been devised, few of them possess the ability of modifying the underlying topology [13, 21, 22, 26, 27, 42, 46, 52].", "startOffset": 144, "endOffset": 176}, {"referenceID": 25, "context": "Regardless of the fact that numerous variants of the SOM has been devised, few of them possess the ability of modifying the underlying topology [13, 21, 22, 26, 27, 42, 46, 52].", "startOffset": 144, "endOffset": 176}, {"referenceID": 26, "context": "Regardless of the fact that numerous variants of the SOM has been devised, few of them possess the ability of modifying the underlying topology [13, 21, 22, 26, 27, 42, 46, 52].", "startOffset": 144, "endOffset": 176}, {"referenceID": 41, "context": "Regardless of the fact that numerous variants of the SOM has been devised, few of them possess the ability of modifying the underlying topology [13, 21, 22, 26, 27, 42, 46, 52].", "startOffset": 144, "endOffset": 176}, {"referenceID": 45, "context": "Regardless of the fact that numerous variants of the SOM has been devised, few of them possess the ability of modifying the underlying topology [13, 21, 22, 26, 27, 42, 46, 52].", "startOffset": 144, "endOffset": 176}, {"referenceID": 51, "context": "Regardless of the fact that numerous variants of the SOM has been devised, few of them possess the ability of modifying the underlying topology [13, 21, 22, 26, 27, 42, 46, 52].", "startOffset": 144, "endOffset": 176}, {"referenceID": 7, "context": "Moreover, only a small subset use a tree as their underlying DS [8, 21, 22, 27, 46, 52].", "startOffset": 64, "endOffset": 87}, {"referenceID": 20, "context": "Moreover, only a small subset use a tree as their underlying DS [8, 21, 22, 27, 46, 52].", "startOffset": 64, "endOffset": 87}, {"referenceID": 21, "context": "Moreover, only a small subset use a tree as their underlying DS [8, 21, 22, 27, 46, 52].", "startOffset": 64, "endOffset": 87}, {"referenceID": 26, "context": "Moreover, only a small subset use a tree as their underlying DS [8, 21, 22, 27, 46, 52].", "startOffset": 64, "endOffset": 87}, {"referenceID": 45, "context": "Moreover, only a small subset use a tree as their underlying DS [8, 21, 22, 27, 46, 52].", "startOffset": 64, "endOffset": 87}, {"referenceID": 51, "context": "Moreover, only a small subset use a tree as their underlying DS [8, 21, 22, 27, 46, 52].", "startOffset": 64, "endOffset": 87}, {"referenceID": 35, "context": "The SOM tries to achieve this by defining an underlying grid-based topology and to fit the grid within the overall shape, as shown in Figure 1a (duplicated from [36]).", "startOffset": 161, "endOffset": 165}, {"referenceID": 7, "context": "As opposed to this, Figure 1b, shows the result of applying one of the techniques developed by us, namely the TTOSOM [8].", "startOffset": 117, "endOffset": 120}, {"referenceID": 22, "context": "Indeed, this is in line with the well-accepted principle [23], that very little can be automatically learned about a data distribution if no assumptions are made! As the next step of motivating this research endeavor, we venture into a world where the neural topology and structure are themselves learned during the training process.", "startOffset": 57, "endOffset": 61}, {"referenceID": 6, "context": "Even though the schemes that we are currently proposing As mentioned earlier, a paper which reported the preliminary results of this study, won the Best Paper Award in a well-known international AI conference [7].", "startOffset": 209, "endOffset": 212}, {"referenceID": 12, "context": ", [13, 24, 37, 46].", "startOffset": 2, "endOffset": 18}, {"referenceID": 23, "context": ", [13, 24, 37, 46].", "startOffset": 2, "endOffset": 18}, {"referenceID": 36, "context": ", [13, 24, 37, 46].", "startOffset": 2, "endOffset": 18}, {"referenceID": 45, "context": ", [13, 24, 37, 46].", "startOffset": 2, "endOffset": 18}, {"referenceID": 36, "context": "The strategy that we have chosen for adapting the tree, namely using Conditional Rotations (CONROT), already utilizes this BMU counter, and, distinct to the previous strategies that attempt to search for a node to be expanded (which in the case of tree-based SOMs is usually at the level of the leaves [37, 46]), we foresee and advocate a different approach.", "startOffset": 302, "endOffset": 310}, {"referenceID": 45, "context": "The strategy that we have chosen for adapting the tree, namely using Conditional Rotations (CONROT), already utilizes this BMU counter, and, distinct to the previous strategies that attempt to search for a node to be expanded (which in the case of tree-based SOMs is usually at the level of the leaves [37, 46]), we foresee and advocate a different approach.", "startOffset": 302, "endOffset": 310}, {"referenceID": 35, "context": "1 The SOM One of the most important families of ANNs used to tackle clustering problems is the well known SOM [36].", "startOffset": 110, "endOffset": 114}, {"referenceID": 31, "context": "There are a vast number of domain fields where the SOM has demonstrated to be useful; a compendium with all the articles that take advantage of the properties of the SOM is surveyed in [32, 45].", "startOffset": 185, "endOffset": 193}, {"referenceID": 44, "context": "There are a vast number of domain fields where the SOM has demonstrated to be useful; a compendium with all the articles that take advantage of the properties of the SOM is surveyed in [32, 45].", "startOffset": 185, "endOffset": 193}, {"referenceID": 31, "context": "The report [32] includes the bibliography published between the year 1981 and 1998, while the report [45] includes the analogous papers published between 1998 and 2001.", "startOffset": 11, "endOffset": 15}, {"referenceID": 44, "context": "The report [32] includes the bibliography published between the year 1981 and 1998, while the report [45] includes the analogous papers published between 1998 and 2001.", "startOffset": 101, "endOffset": 105}, {"referenceID": 47, "context": "Further, additional recent references including the related work up to the year 2005 have been collected in a technical report [48].", "startOffset": 127, "endOffset": 131}, {"referenceID": 1, "context": "The more recent literature reports a host of application domains, including Medical Image Processing [2], Human Eye Detection [33], Handwriting Recognition [39], Image Segmentation [56], Information Retrieval [20], Object Tracking [30], etc.", "startOffset": 101, "endOffset": 104}, {"referenceID": 32, "context": "The more recent literature reports a host of application domains, including Medical Image Processing [2], Human Eye Detection [33], Handwriting Recognition [39], Image Segmentation [56], Information Retrieval [20], Object Tracking [30], etc.", "startOffset": 126, "endOffset": 130}, {"referenceID": 38, "context": "The more recent literature reports a host of application domains, including Medical Image Processing [2], Human Eye Detection [33], Handwriting Recognition [39], Image Segmentation [56], Information Retrieval [20], Object Tracking [30], etc.", "startOffset": 156, "endOffset": 160}, {"referenceID": 55, "context": "The more recent literature reports a host of application domains, including Medical Image Processing [2], Human Eye Detection [33], Handwriting Recognition [39], Image Segmentation [56], Information Retrieval [20], Object Tracking [30], etc.", "startOffset": 181, "endOffset": 185}, {"referenceID": 19, "context": "The more recent literature reports a host of application domains, including Medical Image Processing [2], Human Eye Detection [33], Handwriting Recognition [39], Image Segmentation [56], Information Retrieval [20], Object Tracking [30], etc.", "startOffset": 209, "endOffset": 213}, {"referenceID": 29, "context": "The more recent literature reports a host of application domains, including Medical Image Processing [2], Human Eye Detection [33], Handwriting Recognition [39], Image Segmentation [56], Information Retrieval [20], Object Tracking [30], etc.", "startOffset": 231, "endOffset": 235}, {"referenceID": 36, "context": "The Tree-Structured VQ (TSVQ) algorithm [37] is a tree-based SOM variant, whose topology is defined a priori and which is static.", "startOffset": 40, "endOffset": 44}, {"referenceID": 36, "context": "Koikkalainen and Oja, in the same paper [37] refine the idea of the TSVQ by defining the TSSOM, which inherits all the properties of the TSVQ, but redefines the search procedure and BoA.", "startOffset": 40, "endOffset": 44}, {"referenceID": 21, "context": "The Self-Organizing Tree Algorithm (SOTA) [22] is a dynamically growing tree-based SOM which, according to their authors, take some analogies from the Growing Cell Structures (GCS) [24].", "startOffset": 42, "endOffset": 46}, {"referenceID": 23, "context": "The Self-Organizing Tree Algorithm (SOTA) [22] is a dynamically growing tree-based SOM which, according to their authors, take some analogies from the Growing Cell Structures (GCS) [24].", "startOffset": 181, "endOffset": 185}, {"referenceID": 36, "context": ", the TSSOM [37] and the Evolving Tree (ET) [46] explained below), it considers the migration of the neurons only if they", "startOffset": 12, "endOffset": 16}, {"referenceID": 45, "context": ", the TSSOM [37] and the Evolving Tree (ET) [46] explained below), it considers the migration of the neurons only if they", "startOffset": 44, "endOffset": 48}, {"referenceID": 20, "context": "In [21] the authors presented a tree-based SOM called the Growing Hierarchical SOM (GHSOM), in which each node corresponds to an independent SOM.", "startOffset": 3, "endOffset": 7}, {"referenceID": 26, "context": "The SOTM [27] is a tree-based SOM which is also inspired by the Adaptive Resonance Theory (ART) [15].", "startOffset": 9, "endOffset": 13}, {"referenceID": 14, "context": "The SOTM [27] is a tree-based SOM which is also inspired by the Adaptive Resonance Theory (ART) [15].", "startOffset": 96, "endOffset": 100}, {"referenceID": 45, "context": "In [46], the authors have proposed a tree-structured NN called the Evolving Tree (ET), which takes advantage of a sub-optimal procedure (adapted from the one utilized by the TSVQ) to identify the BMU in O(log |V |) time, where V is the set of neurons.", "startOffset": 3, "endOffset": 7}, {"referenceID": 7, "context": "The Tree-based Topology Oriented SOM (TTOSOM) [8], which is central to this paper, is a tree-based SOM in which each node can possess an arbitrary number of children.", "startOffset": 46, "endOffset": 49}, {"referenceID": 7, "context": "In this case, the TTOSOM is able to adapt this 1-dimensional grid to a 2-dimensional (or multi-dimensional) object in the same way as the SOM algorithm does [8].", "startOffset": 157, "endOffset": 160}, {"referenceID": 7, "context": "Additionally, if the original topology of the tree followed the overall shape of the data distribution, the results reported in [8] (and also depicted in the motivational section) showed that is also possible to obtain a symmetric topology for the codebook vectors.", "startOffset": 128, "endOffset": 131}, {"referenceID": 8, "context": "In a more recent work [9], the authors have enhanced the TTOSOM to perform classification in a semi-supervised fashion.", "startOffset": 22, "endOffset": 25}, {"referenceID": 8, "context": "The method presented in [9] first learns the data distribution in an unsupervised manner.", "startOffset": 24, "endOffset": 27}, {"referenceID": 8, "context": "According to the results presented in [9], the number of neurons required to accurately predict the category The SOM possesses the ability to learn the data distribution by utilizing a unidimensional topology [36], i.", "startOffset": 38, "endOffset": 41}, {"referenceID": 35, "context": "According to the results presented in [9], the number of neurons required to accurately predict the category The SOM possesses the ability to learn the data distribution by utilizing a unidimensional topology [36], i.", "startOffset": 209, "endOffset": 213}, {"referenceID": 46, "context": "Further, when this is the case, one can encounter that the unidimensional topology forms a so-called Peano curve [47].", "startOffset": 113, "endOffset": 117}, {"referenceID": 7, "context": "The details of how this is achieved is presented in detail in [8], including the explanation of how other tree-based techniques fail to achieve this task.", "startOffset": 62, "endOffset": 65}, {"referenceID": 34, "context": "The solution to obtain the optimal BST is well known when the access probabilities of the nodes are known a priori [35].", "startOffset": 115, "endOffset": 119}, {"referenceID": 15, "context": "and uses the concept of CONROT [16], which reorganizes the BST so as to asymptotically produce the optimal form.", "startOffset": 31, "endOffset": 35}, {"referenceID": 34, "context": "The optimal algorithm due to Knuth [35], uses dynamic programming and produces the optimal BST using O(N) time and space.", "startOffset": 35, "endOffset": 39}, {"referenceID": 0, "context": "The primitive tree restructuring operation used in most BST schemes is the well known operation of Rotation [1].", "startOffset": 108, "endOffset": 111}, {"referenceID": 3, "context": "A few memory-less tree reorganizing schemes which use this operation have been presented in the literature among which are the Move-to-Root and the simple Exchange rules [4].", "startOffset": 170, "endOffset": 173}, {"referenceID": 17, "context": "A more detailed version is found in [18, 38].", "startOffset": 36, "endOffset": 44}, {"referenceID": 37, "context": "A more detailed version is found in [18, 38].", "startOffset": 36, "endOffset": 44}, {"referenceID": 53, "context": "Sleator and Tarjan [54] introduced a technique, which also moves the accessed record up to the root of the tree using a restructuring operation called \u201cSplaying\u201d, which actually is a multi-level generalization of the rotation.", "startOffset": 19, "endOffset": 23}, {"referenceID": 11, "context": "Prominent among them is the Monotonic Tree (MT) [12] and Mehlhorn\u2019s D-Tree (DT) [41].", "startOffset": 48, "endOffset": 52}, {"referenceID": 40, "context": "Prominent among them is the Monotonic Tree (MT) [12] and Mehlhorn\u2019s D-Tree (DT) [41].", "startOffset": 80, "endOffset": 84}, {"referenceID": 34, "context": "The MT is a dynamic version of a tree structuring method originally suggested by Knuth [35].", "startOffset": 87, "endOffset": 91}, {"referenceID": 15, "context": "But, as reported in [16], in practice, it does not perform well.", "startOffset": 20, "endOffset": 24}, {"referenceID": 15, "context": "This paper uses a particular heuristic, namely, the Conditional Rotations for a BST (CONROT-BST) [16], which has been shown to reorganize a BST so as to asymptotically arrive at an optimal form.", "startOffset": 97, "endOffset": 101}, {"referenceID": 15, "context": "The authors of [16] have shown that this single rotation leads to a decrease in the overall WPL of the entire tree.", "startOffset": 15, "endOffset": 19}, {"referenceID": 15, "context": "More specifically we shall concentrate on the integration of the CONROT-BST heuristic [16] into a TTOSOM [8], both of which were explained in the preceding sections.", "startOffset": 86, "endOffset": 90}, {"referenceID": 7, "context": "More specifically we shall concentrate on the integration of the CONROT-BST heuristic [16] into a TTOSOM [8], both of which were explained in the preceding sections.", "startOffset": 105, "endOffset": 108}, {"referenceID": 7, "context": "1 Neural Distance As in the case of the TTOSOM [8], the Neural Distance, dN , between two neurons depends on the number of unweighted connections that separate them in the user-defined tree.", "startOffset": 47, "endOffset": 50}, {"referenceID": 7, "context": "The BoA can be formally defined as [8]", "startOffset": 35, "endOffset": 38}, {"referenceID": 7, "context": "As presented in [8], the function TTOSOM Calculate Neighborhood (see Algorithm 3) specifies the steps involved in the calculation of the subset of neurons that are part of the neighborhood of the BMU.", "startOffset": 16, "endOffset": 19}, {"referenceID": 30, "context": "Although storing the history of changes made to the tree can be done optimally [31], the question of explicitly storing the entire history of the BoAs for all the nodes in the tree remains open.", "startOffset": 79, "endOffset": 83}, {"referenceID": 15, "context": "3 Enforcing the BST Property The CONROT-BST heuristic [16] requires that the tree should possess the BST property [18]: Let x be a node in a BST.", "startOffset": 54, "endOffset": 58}, {"referenceID": 17, "context": "3 Enforcing the BST Property The CONROT-BST heuristic [16] requires that the tree should possess the BST property [18]: Let x be a node in a BST.", "startOffset": 114, "endOffset": 118}, {"referenceID": 7, "context": "In this case the consequence of incorporating ADS-based enhancements to the TTOSOM will imply that the results obtained will be significantly different from those shown in [8].", "startOffset": 172, "endOffset": 175}, {"referenceID": 34, "context": "As shown in [35], an optimal arrangement of the nodes of the tree can be obtained using the probabilities of accesses.", "startOffset": 12, "endOffset": 16}, {"referenceID": 15, "context": "The rationale for following this sequence of steps is that the parameters needed to perform the conditional rotation, as specified in [16], includes the \u201ckey\u201d of the element queried, which, in the present context, corresponds to the identity of the BMU.", "startOffset": 134, "endOffset": 138}, {"referenceID": 7, "context": "We know that, as per the results obtained in [8], the TTOSOM is capable of inferring the distribution and structure of the data.", "startOffset": 45, "endOffset": 48}, {"referenceID": 7, "context": "2 Two Dimensional Data Points To demonstrate the power of including ADS in SOMs, we shall now consider the same two-dimensional data sets studied in [8].", "startOffset": 149, "endOffset": 152}, {"referenceID": 7, "context": "This experiment serves as an excellent example to show the differences between our current method and the original TTOSOM algorithm [8], where the same data set with similar settings was utilized.", "startOffset": 132, "endOffset": 135}, {"referenceID": 46, "context": "In this case, the one-dimensional list of neurons is evenly distributed over the sphere, preserving the original properties of the 3-dimensional object and also presenting a shape which reminds the viewer of the so-called Peano curve [47].", "startOffset": 234, "endOffset": 238}, {"referenceID": 22, "context": "According to [23], there are several reasons for performing pattern classification using an unsupervised approach.", "startOffset": 13, "endOffset": 17}, {"referenceID": 43, "context": "The authors of [44] refer to skeletonization in the plane as the process by which a 2-dimensional shape is transformed into a 1-dimensional one, similar to a \u201cstick\u201d figure.", "startOffset": 15, "endOffset": 19}, {"referenceID": 7, "context": "As explained in [8], the traditional methods for skeletonization assume the connectivity of the data points and when this is not the case, more sophisticated methods are required.", "startOffset": 16, "endOffset": 19}, {"referenceID": 7, "context": "Previous efforts involving SOM variants to achieve skeletonization has been proposed [8, 19, 53].", "startOffset": 85, "endOffset": 96}, {"referenceID": 18, "context": "Previous efforts involving SOM variants to achieve skeletonization has been proposed [8, 19, 53].", "startOffset": 85, "endOffset": 96}, {"referenceID": 52, "context": "Previous efforts involving SOM variants to achieve skeletonization has been proposed [8, 19, 53].", "startOffset": 85, "endOffset": 96}, {"referenceID": 7, "context": "We remark that the TTOSOM [8] is the only one which uses a tree-based structure.", "startOffset": 26, "endOffset": 29}, {"referenceID": 7, "context": "Our results reported in [8] confirm that this is actually possible, and we now focus on how the conditional rotations will affect such a skeletonization.", "startOffset": 24, "endOffset": 27}, {"referenceID": 33, "context": "3 Theoretical Analysis According to Kiviluoto [34], there are three different criteria for evaluating the quality of a map.", "startOffset": 46, "endOffset": 50}, {"referenceID": 4, "context": "There exist a variety of measures for quantifying the quality of the topology preservation [5].", "startOffset": 91, "endOffset": 94}, {"referenceID": 48, "context": "The author of [49] surveys a number of relevant measures for the quality of maps, and these include the Quantization Error, the Topographic Product [11], the Topographic Error [34] and the Trustworthiness and Neighborhood Preservation [55].", "startOffset": 14, "endOffset": 18}, {"referenceID": 10, "context": "The author of [49] surveys a number of relevant measures for the quality of maps, and these include the Quantization Error, the Topographic Product [11], the Topographic Error [34] and the Trustworthiness and Neighborhood Preservation [55].", "startOffset": 148, "endOffset": 152}, {"referenceID": 33, "context": "The author of [49] surveys a number of relevant measures for the quality of maps, and these include the Quantization Error, the Topographic Product [11], the Topographic Error [34] and the Trustworthiness and Neighborhood Preservation [55].", "startOffset": 176, "endOffset": 180}, {"referenceID": 54, "context": "The author of [49] surveys a number of relevant measures for the quality of maps, and these include the Quantization Error, the Topographic Product [11], the Topographic Error [34] and the Trustworthiness and Neighborhood Preservation [55].", "startOffset": 235, "endOffset": 239}, {"referenceID": 5, "context": "Although we are currently investigating [6] how the quality of any tree-based SOM (not just our scheme) can be quantified using these metrics.", "startOffset": 40, "endOffset": 43}, {"referenceID": 16, "context": "The ordering of the weights (with respect to the position) of the neurons of the SOM has been proved for unidimensional topologies [17, 36, 51].", "startOffset": 131, "endOffset": 143}, {"referenceID": 35, "context": "The ordering of the weights (with respect to the position) of the neurons of the SOM has been proved for unidimensional topologies [17, 36, 51].", "startOffset": 131, "endOffset": 143}, {"referenceID": 50, "context": "The ordering of the weights (with respect to the position) of the neurons of the SOM has been proved for unidimensional topologies [17, 36, 51].", "startOffset": 131, "endOffset": 143}, {"referenceID": 13, "context": "Budinich, in [14], explains intuitively the problems related to the ordering of neurons in higher dimensional configurations.", "startOffset": 13, "endOffset": 17}, {"referenceID": 28, "context": "[29] introduce a definition of the ordering and show that even though the position of the codebook vectors of the SOM have been ordered, there is still the possibility that a sequence of stimuli will cause their disarrangement.", "startOffset": 0, "endOffset": 4}, {"referenceID": 9, "context": "Some statistical indexes of correlation between the measures of the weights and distances of the related positions have been introduced in [10].", "startOffset": 139, "endOffset": 143}, {"referenceID": 10, "context": "With regard to the topographic product, the authors of [11] have shown the power of the metric by applying it on different artificial and real-world data sets, and also compared it with different measures to quantify the topology [10].", "startOffset": 55, "endOffset": 59}, {"referenceID": 9, "context": "With regard to the topographic product, the authors of [11] have shown the power of the metric by applying it on different artificial and real-world data sets, and also compared it with different measures to quantify the topology [10].", "startOffset": 230, "endOffset": 234}, {"referenceID": 27, "context": "In [28], Haykin mention that the Topographic Product may be employed to compare the quality of different maps, even when these maps possess different dimensionality.", "startOffset": 3, "endOffset": 7}, {"referenceID": 10, "context": "To be more precise, most of the effort towards determining the concept of topology preservation for dimensions greater than unity are specifically focused on the SOM [11, 17, 36, 14, 29, 10], and do not define how a treelike topology should be measured nor how to define the order in topologies which are not grid-based.", "startOffset": 166, "endOffset": 190}, {"referenceID": 16, "context": "To be more precise, most of the effort towards determining the concept of topology preservation for dimensions greater than unity are specifically focused on the SOM [11, 17, 36, 14, 29, 10], and do not define how a treelike topology should be measured nor how to define the order in topologies which are not grid-based.", "startOffset": 166, "endOffset": 190}, {"referenceID": 35, "context": "To be more precise, most of the effort towards determining the concept of topology preservation for dimensions greater than unity are specifically focused on the SOM [11, 17, 36, 14, 29, 10], and do not define how a treelike topology should be measured nor how to define the order in topologies which are not grid-based.", "startOffset": 166, "endOffset": 190}, {"referenceID": 13, "context": "To be more precise, most of the effort towards determining the concept of topology preservation for dimensions greater than unity are specifically focused on the SOM [11, 17, 36, 14, 29, 10], and do not define how a treelike topology should be measured nor how to define the order in topologies which are not grid-based.", "startOffset": 166, "endOffset": 190}, {"referenceID": 28, "context": "To be more precise, most of the effort towards determining the concept of topology preservation for dimensions greater than unity are specifically focused on the SOM [11, 17, 36, 14, 29, 10], and do not define how a treelike topology should be measured nor how to define the order in topologies which are not grid-based.", "startOffset": 166, "endOffset": 190}, {"referenceID": 9, "context": "To be more precise, most of the effort towards determining the concept of topology preservation for dimensions greater than unity are specifically focused on the SOM [11, 17, 36, 14, 29, 10], and do not define how a treelike topology should be measured nor how to define the order in topologies which are not grid-based.", "startOffset": 166, "endOffset": 190}], "year": 2009, "abstractText": "Numerous variants of Self-Organizing Maps (SOMs) have been proposed in the literature, including those which also possess an underlying structure, and in some cases, this structure itself can be defined by the user Although the concepts of growing the SOM and updating it have been studied, the whole issue of using a self-organizing Adaptive Data Structure (ADS) to further enhance the properties of the underlying SOM, has been unexplored. In an earlier work, we impose an arbitrary, user-defined, tree-like topology onto the codebooks, which consequently enforced a neighborhood phenomenon and the so-called tree-based Bubble of Activity (BoA). In this paper, we consider how the underlying tree itself can be rendered dynamic and adaptively transformed. To do this, we present methods by which a SOM with an underlying Binary Search Tree (BST) structure can be adaptively re-structured using Conditional Rotations (CONROT). These rotations on the nodes of the tree are local, can be done in constant time, and performed so as to decrease the Weighted Path Length (WPL) of the entire tree. In doing this, we introduce the pioneering concept referred to as Neural Promotion, where neurons gain prominence in the Neural Network (NN) as their significance increases. We are not aware of any research which deals with the issue of Neural Promotion. The advantages of such a scheme is that the user need not be aware of any of the topological peculiarities of the stochastic data distribution. Rather, the algorithm, referred to as the TTOSOM with Conditional Rotations (TTOCONROT), converges in such a manner that the neurons are ultimately placed in the input space so as to represent its stochastic distribution, and additionally, the neighborhood properties of the neurons suit the best BST that represents the data. These properties have been confirmed by our experimental results on a variety of data sets. We submit that all of these concepts are both novel and of a pioneering sort.", "creator": "gnuplot 4.2 patchlevel 4 "}}}