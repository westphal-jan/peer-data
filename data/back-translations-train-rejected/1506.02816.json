{"id": "1506.02816", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "9-Jun-2015", "title": "Leveraging Textual Features for Best Answer Prediction in Community-based Question Answering", "abstract": "This paper addresses the problem of determining the best answer in Community-based Question Answering (CQA) websites by focussing on the content. In particular, we present a system, ACQUA [", "histories": [["v1", "Tue, 9 Jun 2015 08:09:34 GMT  (162kb)", "https://arxiv.org/abs/1506.02816v1", "1 figure, 2 tables"], ["v2", "Wed, 17 Jun 2015 10:07:48 GMT  (162kb)", "http://arxiv.org/abs/1506.02816v2", "1 figure, 2 tables"]], "COMMENTS": "1 figure, 2 tables", "reviews": [], "SUBJECTS": "cs.CL cs.IR", "authors": ["george gkotsis", "maria liakata", "carlos pedrinaci", "john domingue"], "accepted": false, "id": "1506.02816"}, "pdf": {"name": "1506.02816.pdf", "metadata": {"source": "CRF", "title": "Leveraging Textual Features for Best Answer Prediction in Community-based Question Answering", "authors": ["George Gkotsis", "Carlos Pedrinaci", "John Domingue", "Maria Liakata"], "emails": ["firstname.lastname@open.ac.uk", "m.liakata@warwick.ac.uk"], "sections": [{"heading": null, "text": "This year is the highest in the history of the country."}, {"heading": "1. FEATURE DISCRETISATION", "text": "Our solution, called discretization, is presented in detail in [2] and claims that a novel method of using shallow characteristics and overcoming the limitations mentioned above is being used. Intuitively, our approach is to treat the aggregation of answers for each question as an information unit that can improve the training process. Instead of treating each answer independently of the other answers it competes with, our approach is to assess the value of the characteristics of each answer in relation to the corresponding characteristics of their competitors. We are introducing a new set of characteristics derived from the linguistic characteristics used so far: instead of dealing with continuous values, these new characteristics are the result of grouping, sorting and discretization. We present an example of the longitudinal characteristic. Consider the example of Table 1, where there are two candidate answers for a question (i.e., the question has Id 5 answers to the answers to Id 6 and 7)."}, {"heading": "2. EVALUATION", "text": "Table 2 presents the results using different characteristics and 10x validation. In addition, the table shows the averages for 21 SE sites (including SO) as output of different ratings for 4 million questions and more than 8 million answers. First, we use the absolute values of textual characteristics with low results (58% accuracy, case 1); the second and third cases both use the discredited characteristics, while the third additionally uses the other characteristics (i.e. AnswerCount and CreationDate); cases 2 and 3 represent our suggested prediction method; in addition, case 4 refers to a \"traditional\" approach based on pure linguistics and user ratings; we can see that while a whole range of new characteristics are included in the dataset, the performance of the classification remains lower than case 3, which is based on linguistic information; case 5 maintains the user ratings in addition to the inclusion of all characteristics of case 3."}, {"heading": "Acknowledgments", "text": "This work was supported by the CARRE (611140) and COMPOSE (317862) projects funded by the Seventh Framework Programme of the European Commission."}, {"heading": "3. REFERENCES", "text": "[1] A. Anderson, D. Huttenlocher, J. Kleinberg and J. Leskovec.Discovering value from community activity on focus question Answer sites: a case study of stack overflow. In Proceedings of the 18th ACM SIGKDD international conference on knowledge discovery and data mining, pp. 850-858. ACM, 2012. [2] G. Gkotsis, K. Stepanyan, C. Pedrinaci, J. Domingue, and M. Liakata. All this is contained in the content: State of the art best answer prediction based on discretization of Flachlinguistic features. In Proceedings of the 2014 ACM Conference on Web Science, WebSci '14, pp. 202-210, New York, NY, USA, 2014. ACM. [3] E. Pitler and A. Nenkova. Revisiting readability: A unified framework for proceedings of the Conference on Empirical Methods in Natural Language Processing, pp. 186-Z4, the Community Aputating Best Aisting S. 2008."}], "references": [{"title": "Discovering value from community activity on focused question answering sites: a case study of stack overflow", "author": ["A. Anderson", "D. Huttenlocher", "J. Kleinberg", "J. Leskovec"], "venue": "In Proceedings of the 18th ACM SIGKDD international conference on Knowledge discovery and data mining,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2012}, {"title": "It\u2019s all in the content: State of the art best answer prediction based on discretisation of shallow linguistic features", "author": ["G. Gkotsis", "K. Stepanyan", "C. Pedrinaci", "J. Domingue", "M. Liakata"], "venue": "In Proceedings of the 2014 ACM Conference on Web Science,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2014}, {"title": "Revisiting readability: A unified framework for predicting text quality", "author": ["E. Pitler", "A. Nenkova"], "venue": "In Proceedings of the Conference on Empirical Methods in Natural Language Processing,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2008}, {"title": "Towards predicting the best answers in community-based question-answering services", "author": ["Q. Tian", "P. Zhang", "B. Li"], "venue": "In Seventh International AAAI Conference on Weblogs and Social Media,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2013}], "referenceMentions": [{"referenceID": 2, "context": "Researchers in related fields have used lexical, syntactic, and discourse features to produce a predictive model of readers\u2019 judgments [3].", "startOffset": 135, "endOffset": 138}, {"referenceID": 0, "context": "or the reputation of the user [1].", "startOffset": 30, "endOffset": 33}, {"referenceID": 2, "context": "In our approach, we revisit the case of shallow linguistic features and use features found in [3].", "startOffset": 94, "endOffset": 97}, {"referenceID": 3, "context": "[4]) these results will constitute our baseline for evaluating the proposed solution.", "startOffset": 0, "endOffset": 3}, {"referenceID": 1, "context": "FEATURE DISCRETISATION Our solution called discretisation is presented in detail in [2] and asserts the adoption of a novel way of leveraging shallow features and overcome the above limitations.", "startOffset": 84, "endOffset": 87}], "year": 2015, "abstractText": "One of the intriguing problems in Community-based Question Answering (CQA) research is the automatic identification of the best answer, which is expected to benefit various stakeholders. First of all, since several answers are provided for each question, the readers of these websites will be able to process the candidate answers more efficiently and mitigate the \u201cinformation overload\u201d phenomenon. Secondly, a mechanism that identifies high quality answers will increase awareness within the community and will help to put more effort into questions that remain poorly answered. For instance, in StackOverflow(SO) alone, as of September 2013, we found that approximately 33% of the questions have yet to be marked as resolved (i.e., out of the 5 million, 1.7 million questions have no answer marked as \u201caccepted\u201d). Researchers in related fields have used lexical, syntactic, and discourse features to produce a predictive model of readers\u2019 judgments [3]. In several cases, the use of shallow features, i.e. features that do not employ semantic or syntactic parsing such as sentence length or word length, have been shown to be effective in assessing properties such as ease of reading or usefulness. However, with respect to CQA, research efforts towards the exploitation of shallow features report relatively low results. To improve the efficacy of their models, researchers refer to more contextual information, such as the score of each answer, the comments received or the reputation of the user [1]. However, these features may not be readily available since a) comments and scores introduce an inherent delay, and b) features based on reputation may not be applicable on a newly formed community or pose a threat to its development (i.e. preferential attachment) and result in the reinforcement of the pre-existing community hierarchy. In our approach, we revisit the case of shallow linguistic features and use features found in [3]. Figure 1 shows the average feature values for the accepted answers together with the non-accepted ones of SO using a one-month window time frame. As seen from the figure, the linguistic features clearly differentiate the accepted from the non-accepted answers. More specifically, accepted answers tend to be longer, use a less common vocabulary, contain longer words, more words per sentence and the longest sentences are lengthier. Even though the above remarks look promising concerning best answer prediction, when training a binary classifier prediction remains weak (58% precision and 0.56 F-Measure", "creator": "LaTeX with hyperref package"}}}