{"id": "1506.03018", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "9-Jun-2015", "title": "On the Interpretability of Conditional Probability Estimates in the Agnostic Setting", "abstract": "Many classification algorithms produce confidence measures in the form of conditional probability of labels given the features of the target instance. It is desirable to be make these confidence measures calibrated or consistent, in the sense that they correctly capture the belief of the algorithm in the label output. For instance, if the algorithm outputs a label with confidence measure $p$ for $n$ times, then the output label should be correct approximately $np$ times overall. Calibrated confidence measures lead to higher interpretability by humans and computers and enable downstream analysis or processing. In this paper, we formally characterize the consistency of confidence measures and prove a PAC-style uniform convergence result for the consistency of confidence measures. We show that finite VC-dimension is sufficient for guaranteeing the consistency of confidence measures produced by empirically consistent classifiers. Our result also implies that we can calibrate confidence measures produced by any existing algorithms with monotonic functions, and still get the same generalization guarantee on consistency.", "histories": [["v1", "Tue, 9 Jun 2015 17:41:48 GMT  (15kb)", "http://arxiv.org/abs/1506.03018v1", null], ["v2", "Tue, 28 Feb 2017 18:21:57 GMT  (210kb,D)", "http://arxiv.org/abs/1506.03018v2", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["yihan gao", "aditya parameswaran", "jian peng"], "accepted": false, "id": "1506.03018"}, "pdf": {"name": "1506.03018.pdf", "metadata": {"source": "CRF", "title": "On the Uniform Convergence of Consistent Confidence Measures", "authors": ["Yihan Gao", "Aditya Parameswaran"], "emails": ["ygao34@illinois.edu", "adityagp@illinois.edu"], "sections": [{"heading": null, "text": "ar Xiv: 150 6.03 018v 1 [cs.L G] 9"}, {"heading": "1 Introduction", "text": "It is indeed the case that we are able to go in search of a solution that is capable of finding a solution with which we can identify."}, {"heading": "1.1 Problem Statement", "text": "Let X be a sample space and Y = {\u00b1 1} be the set of labels. Let P be a probability measure defined on X \u00b7 Y that indicates the common distribution of the sample x and its label y. Let Sn = {(x1, y1),.., (xn, yn)} i.i.i.d. Samples from (X \u00b7 Y, P). Let us consider a set of classifiers H so that each h-H predicts a confidence quantity h (x), y = 1) [0, 1] that indicates how likely the label y 1 x is given. Let us define the consistency measurement of h with respect to P as: c (h) = sup p1, p2 | p (p1 < h (x) \u2264 p2 < h (x) > empi.i.i.i.i.i.i.i.i.i.i.i.i.i.i.i.i.ii.i.id. Let us define the consistency measurement as: h measurement in relation to P."}, {"heading": "1.2 Main Results", "text": "(1) Theorem 1: (1) Theorem 1: (1) Theorem 1: (1) Theorem 1: (1) Theorem 1: (2) (2) (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2, (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2, (2), (2), (2), (2), (2), (2, (2), (2), (2), (, (2), (2), (2), (2), (, (2), \"(2), (, (2), (,\" (, (2), (, (2), (, (2), (, (2), (2), \"(, (, (,\"), (, (2), (, (, (, \"), (, (,\"), (, (, (, \"), (, (,\"), (, (, \"), (, (, (, (), (, (, (, (,), (, (, (,), (, (,), (,), (, (, (,), (, (,), (, (,), (,), (,),), (,), (, (, (, ("}, {"heading": "2 Preliminaries", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "2.1 VC-dimension and Sauer\u2019s Lemma", "text": "Let A be a collection of random events, i.e. of subsets of X \u00b7 Y. Let Sn = {(x1, y1),..., (xn, yn) be a series of samples from X \u00b7 Y. Define the constraint of A to Sn as follows: ASn = {A-Sn: A-A-A} We say that A Sn will break if the constraint of A to Sn contains all subsets of Sn: B Sn, A-A, B = A-Snor equivalent, | ASn | = 2 | Sn | The VC dimension [13] of A is defined as the maximum number of samples that can be arranged so that A shatters them. Let's assume that the VC dimension of A is finite, then Sauer's lemma [10] [13] can be used to bind the size of ASn. Let the VC dimension of A be d, then for each Sn: | \u2264 (1) and then \u2264 (d)."}, {"heading": "2.2 Rademacher Complexity", "text": "If H is a collection of functions from X \u00b7 Y to [0, 1], the Rademacher complexity [3] of H with respect to Sn is defined as follows: RSn (H) = 1n E\u03c3 \u0445 {\u00b1 1} n [suph-Hn-i = 1\u03c3ih (xi, yi)] Rademacher complexity can be used to characterize the uniform convergence rate of a function collection. If Sn is i.i.d. samples of X \u00b7 Y, then, with a probability of at least 1 \u2212 \u03b4 [11]: sup h-H | 1nn-H | 1nn-H = 1h (xi, yi) \u2212 Eh (x, y) | \u2264 2ESnRSn (H) + \u221a 2 ln (2 / \u03b4) n (3) sup h-H | 1nn-H = 1h-H (xi, yi) \u2212 Eh (x, y) | 2h (x, y) | R2h (H), Sn \u00b0 (H) + 4 (4 / \u03b4) n (4) (sup) n), we do not assume that the function is also (4-H)."}, {"heading": "2.3 Massart Lemma", "text": "Let H be a collection of functions from X \u00b7 Y to [0, 1]. Let HSn be the constraint of H to Sn: HSn = {(h (x1, y1), h (x2, y2),.., h (xn, yn): h-H} If the cardinality of HSn is finite, then the wheel-maker complexity of H with respect to Sn can be limited as follows [11]: RSn (H) \u2264 sup a-HSn | | | a \u00b2 2 \u00b0 2 ln | HSn | n (5) Combined with Sauer's Lemma, it follows that A is a collection of subsets of X \u00d7 Y so that the VC dimension of A is at most d, then RSn ({1xA: A \u00b2 A}) \u2264 2d (ln (n / d) + 1) n (6)."}, {"heading": "3 Uniform Convergence of Consistency Measure", "text": "In this section we prove theorem 1 and outline the proof of theorem 2. For later convenience, we define the relative frequency of the event {p1 < h (x) < h (x) \u2264 p2, yi = 1to be the relative frequency of the event {p1 < h (x) \u2264 p2, y = 1}. Define FD, p1, p2 (h) = P (p1 < h) \u2264 p2, y = 1) to be the relative frequency of the same event. We define ES, p1, p2 (h) = 1nn \u00b2 ph (xi) 1p1 < h (h) < h (x) \u2264 p2 < h (p2) \u2264 p2, p2 (h) \u2264 p2, ph (ph)."}, {"heading": "4 Generalization Bound for Linear Classifier", "text": "In this section, we examine the consistency of confidence measures generated by a class of linear classifiers."}, {"heading": "5 Learning Consistent Classifier", "text": "In this section, we discuss methods for learning classifiers with low inconsistencies. In Section 5.1, we justify the use of protocol probability as an objective function when learning consistent classifiers. In Section 5.2, we discuss the use of monotonous functions to calibrate existing classifiers and show that this problem leads to linear programming formulation."}, {"heading": "5.1 Maximum Likelihood Classifier", "text": "The log probability function of the classifier h to Sn is defined as: LS (h) = 1nn \u2211 i = 1 [1yi = 1 lnh (xi) + 1yi = \u2212 1 ln (1 \u2212 h (xi))] The expectation of the log probability is: LD (h) = EP [1y = 1 lnh (x) + 1y = \u2212 1 ln (1 \u2212 h (x))) ln (1 \u2212 hx)] the random variable hx = h (x), rewrite the expected log probability as: LD (h) = 1 = hx hx [P (y = 1 | hx) lnhx \u2212 P (y = \u2212 hx) ln (1 \u2212 hx) the random variable hx (1 \u2212 hx)."}, {"heading": "5.2 Calibrating Confidence Measure with Monotonic Function", "text": "As we have proved in Lemma 2, the application of left-standing monotonic figures to existing confidence measures does not increase the Rademacher complexity of the original hypotheses space. Therefore, another natural approach is to learn such figures in order to calibrate the confidence measures produced by existing classifications. Assuming that we are given the classifier h: X \u2192 R, we want to find a link-continuously increasing function f from R to [0, 1], the empirical inconsistency measures cemp (f) h: (f) h: (x1, y1),., (xn, yn)} the h: (x1) < h (x2) < h: h: If h (xi) s: are indistinguishable, we can assign weights to the samples and the algorithm remains the same."}], "references": [{"title": "Blackwell approachability and low-regret learning are equivalent", "author": ["J. Abernethy", "P.L. Bartlett", "E. Hazan"], "venue": "arXiv preprint arXiv:1011.1936", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2010}, {"title": "A uniform convergence bound for the area under the roc curve", "author": ["S. Agarwal", "S. Har-Peled", "D. Roth"], "venue": "Proceedings of the Tenth International Workshop on Artificial Intelligence and Statistics, pages 1\u20138", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2005}, {"title": "Rademacher and gaussian complexities: Risk bounds and structural results", "author": ["P.L. Bartlett", "S. Mendelson"], "venue": "The Journal of Machine Learning Research, 3:463\u2013482", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2003}, {"title": "Pattern recognition and machine learning, volume 4. springer", "author": ["C.M. Bishop"], "venue": "New York,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2006}, {"title": "Minimizing uncertainty in pipelines", "author": ["N.N. Dalvi", "A.G. Parameswaran", "V. Rastogi"], "venue": "NIPS", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2012}, {"title": "Asymptotic calibration", "author": ["D.P. Foster", "R.V. Vohra"], "venue": "Biometrika, 85(2):379\u2013390", "citeRegEx": "6", "shortCiteRegEx": null, "year": 1998}, {"title": "Decision theoretic generalizations of the pac model for neural net and other learning applications", "author": ["D. Haussler"], "venue": "Information and computation, 100(1):78\u2013150", "citeRegEx": "7", "shortCiteRegEx": null, "year": 1992}, {"title": "Knows what it knows: a framework for self-aware learning", "author": ["L. Li", "M.L. Littman", "T.J. Walsh", "A.L. Strehl"], "venue": "Machine learning, 82(3):399\u2013443", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2011}, {"title": "Beyond myopic inference in big data pipelines", "author": ["K. Raman", "A. Swaminathan", "J. Gehrke", "T. Joachims"], "venue": "Proceedings of the 19th ACM SIGKDD international conference on Knowledge discovery and data mining, pages 86\u201394. ACM", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2013}, {"title": "On the density of families of sets", "author": ["N. Sauer"], "venue": "Journal of Combinatorial Theory, Series A, 13(1):145\u2013147", "citeRegEx": "10", "shortCiteRegEx": null, "year": 1972}, {"title": "Understanding Machine Learning: From Theory to Algorithms", "author": ["S. Shalev-Shwartz", "S. Ben-David"], "venue": "Cambridge University Press", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2014}, {"title": "On the consistency of multiclass classification methods", "author": ["A. Tewari", "P.L. Bartlett"], "venue": "The Journal of Machine Learning Research, 8:1007\u20131025", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2007}, {"title": "On the uniform convergence of relative frequencies of events to their probabilities", "author": ["V.N. Vapnik", "A.Y. Chervonenkis"], "venue": "Theory of Probability & Its Applications, 16(2):264\u2013280", "citeRegEx": "13", "shortCiteRegEx": null, "year": 1971}, {"title": "Crowder: Crowdsourcing entity resolution", "author": ["J. Wang", "T. Kraska", "M.J. Franklin", "J. Feng"], "venue": "Proceedings of the VLDB Endowment, 5(11):1483\u20131494", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2012}], "referenceMentions": [{"referenceID": 10, "context": "In standard PAC learning theory [11], these confidence measures are usually validated by bounding the expected value of some loss function (for example, the log-likelihood function).", "startOffset": 32, "endOffset": 36}, {"referenceID": 3, "context": "Consistent confidence measures have many applications: they can be used in decision making processes where misclassifications have asymmetric loss [4], in data processing pipelines where output of upstream classifiers are used in downstream programs [9, 5], or in adaptive data verification processes where instances are referred to a human verifier when the automatic classifier is not confident enough [14].", "startOffset": 147, "endOffset": 150}, {"referenceID": 8, "context": "Consistent confidence measures have many applications: they can be used in decision making processes where misclassifications have asymmetric loss [4], in data processing pipelines where output of upstream classifiers are used in downstream programs [9, 5], or in adaptive data verification processes where instances are referred to a human verifier when the automatic classifier is not confident enough [14].", "startOffset": 250, "endOffset": 256}, {"referenceID": 4, "context": "Consistent confidence measures have many applications: they can be used in decision making processes where misclassifications have asymmetric loss [4], in data processing pipelines where output of upstream classifiers are used in downstream programs [9, 5], or in adaptive data verification processes where instances are referred to a human verifier when the automatic classifier is not confident enough [14].", "startOffset": 250, "endOffset": 256}, {"referenceID": 13, "context": "Consistent confidence measures have many applications: they can be used in decision making processes where misclassifications have asymmetric loss [4], in data processing pipelines where output of upstream classifiers are used in downstream programs [9, 5], or in adaptive data verification processes where instances are referred to a human verifier when the automatic classifier is not confident enough [14].", "startOffset": 404, "endOffset": 408}, {"referenceID": 12, "context": "\u2022 We apply our result to a class of linear classifiers and show that we can achieve uniform convergence of consistency using essentially the same order of sample size implied by the standard argument based on VC-dimension [13].", "startOffset": 222, "endOffset": 226}, {"referenceID": 6, "context": "Some existing papers have considered the uniform convergence of the expected value of certain loss functions on confidence measures: Haussler [7] considered the expected loss of the action that we take based on confidence measures as opposed to confidence measures themselves; Agarwal et al.", "startOffset": 142, "endOffset": 145}, {"referenceID": 1, "context": "[2] studied the uniform convergence of the area under ROC curve.", "startOffset": 0, "endOffset": 3}, {"referenceID": 7, "context": "[8] proposed a learning framework that explicitly allows a classifier to respond \u201cI don\u2019t know\u201d, but whenever it produces a label it must be correct.", "startOffset": 0, "endOffset": 3}, {"referenceID": 5, "context": "Our notion of consistency is similar to the notion of calibration in prediction theory [6, 1], where the goal is also to make predicted probability values match the empirical frequency of correct predictions.", "startOffset": 87, "endOffset": 93}, {"referenceID": 0, "context": "Our notion of consistency is similar to the notion of calibration in prediction theory [6, 1], where the goal is also to make predicted probability values match the empirical frequency of correct predictions.", "startOffset": 87, "endOffset": 93}, {"referenceID": 11, "context": "Tewari and Bartlett [12] studied the consistency of a conditional probability estimator.", "startOffset": 20, "endOffset": 24}, {"referenceID": 0, "context": "Consider a set of classifiers H, so that each h \u2208 H predicts a confidence measure h(x) \u2208 [0, 1] indicating how likely the label y is 1 given x.", "startOffset": 89, "endOffset": 95}, {"referenceID": 0, "context": ", functions from X to [0, 1].", "startOffset": 22, "endOffset": 28}, {"referenceID": 12, "context": "Define the restriction of A to Sn as: ASn = {A \u2229 Sn : A \u2208 A} We say that A shatters Sn if the restriction of A to Sn contains all subsets of Sn: \u2200B \u2286 Sn, \u2203A \u2208 A, B = A \u2229 Sn or equivalently, |ASn | = 2 |Sn| The VC-Dimension [13] of A is defined as the maximum number of samples that can be arranged so that A shatters them.", "startOffset": 223, "endOffset": 227}, {"referenceID": 9, "context": "Suppose the VC-dimension of A is finite, then Sauer\u2019s Lemma [10][13] can be used to bound the size of ASn .", "startOffset": 60, "endOffset": 64}, {"referenceID": 12, "context": "Suppose the VC-dimension of A is finite, then Sauer\u2019s Lemma [10][13] can be used to bound the size of ASn .", "startOffset": 64, "endOffset": 68}, {"referenceID": 0, "context": "2 Rademacher Complexity Let H be a collection of functions from X \u00d7 Y to [0, 1], the Rademacher Complexity [3] of H with respect to Sn is defined as: RSn(H) = 1 n E\u03c3\u223c{\u00b11}n [sup h\u2208H n", "startOffset": 73, "endOffset": 79}, {"referenceID": 2, "context": "2 Rademacher Complexity Let H be a collection of functions from X \u00d7 Y to [0, 1], the Rademacher Complexity [3] of H with respect to Sn is defined as: RSn(H) = 1 n E\u03c3\u223c{\u00b11}n [sup h\u2208H n", "startOffset": 107, "endOffset": 110}, {"referenceID": 10, "context": "samples of X \u00d7 Y , then with probability of at least 1\u2212 \u03b4 [11]:", "startOffset": 58, "endOffset": 62}, {"referenceID": 0, "context": "2 ln(4/\u03b4) n (4) Sometimes we also allow H to be collection of functions from X to [0, 1] in the above results.", "startOffset": 82, "endOffset": 88}, {"referenceID": 0, "context": "3 Massart Lemma Let H be a collection of functions from X \u00d7 Y to [0, 1].", "startOffset": 65, "endOffset": 71}, {"referenceID": 10, "context": ", h(xn, yn)) : h \u2208 H} If the cardinality of HSn is finite, then the Rademacher Complexity of H with respect to Sn can be bounded as follows [11]: RSn(H) \u2264 sup a\u2208HSn ||a||2 \u221a", "startOffset": 140, "endOffset": 144}, {"referenceID": 0, "context": "Let HR be a collection of classifiers that output confidence measures in R, let M be the collection of all left-continuous monotonically increasing functions from R to [0, 1], define H4 = {1p1<m(h(x))\u2264p2 : p1, p2 \u2208 R,m \u2208 M, h \u2208 HR} H5 = {1\u03b1<h(x)\u2264\u03b2 : \u03b1, \u03b2 \u2208 R, h \u2208 HR} Then, RSn(H4) = RSn(H5)", "startOffset": 168, "endOffset": 174}, {"referenceID": 0, "context": "Let X = R and M be the collection of all left-continuous monotonically increasing functions from R to [0, 1].", "startOffset": 102, "endOffset": 108}, {"referenceID": 0, "context": "Assume that we are given classifier h : X \u2192 R, we want to find a left-continuous monotonically increasing function f from R to [0, 1] that minimizes empirical inconsistency measures cemp(f \u25e6 h).", "startOffset": 127, "endOffset": 133}], "year": 2017, "abstractText": "Many classification algorithms produce confidence measures in the form of conditional probability of labels given the features of the target instance. It is desirable to be make these confidence measures calibrated or consistent, in the sense that they correctly capture the belief of the algorithm in the label output. For instance, if the algorithm outputs a label with confidence measure p for n times, then the output label should be correct approximately np times overall. Calibrated confidence measures lead to higher interpretability by humans and computers and enable downstream analysis or processing. In this paper, we formally characterize the consistency of confidence measures and prove a PAC-style uniform convergence result for the consistency of confidence measures. We show that finite VCdimension is sufficient for guaranteeing the consistency of confidence measures produced by empirically consistent classifiers. Our result also implies that we can calibrate confidence measures produced by any existing algorithms with monotonic functions, and still get the same generalization guarantee on consistency.", "creator": "LaTeX with hyperref package"}}}