{"id": "1506.03229", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "10-Jun-2015", "title": "A cognitive neural architecture able to learn and communicate through natural language", "abstract": "Communicative interactions involve a kind of procedural knowledge that is used by the human brain for the elaboration of verbal and nonverbal inputs and for language production. Although considerable work has been done on modeling human language abilities, it has been difficult to bring them together to a comprehensive tabula rasa system. This work presents a cognitive system, entirely based on a large-scale neural architecture, which was developed to shed light on how the procedural knowledge involved in language elaboration arises from neural processes. The main component of this system is the central executive, which is a supervising system that coordinates the other components of the working memory. In our model, the central executive is a neural network that takes as input the neural activation states of the short-term memory and yields as output mental actions, which control the flow of information among the working memory components through neural gating mechanisms. The proposed system is capable of learning to communicate through natural language starting from tabula rasa, without any a priori knowledge of the structure of phrases, meaning of words, role of the different classes of words, only by interacting with a human through a text-based interface, using an open-ended incremental learning process. It is able to learn nouns, verbs, adjectives, pronouns and other word classes, to use them in sentences and to generalize this knowledge both in receptive and expressive language. The model was validated on a corpus of 1587 input sentences, based on literature on early language assessment, at the level of about 4-years old child, and produced 521 output sentences, expressing a broad range of functionalities that characterize human communication in language elaboration.", "histories": [["v1", "Wed, 10 Jun 2015 09:25:59 GMT  (2758kb)", "http://arxiv.org/abs/1506.03229v1", null], ["v2", "Fri, 12 Jun 2015 16:58:57 GMT  (2762kb)", "http://arxiv.org/abs/1506.03229v2", null], ["v3", "Mon, 22 Jun 2015 16:43:59 GMT  (3352kb)", "http://arxiv.org/abs/1506.03229v3", "The source code of the software, the User Guide and the datasets used for its validation are available in the ANNABELL web site atthis https URL"]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["bruno golosio", "angelo cangelosi", "olesya gamotina", "giovanni luca masala"], "accepted": false, "id": "1506.03229"}, "pdf": {"name": "1506.03229.pdf", "metadata": {"source": "CRF", "title": "A cognitive neural architecture able to learn and communicate through natural language", "authors": ["Bruno Golosio", "Angelo Cangelosi", "Olesya Gamotina", "Giovanni Luca Masala"], "emails": ["golosio@uniss.it"], "sections": [{"heading": null, "text": "The proposed system is able to communicate through natural language starting from tabula rasa, without a priori knowing the structure of phrases, the meaning of words, the role of the various word classes, only by interacting with a human being through a text-based user interface, by means of an unlimited incremental learning process. It is able to learn nouns, verbs, adjectives, pronouns and other word classes, to use them in sentences and to generalize this knowledge in both receptive and expressive language. The model was validated using a corpus of 1587 input sentences, based on literature on early language tests, at the level of a 4-year-old child, and produced 521 output sentences expressing a wide range of functionalities that characterize human communication in language development. Keywords: large-scale artificial neural networks, understanding of human language, verbal working memory, cognitive cultures, architectural levers, e-mail-mail correspondence rule unrespondent @;"}, {"heading": "1 Introduction", "text": "The attempts to build artificial systems capable of simulating important aspects of human cognitive abilities have a long history, and they have contributed to the debate about two different theoretical approaches, computationism and connectionism. According to the computer theory of mind, the brain is an information processing system, and thinking can be described as a calculation based on mental states [1,2]. This perspective has led to the implementation of a class of cognitive architectures called symbolic [3-5] (see Ref.s [7] for a review). Symbolic architectures can perform high-level cognitive functions, such as complex reasoning and planning."}, {"heading": "1.1 Working memory models", "text": "In fact, it is a purely mental game, trying to find a solution that paves the way for the future; it is only a matter of time before there is a solution, until there is an agreement."}, {"heading": "1.2 The mental action sequence", "text": "This year, it is only a matter of time before there is an agreement, until there is an agreement."}, {"heading": "1.3 Localization of the verbal working memory in the brain", "text": "The localization of areas of the brain involved in language comprehension and speech production requires a combination of neuroimaging and psycholinguistic research. Several studies on the functional neuroanatomy of language suggest that both semantic and syntactic processes mainly affect the left frontal cortex and part of the temporal cortex; the left frontal cortex is considered responsible for strategic and executive aspects of speech processing; the left temporal cortex supports the processes that identify phonetic and lexical elements; it is involved in the storage and recovery of phonological, syntactic and semantic information; and all classical neurobiological models of language assign a fundamental role to the Broca area, which includes Brodman's areas (BA) 44 and 45, in the left frontal cortex. Several studies show that BA 47 and the central part of language are also involved in language processing tasks [28-30]."}, {"heading": "2 The ANNABELL model", "text": "In fact, it is such that most of them will be able to survive themselves without there being a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there"}, {"heading": "2.1 Learning mechanisms and signal flow control", "text": "It is only a matter of time before it will happen, until it will happen."}, {"heading": "2.2 Global organization of the model", "text": "In fact, it is the case that you see yourself in a position to be in, without being able to do what you need to do in order to be able to do it."}, {"heading": "3 The database", "text": "The answer to this question is: \"What is the answer to this question?,\" \"What is the answer?,\" \"What is the answer?,\" \"What is the answer?,\" \"What is the answer?,\" \"What is the answer?,\" \"What is the answer?,\" \"What is the answer?,\" \"What is the answer?,\" \"What is the answer?,\" \"\" What is the answer?, \"\" \"What is the answer?,\" \"\" What is the answer?, \"\" \"The answer?,\" \"\" The answer?, \"\" The answer \"The answer\" The answer \"\" The answer \"\" The answer \"\" The answer \"\" The answer \"\" \"\" \"\" \"\" \"\" \"\" \""}, {"heading": "3.3 The categorization dataset", "text": "This data set uses 62 different animal names from 6 categories: 13 mammals, 13 birds, 13 fish, 8 reptiles, 4 amphibians, 11 insects. The animal name Affiliations to the six categories are specified by 62 declarative sentences of the form: the < animal > is a < category > where < animal > is an animal name, and < category > is one of the six categories listed above, such as: the tortoise is a reptile Other 6 sentences of the form: < category > s are animalities and a how-sentences are included in the system to deal with categorization hierarchies. Thedataset also contains 48 declarative sentences of the form: the < animal > is a reptile > is an adjective > where < adjective > is one of the five adjectives: large, dangerous, domestic, fast or small."}, {"heading": "3.4 The communicative interactions dataset", "text": "The fourth session is dedicated to communicative interactions, and it is based on a mother-child dialogue taken from the Warren-Leubecker corpus [41,42], which is part of the CHILDES database [43]. This corpus contains data from 20 children who interact with one of their parents, and the sessions took place at the child's home. Parents were instructed to engage the child in conversation and speak to him as naturally as possible, which seemed more appropriate for training the system than others, because the children were appropriate and because verbal communication prevailed over non-verbal communication, play and action. The session used in this work is based on the file \"david.cha,\" which contains a transcription of the dialogue between a 5-year-old child and his mother. The system was trained in a text-based virtual environment in which we syndicate the system. First, we guessed what kind of past experiences the child might be compatible with the David dialogue."}, {"heading": "3.5 The virtual environment dataset", "text": "The fifth data set is a text-based virtual environment in which the system is trained to perform simple tasks using verbal commands. Training takes place in a virtual house called room _ 0,..., room _ 24, arranged in a 5 x 5 square. A person is located in the central room, i.e. room _ 12, which is also the starting point of the system. Eight objects, called object _ 1,..., object _ 8, are randomly distributed in the eight closest secondary neighboring rooms, with the caveat that different objects should be located in different rooms. Each room has a description indicating the closest accessible neighbors and finally the persons and / or objects that are present in it, since you are in room _ 12to the north, there is room _ 7to the west there is room _ 11to the south is there is the room _ 17John is herewhat do the commands the comms."}, {"heading": "4 Results", "text": "This year, it is only a matter of time before an agreement is reached."}, {"heading": "5 Evaluation of the system components and free-parameters", "text": "In this section, we discuss the relative importance of each component to this decision-making process, and the effect of their complete removal from the system.Table 6 shows how system performance is affected by the removal of connections from the STM components to the central executive, and how it is affected by the complete removal of these components from the system.The percentage of correct responses to the three sets of data people, parts of the body and the categorization, and over the four rounds of cross-validation. The contents of the STM components are not independent of each other: there is a redundancy in the information they provide."}, {"heading": "6 Generalization", "text": "The global organization of the ANNABELL model is compatible with current knowledge from neurological and psychological observations. Our study focuses on the age range of children between about 3 and 5 years of age, which is a critical area for the acquisition of linguistic competences and is therefore considered particularly interesting for language development studies. Sentences in the databases described in Section 3 were selected according to the purpose of this work, based on the early language assessment literature [39-43]. Therefore, the grammatical structure of the records described in Section 3 is relatively easy to assess. However, it is important to evaluate the generalization capabilities of the system on a larger data set and on more complex grammatical constructions."}, {"heading": "6.1 Generalization 1", "text": "For this experiment, we built an expanded database by replacing the open class words of the three data sets persons, body parts and categorization with new randomly generated words. The declarative records of this database were created using the constructions shown in Table 1 for the data set persons, the constructions of Table 2 for the data set parts of the body, and the records described in Section 3.3 for the data set of categorization; the open class words within the brackets of these constructions were replaced by randomly generated words, i.e. random alphabetic strings. The open words in the question sets used for the test were modified accordingly. By iterating this procedure, we created a database of 5352 declarative records and 4028 question sets. This database was used as an independent test set for testing the four instances of the system trained on the original database during the four rounds of cross-validation."}, {"heading": "6.2 Generalization 2", "text": "In fact, it is the case that most people who are able are able to determine for themselves what they want and what they do not want."}, {"heading": "7 Discussion", "text": "This year, it has reached the stage where it will be able to take the lead."}, {"heading": "8 Conclusion", "text": "The results of the validation show that, compared to previous cognitive neural models of speech cognition, the ANNABELL model is able to develop a much broader range of functionalities, starting from a tabula rasa state, on a relatively large database. ANNABELL system is the first large-scale neural architecture that can learn dialogue with natural language through an open incremental learning process. Cross-validation and generalization experiments show the robustness of the system. ANNABELL system compiles verbal information through sequences of mental operations consistent with psychological insights. These results support the hypothesis that the central executive plays a fundamental role in the elaboration of verbal information in the brain. Our work emphasizes that the decision-making processes operated by the central executive are not based on precoded rules. On the contrary, they are statistical decision-making processes that are learned through exploration and reward mechanisms."}, {"heading": "Appendix A Examples from the learning sessions", "text": "In fact, it is the case that you will be able to go to the top without having to go to the top."}, {"heading": "Appendix B Example from CHILDES database", "text": "From the database of CHILDES, Warren-Leubecker corpus, file david.cha.B.1 Comparison between ANNABELL system (left) and real child (right) ANNABELL system Participants: - SYS, Ann, ANNABELL system; - TEA, Teacher. * TEA: Ann, why do not you tell, tell Dad about you playing Pac _ Man. * SYS: Michael brought me to the bumpers junction. * SYS: I have you won? * TEA: did you won? * SYS: yes, I won't tell. \"TEA: I won't play other games."}, {"heading": "Appendix C Neural activation patterns in a concrete example", "text": "The idea behind this is that the people who stay in the city, put themselves in a different world, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they, in which they, in which they live, in which they, in which they, in which they live, in which they, in which they, in which they live, in which they, in which they, in which they, in which they, in which they live, in which they live, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which, in which they, in which they, in which they, in which, in which they, in which they, in which, in which they, in which they, in which, in which they, in which, in which they, in which, in which they, in which, in which they, in which they, in which, live, in which they, in which, in which, they, in which, in which, in which, they, in which, they, in which, in which, in which, they, in which, they, in which, in which, they, in which, in which, in which, in which, they, in which, in which, they, in which, in which, they live, they, in which, they, in which, in which, in which"}, {"heading": "Appendix D Mathematical properties of the state-action association system", "text": "In fact, it is such that most of them will be able to move to another world, in which they will be able to move to another world, in which they will be able to move to another world, in which they will be able to move, in which they will live, in which they will live, in which they will live, in which they will live, in which they will live, in which they will live, in which they will live, in which they will live, in which they will live, in which they will live, in which they will live, in which they will live, in which they will live, in which they will live, in which they will live, in which they will live, in which they will live, in which they will live, in which they will live, in which they will live, in which they will live, in which they will live, in which they will live, in which they will live, in which they will live, in which they will live, in which they will live, in which they will live, in which they will live, in which they will live, in which they will live, in which they will live in which they will live, in which they will live, in which they will live, in which they will live, in which they will live, in which they will live, in which they will live, in which they will live, in which they will live, in which they will live, in which they will live, in which they will live, in which they will live, in which they will live, in which they will live, in which they will live, in which they will live, in which they will live, in which they will live, in which they will live, in which they will live, in which they will live in which they will live, in which they will live, in which they will live, in which they will live, in which they will live, in which they will live, in which they will live, in which they will live, in which they will live, in which they will live, in which they will live, in which they will live, in which they will live, in which they will live, in which they will live, in which they will live, in which they will live, in which will live, in which they will live, in which they will live, in which they will live, in which they will live, in which they will live"}], "references": [{"title": "The Language of Thought", "author": ["J Fodor"], "venue": null, "citeRegEx": "1", "shortCiteRegEx": "1", "year": 1975}, {"title": "LOT2: The Language of Thought Revisited (Oxford University Press, Oxford and New York)", "author": ["J Fodor"], "venue": null, "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2010}, {"title": "Soar: An Architecture for General Intelligence", "author": ["JE Laird", "PS Rosenbloom", "A Newell"], "venue": "Artif Intell", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 1987}, {"title": "An overview of the EPIC architecture for cognition and performance with application to human-computer interaction", "author": ["DE Kieras", "DE Meyer"], "venue": "Hum Comput Int", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 1997}, {"title": "An adaptive architecture for physical agents", "author": ["P Langley"], "venue": "Proceedings of the 2005 IEEE/WIC/ACM International Conference on Intelligent Agent Technology (IEEE Computer Society Press, Compiegne,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2005}, {"title": "Cognitive architectures: where do we go from here", "author": ["W Duch", "RJ Oentaryo", "M Pasquier"], "venue": "Artificial General Intelligence,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2008}, {"title": "Cognitive architectures: Research issues and challenges", "author": ["P Langley"], "venue": "Cognitive Syst Res", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2009}, {"title": "A large-scale model of the functioning", "author": ["C Eliasmith"], "venue": "brain. Science", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2012}, {"title": "Anatomy of a cortical simulator, in Proceedings of the 2007 ACM/IEEE Conference on Supercomputing-SC '07 (Association for Computing", "author": ["R Ananthanarayanan", "DS Modha"], "venue": null, "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2007}, {"title": "Large-scale model of mammalian thalamocortical systems", "author": ["EM Izhikevich", "GM Edelman"], "venue": "Proc Natl Acad Sci USA", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2008}, {"title": "Distributed representations, simple recurrent networks, and grammatical structure", "author": ["JL Elman"], "venue": "Mach Learn", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 1991}, {"title": "Mechanisms of sentence processing: Assigning roles to constituents of sentences. in Parallel Distributed Processing. Explorations in the Microstructure of Cognition, eds", "author": ["JL McClelland", "AH Kawamoto"], "venue": null, "citeRegEx": "12", "shortCiteRegEx": "12", "year": 1986}, {"title": "Subsymbolic Natural Language Processing: An Integrated Model of Scripts, Lexicon, and Memory", "author": ["R Miikkulainen"], "venue": null, "citeRegEx": "13", "shortCiteRegEx": "13", "year": 1993}, {"title": "A Subsymbolic Model of Complex Story Understanding", "author": ["P Fidelman", "R Miikkulainen", "R Hoffman"], "venue": "Proceedings of the 27th Annual Meeting of the Cognitive Science Society", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2005}, {"title": "Context dependent recurrent neural network language", "author": ["T Mikolov", "G Zweig"], "venue": "Proceedings of the IEEE Workshop on Spoken Language Technology,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2012}, {"title": "Parsing natural scenes and natural language with recursive neural networks", "author": ["R Socher", "CC Lin", "AY Ng", "CD Manning"], "venue": "Proceedings of the 26th International Conference on Machine Learning (ICML 2011),", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2011}, {"title": "Parsing with compositional vector grammars", "author": ["R Socher", "J Bauer", "CD Manning"], "venue": "Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2013}, {"title": "Recursive deep models for semantic compositionality over a sentiment treebank", "author": ["R Socher", "A Perelygin", "J Wu", "J Chuang", "CD Manning", "AY Ng", "C. Potts"], "venue": "in Conference on Empirical Methods in Natural Language Processing (EMNLP", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2013}, {"title": "Advances in research and theory (Vol", "author": ["A.D. Baddeley", "Hitch", "G. Working memory. In G.H. Bower (Ed.)", "The psychology of learning", "motivation"], "venue": "8, pp. 47\u201389). New York: Academic Press", "citeRegEx": "20", "shortCiteRegEx": null, "year": 1974}, {"title": "The episodic buffer: a new component of working memory", "author": ["A.D. Baddeley"], "venue": "Trends in Cognitive Science", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2000}, {"title": "Theories, Models, and Controversies", "author": ["Baddeley", "A.D. Working Memory"], "venue": "Annual Review of Psychology 63: 1-29", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2012}, {"title": "an integrated framework", "author": ["Cowan", "N. Attention", "memory"], "venue": "Oxford [Oxfordshire]: Oxford University Press", "citeRegEx": "23", "shortCiteRegEx": null, "year": 1995}, {"title": "Working memory and focal attention", "author": ["B. McElree"], "venue": "Journal of Experimental Psychology: Learning, Memory & Cognition", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2001}, {"title": "exploring the focus of attention", "author": ["Oberauer", "K. Access to information in working memory"], "venue": "Journal of Experimental Psychology. Learning, Memory, and Cognition 28 (3): 411\u201321", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2002}, {"title": "switching or serial order control? Mem", "author": ["RL Bryck", "Mayr", "U. On the role of verbalization during task set selection"], "venue": "Cognit. 33(4), 611-623", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2005}, {"title": "Role of Working Memory in Task Switching", "author": ["A. Vandierendonck"], "venue": "Psychologica Belgica,", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2012}, {"title": "New approaches to understanding the cortical organization of semantic processing", "author": ["Bookheimer", "S. Functional MRI of Language"], "venue": "Annu. Rev. Neurosci. 25, 151\u2013188", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2002}, {"title": "A combined functional magnetic resonance imaging and transcranial magnetic stimulation study", "author": ["Devlin", "J.T. et al. Semantic processing in the left prefrontal cortex"], "venue": "J. Cogn. Neurosci. 15, 71\u201384", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2003}, {"title": "Integration of word meaning and world knowledge in language comprehension", "author": ["P Hagoort"], "venue": "Science 304,", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2004}, {"title": "a new framework", "author": ["Hagoort", "P. On Broca", "brain", "binding"], "venue": "TRENDS in Cognitive Sciences 9(9) 416-423", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2005}, {"title": "Towards a neural basis of auditory sentence processing", "author": ["A. Friederici"], "venue": "TRENDS in Cognitive Sciences", "citeRegEx": "32", "shortCiteRegEx": "32", "year": 2002}, {"title": "Integration of action and language knowledge: A roadmap for developmental robotics", "author": ["A Cangelosi"], "venue": "IEEE Trans Auton Ment Dev", "citeRegEx": "33", "shortCiteRegEx": "33", "year": 2010}, {"title": "Neural Networks: A Comprehensive Foundation (Prentice Hall PTR Upper Saddle River, NJ, USA, 2nd edition)", "author": ["S Haykin"], "venue": null, "citeRegEx": "34", "shortCiteRegEx": "34", "year": 1998}, {"title": "Computational Explorations in Cognitive Neuroscience: Understanding the Mind by Simulating the Brain", "author": ["RC O\u2019Reilly", "Y Munakata"], "venue": null, "citeRegEx": "35", "shortCiteRegEx": "35", "year": 2000}, {"title": "Neural Networks: A Comprehensive Foundation (Prentice Hall PTR Upper Saddle River, NJ, USA, 2nd edition)", "author": ["S Haykin"], "venue": null, "citeRegEx": "37", "shortCiteRegEx": "37", "year": 1998}, {"title": "Mechanisms Gating the Flow of Information in the Cortex: What They Might Look Like and What Their Uses may be, Frontiers in Computational Neuroscience", "author": ["M Boukadoum", "T Gisiger"], "venue": null, "citeRegEx": "38", "shortCiteRegEx": "38", "year": 2011}, {"title": "Validation of the language development survey (LDS): a parent report tool for identifying language delay in toddlers", "author": ["L Rescorla", "A Alley"], "venue": "J Speech Lang Hear Res", "citeRegEx": "39", "shortCiteRegEx": "39", "year": 2001}, {"title": "Use of the Language Development Survey (LDS) in a National probability sample of children 18 to 35 months old", "author": ["L Rescorla", "T Achenbach"], "venue": "J Speech Lang Hear Res", "citeRegEx": "40", "shortCiteRegEx": "40", "year": 2002}, {"title": "Intonation patterns in child-directed speech: Mother-father speech", "author": ["A Warren-Leubecker", "JN Bohannon"], "venue": "Child Dev", "citeRegEx": "42", "shortCiteRegEx": "42", "year": 1984}, {"title": "The CHILDES Project: Tools for analyzing talk (Lawrence", "author": ["B MacWhinney"], "venue": "Erlbaum Associates,", "citeRegEx": "43", "shortCiteRegEx": "43", "year": 2000}, {"title": "Recurrent temporal networks and language acquisition\u2014from corticostriatal neurophysiology to reservoir computing", "author": ["P.F. Dominey"], "venue": "Front Psychol", "citeRegEx": "44", "shortCiteRegEx": "44", "year": 2013}, {"title": "Syntactic determinants of sentence comprehension in aphasia", "author": ["D. Caplan", "C. Baker", "F. Dehaut"], "venue": "Cognition 21,", "citeRegEx": "45", "shortCiteRegEx": "45", "year": 1985}, {"title": "A Recurrent Network Simulation Study Using Reservoir Computing", "author": ["Hinaut X.", "Dominey P.F. Real-Time Parallel Processing of Grammatical Structure in the Fronto-Striatal System"], "venue": "PLoS ONE 8(2): e52946", "citeRegEx": "46", "shortCiteRegEx": null, "year": 2013}, {"title": "Connecting language to the world", "author": ["Roy D", "E. Reiter"], "venue": "Artif. Intell. 167,", "citeRegEx": "47", "shortCiteRegEx": "47", "year": 2005}, {"title": "Posture affects how robots and infants map words to objects PLoS ONE", "author": ["A. Morse", "T. Belpaeme", "L. Smith", "A. Cangelosi"], "venue": null, "citeRegEx": "48", "shortCiteRegEx": "48", "year": 2015}, {"title": "Developmental Robotics: From Babies to Robots <http://mitpress.mit.edu/books/developmental-robotics>", "author": ["A. Cangelosi", "M. Schlesinger"], "venue": null, "citeRegEx": "49", "shortCiteRegEx": "49", "year": 2015}, {"title": "Learning to talk about events from narrated video in a construction grammar framework", "author": ["P. Dominey", "J.D. Boucher"], "venue": "Artif. Intell. 167,", "citeRegEx": "50", "shortCiteRegEx": "50", "year": 2005}, {"title": "Nearest neighbor pattern classification", "author": ["T.M. Cover", "P.E. Hart"], "venue": "IEEE Transactions on Information Theory", "citeRegEx": "51", "shortCiteRegEx": "51", "year": 1967}], "referenceMentions": [{"referenceID": 0, "context": "According to the computational theory of mind, the brain is an information processing system, and thought can be described as a computation that operates on mental states [1,2].", "startOffset": 171, "endOffset": 176}, {"referenceID": 1, "context": "According to the computational theory of mind, the brain is an information processing system, and thought can be described as a computation that operates on mental states [1,2].", "startOffset": 171, "endOffset": 176}, {"referenceID": 2, "context": "This perspective has led to the implementation of a class of cognitive architectures called symbolic [3-5] (see Ref.", "startOffset": 101, "endOffset": 106}, {"referenceID": 3, "context": "This perspective has led to the implementation of a class of cognitive architectures called symbolic [3-5] (see Ref.", "startOffset": 101, "endOffset": 106}, {"referenceID": 4, "context": "This perspective has led to the implementation of a class of cognitive architectures called symbolic [3-5] (see Ref.", "startOffset": 101, "endOffset": 106}, {"referenceID": 5, "context": "s [6] and [7] for a review).", "startOffset": 2, "endOffset": 5}, {"referenceID": 6, "context": "s [6] and [7] for a review).", "startOffset": 10, "endOffset": 13}, {"referenceID": 5, "context": "However, up to now they have never been implemented in large scale simulations for tasks that require complex reasoning [6].", "startOffset": 120, "endOffset": 123}, {"referenceID": 7, "context": "5-million neuron model of the brain, able to elaborate visual image sequences and to respond through movements of a physically modeled arm [8].", "startOffset": 139, "endOffset": 142}, {"referenceID": 8, "context": "Other large-scale neural simulations have been reported [9,10], however they focus on biological realism of the neuron model, while none of them deal with the problem of natural language elaboration.", "startOffset": 56, "endOffset": 62}, {"referenceID": 9, "context": "Other large-scale neural simulations have been reported [9,10], however they focus on biological realism of the neuron model, while none of them deal with the problem of natural language elaboration.", "startOffset": 56, "endOffset": 62}, {"referenceID": 10, "context": "On the other hand, the subsymbolic approach demonstrated to be more suitable for modeling the cognitive foundations of language processing and for representing statistical regularities in natural language [11-13].", "startOffset": 205, "endOffset": 212}, {"referenceID": 11, "context": "On the other hand, the subsymbolic approach demonstrated to be more suitable for modeling the cognitive foundations of language processing and for representing statistical regularities in natural language [11-13].", "startOffset": 205, "endOffset": 212}, {"referenceID": 12, "context": "On the other hand, the subsymbolic approach demonstrated to be more suitable for modeling the cognitive foundations of language processing and for representing statistical regularities in natural language [11-13].", "startOffset": 205, "endOffset": 212}, {"referenceID": 13, "context": "presented a neural network architecture able to parse and paraphrase scriptbased stories [14].", "startOffset": 89, "endOffset": 93}, {"referenceID": 14, "context": "Recently, deep learning techniques based on recurrent neural networks (RNNs) have been used successfully for several NLP tasks, including speech recognition [15], parsing [16,17], machine translation [18], sentiment analysis of text [19].", "startOffset": 157, "endOffset": 161}, {"referenceID": 15, "context": "Recently, deep learning techniques based on recurrent neural networks (RNNs) have been used successfully for several NLP tasks, including speech recognition [15], parsing [16,17], machine translation [18], sentiment analysis of text [19].", "startOffset": 171, "endOffset": 178}, {"referenceID": 16, "context": "Recently, deep learning techniques based on recurrent neural networks (RNNs) have been used successfully for several NLP tasks, including speech recognition [15], parsing [16,17], machine translation [18], sentiment analysis of text [19].", "startOffset": 171, "endOffset": 178}, {"referenceID": 17, "context": "Recently, deep learning techniques based on recurrent neural networks (RNNs) have been used successfully for several NLP tasks, including speech recognition [15], parsing [16,17], machine translation [18], sentiment analysis of text [19].", "startOffset": 233, "endOffset": 237}, {"referenceID": 18, "context": "In 1974, Baddeley and Hitch [20] proposed a working memory model composed of three main components: a central executive and two slave systems, i.", "startOffset": 28, "endOffset": 32}, {"referenceID": 19, "context": "In 2000, Baddeley [21] extended this model by adding a third slave system, the episodic buffer, which binds information from different domains (phonological, visual, spatial, semantic) to form integrated units of information with chronological ordering.", "startOffset": 18, "endOffset": 22}, {"referenceID": 20, "context": "[22] for a review).", "startOffset": 0, "endOffset": 4}, {"referenceID": 21, "context": "Cowan [23] proposed a working memory model in which the LTM was not a separate component, but a part of the working memory.", "startOffset": 6, "endOffset": 10}, {"referenceID": 22, "context": "McElree [24] suggested a focus of attention limited to a single chunk.", "startOffset": 8, "endOffset": 12}, {"referenceID": 23, "context": "Oberauer [25] proposes an integration of Cowan's and McElree's perspectives into a model that distinguishes three states of representations in WM: the activated part of LTM, the region of direct access and the focus of attention.", "startOffset": 9, "endOffset": 13}, {"referenceID": 23, "context": "In classical tasks used to study working memory capacity [25], a subject is asked to hold in mind a short sequence of digits and to perform some simple process on each of these digits (or on a subset), for example adding the number two to each digit.", "startOffset": 57, "endOffset": 61}, {"referenceID": 24, "context": "Additionally, several studies [26,27] suggest that the task goal should be stored in the working memory in some directly accessible form.", "startOffset": 30, "endOffset": 37}, {"referenceID": 25, "context": "Additionally, several studies [26,27] suggest that the task goal should be stored in the working memory in some directly accessible form.", "startOffset": 30, "endOffset": 37}, {"referenceID": 26, "context": "BA 47 and the ventral part of BA 6 are also involved in language processing tasks [28-30].", "startOffset": 82, "endOffset": 89}, {"referenceID": 27, "context": "BA 47 and the ventral part of BA 6 are also involved in language processing tasks [28-30].", "startOffset": 82, "endOffset": 89}, {"referenceID": 28, "context": "BA 47 and the ventral part of BA 6 are also involved in language processing tasks [28-30].", "startOffset": 82, "endOffset": 89}, {"referenceID": 29, "context": "Results from neuroimaging and psycholinguistic studies show that LIFG is involved in the unification operations required for binding individual words into larger structures [31,32].", "startOffset": 173, "endOffset": 180}, {"referenceID": 30, "context": "Results from neuroimaging and psycholinguistic studies show that LIFG is involved in the unification operations required for binding individual words into larger structures [31,32].", "startOffset": 173, "endOffset": 180}, {"referenceID": 29, "context": "Hagoort [31] proposes a model that distinguishes three functional components of language processing: memory, unification and control.", "startOffset": 8, "endOffset": 12}, {"referenceID": 7, "context": "[8], although our model privileges computational efficiency over biological details.", "startOffset": 0, "endOffset": 3}, {"referenceID": 31, "context": "The ability to perform real time communication and the large scale of the network make our system adequate for sustaining a long developmental process (this property is called open-ended, cumulative learning in developmental robotics [33] ).", "startOffset": 234, "endOffset": 238}, {"referenceID": 32, "context": "The neuron output is computed from the total input by a nonlinear activation function [34]:", "startOffset": 86, "endOffset": 90}, {"referenceID": 32, "context": "the Heaviside step function for the neurons that receive their input from fixed-weight connections, and the logistic function [34] for the neurons that receive it from variable-weight connections.", "startOffset": 126, "endOffset": 130}, {"referenceID": 33, "context": "This rule provides a computationally effective approximation of the activation dynamics produced by inhibitory interneurons [35].", "startOffset": 124, "endOffset": 128}, {"referenceID": 34, "context": "The Hebbian theory provides a theoretical basis for the learning mechanisms in biological neural networks [36,37].", "startOffset": 106, "endOffset": 113}, {"referenceID": 35, "context": "Neural gating mechanisms play an important role in the cortex and in other regions of the brain [38].", "startOffset": 96, "endOffset": 100}, {"referenceID": 35, "context": "[38]).", "startOffset": 0, "endOffset": 4}, {"referenceID": 18, "context": "The global organization of our model combines Baddeley's model of the working memory [20-22] with ideas from Cowan's [23] and from Oberauer's [25] models.", "startOffset": 85, "endOffset": 92}, {"referenceID": 19, "context": "The global organization of our model combines Baddeley's model of the working memory [20-22] with ideas from Cowan's [23] and from Oberauer's [25] models.", "startOffset": 85, "endOffset": 92}, {"referenceID": 20, "context": "The global organization of our model combines Baddeley's model of the working memory [20-22] with ideas from Cowan's [23] and from Oberauer's [25] models.", "startOffset": 85, "endOffset": 92}, {"referenceID": 21, "context": "The global organization of our model combines Baddeley's model of the working memory [20-22] with ideas from Cowan's [23] and from Oberauer's [25] models.", "startOffset": 117, "endOffset": 121}, {"referenceID": 23, "context": "The global organization of our model combines Baddeley's model of the working memory [20-22] with ideas from Cowan's [23] and from Oberauer's [25] models.", "startOffset": 142, "endOffset": 146}, {"referenceID": 36, "context": "[39,40].", "startOffset": 0, "endOffset": 7}, {"referenceID": 37, "context": "[39,40].", "startOffset": 0, "endOffset": 7}, {"referenceID": 36, "context": "The questions used in the people dataset are also inspired by the work of Rescorla [39,40], and they are appropriate for a preschool child, as in the following examples: what does your father do? what games do you like? do you have a sister? is Dad older than Mum? etc.", "startOffset": 83, "endOffset": 90}, {"referenceID": 37, "context": "The questions used in the people dataset are also inspired by the work of Rescorla [39,40], and they are appropriate for a preschool child, as in the following examples: what does your father do? what games do you like? do you have a sister? is Dad older than Mum? etc.", "startOffset": 83, "endOffset": 90}, {"referenceID": 38, "context": "The fourth session is devoted to communicative interactions, and it is based on a mother/child dialogue extracted from the Warren-Leubecker corpus [41,42], which is part of the CHILDES database [43].", "startOffset": 147, "endOffset": 154}, {"referenceID": 39, "context": "The fourth session is devoted to communicative interactions, and it is based on a mother/child dialogue extracted from the Warren-Leubecker corpus [41,42], which is part of the CHILDES database [43].", "startOffset": 194, "endOffset": 198}, {"referenceID": 39, "context": "68 for our system (this analysis was performed using the CLAN program [43] ).", "startOffset": 70, "endOffset": 74}, {"referenceID": 36, "context": "3 have been chosen according to the purpose of this work, based on the literature on early language assessment [39-43].", "startOffset": 111, "endOffset": 118}, {"referenceID": 37, "context": "3 have been chosen according to the purpose of this work, based on the literature on early language assessment [39-43].", "startOffset": 111, "endOffset": 118}, {"referenceID": 38, "context": "3 have been chosen according to the purpose of this work, based on the literature on early language assessment [39-43].", "startOffset": 111, "endOffset": 118}, {"referenceID": 39, "context": "3 have been chosen according to the purpose of this work, based on the literature on early language assessment [39-43].", "startOffset": 111, "endOffset": 118}, {"referenceID": 40, "context": "We can distinguish two types of generalization [44]: 1) handling learned grammatical constructions with new open-class words; 2) compositional generalization, i.", "startOffset": 47, "endOffset": 51}, {"referenceID": 41, "context": "[45].", "startOffset": 0, "endOffset": 4}, {"referenceID": 42, "context": "For this experiment we used a dataset of 462 distinct grammatical constructions developed by Hinaut and Dominey [46], who used a context free grammar to generate systematically distinct grammatical constructions, each consisting of between 1 and 6 nouns, with 1 to 2 levels of hierarchical structure (i.", "startOffset": 112, "endOffset": 116}, {"referenceID": 42, "context": "[46], the compositional generalization capacity of our model was tested in a ten-fold cross-validation.", "startOffset": 0, "endOffset": 4}, {"referenceID": 43, "context": "Many researchers argued that a true understanding can not be achieved if language is not grounded in the agent's physical environment through actions and perceptions [47].", "startOffset": 166, "endOffset": 170}, {"referenceID": 44, "context": "[48], Cangelosi and Schlesinger [49] highlighted the role of embodiment in early language development.", "startOffset": 0, "endOffset": 4}, {"referenceID": 45, "context": "[48], Cangelosi and Schlesinger [49] highlighted the role of embodiment in early language development.", "startOffset": 32, "endOffset": 36}, {"referenceID": 46, "context": "Boucher [50] argued that we learn to translate the surface form of language into a functional form through the integration of speech inputs and non-speech inputs.", "startOffset": 8, "endOffset": 12}, {"referenceID": 44, "context": "visual input and action capabilities) would lead to the extension of the model for handling the developmental stages in the grounding and acquisition of language [48].", "startOffset": 162, "endOffset": 166}, {"referenceID": 45, "context": "The system could then be used as a language module for artificial systems ready to participate to real children educational settings, as well for interactive robot systems [49].", "startOffset": 172, "endOffset": 176}], "year": 2015, "abstractText": "Communicative interactions involve a kind of procedural knowledge that is used by the human brain for the elaboration of verbal and nonverbal inputs and for language production. Although considerable work has been done on modeling human language abilities, it has been difficult to bring them together to a comprehensive tabula rasa system. This work presents a cognitive system, entirely based on a large-scale neural architecture, which was developed to shed light on how the procedural knowledge involved in language elaboration arises from neural processes. The main component of this system is the central executive, which is a supervising system that coordinates the other components of the working memory. In our model, the central executive is a neural network that takes as input the neural activation states of the short-term memory and yields as output mental actions, which control the flow of information among the working memory components through neural gating mechanisms. The proposed system is capable of learning to communicate through natural language starting from tabula rasa, without any a priori knowledge of the structure of phrases, meaning of words, role of the different classes of words, only by interacting with a human through a text-based interface, using an open-ended incremental learning process. It is able to learn nouns, verbs, adjectives, pronouns and other word classes, to use them in sentences and to generalize this knowledge both in receptive and expressive language. The model was validated on a corpus of 1587 input sentences, based on literature on early language assessment, at the level of about 4-years old child, and produced 521 output sentences, expressing a broad range of functionalities that characterize human communication in language elaboration.", "creator": "Writer"}}}