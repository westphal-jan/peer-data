{"id": "1506.03379", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "10-Jun-2015", "title": "The Online Coupon-Collector Problem and Its Application to Lifelong Reinforcement Learning", "abstract": "Transferring knowledge across a sequence of related tasks is an important challenge in reinforcement learning. Despite much encouraging empirical evidence that shows benefits of transfer, there has been very little theoretical analysis. In this paper, we study a class of lifelong reinforcement-learning problems: the agent solves a sequence of tasks modeled as finite Markov decision processes (MDPs), each of which is from a finite set of MDPs with the same state/action spaces and different transition/reward functions. Inspired by the need for cross-task exploration in lifelong learning, we formulate a novel online discovery problem and give an optimal learning algorithm to solve it. Such results allow us to develop a new lifelong reinforcement-learning algorithm, whose overall sample complexity in a sequence of tasks is much smaller than that of single-task learning, with high probability, even if the sequence of tasks is generated by an adversary. Benefits of the algorithm are demonstrated in a simulated problem.", "histories": [["v1", "Wed, 10 Jun 2015 16:23:29 GMT  (102kb,D)", "http://arxiv.org/abs/1506.03379v1", "17 pages"], ["v2", "Mon, 21 Sep 2015 22:55:59 GMT  (75kb,D)", "http://arxiv.org/abs/1506.03379v2", "13 pages"]], "COMMENTS": "17 pages", "reviews": [], "SUBJECTS": "cs.LG cs.AI", "authors": ["emma brunskill", "lihong li"], "accepted": false, "id": "1506.03379"}, "pdf": {"name": "1506.03379.pdf", "metadata": {"source": "CRF", "title": "The Online Discovery Problem and Its Application to Lifelong Reinforcement Learning", "authors": ["Emma Brunskill", "Lihong Li"], "emails": ["ebrunskill@cs.cmu.edu", "lihongli@microsoft.com"], "sections": [{"heading": "1 Introduction", "text": "In fact, it is such that we are in a position to claim that we are in a position to assert that we are in a position to assert ourselves in the world, and that we are in a position to assert ourselves in the world, that we are in the world, in which we live, in which we live, in which we live, in which we live, in which we live, in the world, in which we live, in which we live, in which we live, in which we live, in which we live, in the world, in which we live, in which we live, in which we live, in which we live, in which we live, in which we live, that we live, that we live, that we live, that we live, that we live, that we live, that we live, that we live, that we live, that we live, that we live, that we live, that we live, that we live, that we live, that we live, that we live, that we live, that we live, that we live, that we live, that we live, that we live, that we live, that we live, that we live, that we live, that we live, that we live, that we live, that we live, that we live, that we live, that we live, that we live, that we live, that we live, that we live, that we live, that we live, that we live, that we live, that we live, that we live, that we live, that we live, that we live, that we live, that we live, that we live, that we live, that we live, that we live, that we live, that we live, that we live, that we live, that we live, that we live, that we live, that we live, that we live, that we live, that we live, that we live, that we live, that we live, that we live, that we live, that we live, that we live, that we live, that we live, that we live, that we live, that we live, that we live, that we live, that we live, that we live, that we live, that we live, that we live, that we live, that we live, that we live, that we live, that we live, that we live, that we live, that we are"}, {"heading": "2 The Online Discovery Problem", "text": "Motivated by the need for cross-task research on Lifelong Directives, in this section we examine a novel online discovery problem that will play a crucial role in the development of new Lifelong Directive Algorithms in Section 3. In addition to the Appli-1Even if there is no identical MDP, MDPs with similar model parameters have similar value functions. Therefore, ultimately, many guidelines are sufficient to provide optimal strategies for all MDPs with common state / joint action, which may be of independent interest to other meta-learning problems where there is a need for efficient research to uncover cross-task relationships."}, {"heading": "2.1 Formulation", "text": "We now describe the online discovery problem (ODP), a sequential game in which the agent decides in each round whether to explore the element in this round. LetM is an unknown set of C elements to be discovered by a learner, and A = {0 (\"exploitation\"), 1 (\"exploration\")} a series of two actions. The learner does not need to know whether the amount of discovered itemsM1 is evaluated online. \u2212 The learner is also given four constants, 0 < 1 \u2264 2 \u2264 3, with the loss of the matrix L in Table 1. The game proceeds as follows: For round t = 1, 2. \u2212 The environment selects an element Mt-M. Without knowing the identity of Mt, the learner selects measures and suffers a loss Lt = L. (At, I {Mt-Mt-Mt}), where L is the loss matrix in Table 1."}, {"heading": "2.2 The Explore-First Algorithm", "text": "In the stochastic case, it can be shown that if an algorithm performs a total number of e-explorations, its expected regret is lowest if these exploration rounds take place at the very beginning. In the multi-armed bandit literature, the resulting strategy is sometimes referred to as E-PLORE-FIRST or EXPFIRST for short, so that all items are detected in M in the first e-round, with a probability of 1 \u2212 \u03b4. After that, it is safe to always exploit them (At-0). The expected total loss can be set as E [L (EXPFIRST, T) upper limit, so that all items are detected in M in the first e-rounds, with a probability of 1 \u2212 \u03b4."}, {"heading": "2.3 Forced Exploration", "text": "While EXPFIRST is effective in stochastic ODPs, in many applications the distribution of task generation is non-stationary (e.g., different types of users can use the Internet at different times of day) or even counterproductive (e.g., an attacker may present certain MDPs in earlier rounds in lifelong directives to cause an algorithm to perform poorly in the future).We will now examine a simple but more general algorithm, FORCEDEXP (for forced exploration), and provide an upper limit for his regret. In the next subsection, we will present a matching lower limit indicating the optimality of this algorithm. \u2212 Before the game begins, the algorithm determines a fixed schedule for exploration. Specifically, it decides a sequence of \"exploration rates\": Definition 1, Definition 2,."}, {"heading": "2.4 Lower Bound", "text": "The main result of this section, Theorem 4, shows that the idea of regret expressed in Section B.5 for FORCEDEXP is essentially incapable of improvement, in the sense of T dependence, even in the stochastic case. The idea of regret is to construct a hard instance of stochastic ODP. On the one hand, regret is suffered if not all C items are detected in M. On the other hand, most items have a low probability of being scanned, requiring the learner to perform the exploratory action A = 1 many times in order to detect all C items. On the other hand, the lower limit follows from a reasonable value of \u00b5m. Theorem 4. There is an online discovery problem where any permissible algorithm suffers an expected regret. Although the lower limit corresponds to the upper limits in relation to T, we have not tried to assign dependence to other variables such as C, which are often less important than T. This limit may be lower than the expected regret."}, {"heading": "3 PAC-MDP Lifelong Reinforcement Learning", "text": "Building on the results of the Excessive Deficit Procedure in Section 2, we now turn to lifelong learning."}, {"heading": "3.1 Preliminaries", "text": "We consider reinforcement learning [24] in discrete time, finite MDPs specified by a quintuple: < S, A, P, R, \u03b3 >, where S is the series of states, A the series of actions, P the transition probability function, R: S > A \u2192 [0, 1] the reward function, and \u03b3 (0, 1) the discount factor. Denote by S and A the number of states and actions, respectively. A policy \u03c0: S \u2192 A determines which action to take in a given state. Its state and state action value functions are denoted by V \u03c0 (s) and Q\u03c0 (s, a). The optimal value of the functions for an optimal policy \u03c0 and Q \u0445, so that V \u0445 (s) = max\u03c0 (s) and Q \u0432 (s, a) denotes."}, {"heading": "3.2 Balancing Cross-task Exploration/Exploitation in Lifelong RL", "text": "In fact, it is not that one is able to find a solution that is able to trump oneself. (...) It is not that one is able to find a solution. (...) It is also not that one is able to find a solution. (...) It is not that one is able to find a solution. (...) It is not that one is able to find a solution. (...) It is not that one is able to find a solution. (...) It is not that one is able to find a solution. (...) It is not that one is able to find a solution. (...) It is not that one is able to find a solution. (...) It is not that one is able to find a solution. (...)"}, {"heading": "3.3 Sample Complexity Analysis", "text": "These are the following assumptions: 2Or has an almost identical MDP model, which leads to an optimal approach in all MDPs."}, {"heading": "4 Experiments", "text": "In this context, it should be noted that this project is a project which is, first and foremost, a project."}, {"heading": "5 Conclusions", "text": "In this paper, we look at a class of lifelong amplification problems that cover a wide range of interesting applications. Our work emphasizes the need for effective cross-task exploration that is unique in lifelong learning, leading to a novel online discovery problem for which we provide optimal algorithms with matching upper and lower limits of regret. Using this technical tool, we developed a new lifelong RL algorithm and analyzed its overall sample complexity across a sequence of tasks. Our theory quantifies how much gain lifelong learning achieves compared to singlet task learning, even when the tasks are generated contradictorily. The algorithm was empirically evaluated in a simulated problem and demonstrates its relative strengths compared to previous work."}, {"heading": "A Algorithm Pseudocode", "text": "In the following you define Rmax: = 1.Algorithm 1 PAC-EXPLORE Algorithm [8] 0: Input: me (known threshold), D (diameter) 1: Set L \u2190 3D 2: while some (s, a) has not visited least me times do 3: Let s be the current state 4: if all a have been tried me times then 5: Start a new L-step Episode 6: Construct an empirical known-state MDP M-K with the reward of all known (s, a) pairs to 0, all unknowns to Rmax, the transition model of all known (s, a) pairs to the estimated parameters and the unknown to themselves loops7: Create an optimistic L-step policy for M-K 8: From the current state follow the development for L-steps, or until an unknown state is reached 9: else10: Execute a Universe that has been at least 11: end if 12: end whileAlgorithm 2: PLORE-probability for PLP-1 Explore Exploration: 2. Exoration: 1 Universe: 1"}, {"heading": "B Proofs for Section 2", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "B.1 Proof for Proposition 1", "text": "As explained in the text, the expected total loss of EXPFIRST is mostE [L (EXPFIRST, T)] \u2264 \u03c11E * + \u03c12C + \u03c10 (T \u2212 E \u0445) + \u03b4\u03c13T, The optimal strategy has the loss of L \u0445 = \u03c12C + \u03c10 (T \u2212 C). Therefore, the regret of EXPFIRST can be limited to asR (EXPFIRST, T) = E [L (EXPFIRST, T)] \u2212 L \u0445 \u03c11E * + \u03b4\u03c13T + \u03c10 (C \u2212 E \u0445) < \u03c11E \u0445 + \u03b43T = \u03c11 \u00b5m ln C \u03b4 + \u03b23T, (2), taking advantage of the fact that E \u0445 > C. The right side of the last equation is a function of \u04211E *, in the form of f (\u04213E): = a \u2212 b ln \u03b4 + c\u043c, for a =.1\u00b5m lengC, can be found because the convection of \u00b2, the equation of \u04211, \u0421\u00b5S \u00b2 and \u0421\u00b5S \u00b2 (\u04211), \u0421\u00b5S \u00b2 and \u0421\u00b5C (\u04211)."}, {"heading": "B.2 Lemma 8", "text": "Lemma 8. Fix M & M, and let 1 \u2264 t1 < t2 < t2 <. < tm \u2264 T be the rounds for which Mt = M. Then the expected total loss incurred in these rounds is limited as follows: L & M (FORCEDEXP) < (m\u03c10 + \u03c12 \u2212 \u03c13) L & # 8222; L & # 8220; 2 & # 8222; L & # 8222; 1 & # 8220; < i (1 & # 8222;) < (L & # 8222; 2; L & # 8222; 3 & # 8222; (L & # 8222; 222;) < i (1 & # 8222; DEXP \") < i & # 8222; i & # 8222;. Proof. Let's let L & # 8222; M (FORCEDEXP) (FORCEXP) make the expected total loss in the rounds."}, {"heading": "B.3 Proof for Theorem 2", "text": "For each M + M, Lemma 8 gives an upper limit of the loss in rounds t = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 2 \u2212 2 \u2212 2 \u2212 2 \u2212 1 + (1 \u2212 2 \u2212 1) 1: 1 + 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 ="}, {"heading": "B.4 Proof for Corollary 3", "text": "The optimal threshold for reducing the upper right side is \u03b7 = O (270 \u00b0 C). At constant exploration rates, the regret results directly from Theorem 2nd sentence 3: Czip3 + Pacific \u2212 Z 1 = Z 3 + Z 1T. The optimal threshold to minimize the upper right side is \u03b7 = Z 2 (270 \u00b0 C). At constant exploration rates, the regret results directly from Theorem 2nd sentence 2: Z 3 + Z 1 = Z 3 + Z 1T. The optimal threshold to minimize the upper right side is \u03b7 = Z 3). The optimal threshold for constant exploration rates results from Theorem 2nd sentence 2: Z 3 + Z \u2212 Z 2 = Z 2."}, {"heading": "B.5 Proof for Theorem 4", "text": "We construct a stochastic online discovery problem with M = {1, 2,., C} and the distribution \u00b5 in such a way that \u00b5 (M) = {3, if M < C 1 \u2212 3, if M = C, where \u00b5m = 1 / 4, T 1. For each M < T2 < < tE \u2264 T is the first time M is discovered; that is TM = min {t | Mt = M, At = 1}. Furthermore, we let 1 \u2264 t1 < t2 < \u00b7 < tE \u2264 T be the rounds in which the exploration takes place (i.e. At = 1); use E to denote the quantity {t1, t2,., tE}. Since the two random variables Mt and At are independent, due to Ht, we have for each i \u04321, 2,.,., E} and each other \u00b5M \u00b2."}, {"heading": "22 25242321", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "C Experiment Details", "text": "All four MDPs had the same 25-cell square grid layout and 4 actions (top, bottom, left, right) as in Figure 2. Actions are successful in the desired direction with a probability of 0.85 and a probability of 0.05 and go in the other directions (unless they are stopped by a wall).For all actions, corner states s5, s20 and s25 are in the same state with a probability of 0.95 or a transition back to the initial state (for all actions).The initial state is in the center of the square grid (s13).The dynamics of all MDPs are identical.All rewards are sampled from binomical distributions.All rewards have a parameter of 0.0 unless otherwise specified. In MDP 1, the corner state s20 has a reward parameter of 0.75. In MDP 2, the corner state s5 has a reward parameter of 0.75. In MDP 3, the corner state has a reward parameter of 0.25."}, {"heading": "D Proofs for Section 3", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "D.1 Proof of Lemma 6", "text": "Proof. The proof closely follows that of [8]. Consider the beginning of an episode and let K be the amount of known state pairs visited by the agent at least once. For each (s, a) k, the \"1-distance between the empirical estimate and the actual distribution of the next state is at most [11] ([Lemma 8.5.5]): \u03b1 = \u221a 8N me log 2N\u03b4. Letter MK is the known state protocol identical to M \u00b2 K, except that the transition probabilities of known state pairs are replaced by the true ones. Following the same reasoning as [8], the probability of reaching an unknown state within the episode can be reduced by pe \u2265 1 / 6 \u2212 3\u03b1D. Therefore, pe is limited by 1 / 12 as long as it is guaranteed to D \u2264 1 / 36. The latter when I arrive \u2265 m0 = O (ND2 log N\u03b4)."}, {"heading": "D.2 Proof of Lemma 7", "text": "Proof. For Task Mt, let Et be the event that all the pairs of cites of action become known after H-steps. (Lemma 6 with a conjunction connection shows all events {Et} t \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7"}, {"heading": "D.3 Proof of Theorem 5", "text": "Evidence. We consider every possible case in solving the tth task, Mt. As shown in previous problems, with probability 1 \u2212 \u03b4, the following event hold Et for all t [T]: after PAC-EXPLORE is executed on Mt, algorithm 2 will correctly detect the identity of Mt. That is, if Mt is a new MDP, it will be added to M \u00b2; otherwise, M \u00b2 will remain unchanged. In the following, we assume that Et is held for each t, and consider the following cases: (a) exploitation in discovered tasks: we choose to exploit (line 12 in Alg 2) and Mt has already been discovered. In this case, finite model RL is used to do model elimination (within M \u00b2) and to transfer samples from previous tasks that correspond to the same MDP as the current task Mt. (line 12 in Alg 2) and Mt has already been discovered."}], "references": [], "referenceMentions": [], "year": 2017, "abstractText": "Transferring knowledge across a sequence of related tasks is an important challenge in reinforce-<lb>ment learning. Despite much encouraging empirical evidence that shows benefits of transfer, there<lb>has been very little theoretical analysis. In this paper, we study a class of lifelong reinforcement-<lb>learning problems: the agent solves a sequence of tasks modeled as finite Markov decision processes<lb>(MDPs), each of which is from a finite set of MDPs with the same state/action spaces and different<lb>transition/reward functions. Inspired by the need for cross-task exploration in lifelong learning, we<lb>formulate a novel online discovery problem and give an optimal learning algorithm to solve it. Such<lb>results allow us to develop a new lifelong reinforcement-learning algorithm, whose overall sample<lb>complexity in a sequence of tasks is much smaller than that of single-task learning, with high prob-<lb>ability, even if the sequence of tasks is generated by an adversary. Benefits of the algorithm are<lb>demonstrated in a simulated problem.", "creator": "LaTeX with hyperref package"}}}