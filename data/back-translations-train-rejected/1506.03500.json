{"id": "1506.03500", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "10-Jun-2015", "title": "Unveiling the Dreams of Word Embeddings: Towards Language-Driven Image Generation", "abstract": "We introduce language-driven image generation, the task of generating an image visualizing the semantic contents of a word embedding, e.g., given the word embedding of grasshopper, we generate a natural image of a grasshopper. We implement a simple method based on two mapping functions. The first takes as input a word embedding (as produced, e.g., by the word2vec toolkit) and maps it onto a high-level visual space (e.g., the space defined by one of the top layers of a Convolutional Neural Network). The second function maps this abstract visual representation to pixel space, in order to generate the target image. Several user studies suggest that the current system produces images that capture general visual properties of the concepts encoded in the word embedding, such as color or typical environment, and are sufficient to discriminate between general categories of objects.", "histories": [["v1", "Wed, 10 Jun 2015 22:57:20 GMT  (2248kb,D)", "http://arxiv.org/abs/1506.03500v1", null], ["v2", "Mon, 23 Nov 2015 16:36:48 GMT  (2248kb,D)", "http://arxiv.org/abs/1506.03500v2", "A 6-page version to appear at the Multimodal Machine Learning NIPS 2015 Workshop"]], "reviews": [], "SUBJECTS": "cs.CV cs.CL", "authors": ["angeliki lazaridou", "dat tien nguyen", "raffaella bernardi", "marco baroni"], "accepted": false, "id": "1506.03500"}, "pdf": {"name": "1506.03500.pdf", "metadata": {"source": "CRF", "title": "Unveiling the Dreams of Word Embeddings: Towards Language-Driven Image Generation", "authors": ["Angeliki Lazaridou", "Dat Tien Nguyen", "Raffaella Bernardi", "Marco Baroni"], "emails": ["firstname.lastname@unitn.it"], "sections": [{"heading": "1 Introduction", "text": "In fact, most people are able to determine for themselves what they want and what they want. Most people in the world have no idea what they want and what they want."}, {"heading": "2 Language-driven image generation", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "2.1 From word to visual vectors", "text": "So far, feature inversion algorithms [10, 22, 24] have been applied to visual representations extracted directly from images (hence the name \"inversion\"). Instead, we aim to create an image that conveys the semantics of a concept encoded in a word representation. Cross-modal mapping was first introduced in the context of zero-shot learning to address the bottleneck of manual annotation in areas where other vector-based representations (e.g. images or brain signals) need to be associated with word names [13, 19]. This is achieved by using training data to learn a mapping function from vector resources in the area of interest in vector resources."}, {"heading": "2.2 From visual vectors to images", "text": "Considering that these models consist of millions of parameters, there is ongoing research on the inversion of different CNN layers to achieve an intuitive visualization of what each of them has learned. Several methods have been proposed to reverse the visual characteristics of CNN, but the exact nature of the task imposes certain limitations on the inversion method. For example, the original work of Zeiler and Fergus [24] cannot easily be adapted to our task of generating images from word embedding, as their DeConvNet method requires information related to the activation of the network in multiple layers."}, {"heading": "3 Experimental Setup", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "3.1 Materials", "text": "The real image contains about 80,000 images that we have seen. The dreamed word comes from the concepts used by McRae et al. [11], in the context of Property1Originally, the HOGGles method for visualizing HOG functions was introduced, but the method does not make specific assumptions, and it has also recently been used to invert the properties of CNN [23]. This sentence contains 541 basic concrete concepts (e.g. cats, apple, car, etc.) spanning 20 general and broad categories (e.g. animals, fruits / vegetables, vehicles, etc.) For the purposes of the current experiments, 69 McRae concepts were excluded (either due to high ambiguity or technical reasons), which we test in 472 dreamed words that we use real images."}, {"heading": "3.2 Model selection and parameter estimation", "text": "In order to determine the optimal visual characteristic type (between Pool-5 and fc-7) and the concept representation method (between prototype and specimen), we conducted a human study by CrowdFlower.5 For 50 randomly selected test concepts, we generate 4 images, each obtained by inverting the visual vector, which is calculated by combining a characteristic type with a concept representation method, e.g. for Pool-5 + prototype, we generate an image by inverting the visual vector resulting from averaging the Pool-5 feature vectors labeled with the test concept (details of our implementation of the feature version below).Participants are then asked to assess which of the 4 images is more likely to think the test concept.For each test concept, we collect 20 judgments. Overall, the participants showed a strong significant preference for the images generated from the inversion of the pool-5 characteristics, in particular those generated for 28 / 50 and those generated from the pool-5 characteristics."}, {"heading": "4 Experiments", "text": "Figure 3 is a snapshot of our results; we randomly selected ten dreamed-up concepts from each of the 20 McRae categories and show the image we generated for them from the corresponding word embedding, as in Section 2. We reiterate that the images of the dreamed concepts have never been used at any stage of the pipeline, either to train cross-modal mapping, or to train the inversion of features so that they are really generated in a zero-shot manner, using their linguistic associations with seen concepts. Unsurprisingly, the images we produce are not as clear as those obtained by retrieving existing images. However, we see in the image that concepts belonging to different categories, with the exception of food and fruit / vegetables (columns 12 and 13), which look very similar (on the other hand, fruits and vegetables are also food, and word vectors extracted from companies)."}, {"heading": "4.1 Experiment 1: Correct word vs. random confounder", "text": "The first experiment is a health test that assesses whether the visual properties in the generated images are meaningful enough to let the subjects guess the correct label using a random alternative. As the subjects are a randomly selected item, the task is relatively simple. We test the 472 dreamed concepts and concepts on a concrete, basic level, so sometimes they are only randomly related to each other. In addition, the subjects were used to train the mapping and inversion functions that could have initiated a systematic bias in their favor. We test the 472 dreamed concepts and collect 20 ratings for each on CrowdFlower. The sequence of words is randomized both across and within studies (the same setup is used in the following experiments, with equally randomized sequence of images)."}, {"heading": "4.2 Experiment 2: Correct image vs. image of similar concept", "text": "The second experiment determines the extent to which the subjects can select the dreamed image via a closely related alternative. (In 379 / 472 cases, the founder belongs to the category of dreamed concepts; therefore, distinguishing the two concepts is a major challenge (e.g. Mandarin versus the pumpkin). Participants were confronted with the images from the dreamed concept and asked which of the two images is more likely to identify the dreamed concepts."}, {"heading": "4.3 Experiment 3: Judging macro-categories of objects", "text": "Previous experiments have shown that our language-driven imaging system visualizes characteristics that are distinctive and relevant enough to distinguish non-coherent concepts (Experiment 1) but not closely related (Experiment 2). The last experiment explicitly incorporates high-level category structures into design. Experiment description We group the McRae categories into three macro categories, namely ANIMAL vs. ORGANIC vs. MAN-MADE, which are universally recognized as fundamental and unambiguous in cognitive science [11]. Participants are given a generated image and are asked to select the macro category that best describes the object in it. Results Again, the number of images for which participants have no significant preferences is high: 28% of ORGANIC images, 47% of MAN-MADE images, and 56% of MADRO images."}, {"heading": "5 Discussion", "text": "We have introduced the new task of creating images that visualize the semantic content of linguistic expressions as encoded in word embedding. Interestingly, vector-based word representations are notoriously poor at capturing color [2], and we do not expect them to better describe the typical color of object classes and aspects of their characteristic environment. Therefore, our results suggest that our system, already in its current form, could also be used to enrich word representations by highlighting aspects of concepts that do not particularly stand out in language, but are likely to be learned through similarity generalization from intermodal mapping training. In this sense, language-driven image generation is more than a simple word embedding an evaluation tool. At the same time, our system ignores visual properties related to the form. Forms are not often expressed through linguistic means. In this sense, language-driven image generation is more than a simple word that we typically describe in the two states that we embed. \""}, {"heading": "A Answer Keys to Figure 1", "text": "We supply the word names of the word embeddings used to generate the images of Figure 1 (we provide Figure 1 again in this document to make it easier for readers to create the images).Due to space constraints, we divide the term names into 3 tables, Table 1-3. Each table contains the word names of the word embeddings used to generate the images of MAN-MADE, ORGANIC and ANIMAL.app lian ce cont aine r furn iture / la rgetoolinst rumentgarm ent tool wea pon toy spor ts veh icle buil ding stru ctur e food fruit / veg etablenatu ral objec tplan t bird fish inse ct reptile / a mphibia nmam malMan-made Organic AnimalsA B C D E FH I JG11"}], "references": [{"title": "Don\u2019t count, predict! a systematic comparison of context-counting vs. context-predicting semantic vectors", "author": ["Marco Baroni", "Georgiana Dinu", "Germ\u00e1n Kruszewski"], "venue": "In Proceedings of ACL,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2014}, {"title": "Distributional semantics in Technicolor", "author": ["Elia Bruni", "Gemma Boleda", "Marco Baroni", "Nam Khanh Tran"], "venue": "In Proceedings of ACL,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2012}, {"title": "Imagenet: A large-scale hierarchical image database", "author": ["Jia Deng", "Wei Dong", "Richard Socher", "Lia-Ji Li", "Li Fei-Fei"], "venue": "In Proceedings of CVPR,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2009}, {"title": "DeViSE: A deep visual-semantic embedding model", "author": ["Andrea Frome", "Greg Corrado", "Jon Shlens", "Samy Bengio", "Jeff Dean", "Marc\u2019Aurelio Ranzato", "Tomas Mikolov"], "venue": "In Proceedings of NIPS,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2013}, {"title": "Draw: A recurrent neural network for image generation", "author": ["Karol Gregor", "Ivo Danihelka", "Alex Graves", "Daan Wierstra"], "venue": "arXiv preprint arXiv:1502.04623,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2015}, {"title": "Caffe: Convolutional architecture for fast feature embedding", "author": ["Yangqing Jia", "Evan Shelhamer", "Jeff Donahue", "Sergey Karayev", "Jonathan Long", "Ross Girshick", "Sergio Guadarrama", "Trevor Darrell"], "venue": "arXiv preprint arXiv:1408.5093,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2014}, {"title": "Deep visual-semantic alignments for generating image descriptions", "author": ["Andrej Karpathy", "Li Fei-Fei"], "venue": "In Proceedings of CVPR,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2015}, {"title": "Unifying visual-semantic embeddings with multimodal neural language models", "author": ["Ryan Kiros", "Ruslan Salakhutdinov", "Richard Zemel"], "venue": "In Proceedings of the NIPS Deep Learning and Representation Learning Workshop,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2014}, {"title": "Understanding deep image representations by inverting them", "author": ["Aravindh Mahendran", "Andrea Vedaldi"], "venue": "In Proceedings of CVPR,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2015}, {"title": "Semantic feature production norms for a large set of living and nonliving things", "author": ["Ken McRae", "George Cree", "Mark Seidenberg", "Chris McNorgan"], "venue": "Behavior Research Methods,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2005}, {"title": "Efficient estimation of word representations in vector space", "author": ["Tomas Mikolov", "Kai Chen", "Greg Corrado", "Jeffrey Dean"], "venue": null, "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2013}, {"title": "Predicting human brain activity associated with the meanings of nouns", "author": ["Tom Mitchell", "Svetlana Shinkareva", "Andrew Carlson", "Kai-Min Chang", "Vincente Malave", "Robert Mason", "Marcel Just"], "venue": "Science, 320:1191\u20131195,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2008}, {"title": "The Big Book of Concepts", "author": ["Gregory Murphy"], "venue": null, "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2002}, {"title": "Reconstructing visual experiences from brain activity evoked by natural movies", "author": ["Shinji Nishimoto", "An T Vu", "Thomas Naselaris", "Yuval Benjamini", "Bin Yu", "Jack L Gallant"], "venue": "Current Biology,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2011}, {"title": "Glove: Global vectors for word representation", "author": ["Jeffrey Pennington", "Richard Socher", "Christopher Manning"], "venue": "In Proceedings of EMNLP,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2014}, {"title": "Deep boltzmann machines", "author": ["Ruslan Salakhutdinov", "Geoffrey E Hinton"], "venue": "Proceedings of AISTATS,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2009}, {"title": "Zero-shot learning through cross-modal transfer", "author": ["Richard Socher", "Milind Ganjoo", "Christopher Manning", "Andrew Ng"], "venue": "In Proceedings of NIPS,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2013}, {"title": "Recursive deep models for semantic compositionality over a sentiment treebank", "author": ["Richard Socher", "Alex Perelygin", "Jean Wu", "Jason Chuang", "Christopher Manning", "Andrew Ng", "Christopher Potts"], "venue": "In Proceedings of EMNLP,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2013}, {"title": "Intriguing properties of neural networks", "author": ["Christian Szegedy", "Wojciech Zaremba", "Ilya Sutskever", "Joan Bruna", "Dumitru Erhan", "Ian Goodfellow", "Rob Fergus"], "venue": "arXiv preprint arXiv:1312.6199,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2013}, {"title": "HOGgles: Visualizing Object Detection Features", "author": ["C. Vondrick", "A. Khosla", "T. Malisiewicz", "A. Torralba"], "venue": "Proceedings of ICCV,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2013}, {"title": "Acquiring visual classifiers from human imagination", "author": ["Carl Vondrick", "Hamed Pirsiavash", "Aude Oliva", "Antonio Torralba"], "venue": "arXiv preprint arXiv:1410.4627,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2014}], "referenceMentions": [{"referenceID": 13, "context": "Recent work in neuroscience attempts to generate reconstructions of these mental images, as encoded in vector-based representations of fMRI patterns [15].", "startOffset": 149, "endOffset": 153}, {"referenceID": 17, "context": "However, given recent advances in compositional distributed semantics [20] that produce embeddings for arbitrarily long linguistic units, we also see our contribution as the first step towards generating images depicting the meaning of phrases (e.", "startOffset": 70, "endOffset": 74}, {"referenceID": 6, "context": ", [7, 8]) that introduced effective methods to generate linguistic descriptions of the contents of a given image.", "startOffset": 2, "endOffset": 8}, {"referenceID": 7, "context": ", [7, 8]) that introduced effective methods to generate linguistic descriptions of the contents of a given image.", "startOffset": 2, "endOffset": 8}, {"referenceID": 10, "context": "Tools such as word2vec [12] and Glove [16] have been shown to produce extremely high-quality vectorbased word embeddings.", "startOffset": 23, "endOffset": 27}, {"referenceID": 14, "context": "Tools such as word2vec [12] and Glove [16] have been shown to produce extremely high-quality vectorbased word embeddings.", "startOffset": 38, "endOffset": 42}, {"referenceID": 3, "context": "Consequently, the problem of translating between linguistic and visual representations has been coached in terms of learning a cross-modal mapping function between vector spaces [4, 19].", "startOffset": 178, "endOffset": 185}, {"referenceID": 16, "context": "Consequently, the problem of translating between linguistic and visual representations has been coached in terms of learning a cross-modal mapping function between vector spaces [4, 19].", "startOffset": 178, "endOffset": 185}, {"referenceID": 8, "context": ", from the top layer of a CNN) back onto pixel space, to produce a real image [24, 10].", "startOffset": 78, "endOffset": 86}, {"referenceID": 19, "context": ", onto a representation in the space defined by a CNN layer), and then applies feature inversion to it (using the method HOGgles method of [22]) to generate an actual image (cell A18 in Figure 3).", "startOffset": 139, "endOffset": 143}, {"referenceID": 8, "context": "Up to now, feature inversion algorithms [10, 22, 24] have been applied to visual representations directly extracted from images (hence the \u201cinversion\u201d name).", "startOffset": 40, "endOffset": 52}, {"referenceID": 19, "context": "Up to now, feature inversion algorithms [10, 22, 24] have been applied to visual representations directly extracted from images (hence the \u201cinversion\u201d name).", "startOffset": 40, "endOffset": 52}, {"referenceID": 11, "context": "images or brain signals) must be associated to word labels [13, 19].", "startOffset": 59, "endOffset": 67}, {"referenceID": 16, "context": "images or brain signals) must be associated to word labels [13, 19].", "startOffset": 59, "endOffset": 67}, {"referenceID": 11, "context": "Following previous work [13, 4], we assume that the mapping is linear.", "startOffset": 24, "endOffset": 31}, {"referenceID": 3, "context": "Following previous work [13, 4], we assume that the mapping is linear.", "startOffset": 24, "endOffset": 31}, {"referenceID": 18, "context": "Nevertheless, these models exhibit \u201cintriguing properties\u201d, that are somewhat surprising given their state-of-the-art performance [21], prompting an effort to reach a deeper understanding of how they really work.", "startOffset": 130, "endOffset": 134}, {"referenceID": 19, "context": "[22] that casts the problem of inversion as paired dictionary learning.", "startOffset": 0, "endOffset": 4}, {"referenceID": 9, "context": "[11], in the context of property", "startOffset": 0, "endOffset": 4}, {"referenceID": 19, "context": "Originally, the HOGgles method of [22] was introduced for visualizing HOG features.", "startOffset": 34, "endOffset": 38}, {"referenceID": 20, "context": "However, the method does not make feature-specific assumptions and it has also recently been used to invert CNN features [23].", "startOffset": 121, "endOffset": 125}, {"referenceID": 2, "context": "The real picture set contains approximately 480K images extracted from ImageNet [3] representing 5K distinct concepts.", "startOffset": 80, "endOffset": 83}, {"referenceID": 0, "context": "3 CBOW, which learns to predict a target word from the ones surrounding it, produces state-of-the-art results in many linguistic tasks [1].", "startOffset": 135, "endOffset": 138}, {"referenceID": 5, "context": "Visual Representations The visual representations, for the set of 480K seen concept images, are extracted with the pre-trained CNN model of [9] through the Caffe toolkit [6].", "startOffset": 170, "endOffset": 173}, {"referenceID": 12, "context": "Inspired from categorization schemes in cognitive science [14], we will refer to them as the prototype and exemplar methods.", "startOffset": 58, "endOffset": 62}, {"referenceID": 10, "context": "com/p/word2vec/ Other hyperparameters, adopted without tuning, include a context window size of 5 words to either side of the target, setting the sub-sampling option to 1e-05 and estimating the probability of target words by negative sampling, drawing 10 samples from the noise distribution [12].", "startOffset": 291, "endOffset": 295}, {"referenceID": 19, "context": "Both paired dictionary learning and feature inversion are conducted using the HOGgles software [22] with default hyperparameters.", "startOffset": 95, "endOffset": 99}, {"referenceID": 9, "context": "[11].", "startOffset": 0, "endOffset": 4}, {"referenceID": 9, "context": "MAN-MADE, that are widely recognized in cognitive science as fundamental and unambiguous [11].", "startOffset": 89, "endOffset": 93}, {"referenceID": 1, "context": "Interestingly, vector-based word representations are notoriously bad at capturing color [2], and we do not expect them to be much better at characterizing environments, so our results suggest that, already in its current form, our system could also be used to enrich word representations, by highlighting aspects of concepts that are not salient in language but are probably learned by similarity-based generalization from the cross-modal mapping training examples.", "startOffset": 88, "endOffset": 91}, {"referenceID": 15, "context": "Inspired from recent work in caption generation that conditions word production on visual vectors, we plan to explore an end-to-end model that conditions the generation process on information encoded in the word embeddings of the word/phrase that we wish to produce an image for, building upon classic generative models of image generation [18, 5].", "startOffset": 340, "endOffset": 347}, {"referenceID": 4, "context": "Inspired from recent work in caption generation that conditions word production on visual vectors, we plan to explore an end-to-end model that conditions the generation process on information encoded in the word embeddings of the word/phrase that we wish to produce an image for, building upon classic generative models of image generation [18, 5].", "startOffset": 340, "endOffset": 347}], "year": 2017, "abstractText": "We introduce language-driven image generation, the task of generating an image visualizing the semantic contents of a word embedding, e.g., given the word embedding of grasshopper, we generate a natural image of a grasshopper. We implement a simple method based on two mapping functions. The first takes as input a word embedding (as produced, e.g., by the word2vec toolkit) and maps it onto a high-level visual space (e.g., the space defined by one of the top layers of a Convolutional Neural Network). The second function maps this abstract visual representation to pixel space, in order to generate the target image. Several user studies suggest that the current system produces images that capture general visual properties of the concepts encoded in the word embedding, such as color or typical environment, and are sufficient to discriminate between general categories of objects.", "creator": "LaTeX with hyperref package"}}}