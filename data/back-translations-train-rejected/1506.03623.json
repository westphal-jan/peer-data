{"id": "1506.03623", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "11-Jun-2015", "title": "Max-Entropy Feed-Forward Clustering Neural Network", "abstract": "The outputs of non-linear feed-forward neural network are positive, which could be treated as probability when they are normalized to one. If we take Entropy-Based Principle into consideration, the outputs for each sample could be represented as the distribution of this sample for different clusters. Entropy-Based Principle is the principle with which we could estimate the unknown distribution under some limited conditions. As this paper defines two processes in Feed-Forward Neural Network, our limited condition is the abstracted features of samples which are worked out in the abstraction process. And the final outputs are the probability distribution for different clusters in the clustering process. As Entropy-Based Principle is considered into the feed-forward neural network, a clustering method is born. We have conducted some experiments on six open UCI datasets, comparing with a few baselines and applied purity as the measurement . The results illustrate that our method outperforms all the other baselines that are most popular clustering methods.", "histories": [["v1", "Thu, 11 Jun 2015 11:01:40 GMT  (456kb)", "http://arxiv.org/abs/1506.03623v1", "This paper has been published in ICANN 2015"]], "COMMENTS": "This paper has been published in ICANN 2015", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["han xiao", "xiaoyan zhu"], "accepted": false, "id": "1506.03623"}, "pdf": {"name": "1506.03623.pdf", "metadata": {"source": "CRF", "title": "Max-Entropy Feed-Forward Clustering Neural Network", "authors": ["Han Xiao", "Xiaoyan Zhu"], "emails": ["xiaoh12@mails.tsinghua.edu.cn)", "zxy-dcs@tsinghua.edu.cn)"], "sections": [{"heading": null, "text": "The fact is that we will be able to hold our own, that we will be able to hold our own, that we will be able to put ourselves in the lead."}, {"heading": "II. BACKGROUND AND RELATED WORK", "text": "This year, it is only a matter of time before there is an agreement, until there is an agreement."}, {"heading": "III. TWO PROCESSES IN FEED-FORWARD NEURAL NETWORK", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "A. A Brief Illustration", "text": "In this section, we propose a new position to review feed-forward neural network, which explains the network with two types of processes, the abstraction and clustering process.As the process that Fig. 1 illustrates, the iterative operations of feed-forward neural information processing could be treated as abstraction processes, where the features of sample are transducted from original feature space to abstracted feature space. At the stage of abstraction, each neuron would play a role as a linear regression learner, the new coordinated system is characterized by this linear regression learner with non-linear output function as its coordinates, and these linear regression learners could be seen as the coordinated system for a new feature space. As the abstraction process in Fig. 1 shows, the hidden neurons correspondence to these dashed lines in original space, and the distance of samples to these clusters construct the coordinated system of new feature space."}, {"heading": "B. Abstraction Process in Min-Entropy Viewpoint", "text": "In the abstraction process, each neuron behaves like a linear regression part that captures a portion of the data characteristics. It seems a bit like the density-based methods, the abstracting neuron works as a density detector for data that is diverse. The output of each neuron could be treated as abstracted characteristics or probable distributions. The output of a neuron is smaller if a sample is closer to its corresponding regression hyperplane, which shows that the linear regression receiver functions better. From the point of view of data abstraction, this means that we want to improve the abstracted ability to optimize a target that enables abstraction capability of abstraction I-I-I-I-I-I-I-I-I-I-I-I-I-I-I-I-I-I-I-I-I-I-I-I-I-I-I-I-I-I-I-I-I-I-I-I-I-I-I-I-I-I-I-I-I-I-I-I-I-I-I-I-I-I-I-I-I-I-I-I-I-I-I-I-I-I-I-I-I-I-I-I-I-I-I-I-I-I-I-I-I-I-I-I-I-I-I-I-I-I-I-I-I-I-I-I-I-I-I-I-I-I-I-I-I-I-I-I-I-I-I-I-I-I-I-I-I-I-I-I-I-I-I-I-I-I-I-I-I-I-I-I-I-I-I-I-I-I-I-I-I-I-I-I-I-I-I-I-I-I-I-I-I-I-I-I-I-I-I-I-I-I-I-I-I-I-I-I-I-I-I-I-I-I-I-I-I-I-"}, {"heading": "C. Clustering Process in Max-Entropy Viewpoint", "text": "If the diverse data density could be analyzed by lower layers that play the role of the abstraction process, the output layer could behave like a clustering process, where we need to determine how a sample belongs to the components recognized by the abstraction process. This is a probable estimation problem, where the 1 \u2212 Oi means the degree of the sample approaching the component, such as the degree of the composition of the component sample. Therefore, we apply the max entropy principle to this estimation problem, with a distribution such as (1 \u2212 O1, 1 \u2212 O2, 1 \u2212 O3... 1 \u2212 On). We get the goal as follows: Max J = \u2212 Nc \u00b2 i = 1 (1 \u2212 Oi \u2022 Nc j = 1 \u2212 Oj) log (1 \u2212 Oi \u2022 Nc j = 1 \u2212 Oj) (2) In the formula above, the Nc is the neuron number in the cluster layer, and Oi is the unit output of this layer."}, {"heading": "D. Difficulties in Clustering And Solution in This Paper", "text": "In the case of distance-based cluster models, they have a fixed distance expression that leads to inflexibility of the cluster model. Meanwhile, in the case of probable cluster models, the assumption of the cluster shape or, as we say, the flexibility of the model for these methods could be very important. Therefore, many clusters with different and particular shapes could not be well recognized. In this paper, our model could overcome this difficulty in clustering by abstraction and clustering provided by an upstream neural network. As we know, nonlinear, advanced neural networks could express many types of functions that could be treated as many types of similarity or distance metrics. In this respect, our model offers a very flexible cluster model or a very different cluster form. In the abstraction process of our model, the data is transferred from a kind of manifold expressions."}, {"heading": "IV. ALGORITHMS FOR MAX-ENTROPY FEED-FORWARD CLUSTERING NEURAL NETWORK", "text": "In the overall view, the last layer means that the output layer behaves as a cluster layer, and other layers that are the hidden layers can be treated as abstraction layers < < p = > J = > J = = > J = = > J = < p = 1 - Oj) log (1 - Oi + 1 - Oj + 1 - Oj + 1 - Oj, l) In the formula above, N means the unit number of the output layer, and Ll means the unit number of the hidden layer. L means the number of layers. Oi means i-th neuron output in the output layer, and Oi, l means the I-th neuron output in the hidden layer."}, {"heading": "V. COMPARISON AMONG POPULAR CLUSTERING MODELS", "text": "The link between distance-based methods and our model is relatively obvious; the training algorithm could be treated as a process to achieve a better distance function encoded in the weights of neural networks; then the inference and clustering process would use this learned distance function, which is our neural network, to cluster data. However, the fact is that the distance of our model is trained from the dataset itself, with a flexible form and reasonable training principle, so our model could not be so easily influenced by different forms of cluster or fixed distance expressions; the link between density-based methods and our model is also relatively obvious; our method could be treated as multi-layered or hierarchical density-based methods; a layer-density-based method could extract some cluster information, and this paper applies many layers of density-based methods, while each layer could extract more cluster-based information than the previous one; this process is merely the abstraction process of a neural model that could be treated as an archical process, with this one."}, {"heading": "VI. EXPERIMENTS", "text": "We have conducted experiments on the effectiveness of Feed-Forward Clustering Neural Networks. Each group of experiments has produced good results to prove our methods and theories to be effective."}, {"heading": "A. Experimental Settings", "text": "We have selected six open UCI datasets, which are often used for clustering tasks. Banknote authentication. The data was extracted from images taken from real and counterfeit banknote-like specimens and the Wavelet Transform tool to extract features from pictures. there are 1,372 elements with 5 attributes for binary classes. Investigation of the classification of types of glass was motivated by criminological investigations. At the crime scene, the glass on the left can be used as evidence. We use this dataset to testify to our cluster methodology. There are 214 cases with 10 attributes for 6 classes. Red wine quality and white wine quality. The two datasets are related to red and white variants of Portuguese wine \"Vinho Verde.\" There are 1,599 cases with 11 attributes in the first dataset for 4 classes. And there are 4,899 cases with 11 attributes in the second dataset."}, {"heading": "B. Effectiveness of Our method", "text": "To evaluate the effectiveness of our cluster algorithms, we choose four baselines, all of which are the most famous and popular methods. Baselines and our model are listed as follows: 1) K-Means algorithm, which is the distance-based cluster method, and the number of clusters is specified, we try many settings for these methods and work out the almost best purity results, and this method is implemented by WEKA, noted as K-Means. 2) Density-based algorithm, which is the density estimation method, it can recognize many forms of clusters, but the number of clusters is specified and this method is implemented by WEKA, noted as density-based. We also try many settings for these methods and work out the almost best purity results. 3) Hierarchically based algorithm, which is the hierarchical cluster method, which is the number of clusters that are note-based."}, {"heading": "C. Network Structure Studies", "text": "In this subsection, we examine the effect of the neural network structure. 1) Experimental settings: We choose the classic three UCI datasets, which are glass, white wine and red wine quality, with different hidden node numbers to investigate the structural effect. It is noteworthy that the results are similar on other datasets in the trend aspect. We also apply the purity defined as our measurement parameter above. 2) Results and discussion: The results are shown in Figures 2, 3 and 4, where the value of the X axis is higher, denser is the neural network. From the trend of these numbers, we could conclude and explain the following: Our feeding neural network may be mired in an inappropriate problem and overloaded problem. However, we could use the trend to improve the cluster effect. But, overall, our method would also be robust enough to cluster the uncertain data."}, {"heading": "VII. CONCLUSION", "text": "We treat the feed-forward clustering neural network as two processes. In the abstraction process, the min-entropy principle is applied for more abstract characteristics, and in the clustering process, the max-entropy principle is applied for the distribution estimation of clusters in a variety of data. We compare four types of popular cluster methods with our model and conclude that our model would be more suitable for flexible metric representation and different shapes. Experiments are conducted on six open UCI datasets and four baselines are selected. Results show that our model outperforms other common cluster methods and would avoid suitable hidden node selections to provide better performance."}], "references": [{"title": "Clustering crowds", "author": ["H. Kajino", "Y. Tsuboi", "H. Kashima"], "venue": "AAAI, 2013.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2013}, {"title": "Semantic clustering of morphologically related chinese words", "author": ["C.-L. Lee", "Y.-N. Chang", "C.-L. Liu", "C.-Y. Lee", "J.Y.-j. Hsu"], "venue": "2014.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2014}, {"title": "Discovering better aaai keywords via clustering with community-sourced constraints", "author": ["K.H. Moran", "B.C. Wallace", "C.E. Brodley"], "venue": "Twenty-Eighth AAAI Conference on Artificial Intelligence, 2014.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2014}, {"title": "Survey of clustering algorithms", "author": ["R. Xu", "D. Wunsch"], "venue": "Neural Networks, IEEE Transactions on, vol. 16, no. 3, pp. 645\u2013678, 2005.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2005}, {"title": "A survey of uncertain data algorithms and applications", "author": ["C.C. Aggarwal", "P.S. Yu"], "venue": "Knowledge and Data Engineering, IEEE Transactions on, vol. 21, no. 5, pp. 609\u2013623.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 0}, {"title": "Formalizing hierarchical clustering as integer linear programming", "author": ["S. Gilpin", "S. Nijssen", "I.N. Davidson"], "venue": "AAAI, 2013.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2013}, {"title": "Efficient anytime density-based clustering", "author": ["S.T. Mai", "X. He", "J. Feng", "C. B\u00f6hm"], "venue": "SIAM, 2013.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2013}, {"title": "Bayesian nonparametric multilevel clustering with group-level contexts", "author": ["V. Nguyen", "D. Phung", "X. Nguyen", "S. Venkatesh", "H.H. Bui"], "venue": "arXiv preprint arXiv:1401.1974, 2014.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 1974}, {"title": "Evolutionary soft co-clustering", "author": ["W. Zhang", "S. Ji", "R. Zhang"], "venue": "Proceedings of the 2013 SIAM International Conference on Data Mining, pp. 121\u2013129, SIAM, 2013.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2013}, {"title": "Demystifying information-theoretic clustering", "author": ["G.V. Steeg", "A. Galstyan", "F. Sha", "S. DeDeo"], "venue": "arXiv preprint arXiv:1310.4210, 2013.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2013}, {"title": "Learning deep architectures for ai", "author": ["Y. Bengio"], "venue": "Foundations and trends R  \u00a9 in Machine Learning, vol. 2, no. 1, pp. 1\u2013127, 2009.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2009}, {"title": "A unified architecture for natural language processing: Deep neural networks with multitask learning", "author": ["R. Collobert", "J. Weston"], "venue": "Proceedings of the 25th international conference on Machine learning, pp. 160\u2013167, ACM, 2008.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2008}, {"title": "Multi-column deep neural networks for image classification", "author": ["D. Ciresan", "U. Meier", "J. Schmidhuber"], "venue": "Computer Vision and Pattern Recognition (CVPR), 2012 IEEE Conference on, pp. 3642\u20133649, IEEE, 2012.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2012}, {"title": "Unsupervised feature learning by deep sparse coding", "author": ["Y. He", "K. Kavukcuoglu", "Y. Wang", "A. Szlam", "Y. Qi"], "venue": "arXiv preprint arXiv:1312.5783, 2013.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2013}, {"title": "A fast learning algorithm for deep belief nets", "author": ["G. Hinton", "S. Osindero", "Y.-W. Teh"], "venue": "Neural computation, vol. 18, no. 7, pp. 1527\u20131554, 2006.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2006}, {"title": "Greedy layer-wise training of deep networks", "author": ["Y. Bengio", "P. Lamblin", "D. Popovici", "H. Larochelle"], "venue": "Advances in neural information processing systems, vol. 19, p. 153, 2007.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2007}, {"title": "Understanding the difficulty of training deep feedforward neural networks", "author": ["X. Glorot", "Y. Bengio"], "venue": "International Conference on Artificial Intelligence and Statistics, pp. 249\u2013256, 2010.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2010}, {"title": "Exploring strategies for training deep neural networks", "author": ["H. Larochelle", "Y. Bengio", "J. Louradour", "P. Lamblin"], "venue": "The Journal of Machine Learning Research, vol. 10, pp. 1\u201340, 2009.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2009}], "referenceMentions": [{"referenceID": 0, "context": "For example, in the area of business, clustering machines help to find the users\u2019 group [1], and in the area of biology, they can help to discover genes and species.", "startOffset": 88, "endOffset": 91}, {"referenceID": 1, "context": "In natural language processing, clustering could help to discover the group of Morphologically Related Chinese Words [2].", "startOffset": 117, "endOffset": 120}, {"referenceID": 2, "context": "Even in the task of discovering better conference paper keywords, clustering could be very useful [3].", "startOffset": 98, "endOffset": 101}, {"referenceID": 3, "context": "[4] and [5] had surveyed the algorithms of clustering methods.", "startOffset": 0, "endOffset": 3}, {"referenceID": 4, "context": "[4] and [5] had surveyed the algorithms of clustering methods.", "startOffset": 8, "endOffset": 11}, {"referenceID": 5, "context": "Methods such as Single linkage, Complete linkage, Group average linkage, Median linkage, Centroid linkage, Ward\u2019s method, Integer Linear Programming Hierarchical Clustering [6], BIRCH, CURE and ROCK are also in this kind of algorithms.", "startOffset": 173, "endOffset": 176}, {"referenceID": 6, "context": "The famous DBSCAN method, ADBSCAN [7] and OPTICS are also in this sort of clustering algorithms.", "startOffset": 34, "endOffset": 37}, {"referenceID": 7, "context": "The final kind of methods are based on the probabilistic viewpoint, the famous EM clustering method, mixed Gaussian Distribution, Bayesian non-parametric multilevel clustering [8] and Evolutionary soft co-clustering [9] pertain to this sort.", "startOffset": 176, "endOffset": 179}, {"referenceID": 8, "context": "The final kind of methods are based on the probabilistic viewpoint, the famous EM clustering method, mixed Gaussian Distribution, Bayesian non-parametric multilevel clustering [8] and Evolutionary soft co-clustering [9] pertain to this sort.", "startOffset": 216, "endOffset": 219}, {"referenceID": 9, "context": "Besides, the clustering methods based on information theory also belong to the final sort [10].", "startOffset": 90, "endOffset": 94}, {"referenceID": 10, "context": "Recently such stated in [11], going to deep catches many eyes, since not only just adding the hidden layers could gain an improvement in performance, but deep neural networks can also automatically select features and amazingly complete the comprehension missions.", "startOffset": 24, "endOffset": 28}, {"referenceID": 11, "context": "[12] had applied deep network into natural languages, and many works such as [13] and [14] had applied deep network into image processing.", "startOffset": 0, "endOffset": 4}, {"referenceID": 12, "context": "[12] had applied deep network into natural languages, and many works such as [13] and [14] had applied deep network into image processing.", "startOffset": 77, "endOffset": 81}, {"referenceID": 13, "context": "[12] had applied deep network into natural languages, and many works such as [13] and [14] had applied deep network into image processing.", "startOffset": 86, "endOffset": 90}, {"referenceID": 14, "context": "Before [15] and [16] proposed the fast unsupervised and supervised methods, multi-layer neural networks are hard to train, this kind of difficulty is analysed and solved in [17] and [18], for the same cause our method could also perform well in deep architecture.", "startOffset": 7, "endOffset": 11}, {"referenceID": 15, "context": "Before [15] and [16] proposed the fast unsupervised and supervised methods, multi-layer neural networks are hard to train, this kind of difficulty is analysed and solved in [17] and [18], for the same cause our method could also perform well in deep architecture.", "startOffset": 16, "endOffset": 20}, {"referenceID": 16, "context": "Before [15] and [16] proposed the fast unsupervised and supervised methods, multi-layer neural networks are hard to train, this kind of difficulty is analysed and solved in [17] and [18], for the same cause our method could also perform well in deep architecture.", "startOffset": 173, "endOffset": 177}, {"referenceID": 17, "context": "Before [15] and [16] proposed the fast unsupervised and supervised methods, multi-layer neural networks are hard to train, this kind of difficulty is analysed and solved in [17] and [18], for the same cause our method could also perform well in deep architecture.", "startOffset": 182, "endOffset": 186}], "year": 2015, "abstractText": "The outputs of non-linear feed-forward neural network are positive, which could be treated as probability when they are normalized to one. If we take Entropy-Based Principle into consideration, the outputs for each sample could be represented as the distribution of this sample for different clusters. Entropy-Based Principle is the principle with which we could estimate the unknown distribution under some limited conditions. As this paper defines two processes in Feed-Forward Neural Network, our limited condition is the abstracted features of samples which are worked out in the abstraction process. And the final outputs are the probability distribution for different clusters in the clustering process. As Entropy-Based Principle is considered into the feed-forward neural network, a clustering method is born. We have conducted some experiments on six open UCI datasets, comparing with a few baselines and applied purity as the measurement . The results illustrate that our method outperforms all the other baselines that are most popular clustering methods. Keywords\u2014Feed-Forward Neural Network, Clustering, Max-Entropy Principle, Probabilistic Models", "creator": "dvips(k) 5.991 Copyright 2011 Radical Eye Software"}}}