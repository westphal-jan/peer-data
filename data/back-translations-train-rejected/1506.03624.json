{"id": "1506.03624", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "11-Jun-2015", "title": "Bootstrapping Skills", "abstract": "The monolithic approach to policy representation in Markov Decision Processes (MDPs) looks for a single policy that can be represented as a function from states to actions. For the monolithic approach to succeed (and this is not always possible), a complex feature representation is often necessary since the policy is a complex object that has to prescribe what actions to take all over the state space. This is especially true in large domains with complicated dynamics. It is also computationally inefficient to both learn and plan in MDPs using a complex monolithic approach. We present a different approach where we restrict the policy space to policies that can be represented as combinations of simpler, parameterized skills---a type of temporally extended action, with a simple policy representation. We introduce Learning Skills via Bootstrapping (LSB) that can use a broad family of Reinforcement Learning (RL) algorithms as a \"black box\" to iteratively learn parametrized skills. Initially, the learned skills are short-sighted but each iteration of the algorithm allows the skills to bootstrap off one another, improving each skill in the process. We prove that this bootstrapping process returns a near-optimal policy. Furthermore, our experiments demonstrate that LSB can solve MDPs that, given the same representational power, could not be solved by a monolithic approach. Thus, planning with learned skills results in better policies without requiring complex policy representations.", "histories": [["v1", "Thu, 11 Jun 2015 11:06:40 GMT  (1157kb,D)", "http://arxiv.org/abs/1506.03624v1", null]], "reviews": [], "SUBJECTS": "cs.AI", "authors": ["daniel j mankowitz", "timothy a mann", "shie mannor"], "accepted": false, "id": "1506.03624"}, "pdf": {"name": "1506.03624.pdf", "metadata": {"source": "CRF", "title": "Bootstrapping Skills", "authors": ["Daniel J. Mankowitz", "Timothy A. Mann"], "emails": ["danielm@tx.technion.ac.il", "timothymann@google.com", "shie@ee.technion.ac.il"], "sections": [{"heading": "1 Introduction", "text": "MDPs are important for both planning and learning in Reinforcement Learning (RL). The RL planning problem uses an MDP model to derive a policy that maximizes the sum of the rewards received, while the RL learning problem learns an MDP model from experience (because the MDP model is unknown in advance). In this paper, we focus on RL planning and use insights from RL that could be used to solve problems that are insoluble with traditional planning approaches (such as Value Iteration and Policy Iteration (c.f., Puterman)."}, {"heading": "2 Background", "text": "Let M = < S, A, P, R, \u03b3 > be an MDP where S is a (possibly infinite) set of states, A is a finite set of actions, P is a mapping of state-action pairs on probability distributions over the next states, R maps each state-action pair to a reward in [0, 1], and Q [0, 1) is the discount factor. Let's say the rewards are in [0, 1] may seem restrictive, any limited space can be rewritten so that this assumption holds. A policy \u03c0 (a | s) indicates the likelihood of performing actions in a state in [0, 1] S. Let M be an MDP. The value function of a policy \u03c0 in relation to a state is V (s) = M (s) = a state in relation to a policy in M (s). The value of a policy in relation to a state in the form of M (s) can be written in the form of a policy in T (s)."}, {"heading": "3 Skills", "text": "One of the key ideas behind skills is that they can be learned locally, but they can be used throughout DP = >. We present a new formal definition of skills and a skills policy. Definition 1. A skill is defined by a pair < f = skills, \u03b2 >, where skills are a parametric policy with parameter vector and \u03b2: S \u2192 {0, 1} indicates whether the skill is completed (i.e., \u03b2 (s) = 1) or not (i.e., \u03b2 (s) = 0) in light of current skills. Definition 2. Let it be a set of m \u2265 1 skills. A skills policy is a mapping \u00b5: S \u2192 [m], where S is the state-space and [m] is the index of skills. A skills policy selects which skills are initialized from the current state by returning the index of one of the skills."}, {"heading": "4 Learning Skills via Bootstrapping (LSB) Algorithm", "text": "\"It's not like it's an approach,\" he said. \"It's not like it's an approach.\" \"It's not like there's an approach.\" \"It's like there's an approach.\" \"It's like there's an approach.\" \"It's like there's an approach.\" \"It's like there's an approach.\" \"It's like there's an approach.\" \"It's like there's an approach.\" \"It's like there's an approach.\" \"It's like there's an approach.\" \"It's like there's an approach.\" \"It's like there's an approach.\" It's like there's an approach. \"It's like there's an approach.\" \"It's like there's an approach.\" It's like there's an approach. \""}, {"heading": "5 Analysis of LSB", "text": "We offer the first convergence guarantee for iterative learning skills in a continuous state MDP using LSB (Lemma 1 and Lemma 2, proven in the supplementary material). We use this guarantee as well as Lemma 2 to prove Theorem 1. However, this theorem allows us to analyze the quality of the skills returned by LSB. It turns out that the quality of policy critically depends on the quality of the qualification learning algorithm. An important parameter for determining the quality of a policy returned by LSB is the qualification error defined below. Definition 4. Let P be a partition over the desired state-space. The skill learning error is isvolutions P = max i [m]."}, {"heading": "6 Experiments and Results", "text": "We have conducted experiments on three well-known RL benchmarks: Mountain Car (MC), Puddle World (PW) Sutton [1996] and the pinball domain Konidaris and Barto [2009]. The MC domain has similar results to PW and has therefore been moved into the supplementary material. We use two variations for the pinball domain, namely the labyrinth world we have created, and pinball world, which is one of the standard pinball benchmark domains. Our experiments show that the monolithic approach is not able to solve the tasks in any case, because political representation is not complex enough."}, {"heading": "6.1 Puddle World", "text": "The state space is the < x, y > location of the agent. Figure 4a compares the monolithic approach to the LSB (for a 2 x 2 grid partition).The monolithic approach achieves a low average reward. However, LSB combines a number of abilities with the same limited political representation, resulting in a richer solution space and a higher average reward as seen in Figure 4a. This is comparable to the near optimal average reward achieved by executing the Approximate Value Iteration (AVI) for a large number of iterations. In this experiment, LSB is not initiated in the partition class containing the target state, but nevertheless achieves near optimal convergence after only 2 iterations. Figure 4b compares the performance of different partitions where a 1 x 1 grid represents the monolithic approach."}, {"heading": "Maze-world Pinball-world", "text": "If you analyze the cost of the 3x3 and 4x4 grids, it becomes clear that in this scenario, the 3x3 partition design is better suited to puddles than the 4x4 partition, resulting in lower costs."}, {"heading": "6.2 Skill Generalization", "text": "In the worst case scenario, the number of skills learned (LSB) is based on the partition. However, LSB can learn similar skills in different partition classes, increasing the redundancy of the qualification set. This suggests that skills can be reused in different parts of the state space, resulting in lower skills than the number of partition classes. To confirm this intuition, a 4 x 4 grid has been created for both the Mountain Car and Puddle World areas. We have operated LSB with this grid on each domain. As it is more intuitive to visualize and analyze the reusable skills generated for the 2D Puddle World, we present these skills in a quiver plot that is superimposed on the Puddle World (Figure 4c). For each skill, the direction (red arrows in Figure 4c) will be used by scanning and averaging measures from the probability distribution of skills, which can be determined in the same way as the entire state space, as many can be seen in Figure 4c."}, {"heading": "6.3 Pinball", "text": "These experiments were conducted in domains with simple dynamics. We decided to test LSB on a domain with much more complicated dynamics, namely in Pinball Konidaris and Barto [2009]. The goal in Pinball is to direct an agent (the blue ball) to the target position (the red region).The Pinball domain provides a stereotypical test for LSB as the speed at which the agent travels must be taken into account in order to circumnavigate obstacles. Furthermore, collisions with obstacles in the vicinity are not linear at obstacles. State space is four times the size < x, x, x, y representing the 2D location of the agent, and x, y representing speeds in each direction."}, {"heading": "7 Discussion", "text": "This approach, however, is similar (and partly inspired by) to the skills shown by Konidaris and Barto (2009). However, the heuristic approach applied through the concatenation of skills may not be able to produce a near-optimal policy, even if the human skills are small. In addition, we provide theoretical results for LSB that allow us to link the quality of the final policy to the error. LSB is the first algorithm that guarantees theoretical convergence, while iteratively learning a set of skills in a continuous state space. Moreover, the theoretical guarantees for LSB enable skills to dovetail with political assessments."}, {"heading": "A Appendix", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "A.1 LSB Skill MDP", "text": "The formal definition of a skill MDP is provided here for completeness. Definition 5. Given a target MDP M = < S, A, P, R, \u03b3 > and a value function VM is a skill MDP for partition Pi which is an MDP defined by M \"i = < S,\" A, P, \"R\" where S \"= Pi\" {sT} is a terminal state and A is the action set by M. The transition probability is P \"(s,\" a) = P (s, \"s,\" s \"s\" s, \"a) if s\" Pi \"s\" s \"and Pi\" s \"P\" (y \"s,\" s, \"s\" s \"s,\" s \"s\" s, \"s\" s \"s,\" s \"s\" s, \"s\" s \"s."}, {"heading": "A.2 Proof of Theorem 1", "text": "In this section we will prove theorem 1.We will make use of the following notations. Form \u2265 1 > > Skills will be designated with [m] of the specified number (1, 2,..,.) Let us be \u03c3 = < \u03c0\u03b8, > Skill will be initialized by a State. 1. P\u03c0\u03b8\u03b2 (s) denotes the probability that the skill will be terminated (i.e., the skill will be returned to the agent) exactly 1 time after its initialization. 2. R is the probability that the skill will be terminated (i.e., the skill will be returned to the agent). 2. R is the probability that the skill will be terminated (i.e., the skill will be returned to the agent) within a period of time after its initialization."}, {"heading": "Proof.", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "Proving that (5) holds:", "text": "First, we show that (5) applies that (5). For each Skill, we designate the Skill Guidelines and the Termination Rule of Skill by (4) and (5) by (5). If s), where j 6 = i, thenV < \u00b5, \u03a3 > M (s) \u2212 V < (s) - (R) - (S) - (S) - (S) - (S) - (S) - (S) - (S) - (S) - (S) - (S) - (S) - (S) - (S) - (S) - (S) - (S) - (S) - (S) - (S) - (S) - (S) - (S) - (S) - (S) - (S) < (S) - (S) - (S) - (S) (S) (S) (S)."}, {"heading": "Proving that (6) holds:", "text": "If we do not know what we are doing, we will not do it. < < p (s) < p (s) p (s) p (s) p (s) p (s) p (s) p (s) p (s) p (s) p (s) p (s) p (s) p (s) p (s) p (s) p (s) p (s) p (s) p (s) p (s) p (s) p (s) p (s) p (s) p (p) p (p) p (s) p (s) p (s) p (p (p) p (p (p) p (s) p (p (s) p (p (s) p (p (p) p (p (s) p (p) p (p) p (p) p (s) p) p (s) p (s) p (p) p (p) p (p) p (p) p (s) p) p (p) p (p) p (p) p (s) p) p (p) p (p) p (s) p (p) p (p) p (s) p (s) p (s) p (p (s) p (p) p (p (s) p (p) p (p (s) p (p (s) p (p (p (s) p (s) p (p (p (s) p (p (s) p (p (p (s) p (p (p) p (p (p (s) p) p (p) p (p (p (p) p (p (p) p (p (p) p (p (p (p) p (p (p) p) p (p (p (p) p) p) p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p"}, {"heading": "A.2.1 Proof of Theorem 1", "text": "Proof. (of Theorem 1) The loss of all policies is limited by 11 \u2212 \u03b3. Therefore, by applying Lemma 2 and repeating (7) for K-log iterations (\u03b5 (1 \u2212 \u03b3)), we obtain that we use \"V,\" \"M,\" \"M,\" \"K,\" \"K\" (1 \u2212 \u03b3 \"),\" m\u03b7 \"(1 \u2212 \u03b3),\" 2, \"\" g, \"\" g, \"g,\" \"g,\" g, \"\" g, \"\" g, \"g,\" \"g,\" \"g,\" \"\" g, \"\" g, \"\" g, \"g,\" g, \"g,\" g, \"g,\" g, \"g,\" g, \"g,\" g, \"g,\" \"g,\" \"g,\" \"g.\""}, {"heading": "A.3 Experiments", "text": "We conducted experiments with three well-known RL benchmarks: Mountain Car (MC), Puddle World (PW) Sutton [1996] and the pinball domain Konidaris and Barto [2009]. The MC domain is discussed here, and the PW and pinball domains can be found in the main article. The purpose of our experiments is to show that LSB can solve a complicated task with a simple political representation by combining capabilities over simple representations, which are simple enough that we can still solve them with richer representations, which allows us to compare LSB with a policy that is very close to the optimum. Our experiments show the potential to expand to higher dimensional domains by combining capabilities over simple representations. Let's remember that LSB is a meta algorithm. We need to provide an algorithm for policy assessment (PE) and skill learning learning."}, {"heading": "A.3.1 Mountain Car", "text": "The mountain car domain consists of an underpowered car located in a valley, and the car must use potential energy to advance to the destination, which is the top of the right hill. State space is the position and speed of the car < p, v >. Figure 6a compares the monolithic approach to the LSB (for a 2 x 2 grid partition).The monolithic approach achieves a low average reward. However, in the same restricted political representation, LSB combines a number of skills, resulting in a richer solution space and a higher average reward, as seen in Figure 6a. This is comparable to the approximately optimal average reward. Convergence is achieved after a single iteration, as LSB is initiated by the partition containing the target location, whereby the value is immediately transferred to subsequent skills. Figure 6b compares the performance of different partitions, where a 1 x grid represents the 1 x monolithic one."}, {"heading": "A.3.2 Puddle World", "text": "Figure 8 compares the value functions for different grid sizes in Puddle World. The monolithic approach (1 x 1 partition) provides a highly sub-optimal solution, as the agent must travel directly through the puddles according to its value function to reach the target position. 3 x 3 grid is a near-optimal solution."}, {"heading": "A.4 Modified Regular-Gradient ActorCritic", "text": "The algorithm is based on RegularGradient ActorCritic et al. [2009]. The algorithm differs from Regular-Gradient ActorCritic because it uses different representations to approximate the value function and the policy. In the case of a state action couple (s, a), a), b), c), c), c), c), c), c), c), c), c), c), c), c), c), c), c), c), c), c), c), c), c), c), c), c), c), c), c), c), c), c), c), c), c), c), c), c), c), c), c), c), c), c), c), c), c), c), c), c), c), c), c), c), c), c), c), c), c), c), c), c), c), c), c), c), c), c), c), c), c), c), c), c), c), c), c), c), c), c), c), c), c), c), c), c), c), c), c), c), c), c), c), c), c), c), c), c), c), c), c), c), c), c), c), c), c), c), c), c), c), c), c), c), c), c), c), c), c), c), c), c), c), c), c), c), c), c), c), c), c), c), c), c), c), c), c), c), c), c), c), c), c), c), c), c), c),"}, {"heading": "Require:", "text": "1. \u03c6: Figure of states on a vector representation used to approximate the value, 2. \u03c9: value-function-approximation parameter, 3. \u0445: Figure of states on a vector representation used to approximate the policy, 4. \u03b8: political parameters, 5. \u03b1: value-learning rate (fast learning rate), 6. \u03b2: the policy-learning rate (slow learning rate, i.e. \u03b2 < \u03b1) and 7. (s, a, s, r): a state-action-next-state-reward-tuple1: V-NEW (r + \u03b3 a) \"A\" (a \"| s\") \u03c9T (s, a \")\") {Estimate V, \"given the new sample.} 2: V\" OLD-digit-digit-digit-digit-digit \"(s, a) {Use the current value-function-approximation to estimate the value.} 3: V (\" NEW V \"-OLD-digit-digit-digit-digit-digit-digit-digit-digit\") {\"Calculate the time difference."}, {"heading": "A.5 Pinball Demonstration Videos", "text": "Attached are two videos demonstrating a policy learned by LSB for the Konidaris and Barto Pinball domains [2009], both of which are analyzed and discussed in the main article. The first video shows a policy learned for Maze-world, and the second video shows a policy learned for Pinball-world, one of the standard pinball benchmark domains. The agent's goal (blue ball) is to circumnavigate the obstacles and reach the target region (red ball)."}], "references": [{"title": "Exploration and apprenticeship learning in reinforcement learning", "author": ["Pieter Abbeel", "Andrew Y Ng"], "venue": "In Proceedings of the 22nd International Conference on Machine Learning,", "citeRegEx": "Abbeel and Ng.,? \\Q2005\\E", "shortCiteRegEx": "Abbeel and Ng.", "year": 2005}, {"title": "A survey of robot learning from demonstration", "author": ["Brenna D Argall", "Sonia Chernova", "Manuela Veloso", "Brett Browning"], "venue": "Robotics and Autonomous Systems,", "citeRegEx": "Argall et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Argall et al\\.", "year": 2009}, {"title": "Behavioral hierarchy: Exploration and representation", "author": ["Andrew Barto", "George Konidaris", "C.M. Vigorito"], "venue": "In Computational and Robotic Models of the Hierarchical Organization of Behavior,", "citeRegEx": "Barto et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Barto et al\\.", "year": 2013}, {"title": "Dynamic programming and optimal control, volume 1", "author": ["Dimitri P Bertsekas"], "venue": "Athena Scientific Belmont, MA,", "citeRegEx": "Bertsekas.,? \\Q1995\\E", "shortCiteRegEx": "Bertsekas.", "year": 1995}, {"title": "Technical update: Least-squares temporal difference learning", "author": ["Justin A Boyan"], "venue": "Machine Learning,", "citeRegEx": "Boyan.,? \\Q2002\\E", "shortCiteRegEx": "Boyan.", "year": 2002}, {"title": "PAC-inspired option discovery in lifelong reinforcement learning", "author": ["Emma Brunskill", "Lihong Li"], "venue": "JMLR, 1:316\u2013324,", "citeRegEx": "Brunskill and Li.,? \\Q2014\\E", "shortCiteRegEx": "Brunskill and Li.", "year": 2014}, {"title": "Optimal policy switching algorithms for reinforcement learning", "author": ["Gheorghe Comanici", "Doina Precup"], "venue": "In Proceedings of the 9 th AAMAS,", "citeRegEx": "Comanici and Precup.,? \\Q2010\\E", "shortCiteRegEx": "Comanici and Precup.", "year": 2010}, {"title": "Accelerating Multi-agent Reinforcement Learning with Dynamic Co-learning", "author": ["Daniel Garant", "Bruno C. da Silva", "Victor Lesser", "Chongjie Zhang"], "venue": "Technical report,", "citeRegEx": "Garant et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Garant et al\\.", "year": 2015}, {"title": "Hierarchical solution of markov decision processes using macro-actions", "author": ["Milos Hauskrecht", "Nicolas Meuleau", "Leslie Pack Kaelbling", "Thomas Dean", "Craig Boutilier"], "venue": "In Proceedings of the 14th Conference on Uncertainty in AI,", "citeRegEx": "Hauskrecht et al\\.,? \\Q1998\\E", "shortCiteRegEx": "Hauskrecht et al\\.", "year": 1998}, {"title": "Efficient planning under uncertainty with macro-actions", "author": ["Ruijie He", "Emma Brunskill", "Nicholas Roy"], "venue": "Journal of Artificial Intelligence Research,", "citeRegEx": "He et al\\.,? \\Q2011\\E", "shortCiteRegEx": "He et al\\.", "year": 2011}, {"title": "Skill discovery in continuous reinforcement learning domains using skill chaining", "author": ["George Konidaris", "Andrew G Barto"], "venue": "In NIPS", "citeRegEx": "Konidaris and Barto.,? \\Q2009\\E", "shortCiteRegEx": "Konidaris and Barto.", "year": 2009}, {"title": "Time regularized interrupting options", "author": ["Daniel J Mankowitz", "Timothy A Mann", "Shie Mannor"], "venue": null, "citeRegEx": "Mankowitz et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Mankowitz et al\\.", "year": 2014}, {"title": "Scaling up approximate value iteration with options: Better policies with fewer iterations", "author": ["Timothy A Mann", "Shie Mannor"], "venue": "In Proceedings of the 31 st ICML,", "citeRegEx": "Mann and Mannor.,? \\Q2014\\E", "shortCiteRegEx": "Mann and Mannor.", "year": 2014}, {"title": "Automatic Discovery of Subgoals in Reinforcement Learning using Diverse Density", "author": ["Amy McGovern", "Andrew G Barto"], "venue": "In Proceedings of the 18th ICML,", "citeRegEx": "McGovern and Barto.,? \\Q2001\\E", "shortCiteRegEx": "McGovern and Barto.", "year": 2001}, {"title": "Hierarchical reinforcement learning: Assignment of behaviours to subpolicies by self-organization", "author": ["Wilco Moerman"], "venue": "PhD thesis,", "citeRegEx": "Moerman.,? \\Q2009\\E", "shortCiteRegEx": "Moerman.", "year": 2009}, {"title": "Reinforcement learning of motor skills with policy gradients", "author": ["Jan Peters", "Stefan Schaal"], "venue": "Neural Networks,", "citeRegEx": "Peters and Schaal.,? \\Q2008\\E", "shortCiteRegEx": "Peters and Schaal.", "year": 2008}, {"title": "Theoretical results on reinforcement learning with temporally abstract options", "author": ["Doina Precup", "Richard S Sutton", "Satinder Singh"], "venue": "In ECML-98,", "citeRegEx": "Precup et al\\.,? \\Q1998\\E", "shortCiteRegEx": "Precup et al\\.", "year": 1998}, {"title": "Markov Decision Processes - Discrete Stochastic Dynamic Programming", "author": ["Martin L Puterman"], "venue": "Jonathan Sorg and Satinder Singh. Linear options. In Proceedings 9 AAMAS,", "citeRegEx": "Puterman.,? \\Q1994\\E", "shortCiteRegEx": "Puterman.", "year": 1994}, {"title": "Using a minimal action grammar for activity understanding in the real world", "author": ["Douglas Summers-Stay", "Ching Lik Teo", "Yezhou Yang", "C Fermuller", "Yiannis Aloimonos"], "venue": "In Intelligent Robots and Systems (IROS),", "citeRegEx": "Summers.Stay et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Summers.Stay et al\\.", "year": 2012}, {"title": "Generalization in reinforcement learning: Successful examples using sparse coarse coding", "author": ["Richard Sutton"], "venue": "In Advances in neural information processing systems,", "citeRegEx": "Sutton.,? \\Q1996\\E", "shortCiteRegEx": "Sutton.", "year": 1996}, {"title": "Reinforcement Learning: An Introduction", "author": ["Richard Sutton", "Andrew Barto"], "venue": null, "citeRegEx": "Sutton and Barto.,? \\Q1998\\E", "shortCiteRegEx": "Sutton and Barto.", "year": 1998}, {"title": "Between MDPs and semi-MDPs: A framework for temporal abstraction in reinforcement learning", "author": ["Richard S Sutton", "Doina Precup", "Satinder Singh"], "venue": null, "citeRegEx": "Sutton et al\\.,? \\Q1999\\E", "shortCiteRegEx": "Sutton et al\\.", "year": 1999}, {"title": "Policy gradient methods for reinforcement learning with function approximation", "author": ["Richard S Sutton", "David McAllester", "Satindar Singh", "Yishay Mansour"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "Sutton et al\\.,? \\Q2000\\E", "shortCiteRegEx": "Sutton et al\\.", "year": 2000}, {"title": "Statistical learning theory, volume 2", "author": ["Vladimir Naumovich Vapnik"], "venue": "Wiley New York,", "citeRegEx": "Vapnik.,? \\Q1998\\E", "shortCiteRegEx": "Vapnik.", "year": 1998}], "referenceMentions": [{"referenceID": 17, "context": ", Puterman [1994]).", "startOffset": 2, "endOffset": 18}, {"referenceID": 14, "context": "chine learning is that the sample complexity of learning increases with the complexity of the representation Vapnik [1998]. In a planning scenario, increased sample complexity directly translates to an increase in computational complexity.", "startOffset": 109, "endOffset": 123}, {"referenceID": 3, "context": "Generalization: the ability of a system to perform accurately on unseen data, is important for machine learning in general, and can be achieved in this context by restricting the policy space, resulting in compact policies Bertsekas [1995], Sutton [1996].", "startOffset": 223, "endOffset": 240}, {"referenceID": 3, "context": "Generalization: the ability of a system to perform accurately on unseen data, is important for machine learning in general, and can be achieved in this context by restricting the policy space, resulting in compact policies Bertsekas [1995], Sutton [1996]. Policy Search (PS) algorithms, a form of generalization, learn and maintain a compact policy representation so that the policy generates similar actions in nearby states Peters and Schaal [2008], Bhatnagar et al.", "startOffset": 223, "endOffset": 255}, {"referenceID": 3, "context": "Generalization: the ability of a system to perform accurately on unseen data, is important for machine learning in general, and can be achieved in this context by restricting the policy space, resulting in compact policies Bertsekas [1995], Sutton [1996]. Policy Search (PS) algorithms, a form of generalization, learn and maintain a compact policy representation so that the policy generates similar actions in nearby states Peters and Schaal [2008], Bhatnagar et al.", "startOffset": 223, "endOffset": 451}, {"referenceID": 3, "context": "Generalization: the ability of a system to perform accurately on unseen data, is important for machine learning in general, and can be achieved in this context by restricting the policy space, resulting in compact policies Bertsekas [1995], Sutton [1996]. Policy Search (PS) algorithms, a form of generalization, learn and maintain a compact policy representation so that the policy generates similar actions in nearby states Peters and Schaal [2008], Bhatnagar et al. [2009]. Temporally Extended Actions [TEAs, Sutton et al.", "startOffset": 223, "endOffset": 476}, {"referenceID": 3, "context": "Generalization: the ability of a system to perform accurately on unseen data, is important for machine learning in general, and can be achieved in this context by restricting the policy space, resulting in compact policies Bertsekas [1995], Sutton [1996]. Policy Search (PS) algorithms, a form of generalization, learn and maintain a compact policy representation so that the policy generates similar actions in nearby states Peters and Schaal [2008], Bhatnagar et al. [2009]. Temporally Extended Actions [TEAs, Sutton et al., 1999]: Compact policies can be represented and combined hierarchically as TEAs. TEAs are control structures that execute for multiple timesteps. They have been extensively studied under different names, including skills Konidaris and Barto [2009], macro-actions Hauskrecht et al.", "startOffset": 223, "endOffset": 774}, {"referenceID": 3, "context": "Generalization: the ability of a system to perform accurately on unseen data, is important for machine learning in general, and can be achieved in this context by restricting the policy space, resulting in compact policies Bertsekas [1995], Sutton [1996]. Policy Search (PS) algorithms, a form of generalization, learn and maintain a compact policy representation so that the policy generates similar actions in nearby states Peters and Schaal [2008], Bhatnagar et al. [2009]. Temporally Extended Actions [TEAs, Sutton et al., 1999]: Compact policies can be represented and combined hierarchically as TEAs. TEAs are control structures that execute for multiple timesteps. They have been extensively studied under different names, including skills Konidaris and Barto [2009], macro-actions Hauskrecht et al. [1998], He et al.", "startOffset": 223, "endOffset": 814}, {"referenceID": 3, "context": "Generalization: the ability of a system to perform accurately on unseen data, is important for machine learning in general, and can be achieved in this context by restricting the policy space, resulting in compact policies Bertsekas [1995], Sutton [1996]. Policy Search (PS) algorithms, a form of generalization, learn and maintain a compact policy representation so that the policy generates similar actions in nearby states Peters and Schaal [2008], Bhatnagar et al. [2009]. Temporally Extended Actions [TEAs, Sutton et al., 1999]: Compact policies can be represented and combined hierarchically as TEAs. TEAs are control structures that execute for multiple timesteps. They have been extensively studied under different names, including skills Konidaris and Barto [2009], macro-actions Hauskrecht et al. [1998], He et al. [2011], and options Sutton et al.", "startOffset": 223, "endOffset": 832}, {"referenceID": 3, "context": "Generalization: the ability of a system to perform accurately on unseen data, is important for machine learning in general, and can be achieved in this context by restricting the policy space, resulting in compact policies Bertsekas [1995], Sutton [1996]. Policy Search (PS) algorithms, a form of generalization, learn and maintain a compact policy representation so that the policy generates similar actions in nearby states Peters and Schaal [2008], Bhatnagar et al. [2009]. Temporally Extended Actions [TEAs, Sutton et al., 1999]: Compact policies can be represented and combined hierarchically as TEAs. TEAs are control structures that execute for multiple timesteps. They have been extensively studied under different names, including skills Konidaris and Barto [2009], macro-actions Hauskrecht et al. [1998], He et al. [2011], and options Sutton et al. [1999]. TEAs are known to speed up the convergence rate of MDP planning algorithms Sutton et al.", "startOffset": 223, "endOffset": 866}, {"referenceID": 3, "context": "Generalization: the ability of a system to perform accurately on unseen data, is important for machine learning in general, and can be achieved in this context by restricting the policy space, resulting in compact policies Bertsekas [1995], Sutton [1996]. Policy Search (PS) algorithms, a form of generalization, learn and maintain a compact policy representation so that the policy generates similar actions in nearby states Peters and Schaal [2008], Bhatnagar et al. [2009]. Temporally Extended Actions [TEAs, Sutton et al., 1999]: Compact policies can be represented and combined hierarchically as TEAs. TEAs are control structures that execute for multiple timesteps. They have been extensively studied under different names, including skills Konidaris and Barto [2009], macro-actions Hauskrecht et al. [1998], He et al. [2011], and options Sutton et al. [1999]. TEAs are known to speed up the convergence rate of MDP planning algorithms Sutton et al. [1999], Mann and Mannor [2014].", "startOffset": 223, "endOffset": 963}, {"referenceID": 3, "context": "Generalization: the ability of a system to perform accurately on unseen data, is important for machine learning in general, and can be achieved in this context by restricting the policy space, resulting in compact policies Bertsekas [1995], Sutton [1996]. Policy Search (PS) algorithms, a form of generalization, learn and maintain a compact policy representation so that the policy generates similar actions in nearby states Peters and Schaal [2008], Bhatnagar et al. [2009]. Temporally Extended Actions [TEAs, Sutton et al., 1999]: Compact policies can be represented and combined hierarchically as TEAs. TEAs are control structures that execute for multiple timesteps. They have been extensively studied under different names, including skills Konidaris and Barto [2009], macro-actions Hauskrecht et al. [1998], He et al. [2011], and options Sutton et al. [1999]. TEAs are known to speed up the convergence rate of MDP planning algorithms Sutton et al. [1999], Mann and Mannor [2014]. However, the effectiveness of planning with TEAs depends critically on the given actions.", "startOffset": 223, "endOffset": 987}, {"referenceID": 10, "context": "Learning a useful set of TEAs has been a topic of intense research McGovern and Barto [2001], Moerman [2009], Konidaris and Barto [2009], Brunskill and Li [2014], Hauskrecht et al.", "startOffset": 67, "endOffset": 93}, {"referenceID": 10, "context": "Learning a useful set of TEAs has been a topic of intense research McGovern and Barto [2001], Moerman [2009], Konidaris and Barto [2009], Brunskill and Li [2014], Hauskrecht et al.", "startOffset": 67, "endOffset": 109}, {"referenceID": 8, "context": "Learning a useful set of TEAs has been a topic of intense research McGovern and Barto [2001], Moerman [2009], Konidaris and Barto [2009], Brunskill and Li [2014], Hauskrecht et al.", "startOffset": 110, "endOffset": 137}, {"referenceID": 5, "context": "Learning a useful set of TEAs has been a topic of intense research McGovern and Barto [2001], Moerman [2009], Konidaris and Barto [2009], Brunskill and Li [2014], Hauskrecht et al.", "startOffset": 138, "endOffset": 162}, {"referenceID": 5, "context": "Learning a useful set of TEAs has been a topic of intense research McGovern and Barto [2001], Moerman [2009], Konidaris and Barto [2009], Brunskill and Li [2014], Hauskrecht et al. [1998]. However, prior work suffers from the following drawbacks: (1) lack of theoretical analysis guaranteeing that the derived policy will be near-optimal in continuous state MDPs, (2) the process of learning TEAs is so expensive that it needs to be ammortized over a sequence of MDPs, (3) the approach is not applicable to MDPs with large or continuous state-spaces, or (4) the learned TEAs do not generalize over the state-space.", "startOffset": 138, "endOffset": 188}, {"referenceID": 5, "context": "Learning a useful set of TEAs has been a topic of intense research McGovern and Barto [2001], Moerman [2009], Konidaris and Barto [2009], Brunskill and Li [2014], Hauskrecht et al. [1998]. However, prior work suffers from the following drawbacks: (1) lack of theoretical analysis guaranteeing that the derived policy will be near-optimal in continuous state MDPs, (2) the process of learning TEAs is so expensive that it needs to be ammortized over a sequence of MDPs, (3) the approach is not applicable to MDPs with large or continuous state-spaces, or (4) the learned TEAs do not generalize over the state-space. We provide the first theoretical guarantees for iteratively learning a set of simple, generalizable parametric TEAs (skills) in a continuous state MDP. The learned TEAs solve the given tasks in a near-optimal manner. Skills: Generalization & Temporal Abstraction: Skills are TEAs defined over a parametrized policy. Thus, they incorporate both temporal abstraction and generalization. As TEAs, skills are closely related to options Sutton et al. [1999] developed in the RL literature.", "startOffset": 138, "endOffset": 1068}, {"referenceID": 5, "context": "Learning a useful set of TEAs has been a topic of intense research McGovern and Barto [2001], Moerman [2009], Konidaris and Barto [2009], Brunskill and Li [2014], Hauskrecht et al. [1998]. However, prior work suffers from the following drawbacks: (1) lack of theoretical analysis guaranteeing that the derived policy will be near-optimal in continuous state MDPs, (2) the process of learning TEAs is so expensive that it needs to be ammortized over a sequence of MDPs, (3) the approach is not applicable to MDPs with large or continuous state-spaces, or (4) the learned TEAs do not generalize over the state-space. We provide the first theoretical guarantees for iteratively learning a set of simple, generalizable parametric TEAs (skills) in a continuous state MDP. The learned TEAs solve the given tasks in a near-optimal manner. Skills: Generalization & Temporal Abstraction: Skills are TEAs defined over a parametrized policy. Thus, they incorporate both temporal abstraction and generalization. As TEAs, skills are closely related to options Sutton et al. [1999] developed in the RL literature. In fact, skills, as defined here, are a special case of options. Therefore, skills inherit many of the useful theoretical properties of options (e.g., Precup et al. [1998]).", "startOffset": 138, "endOffset": 1272}, {"referenceID": 17, "context": "It is important to note that this paper deals primarily with learning TEAs or Skills that aid in both speeding up the convergence rate of RL planning algorithms Sutton et al. [1999], Mann and Mannor [2014], as well as enabling larger problems to be solved using skills with simple policy representations.", "startOffset": 161, "endOffset": 182}, {"referenceID": 11, "context": "[1999], Mann and Mannor [2014], as well as enabling larger problems to be solved using skills with simple policy representations.", "startOffset": 8, "endOffset": 31}, {"referenceID": 2, "context": "Thus, planning with learned skills allows us to work with simpler representations Barto et al. [2013], which ultimately allows us to solve larger MDPs.", "startOffset": 82, "endOffset": 102}, {"referenceID": 18, "context": "Given a good set of skills, planning can be significantly faster Sutton et al. [1999], Mann and Mannor [2014].", "startOffset": 65, "endOffset": 86}, {"referenceID": 12, "context": "[1999], Mann and Mannor [2014]. However, in many domains we may not be given a good set of skills.", "startOffset": 8, "endOffset": 31}, {"referenceID": 18, "context": "Any number of policy evaluation algorithms could be used here, such as TD(\u03bb) with function approximation Sutton and Barto [1998] or LSTD Boyan [2002], modified to be used with skills.", "startOffset": 105, "endOffset": 129}, {"referenceID": 4, "context": "Any number of policy evaluation algorithms could be used here, such as TD(\u03bb) with function approximation Sutton and Barto [1998] or LSTD Boyan [2002], modified to be used with skills.", "startOffset": 137, "endOffset": 150}, {"referenceID": 4, "context": "Any number of policy evaluation algorithms could be used here, such as TD(\u03bb) with function approximation Sutton and Barto [1998] or LSTD Boyan [2002], modified to be used with skills. In our experiments, we used a straighforward variant of LSTD Sorg and Singh [2010]. Then we use the target MDP M to construct a Skill MDP M \u2032 (line 9).", "startOffset": 137, "endOffset": 267}, {"referenceID": 8, "context": "At first, the guarantee provided by Theorem 1 may appear similar to (Hauskrecht et al. [1998], Theorem 1).", "startOffset": 69, "endOffset": 94}, {"referenceID": 8, "context": "At first, the guarantee provided by Theorem 1 may appear similar to (Hauskrecht et al. [1998], Theorem 1). However, Hauskrecht et al. [1998] derive TEAs only at the beginning of the learning process and do not update them.", "startOffset": 69, "endOffset": 141}, {"referenceID": 18, "context": "We performed experiments on three well-known RL benchmarks: Mountain Car (MC), Puddle World (PW) Sutton [1996] and the Pinball domain Konidaris and Barto [2009].", "startOffset": 97, "endOffset": 111}, {"referenceID": 10, "context": "We performed experiments on three well-known RL benchmarks: Mountain Car (MC), Puddle World (PW) Sutton [1996] and the Pinball domain Konidaris and Barto [2009]. The MC domain has similar results to PW and therefore has been moved to the supplementary material.", "startOffset": 134, "endOffset": 161}, {"referenceID": 10, "context": "We performed experiments on three well-known RL benchmarks: Mountain Car (MC), Puddle World (PW) Sutton [1996] and the Pinball domain Konidaris and Barto [2009]. The MC domain has similar results to PW and therefore has been moved to the supplementary material. We use two variations for the Pinball domain, namely maze-world, which we created, and pinball-world which is one of the standard pinball benchmark domains. Our experiments show that, using a simple policy representation, the monolithic approach is unable to adequately solve the tasks in each case as the policy representation is not complex enough. However, LSB can solve these tasks with the same simple policy representation by combining bootstrapped skills. These domains are simple enough that we can still solve them using richer representations. This allows us to compare LSB to a policy that is very close to optimal. Our experiments demonstrate potential to scale up to higher dimensional domains by combining skills over simple representations. Recall that LSB is a meta-algorithm. We must provide an algorithm for Policy Evaluation (PE) and skill learning. In our experiments, for the MC and PW domains, we used SMDP-LSTD Sorg and Singh [2010] for PE and a modified version of Regular-Gradient Actor-Critic Bhatnagar et al.", "startOffset": 134, "endOffset": 1218}, {"referenceID": 10, "context": "We performed experiments on three well-known RL benchmarks: Mountain Car (MC), Puddle World (PW) Sutton [1996] and the Pinball domain Konidaris and Barto [2009]. The MC domain has similar results to PW and therefore has been moved to the supplementary material. We use two variations for the Pinball domain, namely maze-world, which we created, and pinball-world which is one of the standard pinball benchmark domains. Our experiments show that, using a simple policy representation, the monolithic approach is unable to adequately solve the tasks in each case as the policy representation is not complex enough. However, LSB can solve these tasks with the same simple policy representation by combining bootstrapped skills. These domains are simple enough that we can still solve them using richer representations. This allows us to compare LSB to a policy that is very close to optimal. Our experiments demonstrate potential to scale up to higher dimensional domains by combining skills over simple representations. Recall that LSB is a meta-algorithm. We must provide an algorithm for Policy Evaluation (PE) and skill learning. In our experiments, for the MC and PW domains, we used SMDP-LSTD Sorg and Singh [2010] for PE and a modified version of Regular-Gradient Actor-Critic Bhatnagar et al. [2009] for skill learning (see supplementary material for details).", "startOffset": 134, "endOffset": 1305}, {"referenceID": 8, "context": "We decided to test LSB on a domain with significantly more complicated dynamics, namely Pinball Konidaris and Barto [2009]. The goal in Pinball is to direct an agent (the blue ball) to the goal location (the red region).", "startOffset": 96, "endOffset": 123}, {"referenceID": 8, "context": "We decided to test LSB on a domain with significantly more complicated dynamics, namely Pinball Konidaris and Barto [2009]. The goal in Pinball is to direct an agent (the blue ball) to the goal location (the red region). The Pinball domain provides a sterner test for LSB as the velocity at which the agent is travelling needs to be taken into account to circumnavigate obstacles. In addition, collisions with obstacles in the environment are non-linear at obstacle vertices. The state space is the four-tuple \u3008x, y, \u1e8b, \u1e8f\u3009 where x, y represents the 2D location of the agent, and \u1e8b, \u1e8f represents the velocities in each direction. Two domains have been utilized, namely maze-world and pinball-world (Figure 5a and Figure 5d respectively). For maze-world, a 4 \u00d7 1 \u00d7 1 \u00d7 1 grid partitioning has been utilized and therefore 4 skills need to be learned using LSB. After running LSB on the maze-world domain, it can be seen in Figure 5b that LSB significantly outperforms the monolithic approach. Note that each skill in LSB has the same parametric representation as the monolithic approach. That is, a five-tuple \u30081, x, y, \u1e8b, \u1e8f\u3009. This simple parametric representation does not have the power to consistently solve maze-world using the monolithic approach. However, using LSB this simple representation is capable of solving the task in a near-optimal fashion as indicated on the average reward graph (Figure 5b) and resulting value function (Figure 5c). We also tested LSB on the more challenging pinball-world domain (5d). The same LSB parameters were used as in maze-world, but the provided partitioning was a 4 \u00d7 3 \u00d7 1 \u00d7 1 grid. Therefore, 12 skills needed to be learned in this domain. More skills were utilized for this domain since the domain is significantly more complicated than maze-world and a more refined skill-set is required to solve the task. As can be seen in the average reward graph in Figure 5e, LSB clearly outperforms the monolithic approach in this domain. It is less than optimal but still manages to sufficiently perform the task (see value function, Figure 5f ). The drop in performance is due to the complicated obstacle setup, the non-linear dynamics when colliding with obstacle edges and the partition design. 7 Discussion In this paper, we introduced an iterative bootstrapping procedure for learning skills. This approach is similar to (and partly inspired by) skill chaining Konidaris and Barto [2009]. However, the heuristic approach applied by skill chaining may not produce a near-optimal policy even when the skill learning error is small.", "startOffset": 96, "endOffset": 2429}, {"referenceID": 0, "context": "One way to identify these regions is by observing an expert\u2019s demonstrations Abbeel and Ng [2005], Argall et al.", "startOffset": 77, "endOffset": 98}, {"referenceID": 11, "context": "In addition, we could apply self-organizing approaches to facilitate skill reuse Moerman [2009]. Skill reuse can be especially useful for transfer learning.", "startOffset": 81, "endOffset": 96}, {"referenceID": 6, "context": "Consider a multi-agent environment Garant et al. [2015] where many of the agents may be performing similar tasks which require a similar skill-set.", "startOffset": 35, "endOffset": 56}, {"referenceID": 6, "context": "Consider a multi-agent environment Garant et al. [2015] where many of the agents may be performing similar tasks which require a similar skill-set. In this environment, skill reuse can facilitate learning complex multi-agent policies (co-learning) with very few samples. Given a task, LSB can learn and combine skills, based on a set of rules, to solve the task. This structure of learned skills and combination rules forms a generative action grammar Summers-Stay et al. [2012] which paves the way for building advanced skill structures that are capable of solving complex tasks in different environments and conditions.", "startOffset": 35, "endOffset": 479}, {"referenceID": 6, "context": "Consider a multi-agent environment Garant et al. [2015] where many of the agents may be performing similar tasks which require a similar skill-set. In this environment, skill reuse can facilitate learning complex multi-agent policies (co-learning) with very few samples. Given a task, LSB can learn and combine skills, based on a set of rules, to solve the task. This structure of learned skills and combination rules forms a generative action grammar Summers-Stay et al. [2012] which paves the way for building advanced skill structures that are capable of solving complex tasks in different environments and conditions. One exciting extension of our work would be to incorporate skill interruption, similar to option interruption. Option interruption involves terminating an option based on an adaptive interruption rule Sutton et al. [1999]. Options are terminated when the value of continuing the current option is lower than the value of switching to a new option.", "startOffset": 35, "endOffset": 844}, {"referenceID": 6, "context": "Consider a multi-agent environment Garant et al. [2015] where many of the agents may be performing similar tasks which require a similar skill-set. In this environment, skill reuse can facilitate learning complex multi-agent policies (co-learning) with very few samples. Given a task, LSB can learn and combine skills, based on a set of rules, to solve the task. This structure of learned skills and combination rules forms a generative action grammar Summers-Stay et al. [2012] which paves the way for building advanced skill structures that are capable of solving complex tasks in different environments and conditions. One exciting extension of our work would be to incorporate skill interruption, similar to option interruption. Option interruption involves terminating an option based on an adaptive interruption rule Sutton et al. [1999]. Options are terminated when the value of continuing the current option is lower than the value of switching to a new option. This also implies that partition classes can overlap one another, as the option interruption rule ensures that the option with the best long term value is always being executed. Mankowitz et al. [2014] interlaced Sutton\u2019s interruption rule between iterations of value iteration and proved convergence to a global optimum.", "startOffset": 35, "endOffset": 1172}, {"referenceID": 6, "context": "Comanici and Precup [2010] have developed a policy gradient technique for learning the termination conditions of options.", "startOffset": 0, "endOffset": 27}, {"referenceID": 18, "context": "We performed experiments on three well-known RL benchmarks: Mountain Car (MC), Puddle World (PW) Sutton [1996] and the Pinball domain Konidaris and Barto [2009].", "startOffset": 97, "endOffset": 111}, {"referenceID": 10, "context": "We performed experiments on three well-known RL benchmarks: Mountain Car (MC), Puddle World (PW) Sutton [1996] and the Pinball domain Konidaris and Barto [2009]. The MC domain is discussed here.", "startOffset": 134, "endOffset": 161}, {"referenceID": 10, "context": "We performed experiments on three well-known RL benchmarks: Mountain Car (MC), Puddle World (PW) Sutton [1996] and the Pinball domain Konidaris and Barto [2009]. The MC domain is discussed here. The PW and Pinball domains are found in the main paper. The purpose of our experiments is to show that LSB can solve a complicated task with a simple policy representation by combining bootstrapped skills. These domains are simple enough that we can still solve them using richer representations. This allows us to compare LSB to a policy that is very close to optimal. Our experiments demonstrate potential to scale up to higher dimensional domains by combining skills over simple representations. Recall that LSB is a meta-algorithm. We must provide an algorithm for Policy Evaluation (PE) and skill learning. In our experiments, for the MC domain, we used SMDP-LSTD Sorg and Singh [2010] for PE and a modified version of Regular-Gradient Actor-Critic Bhatnagar et al.", "startOffset": 134, "endOffset": 886}, {"referenceID": 10, "context": "We performed experiments on three well-known RL benchmarks: Mountain Car (MC), Puddle World (PW) Sutton [1996] and the Pinball domain Konidaris and Barto [2009]. The MC domain is discussed here. The PW and Pinball domains are found in the main paper. The purpose of our experiments is to show that LSB can solve a complicated task with a simple policy representation by combining bootstrapped skills. These domains are simple enough that we can still solve them using richer representations. This allows us to compare LSB to a policy that is very close to optimal. Our experiments demonstrate potential to scale up to higher dimensional domains by combining skills over simple representations. Recall that LSB is a meta-algorithm. We must provide an algorithm for Policy Evaluation (PE) and skill learning. In our experiments, for the MC domain, we used SMDP-LSTD Sorg and Singh [2010] for PE and a modified version of Regular-Gradient Actor-Critic Bhatnagar et al. [2009] for skill learning.", "startOffset": 134, "endOffset": 973}, {"referenceID": 19, "context": "Although using different representations for approximating the value function and the policy strictly violates the policy gradient theorem Sutton et al. [2000], it still tends to work well in practice.", "startOffset": 139, "endOffset": 160}, {"referenceID": 10, "context": "There are two videos attached showing a demonstration of a policy learned by LSB for the Pinball domain Konidaris and Barto [2009]. Both of these domains are analyzed and discussed in the main paper.", "startOffset": 104, "endOffset": 131}], "year": 2015, "abstractText": "The monolithic approach to policy representation in Markov Decision Processes (MDPs) looks for a single policy that can be represented as a function from states to actions. For the monolithic approach to succeed (and this is not always possible), a complex feature representation is often necessary since the policy is a complex object that has to prescribe what actions to take all over the state space. This is especially true in large domains with complicated dynamics. It is also computationally inefficient to both learn and plan in MDPs using a complex monolithic approach. We present a different approach where we restrict the policy space to policies that can be represented as combinations of simpler, parameterized skills\u2014a type of temporally extended action, with a simple policy representation. We introduce Learning Skills via Bootstrapping (LSB) that can use a broad family of Reinforcement Learning (RL) algorithms as a \u201cblack box\u201d to iteratively learn parametrized skills. Initially, the learned skills are short-sighted but each iteration of the algorithm allows the skills to bootstrap off one another, improving each skill in the process. We prove that this bootstrapping process returns a near-optimal policy. Furthermore, our experiments demonstrate that LSB can solve MDPs that, given the same representational power, could not be solved by a monolithic approach. Thus, planning with learned skills results in better policies without requiring complex policy representations.", "creator": "LaTeX with hyperref package"}}}