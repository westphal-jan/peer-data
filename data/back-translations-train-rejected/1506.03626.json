{"id": "1506.03626", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "11-Jun-2015", "title": "Margin-Based Feed-Forward Neural Network Classifiers", "abstract": "Margin-Based Principle has been proposed for a long time, it has been proved that this principle could reduce the structural risk and improve the performance in both theoretical and practical aspects. Meanwhile, feed-forward neural network is a traditional classifier, which is very hot at present with a deeper architecture. However, the training algorithm of feed-forward neural network is developed and generated from Widrow-Hoff Principle that means to minimize the squared error. In this paper, we propose a new training algorithm for feed-forward neural networks based on Margin-Based Principle, which could effectively promote the accuracy and generalization ability of neural network classifiers with less labelled samples and flexible network. We have conducted experiments on four UCI open datasets and achieved good results as expected. In conclusion, our model could handle more sparse labelled and more high-dimension dataset in a high accuracy while modification from old ANN method to our method is easy and almost free of work.", "histories": [["v1", "Thu, 11 Jun 2015 11:10:25 GMT  (378kb)", "http://arxiv.org/abs/1506.03626v1", "This paper has been published in ICANN 2015: International Conference on Artificial Neural Networks, Amsterdam, The Netherlands, (May 14-15, 2015)"]], "COMMENTS": "This paper has been published in ICANN 2015: International Conference on Artificial Neural Networks, Amsterdam, The Netherlands, (May 14-15, 2015)", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["han xiao", "xiaoyan zhu"], "accepted": false, "id": "1506.03626"}, "pdf": {"name": "1506.03626.pdf", "metadata": {"source": "CRF", "title": "Margin-Based Feed-Forward Neural Network Classifiers", "authors": ["Han Xiao", "Xiaoyan Zhu"], "emails": ["xiaoh12@mails.tsinghua.edu.cn)", "zxy-dcs@tsinghua.edu.cn)"], "sections": [{"heading": null, "text": "We have set out to find new ways to reduce structural risk and improve performance in both theoretical and practical aspects. In this paper, we propose a new training algorithm that is currently very hot with a deeper architecture. However, the feed-forward neural network training algorithm is developed and generated by the Widrow-Hoff Principle, which means minimizing square error. In this paper, we propose a new training algorithm based on margin-based neural networks that could effectively promote the accuracy and generalizability of neural network classification. We have conducted experiments on four UCI open datasets and have achieved good results that are expected."}, {"heading": "II. RELATED WORK", "text": "Neural network is one of the early models of artificial intelligence, and is now becoming a large branch of learning algorithms that can not only bring about deep improvements. What this paper is about is the most famous principle that drives feed-forward classification networks with non-linear mapping. There are three types of popular ways to promote the structure of the neural network, which are intended to add regularity to the optimization goal, to cut the excess linkages and go into deep architectures. The first way is to refine the weights of the neural network and the other ways are to revise the structure of the neural network. [1] had studied the effect of weight decay and found that it has two effects by suppressing any irrelevant meaning of the weights and improving generalization capability. [2] The pruning of the neural network had studied and the other ways are the structure of the ceramic effect, and that the neural effect had two ceramic effects [1]."}, {"heading": "III. TWO PROCESSES IN FEED-FORWARD NEURAL NETWORKS", "text": "The process of the input layer to hidden layers corresponds to the abstraction process. At this stage, each hidden neuron is treated as a linear regression learner to adapt some parts of the data, and hidden neurons treat the regression results with nonlinear function to obtain its results. In this process, the characteristics of the samples could be transformed into a new space in which the hidden neurons play a role as a basis. As Figure 1 shows, the hidden neuron H1 captures the characteristics of the black samples, and the hidden neuron H2 captures the characteristics of the white samples, and other hidden neurons such as Hn capture an aspect of the samples. All of them could abstract the original space and data distribution into an abstraction process in which the data could be analyzed in a variety of ways."}, {"heading": "A. Abstraction Process And Min-Margin Principle", "text": "As explained above, the abstraction process uses linear regression learners to capture different characteristics of samples. Traditionally, the process is optimized by minimal square errors, as follows: min J = (< ~ w, ~ x > \u2212 y) 2However, the margin-based principle does not minimize the distance, so the min margin principle takes the minimum Fig. 2. Minimum square error and the min margin principle in the abstraction process, (a) shows the least square error and (b) shows the margin-based principle error.Fig. 3. Minimum square error and the max margin principle in the classification process distance of samples to the regression level, the optimized target as follows: min J = (< ~ w, ~ x >) y | ~ w | In the equation, the y is the name of the sample for some linear abstraction regression learners, and labels may be learned through the optimization process."}, {"heading": "B. Classification Process And Max-Margin Principle", "text": "As mentioned above, the classification process should use linear classifiers to distinguish different classes. In the conventional way, the linear hyperplane could stop at any appropriate point, so the network would quickly run into an overpass problem. While the max margin principle could effectively reduce structural risk and improve generalization capability, the max margin principle takes up the idea as the next optimization problem. max J = (< ~ w, ~ x >) y | ~ w | As Fig. 3 shows, the solid line corresponds to the optimization of the minimum square and the dashed line to the optimization of the max margin principle. If more test patterns arrive, the dashed line may be better than the fixed one."}, {"heading": "IV. MARGIN-BASED PRINCIPLE FOR FEED-FORWARD NEURAL NETWORKS", "text": "The input layer on the hidden layers is the abstraction process, in which we should apply the min margin principle, and the hidden layers on the output layer are the classification process, in which we should apply the max margin principle. It means that all the hidden layers should be an abstraction process, in which the min margin principle works, and the last layer corresponding to the output should be a classification process, in which the max margin principle is into.The forward neural network has M hidden layers, each of which has MH hidden neurons and N output neurons, the number of samples designated is T. So we combine the two principles in an optimization problem, as formulations followed by maximization: Objective = 1 {imp}."}, {"heading": "V. EXPERIMENTS", "text": "We have conducted two groups of experiments, one group for the effectiveness of margin-based feed-forward neural network, another group for the study of network structure or we say hyperparameters."}, {"heading": "A. Datasets", "text": "This paper has selected four open UCI datasets as experimental datasets: Banknote Authentication, MAGIC Gamma Telescope, ISOLET and FarmAD. The rule we choose is to verify our methods in different settings and ranges. To create training datasets and test datasets, we randomly divide the dataset, and the result of the performance is the average of at least five times split tests. Our data on performance is therefore statistically robust and credible. Banknote authentication. The data is extracted from images taken from real and fake banknote-like samples, and the Wavelet Transform tool was used to extract features from images. There are 1,372 articles and 5 attributes for binary classes. MAGIC Gamma Telescope, the data is generated to simulate the registration of high-energy gamma particles in a booted atmosphere."}, {"heading": "B. Evaluation Of Effectiveness", "text": "To evaluate the effectiveness of our training algorithms, we select three baselines, and the baselines and our model are listed below. 2) Supports vector machine (SVM) with functional core. It is implemented by Weka using the SMO algorithm, noted as SVM-RBF. This model is used to prove our effectiveness of the min-margin principle for abstraction processes. 3) AdaBoost, implemented by Weka, is listed as AdaBoost. This model is used. What we want to evaluate is how effective all four classifiers are in the various scalable training processes. For example, take 5% as derived training data from the dataset and other than test data."}, {"heading": "C. Evaluation Of Network Structure", "text": "To investigate the effects of the network structure, we are conducting experiments with the ISOLET dataset, with the proportion of suitable sparse training datasets being 3.33%. We are constructing the same network structure, but with a different training algorithm to learn. The result is shown in Fig. 4. In the experiment, different neural networks have different hidden neuron numbers, which means that denser networks have more hidden neuron numbers, in Fig. 4 is more x-axis, denser is the network, and the y-axis is the accuracy. For the results in Fig. 4, we can see some key points: 1) There is a suitable hidden number of neurons, which leads less to an inappropriate problem than to an overhaul of the problem. 2) In an overview of trends and values, our method outperforms ANN and also shares almost the same trend with ANN, which means that the hyperparameter of ANN could be transferred to our method, without the practical application of the ANN being changed to replace the structure."}, {"heading": "VI. CONCLUSION", "text": "In this paper, we formulate the margin-based principle into the training algorithms of Feed-Forward Neural Networks, and two types of processes are examined in detail: in the abstraction process, the min-margin principle is applied, and in the classification process, the max-margin principle. On this basis, we propose the training algorithm to solve the margin-based optimization problem. All theory is evaluated in real open data sets, and good results are achieved as expected. In summary, our model could handle more economically labeled data sets and high-dimensional data sets with high accuracy, while the modification from the old ANN method to our method is simple and virtually labor-free."}], "references": [{"title": "A simple weight decay can improve generalization", "author": ["J. Moody", "S. Hanson", "A. Krogh", "J.A. Hertz"], "venue": "Advances in neural information processing systems, vol. 4, pp. 950\u2013957, 1995.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 1995}, {"title": "A generalized growing and pruning rbf (ggap-rbf) neural network for function approximation", "author": ["G.-B. Huang", "P. Saratchandran", "N. Sundararajan"], "venue": "Neural Networks, IEEE Transactions on, vol. 16, no. 1, pp. 57\u201367, 2005.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2005}, {"title": "Learning deep architectures for ai", "author": ["Y. Bengio"], "venue": "Foundations and trends R  \u00a9 in Machine Learning, vol. 2, no. 1, pp. 1\u2013127, 2009.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2009}, {"title": "A unified architecture for natural language processing: Deep neural networks with multitask learning", "author": ["R. Collobert", "J. Weston"], "venue": "Proceedings of the 25th international conference on Machine learning, pp. 160\u2013167, ACM, 2008.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2008}, {"title": "Multi-column deep neural networks for image classification", "author": ["D. Ciresan", "U. Meier", "J. Schmidhuber"], "venue": "Computer Vision and Pattern Recognition (CVPR), 2012 IEEE Conference on, pp. 3642\u20133649, IEEE, 2012.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2012}, {"title": "A fast learning algorithm for deep belief nets", "author": ["G. Hinton", "S. Osindero", "Y.-W. Teh"], "venue": "Neural computation, vol. 18, no. 7, pp. 1527\u20131554, 2006.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2006}, {"title": "Greedy layer-wise training of deep networks", "author": ["Y. Bengio", "P. Lamblin", "D. Popovici", "H. Larochelle"], "venue": "Advances in neural information processing systems, vol. 19, p. 153, 2007.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2007}, {"title": "Understanding the difficulty of training deep feedforward neural networks", "author": ["X. Glorot", "Y. Bengio"], "venue": "International Conference on Artificial Intelligence and Statistics, pp. 249\u2013256, 2010.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2010}, {"title": "Exploring strategies for training deep neural networks", "author": ["H. Larochelle", "Y. Bengio", "J. Louradour", "P. Lamblin"], "venue": "The Journal of Machine Learning Research, vol. 10, pp. 1\u201340, 2009.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2009}, {"title": "Max-margin markov networks", "author": ["B.T.C.G.D. Roller"], "venue": "Advances in neural information processing systems, vol. 16, p. 25, 2004.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2004}, {"title": "Max-margin classification of data with absent features", "author": ["G. Chechik", "G. Heitz", "G. Elidan", "P. Abbeel", "D. Koller"], "venue": "The Journal of Machine Learning Research, vol. 9, pp. 1\u201321, 2008.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2008}, {"title": "Margin based feature selection-theory and algorithms", "author": ["R. Gilad-Bachrach", "A. Navot", "N. Tishby"], "venue": "Proceedings of the twenty-first international conference on Machine learning, p. 43, ACM, 2004.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2004}, {"title": "Support cluster machine", "author": ["B. Li", "M. Chi", "J. Fan", "X. Xue"], "venue": "Proceedings of the 24th international conference on Machine learning, pp. 505\u2013512.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 0}, {"title": "Online max-margin weight learning for markov logic networks", "author": ["T.N. Huynh", "R.J. Mooney"], "venue": "SDM, pp. 642\u2013651, 2011.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2011}, {"title": "Max-margin early event detectors", "author": ["M. Hoai", "F. De la Torre"], "venue": "International Journal of Computer Vision, vol. 107, no. 2, pp. 191\u2013202, 2014.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2014}], "referenceMentions": [{"referenceID": 0, "context": "[1] had studied the effect of weight decay and states that it has two effects, suppressing any irrelevant of the weights and improving the generalization ability.", "startOffset": 0, "endOffset": 3}, {"referenceID": 1, "context": "[2] had studied the pruning of RBF Neural Network, which firstly introduces the concept of significance of the hidden neurons and then uses it in the learning algorithm to realize parsimonious networks.", "startOffset": 0, "endOffset": 3}, {"referenceID": 2, "context": "Recently such stated in [3], going to deep catches many eyes, since not only just adding the hidden layers could gain an improvement in performance, but deep neural networks can also automatically select features and amazingly complete the comprehension missions.", "startOffset": 24, "endOffset": 27}, {"referenceID": 3, "context": "[4] had applied deep network into natural languages, and many works such as [5] had applied deep network into image processing, deep learning is one of the hottest topic in today\u2019s AI.", "startOffset": 0, "endOffset": 3}, {"referenceID": 4, "context": "[4] had applied deep network into natural languages, and many works such as [5] had applied deep network into image processing, deep learning is one of the hottest topic in today\u2019s AI.", "startOffset": 76, "endOffset": 79}, {"referenceID": 5, "context": "Before [6] and [7] proposed the fast unsupervised or supervised methods, multi-layer neural networks are hard to train, this kind of difficulty is analysed in [8] and [9], for the reason that the optimization process is often stuck into the local optima.", "startOffset": 7, "endOffset": 10}, {"referenceID": 6, "context": "Before [6] and [7] proposed the fast unsupervised or supervised methods, multi-layer neural networks are hard to train, this kind of difficulty is analysed in [8] and [9], for the reason that the optimization process is often stuck into the local optima.", "startOffset": 15, "endOffset": 18}, {"referenceID": 7, "context": "Before [6] and [7] proposed the fast unsupervised or supervised methods, multi-layer neural networks are hard to train, this kind of difficulty is analysed in [8] and [9], for the reason that the optimization process is often stuck into the local optima.", "startOffset": 159, "endOffset": 162}, {"referenceID": 8, "context": "Before [6] and [7] proposed the fast unsupervised or supervised methods, multi-layer neural networks are hard to train, this kind of difficulty is analysed in [8] and [9], for the reason that the optimization process is often stuck into the local optima.", "startOffset": 167, "endOffset": 170}, {"referenceID": 9, "context": "[10] had applied the Max-Margin Principle into the Markov Networks, and [11] had applied the Max-Margin Fig.", "startOffset": 0, "endOffset": 4}, {"referenceID": 10, "context": "[10] had applied the Max-Margin Principle into the Markov Networks, and [11] had applied the Max-Margin Fig.", "startOffset": 72, "endOffset": 76}, {"referenceID": 11, "context": "[12] had applied Margin-Based Principle into feature selection and [13] had applied Max-Margin Principle into Clustering.", "startOffset": 0, "endOffset": 4}, {"referenceID": 12, "context": "[12] had applied Margin-Based Principle into feature selection and [13] had applied Max-Margin Principle into Clustering.", "startOffset": 67, "endOffset": 71}, {"referenceID": 13, "context": "Recently, [14] had introduced this principle to on-line learning for Markov Logic Networks, and [15] had introduced it to early event detection.", "startOffset": 10, "endOffset": 14}, {"referenceID": 14, "context": "Recently, [14] had introduced this principle to on-line learning for Markov Logic Networks, and [15] had introduced it to early event detection.", "startOffset": 96, "endOffset": 100}], "year": 2015, "abstractText": "Margin-Based Principle has been proposed for a long time, it has been proved that this principle could reduce the structural risk and improve the performance in both theoretical and practical aspects. Meanwhile, feed-forward neural network is a traditional classifier, which is very hot at present with a deeper architecture. However, the training algorithm of feed-forward neural network is developed and generated from Widrow-Hoff Principle that means to minimize the squared error. In this paper, we propose a new training algorithm for feed-forward neural networks based on Margin-Based Principle, which could effectively promote the accuracy and generalization ability of neural network classifiers with less labelled samples and flexible network. We have conducted experiments on four UCI open datasets and achieved good results as expected. In conclusion, our model could handle more sparse labelled and more high-dimension dataset in a high accuracy while modification from old ANN method to our method is easy and almost free of work. Keywords\u2014Max-Margin Principle, Feed-Forward Neural Network, Classifier", "creator": "dvips(k) 5.991 Copyright 2011 Radical Eye Software"}}}