{"id": "1506.03879", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "12-Jun-2015", "title": "Leading Tree in DPCLUS and Its Impact on Building Hierarchies", "abstract": "This paper reveals the tree structure as an intermediate result of clustering by fast search and find of density peaks (DPCLUS), and explores the power of using this tree to perform hierarchical clustering. The array used to hold the index of the nearest higher-densitied object for each object can be transformed into a Leading Tree (LT), in which each parent node P leads its child nodes to join the same cluster as P itself, and the child nodes are sorted by their g values in descendant order to accelerate the disconnecting of root in each subtree. There are two major advantages with the LT: One is dramatically reducing the running time of assigning noncenter data points to their cluster ID, because the assigning process is turned into just disconnecting the links from each center to its parent. The other is that the tree model for representing clusters is more informative. Because we can check which objects are more likely to be selected as centers in finer grained clustering, or which objects reach to its center via less jumps. Experiment results and analysis show the effectiveness and efficiency of the assigning process with an LT.", "histories": [["v1", "Fri, 12 Jun 2015 00:37:54 GMT  (715kb)", "http://arxiv.org/abs/1506.03879v1", "11 Pages, 5 figures. It is a very fundamental topic with respect to the research (clustering by fast search and find of density peaks)"], ["v2", "Mon, 15 Jun 2015 00:38:53 GMT  (714kb)", "http://arxiv.org/abs/1506.03879v2", "11 Pages, 5 figures. It is a very fundamental topic with respect to the research (clustering by fast search and find of density peaks)"]], "COMMENTS": "11 Pages, 5 figures. It is a very fundamental topic with respect to the research (clustering by fast search and find of density peaks)", "reviews": [], "SUBJECTS": "cs.AI", "authors": ["ji xu", "guoyin wang"], "accepted": false, "id": "1506.03879"}, "pdf": {"name": "1506.03879.pdf", "metadata": {"source": "CRF", "title": "Leading Tree in DP_CLUS and Its Impact for Building Hierarchies", "authors": ["Ji Xu", "Guoyin Wang"], "emails": [], "sections": [{"heading": null, "text": "The array, in which the index of the nearest higher density object is stored for each object, can be converted into a Leading Tree (LT) in which each parent node P causes its child nodes to join the same cluster as P itself, and the child nodes are sorted by their g-values to speed up the separation of the root in each subtree. There are two main advantages of LT: One drastically reduces the runtime of assigning non-centric data to their cluster ID, as the assignment process is transformed into a mere separation of connections from each center to its parent tree; the other is that the tree model is more meaningful for representing clusters; because we can verify which objects are more likely to be selected as centers in finer-grained clusters, or which objects get to their center via fewer jumps."}, {"heading": "1. Introduction", "text": "In fact, it is so that most of us are able to surpass ourselves by putting ourselves in the centre of attention. (...) Indeed, it is so that they are able to put themselves in the centre of attention. (...) It is not so that they put themselves in the centre of attention. (...) It is as if they put themselves in the centre. (...) It is as if they put themselves in the centre. (...). \"(...).\" (.). \"(.).\" (.). \"(.).\" (.). \"(.).\" (.). \"(.).\" (. \"(.).\" (.). \"(.).\" (.). \"(.\" (.). \"(.).\" (.). (.). \"(.).\" (.). (.). \"(.).\" (.). \"(.\" (.). \"(.).\" (.). \"(.\" (.). \"(.).\" (.). \"(.). (.).\" (.). \"(.).\" (.). \"(.).\" (.). \"(.).\" (.). \"(.).\" (.).). (. \"(.).\" (.). \"(.).\" (.).). (. \"(.).\" (.). \"(.). (.).\" (.). (.). (. (.).). (.). (.). (.). (. (.). (.).). (.). (. (.). (.). (.). (.).). (.). (.). (. (.). (.).). (.). (.). (.). (. (.).).). (.). (.). (.). (.). (.). (.).). (.). (.). (.). (.). (.).). (.).).). ("}, {"heading": "2. Preliminaries", "text": "The proposed method is essentially based on [4], so here we give a brief introduction to their idea and algorithm. First, the authors started from the intuitive assumption that centers, no matter what the shape of clusters looks like, are always surrounded by lower density non-central data points and the distance between two centers is relatively long. Then, two simple metrics, local density (referred to as \u03c1) and minimum distance to higher density data points (referred to as \u03b4), are used to do the cluster job. Notations used to describe the algorithms in DP _ CLUS and our method are listed in Table 1.TABLE 1: Notations in DP _ CLUSDP _ CLUS take as input the distance matrix of a given dataset and perform the following steps: (1) Compute 1 2 {,...,} N via cut-off kernel:, () i i i i i-j I-d - d, where 1, 0; jort (jj) = 0.1, or (0.jij-1)."}, {"heading": "3. Leading Tree as an Intermediate Result of DP_CLUS", "text": "The assignment process in DP _ CLUS is based on two arrays: Nn and Q. Q keeps the indices of the data points sorted according to their \u03c1 values in descending order. All data points are assigned to the clusters in descending order, referring to Nn. See Algorithm 1. Algorithm 1. AssignDP _ CLUS [4] Input: Nn and Q Output: Cluster label for each object in the form of an array cl Step 1. Initialize all elements in cl with -1; Step 2. Label each center with a cluster ID; Step 3. For each Qi in Q doif cl [qi] = = -1cl [qi]: = Nn [qi]; End ifEnd forStep 4. return cl; The computational complexity for algorithm 1 is 2 \u00d7 N, where N is the number of objects."}, {"heading": "3.1 Constructing Leading Tree", "text": "The LT of a give distance matrix can be constructed by directly transforming the Nn array and another array called GammaSortInds. Since Nn specifies its direct leading parent P for each object, it is easy to find out which child nodes led by P. By adding the corresponding child nodes to all parent nodes, an LT is constructed. See Algorithm 2. Note that the child nodes of each parent are added in descending order, so that when a newly updated center is separated from its parent, we simply remove the links from the parent to its first child. This speeds up the construction of the LT. See Algorithm 2. Algorithm 2. Transform the Nn into an LT input: Nn and SortedGammaInds Output: An LT in the form of the adjacent list AL Step 1. Initializes the adjacent list for each object."}, {"heading": "3.2 Using Leading Tree to build clustering Hierarchy", "text": "From the angle of averages, hierarchical clustering can be regarded as a series of flat clusters that takes different groups of objects as their centers. In this section, therefore, we will discuss only the method of building the hierarchy of clusters with an LT and each given set of centers at a corresponding level. How to select the centers to form a layer in the hierarchy is beyond the scope of this paper. With an LT, the assignment process is transformed into a mere separation of the m-1 associations from each center to its parent. See algorithm 3. Split the LT Input: An LT in the form of an adjacence list AL, Nn and an arrangement of m centers C sorted by gamma value in descending order Output: A forest to represent the clustering result Step 1. For i = 2 to mroot = C [i]; parentID = Nneigh [root]; AL [parentID].Remocenter First (2.) Forcenter (The AL is return; The AL is 3."}, {"heading": "3.3 Example", "text": "We sampled the longitude and latitude of 13 cities in northern China to form an illustrative dataset (see Fig. 1) to determine the process and result of using LT to represent the intermediate result of CP _ CLUS and to cluster data points based on the constructed LT. Using DB _ CLUS algorithm, the intermediate results Nn, Q and SortGammaInd for DS1 and the final result cl for DS1 are calculated as shown in Table 2. (Fig. 2a), and the result of clustering using {13, 6, 11} as centers is presented in the form of a forest (Fig. 2b).132123 11064875 119648113212310119 (a) (b) Fig. Unlike the intermediate results of DS1 in the form of an LT, (b) taking into account the sequences, the LT is used for DS1."}, {"heading": "4. Experiments", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.1 Datasets and running settings", "text": "The experiments are carried out on a personal computer with Intel i5-2430M CPU, 8G RAM, Windows 7 64bit OS and Eclipse programming environment with JDK 1.7. We test our method on 3 sets of data: two of which are synthetic and one real data set from the UCI Machine Learning Repository. The first set (5Spherical) is generated by placing centers of five spheres and randomly scanning points on the surface of the spheres and then projecting the points onto the plane (see Fig. 4a). 5Spherical is designed in such a way that the entire data set may be grouped into five, four or two groups; the second set (5Spiral) is 5 spiral curves using the function (4): / 8 cos () / 8 cos () / 8 sin () x t t y t t t t t t t t t t t t t t t t t t t t t t t t t t."}, {"heading": "4.2 Results and discussion", "text": "The runtime of assigning objects to a cluster with Nn, the division of the LTs to obtain the clusters, and the construction of the LTs is shown in Figure 5.The runtime of splitting an LT into a forest TSplitLT is always much shorter than the assignment with Nn (referred to as TAssign), but the cost is the time for constructing the LT (TConstrLT). If the number of potential layers in the Nl hierarchy is small like the data sets we are testing, then TConsLT + Nl \u00d7 TSplitLT > Nl \u00b7 TAssign. However, if Nl is large enough to meet Equation 5, then using an LT to perform hierarchical clustering saves computational time. ConsLTAssign SplitT NlT T T T T T T T T (5) In addition, users can consider the differences between objects within the object located in the center of the cluster as the most explained by us."}, {"heading": "5. Conclusion", "text": "In this paper, we reveal the hidden tree structure of the intermediate result in DP _ CLUS and develop the algorithms to construct the Leading Tree (LT) and perform hierarchical clustering with a number of center groups with the LT. Both theoretical analysis and experimental results show that the LT approach is much more efficient at assigning non-center objects in DP _ CLUS than the original Nn approach, and the clusters presented with trees can provide more information about the potential of non-center objects to be selected in the future and how many jumps a particular object needs to reach its center. This discovery not only has the potential to accelerate assignment processing during hierarchical clustering with DP _ CLUS, but also provides us with a deeper understanding of the result structure of DP _ CLUS."}, {"heading": "Acknowledgement", "text": "This work is supported in part by the National Science and Technology Major Project (NO.2014ZX07104-006), the National Natural Science Foundation of China under grant numbers 61272060 and 61472056, and the Natural Science Foundation Key Project of Chongqing of P. R. China under grant number CSTC2013jjB40003."}], "references": [{"title": "Knowledge-Based Clustering: From Data to Information Granules", "author": ["W. Pedrycz"], "venue": null, "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2005}, {"title": "The magical number seven, plus or minus two: some limits on our capacity for processing information", "author": ["G.A. Miller"], "venue": "Psychological review, vol. 63, no. 2, p. 81, 1956.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 1956}, {"title": "Clustering by fast search and find of density peaks", "author": ["A. Rodriguez", "A. Laio"], "venue": "Science, vol. 344, no. 6191, pp. 1492\u20131496, 2014.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2014}, {"title": "Demixed principal component analysis of population activity in higher cortical areas reveals independent representation of task parameters", "author": ["D. Kobak", "W. Brendel", "C. Constantinidis", "C.E. Feierstein", "A. Kepecs", "Z.F. Mainen", "R. Romo", "X.-L. Qi", "N. Uchida", "C.K. Machens"], "venue": "arXiv preprint arXiv:1410.6031, 2014.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2014}, {"title": "Exemplar component analysis: A fast band selection method for hyperspectral imagery", "author": ["K. Sun", "X. Geng", "L. Ji"], "venue": "Geoscience and Remote Sensing Letters, IEEE, vol. 12, no. 5, pp. 998\u20131002, 2015.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2015}, {"title": "An efficient algorithm to perform local concerted movements of a chain molecule", "author": ["S. Zamuner", "A. Rodriguez", "F. Seno", "A. Trovato"], "venue": "PloS one, vol. 10, no. 3, p. e0118342, 2015.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2015}, {"title": "Both piston-like and rotational motions are present in bacterial chemoreceptor signaling", "author": ["D. Yu", "X. Ma", "Y. Tu", "L. Lai"], "venue": "Scientific reports, vol. 5, 2015.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2015}, {"title": "A new method to estimate ages of facial image for large database", "author": ["Y.-W. Chen", "D.-H. Lai", "H. Qi", "J.-L. Wang", "J.-X. Du"], "venue": "Multimedia Tools and Applications, pp. 1\u201319, 2015.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2015}, {"title": "Finding your food soulmate", "author": ["A. Sy", "L. Liu", "B. Villanueva"], "venue": "2014. http://web.stanford.edu/ angelasy/ data/food soulmate paper.pdf.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2014}, {"title": "Clustering assisted fundamental matrix estimation", "author": ["H. Wu", "Y. Wan"], "venue": "arXiv preprint arXiv:1504.03409, 2015.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2015}, {"title": "High-speed multiparameter photophysical analyses of fluorophore libraries", "author": ["K.M. Dean", "L.M. Davis", "J.L. Lubbeck", "P. Manna", "P. Friis", "A.E. Palmer", "R. Jimenez"], "venue": "Analytical chemistry, 2015.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2015}, {"title": "Clustering sentences with density peaks for multi-document summarization", "author": ["Y. Zhang", "Y. Xia", "Y. Liu", "P. IMSL", "W. Wang"], "venue": "Human Language Technologies: The 2015 Annual Conference of the North American Chapter of the ACL, pp. 1262\u20131267, June 2015.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2015}, {"title": "High density-focused uncertainty sampling for active learning over evolving stream data", "author": ["D. Ienco", "B. Pfahringer", "I. Zliobaite"], "venue": "Proceedings of the 3rd International Workshop on Big Data, Streams and Heterogeneous  Source Mining: Algorithms, Systems, Programming Models and Applications, pp. 133\u2013148, 2014.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2014}, {"title": "A synthetic minority oversampling method based on local densities in low-dimensional space for imbalanced learning", "author": ["Z. Xie", "L. Jiang", "T. Ye", "X. Li"], "venue": "Database Systems for Advanced Applications, pp. 3\u201318, Springer, 2015.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2015}, {"title": "A generalized affinity propagation clustering algorithm for nonspherical cluster discovery", "author": ["T. Qiu", "Y. Li"], "venue": "arXiv preprint arXiv: 1501.04318, 2015.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2015}, {"title": "Solving the inverse ising problem by mean-field methods in a clustered phase space with many states", "author": ["A. Decelle", "F. Ricci-Tersenghi"], "venue": "arXiv preprint arXiv:1501.03034, 2015.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2015}, {"title": "K-means clustering based on density for scene image classification", "author": ["K. Xie", "J. Wu", "W. Yang", "C. Sun"], "venue": "Proceedings of the 2015 Chinese Intelligent Automation Conference, pp. 379\u2013386, Springer, 2015.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2015}, {"title": "Comment on\u201d clustering by fast search and find of density peaks", "author": ["S. Wang", "D. Wang", "C. Li", "Y. Li"], "venue": "arXiv preprint arXiv: 1501.04267, 2015.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2015}, {"title": "Extended fast search clustering algorithm: widely density clusters, no density peaks", "author": ["W. Zhang", "J. Li"], "venue": "arXiv preprint arXiv: 1505.05610, 2015.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2015}, {"title": "Efficient cluster detection by ordered neighborhoods", "author": ["E. Aksehirli", "E. M \u0308 uller", "B. Goethals"], "venue": "http://win.ua.ac.be/ adrem/bibrem/pubs/clon.pdf.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 0}, {"title": "Spike sorting for large, dense electrode arrays", "author": ["C. Rossant", "S.N. Kadir", "D.F. Goodman", "J. Schulman", "M. Belluscio", "G. Buzsaki", "K.D. Harris"], "venue": "bioRxiv, p. 015198, 2015.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2015}, {"title": "An improved community detection method in massive social networks", "author": ["Y. Yao", "B. Li", "L. Peng", "Z. Liu"], "venue": "2015 International Conference on Automation, Mechanical Control and Computational Engineering, Atlantis Press, 2015.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2015}], "referenceMentions": [{"referenceID": 0, "context": "Clustering is a general methodology and a rich conceptual and algorithmic framework for data analysis and interpretation, which gathers the objects into groups [1].", "startOffset": 160, "endOffset": 163}, {"referenceID": 1, "context": "Because according to Miller's \"seven plus or minus two\" theory, there are the 7 objects in the span of attention, and the 7 digits in the span of immediate memory [2].", "startOffset": 163, "endOffset": 166}, {"referenceID": 2, "context": "DP_CLUS is a flat clustering method able to efficiently and accurately cluster datasets of any shape with the aid of defining two simple measures: local density \u03c1 and the distance to the nearest neighbor with higher density \u03b4[4].", "startOffset": 225, "endOffset": 228}, {"referenceID": 3, "context": "The majority of the citing works directly adopted DP_CLUS to the problems from specific domain, such as neuroscience [5], geoscience and remote sensing [6], molecular biology [7], computational biophysics [8], age estimation in image processing [9], finding a food soulmate [10], fundamental matrix estimation in computer vision [11], analytic chemistry [12], clustering Sentences for multi-document Summarization [13], and so forth.", "startOffset": 117, "endOffset": 120}, {"referenceID": 4, "context": "The majority of the citing works directly adopted DP_CLUS to the problems from specific domain, such as neuroscience [5], geoscience and remote sensing [6], molecular biology [7], computational biophysics [8], age estimation in image processing [9], finding a food soulmate [10], fundamental matrix estimation in computer vision [11], analytic chemistry [12], clustering Sentences for multi-document Summarization [13], and so forth.", "startOffset": 152, "endOffset": 155}, {"referenceID": 5, "context": "The majority of the citing works directly adopted DP_CLUS to the problems from specific domain, such as neuroscience [5], geoscience and remote sensing [6], molecular biology [7], computational biophysics [8], age estimation in image processing [9], finding a food soulmate [10], fundamental matrix estimation in computer vision [11], analytic chemistry [12], clustering Sentences for multi-document Summarization [13], and so forth.", "startOffset": 175, "endOffset": 178}, {"referenceID": 6, "context": "The majority of the citing works directly adopted DP_CLUS to the problems from specific domain, such as neuroscience [5], geoscience and remote sensing [6], molecular biology [7], computational biophysics [8], age estimation in image processing [9], finding a food soulmate [10], fundamental matrix estimation in computer vision [11], analytic chemistry [12], clustering Sentences for multi-document Summarization [13], and so forth.", "startOffset": 205, "endOffset": 208}, {"referenceID": 7, "context": "The majority of the citing works directly adopted DP_CLUS to the problems from specific domain, such as neuroscience [5], geoscience and remote sensing [6], molecular biology [7], computational biophysics [8], age estimation in image processing [9], finding a food soulmate [10], fundamental matrix estimation in computer vision [11], analytic chemistry [12], clustering Sentences for multi-document Summarization [13], and so forth.", "startOffset": 245, "endOffset": 248}, {"referenceID": 8, "context": "The majority of the citing works directly adopted DP_CLUS to the problems from specific domain, such as neuroscience [5], geoscience and remote sensing [6], molecular biology [7], computational biophysics [8], age estimation in image processing [9], finding a food soulmate [10], fundamental matrix estimation in computer vision [11], analytic chemistry [12], clustering Sentences for multi-document Summarization [13], and so forth.", "startOffset": 274, "endOffset": 278}, {"referenceID": 9, "context": "The majority of the citing works directly adopted DP_CLUS to the problems from specific domain, such as neuroscience [5], geoscience and remote sensing [6], molecular biology [7], computational biophysics [8], age estimation in image processing [9], finding a food soulmate [10], fundamental matrix estimation in computer vision [11], analytic chemistry [12], clustering Sentences for multi-document Summarization [13], and so forth.", "startOffset": 329, "endOffset": 333}, {"referenceID": 10, "context": "The majority of the citing works directly adopted DP_CLUS to the problems from specific domain, such as neuroscience [5], geoscience and remote sensing [6], molecular biology [7], computational biophysics [8], age estimation in image processing [9], finding a food soulmate [10], fundamental matrix estimation in computer vision [11], analytic chemistry [12], clustering Sentences for multi-document Summarization [13], and so forth.", "startOffset": 354, "endOffset": 358}, {"referenceID": 11, "context": "The majority of the citing works directly adopted DP_CLUS to the problems from specific domain, such as neuroscience [5], geoscience and remote sensing [6], molecular biology [7], computational biophysics [8], age estimation in image processing [9], finding a food soulmate [10], fundamental matrix estimation in computer vision [11], analytic chemistry [12], clustering Sentences for multi-document Summarization [13], and so forth.", "startOffset": 414, "endOffset": 418}, {"referenceID": 12, "context": "Among the remaining part, some ensemble DP_CLUS with other methods to deal with streaming data [14] and imbalanced dataset oversampling [15], to find nonspherical clusters [16], to resolve inverse Ising problem [17], to classify scene image combined with k means [18], etc.", "startOffset": 95, "endOffset": 99}, {"referenceID": 13, "context": "Among the remaining part, some ensemble DP_CLUS with other methods to deal with streaming data [14] and imbalanced dataset oversampling [15], to find nonspherical clusters [16], to resolve inverse Ising problem [17], to classify scene image combined with k means [18], etc.", "startOffset": 136, "endOffset": 140}, {"referenceID": 14, "context": "Among the remaining part, some ensemble DP_CLUS with other methods to deal with streaming data [14] and imbalanced dataset oversampling [15], to find nonspherical clusters [16], to resolve inverse Ising problem [17], to classify scene image combined with k means [18], etc.", "startOffset": 172, "endOffset": 176}, {"referenceID": 15, "context": "Among the remaining part, some ensemble DP_CLUS with other methods to deal with streaming data [14] and imbalanced dataset oversampling [15], to find nonspherical clusters [16], to resolve inverse Ising problem [17], to classify scene image combined with k means [18], etc.", "startOffset": 211, "endOffset": 215}, {"referenceID": 16, "context": "Among the remaining part, some ensemble DP_CLUS with other methods to deal with streaming data [14] and imbalanced dataset oversampling [15], to find nonspherical clusters [16], to resolve inverse Ising problem [17], to classify scene image combined with k means [18], etc.", "startOffset": 263, "endOffset": 267}, {"referenceID": 17, "context": "; Wang discussed its parameter the cut off distance dc setting [19]; Zhang extended it to cluster the datasets without density peaks [20].", "startOffset": 63, "endOffset": 67}, {"referenceID": 18, "context": "; Wang discussed its parameter the cut off distance dc setting [19]; Zhang extended it to cluster the datasets without density peaks [20].", "startOffset": 133, "endOffset": 137}, {"referenceID": 19, "context": "There are some works only used its partial idea, such as neighbor to build a k-nearest neighborhood to explore the clusters in high dimension and large scale datasets [21]; or just mentioned it as a clustering method [22, 23].", "startOffset": 167, "endOffset": 171}, {"referenceID": 20, "context": "There are some works only used its partial idea, such as neighbor to build a k-nearest neighborhood to explore the clusters in high dimension and large scale datasets [21]; or just mentioned it as a clustering method [22, 23].", "startOffset": 217, "endOffset": 225}, {"referenceID": 21, "context": "There are some works only used its partial idea, such as neighbor to build a k-nearest neighborhood to explore the clusters in high dimension and large scale datasets [21]; or just mentioned it as a clustering method [22, 23].", "startOffset": 217, "endOffset": 225}, {"referenceID": 2, "context": "The method proposed is mainly based on [4], so we give a brief introduction to its idea and the algorithm here.", "startOffset": 39, "endOffset": 42}, {"referenceID": 2, "context": "AssignDP_CLUS [4]", "startOffset": 14, "endOffset": 17}], "year": 2015, "abstractText": "This paper reveals the tree structure as an intermediate result of \u201dclustering by fast search and find of density peaks\u201d (DP CLUS), and explores the power of using this tree to perform hierarchical clustering. The array used to hold the index of the nearest higher-densitied object for each object can be transformed into a Leading Tree (LT), in which each parent node P leads its child nodes to join the same cluster as P itself, and the child nodes are sorted by their g values in descendant order to accelerate the disconnecting of root in each subtree. There are two major advantages with the LT: One is dramatically reducing the running time of assigning noncenter data points to their cluster ID, because the assigning process is turned into just disconnecting the links from each center to its parent. The other is that the tree model for representing clusters is more informative. Because we can check which objects are more likely to be selected as centers in finer grained clustering, or which objects reach to its center via less jumps. Experiment results and analysis show the effectiveness and high efficiency of the assigning process with an LT.", "creator": "Microsoft\u00ae Word 2010"}}}