{"id": "1506.04334", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "13-Jun-2015", "title": "A Bayesian Model for Generative Transition-based Dependency Parsing", "abstract": "We propose a simple, scalable, fully generative model for transition-based dependency parsing with high accuracy. The model, parameterized by Hierarchical Pitman-Yor Processes, overcomes the limitations of previous generative models by allowing fast and accurate inference. We propose an efficient decoding algorithm based on particle filtering that can adapt the beam size to the uncertainty in the model while jointly predicting POS tags and parse trees. The UAS of the parser is on par with that of a greedy discriminative baseline. As a language model, it obtains better perplexity than a n-gram model by performing semi-supervised learning over a large unlabelled corpus. We show that the model is able to generate locally and syntactically coherent sentences, opening the door to further applications in language generation.", "histories": [["v1", "Sat, 13 Jun 2015 23:39:09 GMT  (26kb,D)", "https://arxiv.org/abs/1506.04334v1", "10 pages"], ["v2", "Sun, 28 Jun 2015 21:18:50 GMT  (29kb,D)", "http://arxiv.org/abs/1506.04334v2", "Depling 2015"]], "COMMENTS": "10 pages", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["jan buys", "phil blunsom"], "accepted": false, "id": "1506.04334"}, "pdf": {"name": "1506.04334.pdf", "metadata": {"source": "CRF", "title": "A Bayesian Model for Generative Transition-based Dependency Parsing", "authors": ["Jan Buys", "Phil Blunsom"], "emails": ["jan.buys@cs.ox.ac.uk", "phil.blunsom@cs.ox.ac.uk"], "sections": [{"heading": "1 Introduction", "text": "In this paper, we aim to transfer the benefits of the transition-based parsing method to generative dependence. While generative models have been widely and successfully used for constitutional learning (Collins, 1997; Petrov et al., 2006), their use in dependency models is limited. Generative models offer a principled approach to semi- and superimposed learning. Generative models have been widely and successfully used for constitutional learning (Collins, 1997; Petrov et al., 2006), their use in dependency saving. Generative models offer a principled approach to semi- and superimposed learning."}, {"heading": "2 Generative Transition-based Parsing", "text": "Our parsing model is based on transformation-based projective dependencies parsing with the arcstandard \u03b2 strategy (Nivre and Scholz, 2004). Parsing is limited to (labeled) projective trees. An arc (i, l, j) \"A\" encodes a dependency between two words, where i is the head node, j is the dependency, and l is the dependence on j. In our generative model, a word can be represented by its lexical (word) configuration and / or its POS tag. We add a root node to the beginning of the sentence (although it could also be added at the end of the sentence), so that the header of the sentence is the dependency of the root node. A parser configuration (\u03b2, A) for sentence s consists of a stack dependency on indices in s, an index \u03b2 to the next word to be generated, and a set of arcs A. The stack elements are called deficiencies."}, {"heading": "3 Probabilistic Generative Model", "text": "Our model defines a common probability distribution over an analyzed set of POS tags t1: n, words w1: n, and a transition sequence a1: 2n asp (t1: n, w1: n, a1: 2n) = n, i = 1 (p (ti | htmi) p (wi | ti, h w mi) mi + 1, j = mi + 1 p (aj | haj)), where mi is the number of transitions performed when (ti, wi) was created, and ht, hw, and ha are sequences that represent the conditional contexts for the tags, word, and transition distributions. In the generative process, a shift transition is followed by a sequence of 0 or more that decreases the transitions, which is repeated until all words are generated and a terminal configuration of the parser is achieved."}, {"heading": "3.1 Hierarchical Pitman-Yor processes", "text": "The probability distributions for predicting words, markers, and transitions are prefixed by hierarchical Pitmar-Yor PPs = Chinese PPs = 1 sequence table (HPYP). HPYP models were originally proposed for n-gram language modeling \u2212 and were applied to various NLP customer tasks. A version of the approximate inference in the HPYP model recovers from interpolated Kneser-Ney distribution formulas (Kneser and Ney, 1995), one of the best preforming n-gram language models. The Pitman-Yor process (PYP) is a generalization of the dirichlet process that defines a distribution over a probability space \u2212 P \u2212 with discount parameters 0 \u2264 d < 1, Strength parameters and base distribution B. PYP priors encode the distribution of power-law distribution found in the distribution of words from the sampling of the Chinese restaurant analogy."}, {"heading": "4 Decoding", "text": "In the standard particle filtering analysis (see algorithm 1), an upper limit is set for the size of the partially controlled derivative capacities (see also: \"There is only a limited number of transitions.\") However, in our model, we cannot compare the derivatives with the same number of transitions, which differ in the number of words. Another option is to decrease the beam each time after the next word, if the number of transitions that can be performed between the individual words is reduced, but this approach leads to a decoding of the complexity. Another option is to decrease the beam each time after the next word - but the number of reduced transitions that can be performed between shifts, so that the decoding of the complexity remains."}, {"heading": "5 Experiments", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "5.1 Parsing Setup", "text": "We evaluate our model as a parser based on the standard setup of the English Penn Treebank (Marcus et al., 1993), train on the basis of WSJ sections 02-21, develop on the basis of section 22, and test on the basis of section 23. We also evaluate on the basis of the Stanford dependency representation (De Marneffe and Manning, 2008) (SD) 2. Words that occur only once in the training data are treated as unknown words. We classify unknown words by capital letters, numbers, punctuation, and frequent suffixes in classes similar to those used in the implementation of generative constituency parsers, such as the Stanford parser (Klein and Manning, 2003).As a discriminatory baseline, we use MaltParser (Nivre et al., 2006), a discriminatory, linear-based parameter-P."}, {"heading": "5.2 Modelling Choices", "text": "This year, it will only be once before such a process takes place."}, {"heading": "5.3 Parsing Results", "text": "Test results comparing our model with existing discriminatory and generative dependency savers can be found in Table 5. Our HPYP model is much better than Eisner's generative model and the Bayesian version of this model proposed by Wallach et al. (2008) (the result for Eis-ner's model is, as reported by Wallach et al. (2008) on the WSJ). The accuracy of our model is only 0.8 UAS below the generative model proposed by Titov and Henderson (2007), although this model is much more powerful. The Titov-Henderson model takes three days to train, and its decoding speed is about 1 set per second. The UAS of our model is very close to the MaltParser model. However, we point out that the performance of our model on LAS is relatively worse than on the UAS. One explanation for this is that we do not include labels in the conditioning context contexts, and the predicted labels are independent of the 8S, the 8S have not yet been generated."}, {"heading": "5.4 Language Modelling", "text": "Next, we evaluate our model as a language model. First, we use standard WSJ modeling to cover sections 00-20, development to 21-22, and testing to 23-24."}, {"heading": "5.5 Generation", "text": "In order to support our claim that our generative model is a good model for sentences, we are generating some examples. The samples given here were generated by generating 1000 samples and selecting the 10 points with the highest score, with a length of more than 10. The models are trained on the standard WSJ training set (including punctuation).The examples are in Table 7. The quality of the sentences generated by the dependency model is superior to that of the n-gram model, although the models have similar test confusions.The sentences generated by the dependency model tend to have a more global syntactic structure (for examples that have verbs where expected), while maintaining the local coherence of n-gram models. The dependency model was also able to generate balanced quotes."}, {"heading": "6 Related work", "text": "One of the earliest graph-based dependency models (Eisner, 1996) is generative, with the probability of addicts estimated on the basis of their head and previously generated siblings. Backoff and smoothing are performed to counteract the sparseness in the conditioning context of distributions, but the first successful model was the Value Dependency Model (DMV) (Klein and Manning, 2004). Several enhancements have been proposed for this model, for example the use of structural annaeling models (Smith and Eisner, 2006), Viterbi EM training (Spitkovsky et al, 2010) or richer contexts (Blunsom and Cohn, 2010). However, these models are not powerful enough for accurate parsing or speech modeling with rich contexts."}, {"heading": "7 Conclusion", "text": "We introduced a generative dependency parsing model that maintains most of the speed and accuracy of discriminatory parsers, unlike previous models; our models can accurately estimate probabilities based on long context sequences; the model is scalable for large training and test sets; and although it defines a complete probability distribution across sentences and parses, the decoding speed is efficient; and the generative model performs strongly as a language model; and for future work, we believe that this model can be successfully applied to natural language generation tasks such as machine translation."}], "references": [{"title": "Particle Markov chain Monte Carlo methods", "author": ["Christophe Andrieu", "Arnaud Doucet", "Roman Holenstein."], "venue": "Journal of the Royal Statistical Society: Series B (Statistical Methodology), 72(3):269\u2013342.", "citeRegEx": "Andrieu et al\\.,? 2010", "shortCiteRegEx": "Andrieu et al\\.", "year": 2010}, {"title": "Unsupervised induction of tree substitution grammars for dependency parsing", "author": ["Phil Blunsom", "Trevor Cohn."], "venue": "EMNLP, pages 1204\u20131213.", "citeRegEx": "Blunsom and Cohn.,? 2010", "shortCiteRegEx": "Blunsom and Cohn.", "year": 2010}, {"title": "A note on the implementation of hierarchical Dirichlet processes", "author": ["Phil Blunsom", "Trevor Cohn", "Sharon Goldwater", "Mark Johnson."], "venue": "ACL/IJCNLP (Short Papers), pages 337\u2013340.", "citeRegEx": "Blunsom et al\\.,? 2009", "shortCiteRegEx": "Blunsom et al\\.", "year": 2009}, {"title": "A transitionbased system for joint part-of-speech tagging and labeled non-projective dependency parsing", "author": ["Bernd Bohnet", "Joakim Nivre."], "venue": "EMNLP-CoNLL, pages 1455\u20131465.", "citeRegEx": "Bohnet and Nivre.,? 2012", "shortCiteRegEx": "Bohnet and Nivre.", "year": 2012}, {"title": "Syntax-based language models for statistical machine translation", "author": ["Eugene Charniak", "Kevin Knight", "Kenji Yamada."], "venue": "Proceedings of MT Summit IX, pages 40\u201346.", "citeRegEx": "Charniak et al\\.,? 2003", "shortCiteRegEx": "Charniak et al\\.", "year": 2003}, {"title": "Immediate-head parsing for language models", "author": ["Eugene Charniak."], "venue": "Proceedings of ACL, pages 124\u2013131.", "citeRegEx": "Charniak.,? 2001", "shortCiteRegEx": "Charniak.", "year": 2001}, {"title": "Structured language modeling", "author": ["Ciprian Chelba", "Frederick Jelinek."], "venue": "Computer Speech & Language, 14(4):283\u2013332.", "citeRegEx": "Chelba and Jelinek.,? 2000", "shortCiteRegEx": "Chelba and Jelinek.", "year": 2000}, {"title": "A fast and accurate dependency parser using neural networks", "author": ["Danqi Chen", "Christopher D Manning."], "venue": "EMNLP.", "citeRegEx": "Chen and Manning.,? 2014", "shortCiteRegEx": "Chen and Manning.", "year": 2014}, {"title": "Transition-based dependency parsing with selectional branching", "author": ["Jinho D. Choi", "Andrew McCallum."], "venue": "ACL.", "citeRegEx": "Choi and McCallum.,? 2013", "shortCiteRegEx": "Choi and McCallum.", "year": 2013}, {"title": "Exact inference for generative probabilistic non-projective dependency parsing", "author": ["Shay B. Cohen", "Carlos G\u00f3mez-Rodr\u0131\u0301guez", "Giorgio Satta"], "venue": "In EMNLP,", "citeRegEx": "Cohen et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Cohen et al\\.", "year": 2011}, {"title": "Three generative, lexicalised models for statistical parsing", "author": ["Michael Collins."], "venue": "ACL, pages 16\u201323.", "citeRegEx": "Collins.,? 1997", "shortCiteRegEx": "Collins.", "year": 1997}, {"title": "The Stanford typed dependencies representation", "author": ["Marie-Catherine De Marneffe", "Christopher D Manning."], "venue": "Coling 2008: Proceedings of the workshop on Cross-Framework and Cross-Domain Parser Evaluation, pages 1\u20138.", "citeRegEx": "Marneffe and Manning.,? 2008", "shortCiteRegEx": "Marneffe and Manning.", "year": 2008}, {"title": "Sequential Monte Carlo methods in practice", "author": ["Arnaud Doucet", "Nando De Freitas", "Neil Gordon."], "venue": "Springer.", "citeRegEx": "Doucet et al\\.,? 2001", "shortCiteRegEx": "Doucet et al\\.", "year": 2001}, {"title": "Transitionbased dependency parsing with stack long shortterm memory", "author": ["Chris Dyer", "Miguel Ballesteros", "Wang Ling", "Austin Matthews", "Noah A. Smith."], "venue": "Proceedings of ACL 2015.", "citeRegEx": "Dyer et al\\.,? 2015", "shortCiteRegEx": "Dyer et al\\.", "year": 2015}, {"title": "Three new probabilistic models for dependency parsing: An exploration", "author": ["Jason Eisner."], "venue": "COLING, pages 340\u2013345.", "citeRegEx": "Eisner.,? 1996", "shortCiteRegEx": "Eisner.", "year": 1996}, {"title": "A neural syntactic language model", "author": ["Ahmad Emami", "Frederick Jelinek."], "venue": "Machine Learning, 60(13):195\u2013227.", "citeRegEx": "Emami and Jelinek.,? 2005", "shortCiteRegEx": "Emami and Jelinek.", "year": 2005}, {"title": "Training deterministic parsers with non-deterministic oracles", "author": ["Yoav Goldberg", "Joakim Nivre."], "venue": "TACL, 1:403\u2013414.", "citeRegEx": "Goldberg and Nivre.,? 2013", "shortCiteRegEx": "Goldberg and Nivre.", "year": 2013}, {"title": "Dynamic programming for linear-time incremental parsing", "author": ["Liang Huang", "Kenji Sagae."], "venue": "ACL, pages 1077\u20131086.", "citeRegEx": "Huang and Sagae.,? 2010", "shortCiteRegEx": "Huang and Sagae.", "year": 2010}, {"title": "Accurate unlexicalized parsing", "author": ["Dan Klein", "Christopher D. Manning."], "venue": "ACL, pages 423\u2013430.", "citeRegEx": "Klein and Manning.,? 2003", "shortCiteRegEx": "Klein and Manning.", "year": 2003}, {"title": "Corpusbased induction of syntactic structure: Models of dependency and constituency", "author": ["Dan Klein", "Christopher D Manning."], "venue": "ACL, pages 478\u2013586.", "citeRegEx": "Klein and Manning.,? 2004", "shortCiteRegEx": "Klein and Manning.", "year": 2004}, {"title": "Improved backing-off for m-gram language modeling", "author": ["Reinhard Kneser", "Hermann Ney."], "venue": "ICASSP, volume 1, pages 181\u2013184. IEEE.", "citeRegEx": "Kneser and Ney.,? 1995", "shortCiteRegEx": "Kneser and Ney.", "year": 1995}, {"title": "Efficient thirdorder dependency parsers", "author": ["Terry Koo", "Michael Collins."], "venue": "ACL, pages 1\u201311.", "citeRegEx": "Koo and Collins.,? 2010", "shortCiteRegEx": "Koo and Collins.", "year": 2010}, {"title": "The insideoutside recursive neural network model for dependency parsing", "author": ["Phong Le", "Willem Zuidema."], "venue": "EMNLP, pages 729\u2013739.", "citeRegEx": "Le and Zuidema.,? 2014", "shortCiteRegEx": "Le and Zuidema.", "year": 2014}, {"title": "Low-rank tensors for scoring dependency structures", "author": ["Tao Lei", "Yu Xin", "Yuan Zhang", "Regina Barzilay", "Tommi Jaakkola."], "venue": "Proceedings of ACL (Volume 1: Long Papers), pages 1381\u20131391.", "citeRegEx": "Lei et al\\.,? 2014", "shortCiteRegEx": "Lei et al\\.", "year": 2014}, {"title": "Modeling the effects of memory on human online sentence processing with particle filters", "author": ["Roger P Levy", "Florencia Reali", "Thomas L Griffiths."], "venue": "Advances in neural information processing systems, pages 937\u2013944.", "citeRegEx": "Levy et al\\.,? 2009", "shortCiteRegEx": "Levy et al\\.", "year": 2009}, {"title": "Building a large annotated corpus of English: The Penn Treebank", "author": ["Mitchell P. Marcus", "Beatrice Santorini", "Mary Ann Marcinkiewicz."], "venue": "Computational Linguistics, 19(2):313\u2013330.", "citeRegEx": "Marcus et al\\.,? 1993", "shortCiteRegEx": "Marcus et al\\.", "year": 1993}, {"title": "Online large-margin training of dependency parsers", "author": ["Ryan T. McDonald", "Koby Crammer", "Fernando C.N. Pereira."], "venue": "ACL.", "citeRegEx": "McDonald et al\\.,? 2005", "shortCiteRegEx": "McDonald et al\\.", "year": 2005}, {"title": "Multi-source transfer of delexicalized dependency parsers", "author": ["Ryan McDonald", "Slav Petrov", "Keith Hall."], "venue": "EMNLP, pages 62\u201372. Association for Computational Linguistics.", "citeRegEx": "McDonald et al\\.,? 2011", "shortCiteRegEx": "McDonald et al\\.", "year": 2011}, {"title": "Deterministic dependency parsing of English text", "author": ["Joakim Nivre", "Mario Scholz."], "venue": "COLING.", "citeRegEx": "Nivre and Scholz.,? 2004", "shortCiteRegEx": "Nivre and Scholz.", "year": 2004}, {"title": "Maltparser: A data-driven parser-generator for dependency parsing", "author": ["Joakim Nivre", "Johan Hall", "Jens Nilsson."], "venue": "Proceedings of LREC, volume 6, pages 2216\u20132219.", "citeRegEx": "Nivre et al\\.,? 2006", "shortCiteRegEx": "Nivre et al\\.", "year": 2006}, {"title": "Algorithms for deterministic incremental dependency parsing", "author": ["Joakim Nivre."], "venue": "Computational Linguistics, 34(4):513\u2013553.", "citeRegEx": "Nivre.,? 2008", "shortCiteRegEx": "Nivre.", "year": 2008}, {"title": "Learning accurate, compact, and interpretable tree annotation", "author": ["Slav Petrov", "Leon Barrett", "Romain Thibaux", "Dan Klein."], "venue": "COLING-ACL, pages 433\u2013440.", "citeRegEx": "Petrov et al\\.,? 2006", "shortCiteRegEx": "Petrov et al\\.", "year": 2006}, {"title": "Efficient structured language modeling for speech recognition", "author": ["Ariya Rastrow", "Mark Dredze", "Sanjeev Khudanpur."], "venue": "INTERSPEECH.", "citeRegEx": "Rastrow et al\\.,? 2012", "shortCiteRegEx": "Rastrow et al\\.", "year": 2012}, {"title": "Probabilistic top-down parsing and language modeling", "author": ["Brian Roark."], "venue": "Computational linguistics, 27(2):249\u2013276.", "citeRegEx": "Roark.,? 2001", "shortCiteRegEx": "Roark.", "year": 2001}, {"title": "Annealing structural bias in multilingual weighted grammar induction", "author": ["Noah A. Smith", "Jason Eisner."], "venue": "Proceedings of COLING-ACL, pages 569\u2013576.", "citeRegEx": "Smith and Eisner.,? 2006", "shortCiteRegEx": "Smith and Eisner.", "year": 2006}, {"title": "Viterbi training improves unsupervised dependency parsing", "author": ["Valentin I. Spitkovsky", "Hiyan Alshawi", "Daniel Jurafsky", "Christopher D. Manning."], "venue": "CoNLL, pages 9\u201317.", "citeRegEx": "Spitkovsky et al\\.,? 2010", "shortCiteRegEx": "Spitkovsky et al\\.", "year": 2010}, {"title": "A hierarchical Bayesian language model based on Pitman-Yor processes", "author": ["Yee Whye Teh."], "venue": "ACL.", "citeRegEx": "Teh.,? 2006", "shortCiteRegEx": "Teh.", "year": 2006}, {"title": "A latent variable model for generative dependency parsing", "author": ["Ivan Titov", "James Henderson."], "venue": "Proceedings of the Tenth International Conference on Parsing Technologies, pages 144\u2013155.", "citeRegEx": "Titov and Henderson.,? 2007", "shortCiteRegEx": "Titov and Henderson.", "year": 2007}, {"title": "Feature-rich part-ofspeech tagging with a cyclic dependency network", "author": ["Kristina Toutanova", "Dan Klein", "Christopher D. Manning", "Yoram Singer."], "venue": "HLT-NAACL, pages 173\u2013180.", "citeRegEx": "Toutanova et al\\.,? 2003", "shortCiteRegEx": "Toutanova et al\\.", "year": 2003}, {"title": "Bayesian modeling of dependency trees using hierarchical Pitman-Yor priors", "author": ["Hanna M Wallach", "Charles Sutton", "Andrew McCallum."], "venue": "ICML Workshop on Prior Knowledge for Text and Language Processing.", "citeRegEx": "Wallach et al\\.,? 2008", "shortCiteRegEx": "Wallach et al\\.", "year": 2008}, {"title": "Structured training for neural network transition-based parsing", "author": ["David Weiss", "Chris Alberti", "Michael Collins", "Slav Petrov."], "venue": "Proceedings of ACL 2015.", "citeRegEx": "Weiss et al\\.,? 2015", "shortCiteRegEx": "Weiss et al\\.", "year": 2015}, {"title": "Statistical dependency analysis with support vector machines", "author": ["Hiroyasu Yamada", "Yuji Matsumoto."], "venue": "Proceedings of IWPT.", "citeRegEx": "Yamada and Matsumoto.,? 2003", "shortCiteRegEx": "Yamada and Matsumoto.", "year": 2003}, {"title": "A tale of two parsers: Investigating and combining graphbased and transition-based dependency parsing", "author": ["Yue Zhang", "Stephen Clark."], "venue": "EMNLP, pages 562\u2013571.", "citeRegEx": "Zhang and Clark.,? 2008", "shortCiteRegEx": "Zhang and Clark.", "year": 2008}, {"title": "Transition-based dependency parsing with rich non-local features", "author": ["Yue Zhang", "Joakim Nivre."], "venue": "ACL-HLT short papers-Volume 2, pages 188\u2013193.", "citeRegEx": "Zhang and Nivre.,? 2011", "shortCiteRegEx": "Zhang and Nivre.", "year": 2011}], "referenceMentions": [{"referenceID": 30, "context": "Transition-based dependency parsing algorithms that perform greedy local inference have proven to be very successful at fast and accurate discriminative parsing (Nivre, 2008; Zhang and Nivre, 2011; Chen and Manning, 2014).", "startOffset": 161, "endOffset": 221}, {"referenceID": 43, "context": "Transition-based dependency parsing algorithms that perform greedy local inference have proven to be very successful at fast and accurate discriminative parsing (Nivre, 2008; Zhang and Nivre, 2011; Chen and Manning, 2014).", "startOffset": 161, "endOffset": 221}, {"referenceID": 7, "context": "Transition-based dependency parsing algorithms that perform greedy local inference have proven to be very successful at fast and accurate discriminative parsing (Nivre, 2008; Zhang and Nivre, 2011; Chen and Manning, 2014).", "startOffset": 161, "endOffset": 221}, {"referenceID": 42, "context": "Beam-search decoding further improves performance (Zhang and Clark, 2008; Huang and Sagae, 2010; Choi and McCallum, 2013), but increases decoding time.", "startOffset": 50, "endOffset": 121}, {"referenceID": 17, "context": "Beam-search decoding further improves performance (Zhang and Clark, 2008; Huang and Sagae, 2010; Choi and McCallum, 2013), but increases decoding time.", "startOffset": 50, "endOffset": 121}, {"referenceID": 8, "context": "Beam-search decoding further improves performance (Zhang and Clark, 2008; Huang and Sagae, 2010; Choi and McCallum, 2013), but increases decoding time.", "startOffset": 50, "endOffset": 121}, {"referenceID": 26, "context": "Graphbased parsers (McDonald et al., 2005; Koo and Collins, 2010; Lei et al., 2014) perform global inference and although they are more accurate in some cases, inference tends to be slower.", "startOffset": 19, "endOffset": 83}, {"referenceID": 21, "context": "Graphbased parsers (McDonald et al., 2005; Koo and Collins, 2010; Lei et al., 2014) perform global inference and although they are more accurate in some cases, inference tends to be slower.", "startOffset": 19, "endOffset": 83}, {"referenceID": 23, "context": "Graphbased parsers (McDonald et al., 2005; Koo and Collins, 2010; Lei et al., 2014) perform global inference and although they are more accurate in some cases, inference tends to be slower.", "startOffset": 19, "endOffset": 83}, {"referenceID": 10, "context": "While generative models have been used widely and successfully for constituency parsing (Collins, 1997; Petrov et al., 2006), their use in dependency parsing has been limited.", "startOffset": 88, "endOffset": 124}, {"referenceID": 31, "context": "While generative models have been used widely and successfully for constituency parsing (Collins, 1997; Petrov et al., 2006), their use in dependency parsing has been limited.", "startOffset": 88, "endOffset": 124}, {"referenceID": 19, "context": "Dependency grammar induction models (Klein and Manning, 2004; Blunsom and Cohn, 2010) are generative, but not expressive enough for high-accuracy parsing.", "startOffset": 36, "endOffset": 85}, {"referenceID": 1, "context": "Dependency grammar induction models (Klein and Manning, 2004; Blunsom and Cohn, 2010) are generative, but not expressive enough for high-accuracy parsing.", "startOffset": 36, "endOffset": 85}, {"referenceID": 37, "context": "A previous generative transition-based dependency parser (Titov and Henderson, 2007) obtains competitive accuracies, but training and decoding is computationally very expensive.", "startOffset": 57, "endOffset": 84}, {"referenceID": 6, "context": "Syntactic language models have also been shown to improve performance in speech recognition and machine translation (Chelba and Jelinek, 2000; Charniak et al., 2003).", "startOffset": 116, "endOffset": 165}, {"referenceID": 4, "context": "Syntactic language models have also been shown to improve performance in speech recognition and machine translation (Chelba and Jelinek, 2000; Charniak et al., 2003).", "startOffset": 116, "endOffset": 165}, {"referenceID": 36, "context": "The model, parameterized by Hierarchical Pitman-Yor Processes (HPYPs) (Teh, 2006), learns a distribution over derivations of parser transitions, words and POS tags (\u00a73).", "startOffset": 70, "endOffset": 81}, {"referenceID": 12, "context": "The algorithm is based on particle filtering (Doucet et al., 2001), a method for sequential Monte Carlo sampling.", "startOffset": 45, "endOffset": 66}, {"referenceID": 28, "context": "Our parsing model is based on transition-based projective dependency parsing with the arcstandard parsing strategy (Nivre and Scholz, 2004).", "startOffset": 115, "endOffset": 139}, {"referenceID": 34, "context": "As a generative model it assigns probabilities to sentences and dependency trees: A word w (including its POS tag) is generated when it is shifted on to the stack, similar to the generative models proposed by Titov and Henderson (2007) and Cohen et al.", "startOffset": 209, "endOffset": 236}, {"referenceID": 8, "context": "As a generative model it assigns probabilities to sentences and dependency trees: A word w (including its POS tag) is generated when it is shifted on to the stack, similar to the generative models proposed by Titov and Henderson (2007) and Cohen et al. (2011), and the joint tagging and parsing model of Bohnet and Nivre (2012).", "startOffset": 240, "endOffset": 260}, {"referenceID": 3, "context": "(2011), and the joint tagging and parsing model of Bohnet and Nivre (2012). The types of transitions in this model are shift (sh), left-arc (la) and right-arc (ra):", "startOffset": 51, "endOffset": 75}, {"referenceID": 16, "context": "In contrast to dynamic oracles (Goldberg and Nivre, 2013), we are only interested in derivations of the correct parse tree, so the oracle can assume that given c there exists a derivation for G.", "startOffset": 31, "endOffset": 57}, {"referenceID": 36, "context": "HPYP models were originally proposed for n-gram language modelling (Teh, 2006), and have been applied to various NLP tasks.", "startOffset": 67, "endOffset": 78}, {"referenceID": 20, "context": "A version of approximate inference in the HPYP model recovers interpolated Kneser-Ney smoothing (Kneser and Ney, 1995), one of the best preforming n-gram language models.", "startOffset": 96, "endOffset": 118}, {"referenceID": 2, "context": "In our implementation we use the efficient data structures proposed by Blunsom et al. (2009). In addition to sampling the seating arrangement, the discount and strength parameters are also sampled, using slice sampling.", "startOffset": 71, "endOffset": 93}, {"referenceID": 42, "context": "In the standard approach to beam search for transition-based parsing (Zhang and Clark, 2008), the beam stores partial derivations with the same number of transitions performed, and the lowestscoring ones are removed when the size of the beam exceeds a set threshold.", "startOffset": 69, "endOffset": 92}, {"referenceID": 8, "context": "The selectional branching method proposed by Choi and McCallum (2013) for discriminative beam-search parsing has a similar goal.", "startOffset": 45, "endOffset": 70}, {"referenceID": 25, "context": "We evaluate our model as a parser on the standard English Penn Treebank (Marcus et al., 1993) setup, training on WSJ sections 02-21, developing on section 22, and testing on section 23.", "startOffset": 72, "endOffset": 93}, {"referenceID": 24, "context": "We evaluate our model as a parser on the standard English Penn Treebank (Marcus et al., 1993) setup, training on WSJ sections 02-21, developing on section 22, and testing on section 23. We use the head-finding rules of Yamada and Matsumoto (2003) (YM)1 for constituencyto-dependency conversion, to enable comparison with previous results.", "startOffset": 73, "endOffset": 247}, {"referenceID": 18, "context": "We classify unknown words according to capitalization, numbers, punctuation and common suffixes into classes similar to those used in the implementation of generative constituency parsers such as the Stanford parser (Klein and Manning, 2003).", "startOffset": 216, "endOffset": 241}, {"referenceID": 29, "context": "As a discriminative baseline we use MaltParser (Nivre et al., 2006), a discriminative, greedy transition-based parser, performing arcstandard parsing with LibLinear as classifier.", "startOffset": 47, "endOffset": 67}, {"referenceID": 38, "context": "We compare predicting tags against using gold standard POS tags and tags obtain using the Stanford POS tagger3 (Toutanova et al., 2003).", "startOffset": 111, "endOffset": 135}, {"referenceID": 3, "context": "Bohnet and Nivre (2012) found that joint prediction increases both POS and parsing accuracy.", "startOffset": 0, "endOffset": 24}, {"referenceID": 27, "context": "Unlexicalised parsing is also considered to be robust for applications such as crosslingual parsing (McDonald et al., 2011).", "startOffset": 100, "endOffset": 123}, {"referenceID": 29, "context": "41 Zhang and Nivre (2011) 92.", "startOffset": 13, "endOffset": 26}, {"referenceID": 8, "context": "8 Choi and McCallum (2013) 92.", "startOffset": 2, "endOffset": 27}, {"referenceID": 37, "context": "Titov and Henderson (2007) was retrained to enable direct comparison.", "startOffset": 0, "endOffset": 27}, {"referenceID": 14, "context": "Our HPYP model performs much better than Eisner\u2019s generative model as well as the Bayesian version of that model proposed by Wallach et al. (2008) (the result for Eis-", "startOffset": 41, "endOffset": 147}, {"referenceID": 38, "context": "ner\u2019s model is given as reported by Wallach et al. (2008) on the WSJ).", "startOffset": 36, "endOffset": 58}, {"referenceID": 37, "context": "8 UAS below the generative model of Titov and Henderson (2007), despite that model being much more powerful.", "startOffset": 36, "endOffset": 63}, {"referenceID": 43, "context": "Despite these promising results, our model\u2019s performance still lags behind recent discriminative parsers (Zhang and Nivre, 2011; Choi and McCallum, 2013) with beam-search and richer feature sets than can be incorporated in our model.", "startOffset": 105, "endOffset": 153}, {"referenceID": 8, "context": "Despite these promising results, our model\u2019s performance still lags behind recent discriminative parsers (Zhang and Nivre, 2011; Choi and McCallum, 2013) with beam-search and richer feature sets than can be incorporated in our model.", "startOffset": 105, "endOffset": 153}, {"referenceID": 13, "context": "Recently proposed neural networks for dependency parsers have further improved performance (Dyer et al., 2015; Weiss et al., 2015), reaching up to 94.", "startOffset": 91, "endOffset": 130}, {"referenceID": 40, "context": "Recently proposed neural networks for dependency parsers have further improved performance (Dyer et al., 2015; Weiss et al., 2015), reaching up to 94.", "startOffset": 91, "endOffset": 130}, {"referenceID": 8, "context": "Despite these promising results, our model\u2019s performance still lags behind recent discriminative parsers (Zhang and Nivre, 2011; Choi and McCallum, 2013) with beam-search and richer feature sets than can be incorporated in our model. In terms of speed, Zhang and Nivre (2011) parse 29 sentences per second, against the 110 sentences per second of Choi and McCallum (2013).", "startOffset": 129, "endOffset": 276}, {"referenceID": 8, "context": "Despite these promising results, our model\u2019s performance still lags behind recent discriminative parsers (Zhang and Nivre, 2011; Choi and McCallum, 2013) with beam-search and richer feature sets than can be incorporated in our model. In terms of speed, Zhang and Nivre (2011) parse 29 sentences per second, against the 110 sentences per second of Choi and McCallum (2013). Recently proposed neural networks for dependency parsers have further improved performance (Dyer et al.", "startOffset": 129, "endOffset": 372}, {"referenceID": 0, "context": "In this unsupervised stage we train the model with particle Gibbs sampling (Andrieu et al., 2010), using a particle filter to sample parse trees.", "startOffset": 75, "endOffset": 97}, {"referenceID": 6, "context": "22 Chelba and Jelinek (2000) 146.", "startOffset": 3, "endOffset": 29}, {"referenceID": 6, "context": "22 Chelba and Jelinek (2000) 146.1 Emami and Jelinek (2005) 131.", "startOffset": 3, "endOffset": 60}, {"referenceID": 14, "context": "One of the earliest graph-based dependency parsing models (Eisner, 1996) is generative, estimating the probability of dependents given their head and previously generated siblings.", "startOffset": 58, "endOffset": 72}, {"referenceID": 14, "context": "One of the earliest graph-based dependency parsing models (Eisner, 1996) is generative, estimating the probability of dependents given their head and previously generated siblings. To counter sparsity in the conditioning context of the distributions, backoff and smoothing are performed. Wallach et al. (2008) proposed a Bayesian HPYP parameterisation of this model.", "startOffset": 59, "endOffset": 310}, {"referenceID": 19, "context": "The first successful model was the dependency model with valence (DMV) (Klein and Manning, 2004).", "startOffset": 71, "endOffset": 96}, {"referenceID": 34, "context": "Several extensions have been proposed for this model, for example using structural annaeling (Smith and Eisner, 2006), Viterbi EM training (Spitkovsky et al.", "startOffset": 93, "endOffset": 117}, {"referenceID": 35, "context": "Several extensions have been proposed for this model, for example using structural annaeling (Smith and Eisner, 2006), Viterbi EM training (Spitkovsky et al., 2010) or richer contexts (Blunsom and Cohn, 2010).", "startOffset": 139, "endOffset": 164}, {"referenceID": 1, "context": ", 2010) or richer contexts (Blunsom and Cohn, 2010).", "startOffset": 27, "endOffset": 51}, {"referenceID": 33, "context": "Lexicalised PCFGs applied to language modelling (Roark, 2001; Charniak, 2001) show improvements over n-gram models, but decoding is prohibitively expensive for practical integration in language generation applications.", "startOffset": 48, "endOffset": 77}, {"referenceID": 5, "context": "Lexicalised PCFGs applied to language modelling (Roark, 2001; Charniak, 2001) show improvements over n-gram models, but decoding is prohibitively expensive for practical integration in language generation applications.", "startOffset": 48, "endOffset": 77}, {"referenceID": 9, "context": "A generative transition-based parsing model for non-projective parsing is proposed in (Cohen et al., 2011), along with a dynamic program for inference.", "startOffset": 86, "endOffset": 106}, {"referenceID": 24, "context": "Finally, incremental parsing with particle filtering has been proposed previously (Levy et al., 2009) to model human online sentence processing.", "startOffset": 82, "endOffset": 101}], "year": 2015, "abstractText": "We propose a simple, scalable, fully generative model for transition-based dependency parsing with high accuracy. The model, parameterized by Hierarchical Pitman-Yor Processes, overcomes the limitations of previous generative models by allowing fast and accurate inference. We propose an efficient decoding algorithm based on particle filtering that can adapt the beam size to the uncertainty in the model while jointly predicting POS tags and parse trees. The UAS of the parser is on par with that of a greedy discriminative baseline. As a language model, it obtains better perplexity than a n-gram model by performing semi-supervised learning over a large unlabelled corpus. We show that the model is able to generate locally and syntactically coherent sentences, opening the door to further applications in language generation.", "creator": "TeX"}}}