{"id": "1506.04364", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "14-Jun-2015", "title": "Localized Multiple Kernel Learning---A Convex Approach", "abstract": "We propose a localized approach to multiple kernel learning that, in contrast to prevalent approaches, can be formulated as a convex optimization problem over a given cluster structure. From which we obtain the first generalization error bounds for localized multiple kernel learning and derive an efficient optimization algorithm based on the Fenchel dual representation. Experiments on real-world datasets from the application domains of computational biology and computer vision show that the convex approach to localized multiple kernel learning can achieve higher prediction accuracies than its global and non-convex local counterparts.", "histories": [["v1", "Sun, 14 Jun 2015 09:11:13 GMT  (102kb,D)", "https://arxiv.org/abs/1506.04364v1", null], ["v2", "Thu, 13 Oct 2016 00:54:24 GMT  (107kb,D)", "http://arxiv.org/abs/1506.04364v2", "to appear in ACML 2016"]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["yunwen lei", "alexander binder", "\\\"ur\\\"un dogan", "marius kloft"], "accepted": false, "id": "1506.04364"}, "pdf": {"name": "1506.04364.pdf", "metadata": {"source": "CRF", "title": "Localized Multiple Kernel Learning\u2014A Convex Approach", "authors": ["Yunwen Lei", "Alexander Binder", "\u00dcr\u00fcn Dogan", "Marius Kloft"], "emails": ["yunwelei@cityu.edu.hk", "alexander_binder@sutd.edu.sg", "udogan@microsoft.com", "kloft@hu-berlin.de"], "sections": [{"heading": null, "text": "Keywords: multiple kernel learning, localized algorithms, generalization analysis"}, {"heading": "1 Introduction", "text": "This year it has come to the point that it will be able to put itself at the top, \"he said in an interview with the German Press Agency.\" We have never hesitated so long, \"he said.\" But we are not yet in a position to be able to. \""}, {"heading": "1.1 Related Work", "text": "In fact, most of them are able to keep to the rules that they have imposed on themselves, and they are able to keep to the rules that they have imposed on themselves. (...) It is not that they are keeping to the rules. (...) It is not that they are keeping to the rules. (...) It is that they are keeping to the rules. (...) It is that they are keeping to the rules. (...) It is that they are keeping to the rules. (...) It is that they are keeping to the rules. (...) It is that they are keeping to the rules. (...) It is that they are keeping to the rules. (...) It is that they are keeping to the rules. (...)"}, {"heading": "2 Convex Localized Multiple Kernel Learning", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "2.1 Problem setting and notation", "text": "Suppose we obtained n training samples (x1, y1),.., (xn, yn), which are divided into l disjunct clusters S1,..., Sl in a probable way, which means that for each cluster Sj we have a function cj: X \u2192 [0, 1], which indicates the probability that x falls into cluster j, i.e., for all x-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y"}, {"heading": "2.2 Proposed convex localized MKL method", "text": "Using the above notation, the proposed convex localized MKL model can be formulated as follows: Problem 1 (CONVEX LOCALIZED MULTIPLE KERNEL LEARNING (CLMKL) - PRIMAL) Let C > 0 and p \u2265 1. In case of a loss function '(t, y): R \u00d7 Y \u2192 R convex w.r.t. the first argument and cluster probability function cj: X \u2192 [0, 1], j-Nl, solveinf w, t, \u03b2, b-j, j-Nl-NM, w (m) j-22 2\u03b2jm + C-i-Nn' (ti, yi) s.t. \u03b2jm."}, {"heading": "2.3 Dualization", "text": "In this section, we derive a dual representation of problem 1. We look at two levels of the problem: a partially dualized problem (with fixed core weights) and the fully dualized problem with respect to all primary variables occurring. From the former, we derive an efficient two-stage optimization program (Section 3), the latter allowing us to calculate the duality gap and thus obtain a solid stop condition for the proposed algorithm. We focus on the fully dualized problem here. Partial dualization will be on the supplemental material C.Dual CLMKL optimization problem Forwj = (w (1) j,., w (M) j) j, we define the \"2, p-norm by solving the problem.\""}, {"heading": "2.4 Representer Theorem", "text": "We can use the above derivative to obtain a lower limit for the optimal value of the primary optimization problem (P), from which we can calculate the duality gap using the theorem below, as proved in supplementary material A.2.Theorem 3 (REPRESENTER THEOREM). For each dual variable (\u03b1i) ni = 1 in (D), the optimal primary variable {w (m) j (\u03b1)} l, M j, m = 1 in the Lagrangian Saddle Problem (3) asw (m) j (\u03b1) = [.m] j (\u03b1) = [.m]."}, {"heading": "2.5 Support-Vector Classification", "text": "For hinge loss, the fennel legendary conjugate becomes \"\u0445 (t, y) = ty (a function of t), if \u2212 1 \u2264 t y \u2264 0 and \u221e otherwise. Consequently, for each i the term\" \u0445 (\u2212 \u03b1iC, yi) decreases to \u2212 \u03b1i Cyi, provided that 0 \u2264 \u03b1iyi \u2264 C. With a variable replacement of the form \u03b1newi = \u03b1i yi, the complete dual problem (D) decreases as follows. Problem 4 (CLMKL - SVM FORMULATION). For hinge loss, the dual CLMKL problem (D) is given by: sup \u03b1: 0 \u2264 \u03b1 \u2264 C, \u2211 i-Nn \u03b1iyi = 0 \u2212 1 2 \u0445 j-Nl-Nl-Nl-Nl-Nl-Nl-Nl-Nl-Nl-Nl-Nl-Nl-Nl-Nl-Nl-Ne (2-Nxi-2) (1-Nm-Nl-Nl)."}, {"heading": "3 Optimization Algorithms", "text": "As a pioneer in the world of financial markets, we have here a two-layered optimization problem to solve the problem (P), in which the variables are divided into two groups: the group of core weights (P) and the group of weight vectors (P). (M) and the group of weight vectors (M). (S). (S). (S). (S). (S). (S). (S). (S). (S). (S). (S). (S). (S). (S). (S). (S). (S). (S). (S). (S). (S). (S). (S). (S). (S). (S). (S). (S). (S). (S). (S). (S). (S. (S). (S). (S. (S). (S). (S. (S). (S). (S). (S. (S). (S). (S). (S. (S). (S). (S). (S. (S). (S). (S. (S). (S). (S. (S). (S). (S. (S). (S). (S. (S). (S. (S). (S. (S). (S). (S). (S. (S). (S. (S). (S. (S). (S. (S). (S). (S. (S. (S). (S). (S. (S. (S). (S. (S). (S). (S). (S. (S.). (S.). (S.). (S. (S. (S). (S. (S. (S). (S). (S). (S). (S. (S. (S). (S. (S). (S). (S). (S. (S. (S). (S. (S). ("}, {"heading": "3.1 Convergence Analysis of the Algorithm", "text": "The following theorem, proven in supplementary material A.4, shows the convergence of algorithm 1. The basic idea is to consider algorithm 1 as an example of the classical block coordinate descend method (BCD), the convergence of which is well understandable.Theorem 6 (CONVERGENCE ANALYSIS OF ALGORITHM 1). Suppose that (B1) the characteristic chart inspm (x) has finite dimensions, i.e., inspm (x) inspiterate \u03b2jm, em < \u043c, \u0435\u043c\u0438\u043c\u0438\u043d\u043d\u0430 (B2) the loss function \"convex, continuous w.r.t.\" (0, y) < \u0432\u0438\u0441y \u0432Y (B3) each iteration passed through algorithm 1 \u03b2jm > 0 (B4) the SVM calculation in line 4 of the algorithm 1 is passed through exactly one point of the iteration."}, {"heading": "3.2 Runtime Complexity Analysis", "text": "For each iteration of the training phase, we need O (n2Ml) operations to calculate the kernel (6) operations, O (n2ns) operations to solve a standard SVM problem, O (Mln2s) operations to calculate the standard according to the representation (8), and O (Ml) operations to update the core weights. Thus, the computing cost for each iteration is O (n2Ml). The time complexity in the test phase is O (ntnsMl)."}, {"heading": "4 Generalization Error Bounds", "text": "In this section, we introduce generalization error limits for our approach: We give a purely data-dependent response to the generalization error achieved with the help of the Rademacher complexity theory (3). To begin with our basic strategy, we must convert the optimal dependence (7) into (P) in order to achieve an equivalent paraphrase (P) as a block-norm problem (3). (m). (m). (1). (.). (.). (.). (.). (.). (.). (.). (.). (.). (.). (.). (.). (.). (.). (.). (.). (.). (.). (.). (. (.). (.). (. (.). (.). (. (.). (. (.). (. (.). (. (.). (. (.). (. (.). (. (.). (. (.). (. (.). (. (.). (. (.). (.). (. (.). (.). (. (.). (.). (. (.). (.). (. (.). (.). (. (. (.). (.). (.). (. (.). (.). (. (.). (.). (. (.). (.). (. (.). (. (.). (. (.). (.). (. (.). (. (.). (.). (. (.). (.). (. (. (. (.).). (.). (. (. (. (.). (. (.). (.). (. (.). (. (. (.). (.). (. (.). (. (.). (.). (.).). (. (.). (. (. (. (.). (. (.).). (. (.).)."}, {"heading": "5 Empirical Analysis and Applications", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "5.1 Experimental Setup", "text": "We implement the proposed convex localized MKL (CLMKL) algorithm in MATLAB and solve the associated canonical SVM problem with LIBSVM [8]. The clusters {S1,.., Sl} are therefore calculated by kernel k-means [e.g., 12], but in principle we require other clustering methods (including convex methods such as Hocking et al. [21]). To further reduce the potential fluctuations of k-means (which are caused by random initialization of cluster means), we repeat kernel k-means t-times, and select the one with minimal clustering error (the sum of the squared distance between the examples and the associated cluster) as the final partition {S1,., Sl}. To uniformly set the parameters we set in (11), we introduce notationAE."}, {"heading": "5.2 Splice Site Recognition", "text": "Our first experiment aims to detect splices in the organism Caenorhabditis elegans, which is an important task in computer-aided gene discovery as splices located on the DNA strand directly on the border with exons (which code for proteins) and introns (which do not). We experiment with the mkl splice data set, which we splice from http: / / mldata.org / repository / data / viewslug / mkl-splice. It comprises 1000 splice instances and 20 weighted nuclei with grades ranging from 1 to 20 [4]. The experimental setup for this experiment is as follows: We create random splits of this data set into the training set, validation and test set, traversed with the size of the training set {50, 200, 300, 300, 800}. We apply kernel with uniform kernel means to create a partition."}, {"heading": "5.3 Transcription Start Site Detection", "text": "We are experimenting with the TSS dataset downloaded from http: / / mldata.org / repository / data / viewslug / tss /. This dataset, which is included in the larger study of [46], contains 5 cores. SVM, which is based on the uniform combination of these 5 cores, has the highest overall performance among 19 promoter prediction programs [1]. It therefore represents a strong baseline. To agree with previous studies [1, 24, 46] we are using the area under the ROC curve (AUC) as an evaluation criterion. We are looking at the same experimental setup as in the splice detection experiment. The gating function and partition are calculated using the TSS kernel, which contains the most discriminatory information."}, {"heading": "5.4 Protein Fold Prediction", "text": "Predicting protein folding is an important step in understanding the function of proteins, since the folding class of a protein is closely linked to its function; therefore, it is crucial for drug design. We are experimenting with the Ding and Dubchak protein folding predictive dataset [13], which was also used in Campbell and Ying [7], Kloft [24], Kloft and Blanchard [25]. This dataset consists of 27 folding classes of 311 proteins used for training and 383 proteins for testing purposes. We are using exactly the same 12 nuclei as in Campbell and Ying [7], Kloft [24], Kloft and Blanchard [25], which reflect different characteristics, such as van der Waal's volume, polarity and hydrophobicity. We are replicating the experimental setup of previous experiments by Campbell and Ying [7], Kloft [24], Kloft and Blanchard [25], which reflect different characteristics, such as volumes, polarity and hydrophobicity."}, {"heading": "5.5 Visual Image Categorization\u2014UIUC Sports", "text": "We are experimenting with the UIUC Sports Event dataset [35], consisting of 1574 images assigned to 8 image classes of sports activities. We calculate 9 \u03c72 cores based on SIFT characteristics and global color histograms, which are described in detail in Supplemental Material E.2, where we also provide background information on the experimental setup. From the results shown in Table 4, we find that CLMKL achieves a performance improvement of 0.26% over the \"p-standard MKL Baseline, while localized MKL undercuts the MKL Baseline as in G\u00f6nen and Alpaydin [14]."}, {"heading": "5.6 Execution Time Experiments", "text": "To demonstrate the efficiency of the proposed implementation, we compare the training time for UNIF, LMKL, 'p standard MKL, HLMKL and CLMKL on the TSS dataset. We specify the regularization parameter."}, {"heading": "6 Conclusions", "text": "Localized approaches to learning multiple cores allow for flexible distribution of kernel weights across the input area, which can be a great advantage when samples require different kernel importance. As we show in this paper, this can be the case in image recognition and multiple computer-aided biological applications. However, in this paper we propose a theoretical approach to localized MKL, which consists of two successive steps: 1. Clustering of training instances and 2. Calculation of kernel weights for each cluster by a single convex optimization problem. For this, we derive an efficient optimization algorithm based on the fennel duality consisting of two successive steps: 1. Clustering of training instances and 2. Calculation of kernel weights for each cluster by a single convex optimization problem."}, {"heading": "Acknowledgments", "text": "YL acknowledges support from the Joint Research Scheme of NSFC and RGC [RGC Project No. N _ CityU120 / 14 and NSFC Project No. 11461161006]. AB acknowledges support from Singapore University of Technology and Design Startup Grant SRIS15105. MK acknowledges support from the German Research Foundation (DFG) with KL 2698 / 2-1 and from the Federal Ministry of Science and Education (BMBF) with 031L0023A and 031B0187B.Supplemental Material"}, {"heading": "A Lemmata and Proofs", "text": "(1)......................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................"}, {"heading": "B Support Vector Regression Formulation of CLMKL", "text": "For the -insensitive loss \"(t, y \u2212 \u2212 \u2212 i) = [| y \u2212 t \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 i] +, which stands for all a + = max (a, 0), we have a\" + \"(\u2212 \u03b1iC, yi) = \u2212 1 C\u03b1iyi + | \u03b1i C | if | \u03b1i | \u2264 C and vice versa [20]. Therefore, the complete dual problem (D) reduces the gathering \u03b1 \u2212 1 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 (xi) M m = 1 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 (m)."}, {"heading": "C Primal and Dual CLMKL Problem Given Fixed Kernel Weights", "text": "In this case, it is not the case that we would be able to solve the problems. (...) In this case, it is the case that we cannot solve the problems. (...) In this case, it is the case that we cannot solve the problems. (...) In this case, it is the case that we cannot solve the problems. (...) In this case, it is the case that we cannot solve the problems. (...) In this case, it is the case that we cannot solve the problems. (...) In this case, it is the case that we cannot solve the problems. (...) In this case, it is the case that we cannot solve the problems. (...) In this case, it is the case that we cannot solve the problems. (...) In this case, it is the case that we cannot solve the problems. (...) In this case, it is the case that we cannot solve the problems. (...) In this case, it is the case that we cannot solve the problems. (...) In this case, it is the case that we cannot solve the problems."}, {"heading": "D Details on Our Implementation of Localized MKL", "text": "# # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # #"}, {"heading": "E Background on the Experimental Setup and Empirical Results", "text": "In recent years, it has become clear that most of those who see themselves in a position are not an isolated case, but a group of people who are able to play by the rules, \"he told the Deutsche Presse-Agentur in an interview with the news magazine\" Der Spiegel. \""}], "references": [{"title": "Toward a gold standard for promoter prediction evaluation", "author": ["T. Abeel", "Y. Van de Peer", "Y. Saeys"], "venue": "Bioinformatics, 25(12):i313\u2013i320,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2009}, {"title": "Multiple kernel learning, conic duality, and the smo algorithm", "author": ["F.R. Bach", "G.R. Lanckriet", "M.I. Jordan"], "venue": "ICML, page 6,", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2004}, {"title": "Rademacher and Gaussian complexities: Risk bounds and structural results", "author": ["P. Bartlett", "S. Mendelson"], "venue": "Journal of Machine Learning Research, 3:463\u2013482,", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2002}, {"title": "Support vector machines and kernels for computational biology", "author": ["A. Ben-Hur", "C.S. Ong", "S. Sonnenburg", "B. Sch\u00f6lkopf", "G. R\u00e4tsch"], "venue": "PLoS Computational Biology, 4,", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2008}, {"title": "Enhanced representation and multi-task learning for image annotation", "author": ["A. Binder", "W. Samek", "K.-R. M\u00fcller", "M. Kawanabe"], "venue": "Computer Vision and Image Understanding,", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2013}, {"title": "Convex optimization", "author": ["S.P. Boyd", "L. Vandenberghe"], "venue": "Cambridge Univ. Press, New York,", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2004}, {"title": "Learning with support vector machines", "author": ["C. Campbell", "Y. Ying"], "venue": "Synthesis Lectures on Artificial Intelligence and Machine Learning, 5(1):1\u201395,", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2011}, {"title": "LIBSVM: a library for support vector machines", "author": ["C.-C. Chang", "C.-J. Lin"], "venue": "ACM Transactions on Intelligent Systems and Technology (TIST), 2(3):27,", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2011}, {"title": "Invited talk: Can learning kernels help performance", "author": ["C. Cortes"], "venue": "In Proceedings of the 26th Annual International Conference on Machine Learning,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2009}, {"title": "Generalization bounds for learning kernels", "author": ["C. Cortes", "M. Mohri", "A. Rostamizadeh"], "venue": "Proceedings of the 28th International Conference on Machine Learning, ICML\u201910,", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2010}, {"title": "Learning kernels using local rademacher complexity", "author": ["C. Cortes", "M. Kloft", "M. Mohri"], "venue": "Advances in Neural Information Processing Systems, pages 2760\u20132768,", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2013}, {"title": "Kernel k-means: spectral clustering and normalized cuts", "author": ["I.S. Dhillon", "Y. Guan", "B. Kulis"], "venue": "ACM SIGKDD international conference on Knowledge discovery and data mining, pages 551\u2013556. ACM,", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2004}, {"title": "Multi-class protein fold recognition using support vector machines and neural networks", "author": ["C.H. Ding", "I. Dubchak"], "venue": "Bioinformatics, 17(4):349\u2013358,", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2001}, {"title": "Localized multiple kernel learning", "author": ["M. G\u00f6nen", "E. Alpaydin"], "venue": "Proceedings of the 25th international conference on Machine learning, pages 352\u2013359. ACM,", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2008}, {"title": "Multiple kernel learning algorithms", "author": ["M. G\u00f6nen", "E. Alpaydin"], "venue": "J. Mach. Learn. Res., 12:2211\u20132268, July", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2011}, {"title": "Localized algorithms for multiple kernel learning", "author": ["M. G\u00f6nen", "E. Alpayd\u0131n"], "venue": "Pattern Recognition, 46(3):795\u2013 807,", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2013}, {"title": "Active and semi-supervised data domain description", "author": ["N. G\u00f6rnitz", "M. Kloft", "U. Brefeld"], "venue": "Joint European Conference on Machine Learning and Knowledge Discovery in Databases, pages 407\u2013422. Springer,", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2009}, {"title": "Toward supervised anomaly detection", "author": ["N. G\u00f6rnitz", "M.M. Kloft", "K. Rieck", "U. Brefeld"], "venue": "Journal of Artificial Intelligence Research,", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2013}, {"title": "Probability-confidence-kernel-based localized multiple kernel learning with norm", "author": ["Y. Han", "G. Liu"], "venue": "IEEE Transactions on Systems, Man, and Cybernetics, Part B, 42(3):827\u2013837,", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2012}, {"title": "Fenchel duality-based algorithms for convex optimization problems with applications in machine learning and image restoration", "author": ["A. Heinrich"], "venue": "PhD thesis, Chemnitz University of Technology,", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2012}, {"title": "Clusterpath: an algorithm for clustering using convex fusion penalties", "author": ["T.D. Hocking", "A. Joulin", "F. Bach", "J.-P. Vert"], "venue": "In 28th international conference on machine learning,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2011}, {"title": "Improved loss bounds for multiple kernel learning", "author": ["Z. Hussain", "J. Shawe-Taylor"], "venue": "AISTATS, pages 370\u2013377,", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2011}, {"title": "Some random series of functions", "author": ["J.-P. Kahane"], "venue": "Cambridge University Press, Cambridge,", "citeRegEx": "23", "shortCiteRegEx": null, "year": 1985}, {"title": "`p-norm multiple kernel learning", "author": ["M. Kloft"], "venue": "PhD thesis, Berlin Institute of Technology,", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2011}, {"title": "The local Rademacher complexity of `p-norm multiple kernel learning", "author": ["M. Kloft", "G. Blanchard"], "venue": "Advances in Neural Information Processing Systems 24, pages 2438\u20132446. MIT Press,", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2011}, {"title": "On the convergence rate of lp-norm multiple kernel learning", "author": ["M. Kloft", "G. Blanchard"], "venue": "Journal of Machine Learning Research, 13(1):2465\u20132502,", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2012}, {"title": "Automatic feature selection for anomaly detection", "author": ["M. Kloft", "U. Brefeld", "P. D\u00fcessel", "C. Gehl", "P. Laskov"], "venue": "Proceedings of the 1st ACM workshop on Workshop on AISec, pages 71\u201376. ACM,", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2008}, {"title": "Non-sparse multiple kernel learning", "author": ["M. Kloft", "U. Brefeld", "P. Laskov", "S. Sonnenburg"], "venue": "NIPS Workshop on Kernel Learning: Automatic Selection of Optimal Kernels, volume 4,", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2008}, {"title": "Efficient and accurate lp-norm multiple kernel learning", "author": ["M. Kloft", "U. Brefeld", "P. Laskov", "K.-R. M\u00fcller", "A. Zien", "S. Sonnenburg"], "venue": "Advances in neural information processing systems, pages 997\u20131005,", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2009}, {"title": "A unifying view of multiple kernel learning", "author": ["M. Kloft", "U. R\u00fcckert", "P. Bartlett"], "venue": "Machine Learning and Knowledge Discovery in Databases, pages 66\u201381,", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2010}, {"title": "Lp-norm multiple kernel learning", "author": ["M. Kloft", "U. Brefeld", "S. Sonnenburg", "A. Zien"], "venue": "The Journal of Machine Learning Research, 12:953\u2013997,", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2011}, {"title": "Learning the kernel matrix with semidefinite programming", "author": ["G.R. Lanckriet", "N. Cristianini", "P. Bartlett", "L.E. Ghaoui", "M.I. Jordan"], "venue": "The Journal of Machine Learning Research, 5:27\u201372,", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2004}, {"title": "Refined Rademacher chaos complexity bounds with applications to the multikernel learning problem", "author": ["Y. Lei", "L. Ding"], "venue": "Neural. Comput., 26(4):739\u2013760,", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2014}, {"title": "Theory and algorithms for the localized setting of learning kernels", "author": ["Y. Lei", "A. Binder", "\u00dc. Dogan", "M. Kloft"], "venue": "Proceedings of The 1st International Workshop on \u201cFeature Extraction: Modern Questions and Challenges\u201d, NIPS, pages 173\u2013195,", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2015}, {"title": "What, where and who? classifying events by scene and object recognition", "author": ["L.-J. Li", "L. Fei-Fei"], "venue": "Computer Vision, 2007. ICCV 2007. IEEE 11th International Conference on, pages 1\u20138. IEEE,", "citeRegEx": "35", "shortCiteRegEx": null, "year": 2007}, {"title": "Multiple kernel clustering with local kernel alignment maximization", "author": ["M. Li", "X. Liu", "L. Wang", "Y. Dou", "J. Yin", "E. Zhu"], "venue": "Proceedings of the Twenty-Fifth International Joint Conference on Artificial Intelligence, IJCAI\u201916,", "citeRegEx": "36", "shortCiteRegEx": null, "year": 2016}, {"title": "Sample-adaptive multiple kernel learning", "author": ["X. Liu", "L. Wang", "J. Zhang", "J. Yin"], "venue": "Proceedings of the Twenty- Eighth AAAI Conference on Artificial Intelligence, July 27 -31, 2014, Qu\u00e9bec City, Qu\u00e9bec, Canada., pages 1975\u2013 1981,", "citeRegEx": "37", "shortCiteRegEx": null, "year": 2014}, {"title": "Absent multiple kernel learning", "author": ["X. Liu", "L. Wang", "J. Yin", "Y. Dou", "J. Zhang"], "venue": "Proceedings of the Twenty-Ninth AAAI Conference on Artificial Intelligence, January 25-30, 2015, Austin, Texas, USA., pages 2807\u20132813,", "citeRegEx": "38", "shortCiteRegEx": null, "year": 2015}, {"title": "Distinctive image features from scale-invariant keypoints", "author": ["D.G. Lowe"], "venue": "International Journal of Computer Vision, 60(2):91\u2013110,", "citeRegEx": "39", "shortCiteRegEx": null, "year": 2004}, {"title": "Learning the kernel function via regularization", "author": ["C.A. Micchelli", "M. Pontil"], "venue": "Journal of Machine Learning Research, pages 1099\u20131125,", "citeRegEx": "40", "shortCiteRegEx": null, "year": 2005}, {"title": "A unified view of localized kernel learning", "author": ["J. Moeller", "S. Swaminathan", "S. Venkatasubramanian"], "venue": "arXiv preprint arXiv:1603.01374,", "citeRegEx": "41", "shortCiteRegEx": null, "year": 2016}, {"title": "Non-uniform multiple kernel learning with cluster-based gating functions", "author": ["Y. Mu", "B. Zhou"], "venue": "Neurocomputing, 74(7):1095\u20131101,", "citeRegEx": "42", "shortCiteRegEx": null, "year": 2011}, {"title": "Learning with Kernels", "author": ["B. Sch\u00f6lkopf", "A. Smola"], "venue": "MIT Press, Cambridge, MA,", "citeRegEx": "44", "shortCiteRegEx": null, "year": 2002}, {"title": "Large scale multiple kernel learning", "author": ["S. Sonnenburg", "G. R\u00e4tsch", "C. Sch\u00e4fer", "B. Sch\u00f6lkopf"], "venue": "The Journal of Machine Learning Research, 7:1531\u20131565,", "citeRegEx": "45", "shortCiteRegEx": null, "year": 2006}, {"title": "Arts: accurate recognition of transcription starts in human", "author": ["S. Sonnenburg", "A. Zien", "G. R\u00e4tsch"], "venue": "Bioinformatics, 22(14):e472\u2013e480,", "citeRegEx": "46", "shortCiteRegEx": null, "year": 2006}, {"title": "Poims: positional oligomer importance matrices\u2014understanding support vector machine-based signal detectors", "author": ["S. Sonnenburg", "A. Zien", "P. Philips", "G. R\u00e4tsch"], "venue": "Bioinformatics, 24(13):i6\u2013i14,", "citeRegEx": "47", "shortCiteRegEx": null, "year": 2008}, {"title": "Learning bounds for support vector machines with learned kernels", "author": ["N. Srebro", "S. Ben-David"], "venue": "COLT, pages 169\u2013183. Springer-Verlag, Berlin,", "citeRegEx": "48", "shortCiteRegEx": null, "year": 2006}, {"title": "Multiple kernel learning and the smo algorithm", "author": ["Z. Sun", "N. Ampornpunt", "M. Varma", "S. Vishwanathan"], "venue": "Advances in neural information processing systems, pages 2361\u20132369,", "citeRegEx": "49", "shortCiteRegEx": null, "year": 2010}, {"title": "Convergence of a block coordinate descent method for nondifferentiable minimization", "author": ["P. Tseng"], "venue": "Journal of optimization theory and applications, 109(3):475\u2013494,", "citeRegEx": "50", "shortCiteRegEx": null, "year": 2001}, {"title": "Evaluating color descriptors for object and scene recognition", "author": ["K.E.A. van de Sande", "T. Gevers", "C.G.M. Snoek"], "venue": "IEEE Trans. Pattern Anal. Mach. Intell.,", "citeRegEx": "51", "shortCiteRegEx": "51", "year": 2010}, {"title": "Probabilistic clustering of time-evolving distance data", "author": ["J.E. Vogt", "M. Kloft", "S. Stark", "S.S. Raman", "S. Prabhakaran", "V. Roth", "G. R\u00e4tsch"], "venue": "Machine Learning, 100(2-3):635\u2013654,", "citeRegEx": "52", "shortCiteRegEx": null, "year": 2015}, {"title": "Simple and efficient multiple kernel learning by group lasso", "author": ["Z. Xu", "R. Jin", "H. Yang", "I. King", "M.R. Lyu"], "venue": "Proceedings of the 27th international conference on machine learning (ICML-10), pages 1175\u20131182,", "citeRegEx": "53", "shortCiteRegEx": null, "year": 2010}, {"title": "Efficient sparse generalized multiple kernel learning", "author": ["H. Yang", "Z. Xu", "J. Ye", "I. King", "M.R. Lyu"], "venue": "IEEE Transactions on Neural Networks, 22(3):433\u2013446,", "citeRegEx": "54", "shortCiteRegEx": null, "year": 2011}, {"title": "Group-sensitive multiple kernel learning for object categorization", "author": ["J. Yang", "Y. Li", "Y. Tian", "L. Duan", "W. Gao"], "venue": "2009 IEEE 12th International Conference on Computer Vision, pages 436\u2013443. IEEE,", "citeRegEx": "55", "shortCiteRegEx": null, "year": 2009}, {"title": "Generalization bounds for learning the kernel", "author": ["Y. Ying", "C. Campbell"], "venue": "COLT,", "citeRegEx": "56", "shortCiteRegEx": null, "year": 2009}, {"title": "Local features and kernels for classification of texture and object categories: A comprehensive study", "author": ["J. Zhang", "M. Marszalek", "S. Lazebnik", "C. Schmid"], "venue": "International Journal of Computer Vision, 73(2):213\u2013238,", "citeRegEx": "57", "shortCiteRegEx": null, "year": 2007}], "referenceMentions": [{"referenceID": 31, "context": "[32], who introduce the multiple kernel learning (MKL) framework [15].", "startOffset": 0, "endOffset": 4}, {"referenceID": 14, "context": "[32], who introduce the multiple kernel learning (MKL) framework [15].", "startOffset": 65, "endOffset": 69}, {"referenceID": 43, "context": "MKL offers a principal way of encoding complementary information with distinct base kernels and automatically learning an optimal combination of those [45].", "startOffset": 151, "endOffset": 155}, {"referenceID": 1, "context": "MKL can be phrased as a single convex optimization problem, which facilitates the application of efficient numerical optimization strategies [2, 27, 29, 43, 45, 53, 54] and theoretical understanding of the generalization performance of the resulting models [10, 11, 22, 25, 26, 30, 33, 48, 56].", "startOffset": 141, "endOffset": 168}, {"referenceID": 26, "context": "MKL can be phrased as a single convex optimization problem, which facilitates the application of efficient numerical optimization strategies [2, 27, 29, 43, 45, 53, 54] and theoretical understanding of the generalization performance of the resulting models [10, 11, 22, 25, 26, 30, 33, 48, 56].", "startOffset": 141, "endOffset": 168}, {"referenceID": 28, "context": "MKL can be phrased as a single convex optimization problem, which facilitates the application of efficient numerical optimization strategies [2, 27, 29, 43, 45, 53, 54] and theoretical understanding of the generalization performance of the resulting models [10, 11, 22, 25, 26, 30, 33, 48, 56].", "startOffset": 141, "endOffset": 168}, {"referenceID": 43, "context": "MKL can be phrased as a single convex optimization problem, which facilitates the application of efficient numerical optimization strategies [2, 27, 29, 43, 45, 53, 54] and theoretical understanding of the generalization performance of the resulting models [10, 11, 22, 25, 26, 30, 33, 48, 56].", "startOffset": 141, "endOffset": 168}, {"referenceID": 51, "context": "MKL can be phrased as a single convex optimization problem, which facilitates the application of efficient numerical optimization strategies [2, 27, 29, 43, 45, 53, 54] and theoretical understanding of the generalization performance of the resulting models [10, 11, 22, 25, 26, 30, 33, 48, 56].", "startOffset": 141, "endOffset": 168}, {"referenceID": 52, "context": "MKL can be phrased as a single convex optimization problem, which facilitates the application of efficient numerical optimization strategies [2, 27, 29, 43, 45, 53, 54] and theoretical understanding of the generalization performance of the resulting models [10, 11, 22, 25, 26, 30, 33, 48, 56].", "startOffset": 141, "endOffset": 168}, {"referenceID": 9, "context": "MKL can be phrased as a single convex optimization problem, which facilitates the application of efficient numerical optimization strategies [2, 27, 29, 43, 45, 53, 54] and theoretical understanding of the generalization performance of the resulting models [10, 11, 22, 25, 26, 30, 33, 48, 56].", "startOffset": 257, "endOffset": 293}, {"referenceID": 10, "context": "MKL can be phrased as a single convex optimization problem, which facilitates the application of efficient numerical optimization strategies [2, 27, 29, 43, 45, 53, 54] and theoretical understanding of the generalization performance of the resulting models [10, 11, 22, 25, 26, 30, 33, 48, 56].", "startOffset": 257, "endOffset": 293}, {"referenceID": 21, "context": "MKL can be phrased as a single convex optimization problem, which facilitates the application of efficient numerical optimization strategies [2, 27, 29, 43, 45, 53, 54] and theoretical understanding of the generalization performance of the resulting models [10, 11, 22, 25, 26, 30, 33, 48, 56].", "startOffset": 257, "endOffset": 293}, {"referenceID": 24, "context": "MKL can be phrased as a single convex optimization problem, which facilitates the application of efficient numerical optimization strategies [2, 27, 29, 43, 45, 53, 54] and theoretical understanding of the generalization performance of the resulting models [10, 11, 22, 25, 26, 30, 33, 48, 56].", "startOffset": 257, "endOffset": 293}, {"referenceID": 25, "context": "MKL can be phrased as a single convex optimization problem, which facilitates the application of efficient numerical optimization strategies [2, 27, 29, 43, 45, 53, 54] and theoretical understanding of the generalization performance of the resulting models [10, 11, 22, 25, 26, 30, 33, 48, 56].", "startOffset": 257, "endOffset": 293}, {"referenceID": 29, "context": "MKL can be phrased as a single convex optimization problem, which facilitates the application of efficient numerical optimization strategies [2, 27, 29, 43, 45, 53, 54] and theoretical understanding of the generalization performance of the resulting models [10, 11, 22, 25, 26, 30, 33, 48, 56].", "startOffset": 257, "endOffset": 293}, {"referenceID": 32, "context": "MKL can be phrased as a single convex optimization problem, which facilitates the application of efficient numerical optimization strategies [2, 27, 29, 43, 45, 53, 54] and theoretical understanding of the generalization performance of the resulting models [10, 11, 22, 25, 26, 30, 33, 48, 56].", "startOffset": 257, "endOffset": 293}, {"referenceID": 46, "context": "MKL can be phrased as a single convex optimization problem, which facilitates the application of efficient numerical optimization strategies [2, 27, 29, 43, 45, 53, 54] and theoretical understanding of the generalization performance of the resulting models [10, 11, 22, 25, 26, 30, 33, 48, 56].", "startOffset": 257, "endOffset": 293}, {"referenceID": 54, "context": "MKL can be phrased as a single convex optimization problem, which facilitates the application of efficient numerical optimization strategies [2, 27, 29, 43, 45, 53, 54] and theoretical understanding of the generalization performance of the resulting models [10, 11, 22, 25, 26, 30, 33, 48, 56].", "startOffset": 257, "endOffset": 293}, {"referenceID": 27, "context": "9, and references therein], it was shown that improved predictive accuracy can be achieved by employing appropriate regularization [28, 31].", "startOffset": 131, "endOffset": 139}, {"referenceID": 30, "context": "9, and references therein], it was shown that improved predictive accuracy can be achieved by employing appropriate regularization [28, 31].", "startOffset": 131, "endOffset": 139}, {"referenceID": 13, "context": "This example motivates studying localized approaches to MKL [14, 19, 34, 36, 42, 55].", "startOffset": 60, "endOffset": 84}, {"referenceID": 18, "context": "This example motivates studying localized approaches to MKL [14, 19, 34, 36, 42, 55].", "startOffset": 60, "endOffset": 84}, {"referenceID": 33, "context": "This example motivates studying localized approaches to MKL [14, 19, 34, 36, 42, 55].", "startOffset": 60, "endOffset": 84}, {"referenceID": 35, "context": "This example motivates studying localized approaches to MKL [14, 19, 34, 36, 42, 55].", "startOffset": 60, "endOffset": 84}, {"referenceID": 41, "context": "This example motivates studying localized approaches to MKL [14, 19, 34, 36, 42, 55].", "startOffset": 60, "endOffset": 84}, {"referenceID": 53, "context": "This example motivates studying localized approaches to MKL [14, 19, 34, 36, 42, 55].", "startOffset": 60, "endOffset": 84}, {"referenceID": 33, "context": "Indeed, besides the recent work by [34], the generalization performance of localized MKL algorithms (as measured through large-deviation bounds) is poorly understood, which potentially could make these algorithms prone to overfitting.", "startOffset": 35, "endOffset": 39}, {"referenceID": 13, "context": "1 Related Work G\u00f6nen and Alpaydin [14] initiate the work on localized MKL by introducing gating models", "startOffset": 34, "endOffset": 38}, {"referenceID": 53, "context": "[55] give a group-sensitive formulation of localized MKL, where kernel weights vary at, instead of the example level, the group level.", "startOffset": 0, "endOffset": 4}, {"referenceID": 41, "context": "Mu and Zhou [42] also introduce a non-uniform MKL allowing the kernel weights to vary at the cluster-level and tune the kernel weights under the graph embedding framework.", "startOffset": 12, "endOffset": 16}, {"referenceID": 18, "context": "Han and Liu [19] built on G\u00f6nen and Alpaydin [14] by complementing the spatial-similarity-based kernels with probability confidence kernels reflecting the likelihood of examples belonging to the same class.", "startOffset": 12, "endOffset": 16}, {"referenceID": 13, "context": "Han and Liu [19] built on G\u00f6nen and Alpaydin [14] by complementing the spatial-similarity-based kernels with probability confidence kernels reflecting the likelihood of examples belonging to the same class.", "startOffset": 45, "endOffset": 49}, {"referenceID": 35, "context": "[36] propose a multiple kernel clustering method by maximizing local kernel alignments.", "startOffset": 0, "endOffset": 4}, {"referenceID": 36, "context": "[37] present sample-adaptive approaches to localized MKL, where kernels can be switched on/off at the example level by introducing a latent binary vector for each individual sample, which and the kernel weights are then jointly optimized via margin maximization principle.", "startOffset": 0, "endOffset": 4}, {"referenceID": 40, "context": "[41] present a unified viewpoint of localized MKL by interpreting gating functions in terms of local reproducing kernel Hilbert spaces acting on the data.", "startOffset": 0, "endOffset": 4}, {"referenceID": 10, "context": "[11] present a convex approach to MKL based on controlling the local Rademacher complexity, the meaning of locality is different in Cortes et al.", "startOffset": 0, "endOffset": 4}, {"referenceID": 10, "context": "[11]: it refers to the localization of the hypothesis class, which can result in sharper excess risk bounds [25, 26], and is not related to localized multiple kernel learning.", "startOffset": 0, "endOffset": 4}, {"referenceID": 24, "context": "[11]: it refers to the localization of the hypothesis class, which can result in sharper excess risk bounds [25, 26], and is not related to localized multiple kernel learning.", "startOffset": 108, "endOffset": 116}, {"referenceID": 25, "context": "[11]: it refers to the localization of the hypothesis class, which can result in sharper excess risk bounds [25, 26], and is not related to localized multiple kernel learning.", "startOffset": 108, "endOffset": 116}, {"referenceID": 37, "context": "[38] extend the idea of sample-adaptive MKL to address the issue with missing kernel information on some examples.", "startOffset": 0, "endOffset": 4}, {"referenceID": 33, "context": "[34] propose a MKL method by decoupling the locality structure learning with a hard clustering strategy from optimizing the parameters in the spirit of multi-task learning.", "startOffset": 0, "endOffset": 4}, {"referenceID": 0, "context": ", Sl in a probabilistic manner, meaning that, for each cluster Sj , we have a function cj : X \u2192 [0, 1] indicating the likelihood of x falling into cluster j, i.", "startOffset": 96, "endOffset": 102}, {"referenceID": 0, "context": "the first argument and cluster likelihood functions cj : X \u2192 [0, 1], j \u2208 Nl, solve", "startOffset": 61, "endOffset": 67}, {"referenceID": 30, "context": ", \u03b2jM ) for each cluster j [31] .", "startOffset": 27, "endOffset": 31}, {"referenceID": 5, "context": "The result (2) now follows by recalling that for a norm \u2016 \u00b7 \u2016, its dual norm \u2016 \u00b7 \u2016\u2217 is defined by \u2016x\u2016\u2217 = sup\u2016\u03bc\u2016=1\u3008x, \u03bc\u3009 and satisfies: ( 12\u2016 \u00b7 \u2016 2)\u2217 = 12\u2016 \u00b7 \u2016 2 \u2217 [6].", "startOffset": 163, "endOffset": 166}, {"referenceID": 43, "context": "[45], we consider here a two-layer optimization procedure to solve the problem (P) where the variables are divided into two groups: the group of kernel weights {\u03b2jm} j,m=1 and the group of weight vectors {w j } l,M j,m=1.", "startOffset": 0, "endOffset": 4}, {"referenceID": 7, "context": "This allows us to employ very efficient existing SVM solvers [8].", "startOffset": 61, "endOffset": 64}, {"referenceID": 43, "context": "[45] Figure 7 in Kloft et al.", "startOffset": 0, "endOffset": 4}, {"referenceID": 30, "context": "[31]).", "startOffset": 0, "endOffset": 4}, {"referenceID": 47, "context": "[49] in the context of `p-norm MKL.", "startOffset": 0, "endOffset": 4}, {"referenceID": 2, "context": "We give a purely data-dependent bound on the generalization error, which is obtained using Rademacher complexity theory [3].", "startOffset": 120, "endOffset": 123}, {"referenceID": 9, "context": "[10], Kloft and Blanchard [25], Kloft et al.", "startOffset": 0, "endOffset": 4}, {"referenceID": 24, "context": "[10], Kloft and Blanchard [25], Kloft et al.", "startOffset": 26, "endOffset": 30}, {"referenceID": 30, "context": "[31].", "startOffset": 0, "endOffset": 4}, {"referenceID": 7, "context": "1 Experimental Setup We implement the proposed convex localized MKL (CLMKL) algorithm in MATLAB and solve the involved canonical SVM problem with LIBSVM [8].", "startOffset": 153, "endOffset": 156}, {"referenceID": 20, "context": "[21]) could be used.", "startOffset": 0, "endOffset": 4}, {"referenceID": 13, "context": "We compare the performance attained by the proposed CLMKL to regular localized MKL (LMKL) [14], localized MKL based on hard clustering (HLMKL) [34], the SVM using a uniform kernel combination (UNIF) [9], and `p-norm MKL [31], which includes classical MKL [32] as a special case.", "startOffset": 90, "endOffset": 94}, {"referenceID": 33, "context": "We compare the performance attained by the proposed CLMKL to regular localized MKL (LMKL) [14], localized MKL based on hard clustering (HLMKL) [34], the SVM using a uniform kernel combination (UNIF) [9], and `p-norm MKL [31], which includes classical MKL [32] as a special case.", "startOffset": 143, "endOffset": 147}, {"referenceID": 8, "context": "We compare the performance attained by the proposed CLMKL to regular localized MKL (LMKL) [14], localized MKL based on hard clustering (HLMKL) [34], the SVM using a uniform kernel combination (UNIF) [9], and `p-norm MKL [31], which includes classical MKL [32] as a special case.", "startOffset": 199, "endOffset": 202}, {"referenceID": 30, "context": "We compare the performance attained by the proposed CLMKL to regular localized MKL (LMKL) [14], localized MKL based on hard clustering (HLMKL) [34], the SVM using a uniform kernel combination (UNIF) [9], and `p-norm MKL [31], which includes classical MKL [32] as a special case.", "startOffset": 220, "endOffset": 224}, {"referenceID": 31, "context": "We compare the performance attained by the proposed CLMKL to regular localized MKL (LMKL) [14], localized MKL based on hard clustering (HLMKL) [34], the SVM using a uniform kernel combination (UNIF) [9], and `p-norm MKL [31], which includes classical MKL [32] as a special case.", "startOffset": 255, "endOffset": 259}, {"referenceID": 13, "context": "The calculation of the gradients in LMKL [14] requires O(nMd) operations, which scales poorly, and the definition of the gating model 8", "startOffset": 41, "endOffset": 45}, {"referenceID": 3, "context": "It includes 1000 splice site instances and 20 weighted-degree kernels with degrees ranging from 1 to 20 [4].", "startOffset": 104, "endOffset": 107}, {"referenceID": 45, "context": "A hypothetical explanation of the improvement from CLMKL is that splice sites are characterized by nucleotide sequences\u2014so-called motifs\u2014the length of which may differ from site to site [47].", "startOffset": 186, "endOffset": 190}, {"referenceID": 44, "context": "This data set, which is included in the larger study of [46], comes with 5 kernels.", "startOffset": 56, "endOffset": 60}, {"referenceID": 0, "context": "The SVM based on the uniform combination of these 5 kernels was found to have the highest overall performance among 19 promoter prediction programs [1].", "startOffset": 148, "endOffset": 151}, {"referenceID": 0, "context": "To be consistent with previous studies [1, 24, 46], we use the area under the ROC curve (AUC) as an evaluation criterion.", "startOffset": 39, "endOffset": 50}, {"referenceID": 23, "context": "To be consistent with previous studies [1, 24, 46], we use the area under the ROC curve (AUC) as an evaluation criterion.", "startOffset": 39, "endOffset": 50}, {"referenceID": 44, "context": "To be consistent with previous studies [1, 24, 46], we use the area under the ROC curve (AUC) as an evaluation criterion.", "startOffset": 39, "endOffset": 50}, {"referenceID": 44, "context": "The gating function and the partition are computed with the TSS kernel, which carries most of the discriminative information [46].", "startOffset": 125, "endOffset": 129}, {"referenceID": 12, "context": "We experiment on the protein folding class prediction dataset by Ding and Dubchak [13], which was also used in Campbell and Ying [7], Kloft [24], Kloft and Blanchard [25].", "startOffset": 82, "endOffset": 86}, {"referenceID": 6, "context": "We experiment on the protein folding class prediction dataset by Ding and Dubchak [13], which was also used in Campbell and Ying [7], Kloft [24], Kloft and Blanchard [25].", "startOffset": 129, "endOffset": 132}, {"referenceID": 23, "context": "We experiment on the protein folding class prediction dataset by Ding and Dubchak [13], which was also used in Campbell and Ying [7], Kloft [24], Kloft and Blanchard [25].", "startOffset": 140, "endOffset": 144}, {"referenceID": 24, "context": "We experiment on the protein folding class prediction dataset by Ding and Dubchak [13], which was also used in Campbell and Ying [7], Kloft [24], Kloft and Blanchard [25].", "startOffset": 166, "endOffset": 170}, {"referenceID": 6, "context": "We use exactly the same 12 kernels as in Campbell and Ying [7], Kloft [24], Kloft and Blanchard [25] reflecting different features, such as van der Waals volume, polarity and hydrophobicity.", "startOffset": 59, "endOffset": 62}, {"referenceID": 23, "context": "We use exactly the same 12 kernels as in Campbell and Ying [7], Kloft [24], Kloft and Blanchard [25] reflecting different features, such as van der Waals volume, polarity and hydrophobicity.", "startOffset": 70, "endOffset": 74}, {"referenceID": 24, "context": "We use exactly the same 12 kernels as in Campbell and Ying [7], Kloft [24], Kloft and Blanchard [25] reflecting different features, such as van der Waals volume, polarity and hydrophobicity.", "startOffset": 96, "endOffset": 100}, {"referenceID": 6, "context": "We precisely replicate the experimental setup of previous experiments by Campbell and Ying [7], Kloft [24], Kloft and Blanchard [25], which is detailed in Supplementary Material E.", "startOffset": 91, "endOffset": 94}, {"referenceID": 23, "context": "We precisely replicate the experimental setup of previous experiments by Campbell and Ying [7], Kloft [24], Kloft and Blanchard [25], which is detailed in Supplementary Material E.", "startOffset": 102, "endOffset": 106}, {"referenceID": 24, "context": "We precisely replicate the experimental setup of previous experiments by Campbell and Ying [7], Kloft [24], Kloft and Blanchard [25], which is detailed in Supplementary Material E.", "startOffset": 128, "endOffset": 132}, {"referenceID": 23, "context": "6% higher than the one reported in Kloft [24], which is higher than the initially reported accuracies in Campbell and Ying [7].", "startOffset": 41, "endOffset": 45}, {"referenceID": 6, "context": "6% higher than the one reported in Kloft [24], which is higher than the initially reported accuracies in Campbell and Ying [7].", "startOffset": 123, "endOffset": 126}, {"referenceID": 34, "context": "5 Visual Image Categorization\u2014UIUC Sports We experiment on the UIUC Sports event dataset [35] consisting of 1574 images, belonging to 8 image classes of sports activities.", "startOffset": 89, "endOffset": 93}, {"referenceID": 13, "context": "26% over the `p-norm MKL baseline while localized MKL as in G\u00f6nen and Alpaydin [14] underperforms the MKL baseline.", "startOffset": 79, "endOffset": 83}, {"referenceID": 16, "context": "Future work could analyze extension of the methodology to semi-supervised learning [17, 18] or using different clustering objectives [21, 52] and how to principally include the construction of the data partition into our framework by constructing partitions that can capture the local variation of prediction importance of different features.", "startOffset": 83, "endOffset": 91}, {"referenceID": 17, "context": "Future work could analyze extension of the methodology to semi-supervised learning [17, 18] or using different clustering objectives [21, 52] and how to principally include the construction of the data partition into our framework by constructing partitions that can capture the local variation of prediction importance of different features.", "startOffset": 83, "endOffset": 91}, {"referenceID": 20, "context": "Future work could analyze extension of the methodology to semi-supervised learning [17, 18] or using different clustering objectives [21, 52] and how to principally include the construction of the data partition into our framework by constructing partitions that can capture the local variation of prediction importance of different features.", "startOffset": 133, "endOffset": 141}, {"referenceID": 50, "context": "Future work could analyze extension of the methodology to semi-supervised learning [17, 18] or using different clustering objectives [21, 52] and how to principally include the construction of the data partition into our framework by constructing partitions that can capture the local variation of prediction importance of different features.", "startOffset": 133, "endOffset": 141}, {"referenceID": 48, "context": "1 in Tseng [50].", "startOffset": 11, "endOffset": 15}, {"referenceID": 9, "context": "[10], which was shown to be tight.", "startOffset": 0, "endOffset": 4}, {"referenceID": 22, "context": "4 (Khintchine-Kahane inequality [23]).", "startOffset": 32, "endOffset": 36}, {"referenceID": 25, "context": "5 (Block-structured H\u00f6lder inequality [26]).", "startOffset": 38, "endOffset": 42}, {"referenceID": 2, "context": "The proof now simply follows by plugging in the bound of Theorem 8 into Theorem 7 of Bartlett and Mendelson [3].", "startOffset": 108, "endOffset": 111}, {"referenceID": 19, "context": "B Support Vector Regression Formulation of CLMKL For the -insensitive loss `(t, y) = [|y \u2212 t| \u2212 ]+, denoting a+ = max(a, 0) for all a \u2208 R, we have `\u2217(\u2212\u03b1i C , yi) = \u2212 1 C\u03b1iyi + | \u03b1i C | if |\u03b1i| \u2264 C and\u221e elsewise [20].", "startOffset": 211, "endOffset": 215}, {"referenceID": 13, "context": "G\u00f6nen and Alpaydin [14] give the first formulation of localized MKL algorithm by using gating model \u03b7m(x) \u221d exp(\u3008vm, x\u3009 + vm0) to realize locality, and optimize the parameters vm, vm0,m \u2208 NM with a gradient descent method.", "startOffset": 19, "endOffset": 23}, {"referenceID": 13, "context": "However, the calculation of the gradients requires O(nMd) operations in G\u00f6nen and Alpaydin [14], which scales poorly w.", "startOffset": 91, "endOffset": 95}, {"referenceID": 15, "context": "Although G\u00f6nen and Alpayd\u0131n [16] propose to use the empirical feature map xG = [kG(x1, x), .", "startOffset": 28, "endOffset": 32}, {"referenceID": 13, "context": "G\u00f6nen and Alpaydin [14] proposed to optimize the objective function", "startOffset": 19, "endOffset": 23}, {"referenceID": 13, "context": "Putting the above discussions together, our implementation of LMKL based on the kernel trick requiresO(nM) operations at each iteration, which is much faster than the original implementation in G\u00f6nen and Alpaydin [14] with O(nMd) operations at each iteration.", "startOffset": 213, "endOffset": 217}, {"referenceID": 6, "context": "1 Details on the Protein Fold Prediction Experiment We precisely replicate the experimental setup of previous experiments by Campbell and Ying [7], Kloft [24], Kloft and Blanchard [25], so we use the train/test split supplied by Campbell and Ying [7] and perform CLMKL via one-versus-all strategy to tackle multiple classes.", "startOffset": 143, "endOffset": 146}, {"referenceID": 23, "context": "1 Details on the Protein Fold Prediction Experiment We precisely replicate the experimental setup of previous experiments by Campbell and Ying [7], Kloft [24], Kloft and Blanchard [25], so we use the train/test split supplied by Campbell and Ying [7] and perform CLMKL via one-versus-all strategy to tackle multiple classes.", "startOffset": 154, "endOffset": 158}, {"referenceID": 24, "context": "1 Details on the Protein Fold Prediction Experiment We precisely replicate the experimental setup of previous experiments by Campbell and Ying [7], Kloft [24], Kloft and Blanchard [25], so we use the train/test split supplied by Campbell and Ying [7] and perform CLMKL via one-versus-all strategy to tackle multiple classes.", "startOffset": 180, "endOffset": 184}, {"referenceID": 6, "context": "1 Details on the Protein Fold Prediction Experiment We precisely replicate the experimental setup of previous experiments by Campbell and Ying [7], Kloft [24], Kloft and Blanchard [25], so we use the train/test split supplied by Campbell and Ying [7] and perform CLMKL via one-versus-all strategy to tackle multiple classes.", "startOffset": 247, "endOffset": 250}, {"referenceID": 55, "context": "2 Details on the Visual Image Categorization Experiment We compute 9 bag-of-words features, each with a dictionary size of 512, resulting in 9 \u03c7-Kernels [57].", "startOffset": 153, "endOffset": 157}, {"referenceID": 38, "context": "The first 6 bag-of-words features are computed over SIFT features [39] at three different scales and the two color channel sets RGB and opponent colors [51].", "startOffset": 66, "endOffset": 70}, {"referenceID": 49, "context": "The first 6 bag-of-words features are computed over SIFT features [39] at three different scales and the two color channel sets RGB and opponent colors [51].", "startOffset": 152, "endOffset": 156}, {"referenceID": 4, "context": "Assignment of local features to visual words is done using rank-mapping [5].", "startOffset": 72, "endOffset": 75}, {"referenceID": 30, "context": "We compare CLMKL to regular `p-norm MKL [31] and to localized MKL as in [14].", "startOffset": 40, "endOffset": 44}, {"referenceID": 13, "context": "We compare CLMKL to regular `p-norm MKL [31] and to localized MKL as in [14].", "startOffset": 72, "endOffset": 76}], "year": 2016, "abstractText": "We propose a localized approach to multiple kernel learning that can be formulated as a convex optimization problem over a given cluster structure. For which we obtain generalization error guarantees and derive an optimization algorithm based on the Fenchel dual representation. Experiments on real-world datasets from the application domains of computational biology and computer vision show that convex localized multiple kernel learning can achieve higher prediction accuracies than its global and non-convex local counterparts.", "creator": "LaTeX with hyperref package"}}}