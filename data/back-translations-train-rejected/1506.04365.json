{"id": "1506.04365", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "14-Jun-2015", "title": "Leveraging Word Embeddings for Spoken Document Summarization", "abstract": "Owing to the rapidly growing multimedia content available on the Internet, extractive spoken document summarization, with the purpose of automatically selecting a set of representative sentences from a spoken document to concisely express the most important theme of the document, has been an active area of research and experimentation. On the other hand, word embedding has emerged as a newly favorite research subject because of its excellent performance in many natural language processing (NLP)-related tasks. However, as far as we are aware, there are relatively few studies investigating its use in extractive text or speech summarization. A common thread of leveraging word embeddings in the summarization process is to represent the document (or sentence) by averaging the word embeddings of the words occurring in the document (or sentence). Then, intuitively, the cosine similarity measure can be employed to determine the relevance degree between a pair of representations. Beyond the continued efforts made to improve the representation of words, this paper focuses on building novel and efficient ranking models based on the general word embedding methods for extractive speech summarization. Experimental results demonstrate the effectiveness of our proposed methods, compared to existing state-of-the-art methods.", "histories": [["v1", "Sun, 14 Jun 2015 09:18:36 GMT  (369kb)", "http://arxiv.org/abs/1506.04365v1", null]], "reviews": [], "SUBJECTS": "cs.CL cs.AI", "authors": ["kuan-yu chen", "shih-hung liu", "hsin-min wang", "berlin chen", "hsin-hsi chen"], "accepted": false, "id": "1506.04365"}, "pdf": {"name": "1506.04365.pdf", "metadata": {"source": "CRF", "title": "Leveraging Word Embeddings for Spoken Document Summarization", "authors": ["Kuan-Yu Chen", "Shih-Hung Liu", "Hsin-Min Wang", "Berlin Chen", "Hsin-Hsi Chen"], "emails": ["whm}@iis.sinica.edu.tw,", "berlin@ntnu.edu.tw,", "hhchen@csie.ntu.edu.tw"], "sections": [{"heading": "1. Introduction", "text": "This year it has come to the point that it will only be a matter of time before it will happen, until it does."}, {"heading": "2. Review of Word Embedding Methods", "text": "Perhaps one of the most well-known groundbreaking studies on the development of word embedding methods was presented in [19], estimating a statistical (N-gram) language model formalized as a feedforward neural network to predict future words in context while simultaneously generating word embeddings (or representations) as a by-product. Such an experiment has already motivated many sequential extensions to develop similar methods for investigating latent semantic and syntactic regularities in word representation. Representative methods include, but are not limited to, the continuous sack-of-words (CBOW) model [21], the skipgram (SG) model [21, 27] and the global vector (GloVe) model [22]. As far as we know, there is little work to contextualize these methods for use in language summaries."}, {"heading": "2.1. The Continuous Bag-of-Words (CBOW) Model", "text": "Instead of trying to learn a statistical language model, the CBOW model succeeds in obtaining a dense vector representation (embedding) of each word directly [21]. The structure of the CBOW resembles a forward-facing neural network, except that the non-linear hidden layer in the former is removed. By bypassing the heavy computing load generated by the non-linear hidden layer, the model can be trained efficiently on a large body, while maintaining good performance. Formally, the objective function of the CBOW is to maximize the protocol probability,...,,,,,,,,... (log1 11 x = + \u2212 T t ctttctt wwwwwwwP (1), where c is the window size of the context words, which is suggested for the central word w, T the length of the training corpus, and (exemplarily) (SWW), SWw, SWw, SWW, SWW, 1, SWW, SWW, SWW, WW, 1, and (ww), SWW (1), SWW, SWW, SWW, SWW, SWW, SWW, SWW, W, SWW, SWW, SWW, SWW, SWW, SWW, SWW, SWW, SWW, SWW, SWW, SWW, SWW, SWW, SWW, SWW, SWW, SWW, SWW, SWW, SWW, SWW, SWW, SWW, SWW, SWW, SWW, SWW, SWW, SWW, SWW, SWW (1, SWW, SWW, SWW, SWW, SWW, SWW, SWW, SWW, SWW, SWW, SWW, SWW (1, SWW), SWW, SWW, SWW, SWW, SWBOW (1)."}, {"heading": "2.2. The Skip-gram (SG) Model", "text": "In contrast to the CBOW model, the SG model uses an inverse training target to learn word representations with a simplified forward-facing neural network [21, 28]. Considering the word sequence w1, w2,..., wT, the objective function of SG is to maximize the following log probability,) | (log1 0, \u2211 = [+ T t c jcj tjt wwP (3), where c is the window size of the context words for the central word wt, and the conditional probability is indicated by,) exp () exp () () = + \u22c5 = + T i wwwwwwwwwwwwtjttitjt wwPvvv (4), where and the word representations of the words at positions t + j and t respectively. In the implementations of CBOW and SG, the hierarchical soft-max algorithm [28, 29] and the process [28] may be more effective."}, {"heading": "2.3. The Global Vector (GloVe) Model", "text": "The GloVe model suggests that a suitable starting point for learning word representation should be linked to the ratios of probabilities of accession and not to the probabilities of prediction [22]. Specifically, GloVe uses the weighted regression of the smallest squares, which aims to learn word representations by maintaining the frequency of accession between each pair of words:,) log) ((((((1 1 1 1 2 x 3 x = = \u2212 + + + \u22c5 V i V j wwwwwwwwww jijiji XbbXf vv (5), encoding the number of words wi and wj in a predefined sliding context window; f (\u00b7) is a monotonic smoothing function used to modulate the effects of each pair of words involved in the model training; and vw and bw denote the word representation or the distorted concept of the word w."}, {"heading": "2.4. Analytic Comparisons", "text": "There are several analytical comparisons between the above three word embedding methods. Firstly, they have different model structures and learning strategies. CBOW and SG use an online learning strategy, i.e., the parameters (word representations) are trained sequentially. Therefore, the sequence in which the training samples are used can dramatically change the resulting models. In contrast, GloVe and GloVe use a stacking strategy, i.e. they accumulate statistics across the entire training corpus and update the model parameters at once. Secondly, it is worth noting that SG (trained with the negative sampling algorithm) and GloVe have an implicit / explicit relationship with the classical weighted matrix factoring approach, whereas the main difference is that SG and GloVe focus on reproducing the word-by-word occurrence cooking matrix matrix matrix matrix matrix matrix matrix matrix matrix."}, {"heading": "3. Sentence Ranking based on Word Embeddings", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "3.1. The Triplet Learning Model", "text": "(8), (8), (8), (8), (8), (8), (8), (8), (8), (8), (8), (8), (8), (8), (8), (8), (8), (8), (8), (8), (8), (8), (8), (8), (8), (8), (8), (8), (8), (8), (8), (8), (8), (8), (8), (8), (8), (8), (8), (8), (8), (8), (8), (8), (8), (8), (8), (8), (8), (8), (8), (8), (8), (8), (8), (8), (8), (8), (8), (8), (8), (8), (8), (8), (8), (8), (8), (8), (8), (8, (8), (8), (8), (8), (8), (8), (8), (8), (8), (8), (8), (8, (8), (8), (8), (8), (8), (8), (8), (8), (8), (8), (8), (8), (8), (8), (8), (8, (8), (8), (8), (8), (8), (8), (8), (8, (8), (8), (8), (8), (8), (8), (8, (8), (8), (8), (8), (8), (8), (8), (8), (8), (8), (8), (8), (8), (8), (8), (8)"}, {"heading": "3.2. The Document Likelihood Measure", "text": "A newer school of thought for extractive SDS is to use a language modeling approach (LM) to select important sentences. A basic insight is to use a probabilistic generative paradigm to evaluate each sentence S of a document D. The simplest way is to estimate a universal language model (ULM) based on the frequency of each word w occurring in S, with the maximum probability (ML) of criterion [37, 38]:, (), (), (S SwnSwP = (12), where n (w, S) is the number of times the word w occurs in S, and | S | is the length of S. Obviously, a major challenge for the LM approach is how to accurately estimate the model parameters for each word."}, {"heading": "4. Experimental Setup", "text": "The data set used in this study is the MATBN News Corpus compiled by Academia Sinica and the Public Television Service Foundation of Taiwan between November 2001 and April 2003. [39] The corpus was split into separate stories and manually transcribed, with each story containing the speech of a studio moderator and several reporters and interviewees on site. A subset of 205 news documents compiled between November 2001 and August 2002 were reserved for the summary experiments. We selected 20 documents as a test set, while the remaining 185 documents were used as a held development set. Reference summaries were compiled by sorting the sentences in the manual transcription of a spoken document by importance, without assigning a score to each sentence. Each document contains three reference summaries commented by three subjects. To evaluate the summary performance, we used the widely used ROUGE metrics [40]. All experimental results are summarized in the following table, which are summarized by the following scores]."}, {"heading": "5. Experimental Results", "text": "This year, it is so far that it is only a matter of time before it is ready, until it is ready."}, {"heading": "6. Conclusions & Future Work", "text": "In this paper, both the triplet learning model and the Document Likelihood measurement were proposed to make the word embedding methods learned through various word embedding methods usable for language summarization. In addition, a new SVD-based word embedding method was also proposed that proved to be efficient and effective. Experimental evidence shows that the proposed methods of word embedding are comparable to several modern methods, demonstrating the potential of the new word embedding-based framework for language summaries. In future work, we will explore other effective methods to enrich the presentation of words and incorporate additional clues, such as speaker identities or prosodic (emotional) information, into the proposed framework."}, {"heading": "SM 0.414 0.286 0.363 0.332 0.204 0.303", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "7. References", "text": "In fact, most people are able to decide for themselves what they want and what they don't want. [1] In fact, most people are able to decide what they want and what they don't want. [2] In fact, most people are able to decide whether they want to or not. [3] In fact, most people are able to decide whether they want to or not. [4] In fact, most people are able to decide whether they want to or not. [4] In fact, most people are able to decide whether they want to or not."}], "references": [{"title": "Fundamental technologies in modern speech recognition", "author": ["S. Furui"], "venue": "IEEE Signal Processing Magazine, 29(6), pp. 16\u2013 17, 2012.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2012}, {"title": "Speech technology and information access", "author": ["M. Ostendorf"], "venue": "IEEE Signal Processing Magazine, 25(3), pp. 150\u2013152, 2008.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2008}, {"title": "Spoken document understanding and organization", "author": ["L.S. Lee", "B. Chen"], "venue": "IEEE Signal Processing Magazine, vol. 22, no. 5, pp. 42\u201360, 2005.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2005}, {"title": "Speech summarization", "author": ["Y. Liu", "D. Hakkani-Tur"], "venue": "Chapter 13 in Spoken Language Understanding: Systems for Extracting Semantic Information from Speech, G. Tur and R. D. Mori (Eds), New York: Wiley, 2011.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2011}, {"title": "A critical reassessment of evaluation baselines for speech summarization", "author": ["G. Penn", "X. Zhu"], "venue": "Proc. of ACL, pp. 470\u2013 478, 2008.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2008}, {"title": "Automatic summarization", "author": ["A. Nenkova", "K. McKeown"], "venue": "Foundations and Trends in Information Retrieval, vol. 5, no. 2\u20133, pp. 103\u2013233, 2011.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2011}, {"title": "Machine-made index for technical literaturean experiment", "author": ["P.B. Baxendale"], "venue": "IBM Journal, October, 1958.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 1958}, {"title": "Generic text summarization using relevance measure and latent semantic analysis", "author": ["Y. Gong", "X. Liu"], "venue": "Proc. of SIGIR, pp. 19\u201325, 2001.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2001}, {"title": "Multi-document summarization using cluster-based link analysis", "author": ["X. Wan", "J. Yang"], "venue": "Proc. of SIGIR, pp. 299\u2013306, 2008.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2008}, {"title": "The use of MMR, diversity based reranking for reordering documents and producing summaries", "author": ["J. Carbonell", "J. Goldstein"], "venue": "Proc. of SIGIR, pp. 335\u2013336, 1998.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 1998}, {"title": "Speech-to-text and speech-to-speech summarization of spontaneous speech", "author": ["S. Furui"], "venue": "IEEE Transactions on Speech and Audio Processing,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2004}, {"title": "LexRank: Graph-based lexical centrality as salience in text summarization", "author": ["G. Erkan", "D.R. Radev"], "venue": "Journal of Artificial Intelligent Research,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2004}, {"title": "Multi-document summarization via budgeted maximization of submodular functions", "author": ["H. Lin", "J. Bilmes"], "venue": "Proc. of NAACL HLT, pp. 912\u2013920, 2010.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2010}, {"title": "Long story short - Global unsupervised models for keyphrase based meeting summarization", "author": ["K. Riedhammer"], "venue": "Speech Communication, vol. 52, no. 10, pp. 801\u2013815, 2010.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2010}, {"title": "A trainable document summarizer", "author": ["J. Kupiec"], "venue": "Proc. of SIGIR, pp. 68\u201373, 1995.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 1995}, {"title": "Speech summarization without lexical features for Mandarin broadcast news", "author": ["J. Zhang", "P. Fung"], "venue": "in Proc. of NAACL HLT, Companion Volume,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2007}, {"title": "Skip-chain conditional random field for ranking meeting utterances by importance", "author": ["M. Galley"], "venue": "Proc. of EMNLP, pp. 364\u2013372, 2006.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2006}, {"title": "A neural probabilistic language model", "author": ["Y. Bengio"], "venue": "Journal of Machine Learning Research (3), pp. 1137\u20131155, 2003.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2003}, {"title": "Three new graphical models for statistical language modeling", "author": ["A. Mnih", "G. Hinton"], "venue": "Proc. of ICML, pp. 641\u2013648, 2007.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2007}, {"title": "Efficient estimation of word representations in vector space", "author": ["T. Mikolov"], "venue": "Proc. of ICLR, pp. 1\u201312, 2013.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2013}, {"title": "GloVe: Global vector for word representation", "author": ["J. Pennington"], "venue": "Proc. of EMNLP, pp. 1532\u20131543, 2014.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2014}, {"title": "Learning sentiment-specific word embedding for twitter sentiment classification", "author": ["D. Tang"], "venue": "in Proc. of ACL, pp", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2014}, {"title": "A unified architecture for natural language processing: deep neural networks with multitask learning", "author": ["R. Collobert", "J. Weston"], "venue": "Proc. of ICML, pp. 160\u2013167, 2008", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2008}, {"title": "Extractive summarization using continuous vector space models", "author": ["M. Kageback"], "venue": "Proc. of CVSC, pp. 31\u201339, 2014.", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2014}, {"title": "Learning word representation considering proximity and ambiguity", "author": ["L. Qiu"], "venue": "Proc. of AAAI, pp. 1572\u20131578, 2014.", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2014}, {"title": "Contextual correlates of semantic similarity", "author": ["G. Miller", "W. Charles"], "venue": "Language and Cognitive Processes, 6(1), pp. 1\u201328, 1991.", "citeRegEx": "27", "shortCiteRegEx": null, "year": 1991}, {"title": "Distributed representations of words and phrases and their compositionality", "author": ["T. Mikolov"], "venue": "Proc. of ICLR, pp. 1\u20139, 2013.", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2013}, {"title": "Hierarchical probabilistic neural network language model", "author": ["F. Morin", "Y. Bengio"], "venue": "Proc. of AISTATS, pp. 246\u2013252, 2005.", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2005}, {"title": "Learning word embeddings efficiently with noise-contrastive estimation", "author": ["A. Mnih", "K. Kavukcuoglu"], "venue": "Proc. of NIPS, pp. 2265\u20132273, 2013.", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2013}, {"title": "Neural word embedding as implicit matrix factorization", "author": ["O. Levy", "Y. Goldberg"], "venue": "Proc. of NIPS, pp. 2177\u20132185, 2014.", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2014}, {"title": "Weighted matrix factorization for spoken document retrieval", "author": ["K.Y. Chen"], "venue": "Proc. of ICASSP, pp. 8530\u20138534, 2013.", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2013}, {"title": "Gaussian mixture language models for speech recognition", "author": ["M. Afify"], "venue": "Proc. of ICASSP, pp. IV-29\u2013IV-32, 2007.", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2007}, {"title": "Online passive-aggressive algorithms", "author": ["K. Crammer"], "venue": "Journal of Machine Learning Research (7), pp. 551\u2013585, 2006.", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2006}, {"title": "Large scale online learning of image similarity through ranking", "author": ["G. Chechik"], "venue": "Journal of Machine Learning Research (11), pp. 1109\u20131135, 2010.", "citeRegEx": "35", "shortCiteRegEx": null, "year": 2010}, {"title": "Hamming distance metric learning", "author": ["M. Norouzi"], "venue": "Proc. of NIPS, pp. 1070\u20131078, 2012.", "citeRegEx": "36", "shortCiteRegEx": null, "year": 2012}, {"title": "A probabilistic generative framework for extractive broadcast news speech summarization,  ", "author": ["Y.T. Chen"], "venue": "IEEE Transactions on Audio, Speech and Language Processing,", "citeRegEx": "37", "shortCiteRegEx": "37", "year": 2009}, {"title": "A study of smoothing methods for language models applied to ad hoc information retrieval", "author": ["C. Zhai", "J. Lafferty"], "venue": "Proc. of SIGIR, pp. 334\u2013342, 2001.", "citeRegEx": "38", "shortCiteRegEx": null, "year": 2001}, {"title": "MATBN: A Mandarin Chinese broadcast news corpus", "author": ["H.M. Wang"], "venue": "International Journal of Computational Linguistics and Chinese Language Processing, vol. 10, no. 2, pp. 219\u2013236, 2005.", "citeRegEx": "39", "shortCiteRegEx": null, "year": 2005}, {"title": "ROUGE: Recall-oriented understudy for gisting evaluation.", "author": ["C.Y. Lin"], "venue": null, "citeRegEx": "40", "shortCiteRegEx": "40", "year": 2003}, {"title": "Discriminative training for automatic speech recognition: Modeling, criteria, optimization, implementation, and performance", "author": ["G. Heigold"], "venue": "IEEE Signal Processing Magazine, vol. 29, no. 6, pp. 58\u201369, 2012.", "citeRegEx": "41", "shortCiteRegEx": null, "year": 2012}], "referenceMentions": [{"referenceID": 0, "context": "Owing to the popularity of various Internet applications, rapidly growing multimedia content, such as music video, broadcast news programs, and lecture recordings, has been continuously filling our daily life [1-3].", "startOffset": 209, "endOffset": 214}, {"referenceID": 1, "context": "Owing to the popularity of various Internet applications, rapidly growing multimedia content, such as music video, broadcast news programs, and lecture recordings, has been continuously filling our daily life [1-3].", "startOffset": 209, "endOffset": 214}, {"referenceID": 2, "context": "Owing to the popularity of various Internet applications, rapidly growing multimedia content, such as music video, broadcast news programs, and lecture recordings, has been continuously filling our daily life [1-3].", "startOffset": 209, "endOffset": 214}, {"referenceID": 3, "context": "Extractive SDS manages to select a set of indicative sentences from a spoken document according to a target summarization ratio and concatenate them together to form a summary [4-7].", "startOffset": 176, "endOffset": 181}, {"referenceID": 4, "context": "Extractive SDS manages to select a set of indicative sentences from a spoken document according to a target summarization ratio and concatenate them together to form a summary [4-7].", "startOffset": 176, "endOffset": 181}, {"referenceID": 5, "context": "Extractive SDS manages to select a set of indicative sentences from a spoken document according to a target summarization ratio and concatenate them together to form a summary [4-7].", "startOffset": 176, "endOffset": 181}, {"referenceID": 3, "context": "The wide spectrum of extractive SDS methods developed so far may be divided into three categories [4, 7]: 1) methods simply based on the sentence position or structure information, 2) methods based on unsupervised sentence ranking, and 3) methods based on supervised sentence classification.", "startOffset": 98, "endOffset": 104}, {"referenceID": 6, "context": "For the first category, the important sentences are selected from some salient parts of a spoken document [8], such as the introductory and/or concluding parts.", "startOffset": 106, "endOffset": 109}, {"referenceID": 7, "context": "Popular methods include the vector space model (VSM) [9], the latent semantic analysis (LSA) method [9], the Markov random walk (MRW) method [10], the maximum marginal relevance (MMR) method [11], the sentence significant score method [12], the unigram language model-based (ULM) method [4], the LexRank method [13], the submodularity-based method [14], and the integer linear programming (ILP) method [15].", "startOffset": 53, "endOffset": 56}, {"referenceID": 7, "context": "Popular methods include the vector space model (VSM) [9], the latent semantic analysis (LSA) method [9], the Markov random walk (MRW) method [10], the maximum marginal relevance (MMR) method [11], the sentence significant score method [12], the unigram language model-based (ULM) method [4], the LexRank method [13], the submodularity-based method [14], and the integer linear programming (ILP) method [15].", "startOffset": 100, "endOffset": 103}, {"referenceID": 8, "context": "Popular methods include the vector space model (VSM) [9], the latent semantic analysis (LSA) method [9], the Markov random walk (MRW) method [10], the maximum marginal relevance (MMR) method [11], the sentence significant score method [12], the unigram language model-based (ULM) method [4], the LexRank method [13], the submodularity-based method [14], and the integer linear programming (ILP) method [15].", "startOffset": 141, "endOffset": 145}, {"referenceID": 9, "context": "Popular methods include the vector space model (VSM) [9], the latent semantic analysis (LSA) method [9], the Markov random walk (MRW) method [10], the maximum marginal relevance (MMR) method [11], the sentence significant score method [12], the unigram language model-based (ULM) method [4], the LexRank method [13], the submodularity-based method [14], and the integer linear programming (ILP) method [15].", "startOffset": 191, "endOffset": 195}, {"referenceID": 10, "context": "Popular methods include the vector space model (VSM) [9], the latent semantic analysis (LSA) method [9], the Markov random walk (MRW) method [10], the maximum marginal relevance (MMR) method [11], the sentence significant score method [12], the unigram language model-based (ULM) method [4], the LexRank method [13], the submodularity-based method [14], and the integer linear programming (ILP) method [15].", "startOffset": 235, "endOffset": 239}, {"referenceID": 3, "context": "Popular methods include the vector space model (VSM) [9], the latent semantic analysis (LSA) method [9], the Markov random walk (MRW) method [10], the maximum marginal relevance (MMR) method [11], the sentence significant score method [12], the unigram language model-based (ULM) method [4], the LexRank method [13], the submodularity-based method [14], and the integer linear programming (ILP) method [15].", "startOffset": 287, "endOffset": 290}, {"referenceID": 11, "context": "Popular methods include the vector space model (VSM) [9], the latent semantic analysis (LSA) method [9], the Markov random walk (MRW) method [10], the maximum marginal relevance (MMR) method [11], the sentence significant score method [12], the unigram language model-based (ULM) method [4], the LexRank method [13], the submodularity-based method [14], and the integer linear programming (ILP) method [15].", "startOffset": 311, "endOffset": 315}, {"referenceID": 12, "context": "Popular methods include the vector space model (VSM) [9], the latent semantic analysis (LSA) method [9], the Markov random walk (MRW) method [10], the maximum marginal relevance (MMR) method [11], the sentence significant score method [12], the unigram language model-based (ULM) method [4], the LexRank method [13], the submodularity-based method [14], and the integer linear programming (ILP) method [15].", "startOffset": 348, "endOffset": 352}, {"referenceID": 13, "context": "Popular methods include the vector space model (VSM) [9], the latent semantic analysis (LSA) method [9], the Markov random walk (MRW) method [10], the maximum marginal relevance (MMR) method [11], the sentence significant score method [12], the unigram language model-based (ULM) method [4], the LexRank method [13], the submodularity-based method [14], and the integer linear programming (ILP) method [15].", "startOffset": 402, "endOffset": 406}, {"referenceID": 7, "context": "In contrast, supervised sentence classification methods, such as the Gaussian mixture model (GMM) [9], the Bayesian classifier (BC) [16], the support vector machine (SVM) [17], and the conditional random fields (CRFs) [18], usually formulate sentence selection as a binary classification problem, i.", "startOffset": 98, "endOffset": 101}, {"referenceID": 14, "context": "In contrast, supervised sentence classification methods, such as the Gaussian mixture model (GMM) [9], the Bayesian classifier (BC) [16], the support vector machine (SVM) [17], and the conditional random fields (CRFs) [18], usually formulate sentence selection as a binary classification problem, i.", "startOffset": 132, "endOffset": 136}, {"referenceID": 15, "context": "In contrast, supervised sentence classification methods, such as the Gaussian mixture model (GMM) [9], the Bayesian classifier (BC) [16], the support vector machine (SVM) [17], and the conditional random fields (CRFs) [18], usually formulate sentence selection as a binary classification problem, i.", "startOffset": 171, "endOffset": 175}, {"referenceID": 16, "context": "In contrast, supervised sentence classification methods, such as the Gaussian mixture model (GMM) [9], the Bayesian classifier (BC) [16], the support vector machine (SVM) [17], and the conditional random fields (CRFs) [18], usually formulate sentence selection as a binary classification problem, i.", "startOffset": 218, "endOffset": 222}, {"referenceID": 3, "context": "Interested readers may refer to [4-7] for comprehensive reviews and new insights into the major methods that have been developed and applied with good success to a wide range of text and speech summarization tasks.", "startOffset": 32, "endOffset": 37}, {"referenceID": 4, "context": "Interested readers may refer to [4-7] for comprehensive reviews and new insights into the major methods that have been developed and applied with good success to a wide range of text and speech summarization tasks.", "startOffset": 32, "endOffset": 37}, {"referenceID": 5, "context": "Interested readers may refer to [4-7] for comprehensive reviews and new insights into the major methods that have been developed and applied with good success to a wide range of text and speech summarization tasks.", "startOffset": 32, "endOffset": 37}, {"referenceID": 17, "context": "Different from the above methods, we explore in this paper various word embedding methods [19-22] for use in extractive SDS, which have recently demonstrated excellent performance in many natural language processing (NLP)-related tasks, such as relational analogy prediction, sentiment analysis, and sentence completion [23-26].", "startOffset": 90, "endOffset": 97}, {"referenceID": 18, "context": "Different from the above methods, we explore in this paper various word embedding methods [19-22] for use in extractive SDS, which have recently demonstrated excellent performance in many natural language processing (NLP)-related tasks, such as relational analogy prediction, sentiment analysis, and sentence completion [23-26].", "startOffset": 90, "endOffset": 97}, {"referenceID": 19, "context": "Different from the above methods, we explore in this paper various word embedding methods [19-22] for use in extractive SDS, which have recently demonstrated excellent performance in many natural language processing (NLP)-related tasks, such as relational analogy prediction, sentiment analysis, and sentence completion [23-26].", "startOffset": 90, "endOffset": 97}, {"referenceID": 20, "context": "Different from the above methods, we explore in this paper various word embedding methods [19-22] for use in extractive SDS, which have recently demonstrated excellent performance in many natural language processing (NLP)-related tasks, such as relational analogy prediction, sentiment analysis, and sentence completion [23-26].", "startOffset": 90, "endOffset": 97}, {"referenceID": 21, "context": "Different from the above methods, we explore in this paper various word embedding methods [19-22] for use in extractive SDS, which have recently demonstrated excellent performance in many natural language processing (NLP)-related tasks, such as relational analogy prediction, sentiment analysis, and sentence completion [23-26].", "startOffset": 320, "endOffset": 327}, {"referenceID": 22, "context": "Different from the above methods, we explore in this paper various word embedding methods [19-22] for use in extractive SDS, which have recently demonstrated excellent performance in many natural language processing (NLP)-related tasks, such as relational analogy prediction, sentiment analysis, and sentence completion [23-26].", "startOffset": 320, "endOffset": 327}, {"referenceID": 23, "context": "Different from the above methods, we explore in this paper various word embedding methods [19-22] for use in extractive SDS, which have recently demonstrated excellent performance in many natural language processing (NLP)-related tasks, such as relational analogy prediction, sentiment analysis, and sentence completion [23-26].", "startOffset": 320, "endOffset": 327}, {"referenceID": 24, "context": "Different from the above methods, we explore in this paper various word embedding methods [19-22] for use in extractive SDS, which have recently demonstrated excellent performance in many natural language processing (NLP)-related tasks, such as relational analogy prediction, sentiment analysis, and sentence completion [23-26].", "startOffset": 320, "endOffset": 327}, {"referenceID": 17, "context": "Perhaps one of the most-known seminal studies on developing word embedding methods was presented in [19].", "startOffset": 100, "endOffset": 104}, {"referenceID": 19, "context": "Representative methods include, but are not limited to, the continuous bag-of-words (CBOW) model [21], the skipgram (SG) model [21, 27], and the global vector (GloVe) model [22].", "startOffset": 97, "endOffset": 101}, {"referenceID": 19, "context": "Representative methods include, but are not limited to, the continuous bag-of-words (CBOW) model [21], the skipgram (SG) model [21, 27], and the global vector (GloVe) model [22].", "startOffset": 127, "endOffset": 135}, {"referenceID": 25, "context": "Representative methods include, but are not limited to, the continuous bag-of-words (CBOW) model [21], the skipgram (SG) model [21, 27], and the global vector (GloVe) model [22].", "startOffset": 127, "endOffset": 135}, {"referenceID": 20, "context": "Representative methods include, but are not limited to, the continuous bag-of-words (CBOW) model [21], the skipgram (SG) model [21, 27], and the global vector (GloVe) model [22].", "startOffset": 173, "endOffset": 177}, {"referenceID": 19, "context": "Rather than seeking to learn a statistical language model, the CBOW model manages to obtain a dense vector representation (embedding) of each word directly [21].", "startOffset": 156, "endOffset": 160}, {"referenceID": 19, "context": "where vvwwtt denotes the vector representation of the word w at position t; V is the size of the vocabulary; and vvww\ufffdtt denotes the (weighted) average of the vector representations of the context words of wt [21, 26].", "startOffset": 209, "endOffset": 217}, {"referenceID": 24, "context": "where vvwwtt denotes the vector representation of the word w at position t; V is the size of the vocabulary; and vvww\ufffdtt denotes the (weighted) average of the vector representations of the context words of wt [21, 26].", "startOffset": 209, "endOffset": 217}, {"referenceID": 25, "context": "The concept of CBOW is motivated by the distributional hypothesis [27], which states that words with similar meanings often occur in similar contexts, and it is thus suggested to look for wt whose word representation can capture its context distributions well.", "startOffset": 66, "endOffset": 70}, {"referenceID": 19, "context": "In contrast to the CBOW model, the SG model employs an inverse training objective for learning word representations with a simplified feed-forward neural network [21, 28].", "startOffset": 162, "endOffset": 170}, {"referenceID": 26, "context": "In contrast to the CBOW model, the SG model employs an inverse training objective for learning word representations with a simplified feed-forward neural network [21, 28].", "startOffset": 162, "endOffset": 170}, {"referenceID": 26, "context": "In the implementations of CBOW and SG, the hierarchical soft-max algorithm [28, 29] and the negative sampling algorithm [28, 30] can make the training process more efficient and effective.", "startOffset": 75, "endOffset": 83}, {"referenceID": 27, "context": "In the implementations of CBOW and SG, the hierarchical soft-max algorithm [28, 29] and the negative sampling algorithm [28, 30] can make the training process more efficient and effective.", "startOffset": 75, "endOffset": 83}, {"referenceID": 26, "context": "In the implementations of CBOW and SG, the hierarchical soft-max algorithm [28, 29] and the negative sampling algorithm [28, 30] can make the training process more efficient and effective.", "startOffset": 120, "endOffset": 128}, {"referenceID": 28, "context": "In the implementations of CBOW and SG, the hierarchical soft-max algorithm [28, 29] and the negative sampling algorithm [28, 30] can make the training process more efficient and effective.", "startOffset": 120, "endOffset": 128}, {"referenceID": 20, "context": "The GloVe model suggests that an appropriate starting point for word representation learning should be associated with the ratios of co-occurrence probabilities rather than the prediction probabilities [22].", "startOffset": 202, "endOffset": 206}, {"referenceID": 7, "context": "Second, it is worthy to note that SG (trained with the negative sampling algorithm) and GloVe have an implicit/explicit relation with the classic weighted matrix factorization approach, while the major difference is that SG and GloVe concentrate on rendering the word-by-word cooccurrence matrix but weighted matrix factorization is usually concerned with decomposing the word-by-document matrix [9, 31, 32].", "startOffset": 396, "endOffset": 407}, {"referenceID": 29, "context": "Second, it is worthy to note that SG (trained with the negative sampling algorithm) and GloVe have an implicit/explicit relation with the classic weighted matrix factorization approach, while the major difference is that SG and GloVe concentrate on rendering the word-by-word cooccurrence matrix but weighted matrix factorization is usually concerned with decomposing the word-by-document matrix [9, 31, 32].", "startOffset": 396, "endOffset": 407}, {"referenceID": 30, "context": "Second, it is worthy to note that SG (trained with the negative sampling algorithm) and GloVe have an implicit/explicit relation with the classic weighted matrix factorization approach, while the major difference is that SG and GloVe concentrate on rendering the word-by-word cooccurrence matrix but weighted matrix factorization is usually concerned with decomposing the word-by-document matrix [9, 31, 32].", "startOffset": 396, "endOffset": 407}, {"referenceID": 31, "context": "It is worthy to note that using SVD to derive the word representations is similar in spirit to latent semantic analysis (LSA) but using the word-word co-occurrence matrix instead of the word-by-document co-occurrence matrix [33].", "startOffset": 224, "endOffset": 228}, {"referenceID": 21, "context": "Inspired by the vector space model (VSM), a straightforward way to leverage the word embedding methods for extractive SDS is to represent a sentence Si (and a document D to be summarized) by averaging the vector representations of words occurring in the sentence Si (and the document D) [23, 25]:", "startOffset": 287, "endOffset": 295}, {"referenceID": 23, "context": "Inspired by the vector space model (VSM), a straightforward way to leverage the word embedding methods for extractive SDS is to represent a sentence Si (and a document D to be summarized) by averaging the vector representations of words occurring in the sentence Si (and the document D) [23, 25]:", "startOffset": 287, "endOffset": 295}, {"referenceID": 32, "context": "To mitigate the deficiency of the cosine similarity measure, we employ a triplet learning model to enhance the estimation of the similarity degree between a pair of representations [34-36].", "startOffset": 181, "endOffset": 188}, {"referenceID": 33, "context": "To mitigate the deficiency of the cosine similarity measure, we employ a triplet learning model to enhance the estimation of the similarity degree between a pair of representations [34-36].", "startOffset": 181, "endOffset": 188}, {"referenceID": 34, "context": "To mitigate the deficiency of the cosine similarity measure, we employ a triplet learning model to enhance the estimation of the similarity degree between a pair of representations [34-36].", "startOffset": 181, "endOffset": 188}, {"referenceID": 32, "context": "By applying the passive-aggressive learning algorithm presented in [34], we can derive the similarity function R such that all triplets obey", "startOffset": 67, "endOffset": 71}, {"referenceID": 32, "context": "Then, W can be obtained by applying an efficient sequential learning algorithm iteratively over the triplets [34, 35].", "startOffset": 109, "endOffset": 117}, {"referenceID": 33, "context": "Then, W can be obtained by applying an efficient sequential learning algorithm iteratively over the triplets [34, 35].", "startOffset": 109, "endOffset": 117}, {"referenceID": 35, "context": "The simplest way is to estimate a unigram language model (ULM) based on the frequency of each distinct word w occurring in S, with the maximum likelihood (ML) criterion [37, 38]:", "startOffset": 169, "endOffset": 177}, {"referenceID": 36, "context": "The simplest way is to estimate a unigram language model (ULM) based on the frequency of each distinct word w occurring in S, with the maximum likelihood (ML) criterion [37, 38]:", "startOffset": 169, "endOffset": 177}, {"referenceID": 37, "context": "The dataset used in this study is the MATBN broadcast news corpus collected by the Academia Sinica and the Public Television Service Foundation of Taiwan between November 2001 and April 2003 [39].", "startOffset": 191, "endOffset": 195}, {"referenceID": 38, "context": "For the assessment of summarization performance, we adopted the widely-used ROUGE metrics [40].", "startOffset": 90, "endOffset": 94}, {"referenceID": 15, "context": "All the experimental results reported hereafter are obtained by calculating the Fscores [17] of these ROUGE metrics.", "startOffset": 88, "endOffset": 92}, {"referenceID": 39, "context": "A subset of 25-hour speech data from MATBN compiled from November 2001 to December 2002 was used to bootstrap the acoustic training with the minimum phone error rate (MPE) criterion and a training data selection scheme [41].", "startOffset": 219, "endOffset": 223}, {"referenceID": 38, "context": "(14) [40].", "startOffset": 5, "endOffset": 9}], "year": 2015, "abstractText": "Owing to the rapidly growing multimedia content available on the Internet, extractive spoken document summarization, with the purpose of automatically selecting a set of representative sentences from a spoken document to concisely express the most important theme of the document, has been an active area of research and experimentation. On the other hand, word embedding has emerged as a newly favorite research subject because of its excellent performance in many natural language processing (NLP)-related tasks. However, as far as we are aware, there are relatively few studies investigating its use in extractive text or speech summarization. A common thread of leveraging word embeddings in the summarization process is to represent the document (or sentence) by averaging the word embeddings of the words occurring in the document (or sentence). Then, intuitively, the cosine similarity measure can be employed to determine the relevance degree between a pair of representations. Beyond the continued efforts made to improve the representation of words, this paper focuses on building novel and efficient ranking models based on the general word embedding methods for extractive speech summarization. Experimental results demonstrate the effectiveness of our proposed methods, compared to existing state-of-the-art methods.", "creator": "Acrobat PDFMaker 11 Word \u7248"}}}