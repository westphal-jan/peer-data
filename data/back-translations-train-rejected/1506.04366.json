{"id": "1506.04366", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "14-Jun-2015", "title": "Artificial general intelligence through recursive data compression and grounded reasoning: a position paper", "abstract": "This paper presents a tentative outline for the construction of an artificial, generally intelligent system (AGI). It is argued that building a general data compression algorithm solving all problems up to a complexity threshold should be the main thrust of research. A measure for partial progress in AGI is suggested. Although the details are far from being clear, some general properties for a general compression algorithm are fleshed out. Its inductive bias should be flexible and adapt to the input data while constantly searching for a simple, orthogonal and complete set of hypotheses explaining the data. It should recursively reduce the size of its representations thereby compressing the data increasingly at every iteration.", "histories": [["v1", "Sun, 14 Jun 2015 09:29:11 GMT  (421kb,D)", "http://arxiv.org/abs/1506.04366v1", "27 pages, 3 figures, position paper"]], "COMMENTS": "27 pages, 3 figures, position paper", "reviews": [], "SUBJECTS": "cs.AI", "authors": ["arthur franz"], "accepted": false, "id": "1506.04366"}, "pdf": {"name": "1506.04366.pdf", "metadata": {"source": "CRF", "title": "Artificial general intelligence through recursive data compression and grounded reasoning: a position paper", "authors": ["Arthur Franz"], "emails": ["franz@fias.uni-frankfurt.de"], "sections": [{"heading": null, "text": "Based on this basic ability, a grounded system of thought is proposed. It is argued how grounded and flexible bases of traits from hypotheses enable imaginative thinking. While the simulation of representational content at the mental level constitutes much of the power of propositional logic, compression leads to simple hypotheses that allow the recognition and verification of universally quantified statements. Together, it highlights how general compression and grounded thinking could explain the birth and growth of first concepts about the world and common sense about it."}, {"heading": "1 Approaching general data compression 4", "text": "1.1 Simple, but general......................................................................"}, {"heading": "2 Properties of the general compressor 8", "text": "2.1 Data-dependent search room expansion.................... 9 2.2 Features and hypotheses.............................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................."}, {"heading": "3 Grounded reasoning 14", "text": "3.1 What is grounding?.........................................................................................................................................................................................................................."}, {"heading": "4 Role of compression and grounding in learning the world\u2019s concepts 23", "text": "All of this is just a matter of time before it becomes a question of whether and what the human condition is. (...) There are a number of ideas that I have developed in recent years regarding the emergence of a system that has artificial intelligence (AGI). Although I have come into contact with them myself, most of them are not new and widespread. (...) The idea that this is a general concept should be emphasized here. (...) Unfortunately, after early unsuccessful attempts in artificial intelligence (AI), we have shifted our focus to narrowly defined problems and tasks that have become known as \"narrow AI\": world level in chess, Jeopardy, backgammon, self-driving cars, and myriad other commercial applications."}, {"heading": "1 Approaching general data compression", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "1.1 Simple but general", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "1.1.1 Simplicity of tasks", "text": "Considering the form of the universal previous one, one can consider the universal search. For example, the Levin search executes all kinds of programs, starting with the shortest until one of them generates the required order. Although generally, it is not surprising that it is a computationally expensive approach that is rarely applicable in practice.On the other side of the spectrum, we do not have general but hopefully mathematically comprehensible approaches: common AI algorithms and machine learning techniques. Why could they not be generalized? The problem that all these techniques face at some point is known as the curse of dimensionality. Looking at the (algorithmic) complexity and variety of tasks that are solved by typical today's algorithms, we observe that most, if not all, will be highly specific and many will be able to solve quite complex tasks (fig. 1.1). Algorithms from the field of data compression are no exception."}, {"heading": "1.1.2 Simplicity of the algorithm", "text": "Given that I have set myself the goal of compressing generic but simple data sets, does the algorithm that performs this task also have to be complex or rather simple? Therefore, from the point of view of the \"narrow AI,\" the programmer has to fully anticipate all the data situations that his algorithm might be exposed to, which would otherwise lead to errors. Does this mean that the general AI algorithm itself could be quite simple? A biological argument points in this direction (Berglas, 2008). Human intelligence ultimately has to be encoded in DNA. Human DNA consists of only 3 billion base pairs, and there are four bases (A, C, T and G), one base carrying the information of 2 bits."}, {"heading": "1.2 A measure for partial progress in AGI", "text": "The reason for this disorientation is that any algorithm that has achieved part of what we call intelligent behavior cannot be generalized to generalize a broader range of behaviors. Therefore, we could not say whether we have made any progress in the right direction or whether we are all on the wrong track. As Dreyfus (1992) cynically notes, progress in AI is like the man trying to climb a tree: \"One can report steady progress until the top of the tree is reached.\" Because the dead ends have been growing skepticism in AI communality around the world, Hutter (2005) has partially solved the problem mathematically. \""}, {"heading": "1.3 How many sequences are constructively compressible?", "text": "In the theory of Kolmogorov complexity, it is known that most strings cannot be compressed; more precisely, only exponentially few O (2n \u2212 m) binary strings of length n can be compressed by m-bits (see e.g. Sipser, 2012). Interestingly, the number of predictable sequences is also tightly limited by an expression of the same order of magnitude. Since we cannot compress most sequences, it is sufficient to find programs for this small fraction of compressible sequences. In addition, we must be aware that an optimal algorithm that compresses all compressible sequences may not exist. Finally, it is not clear whether all the information needed to close the shortest program is present in the sequence itself or whether additional knowledge is required."}, {"heading": "2 Properties of the general compressor", "text": "Conventionally, when developing an algorithm, one is implicitly forced to make a decision: Either the algorithm is equipped with a strong inductive bias against a specific, narrow class of data (e.g. linear regression), which requires careful processing of the data and verification of the requirements of the algorithm (e.g. normality of distributions), or one uses structures that can handle broad classes of data, such as neural networks, but lead to the curse of dimensionality. The former leads to efficient conclusions, but collapses when the data is not available in the appropriate format. The latter is widely applicable, but the struggle is associated with low convergence rates, local minima, or overadjustments. In both cases, careful coordination by the programmer is required. Christoph von der Malsburg2 diagnosed this situation rather cynically, saying that most of the intelligence of the final algorithm lies not in the algorithm itself, but in the intelligent coordination of the programmer."}, {"heading": "2.1 Data-dependent search space expansion", "text": "How are we to solve this dilemma? My suggestion is that the inductive bias should change dynamically when data arrives. To illustrate the idea, consider the following order: 1, 3, 1, 3, 2, 4, 2, 4, 2, 3, 5, 3, 5, 4, 6, 4, 6, 6, 6, 4, 4, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6,"}, {"heading": "1, 4, 1, 4, 2, 5, 2, 5, 2, 3, 6, 3, 6, 3, 6, 4, 7, 4, 7, 4, 7, 4, ...", "text": "This year it is more than ever before."}, {"heading": "2.2 Features and hypothesis sequences", "text": "The crucial question will be how to construct the set of the simplest hypotheses consistent with the sequence part seen so far. If we solve this problem for arbitrary sequences, I suspect that the most difficult task for a general data compressor will be solved. If, for example, we look at a sequence starting with 1, 3,.. Suppose we look at the null hypothesis that it is a first-order deterministic sequence that can be expanded by addition. Then, the only unknown is the sum that can be adjusted to 2 - a small search space - and the sequence can be continued to 1, 3, 5, 7, 9,. There are several ways to expand the null hypothesis: it can be challenged in three possible ways. The sequence could be \u2022 indeterministic, \u2022 higher-order elements of higher order Markov or non-Markovian hypotheses at all, or \u2022 meaningless mathematical functions or any combination of them."}, {"heading": "2.3 Measuring progress: the compression rate", "text": "In our case, we want to derive programs that generate a sequence from it. Consider the finite string with the length 16: 00011100000000. Suppose it is defined on the domain {0, 1} and every entry drawn from it with the probability p = 0.5, then its entropy is H0 = \u2212 16 log2 (p) = 16 bit. It takes 16 bit memory to store it. Suppose we have derived a parametric program that \"Start at position n and write l entries, all others are zero.\" Since n and l can be between 1 and 16, each of them requires the specification of Hpars = log2 16 = 4 bit. In addition, the program itself needs memory Hprog. The goal is to maximize the compression rate 1 \u2212 Hprog + HparsH0every time a new representation is found."}, {"heading": "2.4 Recursiveness", "text": "Suppose such blocks of ones occur 10 times in a series of lengths 1024. Specifying all starting positions and lengths then takes H (1) pars = 2 \u00b7 10 \u00b7 log2 (1024) = 200 bits. Neglecting the size of the program corresponds to a considerable compression rate of 1 \u2212 200 / 1024 = 80.4%. But suppose we discover a regularity in the starting positions and lengths, say ni = 100 \u00b7 i and li = 4. Then only one length needs to be specified and the step size (100), which only takes H (2) Pars = 20 bits and presses the total compression rate to 1 \u2212 20 / 1024 = 98%. In this way, data can be compressed recursively in the sense that the same data compression machine is first applied to the data itself and then compressed recursively to the parameters of the models."}, {"heading": "2.5 Orthogonality of the feature basis", "text": "In Chapter 2.2, we gave an example of three characteristics. These characteristics are orthogonal in the sense that the specification of each of these characteristics does not contain information about any of the others. Markovianity has nothing to do with determinism of the function applied, nor specifies (in) determinism dependency structures or the function applied, etc. Formally, the pairings of mutual information between all the characteristics should be zero. We should also aim to find an orthogonal basis for the description of a data set, otherwise characteristics share information and lead to redundancy in the representations, which implies a lower overall compression rate. Orthogonality also specifies the search process. Suppose we have answered the question about the dependency structure and found that the current sequence input only depends on the previous one. Then, all remaining questions boil down to describing this dependence and can be tackled independently. In other words, only orthogonal characteristics need to be taken into account."}, {"heading": "2.6 Extracting orthogonal features", "text": "Once orthogonality is established, the search space for a feature base is greatly reduced. However, features must be extracted somehow. In the Principal Component Analysis (PCA), a vector is first defined in each step that points in the direction of the greatest variance, and then the data cloud is projected onto the surface perpendicular to that vector, so that the variance is canceled in its direction. Afterwards, only the residual variance in the perpendicular direction is looked at in such a way that ultimately an orthogonal base is found that is \"explained\" by the vectors according to the variance. Similarly, orthogonal features can be extracted by focusing on the residual variance of the data. For example, if we look at a point B, which lies exactly in the middle between two other points, A and C. Let us provide the features \"distance\" and \"angle\" to the system. First, the system would notice that the distances A-B and B-C are equal, and thus the distances are equivalent."}, {"heading": "2.7 Interpretation rivalry", "text": "Someone said that the idea of dividing the AI field into a multitude of subfields marked the beginning of the failure of the entire field. Finally, perception requires reasoning, reasoning requires learning, learning must be based on planning and vice versa - all subfields are actually closely interconnected in the human mind. Therefore, trying to solve them separately from the others can progress in general AI.For example, image segmentation in computer vision suffers from the problem that our ability to segment an image into individual objects and their parts depends heavily on our knowledge of the objects. As I will argue in ch. 4.1, the conceptualization of objects is driven by compression. Therefore, an image or sequence should be segmented in such a way that compression is maximized. I suggest that a sequence should be segmented in these cases if all segments are highly compressible, while the whole sequence cannot be easily compressed."}, {"heading": "3 Grounded reasoning", "text": "Although general data compression seems to be a central part of AGI, some other important issues, such as language, memory, reasoning, common sense, resourcefulness, brittleness, and many others, keep intimidating the researcher. Far from claiming to have solved anything, I will introduce my ideas about some of them and highlight the way general data compression works. Suppress, general compression works. Then, what? One of the most burning questions in AI is the problem that AI systems don't really know anything about the world, healthy knowledge that a three-year-old has. As Marvin Minksy put it, \"No program can look around a room today and identify the things that correspond to his eyes\" (Minsky, 2011). The problem is not just the identification, but the ability to understand and describe the objects and to know how they work. In this section, I will argue why decompressed thinking is an important step toward, of course, compressing them and AGI is common sense and AGI."}, {"heading": "3.1 What is grounding?", "text": "In his policy paper, Harnad (1990) addresses the so-called symbol eradication problem - a symptom of a disease of purely symbolic AI systems: \"How can the semantic interpretation of a formal system of symbols only be based on its (arbitrary) forms, based on anything other than other meaningless symbols?\" However, I propose to divide the problem into two sub-problems. The first is a philosophical problem called intentionality: How can symbols or other representations of this matter re-exist in the world? Where is the invisible arrow pointing from the symbol DOG to the real dog? And how can the symbol DOG ever express the unspeakable meaning of a real dog dog? There is a huge philosophical discussion about whether computers could ever think (see e.g. Dreyfus, 1992)."}, {"heading": "3.2 Resourcefulness", "text": "In fact, the fact is that most of them are able to survive themselves without there being a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is"}, {"heading": "3.3 Formal logic for commonsense?", "text": "The AI community has been aware of the problem of common sense for some time, but has tackled it with limited success, unfortunately (Mueller, 2010). I suggest that the main reason for this is the lack of grounding of representations. Grounding of representations has been neglected for quite a while, although there have been calls for it (Harnad, 1990; Barsalou, 1999). Originally, the call for grounding of symbols was because symbols are grounding, which is usually represented by the use of formal logic. Consider a healthy problem, such as the use of a string to bind a plant to a rod that has penetrated the ground. The prevailing method in the rational argumentation community is to formalize the problem so that all possible valid statements can be logically inferred from the formal logical system. The idea is to encode abstract relationships between the units of the situation, add some general characteristics of the space, and then express the situation through multiple situations in the system, all of the situation, and the situation of time."}, {"heading": "3.4 The world as its own model: reasoning without formal logic", "text": "In fact, most of them will be able to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move and to move."}, {"heading": "3.5 Universal quantification", "text": "An important problem to be solved in simulation theories is universal quantification. After verifying the truthfulness of statements for some particular simulated samples, how can it be concluded that the same truth value is observed for all samples under current conditions? Consider a probable explanation: What are the chances of selecting a random isosceles triangle so that the base is randomly cut exactly in the middle of the vertical plane, but just good enough to handle most situations in life correctly. If the system maintains hypotheses about the background of the reasoning, it can evaluate the probability that a certain unpredicted curiosity of the sample was generated by chance. Of course, one would have to specify what a curiosity is and how it should be identified."}, {"heading": "3.6 Testing hypotheses, intervention", "text": "Considering a number of possible hypotheses, the goal was to test whether the other end of the bar could be reached. As can be shown, structural learning in Bayesian networks proceeds much faster when it is possible to intervene and observe the effects of the intervention (Pearl, 1988). Essentially, the question is about setting up scientific experiments. Which actions should be chosen to gain the most information about the given current hypotheses? The solution is known as the principle of maximum entropy (MacKay, 2003). Given a number of hypotheses and their previous probabilities resulting from the two previous data and the occam bias, the probability of each result of an experiment can be calculated. Maximum entropy states that the action should be chosen so that the entropy of the probability distribution of the possible results is maximum. In other words, all results should be viewed with the same probability."}, {"heading": "4 Role of compression and grounding in learning the world\u2019s concepts", "text": "How would all this, compression, grounding, help the system understand the real world with its complex concepts? Let's take static, black line drawings on a white background as an example of the surroundings of an AGI system. Eventually, the system will meet Minsky's demand for the ability to talk about the objects in the drawing. The scene should not be pre-determined and could include any everyday scene, such as a landscape of houses, cars, and trees, or a room with furniture and various artifacts. How is compression and grounding useful to accomplish such a task?"}, {"heading": "4.1 Conceptualizing objects and relations through compression", "text": "This year it is more than ever before."}, {"heading": "4.2 Grounded knowledge bases", "text": "Despite all the advantages of grounded thinking and representation, the close connection to the present input will result in mere fleeting representations that will be forgotten immediately after the relevant input disappears, raising the question of permanent knowledge retention in a manner consistent with current ideas.A timid and admittedly incomplete idea is to store knowledge about objects in the form of typical templates. A template is an image of an object that is fully stored in the system's memory. Storing complete images is necessary for grounded reasoning, since the system must retain the ability to think about imaginary objects in arbitrary granularity. Finally, it is doubtful for sufficiently complex objects such as a Mercedes that the system can or should store a complete feature base for it. If the basis is not complete, not all the details of the object can be restored from the foundation to the mental stage by sampling."}, {"heading": "4.3 Grounded commonsense reasoning", "text": "In fact, the fact is that most of them are able to move to another world, where they are able, where they are able to move."}], "references": [{"title": "Destin: A scalable deep learning architecture with application to high-dimensional robust pattern recognition", "author": ["Itamar Arel", "Derek Rose", "Robert Coop"], "venue": "In AAAI Fall Symposium: Biologically Inspired Cognitive Architectures,", "citeRegEx": "Arel et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Arel et al\\.", "year": 2009}, {"title": "Principles of synthetic intelligence PSI: an architecture of motivated cognition, volume 4", "author": ["Joscha Bach"], "venue": null, "citeRegEx": "Bach.,? \\Q2009\\E", "shortCiteRegEx": "Bach.", "year": 2009}, {"title": "Perceptions of perceptual symbols", "author": ["Lawrence W Barsalou"], "venue": "Behavioral and brain sciences,", "citeRegEx": "Barsalou.,? \\Q1999\\E", "shortCiteRegEx": "Barsalou.", "year": 1999}, {"title": "Artificial intelligence will kill our grandchildren", "author": ["Anthony Berglas"], "venue": "URL=<http://berglas.org/Articles/AIKillGrandchildren/ AIKillGrandchildren.html> Accessed May,", "citeRegEx": "Berglas.,? \\Q2008\\E", "shortCiteRegEx": "Berglas.", "year": 2008}, {"title": "Elements of information theory", "author": ["Thomas M Cover", "Joy A Thomas"], "venue": null, "citeRegEx": "Cover and Thomas.,? \\Q2012\\E", "shortCiteRegEx": "Cover and Thomas.", "year": 2012}, {"title": "Field review: Metacognition in computation: A selected research review", "author": ["Michael T Cox"], "venue": "Artificial intelligence,", "citeRegEx": "Cox.,? \\Q2005\\E", "shortCiteRegEx": "Cox.", "year": 2005}, {"title": "What computers still can\u2019t do: a critique of artificial reason", "author": ["Hubert L Dreyfus"], "venue": "MIT press,", "citeRegEx": "Dreyfus.,? \\Q1992\\E", "shortCiteRegEx": "Dreyfus.", "year": 1992}, {"title": "OpenCogPrime: A cognitive synergy based architecture for artificial general intelligence", "author": ["Ben Goertzel"], "venue": "In Cognitive Informatics,", "citeRegEx": "Goertzel.,? \\Q2009\\E", "shortCiteRegEx": "Goertzel.", "year": 2009}, {"title": "The scientist in the crib: Minds, brains, and how children learn", "author": ["Alison Gopnik", "Andrew N Meltzoff", "Patricia K Kuhl"], "venue": "William Morrow & Co,", "citeRegEx": "Gopnik et al\\.,? \\Q1999\\E", "shortCiteRegEx": "Gopnik et al\\.", "year": 1999}, {"title": "The symbol grounding problem", "author": ["Stevan Harnad"], "venue": "Physica D: Nonlinear Phenomena,", "citeRegEx": "Harnad.,? \\Q1990\\E", "shortCiteRegEx": "Harnad.", "year": 1990}, {"title": "Universal Artificial Intelligence: Sequential Decisions based on Algorithmic Probability", "author": ["Marcus Hutter"], "venue": "doi: 10. 1007/b138233. URL http://www.hutter1.net/ai/uaibook.htm", "citeRegEx": "Hutter.,? \\Q2005\\E", "shortCiteRegEx": "Hutter.", "year": 2005}, {"title": "How many strings are easy to predict", "author": ["Yuri Kalnishkan", "Volodya Vovk", "Michael V Vyugin"], "venue": "In Learning Theory and Kernel Machines,", "citeRegEx": "Kalnishkan et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Kalnishkan et al\\.", "year": 2003}, {"title": "The discovery of structural form", "author": ["Charles Kemp", "Joshua B Tenenbaum"], "venue": "Proceedings of the National Academy of Sciences,", "citeRegEx": "Kemp and Tenenbaum.,? \\Q2008\\E", "shortCiteRegEx": "Kemp and Tenenbaum.", "year": 2008}, {"title": "The singularity is near", "author": ["Ray Kurzweil"], "venue": "When humans transcend biology. Penguin,", "citeRegEx": "Kurzweil.,? \\Q2005\\E", "shortCiteRegEx": "Kurzweil.", "year": 2005}, {"title": "A collection of definitions of intelligence", "author": ["Shane Legg", "Marcus Hutter"], "venue": "Frontiers in Artificial Intelligence and Applications,", "citeRegEx": "Legg and Hutter.,? \\Q2007\\E", "shortCiteRegEx": "Legg and Hutter.", "year": 2007}, {"title": "Cyc: A large-scale investment in knowledge infrastructure", "author": ["Douglas B Lenat"], "venue": "Communications of the ACM,", "citeRegEx": "Lenat.,? \\Q1995\\E", "shortCiteRegEx": "Lenat.", "year": 1995}, {"title": "An introduction to Kolmogorov complexity and its applications", "author": ["Ming Li", "Paul MB Vit\u00e1nyi"], "venue": null, "citeRegEx": "Li and Vit\u00e1nyi.,? \\Q2009\\E", "shortCiteRegEx": "Li and Vit\u00e1nyi.", "year": 2009}, {"title": "Information theory, inference, and learning algorithms, volume", "author": ["David JC MacKay"], "venue": "Citeseer,", "citeRegEx": "MacKay.,? \\Q2003\\E", "shortCiteRegEx": "MacKay.", "year": 2003}, {"title": "Artificial intelligence meets natural stupidity", "author": ["Drew McDermott"], "venue": "ACM SIGART Bulletin,", "citeRegEx": "McDermott.,? \\Q1976\\E", "shortCiteRegEx": "McDermott.", "year": 1976}, {"title": "The emotion machine", "author": ["Marvin Minsky"], "venue": "New York: Pantheon,", "citeRegEx": "Minsky.,? \\Q2006\\E", "shortCiteRegEx": "Minsky.", "year": 2006}, {"title": "Interior grounding, reflection, and self-consciousness", "author": ["Marvin Minsky"], "venue": "Information and Computation,", "citeRegEx": "Minsky.,? \\Q2011\\E", "shortCiteRegEx": "Minsky.", "year": 2011}, {"title": "A substrate for accountable layered systems", "author": ["Bo Morgan"], "venue": "PhD thesis, Massachusetts Institute of Technology,", "citeRegEx": "Morgan.,? \\Q2013\\E", "shortCiteRegEx": "Morgan.", "year": 2013}, {"title": "Commonsense reasoning", "author": ["Erik T Mueller"], "venue": null, "citeRegEx": "Mueller.,? \\Q2010\\E", "shortCiteRegEx": "Mueller.", "year": 2010}, {"title": "Probabilistic reasoning in intelligent systems: networks of plausible inference", "author": ["Judea Pearl"], "venue": null, "citeRegEx": "Pearl.,? \\Q1988\\E", "shortCiteRegEx": "Pearl.", "year": 1988}, {"title": "EM-ONE: Architecture for Reflective Commonsense Thinking", "author": ["Push Singh"], "venue": "PhD thesis, Massachusetts Institute of Technology,", "citeRegEx": "Singh.,? \\Q2005\\E", "shortCiteRegEx": "Singh.", "year": 2005}, {"title": "Introduction to the Theory of Computation", "author": ["Michael Sipser"], "venue": "Cengage Learning,", "citeRegEx": "Sipser.,? \\Q2012\\E", "shortCiteRegEx": "Sipser.", "year": 2012}, {"title": "Complexity-based induction systems: comparisons and convergence theorems", "author": ["Ray Solomonoff"], "venue": "Information Theory, IEEE Transactions on,", "citeRegEx": "Solomonoff.,? \\Q1978\\E", "shortCiteRegEx": "Solomonoff.", "year": 1978}, {"title": "A formal theory of inductive inference", "author": ["Ray J Solomonoff"], "venue": "i. Information and control,", "citeRegEx": "Solomonoff.,? \\Q1964\\E", "shortCiteRegEx": "Solomonoff.", "year": 1964}, {"title": "Experimentally induced visual projections into auditory thalamus and cortex", "author": ["Mriganka Sur", "Preston E Garraghty", "Anna W Roe"], "venue": null, "citeRegEx": "Sur et al\\.,? \\Q1988\\E", "shortCiteRegEx": "Sur et al\\.", "year": 1988}, {"title": "Perceptual simulations can be as expressive as first-order logic", "author": ["Hiroyuki Uchida", "Nicholas L Cassimatis", "JR Scally"], "venue": "Cognitive processing,", "citeRegEx": "Uchida et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Uchida et al\\.", "year": 2012}, {"title": "A Monte-Carlo AIXI approximation", "author": ["Joel Veness", "Kee Siong Ng", "Marcus Hutter", "William Uther", "David Silver"], "venue": "Journal of Artificial Intelligence Research,", "citeRegEx": "Veness et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Veness et al\\.", "year": 2011}, {"title": "Procedures as a representation for data in a computer program for understanding natural language", "author": ["Terry Winograd"], "venue": "Technical report, DTIC Document,", "citeRegEx": "Winograd.,? \\Q1971\\E", "shortCiteRegEx": "Winograd.", "year": 1971}], "referenceMentions": [{"referenceID": 13, "context": "Unfortunately, after early unsuccessful attempts research in artificial intelligence (AI) has moved its focus on solving narrowly defined problems and tasks, which became known as \u201cnarrow AI\u201d (Kurzweil, 2005): world level in chess, jeopardy, backgammon, self-driving cars, talking personal assistants and a myriad of other commercial applications.", "startOffset": 192, "endOffset": 208}, {"referenceID": 28, "context": "It is also known that in newborn ferrets neurons in the auditory cortex adopt characteristics of visual cells, if fed with stimuli from the visual pathway (Sur et al., 1988).", "startOffset": 155, "endOffset": 173}, {"referenceID": 10, "context": "After compiling a large set of definitions in the literature Legg and Hutter (2007) came up with a definition of intelligence that is consistent with most other attempts: \u201cIntelligence measures an agent\u2019s ability to achieve goals in a wide range of environments.", "startOffset": 70, "endOffset": 84}, {"referenceID": 10, "context": "After compiling a large set of definitions in the literature Legg and Hutter (2007) came up with a definition of intelligence that is consistent with most other attempts: \u201cIntelligence measures an agent\u2019s ability to achieve goals in a wide range of environments.\u201d This is exactly, what narrow AI does not achieve: it is programmed for a very specific well-defined set of environments. Any deviation from that narrow set most probably leads to failure of the system. Conversely, humans are usually able to solve all sorts of tasks in very diverse environments. Moreover, neuroscientific evidence teaches us, that brains are able to process data cross-modally, e.g. by transforming visual data to auditory or tactile stimuli in sensory substitution devices. It is also known that in newborn ferrets neurons in the auditory cortex adopt characteristics of visual cells, if fed with stimuli from the visual pathway (Sur et al., 1988). Those observations point to the hypothesis that the human brain is a general processor of quite diversely structured data. This idea is, of course, not new and is around at least since Simon and Newell\u2019s General Problem Solver developed in 1957. Although the problem is far from being solved practically, Hutter (2005) has developed a mathematical formulation and theoretical solution to the universal AGI problem, called AIXI.", "startOffset": 70, "endOffset": 1250}, {"referenceID": 8, "context": "Arguably, it is a formal and institutionalized reasoning method, but people, even infants, seem use it in more simple everyday situations (Gopnik et al., 1999).", "startOffset": 138, "endOffset": 159}, {"referenceID": 30, "context": "a Monte-Carlo approximation that uses prediction suffix trees that enable predicting binary D-order Markov sequences (Veness et al., 2011).", "startOffset": 117, "endOffset": 138}, {"referenceID": 7, "context": "Others try to fill the task space by \u201cgluing together\u201d various narrow algorithms that would, hopefully, synergistically cancel each other\u2019s combinatorial explosions (Goertzel, 2009).", "startOffset": 165, "endOffset": 181}, {"referenceID": 3, "context": "Does it mean that the general AI algorithm could actually be quite simple itself? A biological argument points in that direction (Berglas, 2008).", "startOffset": 129, "endOffset": 144}, {"referenceID": 16, "context": "In contrast to Kolmogorov complexity1, we use the time-bounded version \u2013 the Levin complexity \u2013 which is computable and includes a penalty term on computation time (Li and Vit\u00e1nyi, 2009):", "startOffset": 164, "endOffset": 186}, {"referenceID": 6, "context": "As Dreyfus (1992) cynically remarks, progress in AI is like the man who tries to get to the moon by climbing a tree: \u201cone can report steady progress, all the way to the top of the tree\u201d.", "startOffset": 3, "endOffset": 18}, {"referenceID": 6, "context": "As Dreyfus (1992) cynically remarks, progress in AI is like the man who tries to get to the moon by climbing a tree: \u201cone can report steady progress, all the way to the top of the tree\u201d. Since dead ends have been ubiquitous there has been growing skepticism in the AI community. However, since Hutter (2005) has mathematically solved the AGI problem (!), and the core part to be made tractable is the compression part, we can formalize partial progress toward AGI as the extent to which general compression has been achieved.", "startOffset": 3, "endOffset": 308}, {"referenceID": 11, "context": "Interestingly, the number of predictable sequences are also tightly bounded by an expression of the same order of magnitude \u0398(2n\u2212m) (Kalnishkan et al., 2003).", "startOffset": 132, "endOffset": 157}, {"referenceID": 12, "context": "Kemp and Tenenbaum (2008) present an algorithm using hierarchical Bayesian inference in order to \u201cdiscover structural form\u201d, e.", "startOffset": 0, "endOffset": 26}, {"referenceID": 18, "context": "Only the remaining \u201cblind\u201d search is performed by the algorithm, which is then proudly announced to be \u201cintelligent\u201d (McDermott, 1976).", "startOffset": 117, "endOffset": 134}, {"referenceID": 5, "context": "In contrast to this, AI algorithms usually do not possess additional compression levels, except in the small field of metacognition research (Cox, 2005).", "startOffset": 141, "endOffset": 152}, {"referenceID": 24, "context": "different algorithms are used at meta-levels, except some notable examples from Marvin Minsky\u2019s group (Singh, 2005; Morgan, 2013).", "startOffset": 102, "endOffset": 129}, {"referenceID": 21, "context": "different algorithms are used at meta-levels, except some notable examples from Marvin Minsky\u2019s group (Singh, 2005; Morgan, 2013).", "startOffset": 102, "endOffset": 129}, {"referenceID": 20, "context": "identify the things that meet its eyes\u201d (Minsky, 2011).", "startOffset": 40, "endOffset": 54}, {"referenceID": 8, "context": "In his seminal paper, Harnad (1990) addresses the so-called symbol grounding problem \u2013 a symptom of a disease of purely symbolic AI systems: \u201cHow can the semantic interpretation of a formal symbol system be made intrinsic to the system, rather than just parasitic on the meanings in our heads? How can the meanings of the meaningless symbol tokens, manipulated solely on the basis of their (arbitrary) shapes, be grounded in anything but other meaningless symbols?\u201d I suggest to split the problem into two subproblems.", "startOffset": 22, "endOffset": 36}, {"referenceID": 0, "context": "It may be interjected that it is exactly those variably fine-grained representations that hierarchical network approaches such as DeSTIN (Arel et al., 2009) or Hawkins\u2019 Hierarchical Temporal Memories (Hawkins and Blakeslee, 2007) develop at each of their levels.", "startOffset": 137, "endOffset": 156}, {"referenceID": 19, "context": "Intelligent problem solving requires the ability to think in different ways about the problem, which Minsky coined as resourcefulness (Minsky, 2006).", "startOffset": 134, "endOffset": 148}, {"referenceID": 22, "context": "The AI community has been aware of the commonsense problem for quite a while, but tackled it with limited success, unfortunately (Mueller, 2010).", "startOffset": 129, "endOffset": 144}, {"referenceID": 9, "context": "The grounding of representations has been neglected for quite a while although there have been calls for it (Harnad, 1990; Barsalou, 1999).", "startOffset": 108, "endOffset": 138}, {"referenceID": 2, "context": "The grounding of representations has been neglected for quite a while although there have been calls for it (Harnad, 1990; Barsalou, 1999).", "startOffset": 108, "endOffset": 138}, {"referenceID": 15, "context": "Of course, this is exactly the ambition of projects like Cyc which try to do that since 1985 (Lenat, 1995).", "startOffset": 93, "endOffset": 106}, {"referenceID": 1, "context": "Second, being ungrounded means having to carry the structure of a real world situation into one\u2019s system which bears a practical danger that the hand coded or concluded relations between entities will not hold as the system is scaled up (Bach, 2009).", "startOffset": 237, "endOffset": 249}, {"referenceID": 1, "context": "Second, being ungrounded means having to carry the structure of a real world situation into one\u2019s system which bears a practical danger that the hand coded or concluded relations between entities will not hold as the system is scaled up (Bach, 2009). There is no guarantee that the encoded relations are actually the correct abstractions from the world situations. Third, ungrounded representations have difficulties reacting to new situations where a new context leads to different conclusions. Therefore, I suggest a simulation theory approach, much along the lines of Barsalou (1999), that keeps a tight connection to the world while being able to reason about it, as I will argue in the subsequent chapters.", "startOffset": 238, "endOffset": 587}, {"referenceID": 2, "context": "On the other hand, consider the approach of a grounded mental simulation, which is arguably the way, humans solve the task (Barsalou, 1999).", "startOffset": 123, "endOffset": 139}, {"referenceID": 29, "context": "can be performed by simulation theories (Uchida et al., 2012).", "startOffset": 40, "endOffset": 61}, {"referenceID": 6, "context": "Although this claim parallels Dreyfus\u2019 (1992) call for Heideggerian AI, I do not share his rejection of representations per se: we do need representations in our systems, but they shall be grounded in the world and the world\u2019s intrinsic structure should be used for reasoning.", "startOffset": 30, "endOffset": 46}, {"referenceID": 23, "context": "As can be shown, structure learning in Bayesian networks proceeds much faster, if it is possible to intervene and observe the effects of the intervention (Pearl, 1988).", "startOffset": 154, "endOffset": 167}, {"referenceID": 17, "context": "Which actions shall be chosen in order to gain most information about the data given current hypotheses? The solution is known as the principle of maximum entropy (MacKay, 2003).", "startOffset": 163, "endOffset": 177}, {"referenceID": 31, "context": "Even if such a description can be given such as in Winograd\u2019s famous Blocks World (Winograd, 1971), it is far from clear that a complete set of rules can be provided and that would be able to answer all queries.", "startOffset": 82, "endOffset": 98}], "year": 2015, "abstractText": "This paper presents a tentative outline for the construction of an artificial, generally intelligent system (AGI). It is argued that building a general data compression algorithm solving all problems up to a complexity threshold should be the main thrust of research. A measure for partial progress in AGI is suggested. Although the details are far from being clear, some general properties for a general compression algorithm are fleshed out. Its inductive bias should be flexible and adapt to the input data while constantly searching for a simple, orthogonal and complete set of hypotheses explaining the data. It should recursively reduce the size of its representations thereby compressing the data increasingly at every", "creator": "LaTeX with hyperref package"}}}