{"id": "1506.04422", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "14-Jun-2015", "title": "A Fast Incremental Gaussian Mixture Model", "abstract": "This work builds upon previous efforts in online incremental learning, namely the Incremental Gaussian Mixture Network (IGMN). The IGMN is capable of learning from data streams in a single-pass by improving its model after analyzing each data point and discarding it thereafter. Nevertheless, it suffers from the scalability point-of-view, due to its asymptotic time complexity of $\\operatorname{O}\\bigl(NKD^3\\bigr)$ for $N$ data points, $K$ Gaussian components and $D$ dimensions, rendering it inadequate for high-dimensional data. In this paper, we manage to reduce this complexity to $\\operatorname{O}\\bigl(NKD^2\\bigr)$ by deriving formulas for working directly with precision matrices instead of covariance matrices. The final result is a much faster and scalable algorithm which can be applied to high dimensional tasks. This is confirmed by applying the modified algorithm to high-dimensional classification datasets.", "histories": [["v1", "Sun, 14 Jun 2015 17:02:49 GMT  (113kb)", "http://arxiv.org/abs/1506.04422v1", "10 pages, no figures, draft submission to Plos One"], ["v2", "Thu, 18 Jun 2015 17:04:01 GMT  (114kb)", "http://arxiv.org/abs/1506.04422v2", "10 pages, no figures, draft submission to Plos One"]], "COMMENTS": "10 pages, no figures, draft submission to Plos One", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["rafael pinto", "paulo engel"], "accepted": false, "id": "1506.04422"}, "pdf": {"name": "1506.04422.pdf", "metadata": {"source": "CRF", "title": "A Fast Incremental Gaussian Mixture Model - Submission to PLOS Journals", "authors": ["Rafael Pinto", "Paulo Engel"], "emails": ["rcpinto@inf.ufrgs.br", "engel@inf.ufrgs.br"], "sections": [{"heading": null, "text": "ar Xiv: 150 6.04 422v 1 [cs.L G] 14 JuThis work builds on previous efforts in the field of online incremental learning, namely the Incremental Gaussian Mixture Network (IGMN). IGMN is able to learn in a single pass of data streams by improving its model after analyzing each data point and then discarding it. Nevertheless, it suffers from the point of view of scalability, due to its asymptotic time complexity of O (NKD3) for N data points, K-Gauss components and D dimensions, making it insufficient for high-dimensional data. In this paper, we succeed in reducing this complexity to O (NKD2) by deriving formulas for working directly with precision matrices instead of covariance matrices. The end result is a much faster and scalable algorithm that can be applied to high-dimensional tasks."}, {"heading": "1 Introduction", "text": "The Incremental Gaussian Mixture Network (IGMN) [?,?] is a monitored algorithm that approaches the EM algorithm [?]. It continuously creates and adapts a probabilistic model that matches all sequentially presented data, after each data point presentation and without the need to store any past data points. Its learning process is aggressive, meaning that only a single scan of the data is needed to obtain a consistent model. IGMN adopts a Gaussian mixing model of distribution components that can be expanded to accommodate new information from a data point entered, or reduced when fake components are identified along the learning process. Each data point assimilated by the model contributes to the sequential update of model parameters based on maximizing the probability of the data. The parameters are updated by the accumulation of relevant information."}, {"heading": "2 Incremental Gaussian Mixture Network", "text": "In the next sections we describe the current version of the IGMN algorithm."}, {"heading": "2.1 Learning", "text": "The algorithm does not start with components that are generated as needed (see subsection 2,2). If you enter x (a single instant data point), the processing step of the IGMN algorithm is as follows: First, the square Mahalanobis distance d2 (x, j) is calculated for each component j: d2M (x, j) = (x \u2212 \u00b5j) TC \u2212 1j (x \u2212 \u00b5j) (1), where \u00b5j is the jth component, Cj is its complete covariance matrix. If any d 2 (x, j) is smaller than \u03c72D, 1 \u2212 \u03b2 (the 1 \u2212 \u03b2 percentile of a chi \u2212 \u00b5 \u2212 spared distribution with D degrees of freedom, where D is the input dimensionality and \u03b2 is a user-defined meta parameter, e.g. 0.1), an update (0.1) occurs, and the following probabilities are calculated for each component as follows: p \u2212 x = 2k = (2) spamej = (D)."}, {"heading": "2.2 Creating New Components", "text": "If the update condition in the previous subsection is not met, then a new component j is created and initialized as follows: \u00b5j = x; spj = 1; vj = 1; p (j) = 1K \u2211 i = 1spi; Cj = \u03c3 2 iniIwo K already contains the new component and \u03c3ini can be obtained by: \u03c3ini = \u03b4std (x) (13), \u03b4 being a manually selected scaling factor (e.g. 0.01) and std being the default deviation of the dataset. Note that the IGMN is an online and incremental algorithm and therefore we may not have the entire dataset to extract descriptive statistics. In this case, the default deviation can only be an estimate (e.g. based on sensor boundaries from a robot platform), without affecting the algorithm."}, {"heading": "2.3 Removing Spurious Components", "text": "A component j is removed if vj > vmin and spj < spmin, where vmin and spmin are selected manually (e.g. 5.0 and 3.0, respectively).Again, p (k) must be adjusted for all k-K, k 6 = j with (12).In other words, each component is given a certain amount of time vmin to show its meaning for the model in the form of an accumulation of its posterior probabilities spj. PLOS 3 / 8"}, {"heading": "2.4 Inference", "text": "In the IGMN, each element can be predicted by any other element, by reconstructing data from the target elements (xt, a disk of the entire input vector x), estimating the trailing probabilities only using the given elements (xi, also a disk of the entire input vector x) as follows: p (j | xi) = p (xi | j) p (j) M \u2211 q = 1p (xi | q) p (q).j (14) It is similar to (3) except that it uses a modified input vector xi, removing the target elements xt from the calculations. Afterwards, xt can be reconstructed using the conditional mean equation: x, t = M \u2211 j = 1p (j | xi) (\u00b5j, t + Cj, tiC \u2212 1j, i (xi \u2212 \u00b5j, i))), where Cj, ti the submatrix of the jth component codiance matrix is the only part of the known element associated with the known part of the jrix."}, {"heading": "3 Fast IGMN", "text": "One of the contributions in this paper is the fact that Equation 1 (the squared formula) requires two updates that require a matrix inversion that exhibits an asymptotic time complexity of O (D3), for D dimensions (O (Dlog27 + O (1))) for the road algorithm, or at best O (D2.3728639) with the most up-to-date algorithms to date [?]. This makes the entire IGMN algorithm impractical for high-dimensional tasks. Here, we show how to work directly with the inversion of the covariance matrix (also referred to as precision or concentration matrix) for the entire process, avoiding costly inversions. First, let us specify C \u2212 1 = the precision matrix. Our task is to adjust all equations that include C to use the equations instead. Let's move on to the adjustment of the comatrix (11)."}, {"heading": "4 Experiments", "text": "To evaluate the performance of the proposed algorithm, 11 classification tasks (Table 1) were divided into the original and improved IGMN algorithms (\u03b4 = 1 and \u03b2 = 0, so that for each run a single component was created and we could only focus on acceleration due to the dimensionality).Although the results from 2x cross-validation and statistical significance from tested t-tests with p = 05.This experiment was intended to verify that both IGMN implementations produce exactly the same results that were confirmed and show that the proposed PLOS 6 / 8 algorithm 3 createInput: xK \u2190 K + 1 Return of the new Gaussian component K with \u00b5K = x, \u03b1 K = iniI-1 iniI, | CK | 1, spj = 1, vj = 1, p (j) = 1K-k = 1spiration improvements really deliver the expected acceleration (the Wemus algorithm for both IGMN packages)."}, {"heading": "5 Conclusion", "text": "We demonstrated how to work directly with precision matrices in the IGMN algorithm and avoid costly matrix inversions by performing rank-one updates, and also avoided determining calculations using a similar method, eliminating virtually any source of cubic complexity for the learning algorithm, resulting in significant acceleration for high-dimensional datasets, making the IGMN a good option for this type of tasks. While the inference operation still has cubic complexity, we argue that it has a much smaller impact on the total lifetime of the algorithm, as the number of outputs is usually much less than the number of inputs, which has been confirmed for one-dimensional outputs that require only scalar operations. In general, we could see that fast IGMN is a good option for monitored learning, with low runtime and good accuracy after adjusting the two main meta parameters and \u03b2 \u03b4."}], "references": [], "referenceMentions": [], "year": 2017, "abstractText": "This work builds upon previous efforts in online incremental learning, namely the Incremental Gaussian Mixture Network (IGMN). The IGMN is capable of learning from data streams in a single-pass by improving its model after analyzing each data point and discarding it thereafter. Nevertheless, it suffers from the scalability point-of-view, due to its asymptotic time complexity of O ( NKD ) for N data points, K Gaussian components and D dimensions, rendering it inadequate for high-dimensional data. In this paper, we manage to reduce this complexity to O ( NKD ) by deriving formulas for working directly with precision matrices instead of covariance matrices. The final result is a much faster and scalable algorithm which can be applied to high dimensional tasks. This is confirmed by applying the modified algorithm to high-dimensional classification datasets.", "creator": "LaTeX with hyperref package"}}}