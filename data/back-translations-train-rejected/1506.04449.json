{"id": "1506.04449", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "14-Jun-2015", "title": "Compressing Convolutional Neural Networks", "abstract": "Convolutional neural networks (CNN) are increasingly used in many areas of computer vision. They are particularly attractive because of their ability to \"absorb\" great quantities of labeled data through millions of parameters. However, as model sizes increase, so do the storage and memory requirements of the classifiers. We present a novel network architecture, Frequency-Sensitive Hashed Nets (FreshNets), which exploits inherent redundancy in both convolutional layers and fully-connected layers of a deep learning model, leading to dramatic savings in memory and storage consumption. Based on the key observation that the weights of learned convolutional filters are typically smooth and low-frequency, we first convert filter weights to the frequency domain with a discrete cosine transform (DCT) and use a low-cost hash function to randomly group frequency parameters into hash buckets. All parameters assigned the same hash bucket share a single value learned with standard back-propagation. To further reduce model size we allocate fewer hash buckets to high-frequency components, which are generally less important. We evaluate FreshNets on eight data sets, and show that it leads to drastically better compressed performance than several relevant baselines.", "histories": [["v1", "Sun, 14 Jun 2015 23:16:40 GMT  (536kb,D)", "http://arxiv.org/abs/1506.04449v1", null]], "reviews": [], "SUBJECTS": "cs.LG cs.CV cs.NE", "authors": ["wenlin chen", "james t wilson", "stephen tyree", "kilian q weinberger", "yixin chen"], "accepted": false, "id": "1506.04449"}, "pdf": {"name": "1506.04449.pdf", "metadata": {"source": "CRF", "title": "Compressing Convolutional Neural Networks", "authors": ["Wenlin Chen", "James T. Wilson", "Stephen Tyree", "Kilian Q. Weinberger", "Yixin Chen"], "emails": ["j.wilson}@wustl.edu", "styree@nvidia.com", "kilian@wustl.edu,", "chen@cse.wustl.edu"], "sections": [{"heading": "1 Introduction", "text": "In recent years, the number of people living in the US has increased significantly, not only in the US, but also in many other countries."}, {"heading": "2 Background", "text": "[8, 25, 30] has so far been investigated only as a technique for reducing the memory size. (8, 25, 30] In general, it can be seen as a method for dimensionality reduction that defines an input vector x-Rd to a much smaller feature space via a mapping function: Rd \u2192 Rk, where the mapping function of two approximately uniform auxiliary functions h: N,.) and N \u2192 {1, + 1}. The jth element of k-dimensional hashed input operations is defined as a combination of two approximately uniform auxiliary functions h: N \u2192 [1,.]. The key property of the hashing function is its preservation in internal product operations, where internal products produce the correct pre-hash internal product in expectation after hashing: E [2, x) >."}, {"heading": "4 Related Work", "text": "In fact, most of them will be able to play by the rules that they have established in recent years, and they will be able to play by the rules."}, {"heading": "5 Experimental Results", "text": "This year is the highest in the history of the country."}, {"heading": "6 Conclusion", "text": "In this paper, we present FreshNets, a method for learning Convolutionary Neural Networks with dramatically compressed model storage. FreshNets uses the hashing trick for parameterization-free random weight distribution and uses the smoothness inherent in Convolutionary Filters. FreshNets compresses parameters frequency-sensitively so that significant model parameters (e.g. low-frequency components) are better preserved. Thus, FreshNets maintains predictive accuracy significantly better than competing baselines at high compression rates."}], "references": [{"title": "Do deep nets really need to be deep", "author": ["J. Ba", "R. Caruana"], "venue": "In NIPS,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2014}, {"title": "Neural Networks for Pattern Recognition", "author": ["C.M. Bishop"], "venue": "Oxford University Press, Inc.,", "citeRegEx": "2", "shortCiteRegEx": null, "year": 1995}, {"title": "Efficient training of convolutional deep belief networks in the frequency domain for application to high-resolution 2d and 3d images", "author": ["T. Brosch", "R. Tam"], "venue": "Neural Computation, 27(1):211\u2013227,", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2015}, {"title": "Model compression", "author": ["C. Bucilua", "R. Caruana", "A. Niculescu-Mizil"], "venue": "KDD,", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2006}, {"title": "Compressing neural networks with the hashing trick", "author": ["W. Chen", "J.T. Wilson", "S. Tyree", "K.Q. Weinberger", "Y. Chen"], "venue": "ICML,", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2015}, {"title": "Torch7: A matlab-like environment for machine learning", "author": ["R. Collobert", "K. Kavukcuoglu", "C. Farabet"], "venue": "BigLearn, NIPS Workshop,", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2011}, {"title": "Low precision storage for deep learning", "author": ["M. Courbariaux", "Y. Bengio", "J.-P. David"], "venue": "arXiv preprint arXiv:1412.7024,", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2014}, {"title": "A sparse johnson: Lindenstrauss transform", "author": ["A. Dasgupta", "R. Kumar", "T. Sarl\u00f3s"], "venue": "Proceedings of the forty-second ACM symposium on Theory of computing, pages 341\u2013350. ACM,", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2010}, {"title": "Imagenet: A large-scale hierarchical image database", "author": ["J. Deng", "W. Dong", "R. Socher", "L.-J. Li", "K. Li", "L. Fei-Fei"], "venue": "Computer Vision and Pattern Recognition, 2009. CVPR 2009. IEEE Conference on, pages 248\u2013255. IEEE,", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2009}, {"title": "Predicting parameters in deep learning", "author": ["M. Denil", "B. Shakibi", "L. Dinh", "N. de Freitas"], "venue": "In NIPS,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2013}, {"title": "Exploiting linear structure within convolutional networks for efficient evaluation", "author": ["E.L. Denton", "W. Zaremba", "J. Bruna", "Y. LeCun", "R. Fergus"], "venue": "Advances in Neural Information Processing Systems, pages 1269\u20131277,", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2014}, {"title": "Neocognitron: A self-organizing neural network model for a mechanism of pattern recognition unaffected by shift in position", "author": ["K. Fukushima"], "venue": "Biological cybernetics, 36(4):193\u2013202,", "citeRegEx": "12", "shortCiteRegEx": null, "year": 1980}, {"title": "Compressing deep convolutional networks using vector quantization", "author": ["Y. Gong", "L. Liu", "M. Yang", "L. Bourdev"], "venue": "arXiv preprint arXiv:1412.6115,", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2014}, {"title": "Deep learning with limited numerical precision", "author": ["S. Gupta", "A. Agrawal", "K. Gopalakrishnan", "P. Narayanan"], "venue": "arXiv preprint arXiv:1502.02551,", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2015}, {"title": "Delving deep into rectifiers: Surpassing human-level performance on imagenet classification", "author": ["K. He", "X. Zhang", "S. Ren", "J. Sun"], "venue": "arXiv preprint arXiv:1502.01852,", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2015}, {"title": "Speeding up convolutional neural networks with low rank expansions", "author": ["M. Jaderberg", "A. Vedaldi", "A. Zisserman"], "venue": "arXiv preprint arXiv:1405.3866,", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2014}, {"title": "Imagenet classification with deep convolutional neural networks", "author": ["A. Krizhevsky", "I. Sutskever", "G.E. Hinton"], "venue": "NIPS,", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2012}, {"title": "An empirical evaluation of deep architectures on problems with many factors of variation", "author": ["H. Larochelle", "D. Erhan", "A.C. Courville", "J. Bergstra", "Y. Bengio"], "venue": "ICML, pages 473\u2013480,", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2007}, {"title": "Speeding-up convolutional neural networks using fine-tuned cp-decomposition", "author": ["V. Lebedev", "Y. Ganin", "M. Rakhuba", "I. Oseledets", "V. Lempitsky"], "venue": "arXiv preprint arXiv:1412.6553,", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2014}, {"title": "Unsupervised feature learning for audio classification using convolutional deep belief networks", "author": ["H. Lee", "P. Pham", "Y. Largman", "A.Y. Ng"], "venue": "Advances in neural information processing systems, pages 1096\u2013 1104,", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2009}, {"title": "Fast training of convolutional networks through ffts", "author": ["M. Mathieu", "M. Henaff", "Y. LeCun"], "venue": "arXiv preprint arXiv:1312.5851,", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2013}, {"title": "Discrete cosine transform: algorithms, advantages, applications", "author": ["K.R. Rao", "P. Yip"], "venue": "Academic press,", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2014}, {"title": "Learning separable filters", "author": ["R. Rigamonti", "A. Sironi", "V. Lepetit", "P. Fua"], "venue": "CVPR,", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2013}, {"title": "Facenet: A unified embedding for face recognition and clustering", "author": ["F. Schroff", "D. Kalenichenko", "J. Philbin"], "venue": "arXiv preprint arXiv:1503.03832,", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2015}, {"title": "Hash kernels for structured data", "author": ["Q. Shi", "J. Petterson", "G. Dror", "J. Langford", "A. Smola", "S. Vishwanathan"], "venue": "Journal of Machine Learning Research, 10:2615\u20132637, Dec.", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2009}, {"title": "Very deep convolutional networks for large-scale image recognition", "author": ["K. Simonyan", "A. Zisserman"], "venue": "CoRR, abs/1409.1556,", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2014}, {"title": "Going deeper with convolutions", "author": ["C. Szegedy", "W. Liu", "Y. Jia", "P. Sermanet", "S. Reed", "D. Anguelov", "D. Erhan", "V. Vanhoucke", "A. Rabinovich"], "venue": "arXiv preprint arXiv:1409.4842,", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2014}, {"title": "Fast convolutional nets with fbfft: A gpu performance evaluation", "author": ["N. Vasilache", "J. Johnson", "M. Mathieu", "S. Chintala", "S. Piantino", "Y. LeCun"], "venue": "arXiv preprint arXiv:1412.7580,", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2014}, {"title": "The jpeg still picture compression standard", "author": ["G.K. Wallace"], "venue": "Communications of the ACM, 34(4):30\u201344,", "citeRegEx": "29", "shortCiteRegEx": null, "year": 1991}, {"title": "Feature hashing for large scale multitask learning", "author": ["K. Weinberger", "A. Dasgupta", "J. Langford", "A. Smola", "J. Attenberg"], "venue": "ICML,", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2009}, {"title": "Deep fried convnets", "author": ["Z. Yang", "M. Moczulski", "M. Denil", "N. de Freitas", "A. Smola", "L. Song", "Z. Wang"], "venue": "arXiv preprint arXiv:1412.7149,", "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2014}], "referenceMentions": [{"referenceID": 16, "context": "In the recent years convolutional neural networks (CNN) have lead to impressive results in object recognition [17], face verification [24] and audio classification [20].", "startOffset": 110, "endOffset": 114}, {"referenceID": 23, "context": "In the recent years convolutional neural networks (CNN) have lead to impressive results in object recognition [17], face verification [24] and audio classification [20].", "startOffset": 134, "endOffset": 138}, {"referenceID": 19, "context": "In the recent years convolutional neural networks (CNN) have lead to impressive results in object recognition [17], face verification [24] and audio classification [20].", "startOffset": 164, "endOffset": 168}, {"referenceID": 14, "context": "Problems that seemed impossibly hard only five years ago can now be solved at better than human accuracy [15].", "startOffset": 105, "endOffset": 109}, {"referenceID": 11, "context": "Although CNNs have been known for a quarter of a century [12], only recently have their superb generalization abilities been accepted widely across the machine learning and computer vision communities.", "startOffset": 57, "endOffset": 61}, {"referenceID": 8, "context": "This broad acceptance coincides with the release of very large collections of labeled data [9].", "startOffset": 91, "endOffset": 94}, {"referenceID": 25, "context": "In 2012, the first winner of the ImageNet competition that used a CNN had already 240MB of parameters and the most recent winning model, in 2014, required 567MB [26].", "startOffset": 161, "endOffset": 165}, {"referenceID": 9, "context": "[10] use low-rank decomposition of the weight matrices to reduce the effective number of parameters in the network.", "startOffset": 0, "endOffset": 4}, {"referenceID": 3, "context": "[4] and Ba et al.", "startOffset": 0, "endOffset": 3}, {"referenceID": 0, "context": "[1] show that complex models can be compressed into 1-layer neural networks.", "startOffset": 0, "endOffset": 3}, {"referenceID": 6, "context": "Independently, the model size of neural networks can be reduced effectively through reduced bit precision [7].", "startOffset": 106, "endOffset": 109}, {"referenceID": 4, "context": "[5], who show that weights of fully connected networks can be effectively compressed with the hashing trick [30].", "startOffset": 0, "endOffset": 3}, {"referenceID": 29, "context": "[5], who show that weights of fully connected networks can be effectively compressed with the hashing trick [30].", "startOffset": 108, "endOffset": 112}, {"referenceID": 21, "context": "We transform these filters into frequency domain with the discrete cosine transform (DCT) [22].", "startOffset": 90, "endOffset": 94}, {"referenceID": 1, "context": "During training, the hashed weights can be learned with simple back-propagation [2]\u2014the gradient of a hash bucket value is the sum of gradients of all hashed frequency components in that bucket.", "startOffset": 80, "endOffset": 83}, {"referenceID": 7, "context": "a the hashing trick) [8, 25, 30] has been previously studied as a technique for reducing model storage size.", "startOffset": 21, "endOffset": 32}, {"referenceID": 24, "context": "a the hashing trick) [8, 25, 30] has been previously studied as a technique for reducing model storage size.", "startOffset": 21, "endOffset": 32}, {"referenceID": 29, "context": "a the hashing trick) [8, 25, 30] has been previously studied as a technique for reducing model storage size.", "startOffset": 21, "endOffset": 32}, {"referenceID": 29, "context": "As shown in [30], a key property of feature hashing is its preservation of inner product operations, where inner products after hashing produce the correct pre-hash inner product in expectation: E[\u03c6(x)\u03c6(y)]\u03c6 = x>y.", "startOffset": 12, "endOffset": 16}, {"referenceID": 24, "context": "The information loss induced by hash collision is much less severe for sparse feature vectors and can be counteracted through multiple hashing [25] or larger hash tables [30].", "startOffset": 143, "endOffset": 147}, {"referenceID": 29, "context": "The information loss induced by hash collision is much less severe for sparse feature vectors and can be counteracted through multiple hashing [25] or larger hash tables [30].", "startOffset": 170, "endOffset": 174}, {"referenceID": 21, "context": "Discrete Cosine Transform (DCT) [22].", "startOffset": 32, "endOffset": 36}, {"referenceID": 28, "context": "Methods built on the DCT are widely used for compressing images and movies, including forming the standard technique for JPEG [29].", "startOffset": 126, "endOffset": 130}, {"referenceID": 21, "context": "Though highly related to the discrete Fourier transformation (DFT), DCT is often preferable for compression tasks because of its spectral compaction property where weights for most images tend to be concentrated in a few low-frequency components of the DCT [22].", "startOffset": 257, "endOffset": 261}, {"referenceID": 4, "context": "[5], we achieve smaller models by randomly forcing weights throughout the network to share identical values.", "startOffset": 0, "endOffset": 3}, {"referenceID": 4, "context": "[5] advocate use of the hashing trick to (pseudo-)randomly assign shared parameters.", "startOffset": 0, "endOffset": 3}, {"referenceID": 16, "context": "In the spatial domain CNN filters are smooth [17] due to the local pixel smoothness in natural images.", "startOffset": 45, "endOffset": 49}, {"referenceID": 9, "context": "[10] learns parameters in fully-connected layers after decomposition into two low-rank matrices, i.", "startOffset": 0, "endOffset": 4}, {"referenceID": 22, "context": "Two works propose to approximate convolutional filters by a weighted linear combination of basis filters [23, 16].", "startOffset": 105, "endOffset": 113}, {"referenceID": 15, "context": "Two works propose to approximate convolutional filters by a weighted linear combination of basis filters [23, 16].", "startOffset": 105, "endOffset": 113}, {"referenceID": 10, "context": "Further speedup can be achieved by learning rank-one basis filters so that the convolution operations are very cheap to compute [11, 19].", "startOffset": 128, "endOffset": 136}, {"referenceID": 18, "context": "Further speedup can be achieved by learning rank-one basis filters so that the convolution operations are very cheap to compute [11, 19].", "startOffset": 128, "endOffset": 136}, {"referenceID": 10, "context": "[11] advocate decomposing the four-dimensional tensor of the filter weights into a sum of different rank-one, four-dimensional tensors.", "startOffset": 0, "endOffset": 4}, {"referenceID": 12, "context": "Other works focus entirely on compressing the fully-connected layers of CNNs [13, 31].", "startOffset": 77, "endOffset": 85}, {"referenceID": 30, "context": "Other works focus entirely on compressing the fully-connected layers of CNNs [13, 31].", "startOffset": 77, "endOffset": 85}, {"referenceID": 26, "context": "However, with the trend toward architectures with fewer fully connected layers and additional convolutional layers [27], compression of filters is of increased importance.", "startOffset": 115, "endOffset": 119}, {"referenceID": 20, "context": "Another technique for speeding up convolutional neural network evaluation is computing convolutions in the Fourier frequency domain, as convolution in the spatial domain is equivalent to (comparatively lower-cost) element-wise multiplication in the frequency domain [21, 28].", "startOffset": 266, "endOffset": 274}, {"referenceID": 27, "context": "Another technique for speeding up convolutional neural network evaluation is computing convolutions in the Fourier frequency domain, as convolution in the spatial domain is equivalent to (comparatively lower-cost) element-wise multiplication in the frequency domain [21, 28].", "startOffset": 266, "endOffset": 274}, {"referenceID": 20, "context": "[21] convert the filter to its frequency domain of size n \u00d7 n by oversampling the frequencies, which is necessary for doing element-wise multiplication with a larger image but also increases the memory overhead at test time.", "startOffset": 0, "endOffset": 4}, {"referenceID": 2, "context": "Training in the Fourier frequency domain may be advantageous for similar reasons, particularly when convolutions are being performed over large 3-D volumes [3].", "startOffset": 156, "endOffset": 159}, {"referenceID": 4, "context": "Most relevant to this work is HashedNets [5] which compresses the fully connected layers of deep neural networks.", "startOffset": 41, "endOffset": 44}, {"referenceID": 16, "context": "To improve accuracy on CIFAR100, we augment by horizontal reflection and cropping [17], resulting in 0.", "startOffset": 82, "endOffset": 86}, {"referenceID": 17, "context": "For the MNIST variants [18], each variation either reduces the training size (MNIST-07) or amends the original digits by rotation (ROT), background superimposition (BGRAND and BG-IMG), or a combination thereof (BG-ROT).", "startOffset": 23, "endOffset": 27}, {"referenceID": 4, "context": "We compare the proposed FreshNets with four baseline methods: HashedNets [5], low-rank decomposition (LRD) [10], filter dropping (DropFilt) and frequency dropping (DropFreq).", "startOffset": 73, "endOffset": 76}, {"referenceID": 9, "context": "We compare the proposed FreshNets with four baseline methods: HashedNets [5], low-rank decomposition (LRD) [10], filter dropping (DropFilt) and frequency dropping (DropFreq).", "startOffset": 107, "endOffset": 111}, {"referenceID": 9, "context": "Additionally, we compare against low-rank decomposition of the convolutional filters [10].", "startOffset": 85, "endOffset": 89}, {"referenceID": 10, "context": "Following the method in [11], we unfold the four-dimensional filter tensor to form a two dimensional matrix on which we apply the low-rank decomposition.", "startOffset": 24, "endOffset": 28}, {"referenceID": 5, "context": "All methods were implemented using Torch7 [6] and run on NVIDIA GTX TITAN graphics cards with 2688 cores and 6GB of global memory.", "startOffset": 42, "endOffset": 45}, {"referenceID": 6, "context": "The compression rates of all methods could be further improved by learning and storing parameters in lower precision [7, 14].", "startOffset": 117, "endOffset": 124}, {"referenceID": 13, "context": "The compression rates of all methods could be further improved by learning and storing parameters in lower precision [7, 14].", "startOffset": 117, "endOffset": 124}, {"referenceID": 4, "context": "3 For all baselines, we apply HashedNets [5] to the fully connected layer at the corresponding level of compression.", "startOffset": 41, "endOffset": 44}, {"referenceID": 4, "context": "For all methods, the fully connected layer (top layer) is compressed by HashedNets [5] at the corresponding compression rate.", "startOffset": 83, "endOffset": 86}], "year": 2015, "abstractText": "Convolutional neural networks (CNN) are increasingly used in many areas of computer vision. They are particularly attractive because of their ability to \u201cabsorb\u201d great quantities of labeled data through millions of parameters. However, as model sizes increase, so do the storage and memory requirements of the classifiers. We present a novel network architecture, Frequency-Sensitive Hashed Nets (FreshNets), which exploits inherent redundancy in both convolutional layers and fully-connected layers of a deep learning model, leading to dramatic savings in memory and storage consumption. Based on the key observation that the weights of learned convolutional filters are typically smooth and low-frequency, we first convert filter weights to the frequency domain with a discrete cosine transform (DCT) and use a low-cost hash function to randomly group frequency parameters into hash buckets. All parameters assigned the same hash bucket share a single value learned with standard back-propagation. To further reduce model size we allocate fewer hash buckets to high-frequency components, which are generally less important. We evaluate FreshNets on eight data sets, and show that it leads to drastically better compressed performance than several relevant baselines.", "creator": "LaTeX with hyperref package"}}}