{"id": "1506.04477", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "15-Jun-2015", "title": "Dual Memory Architectures for Fast Deep Learning of Stream Data via an Online-Incremental-Transfer Strategy", "abstract": "The online learning of deep neural networks is an interesting problem of machine learning because, for example, major IT companies want to manage the information of the massive data uploaded on the web daily, and this technology can contribute to the next generation of lifelong learning. We aim to train deep models from new data that consists of new classes, distributions, and tasks at minimal computational cost, which we call online deep learning. Unfortunately, deep neural network learning through classical online and incremental methods does not work well in both theory and practice. In this paper, we introduce dual memory architectures for online incremental deep learning. The proposed architecture consists of deep representation learners and fast learnable shallow kernel networks, both of which synergize to track the information of new data. During the training phase, we use various online, incremental ensemble, and transfer learning techniques in order to achieve lower error of the architecture. On the MNIST, CIFAR-10, and ImageNet image recognition tasks, the proposed dual memory architectures performs much better than the classical online and incremental ensemble algorithm, and their accuracies are similar to that of the batch learner.", "histories": [["v1", "Mon, 15 Jun 2015 04:44:38 GMT  (669kb)", "http://arxiv.org/abs/1506.04477v1", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["sang-woo lee", "min-oh heo", "jiwon kim", "jeonghee kim", "byoung-tak zhang"], "accepted": false, "id": "1506.04477"}, "pdf": {"name": "1506.04477.pdf", "metadata": {"source": "META", "title": "Dual Memory Architectures for Fast Deep Learning of Stream Data via an Online-Incremental-Transfer Strategy", "authors": ["Sang-Woo Lee", "Jiwon Kim"], "emails": ["SLEE@BI.SNU.AC.KR", "MOHEO@BI.SNU.AC.KR", "G1.KIM@NAVERCORP.COM", "JEONGHEE.KIM@NAVERCORP.COM", "BTZHANG@BI.SNU.AC.KR"], "sections": [{"heading": null, "text": "ar Xiv: 150 6.04 477v 1 [cs.L G] 1Online learning of deep neural networks is an interesting problem of machine learning because, for example, large IT companies want to manage the information of the massive data uploaded daily on the web and this technology can contribute to the next generation of lifelong learning. Our goal is to develop deep models of new data consisting of new classes, distributions and tasks at minimal computing costs, which we call online deep learning. Unfortunately, deep neural network learning through classic online and incremental methods does not work well in theory or practice. In this paper we introduce dual storage architectures for incremental online deep learning. The proposed architecture consists of learners with deep representation and fast-learnable flat core networks, which both have synergies to track the information of new data. During the training phase, we use different online, incremental ensembles and transfer learning techniques to achieve lower CT errors than traditional architectures, which are much better at MT architecture in 2015."}, {"heading": "1. Introduction", "text": "It is not only a question of expression, but also of the language, the language, the expression, the language, the language, the expression, the language, the expression, the language, the language, the language, the language, the language, the language, the language, the language, the language, the language, the language, the language, the language, the language, the language, the language, the language, the language, the language, the language, the language, the language, the language, the language, the language, the language, the language, the language, the language, the language, the language, the language, the language, the language, the language, the language, the language of the language, the language, the language, the language, the language, the language, the language, the language, the language, the language, the language, the language, the language, the language, the language, the language, the language, the language, the language, the language, the language, the language, the language, the language, the language, the language, the language, the language, the language, the language, the language, the language, the language, the language, the language, the language, the language, the language, the language, the language, the language, the language, the language, the language, the language, the language, the language, the language, the language, the language, the language, the language, the language, the language, the language, the language, the language, the language, the language, the language, the language, the language, the language, the language, the language, the language, the language, the language, the language, the language, the language, the language, the language, the language, the language, the language, the language, the language, the language, the language, the language, the language, the language, the language, the language, the language, the language, the language, the language, the language, the language, the language, the language, the language, the language, the language, the language, the language, the language, the language, the language, the language, the language, the language, the language, the language, the language, the language, the language, the language, the language, the language, the language, the language, the language, the language, the language, the language, the language, the language, the language,"}, {"heading": "2. Dual Memory Architectures", "text": "In addition to the strategies described in the previous section, we will generally explain what dual storage architectures mean and discuss the type of algorithms included in this framework. However, this description is not limited and can be extended beyond the given procedure. However, dual storage architecture is the learnable system that consists of deep and fast storage."}, {"heading": "3. Online Incremental Learning Algorithms for Deep Memory", "text": "For practical online learning from an enormous amount of data, it is good to store an appropriate number of instances and discard those that seem less important for learning in the near future. As a kind of practical online learning environment, we consider the \"mini-dataset-layer learning problem,\" which allows us to store at most nsubset training examples in an online learning memory (algorithm 1). After the mini-dataset-shift learning problem, we randomly initialize a model. Let's repeat new data. Let's merge new data into memory D (i.e. D-Dnew). Let's throw away some data in memory to make | D-dataset-shift learning problem. Let's train a model with D.until foreverververTo solve this problem, many researchers are studying incremental ensemble learning. We point to incremental learning as a structure for new instances. Following the new part 00,00.1, the data is taken from a new structure, a multiple ensemble is taken."}, {"heading": "3.1. Mini-Batch-Shift Gradient Descent Ensemble", "text": "First, we start with an alternative approach - online learning - to complement the simple incremental ensemble approach. The first step of our first algorithm involves using mini-batch gradient pedigree at each epoch with new nsubset training examples for adapting nnew new new data. We refer to this method as \"mini-batch shifting of gradient pedigree.\" In this algorithm we first learn the 1,001 - 11,000th instances with an epoch, and so on (if nsubset is 10,000 and nnew 500). Algorithm 2 mini-batch shift descent ensembles with an epoch. Then the model learns the 1,001 - 11,000th instances with an epoch, and so on."}, {"heading": "3.2. Neural Prior Ensemble", "text": "Dual Memory Architecture is not only a specific learning procedure, but a framework for learning data streams. We introduce \"Neural Precursor Network,\" another learning algorithm for deep memory. In Neural Precursor Network, a recently trained weak neural network Wprev assumes the role of the general neural network C, which is used in the gradient descent with mini-batch shift, and it is transferred to a new weak neural network Wnew (Algorithm 3). We refer to \"Neural Precursor Network\" as a strategy for using the last neural network Wnew for inferences and neglect the previous neural networks in the next experiments. Algorithm 3 Neural Prior Ensemble repeatCollect Nsubset new. Initialize a new neural network Wnew according to parameters of Wprev. Zug Wnew with Dnew. Combine a weak learning new model (Wnew to a weak adult)."}, {"heading": "3.3. Experiments", "text": "In fact, most of us are capable of outdoing ourselves."}, {"heading": "4. Online Incremental Learning Algorithms for Fast Memory", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.1. Shallow Kernel Networks on the Neural Networks", "text": "We are introducing fast memory; flat kernel networks on neural networks. In dual memory architectures, the input functions of flat kernel networks, which we use as fast memory, are the activation of deep neural networks. In addition to dual memory, fast memory plays two important roles in the handling of stream data. First, fast memory does not integrate the information distributed in each neural network. On the non-stationary data stream, not only does the proposed learning algorithm for mini-datasets layer of a single neural network work, but also the learning algorithm for deep memory does not work well. Training fast memory with complete training data makes much better performance than deep memory alone, especially when new data includes new distributions and additional classes. It is quite convenient because the cost of learning parameters of flat networks is low. Second, fast memory can be updated from any new instance, requiring a small amount of computing power until the functions remain unchanged."}, {"heading": "4.2. Multiplicative Hypernetworks", "text": "In this section, we present a multiplicative hypernetwork (mHN) as an example of fast memory capacity, inspired by the sparse population coding model (Zhang et al., 2012) and revised to meet the classification task we want to solve. We choose mHNs for their good online learning ability via sparse, well-divided cores between classes. However, there are alternative choices, such as a support vector engine (SVM) (Liu et al., 2008) and an efficient lifelong learning algorithm (Zhou et al., 2012), among which SVM is our comparative model. mHNs are flat kernel networks that use a multiplicative function as an explicit kernel function (1)."}, {"heading": "4.3. Experiments", "text": "We evaluate the performance of the proposed fast storage algorithm using conventional neural networks (CNNs) and mHNs on the CIFAR-10 dataset. In this environment, we divide the total training data into the 10 online datasets with the non-stationary distribution of the class. The first online dataset consists of 40% of Class 1, 40% of Class 2 and 20% of Class 3. The second online dataset consists of 40% of Class 1, and 20% of Class 2, 5 datasets. The third online dataset consists of 20% of Class 1, 5 of the data. The fourth online dataset consists of 20% of Class 2, and so on."}, {"heading": "5. Conclusion", "text": "In this paper, we examined some of the characteristics of online deep learning. First, deep neural networks have online learning capability in large-scale object classification tasks for stationary data streams. Second, deep neural networks forget what they have learned before in extreme non-stationary data streams; therefore, creating a new module can gradually alleviate this problem. Third, by transferring knowledge from an old module to a new module, the performance of online learning systems is enhanced. Fourthly, placing flat core networks on deep neural networks increases the online learning capability of the architecture.This paper reveals numerous practical and theoretical problems that will soon be discovered in our follow-up studies. We hope that these topics will be discussed in the workshop."}, {"heading": "Acknowledgments", "text": "This work was partially supported by the NRF Scholarship of the Korean Government (MSIP) (NRF-2010-0017734Videome) and the IITP Scholarship of the Korean Government (MSIP) (R0126-15-1072-SW.StarLab, 10035348-mLife, 10044009-HRI.MESSI)."}], "references": [{"title": "Discovering structure in multiple learning tasks: The TC algorithm", "author": ["S. Thrun", "J. O\u2019Sullivan"], "venue": "In ICML,", "citeRegEx": "Thrun and O.Sullivan.,? \\Q1996\\E", "shortCiteRegEx": "Thrun and O.Sullivan.", "year": 1996}, {"title": "ELLA: An Efficient Lifelong Learning Algorithm", "author": ["P. Ruvolo", "E. Eaton"], "venue": "In ICML,", "citeRegEx": "Ruvolo and Eaton.,? \\Q2013\\E", "shortCiteRegEx": "Ruvolo and Eaton.", "year": 2013}, {"title": "Multilingual acoustic models using distributed deep neural networks", "author": ["G. Heigold", "V. Vanhoucke", "A. Senior", "P. Nguyen", "M. Ranzato", "M. Devin", "J. Dean"], "venue": "In ICASSP,", "citeRegEx": "Heigold et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Heigold et al\\.", "year": 2013}, {"title": "DeCAF: A Deep Convolutional Activation Feature for Generic Visual Recognition", "author": ["J. Donahue", "Y. Jia", "O. Vinyals", "J. Hoffman", "N. Zhang", "E. Tzeng", "T. Darrell"], "venue": "In ICML,", "citeRegEx": "Donahue et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Donahue et al\\.", "year": 2014}, {"title": "How transferable are features in deep neural networks", "author": ["J. Yosinski", "J. Clune", "Y. Bengio", "H. Lipson"], "venue": "In NIPS,", "citeRegEx": "Yosinski et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Yosinski et al\\.", "year": 2014}, {"title": "Learn++: An Incremental Learning Algorithm for Supervised Neural Networks", "author": ["R. Polikar", "L. Udpa", "S.S. Udpa"], "venue": "IEEE TRANSACTIONS ON SYSTEMS, MAN, AND CYBERNETICS PART C: APPLICATIONS AND REVIEWS,", "citeRegEx": "Polikar et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Polikar et al\\.", "year": 2001}, {"title": "Online Bagging and Boosting", "author": ["N.C. Oza", "S. Russell"], "venue": "In AISTATS,", "citeRegEx": "Oza and Russell.,? \\Q2001\\E", "shortCiteRegEx": "Oza and Russell.", "year": 2001}, {"title": "MatConvNet \u2013 Convolutional Neural Networks for MATLAB", "author": ["A. Vedaldi", "K. Lenc"], "venue": null, "citeRegEx": "Vedaldi and Lenc.,? \\Q2014\\E", "shortCiteRegEx": "Vedaldi and Lenc.", "year": 2014}, {"title": "Sparse population code models of word learning in concept drift", "author": ["B.-T. Zhang", "J.-W. Ha", "M. Kang"], "venue": "In CogSci,", "citeRegEx": "Zhang et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Zhang et al\\.", "year": 2012}, {"title": "An Incremental Feature Learning Algorithm Based on Least Square Support Vector Machine", "author": ["X. Liu", "G. Zhang", "Y. Zhan", "E. Zhu"], "venue": "Frontiers in Algorithmics,", "citeRegEx": "Liu et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Liu et al\\.", "year": 2008}, {"title": "Online Incremental Feature Learning with Denoising Autoencoders", "author": ["G. Zhou", "K. Shon", "H. Lee"], "venue": "In AISTATS,", "citeRegEx": "Zhou et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Zhou et al\\.", "year": 2012}], "referenceMentions": [{"referenceID": 2, "context": "Online deep learning can have good properties from the perspective of lifelong learning because deep neural networks show good performance on recognition problems, and their transfer and multi-task learning problem (Heigold et al., 2013; Donahue et al., 2014; Yosinski et al., 2014).", "startOffset": 215, "endOffset": 282}, {"referenceID": 3, "context": "Online deep learning can have good properties from the perspective of lifelong learning because deep neural networks show good performance on recognition problems, and their transfer and multi-task learning problem (Heigold et al., 2013; Donahue et al., 2014; Yosinski et al., 2014).", "startOffset": 215, "endOffset": 282}, {"referenceID": 4, "context": "Online deep learning can have good properties from the perspective of lifelong learning because deep neural networks show good performance on recognition problems, and their transfer and multi-task learning problem (Heigold et al., 2013; Donahue et al., 2014; Yosinski et al., 2014).", "startOffset": 215, "endOffset": 282}, {"referenceID": 5, "context": "There are several studies that use the incremental ensemble approach (Polikar et al., 2001; Oza & Russell, 2001).", "startOffset": 69, "endOffset": 112}, {"referenceID": 3, "context": "of the transferability of the deep memory, the fast memory has remarkable performance, especially for new distributions and additional classes, as though the fast memory had already trained from many new instances with the same class and similar style (Donahue et al., 2014).", "startOffset": 252, "endOffset": 274}, {"referenceID": 4, "context": "To transfer from a general neural network C to each weak neural network W , we use the initialize and fine-tune approach suggested in (Yosinski et al., 2014).", "startOffset": 134, "endOffset": 157}, {"referenceID": 4, "context": "Using this method, (Yosinski et al., 2014) achieved 2.", "startOffset": 19, "endOffset": 42}, {"referenceID": 8, "context": "This model is inspired by the sparse population coding model (Zhang et al., 2012) and it is revised to be fit to the classification task we want to solve.", "startOffset": 61, "endOffset": 81}, {"referenceID": 9, "context": ", a support vector machine (SVM) (Liu et al., 2008), and an efficient lifelong learning algorithm (ELLA) (Zhou et al.", "startOffset": 33, "endOffset": 51}, {"referenceID": 10, "context": ", 2008), and an efficient lifelong learning algorithm (ELLA) (Zhou et al., 2012), among which SVM is our comparative model.", "startOffset": 61, "endOffset": 80}], "year": 2015, "abstractText": "The online learning of deep neural networks is an interesting problem of machine learning because, for example, major IT companies want to manage the information of the massive data uploaded on the web daily, and this technology can contribute to the next generation of lifelong learning. We aim to train deep models from new data that consists of new classes, distributions, and tasks at minimal computational cost, which we call online deep learning. Unfortunately, deep neural network learning through classical online and incremental methods does not work well in both theory and practice. In this paper, we introduce dual memory architectures for online incremental deep learning. The proposed architecture consists of deep representation learners and fast learnable shallow kernel networks, both of which synergize to track the information of new data. During the training phase, we use various online, incremental ensemble, and transfer learning techniques in order to achieve lower error of the architecture. On the MNIST, CIFAR-10, and ImageNet image recognition tasks, the proposed dual memory architectures performs much better than the classical online and incremental ensemble algorithm, and their accuracies are similar to that of the batch learner. ICML workshop on Deep Learning 2015, Lille, France, 2015. Copyright 2015 by the author(s).", "creator": "LaTeX with hyperref package"}}}