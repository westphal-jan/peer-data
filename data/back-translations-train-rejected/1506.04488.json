{"id": "1506.04488", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "15-Jun-2015", "title": "Distilling Word Embeddings: An Encoding Approach", "abstract": "Distilling knowledge from a well-trained cumbersome network to a small one has become a new research topic recently, as lightweight neural networks with high performance are particularly in need in various resource-restricted systems. This paper addresses the problem of distilling embeddings for NLP tasks. We propose an encoding approach to distill task-specific knowledge from high-dimensional embeddings, which can retain high performance and reduce model complexity to a large extent. Experimental results show our method is better than directly training neural networks with small embeddings.", "histories": [["v1", "Mon, 15 Jun 2015 06:30:36 GMT  (32kb,D)", "https://arxiv.org/abs/1506.04488v1", null], ["v2", "Sun, 24 Jul 2016 16:22:09 GMT  (152kb,D)", "http://arxiv.org/abs/1506.04488v2", "Accepted by CIKM-16 as a short paper, and by the Representation Learning for Natural Language Processing (RL4NLP) Workshop @ACL-16 for presentation"]], "reviews": [], "SUBJECTS": "cs.CL cs.LG", "authors": ["lili mou", "ran jia", "yan xu", "ge li", "lu zhang", "zhi jin"], "accepted": false, "id": "1506.04488"}, "pdf": {"name": "1506.04488.pdf", "metadata": {"source": "CRF", "title": "Distilling Word Embeddings: An Encoding Approach", "authors": ["Lili Mou", "Ran Jia", "Yan Xu", "Ge Li", "Lu Zhang", "Zhi Jin"], "emails": ["jiaran1994}@gmail.com", "zhijin}@sei.pku.edu.cn", "@ACL-16"], "sections": [{"heading": null, "text": "CCS Concepts \u2022 Computer Methods \u2192 Neural Networks; Keywords Model Compression; Neural Networks; Word Embedding"}, {"heading": "1. INTRODUCTION", "text": "Dre rf\u00fc nde nlrrfEe\u00fceegnr rf\u00fc eid nlrfEe\u00fceegnrrsrtee\u00fceegnr rf\u00fc ide nlrfEe\u00fceegnlrrrrrrrteeoiiiiiietlrrrteeVnlrrrrrrrrrrrrrln rf\u00fc ide nlrfEe\u00fceegnln in rde eeirlrrrrrrrrrrrrteerrrrsrsrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr"}, {"heading": "2. BACKGROUND OF MATCHING SOFTMAX", "text": "As I said, existing approaches to knowledge transfer between two neural networks essentially follow a two-step strategy: first, a teacher network is trained; then the teacher network is used to guide a student model through the synchronization with softmax, illustrated in Figure 1. For a classification problem, softmax is typically used as the activation function of the output layer. Let's use z-Rnc as the input of softmax. (nc is the number of classes.) The output of softmax isyi = ezi / T \u2211 nc j = 1 e zj / Twhere T is a (later used) relaxation variable, called temperature. T = 1 for standard softmax. Let's take as an example a 3-way classification problem. If a teacher model estimates y = (0.95, 0.04, 0.01) > for three classes, it is valuable information for the student model that grade 2 is more similar to class 1 than class 3 is to the limitations of class 1.However, the maximum output can be."}, {"heading": "3. THE PROPOSED ENCODING APPROACH FOR DISTILLING EMBEDDINGS", "text": "We also analyze the model capacities of the neural network with its decentralized embeddings, and discuss the reasons for outsourcing small embeddings from large networks to a real vector level called embeddings, where each dimension represents a specific aspect of the underlying word semantics. Usually, they are trained in an unverified manner, e.g. when maximizing the likelihood of a large corpus [2, 9], or maximizing a scoring function [6, 8]. The learned embeddings can be fed into common neural networks for superior learning, e.g. POS tagging, designated recognition, and secondary role."}, {"heading": "4. EVALUATION", "text": "In this section, we present our experimental results. In Section 4.1, we first describe the test stand and protocol of our experiments. Then, in Section 4.2, we analyze the performance of our approach in terms of several aspects, namely accuracy, memory and time consumption."}, {"heading": "4.1 Tasks, Models, and Protocols", "text": "We tested our distillation approach in two tasks: sentiment analysis and relation classification. The sentiment analysis task aims to classify a sentence according to its sensation into 5 categories: strong / weak positive / negative and neuronal. We used Stanford Sentiment Treebank1 as our data set, which contains 8544 / 1101 / 221 sets for training, validation and testing. Phrases (subsets) in the training set are also labeled with sentiment, which enriches the training set to more than 150k samples. For validation and testing, only the sensation of a whole set was taken into consideration. The second task is to classify the relationship between two 1http: / / nlp.stanford.edu / sentiment / tagged units in one set. The SemEval 2010 data set, 2 which we used, includes 8000 training samples from which we split 10% for validation; for example, there is a further sample count of 3000 for the network."}, {"heading": "4.2 Results", "text": "In fact, it is that we are able to assert ourselves, that we are able, that we are able to hide ourselves, and that we are able, that we are able to hide ourselves, \"he said."}, {"heading": "5. CONCLUSION", "text": "In this paper, we addressed the issue of distillation of NLP embedding, which is important when using a neural network in resource-constrained scenarios. We proposed a coding approach that distils cumbersome word embedding into a low-dimensional space. Experimental results have demonstrated the superiority of our proposed distillation method over training neural networks directly with small embedding; that performance increases significantly, especially when the dimension becomes small. Furthermore, our approach does not respond to a teacher model that complements Softmax Matching; these two methods of knowledge distillation could also be combined. 6. ACKNOWLEDGMENTSWe thank all the critics for their insightful comments and Rui Yan for discussing the manuscript. This research is supported by the National Basic Research Program of China (the 973 Program) under funding number 2015CB352201, the National Natural Science Foundation of China under funding number 126139153301, 2015, 015630291 and 2015."}, {"heading": "7. REFERENCES", "text": "[1] J. Ba and R. Caruana. Must deep nets really be deepened? In NIPS, pp. 2654-2662, 2014. [2] Y. Bengio, R. Ducharme, P. Vincent and C. Janvin. A neural probabilistic language model. JMLR, 3: 1137-1155, 2003. [3] C. Bucilua, R. Caruana, and A. Niculescu-Mizil. Model compression. In SIGKDD, pp. 535-541, 2006. [4] W. Chan, N. R. Ke, and I. Lane. Transfer of knowledge from an RNN to a DNN. arXiv: 1504.01483, 2015. [5] W. Chen, J. T. Wilson, S. Tyree, K. Q. Weinberger, and Y. Chen."}], "references": [{"title": "Do deep nets really need to be deep? In NIPS", "author": ["J. Ba", "R. Caruana"], "venue": "pages 2654\u20132662", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2014}, {"title": "A neural probabilistic language model", "author": ["Y. Bengio", "R. Ducharme", "P. Vincent", "C. Janvin"], "venue": "JMLR, 3:1137\u20131155", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2003}, {"title": "Model compression", "author": ["C. Bucilu\u01ce", "R. Caruana", "A. Niculescu-Mizil"], "venue": "SIGKDD, pages 535\u2013541", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2006}, {"title": "Transferring knowledge from a RNN to a DNN", "author": ["W. Chan", "N.R. Ke", "I. Lane"], "venue": "arXiv:1504.01483", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2015}, {"title": "Compressing neural networks with the hashing trick", "author": ["W. Chen", "J.T. Wilson", "S. Tyree", "K.Q. Weinberger", "Y. Chen"], "venue": "ICML, pages 2285\u20132294", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2015}, {"title": "A unified architecture for natural language processing: Deep neural networks with multitask learning", "author": ["R. Collobert", "J. Weston"], "venue": "ICML, pages 160\u2013167", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2008}, {"title": "Distilling the knowledge in a neural network", "author": ["G. Hinton", "O. Vinyals", "J. Dean"], "venue": "arXiv:1503.02531", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2014}, {"title": "Distributed representations of words and phrases and their compositionality", "author": ["T. Mikolov", "I. Sutskever", "K. Chen", "G. Corrado", "J. Dean"], "venue": "NIPS, pages 3111\u20133119", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2013}, {"title": "Hierarchical probabilistic neural network language model", "author": ["F. Morin", "Y. Bengio"], "venue": "AISTAT, pages 246\u2013252", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2005}, {"title": "Discriminative neural sentence modeling by tree-based convolution", "author": ["L. Mou", "H. Peng", "G. Li", "Y. Xu", "L. Zhang", "Z. Jin"], "venue": "EMNLP, pages 2315\u20132325", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2015}, {"title": "FitNets: Hints for thin deep nets", "author": ["A. Romero", "N. Ballas", "S.E. Kahou", "A. Chassang", "C. Gatta", "Y. Bengio"], "venue": "ICLR", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2014}, {"title": "Recurrent neural network training with dark knowledge transfer", "author": ["D. Wang", "C. Liu", "Z. Tang", "Z. Zhang", "M. Zhao"], "venue": "arXiv:1505.04630", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2015}, {"title": "Classifying relations via long short term memory networks along shortest dependency paths", "author": ["Y. Xu", "L. Mou", "G. Li", "Y. Chen", "H. Peng", "Z. Jin"], "venue": "EMNLP, pages 1785\u20131794", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2015}], "referenceMentions": [{"referenceID": 2, "context": "[3]; it has attracted increasing attention over the last two years [7, 5].", "startOffset": 0, "endOffset": 3}, {"referenceID": 6, "context": "[3]; it has attracted increasing attention over the last two years [7, 5].", "startOffset": 67, "endOffset": 73}, {"referenceID": 4, "context": "[3]; it has attracted increasing attention over the last two years [7, 5].", "startOffset": 67, "endOffset": 73}, {"referenceID": 6, "context": "[7], the objective of training networks is probably different from deploying networks: during training we focus on extracting as much knowledge as possible from a large dataset, whereas deploying networks takes into consideration multiple aspects, including accuracy, memory, time, and energy consumption.", "startOffset": 0, "endOffset": 3}, {"referenceID": 10, "context": "Much evidence in the literature shows the feasibility of transferring knowledge from one neural network to another, for instance, from shallow networks to deep ones [11], from feed-forward networks to recurrent ones [12], or vice versa [1, 4].", "startOffset": 165, "endOffset": 169}, {"referenceID": 11, "context": "Much evidence in the literature shows the feasibility of transferring knowledge from one neural network to another, for instance, from shallow networks to deep ones [11], from feed-forward networks to recurrent ones [12], or vice versa [1, 4].", "startOffset": 216, "endOffset": 220}, {"referenceID": 0, "context": "Much evidence in the literature shows the feasibility of transferring knowledge from one neural network to another, for instance, from shallow networks to deep ones [11], from feed-forward networks to recurrent ones [12], or vice versa [1, 4].", "startOffset": 236, "endOffset": 242}, {"referenceID": 3, "context": "Much evidence in the literature shows the feasibility of transferring knowledge from one neural network to another, for instance, from shallow networks to deep ones [11], from feed-forward networks to recurrent ones [12], or vice versa [1, 4].", "startOffset": 236, "endOffset": 242}, {"referenceID": 0, "context": "Several variants of training objectives include applying regression over the input of softmax [1] and softening the teacher model\u2019s probabilities [7].", "startOffset": 94, "endOffset": 97}, {"referenceID": 6, "context": "Several variants of training objectives include applying regression over the input of softmax [1] and softening the teacher model\u2019s probabilities [7].", "startOffset": 146, "endOffset": 149}, {"referenceID": 6, "context": "It is also argued that the estimated probabilities by a teacher model convey more information than one-hot represented ground truth; hence knowledge distillation is feasible and beneficial [7].", "startOffset": 189, "endOffset": 192}, {"referenceID": 0, "context": "[1] match the input of softmax, z, rather than y.", "startOffset": 0, "endOffset": 3}, {"referenceID": 6, "context": "[7] raise the temperature T during training, which makes the estimated probabilities softer over different classes.", "startOffset": 0, "endOffset": 3}, {"referenceID": 10, "context": "Matching softmax can also be applied along with standard crossentropy loss (with one-hot ground truth), or more elaborately, the teacher model\u2019s effect declines in an annealing fashion when the student model is more aware of data [11].", "startOffset": 230, "endOffset": 234}, {"referenceID": 1, "context": ", maximizing the probability of a large corpus [2, 9], or maximizing a scoring function [6, 8].", "startOffset": 47, "endOffset": 53}, {"referenceID": 8, "context": ", maximizing the probability of a large corpus [2, 9], or maximizing a scoring function [6, 8].", "startOffset": 47, "endOffset": 53}, {"referenceID": 5, "context": ", maximizing the probability of a large corpus [2, 9], or maximizing a scoring function [6, 8].", "startOffset": 88, "endOffset": 94}, {"referenceID": 7, "context": ", maximizing the probability of a large corpus [2, 9], or maximizing a scoring function [6, 8].", "startOffset": 88, "endOffset": 94}, {"referenceID": 5, "context": ", POS tagging, named entity recognition, and semantic role labeling [6].", "startOffset": 68, "endOffset": 71}, {"referenceID": 9, "context": "To set up our experiments, we leveraged two state-of-theart neural models: a tree-based convolutional neural network (TBCNN) for sentiment analysis [10], and a long short term memory-based recurrent network along shortest dependency path (SDP-LSTM) between two entities for relation classification [13].", "startOffset": 148, "endOffset": 152}, {"referenceID": 12, "context": "To set up our experiments, we leveraged two state-of-theart neural models: a tree-based convolutional neural network (TBCNN) for sentiment analysis [10], and a long short term memory-based recurrent network along shortest dependency path (SDP-LSTM) between two entities for relation classification [13].", "startOffset": 298, "endOffset": 302}, {"referenceID": 12, "context": "php?location=data 3 We only used word embeddings, and ignored other features like hyponymy, dependency types, which were used in [13].", "startOffset": 129, "endOffset": 133}, {"referenceID": 6, "context": "In both tasks, we also tried the matching softmax approach, whose settings and hyperparameters are mainly derived from [7], i.", "startOffset": 119, "endOffset": 122}], "year": 2016, "abstractText": "Distilling knowledge from a well-trained cumbersome network to a small one has recently become a new research topic, as lightweight neural networks with high performance are particularly in need in various resource-restricted systems. This paper addresses the problem of distilling word embeddings for NLP tasks. We propose an encoding approach to distill task-specific knowledge from a set of highdimensional embeddings, which can reduce model complexity by a large margin as well as retain high accuracy, showing a good compromise between efficiency and performance. Experiments in two tasks reveal the phenomenon that distilling knowledge from cumbersome embeddings is better than directly training neural networks with small embeddings.", "creator": "LaTeX with hyperref package"}}}