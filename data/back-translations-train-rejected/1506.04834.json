{"id": "1506.04834", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "16-Jun-2015", "title": "Tree-structured composition in neural networks without tree-structured architectures", "abstract": "Tree-structured neural networks encode a particular tree geometry for a sentence in the network design. However, these models have at best only slightly outperformed simpler sequence-based models. We hypothesize that neural sequence models like LSTMs are in fact able to discover and implicitly use recursive compositional structure, at least for tasks with clear cues to that structure in the data. We demonstrate this possibility using an artificial data task for which recursive compositional structure is crucial, and find that the sequence model can learn the underlying patterning. The sequence model is better in that it learns the value of tree structure from the data in an emergent way, while the tree-structured model is better in being able to learn with greater statistical efficiency due to its informative prior model structure.", "histories": [["v1", "Tue, 16 Jun 2015 05:12:52 GMT  (553kb,D)", "http://arxiv.org/abs/1506.04834v1", null], ["v2", "Tue, 18 Aug 2015 16:08:26 GMT  (361kb,D)", "http://arxiv.org/abs/1506.04834v2", null], ["v3", "Mon, 9 Nov 2015 19:45:09 GMT  (371kb,D)", "http://arxiv.org/abs/1506.04834v3", "To appear in the proceedings of the 2015 NIPS Workshop on Cognitive Computation: Integrating Neural and Symbolic Approaches"]], "reviews": [], "SUBJECTS": "cs.CL cs.LG", "authors": ["samuel r bowman", "christopher d manning", "christopher potts"], "accepted": false, "id": "1506.04834"}, "pdf": {"name": "1506.04834.pdf", "metadata": {"source": "CRF", "title": "Tree-structured composition in neural networks without tree-structured architectures", "authors": ["Samuel R. Bowman", "Christopher D. Manning", "Christopher Potts"], "emails": ["sbowman@stanford.edu", "manning@stanford.edu", "cgpotts@stanford.edu"], "sections": [{"heading": "1 Introduction", "text": "This year, it has reached the point where it will be able to retaliate."}, {"heading": "2 Recursive structure in artificial data", "text": "In fact, most of them will be able to move to a different world in which they are able than to another world in which they are able to escape, in which they are able to escape."}, {"heading": "3 Testing sentence models on entailment", "text": "The model architecture uses two copies of a single sentence model (a tree or sequence model) to encode the premise and hypothesis (left and right), and then uses these encodings as characteristics for a multilayer classifier that predicts one of the seven relations. Since the encodings are calculated separately, the sentence models are forced to encode each element of the sentence, meaning that the downstream model is needed. Classifier The classification component of the model consists of a layer that takes the two sentence representations as inputs, followed by two neural network layers. For the combined layer, we use a neural tensor network (NTN, Chen et al. 2013) layer that summarizes the output of a simple recursive / recursive network with a vector that uses two multiplications."}, {"heading": "4 Results and discussion", "text": "The results are shown in Figure 2. The tree models fit well with the training data, reaching 99.6, 98.5 and 97.7% overall accuracy in setting \u2264 6, with LSTM slightly below at 94.4%. In this setting, all models generalized well to structures of known length, with the tree models exceeding all 97% in size 4 examples, and the LSTM at 94.8%. In the longer test sets, the tree models performed smoothly across the line, while the LSTM decomposed faster and more abruptly, with a striking difference in setting \u2264 4, where LSTM performance dropped by 10% from 4 to 5, compared with 4.4% for the next worse model. However, the LSTM improves considerably with more comprehensive training data in condition \u2264 6, with only a decrease of 3% and the generalization of results better than that of the best model in setting \u2264 3."}, {"heading": "5 Conclusion", "text": "We found that all four models can use recursively defined language to interpret sentences with complex, invisible structures, and that the distortions of tree models allow them to learn this more effectively from less data. We interpret these results as evidence that both tree and sequence architectures can play a valuable role in constructing sentence models over data with recursive syntactical structure: tree architectures provide explicit distortion that allows learning from relatively small and poor data sets, while sequence-based architectures - with enough data - can solve the same problems without being constrained by this distortion. Finally, we suggest that, because of the well-founded linguistic assertion that the type of recursive structure we are investigating here is the key to understanding real natural languages, there is likely to be value in developing sequence models that can use this structure more efficiently without sacrificing the flexibility that makes them successful."}, {"heading": "Acknowledgments", "text": "We are grateful for the support of a Google Faculty Research Award, a gift from Bloomberg L.P., the Defense Advanced Research Projects Agency (DARPA) Deep Exploration and Filtering of Text (DEFT) Program under Air Force Research Laboratory (AFRL) Contract No FA875013-2-0040, the National Science Foundation under Funding No IIS 1159679, and the Department of the Navy, Office of Naval Research, under Funding No N00014-10-1-0109. Any opinions, findings, conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect the views of Google, Bloomberg L.P., DARPA, AFRL NSF, ONR or the U.S. Government."}], "references": [{"title": "Recursive neural networks can learn logical semantics", "author": ["Samuel R. Bowman", "Christopher Potts", "Christopher D. Manning."], "venue": "Proc. of the 3rd Workshop on Continuous Vector Space Models and their Compositionality.", "citeRegEx": "Bowman et al\\.,? 2015", "shortCiteRegEx": "Bowman et al\\.", "year": 2015}, {"title": "Learning new facts from knowledge bases with neural tensor networks and semantic word vectors", "author": ["Danqi Chen", "Richard Socher", "Christopher D. Manning", "Andrew Y. Ng."], "venue": "Proc. ICLR.", "citeRegEx": "Chen et al\\.,? 2013", "shortCiteRegEx": "Chen et al\\.", "year": 2013}, {"title": "Transitionbased dependency parsing with stack long shortterm memory", "author": ["Chris Dyer", "Miguel Ballesteros", "Wang Ling", "Austin Matthews", "Noah A. Smith."], "venue": "Proc. ACL.", "citeRegEx": "Dyer et al\\.,? 2015", "shortCiteRegEx": "Dyer et al\\.", "year": 2015}, {"title": "Finding structure in time", "author": ["Jeffrey L. Elman."], "venue": "Cognitive science, 14(2):179\u2013211.", "citeRegEx": "Elman.,? 1990", "shortCiteRegEx": "Elman.", "year": 1990}, {"title": "Learning task-dependent distributed representations by backpropagation through structure", "author": ["Christoph Goller", "Andreas Kuchler."], "venue": "Proc. IEEE International Conference on Neural Networks.", "citeRegEx": "Goller and Kuchler.,? 1996", "shortCiteRegEx": "Goller and Kuchler.", "year": 1996}, {"title": "Long short-term memory", "author": ["Sepp Hochreiter", "J\u00fcrgen Schmidhuber."], "venue": "Neural computation, 9(8):1735\u20131780.", "citeRegEx": "Hochreiter and Schmidhuber.,? 1997", "shortCiteRegEx": "Hochreiter and Schmidhuber.", "year": 1997}, {"title": "Visualizing and understanding recurrent networks", "author": ["Andrej Karpathy", "Justin Johnson", "Fei-Fei Li."], "venue": "arXiv:1506.02078.", "citeRegEx": "Karpathy et al\\.,? 2015", "shortCiteRegEx": "Karpathy et al\\.", "year": 2015}, {"title": "When are tree structures necessary for deep learning of representations? arXiv:1503.00185", "author": ["Jiwei Li", "Dan Jurafsky", "Eudard Hovy"], "venue": null, "citeRegEx": "Li et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Li et al\\.", "year": 2015}, {"title": "An extended model of natural logic", "author": ["Bill MacCartney", "Christopher D. Manning."], "venue": "Proc. of the Eighth International Conference on Computational Semantics.", "citeRegEx": "MacCartney and Manning.,? 2009", "shortCiteRegEx": "MacCartney and Manning.", "year": 2009}, {"title": "Semi-supervised recursive autoencoders for predicting sentiment distributions", "author": ["Richard Socher", "Jeffrey Pennington", "Eric H. Huang", "Andrew Y. Ng", "Christopher D. Manning."], "venue": "Proc. EMNLP.", "citeRegEx": "Socher et al\\.,? 2011", "shortCiteRegEx": "Socher et al\\.", "year": 2011}, {"title": "Sequence to sequence learning with neural networks", "author": ["Ilya Sutskever", "Oriol Vinyals", "Quoc V. Le."], "venue": "Proc. NIPS.", "citeRegEx": "Sutskever et al\\.,? 2014", "shortCiteRegEx": "Sutskever et al\\.", "year": 2014}, {"title": "Improved semantic representations from tree-structured long short-term memory networks", "author": ["Kai Sheng Tai", "Richard Socher", "Christopher D. Manning."], "venue": "Proc. ACL.", "citeRegEx": "Tai et al\\.,? 2015", "shortCiteRegEx": "Tai et al\\.", "year": 2015}, {"title": "Grammar as a foreign language", "author": ["Oriol Vinyals", "Lukasz Kaiser", "Terry Koo", "Slav Petrov", "Ilya Sutskever", "Geoffrey Hinton."], "venue": "arXiv:1412.7449.", "citeRegEx": "Vinyals et al\\.,? 2014", "shortCiteRegEx": "Vinyals et al\\.", "year": 2014}, {"title": "Recurrent neural network regularization", "author": ["Wojciech Zaremba", "Ilya Sutskever", "Oriol Vinyals."], "venue": "Proc. ICLR.", "citeRegEx": "Zaremba et al\\.,? 2015", "shortCiteRegEx": "Zaremba et al\\.", "year": 2015}, {"title": "ADADELTA: an adaptive learning rate method", "author": ["Matthew D. Zeiler."], "venue": "arXiv:1212.5701.", "citeRegEx": "Zeiler.,? 2012", "shortCiteRegEx": "Zeiler.", "year": 2012}], "referenceMentions": [{"referenceID": 10, "context": "Neural networks that encode sentences as realvalued vectors have been successfully used in a wide array of NLP tasks, including translation (Sutskever et al., 2014), parsing (Dyer et al.", "startOffset": 140, "endOffset": 164}, {"referenceID": 2, "context": ", 2014), parsing (Dyer et al., 2015), and sentiment analysis (Tai et al.", "startOffset": 17, "endOffset": 36}, {"referenceID": 11, "context": ", 2015), and sentiment analysis (Tai et al., 2015).", "startOffset": 32, "endOffset": 50}, {"referenceID": 3, "context": "These models are generally either sequence models based on recurrent neural networks, which build representations incrementally from left to right (Elman, 1990; Sutskever et al., 2014), or tree models based on recursive neural networks, which build representations incrementally according to the hierarchical structure of linguistic phrases (Goller and Kuchler, 1996; Socher et al.", "startOffset": 147, "endOffset": 184}, {"referenceID": 10, "context": "These models are generally either sequence models based on recurrent neural networks, which build representations incrementally from left to right (Elman, 1990; Sutskever et al., 2014), or tree models based on recursive neural networks, which build representations incrementally according to the hierarchical structure of linguistic phrases (Goller and Kuchler, 1996; Socher et al.", "startOffset": 147, "endOffset": 184}, {"referenceID": 4, "context": ", 2014), or tree models based on recursive neural networks, which build representations incrementally according to the hierarchical structure of linguistic phrases (Goller and Kuchler, 1996; Socher et al., 2011).", "startOffset": 164, "endOffset": 211}, {"referenceID": 9, "context": ", 2014), or tree models based on recursive neural networks, which build representations incrementally according to the hierarchical structure of linguistic phrases (Goller and Kuchler, 1996; Socher et al., 2011).", "startOffset": 164, "endOffset": 211}, {"referenceID": 11, "context": "Nevertheless, tree models have not shown the kinds of dramatic performance improvements over sequence models that their billing would lead one to expect: head-to-head comparisons with sequence models show either modest improvements (Tai et al., 2015) or none at all (Li et al.", "startOffset": 232, "endOffset": 250}, {"referenceID": 7, "context": ", 2015) or none at all (Li et al., 2015).", "startOffset": 23, "endOffset": 40}, {"referenceID": 12, "context": "We believe this is plausible, on the basis of other recent research (Vinyals et al., 2014; Karpathy et al., 2015).", "startOffset": 68, "endOffset": 113}, {"referenceID": 6, "context": "We believe this is plausible, on the basis of other recent research (Vinyals et al., 2014; Karpathy et al., 2015).", "startOffset": 68, "endOffset": 113}, {"referenceID": 0, "context": "Our methods are based on Bowman et al. (2015), in which we describe an experiment and corresponding artificial dataset which tests this ability in two tree models.", "startOffset": 25, "endOffset": 46}, {"referenceID": 11, "context": "Our results engage with those of Vinyals et al. (2014) and Dyer et al.", "startOffset": 33, "endOffset": 55}, {"referenceID": 2, "context": "(2014) and Dyer et al. (2015), who find that sequence models can learn to recognize syntactic structure in natural language, at least when trained on explicitly syntactic tasks.", "startOffset": 11, "endOffset": 30}, {"referenceID": 0, "context": "We use the architecture depicted in Figure 1a, which builds on the one used in Bowman et al. (2015). The model architecture uses two copies of a single sentence model (a tree or sequence model) to encode the premise and hypothesis (left and right side) sentences, and then uses those encodings as the features for a multilayer classifier which predicts one of the seven relations.", "startOffset": 79, "endOffset": 100}, {"referenceID": 11, "context": "2), and TreeLSTM (Tai et al., 2015) activation functions.", "startOffset": 17, "endOffset": 35}, {"referenceID": 5, "context": "In addition, we use a sequence model (Figure 1c) with an LSTM activation function (Hochreiter and Schmidhuber, 1997) implemented as in Zaremba et al.", "startOffset": 82, "endOffset": 116}, {"referenceID": 5, "context": "In addition, we use a sequence model (Figure 1c) with an LSTM activation function (Hochreiter and Schmidhuber, 1997) implemented as in Zaremba et al. (2015). Experiments with simpler non-LSTM RNN sequence models tended to badly underfit the training data, and are not included here.", "startOffset": 83, "endOffset": 157}, {"referenceID": 14, "context": "Training We randomly initialize all embeddings and layer parameters, and train them using minibatch stochastic gradient descent with AdaDelta (Zeiler, 2012) learning rates.", "startOffset": 142, "endOffset": 156}], "year": 2017, "abstractText": "Tree-structured neural networks encode a particular tree geometry for a sentence in the network design. However, these models have at best only slightly outperformed simpler sequence-based models. We hypothesize that neural sequence models like LSTMs are in fact able to discover and implicitly use recursive compositional structure, at least for tasks with clear cues to that structure in the data. We demonstrate this possibility using an artificial data task for which recursive compositional structure is crucial, and find that the sequence model can learn the underlying patterning. The sequence model is better in that it learns the value of tree structure from the data in an emergent way, while the treestructured model is better in being able to learn with greater statistical efficiency due to its informative prior model structure.", "creator": "TeX"}}}