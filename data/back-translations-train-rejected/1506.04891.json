{"id": "1506.04891", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "16-Jun-2015", "title": "Author Identification using Multi-headed Recurrent Neural Networks", "abstract": "Recurrent neural networks (RNNs) are very good at modelling the flow of text, but typically need to be trained on a far larger corpus than is available for the PAN 2015 Author Identification task. This paper describes a novel approach where the output layer of a character-level RNN language model is split into several independent predictive sub-models, each representing an author, while the recurrent layer is shared by all. This allows the recurrent layer to model the language as a whole without over-fitting, while the outputs select aspects of the underlying model that reflect their author's style. The method proves competitive, ranking first in two of the four languages.", "histories": [["v1", "Tue, 16 Jun 2015 09:41:55 GMT  (503kb)", "http://arxiv.org/abs/1506.04891v1", "8 pages, 3 figures Notebook for the PAN@CLEF Author Identification challange"], ["v2", "Tue, 16 Aug 2016 05:04:57 GMT  (502kb)", "http://arxiv.org/abs/1506.04891v2", "8 pages, 3 figures Version 1 was a notebook for the PAN@CLEF Author Identification challenge. Version 2 is expanded to be a full paper for CLEF2016"]], "COMMENTS": "8 pages, 3 figures Notebook for the PAN@CLEF Author Identification challange", "reviews": [], "SUBJECTS": "cs.CL cs.LG cs.NE", "authors": ["douglas bagnall"], "accepted": false, "id": "1506.04891"}, "pdf": {"name": "1506.04891.pdf", "metadata": {"source": "CRF", "title": "Author Identification using multi-headed Recurrent Neural Networks Notebook for PAN at CLEF 2015", "authors": ["Douglas Bagnall"], "emails": ["douglas@halo.gen.nz"], "sections": [{"heading": "1 Introduction", "text": "In fact, it is in such a way that most of us will be able to go to another world, in which they are able to move, and in which they are able to live, to live, to live, to live, to live, to live, to live, to live, to live, to live, to live, to live, to live, to live, to live, to live, to live, to live, to live, to live, to live, to live, to live, to live, to live, to live, to live, to live, to live, to live, to live, to live, to live, to live, to live, to live, to live, to live, to live, to live, to live, to live, to live, to live, to live, to live, to live, to live, to live, to live, to live, to live, to live, to live, to live, to live, to live, to live, to live"}, {"heading": "2 Method", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "2.1 Text preprocessing", "text": "Known and unknown texts are mapped to a smaller set of characters to reduce arithmetic complexity and eliminate the overwhelming intrinsic meaning of extremely rare characters; a separate mapping is used for each language; the text is first converted to the NFKD standard form, which splits accented letters into letters, followed by a combined accent (for example, the codepoint < U + 00E0 > (\"A\") becomes < U + 0061 > < U + 0300 > (\"A\" followed by a combination of heavy letters); capital letters are further separated into uppercase letters, followed by the corresponding lowercase letter. Thus, the triple < right combining uppercase > a < left combining heavy accent >. Various rare characters that seemed largely equivalent are mapped together; for example, the en-dash letters (\"-\" seem to be too rare and too common in practice)."}, {"heading": "2.2 Multi-headed recurrent neural network language model", "text": "A single recurrent neural network is trained to predict the text flow of many authors while simultaneously sharing a collective model of the entire language. In the output layer, there is a Softmax group for each author and sometimes a different one for a \"control corpus\" - a large body of text designed to help prevent overfitting of the recurrent layer. To simplify matters, the PAN-2014 training set is used for the control corpus. It turns out that there is some overlap between the training sets for 2014 and 2015 (and possibly test sets), so the control corpses are not completely independent; empirically, they seem to have very little positive effect."}, {"heading": "2.3 ReSQRT activation", "text": "The activation function used for the recurring layer is a corrected displaced square root, or ReSQRT, defined as asf (x) = {\u221a x + 1 \u2212 1 if x \u2265 00 otherwise. The derivative is 1 2 \u221a x + 1 for x > 0 and otherwise 0. With respect to y = f (x) (which is of practical use in training), the non-zero part is 12 (y + 1), following the model of the widely used rectified linear unit (ReLU, [2] defined as f (x) = max (x, 0)), since the output and derivative is zero for all non-positive numbers, which provides performance and propagative benefit and allows neurons to distance themselves from the opination to areas outside their specialty. ReLU can be difficult in recursive neural networks, as the absence of an inherent scale means that it can more easily slip into an explosive amplification cycle than conventional mercury activations."}, {"heading": "2.4 Training", "text": "The training is structured as a number of \"sub-epochs.\" In a sub-epoch, each author delivers a training text. Where there is a text for each author, a sub-epoch is the same as a conventional epoch; otherwise, the author's texts are used cyclically. In some runs, all the texts of each author are linked - in these cases, the sub-epoch is also a true epoch. In another epoch, each sub-epoch is \"balanced,\" with the documents drawn by each author until everyone has learned roughly the same amount. At each training step, there is a chance that the training example \"seeps\" and influences other authors as if they had also made that particular character choice. The initial empty rate is in the order of 1 / N, where N is the number of authors, and it decays exponentially with each sub-epoch."}, {"heading": "2.5 Ensemble of variations", "text": "The final results combine several runs using different configurations of meta-parameters. Approximate ranges are given in Table 1. Each run consists of a training phase lasting several minutes or hours, followed by the calculation of cross-entropies for the unknown documents, which take less than a second. An arbitrary search selects seemingly reasonable meta-parameters for each language, and random selections of these configurations are applied in the evaluation ensembles. 3 The evaluations take place with a time-out; after a specified number of hours, all test results obtained are calculated collectively. The duration devoted to each ensemble has been determined by the looming submission deadlines. In tests with the training set (and according to accepted wisdom), ensembles performed slightly better than the majority of their selectors. 2 See https: / / github.com / douglasbagnall / recur and https: / / github.com / pan-bisde / carpools for the configuration of the ensembles needed."}, {"heading": "2.6 Interpretation", "text": "For N authors, the system generates N cross entropy values for each document. Values are not directly comparable with each other because there are both random deviations in the performance of the various sub-models and inherent deviations in the predictability of the texts. Values have been normalized on both axes. For each author's submodel, the ranking of values for the unknown text determines the probability that the author wrote the text. For example, if a text takes the first place (i.e. has the lowest score), it is probably the work of the author; if he occupies the last place, probably not. This system is probably not the best, but it has worked well enough and has fallen into oblivion. the end result must take the form of a probability between 0 and 1, with 0.5 having a special meaning for the C @ 1 calculation. The task is designed in such a way that in exactly half of the cases the unknown documents come from the known author, i.e. (assuming confidence in the model is given) the correct method of getting the point to point the Spaniards, while the median earlier median is 0.5, while the median is better to score the documents."}, {"heading": "3 Results", "text": "The results are in Table 2.The evaluation of the Greek task is higher than expected, based on experiments with the training set, while the evaluation of the Spanish task is lower. English and Dutch results are no surprise."}, {"heading": "4 Discussion", "text": "The consistently poor results for the Dutch task appear to reflect a drastic genre difference between the known and unknown texts. On the other hand, where the genre and topic seem to be closer, many competitors performed much better on this task, presumably by not focusing on the local flow of characters and instead using linguistic knowledge or otherwise deriving greater ranges. On the other hand, where the genre and topic seem to be closer, this model performs very well, although it does not use linguistic or natural language processing techniques. It should work well for transferring to other languages. It should also work very well in accordance with unrelated methods. Without an active recurring layer, the model fails to predict the likelihood of unigram signs per author via the output bias vector. Cross-entropy in this case boils down to weighting the frequency of characters in the text. If executed this way on the training set (accidentally), the AUC score is 0.85 for English and 0.91 for Spanish, which was the best for English."}], "references": [{"title": "Statistical Language Models Based on Neural Networks", "author": ["T. Mikolov"], "venue": "Ph.D. thesis, Ph. D. thesis, Brno University of Technology", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2012}, {"title": "Rectified linear units improve restricted boltzmann machines", "author": ["V. Nair", "G.E. Hinton"], "venue": "Proceedings of the 27th International Conference on Machine Learning (ICML-10). pp. 807\u2013814", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2010}, {"title": "Overview of the Author Identification Task at PAN 2015", "author": ["E. Stamatatos", "W. Daelemans", "B. Verhoeven", "P. Juola", "A. Lopez Lopez", "M. Potthast", "B. Stein"], "venue": "Working Notes Papers of the CLEF 2015 Evaluation Labs. CEUR Workshop Proceedings, CLEF and CEUR-WS.org (Sep", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2015}], "referenceMentions": [{"referenceID": 0, "context": "Tom\u00e1\u0161 Mikolov\u2019s PhD thesis [1] offers a good introduction to these algorithms and the use of simple recurrent neural networks for language modelling, a task he shows they excel at.", "startOffset": 27, "endOffset": 30}, {"referenceID": 2, "context": "[3] 1 Each mini-corpus contains 1 to 5 documents known to be by a single author.", "startOffset": 0, "endOffset": 3}, {"referenceID": 1, "context": "This follows the model of the widely used rectified linear unit (ReLU,[2] defined as f(x) = max(x, 0)) in that the output and derivative is zero for all non-positive numbers, which offers performance and propagative benefits and allows neurons to opt out of opining on areas outside their speciality.", "startOffset": 70, "endOffset": 73}], "year": 2015, "abstractText": "Recurrent neural networks (RNNs) are very good at modelling the flow of text, but typically need to be trained on a far larger corpus than is available for the PAN Author Identification task. This paper describes a novel approach where the output layer of a characterlevel RNN language model is split into several independent predictive clusters, each representing an author, while the recurrent layer is shared by all. This model allows the recurrent layer to model the language as a whole without over-fitting, while the outputs select aspects of the underlying model that reflect their author\u2019s style. The method proved to be quite competitive.", "creator": " XeTeX output 2015.06.13:2127"}}}