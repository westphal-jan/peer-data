{"id": "1506.04908", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "16-Jun-2015", "title": "Learning with Clustering Structure", "abstract": "We study a supervised clustering problem seeking to cluster either features, tasks or sample points using losses extracted from supervised learning problems. We formulate a unified optimization problem handling these three settings and derive algorithms whose core iteration complexity is concentrated in a k-means clustering step, which can be approximated efficiently. We test our methods on both artificial and realistic data sets extracted from movie reviews and 20NewsGroup.", "histories": [["v1", "Tue, 16 Jun 2015 10:44:30 GMT  (241kb,D)", "http://arxiv.org/abs/1506.04908v1", "Submitted to NIPS"], ["v2", "Fri, 11 Mar 2016 09:22:47 GMT  (244kb,D)", "http://arxiv.org/abs/1506.04908v2", "Submitted to ICML"], ["v3", "Mon, 19 Sep 2016 08:37:40 GMT  (212kb,D)", "http://arxiv.org/abs/1506.04908v3", "Completely rewritten. New convergence proofs in the clustered and sparse clustered case. New projection algorithm on sparse clustered vectors"]], "COMMENTS": "Submitted to NIPS", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["vincent roulet", "fajwel fogel", "alexandre d'aspremont", "francis bach"], "accepted": false, "id": "1506.04908"}, "pdf": {"name": "1506.04908.pdf", "metadata": {"source": "CRF", "title": null, "authors": [], "emails": [], "sections": [{"heading": "1. INTRODUCTION", "text": "Using information from a monitored problem such as regression or classification, attempts are made to discover hidden clusters of characteristics, tasks or samples that allow both inferences, and additional structural insights into the data.In the classic multi-task setting clustering predictors of related tasks often helps prediction. In computer vision, for example, classifiers that are associated with different types of cats should be fairly similar, but well separated from classifiers that are associated with cars, and the inclusion of this structural information in the training phase should improve performance. This problem has been well investigated in the multi-task literature [1, 2, 3].Likewise, if there are some groups of highly correlated characteristics, reducing dimensionality by assigning some groups of characteristics that are beneficial in both prediction and interpretation."}, {"heading": "2. SUPERVISED CLUSTERING OF FEATURES, TASKS OR SAMPLES", "text": "In fact, the fact is that most of us are able to go in search of a solution that is capable, in which they are able, in which they are able to move."}, {"heading": "3. ALGORITHMS", "text": "We start with simple procedures, then we propose a non-convex projected gradient pedigree, such as can be used in a classic supervised learning problem, and then cluster predictors together with k-means. In the same spirit, the procedure can be repeated in the case of a soft clustering penalty, when clustering points are collected, which can be applied to the predictors of each group and the mapping of each point. These methods are fast, but very dependent on initialization."}, {"heading": "Frobenius Logistic Trace Ridge OM Ridge PGK Logistic", "text": "4.2.2. Predictions of ratings related to ratings based on word groups. We perform \"sentiment\" analysis of newspaper reviews. We use the publicly available data set introduced in [14], which contains movie reviews paired with star ratings. We treat it as a regression problem, considering log responses for y in (0, 1) and empirical distribution of words as covariates. With a vocabulary of 5000 terms selected by tf-idf, the corpus contains 5006 documents and contains 1.6 million words. We rate our regression algorithms with cluster characteristics against standard regression approaches: lasso, ridge and ridge followed by mediums on predictors. All algorithms have been cross-validated to 80% of the data set 5 times and then tested for the remaining 20%. The number of clusters has been arbitrarily set to 10, although we do not indicate significant changes in the standard characters on predictors. All algorithms have been cross-validated to 80% of the data set 5 times and then tested for the remaining 20%. The number of clusters has been arbitrarily set to 10, although we do not indicate any significant changes in the variation of the number of the additional numbers shown in the table 10, where the results are similar to the ones in the table 5."}, {"heading": "PGK OM+PGK Ridge+Kmeans Lasso Ridge", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "5. DISCUSSION", "text": "We have developed a unified framework for supervised clustering of tasks, characteristics, or samples, and provided two robust algorithms to solve the associated optimization problems. Results on synthetic and realistic text datasets indicate that our method is competitive compared to traditional regression and classification methods, while it has the added benefit of providing clusters of tasks, features, or samples. Similar to compressed capture, optimization occurs on a unification of subspaces, so it would be interesting to see under what assumptions our algorithms can restore the optimal solution to the supervised cluster problem, analogous to the RIP conditions for iterative hard thresholds."}, {"heading": "6. APPENDIX", "text": "We always assume that ZTZ is inversible, i.e. that the identity matrix in Rp \u00b7 p, p = 1p1 T pp and p = Ip \u2212 p is the corresponding centering matrix. For all settings, the input samples are represented by the matrix X = (x1, xn) T-matrix X = (xn) T-matrix Rn \u00b7 d and their respective labels by y = (y1,... yn) Y = R-matrix.For all settings problems, the input samples are represented by the matrix X = (x1, xn) T-matrix Z \u00b7 d and their respective labels by y = (y1,) Y = R-matrix. The classification problems are placed in the multitasking environment, each class corresponds to a task. We denote the vector of the respective labels by yk = (yk1, y k n)."}, {"heading": "INRIA - SIERRA PROJECT TEAM & D.I., UMR 8548,", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "E\u0301COLE NORMALE SUPE\u0301RIEURE, PARIS, FRANCE.", "text": "Email address: vincent.roulet @ inria.fr"}, {"heading": "C.M.A.P., E\u0301COLE POLYTECHNIQUE, UMR CNRS 7641", "text": "E-mail address: fajwel.fogel @ cmap.polytechnique.frCNRS & D.I., UMR 8548, E-COLE NORMALE SUPE \u0301 RIEURE, PARIS, FRANCE E-mail address: aspremon @ ens.fr"}, {"heading": "INRIA - SIERRA PROJECT TEAM & D.I., UMR 8548,", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "E\u0301COLE NORMALE SUPE\u0301RIEURE, PARIS, FRANCE.", "text": "E-mail address: francis.bach @ inria.fr"}], "references": [{"title": "Convex multi-task feature learning", "author": ["Andreas Argyriou", "Theodoros Evgeniou", "Massimiliano Pontil"], "venue": "Machine Learning,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2008}, {"title": "Clustered multi-task learning: A convex formulation", "author": ["Laurent Jacob", "Jean philippe Vert", "Francis R. Bach"], "venue": "Advances in Neural Information Processing Systems", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2009}, {"title": "Convex learning of multiple tasks and their structure", "author": ["Carlo Ciliberto", "Tomaso Poggio", "Lorenzo Rosasco"], "venue": "arXiv preprint arXiv:1504.03101,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2015}, {"title": "Simultaneous regression shrinkage, variable selection, and supervised clustering of predictors with oscar", "author": ["Howard D. Bondell", "Brian J. Reich"], "venue": null, "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2008}, {"title": "A divisive information theoretic feature clustering algorithm for text classification", "author": ["Inderjit S. Dhillon", "Subramanyam Mallela", "Rahul Kumar"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2003}, {"title": "A fuzzy self-constructing feature clustering algorithm for text classification", "author": ["Jung-Yi Jiang", "Ren-Jia Liou", "Shie-Jue Lee"], "venue": "Knowledge and Data Engineering, IEEE Transactions on,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2011}, {"title": "Efficiently enforcing diversity in multi-output structured prediction", "author": ["Abner Guzman-Rivera", "Pushmeet Kohli", "Dhruv Batra", "Rob Rutenbar"], "venue": "In Proceedings of the Seventeenth International Conference on Artificial Intelligence and Statistics,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2014}, {"title": "Large-scale learning for image classification", "author": ["Zaid Harchaoui"], "venue": "http://www.di.ens.fr/ willow/events/cvml2013/materials/slides/thursday/harch_cvml13.pdf,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2013}, {"title": "k-means++: The advantages of careful seeding", "author": ["David Arthur", "Sergei Vassilvitskii"], "venue": "In Proceedings of the eighteenth annual ACM-SIAM symposium on Discrete algorithms,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2007}, {"title": "A note on cluster analysis and dynamic programming", "author": ["Richard Bellman"], "venue": "Mathematical Biosciences,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 1973}, {"title": "Ckmeans. 1d. dp: optimal k-means clustering in one dimension by dynamic programming", "author": ["Haizhou Wang", "Mingzhou Song"], "venue": "The R Journal,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2011}, {"title": "An algorithm for quadratic programming", "author": ["Marguerite Frank", "Philip Wolfe"], "venue": "Naval research logistics quarterly,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 1956}, {"title": "Revisiting frank-wolfe: Projection-free sparse convex optimization", "author": ["Martin Jaggi"], "venue": "In Proceedings of the 30th International Conference on Machine Learning", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2013}, {"title": "Seeing stars: Exploiting class relationships for sentiment categorization with respect to rating scales", "author": ["Bo Pang", "Lillian Lee"], "venue": "In Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2005}, {"title": "Iterative hard thresholding for compressed sensing", "author": ["Thomas Blumensath", "Mike E. Davies"], "venue": "Applied and Computational Harmonic Analysis,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2009}], "referenceMentions": [{"referenceID": 0, "context": "This problem has been well studied in the multi-task learning literature [1, 2, 3].", "startOffset": 73, "endOffset": 82}, {"referenceID": 1, "context": "This problem has been well studied in the multi-task learning literature [1, 2, 3].", "startOffset": 73, "endOffset": 82}, {"referenceID": 2, "context": "This problem has been well studied in the multi-task learning literature [1, 2, 3].", "startOffset": 73, "endOffset": 82}, {"referenceID": 3, "context": "Similarly, when there exists some groups of highly correlated features, reducing dimensionality by assigning the same weights to some groups of features can be beneficial both in terms of prediction and interpretation [4].", "startOffset": 218, "endOffset": 221}, {"referenceID": 4, "context": "This often occurs in text classification, where it is natural to group together words having the same meaning for a given task [5, 6].", "startOffset": 127, "endOffset": 133}, {"referenceID": 5, "context": "This often occurs in text classification, where it is natural to group together words having the same meaning for a given task [5, 6].", "startOffset": 127, "endOffset": 133}, {"referenceID": 6, "context": "Finally, in some settings, it can be valuable to cluster sample points, with each cluster having its own distinct prediction function [7].", "startOffset": 134, "endOffset": 137}, {"referenceID": 7, "context": "Here, we present a unified and flexible framework for supervised clustering over the \u201cdata cube\u201d of either tasks, features or sample points (a representation introduced by [8]).", "startOffset": 172, "endOffset": 175}, {"referenceID": 8, "context": "While the original optimization problem is non-convex, we show that the core nonconvexity is concentrated in a subproblem similar to k-means, which we solve using classical approximation techniques [9].", "startOffset": 198, "endOffset": 201}, {"referenceID": 9, "context": "In the particular case of feature clustering for regression, the k-means steps are performed in dimension one, and can therefore be solved exactly by dynamic programming [10, 11].", "startOffset": 170, "endOffset": 178}, {"referenceID": 10, "context": "In the particular case of feature clustering for regression, the k-means steps are performed in dimension one, and can therefore be solved exactly by dynamic programming [10, 11].", "startOffset": 170, "endOffset": 178}, {"referenceID": 11, "context": "Our formulation is then an explicit convex relaxation which can be solved efficiently using the conditional gradient algorithm [12, 13].", "startOffset": 127, "endOffset": 135}, {"referenceID": 12, "context": "Our formulation is then an explicit convex relaxation which can be solved efficiently using the conditional gradient algorithm [12, 13].", "startOffset": 127, "endOffset": 135}, {"referenceID": 1, "context": "As in [2], the clustering penalty \u03a9SC(W,V ) can be decomposed into three separable terms as follows (see Figure 1 for an illustration).", "startOffset": 6, "endOffset": 9}, {"referenceID": 3, "context": "We present the linear regression case [4], which can be extended to logistic regression and classification.", "startOffset": 38, "endOffset": 41}, {"referenceID": 0, "context": "Given a set of K supervised tasks like regression or binary classification, multitask learning aims at jointly solving these tasks, hoping that each task can benefit from the information given by other tasks [1, 2, 3].", "startOffset": 208, "endOffset": 217}, {"referenceID": 1, "context": "Given a set of K supervised tasks like regression or binary classification, multitask learning aims at jointly solving these tasks, hoping that each task can benefit from the information given by other tasks [1, 2, 3].", "startOffset": 208, "endOffset": 217}, {"referenceID": 2, "context": "Given a set of K supervised tasks like regression or binary classification, multitask learning aims at jointly solving these tasks, hoping that each task can benefit from the information given by other tasks [1, 2, 3].", "startOffset": 208, "endOffset": 217}, {"referenceID": 8, "context": "Although it is a non-convex problem, k-means++ gives general approximation bounds on its solution [9].", "startOffset": 98, "endOffset": 101}, {"referenceID": 11, "context": "conditional gradient, [12, 13]).", "startOffset": 22, "endOffset": 30}, {"referenceID": 12, "context": "conditional gradient, [12, 13]).", "startOffset": 22, "endOffset": 30}, {"referenceID": 9, "context": "When clustering features, we optimize over the coefficients associated with each feature and the k-means step can be performed exactly using dynamic programming [10, 11].", "startOffset": 161, "endOffset": 169}, {"referenceID": 10, "context": "When clustering features, we optimize over the coefficients associated with each feature and the k-means step can be performed exactly using dynamic programming [10, 11].", "startOffset": 161, "endOffset": 169}, {"referenceID": 9, "context": "In fact, in this particular case, the k-means subproblem is one-dimensional and can be solved exactly using dynamic programming [10, 11].", "startOffset": 128, "endOffset": 136}, {"referenceID": 10, "context": "In fact, in this particular case, the k-means subproblem is one-dimensional and can be solved exactly using dynamic programming [10, 11].", "startOffset": 128, "endOffset": 136}, {"referenceID": 3, "context": "We compare in Table 3 the proposed algorithms to OSCAR [4], Ridge, Lasso and Ridge followed by k-means on the weights (using associated centroids as predictors).", "startOffset": 55, "endOffset": 58}, {"referenceID": 2, "context": "In Table 4, we compare our approach to other classical regularizations such as the Frobenius norm and the trace norm, as implemented in [3], using either a ridge or a logistic loss.", "startOffset": 136, "endOffset": 139}, {"referenceID": 13, "context": "We use the publicly available dataset introduced in [14] which contains movie reviews paired with star ratings.", "startOffset": 52, "endOffset": 56}, {"referenceID": 14, "context": "Similarly as in compressed sensing, optimization is made on a union of subspaces, hence in analogy with RIP conditions for iterative hard thresholding [15], it would be interesting to see under which assumptions our algorithms can recover the optimal solution to the supervised clustering problem.", "startOffset": 151, "endOffset": 155}], "year": 2015, "abstractText": "We study a supervised clustering problem seeking to cluster either features, tasks or sample points using losses extracted from supervised learning problems. We formulate a unified optimization problem handling these three settings and derive algorithms whose core iteration complexity is concentrated in a k-means clustering step, which can be approximated efficiently. We test our methods on both artificial and realistic data sets extracted from movie reviews and 20NewsGroup.", "creator": "LaTeX with hyperref package"}}}