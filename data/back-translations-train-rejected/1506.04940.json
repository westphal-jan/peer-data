{"id": "1506.04940", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "16-Jun-2015", "title": "Recognize Foreign Low-Frequency Words with Similar Pairs", "abstract": "Low-frequency words place a major challenge for automatic speech recognition (ASR). The probabilities of these words, which are often important name entities, are generally under-estimated by the language model (LM) due to their limited occurrences in the training data. Recently, we proposed a word-pair approach to deal with the problem, which borrows information of frequent words to enhance the probabilities of low-frequency words. This paper presents an extension to the word-pair method by involving multiple `predicting words' to produce better estimation for low-frequency words. We also employ this approach to deal with out-of-language words in the task of multi-lingual speech recognition.", "histories": [["v1", "Tue, 16 Jun 2015 12:31:43 GMT  (26kb,D)", "http://arxiv.org/abs/1506.04940v1", null]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["xi ma", "xiaoxi wang", "dong wang", "zhiyong zhang"], "accepted": false, "id": "1506.04940"}, "pdf": {"name": "1506.04940.pdf", "metadata": {"source": "CRF", "title": "Recognize Foreign Low-Frequency Words with Similar Pairs", "authors": ["Xi Ma", "Xiaoxi Wang", "Dong Wang", "Zhiyong Zhang"], "emails": ["mx@cslt.riit.tsinghua.edu.cn", "wxx@cslt.riit.tsinghua.edu.cn", "wangdong99@mails.tsinghua.edu.cn", "zhangzy@cslt.riit.tsinghua.edu.cn"], "sections": [{"heading": "1. Introduction", "text": "Most of them are unable to grasp the probability of a word that, in its entirety, is capable of underestimating the probability of words used in the past; most of them have an obvious limitation on the probability of words that play a role in education; and the words that are missing from education are simply unusable."}, {"heading": "2. Related works", "text": "This paper is related to dynamic language modeling, which adds new words and reweights word probabilities, especially approaches based on FSTs. This section examines some typical techniques of this approach, focusing primarily on class-based LM, which deals with dynamic vocabulary and low-frequency words. Class-based language modeling [7] is an approach that bundles similar words into classes and redistributes the probabilities of words in each class, for example according to their universal statistics. Class-based LM typically provides better representations than word-based LM for low-frequency words [4], because the class-based structure factorizes probabilities of low-frequency words into class probabilities, thus increasing the robustness of probability estimation. In addition, new words can easily be added to classes using class-based LM, resulting in dynamic vocabulary."}, {"heading": "3. Word enhancement by similar-pairs", "text": "In this section, we will first give a brief introduction to the FST-based ASR architecture and then introduce the similar pair method implemented on FSTs."}, {"heading": "3.1. Finite state transducer", "text": "A Finite State Transducer (FST) is essentially a Finite State Automaton (FSA) that generates both output and read input, and is represented as a graph in which each node indicates a state and each arc that links two nodes. A weight is assigned to each transition and each completed state. An FST example is shown in Fig. 1. In this example, the initial state is state 0, and the final state is state 2. A weight 3.5 has been assigned to the final state. Let (s, t, i: o / w) denote a transition where s and t are the input and output states, respectively, and i is the input symbol and o is the output symbol, and w is the associated transition weight. From initial state 0 to state 1, there are two transitions (0.1, a: x / 0.5) and (0.1, b: y / 1.5)."}, {"heading": "3.2. FST-based speech recognition", "text": "Most of the current large-format speech recognition systems are based on statistical models, such as hidden Markov models (HMMs), lexicon, decision trees, and n-gram LMs. All of these models can be converted to the FST models. In an FST, the correlation between input and output symbols represents the mapping from a low sequence (e.g. phones) to a high-level sequence (e.g. words), and the weights encode the probability distribution of the mapping. More importantly, FSTs representing different levels of statistical models can be assembled into a unified mapping function that links primary inputs to high-level outputs. The composed FST can be further optimized by standard FST operations, including determination, minimization, and weight shifting. This results in very compact and efficient graphs that represent knowledge of all the sequences involved in the word composition (LCM), which can be used to represent the language composition (LCM)."}, {"heading": "3.3. Low-frequency word enhancement with similar pairs", "text": "The method of similar pairs is based on the FST architecture. To improve low-frequency words and to perform the improvement on the LM FST or G chart, a list of manually defined similar pairs is provided with corresponding frequency information from training data. Low-frequency words are selected to be improved and the high-frequency words are selected to provide the enhancement information. Each similar pair in the list contains a high-frequency word and some low-frequency words. In the face of a number of similar words, the low-frequency words are improved by looking at the information of the high-frequency word, including its transitions in the G chart and the associated weights. The high-frequency words are preserved because they have already been well represented by the gram model."}, {"heading": "4. The Method", "text": "The extension of similar pairs method will be introduced in this section. Based on the method of similar pairs, the probability of a multiple transition of low frequency or new words is increased by looking at the information of high frequency words. In view of a series of words W = xi, x2,..., xm} that need to be increased, for each word xi-W a series of words Si = {y1, y2,..., yn} that are similar to xi can be selected manually from the training data. The similarity can be defined both in terms of syntactic roles and semantic meanings. We assume that for each word a series of words Si = {y1, y2,... if there is an n-gram of yj in the training process, the corresponding n-gram of xi should also have a relatively higher probability of appearance. As the probabilities are represented as the weights in the G graph in FST, according to this assumption, the new weight (probability) of xi (probability) of xi can be updated."}, {"heading": "5. Experiment", "text": "Bilingual telecom ASR tasks are selected to evaluate the proposed approach. We first present the experimental configurations and then present the performance with the proposed enhancement of low frequency English words based on similar pairs."}, {"heading": "5.1. Database", "text": "Our ASR task aims to transcribe conversations recorded by online service calls. The domain is the telecommunications service, and the language is either Chinese or English. The acoustic model (AM) is trained on a 1400-hour online voice recording manually transcribed by a major call center service provider. Chinese LM is trained on a corpus, including the transcription of AM training language and some protocols of web-based support systems in the telecommunications service.22 similar pairs are selected to evaluate the performance of the similar pair methodology. Each similar pair contains 1-5 high-frequency Chinese words and 1-4 new English words. A \"FOREIGN\" test set has been deliberately developed to test the improvement with these similar pairs, which consist of 42 sentences from the online language recording. For each transcription, some new English words occurring in the similar pairs will inevitably be used. In addition, a \"08 test group of common expressions will be selected to include the proposed expression of the GERAL 26th."}, {"heading": "5.2. Acoustic model training", "text": "The ASR system is based on the state-of-the-art approach of acoustic modelling HMM-DNN, which represents the dynamic properties of speech signals using the hidden Markov model (HMM) and represents the state-dependent signal distribution by the Deep Neural Network (DNN) model. The feature used is the 40-dimensional power spectrum of the FBank. An 11-frame splice window is used to link adjacent frames to capture long-term time dependencies on speech signals. Linear Discrimination Analysis (LDA) is applied to extend the dimension of the concatenated feature to the year 200.The Kaldi toolkit [13] is used to train the HMM and DNN models. The training process largely follows the WSJ s5 GPU recipe published by Kaldi. Specifically, a Gaussian-based mixing model (GMM) is used to align the pre-DNN telephone system."}, {"heading": "5.3. Language model training", "text": "Normalization involves removing unrecognized characters, standardizing various encoding schemes and normalizing the spelling of numbers and letters, then segmenting the training text into word sequences. In this study, a word segmentation tool provided by Google is used. A total of 150,000 words are selected as an LM lexicon, according to the word frequency in the segmented training text. Subsequently, a 3-gram LM is trained with the SRILM Toolkit 1, which is smoothed by Kneser-Ney discounting. With the Kaldi Toolkit, n-gram LMs are converted into G diagrams, and with the openFST Toolkit2, FSTs are manipulated."}, {"heading": "5.4. Experiment result and analysis", "text": "ASR performance in terms of word error rate (WHO) is presented in Table 1, Table 2 and Table 3. We report the results on two test sets: \"GENERAL\" and \"FOREIGN,\" and the results with different scores on the improvement scale \"\u03b8\" and different amounts of radio frequency Chinese words are presented. It can be noted that ASR performance in expressions with new English words is significantly improved by the approach of several radio frequency Chinese words. Furthermore, the approach of several radio frequency Chinese words leads to better results. Interestingly, the improvement of these rare words does not result in a deterioration of the performance of other Chinese words in the \"GENERAL\" test set. Furthermore, the performance of the \"GENERAL\" test set remains virtually unchanged, suggesting that the proposed approach does not affect the overall performance of the ASR systems and is therefore safe to use."}, {"heading": "6. Conclusion", "text": "The experimental results showed that the proposed method can significantly improve the performance of speech recognition in low frequency and new words and does not affect ASR performance in general. This gives this method a fast area-specific adaptation where low frequency words need to be improved and new words need to be supported. Future work will focus on improving low frequency words using several similar words and combining this method with other dynamic LM approaches such as class-based LM."}, {"heading": "7. Acknowledgements", "text": "This research was supported by the National Science Foundation of China (NSFC) under Project No. 61371136 and MESTDC PhD Foundation Project No. 20130002120011 and was also supported by Sinovoice and Huilan Ltd."}, {"heading": "8. References", "text": "[1] S. Katz, \"Estimation of probabilities from spare data for thelanguage model component of a speech recognizer,\" Acoustics, Speech and Signal Processing, IEEE vocactions on, vol. 35, no. 3, pp. 400-401, 1987. [2] R. Kneser and H. Ney, \"Improved backing-off for m-gram language modeling,\" Computer Speech & Language, vol. 4, pp. S. F. Chen and J. Goodman, \"An empirical study of smoothing techniques for language modeling, computer speech & Language, vol."}], "references": [{"title": "Estimation of probabilities from sparse data for the language model component of a speech recognizer", "author": ["S. Katz"], "venue": "Acoustics, Speech and Signal Processing, IEEE Transactions on, vol. 35, no. 3, pp. 400\u2013401, 1987.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 1987}, {"title": "Improved backing-off for m-gram language modeling", "author": ["R. Kneser", "H. Ney"], "venue": "Proceedings of ICASSP, 1995, pp. 181\u2013184.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 1995}, {"title": "An empirical study of smoothing techniques for language modeling", "author": ["S.F. Chen", "J. Goodman"], "venue": "Computer Speech & Language, vol. 13, no. 4, pp. 359\u2013393, 1999.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 1999}, {"title": "A class based language model for speech recognition", "author": ["W. Ward", "S. Issar"], "venue": "Acoustics, Speech, and Signal Processing, 1996. ICASSP-96. Conference Proceedings., 1996 IEEE International Conference on, vol. 1. IEEE, 1996, pp. 416\u2013418.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 1996}, {"title": "Low-frequency word enhancement with similar pairs in speech recognition", "author": ["D. Povey", "A. Ghoshal", "G. Boulianne", "L. Burget", "O. Glembek", "N. Goel", "M. Hannemann", "P. Motlicek", "Y. Qian", "P. Schwarz", "J. Silovsky", "G. Stemmer", "K. Vesely"], "venue": "ChinaSIP15 (submitted), 2015.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2015}, {"title": "Weighted finite-state transducers in speech recognition", "author": ["M. Mohri", "F. Pereira", "M. Riley"], "venue": "Computer Speech & Language, vol. 16, no. 1, pp. 69\u201388, 2002.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2002}, {"title": "Class-based n-gram models of natural language", "author": ["P.F. Brown", "P.V. Desouza", "R.L. Mercer", "V.J.D. Pietra", "J.C. Lai"], "venue": "Computational linguistics, vol. 18, no. 4, pp. 467\u2013479, 1992.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 1992}, {"title": "Transducer-based speech recognition with dynamic language models.", "author": ["M. Georges", "S. Kanthak", "D. Klakow"], "venue": "in INTER- SPEECH. Citeseer,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2013}, {"title": "A specialized wfst approach for class models and dynamic vocabulary", "author": ["P.R. Dixon", "C. Hori", "H. Kashioka"], "venue": "Thirteenth Annual Conference of the International Speech Communication Association, 2012.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2012}, {"title": "Topic-dependentclass-based-gram language model", "author": ["W. Naptali", "M. Tsuchiya", "S. Nakagawa"], "venue": "Audio, Speech, and Language Processing, IEEE Transactions on, vol. 20, no. 5, pp. 1513\u20131525, 2012.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2012}, {"title": "A class-based language model for large-vocabulary speech recognition extracted from part-ofspeech statistics", "author": ["C. Samuelsson", "W. Reichl"], "venue": "Acoustics, Speech, and Signal Processing, 1999. Proceedings., 1999 IEEE International Conference on, vol. 1. IEEE, 1999, pp. 537\u2013540.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 1999}, {"title": "The kaldi speech recognition toolkit", "author": ["D. Povey", "A. Ghoshal", "G. Boulianne", "L. Burget", "O. Glembek", "N. Goel", "M. Hannemann", "P. Motlicek", "Y. Qian", "P. Schwarz", "J. Silovsky", "G. Stemmer", "K. Vesely"], "venue": "Proceedings of ASRU, 2011, pp. 1\u20134.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2011}], "referenceMentions": [{"referenceID": 0, "context": "A well-known approach to dealing with low-frequency and absent words is various smoothing techniques such as backoff [1] and discount [2, 3].", "startOffset": 117, "endOffset": 120}, {"referenceID": 1, "context": "A well-known approach to dealing with low-frequency and absent words is various smoothing techniques such as backoff [1] and discount [2, 3].", "startOffset": 134, "endOffset": 140}, {"referenceID": 2, "context": "A well-known approach to dealing with low-frequency and absent words is various smoothing techniques such as backoff [1] and discount [2, 3].", "startOffset": 134, "endOffset": 140}, {"referenceID": 3, "context": ", the class-based LM with classes that are adaptable online [4].", "startOffset": 60, "endOffset": 63}, {"referenceID": 4, "context": "Recently, we proposed a similar-pair approach to deal with the problem [5].", "startOffset": 71, "endOffset": 74}, {"referenceID": 5, "context": "This approach has been implement with the LM FST graph [6].", "startOffset": 55, "endOffset": 58}, {"referenceID": 4, "context": "This paper is a following work of [5].", "startOffset": 34, "endOffset": 37}, {"referenceID": 6, "context": "The class-based language modeling [7] is an approach that clusters similar words into classes and the probabilities of words in each class are re-distributed, for instance according to their unigram statistics.", "startOffset": 34, "endOffset": 37}, {"referenceID": 3, "context": "Typically, the class-based LM delivers better representations than the word-based LM for low-frequency words [4], since the class-based structure factorizes probabilities of low-frequency words into class probabilities and class member probabilities, and so increases robustness of the probability estimation.", "startOffset": 109, "endOffset": 112}, {"referenceID": 7, "context": "Additionally, [8] and [9] introduced two techniques to build both the class-based LMs and the class words into FST graphs and embed class FSTs into the class-based LM FST.", "startOffset": 14, "endOffset": 17}, {"referenceID": 8, "context": "Similar approaches have been proposed in [10, 11, 12], where various dynamic embedding methods are introduced, and the classes are extended to complex grammars.", "startOffset": 41, "endOffset": 53}, {"referenceID": 9, "context": "Similar approaches have been proposed in [10, 11, 12], where various dynamic embedding methods are introduced, and the classes are extended to complex grammars.", "startOffset": 41, "endOffset": 53}, {"referenceID": 10, "context": "Similar approaches have been proposed in [10, 11, 12], where various dynamic embedding methods are introduced, and the classes are extended to complex grammars.", "startOffset": 41, "endOffset": 53}, {"referenceID": 4, "context": "posed in [5].", "startOffset": 9, "endOffset": 12}, {"referenceID": 4, "context": "The extensions we made in this paper for the work in [5] are two-fold: firstly, the similar-pair algorithm is extended to allow multiple predicting words, which enables multiple information engaged and thus better enhancement; second, the similar-pair approach is employed to deal with OOL words, which demonstrated that similar pairs can be cross-lingual.", "startOffset": 53, "endOffset": 56}, {"referenceID": 11, "context": "The Kaldi toolkit [13] is used to train the HMM and DNN models.", "startOffset": 18, "endOffset": 22}], "year": 2015, "abstractText": "Low-frequency words place a major challenge for automatic speech recognition (ASR). The probabilities of these words, which are often important name entities, are generally underestimated by the language model (LM) due to their limited occurrences in the training data. Recently, we proposed a wordpair approach to deal with the problem, which borrows information of frequent words to enhance the probabilities of lowfrequency words. This paper presents an extension to the wordpair method by involving multiple \u2018predicting words\u2019 to produce better estimation for low-frequency words. We also employ this approach to deal with out-of-language words in the task of multi-lingual speech recognition.", "creator": "LaTeX with hyperref package"}}}