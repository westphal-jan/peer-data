{"id": "1506.05001", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "16-Jun-2015", "title": "Using Hankel Matrices for Dynamics-based Facial Emotion Recognition and Pain Detection", "abstract": "This paper proposes a new approach to model the temporal dynamics of a sequence of facial expressions. To this purpose, a sequence of Face Image Descriptors (FID) is regarded as the output of a Linear Time Invariant (LTI) system. The temporal dynamics of such sequence of descriptors are represented by means of a Hankel matrix. The paper presents different strategies to compute dynamics-based representation of a sequence of FID, and reports classification accuracy values of the proposed representations within different standard classification frameworks. The representations have been validated in two very challenging application domains: emotion recognition and pain detection. Experiments on two publicly available benchmarks and comparison with state-of-the-art approaches demonstrate that the dynamics-based FID representation attains competitive performance when off-the-shelf classification tools are adopted.", "histories": [["v1", "Tue, 16 Jun 2015 15:22:46 GMT  (135kb)", "http://arxiv.org/abs/1506.05001v1", "in IEEE Proceedings of Workshop on Analysis and Modeling of Face and Gestures (CVPRW 2015)"]], "COMMENTS": "in IEEE Proceedings of Workshop on Analysis and Modeling of Face and Gestures (CVPRW 2015)", "reviews": [], "SUBJECTS": "cs.CV cs.AI cs.RO", "authors": ["liliana lo presti", "marco la cascia"], "accepted": false, "id": "1506.05001"}, "pdf": {"name": "1506.05001.pdf", "metadata": {"source": "CRF", "title": "Using Hankel Matrices for Dynamics-based Facial Emotion Recognition and Pain Detection \ufffdPre-Print of Proceedings of AMFG CVPRW 2015)", "authors": ["Liliana Lo Presti", "Marco La Cascia"], "emails": ["lilianalopresti@unipait"], "sections": [{"heading": null, "text": "This paper proposes a new approach to modelling the temporal dynamics of a sequence of facial expressions. To this end, a sequence of face image descriptors (FID) is considered as the output of a linear time invariant (LTI) system.The temporal dynamics of such a sequence of descriptors are represented using a Hankel matrix.The paper presents various strategies for calculating dynamically based representation of a sequence of FID and reports on the accuracy values of the proposed representations within different standard classification frameworks. The representations have been validated in two very demanding application areas: emotion recognition and pain detection. Experiments on two publicly available benchmarks and comparisons with modern approaches show that dynamics-based FID representation achieves competitive performance when tools for classification are used outside of these areas."}, {"heading": "1. Introduction", "text": "In fact, it is such that most people are able to move into another world, in which they are able, in which they are able, in which they are able, in which they are able, in which they are able, in which they are able to change the world, in which they are able to change the world, in which they are able, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live."}, {"heading": "2. Related Work", "text": "There is a comprehensive literature on facial recognition [37], [15] and facial expression analysis [35], [7]. Here, we focus on work that attempts to embed the temporal structure of the sequence of facial expression, either in the property of representation step by step or in emotion modeling. Classification is done using a limited local model (CLM) to obtain facial expressions, then extracts patches around these markers. A sparse representation of the patches is achieved by applying non-negative matrix factoring. Classification is performed using a minimum square SVM."}, {"heading": "3. Dynamics-based Emotion Representation", "text": "In this paper, a sequence of facial images is processed to extract a face representation on a frame-by-frame basis, resulting in a time series of face vectors [yo \ufffd.. \ufffd y\u03c4], where yt is the face representation associated with the t-th face image. Such a time sequence can be considered an output of an LTI system of unknown parameters [31]."}, {"heading": "3.1. Representation of Temporal Dynamics", "text": "In a linear temporally invariant system, two linear equations regulate the behaviour of the system in the following way: xk \ufffd 1 = A \u00b7 xk + wk; yk = C \u00b7 xk. (1) The first equation is known as the equation of state and includes the variable xk \u00b7 R u, which represents the udimensional internal state of the LTI system; the second equation is known as the measurement equation and establishes a link between the state of the system xk and the v-dimensional observable measurement yk. In such equations, the matrices A and C are constant over time, and wk \u00b2 N \u2212 Q is uncorrelated zero \u2212 mean Gaussian measurement noise.It is generally known [32] that in a sequence of measurements [yo \u00b7.....] the Y matrices of the matrix."}, {"heading": "3.2. Dynamics-based Expression Representation", "text": "In this paper, we propose to use a Hankel matrix to represent the dynamics of a sequence of facial images whose representation of properties results in a time series of vectors Y = [yo \ufffd.. \ufffd y\u03c4]. We compare three different dynamics-based emotion representations, which we describe below. \u2022 Single Hankel matrix representation: This representation uses the entire time series Y to form the Hankel matrix. \u2022 Moving window-based representation: While the earlier representation assumes a segmentation of the frame sequence in emotions, the matrix H \u00b7 HT used in Equation 4 is a square symmetrical matrix and Hankel matrices of sequences of different lengths easily comparable. \u2022 Moving window-based representation: While the earlier representation assumes a frame sequence in emotions, this representation could overcome this requirement by representing Y by a sequence of overlapping time windows (similar to [16]."}, {"heading": "4. Adopted Classification Framework", "text": "We have tested our dynamics-based emotion representations within multiple classification frames to test their robustness. \u2022 Nearest Neighbor Classifier NN): Given the dynamics-based representation of a test sequence, the recognized class is determined by the class name of the closest sequence in the training set; \u2022 Codebook-based Support Vector Machine CSVM): Linear one-on-all SVM models1 is drawn on the LTI codebook-based representation, and the estimated margin is used to classify the test sequence. \u2022 Dynamic Time Delay and NN-DTW + NN: This method is applied to the rolling window-based re-evaluation. DTW2 is used to align sequences of the Hankel matrices in the same way."}, {"heading": "5. Experimental Results", "text": "This paper focuses on the analysis of facial expressions in two challenging application areas: emotion recognition and pain recognition. In the following, we first describe the frame-based representation used to obtain the measurements [y0 \ufffd \u00b7 \u00b7 \u00b7 y\u03c4], then provide a brief description of the individual application areas and present our results."}, {"heading": "5.1. Feature Extraction", "text": "Our formulation is general and can be adopted with several types of facial characteristics. In this essay, to demonstrate the entire frame, we look at shape characteristics provided by an active appearance model [19], [20]. Therefore, facial expressions are presented as trajectories of 2D facial characteristics, as shown in Fig. 1. To create the Hankel matrices, we use the following frame-based characteristics representations: \u2022 concatenated 2D facial characteristic coordinates (L); \u2022 paired boundary distances (D); \u2022 concatenation of paired boundary distances and boundary coefficients (L + D). For each of these representations, the main component analysis (PCA) has been applied to reduce noise and dimensionality. We have selected a number of projections covering 99% of the total variance. The retained PCA coefficients are then used to create the dynamic-based representation of facial characteristics, as represented by the toolbox we have in the Hht3M section www.Hc.c.The toolbox is used in the Hht3L."}, {"heading": "5.2. Emotion Recognition", "text": "Emotion recognition deals with the problem of the inference of emotion (such as fear, anger, surprise, etc.) due to a sequence of facial images. The greatest difficulty in this area arises from the strong variation between subjects, especially in any type of emotion (such as sadness). Other challenges are related to the difficulty of extracting reliable feature representations due to lighting changes, biometric differences, and changes in head posture. Furthermore, the lack of depth information complicates emotion recognition due to ambiguities in the representation of the face shape. To demonstrate the idea behind this paper, we limit attention to segmented emotion recognition in the frontal view, as was the case in previous work such as [18], [1], [22], [33]."}, {"heading": "5.2.1 Data and Validation Protocols", "text": "We conducted emotion recognition experiments using the widely recognized Extended Cohn-Kanade (CK +) dataset [19], which provides facial expressions from 210 adults. Participants were instructed to perform multiple facial representations representing either individual or combination of action units. Based on the coded action units and using a validation procedure of the assigned label, the segmented recordings of participants \"emotions were divided into 7 categories: anger, contempt, disgust, fear, joy, sadness, surprise. In total, there are 327 sequences of the 7 commented emotions, performed by 118 different individuals. The number of images of these sequences ranges in [6 \ufffd 71] with an average value of about 18 \u00b1 8.6. The dataset provides ground-breaking tracking results obtained through an active appearance model that we use in our experiments. We adopted the validation protocol proposed in [19], which represents cross-validation of objects."}, {"heading": "5.2.2 Emotion Recognition \u2013 Results", "text": "We have performed a comprehensive validation of the dynamic-based emotion representations, the results of which are reported in Table 1. The table reports on the accuracy values per class for each emotion class and the average accuracy value. The table is divided into 4 parts. The first part compares dynamics-based representations when calculating the Hankel matrix over the entire sequence. We compare the single Hankel matrix representation within three classification frames: dynamic time distortion and NN classifier, NN classifier and majority selection, hidden Markov models. The third part of Table 1 reports on the results obtained directly from the raw data (without calculating a Han matrix-based representation) to highlight the advantage of using M dynamics."}, {"heading": "5.3. Pain Detection", "text": "In terms of the previously segmented emotion detection task, pain detection is even more difficult because the pain event needs to be located within the frame sequence. Pain can be a sporadic episode of varying duration, and painful facial expressions can vary greatly depending on the object or be confused with other emotions. In this essay, we treat pain detection as a binary classification problem. We use a sliding window approach and classify each time window to detect the pain event. We set the length of the time window empirically to 10 frames."}, {"heading": "5.3.1 Data and Validation Protocols", "text": "To test our dynamics-based FID representation for pain detection, we use the Painful dataset [20], which contains videos of patients \"faces as they moved their painful shoulder, with the goal of detecting between pain and pain-free events, commented on frame-per-frame with the Prkachin and Solomon pain intensity score (PSPI) [20] moving in [0, 15], where 0 does not mean pain, while a value greater than 0 indicates a certain pain intensity. However, our method works on time windows and validation requires a time window-based annotation. To take this into account, we consider the integral score IS, which is achieved by summing the PSPI score in a sliding window. We set the term of the time window to 0 if IS < and 1 if IS-based annotation."}, {"heading": "5.3.2 Pain Detection \u2013 Results", "text": "Table 2 reports on the accuracy of our moving window-based representation of the Hankel matrix. We test this representation using landmarks (L) and paired distances (D) within two frameworks: NN and CSVM. However, when NN was introduced, the training set was reduced by selecting only K-medoids for each class, with K set to 300. When CSVM is introduced, a code book of 50 Hankel matrices is displayed using K-medoids. We report in columns on the values of the confusion matrices for our binary classification experiment taking into account the corresponding threshold value. Positive indicates the pain event, while negative indicates the pain-free event. Therefore, TPR (true positive rate) and TNR (true negative rate) are the diagonal values of the confusion matrix. FNR (false negative rate) and FPR (false positive rate) are the agonal values."}, {"heading": "6. Conclusions and Future Work", "text": "In this paper, we have proposed using Hankel matrices to represent the dynamics of FIDs and to detect between different emotions. While Hankel matrices have already been used in action detection, to the best of our knowledge this work is the first to use such a type of representation for the analysis of facial expression. To investigate the performance of our dynamic based emotion representations, we have conducted extensive tests within different standard classification frames on a widely available publicly available benchmark (CK +). Our experiments show that under the same conditions of the classification framework, it is possible to achieve an increase of approximately 63 \u00b0 of the average accuracy values in the use of the observed measurements by using our dynamic based emotion representations. Overall, our approach achieves state-of-the-art performance by applying standard classification frames. We have also conducted experiments with the painful dataset to test whether our representation can be used to detect pain events."}, {"heading": "7. Acknowledgement", "text": "This work was partially supported by the Italian MIUR grant PON01 01687, SINTESYS - Security and INTElligence SYStem."}], "references": [{"title": "Person-independent facial expression detection using constrained local models", "author": ["S.W. Chew", "P. Lucey", "S. Lucey", "J. Saragih", "J.F. Cohn", "S. Sridharan"], "venue": "In Proc. of Conf. and Workshop on Automatic Face \ufffd Gesture Recognition (FG),", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2011}, {"title": "Improved facial expression recognition via uni-hyperplane classification", "author": ["S.W. Chew", "S. Lucey", "P. Lucey", "S. Sridharan", "J.F. Conn"], "venue": "In Proc. of Conf. on Computer Vision and Pattern Recognition (CVPR),", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2012}, {"title": "Sparse temporal representations for facial expression recognition", "author": ["S.W. Chew", "R. Rana", "P. Lucey", "S. Lucey", "S. Sridharan"], "venue": "In Advances in Image and Video Technology,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2012}, {"title": "Fast global alignment kernels", "author": ["M. Cuturi"], "venue": "In Int. Conf. on Machine Learning (ICML),", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2011}, {"title": "A kernel for time series based on global alignments", "author": ["M. Cuturi", "J. Vert", "O. Birkenes", "T. Matsui"], "venue": "In Proc. of Int. Conf. on Acoustics, Speech and Signal Processing (ICASSP),", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2007}, {"title": "Enabling dynamics in face analysis", "author": ["H. Dibeklio\u011flu"], "venue": "PhD thesis, University of Amsterdam,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2014}, {"title": "Automatic facial expression analysis: a survey", "author": ["B. Fasel", "J. Luettin"], "venue": "Pattern recognition,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2003}, {"title": "Matrix rank minimization with applications", "author": ["M. Fazel"], "venue": "PhD thesis,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2002}, {"title": "Automatic detection of pain intensity", "author": ["Z. Hammal", "J.F. Cohn"], "venue": "In Proceedings of the 14th ACM international conference on Multimodal interaction,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2012}, {"title": "Pain monitoring: A dynamic and context-sensitive system", "author": ["Z. Hammal", "M. Kunz"], "venue": "Pattern Recognition,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2012}, {"title": "Continuous AU intensity estimation using localized, sparse  facial feature space", "author": ["L.A. Jeni", "J.M. Girard", "J.F. Cohn", "F. De La Torre"], "venue": "In Proc. of Conf. on Automatic Face \ufffd Gesture Recognition (FG),", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2013}, {"title": "Continuous pain intensity estimation from facial expressions", "author": ["S. Kaltwang", "O. Rudovic", "M. Pantic"], "venue": "In Advances in Visual Computing,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2012}, {"title": "Controllability of dynamical systems. a survey", "author": ["J. Klamka"], "venue": "Bulletin of the Polish Academy of Sciences: Technical Sciences,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2013}, {"title": "Cross-view activity recognition using Hankelets", "author": ["B. Li", "O.I. Camps", "M. Sznaier"], "venue": "In Proc. of Conf. on Computer Vision and Pattern Recognition (CVPR),", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2012}, {"title": "An on-line learning method for face association in personal photo collection", "author": ["L. Lo Presti", "M. La Cascia"], "venue": "Image and Vision Computing,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2012}, {"title": "Gesture modeling by Hanklet-based hidden Markov model", "author": ["L. Lo Presti", "M. La Cascia", "S. Sclaroff", "O. Camps"], "venue": "In Computer Vision \u2013 ACCV 2014,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2015}, {"title": "Joint alignment and modeling of correlated behavior streams", "author": ["L. Lo Presti", "S. Sclaroff", "A. Rozga"], "venue": "In Int. Conf. on Computer Vision-Workshops,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2013}, {"title": "Emotional expression classification using time-series kernels", "author": ["A. Lorincz", "L.A. Jeni", "Z. Szab\u00f3", "J.F. Cohn", "T. Kanade"], "venue": "In Conf. on Computer Vision and Pattern Recognition Workshops (CVPRW),", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2013}, {"title": "The Extended Cohn-Kanade dataset (CK+): A complete dataset for action unit and emotion-specified expression", "author": ["P. Lucey", "J.F. Cohn", "T. Kanade", "J. Saragih", "Z. Ambadar", "I. Matthews"], "venue": "In Proc. of Conf. on Computer Vision and Pattern Recognition Workshops (CVPRW),", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2010}, {"title": "Painful data: The UNBC-McMaster shoulder pain expression archive database", "author": ["P. Lucey", "J.F. Cohn", "K.M. Prkachin", "P.E. Solomon", "I. Matthews"], "venue": "In Proc. of Conf. and Workshop on Automatic Face \ufffd Gesture Recognition (FG),", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2011}, {"title": "Affective state level recognition in naturalistic facial and vocal expressions", "author": ["H. Meng", "N. Bianchi-Berthouze"], "venue": "IEEE Transactions on Cybernetics,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2014}, {"title": "A generative restricted Boltzmann machine based method for high-dimensional motion data modeling", "author": ["S. Nie", "Z. Wang", "Q. Ji"], "venue": "Computer Vision and Image Understanding,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2015}, {"title": "Integrating socially assistive robotics into mental healthcare interventions: Applications and recommendations for expanded use", "author": ["S.M. Rabbitt", "A.E. Kazdin", "B. Scassellati"], "venue": "Clinical psychology review,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2015}, {"title": "Decoding children\u2019s social behavior", "author": ["J.M. Rehg"], "venue": "In Proc. of Conf. on Computer Vision and Pattern Recognition (CVPR),", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2013}, {"title": "Exploiting unrelated tasks in multi-task learning", "author": ["B. Romera-Paredes", "A. Argyriou", "N. Berthouze", "M. Pontil"], "venue": "In International Conference on Artificial Intelligence and Statistics,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2012}, {"title": "Multilinear multitask learning", "author": ["B. Romera-Paredes", "H. Aung", "N. Bianchi-Berthouze", "M. Pontil"], "venue": "In Proceedings  of the 30th International Conference on Machine Learning,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2013}, {"title": "Automatic pain intensity estimation with heteroscedastic conditional ordinal random fields", "author": ["O. Rudovic", "V. Pavlovic", "M. Pantic"], "venue": "In Advances in Visual Computing,", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2013}, {"title": "Compressive acquisition of dynamic scenes", "author": ["A.C. Sankaranarayanan", "P.K. Turaga", "R.G. Baraniuk", "R. Chellappa"], "venue": "In Proc. of European Conf. on Computer Vision (ECCV),", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2010}, {"title": "Robust facial expression recognition using local binary patterns", "author": ["C. Shan", "S. Gong", "P.W. McOwan"], "venue": "In Int. Conf. on Image Processing,", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2005}, {"title": "Dynamic facial expression recognition using a Bayesian temporal manifold model", "author": ["C. Shan", "S. Gong", "P.W. McOwan"], "venue": "In BMVC,", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2006}, {"title": "Nonlinear regulation: The piecewise linear approach", "author": ["E.D. Sontag"], "venue": "IEEE Transactions on Automatic Control,", "citeRegEx": "31", "shortCiteRegEx": "31", "year": 1981}, {"title": "Subspace-based methods for the identification of linear time-invariant systems", "author": ["M. Viberg"], "venue": null, "citeRegEx": "32", "shortCiteRegEx": "32", "year": 1995}, {"title": "Capturing complex spatiotemporal relations among facial muscles for facial expression recognition", "author": ["Z. Wang", "S. Wang", "Q. Ji"], "venue": "In Conf. on Computer Vision and Pattern Recognition (CVPR),", "citeRegEx": "33", "shortCiteRegEx": "33", "year": 2013}, {"title": "The moving pose: An efficient 3D kinematics descriptor for low-latency action recognition and detection", "author": ["M. Zanfir", "M. Leordeanu", "C. Sminchisescu"], "venue": "In Proc. of Int. Conf. on Computer Vision (ICCV),", "citeRegEx": "34", "shortCiteRegEx": "34", "year": 2013}, {"title": "A survey of affect recognition methods: Audio, visual, and spontaneous expressions", "author": ["Z. Zeng", "M. Pantic", "G.I. Roisman", "T.S. Huang"], "venue": "IEEE Trans. on Pattern Analysis and Machine Intelligence (PAMI),", "citeRegEx": "35", "shortCiteRegEx": "35", "year": 2009}, {"title": "Dynamic texture recognition using local binary patterns with an application to facial expressions", "author": ["G. Zhao", "M. Pietikainen"], "venue": "IEEE Trans. on Pattern Analysis and Machine Intelligence (PAMI),", "citeRegEx": "36", "shortCiteRegEx": "36", "year": 2007}, {"title": "Face recognition: A literature survey", "author": ["W. Zhao", "R. Chellappa", "P.J. Phillips", "A. Rosenfeld"], "venue": "Acm Computing Surveys (CSUR),", "citeRegEx": "37", "shortCiteRegEx": "37", "year": 2003}], "referenceMentions": [{"referenceID": 22, "context": "Especially in socially assistive robotics [23] and computational behavioral science [24], [19], recognition of face expressions and emotions may help either to improve interactions with a robot, or to study people\u2019s social engagement in collaborative tasks [17], [35].", "startOffset": 42, "endOffset": 46}, {"referenceID": 23, "context": "Especially in socially assistive robotics [23] and computational behavioral science [24], [19], recognition of face expressions and emotions may help either to improve interactions with a robot, or to study people\u2019s social engagement in collaborative tasks [17], [35].", "startOffset": 84, "endOffset": 88}, {"referenceID": 18, "context": "Especially in socially assistive robotics [23] and computational behavioral science [24], [19], recognition of face expressions and emotions may help either to improve interactions with a robot, or to study people\u2019s social engagement in collaborative tasks [17], [35].", "startOffset": 90, "endOffset": 94}, {"referenceID": 16, "context": "Especially in socially assistive robotics [23] and computational behavioral science [24], [19], recognition of face expressions and emotions may help either to improve interactions with a robot, or to study people\u2019s social engagement in collaborative tasks [17], [35].", "startOffset": 257, "endOffset": 261}, {"referenceID": 34, "context": "Especially in socially assistive robotics [23] and computational behavioral science [24], [19], recognition of face expressions and emotions may help either to improve interactions with a robot, or to study people\u2019s social engagement in collaborative tasks [17], [35].", "startOffset": 263, "endOffset": 267}, {"referenceID": 19, "context": "Moreover, face expression analysis could be useful in automatic pain monitoring, which in turn may help to ensure proper treatment to the patient [20].", "startOffset": 146, "endOffset": 150}, {"referenceID": 20, "context": "Recent works, such as [21],[12], [27], [2],[25], [26], [9], [10] have focused on pain/no-pain detection and pain intensity estimation.", "startOffset": 22, "endOffset": 26}, {"referenceID": 11, "context": "Recent works, such as [21],[12], [27], [2],[25], [26], [9], [10] have focused on pain/no-pain detection and pain intensity estimation.", "startOffset": 27, "endOffset": 31}, {"referenceID": 26, "context": "Recent works, such as [21],[12], [27], [2],[25], [26], [9], [10] have focused on pain/no-pain detection and pain intensity estimation.", "startOffset": 33, "endOffset": 37}, {"referenceID": 1, "context": "Recent works, such as [21],[12], [27], [2],[25], [26], [9], [10] have focused on pain/no-pain detection and pain intensity estimation.", "startOffset": 39, "endOffset": 42}, {"referenceID": 24, "context": "Recent works, such as [21],[12], [27], [2],[25], [26], [9], [10] have focused on pain/no-pain detection and pain intensity estimation.", "startOffset": 43, "endOffset": 47}, {"referenceID": 25, "context": "Recent works, such as [21],[12], [27], [2],[25], [26], [9], [10] have focused on pain/no-pain detection and pain intensity estimation.", "startOffset": 49, "endOffset": 53}, {"referenceID": 8, "context": "Recent works, such as [21],[12], [27], [2],[25], [26], [9], [10] have focused on pain/no-pain detection and pain intensity estimation.", "startOffset": 55, "endOffset": 58}, {"referenceID": 9, "context": "Recent works, such as [21],[12], [27], [2],[25], [26], [9], [10] have focused on pain/no-pain detection and pain intensity estimation.", "startOffset": 60, "endOffset": 64}, {"referenceID": 19, "context": "Whilst patient\u2019s self-report is inexpensive and does not require for special skills, it has the drawback to be subjective, and it lacks of specific timing information [20].", "startOffset": 167, "endOffset": 171}, {"referenceID": 28, "context": "Some approaches [29] use a frame-based representation; others [6], [18], [30] use also temporal information.", "startOffset": 16, "endOffset": 20}, {"referenceID": 5, "context": "Some approaches [29] use a frame-based representation; others [6], [18], [30] use also temporal information.", "startOffset": 62, "endOffset": 65}, {"referenceID": 17, "context": "Some approaches [29] use a frame-based representation; others [6], [18], [30] use also temporal information.", "startOffset": 67, "endOffset": 71}, {"referenceID": 29, "context": "Some approaches [29] use a frame-based representation; others [6], [18], [30] use also temporal information.", "startOffset": 73, "endOffset": 77}, {"referenceID": 28, "context": "Holistic representation of a sequence of FIDs, such as [29], lacks of temporal information, which instead proved to be useful [3].", "startOffset": 55, "endOffset": 59}, {"referenceID": 2, "context": "Holistic representation of a sequence of FIDs, such as [29], lacks of temporal information, which instead proved to be useful [3].", "startOffset": 126, "endOffset": 129}, {"referenceID": 5, "context": "Dynamics-based methods for emotion recognition have been proposed in [6] where a descriptor based on the movement of facial landmarks along the image sequence, and spatio-temporal appearance features are adopted.", "startOffset": 69, "endOffset": 72}, {"referenceID": 5, "context": "While [6] attempts to embed information about the dynamics at a feature representation level, works such as [18], [30] attempt to account for the temporal structure of the sequences of FIDs in the emotion model.", "startOffset": 6, "endOffset": 9}, {"referenceID": 17, "context": "While [6] attempts to embed information about the dynamics at a feature representation level, works such as [18], [30] attempt to account for the temporal structure of the sequences of FIDs in the emotion model.", "startOffset": 108, "endOffset": 112}, {"referenceID": 29, "context": "While [6] attempts to embed information about the dynamics at a feature representation level, works such as [18], [30] attempt to account for the temporal structure of the sequences of FIDs in the emotion model.", "startOffset": 114, "endOffset": 118}, {"referenceID": 12, "context": "System identification [13], [8] or compressive sensing-based techniques [28] could be used to compare different emotion instances.", "startOffset": 22, "endOffset": 26}, {"referenceID": 7, "context": "System identification [13], [8] or compressive sensing-based techniques [28] could be used to compare different emotion instances.", "startOffset": 28, "endOffset": 31}, {"referenceID": 27, "context": "System identification [13], [8] or compressive sensing-based techniques [28] could be used to compare different emotion instances.", "startOffset": 72, "endOffset": 76}, {"referenceID": 13, "context": "However, previous works [14], [16] have shown that it may be possible to avoid the burden of performing system identification by representing the output of a LTI system through the corresponding Hankel matrix.", "startOffset": 24, "endOffset": 28}, {"referenceID": 15, "context": "However, previous works [14], [16] have shown that it may be possible to avoid the burden of performing system identification by representing the output of a LTI system through the corresponding Hankel matrix.", "startOffset": 30, "endOffset": 34}, {"referenceID": 13, "context": "The use of Hankel matrices, jointly with the dissimilarity score in [14], presents advantages in terms of space and time complexity.", "startOffset": 68, "endOffset": 72}, {"referenceID": 18, "context": "The first benchmark is the extended Cohn-Kanade dataset [19], which allows us to study the validity of this kind of feature representation for emotion recognition; the second dataset is the very challenging PAINFUL dataset [20], which allows us to study the proposed feature representation for pain localization in the wild.", "startOffset": 56, "endOffset": 60}, {"referenceID": 19, "context": "The first benchmark is the extended Cohn-Kanade dataset [19], which allows us to study the validity of this kind of feature representation for emotion recognition; the second dataset is the very challenging PAINFUL dataset [20], which allows us to study the proposed feature representation for pain localization in the wild.", "startOffset": 223, "endOffset": 227}, {"referenceID": 36, "context": "There is an extensive literature on face recognition [37], [15] and on facial expression analysis [35], [7].", "startOffset": 53, "endOffset": 57}, {"referenceID": 14, "context": "There is an extensive literature on face recognition [37], [15] and on facial expression analysis [35], [7].", "startOffset": 59, "endOffset": 63}, {"referenceID": 34, "context": "There is an extensive literature on face recognition [37], [15] and on facial expression analysis [35], [7].", "startOffset": 98, "endOffset": 102}, {"referenceID": 6, "context": "There is an extensive literature on face recognition [37], [15] and on facial expression analysis [35], [7].", "startOffset": 104, "endOffset": 107}, {"referenceID": 10, "context": "In particular, [11] uses a Constrained Local Model (CLM) to obtain facial landmarks.", "startOffset": 15, "endOffset": 19}, {"referenceID": 5, "context": "In [6], a descriptor based on the movement of facial landmark points over time, jointly with spatio-temporal appearance features is extracted for each face image sequence.", "startOffset": 3, "endOffset": 6}, {"referenceID": 35, "context": "To account for temporal changes in the face appearance, Complete Local Binary Patterns from Three Orthogonal Planes (LBPTOP) [36] are used.", "startOffset": 125, "endOffset": 129}, {"referenceID": 21, "context": "In [22], restricted Boltzmann machine with local interactions (LRBM) is used to capture spatio-temporal patterns in the data.", "startOffset": 3, "endOffset": 7}, {"referenceID": 17, "context": "Since a sequence of FIDs is a time series, and may be affected by temporal warping, in [18] time-series kernel methods are used for emotional expression estimation using landmark data only.", "startOffset": 87, "endOffset": 91}, {"referenceID": 3, "context": "The work shows that emotion recognition may be done by adopting either the Dynamic Time Warping (DTW) kernel or the Global Alignment (GA) kernel [4, 5].", "startOffset": 145, "endOffset": 151}, {"referenceID": 4, "context": "The work shows that emotion recognition may be done by adopting either the Dynamic Time Warping (DTW) kernel or the Global Alignment (GA) kernel [4, 5].", "startOffset": 145, "endOffset": 151}, {"referenceID": 32, "context": "[33] propose to use Interval Temporal Bayesian Network (ITBN) to capture the spatial and temporal relations among primitive facial events.", "startOffset": 0, "endOffset": 4}, {"referenceID": 29, "context": "In [30], a Bayesian approach is used to model dynamic facial expression temporal transitions.", "startOffset": 3, "endOffset": 7}, {"referenceID": 13, "context": "Hankel matrices have already been adopted for action recognition in [14], which adopts a Hankel matrix-based bag-of-words approach, and in [16], which models an action as a sequence of Hankel matrices and uses a set of HMM trained in a discriminative way to model the switching between LTI systems.", "startOffset": 68, "endOffset": 72}, {"referenceID": 15, "context": "Hankel matrices have already been adopted for action recognition in [14], which adopts a Hankel matrix-based bag-of-words approach, and in [16], which models an action as a sequence of Hankel matrices and uses a set of HMM trained in a discriminative way to model the switching between LTI systems.", "startOffset": 139, "endOffset": 143}, {"referenceID": 30, "context": "rameters [31].", "startOffset": 9, "endOffset": 13}, {"referenceID": 31, "context": "It is well known [32] that, given a sequence of output measurements [yo\ufffd .", "startOffset": 17, "endOffset": 21}, {"referenceID": 13, "context": "As previously done in [14], [16], we normalize the Hankel matrix \ufffd H as follows:", "startOffset": 22, "endOffset": 26}, {"referenceID": 15, "context": "As previously done in [14], [16], we normalize the Hankel matrix \ufffd H as follows:", "startOffset": 28, "endOffset": 32}, {"referenceID": 13, "context": "Such score, introduced in [14], does not define a distance.", "startOffset": 26, "endOffset": 30}, {"referenceID": 15, "context": "\u2022 Sliding window-based representation: while the former representation assumes segmentation of the frame sequence into emotions, this representation could overcome this requirement by representing Y through a sequence of overlapping temporal window (similar to [16]).", "startOffset": 261, "endOffset": 265}, {"referenceID": 33, "context": "Inspired by [34], we use the NN classifier to predict the class label of each temporal window.", "startOffset": 12, "endOffset": 16}, {"referenceID": 15, "context": "Similarly to [16], a HMM is used to model the transition from a LTI system to the other.", "startOffset": 13, "endOffset": 17}, {"referenceID": 15, "context": "In contrast to [16], we use standard HMM with independently estimated state spaces.", "startOffset": 15, "endOffset": 19}, {"referenceID": 18, "context": "In this paper, to demonstrate the whole framework, we consider shape features provided by an active appearance model [19], [20].", "startOffset": 117, "endOffset": 121}, {"referenceID": 19, "context": "In this paper, to demonstrate the whole framework, we consider shape features provided by an active appearance model [19], [20].", "startOffset": 123, "endOffset": 127}, {"referenceID": 17, "context": "To demonstrate the idea behind this paper, we restrict the attention to segmented emotion recognition in frontal view as done also in previous works such as [18], [1], [22], [33].", "startOffset": 157, "endOffset": 161}, {"referenceID": 0, "context": "To demonstrate the idea behind this paper, we restrict the attention to segmented emotion recognition in frontal view as done also in previous works such as [18], [1], [22], [33].", "startOffset": 163, "endOffset": 166}, {"referenceID": 21, "context": "To demonstrate the idea behind this paper, we restrict the attention to segmented emotion recognition in frontal view as done also in previous works such as [18], [1], [22], [33].", "startOffset": 168, "endOffset": 172}, {"referenceID": 32, "context": "To demonstrate the idea behind this paper, we restrict the attention to segmented emotion recognition in frontal view as done also in previous works such as [18], [1], [22], [33].", "startOffset": 174, "endOffset": 178}, {"referenceID": 18, "context": "We have performed experiments for emotion recognition on the widely adopted Extended Cohn-Kanade dataset (CK+) [19].", "startOffset": 111, "endOffset": 115}, {"referenceID": 18, "context": "We adopted the validation protocol suggested in [19], which is leave-onesubject-out cross-validation.", "startOffset": 48, "endOffset": 52}, {"referenceID": 18, "context": "CK+ [19] 35 25 68.", "startOffset": 4, "endOffset": 8}, {"referenceID": 0, "context": "4 CLM-based [1] 70.", "startOffset": 12, "endOffset": 15}, {"referenceID": 21, "context": "4 LRBM [22] 97.", "startOffset": 7, "endOffset": 11}, {"referenceID": 32, "context": "6 ITBN [33] 91.", "startOffset": 7, "endOffset": 11}, {"referenceID": 18, "context": "We are not presenting results on the raw data via SVM because this experiment would be similar to the baseline method reported in [19] (in the lower part of the table).", "startOffset": 130, "endOffset": 134}, {"referenceID": 21, "context": "Among the sliding window-based approaches, only when adopting NN and majority vote the performance are comparable or higher than the one reported in [22].", "startOffset": 149, "endOffset": 153}, {"referenceID": 15, "context": "These experiments suggest that probably emotions can be represented as the output of just one LTI system and there may be no dynamics switching as instead may happen in human actions [16].", "startOffset": 183, "endOffset": 187}, {"referenceID": 19, "context": "To test our dynamics-based FID representation for pain detection, we use the Painful dataset [20].", "startOffset": 93, "endOffset": 97}, {"referenceID": 19, "context": "The videos were annotated on a frame-per-frame basis with the Prkachin and Solomon pain intensity (PSPI) score [20], which ranges in [0, 15] where 0 means no pain, while a value greater than 0 indicates a certain intensity of pain.", "startOffset": 111, "endOffset": 115}, {"referenceID": 14, "context": "The videos were annotated on a frame-per-frame basis with the Prkachin and Solomon pain intensity (PSPI) score [20], which ranges in [0, 15] where 0 means no pain, while a value greater than 0 indicates a certain intensity of pain.", "startOffset": 133, "endOffset": 140}], "year": 0, "abstractText": "This paper proposes a new approach to model the temporal dynamics of a sequence of facial expressions. To this purpose, a sequence of Face Image Descriptors (FID) is regarded as the output of a Linear Time Invariant (LTI) system. The temporal dynamics of such sequence of descriptors are represented by means of a Hankel matrix. The paper presents different strategies to compute dynamics-based representation of a sequence of FID, and reports classification accuracy values of the proposed representations within different standard classification frameworks. The representations have been validated in two very challenging application domains: emotion recognition and pain detection. Experiments on two publicly available benchmarks and comparison with state-of-the-art approaches demonstrate that the dynamics-based FID representation attains competitive performance when off-theshelf classification tools are adopted.", "creator": "cairo 1.8.8 (http://cairographics.org)"}}}