{"id": "1506.05012", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "16-Jun-2015", "title": "Emotion Analysis of Songs Based on Lyrical and Audio Features", "abstract": "In this paper, a method is proposed to detect the emotion of a song based on its lyrical and audio features. Lyrical features are generated by segmentation of lyrics during the process of data extraction. ANEW and WordNet knowledge is then incorporated to compute Valence and Arousal values. In addition to this, linguistic association rules are applied to ensure that the issue of ambiguity is properly addressed. Audio features are used to supplement the lyrical ones and include attributes like energy, tempo, and danceability. These features are extracted from The Echo Nest, a widely used music intelligence platform. Construction of training and test sets is done on the basis of social tags extracted from the last.fm website. The classification is done by applying feature weighting and stepwise threshold reduction on the k-Nearest Neighbors algorithm to provide fuzziness in the classification.", "histories": [["v1", "Tue, 16 Jun 2015 16:04:08 GMT  (321kb)", "http://arxiv.org/abs/1506.05012v1", "16 pages, 2 figures, 6 tables, 5 equations in International journal of Artificial Intelligence &amp; Applications (IJAIA)"]], "COMMENTS": "16 pages, 2 figures, 6 tables, 5 equations in International journal of Artificial Intelligence &amp; Applications (IJAIA)", "reviews": [], "SUBJECTS": "cs.CL cs.AI cs.SD", "authors": ["adit jamdar", "jessica abraham", "karishma khanna", "rahul dubey"], "accepted": false, "id": "1506.05012"}, "pdf": {"name": "1506.05012.pdf", "metadata": {"source": "CRF", "title": null, "authors": ["AUDIO FEATURES", "Adit Jamdar", "Jessica Abraham", "Karishma Khanna", "Rahul Dubey"], "emails": [], "sections": [{"heading": null, "text": "DOI: 10.5121 / ijaia.2015.6304 35In this essay, a method is proposed for recognizing the emotions of a song based on its lyrical and acoustic characteristics. Lyrical characteristics are generated by segmenting the lyrics during the process of data extraction. ANEW and WordNet knowledge are then incorporated into the calculation of valence and aroma values. In addition, linguistic association rules are applied to ensure that the problem of ambiguity is adequately addressed. Audio characteristics are used to supplement the lyrical ones, and include attributes such as energy, tempo and danceability. These characteristics are extracted from Echo Nest, a widely used music intelligence platform. The creation of training and test kits is based on social tags extracted from the last.fm website. Classification is performed by applying DSDSweighting and incremental threshold adjustment algorithm to the Neightening algorithm of the Natural, DSD, DSD, DSD, DSD, DSD and DSD."}, {"heading": "1. INTRODUCTION", "text": "It is said that the language of emotions and the activity of listening to music is indeed a part of everyday life. If you ask for the song you want to listen to at a particular time, you would certainly choose a song that is relevant to your mood. Reliable emotional classification systems are required to facilitate this. The task of retrieving music information is a fascinating one. However, most people are able to connect better with the words of a song than with its musical characteristics. In most cases, the words of the song are really an expression of the emotions associated with the music, while the musical aspects play an important role in deciding the emotion of a song."}, {"heading": "2. RELATED WORK", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "2.1. Music analysis based on audio features", "text": "Many researchers have focused on restoring music by focusing on genre classification, using audio functions and metadata on the song [1], as well as analyzing low characteristics (such as pitch, tempo, or rhythm) [2], while music psychologists have been interested in studying how music communicates emotions [5], [6]. They have analyzed these factors by measuring the psychological relationship between each musical factor and the emotion they evoke. [5] Other researchers have studied the influence of musical factors such as loudness and tonality on perceived emotional expression [5], [6]. They have analyzed these factors by examining the psychological relationship between individual musical factors and the emotions they evoke. Juslin and Sloboda have studied the use of acoustic factors in the communication of musical feelings by performers and listeners, and have analyzed the correlation between emotional expressions (such as anger, sadness, and happiness) and acoustic cues (such as tempo, spectrum, and articulation)."}, {"heading": "2.3. Emotion Model", "text": "Emotions can be illustrated in two ways: core affect and prototypical emotional episode [11]. Core affect, however, refers to consciously accessible elementary processes of pleasure and activation (valence and arousal), has many causes and is always present. Its structure includes bipolar dimensions (valence and arousal). Prototypical emotional episodes refer to a complex process that unfolds over time, involving incidentally related partial events (prehistory, judgment and self-categorization). Although related, core affections and prototypical emotional episodes are different concepts. Core affective feelings vary in intensity, and a person is always in a state of core effect [11]. They can be considered independent of prototypical emotional episodes, such as the feeling miserable due to infection, joy or sadness while listening to a song, excited while reading a mystery novel, etc. Since songs do not contain sub-events, we consider them as emotional effects, as they are in the core of emotions."}, {"heading": "2.3. Use of Social Tags", "text": "To broaden the pool of contextual knowledge about music for computers, researchers use a number of social tags that people apply to music. Social tags arise when many people add text comments to songs, typically to organize their personal content. These tags are then combined with tags created by other individuals to form a collective body of social tags. Last.fm is such a commercial website that allows a user to apply social tags to tracks, albums, artists, and playlists. Unfortunately, there is also a lot of irrelevant information in the day. In addition, the mood is very subjective, so songs sometimes have conflicting tags, making it difficult to construct a record using those social tags. Nevertheless, they are a source of human-generated text.] In days.14 and 17 [we] are widely used."}, {"heading": "3. FEATURES USED", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "3.1. Feature Selection", "text": "Following Russell's model, the goal is to have characteristics that can classify songs along two axes: \u2022 Positive emotions versus Negative emotions \u2022 High energy versus Low energyValence and mode are the two characteristics that are used to represent the first (X) axis. Valence is the most important lyrical trait that is used to characterize positivity or negativity of emotions. Its values come from the ANEW dictionary and represent the perception of the average person of the positivity or negativity of a word and are therefore very useful for lyrical analysis.Musical mode is a theoretical concept in music that includes notes and scale selection and plays a major role in the overall positivity or negativity of a song. Listeners perceive songs that are either happy or sad in different modes.Beats per minute, loudness, energy and excitement are the characteristics that represent the second (Y) axis Asensus is the only axial characteristic that refers to this realism and the analogy in the dispositivity."}, {"heading": "3.2. Features", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "3.2.1. Arousal and Valence", "text": "In fact, it is a kind of vanity that is able to help people, \"he said in an interview.\" It is as if it is a person who is able to change the world. \""}, {"heading": "3.2.2. BPM", "text": "BPM (Beats per Minute) or tempo is an indication of the speed and intensity of emotions associated with a song. A song with a higher BPM is generally perceived as faster and more energetic than a song with a lower BPM. This feature is used to complement the Arousal feature and is calculated using beat tracking algorithms from The Echo Nest, an online music intelligence platform."}, {"heading": "3.2.3. Mode", "text": "In the theory of Western music, the mode refers to a type of scale paired with a series of characteristic melodic behaviors. Modality refers to the placement of the eight diatonic notes of an octave and is roughly divided into two types: major and minor. The primary distinction between major mode and minor mode is the placement of the mediator or third. In major mode, the third mode consists of four semitones, while in minor mode the third only consists of three semitones [21]. Traditionally, minor mode is attributed to feelings of sadness and melancholy, while major mode is attributed to feelings of joy and happiness [22], [23]. Henvers [24] study of mood associations as well as major and minor mode support the above attributes of the mode."}, {"heading": "3.2.4. Loudness", "text": "The volume of a song is calculated in decibels by measuring the intensity of the audio wave over the duration of the song. It gives a good indication of how the average listener would perceive the song and complements the aroma function. Louder songs tend to be more energetic or aggressive, whereas quieter songs tend to use softer instruments and display calmer emotions. Unlike traditional volume measurements, Echonest analysis models the volume using a human hearing model, rather than mapping the volume directly from the recorded signal. This helps provide a feature that highly represents the perception of the average listener."}, {"heading": "3.2.5. Danceability", "text": "Danceability provides information about the structure of a song's beats. This feature is designed to show how well the song is suitable for the average dancer. In general, songs with higher danceability have smoother beats without large speed differences. Beats that are very complex or varied tend to have lower values for danceability, as it becomes more difficult for the listener to dance to these beats. Echo Nest calculates danceability on a scale from 0 to 1, using a mix of attributes such as beat strength, tempo stability and overall tempo."}, {"heading": "3.2.6. Energy", "text": "The energy of a song is the best indicator of the intensity of the emotion and complements the excitation value from the text analysis. The energy is calculated on a linear scale, with 0 being the lowest and 1 the highest value. The calculation itself is performed by the Echonest server, which decodes the audio data from the mp3 file and analyzes it for information."}, {"heading": "3.3. Feature Scaling", "text": "The k-nearest neighbors classification algorithm uses a Euclidean distance as a similarity metric (as mentioned in Section 5.1). However, if one of the characteristics has a wide range of values, the distance from that particular characteristic is determined. Therefore, the range of all characteristics should be normalized so that each characteristic contributes proportionally to the final distance. Therefore, the minmax scaling algorithm was used to scale all characteristics in the range of 0-1. The scaling of the characteristics is shown in Equation 2.Equation 2: Scaling of characteristics."}, {"heading": "4. TRAINING DATASET", "text": "Last.fm provides an API that makes it easy to query its database. Several previous papers have used last.fm to create their datasets [14], [17], [25], [26]. A list of 9 broad categories with a set of 18 representative tags was selected by examining various emotion models such as Russell's Model and Plutchik's Wheel of Emotions. Last.fm's most popular tags were also taken into account when creating the tags, which were considered sufficiently broad with a set of 18 representative tags.Last.fm was then queried to retrieve a series of top 50 songs for each of the tags. Last.fm's most popular tags were also taken into account when creating the tags."}, {"heading": "5. CLASSIFICATION", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "5.1. k-Nearest Neighbors", "text": "The idea behind the k-Nearest Neighbor algorithm is quite simple: The algorithm uses a similarity function to calculate the similarity between any two sets of data. To classify a new song, the system compares the new song with all other songs in the training data set. It calculates the similarity between the new song and the other song using the provided similarity function. The system then finds k-training songs that have the highest similarity values to the new song. These k songs are called the k closest neighbors. It then selects the most common classes in that set of neighbors and classifies the new song into these classes. The performance of this algorithm depends heavily on two factors, namely a suitable similarity function and an appropriate value for the parameter k.The algorithm used in this paper makes use of a distance measure that takes into account n different characteristics of the Euclidean distance to calculate the similarity."}, {"heading": "5.2. Cross Validation", "text": "To minimize the risk of overfitting and to generate more test data to determine accuracy, the training data set is divided into 4 different subsets to apply 4-fold cross-validation; the partitioning algorithm randomly selects songs from each category and assigns them to subsets, ensuring that a relatively similar number of songs from each category are retained in each subset; the tests have been performed 4 times, using one of the subsets as a test set and the remaining 3 subsets as training data sets; since each song can belong to several categories, the number of songs in each subset may vary; however, the representation of each category is approximately the same. Table 3 indicates the number of songs in each subset of the training dataset: Table 2: Cross-validation subset 1 subset 2 subset 4 197 218 216 164"}, {"heading": "5.3. Classification of a New Song", "text": "Figure 2: Classification of a new song The figure above describes the process of classifying a new song. We use the mp3 ID3 tags of a song to retrieve the artist and title information of the song. The text file is then obtained from the Internet via a text crawler, which searches leading text websites to find a match for the artist and the title, and downloads the associated text file for analysis. Text analysis is then performed to calculate the value and aroma attributes. This analysis follows a 5-step process: \u2022 Segmentation of the texts into verses and choral segments \u2022 Run the POS tagging over the entire text file \u2022 Apply the assignment rules for each sentence to establish relationships between verbs, adjectives, nouns and negation words \u2022 Calculate the value and aroma of each sentence using the value book characteristics of each phrase \u2022 Then use the value book with the classification characteristics of each phrase \u2022 The classification rules of each word are rooted"}, {"heading": "6. EXPERIMENTS", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "6.1. Feature Weighting", "text": "The k-Nearest Neighbors algorithm uses a Euclidean distance measure to find the closest K neighbors for a song. However, the standard Euclidean distance formula does not take into account the fact that different characteristics can have different meanings. Some characteristics, such as energy and valence, are better suited for a song than other characteristics such as tempo (beats per minute) or mode. To improve the classifier, the distance formula has been modified to include weights for each characteristic, and Equation 4.Equation 4: Feature WeightingFeature Weighting shows that the list of K neighbors returned by the algorithm can be radically changed. Extensive tests with different weights for different characteristics resulted in a combination that provides a slight increase in accuracy across all test sets, resulting in an overall increase in accuracy from 82.09% to 83.10%."}, {"heading": "6.2. Fuzzy Classification using a Threshold", "text": "In this scheme, a song could be divided into any number of classes, provided that the song has a sufficient number of neighbors in each of these classes. This sufficient number of songs is referred to as a threshold, and their value is crucial for maintaining accuracy with the KNN algorithm. Formally, the threshold means that a class must appear at least 13 times in the 30 closest neighbors of the song to be a potential class. There are 2 challenges in using a threshold: 1. Although increasing the value of the threshold increases the accuracy of the song classification, it also reduces the fuzziness of the classification. In the worst case, a song cannot be classified into one of the classes. If none of the categories contained in its extensive threshold is classified, the one must select a high level of classification."}, {"heading": "6.3. Deciding the Number of Neighbors", "text": "The number of neighbors considered (value of K) is also important. If the neighborhood is too large, it can introduce several classes that do not match the correct class of the song and also give larger classes an advantage. If the neighborhood is too small, however, there may not be an adequate representation of one or more correct classes. The value of k and the value of the threshold were chosen as 30 and 13, respectively. Table 4 compares the performance of the algorithm with and without the use of a threshold."}, {"heading": "6.4. Expanding ANEW with WordNet", "text": "In order to expand the range of words taken into account in the calculation of aroma and valence attributes, a third dictionary was created by combining the ANEW dictionary [19] with WordNet [14], [15], [17], [25], [26], [27]. WordNet is a large lexical database for the English language. It groups words that denote the same concept or meaning (synonyms) into disordered sentences, so-called synsets, and links them with each other by means of conceptual relationships (example: super-subordinate relation)."}, {"heading": "6.5. Dealing with Ambiguity and Context", "text": "One of the biggest challenges in lyrical analysis is dealing with ambiguity. However, words can have multiple meanings based on context and the way sentences are structured. Several approaches have been taken to try to gather information from context and association in order to appropriately modify the value and arousal of a word. This work uses a Parts-Of-Speech (POS) tagger provided by the Python Natural Language Toolkit (NLTK) library to highlight all words in a text file. Multiple rules have been used to determine the context of a sentence using adjectives, verbs, and contractions as modifiers. These rules are outlined in detail in Section 3.2.1.Lyrics, which repeated words or anaphoras tend to do this as a form of emphasis. Reducing the impact of repeated words would not capture how the average listener would perceive texts."}, {"heading": "7. RESULTS AND CONCLUSION", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "7.1. Observations", "text": "Accuracy can be measured by comparing the classes assigned by the algorithm with the classes derived from the last.fm social tags. However, emotions are inherently very subjective, which makes it somewhat difficult to determine exactly what constitutes a \"correct\" classification. Because the algorithm uses a fuzzy classification, it assigns several classes to each song. Some of these assigned classes may not be present in the social tags of this song, but they are correct nonetheless. Therefore, a good way to detect accuracy is to first identify incorrectly classified songs. The easiest way to do this is to search for conflicts between the assigned classes and the social tags. A single contradictory tag is sufficient to label a classification as wrong. A list of conflicting tags is in Table 5.After identifying incorrect classifications, the accuracy is calculated in a simple way. The results are presented in Table 6."}, {"heading": "7.2. Conclusion and Future Work", "text": "This combination is achieved through the use of numerous statistical experiments that have resulted in an eventual convergence of the desired weights for each trait. Experiments have achieved a striking balance between accuracy and fuzziness by adjusting the threshold value and the number of neighbors taken into account by the kNN algorithm. This classification algorithm can be used to build music recommendation systems and automated playlist generation systems based on the user's mood. This research can also be used to analyze poems as they resemble song lyrics. For further research in this area, we plan to include more spectral and stylistic features and to further address the problems arising from the ambiguity of the meaning of the word. We also intend to perform similar techniques to songs in different languages, starting with Hindi songs."}, {"heading": "ACKNOWLEDGEMENTS", "text": "The authors thank Prof. Manasi Kulkarni (VJTI) for their help and guidance in this endeavor."}], "references": [{"title": "The Biopsychology of Mood and Arousal", "author": ["Robert E. Thayer"], "venue": null, "citeRegEx": "6", "shortCiteRegEx": "6", "year": 1989}, {"title": "NamrataMahender. Text Classification and Classifiers Survey.International", "author": ["C VandanaKorde"], "venue": "Journal of Artificial Intelligence & Applications (IJAIA),", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2012}, {"title": "Genre-based Classification for Sentiment Analysis", "author": ["MaiteTabode", "Julian Brooke", "Manfred Stede"], "venue": "Proceedings of SIGDIAL 2009: the 10th Annual Meeting of the Special Interest Group in Discourse and Dialogue,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2009}, {"title": "Supervised Learning Methods for Bangla Web Document Categorization", "author": ["Ashis Kumar Mandal", "Rikta Sen"], "venue": "International Journal of Artificial Intelligence & Applications (IJAIA),", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2014}, {"title": "Automatic Mood Classification using TF*IDF based on Lyrics", "author": ["Menno Van Zaanen", "PeiterKanters"], "venue": "International Society for Music Information Retrieval,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2010}, {"title": "A comparison of Dimensional Models of Emotion: Evidence from Emotions, Prototypical Events, Autobiographical Memories, and Words", "author": ["David Rubin", "Jennifer M. Talarico"], "venue": "US National Library of Medicine, National Institute of Health, PMC,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2010}, {"title": "When Lyrics outperform audio for Music Mood Classification: A feature Analysis. 11th International Society for Music Information Retrieval", "author": ["Xiao Hu", "J. Stephen Downie"], "venue": null, "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2010}, {"title": "Lyrics based Song Emotion Detection with Affective Lexicon and Fuzzy Clustering Method. 10th International Society for Music Information Retrieval", "author": ["Yajie Hu", "Xiaoou Chen", "Deshun Yang"], "venue": null, "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2009}, {"title": "Automatic Emotion Prediction of Song Excerpts: Index, Construction, Algorithm Design, and Empirical Comparison.Journal", "author": ["Karl F. MacDorman", "Stuart Ough", "Chin-Chang Ho"], "venue": "Music Research,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2007}, {"title": "Improving Music Mood Classification using Lyrics, Audio and Social Tags.University of Illinois at Urbana-Champaign", "author": ["Xiao Hu"], "venue": null, "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2010}, {"title": "Affective Normals for English Words(ANEW): Instruction Mangual and Affective Ratings.NIMH Center for the Study of Emotion and Attention", "author": ["Margaret M. Bradley", "Peter J. Lang"], "venue": null, "citeRegEx": "19", "shortCiteRegEx": "19", "year": 1999}, {"title": "J.D.Psychological foundation of musical behavior.Springfield", "author": ["Radocy R.E", "Boyle"], "venue": "IL 1988. International Journal of Artificial Intelligence & Applications (IJAIA) Vol. 6,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2015}, {"title": "Objective Psychology of Music.New", "author": ["Lundin R.W.An"], "venue": null, "citeRegEx": "22", "shortCiteRegEx": "22", "year": 1967}, {"title": "Psychology of Music: A survey for Teacher and Musician.New", "author": ["Schoen M.The"], "venue": null, "citeRegEx": "23", "shortCiteRegEx": "23", "year": 1940}], "referenceMentions": [{"referenceID": 0, "context": "Other researchers have investigated the influence of musical factors like loudness and tonality on the perceived emotional expression [5], [6].", "startOffset": 139, "endOffset": 142}, {"referenceID": 1, "context": "A survey [7] of text classification techniques, feature selection and performance evaluation was examined which helped to review and compare algorithms and examine their tradeoffs.", "startOffset": 9, "endOffset": 12}, {"referenceID": 2, "context": "N-grams, being one of the most popular features in text classification that are used in tasks like sentiment analysis of movie reviews [8], were also considered for this purpose.", "startOffset": 135, "endOffset": 138}, {"referenceID": 3, "context": "It has been successfully employed in categorization of Web documents [9].", "startOffset": 69, "endOffset": 72}, {"referenceID": 4, "context": "They have been used for lyrics classification as well [10], however they are known to not work well with documents of short length and large vocabulary.", "startOffset": 54, "endOffset": 58}, {"referenceID": 5, "context": "Analysis of PANA model shows that it is a mere 45 degree rotation of Russell\u2019s circumplex model [12].", "startOffset": 96, "endOffset": 100}, {"referenceID": 6, "context": "Several papers make use of Russell's model, a two-dimensional model to rate emotions [3], [14], [15], [16].", "startOffset": 90, "endOffset": 94}, {"referenceID": 7, "context": "Several papers make use of Russell's model, a two-dimensional model to rate emotions [3], [14], [15], [16].", "startOffset": 96, "endOffset": 100}, {"referenceID": 8, "context": "Several papers make use of Russell's model, a two-dimensional model to rate emotions [3], [14], [15], [16].", "startOffset": 102, "endOffset": 106}, {"referenceID": 6, "context": "[14], [15], [17], [18].", "startOffset": 0, "endOffset": 4}, {"referenceID": 7, "context": "[14], [15], [17], [18].", "startOffset": 6, "endOffset": 10}, {"referenceID": 9, "context": "[14], [15], [17], [18].", "startOffset": 12, "endOffset": 16}, {"referenceID": 8, "context": "These features are based on the concept of Thayer\u2019s Model of Emotion [16].", "startOffset": 69, "endOffset": 73}, {"referenceID": 10, "context": "These values are extracted from a dictionary database consisting of 3 dictionaries, one of which is the Affective Norms for English Words [19] and another that has been obtained from [20].", "startOffset": 138, "endOffset": 142}, {"referenceID": 11, "context": "In the major mode, the third is comprised of four semitones whereas in the minor mode, the third is comprised of only three semitones [21].", "startOffset": 134, "endOffset": 138}, {"referenceID": 12, "context": "Traditionally, the minor mode has been attributed to feelings of grief and melancholy whereas the major mode has been attributed to feelings of joy and happiness [22], [23].", "startOffset": 162, "endOffset": 166}, {"referenceID": 13, "context": "Traditionally, the minor mode has been attributed to feelings of grief and melancholy whereas the major mode has been attributed to feelings of joy and happiness [22], [23].", "startOffset": 168, "endOffset": 172}, {"referenceID": 6, "context": "fm to construct their datasets [14], [17], [25], [26].", "startOffset": 31, "endOffset": 35}, {"referenceID": 9, "context": "fm to construct their datasets [14], [17], [25], [26].", "startOffset": 37, "endOffset": 41}, {"referenceID": 10, "context": "In order to expand the domain of words that are considered during the calculation of the Arousal and Valence features, a third dictionary was constructed by combining the ANEW dictionary [19] with WordNet [14], [15], [17], [25], [26], [27].", "startOffset": 187, "endOffset": 191}, {"referenceID": 6, "context": "In order to expand the domain of words that are considered during the calculation of the Arousal and Valence features, a third dictionary was constructed by combining the ANEW dictionary [19] with WordNet [14], [15], [17], [25], [26], [27].", "startOffset": 205, "endOffset": 209}, {"referenceID": 7, "context": "In order to expand the domain of words that are considered during the calculation of the Arousal and Valence features, a third dictionary was constructed by combining the ANEW dictionary [19] with WordNet [14], [15], [17], [25], [26], [27].", "startOffset": 211, "endOffset": 215}, {"referenceID": 9, "context": "In order to expand the domain of words that are considered during the calculation of the Arousal and Valence features, a third dictionary was constructed by combining the ANEW dictionary [19] with WordNet [14], [15], [17], [25], [26], [27].", "startOffset": 217, "endOffset": 221}, {"referenceID": 7, "context": "To address the issue, a new dictionary would need to be constructed by conducting a survey with sufficient test subjects, similar to the methods used to construct a Chinese version of ANEW (ANCW) in [15].", "startOffset": 199, "endOffset": 203}], "year": 2015, "abstractText": "In this paper, a method is proposed to detect the emotion of a song based on its lyrical and audio features. Lyrical features are generated by segmentation of lyrics during the process of data extraction. ANEW and WordNet knowledge is then incorporated to compute Valence and Arousal values. In addition to this, linguistic association rules are applied to ensure that the issue of ambiguity is properly addressed. Audio features are used to supplement the lyrical ones and include attributes like energy, tempo, and danceability. These features are extracted from The Echo Nest, a widely used music intelligence platform. Construction of training and test sets is done on the basis of social tags extracted from the last.fm website. The classification is done by applying feature weighting and stepwise threshold reduction on the k-Nearest Neighbors algorithm to provide fuzziness in the classification.", "creator": "PScript5.dll Version 5.2.2"}}}