{"id": "1506.05163", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "16-Jun-2015", "title": "Deep Convolutional Networks on Graph-Structured Data", "abstract": "Deep Learning's recent successes have mostly relied on Convolutional Networks, which exploit fundamental statistical properties of images, sounds and video data: the local stationarity and multi-scale compositional structure, that allows expressing long range interactions in terms of shorter, localized interactions. However, there exist other important examples, such as text documents or bioinformatic data, that may lack some or all of these strong statistical regularities.", "histories": [["v1", "Tue, 16 Jun 2015 22:31:09 GMT  (4299kb,D)", "http://arxiv.org/abs/1506.05163v1", null]], "reviews": [], "SUBJECTS": "cs.LG cs.CV cs.NE", "authors": ["mikael henaff", "joan bruna", "yann lecun"], "accepted": false, "id": "1506.05163"}, "pdf": {"name": "1506.05163.pdf", "metadata": {"source": "CRF", "title": "Deep Convolutional Networks on Graph-Structured Data", "authors": ["Mikael Henaff", "Joan Bruna"], "emails": ["mbh305@nyu.edu", "joan.bruna@berkeley.edu", "yann@cs.nyu.edu"], "sections": [{"heading": "1 Introduction", "text": "It is indeed the case that we are able to go in search of a solution."}, {"heading": "2 Related Work", "text": "There have been several studies that have examined architectures using so-called local receptive fields [6, 4, 14], mostly with image recognition applications. In particular, [4] proposes a scheme to learn how to group features based on a measure of similarity that is achieved in an unattended manner. However, it does not attempt to exploit any strategy of weight distribution. Recently, [2] proposed a generalization of the coils to graph Laplacian. By combining this pendulum property with a rule to find localized filters, the model requires only O (1) parameters per \"characteristic map.\" However, this construction requires prior knowledge of the structure and was only demonstrated by comparing it."}, {"heading": "3 Generalizing Convolutions to Graphs", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "3.1 Spectral Networks", "text": "It is not only a matter of time, but also a matter of time before there is an agreement. (uN) It is a matter of time before there is an agreement. (uN) It is a matter of time before there is an agreement. (uN) It is a matter of time before there is an agreement. (uN) It is a matter of time before there is an agreement. (Ux Ug) It is a matter of time before there is an agreement. (uN) It is a matter of time before there is an agreement. (Ux Ug), where the role of the Fourier comes in several forms. (Ux Ug) It is a matter of time until there is an agreement. (uN) It is a matter of time until there is an agreement. (Ux Ug), where there is an agreement. (Ux Ug), where the role of the Fourier exists in several forms."}, {"heading": "3.2 Pooling with Hierarchical Graph Clustering", "text": "In image and voice applications, and to reduce the complexity of the model, it is often useful to swap spatial resolution for functional resolution as the representation deepens. To this end, pooling layers compute statistics in local neighborhoods, such as average amplitude, energy, or maximum activation. The same layers can be defined in a diagram by providing the corresponding concept of neighborhood. In this work, we construct such neighborhoods on different scales using multi-resolution spectral clustering [20] and consider both average and maximum pooling layers as in conventional Convolutionary Network Architectures."}, {"heading": "4 Graph Construction", "text": "While some recognition tasks in non-Euclidean domains such as those considered in [2] or [12] have prior knowledge of the graph structure of the input data, many other applications in the real world do not have this knowledge. Therefore, it is necessary to estimate a similarity matrix W from the data before setting up the spectral network. In this paper we will consider two possible graph constructions, one unattended by measuring common characteristic statistics, and another monitored by using an initial network as a proxy for the estimate."}, {"heading": "4.1 Unsupervised Graph Estimation", "text": "In view of the data X-RL-N, where L is the number of samples and N is the number of features, the simplest approach to estimating a graph structure from the data is to take into account a distance between characters i and j given by d (i, j) = \u0442 Xi-Xj-2, Xi being the i-th column of X. While correlations typically suffice to show the intrinsic geometric structure of images [16], the effects of higher statistics in other contexts could not be negligible, especially in the presence of scarcity. In fact, in many situations, pairwise euclidal distances may suffer from unnormalized measurements. There are several strategies and variants to achieve a certain robustness, e.g. by replacing the euclidean distance with the Z-Score (whereby each feature is renormalized by its standard deviation), the \"quadratic correlation\" (calculation of the correlation of the square distance from the previous white characteristics to the \u2212 1) is used to create a softness."}, {"heading": "4.2 Supervised Graph Estimation", "text": "So it is interesting to ask for the trait similarity that best fits a particular classification task. An especially simple approach is to use a fully connected network to determine the trait similarity, given a training set with normalized 1 traits X-RL-N and labels y-RL,.. C-L, which is a fully connected network with K-layers of weights W1,... WK, with standard ReLU activations and suspensions. We then extract the first layer of traits X-RL,... C-L, which is a fully connected network with K-layers of weights W1,.. WK, with default ReLU activations and suspensions. We extract the first layer of traits W1, where M1 is the number of first traits hidden."}, {"heading": "5 Experiments", "text": "In order to measure the performance of spectral networks on real data and to investigate the effects of the graph estimation process, we conducted experiments with three sets of data in the fields of text categorization, computer biology, and computer vision. All experiments were conducted with the Torch environment of machine learning using a custom CUDA backend. As mentioned above, forming a spectral network requires an O (N2) matrix multiplication for each input and output function card in order to perform the graph Fourier transform, compared to the efficient O (N logN) Fast Fourier Transform used in classic ConvNets. We found that forming spectral networks with a large number of feature maps is very time consuming and therefore decided to experiment primarily with architectures with fewer features and build a smaller pool that is costly."}, {"heading": "5.1 Reuters", "text": "We used the Reuters data set described in [18], which consists of training and test sets, each containing 201,369 documents from 50 mutually exclusive classes. Each document is presented as a logged bag of words for 2000 common non-stop words. As a starting point, we used the fully connected network of [18] with two hidden layers consisting of 2000 and 1000 hidden units using dropout. We selected hyperparameters by conducting initial experiments on a validation set consisting of Ottenth of training data. Specifically, we set the number of weights studied to k = 60, learning rate to 0.01, and used maximum pooling instead of average pooling. We also found that the use of AdaGrad [5] accelerated training. All architectures were then trained with the same hyperparameters. As the experiments were compatible, we were expensive, we did not train all models to full fitness."}, {"heading": "5.2 Merck Molecular Activity Challenge", "text": "For our experiments, we used the DPP4 dataset, which has 8193 samples and 2796 features. We chose this dataset because it was one of the more difficult and of relatively low dimensionality, which made the spectral networks tractable. As a basic architecture, we used the network of [10], which has 4 hidden layers and is regulated by drop-out and weight decay. We used the same hyperparameter settings and data normalizations recommended in the papers. As before, we used a tenth of the training set to optimize hyperparameters of the network. For this task, we found that k = 40 subsampled weights worked best, and that the average pooling performance was better than the maximum pooling method. As the task is to predict a continuous variable, all networks were trained by minimizing the hidden losses between the two mediated networks."}, {"heading": "5.3 ImageNet", "text": "In the experiments above our graph construction, we relied on estimates from the data. To measure the influence of graph construction as compared to filter learning in the graph frequency domain, we performed the same experiments with the ImageNet dataset for which the graph is already known, namely the 2-D grating. Thus, the spectral network was a revolutionary network whose weights in the frequency domain were defined by means of frequency smoothing, rather than using compactly supported filters. Training was performed exactly as in Figure 1, except that linear transformation was a Fast Fourier transformation.Our network consisted of 4 convolution / ReLU / max pooling layers with 48, 128, 256 and 256 feature maps, followed by 3 fully connected layers, each with 4096 hidden units, each regulated by a suspender. We tracted two versions of the network: a classical network weight and the speculative weight in the domain."}, {"heading": "6 Discussion", "text": "In this context, it should be noted that the generalisations of high-dimensional, unstructured data in our model satisfy both stability and component diversity, but the model does not increase learning complexity, but rather promotes general networking. Furthermore, in contexts where the presence of input sizes is known, it is necessary to estimate the similarities in such a way that the model is more deeply involved in learning processes than in contexts where the characteristics of input sizes are known (e.g. word representations)."}], "references": [{"title": "Laplacian eigenmaps and spectral techniques for embedding and clustering", "author": ["Mikhail Belkin", "Partha Niyogi"], "venue": "In NIPS,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2001}, {"title": "Spectral networks and deep locally connected networks on graphs", "author": ["Joan Bruna", "Wojciech Zaremba", "Arthur Szlam", "Yann LeCun"], "venue": "In Proceedings of the 2nd International Conference on Learning Representations,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2013}, {"title": "Unsupervised deep haar scattering on graphs", "author": ["Xu Chen", "Xiuyuan Cheng", "St\u00e9phane Mallat"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2014}, {"title": "Selecting receptive fields in deep networks", "author": ["Adam Coates", "Andrew Y Ng"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2011}, {"title": "Adaptive subgradient methods for online learning and stochastic optimization", "author": ["John Duchi", "Elad Hazan", "Yoram Singer"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2011}, {"title": "Emergence of complex-like cells in a temporal product network with local receptive fields", "author": ["Karol Gregor", "Yann LeCun"], "venue": "arXiv preprint arXiv:1006.0448,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2010}, {"title": "Deep neural networks for acoustic modeling in speech recognition", "author": ["Geoffrey Hinton", "Li Deng", "Dong Yu", "George Dahl", "Abdel rahman Mohamed", "Navdeep Jaitly", "Andrew Senior", "Vincent Vanhoucke", "Patrick Nguyen", "Tara Sainath", "Brian Kingsbury"], "venue": "Signal Processing Magazine,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2012}, {"title": "Imagenet classification with deep convolutional neural networks", "author": ["Alex Krizhevsky", "Ilya Sutskever", "Geoffrey E. Hinton"], "venue": "Advances in Neural Information Processing Systems", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2012}, {"title": "Deep neural networks as a method for quantitative structure-activity relationships", "author": ["Junshui Ma", "Robert P. Sheridan", "Andy Liaw", "George E. Dahl", "Vladimir Svetnik"], "venue": "Journal of Chemical Information and Modeling,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2015}, {"title": "A wavelet tour of signal processing", "author": ["St\u00e9phane Mallat"], "venue": "Academic press,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 1999}, {"title": "Shapenet: Convolutional neural networks on non-euclidean manifolds", "author": ["Jonathan Masci", "Davide Boscaini", "Michael M. Bronstein", "Pierre Vandergheynst"], "venue": "CoRR, abs/1501.06297,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2015}, {"title": "Fast training of convolutional networks through ffts", "author": ["Michael Mathieu", "Mikael Henaff", "Yann LeCun"], "venue": "arXiv preprint arXiv:1312.5851,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2013}, {"title": "Tiled convolutional neural networks", "author": ["Jiquan Ngiam", "Zhenghao Chen", "Daniel Chia", "Pang W Koh", "Quoc V Le", "Andrew Y Ng"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2010}, {"title": "High-dimensional ising model selection using `1-regularized logistic regression", "author": ["Pradeep Ravikumar", "Martin J Wainwright", "John D Lafferty"], "venue": "The Annals of Statistics,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2010}, {"title": "Learning the 2-d topology of images", "author": ["Nicolas L Roux", "Yoshua Bengio", "Pascal Lamblin", "Marc Joliveau", "Bal\u00e1zs K\u00e9gl"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2008}, {"title": "Dropout: A simple way to prevent neural networks from overfitting", "author": ["Nitish Srivastava", "Geoffrey Hinton", "Alex Krizhevsky", "Ilya Sutskever", "Ruslan Salakhutdinov"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 1929}, {"title": "Dropout: A simple way to prevent neural networks from overfitting", "author": ["Nitish Srivastava", "Geoffrey Hinton", "Alex Krizhevsky", "Ilya Sutskever", "Ruslan Salakhutdinov"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2014}, {"title": "Fast convolutional nets with fbfft: A GPU performance evaluation", "author": ["Nicolas Vasilache", "Jeff Johnson", "Micha\u00ebl Mathieu", "Soumith Chintala", "Serkan Piantino", "Yann LeCun"], "venue": "CoRR, abs/1412.7580,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2014}, {"title": "A tutorial on spectral clustering", "author": ["Ulrike Von Luxburg"], "venue": "Statistics and computing,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2007}, {"title": "Self-tuning spectral clustering", "author": ["Lihi Zelnik-Manor", "Pietro Perona"], "venue": "In Advances in neural information processing systems,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2004}], "referenceMentions": [{"referenceID": 7, "context": "These properties are exploited efficiently by ConvNets [8, 7], which are designed to extract local features that are shared across the signal domain.", "startOffset": 55, "endOffset": 61}, {"referenceID": 6, "context": "These properties are exploited efficiently by ConvNets [8, 7], which are designed to extract local features that are shared across the signal domain.", "startOffset": 55, "endOffset": 61}, {"referenceID": 15, "context": "For such type of data of dimension N , deep learning strategies are reduced to learning with fully-connected layers, which have O(N) parameters, and regularization is carried out via weight decay and dropout [17].", "startOffset": 208, "endOffset": 212}, {"referenceID": 1, "context": "When the graph structure of the input is known, [2] introduced a model to generalize ConvNets using low learning complexity similar to that of a ConvNet, and which was demonstrated on simple lowdimensional graphs.", "startOffset": 48, "endOffset": 51}, {"referenceID": 1, "context": "\u2022 We extend the ideas from [2] to large-scale classification problems, specifically Imagenet Object Recognition, text categorization and bioinformatics.", "startOffset": 27, "endOffset": 30}, {"referenceID": 5, "context": "There have been several works which have explored architectures using the so-called local receptive fields [6, 4, 14], mostly with applications to image recognition.", "startOffset": 107, "endOffset": 117}, {"referenceID": 3, "context": "There have been several works which have explored architectures using the so-called local receptive fields [6, 4, 14], mostly with applications to image recognition.", "startOffset": 107, "endOffset": 117}, {"referenceID": 12, "context": "There have been several works which have explored architectures using the so-called local receptive fields [6, 4, 14], mostly with applications to image recognition.", "startOffset": 107, "endOffset": 117}, {"referenceID": 3, "context": "In particular, [4] proposes a scheme to learn how to group together features based upon a measure of similarity that is obtained in an unsupervised fashion.", "startOffset": 15, "endOffset": 18}, {"referenceID": 1, "context": "Recently, [2] proposed a generalization of convolutions to graphs via the Graph Laplacian.", "startOffset": 10, "endOffset": 13}, {"referenceID": 10, "context": "More recently, [12] introduced Shapenet, another generalization of convolutions on non-Euclidean domains based on geodesic polar coordinates, which was successfully applied to shape analysis, and allows comparison across different manifolds.", "startOffset": 15, "endOffset": 19}, {"referenceID": 13, "context": "For instance, [15] studies the estimation of the graph from a statistical point of view, through the identification of a certain graphical model using `1-penalized logistic regression.", "startOffset": 14, "endOffset": 18}, {"referenceID": 2, "context": "Also, [3] considers the problem of learning a deep architecture through a series of Haar contractions, which are learnt using an unsupervised pairing criteria over the features.", "startOffset": 6, "endOffset": 9}, {"referenceID": 1, "context": "Our work builds upon [2] which introduced spectral networks.", "startOffset": 21, "endOffset": 24}, {"referenceID": 0, "context": "There are several ways of computing the graph Laplacian L [1].", "startOffset": 58, "endOffset": 61}, {"referenceID": 9, "context": "We then recover a classical convolution operator by noting that convolutions are by definition linear operators that diagonalize in the Fourier domain (also known as the Convolution Theorem [11]).", "startOffset": 190, "endOffset": 194}, {"referenceID": 1, "context": "In [2] it was suggested to use the same principle in a general graph, by considering a smoothing kernel K \u2208 RN\u00d7N0 , such as splines, and searching for spectral multipliers of the form wg = Kw\u0303g .", "startOffset": 3, "endOffset": 6}, {"referenceID": 18, "context": "In this work, we construct such neighborhoods at different scales using multi-resolution spectral clustering [20], and consider both average and max-pooling as in standard convolutional network architectures.", "startOffset": 109, "endOffset": 113}, {"referenceID": 1, "context": "Whereas some recognition tasks in non-Euclidean domains, such as those considered in [2] or [12], might have a prior knowledge of the graph structure of the input data, many other real-world applications do not have such knowledge.", "startOffset": 85, "endOffset": 88}, {"referenceID": 10, "context": "Whereas some recognition tasks in non-Euclidean domains, such as those considered in [2] or [12], might have a prior knowledge of the graph structure of the input data, many other real-world applications do not have such knowledge.", "startOffset": 92, "endOffset": 96}, {"referenceID": 14, "context": "While correlations are typically sufficient to reveal the intrinsic geometrical structure of images [16], the effects of higher-order statistics might be non-negligible in other contexts, especially in presence of sparsity.", "startOffset": 100, "endOffset": 104}, {"referenceID": 0, "context": "This distance is then used to build a Gaussian diffusion Kernel [1]", "startOffset": 64, "endOffset": 67}, {"referenceID": 19, "context": "In our experiments, we also consider the variant of self-tuning diffusion kernel [21]", "startOffset": 81, "endOffset": 85}, {"referenceID": 16, "context": "We used the Reuters dataset described in [18], which consists of training and test sets each containing 201,369 documents from 50 mutually exclusive classes.", "startOffset": 41, "endOffset": 45}, {"referenceID": 16, "context": "As a baseline we used the fullyconnected network of [18] with two hidden layers consisting of 2000 and 1000 hidden units regularized with dropout.", "startOffset": 52, "endOffset": 56}, {"referenceID": 4, "context": "We also found that using AdaGrad [5] made training faster.", "startOffset": 33, "endOffset": 36}, {"referenceID": 8, "context": "As a baseline architecture, we used the network of [10] which has 4 hidden layers and is regularized using dropout and weight decay.", "startOffset": 51, "endOffset": 55}, {"referenceID": 8, "context": "Following [10], we measured performance by computing the squared correlation between predictions and targets.", "startOffset": 10, "endOffset": 14}, {"referenceID": 11, "context": "Fourier implementations of Convnets [13, 19] bring the complexity to O(N logN) thanks again to the specific symmetries of the grid.", "startOffset": 36, "endOffset": 44}, {"referenceID": 17, "context": "Fourier implementations of Convnets [13, 19] bring the complexity to O(N logN) thanks again to the specific symmetries of the grid.", "startOffset": 36, "endOffset": 44}], "year": 2015, "abstractText": "Deep Learning\u2019s recent successes have mostly relied on Convolutional Networks, which exploit fundamental statistical properties of images, sounds and video data: the local stationarity and multi-scale compositional structure, that allows expressing long range interactions in terms of shorter, localized interactions. However, there exist other important examples, such as text documents or bioinformatic data, that may lack some or all of these strong statistical regularities. In this paper we consider the general question of how to construct deep architectures with small learning complexity on general non-Euclidean domains, which are typically unknown and need to be estimated from the data. In particular, we develop an extension of Spectral Networks which incorporates a Graph Estimation procedure, that we test on large-scale classification problems, matching or improving over Dropout Networks with far less parameters to estimate.", "creator": "LaTeX with hyperref package"}}}