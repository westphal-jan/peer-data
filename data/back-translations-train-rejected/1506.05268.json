{"id": "1506.05268", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "17-Jun-2015", "title": "Deep Denoising Auto-encoder for Statistical Speech Synthesis", "abstract": "This paper proposes a deep denoising auto-encoder technique to extract better acoustic features for speech synthesis. The technique allows us to automatically extract low-dimensional features from high dimensional spectral features in a non-linear, data-driven, unsupervised way. We compared the new stochastic feature extractor with conventional mel-cepstral analysis in analysis-by-synthesis and text-to-speech experiments. Our results confirm that the proposed method increases the quality of synthetic speech in both experiments.", "histories": [["v1", "Wed, 17 Jun 2015 10:17:59 GMT  (5330kb,D)", "http://arxiv.org/abs/1506.05268v1", null]], "reviews": [], "SUBJECTS": "cs.SD cs.LG", "authors": ["zhenzhou wu", "shinji takaki", "junichi yamagishi"], "accepted": false, "id": "1506.05268"}, "pdf": {"name": "1506.05268.pdf", "metadata": {"source": "CRF", "title": "DEEP DENOISING AUTO-ENCODER FOR STATISTICAL SPEECH SYNTHESIS", "authors": ["Zhenzhou Wu", "Shinji Takaki", "Junichi Yamagishi"], "emails": [], "sections": [{"heading": null, "text": "This year it is more than ever before in the history of the city."}, {"heading": "3.1. Basic Auto-encoder", "text": "Auto-encoder is an artificial neural network that is generally used to learn a compressed and distributed representation of a dataset. It consists of the encoder and the decoder. The encoder forms an Inputar Xiv: 150 6.05 268v 1 [cs.S D] 17 Jun 2015 Vector x to a hidden representation y as follows: y = f\u03b8 (x) = s (Wx + b), (1) where \u03b8 = {W, b}. W and b represent a m \u00b7 n weight matrix or a bias vector of dimensionality m, where n is the dimension of x. The function s is a nonlinear transformation on the linear mapping Wx + b. Frequently s is a sigmoid, a tanh and a relay function. y, the output of the encoder x x, is then mapped to z, the output of the decoder z."}, {"heading": "3.2. Denoising Auto-encoder", "text": "The denossing auto encoder is a variant of the basic auto encoder. It is reported that the denossing auto encoder can extract properties more robustly than the basic auto encoder [25]. In the denossing auto encoder, the original data x is initially corrupted to x before being mapped by an encoder to a higher representation f\u03b8 (x). The decoder then maps the higher representation to the output z to reconstruct the original x. The denossing auto encoder is designed so that the reconstructed z comes as close as possible to the original data x. Note that the denossing auto encoder is only used during training to reconstruct the original x from the corrupt x."}, {"heading": "3.3. Deep Auto-encoder", "text": "Auto-encoder or denoisng auto-encoder can be deepened by stacking multiple layers of encoders and decoders to form a deep architecture. [26] Shows that deeper architecture produces better high-level characteristics compared to flat architecture up to 4 encoding layers and 4 decoding layers. It is commonly used to build a deep auto-encoder lecture. In pre-training, the number of layers in a deep auto-encoder increases twice as much as in a deep neural network (DNN) when stacking each unit. It is reported that fine-tuning with back propagation by a deep auto-encoder is ineffective as the gradients disappear in the lower layers [27]. To address this problem, we limit the decoding weight as the encoding weight transposition to [24], i.e. W \u2032 = WT, where WT means transposing W."}, {"heading": "4.1. Greedy Layer-wise Pre-training", "text": "Each layer of a deep auto encoder can be greedily pre-trained to minimize the reconstruction loss L (x, z) of the data locally. Figure 1 shows a method of constructing a deep auto encoder by means of pre-training. In pre-training, a 1-layer auto encoder is trained, and the encoding output of the locally trained layer is used as input for the next layer. This layer-by-layer training repeats until the desired layer size is reached. Coding, decoding and loss functions of each layer are represented as follows: Layer 1: y1 = fW1, b1 (x), z1 = gW \u2032 1, b \u2032 1 (y1), L (x, z1) = x \u2212 z1 | 2, Layer (k > 1): yk = fWk, bk (yk \u2212 1), zk \u2032 k (yk), L (yk \u2212 1), zk \u2212 layer are all of the incoding layer (during the school layer)."}, {"heading": "4.2. Fine-tuning", "text": "The purpose of fine-tuning is to minimize the reconstruction error L (x, z) over the entire dataset and a model architecture by means of error propagation [28]. We use the mean square error (MSE) for the loss function of a deep auto encoder and it is represented as follows: E = N \u2211 i = 1 | x (i) \u2212 z (i) | 2, (5) whereN is the total number of training examples. The partial derivatives w.r.t weight w (l) j is the fan input to neuron j in layer l, and v (l) j (l) v (l) v (l) v (l) v (l) v (l), j (l) \u2212 1, where o is the output from l \u2212 1."}, {"heading": "4.3. Corrupted data", "text": "This technique set the values of the training data in different dimensions independently and randomly after a Bernoulli distribution to zero. Figure 2 shows an example of original and masked spectra. In this figure, black dots indicate masked regions.5. EVALUATIONThis section shows experimental results. We evaluated the proposed auto-encoder method in the context of analysis-by-synthesis condition and text-to-speech conditions.In the text-to-speech experiments, the synthetic voices using the proposed acoustic features were modeled with two state-of-the-art speech synthesis systems: HMM and DNN."}, {"heading": "5.1. Dataset", "text": "The dataset we use consists of 4569 short audio waveforms uttered by a professional English speaker, and each waveform is about 5 seconds long. For each waveform, we first extract its frequency spectrum using a STRAIGHT vocoder with 2049 FFT points. Then, we extract the low-dimensional characteristic from each 2049 weak STRAIGHT spectrum using an auto-encoder. All data was sampled at 48 kHz. To compare the proposed method, we extracted mel-cepstral coefficients that use the same dimensions as the auto-encoder. All other acoustic characteristics such as log F0 and 25 periodicity band energies are the same for all systems."}, {"heading": "5.2. Configurations of the deep denoising auto-encoder", "text": "Figure 3 shows that the error decreases with more hidden layers, and that a deeper auto encoder is better than a flat auto encoder with the same bottleneck. For the results in the rest of the paper, we use the auto encoder architecture as 2049-500-180-120 to generate the 120 dimming acoustic characteristics, tanh units for all layers and the inputs are a 2049 dimming bark scaled frequency distorted spectrum that is pre-processed with global contrast normalization, the hyperparameters used for layer-by-layer pre-training are randomly searched, and the values that provide the best results are selected. Table 1 shows the hyperparameters for the auto encoders used in the experiments."}, {"heading": "5.3. Analysis-by-synthesis experimental results", "text": "First, we report on the experimental results of the synthesis analysis. For this evaluation, we divided the above database into three subsets, i.e. training, validation and test. The subset of training was used as training data for the setup of the auto-encoder, but the subset of validation was used as a stop criterion during training to prevent overfitting, and the subset of test was used to measure log spectral distortion and hearing test.Figure 4 shows the original and reconstructed spectra using any technique (mel-cepstral analysis, deep auto-encoder, deeper denosifying auto-encoder).We can clearly see that the deep auto-encoder reconstructs high-frequency parts more accurately than mel-cepstral analysis. Figure 5 shows logged spectral distortions between the original spectra and reconstructed spectra calculated on the test subset."}, {"heading": "5.4. Text-to-speech experimental results", "text": "This year it has come to the point where there is only one person left who is able to establish himself in the region, \"he said in an interview with the German Press Agency.\" I am very happy that it has come to this point, \"he said."}], "references": [{"title": "Statistical parametric speech synthesis,", "author": ["H. Zen", "K. Tokuda", "A.W. Black"], "venue": "Speech Communication,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2009}, {"title": "Speaker interpolation in HMM-based speech synthesis system,", "author": ["T. Yoshimura", "K. Tokuda", "T. Masuko", "T. Kobayashi", "T. Kitamura"], "venue": "Proceedings of Eurospeech", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 1997}, {"title": "Simultaneous modeling of spectrum, pitch and duration in HMM-based speech synthesis,", "author": ["T. Yoshimura", "K. Tokuda", "T. Masuko", "T. Kobayashi", "T. Kitamura"], "venue": "Proceedings of Eurospeech", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 1999}, {"title": "Constructing emotional speech synthesizers with limited speech database,", "author": ["R. Tsuzuki", "H. Zen", "K. Tokuda", "T. Kitamura", "M. Bulut", "S. Narayanan"], "venue": "Proceedings of ICSLP,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2004}, {"title": "Acoustic modeling of speaking styles and emotional expressions in HMM-based speech synthesis,", "author": ["J. Yamagishi", "K. Onishi", "T. Masuko", "T. Kobayashi"], "venue": "IEICE Transactions on Information & Systems, vol. E88-D,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2005}, {"title": "DNN-based stochastic postfilter for HMM-based speech synthesis,", "author": ["L.-H. Chen", "T. Raitio", "C. Valentini-Botinhao", "J. Yamagishi", "Z.-H. Ling"], "venue": "Proceedings of Interspeech,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 1954}, {"title": "Cheveigne, \u201cRestructuring speech representations using a pitch-adaptive timefrequency smoothing and an instantaneous-frequency-based F0 extraction: Possible role of a repetitive structure in sounds,", "author": ["H. Kawahara", "I. Masuda-Katsuse"], "venue": "Speech Communication,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 1999}, {"title": "Statistical parametric speech synthesis using deep neural networks,", "author": ["H. Zen", "A. Senior", "M. Schuster"], "venue": "Proceedings of ICASSP,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2013}, {"title": "Modeling spectral envelopes using restricted Boltzmann machines and deep belief networks for statistical parametric speech synthesis,", "author": ["Z.-H. Ling", "L. Deng", "D. Yu"], "venue": "Audio, Speech, and Language Processing, IEEE Transactions on,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2013}, {"title": "TTS synthesis with bidirectional LSTM based recurrent neural networks,", "author": ["Y. Fan", "Y. Qian", "F. Xie", "F.K. Soong"], "venue": "Proceedings of Interspeech,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 1964}, {"title": "Prosody contour prediction with long short-term memory, bidirectional, deep recurrent neural networks,", "author": ["R. Fernandez", "A. Rendel", "B. Ramabhadran", "R. Hoory"], "venue": "Proceedings of Interspeech,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2014}, {"title": "Autoencoder bottleneck features using deep belief networks,", "author": ["T.N. Sainath", "B. Kingsbury", "B. Ramabhadran"], "venue": "Proceedings of ICASSP,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2012}, {"title": "Extracting deep bottleneck features using stacked auto-encoders,", "author": ["J. Gehring", "Y. Miao", "F. Metze", "A. Waibel"], "venue": "Proceedings of ICASSP,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2013}, {"title": "Recurrent neural networks for noise reduction in robust ASR,", "author": ["A.L. Maas", "Q.V. Le", "T.M. O?Neil", "O. Vinyals", "P. Nguyen", "Andrew Ng"], "venue": "Proceedings of Interspeech,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2012}, {"title": "Reverberant speech recognition based on denoising autoencoder,", "author": ["T. Ishii", "H. Komiyama", "T. Shinozaki", "Y. Horiuchi", "S Kuroiwa"], "venue": "Proceedings of Interspeech,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2013}, {"title": "Speech feature denoising and dereverberation via deep autoencoders for noisy reverberant speech recognition,", "author": ["X. Feng", "Y. Zhang", "J. Glass"], "venue": "Proceedings of ICASSP,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2014}, {"title": "Binary coding of speech spectrograms using a deep auto-encoder,", "author": ["L. Deng", "M. Seltzer", "D. Yu", "A. Acero", "A. Mohamed", "G. Hinton"], "venue": "Proceedings of Interspeech,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2010}, {"title": "Speech enhancement based on deep denoising autoencoder,", "author": ["X. Lu", "Y. Tsao", "S. Matsuda", "C. Hori"], "venue": "Proceedings of Interspeech,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2013}, {"title": "Heteroscedastic discriminant analysis and reduced rank hmms for improved speech recognition,", "author": ["N. Kumar", "A.G. Andreou"], "venue": "Speech Communication,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 1998}, {"title": "Probabilistic linear discriminant analysis for inferences about identity,", "author": ["S.J.D. Prince", "J.H. Elder"], "venue": "ICCV, pp", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2007}, {"title": "Bayesian speaker verification with heavy-tailed priors,", "author": ["P. Kenny"], "venue": "Odyssey, p", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2010}, {"title": "Probabilistic linear discriminant analysis for acoustic modelling,", "author": ["L. Lu", "S. Renals"], "venue": "Signal Processing Letters,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2014}, {"title": "Reducing the dimensionality of data with neural networks,", "author": ["G.E. Hinton", "R. Salakhutdinov"], "venue": "Science 28,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2006}, {"title": "Extracting and composing robust features with denoising autoencoders,", "author": ["P. Vincent", "H. Larochelle", "Y. Bengio", "P. Manzagol"], "venue": "ICML, pp", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2008}, {"title": "Improved bottleneck features using pretrained deep neural networks,", "author": ["D. Yu", "M. Seltzer"], "venue": "Proceedings of Interspeech, pp", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2011}, {"title": "Gradient flow in recurrent nets: the difficulty of learning longterm dependencies,", "author": ["S. Hochreiter", "Y. Bengio", "P. Frasconi", "J. Schmidhuber"], "venue": null, "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2001}, {"title": "On the importance of initialization and momentum in deep learning,", "author": ["I. Sutskever", "J. Martens", "George E. Dahl", "Geoffrey E. Hinton"], "venue": null, "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2013}, {"title": "On generating combilex pronunciations via morphological analysis,", "author": ["K. Richmond", "R. Clark", "S. Fitt"], "venue": "Proceedings of Interspeech, pp. 1974?\u20131977,", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2010}, {"title": "Random search for hyperparameter optimization,", "author": ["J. Bergstra", "Y. Bengio"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2012}, {"title": "Speech feature denoising and dereverberation via deep autoencoders for noisy reverberant speech recognition,", "author": ["X. Feng", "Y. Zhang", "J. Glass"], "venue": "Proceedings of ICASSP,", "citeRegEx": "32", "shortCiteRegEx": "32", "year": 2014}], "referenceMentions": [{"referenceID": 0, "context": "Current statistical parametric speech synthesis typically uses hidden Markov models (HMMs) to represent probability densities of speech trajectories given text [1].", "startOffset": 160, "endOffset": 163}, {"referenceID": 1, "context": "It also offers interesting advantages in terms of flexibility and compact footprint [2, 3, 4, 5].", "startOffset": 84, "endOffset": 96}, {"referenceID": 2, "context": "It also offers interesting advantages in terms of flexibility and compact footprint [2, 3, 4, 5].", "startOffset": 84, "endOffset": 96}, {"referenceID": 3, "context": "It also offers interesting advantages in terms of flexibility and compact footprint [2, 3, 4, 5].", "startOffset": 84, "endOffset": 96}, {"referenceID": 4, "context": "It also offers interesting advantages in terms of flexibility and compact footprint [2, 3, 4, 5].", "startOffset": 84, "endOffset": 96}, {"referenceID": 5, "context": "A stochastic postfilter approach [6] proposes to use a deep neural network (DNN) to model the conditional probability of the spectral differences between natural and synthetic speech.", "startOffset": 33, "endOffset": 36}, {"referenceID": 5, "context": "The approach is able to reconstruct the spectral fine structure lost during modeling and has achieved significantly quality improvement for synthetic speech [6].", "startOffset": 157, "endOffset": 160}, {"referenceID": 6, "context": "More specifically we propose to use a deep denoising auto-encoder technique as a non-linear robust feature extractor for speech synthesis and apply it to high-dimensional spectral features obtained from STRAIGHT vocoder [7].", "startOffset": 220, "endOffset": 223}, {"referenceID": 7, "context": "For instance [8] uses DNN to learn the relationship between input texts and the extract features instead of decision tree-based state tying.", "startOffset": 13, "endOffset": 16}, {"referenceID": 8, "context": "Restricted Boltzmann machines or deep belief networks have been used for modelling output probabilities of HMM states instead of GMMs [9].", "startOffset": 134, "endOffset": 137}, {"referenceID": 9, "context": "Recurrent neural network or long-short term memory was used for prosody modelling [10] or acoustic trajectory modelling [11].", "startOffset": 82, "endOffset": 86}, {"referenceID": 10, "context": "Recurrent neural network or long-short term memory was used for prosody modelling [10] or acoustic trajectory modelling [11].", "startOffset": 120, "endOffset": 124}, {"referenceID": 11, "context": "To the best of our knowledge, this is the first work to use deep denoising auto-encoder for speech synthesis, but, deep auto-encoder based bottleneck features are used by several groups for ASR [12, 13] and deep denoising auto-encoder is also verified for noise-robust ASR [14] or reverberant ASR tasks [15, 16].", "startOffset": 194, "endOffset": 202}, {"referenceID": 12, "context": "To the best of our knowledge, this is the first work to use deep denoising auto-encoder for speech synthesis, but, deep auto-encoder based bottleneck features are used by several groups for ASR [12, 13] and deep denoising auto-encoder is also verified for noise-robust ASR [14] or reverberant ASR tasks [15, 16].", "startOffset": 194, "endOffset": 202}, {"referenceID": 13, "context": "To the best of our knowledge, this is the first work to use deep denoising auto-encoder for speech synthesis, but, deep auto-encoder based bottleneck features are used by several groups for ASR [12, 13] and deep denoising auto-encoder is also verified for noise-robust ASR [14] or reverberant ASR tasks [15, 16].", "startOffset": 273, "endOffset": 277}, {"referenceID": 14, "context": "To the best of our knowledge, this is the first work to use deep denoising auto-encoder for speech synthesis, but, deep auto-encoder based bottleneck features are used by several groups for ASR [12, 13] and deep denoising auto-encoder is also verified for noise-robust ASR [14] or reverberant ASR tasks [15, 16].", "startOffset": 303, "endOffset": 311}, {"referenceID": 15, "context": "To the best of our knowledge, this is the first work to use deep denoising auto-encoder for speech synthesis, but, deep auto-encoder based bottleneck features are used by several groups for ASR [12, 13] and deep denoising auto-encoder is also verified for noise-robust ASR [14] or reverberant ASR tasks [15, 16].", "startOffset": 303, "endOffset": 311}, {"referenceID": 16, "context": "Techniques that are closely related to this paper are a spectral binary coding approach using deep auto-encoder proposed by Deng et al [17] and a speech enhancement approach using deep denoising auto-encoder where they try to reconstruct clean spectrum from noisy spectrum [18].", "startOffset": 135, "endOffset": 139}, {"referenceID": 17, "context": "Techniques that are closely related to this paper are a spectral binary coding approach using deep auto-encoder proposed by Deng et al [17] and a speech enhancement approach using deep denoising auto-encoder where they try to reconstruct clean spectrum from noisy spectrum [18].", "startOffset": 273, "endOffset": 277}, {"referenceID": 18, "context": "The approach proposed here is also related to heteroscedastic linear discriminant analysis (HLDA) [19, 20] and probabilistic linear discriminant analysis (PLDA) [21, 22, 23].", "startOffset": 98, "endOffset": 106}, {"referenceID": 19, "context": "The approach proposed here is also related to heteroscedastic linear discriminant analysis (HLDA) [19, 20] and probabilistic linear discriminant analysis (PLDA) [21, 22, 23].", "startOffset": 161, "endOffset": 173}, {"referenceID": 20, "context": "The approach proposed here is also related to heteroscedastic linear discriminant analysis (HLDA) [19, 20] and probabilistic linear discriminant analysis (PLDA) [21, 22, 23].", "startOffset": 161, "endOffset": 173}, {"referenceID": 21, "context": "The approach proposed here is also related to heteroscedastic linear discriminant analysis (HLDA) [19, 20] and probabilistic linear discriminant analysis (PLDA) [21, 22, 23].", "startOffset": 161, "endOffset": 173}, {"referenceID": 22, "context": "The weight for the decoding is set as the transpose of the encoding weight [24] in order to allow more layers to be stacked together and be fine-tuned with stochastic gradient descend (SGD).", "startOffset": 75, "endOffset": 79}, {"referenceID": 23, "context": "It is reported that the denoising auto-encoder can extract features more robustly than the basic auto-encoder [25].", "startOffset": 110, "endOffset": 114}, {"referenceID": 24, "context": "[26] shows that deeper architecture produces better high-level features compared to the shallow architecture up to 4 encoding and 4 decoding layers.", "startOffset": 0, "endOffset": 4}, {"referenceID": 25, "context": "It is reported that fine-tuning with back-propagation through a deep auto-encoder is ineffective due to vanishing gradients at the lower layers [27].", "startOffset": 144, "endOffset": 148}, {"referenceID": 22, "context": "To over come this issue we restrict the decoding weight as the transpose of the encoding weight following [24], that is, W\u2032 = W where W denotes transpose of W.", "startOffset": 106, "endOffset": 110}, {"referenceID": 23, "context": "We used a masking technique reported in [25] to corrupt the training data for the denoising auto-encoder.", "startOffset": 40, "endOffset": 44}, {"referenceID": 26, "context": "lr: learning rate, m: momentum, b: batch size, s: numpy random variable weight initialization seed [29], d: masking probability of each input dimension [25].", "startOffset": 99, "endOffset": 103}, {"referenceID": 23, "context": "lr: learning rate, m: momentum, b: batch size, s: numpy random variable weight initialization seed [29], d: masking probability of each input dimension [25].", "startOffset": 152, "endOffset": 156}, {"referenceID": 27, "context": "pronunciation lexicon Combilex [30].", "startOffset": 31, "endOffset": 35}, {"referenceID": 7, "context": "Random initialisation was used in a similar way to [8].", "startOffset": 51, "endOffset": 54}], "year": 2015, "abstractText": "This paper proposes a deep denoising auto-encoder technique to extract better acoustic features for speech synthesis. The technique allows us to automatically extract low-dimensional features from high dimensional spectral features in a non-linear, data-driven, unsupervised way. We compared the new stochastic feature extractor with conventional mel-cepstral analysis in analysis-by-synthesis and text-to-speech experiments. Our results confirm that the proposed method increases the quality of synthetic speech in both experiments.", "creator": "LaTeX with hyperref package"}}}