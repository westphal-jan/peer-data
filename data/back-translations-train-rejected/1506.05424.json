{"id": "1506.05424", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "17-Jun-2015", "title": "Hybrid Algorithm for Multi-Objective Optimization by Greedy Hypervolume Maximization", "abstract": "This paper introduces a high-performance hybrid algorithm, called Hybrid Hypervolume Maximization Algorithm (H2MA), for multi-objective optimization that alternates between exploring the decision space and exploiting the already obtained non-dominated solutions. The proposal is centered on maximizing the hypervolume indicator, thus converting the multi-objective problem into a single-objective one. The exploitation employs gradient-based methods, but considering a single candidate efficient solution at a time, to overcome limitations associated with population-based approaches and also to allow an easy control of the number of solutions provided. There is an interchange between two steps. The first step is a deterministic local exploration, endowed with an automatic procedure to detect stagnation. When stagnation is detected, the search is switched to a second step characterized by a stochastic global exploration using an evolutionary algorithm. Using five ZDT benchmarks with 30 variables, the performance of the new algorithm is compared to state-of-the-art algorithms for multi-objective optimization, more specifically NSGA-II, SPEA2, and SMS-EMOA. The solutions found by the H2MA guide to higher hypervolume and smaller distance to the true Pareto frontier with significantly less function evaluations, even when the gradient is estimated numerically. Furthermore, although only continuous decision spaces have been considered here, discrete decision spaces could also have been treated, replacing gradient-based search by hill-climbing. Finally, a thorough explanation is provided to support the expressive gain in performance that was achieved.", "histories": [["v1", "Wed, 17 Jun 2015 18:52:18 GMT  (271kb)", "http://arxiv.org/abs/1506.05424v1", null]], "reviews": [], "SUBJECTS": "cs.NE cs.AI", "authors": ["conrado silva miranda", "fernando jos\\'e von zuben"], "accepted": false, "id": "1506.05424"}, "pdf": {"name": "1506.05424.pdf", "metadata": {"source": "CRF", "title": "Hybrid Algorithm for Multi-Objective Optimization by Greedy Hypervolume Maximization", "authors": ["Conrado S. Miranda", "Fernando J. Von Zuben"], "emails": ["conrado@dca.fee.unicamp.br", "vonzuben@dca.fee.unicamp.br"], "sections": [{"heading": null, "text": "This year is the highest in the history of the country."}, {"heading": "II. MULTI-OBJECTIVE OPTIMIZATION AND THE", "text": "HYPERVOLUME INDICATORA multi-objective optimization problem is described by its decision space X and a series of objective functions fi (x). Due to the symmetry between maximization and minimization, only the minimization problem is considered here. Each point x in the decision space has a counterpart in objective space Y = Y1 \u00b7 \u00b7 \u00b7 \u00b7 YM given by y = f (x) = (f1 (x),.., fM (x). Since there are several goals, a new operator must be used to compare solutions, since the traditional \"less than\" operator < can only compare two numbers. This operator is called a dominance operator and is defined as a follow-up point. Definition 1 (dominance) x and y \u00b2 is defined by objective space. < Then this inequality dominates."}, {"heading": "A. The Hypervolume Indicator", "text": "In order to define the hypervolume indicator [5], we must first define the nadir point, which is a point in objective space dominated by each point in a specified space. Let X = {x1,.., xN}, XN, be a set of points in the decision space. Let z-RM be a point in objective space. Then, let z be a valid nadir point if we allow for all x-X and i points, just as in the definition of dominance, this fi (x) < zi. With definition 1, this can be written as f (x) x. Again, it is possible to allow equality in the definition of the nadir point, just as in the definition of dominance. However, if equality is allowed, it is possible for a certain point to have a zero hypervolume point, which can lead to undesirable decisions, if the hypervolume indicator is used, since such points would not contribute to the hypervolume."}, {"heading": "B. Gradient of the Hypervolume", "text": "In fact, the fact is that most of them will be able to put themselves at the top without being able to put themselves at the top."}, {"heading": "III. HYBRID HYPERVOLUME MAXIMIZATION ALGORITHM", "text": "From the discussion in section II-B, it can be seen that the main problem with optimizing the hypervolume is directly circumscribed by its gradient. (That is, our proposed algorithm optimizes a single solution at a point in time when it is no longer able to choose a new method that maximizes the hypervolume by taking into account the previous points, so that its recurring equation can be written as follows: xt = argmax x H (Xt \u2212 1), Xt = Xt \u2212 1), t N, (2) where the initial position of X \u2212 1 = {} is given. Since a single point is optimized at a time, the optimization in section IV, as we will show, requires fewer functional evaluations. In addition, it could be argued that maintaining the previous system reduces the flexibility allowed in comparison with a set where all points are adjusted simultaneously.Although this can be realized, we will also in section IV, requiring less functional evaluations."}, {"heading": "IV. EXPERIMENTAL RESULTS", "text": "This year it is so far that it will only take one year to move on to the next round."}, {"heading": "A. Analysis of the H2MA\u2019s performance", "text": "As in every year, so also this year, it is also this year again so far."}, {"heading": "V. CONCLUSION", "text": "This paper proposed the Hybrid Hypervolume Maximization Algorithm (H2MA) for multi-objective optimization, which only tries to maximize the hypervolume one at a time. It initially tries to perform deterministic local exploration and, when stuck, switches to stochastic global exploration using an evolutionary algorithm. The optimization algorithm used during deterministic optimization is problematic and can be given by a gradient-based method if the decision space is continuous when the decision space is discrete. Here, we have exclusively explored continuous decision spaces. The algorithm has been compared with state-of-the-art multi-objective optimization algorithms, namely NSGA-II, SPEA2, and SMS-EMOA on the ZDT problems. Despite the use of numerical gradients for the objective functions that increase the number of function calls, the algorithm is provided with an equal number of evaluations for the total hyper volume."}, {"heading": "ACKNOWLEDGMENT", "text": "The authors thank CNPq for the financial support."}], "references": [{"title": "Nonlinear Multiobjective Optimization", "author": ["K. Miettinen"], "venue": "Springer US,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 1999}, {"title": "Multi-Objective Optimization Using Evolutionary Algorithms", "author": ["K. Deb"], "venue": null, "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2001}, {"title": "A fast and elitist multiobjective genetic algorithm: NSGA-II", "author": ["K. Deb", "A. Pratap", "S. Agarwal", "T.A.M.T. Meyarivan"], "venue": "Evolutionary Computation, IEEE Transactions on, vol. 6, no. 2, pp. 182\u2013197, 2002.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2002}, {"title": "SPEA2: Improving the strength Pareto evolutionary algorithm", "author": ["E. Zitzler", "M. Laumanns", "L. Thiele"], "venue": "2001.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2001}, {"title": "The hypervolume indicator revisited: On the design of Pareto-compliant indicators via weighted integration", "author": ["E. Zitzler", "D. Brockhoff", "L. Thiele"], "venue": "Evolutionary multi-criterion optimization. Springer, 2007, pp. 862\u2013876.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2007}, {"title": "Performance Assessment of Multiobjective Optimizers: An Analysis and Review", "author": ["E. Zitzler", "L. Thiele", "M. Laumanns", "C.M. Fonseca", "V.G. Da Fonseca"], "venue": "Evolutionary Computation, IEEE Transactions on, vol. 7, no. 2, pp. 117\u2013132, 2003.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2003}, {"title": "Theory of the hypervolume indicator: optimal \u03bc-distributions and the choice of the reference point", "author": ["A. Auger", "J. Bader", "D. Brockhoff", "E. Zitzler"], "venue": "Proceedings of the tenth ACM SIGEVO workshop on Foundations of genetic algorithms. ACM, 2009, pp. 87\u2013102.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2009}, {"title": "SMS-EMOA: Multiobjective selection based on dominated hypervolume", "author": ["N. Beume", "B. Naujoks", "M. Emmerich"], "venue": "European Journal of Operational Research, vol. 181, no. 3, pp. 1653\u20131669, 2007.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2007}, {"title": "On gradients and hybrid evolutionary algorithms for real-valued multiobjective optimization", "author": ["P.A.N. Bosman"], "venue": "Evolutionary Computation, IEEE Transactions on, vol. 16, no. 1, pp. 51\u201369, 2012.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2012}, {"title": "Time Complexity and Zeros of the Hypervolume Indicator Gradient Field", "author": ["M. Emmerich", "A. Deutz"], "venue": "EVOLVE-A Bridge between Probability, Set Oriented Numerics, and Evolutionary Computation III. Springer, 2014, pp. 169\u2013193.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2014}, {"title": "Hypervolume Maximization via Set Based Newton\u2019s Method", "author": ["V.A.S. Hern\u00e1ndez", "O. Sch\u00fctze", "M. Emmerich"], "venue": "EVOLVE-A Bridge between Probability, Set Oriented Numerics, and Evolutionary Computation V. Springer, 2014, pp. 15\u201328.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2014}, {"title": "Comparison of Multiobjective Evolutionary Algorithms: Empirical Results", "author": ["E. Zitzler", "K. Deb", "L. Thiele"], "venue": "Evolutionary Computation, vol. 8, no. 2, pp. 173\u2013195, 2000.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2000}, {"title": "Multi-objective optimization", "author": ["K. Deb"], "venue": "Search methodologies. Springer, 2014, pp. 403\u2013449.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2014}, {"title": "A global optimisation toolbox for massively parallel engineering optimisation", "author": ["F. Biscani", "D. Izzo", "C.H. Yam"], "venue": "arXiv preprint arXiv:1004.3824, 2010.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2010}, {"title": "SciPy: Open source scientific tools for Python", "author": ["E. Jones", "T. Oliphant", "P. Peterson"], "venue": "2001\u2013, [Online; accessed 2015-05-10]. [Online]. Available: http://www.scipy.org/", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2001}, {"title": "A Comprehensive Survey of Fitness Approximation in Evolutionary Computation", "author": ["Y. Jin"], "venue": "Soft computing, vol. 9, no. 1, pp. 3\u201312, 2005.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2005}, {"title": "Meta-Modeling in Multiobjective Optimization", "author": ["J. Knowles", "H. Nakayama"], "venue": "Multiobjective Optimization. Springer, 2008, pp. 245\u2013284.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2008}, {"title": "Multi-objective Optimization Using Surrogates", "author": ["I. Voutchkov", "A. Keane"], "venue": "Computational Intelligence in Optimization. Springer, 2010, pp. 155\u2013175.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2010}], "referenceMentions": [{"referenceID": 0, "context": "MULTI-OBJECTIVE optimization (MOO) is a generalization of the standard single-objective optimization to problems where multiple criteria are defined and they conflict with each other [1].", "startOffset": 183, "endOffset": 186}, {"referenceID": 1, "context": "The current state-of-the-art for MOO relies on the use of evolutionary algorithms for finding the desired samples [2].", "startOffset": 114, "endOffset": 117}, {"referenceID": 2, "context": "One of these algorithms is the NSGA-II [3], which performs non-dominance sorting, thus ordering the proposed solutions according to their relative dominance degree, and dividing the", "startOffset": 39, "endOffset": 42}, {"referenceID": 3, "context": "Another well-known algorithm is the SPEA2 [4], where the solutions have a selective pressure to move towards the Pareto frontier and also to stay away from each other.", "startOffset": 42, "endOffset": 45}, {"referenceID": 4, "context": "However, the hypervolume indicator [5] defines a metric of performance for a set of solutions, thus allowing a direct comparison of multiple distinct sets of solutions [6], with higher values indicating possible better quality.", "startOffset": 35, "endOffset": 38}, {"referenceID": 5, "context": "However, the hypervolume indicator [5] defines a metric of performance for a set of solutions, thus allowing a direct comparison of multiple distinct sets of solutions [6], with higher values indicating possible better quality.", "startOffset": 168, "endOffset": 171}, {"referenceID": 6, "context": "The hypervolume is maximal at the Pareto frontier and increases if the samples are better distributed along the frontier [7].", "startOffset": 121, "endOffset": 124}, {"referenceID": 7, "context": "Due to these properties, it represents a good candidate to be maximized in MOO, being explicitly explored in the SMS-EMOA [8], where solutions that contribute the least to the hypervolume are discarded.", "startOffset": 122, "endOffset": 125}, {"referenceID": 8, "context": "For instance, [11] defined a method for finding all minimizing directions in a MOO problem, but the proposed algorithm achieved low performance on usual benchmark functions.", "startOffset": 14, "endOffset": 18}, {"referenceID": 9, "context": "Based on this idea, [12] proposed a method to compute the hypervolume\u2019s gradient for a given population, so that the optimal search direction for each individual could be established.", "startOffset": 20, "endOffset": 24}, {"referenceID": 10, "context": "However, [13] showed that adjusting the population through integration of the hypervolume\u2019s gradient not always work, with some initially non-dominated points becoming dominated and others changing very little over the integration.", "startOffset": 9, "endOffset": 13}, {"referenceID": 10, "context": "We found that this restriction is enough to overcome the issues presented in [13] when using the hypervolume\u2019s gradient.", "startOffset": 77, "endOffset": 81}, {"referenceID": 11, "context": "Results over the ZDT benchmark [14] show that the new algorithm performs better than the state-of-the-art evolutionary algorithms, both in terms of total hypervolume and distance to the Pareto frontier.", "startOffset": 31, "endOffset": 35}, {"referenceID": 10, "context": "Section II introduces the concepts of multi-objective optimization required, including the hypervolume indicator, and discusses the problems with the gradient-based approach for hypervolume maximization introduced in [13].", "startOffset": 217, "endOffset": 221}, {"referenceID": 12, "context": ",M}, where Yi \u2286 R is the associated objective space for each objective function [15].", "startOffset": 80, "endOffset": 84}, {"referenceID": 12, "context": "Thus, there is no reason someone would choose y over y, and it can be discarded, as occurs in many multi-objective optimization algorithms [15].", "startOffset": 139, "endOffset": 143}, {"referenceID": 5, "context": "Note that there are other definitions of the dominance operator [6], where one considers the inequality \u2264 instead of the strict inequality < used here.", "startOffset": 64, "endOffset": 67}, {"referenceID": 9, "context": "However, equality in some of the coordinates may be an issue when using the hypervolume indicator, such as when taking its derivative [12].", "startOffset": 134, "endOffset": 138}, {"referenceID": 4, "context": "In order to define the hypervolume indicator [5], we must first define the Nadir point, which is a point in the objective space that is dominated by every point in a set.", "startOffset": 45, "endOffset": 48}, {"referenceID": 7, "context": "Although such approach proved to be successful when using evolutionary algorithms as the optimization method [8], the same did not happen when using the hypervolume\u2019s gradient", "startOffset": 109, "endOffset": 112}, {"referenceID": 10, "context": "to perform the optimization [13].", "startOffset": 28, "endOffset": 32}, {"referenceID": 9, "context": "The hypervolume\u2019s gradient for a set of points was introduced in [12], and it can be used to compute the optimal direction in which a given point should move to increase the hypervolume associated with the current set of non-dominated solutions.", "startOffset": 65, "endOffset": 69}, {"referenceID": 10, "context": "Based on this motivation, [13] used the hypervolume\u2019s gradient as a guide for adjusting a set of points by numerical integration, that is, performing a small step in the direction pointed by the gradient.", "startOffset": 26, "endOffset": 30}, {"referenceID": 10, "context": "In the analysis presented in [13], it is clear that the cases where some points got stuck had higher gradients for the border points in the objective space, which led to the dominance or decrease of contribution of some or all central points.", "startOffset": 29, "endOffset": 33}, {"referenceID": 10, "context": "It is important to highlight that this behavior does not happen always, but can occur along the iterative process, as shown in [13].", "startOffset": 127, "endOffset": 131}, {"referenceID": 0, "context": "(4) [0, 1] (2, 11) ZDT2 Eq.", "startOffset": 4, "endOffset": 10}, {"referenceID": 0, "context": "(5) [0, 1] (2, 11) ZDT3 Eq.", "startOffset": 4, "endOffset": 10}, {"referenceID": 0, "context": "(6) [0, 1] (2, 11) ZDT4 Eq.", "startOffset": 4, "endOffset": 10}, {"referenceID": 0, "context": "(7) [0, 1]\u00d7 [\u22125, 5]n\u22121 (2, 2 + 50(n \u2212 1)) ZDT6 Eq.", "startOffset": 4, "endOffset": 10}, {"referenceID": 0, "context": "(8) [0, 1] (2, 11)", "startOffset": 4, "endOffset": 10}, {"referenceID": 11, "context": "the existing algorithms, the ZDT family of functions [14] was chosen.", "startOffset": 53, "endOffset": 57}, {"referenceID": 11, "context": "All functions defined in [14] have a continuous decision space X , except for the ZDT5 which has a binary space.", "startOffset": 25, "endOffset": 29}, {"referenceID": 13, "context": "The evolutionary algorithms\u2019 and evaluation functions\u2019 implementations were given by the PaGMO library [16].", "startOffset": 103, "endOffset": 107}, {"referenceID": 2, "context": "We compare our algorithm with existing state-of-the-art multi-objective optimization algorithms, namely NSGA-II [3], SPEA2 [4], and SMS-EMOA [8].", "startOffset": 112, "endOffset": 115}, {"referenceID": 3, "context": "We compare our algorithm with existing state-of-the-art multi-objective optimization algorithms, namely NSGA-II [3], SPEA2 [4], and SMS-EMOA [8].", "startOffset": 123, "endOffset": 126}, {"referenceID": 7, "context": "We compare our algorithm with existing state-of-the-art multi-objective optimization algorithms, namely NSGA-II [3], SPEA2 [4], and SMS-EMOA [8].", "startOffset": 141, "endOffset": 144}, {"referenceID": 14, "context": "In this paper, we used the LBFGS-B method implemented in the library SciPy [17], which is able to handle the bounds of X and is very efficient to find a local optimum.", "startOffset": 75, "endOffset": 79}, {"referenceID": 9, "context": "The analytic method for computing the hypervolume\u2019s gradient is described in [12].", "startOffset": 77, "endOffset": 81}, {"referenceID": 15, "context": "Future work should focus on using surrogates to reduce the number of evaluations [18], [19], [20].", "startOffset": 81, "endOffset": 85}, {"referenceID": 16, "context": "Future work should focus on using surrogates to reduce the number of evaluations [18], [19], [20].", "startOffset": 87, "endOffset": 91}, {"referenceID": 17, "context": "Future work should focus on using surrogates to reduce the number of evaluations [18], [19], [20].", "startOffset": 93, "endOffset": 97}], "year": 2015, "abstractText": "This paper introduces a high-performance hybrid algorithm, called Hybrid Hypervolume Maximization Algorithm (H2MA), for multi-objective optimization that alternates between exploring the decision space and exploiting the already obtained non-dominated solutions. The proposal is centered on maximizing the hypervolume indicator, thus converting the multi-objective problem into a single-objective one. The exploitation employs gradient-based methods, but considering a single candidate efficient solution at a time, to overcome limitations associated with population-based approaches and also to allow an easy control of the number of solutions provided. There is an interchange between two steps. The first step is a deterministic local exploration, endowed with an automatic procedure to detect stagnation. When stagnation is detected, the search is switched to a second step characterized by a stochastic global exploration using an evolutionary algorithm. Using five ZDT benchmarks with 30 variables, the performance of the new algorithm is compared to state-of-the-art algorithms for multi-objective optimization, more specifically NSGA-II, SPEA2, and SMS-EMOA. The solutions found by the H2MA guide to higher hypervolume and smaller distance to the true Pareto frontier with significantly less function evaluations, even when the gradient is estimated numerically. Furthermore, although only continuous decision spaces have been considered here, discrete decision spaces could also have been treated, replacing gradient-based search by hill-climbing. Finally, a thorough explanation is provided to support the expressive gain in performance that was achieved.", "creator": "LaTeX with hyperref package"}}}