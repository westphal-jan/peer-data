{"id": "1506.05703", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "18-Jun-2015", "title": "\"The Sum of Its Parts\": Joint Learning of Word and Phrase Representations with Autoencoders", "abstract": "Recently, there has been a lot of effort to represent words in continuous vector spaces. Those representations have been shown to capture both semantic and syntactic information about words. However, distributed representations of phrases remain a challenge. We introduce a novel model that jointly learns word vector representations and their summation. Word representations are learnt using the word co-occurrence statistical information. To embed sequences of words (i.e. phrases) with different sizes into a common semantic space, we propose to average word vector representations. In contrast with previous methods which reported a posteriori some compositionality aspects by simple summation, we simultaneously train words to sum, while keeping the maximum information from the original vectors. We evaluate the quality of the word representations on several classical word evaluation tasks, and we introduce a novel task to evaluate the quality of the phrase representations. While our distributed representations compete with other methods of learning word representations on word evaluations, we show that they give better performance on the phrase evaluation. Such representations of phrases could be interesting for many tasks in natural language processing.", "histories": [["v1", "Thu, 18 Jun 2015 14:46:44 GMT  (149kb,D)", "http://arxiv.org/abs/1506.05703v1", "Deep Learning Workshop, ICML 2015"]], "COMMENTS": "Deep Learning Workshop, ICML 2015", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["r\\'emi lebret", "ronan collobert"], "accepted": false, "id": "1506.05703"}, "pdf": {"name": "1506.05703.pdf", "metadata": {"source": "CRF", "title": "\u201cThe Sum of Its Parts\u201d: Joint Learning of Word and Phrase Representations with Autoencoders", "authors": ["R\u00e9mi Lebret", "Ronan Collobert"], "emails": ["REMI@LEBRET.CH", "RONAN@COLLOBERT.COM"], "sections": [{"heading": null, "text": "These representations have been shown to capture both semantic and syntactic information about words. However, distributed representations of phrases remain a challenge. We are introducing a novel model in which word vector representations and their summation are learned together. Word representations are learned using statistical information about the occurrence of the word. In order to embed word sequences (i.e. phrases) of different sizes in a common semantic space, we suggest average word vector representations. Unlike earlier methods, which reported some compositivity aspects a posteriori through simple summing, we train words simultaneously to sum while retaining the maximum information from the original vectors. We evaluate the quality of word representations on several classic word evaluation tasks and introduce a novel task to evaluate the quality of phrase representations. While our distributed representations compete with other methods of learning word representations through word evaluations, we show that they are better before we have combined the phrases in the Rondip research."}, {"heading": "1. Introduction", "text": "A large number of sentences can be generated from a finite set of words, so there has been a lot of effort to grasp the meaning of words; some approaches are based on distributed representations (Lund & Burgess, 1996; Patel et al., 1998), others are based on distributed representations (Bengio, 2008; Collobert et al., 2011; Mnih & Kavukcuoglu, 2013; Mikolov et al., 2013b; Lebret & Collobert, 2014; Pennington et al., 2014; Levy & Goldberg, 2014), where the meaning of a word is encoded as a vector from co-occurrence statistics of a word and its adjacent words. Finally, distributed representations emerged as the solution to many natural language processing (NLP) tasks."}, {"heading": "2. Related Works", "text": "In the literature, there are two large model families for learning distributed word representations: the counting methods and the predictive methods. Counting methods consist of using the statistical information contained in large corpora of unlabeled text to form large matrices simply counting words (word cocoocurrence statistics); the lines correspond to words or terms; and the columns correspond to a local context. The context can be documents, as in latent semantic analysis (LSA) (Deerwester et al., 1990); or other words (Lund Burgess, 1996). To generate low-dimensional word representations, an approximation of these large matrices is performed, mainly with a singular value decomposition (SVD). Many authors suggested to improve this model with different transformations of the number matrix, such as positive pointedly mutual information (PPMI)."}, {"heading": "3. A Joint Model", "text": "Some previous works have designed models to learn word representations (Mnih & Kavukcuoglu, 2013; Mikolov et al., 2013b; Lebret & Collobert, 2014), while others have suggested models to compose these word representations (Mitchell & Lapata, 2010; Socher et al., 2012)."}, {"heading": "3.1. Learning Word Representations w.r.t. the Hellinger Distance", "text": "Since words that occur in similar contexts tend to have similar meanings (Harris, 1954), statistics on the occurrence of words are usually used to embed similar words in a common vector space (Turney & Pantel, 2010). Common approaches calculate frequencies, apply some transformations (tfidf, PPMI), reduce dimensionality and calculate similarities. More recently, Lebret & Collobert (2014) proposed a new method based on a Hellinger PCA of the word co-occurence matrix, which showed that word representations can also be learned with an appropriate number of context words. Inspired by this work, we propose to perform this low-level approximation stochastically. To this end, we use an autoencoder with only linear activations to find an optimal solution in connection with the Hellinger PCA (Bourlard & Kamp 1988)."}, {"heading": "3.1.1. WORD CO-OCCURRENCE PROBABILITIES", "text": "Taking this famous quote into account, the probabilities for the occurrence of words are calculated by counting the number of times each contextual word c-D (where D-W) occurs by one word w-W: p (c-w) = p (c-w) p (w) = n (c-w) \u2211 cj-D n (cj-w), (1) where n (c-w) is the number of times a contextual word c occurs in the vicinity of the word w. Thus, a multinomial distribution of | D classes (words) is obtained for each word w: Pw = {p (c1-w),.., p (c-D-w). (2)"}, {"heading": "3.1.2. HELLINGER DISTANCE", "text": "Similarities between words can be derived by calculating a distance between their corresponding word distributions. There are several distances (or metrics) via discrete distributions, such as the Bhattacharyya distance, the Hellinger distance, or the Kullback-Leibler divergence. We chose the Hellinger distance here for its simplicity and symmetry property (since it is a true distance). Taking into account two discrete probability distributions P = (p1,..., pk) and Q = (q1,., qk), the Hellinger distance is formally defined as: H (P, Q) = 1 \u221a 2 \u221a k \u0445 i = 1 (\u221a pi \u2212 qi) 2, (3), which is directly related to the euclidean norm of the difference of the square root vectors: H (P, Q) = 1 \u221a 2 \u221a P \u2212 \u221a Q \u0445 2. (4) Note that it makes more sense to consider the Hellinger distance from the Euclidean and the Hell rectors than the Euclidean distance."}, {"heading": "3.1.3. AUTOENCODER", "text": "An auto encoder is used to represent words in a lower dimensional space. It takes a distribution \u221a Pw as input, encodes it in a more compact representation, and is trained to reconstruct its own input from this representation: | | g (f (\u221a Pw) \u2212 \u221a Pw | | 2, (5) where g (f (\u221a Pw) is the output of the network, f is the encoding function that maps distributions in a map dimension (with m < < | D |), and g is the decoding function. f (\u221a Pw) is a distributed representation that captures the most important variation factors in the data, as the Hellinger PCA does (Bourlard & Kamp, 1988)."}, {"heading": "3.2. Learning to Sum Word Representations", "text": "Interesting compositional properties have been observed using models based on the addition of representations (Mikolov et al., 2013b). An exhaustive comparison of different compositional properties has indeed shown that an additive model performs well in pre-trained word representations (Mitchell & Lapata, 2010). Since our word representations are learned from linear operations, the inherent structure of these representations is linear. To combine a sequence of words into a common vector space, we simply apply an elementary addition of their vector representations. This approach makes sense and works well when the meaning of a text is literally \"the sum of its parts.\" This is usually the case with chunks of noun and verb phrases. For example, in phrases such as \"the red cat\" or \"struggle to deal\" each word has its own meaning independently. Distributed representations for such phrase drops must retain information from the individual words. An objective function is likely to be learned in order to combine the word as the one can."}, {"heading": "3.2.1. ADDITIVE MODEL", "text": "We define s = (w1,.., wT) and S as a phrase block of T-words, where S is a series of phrases. By entering all \u221a Pw into the autoencoder, we obtain a representation xw-Rm of each word w-D: xw = f (\u221a Pw). (6) By adding elements, a representation of the phrase piece can be calculated as follows: xs = 1T \u2211 wt-s xwt. (7)"}, {"heading": "3.2.2. TRAINING", "text": "In predictive models such as the Skip-gram model, the goal is to maximize the probability of a word based on other words in the same order. Instead, our training differs slightly in the sense that we aim to distinguish whether words are in the phrase block or not. Therefore, an objective function is defined to encourage words that occur in the phrase block to achieve high scores when calculating the point product between xwt and xs. On the other hand, these values must be low for words that do not occur in the phrase block. We train this problem with a ranking-cost approach: We train this problem with a ranking-type approach to speed up the training. In Equation 8, the entire dictionary W is replaced with a subset of W \u2212 W with randomly selected negative samples in the order W \u2212 W."}, {"heading": "3.3. Joint Learning", "text": "Unlike other methods, which later found beautiful compositional properties through simple summing, the novelty of our method consists in the explicit learning of word representations suitable for summing up. The system is then designed so that words with similar context are close in a m-dimensional space, while these dimensions are learned to be combined with other related words. This common learning is illustrated in Figure 1. The entire system is trained by minimizing both objective functions (5) and (8) via training data using stochastic gradient descent."}, {"heading": "4. Experiments", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.1. Datasets", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.1.1. BUILDING WORD REPRESENTATION OVER LARGE CORPORA", "text": "Our English corpus consists of the entire English Wikipedia1 (where all labels in the MediaWiki have been removed), and we look at lowercase letters to limit the number of words in the dictionary. Additionally, all occurrences of strings within a word are replaced by the string \"NUMBER.\" The resulting text is tokenized using the Stanford Tokenizer2. The dataset contains about 1.61 billion words. As dictionaryW, we consider all words within our corpus that occur at least a hundred times, resulting in a dictionary of 191,268 words. Only the 10,000 most common words within this dictionary were used as context words D to calculate the probabilities of the common occurrence of the word."}, {"heading": "4.1.2. SUMMING WORDS FOR PHRASE REPRESENTATION", "text": "To learn the summation of words that often occur together, we decide to consider only the phrase blocks of nouns and verbs that occur at least ten times. We extract these blocks using the SENNA software3. By retaining only the phrase blocks that occur at least ten times, we have divided these sentences into three sentences: 1,000 phrases for validation, 5,000 phrases for testing, and the rest for training (2,072,491 phrases). An unattended frame requires a large amount of data. As our main focus is on providing good word representation, validation and test sentences are deliberately kept small to keep as many phrases as possible in the training set."}, {"heading": "4.2. Other Methods", "text": "We compare our distributed representations with other available models for calculating word vector representations: (1) the GloVe model, which is also based on corpora random statistics (Pennington et al., 2014) 4, (2) the Continuous Word Wallet (CBOW) and the Skipgram (SG) architectures, which learn representations from predictive models (Mikolov et al., 2013b) 5. To see the improvement compared to a standalone SVD, we create word representations with a truncated SVD of matrix X, each line of X being a ten-word distribution window and the default values set by the authors for the other hyperparameters."}, {"heading": "4.3. Evaluating Word Representations", "text": "The first goal of the model is to learn distributed representations that capture both syntactical and semantic information about words. To evaluate the quality of these representations, we used both analogy and similarity tasks."}, {"heading": "4.3.1. WORD ANALOGIES", "text": "The word analogy problem consists of questions such as \"a is to b as c is to?.\" It was introduced in Mikolov et al. (2013a) and contains 19,544 such questions, divided into a semantic subset and a syntactic subset. The 8,869 semantic questions are analogies to places such as \"Bern is to Switzerland like Paris?\" or family relationships such as \"Uncle is to aunt like boy?.\" The 10,675 syntactic questions are grammatical analogies encompassing plural and adjective forms, superlatives, verbs, etc. To answer the question correctly, the model should clearly identify the missing term, with only an exact match being considered the correct match."}, {"heading": "4.3.2. WORD SIMILARITIES", "text": "We also evaluate our model based on a variety of word similarity tasks, including the WordSimilarity-353 Test Collection (WS-353) (Finkelstein et al., 2001), the Rubenstein and Goodenough datasets (RG-65) (Rubenstein & Goodenough, 1965), and the Stanford Rare Word (Luong et al., 2013), all of which contain sentences of English word pairs along with human-assigned similarity judgments. WS353 and RG-65 datasets contain 353 and 65 word pairs, respectively. These are relatively common word pairs, such as computers: Internet or football: tennis. The RW dataset differs from these two datasets because it contains 2034 pairs in which one of the words is rare or morphologically complex, such as brigadier: general or cognitive: know."}, {"heading": "4.3.3. RESULTS", "text": "The results mentioned in Table 1 show that our model delivers similar results to other state-of-the-art methods of word similarity tasks. However, there is a significant performance boost between the slight approximation of X with an SVD and the same approximation with our common model. This shows that combining a count-based model with a predictive approach helps to generate better word representations. Word analogy performance shows that our common model competes with others on syntactic questions, but that it offers less accuracy on semantic questions. One possible explanation is that, compared to syntactic questions, less common words are involved in semantic questions. Of the four words that make up a semantic question, one of them is, on average, the 34328th most common word in W, while it is the 20819th for a syntactic question. Compared to other methods that use the entire dictionary as a context dictionary, we consider only a small subset of them (containing only 10,000 words in this context)."}, {"heading": "4.4. Evaluating Phrase Representations", "text": "As a second goal, we want to learn to summarize word representations in order to generate phrase representations while maintaining the original information that comes from the words."}, {"heading": "4.4.1. DESCRIPTION OF THE TASK", "text": "As a data set, we use the collection of test phrases described in Section 4.1.2. It contains 5000 phrases (noun and verb phrases) extracted from Wikipedia using a chunking approach. 2244, 2030, and 547, respectively, consist of two, three, and four words; the remaining 179 consist of at least five words with a maximum of eight words. For a given expression s = (w1,..., wT) of T-words, the goal is to retrieve the T-words from their distributed representation xs. Scores between the phrase s and any possible word wi-W are calculated using the dot product between their distributed representations xs \u00b7 xwi, as shown in Figure 1. The uppermost T-scores are considered to be the words that compose the phrase s."}, {"heading": "4.4.2. RESULTS", "text": "To assess whether words that form a particular phrase can be retrieved from the distributed phrase representation, we use Recall @ K, which measures the fraction of the time it takes to find a correct word among the top K results. K is proportional to the number of words per phrase, for example, for a 3-word phrase6There has been no research due to hardware resource constraints. It would be easy to calculate with a cluster of CPU.with a Recall @ 5, the correct words are found among the top 15 results. Higher Recall @ K means better retrieval performance. As we are most interested in the best retrieved results, the Recall @ K with a small K. The results recorded in Table 2 show that our distributed word representations can be averaged together to produce meaningful phrase representations because the words are retrieved with a high memory. Our model clearly outperforms other methods in this task."}, {"heading": "4.5. Inferring New Phrase Representations", "text": "Consider that the dictionary Wn tends to grow exponentially with n, it provides a nice framework for producing the enormous variety of possible sequences of n words in a timely and efficient manner with low memory consumption, as opposed to other methods. Relying on word coincidence statistics to represent words in vector space also provides a framework for easily generating representations for invisible words, which is another advantage over methods that focus on learning distributed word representations (such as CBOW, Skip-gram, and GloVe models), where the entire system needs to be retrained to learn representations for these new constituents. To derive a representation for a new word, you just have to count your context words over a large text corpus to form the distribution patterns. This beautiful feature can be extrapolated on phrases, which gives another alternative to the representation of phrases."}, {"heading": "5. Conclusion", "text": "We introduce a model that combines both count-based methods and predictive methods for generating distributed word and phrase representations, using a chunking approach to extract a collection of noun and verb phrases from Wikipedia. For a given n-word phrase, we train our model to generate a low-dimensional representation for each word based on its probability distribution. These n-representations are averaged together to produce a distributed phrase representation in the same semantic space. Thanks to an auto-encoder approach, we can simultaneously train the model to retrieve the original n words from the phrase representation and thus learn complex linear substructures. Compared to modern methods of word evaluation for some classic word evaluation tasks, the competitive results show that our common model produces meaningful word representations. Performance on a novel task to evaluate phrase representation confirms our model's ability to enable complex word substructures to accommodate the low aggregation of multiple words within a single word."}, {"heading": "Acknowledgements", "text": "This work was supported by the HASLER Foundation with the support of \"Information and Communication Technology for a Better World 2020\" (SmartWorld)."}], "references": [{"title": "Neural machine translation by jointly learning to align and translate", "author": ["Bahdanau", "Dzmitry", "Cho", "Kyunghyun", "Bengio", "Yoshua"], "venue": "In Proceedings of the 3th International Conference on Learning Representations (ICLR),", "citeRegEx": "Bahdanau et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2015}, {"title": "Nouns are Vectors, Adjectives are Matrices: Representing Adjective-Noun Constructions in Semantic Space", "author": ["M. Baroni", "R. Zamparelli"], "venue": "In Proceedings of the EMNLP, pp", "citeRegEx": "Baroni and Zamparelli,? \\Q2010\\E", "shortCiteRegEx": "Baroni and Zamparelli", "year": 2010}, {"title": "Neural net language models. Scholarpedia", "author": ["Y. Bengio"], "venue": null, "citeRegEx": "Bengio,? \\Q2008\\E", "shortCiteRegEx": "Bengio", "year": 2008}, {"title": "A neural probabilistic language model", "author": ["Y. Bengio", "R. Ducharme", "P. Vincent", "C. Janvin"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "Bengio et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Bengio et al\\.", "year": 2003}, {"title": "A Comparison of Vectorbased Representations for Semantic Composition", "author": ["W. Blacoe", "M. Lapata"], "venue": "In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning,", "citeRegEx": "Blacoe and Lapata,? \\Q2012\\E", "shortCiteRegEx": "Blacoe and Lapata", "year": 2012}, {"title": "Auto-association by multilayer perceptrons and singular value decomposition", "author": ["H. Bourlard", "Y. Kamp"], "venue": "Biological cybernetics,", "citeRegEx": "Bourlard and Kamp,? \\Q1988\\E", "shortCiteRegEx": "Bourlard and Kamp", "year": 1988}, {"title": "Extracting semantic representations from word co-occurrence statistics: A computational study", "author": ["J.A. Bullinaria", "J.P. Levy"], "venue": "Behavior Research Methods,", "citeRegEx": "Bullinaria and Levy,? \\Q2007\\E", "shortCiteRegEx": "Bullinaria and Levy", "year": 2007}, {"title": "A compositional distributional model of meaning", "author": ["S. Clark", "B. Coecke", "M. Sadrzadeh"], "venue": "In Proceedings of the Second Quantum Interaction Symposium", "citeRegEx": "Clark et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Clark et al\\.", "year": 2008}, {"title": "Natural Language Processing (Almost) from Scratch", "author": ["R. Collobert", "J. Weston", "L. Bottou", "M. Karlen", "K. Kavukcuoglu", "P. Kuksa"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "Collobert et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Collobert et al\\.", "year": 2011}, {"title": "Indexing by latent semantic analysis", "author": ["S. Deerwester", "S.T. Dumais", "G.W. Furnas", "T.K. Landauer", "R. Harshman"], "venue": "Journal of the American Society for Information Science,", "citeRegEx": "Deerwester et al\\.,? \\Q1990\\E", "shortCiteRegEx": "Deerwester et al\\.", "year": 1990}, {"title": "Placing Search in Context: The Concept Revisited", "author": ["L. Finkelstein", "E. Gabrilovich", "Y. Matias", "E. Rivlin", "Z. Solan", "G. Wolfman", "E. Ruppin"], "venue": "In Proceedings of the 10th international conference on World Wide Web,", "citeRegEx": "Finkelstein et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Finkelstein et al\\.", "year": 2001}, {"title": "A Synopsis of Linguistic Theory 1930-55", "author": ["J.R. Firth"], "venue": null, "citeRegEx": "Firth,? \\Q1957\\E", "shortCiteRegEx": "Firth", "year": 1957}, {"title": "Proceedings of the 10th International Conference on Computational Semantics (IWCS 2013) \u2013 Long Papers, chapter Multi-Step Regression Learning for Compositional Distributional Semantics", "author": ["E. Grefenstette", "G. Dinu", "Y. Zhang", "M. Sadrzadeh", "M. Baroni"], "venue": null, "citeRegEx": "Grefenstette et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Grefenstette et al\\.", "year": 2013}, {"title": "Phrasebased image captioning", "author": ["R. Lebret", "P.H.O. Pinheiro", "R. Collobert"], "venue": "In Proceedings of the 32st International Conference on Machine Learning (ICML),", "citeRegEx": "Lebret et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Lebret et al\\.", "year": 2015}, {"title": "Joint rnn-based greedy parsing and word composition", "author": ["J. Legrand", "R. Collobert"], "venue": "In Proceedings of the 3th International Conference on Learning Representations (ICLR),", "citeRegEx": "Legrand and Collobert,? \\Q2015\\E", "shortCiteRegEx": "Legrand and Collobert", "year": 2015}, {"title": "Neural Word Embedding as Implicit Matrix Factorization", "author": ["O. Levy", "Y. Goldberg"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Levy and Goldberg,? \\Q2014\\E", "shortCiteRegEx": "Levy and Goldberg", "year": 2014}, {"title": "Producing high-dimensional semantic spaces from lexical co-occurrence", "author": ["K. Lund", "C. Burgess"], "venue": "Behavior Research Methods, Instruments, & Computers,", "citeRegEx": "Lund and Burgess,? \\Q1996\\E", "shortCiteRegEx": "Lund and Burgess", "year": 1996}, {"title": "Better Word Representations with Recursive Neural Networks for Morphology", "author": ["M. Luong", "R. Socher", "C.D. Manning"], "venue": "In CoNLL,", "citeRegEx": "Luong et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Luong et al\\.", "year": 2013}, {"title": "Efficient Estimation of Word Representations in Vector Space", "author": ["T. Mikolov", "K. Chen", "G. Corrado", "J. Dean"], "venue": "In Proceedings of Workshop at International Conference on Learning Representations (ICLR", "citeRegEx": "Mikolov et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Distributed Representations of Words and Phrases and their Compositionality", "author": ["T. Mikolov", "I. Sutskever", "K. Chen", "G. Corrado", "J. Dean"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Mikolov et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Composition in Distributional Models of Semantics", "author": ["J. Mitchell", "M. Lapata"], "venue": "Cognitive Science,", "citeRegEx": "Mitchell and Lapata,? \\Q2010\\E", "shortCiteRegEx": "Mitchell and Lapata", "year": 2010}, {"title": "Learning word embeddings efficiently with noise-contrastive estimation", "author": ["A. Mnih", "K. Kavukcuoglu"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Mnih and Kavukcuoglu,? \\Q2013\\E", "shortCiteRegEx": "Mnih and Kavukcuoglu", "year": 2013}, {"title": "Extracting Semantic Representations from Large Text Corpora", "author": ["M. Patel", "J.A. Bullinaria", "J.P. Levy"], "venue": "Neural Computation and Psychology Workshop, London,", "citeRegEx": "Patel et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Patel et al\\.", "year": 1997}, {"title": "GloVe: Global Vectors for Word Representation", "author": ["J. Pennington", "R. Socher", "C.D. Manning"], "venue": "In Proceedings of the Empiricial Methods in Natural Language Processing (EMNLP 2014),", "citeRegEx": "Pennington et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Pennington et al\\.", "year": 2014}, {"title": "Contextual Correlates of Synonymy", "author": ["H. Rubenstein", "J.B. Goodenough"], "venue": "Communications of the ACM,", "citeRegEx": "Rubenstein and Goodenough,? \\Q1965\\E", "shortCiteRegEx": "Rubenstein and Goodenough", "year": 1965}, {"title": "Semantic Compositionality Through Recursive Matrix-Vector Spaces", "author": ["R. Socher", "B. Huval", "C. Manning", "A. Ng"], "venue": "In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning,", "citeRegEx": "Socher et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Socher et al\\.", "year": 2012}, {"title": "Recursive Deep Models for Semantic Compositionality Over a Sentiment Treebank", "author": ["R. Socher", "A. Perelygin", "J.Y. Wu", "J. Chuang", "C.D. Manning", "Ng", "A. Y", "C. Potts"], "venue": "In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing (EMNLP),", "citeRegEx": "Socher et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Socher et al\\.", "year": 2013}, {"title": "From Frequency to Meaning: Vector Space Models of Semantics", "author": ["P. Turney", "P. Pantel"], "venue": "Journal of Artificial Intelligence Research,", "citeRegEx": "Turney and Pantel,? \\Q2010\\E", "shortCiteRegEx": "Turney and Pantel", "year": 2010}], "referenceMentions": [{"referenceID": 2, "context": ", 1998), others are based on distributed representations (Bengio, 2008; Collobert et al., 2011; Mnih & Kavukcuoglu, 2013; Mikolov et al., 2013b; Lebret & Collobert, 2014; Pennington et al., 2014; Levy & Goldberg, 2014) where the meaning of a word is encoded as a vector computed from co-occurrence statistics of a word and its neighboring words.", "startOffset": 57, "endOffset": 218}, {"referenceID": 8, "context": ", 1998), others are based on distributed representations (Bengio, 2008; Collobert et al., 2011; Mnih & Kavukcuoglu, 2013; Mikolov et al., 2013b; Lebret & Collobert, 2014; Pennington et al., 2014; Levy & Goldberg, 2014) where the meaning of a word is encoded as a vector computed from co-occurrence statistics of a word and its neighboring words.", "startOffset": 57, "endOffset": 218}, {"referenceID": 23, "context": ", 1998), others are based on distributed representations (Bengio, 2008; Collobert et al., 2011; Mnih & Kavukcuoglu, 2013; Mikolov et al., 2013b; Lebret & Collobert, 2014; Pennington et al., 2014; Levy & Goldberg, 2014) where the meaning of a word is encoded as a vector computed from co-occurrence statistics of a word and its neighboring words.", "startOffset": 57, "endOffset": 218}, {"referenceID": 8, "context": "Finally, distributed representations emerged as the solution to many natural language processing (NLP) tasks (Turney & Pantel, 2010; Collobert et al., 2011).", "startOffset": 109, "endOffset": 156}, {"referenceID": 7, "context": "More sophisticated approaches use techniques from logic, category theory, and quantum information (Clark et al., 2008).", "startOffset": 98, "endOffset": 118}, {"referenceID": 12, "context": "Others use the syntactic relations between words to treat certain words as functions and other as arguments such as adjective-noun composition (Baroni & Zamparelli, 2010) or noun-verb composition (Grefenstette et al., 2013).", "startOffset": 196, "endOffset": 223}, {"referenceID": 25, "context": "Recursive neural network model for semantic compositionality has also been proposed (Socher et al., 2012), where each word has a matrixvector representation: the vector captures its meaning (as it is initialized with a pre-trained distributed representation), while the matrix learns throught a parse tree how it modifies the meaning of the other word that it combines ar X iv :1 50 6.", "startOffset": 84, "endOffset": 105}, {"referenceID": 0, "context": "Many recent works are based on distributed representations of phrases to tackle a wide range of application in NLP: machine translation (Bahdanau et al., 2015), constituency parsing (Legrand & Collobert, 2015), sentiment analysis (Socher et al.", "startOffset": 136, "endOffset": 159}, {"referenceID": 26, "context": ", 2015), constituency parsing (Legrand & Collobert, 2015), sentiment analysis (Socher et al., 2013), or image captioning (Lebret et al.", "startOffset": 78, "endOffset": 99}, {"referenceID": 13, "context": ", 2013), or image captioning (Lebret et al., 2015).", "startOffset": 29, "endOffset": 50}, {"referenceID": 23, "context": "We compare our model against other state-of-the-art methods for distributed word representations which capture meaningful linear substructures (Mikolov et al., 2013a; Pennington et al., 2014).", "startOffset": 143, "endOffset": 191}, {"referenceID": 9, "context": "The context can be documents, such as in latent semantic analysis (LSA) (Deerwester et al., 1990); or other words (Lund & Burgess, 1996).", "startOffset": 72, "endOffset": 97}, {"referenceID": 23, "context": "Instead of using the co-occurrence probabilities, (Pennington et al., 2014) suggest that word vector representations should be learnt with ratios of co-occurrence probabilities.", "startOffset": 50, "endOffset": 75}, {"referenceID": 3, "context": "The predictive-based model has first been introduced as a neural probabilistic language model (Bengio et al., 2003).", "startOffset": 94, "endOffset": 115}, {"referenceID": 8, "context": "(Collobert et al., 2011) train a language model to discriminate a two-class classification task: if the word in the middle of the input window is related to its context or not.", "startOffset": 0, "endOffset": 24}, {"referenceID": 2, "context": "The predictive-based model has first been introduced as a neural probabilistic language model (Bengio et al., 2003). A neural network architecture is trained to predict the next word given a window of preceding words, where words are representated by low-dimensional vector. Since, some variations of this architecture have been proposed. (Collobert et al., 2011) train a language model to discriminate a two-class classification task: if the word in the middle of the input window is related to its context or not. More recently, the need of full neural architectures has been questioned (Mnih & Kavukcuoglu, 2013; Mikolov et al., 2013a). Mikolov et al. (2013a) propose two predictivebased log-linear models for learning distributed representations of words: (i) the continous bag-of-words model (CBOW), where the objective is to correctly classify the current (middle) word given a symmetric window of context words around it; (ii) the skip-gram model, where instead of predicting the current word based on the context, it tries to maximize classification of a word based on another word in the same sentence.", "startOffset": 95, "endOffset": 663}, {"referenceID": 2, "context": "The predictive-based model has first been introduced as a neural probabilistic language model (Bengio et al., 2003). A neural network architecture is trained to predict the next word given a window of preceding words, where words are representated by low-dimensional vector. Since, some variations of this architecture have been proposed. (Collobert et al., 2011) train a language model to discriminate a two-class classification task: if the word in the middle of the input window is related to its context or not. More recently, the need of full neural architectures has been questioned (Mnih & Kavukcuoglu, 2013; Mikolov et al., 2013a). Mikolov et al. (2013a) propose two predictivebased log-linear models for learning distributed representations of words: (i) the continous bag-of-words model (CBOW), where the objective is to correctly classify the current (middle) word given a symmetric window of context words around it; (ii) the skip-gram model, where instead of predicting the current word based on the context, it tries to maximize classification of a word based on another word in the same sentence. In Mikolov et al. (2013b), the authors also introduce a data-driven approach for learning phrases, where the phrases are treated as individual tokens during the training.", "startOffset": 95, "endOffset": 1138}, {"referenceID": 25, "context": ", 2013b; Lebret & Collobert, 2014), while others have proposed models to compose these word representations (Mitchell & Lapata, 2010; Socher et al., 2012).", "startOffset": 108, "endOffset": 154}, {"referenceID": 11, "context": "\u201cYou shall know a word by the company it keeps\u201d (Firth, 1957).", "startOffset": 48, "endOffset": 61}, {"referenceID": 23, "context": "We compare our distributed representations with other available models for computing vector representations of words: (1) the GloVe model which is also based on cooccurrence statistics of corpora (Pennington et al., 2014)4, (2) the continuous bag-of-words (CBOW) and the skipgram (SG) architectures which learn representations from prediction-based models (Mikolov et al.", "startOffset": 196, "endOffset": 221}, {"referenceID": 18, "context": "It was introduced in Mikolov et al. (2013a) and contains 19,544 such questions, divided into a semantic subset and a syntactic subset.", "startOffset": 21, "endOffset": 44}, {"referenceID": 10, "context": "These include the WordSimilarity-353 Test Collection (WS-353) (Finkelstein et al., 2001), the Rubenstein and Goodenough dataset (RG-65) (Rubenstein & Goodenough, 1965), and the Stanford Rare Word (RW) (Luong et al.", "startOffset": 62, "endOffset": 88}, {"referenceID": 17, "context": ", 2001), the Rubenstein and Goodenough dataset (RG-65) (Rubenstein & Goodenough, 1965), and the Stanford Rare Word (RW) (Luong et al., 2013).", "startOffset": 120, "endOffset": 140}], "year": 2015, "abstractText": "Recently, there has been a lot of effort to represent words in continuous vector spaces. Those representations have been shown to capture both semantic and syntactic information about words. However, distributed representations of phrases remain a challenge. We introduce a novel model that jointly learns word vector representations and their summation. Word representations are learnt using the word co-occurrence statistical information. To embed sequences of words (i.e. phrases) with different sizes into a common semantic space, we propose to average word vector representations. In contrast with previous methods which reported a posteriori some compositionality aspects by simple summation, we simultaneously train words to sum, while keeping the maximum information from the original vectors. We evaluate the quality of the word representations on several classical word evaluation tasks, and we introduce a novel task to evaluate the quality of the phrase representations. While our distributed representations compete with other methods of learning word representations on word evaluations, we show that they give better performance on the phrase evaluation. Such representations of phrases could be interesting for many tasks in natural language processing. All research was conducted at Idiap Research Institute, before Ronan Collobert joined Facebook AI Research.", "creator": "LaTeX with hyperref package"}}}