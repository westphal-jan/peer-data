{"id": "1506.05869", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "19-Jun-2015", "title": "A Neural Conversational Model", "abstract": "Conversational modeling is an important task in natural language understanding and machine intelligence. Although previous approaches exist, they are often restricted to specific domains (e.g., booking an airline ticket) and require hand-crafted rules. In this paper, we present a simple approach for this task which uses the recently proposed sequence to sequence framework. Our model converses by predicting the next sentence given the previous sentence or sentences in a conversation. The strength of our model is that it can be trained end-to-end and thus requires much fewer hand-crafted rules. We find that this straightforward model can generate simple conversations given a large conversational training dataset. Our preliminary suggest that, despite optimizing the wrong objective function, the model is able to extract knowledge from both a domain specific dataset, and from a large, noisy, and general domain dataset of movie subtitles. On a domain-specific IT helpdesk dataset, the model can find a solution to a technical problem via conversations. On a noisy open-domain movie transcript dataset, the model can perform simple forms of common sense reasoning. As expected, we also find that the lack of consistency is a common failure mode of our model.", "histories": [["v1", "Fri, 19 Jun 2015 02:52:23 GMT  (46kb)", "http://arxiv.org/abs/1506.05869v1", "ICML Deep Learning Workshop 2015"], ["v2", "Tue, 23 Jun 2015 22:12:47 GMT  (26kb)", "http://arxiv.org/abs/1506.05869v2", "ICML Deep Learning Workshop 2015"], ["v3", "Wed, 22 Jul 2015 03:29:47 GMT  (28kb)", "http://arxiv.org/abs/1506.05869v3", "ICML Deep Learning Workshop 2015"]], "COMMENTS": "ICML Deep Learning Workshop 2015", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["oriol vinyals", "quoc le"], "accepted": false, "id": "1506.05869"}, "pdf": {"name": "1506.05869.pdf", "metadata": {"source": "META", "title": "A Neural Conversational Model", "authors": ["Oriol Vinyals"], "emails": ["VINYALS@GOOGLE.COM", "QVL@GOOGLE.COM"], "sections": [{"heading": null, "text": "ar Xiv: 150 6.05 869v 1 [cs.C L] 19 Jun 20"}, {"heading": "1. Introduction", "text": "Advances in end-to-end neural network training have led to remarkable advances in many areas, such as speech recognition, computer vision and language processing, and recent work suggests that neural networks can do more than just classification, they can be used to map the workings of the 31st International Conference on Machine Learning, Lille, France, 2015. JMLR: W & CP Volume 37. Copyright 2015 by the author (s). Duplicated structures to other complicated structures. An example of this is the task of mapping a sequence to another sequence that has direct applications in the understanding of natural language (Sutskever et al., 2014). One of the main advantages of this framework is that it requires little feature engineering and domain specificity, while matching or surpassing state-of-the-art conversations, which, in our opinion, allows researchers to work on tasks for which domain knowledge is not readily available, or for tasks that are easy to model."}, {"heading": "2. Related Work", "text": "Our approach is based on recent work suggesting the use of neural networks to map sequences to sequences (Kalchbrenner & Blunsom, 2013; Sutskever et al., 2014; Bahdanau et al., 2014).This framework has also been used for neural machine translation, improving English-French and English-German translation tasks from the WMT '14 dataset (Luong et al., 2014; Jean et al., 2014).It has also been used for other tasks such as parsing (Vinyals et al., 2014a) and image captioning (Vinyals et al., 2014b).Since vanilla RNNs are known to suffer from disappearing gradients, most researchers use variants of the Long Short Term Memory (LSTM) for neuronal networks (Hochreiter & Schmidhuber, 1997)."}, {"heading": "3. Model", "text": "Our approach uses the sequence to sequence the sequence model (seq2seq) described in (Sutskever et al., 2014), which is based on a recursive neural network that reads the input sequence one token at a time and predicts the output sequence one token at a time, including one token at a time. During the training, the model is given the true output sequence so that learning can take place through back propagation, and the model is trained to maximize the cross entropy of the correct sequence based on its context. During the inference, since the true output sequence is not observed, we simply feed the predicted output token as input to predict the next issue. This is a \"greedy\" follow-up approach. A less greedy approach would be to use the beam search and feed multiple candidates to the next step in the previous step."}, {"heading": "4. Datasets", "text": "In our experiments we used two datasets: a cross-domain IT helpdesk dataset for troubleshooting and an open domain dataset for film transcripts. The details of the two datasets are as follows."}, {"heading": "4.1. IT Helpdesk Troubleshooting dataset", "text": "Our initial experiments used a data set extracted from an IT helpdesk chat service for troubleshooting. In this service, customers deal with computer-related problems and a specialist helps them navigate through a solution. Typical interactions (or threads) are 400 words long, and cornering is clearly signaled. Our training set includes 30M tokens, and 3M tokens have been used as validation."}, {"heading": "4.2. OpenSubtitles dataset", "text": "We also experimented with the OpenSubtitles dataset (Tiedemann, 2009), which consists of film conversations in XML format. It contains sentences uttered by characters in movies. In a simple processing step, we removed XML tags and obvious non-conversation text (e.g. hyperlinks) from the dataset. Since the order is not clearly stated, we treated consecutive sentences on the assumption that they were pronounced by different characters. We trained our model to predict the next sentence given to the previous one, and we did so for each sentence (noting that this doubles our dataset size, as each sentence is used for both context and target). Our training and validation split has 62 million records (923 million tokens) as training examples, and the validation set has 26 million records (395 million tokens). The split is such that each sentence in a set of a pair of test sets appears together in the scope of the training set, but the two are not quite large, as opposed to the two sets of subtitles."}, {"heading": "5. Experiments", "text": "In this section we describe the experimental results with the two sets of data. We present a fairly objective measure, namely the perplexity of the model on the test set. We also show some examples of interactions with the system we have trained."}, {"heading": "5.1. IT Helpdesk Troubleshooting experiments", "text": "In fact, it is not that we are in a position to do what we need to do to do it."}, {"heading": "5.2. OpenSubtitles experiments", "text": "We trained a two-layer LSTM with AdaGrad with gradient clipping. Each layer of LSTM is here a limited question of 4096 memory cells, and we built a vocabulary consisting of the most common 100K words. To accelerate the Softmax, we project the memory cells to 2048 linear units before passing the information on to the classification. In convergence, the perplexity of the recurring model is set on validation 17. Our smoothed 5-gram model reaches a perplexity of 28. Interestingly, adding the soft attention mechanisms of (Bahdanau et al., 2014) does not significantly improve the perplexity on neither the training nor the validation of machines. Besides the objective evaluation on perplexity, our simple recurring model often produces plausible answers. Here are a few sample question-answer pairs that allow us to test the capabilities of the model when it is trained."}, {"heading": "6. Discussion", "text": "In this paper, we show that a simple language model based on the seq2seq framework can be used to train a conversation motor. Our modest results show that it can generate simple and basic conversations and extract knowledge from a loud but domain-open dataset. Although the model has obvious limitations, we are surprised that a purely data-driven approach without rules can provide fairly correct answers to many types of questions. However, the model may require significant modifications to be able to conduct realistic conversations. In addition to the many limitations, the lack of a coherent personality makes it difficult for our system to pass the Turing test (Turing, 1950)."}, {"heading": "ACKNOWLEDGMENTS", "text": "We thank Greg Corrado, Andrew Dai, Jeff Dean, Tom Dean, Matthieu Devin, Rajat Monga, Mike Schuster, Noam Shazeer, Ilya Sutskever and the Google Brain team for their help with the project."}], "references": [{"title": "Neural machine translation by jointly learning to align and translate", "author": ["D. Bahdanau", "K. Cho", "Y. Bengio"], "venue": "arXiv preprint arXiv:1409.0473,", "citeRegEx": "Bahdanau et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2014}, {"title": "A neural probabilistic language model", "author": ["Y. Bengio", "R. Ducharme", "P. Vincent", "C. Janvin"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "Bengio et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Bengio et al\\.", "year": 2003}, {"title": "Long short-term memory", "author": ["S. Hochreiter", "J. Schmidhuber"], "venue": "Neural Computation,", "citeRegEx": "Hochreiter and Schmidhuber,? \\Q1997\\E", "shortCiteRegEx": "Hochreiter and Schmidhuber", "year": 1997}, {"title": "On using very large target vocabulary for neural machine", "author": ["S. Jean", "K. Cho", "R. Memisevic", "Y. Bengio"], "venue": null, "citeRegEx": "Jean et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Jean et al\\.", "year": 2007}, {"title": "Speech and language processing", "author": ["D. Jurafsky", "J. Martin"], "venue": "Pearson International,", "citeRegEx": "Jurafsky and Martin,? \\Q2009\\E", "shortCiteRegEx": "Jurafsky and Martin", "year": 2009}, {"title": "Recurrent continuous translation models", "author": ["N. Kalchbrenner", "P. Blunsom"], "venue": "In EMNLP,", "citeRegEx": "Kalchbrenner and Blunsom,? \\Q2013\\E", "shortCiteRegEx": "Kalchbrenner and Blunsom", "year": 2013}, {"title": "Conversational agents", "author": ["J. Lester", "K. Branting", "B. Mott"], "venue": "In Handbook of Internet Computing", "citeRegEx": "Lester et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Lester et al\\.", "year": 2004}, {"title": "Addressing the rare word problem in neural machine translation", "author": ["T. Luong", "I. Sutskever", "Q.V. Le", "O. Vinyals", "W. Zaremba"], "venue": "arXiv preprint arXiv:1410.8206,", "citeRegEx": "Luong et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Luong et al\\.", "year": 2014}, {"title": "Statistical Language Models based on Neural Networks", "author": ["T. Mikolov"], "venue": "PhD thesis, Brno University of Technology,", "citeRegEx": "Mikolov,? \\Q2012\\E", "shortCiteRegEx": "Mikolov", "year": 2012}, {"title": "Recurrent neural network based language model", "author": ["T. Mikolov", "M. Karafi\u00e1t", "L. Burget", "J. Cernock\u1ef3", "S. Khudanpur"], "venue": "In INTERSPEECH,", "citeRegEx": "Mikolov et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2010}, {"title": "Neural responding machine for short-text conversation", "author": ["L. Shang", "Z. Lu", "H. Li"], "venue": "In Proceedings of ACL,", "citeRegEx": "Shang et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Shang et al\\.", "year": 2015}, {"title": "A neural network approach to context-sensitive generation of conversational responses", "author": ["A. Sordoni", "M. Galley", "M. Auli", "C. Brockett", "Y. Ji", "M. Mitchell", "J. Gao", "B. Dolan", "Nie", "J.-Y"], "venue": "Proceedings of NAACL,", "citeRegEx": "Sordoni et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Sordoni et al\\.", "year": 2015}, {"title": "Sequence to sequence learning with neural networks", "author": ["I. Sutskever", "O. Vinyals", "Q.V. Le"], "venue": "In NIPS,", "citeRegEx": "Sutskever et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Sutskever et al\\.", "year": 2014}, {"title": "Computing machinery and intelligence", "author": ["A.M. Turing"], "venue": "Mind, pp", "citeRegEx": "Turing,? \\Q1950\\E", "shortCiteRegEx": "Turing", "year": 1950}, {"title": "Grammar as a foreign language", "author": ["O. Vinyals", "L. Kaiser", "T. Koo", "S. Petrov", "I. Sutskever", "G. Hinton"], "venue": "arXiv preprint arXiv:1412.7449,", "citeRegEx": "Vinyals et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Vinyals et al\\.", "year": 2014}, {"title": "Show and tell: A neural image caption generator", "author": ["O. Vinyals", "A. Toshev", "S. Bengio", "D. Erhan"], "venue": "arXiv preprint arXiv:1411.4555,", "citeRegEx": "Vinyals et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Vinyals et al\\.", "year": 2014}, {"title": "Creating a Dynamic Speech Dialogue", "author": ["T. Will"], "venue": "VDM Verlag Dr,", "citeRegEx": "Will,? \\Q2007\\E", "shortCiteRegEx": "Will", "year": 2007}], "referenceMentions": [{"referenceID": 12, "context": "An example of this is the task of mapping a sequence to another sequence which has direct applications in natural language understanding (Sutskever et al., 2014).", "startOffset": 137, "endOffset": 161}, {"referenceID": 12, "context": "In this work, we experiment with the conversation modeling task by casting it to a task of predicting the next sequence given the previous sequence or sequences using recurrent networks (Sutskever et al., 2014).", "startOffset": 186, "endOffset": 210}, {"referenceID": 12, "context": "Our approach is based on recent work which proposed to use neural networks to map sequences to sequences (Kalchbrenner & Blunsom, 2013; Sutskever et al., 2014; Bahdanau et al., 2014).", "startOffset": 105, "endOffset": 182}, {"referenceID": 0, "context": "Our approach is based on recent work which proposed to use neural networks to map sequences to sequences (Kalchbrenner & Blunsom, 2013; Sutskever et al., 2014; Bahdanau et al., 2014).", "startOffset": 105, "endOffset": 182}, {"referenceID": 7, "context": "provements on the English-French and English-German translation tasks from the WMT\u201914 dataset (Luong et al., 2014; Jean et al., 2014).", "startOffset": 94, "endOffset": 133}, {"referenceID": 1, "context": "Our work is also inspired by the recent success of neural language modeling (Bengio et al., 2003; Mikolov et al., 2010; Mikolov, 2012), which shows that recurrent neural networks are rather effective models for natural language.", "startOffset": 76, "endOffset": 134}, {"referenceID": 9, "context": "Our work is also inspired by the recent success of neural language modeling (Bengio et al., 2003; Mikolov et al., 2010; Mikolov, 2012), which shows that recurrent neural networks are rather effective models for natural language.", "startOffset": 76, "endOffset": 134}, {"referenceID": 8, "context": "Our work is also inspired by the recent success of neural language modeling (Bengio et al., 2003; Mikolov et al., 2010; Mikolov, 2012), which shows that recurrent neural networks are rather effective models for natural language.", "startOffset": 76, "endOffset": 134}, {"referenceID": 11, "context": "(Sordoni et al., 2015) and Shang et al.", "startOffset": 0, "endOffset": 22}, {"referenceID": 10, "context": "(Shang et al., 2015), used recurrent neural networks to model dialogue in short conversations (trained on Twitter-style chats).", "startOffset": 0, "endOffset": 20}, {"referenceID": 6, "context": "However, most of these systems require a rather complicated processing pipeline of many stages (Lester et al., 2004; Will, 2007; Jurafsky & Martin, 2009).", "startOffset": 95, "endOffset": 153}, {"referenceID": 16, "context": "However, most of these systems require a rather complicated processing pipeline of many stages (Lester et al., 2004; Will, 2007; Jurafsky & Martin, 2009).", "startOffset": 95, "endOffset": 153}, {"referenceID": 12, "context": "Our approach makes use of the sequence to sequence (seq2seq) model described in (Sutskever et al., 2014).", "startOffset": 80, "endOffset": 104}, {"referenceID": 0, "context": "Interestingly, adding the soft attention mechanism of (Bahdanau et al., 2014) did not significantly improve the perplexity on neither training or validation sets.", "startOffset": 54, "endOffset": 77}, {"referenceID": 13, "context": "Amongst the many limitations, the lack of a coherent personality makes it difficult for our system to pass the Turing test (Turing, 1950).", "startOffset": 123, "endOffset": 137}], "year": 2015, "abstractText": "Conversational modeling is an important task in natural language understanding and machine intelligence. Although previous approaches exist, they are often restricted to specific domains (e.g., booking an airline ticket) and require handcrafted rules. In this paper, we present a simple approach for this task which uses the recently proposed sequence to sequence framework. Our model converses by predicting the next sentence given the previous sentence or sentences in a conversation. The strength of our model is that it can be trained end-to-end and thus requires much fewer hand-crafted rules. We find that this straightforward model can generate simple conversations given a large conversational training dataset. Our preliminary suggest that, despite optimizing the wrong objective function, the model is able to extract knowledge from both a domain specific dataset, and from a large, noisy, and general domain dataset of movie subtitles. On a domain-specific IT helpdesk dataset, the model can find a solution to a technical problem via conversations. On a noisy open-domain movie transcript dataset, the model can perform simple forms of common sense reasoning. As expected, we also find that the lack of consistency is a common failure mode of our model.", "creator": "LaTeX with hyperref package"}}}