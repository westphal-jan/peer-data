{"id": "1506.06112", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "19-Jun-2015", "title": "The Extreme Value Machine", "abstract": "This paper provides a novel characterization of the max-margin distribution in input space. The use of the statistical Extreme Value Theory (EVT) is introduced for modeling margin distances, allowing us to derive a scalable non-linear model called the Extreme Value Machine (EVM). Without the need for a kernel, the EVM leverages a margin model to estimate the probability of sample inclusion in each class. The EVM selects a near-optimal subset of the training vectors to optimize the gain in terms of points covered versus parameters used. We show that the problem reduces to the NP-hard Set Cover problem which has a provable polynomial time approximation. The resulting machine has comparable closed set accuracy (i.e., when all testing classes are known at training time) to optimized RBF SVMs and exhibits far superior performance in open set recognition (i.e., when unknown classes exist at testing time). In open set recognition performance, the EVM is more accurate and more scalable than the state of the art.", "histories": [["v1", "Fri, 19 Jun 2015 19:04:54 GMT  (507kb,D)", "http://arxiv.org/abs/1506.06112v1", "Submitted to NIPS 2015"], ["v2", "Tue, 12 Jan 2016 00:21:24 GMT  (738kb,D)", "http://arxiv.org/abs/1506.06112v2", null], ["v3", "Wed, 18 May 2016 00:57:06 GMT  (801kb,D)", "http://arxiv.org/abs/1506.06112v3", "Submitted as an IEEE Pattern Analysis and Machine Intelligence (PAMI) short article"], ["v4", "Sun, 21 May 2017 01:47:04 GMT  (3633kb,D)", "http://arxiv.org/abs/1506.06112v4", "Pre-print of a manuscript accepted to the IEEE Transactions on Pattern Analysis and Machine Intelligence (T-PAMI) journal"]], "COMMENTS": "Submitted to NIPS 2015", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["ethan m rudd", "lalit p jain", "walter j scheirer", "terrance e boult"], "accepted": false, "id": "1506.06112"}, "pdf": {"name": "1506.06112.pdf", "metadata": {"source": "CRF", "title": "The Extreme Value Machine", "authors": ["Ethan M. Rudd", "Lalit P. Jain", "Walter J. Scheirer", "Terrance E. Boult"], "emails": [], "sections": [{"heading": "1 Introduction", "text": "This year it is so far that it will be able to ereniegn the aforementioned lcihsrcnlrVo rf\u00fc eid nlrlrVe\u00fce\u00fceegnlrteeaeegnn rf\u00fc eid nlrVeirlteeoiietlrsrteeoiKn rf\u00fc eid nlrVeirlrteeogn."}, {"heading": "2 Input Margin Distributions and Probability of Sample Inclusion", "text": "Instead of defining the maximum margin distribution by assuming a decision limit and taking into account the locations of the samples, we define our Input Margin Distribution (IMD) based on the distribution of distances to hypothesized maximum margin decisions proposed by the samples. (IMD) Although we then calculate our probability of multiple positive samples in a multi-stage sample seting.To formalize these distributions, we leave the x-th distribution patterns in a metric space X with standard distribution models for a single positive point with multi-stage positive samples in a multi-stage seting.To formalize these distributions, we leave the x-th distribution patterns in a metric space X with norm-en. (Let's be the class label for xi-X.) Assume that there is now only one single positive instance x-i for any class."}, {"heading": "3 The Extreme Value Machine", "text": "One could apply the probability estimates in many ways, for example, the weights in an SVM-like algorithm. (However, we have the choice to directly apply Eq.3 and define a near-optimal subset, which we call extreme vectors, to solve another problem.) Not only does finding a subset reduce the calculation cost, but it also reduces the sensitivity to noise (e.g. a mislabeled training model). In the soft SVM case, such regulation is done by introducing a regulation coefficient into the optimization function and selecting the resulting support vectors. However, if we need a different approach to regulation, it will. (Algorithm 1 EVM Training for i = 1 bis | C-Tree)."}, {"heading": "4 Experimental Evaluation", "text": "This year we are dealing with a similar approach to previous years: \"We are dealing with a different approach,\" he said, \"but we are dealing with a different approach.\" (\"We are dealing with a different approach.\") \"We are dealing with a different approach.\" (\"We are dealing with a different approach.\") \"We are dealing with a different approach.\" (\"We are dealing with a different approach.\") \"We are dealing with a different approach.\" (\"We are dealing with a different approach.\") \"(\" We are dealing with a different approach. \")\" We are dealing with a different approach. \"(\" We are dealing with a different approach. \")\" (\"We are dealing with a different approach.\") \"(\" We are dealing with a different approach. \"(\" We are dealing with a different approach. \")\" (\"We are dealing with a different approach.\") \""}, {"heading": "5 Discussion", "text": "This year, it is so far that it is only a matter of time before it is ready, until it is ready."}, {"heading": "Acknowledgments", "text": "This research was funded by the NSF Research Grant IIS-1320956 (Open Vision - Tools for Open Set Computer Vision and Learning)."}], "references": [{"title": "Vc theory of large margin multi-category classifiers", "author": ["Yann Guermeur"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2007}, {"title": "Margin distribution and soft margin", "author": ["John Shawe-Taylor", "Nello Cristianini"], "venue": "Advances in Large Margin Classifiers,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2000}, {"title": "Margin distribution and learning algorithms", "author": ["Ashutosh Garg", "Dan Roth"], "venue": "In Proc. of the Fifteenth Int. Conf. on Machine Learning (ICML),", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2003}, {"title": "How boosting the margin can also boost classifier complexity", "author": ["L. Reyzin", "R.E. Schapire"], "venue": "In Proc. of Int. Conf. on Machine learning,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2006}, {"title": "A kernel method for the optimization of the margin distribution", "author": ["F. Aiolli", "G. Da San Martino", "A. Sperduti"], "venue": "In Artificial Neural Networks-ICANN,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2008}, {"title": "A risk minimization principle for a class of parzen estimators", "author": ["K. Pelckmans", "J. Suykens", "B.D. Moor"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2008}, {"title": "Large margin distribution learning", "author": ["Z.H. Zhou"], "venue": "In Artificial Neural Networks in Pattern Recognition,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2014}, {"title": "K-local hyperplane and convex distance nearest neighbor algorithms", "author": ["Pascal Vincent", "Yoshua Bengio"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2001}, {"title": "Svm maximizing margin in the input space", "author": ["A. Shotaro"], "venue": "In Neural Information Processing,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2002}, {"title": "Separating hypersurfaces of svms in input spaces", "author": ["Xun Liang", "Chao Wang"], "venue": "Pattern Recognition Letters,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2009}, {"title": "An introduction to statistical modeling of extreme values", "author": ["S. Coles"], "venue": null, "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2001}, {"title": "Towards open set recognition", "author": ["W.J. Scheirer", "A. Rocha", "A. Sapkota", "T.E. Boult"], "venue": "IEEE Trans. on Pattern Analysis and Machine Intelligence,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2013}, {"title": "Probability models for open set recognition", "author": ["W.J. Scheirer", "L.P. Jain", "T.E. Boult"], "venue": "IEEE Trans. on Pattern Analysis and Machine Intelligence,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2014}, {"title": "Limiting forms of the frequency distribution of the largest or smallest member of a sample", "author": ["R.A. Fisher", "L.H. Tippett"], "venue": "In Mathematical Proc. of the Cambridge Philosophical Society,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 1928}, {"title": "Statistical Inference Using Extreme Order Statistics", "author": ["J. Pickands"], "venue": "The Annals of Statistics,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 1975}, {"title": "Extreme Value Distributions: Theory and Applications", "author": ["S. Kotz", "S. Nadarajah"], "venue": "World Sci. Pub. Co.,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2001}, {"title": "Meta-recognition: The theory and practice of recognition score analysis", "author": ["W.J. Scheirer", "A. Rocha", "R.J. Micheals", "T.E. Boult"], "venue": "IEEE Trans. on Pattern Analysis and Machine Intelligence,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2011}, {"title": "Extreme bandits", "author": ["Alexandra Carpentier", "Michal Valko"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2014}, {"title": "Data structures and algorithms for nearest neighbor search in general metric spaces", "author": ["Peter N Yianilos"], "venue": "In SODA,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 1993}, {"title": "A tight analysis of the greedy algorithm for set cover", "author": ["Petr Slav\u0131\u0301k"], "venue": "In Proceedings of the twenty-eighth annual ACM symposium on Theory of computing,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 1996}, {"title": "Towards open world recognition", "author": ["A. Bendale", "T.E. Boult"], "venue": "In IEEE Conf. on Computer Vision and Pattern Recognition (CVPR),", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2015}, {"title": "Letter recognition using holland-style adaptive classifiers", "author": ["P.W. Frey", "D.J. Slate"], "venue": "Machine Learning,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 1991}, {"title": "Gradient-based learning applied to document recognition", "author": ["Y. LeCun", "L. Bottou", "Y. Bengio", "P. Haffner"], "venue": "Proc. of the IEEE,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 1998}, {"title": "The amsterdam library of object images", "author": ["J.-M. Geusebroek", "G.J. Burghouts", "A.W.M. Smeulders"], "venue": "Int. Journal of Computer Vision,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2005}, {"title": "LIBSVM: A library for support vector machines", "author": ["C.-C. Chang", "C.-J Lin"], "venue": "ACM Trans. on Intelligent Systems and Technology,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2011}, {"title": "Multiclass from binary: Expanding one-versus-all, one-versus-one and ECOC-based approaches", "author": ["A. Rocha", "S. Goldenstein"], "venue": "IEEE Trans. on Neural Net. and Learning Sys.,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2014}, {"title": "Probabilistic outputs for support vector machines and comparison to regularize likelihood methods", "author": ["J. Platt"], "venue": "Advances in Large Margin Classifiers,", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2000}], "referenceMentions": [{"referenceID": 0, "context": "The multi-class margin is traditionally defined as the distance between the decision boundary and the \u201cclosest\u201d training samples from a different class [1].", "startOffset": 152, "endOffset": 155}, {"referenceID": 1, "context": "The idea of using \u201cmargin distributions\u201d is not new; multiple researchers have explored various definitions and uses of margin distributions, [2, 3, 4, 5, 6], involving techniques such as maximizing the mean or median margin, taking a weighted combination margin, or optimizing the margin mean and variance.", "startOffset": 142, "endOffset": 157}, {"referenceID": 2, "context": "The idea of using \u201cmargin distributions\u201d is not new; multiple researchers have explored various definitions and uses of margin distributions, [2, 3, 4, 5, 6], involving techniques such as maximizing the mean or median margin, taking a weighted combination margin, or optimizing the margin mean and variance.", "startOffset": 142, "endOffset": 157}, {"referenceID": 3, "context": "The idea of using \u201cmargin distributions\u201d is not new; multiple researchers have explored various definitions and uses of margin distributions, [2, 3, 4, 5, 6], involving techniques such as maximizing the mean or median margin, taking a weighted combination margin, or optimizing the margin mean and variance.", "startOffset": 142, "endOffset": 157}, {"referenceID": 4, "context": "The idea of using \u201cmargin distributions\u201d is not new; multiple researchers have explored various definitions and uses of margin distributions, [2, 3, 4, 5, 6], involving techniques such as maximizing the mean or median margin, taking a weighted combination margin, or optimizing the margin mean and variance.", "startOffset": 142, "endOffset": 157}, {"referenceID": 5, "context": "The idea of using \u201cmargin distributions\u201d is not new; multiple researchers have explored various definitions and uses of margin distributions, [2, 3, 4, 5, 6], involving techniques such as maximizing the mean or median margin, taking a weighted combination margin, or optimizing the margin mean and variance.", "startOffset": 142, "endOffset": 157}, {"referenceID": 6, "context": "As recently pointed out by Zhou in [7], \u201cThese arguments, however, are all heuristics without theoretical justification.", "startOffset": 35, "endOffset": 38}, {"referenceID": 7, "context": "As noted by [8]: \u201cIn this setting, the margin being maximized is still the smallest Euclidean distance between the decision surface and the training points, but this time measured in some strange, sometimes infinite dimensional, kernel-induced feature space rather than the original input space.", "startOffset": 12, "endOffset": 15}, {"referenceID": 8, "context": "\u201d Considerable research has demonstrated that a large margin in kernel space does not necessarily translate into a large margin in the input space [9, 10].", "startOffset": 147, "endOffset": 154}, {"referenceID": 9, "context": "\u201d Considerable research has demonstrated that a large margin in kernel space does not necessarily translate into a large margin in the input space [9, 10].", "startOffset": 147, "endOffset": 154}, {"referenceID": 10, "context": "Therefore, the margin distances are extreme values generated from a stochastic sampling process, and with some mild assumptions we show that one can invoke EVT [11] and use it to model the extreme value distribution.", "startOffset": 160, "endOffset": 164}, {"referenceID": 11, "context": "[12, 13] recently formalized the concept of open set risk and its role in open set recognition.", "startOffset": 0, "endOffset": 8}, {"referenceID": 12, "context": "[12, 13] recently formalized the concept of open set risk and its role in open set recognition.", "startOffset": 0, "endOffset": 8}, {"referenceID": 12, "context": "Our results demonstrate that the EVM is generally superior to or competitive with linear and optimized Gaussian RBF SVMs in the closed set regime and offers comparable or better performance relative to the leading open set W-SVM algorithm[13] for open set problems.", "startOffset": 238, "endOffset": 242}, {"referenceID": 13, "context": "To estimate this distribution, we turn to the Fisher-Tippet Theorem [14] also known as the statistical Extreme Value Theory (EVT)2.", "startOffset": 68, "endOffset": 72}, {"referenceID": 15, "context": "Theorem 1 (Fisher-Tippet Theorem [16]:).", "startOffset": 33, "endOffset": 37}, {"referenceID": 15, "context": "This extreme value theorem is widely used in many fields [16], such as manufacturing (e.", "startOffset": 57, "endOffset": 61}, {"referenceID": 16, "context": "EVT has recently been (re)introduced as applying to recognition, machine learning, and computer vision[17, 13, 18].", "startOffset": 102, "endOffset": 114}, {"referenceID": 12, "context": "EVT has recently been (re)introduced as applying to recognition, machine learning, and computer vision[17, 13, 18].", "startOffset": 102, "endOffset": 114}, {"referenceID": 17, "context": "EVT has recently been (re)introduced as applying to recognition, machine learning, and computer vision[17, 13, 18].", "startOffset": 102, "endOffset": 114}, {"referenceID": 16, "context": "Prior work such as [17, 13] used a percentage of the data.", "startOffset": 19, "endOffset": 27}, {"referenceID": 12, "context": "Prior work such as [17, 13] used a percentage of the data.", "startOffset": 19, "endOffset": 27}, {"referenceID": 16, "context": "For this work, we slightly modified the libMR library provided by the authors of [17], which uses Maximum Likelihood Estimation (MLE) to find \u03bai, \u03bbi.", "startOffset": 81, "endOffset": 85}, {"referenceID": 14, "context": ", the second EVT theorem (Pickands-Balkema-de Haan Theorem [15]), which addresses probabilities conditioned on the process exceeding a sufficiently high threshold.", "startOffset": 59, "endOffset": 63}, {"referenceID": 18, "context": "for i = 1 to |C| do Create a VP-Tree ([19]) Ti of negatives for class yi using all xj s.", "startOffset": 38, "endOffset": 42}, {"referenceID": 19, "context": "Theorem 2 in [20] provides the stated complexity upper bound for the greedy algorithm for Set Cover.", "startOffset": 13, "endOffset": 17}, {"referenceID": 12, "context": "1 is monotonically decreasing, it follows that the per-sample \u03a8-model is a compact abating probability (CAP) model as defined by [13] and therefore each \u03a8-model manages open space risk.", "startOffset": 129, "endOffset": 133}, {"referenceID": 20, "context": "Furthermore, by Theorem 1 in [21], thresholding a max-combination of CAP models also manages open space risk.", "startOffset": 29, "endOffset": 33}, {"referenceID": 12, "context": "Figure 2: Multi-class open set recognition accuracy on OLETTER [13].", "startOffset": 63, "endOffset": 67}, {"referenceID": 21, "context": "Therefore, we evaluated performance of the EVM on a variety of standard multi-class benchmarks including LETTER [22], which has 26 classes, 16 features and 20000 instances; MNIST [23] with 10 classes, 785 features and 70000 instances; and the largest test ALOI [24] with 1000 classes, 128 features, and 108000 instances.", "startOffset": 112, "endOffset": 116}, {"referenceID": 22, "context": "Therefore, we evaluated performance of the EVM on a variety of standard multi-class benchmarks including LETTER [22], which has 26 classes, 16 features and 20000 instances; MNIST [23] with 10 classes, 785 features and 70000 instances; and the largest test ALOI [24] with 1000 classes, 128 features, and 108000 instances.", "startOffset": 179, "endOffset": 183}, {"referenceID": 23, "context": "Therefore, we evaluated performance of the EVM on a variety of standard multi-class benchmarks including LETTER [22], which has 26 classes, 16 features and 20000 instances; MNIST [23] with 10 classes, 785 features and 70000 instances; and the largest test ALOI [24] with 1000 classes, 128 features, and 108000 instances.", "startOffset": 261, "endOffset": 265}, {"referenceID": 24, "context": "The comparison algorithms we use for multi-class classification are Gaussian RBF 1-vs-Rest SVMs [25], Gaussian RBF W-SVMs [13] and the Multiclass from Binary (MBAS) [26].", "startOffset": 96, "endOffset": 100}, {"referenceID": 12, "context": "The comparison algorithms we use for multi-class classification are Gaussian RBF 1-vs-Rest SVMs [25], Gaussian RBF W-SVMs [13] and the Multiclass from Binary (MBAS) [26].", "startOffset": 122, "endOffset": 126}, {"referenceID": 25, "context": "The comparison algorithms we use for multi-class classification are Gaussian RBF 1-vs-Rest SVMs [25], Gaussian RBF W-SVMs [13] and the Multiclass from Binary (MBAS) [26].", "startOffset": 165, "endOffset": 169}, {"referenceID": 25, "context": "Prior to our evaluation, the state-of-the-art classification accuracy for any algorithm on the ALOI dataset was \u224893% [26], obtained using MBAS.", "startOffset": 117, "endOffset": 121}, {"referenceID": 26, "context": "The comparison algorithms we use for multi-class open set recognition are the open set-specific W-SVM, which is currently the best performing algorithm in the literature for this class of problems, as well as Nearest Neighbor (NN) classifiers, 1-vs-Rest RBF SVMs with Platt\u2019s probability estimator [27], Pairwise RBF SVMs with Platt probability calibration, 1-vs-Rest RBF SVMs, and Logistic Regression.", "startOffset": 298, "endOffset": 302}, {"referenceID": 12, "context": "We use an open source libsvm extension provided by [13] for both W-SVM and 1-vs-rest SVM with Platt calibration.", "startOffset": 51, "endOffset": 55}, {"referenceID": 12, "context": "[13] define this threshold with respect to a measure that they call \u201copenness.", "startOffset": 0, "endOffset": 4}, {"referenceID": 12, "context": "[13], then we created a new multi-class open set benchmark for ALOI dubbed \u201cOALOI\u201d.", "startOffset": 0, "endOffset": 4}, {"referenceID": 12, "context": "[13], we created the OLETTER multi-class open set benchmark by randomly selecting 15 distinct labels from the known classes during training and adding unknown classes by incrementally including subsets of the remaining 11 labels during testing.", "startOffset": 0, "endOffset": 4}], "year": 2015, "abstractText": "This paper provides a novel characterization of the max-margin distribution in input space. The use of the statistical Extreme Value Theory (EVT) is introduced for modeling margin distances, allowing us to derive a scalable non-linear model called the Extreme Value Machine (EVM). Without the need for a kernel, the EVM leverages a margin model to estimate the probability of sample inclusion in each class. The EVM selects a nearoptimal subset of the training vectors to optimize the gain in terms of points covered versus parameters used. We show that the problem reduces to the NP-hard Set Cover problem which has a provable polynomial time approximation. The resulting machine has comparable closed set accuracy (i.e., when all testing classes are known at training time) to optimized RBF SVMs and exhibits far superior performance in open set recognition (i.e., when unknown classes exist at testing time). In open set recognition performance, the EVM is more accurate and more scalable than the state of the art.", "creator": "LaTeX with hyperref package"}}}