{"id": "1506.06155", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "19-Jun-2015", "title": "CO2 Forest: Improved Random Forest by Continuous Optimization of Oblique Splits", "abstract": "We propose a novel algorithm for optimizing multivariate linear threshold functions as split functions of decision trees to create improved Random Forest classifiers. Standard tree induction methods resort to sampling and exhaustive search to find good univariate split functions. In contrast, our method computes a linear combination of the features at each node, and optimizes the parameters of the linear combination (oblique) split functions by adopting a variant of latent variable SVM formulation. We develop a convex-concave upper bound on the classification loss for a one-level decision tree, and optimize the bound by stochastic gradient descent at each internal node of the tree. Forests of up to 1000 Continuously Optimized Oblique (CO2) decision trees are created, which significantly outperform Random Forest with univariate splits and previous techniques for constructing oblique trees. Experimental results are reported on multi-class classification benchmarks and on Labeled Faces in the Wild (LFW) dataset.", "histories": [["v1", "Fri, 19 Jun 2015 20:42:47 GMT  (2002kb,D)", "https://arxiv.org/abs/1506.06155v1", null], ["v2", "Wed, 24 Jun 2015 21:23:43 GMT  (2003kb,D)", "http://arxiv.org/abs/1506.06155v2", null]], "reviews": [], "SUBJECTS": "cs.LG cs.CV", "authors": ["mohammad norouzi", "maxwell d collins", "david j fleet", "pushmeet kohli"], "accepted": false, "id": "1506.06155"}, "pdf": {"name": "1506.06155.pdf", "metadata": {"source": "CRF", "title": "CO2 Forest: Improved Random Forest by Continuous Optimization of Oblique Splits", "authors": ["Mohammad Norouzi", "Maxwell D. Collins", "David J. Fleet", "Pushmeet Kohli"], "emails": ["fleet}@cs.toronto.edu", "mcollins@cs.wisc.edu", "pkohli@microsoft.com"], "sections": [{"heading": null, "text": "Index terms - decision trees, random forests, oblique splits, ramp losses"}, {"heading": "1 INTRODUCTION", "text": "In fact, it is the case that most people are able to determine themselves what they want and what they don't want. In fact, it is the case that most people are able to determine themselves. In fact, it is the case that most people are able to determine themselves. In fact, it is the case that most people are able to determine themselves. In fact, it is the case that most people are able to determine themselves. In fact, it is the case that most people are able to determine themselves. In fact, it is the case that most people are able to determine themselves what they want. In fact, it is so. In fact, it is the case that most people are able to determine themselves."}, {"heading": "2 RELATED WORK", "text": "It is indeed the case that we are able to move to another world, in which we move to another world."}, {"heading": "3 PRELIMINARIES", "text": "For the simplicity of exposure, we focus on binary classification trees, with m internal (split) nodes and m + 1 leaf (terminal) nodes. [1] Each internal node is directed from the root of the tree down to a leaf node, which determines distribution via k class labels. [2] Each internal node indexed by i [3] performs a binary test by evaluating a node-specific split function ti (x): Rp \u2192 1, + 1} If ti (x) is rated to \u2212 1, then x is directed to the left child of node i. Otherwise, x is directed to the right child. And so to the tree. Each split function ti (\u00b7), parameterized by a weight vector wi, is assumed to be a linear threshold function of the form ti (x) = sgn."}, {"heading": "4 CONTINUOUS OPTIMIZATION OF OBLIQUE (CO2) DECISION STUMPS", "text": "A binary decision stump is parameterized by a weight vector w, and two vectors of non-normalized log probabilities for the two leaf nodes, \u03b80 and \u03b81. The loss function of the stump comprises two terms, one for each leaf, designated as \"(\u03b80, y) and\" (\u03b81, y), where \": Rk \u00b7 {1,.., k} \u2192 R +. They measure the discrepancy between the label y and the distribution parameters parameterized by \u03b80 and \u03b81. The binary test at the root of the stump acts as a gating function to select a leaf and its associated loss. The empirical loss for the stump, i.e. the sum of the loss over the formation set D, is narrowly defined as L (w, \u03b80,1; D) = implemented model (x, y).D 1 (wTx < 0).\""}, {"heading": "4.1 Upper Bound on Empirical Loss", "text": "(5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5)) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5"}, {"heading": "4.2 Convex-Concave Optimization", "text": "While it is still difficult, it is important that L \u00b2 (w \u00b2, 2; D \u00b2) is better than empirical loss. It is piecewise smooth and convex-concavant in w, and the constraint on w defines a convex-based optimization, although the surrogate target is not differentiable at isolated points. The lens also depends on the blade parameters, and the constraint on w defines a convex-based optimization, which we restrict only by the loss terms', which we restrict to be convex-based. Therefore, it follows that L \u00b2 (w, 0; D) convex in the convex-convex method is that the convex-convex objectivity allows us to use difference in convex-convex programming."}, {"heading": "5 IMPLEMENTATION AND EXPERIMENTAL DETAILS", "text": "In all tree construction methods considered here, we grow each decision tree as deep as possible until we reach a pure CO2 q level. We use the dead-end learning algorithm [4] to design the forest so that each tree is built using a new, uniformly sampled dataset from the original forest dataset. In Random Forest, to determine each univariate split function, we consider only one candidate set the size of q random trait dimensions, where q is the only hyperparameter in our random forest implementation. We set the parameter q by growing a forest of 1000 trees and test it on a hold-out validation set the size of 20% of the training set. Let's specify the dimensionality of the characteristic descriptors. We select q from the candidate set of {p0.5, p0.6, p0.7, p0.8, p0.9} we use the authors \"hold-out validation set to enhance the minimum validation parameters we have provided to optimize their O1, some of the earlier use of O1, some of which suggests uniqueness."}, {"heading": "6 EXPERIMENTS", "text": "Before presenting the classification results, we examine the effects of the hyperparameter \u03bd on our oblique decision trees. Figure 1 shows training and validation error rates for the MNIST dataset for different values of \u03bd {0,1, 1, 10, 100} and different tree depths. It can be seen that as the tree depth increases, the error rate in training decreases monotonously. However, the error rate in validation at a certain depth, e.g. a depth of 10 for MNIST, saturates. The growth of the trees deeper beyond this point either has no effect or slightly harms performance. The diagrams show that \u03bd = 10 has the best training and validation error rates. The difference between the different values of the constants appears to be greater for the validation error. As shown in the equation above (8), the upper limit gets tighter as the upper limit gets tighter. Thus, one might assume that greater constants imply better and better training error rates."}, {"heading": "6.1 UCI multi-class benchmarks", "text": "We conduct experiments with nine UCI multi-class benchmarks, namely SatImage, USPS, Pendigits, Letter, Protein, Connect4, MNIST, SensIT, Covertype. Table 1 provides a summary of the data sets, including the number of training and test points, the number of class names, and the dimensionality of the characteristics. We use the training and test splits specified by previous work, with the exception of Connect4 and Covertype. Further details of the data sets, including references to the corresponding publications, can be found on the LIBSVM dataset page. [2] Test error rates for random forests, OC1 forests, and CO2 forests with varying numbers of trees (10, 30, 1000) are presented in Table 1. OC1 results are not presented on some datasets because the derivative-free dataset method of 30 datasets is used."}, {"heading": "6.2 Labeled Faces in the Wild (LFW)", "text": "In fact, it is true that this is a way in which people are able to survive on their own. \"(S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S."}, {"heading": "7 CONCLUSION", "text": "Although the information gain criteria used to generate decision trees are discontinuous and difficult to optimize, we propose a continuous upper limit based on the development of oblique decision trees. Unlike OC1 trees, which cause problems with high-dimensional inputs and large training sets that greatly improve classification benchmarks over a random forest baseline and previous methods of building oblique decision trees, our method is designed to address problems with high-dimensional inputs and large training sets common in computer vision and machine learning. Our framework is straightforward to generalize other tasks such as regression or structured forecasting, and applies to any form of convex losure.8 Input Ground Truth Axis-aligned."}], "references": [{"title": "A support vector machine approach to decision trees", "author": ["K.P. Bennett", "J.A. Blue"], "venue": "IJCNN", "citeRegEx": "3", "shortCiteRegEx": null, "year": 1998}, {"title": "Bagging predictors", "author": ["L. Breiman"], "venue": "Machine learning, 24(2)", "citeRegEx": "4", "shortCiteRegEx": null, "year": 1996}, {"title": "Random forests", "author": ["L. Breiman"], "venue": "Machine Learning, 45(1):5\u201332", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2001}, {"title": "Classification and regression trees", "author": ["L. Breiman", "J. Friedman", "R.A. Olshen", "C.J. Stone"], "venue": "Chapman & Hall/CRC", "citeRegEx": "6", "shortCiteRegEx": null, "year": 1984}, {"title": "Decision Forests for Computer Vision and Medical Image Analysis", "author": ["A. Criminisi", "J. Shotton"], "venue": "Springer", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2013}, {"title": "Narrowing the gap: Random forests in theory and in practice", "author": ["M. Denil", "D. Matheson", "N. De Freitas"], "venue": "ICML", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2014}, {"title": "Object detection with discriminatively trained part-based models", "author": ["P.F. Felzenszwalb", "R.B. Girshick", "D. McAllester", "D. Ramanan"], "venue": "IEEE Trans. PAMI, pages 1627\u20131645", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2010}, {"title": "Greedy function approximation: a gradient boosting machine", "author": ["J.H. Friedman"], "venue": "Annals of Statistics, pages 1189\u20131232", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2001}, {"title": "The elements of statistical learning (Ed", "author": ["T. Hastie", "R. Tibshirani", "J. Friedman"], "venue": "2). Springer", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2009}, {"title": "Induction of oblique decision trees", "author": ["D. Heath", "S. Kasif", "S. Salzberg"], "venue": "IJCAI", "citeRegEx": "12", "shortCiteRegEx": null, "year": 1993}, {"title": "The random subspace method for constructing decision forests", "author": ["T.K. Ho"], "venue": "IEEE Trans. PAMI, pages 832\u2013844", "citeRegEx": "13", "shortCiteRegEx": null, "year": 1998}, {"title": "Labeled faces in the wild: A database for studying face recognition in unconstrained environments", "author": ["G.B. Huang", "M. Ramesh", "T. Berg", "E. Learned-Miller"], "venue": "Technical Report 07-49, University of Massachusetts, Amherst", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2007}, {"title": "Constructing optimal binary decision trees is NP-complete", "author": ["L. Hyafil", "R.L. Rivest"], "venue": "Information Processing Letters, 5(1):15\u201317", "citeRegEx": "15", "shortCiteRegEx": null, "year": 1976}, {"title": "Hierarchical mixtures of experts and the EM algorithm", "author": ["M.I. Jordan", "R.A. Jacobs"], "venue": "Neural Comput., 6(2):181\u2013214", "citeRegEx": "16", "shortCiteRegEx": null, "year": 1994}, {"title": "GeoF: Geodesic forests for learning coupled predictors", "author": ["P. Kontschieder", "P. Kohli", "J. Shotton", "A. Criminisi"], "venue": "CVPR", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2013}, {"title": "On oblique random forests", "author": ["B.H. Menze", "B.M. Kelm", "D.N. Splitthoff", "U. Koethe", "F.A. Hamprecht"], "venue": "In Machine Learning and Knowledge Discovery in Databases,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2011}, {"title": "A system for induction of oblique decision trees", "author": ["S.K. Murthy", "S. Kasif", "S. Salzberg"], "venue": "Journal of Artificial Intelligence Research", "citeRegEx": "19", "shortCiteRegEx": null, "year": 1994}, {"title": "Minimal Loss Hashing for Compact Binary Codes", "author": ["M. Norouzi", "D.J. Fleet"], "venue": "ICML", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2011}, {"title": "Hamming Distance Metric Learning", "author": ["M. Norouzi", "D.J. Fleet", "R. Salakhutdinov"], "venue": "NIPS", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2012}, {"title": "Decision tree fields", "author": ["S. Nowozin", "C. Rother", "S. Bagon", "T. Sharp", "B. Yao", "P. Kohli"], "venue": "ICCV", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2011}, {"title": "Scikit-learn: Machine learning in Python", "author": ["F. Pedregosa", "G. Varoquaux", "A. Gramfort", "V. Michel", "B. Thirion", "O. Grisel", "M. Blondel", "P. Prettenhofer", "R. Weiss", "V. Dubourg", "J. Vanderplas", "A. Passos", "D. Cournapeau", "M. Brucher", "M. Perrot", "E. Duchesnay"], "venue": "JMLR, pages 2825\u20132830", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2011}, {"title": "Induction of decision trees", "author": ["J.R. Quinlan"], "venue": "Machine learning", "citeRegEx": "24", "shortCiteRegEx": null, "year": 1986}, {"title": "Neural decision forests for semantic image labelling", "author": ["S. Rota Bul\u00f3", "P. Kontschieder"], "venue": "CVPR", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2014}, {"title": "Efficient human pose estimation from single depth images", "author": ["J. Shotton", "R. Girshick", "A. Fitzgibbon", "T. Sharp", "M. Cook", "M. Finocchio", "R. Moore", "P. Kohli", "A. Criminisi", "A. Kipman", "A. Blake"], "venue": "IEEE Trans. PAMI", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2013}, {"title": "Decision jungles: Compact and rich models for classification", "author": ["J. Shotton", "T. Sharp", "P. Kohli", "S. Nowozin", "J. Winn", "A. Criminisi"], "venue": "In NIPS,", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2013}, {"title": "Exemplarbased face parsing", "author": ["B.M. Smith", "L. Zhang", "J. Brandt", "Z. Lin", "J. Yang"], "venue": "CVPR, pages 3484\u20133491", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2013}, {"title": "Margin trees for high-dimensional classification", "author": ["R. Tibshirani", "T. Hastie"], "venue": "JMLR, 8", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2007}, {"title": "Support vector machine learning for interdependent and structured output spaces", "author": ["I. Tsochantaridis", "T. Hofmann", "T. Joachims", "Y. Altun"], "venue": "ICML", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2004}, {"title": "Learning structural SVMs with latent variables", "author": ["C.-N.J. Yu", "T. Joachims"], "venue": "ICML", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2009}, {"title": "The concave-convex procedure", "author": ["A.L. Yuille", "A. Rangarajan"], "venue": "Neural Comput., pages 915\u2013936", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2003}, {"title": "Multi-class adaboost", "author": ["J. Zhu", "H. Zou", "S. Rosset", "T. Hastie"], "venue": "Statistics and Its Interface", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2009}], "referenceMentions": [{"referenceID": 3, "context": "Decision trees [6], [24] and random forests [5], [13] have a long, successful history in machine learning, in part due to their computational efficiency and their applicability to large-scale classification and regression tasks (e.", "startOffset": 15, "endOffset": 18}, {"referenceID": 21, "context": "Decision trees [6], [24] and random forests [5], [13] have a long, successful history in machine learning, in part due to their computational efficiency and their applicability to large-scale classification and regression tasks (e.", "startOffset": 20, "endOffset": 24}, {"referenceID": 2, "context": "Decision trees [6], [24] and random forests [5], [13] have a long, successful history in machine learning, in part due to their computational efficiency and their applicability to large-scale classification and regression tasks (e.", "startOffset": 44, "endOffset": 47}, {"referenceID": 10, "context": "Decision trees [6], [24] and random forests [5], [13] have a long, successful history in machine learning, in part due to their computational efficiency and their applicability to large-scale classification and regression tasks (e.", "startOffset": 49, "endOffset": 53}, {"referenceID": 4, "context": ", see [7], [11]).", "startOffset": 6, "endOffset": 9}, {"referenceID": 8, "context": ", see [7], [11]).", "startOffset": 11, "endOffset": 15}, {"referenceID": 23, "context": "A case in point is the Microsoft Kinect, where multiple decision trees are learned on millions of training exemplars to enable real time human pose estimation from depth images [27].", "startOffset": 177, "endOffset": 181}, {"referenceID": 3, "context": "The building block of this procedure is an optimization at each internal node of the tree, which divides the training data at that node into two subsets according to a splitting criterion, such as Gini impurity index in CART [6], or information gain in C4.", "startOffset": 225, "endOffset": 228}, {"referenceID": 19, "context": "This upper bound resembles a ramp loss, and accommodates any convex loss that is useful for multi-class classification, regression, or structured prediction [22].", "startOffset": 157, "endOffset": 161}, {"referenceID": 29, "context": "As explained below, the bound is the difference of two convex terms, the optimization of which is effectively accomplished using the Convex-Concave Procedure of [33].", "startOffset": 161, "endOffset": 165}, {"referenceID": 17, "context": "The proposed bound resembles the bound used for learning binary hash functions [20].", "startOffset": 79, "endOffset": 83}, {"referenceID": 16, "context": "[19] proposed a method called OC1, which yields some performance gains over CART and C4.", "startOffset": 0, "endOffset": 4}, {"referenceID": 11, "context": "As a largescale experiment, we consider the task of segmenting faces from the Labeled Faces in the Wild (LFW) dataset [14].", "startOffset": 118, "endOffset": 122}, {"referenceID": 3, "context": "[6] proposed a version of CART that employs linear combination splits, known as CART-linearcombination (CART-LC).", "startOffset": 0, "endOffset": 3}, {"referenceID": 9, "context": "[12], [19] proposed OC1, a refinement of CART-LC that uses random restarts and random perturbations to escape local minima.", "startOffset": 0, "endOffset": 4}, {"referenceID": 16, "context": "[12], [19] proposed OC1, a refinement of CART-LC that uses random restarts and random perturbations to escape local minima.", "startOffset": 6, "endOffset": 10}, {"referenceID": 28, "context": "Here, by adopting a formulation based on the latent variable SVM [32], our algorithm provides a natural means of regularizing the oblique split stumps, thereby improving the generalization power of the trees.", "startOffset": 65, "endOffset": 69}, {"referenceID": 13, "context": "The hierarchical mixture of experts (HME) [16] uses soft splits rather than hard binary decisions to capture situations where the transition from low to high response is gradual.", "startOffset": 42, "endOffset": 46}, {"referenceID": 2, "context": "Our work builds upon random forest [5].", "startOffset": 35, "endOffset": 38}, {"referenceID": 1, "context": "Random forest combines bootstrap aggregating (bagging) [4] and the random selection of features [13] to construct an ensemble of non-correlated decision trees.", "startOffset": 55, "endOffset": 58}, {"referenceID": 10, "context": "Random forest combines bootstrap aggregating (bagging) [4] and the random selection of features [13] to construct an ensemble of non-correlated decision trees.", "startOffset": 96, "endOffset": 100}, {"referenceID": 5, "context": "The method is used widely for classification and regression tasks, and research still investigates its theoretical characteristics [8].", "startOffset": 131, "endOffset": 134}, {"referenceID": 7, "context": "There also exist boosting based techniques for creating ensembles of decision trees [10], [34].", "startOffset": 84, "endOffset": 88}, {"referenceID": 30, "context": "There also exist boosting based techniques for creating ensembles of decision trees [10], [34].", "startOffset": 90, "endOffset": 94}, {"referenceID": 15, "context": "[18] also consider a variant of oblique random forest.", "startOffset": 0, "endOffset": 4}, {"referenceID": 0, "context": "Like other previous work [3], [30], the technique of [18] is only conveniently applicable to binary classification tasks.", "startOffset": 25, "endOffset": 28}, {"referenceID": 26, "context": "Like other previous work [3], [30], the technique of [18] is only conveniently applicable to binary classification tasks.", "startOffset": 30, "endOffset": 34}, {"referenceID": 15, "context": "Like other previous work [3], [30], the technique of [18] is only conveniently applicable to binary classification tasks.", "startOffset": 53, "endOffset": 57}, {"referenceID": 15, "context": "In contrast to [18], our technique is more general, and allows for optimization of multi-class classification and regression loss functions.", "startOffset": 15, "endOffset": 19}, {"referenceID": 22, "context": "Rota Bul\u00f3 & Kontschieder [26] recently proposed the use of multi-layer neural nets as split functions at internal nodes.", "startOffset": 25, "endOffset": 29}, {"referenceID": 18, "context": ", [21]).", "startOffset": 2, "endOffset": 6}, {"referenceID": 12, "context": "Joint optimization of the split functions and leaf parameters according to a global objective is, however, known to be extremely challenging [15] due to the discrete and sequential nature of the decisions within the tree.", "startOffset": 141, "endOffset": 145}, {"referenceID": 27, "context": "Indeed, like the soft-margin binary SVM formulation, and margin rescaling formulations of structural SVM [31], the norm of w affects the interplay between the upper bound and empirical loss.", "startOffset": 105, "endOffset": 109}, {"referenceID": 29, "context": "The convex-concave nature of the surrogate objective allows us to use difference of convex (DC) programming, or the Convex-Concave Procedure (CCCP) [33], a method for minimizing objective functions expressed as sum of a convex and a concave term.", "startOffset": 148, "endOffset": 152}, {"referenceID": 6, "context": "[9] and Yu & Joachims [32] to optimize latent variable SVM models that employ a similar convex-concave surrogate objective.", "startOffset": 0, "endOffset": 3}, {"referenceID": 28, "context": "[9] and Yu & Joachims [32] to optimize latent variable SVM models that employ a similar convex-concave surrogate objective.", "startOffset": 22, "endOffset": 26}, {"referenceID": 1, "context": "We exploit the bagging ensemble learning algorithm [4] to create the forest such that each tree is built by using a new data set sampled uniformly with replacement from the original dataset.", "startOffset": 51, "endOffset": 54}, {"referenceID": 8, "context": "Some previous work suggests the use of q = \u221a p as a heuristic [11], which is included in the candidate set.", "startOffset": 62, "endOffset": 66}, {"referenceID": 2, "context": "Here, we compare our Continuously Optimized Oblique (CO2) decision forest with random forest [5] and OC1 forest, forest built using OC1 [19].", "startOffset": 93, "endOffset": 96}, {"referenceID": 16, "context": "Here, we compare our Continuously Optimized Oblique (CO2) decision forest with random forest [5] and OC1 forest, forest built using OC1 [19].", "startOffset": 136, "endOffset": 140}, {"referenceID": 20, "context": "Results for random forest are obtained with the implementation of the scikit-learn package [23].", "startOffset": 91, "endOffset": 95}, {"referenceID": 11, "context": "(LFW) dataset [14].", "startOffset": 14, "endOffset": 18}, {"referenceID": 14, "context": "To correct for the class label imbalance, like [17], we subsample training windows so that each label has an equal number of training examples.", "startOffset": 47, "endOffset": 51}, {"referenceID": 23, "context": "This method produces decision forests analogous to [27].", "startOffset": 51, "endOffset": 55}, {"referenceID": 24, "context": "We note that some other tree-like structures [28] and more sophisticated Computer Vision systems built for face segmentation [29] achieve better segmentation accuracy on LFW.", "startOffset": 45, "endOffset": 49}, {"referenceID": 25, "context": "We note that some other tree-like structures [28] and more sophisticated Computer Vision systems built for face segmentation [29] achieve better segmentation accuracy on LFW.", "startOffset": 125, "endOffset": 129}], "year": 2015, "abstractText": "We propose a novel algorithm for optimizing multivariate linear threshold functions as split functions of decision trees to create improved Random Forest classifiers. Standard tree induction methods resort to sampling and exhaustive search to find good univariate split functions. In contrast, our method computes a linear combination of the features at each node, and optimizes the parameters of the linear combination (oblique) split functions by adopting a variant of latent variable SVM formulation. We develop a convex-concave upper bound on the classification loss for a one-level decision tree, and optimize the bound by stochastic gradient descent at each internal node of the tree. Forests of up to 1000 Continuously Optimized Oblique (CO2) decision trees are created, which significantly outperform Random Forest with univariate splits and previous techniques for constructing oblique trees. Experimental results are reported on multi-class classification benchmarks and on Labeled Faces in the Wild (LFW) dataset.", "creator": "LaTeX with hyperref package"}}}