{"id": "1506.06256", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "20-Jun-2015", "title": "Collective Mind, Part II: Towards Performance- and Cost-Aware Software Engineering as a Natural Science", "abstract": "Nowadays, engineers have to develop software often without even knowing which hardware it will eventually run on in numerous mobile phones, tablets, desktops, laptops, data centers, supercomputers and cloud services. Unfortunately, optimizing compilers are not keeping pace with ever increasing complexity of computer systems anymore and may produce severely underperforming executable codes while wasting expensive resources and energy.", "histories": [["v1", "Sat, 20 Jun 2015 15:34:39 GMT  (4858kb,D)", "http://arxiv.org/abs/1506.06256v1", "Presented at the 18th International Workshop on Compilers for Parallel Computing (CPC'15), London, UK"]], "COMMENTS": "Presented at the 18th International Workshop on Compilers for Parallel Computing (CPC'15), London, UK", "reviews": [], "SUBJECTS": "cs.SE cs.LG cs.PF", "authors": ["grigori fursin", "abdul memon", "christophe guillon", "anton lokhmotov"], "accepted": false, "id": "1506.06256"}, "pdf": {"name": "1506.06256.pdf", "metadata": {"source": "CRF", "title": "Collective Mind, Part II: Towards Performance- and Cost-Aware Software Engineering as a Natural Science", "authors": ["Grigori Fursin", "Abdul Memon", "Christophe Guillon", "Anton Lokhmotov"], "emails": [], "sections": [{"heading": null, "text": "We help the software engineering community to gradually monitor all the important features of these devices (computational species) in realistic environments, along with randomly selected optimizations. At the same time, it allows us to monitor the time distribution of the data generated."}, {"heading": "1 Introduction and Related Work", "text": "This year it is so far that it will only be a matter of time before it is so far, until it is so far, until it is so far."}, {"heading": "2 Real-life motivating example", "text": "In fact, most of them will be able to play by the rules they have set for the future."}, {"heading": "3 Public and Open Source Collective Mind Infrastructure and Repos-", "text": "It is not only a question of time, when and where, but also of time, when and in what form and in what form."}, {"heading": "4 Classifying computational species", "text": "It is indeed the case that we are able to engage on a wide range of issues that affect all of us, and that we engage on a wide range of issues that we want to focus on."}, {"heading": "5 Understanding and improving machine learning to predict optimiza-", "text": "This year, it has come to the point where there is only one person who is able to realize that he is able to retaliate."}, {"heading": "6 Learning data set features to enable adaptive software", "text": "Although our approach and methodology help to optimize the development of software, we have not been able to solve such a problem."}, {"heading": "7 Conclusions and Future Work", "text": "In fact, most of them are able to survive on their own if they do not play by the rules they have imposed on themselves."}, {"heading": "8 Acknowledgments", "text": "The presented work was supported by HiPEAC, STMicroelectronics, cTuning foundation, the EU FP7 609491 TETRACOM project and ARM. We thank Ed Plowman (ARM), Marco Cornero (ARM) and Sergey Yakushkin (Synopsys) for interesting feedback and discussions."}, {"heading": "9 Appendix: Collective Knowledge \u2013 a customizable knowledge man-", "text": "This year it is so far that it will only take one year to move on to the next round."}], "references": [{"title": "OCEANS: Optimizing compilers for embedded applications", "author": ["B. Aarts", "et.al"], "venue": "In Proc. Euro-Par 97,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 1997}, {"title": "Petabricks: a language and compiler for algorithmic choice", "author": ["J. Ansel", "C. Chan", "Y.L. Wong", "M. Olszewski", "Q. Zhao", "A. Edelman", "S. Amarasinghe"], "venue": "Proceedings of the 2009 ACM SIGPLAN conference on Programming language design and implementation, PLDI \u201909, pages 38\u201349, New York, NY, USA,", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2009}, {"title": "The landscape of parallel computing research: a view from Berkeley", "author": ["K. Asanovic", "R. Bodik", "B.C. Catanzaro", "J.J. Gebis", "P. Husbands", "K. Keutzer", "D.A. Patterson", "W.L. Plishker", "J. Shalf", "S.W. Williams", "K.A. Yelick"], "venue": "Technical Report UCB/EECS-2006-183, Electrical Engineering and Computer Sciences, University of California at Berkeley, Dec.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2006}, {"title": "The landscape of parallel computing research: a view from Berkeley", "author": ["K. Asanovic et.al"], "venue": "Technical Report UCB/EECS-2006-183, Electrical Engineering and Computer Sciences,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2006}, {"title": "Pattern Recognition and Machine Learning (Information Science and Statistics)", "author": ["C.M. Bishop"], "venue": "Springer, 1st ed. 2006. corr. 2nd printing 2011 edition, Oct.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2007}, {"title": "A practical automatic polyhedral program optimization system", "author": ["U. Bondhugula", "A. Hartono", "J. Ramanujam", "P. Sadayappan"], "venue": "ACM SIGPLAN Conference on Programming Language Design and Implementation (PLDI), June", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2008}, {"title": "Evidence-based static branch prediction using machine learning", "author": ["B. Calder", "D. Grunwald", "M. Jones", "D. Lindsay", "J. Martin", "M. Mozer", "B. Zorn"], "venue": "ACM Transactions on Programming Languages and Systems (TOPLAS),", "citeRegEx": "25", "shortCiteRegEx": null, "year": 1997}, {"title": "Rapidly selecting good compiler optimizations using performance counters", "author": ["J. Cavazos", "G. Fursin", "F. Agakov", "E. Bonilla", "M. O\u2019Boyle", "O. Temam"], "venue": "In Proceedings of the International Symposium on Code Generation and Optimization (CGO),", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2007}, {"title": "Evaluating iterative optimization across 1000 data sets", "author": ["Y. Chen", "Y. Huang", "L. Eeckhout", "G. Fursin", "L. Peng", "O. Temam", "C. Wu"], "venue": "Proceedings of the ACM SIGPLAN Conference on Programming Language Design and Implementation (PLDI), June", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2010}, {"title": "Optimizing for reduced code space using genetic algorithms", "author": ["K. Cooper", "P. Schielke", "D. Subramanian"], "venue": "Proceedings of the Conference on Languages, Compilers, and Tools for Embedded Systems (LCTES), pages 1\u20139,", "citeRegEx": "28", "shortCiteRegEx": null, "year": 1999}, {"title": "Active harmony: towards automated performance tuning", "author": ["C. \u0162\u0103pu\u015f", "I.-H. Chung", "J.K. Hollingsworth"], "venue": "Proceedings of the 2002 ACM/IEEE conference on Supercomputing, Supercomputing \u201902, pages 1\u201311, Los Alamitos, CA, USA,", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2002}, {"title": "The international exascale software project roadmap", "author": ["J. Dongarra et.al"], "venue": "Int. J. High Perform. Comput. Appl.,", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2011}, {"title": "Portable compiler optimization across embedded programs and microarchitectures using machine learning", "author": ["C. Dubach", "T.M. Jones", "E.V. Bonilla", "G. Fursin", "M.F. O\u2019Boyle"], "venue": "In Proceedings of the IEEE/ACM International Symposium on Microarchitecture (MICRO),", "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2009}, {"title": "A test for normality based on the empirical characteristic function", "author": ["T.W. Epps", "L.B. Pulley"], "venue": "Biometrika, 70(3):pp. 723\u2013726,", "citeRegEx": "32", "shortCiteRegEx": null, "year": 1983}, {"title": "Building Watson: An Overview of the DeepQA Project", "author": ["D. Ferrucci et.al"], "venue": "AI Magazine,", "citeRegEx": "33", "shortCiteRegEx": "33", "year": 2010}, {"title": "Probabilistic source-level optimisation of embedded programs", "author": ["B. Franke", "M. O\u2019Boyle", "J. Thomson", "G. Fursin"], "venue": "In Proceedings of the Conference on Languages, Compilers, and Tools for Embedded Systems (LCTES),", "citeRegEx": "34", "shortCiteRegEx": "34", "year": 2005}, {"title": "Collective Tuning Initiative: automating and accelerating development and optimization of computing systems", "author": ["G. Fursin"], "venue": "Proceedings of the GCC Developers\u2019 Summit, June", "citeRegEx": "35", "shortCiteRegEx": null, "year": 2009}, {"title": "MiDataSets: Creating the conditions for a more realistic evaluation of iterative optimization", "author": ["G. Fursin", "J. Cavazos", "M. O\u2019Boyle", "O. Temam"], "venue": "In Proceedings of the International Conference on High Performance Embedded Architectures & Compilers (HiPEAC", "citeRegEx": "36", "shortCiteRegEx": "36", "year": 2007}, {"title": "A practical method for quickly evaluating program optimizations", "author": ["G. Fursin", "A. Cohen", "M. O\u2019Boyle", "O. Temam"], "venue": "In Proceedings of the International Conference on High Performance Embedded Architectures & Compilers (HiPEAC", "citeRegEx": "37", "shortCiteRegEx": "37", "year": 2005}, {"title": "Experience report: community-driven reviewing and validation of publications", "author": ["G. Fursin", "C. Dubach"], "venue": "Proceedings of the 1st Workshop on Reproducible Research Methodologies and New Publication Models in Computer Engineering (ACM SIGPLAN TRUST\u201914). ACM,", "citeRegEx": "38", "shortCiteRegEx": null, "year": 2014}, {"title": "Milepost gcc: Machine learning enabled self-tuning compiler", "author": ["G. Fursin", "Y. Kashnikov", "A.W. Memon", "Z. Chamski", "O. Temam", "M. Namolaru", "E. Yom-Tov", "B. Mendelson", "A. Zaks", "E. Courtois", "F. Bodin", "P. Barnard", "E. Ashton", "E. Bonilla", "J. Thomson", "C. Williams", "M.F.P. OBoyle"], "venue": "International Journal of Parallel Programming, 39:296\u2013327,", "citeRegEx": "39", "shortCiteRegEx": null, "year": 2011}, {"title": "Collective mind: Towards practical and collaborative auto-tuning", "author": ["G. Fursin", "R. Miceli", "A. Lokhmotov", "M. Gerndt", "M. Baboulin", "D. Malony", "Allen", "Z. Chamski", "D. Novillo", "D.D. Vento"], "venue": "Scientific Programming,", "citeRegEx": "40", "shortCiteRegEx": "40", "year": 2014}, {"title": "Evaluating iterative compilation", "author": ["G. Fursin", "M. O\u2019Boyle", "P. Knijnenburg"], "venue": "In Proceedings of the Workshop on Languages and Compilers for Parallel Computers (LCPC),", "citeRegEx": "41", "shortCiteRegEx": "41", "year": 2002}, {"title": "Compiler research: The next 50 years", "author": ["M. Hall", "D. Padua", "K. Pingali"], "venue": "Commun. ACM, 52(2):60\u201367, Feb.", "citeRegEx": "43", "shortCiteRegEx": null, "year": 2009}, {"title": "The Fourth Paradigm: Data-Intensive Scientific Discovery", "author": ["T. Hey", "S. Tansley", "K.M. Tolle", "editors"], "venue": "Microsoft Research,", "citeRegEx": "44", "shortCiteRegEx": "44", "year": 2009}, {"title": "A fast learning algorithm for deep belief nets", "author": ["G.E. Hinton", "S. Osindero"], "venue": "Neural Computation, 18:2006,", "citeRegEx": "45", "shortCiteRegEx": null, "year": 2006}, {"title": "Neural networks and physical systems with emergent collective computational abilities", "author": ["J.J. Hopfield"], "venue": "Proceedings of the National Academy of Sciences, 79(8):2554\u20132558,", "citeRegEx": "46", "shortCiteRegEx": null, "year": 1982}, {"title": "Cole: Compiler optimization level exploration", "author": ["K. Hoste", "L. Eeckhout"], "venue": "Proceedings of the International Symposium on Code Generation and Optimization (CGO),", "citeRegEx": "47", "shortCiteRegEx": null, "year": 2008}, {"title": "Eye, Brain, and Vision (Scientific American Library, No 22)", "author": ["D.H. Hubel"], "venue": "W. H. Freeman, 2nd edition, May", "citeRegEx": "48", "shortCiteRegEx": null, "year": 1995}, {"title": "Care, the comprehensive archiver for reproducible execution", "author": ["Y. Janin", "C. Vincent", "R. Duraffort"], "venue": "Proceedings of the 1st ACM SIGPLAN Workshop on Reproducible Research Methodologies and New Publication Models in Computer Engineering, TRUST \u201914, pages 1:1\u20131:7, New York, NY, USA,", "citeRegEx": "49", "shortCiteRegEx": null, "year": 2014}, {"title": "Predictive runtime code scheduling for heterogeneous architectures", "author": ["V. Jimenez", "I. Gelado", "L. Vilanova", "M. Gil", "G. Fursin", "N. Navarro"], "venue": "Proceedings of the International Conference on High Performance Embedded Architectures & Compilers (HiPEAC 2009), January", "citeRegEx": "50", "shortCiteRegEx": null, "year": 2009}, {"title": "Combined selection of tile sizes and unroll factors using iterative compilation", "author": ["T. Kisuki", "P. Knijnenburg", "M. O\u2019Boyle"], "venue": "In Proceedings of the International Conference on Parallel Architectures and Compilation Techniques (PACT),", "citeRegEx": "51", "shortCiteRegEx": "51", "year": 2000}, {"title": "Finding effective optimization phase sequences", "author": ["P. Kulkarni", "W. Zhao", "H. Moon", "K. Cho", "D. Whalley", "J. Davidson", "M. Bailey", "Y. Paek", "K. Gallivan"], "venue": "Proceedings of the Conference on Languages, Compilers, and Tools for Embedded Systems (LCTES), pages 12\u201323,", "citeRegEx": "52", "shortCiteRegEx": null, "year": 2003}, {"title": "On finding the maxima of a set of vectors", "author": ["H.T. Kung", "F. Luccio", "F.P. Preparata"], "venue": "J. ACM, 22(4):469\u2013476, Oct.", "citeRegEx": "53", "shortCiteRegEx": null, "year": 1975}, {"title": "LLVM: A compilation framework for lifelong program analysis & transformation", "author": ["C. Lattner", "V. Adve"], "venue": "Proceedings of the 2004 International Symposium on Code Generation and Optimization (CGO\u201904), Palo Alto, California, March", "citeRegEx": "54", "shortCiteRegEx": null, "year": 2004}, {"title": "Building high-level features using large scale unsupervised learning", "author": ["Q. Le", "M. Ranzato", "R. Monga", "M. Devin", "K. Chen", "G. Corrado", "J. Dean", "A. Ng"], "venue": "International Conference in Machine Learning,", "citeRegEx": "55", "shortCiteRegEx": null, "year": 2012}, {"title": "Design and implementation of a lightweight dynamic optimization system", "author": ["J. Lu", "H. Chen", "P.-C. Yew", "W.-C. Hsu"], "venue": "Journal of Instruction-Level Parallelism, volume 6,", "citeRegEx": "56", "shortCiteRegEx": null, "year": 2004}, {"title": "Finding representative sets of optimizations for adaptive multiversioning applications", "author": ["L. Luo", "Y. Chen", "C. Wu", "S. Long", "G. Fursin"], "venue": "3rd Workshop on Statistical and Machine Learning Approaches Applied to Architectures and Compilation (SMART\u201909), colocated with HiPEAC\u201909 conference, January", "citeRegEx": "57", "shortCiteRegEx": null, "year": 2009}, {"title": "Seeds: A software engineer\u2019s energy-optimization decision support framework", "author": ["I. Manotas", "L. Pollock", "J. Clause"], "venue": "Proceedings of the 36th International Conference on Software Engineering, ICSE 2014, pages 503\u2013514, New York, NY, USA,", "citeRegEx": "58", "shortCiteRegEx": null, "year": 2014}, {"title": "Contention aware execution: Online contention detection and response", "author": ["J. Mars", "N. Vachharajani", "R. Hundt", "M.L. Soffa"], "venue": "Proceedings of the 8th Annual IEEE/ACM International Symposium on Code Generation and Optimization, CGO \u201910, pages 257\u2013265, New York, NY, USA,", "citeRegEx": "59", "shortCiteRegEx": null, "year": 2010}, {"title": "FFTW: An adaptive software architecture for the FFT", "author": ["F. Matteo", "S. Johnson"], "venue": "Proceedings of the IEEE International Conference on Acoustics, Speech, and Signal Processing, volume 3, pages 1381\u20131384, Seattle, WA, May", "citeRegEx": "60", "shortCiteRegEx": null, "year": 1998}, {"title": "Autotune: A plugin-driven approach to the automatic tuning of parallel applications", "author": ["R. Miceli et.al"], "venue": "In Proceedings of the 11th International Conference on Applied Parallel and Scientific Computing,", "citeRegEx": "61", "shortCiteRegEx": "61", "year": 2013}, {"title": "Automatic generation of program affinity policies using machine learning", "author": ["R.W. Moore", "B.R. Childers"], "venue": "CC, pages 184\u2013203,", "citeRegEx": "62", "shortCiteRegEx": null, "year": 2013}, {"title": "Iterative feedback directed parallelisation using genetic algorithms", "author": ["A. Nisbet"], "venue": "Proceedings of the Workshop on Profile and Feedback Directed Compilation in conjunction with International Conference on Parallel Architectures and Compilation Technique (PACT),", "citeRegEx": "63", "shortCiteRegEx": null, "year": 1998}, {"title": "Algorithmic species: A classification of affine loop nests for parallel programming", "author": ["C. Nugteren", "P. Custers", "H. Corporaal"], "venue": "ACM Transactions on Architecture and Code Optimization, 9(4):Article\u201340,", "citeRegEx": "64", "shortCiteRegEx": null, "year": 2013}, {"title": "Fast and effective orchestration of compiler optimizations for automatic performance tuning", "author": ["Z. Pan", "R. Eigenmann"], "venue": "Proceedings of the International Symposium on Code Generation and Optimization (CGO), pages 319\u2013332,", "citeRegEx": "65", "shortCiteRegEx": null, "year": 2006}, {"title": "Glinda: a framework for accelerating imbalanced applications on heterogeneous platforms", "author": ["J. Shen", "A.L. Varbanescu", "H.J. Sips", "M. Arntzen", "D.G. Simons"], "venue": "Conf. Computing Frontiers, page 14,", "citeRegEx": "66", "shortCiteRegEx": null, "year": 2013}, {"title": "Multiagent Systems: Algorithmic, Game-Theoretic, and Logical Foundations", "author": ["Y. Shoham", "K. Leyton-Brown"], "venue": "Cambridge University Press, New York, NY, USA,", "citeRegEx": "67", "shortCiteRegEx": null, "year": 2008}, {"title": "Learning to predict performance from formula modeling and training data", "author": ["B. Singer", "M. Veloso"], "venue": "Proceedings of the Conference on Machine Learning,", "citeRegEx": "68", "shortCiteRegEx": null, "year": 2000}, {"title": "Meta optimization: Improving compiler heuristics with machine learning", "author": ["M. Stephenson", "S. Amarasinghe", "M. Martin", "U.-M. O\u2019Reilly"], "venue": "In Proceedings of the ACM SIGPLAN Conference on Programming Language Design and Implementation", "citeRegEx": "70", "shortCiteRegEx": "70", "year": 2003}, {"title": "ADAPT: Automated de-coupled adaptive program transformation", "author": ["M. Voss", "R. Eigenmann"], "venue": "Proceedings of International Conference on Parallel Processing,", "citeRegEx": "71", "shortCiteRegEx": null, "year": 2000}, {"title": "Automatically tuned linear algebra software", "author": ["R. Whaley", "J. Dongarra"], "venue": "Proceedings of the Conference on High Performance Networking and Computing,", "citeRegEx": "72", "shortCiteRegEx": null, "year": 1998}], "referenceMentions": [{"referenceID": 51, "context": "They nowadays include hundreds of optimizations and often fail to produce efficient code while wasting expensive resources and energy [72, 18, 20, 43, 40].", "startOffset": 134, "endOffset": 154}, {"referenceID": 0, "context": "They nowadays include hundreds of optimizations and often fail to produce efficient code while wasting expensive resources and energy [72, 18, 20, 43, 40].", "startOffset": 134, "endOffset": 154}, {"referenceID": 2, "context": "They nowadays include hundreds of optimizations and often fail to produce efficient code while wasting expensive resources and energy [72, 18, 20, 43, 40].", "startOffset": 134, "endOffset": 154}, {"referenceID": 23, "context": "They nowadays include hundreds of optimizations and often fail to produce efficient code while wasting expensive resources and energy [72, 18, 20, 43, 40].", "startOffset": 134, "endOffset": 154}, {"referenceID": 21, "context": "They nowadays include hundreds of optimizations and often fail to produce efficient code while wasting expensive resources and energy [72, 18, 20, 43, 40].", "startOffset": 134, "endOffset": 154}, {"referenceID": 51, "context": "Numerous autotuning, run-time adaptation, genetic and machine learning techniques (including our own) have been introduced in the past two decades to help software engineers optimize their applications for rapidly evolving hardware [72, 18, 25, 63, 60, 28, 71, 51, 41, 70, 29, 52, 68, 56, 54, 34, 65, 47, 22, 50, 19, 59, 62, 66, 61, 58].", "startOffset": 232, "endOffset": 336}, {"referenceID": 0, "context": "Numerous autotuning, run-time adaptation, genetic and machine learning techniques (including our own) have been introduced in the past two decades to help software engineers optimize their applications for rapidly evolving hardware [72, 18, 25, 63, 60, 28, 71, 51, 41, 70, 29, 52, 68, 56, 54, 34, 65, 47, 22, 50, 19, 59, 62, 66, 61, 58].", "startOffset": 232, "endOffset": 336}, {"referenceID": 6, "context": "Numerous autotuning, run-time adaptation, genetic and machine learning techniques (including our own) have been introduced in the past two decades to help software engineers optimize their applications for rapidly evolving hardware [72, 18, 25, 63, 60, 28, 71, 51, 41, 70, 29, 52, 68, 56, 54, 34, 65, 47, 22, 50, 19, 59, 62, 66, 61, 58].", "startOffset": 232, "endOffset": 336}, {"referenceID": 43, "context": "Numerous autotuning, run-time adaptation, genetic and machine learning techniques (including our own) have been introduced in the past two decades to help software engineers optimize their applications for rapidly evolving hardware [72, 18, 25, 63, 60, 28, 71, 51, 41, 70, 29, 52, 68, 56, 54, 34, 65, 47, 22, 50, 19, 59, 62, 66, 61, 58].", "startOffset": 232, "endOffset": 336}, {"referenceID": 40, "context": "Numerous autotuning, run-time adaptation, genetic and machine learning techniques (including our own) have been introduced in the past two decades to help software engineers optimize their applications for rapidly evolving hardware [72, 18, 25, 63, 60, 28, 71, 51, 41, 70, 29, 52, 68, 56, 54, 34, 65, 47, 22, 50, 19, 59, 62, 66, 61, 58].", "startOffset": 232, "endOffset": 336}, {"referenceID": 9, "context": "Numerous autotuning, run-time adaptation, genetic and machine learning techniques (including our own) have been introduced in the past two decades to help software engineers optimize their applications for rapidly evolving hardware [72, 18, 25, 63, 60, 28, 71, 51, 41, 70, 29, 52, 68, 56, 54, 34, 65, 47, 22, 50, 19, 59, 62, 66, 61, 58].", "startOffset": 232, "endOffset": 336}, {"referenceID": 50, "context": "Numerous autotuning, run-time adaptation, genetic and machine learning techniques (including our own) have been introduced in the past two decades to help software engineers optimize their applications for rapidly evolving hardware [72, 18, 25, 63, 60, 28, 71, 51, 41, 70, 29, 52, 68, 56, 54, 34, 65, 47, 22, 50, 19, 59, 62, 66, 61, 58].", "startOffset": 232, "endOffset": 336}, {"referenceID": 31, "context": "Numerous autotuning, run-time adaptation, genetic and machine learning techniques (including our own) have been introduced in the past two decades to help software engineers optimize their applications for rapidly evolving hardware [72, 18, 25, 63, 60, 28, 71, 51, 41, 70, 29, 52, 68, 56, 54, 34, 65, 47, 22, 50, 19, 59, 62, 66, 61, 58].", "startOffset": 232, "endOffset": 336}, {"referenceID": 22, "context": "Numerous autotuning, run-time adaptation, genetic and machine learning techniques (including our own) have been introduced in the past two decades to help software engineers optimize their applications for rapidly evolving hardware [72, 18, 25, 63, 60, 28, 71, 51, 41, 70, 29, 52, 68, 56, 54, 34, 65, 47, 22, 50, 19, 59, 62, 66, 61, 58].", "startOffset": 232, "endOffset": 336}, {"referenceID": 49, "context": "Numerous autotuning, run-time adaptation, genetic and machine learning techniques (including our own) have been introduced in the past two decades to help software engineers optimize their applications for rapidly evolving hardware [72, 18, 25, 63, 60, 28, 71, 51, 41, 70, 29, 52, 68, 56, 54, 34, 65, 47, 22, 50, 19, 59, 62, 66, 61, 58].", "startOffset": 232, "endOffset": 336}, {"referenceID": 10, "context": "Numerous autotuning, run-time adaptation, genetic and machine learning techniques (including our own) have been introduced in the past two decades to help software engineers optimize their applications for rapidly evolving hardware [72, 18, 25, 63, 60, 28, 71, 51, 41, 70, 29, 52, 68, 56, 54, 34, 65, 47, 22, 50, 19, 59, 62, 66, 61, 58].", "startOffset": 232, "endOffset": 336}, {"referenceID": 32, "context": "Numerous autotuning, run-time adaptation, genetic and machine learning techniques (including our own) have been introduced in the past two decades to help software engineers optimize their applications for rapidly evolving hardware [72, 18, 25, 63, 60, 28, 71, 51, 41, 70, 29, 52, 68, 56, 54, 34, 65, 47, 22, 50, 19, 59, 62, 66, 61, 58].", "startOffset": 232, "endOffset": 336}, {"referenceID": 48, "context": "Numerous autotuning, run-time adaptation, genetic and machine learning techniques (including our own) have been introduced in the past two decades to help software engineers optimize their applications for rapidly evolving hardware [72, 18, 25, 63, 60, 28, 71, 51, 41, 70, 29, 52, 68, 56, 54, 34, 65, 47, 22, 50, 19, 59, 62, 66, 61, 58].", "startOffset": 232, "endOffset": 336}, {"referenceID": 36, "context": "Numerous autotuning, run-time adaptation, genetic and machine learning techniques (including our own) have been introduced in the past two decades to help software engineers optimize their applications for rapidly evolving hardware [72, 18, 25, 63, 60, 28, 71, 51, 41, 70, 29, 52, 68, 56, 54, 34, 65, 47, 22, 50, 19, 59, 62, 66, 61, 58].", "startOffset": 232, "endOffset": 336}, {"referenceID": 34, "context": "Numerous autotuning, run-time adaptation, genetic and machine learning techniques (including our own) have been introduced in the past two decades to help software engineers optimize their applications for rapidly evolving hardware [72, 18, 25, 63, 60, 28, 71, 51, 41, 70, 29, 52, 68, 56, 54, 34, 65, 47, 22, 50, 19, 59, 62, 66, 61, 58].", "startOffset": 232, "endOffset": 336}, {"referenceID": 15, "context": "Numerous autotuning, run-time adaptation, genetic and machine learning techniques (including our own) have been introduced in the past two decades to help software engineers optimize their applications for rapidly evolving hardware [72, 18, 25, 63, 60, 28, 71, 51, 41, 70, 29, 52, 68, 56, 54, 34, 65, 47, 22, 50, 19, 59, 62, 66, 61, 58].", "startOffset": 232, "endOffset": 336}, {"referenceID": 45, "context": "Numerous autotuning, run-time adaptation, genetic and machine learning techniques (including our own) have been introduced in the past two decades to help software engineers optimize their applications for rapidly evolving hardware [72, 18, 25, 63, 60, 28, 71, 51, 41, 70, 29, 52, 68, 56, 54, 34, 65, 47, 22, 50, 19, 59, 62, 66, 61, 58].", "startOffset": 232, "endOffset": 336}, {"referenceID": 27, "context": "Numerous autotuning, run-time adaptation, genetic and machine learning techniques (including our own) have been introduced in the past two decades to help software engineers optimize their applications for rapidly evolving hardware [72, 18, 25, 63, 60, 28, 71, 51, 41, 70, 29, 52, 68, 56, 54, 34, 65, 47, 22, 50, 19, 59, 62, 66, 61, 58].", "startOffset": 232, "endOffset": 336}, {"referenceID": 30, "context": "Numerous autotuning, run-time adaptation, genetic and machine learning techniques (including our own) have been introduced in the past two decades to help software engineers optimize their applications for rapidly evolving hardware [72, 18, 25, 63, 60, 28, 71, 51, 41, 70, 29, 52, 68, 56, 54, 34, 65, 47, 22, 50, 19, 59, 62, 66, 61, 58].", "startOffset": 232, "endOffset": 336}, {"referenceID": 1, "context": "Numerous autotuning, run-time adaptation, genetic and machine learning techniques (including our own) have been introduced in the past two decades to help software engineers optimize their applications for rapidly evolving hardware [72, 18, 25, 63, 60, 28, 71, 51, 41, 70, 29, 52, 68, 56, 54, 34, 65, 47, 22, 50, 19, 59, 62, 66, 61, 58].", "startOffset": 232, "endOffset": 336}, {"referenceID": 39, "context": "Numerous autotuning, run-time adaptation, genetic and machine learning techniques (including our own) have been introduced in the past two decades to help software engineers optimize their applications for rapidly evolving hardware [72, 18, 25, 63, 60, 28, 71, 51, 41, 70, 29, 52, 68, 56, 54, 34, 65, 47, 22, 50, 19, 59, 62, 66, 61, 58].", "startOffset": 232, "endOffset": 336}, {"referenceID": 42, "context": "Numerous autotuning, run-time adaptation, genetic and machine learning techniques (including our own) have been introduced in the past two decades to help software engineers optimize their applications for rapidly evolving hardware [72, 18, 25, 63, 60, 28, 71, 51, 41, 70, 29, 52, 68, 56, 54, 34, 65, 47, 22, 50, 19, 59, 62, 66, 61, 58].", "startOffset": 232, "endOffset": 336}, {"referenceID": 46, "context": "Numerous autotuning, run-time adaptation, genetic and machine learning techniques (including our own) have been introduced in the past two decades to help software engineers optimize their applications for rapidly evolving hardware [72, 18, 25, 63, 60, 28, 71, 51, 41, 70, 29, 52, 68, 56, 54, 34, 65, 47, 22, 50, 19, 59, 62, 66, 61, 58].", "startOffset": 232, "endOffset": 336}, {"referenceID": 41, "context": "Numerous autotuning, run-time adaptation, genetic and machine learning techniques (including our own) have been introduced in the past two decades to help software engineers optimize their applications for rapidly evolving hardware [72, 18, 25, 63, 60, 28, 71, 51, 41, 70, 29, 52, 68, 56, 54, 34, 65, 47, 22, 50, 19, 59, 62, 66, 61, 58].", "startOffset": 232, "endOffset": 336}, {"referenceID": 38, "context": "Numerous autotuning, run-time adaptation, genetic and machine learning techniques (including our own) have been introduced in the past two decades to help software engineers optimize their applications for rapidly evolving hardware [72, 18, 25, 63, 60, 28, 71, 51, 41, 70, 29, 52, 68, 56, 54, 34, 65, 47, 22, 50, 19, 59, 62, 66, 61, 58].", "startOffset": 232, "endOffset": 336}, {"referenceID": 3, "context": "Practically all recent long-term research visions acknowledge above problem [21, 30, 15, 43, 17].", "startOffset": 76, "endOffset": 96}, {"referenceID": 11, "context": "Practically all recent long-term research visions acknowledge above problem [21, 30, 15, 43, 17].", "startOffset": 76, "endOffset": 96}, {"referenceID": 23, "context": "Practically all recent long-term research visions acknowledge above problem [21, 30, 15, 43, 17].", "startOffset": 76, "endOffset": 96}, {"referenceID": 21, "context": "We then propose to use our recent Collective Mind framework and Hadoop-based repository of knowledge (cM for short) [40, 4] to extract and share such open source software pieces together with various possible inputs and meta data at c-mind.", "startOffset": 116, "endOffset": 123}, {"referenceID": 33, "context": "cM infrastructure then continuously records only the winning solutions (optimizations for a given data set and hardware) that minimize all or only monitored costs (execution time, power consumption, code size, failures, memory and storage footprint, and optimization time) of a given software piece on a Pareto frontier [53] in our public cM repository.", "startOffset": 320, "endOffset": 324}, {"referenceID": 20, "context": "This plugin-based interface is already available in mainline GCC [39], and we plan to add it to LLVM in the future [40].", "startOffset": 65, "endOffset": 69}, {"referenceID": 21, "context": "This plugin-based interface is already available in mainline GCC [39], and we plan to add it to LLVM in the future [40].", "startOffset": 115, "endOffset": 119}, {"referenceID": 4, "context": "optimizations to improve execution time, power consumption or some other characteristics using some off-the-shelf machine learning techniques such as SVM, (deep) neural networks or KNN [23, 45, 55] combined with a few ad-hoc program or architecture features.", "startOffset": 185, "endOffset": 197}, {"referenceID": 25, "context": "optimizations to improve execution time, power consumption or some other characteristics using some off-the-shelf machine learning techniques such as SVM, (deep) neural networks or KNN [23, 45, 55] combined with a few ad-hoc program or architecture features.", "startOffset": 185, "endOffset": 197}, {"referenceID": 35, "context": "optimizations to improve execution time, power consumption or some other characteristics using some off-the-shelf machine learning techniques such as SVM, (deep) neural networks or KNN [23, 45, 55] combined with a few ad-hoc program or architecture features.", "startOffset": 185, "endOffset": 197}, {"referenceID": 24, "context": "In contrast, our growing, large and diverse benchmark allows the community for the first time to apply methodology from sciences such as biology, medicine and AI based on big data predictive analytics [44].", "startOffset": 201, "endOffset": 205}, {"referenceID": 4, "context": "Also, in contrast with using more and more complex and computationally intensive machine learning techniques to predict optimizations such as deep neural networks [23, 45, 55], we decided to provide a new manual option useful for compiler and hardware designers.", "startOffset": 163, "endOffset": 175}, {"referenceID": 25, "context": "Also, in contrast with using more and more complex and computationally intensive machine learning techniques to predict optimizations such as deep neural networks [23, 45, 55], we decided to provide a new manual option useful for compiler and hardware designers.", "startOffset": 163, "endOffset": 175}, {"referenceID": 35, "context": "Also, in contrast with using more and more complex and computationally intensive machine learning techniques to predict optimizations such as deep neural networks [23, 45, 55], we decided to provide a new manual option useful for compiler and hardware designers.", "startOffset": 163, "endOffset": 175}, {"referenceID": 14, "context": "Thus, we are collaboratively building a giant optimization advice web service that links together all shared software species, optimizations and hardware configurations while resembling Wikipedia, IBM Watson advice engine [33], Google knowledge graph [9] and a brain.", "startOffset": 222, "endOffset": 226}, {"referenceID": 18, "context": "We also managed to derive 79 distinct optimization optimization classes covering all shared species (small real applications or hotspot kernels extracted from large applications with their run-time data set either manually as we did in [37], or using Codelet Finder from CAPS Entreprise as we did om the MILEPOST project [39], or using semi-manual extraction of OpenCL/CUDA kernels combined with OpenME plugin interface to extract run-time state [40]) that we correlated with program semantic and dynamic features using SVM and other predictive analytics techniques.", "startOffset": 236, "endOffset": 240}, {"referenceID": 20, "context": "We also managed to derive 79 distinct optimization optimization classes covering all shared species (small real applications or hotspot kernels extracted from large applications with their run-time data set either manually as we did in [37], or using Codelet Finder from CAPS Entreprise as we did om the MILEPOST project [39], or using semi-manual extraction of OpenCL/CUDA kernels combined with OpenME plugin interface to extract run-time state [40]) that we correlated with program semantic and dynamic features using SVM and other predictive analytics techniques.", "startOffset": 321, "endOffset": 325}, {"referenceID": 21, "context": "We also managed to derive 79 distinct optimization optimization classes covering all shared species (small real applications or hotspot kernels extracted from large applications with their run-time data set either manually as we did in [37], or using Codelet Finder from CAPS Entreprise as we did om the MILEPOST project [39], or using semi-manual extraction of OpenCL/CUDA kernels combined with OpenME plugin interface to extract run-time state [40]) that we correlated with program semantic and dynamic features using SVM and other predictive analytics techniques.", "startOffset": 446, "endOffset": 450}, {"referenceID": 17, "context": "1We currently have more than 15000 input samples collected in our past projects for our shared computational species [36, 11, 27].", "startOffset": 117, "endOffset": 129}, {"referenceID": 8, "context": "1We currently have more than 15000 input samples collected in our past projects for our shared computational species [36, 11, 27].", "startOffset": 117, "endOffset": 129}, {"referenceID": 4, "context": "Such networks can mimic brain functions and are often used for machine learning and data mining [23].", "startOffset": 96, "endOffset": 100}, {"referenceID": 26, "context": "For example, Figure 2 shows one of the oldest and well-known one-layer, fully interconnected, recurrent (with feedback connections) Hopfield neural network [46].", "startOffset": 156, "endOffset": 160}, {"referenceID": 28, "context": "It heavily depends on the total number of neurons, connections and layers [48], and is primarily limited by the speed and resources of the available hardware including specialized accelerators.", "startOffset": 74, "endOffset": 78}, {"referenceID": 20, "context": "Furthermore, we decided to perform a simple and well-known optimization compiler flag autotuning [1, 39] with at least 100 iterations to see whether there is still room for improvement over the fastest default compiler optimization level (-O3).", "startOffset": 97, "endOffset": 104}, {"referenceID": 33, "context": "We then gradually track the winning solutions that maximize performance and at the same time minimize all costs using our experience in physics and electronics, namely by applying Pareto frontier filter [53].", "startOffset": 203, "endOffset": 207}, {"referenceID": 13, "context": "4Similar to physics, we execute optimized code many times, check distribution of characteristics for normality [32], and report expected value if variation is less than 3%", "startOffset": 111, "endOffset": 115}, {"referenceID": 16, "context": "At first, we tried to create a simple database of optimizations and connect it to some existing benchmarking and autotuning tools to keep track of all optimizations [35, 39].", "startOffset": 165, "endOffset": 173}, {"referenceID": 20, "context": "At first, we tried to create a simple database of optimizations and connect it to some existing benchmarking and autotuning tools to keep track of all optimizations [35, 39].", "startOffset": 165, "endOffset": 173}, {"referenceID": 21, "context": "It was developed to unify and systematize software autotuning, make it practical and reproducible, and distribute it among numerous computing resources such as mobile phones and data centers shared by volunteers [40, 4].", "startOffset": 212, "endOffset": 219}, {"referenceID": 21, "context": "This allowed us to formalize almost all existing autotuning techniques as finding a function of a behavior of a given software piece B running on a given computer system with a given data set, selected hardware design and software optimization choices c, and a system state s ([40]):", "startOffset": 277, "endOffset": 281}, {"referenceID": 21, "context": "However, in case of further interest, more details can be found in [40, 4]", "startOffset": 67, "endOffset": 74}, {"referenceID": 4, "context": "However, we have already faced a similar problem in natural sciences and AI for many years and managed to effectively tackle it using predictive analytics (statistical analysis, data mining and machine learning) [23, 45, 55, 44].", "startOffset": 212, "endOffset": 228}, {"referenceID": 25, "context": "However, we have already faced a similar problem in natural sciences and AI for many years and managed to effectively tackle it using predictive analytics (statistical analysis, data mining and machine learning) [23, 45, 55, 44].", "startOffset": 212, "endOffset": 228}, {"referenceID": 35, "context": "However, we have already faced a similar problem in natural sciences and AI for many years and managed to effectively tackle it using predictive analytics (statistical analysis, data mining and machine learning) [23, 45, 55, 44].", "startOffset": 212, "endOffset": 228}, {"referenceID": 24, "context": "However, we have already faced a similar problem in natural sciences and AI for many years and managed to effectively tackle it using predictive analytics (statistical analysis, data mining and machine learning) [23, 45, 55, 44].", "startOffset": 212, "endOffset": 228}, {"referenceID": 20, "context": "At the same time, from our practical experience in using machine learning for program optimization and hardware designs [39, 31], we believe that it is not currently possible to fully automate this process due to a practically infinite feature space.", "startOffset": 120, "endOffset": 128}, {"referenceID": 12, "context": "At the same time, from our practical experience in using machine learning for program optimization and hardware designs [39, 31], we believe that it is not currently possible to fully automate this process due to a practically infinite feature space.", "startOffset": 120, "endOffset": 128}, {"referenceID": 20, "context": "Number of species Prediction accuracy 12 from prior work [39] 87% 285 from current work 56%", "startOffset": 57, "endOffset": 61}, {"referenceID": 20, "context": "We also decided to reuse and validate already available semantic and dynamic features from our previous work on machine learning based compiler [39].", "startOffset": 144, "endOffset": 148}, {"referenceID": 20, "context": "For this purpose, we generated and shared in the cM repository a feature vector f for each software species using 56 semantic program features from the MILEPOST GCC [39] (extracted during compilation at -O1 optimization level after pre pass)).", "startOffset": 165, "endOffset": 169}, {"referenceID": 4, "context": "We then passed 79 optimization clusters, 86 features and either all 285 shared species or a small subset of 12 ones used in some of our past works through a standard SVM classifier from R package [23, 16] with full cross-validation as described in [39, 26].", "startOffset": 196, "endOffset": 204}, {"referenceID": 20, "context": "We then passed 79 optimization clusters, 86 features and either all 285 shared species or a small subset of 12 ones used in some of our past works through a standard SVM classifier from R package [23, 16] with full cross-validation as described in [39, 26].", "startOffset": 248, "endOffset": 256}, {"referenceID": 7, "context": "We then passed 79 optimization clusters, 86 features and either all 285 shared species or a small subset of 12 ones used in some of our past works through a standard SVM classifier from R package [23, 16] with full cross-validation as described in [39, 26].", "startOffset": 248, "endOffset": 256}, {"referenceID": 16, "context": "Interestingly, cM approach can also help solve \u201dbig data problem\u201d that we experienced in our first public cTuning framework [35, 39].", "startOffset": 124, "endOffset": 132}, {"referenceID": 20, "context": "Interestingly, cM approach can also help solve \u201dbig data problem\u201d that we experienced in our first public cTuning framework [35, 39].", "startOffset": 124, "endOffset": 132}, {"referenceID": 3, "context": "The computer engineering community has been desperately trying to find some practical ways to automatically improve software performance while reducing power consumption and other usage costs across numerous and rapidly evolving computer systems for several decades [21, 30, 15, 43, 17].", "startOffset": 266, "endOffset": 286}, {"referenceID": 11, "context": "The computer engineering community has been desperately trying to find some practical ways to automatically improve software performance while reducing power consumption and other usage costs across numerous and rapidly evolving computer systems for several decades [21, 30, 15, 43, 17].", "startOffset": 266, "endOffset": 286}, {"referenceID": 23, "context": "The computer engineering community has been desperately trying to find some practical ways to automatically improve software performance while reducing power consumption and other usage costs across numerous and rapidly evolving computer systems for several decades [21, 30, 15, 43, 17].", "startOffset": 266, "endOffset": 286}, {"referenceID": 4, "context": "This, in turn, allows the interdisciplinary community to collaboratively correlate found classes with gradually exposed features from the software, hardware, data sets and environment state either manually or using popular big data predictive analytics [23, 44].", "startOffset": 253, "endOffset": 261}, {"referenceID": 24, "context": "This, in turn, allows the interdisciplinary community to collaboratively correlate found classes with gradually exposed features from the software, hardware, data sets and environment state either manually or using popular big data predictive analytics [23, 44].", "startOffset": 253, "endOffset": 261}, {"referenceID": 37, "context": "Resulting predictive models are then integrated into cM plugins together with several pre-optimized (specialized) versions of a given species that maximize performance and minimize costs across as many inputs, hardware and environment states as possible, as described in [57] Software engineers can now assemble self-tuning applications just like \u201dLEGO\u201d from the shared cM plugins with continuously optimized species.", "startOffset": 271, "endOffset": 275}, {"referenceID": 20, "context": "At the same time, we also shared various features as cM meta-data from our past research on machine learning based optimization including MILEPOST semantic code properties [39], code patterns and control flow graph extracted by our GCC/LLVM Alchemist plugin [40], image and matrix dimensions together with data set sizes from [57], OS parameters, system descriptions, hardware performance counters, CPU frequency and many other.", "startOffset": 172, "endOffset": 176}, {"referenceID": 21, "context": "At the same time, we also shared various features as cM meta-data from our past research on machine learning based optimization including MILEPOST semantic code properties [39], code patterns and control flow graph extracted by our GCC/LLVM Alchemist plugin [40], image and matrix dimensions together with data set sizes from [57], OS parameters, system descriptions, hardware performance counters, CPU frequency and many other.", "startOffset": 258, "endOffset": 262}, {"referenceID": 37, "context": "At the same time, we also shared various features as cM meta-data from our past research on machine learning based optimization including MILEPOST semantic code properties [39], code patterns and control flow graph extracted by our GCC/LLVM Alchemist plugin [40], image and matrix dimensions together with data set sizes from [57], OS parameters, system descriptions, hardware performance counters, CPU frequency and many other.", "startOffset": 326, "endOffset": 330}, {"referenceID": 29, "context": "For this purpose, we plan to use and extend our Interactive Compilation Interface for GCC and LLVM while connecting cM framework with Eclipse IDE [6] to simplify integration of our cM wrappers and performance/cost monitoring plugins with real applications, with Docker [5] and CARE [49] to automatically detect all software dependencies for sharing, and with Phoronix open benchmarking infrastructure [13] to add even more realistic software pieces to our repository.", "startOffset": 282, "endOffset": 286}, {"referenceID": 5, "context": "At the same time, our top-down methodology originating from physics allows the software engineering community benefit from all existing optimizations including powerful polyhedral source-to-source code restructuring and parallelization [24] by gradually adding them to our cM performance tracking and tuning framework [40].", "startOffset": 236, "endOffset": 240}, {"referenceID": 21, "context": "At the same time, our top-down methodology originating from physics allows the software engineering community benefit from all existing optimizations including powerful polyhedral source-to-source code restructuring and parallelization [24] by gradually adding them to our cM performance tracking and tuning framework [40].", "startOffset": 318, "endOffset": 322}, {"referenceID": 47, "context": "Unified \u201dbig data\u201d repository of optimization knowledge also helped us initiate collaboration with AI and physics departments to gradually characterize complex interactions between shared software pieces inside large applications using agent based techniques [67].", "startOffset": 259, "endOffset": 263}, {"referenceID": 44, "context": "For example, we would like to add algorithmic species from [64] to our system and continuously optimize and characterize them.", "startOffset": 59, "endOffset": 63}, {"referenceID": 19, "context": "Finally, we plan to use our public repository to promote and support initiatives on artifact evaluation as a part of reproducible and sustainable software engineering [38].", "startOffset": 167, "endOffset": 171}], "year": 2015, "abstractText": "Nowadays, engineers have to develop software often without even knowing which hardware it will eventually run on in numerous mobile phones, tablets, desktops, laptops, data centers, supercomputers and cloud services. Unfortunately, optimizing compilers are not keeping pace with ever increasing complexity of ever changing computer systems anymore and may produce severely underperforming executable codes while wasting expensive resources and energy. We present the first to our knowledge practical, collaborative and publicly available solution to this problem. We help the software engineering community gradually implement and share light-weight wrappers around any software piece with more than one implementation or optimization choice available. These wrappers are connected with a public Collective Mind autotuning infrastructure and repository of knowledge to continuously monitor all important characteristics of these pieces (computational species) across numerous existing hardware configurations in realistic environments together with randomly selected optimizations. At the same time, Collective Mind Node) allows to easily crowdsource time-consuming autotuning across existing Android-based mobile device including commodity mobile phones and tables. Similar to natural sciences, we can now continuously track all winning solutions (optimizations for a given hardware such as compiler flags, OpenCL/CUDA/OpenMP/MPI/skeleton parameters, number of threads and any other exposed by users) that minimize all costs of a computation (execution time, energy spent, code size, failures, memory and storage footprint, optimization time, faults, contentions, inaccuracy and so on) of a given species on a Pareto frontier along with any unexpected behavior at c-mind.org/repo . Furthermore, the community can continuously classify solutions, prune redundant ones, and correlate them with various features of software, its inputs (data sets) and used hardware either manually (similar to Wikipedia) or using available big data analytics and machine learning techniques. Our approach can also help computer engineering community create the first public, realistic, large, diverse, distributed, representative, and continuously evolving benchmark with related optimization knowledge while gradually covering all possible software and hardware to be able to predict best optimizations and improve compilers depending on usage scenarios and requirements. Such continuously growing collective knowledge accessible via simple web service can become an integral part of the practical software and hardware co-design of self-tuning computer systems as we demonstrate in several real usage scenarios validated in industry..", "creator": "LaTeX with hyperref package"}}}