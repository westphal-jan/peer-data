{"id": "1506.06442", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "22-Jun-2015", "title": "A Deep Memory-based Architecture for Sequence-to-Sequence Learning", "abstract": "We propose Neural Transformation Machine (NTram), a novel architecture for sequence-to-sequence learning, which performs the task through a series of nonlinear transformations from the representation of the input sequence (e.g., a Chinese sentence) to the final output sequence (e.g., translation to English). Inspired by the recent Neural Turing Machines [8], we store the intermediate representations in stacked layers of memories, and use read-write operations on the memories to realize the nonlinear transformations of those representations. Those transformations are designed in advance but the parameters are learned from data. Through layer-by-layer transformations, NTram can model complicated relations necessary for applications such as machine translation between distant languages. The architecture can be trained with normal back-propagation on parallel texts, and the learning can be easily scaled up to a large corpus. NTram is broad enough to subsume the state-of-the-art neural translation model in [2] as its special case, while significantly improves upon the model with its deeper architecture. Remarkably, NTram, being purely neural network-based, can achieve performance comparable to the traditional phrase-based machine translation system (Moses) with a small vocabulary and a modest parameter size.", "histories": [["v1", "Mon, 22 Jun 2015 02:12:54 GMT  (369kb,D)", "http://arxiv.org/abs/1506.06442v1", "12 pages"], ["v2", "Wed, 18 Nov 2015 13:55:44 GMT  (810kb,D)", "http://arxiv.org/abs/1506.06442v2", "12 pages, Under review as a conference paper at ICLR 2016"], ["v3", "Thu, 19 Nov 2015 14:23:34 GMT  (874kb,D)", "http://arxiv.org/abs/1506.06442v3", "12 pages, Under review as a conference paper at ICLR 2016"], ["v4", "Thu, 7 Jan 2016 08:14:08 GMT  (901kb,D)", "http://arxiv.org/abs/1506.06442v4", "13 pages, Under review as a conference paper at ICLR 2016"]], "COMMENTS": "12 pages", "reviews": [], "SUBJECTS": "cs.CL cs.LG cs.NE", "authors": ["fandong meng", "zhengdong lu", "zhaopeng tu", "hang li", "qun liu"], "accepted": false, "id": "1506.06442"}, "pdf": {"name": "1506.06442.pdf", "metadata": {"source": "CRF", "title": "Neural Transformation Machine: A New Architecture for Sequence-to-Sequence Learning", "authors": ["Fandong Meng", "Zhengdong Lu", "Zhaopeng Tu", "Hang Li", "Qun Liu"], "emails": ["liuqun}@ict.ac.cn", "HangLi.HL}@huawei.com"], "sections": [{"heading": "1 Introduction", "text": "In fact, most of the people who are able to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to,"}, {"heading": "2 Read-Write as a Nonlinear Transformation", "text": "We begin by discussing read / write operations between two memory pieces as a generalized form of nonlinear transformation. As illustrated in Figure 1, this transformation consists of two layers of memory (R memory and W memory), read heads, a write head and a controller. Essentially, the controller serves the read heads sequentially to obtain the values from the R memory (\"read\"), which are then sent to the write head to change the values at specific points in the W memory (\"write\"). These basic components are formally defined below, following those in Neural Turing Machines (NTM) [8], but with important modifications to the nesting architecture, implementation efficiency and description simplicity. \u2022 Memory: a memory is generally defined as a matrix with potentially infinite size, while we focus here on pre-defined (pre-claimed) memory writings with N and N locations in each matrix."}, {"heading": "2.1 Controller", "text": "The core of the controller is a state machine implemented as the Long Short-Term Memory RNN (LSTM) [10], with the state at the time denoting t as st. With st, the controller determines the read and write at the time t, while the return of the read in turn participates in the update of the state. For convenience, only one read and write in one step is allowed, but more than one read head is allowed (see, for example, Section 3.1). Now, assuming for a particular instance (index omitted for notation reasons), the system reads from the R memory (master, with Nr units) and writes to the W memory (denotes Mw, with Nw units) R memory: Mr = {xr1, xr2, \u00b7, xrNr}, W memory: M w = {xw1, xwNw}, with xrn units."}, {"heading": "2.2 Addressing for Reading", "text": "Location-based Addressing With location-based address (L-address), the reading is simply rt = x r t. Note that with L-address, the state address automatically runs on a clock determined by the spatial structure of the R-memory. Following this clock, the write head works the same number of times. An important variant, as proposed in [2, 19], is to reverse the automatic addressing by R-memory, where the state address (LSTM) has the same structure but is parameterized differently. 1Note that our definition of writing is slightly different from that in [8]. Content-based Addressing With a content-based address (C-address), the return at t isrt = Fr (Mr, st;) = Nr \u00b2 n = 1 g (st, x r; n) that the font address is slightly different from that in [8]. Content-based Addressing With a content-based address (C-address = Fst) is determined by the state address (x)."}, {"heading": "2.3 Addressing for Writing", "text": "Location-based addressing With location-based addressing (L addressing), writing is simple. At any time, t is updated only to the tallest location in W memory: xWt = vt def = Fw (st; w), which is subsequently retained unchanged. Content-based addressing Similar to C addressing for reading, the location of the units to be written is determined by a gating network g (st, x w n, t; w), where the values in W memory at the time are given t by \u2206 n, t = g (st, x w, t; w) Fw (st; w), x w n, t = (1 \u2212 \u03b1) xwn, t \u2212 1 + \u03b1 \u0432n, t = 1, 2 \u00b7 \u00b7, Nw, where xwn, t stands for the values of the n place in W memory at the time t, \u03b1 is the forgetfulness factor (similarly defined as in [8]), g is the normalized weight (also with a value of N)."}, {"heading": "2.4 Read-Write as a Nonlinear Transformation", "text": "The read / write strategy defined above in this section clearly transforms the representation in the R memory into the representation in the W memory, with the spatial structure 2 embedded in a way that encodes the temporal structure of input and output sequences, collectively through design (in defining the strategy and implementation details) and later supervised learning (in adjusting the parameters). We argue that the memory provides more representative flexibility for encoding sequences with complicated structures, and the transformation introduced above provides more flexibility for sequence-to-sequence learning. As the most \"conventional\" special case, when we use the L address for both reading and writing, we actually get the familiar structure in RNN with stacked layers [16]. In fact, as shown in the figure to the right of the text, this read / write strategy will create a relatively local dependency based on the original spatial order that is read in the R memory."}, {"heading": "3 NTram: Stacking Them Together", "text": "We stack the transformations together to form a deep architecture for sequence-to-sequence learning (called NTram), in a manner similar to the layers in DNNs. The goal of NTram is to learn the representation of sequences that are more suitable for the task (e.g. machine translation) through layer-by-layer transformations. Just as in DNN, we expect that stacking relatively simple transformations greatly increases the expressiveness and efficiency of NTram, especially in handling translations between languages with vastly different syntactic structures (e.g. Chinese and English). As shown in Figure 3, stacking sequences is simple: we can apply one transformation to another, with the W memory in the lower layer being the R memory of the upper layer."}, {"heading": "3.1 Cross-Layer Reading", "text": "In addition to the general read-write strategies in Section 2.4, we are introducing the read-crossword puzzle in NTram to have more flexibility in modeling. In other words, in order to write to any layer, NTram allows reading from any layer that is lower than \"rather than just from layers\" \u2212 1. More specifically, we are looking at the following two traps. Memory bundles: A memory bundle, as shown in Figure 4 (left panel), concatenates the units of two aligned memory when reading, regardless of the addressing strategy. Formally, the ninth position in the memory bundle of layer \"and layer\" \"would normally be the ninth position in the memory bundle of layer (\" + \") n = [(x '\u2032 n) >, (x '\u2032 n) >] >. Since it requires a strict alignment between the memories that need to be merged, memory bundles are usually located on layers that are created with a spatial structure of the same origin (see section 4)."}, {"heading": "3.2 Optimization", "text": "For each designed architecture, the parameters to be optimized for each controller include the parameters for the LSTM in the output layer and the word embedding. Since reading from each memory can only occur after writing, the \"feed-forward\" process can be described in two scales: 1) the flow from the memory of the lower layer to the memory of the higher layer and 2) the formation of a memory in each layer controlled by the corresponding state machine. Accordingly, the flow of the \"correction signal\" in the optimization also spreads on two scales: \u2022 on the \"cross-layer\" scale: the signal begins with the output layer and spreads from higher layers to lower layers, until layer-1 for the coordination of the word embedding; \u2022 on the \"inside layer\" scale: the signal spreads back over time (BPTT) controlled by the corresponding state machine (LSTM)."}, {"heading": "4 NTram: Some Special Cases", "text": "We will discuss four representative special cases of NTram: Arc-I, II, III and IV, as novel, deep architectures for machine translation. We will also show that RNNsearch [2] can be described as a relatively flat case in the context of NTram. Arc-I The first proposal, which initially includes two variants (Arc-Ihyb and Arc-Iloc), is designed to demonstrate the effect of C addressing in cache layers, with the diagram shown in the layer right next to the text. It employs a charge dressing reading from memory layer-1 (the embedding layer), and L addressing writing to layer-2. After that, Arc-Ihyb writes to layer-3 (L addressing) on layer-reading (two reading heads) on layer-2, while Arc-Iloc addressing is used on layer-2."}, {"heading": "4.1 Special Cases: RNNsearch", "text": "Interestingly, RNNsearch [2], the pioneering work of the automatic-alignment neural translation model, can be considered a special case of flat-architecture NTram. Specifically, as shown in the figure to the right of the text, it uses L-addressing reading on memory layer-1 (the embedding layer) and L-addressing writing on level-2, which is then read (C-addressing) by the output layer, to generate the target sequence. Essentially, level-2 is the only intermediate layer created by non-trivial read-write operations."}, {"heading": "5 Experiments", "text": "We report on our empirical study on the application of NTram to translations from Chinese to English and compare it with state-of-the-art NMT models and statistical machine translation models."}, {"heading": "5.1 Setup", "text": "The bilingual training data contains 27.9 million Chinese words and 34.5 million English words. As test sets, we select NIST MT Evaluation test set 2002 (MT02) as development kit, NIST MT Evaluation test sets 2003 (MT03), 2004 (MT04) and 2005 (MT05) as test sets. The number of sentences in NIST MT02, MT03, MT04 and MT05 is 878, 919, 1788 and 1082, respectively. We use the fall-resistant 4-gram NIST BLEU score4 as evaluation yardstick and character test [6] as statistical significance test.Pre-processing: We perform word segmentation for Chinese using http, 919, 1788 and 1082 software."}, {"heading": "5.2 Comparisons to Other Models", "text": "We compare our method with the following models: \u2022 Moses: We use the open source translation system Moses [12] (with standard configuration) as a comparison system of conventional SMT. We obtain word alignments with GIZA + + [15] on the corpora in both directions, applying the strategy of \"grow-diag-final-and\" balance [13]. We use the entire vocabulary of the training data. \u2022 RNNsearch: We also compare NTram with the automatic alignment model proposed in [2]. We use the default setting as in [2], which is called RNsearch (default), as well as the optimal re-calibration of the model (for the sizes of both the embedding and the hidden layers)."}, {"heading": "5.3 Results", "text": "The main results of different models are in Table 1. RNNsearch (best) is on average about 1.7 points behind Moses in BLEU7, which is in line with the observations of others. Moses reported does not include a language model that was trained with a separate monolingual corpus. Authors for various machine translation tasks [2, 11]. Remarkably, some reasonable designs of NTram (e.g. Arc-II) can already achieve performance comparable to Moses, with only 42M parameters, while RNNsearch (best) has 46M parameters. Most NTram architectures (Arc-Ihyb, Arc-II and Arc-III) clearly perform better than the NMT baselines. Among them, Arc-II outperforms the best setting of NMT baseline RNNsearch (best) by an average of 1.5 BLEU with fewer parameters. This indicates that the deeper architectures in NMT help to capture the essential translation engine."}, {"heading": "5.4 Discussion", "text": "Another comparison between Arc-Ihyb and Arc-Iloc (similar parameter sizes) suggests that C-addressing reading plays an important role in learning the powerful transformation between intermediate representations needed for translation between language pairs with very different syntactic structures, a guess confirmed by the good performance of Arc-II and Arc-III, both of which have C-addressing read heads in their clipboard layers. Arc-IV's BLEU values are clearly significantly lower than those of other architectures, while a close analysis shows that the translation results are usually shorter than Arc-I-Arc-III's for about 15%. One possibility is that our specific implementation of C addressing for writing in Arc-IV (Section 4) is difficult to optimize, which may need to be \"guided\" by another write head or intelligent initialization."}, {"heading": "6 Conclusion", "text": "We propose NTram, a novel architecture for sequence-to-sequence learning, which is stimulated by the recent work of Neural Turing Machine [8] and Neural Machine Translation [2]. NTram builds its deep architecture for sequence-to-sequence data processing based on a series of transformations induced by read-write operations on a stack of memory. This new architecture greatly improves the validity of models in sequence-to-sequence learning, as confirmed by our empirical study of machine translation benchmark tasks."}], "references": [{"title": "Joint language and translation modeling with recurrent neural networks", "author": ["M. Auli", "M. Galley", "C. Quirk", "G. Zweig"], "venue": "In Proceedings of EMNLP,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2013}, {"title": "Neural machine translation by jointly learning to align and translate", "author": ["D. Bahdanau", "K. Cho", "Y. Bengio"], "venue": "In Proceedings of ICLR,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2015}, {"title": "A fast and accurate dependency parser using neural networks", "author": ["D. Chen", "C.D. Manning"], "venue": "In Proceedings of EMNLP,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2014}, {"title": "Learning phrase representations using rnn encoder-decoder for statistical machine translation", "author": ["K. Cho", "B. van Merrienboer", "C. Gulcehre", "F. Bougares", "H. Schwenk", "Y. Bengio"], "venue": "In Proceedings of EMNLP,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2014}, {"title": "Empirical evaluation of gated recurrent neural networks on sequence modeling", "author": ["J. Chung", "C. Gulcehre", "K. Cho", "Y. Bengio"], "venue": "In Proceedings of NIPS Deep Learning and Representation Learning Workshop,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2014}, {"title": "Clause restructuring for statistical machine translation", "author": ["M. Collins", "P. Koehn", "I. Ku\u010derov\u00e1"], "venue": "In Proceedings of ACL,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2005}, {"title": "Natural language processing (almost) from scratch", "author": ["R. Collobert", "J. Weston", "L. Bottou", "M. Karlen", "K. Kavukcuoglu", "P. Kuksa"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2011}, {"title": "Neural turing machines", "author": ["A. Graves", "G. Wayne", "I. Danihelka"], "venue": "arXiv preprint arXiv:1410.5401,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2014}, {"title": "Draw: A recurrent neural network for image generation", "author": ["K. Gregor", "I. Danihelka", "A. Graves", "D. Wierstra"], "venue": "arXiv preprint arXiv:1502.04623,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2015}, {"title": "Long short-term memory", "author": ["S. Hochreiter", "J. Schmidhuber"], "venue": "Neural computation,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 1997}, {"title": "On using very large target vocabulary for neural machine translation", "author": ["S. Jean", "K. Cho", "R. Memisevic", "Y. Bengio"], "venue": "arXiv preprint arXiv:1412.2007,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2014}, {"title": "Open source toolkit for statistical machine translation", "author": ["P. Koehn", "H. Hoang", "A. Birch", "C. Callison-Burch", "M. Federico", "N. Bertoldi", "B. Cowan", "W. Shen", "C. Moran", "R. Zens", "C. Dyer", "O. Bojar", "A. Constantin", "E. Herbst. Moses"], "venue": "In Proceedings of ACL on interactive poster and demonstration sessions,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2007}, {"title": "Statistical phrase-based translation", "author": ["P. Koehn", "F.J. Och", "D. Marcu"], "venue": "In Proceedings of NAACL,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2003}, {"title": "Efficient backprop", "author": ["Y. LeCun", "L. Bottou", "G. Orr", "K. Muller"], "venue": "In Neural Networks: Tricks of the trade. Springer,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 1998}, {"title": "A systematic comparison of various statistical alignment models", "author": ["F.J. Och", "H. Ney"], "venue": "Computational linguistics,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2003}, {"title": "How to construct deep recurrent neural networks", "author": ["R. Pascanu", "C. Gulcehre", "K. Cho", "Y. Bengio"], "venue": "In Proceedings of ICLR,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2014}, {"title": "Bidirectional recurrent neural networks", "author": ["M. Schuster", "K.K. Paliwal"], "venue": "Signal Processing, IEEE Transactions on,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 1997}, {"title": "Srilm-an extensible language modeling toolkit", "author": ["A. Stolcke"], "venue": "In Proceedings of ICSLP,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2002}, {"title": "Sequence to sequence learning with neural networks", "author": ["I. Sutskever", "O. Vinyals", "Q.V. Le"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2014}, {"title": "Grammar as a foreign language", "author": ["O. Vinyals", "L. Kaiser", "T. Koo", "S. Petrov", "I. Sutskever", "G. Hinton"], "venue": "arXiv preprint arXiv:1412.7449,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2014}, {"title": "gencnn: A convolutional architecture for word sequence prediction", "author": ["M. Wang", "Z. Lu", "H. Li", "W. Jiang", "Q. Liu"], "venue": "arXiv preprint arXiv:1503.05034,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2015}, {"title": "Adadelta: an adaptive learning rate method", "author": ["M.D. Zeiler"], "venue": "arXiv preprint arXiv:1212.5701,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2012}], "referenceMentions": [{"referenceID": 7, "context": "Inspired by the recent Neural Turing Machines [8], we store the intermediate representations in stacked layers of memories, and use read-write operations on the memories to realize the nonlinear transformations of those representations.", "startOffset": 46, "endOffset": 49}, {"referenceID": 1, "context": "NTram is broad enough to subsume the state-of-the-art neural translation model in [2] as its special case, while significantly improves upon the model with its deeper architecture.", "startOffset": 82, "endOffset": 85}, {"referenceID": 1, "context": "Sequence-to-sequence learning is a fundamental problem in natural language processing, with many important applications such as machine translation [2, 19], part-of-speech tagging [7, 20] and dependency parsing [3, 21].", "startOffset": 148, "endOffset": 155}, {"referenceID": 18, "context": "Sequence-to-sequence learning is a fundamental problem in natural language processing, with many important applications such as machine translation [2, 19], part-of-speech tagging [7, 20] and dependency parsing [3, 21].", "startOffset": 148, "endOffset": 155}, {"referenceID": 6, "context": "Sequence-to-sequence learning is a fundamental problem in natural language processing, with many important applications such as machine translation [2, 19], part-of-speech tagging [7, 20] and dependency parsing [3, 21].", "startOffset": 180, "endOffset": 187}, {"referenceID": 19, "context": "Sequence-to-sequence learning is a fundamental problem in natural language processing, with many important applications such as machine translation [2, 19], part-of-speech tagging [7, 20] and dependency parsing [3, 21].", "startOffset": 180, "endOffset": 187}, {"referenceID": 2, "context": "Sequence-to-sequence learning is a fundamental problem in natural language processing, with many important applications such as machine translation [2, 19], part-of-speech tagging [7, 20] and dependency parsing [3, 21].", "startOffset": 211, "endOffset": 218}, {"referenceID": 20, "context": "Sequence-to-sequence learning is a fundamental problem in natural language processing, with many important applications such as machine translation [2, 19], part-of-speech tagging [7, 20] and dependency parsing [3, 21].", "startOffset": 211, "endOffset": 218}, {"referenceID": 0, "context": "\u2022 Encoder-Decoder: models of this type first summarize the source sentence into a fixedlength vector by the encoder, typically implemented with a recurrent neural network (RNN) or a convolutional neural network (CNN), and then unfold the vector into the target sentence by the decoder, typically implemented with a RNN [1, 4, 19]; \u2217The work is done when the first author worked as intern at Noahs Ark Lab, Huawei Technologies.", "startOffset": 319, "endOffset": 329}, {"referenceID": 3, "context": "\u2022 Encoder-Decoder: models of this type first summarize the source sentence into a fixedlength vector by the encoder, typically implemented with a recurrent neural network (RNN) or a convolutional neural network (CNN), and then unfold the vector into the target sentence by the decoder, typically implemented with a RNN [1, 4, 19]; \u2217The work is done when the first author worked as intern at Noahs Ark Lab, Huawei Technologies.", "startOffset": 319, "endOffset": 329}, {"referenceID": 18, "context": "\u2022 Encoder-Decoder: models of this type first summarize the source sentence into a fixedlength vector by the encoder, typically implemented with a recurrent neural network (RNN) or a convolutional neural network (CNN), and then unfold the vector into the target sentence by the decoder, typically implemented with a RNN [1, 4, 19]; \u2217The work is done when the first author worked as intern at Noahs Ark Lab, Huawei Technologies.", "startOffset": 319, "endOffset": 329}, {"referenceID": 1, "context": "\u2022 Automatic Alignment: with RNNsearch [2] as representative, it represents the source sentence as a sequence of vectors after a processing step (e.", "startOffset": 38, "endOffset": 41}, {"referenceID": 16, "context": ", through a bi-directional RNN [17]), and then simultaneously conducts dynamic alignment through a gating neural network and generation of the target sentence through another RNN.", "startOffset": 31, "endOffset": 35}, {"referenceID": 10, "context": "Empirical comparison between the two schools of methods indicates that the automatic alignment approach is more efficient than the encoder-decoder approach: it can achieve comparable results with far less parameters and training instances [11].", "startOffset": 239, "endOffset": 243}, {"referenceID": 18, "context": "This superiority in efficiency comes mainly from the mechanism of dynamic alignment, which avoids the need to represent the entire source sentence with a fixed-length vector [19].", "startOffset": 174, "endOffset": 178}, {"referenceID": 1, "context": "The dynamic alignment mechanism [2] is intrinsically related to the content-based addressing on an external memory in the recently proposed Neural Turing Machines (NTM) [8].", "startOffset": 32, "endOffset": 35}, {"referenceID": 7, "context": "The dynamic alignment mechanism [2] is intrinsically related to the content-based addressing on an external memory in the recently proposed Neural Turing Machines (NTM) [8].", "startOffset": 169, "endOffset": 172}, {"referenceID": 7, "context": "Similar to the notion of memory in [8], NTram stores the series of intermediate representations with stacked layers of memories, and conducts transformations between those representations with read-write operations on the memories.", "startOffset": 35, "endOffset": 38}, {"referenceID": 1, "context": "NTram naturally takes RNNsearch [2] as a special case with a relatively shallow architecture.", "startOffset": 32, "endOffset": 35}, {"referenceID": 7, "context": "These basic components are more formally defined below, following those in Neural Turing Machines (NTM) [8], with however important modifications for the nesting architecture, implementation efficiency and description simplicity.", "startOffset": 104, "endOffset": 107}, {"referenceID": 7, "context": "As another constraint, reading memory can only be performed after the writing to it completes, following a similar convention as in NTM [8].", "startOffset": 136, "endOffset": 139}, {"referenceID": 9, "context": "The core to the controller is a state machine, implemented as a Long Short-Term Memory RNN (LSTM) [10], with state at time t denoted as st.", "startOffset": 98, "endOffset": 102}, {"referenceID": 1, "context": "One important variant, as suggested in [2, 19], is to go through R-memory backwards after the forward reading pass, where the state machine (LSTM) has the same structure but parameterized differently.", "startOffset": 39, "endOffset": 46}, {"referenceID": 18, "context": "One important variant, as suggested in [2, 19], is to go through R-memory backwards after the forward reading pass, where the state machine (LSTM) has the same structure but parameterized differently.", "startOffset": 39, "endOffset": 46}, {"referenceID": 7, "context": "Note that our definition of writing is slightly different from that in [8].", "startOffset": 71, "endOffset": 74}, {"referenceID": 1, "context": "It is strongly related to the automatic alignment mechanism first introduced in [2] and general attention models discussed in computer vision [9].", "startOffset": 80, "endOffset": 83}, {"referenceID": 8, "context": "It is strongly related to the automatic alignment mechanism first introduced in [2] and general attention models discussed in computer vision [9].", "startOffset": 142, "endOffset": 145}, {"referenceID": 7, "context": "where xn,t stands for the values of the n th location in W -memory at time t, \u03b1 is the forgetting factor (similarly defined as in [8]), g\u0303 is the normalized weight (with unnormalized score implemented also with a DNN) given to the nth location at time t.", "startOffset": 130, "endOffset": 133}, {"referenceID": 15, "context": "As the most \u201cconventional\u201d special case, if we use L-addressing for both reading and writing, we actually get the familiar structure in units found in RNN with stacked layers [16].", "startOffset": 175, "endOffset": 179}, {"referenceID": 15, "context": "It is not hard to show that we can recover some deep RNN model in [16] after stacking layers of read-write operations like this.", "startOffset": 66, "endOffset": 70}, {"referenceID": 13, "context": "In practice, we use the standard stochastic gradient descent (SGD [14]) and mini-batch (size 80) with learning rate controlled by AdaDelta [22].", "startOffset": 66, "endOffset": 70}, {"referenceID": 21, "context": "In practice, we use the standard stochastic gradient descent (SGD [14]) and mini-batch (size 80) with learning rate controlled by AdaDelta [22].", "startOffset": 139, "endOffset": 143}, {"referenceID": 1, "context": "We also show that RNNsearch [2] can be described in the framework in NTram as a relatively shallow case.", "startOffset": 28, "endOffset": 31}, {"referenceID": 1, "context": "1 Special Cases: RNNsearch Interestingly, RNNsearch [2], the seminal work of neural translation model with automatic alignment, can be viewed as a special case of NTram with shallow architecture.", "startOffset": 52, "endOffset": 55}, {"referenceID": 5, "context": "We use the case-insensitive 4-gram NIST BLEU score4 as our evaluation metric, and sign-test [6] as statistical significance test.", "startOffset": 92, "endOffset": 95}, {"referenceID": 11, "context": "\u2022 Moses: We take the open source phrase-based translation system Moses [12] (with default configuration) as the comparison system of conventional SMT.", "startOffset": 71, "endOffset": 75}, {"referenceID": 14, "context": "The word alignments are obtained with GIZA++ [15] on the corpora in both directions, using the \u201cgrow-diag-final-and\u201d balance strategy [13].", "startOffset": 45, "endOffset": 49}, {"referenceID": 12, "context": "The word alignments are obtained with GIZA++ [15] on the corpora in both directions, using the \u201cgrow-diag-final-and\u201d balance strategy [13].", "startOffset": 134, "endOffset": 138}, {"referenceID": 17, "context": "We adopt SRI Language Modeling Toolkit [18] to train a 4-gram language model with modified Kneser-Ney smoothing on the target portion of training data.", "startOffset": 39, "endOffset": 43}, {"referenceID": 1, "context": "\u2022 RNNsearch: We also compare NTram against the automatic alignment model proposed in [2].", "startOffset": 85, "endOffset": 88}, {"referenceID": 1, "context": "We use the default setting as in [2], denoted as RNNsearch (default), as well as the optimal re-scaling of the model (on sizes of both embedding and hidden layers, with about 50% more parameters than the default setting) in terms of the best testing performance, denoted as RNNsearch (best).", "startOffset": 33, "endOffset": 36}, {"referenceID": 1, "context": "For a fair comparison to RNNsearch, 1) the output layer in all NTram architectures are implemented as gated-RNN in [2] as a variant of LSTM [5], and 2) all the NTram architectures are designed to have the same embedding size as in RNNsearch (default) with parameter size less or comparable to RNNsearch (best).", "startOffset": 115, "endOffset": 118}, {"referenceID": 4, "context": "For a fair comparison to RNNsearch, 1) the output layer in all NTram architectures are implemented as gated-RNN in [2] as a variant of LSTM [5], and 2) all the NTram architectures are designed to have the same embedding size as in RNNsearch (default) with parameter size less or comparable to RNNsearch (best).", "startOffset": 140, "endOffset": 143}, {"referenceID": 1, "context": "authors on different machine translation tasks [2, 11].", "startOffset": 47, "endOffset": 54}, {"referenceID": 10, "context": "authors on different machine translation tasks [2, 11].", "startOffset": 47, "endOffset": 54}, {"referenceID": 7, "context": "We propose NTram, a novel architecture for sequence-to-sequence learning, which is stimulated by the recent work of Neural Turing Machine[8] and Neural Machine Translation [2].", "startOffset": 137, "endOffset": 140}, {"referenceID": 1, "context": "We propose NTram, a novel architecture for sequence-to-sequence learning, which is stimulated by the recent work of Neural Turing Machine[8] and Neural Machine Translation [2].", "startOffset": 172, "endOffset": 175}], "year": 2015, "abstractText": "We propose Neural Transformation Machine (NTram), a novel architecture for sequenceto-sequence learning, which performs the task through a series of nonlinear transformations from the representation of the input sequence (e.g., a Chinese sentence) to the final output sequence (e.g., translation to English). Inspired by the recent Neural Turing Machines [8], we store the intermediate representations in stacked layers of memories, and use read-write operations on the memories to realize the nonlinear transformations of those representations. Those transformations are designed in advance but the parameters are learned from data. Through layer-by-layer transformations, NTram can model complicated relations necessary for applications such as machine translation between distant languages. The architecture can be trained with normal back-propagation on parallel texts, and the learning can be easily scaled up to a large corpus. NTram is broad enough to subsume the state-of-the-art neural translation model in [2] as its special case, while significantly improves upon the model with its deeper architecture. Remarkably, NTram, being purely neural network-based, can achieve performance comparable to the traditional phrase-based machine translation system (Moses) with a small vocabulary and a modest parameter size.", "creator": "LaTeX with hyperref package"}}}