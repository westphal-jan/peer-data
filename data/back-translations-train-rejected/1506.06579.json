{"id": "1506.06579", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "22-Jun-2015", "title": "Understanding Neural Networks Through Deep Visualization", "abstract": "Recent years have produced great advances in training large, deep neural networks (DNNs), including notable successes in training convolutional neural networks (convnets) to recognize natural images. However, our understanding of how these models work, especially what computations they perform at intermediate layers, has lagged behind. Progress in the field will be further accelerated by the development of better tools for visualizing and interpreting neural nets. We introduce two such tools here. The first is a tool that visualizes the activations produced on each layer of a trained convnet as it processes an image or video (e.g. a live webcam stream). We have found that looking at live activations that change in response to user input helps build valuable intuitions about how convnets work. The second tool enables visualizing features at each layer of a DNN via regularized optimization in image space. Because previous versions of this idea produced less recognizable images, here we introduce several new regularization methods that combine to produce qualitatively clearer, more interpretable visualizations. Both tools are open source and work on a pre-trained convnet with minimal setup.", "histories": [["v1", "Mon, 22 Jun 2015 12:57:15 GMT  (8509kb,D)", "http://arxiv.org/abs/1506.06579v1", "12 pages. To appear at ICML Deep Learning Workshop 2015"]], "COMMENTS": "12 pages. To appear at ICML Deep Learning Workshop 2015", "reviews": [], "SUBJECTS": "cs.CV cs.LG cs.NE", "authors": ["jason yosinski", "jeff clune", "anh nguyen", "thomas fuchs", "hod lipson"], "accepted": false, "id": "1506.06579"}, "pdf": {"name": "1506.06579.pdf", "metadata": {"source": "META", "title": "Understanding Neural Networks Through Deep Visualization", "authors": ["Jason Yosinski", "Jeff Clune", "Anh Nguyen", "Thomas Fuchs", "Hod Lipson"], "emails": ["YOSINSKI@CS.CORNELL.EDU", "JEFFCLUNE@UWYO.EDU", "ANGUYEN8@UWYO.EDU", "FUCHS@CALTECH.EDU", "HOD.LIPSON@CORNELL.EDU"], "sections": [{"heading": null, "text": "In recent years, great strides have been made in the formation of large, deep neural networks (DNNs), including notable successes in the formation of Convolutionary Neural Networks (Conventional Networks) to detect natural images. However, our understanding of how these models work, in particular what calculations they perform on interlayers, is lagging behind. Progress in this area is being further accelerated by the development of better tools for visualizing and interpreting neural networks. We present two such tools here: the first is a tool that visualizes the activations generated at each level of a trained Convennet while processing an image or video (such as a live webcam stream). We have found that watching live activations that change in response to user input contributes valuable intuitions to how Conventional Networks."}, {"heading": "1. Introduction", "text": "In fact, the fact is that most of them will be able to move to another world, in which they are able, in which they are able to integrate, and in which they are able, in which they are able to change the world."}, {"heading": "2. Visualizing Live Convnet Activations", "text": "The first visualization method is straightforward: the representation of the neuron activation values in each level of a convention in response to an image or video. In fully interconnected neural networks, the sequence of units is irrelevant, so the actions of these vectors are not merely spatially informative. Nevertheless, filters are applied in a way that takes into account the underlying geometry of the input; in the case of 2D images, filters are applied in a 2D constellation across the two spatial dimensions of the image. This constellation produces activations on subsequent layers that are also spatially ordered for each channel. Figure 1 shows examples of this type of action for the Conv5 layer. The Conv5 layer has a size of 256 x 13 x 13, which we present as 256 different 13 x 16 images. Each of the 256 small images contains activations in the same spatial arrangement as the inputs."}, {"heading": "3. Visualizing via Regularized Optimization", "text": "The second contribution of this paper is the introduction of several regularization methods, which we imagine to be a problem that we can consider as zero years. (While each of these regularization methods helps in its own way, they are even more effective in their entirety.) We found useful combinations via a random hyperparameter search pattern, as discussed below. (Formally, however, one considers an image x in which all color channels and the height (H) and the width (W) are both 227 pixels.) When this image is presented in a neural network, it causes an activation ai (x) for some units in which the simplicity i is an index running across all levels. We also define a parameterized regularization function that penizes images in various ways. Our network was trained on ImageNet by incorporating the per-pixel averages of the examples in ImageNet into the network before starting examples."}, {"heading": "4. Discussion and Conclusion", "text": "In fact, most people who are able to survive themselves are not able to survive themselves, \"he said.\" I don't think they're able to survive me, \"he said.\" But I don't think they're going to be able to survive me. \"He added,\" I don't think they're going to be able to survive me. \"He added,\" I don't think they're going to be able to survive me. \"He added,\" I don't think they're going to be able to survive me. \"He added,\" I don't think I'm able to survive myself. \""}, {"heading": "Acknowledgments", "text": "The authors thank the NASA Space Technology Research Fellowship (JY) for funding, Wendy Shang, Yoshua Bengio, Brian Cheung and Andrej Karpathy for helpful discussions and the cat's freckles for her feline face."}, {"heading": "S1. Why are gradient optimized images dominated by high frequencies?", "text": "In the main text, we mentioned that images generated by gradient ascent to maximize neuron activation in revolutionary networks tend to be dominated by radiofrequency information (cf. the left column of Figure 3). A hypothesis for why this happens centers on the different statistics of channel activations in a convent. The convex 1 layer consists of color filters and oriented gabor edge filters of different frequencies. The average activation values (according to the rectifier) of edge filters vary across filters, with low frequency filters generally having much higher average activation values than high frequency filters. In one experiment, we observed that the average activation values of the five lowest frequency edge filters fall 90 compared to an average for the five highest frequency filters of 5.4, a difference of a factor of 17 (manuscript in preparation) 2.3."}, {"heading": "S2. Conv Layer Montages", "text": "An example of an optimized image using the hyperparameter settings from the third row of Table 1 for each filter of all five convective layers is shown in Figure S1.4We have observed that statistics on higher layers vary, but in different ways: Most channels on these layers exhibit similar average activations, with most cross-channel variances dominated by a small number of channels with unusually small or unusually large averages (Li, Yosinski, Clune, Song, Hopcroft, Lipson. 2015. How similar are the characteristics learned from different deep neural networks? In preparation.) Conv5conve3-Conv4Conv2Conv1Figure S1. An optimized preferred image for each channel of all five convective layers. These images were created using the hyperparameter combinations from the third row of Table 1."}], "references": [{"title": "Torch7: A matlab-like environment for machine learning", "author": ["Collobert", "Ronan", "Kavukcuoglu", "Koray", "Farabet", "Cl\u00e9ment"], "venue": "In BigLearn, NIPS Workshop, number EPFL-CONF-192376,", "citeRegEx": "Collobert et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Collobert et al\\.", "year": 2011}, {"title": "Generative modeling of convolutional neural networks", "author": ["Dai", "Jifeng", "Lu", "Yang", "Wu", "Ying Nian"], "venue": "In International Conference on Learning Representations (ICLR),", "citeRegEx": "Dai et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Dai et al\\.", "year": 2015}, {"title": "Imagenet: A large-scale hierarchical image database", "author": ["Deng", "Jia", "Dong", "Wei", "Socher", "Richard", "Li", "Li-Jia", "Kai", "Fei-Fei"], "venue": "In Computer Vision and Pattern Recognition,", "citeRegEx": "Deng et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Deng et al\\.", "year": 2009}, {"title": "Visualizing higher-layer features of a deep", "author": ["Erhan", "Dumitru", "Bengio", "Yoshua", "Courville", "Aaron", "Vincent", "Pascal"], "venue": null, "citeRegEx": "Erhan et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Erhan et al\\.", "year": 2009}, {"title": "Deep sparse rectifier networks", "author": ["Glorot", "Xavier", "Bordes", "Antoine", "Bengio", "Yoshua"], "venue": "In Proceedings of the 14th International Conference on Artificial Intelligence and Statistics. JMLR W&CP Volume,", "citeRegEx": "Glorot et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Glorot et al\\.", "year": 2011}, {"title": "Pylearn2: a machine learning research", "author": ["Goodfellow", "Ian J", "Warde-Farley", "David", "Lamblin", "Pascal", "Dumoulin", "Vincent", "Mirza", "Mehdi", "Pascanu", "Razvan", "Bergstra", "James", "Bastien", "Fr\u00e9d\u00e9ric", "Bengio", "Yoshua"], "venue": "library. arXiv preprint arXiv:1308.4214,", "citeRegEx": "Goodfellow et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Goodfellow et al\\.", "year": 2013}, {"title": "Explaining and Harnessing Adversarial Examples", "author": ["Goodfellow", "Ian J", "Shlens", "Jonathon", "Szegedy", "Christian"], "venue": "ArXiv e-prints,", "citeRegEx": "Goodfellow et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Goodfellow et al\\.", "year": 2014}, {"title": "Deep Speech: Scaling up end-to-end speech recognition", "author": ["A. Hannun", "C. Case", "J. Casper", "B. Catanzaro", "G. Diamos", "E. Elsen", "R. Prenger", "S. Satheesh", "S. Sengupta", "A. Coates", "A.Y. Ng"], "venue": null, "citeRegEx": "Hannun et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Hannun et al\\.", "year": 2014}, {"title": "Improving neural networks by preventing co-adaptation of feature detectors", "author": ["Hinton", "Geoffrey E", "Srivastava", "Nitish", "Krizhevsky", "Alex", "Sutskever", "Ilya", "Salakhutdinov", "Ruslan R"], "venue": "arXiv preprint arXiv:1207.0580,", "citeRegEx": "Hinton et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Hinton et al\\.", "year": 2012}, {"title": "Caffe: Convolutional architecture for fast feature embedding", "author": ["Jia", "Yangqing", "Shelhamer", "Evan", "Donahue", "Jeff", "Karayev", "Sergey", "Long", "Jonathan", "Girshick", "Ross", "Guadarrama", "Sergio", "Darrell", "Trevor"], "venue": "arXiv preprint arXiv:1408.5093,", "citeRegEx": "Jia et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Jia et al\\.", "year": 2014}, {"title": "Imagenet classification with deep convolutional neural networks", "author": ["Krizhevsky", "Alex", "Sutskever", "Ilya", "Hinton", "Geoff"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "Krizhevsky et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Krizhevsky et al\\.", "year": 2012}, {"title": "Microsoft COCO: common objects in context", "author": ["Lin", "Tsung-Yi", "Maire", "Michael", "Belongie", "Serge", "Hays", "James", "Perona", "Pietro", "Ramanan", "Deva", "Doll\u00e1r", "Piotr", "Zitnick", "C. Lawrence"], "venue": "CoRR, abs/1405.0312,", "citeRegEx": "Lin et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Lin et al\\.", "year": 2014}, {"title": "Understanding Deep Image Representations by Inverting Them", "author": ["A. Mahendran", "A. Vedaldi"], "venue": "ArXiv e-prints,", "citeRegEx": "Mahendran and Vedaldi,? \\Q2014\\E", "shortCiteRegEx": "Mahendran and Vedaldi", "year": 2014}, {"title": "Deep Neural Networks are Easily Fooled: High Confidence Predictions for Unrecognizable Images", "author": ["Nguyen", "Anh", "Yosinski", "Jason", "Clune", "Jeff"], "venue": null, "citeRegEx": "Nguyen et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Nguyen et al\\.", "year": 2014}, {"title": "FaceNet: A Unified Embedding for Face Recognition and Clustering", "author": ["F. Schroff", "D. Kalenichenko", "J. Philbin"], "venue": null, "citeRegEx": "Schroff et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Schroff et al\\.", "year": 2015}, {"title": "Deep inside convolutional networks: Visualising image classification models and saliency maps", "author": ["Simonyan", "Karen", "Vedaldi", "Andrea", "Zisserman", "Andrew"], "venue": "arXiv preprint arXiv:1312.6034,", "citeRegEx": "Simonyan et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Simonyan et al\\.", "year": 2014}, {"title": "Intriguing properties of neural networks", "author": ["Szegedy", "Christian", "Zaremba", "Wojciech", "Sutskever", "Ilya", "Bruna", "Joan", "Erhan", "Dumitru", "Goodfellow", "Ian J", "Fergus", "Rob"], "venue": "CoRR, abs/1312.6199,", "citeRegEx": "Szegedy et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Szegedy et al\\.", "year": 2013}, {"title": "Deepface: Closing the gap to human-level performance in face verification", "author": ["Taigman", "Yaniv", "Yang", "Ming", "Ranzato", "Marc\u2019Aurelio", "Wolf", "Lior"], "venue": "In Computer Vision and Pattern Recognition (CVPR),", "citeRegEx": "Taigman et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Taigman et al\\.", "year": 2014}, {"title": "Statistics of natural image categories. Network: computation in neural systems", "author": ["Torralba", "Antonio", "Oliva", "Aude"], "venue": null, "citeRegEx": "Torralba et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Torralba et al\\.", "year": 2003}, {"title": "Visualizing and understanding convolutional neural networks", "author": ["Zeiler", "Matthew D", "Fergus", "Rob"], "venue": "arXiv preprint arXiv:1311.2901,", "citeRegEx": "Zeiler et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Zeiler et al\\.", "year": 2013}, {"title": "Object detectors emerge in deep scene cnns", "author": ["Zhou", "Bolei", "Khosla", "Aditya", "Lapedriza", "\u00c0gata", "Oliva", "Aude", "Torralba", "Antonio"], "venue": "CoRR, abs/1412.6856,", "citeRegEx": "Zhou et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Zhou et al\\.", "year": 2014}, {"title": "How similar are features learned by different deep neural networks? In preparation", "author": ["Li", "Yosinski", "Clune", "Song", "Hopcroft", "Lipson"], "venue": null, "citeRegEx": "Li et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Li et al\\.", "year": 2015}, {"title": "How similar are features learned by different deep neural networks", "author": ["Lipson"], "venue": null, "citeRegEx": "2015.,? \\Q2015\\E", "shortCiteRegEx": "2015.", "year": 2015}], "referenceMentions": [{"referenceID": 17, "context": "The last several years have produced tremendous progress in training powerful, deep neural network models that are approaching and even surpassing human abilities on a variety of challenging machine learning tasks (Taigman et al., 2014; Schroff et al., 2015; Hannun et al., 2014).", "startOffset": 214, "endOffset": 279}, {"referenceID": 14, "context": "The last several years have produced tremendous progress in training powerful, deep neural network models that are approaching and even surpassing human abilities on a variety of challenging machine learning tasks (Taigman et al., 2014; Schroff et al., 2015; Hannun et al., 2014).", "startOffset": 214, "endOffset": 279}, {"referenceID": 7, "context": "The last several years have produced tremendous progress in training powerful, deep neural network models that are approaching and even surpassing human abilities on a variety of challenging machine learning tasks (Taigman et al., 2014; Schroff et al., 2015; Hannun et al., 2014).", "startOffset": 214, "endOffset": 279}, {"referenceID": 10, "context": "A flagship example is training deep, convolutional neural networks (CNNs) with supervised learning to classify natural images (Krizhevsky et al., 2012).", "startOffset": 126, "endOffset": 151}, {"referenceID": 8, "context": "dropout (Hinton et al., 2012)), better activation units (e.", "startOffset": 8, "endOffset": 29}, {"referenceID": 4, "context": "rectified linear units (Glorot et al., 2011)), and larger labeled datasets (Deng et al.", "startOffset": 23, "endOffset": 44}, {"referenceID": 2, "context": ", 2011)), and larger labeled datasets (Deng et al., 2009; Lin et al., 2014).", "startOffset": 38, "endOffset": 75}, {"referenceID": 11, "context": ", 2011)), and larger labeled datasets (Deng et al., 2009; Lin et al., 2014).", "startOffset": 38, "endOffset": 75}, {"referenceID": 5, "context": ", 2010), Pylearn2 (Goodfellow et al., 2013), Caffe (Jia et al.", "startOffset": 18, "endOffset": 43}, {"referenceID": 9, "context": ", 2013), Caffe (Jia et al., 2014), and Torch (Collobert et al.", "startOffset": 15, "endOffset": 33}, {"referenceID": 0, "context": ", 2014), and Torch (Collobert et al., 2011) \u2014 in new domains, but who may not have any intuition for why their models work (or do not).", "startOffset": 19, "endOffset": 43}, {"referenceID": 3, "context": "For example, Erhan et al. (2009) synthesized images that cause high activations for particular units.", "startOffset": 13, "endOffset": 33}, {"referenceID": 13, "context": "Others have followed suit, using the gradient to find images that cause higher activations (Simonyan et al., 2013; Nguyen et al., 2014) or lower activations (Szegedy et al.", "startOffset": 91, "endOffset": 135}, {"referenceID": 16, "context": ", 2014) or lower activations (Szegedy et al., 2013) for output units.", "startOffset": 29, "endOffset": 51}, {"referenceID": 13, "context": "Instead, they are composed of a collection of \u201chacks\u201d that happen to cause high (or low) activations: extreme pixel values, structured high frequency patterns, and copies of common motifs without global structure (Simonyan et al., 2013; Nguyen et al., 2014; Szegedy et al., 2013; Goodfellow et al., 2014).", "startOffset": 213, "endOffset": 304}, {"referenceID": 16, "context": "Instead, they are composed of a collection of \u201chacks\u201d that happen to cause high (or low) activations: extreme pixel values, structured high frequency patterns, and copies of common motifs without global structure (Simonyan et al., 2013; Nguyen et al., 2014; Szegedy et al., 2013; Goodfellow et al., 2014).", "startOffset": 213, "endOffset": 304}, {"referenceID": 6, "context": "Instead, they are composed of a collection of \u201chacks\u201d that happen to cause high (or low) activations: extreme pixel values, structured high frequency patterns, and copies of common motifs without global structure (Simonyan et al., 2013; Nguyen et al., 2014; Szegedy et al., 2013; Goodfellow et al., 2014).", "startOffset": 213, "endOffset": 304}, {"referenceID": 16, "context": "Specifically, it has been shown that such hacks may be applied to correctly classified images to cause them to be misclassified even via imperceptibly small changes (Szegedy et al., 2013), that such hacks can be found even without the gradient information to produce unrecognizable \u201cfooling examples\u201d (Nguyen et al.", "startOffset": 165, "endOffset": 187}, {"referenceID": 13, "context": ", 2013), that such hacks can be found even without the gradient information to produce unrecognizable \u201cfooling examples\u201d (Nguyen et al., 2014), and that the abundance of non-natural looking images that cause extreme activations can be explained by the locally linear behavior of neural nets (Goodfellow et al.", "startOffset": 121, "endOffset": 142}, {"referenceID": 6, "context": ", 2014), and that the abundance of non-natural looking images that cause extreme activations can be explained by the locally linear behavior of neural nets (Goodfellow et al., 2014).", "startOffset": 156, "endOffset": 181}, {"referenceID": 14, "context": "Simonyan et al. (2013) showed that slightly discernible images for the final layers of a convnet could be produced withL2-regularization.", "startOffset": 0, "endOffset": 23}, {"referenceID": 12, "context": "Mahendran and Vedaldi (2014) also showed the importance of incorporating natural-image priors in the optimization process when producing images that mimic an entire-layer\u2019s firing pattern produced by a specific input image.", "startOffset": 0, "endOffset": 29}, {"referenceID": 9, "context": "While the tools could be adapted to integrate with any DNN software framework, they work out of the box with the popular Caffe DNN software package (Jia et al., 2014).", "startOffset": 148, "endOffset": 166}, {"referenceID": 10, "context": "Our pre-trained network is nearly identical to the \u201cAlexNet\u201d architecture (Krizhevsky et al., 2012), but with local reponse normalization layers after pooling layers following (Jia et al.", "startOffset": 74, "endOffset": 99}, {"referenceID": 9, "context": ", 2012), but with local reponse normalization layers after pooling layers following (Jia et al., 2014).", "startOffset": 84, "endOffset": 102}, {"referenceID": 2, "context": "It was trained with the Caffe framework on the ImageNet 2012 dataset (Deng et al., 2009).", "startOffset": 69, "endOffset": 88}, {"referenceID": 20, "context": "Zhou et al. (2014) recently observed a similar effect where convnets trained only to recognize different scene types \u2014 playgrounds, restaurant patios, living rooms, etc.", "startOffset": 0, "endOffset": 19}, {"referenceID": 15, "context": "L2 decay was also used by Simonyan et al. (2013).", "startOffset": 26, "endOffset": 49}, {"referenceID": 13, "context": "While these images cause high activations, they are neither realistic nor interpretable (Nguyen et al., 2014).", "startOffset": 88, "endOffset": 109}, {"referenceID": 20, "context": "These visualizations suggest that further study into the exact nature of learned representations \u2014 whether they are local to a single channel or distributed across several \u2014 is likely to be interesting (see Zhou et al. (2014) for work in this direction).", "startOffset": 207, "endOffset": 226}, {"referenceID": 16, "context": "ies have shown that discriminative networks can easily be fooled or hacked by the addition of certain structured noise in image space (Szegedy et al., 2013; Nguyen et al., 2014).", "startOffset": 134, "endOffset": 177}, {"referenceID": 13, "context": "ies have shown that discriminative networks can easily be fooled or hacked by the addition of certain structured noise in image space (Szegedy et al., 2013; Nguyen et al., 2014).", "startOffset": 134, "endOffset": 177}, {"referenceID": 13, "context": "Past attempts have largely supported this view by producing unrealistic images using this method (Nguyen et al., 2014; Simonyan et al., 2013).", "startOffset": 97, "endOffset": 141}, {"referenceID": 1, "context": "Work by Dai et al. (2015) shows some interesting results in this direction.", "startOffset": 8, "endOffset": 26}], "year": 2015, "abstractText": "Recent years have produced great advances in training large, deep neural networks (DNNs), including notable successes in training convolutional neural networks (convnets) to recognize natural images. However, our understanding of how these models work, especially what computations they perform at intermediate layers, has lagged behind. Progress in the field will be further accelerated by the development of better tools for visualizing and interpreting neural nets. We introduce two such tools here. The first is a tool that visualizes the activations produced on each layer of a trained convnet as it processes an image or video (e.g. a live webcam stream). We have found that looking at live activations that change in response to user input helps build valuable intuitions about how convnets work. The second tool enables visualizing features at each layer of a DNN via regularized optimization in image space. Because previous versions of this idea produced less recognizable images, here we introduce several new regularization methods that combine to produce qualitatively clearer, more interpretable visualizations. Both tools are open source and work on a pretrained convnet with minimal setup. Published in the Deep Learning Workshop, 31 st International Conference on Machine Learning, Lille, France, 2015. Copyright 2015 by the author(s).", "creator": "LaTeX with hyperref package"}}}