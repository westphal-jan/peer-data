{"id": "1506.06628", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "22-Jun-2015", "title": "Modality-dependent Cross-media Retrieval", "abstract": "In this paper, we investigate the cross-media retrieval between images and text, i.e., using image to search text (I2T) and using text to search images (T2I). Existing cross-media retrieval methods usually learn one couple of projections, by which the original features of images and text can be projected into a common latent space to measure the content similarity. However, using the same projections for the two different retrieval tasks (I2T and T2I) may lead to a tradeoff between their respective performances, rather than their best performances. Different from previous works, we propose a modality-dependent cross-media retrieval (MDCR) model, where two couples of projections are learned for different cross-media retrieval tasks instead of one couple of projections. Specifically, by jointly optimizing the correlation between images and text and the linear regression from one modal space (image or text) to the semantic space, two couples of mappings are learned to project images and text from their original feature spaces into two common latent subspaces (one for I2T and the other for T2I). Extensive experiments show the superiority of the proposed MDCR compared with other methods. In particular, based the 4,096 dimensional convolutional neural network (CNN) visual feature and 100 dimensional LDA textual feature, the mAP of the proposed method achieves 41.5\\%, which is a new state-of-the-art performance on the Wikipedia dataset.", "histories": [["v1", "Mon, 22 Jun 2015 14:33:39 GMT  (821kb,D)", "https://arxiv.org/abs/1506.06628v1", "in ACM Transactions on Intelligent Systems and Technology"], ["v2", "Tue, 23 Jun 2015 01:34:01 GMT  (889kb,D)", "http://arxiv.org/abs/1506.06628v2", "in ACM Transactions on Intelligent Systems and Technology"]], "COMMENTS": "in ACM Transactions on Intelligent Systems and Technology", "reviews": [], "SUBJECTS": "cs.CV cs.IR cs.LG", "authors": ["yunchao wei", "yao zhao", "zhenfeng zhu", "shikui wei", "yanhui xiao", "jiashi feng", "shuicheng yan"], "accepted": false, "id": "1506.06628"}, "pdf": {"name": "1506.06628.pdf", "metadata": {"source": "CRF", "title": "A Modality-dependent Cross-media Retrieval", "authors": ["YUNCHAO WEI", "YAO ZHAO", "ZHENFENG ZHU", "SHIKUI WEI", "YANHUI XIAO", "JIASHI FENG", "SHUICHENG YAN"], "emails": ["wychao1987@gmail.com,", "shkwei}@bjtu.edu.cn,", "xiaoyanhui@gmail.com.", "jshfeng@gmail.com.", "eleyans@nus.edu.sg."], "sections": [{"heading": null, "text": "In fact, most of them will be able to move into a different world, in which they are able to move, in which they move, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they, in which they, in which they live, in which they, in which they, in which they live, in which they, in which they live, in which they, in which they, in which they, in which they live."}, {"heading": "1. INTRODUCTION", "text": "In fact, most of them are able to survive on their own, without having to orient themselves in a different direction."}, {"heading": "2. RELATED WORK", "text": "Some papers [Hardoon et al. 2004; Tenenbaum et al. 2000; Rosipal and Kra \u00a4mer 2006; Yang et al. 2008; Sharma and Jacobs 2011; Hwang and Grauman 2010; Rasiwasia et al. 2010; Sharma et al. 2012; Gong et al. 2013; Wei et al. 2014; Zhang et al. 2014] attempt to learn an optimal common latent subspace for multimodal data. These types of methods project representations of multiple modalities into an isomorphic space, as this similarity measurement can be applied directly between multimodal data. Two popular approaches, Canonical Correlation Analysis (CCA) [Hardoon et al. 2004] and Partial Least Squares (PLS) [Rosipal and Kra \u00a4mer 2006; Sharma and Jacobs 2011], are commonly used to find a pair of associations."}, {"heading": "3. MODALITY-DEPENDENT CROSS-MEDIA RETRIEVAL", "text": "In this section, we detail the proposed intermedia retrieval method, which we call modality-dependent cross-media retrieval (MDCR). Each pair of image and text in the training set is accompanied by semantic information (e.g. class names). Unlike [Gong et al. 2013], which includes semantic information as a third view, this paper uses semantic information to determine a common latent space with a fixed dimension in which samples with the same label clustered.Let us assume a dataset of n data instances, i.e., G = (xi, ti)} ni = 1, where xi-Rp and ti-Rq are original low features of image and text document. Let X = [x1, xn] T-Rn x-P be the feature matrix-W."}, {"heading": "3.1. Algorithm for I2T", "text": "Describe the two optimal mapping matrices for images and text as V1, W1f (V1, W1) and W1, Rc), respectively. On the basis of the optimization framework Eq. (1), the objective function of I2T is defined as follows: min V1, W1f (V1, W1) = 1, Vol. V, No. N, Article A, Release date: January YYYYYY.where 0 \u2264 1 is a compromise parameter to balance the meaning of the correlation analysis concept and the linear regression concept."}, {"heading": "3.2. Algorithm for T2I", "text": "In contrast to the objective function of I2T, the regression term for T2I is a regression operation from textual space to semantic space. Describe the two optimal mapping matrices for images and text in T2I as V2, Rc \u00b7 p or W2, Rc \u00b7 q. Based on the optimization frame \u00c4q. (1) The objective function of T2I is defined as follows: min V2, W2f (V2, W2) = Rc \u00b7 q. The objective function of T2I is defined as follows: min V2, W2f (V2, W2) = Rc \u00b7 q. (1) The objective function of T2I is defined as: min V2, W2f (V2) = Rc \u00b7 q."}, {"heading": "3.3. Optimization", "text": "The optimization problems for I2T and T2I are unlimited optimizations in relation to two matrices. Therefore, for the non-convex problem, we usually design algorithms to search for stationary points. We note that equivalent (2) is convex in relation to either V1 or W1, while the other is fixed. Equally convex is equivalent (3) also in relation to either V2 or W2, while the other is fixed. Specifically, by fixing V1 (V2) or W1 (W2), minimization over the other can be terminated with the gradient descend method. Partial derivatives of V1 or W1 over Eq. (2) are presented as follows:"}, {"heading": "4. EXPERIMENTAL RESULTS", "text": "To evaluate the proposed MDCR algorithm, we systematically compare it with other state-of-the-art methods on three datasets, i.e., Wikipedia [Rasiwasia et al. 2010], Pascal Sentence [Rashtchian et al. 2010] and a subset of INRIA-Websearch [Krapac et al. 2010].ACM Transactions on Intelligent Systems and Technologies, Vol. V, No N, Article A, Release Date: January YYYY.ALGORITHM 1: Optimization for Modality-Dependent Cross-Media Retrieval Input: The feature matrix of image data X = [x1,..., xn] T-Rn \u00b7 p, the feature matrix of text data T = [t1,..., tn] T-Rn \u00b7 q, the semantic matrix of image data X = [s1,..., sn] T-value value."}, {"heading": "4.1. Datasets", "text": "The entire data set is randomly divided into a training set and a test set of 2,173 and 693 pairs. We use the publicly available features provided by [Rasiwasia et al. 2010], i.e. 128 dimensional SIFT BoVW for images and 10 dimensional LDA for text, to directly compare with existing results. We also present the cross-media retrieval results based on the 4,096 dimensional CNN Visual Features3 and the 100 dimensional latent dirichlet allocation model (LDA) [Lead et al. 2003] textual features (we get the textual feature vector based on 500 tokens and then the LDA model is used to compress the probability of each document under 100 topics. We have compressed the probability of each document under 100 topics."}, {"heading": "4.2. Experimental Settings", "text": "In the experiment, the Euclidean distance is used to measure the similarity between characteristics in the latent subspace. Query performance is evaluated by the mean mean precision (MAP), which is one of the standard query quantities. Specifically, the average precision (AP) of each query is defined as: AP = \u2211 R k = 1 P (k) rel (k) \u2211 Rk = 1 rel (k), where R is the size of the test dataset. rel (k) = 1 if the item in rank k is relevant, rel (k) = 0 otherwise. P (k) denotes the precision of the result in rank k. We can get the MAP value by averaging AP for all queries."}, {"heading": "4.3. Results", "text": "In the experiments we mainly compare the proposed MDCR with six algorithms, including CCA, Semantic Matching (SM) [Rasiwasia et al. 2010], Semantic Correlation Matching (SCM) [Rasiwasia et al. 2010], Three-View CCA (T-V CCA) [Gong et al. 2013], Generalized Multiview Marginal Fisher Analysis (GMMFA) [Sharma et al. 2012] and Generalized Multiview Discriminant Analysis (GMLDA) [Sharma et al. 2012].For the Wikipedia dataset, we first compare the proposed MDCR with other methods based on the publicly available features [Rasiwasia et al. 2010], i.e., 128-SIFT BoVW for images and 10-LDA for text. We fix \u00b5 = 0.02 and = 10 \u2212 4, and experimentally set alike."}, {"heading": "5. CONCLUSIONS", "text": "In this paper, we focus on the development of an effective cross-media retrieval model for images and texts, i.e. the use of image to ACM Transactions on Intelligent Systems and Technology, Vol. V, No. N, Article A, Publication date: January YYYY.search text (I2T) and using text to search images (T2I). In contrast to conventional learning algorithms in conventional common space, we propose a modality-dependent scheme that recommends different treatments for I2T and T2I by learning two pairs of mappings for different cross-media retrieval tasks. In particular, by jointly optimizing a correlation concept (between images and text) and a linear regression concept (from a modal space, i.e. image or text to semantic space), two pairs of mappings are obtained for different retrieval tasks. Extensive experiments on the Wikipedia dataset, the Pascal Sentence method for data security and the proposed web page search method."}], "references": [{"title": "Latent dirichlet allocation", "author": ["D.M. Blei", "A.Y. Ng", "M.I. Jordan."], "venue": "Journal of Machine Learning Research", "citeRegEx": "Blei et al\\.,? 2003", "shortCiteRegEx": "Blei et al\\.", "year": 2003}, {"title": "A Multi-View Embedding Space for Modeling Internet Images, Tags, and Their Semantics", "author": ["Yunchao Gong", "Qifa Ke", "Michael Isard", "Svetlana Lazebnik."], "venue": "International Journal of Computer Vision (2013), 1\u201324.", "citeRegEx": "Gong et al\\.,? 2013", "shortCiteRegEx": "Gong et al\\.", "year": 2013}, {"title": "Canonical correlation analysis: An overview with application to learning methods", "author": ["D.R. Hardoon", "S. Szedmak", "J. Shawe-Taylor."], "venue": "Neural Computation 16, 12 (2004), 2639\u20132664.", "citeRegEx": "Hardoon et al\\.,? 2004", "shortCiteRegEx": "Hardoon et al\\.", "year": 2004}, {"title": "Accounting for the Relative Importance of Objects in Image Retrieval", "author": ["Sung Ju Hwang", "Kristen Grauman."], "venue": "British Machine Vision Conference. 1\u201312.", "citeRegEx": "Hwang and Grauman.,? 2010", "shortCiteRegEx": "Hwang and Grauman.", "year": 2010}, {"title": "CrossModal Similarity Learning: A Low Rank Bilinear Formulation", "author": ["Cuicui Kang", "Shengcai Liao", "Yonghao He", "Jian Wang", "Shiming Xiang", "Chunhong Pan."], "venue": "arXiv preprint arXiv:1411.4738 (2014).", "citeRegEx": "Kang et al\\.,? 2014", "shortCiteRegEx": "Kang et al\\.", "year": 2014}, {"title": "Improving web-image search results using query-relative classifiers", "author": ["Josip Krapac", "Moray Allan", "Jakob Verbeek", "Fr\u00e9d\u00e9ric Jurie."], "venue": "IEEE Conference on Computer Vision and Pattern Recognition. 1094\u2013 1101. http://lear.inrialpes.fr/pubs/2010/KAVJ10", "citeRegEx": "Krapac et al\\.,? 2010", "shortCiteRegEx": "Krapac et al\\.", "year": 2010}, {"title": "Imagenet classification with deep convolutional neural networks", "author": ["Alex Krizhevsky", "Ilya Sutskever", "Geoff Hinton."], "venue": "Advances in Neural Information Processing Systems. 1106\u20131114.", "citeRegEx": "Krizhevsky et al\\.,? 2012", "shortCiteRegEx": "Krizhevsky et al\\.", "year": 2012}, {"title": "Learning hash functions for cross-view similarity search", "author": ["Shaishav Kumar", "Raghavendra Udupa."], "venue": "IJCAI Proceedings-International Joint Conference on Artificial Intelligence, Vol. 22. 1360.", "citeRegEx": "Kumar and Udupa.,? 2011", "shortCiteRegEx": "Kumar and Udupa.", "year": 2011}, {"title": "Learning Multimodal Neural Network with Ranking Examples", "author": ["Xinyan Lu", "Fei Wu", "Xi Li", "Yin Zhang", "Weiming Lu", "Donghui Wang", "Yueting Zhuang."], "venue": "Proceedings of the international conference on Multimedia. 985\u2013988.", "citeRegEx": "Lu et al\\.,? 2014", "shortCiteRegEx": "Lu et al\\.", "year": 2014}, {"title": "Collecting image annotations using Amazon\u2019s Mechanical Turk", "author": ["C. Rashtchian", "P. Young", "M. Hodosh", "J. Hockenmaier."], "venue": "Workshop on Creating Speech and Language Data with Amazon\u2019s Mechanical Turk. 139\u2013147.", "citeRegEx": "Rashtchian et al\\.,? 2010", "shortCiteRegEx": "Rashtchian et al\\.", "year": 2010}, {"title": "A new approach to cross-modal multimedia retrieval", "author": ["N. Rasiwasia", "J. Costa Pereira", "E. Coviello", "G. Doyle", "G.R.G. Lanckriet", "R. Levy", "N. Vasconcelos."], "venue": "Proceedings of the international conference on Multimedia. 251\u2013260.", "citeRegEx": "Rasiwasia et al\\.,? 2010", "shortCiteRegEx": "Rasiwasia et al\\.", "year": 2010}, {"title": "Overview and recent advances in partial least squares", "author": ["Roman Rosipal", "Nicole Kr\u00e4mer."], "venue": "Subspace, Latent Structure and Feature Selection. Springer, 34\u201351.", "citeRegEx": "Rosipal and Kr\u00e4mer.,? 2006", "shortCiteRegEx": "Rosipal and Kr\u00e4mer.", "year": 2006}, {"title": "Bypassing synthesis: PLS for face recognition with pose, lowresolution and sketch", "author": ["Abhishek Sharma", "David W Jacobs."], "venue": "IEEE Conference on Computer Vision and Pattern Recognition. 593\u2013600.", "citeRegEx": "Sharma and Jacobs.,? 2011", "shortCiteRegEx": "Sharma and Jacobs.", "year": 2011}, {"title": "Generalized multiview analysis: A discriminative latent space", "author": ["Abhishek Sharma", "Abhishek Kumar", "H Daume", "David W Jacobs."], "venue": "IEEE Conference on Computer Vision and Pattern Recognition. 2160\u2013 2167.", "citeRegEx": "Sharma et al\\.,? 2012", "shortCiteRegEx": "Sharma et al\\.", "year": 2012}, {"title": "Separating style and content with bilinear models", "author": ["Joshua B Tenenbaum", "William T Freeman."], "venue": "Neural computation 12, 6 (2000), 1247\u20131283.", "citeRegEx": "Tenenbaum and Freeman.,? 2000", "shortCiteRegEx": "Tenenbaum and Freeman.", "year": 2000}, {"title": "Effective MultiModal Retrieval based on Stacked Auto-Encoders", "author": ["Wei Wang", "Beng Chin Ooi", "Xiaoyan Yang", "Dongxiang Zhang", "Yueting Zhuang."], "venue": "International Conference on Very Large Data Bases 7, 8 (2014).", "citeRegEx": "Wang et al\\.,? 2014", "shortCiteRegEx": "Wang et al\\.", "year": 2014}, {"title": "Learning a mid-level feature space for cross-media regularization", "author": ["Yunchao Wei", "Yao Zhao", "Zhenfeng Zhu", "Yanhui Xiao", "Shikui Wei."], "venue": "IEEE International Conference on Multimedia and Expo. 1\u20136.", "citeRegEx": "Wei et al\\.,? 2014", "shortCiteRegEx": "Wei et al\\.", "year": 2014}, {"title": "Cross-media semantic representation via bi-directional learning to rank", "author": ["Fei Wu", "Xinyan Lu", "Zhongfei Zhang", "Shuicheng Yan", "Yong Rui", "Yueting Zhuang."], "venue": "Proceedings of the international conference on Multimedia. 877\u2013886.", "citeRegEx": "Wu et al\\.,? 2013", "shortCiteRegEx": "Wu et al\\.", "year": 2013}, {"title": "Sparse Multi-Modal Hashing", "author": ["Fei Wu", "Zhou Yu", "Yi Yang", "Siliang Tang", "Yin Zhang", "Yueting Zhuang."], "venue": "IEEE Transactions on Multimedia 16, 2 (2014), 427\u2013439.", "citeRegEx": "Wu et al\\.,? 2014", "shortCiteRegEx": "Wu et al\\.", "year": 2014}, {"title": "A multimedia retrieval framework based on semi-supervised ranking and relevance feedback", "author": ["Yi Yang", "Feiping Nie", "Dong Xu", "Jiebo Luo", "Yueting Zhuang", "Yunhe Pan."], "venue": "IEEE Trans. Pattern Anal. Mach. Intell. 34, 4 (2012), 723\u2013742.", "citeRegEx": "Yang et al\\.,? 2012", "shortCiteRegEx": "Yang et al\\.", "year": 2012}, {"title": "Cross-media retrieval using query dependent search methods", "author": ["Yi Yang", "Fei Wu", "Dong Xu", "Yueting Zhuang", "Liang-Tien Chia."], "venue": "Pattern Recognition 43, 8 (2010), 2927\u20132936.", "citeRegEx": "Yang et al\\.,? 2010", "shortCiteRegEx": "Yang et al\\.", "year": 2010}, {"title": "Ranking with local regression and global alignment for cross media retrieval", "author": ["Yi Yang", "Dong Xu", "Feiping Nie", "Jiebo Luo", "Yueting Zhuang."], "venue": "Proceedings of the international conference on Multimedia. 175\u2013184.", "citeRegEx": "Yang et al\\.,? 2009", "shortCiteRegEx": "Yang et al\\.", "year": 2009}, {"title": "Harmonizing hierarchical manifolds for multimedia document semantics understanding and cross-media retrieval", "author": ["Yi Yang", "Yue-Ting Zhuang", "Fei Wu", "Yun-He Pan."], "venue": "IEEE Transactions on Multimedia 10, 3 (2008), 437\u2013446.", "citeRegEx": "Yang et al\\.,? 2008", "shortCiteRegEx": "Yang et al\\.", "year": 2008}, {"title": "Cross-media retrieval by intra-media and inter-media correlation mining", "author": ["Xiaohua Zhai", "Yuxin Peng", "Jianguo Xiao."], "venue": "Multimedia systems 19, 5 (2013), 395\u2013406.", "citeRegEx": "Zhai et al\\.,? 2013", "shortCiteRegEx": "Zhai et al\\.", "year": 2013}, {"title": "Mining Semantically Consistent Patterns for Cross-View Data", "author": ["Lei Zhang", "Yao Zhao", "Zhenfeng Zhu", "Shikui Wei", "Xindong Wu."], "venue": "IEEE Trans. Knowl. Data Eng. 26, 11 (2014), 2745\u20132758.", "citeRegEx": "Zhang et al\\.,? 2014", "shortCiteRegEx": "Zhang et al\\.", "year": 2014}, {"title": "Cross-Media Hashing with Neural Networks", "author": ["Yueting Zhuang", "Zhou Yu", "Wei Wang", "Fei Wu", "Siliang Tang", "Jian Shao."], "venue": "Proceedings of the international conference on Multimedia. 901\u2013904.", "citeRegEx": "Zhuang et al\\.,? 2014", "shortCiteRegEx": "Zhuang et al\\.", "year": 2014}], "referenceMentions": [{"referenceID": 2, "context": "We observe that most exiting works [Hardoon et al. 2004; Rasiwasia et al. 2010; Sharma et al. 2012; Gong et al. 2013] focus on learning one couple of mapping matrices to project high-dimensional features from different modalities into a common latent space.", "startOffset": 35, "endOffset": 117}, {"referenceID": 10, "context": "We observe that most exiting works [Hardoon et al. 2004; Rasiwasia et al. 2010; Sharma et al. 2012; Gong et al. 2013] focus on learning one couple of mapping matrices to project high-dimensional features from different modalities into a common latent space.", "startOffset": 35, "endOffset": 117}, {"referenceID": 13, "context": "We observe that most exiting works [Hardoon et al. 2004; Rasiwasia et al. 2010; Sharma et al. 2012; Gong et al. 2013] focus on learning one couple of mapping matrices to project high-dimensional features from different modalities into a common latent space.", "startOffset": 35, "endOffset": 117}, {"referenceID": 1, "context": "We observe that most exiting works [Hardoon et al. 2004; Rasiwasia et al. 2010; Sharma et al. 2012; Gong et al. 2013] focus on learning one couple of mapping matrices to project high-dimensional features from different modalities into a common latent space.", "startOffset": 35, "endOffset": 117}, {"referenceID": 2, "context": "However, only considering pair-wise closeness [Hardoon et al. 2004] is not sufficient for cross-media retrieval tasks, since it is required that multi-modal data from the same semantics should be united in the common latent subspace.", "startOffset": 46, "endOffset": 67}, {"referenceID": 13, "context": "Although [Sharma et al. 2012] and [Gong et al.", "startOffset": 9, "endOffset": 29}, {"referenceID": 1, "context": "2012] and [Gong et al. 2013] have proposed to use supervised information to cluster the multi-modal data with the same semantics, learning one couple of projections may only lead to compromised results for each retrieval task.", "startOffset": 10, "endOffset": 28}, {"referenceID": 5, "context": "\u2022Based on the INRIA-Websearch dataset [Krapac et al. 2010], we construct a new dataset for cross-media retrieval evaluation.", "startOffset": 38, "endOffset": 58}, {"referenceID": 2, "context": "Some works [Hardoon et al. 2004; Tenenbaum and Freeman 2000; Rosipal and Kr\u00e4mer 2006; Yang et al. 2008; Sharma and Jacobs 2011; Hwang and Grauman 2010; Rasiwasia et al. 2010; Sharma et al. 2012; Gong et al. 2013; Wei et al. 2014; Zhang et al. 2014] try to learn an optimal common latent subspace for multi-modal data.", "startOffset": 11, "endOffset": 248}, {"referenceID": 22, "context": "Some works [Hardoon et al. 2004; Tenenbaum and Freeman 2000; Rosipal and Kr\u00e4mer 2006; Yang et al. 2008; Sharma and Jacobs 2011; Hwang and Grauman 2010; Rasiwasia et al. 2010; Sharma et al. 2012; Gong et al. 2013; Wei et al. 2014; Zhang et al. 2014] try to learn an optimal common latent subspace for multi-modal data.", "startOffset": 11, "endOffset": 248}, {"referenceID": 10, "context": "Some works [Hardoon et al. 2004; Tenenbaum and Freeman 2000; Rosipal and Kr\u00e4mer 2006; Yang et al. 2008; Sharma and Jacobs 2011; Hwang and Grauman 2010; Rasiwasia et al. 2010; Sharma et al. 2012; Gong et al. 2013; Wei et al. 2014; Zhang et al. 2014] try to learn an optimal common latent subspace for multi-modal data.", "startOffset": 11, "endOffset": 248}, {"referenceID": 13, "context": "Some works [Hardoon et al. 2004; Tenenbaum and Freeman 2000; Rosipal and Kr\u00e4mer 2006; Yang et al. 2008; Sharma and Jacobs 2011; Hwang and Grauman 2010; Rasiwasia et al. 2010; Sharma et al. 2012; Gong et al. 2013; Wei et al. 2014; Zhang et al. 2014] try to learn an optimal common latent subspace for multi-modal data.", "startOffset": 11, "endOffset": 248}, {"referenceID": 1, "context": "Some works [Hardoon et al. 2004; Tenenbaum and Freeman 2000; Rosipal and Kr\u00e4mer 2006; Yang et al. 2008; Sharma and Jacobs 2011; Hwang and Grauman 2010; Rasiwasia et al. 2010; Sharma et al. 2012; Gong et al. 2013; Wei et al. 2014; Zhang et al. 2014] try to learn an optimal common latent subspace for multi-modal data.", "startOffset": 11, "endOffset": 248}, {"referenceID": 16, "context": "Some works [Hardoon et al. 2004; Tenenbaum and Freeman 2000; Rosipal and Kr\u00e4mer 2006; Yang et al. 2008; Sharma and Jacobs 2011; Hwang and Grauman 2010; Rasiwasia et al. 2010; Sharma et al. 2012; Gong et al. 2013; Wei et al. 2014; Zhang et al. 2014] try to learn an optimal common latent subspace for multi-modal data.", "startOffset": 11, "endOffset": 248}, {"referenceID": 24, "context": "Some works [Hardoon et al. 2004; Tenenbaum and Freeman 2000; Rosipal and Kr\u00e4mer 2006; Yang et al. 2008; Sharma and Jacobs 2011; Hwang and Grauman 2010; Rasiwasia et al. 2010; Sharma et al. 2012; Gong et al. 2013; Wei et al. 2014; Zhang et al. 2014] try to learn an optimal common latent subspace for multi-modal data.", "startOffset": 11, "endOffset": 248}, {"referenceID": 2, "context": "Two popular approaches, Canonical Correlation Analysis (CCA) [Hardoon et al. 2004] and Partial Least Squares (PLS) [Rosipal and Kr\u00e4mer 2006; Sharma and Jacobs 2011], are usually employed to find a couple of mappings to maximize the correlations between two variables.", "startOffset": 61, "endOffset": 82}, {"referenceID": 9, "context": "Based on CCA, a number of successful algorithms have been developed for cross-media retrieval tasks [Rashtchian et al. 2010; Hwang and Grauman 2010; Sharma et al. 2012; Gong et al. 2013].", "startOffset": 100, "endOffset": 186}, {"referenceID": 13, "context": "Based on CCA, a number of successful algorithms have been developed for cross-media retrieval tasks [Rashtchian et al. 2010; Hwang and Grauman 2010; Sharma et al. 2012; Gong et al. 2013].", "startOffset": 100, "endOffset": 186}, {"referenceID": 1, "context": "Based on CCA, a number of successful algorithms have been developed for cross-media retrieval tasks [Rashtchian et al. 2010; Hwang and Grauman 2010; Sharma et al. 2012; Gong et al. 2013].", "startOffset": 100, "endOffset": 186}, {"referenceID": 9, "context": "The work [Rashtchian et al. 2010] investigated the cross-media retrieval problem in terms of correlation hypothesis and abstraction hypothesis.", "startOffset": 9, "endOffset": 33}, {"referenceID": 1, "context": "More recently, the work [Gong et al. 2013] proposed a three-view CCA model by introducing a semantic view to produce a better separation for multi-modal data of different classes in the learned latent subspace.", "startOffset": 24, "endOffset": 42}, {"referenceID": 18, "context": "To address the problem of prohibitively expensive nearest neighbor search, some hashing-based approaches [Kumar and Udupa 2011; Wu et al. 2014] to large scale similarity search have drawn much interest from the cross-media retrieval community.", "startOffset": 105, "endOffset": 143}, {"referenceID": 18, "context": "Recently, [Wu et al. 2014] proposed a sparse multi-modal hashing method, which can obtain sparse codes for the data across different modalities via joint multi-modal dictionary learning, to address cross-modal retrieval.", "startOffset": 10, "endOffset": 26}, {"referenceID": 15, "context": "Besides, with the development of deep learning, some deep models [Frome et al. 2013; Wang et al. 2014; Lu et al. 2014; Zhuang et al. 2014] have also been proposed to address cross-media problems.", "startOffset": 65, "endOffset": 138}, {"referenceID": 8, "context": "Besides, with the development of deep learning, some deep models [Frome et al. 2013; Wang et al. 2014; Lu et al. 2014; Zhuang et al. 2014] have also been proposed to address cross-media problems.", "startOffset": 65, "endOffset": 138}, {"referenceID": 25, "context": "Besides, with the development of deep learning, some deep models [Frome et al. 2013; Wang et al. 2014; Lu et al. 2014; Zhuang et al. 2014] have also been proposed to address cross-media problems.", "startOffset": 65, "endOffset": 138}, {"referenceID": 21, "context": "Beyond the above mentioned models, some other works [Yang et al. 2009; Yang et al. 2010; Yang et al. 2012; Wu et al. 2013; Zhai et al. 2013; Kang et al. 2014] have also been proposed to address cross-media problems.", "startOffset": 52, "endOffset": 158}, {"referenceID": 20, "context": "Beyond the above mentioned models, some other works [Yang et al. 2009; Yang et al. 2010; Yang et al. 2012; Wu et al. 2013; Zhai et al. 2013; Kang et al. 2014] have also been proposed to address cross-media problems.", "startOffset": 52, "endOffset": 158}, {"referenceID": 19, "context": "Beyond the above mentioned models, some other works [Yang et al. 2009; Yang et al. 2010; Yang et al. 2012; Wu et al. 2013; Zhai et al. 2013; Kang et al. 2014] have also been proposed to address cross-media problems.", "startOffset": 52, "endOffset": 158}, {"referenceID": 17, "context": "Beyond the above mentioned models, some other works [Yang et al. 2009; Yang et al. 2010; Yang et al. 2012; Wu et al. 2013; Zhai et al. 2013; Kang et al. 2014] have also been proposed to address cross-media problems.", "startOffset": 52, "endOffset": 158}, {"referenceID": 23, "context": "Beyond the above mentioned models, some other works [Yang et al. 2009; Yang et al. 2010; Yang et al. 2012; Wu et al. 2013; Zhai et al. 2013; Kang et al. 2014] have also been proposed to address cross-media problems.", "startOffset": 52, "endOffset": 158}, {"referenceID": 4, "context": "Beyond the above mentioned models, some other works [Yang et al. 2009; Yang et al. 2010; Yang et al. 2012; Wu et al. 2013; Zhai et al. 2013; Kang et al. 2014] have also been proposed to address cross-media problems.", "startOffset": 52, "endOffset": 158}, {"referenceID": 17, "context": "In particular, [Wu et al. 2013] presented a bi-directional cross-media semantic representation model by optimizing the bi-directional list-wise ranking loss with a latent space embedding.", "startOffset": 15, "endOffset": 31}, {"referenceID": 23, "context": "In [Zhai et al. 2013], both the intra-media and the inter-media correlation are explored for crossmedia retrieval.", "startOffset": 3, "endOffset": 21}, {"referenceID": 4, "context": "Most recently, [Kang et al. 2014] presented a heterogeneous similarity learning approach based on metric learning for cross-media retrieval.", "startOffset": 15, "endOffset": 33}, {"referenceID": 4, "context": "With the convolutional neural network (CNN) visual feature, some new state-of-the-art cross-media retrieval results have been achieved in [Kang et al. 2014].", "startOffset": 138, "endOffset": 156}, {"referenceID": 1, "context": "Different from [Gong et al. 2013] which incorporates the semantic information as a third view, in this paper, semantic information is employed to determine a common latent space with a fixed dimension where samples with the same label can be clustered.", "startOffset": 15, "endOffset": 33}, {"referenceID": 10, "context": ", Wikipedia [Rasiwasia et al. 2010], Pascal Sentence [Rashtchian et al.", "startOffset": 12, "endOffset": 35}, {"referenceID": 9, "context": "2010], Pascal Sentence [Rashtchian et al. 2010] and a subset of INRIA-Websearch [Krapac et al.", "startOffset": 23, "endOffset": 47}, {"referenceID": 5, "context": "2010] and a subset of INRIA-Websearch [Krapac et al. 2010].", "startOffset": 38, "endOffset": 58}, {"referenceID": 10, "context": "We utilize the publicly available features provided by [Rasiwasia et al. 2010] i.", "startOffset": 55, "endOffset": 78}, {"referenceID": 0, "context": "Besides, we also present the cross-media retrieval results based on the 4,096 dimensional CNN visual features3 and the 100 dimensional Latent Dirichlet Allocation model (LDA) [Blei et al. 2003] textual features (we firstly obtain the textual feature vector based on 500 tokens and then LDA model is used to compute the probability of each document under 100 topics).", "startOffset": 175, "endOffset": 193}, {"referenceID": 6, "context": "For more details, please refer to [Krizhevsky et al. 2012].", "startOffset": 34, "endOffset": 58}, {"referenceID": 10, "context": "Results In the experiments, we mainly compare the proposed MDCR with six algorithms, including CCA, Semantic Matching (SM) [Rasiwasia et al. 2010], Semantic Correlation Matching (SCM) [Rasiwasia et al.", "startOffset": 123, "endOffset": 146}, {"referenceID": 10, "context": "2010], Semantic Correlation Matching (SCM) [Rasiwasia et al. 2010], Three-View CCA (T-V CCA) [Gong et al.", "startOffset": 43, "endOffset": 66}, {"referenceID": 1, "context": "2010], Three-View CCA (T-V CCA) [Gong et al. 2013], Generalized Multiview Marginal Fisher Analysis (GMMFA) [Sharma et al.", "startOffset": 32, "endOffset": 50}, {"referenceID": 13, "context": "2013], Generalized Multiview Marginal Fisher Analysis (GMMFA) [Sharma et al. 2012] and Generalized Multiview Linear Discriminant Analysis (GMLDA) [Sharma et al.", "startOffset": 62, "endOffset": 82}, {"referenceID": 13, "context": "2012] and Generalized Multiview Linear Discriminant Analysis (GMLDA) [Sharma et al. 2012].", "startOffset": 69, "endOffset": 89}, {"referenceID": 10, "context": "For the Wikipedia dataset, we firstly compare the proposed MDCR with other methods based on the publicly available features [Rasiwasia et al. 2010], i.", "startOffset": 124, "endOffset": 147}, {"referenceID": 18, "context": "With a different train/test division, [Wu et al. 2014] achieved an average mAP score of 0.", "startOffset": 38, "endOffset": 54}, {"referenceID": 15, "context": "224) through a sparse hash model and [Wang et al. 2014] achieved an average mAP score of 0.", "startOffset": 37, "endOffset": 55}, {"referenceID": 4, "context": "In addition, we also compare our method with the recent work [Kang et al. 2014], which utilizes 4,096-CNN for images and 200-LDA for text, in Table III.", "startOffset": 61, "endOffset": 79}, {"referenceID": 4, "context": "Cross-media retrieval comparition with results of four methods reported by [Kang et al. 2014] on the Wikipedia dataset.", "startOffset": 75, "endOffset": 93}], "year": 2015, "abstractText": "In this paper, we investigate the cross-media retrieval between images and text, i.e., using image to search text (I2T) and using text to search images (T2I). Existing cross-media retrieval methods usually learn one couple of projections, by which the original features of images and text can be projected into a common latent space to measure the content similarity. However, using the same projections for the two different retrieval tasks (I2T and T2I) may lead to a tradeoff between their respective performances, rather than their best performances. Different from previous works, we propose a modality-dependent cross-media retrieval (MDCR) model, where two couples of projections are learned for different cross-media retrieval tasks instead of one couple of projections. Specifically, by jointly optimizing the correlation between images and text and the linear regression from one modal space (image or text) to the semantic space, two couples of mappings are learned to project images and text from their original feature spaces into two common latent subspaces (one for I2T and the other for T2I). Extensive experiments show the superiority of the proposed MDCR compared with other methods. In particular, based the 4,096 dimensional convolutional neural network (CNN) visual feature and 100 dimensional LDA textual feature, the mAP of the proposed method achieves 41.5%, which is a new state-of-the-art performance on the Wikipedia dataset.", "creator": "TeX"}}}