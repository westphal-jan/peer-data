{"id": "1506.06832", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "23-Jun-2015", "title": "Detection and Analysis of Emotion From Speech Signals", "abstract": "Recognizing emotion from speech has become one the active research themes in speech processing and in applications based on human-computer interaction. This paper conducts an experimental study on recognizing emotions from human speech. The emotions considered for the experiments include neutral, anger, joy and sadness. The distinuishability of emotional features in speech were studied first followed by emotion classification performed on a custom dataset. The classification was performed for different classifiers. One of the main feature attribute considered in the prepared dataset was the peak-to-peak distance obtained from the graphical representation of the speech signals. After performing the classification tests on a dataset formed from 30 different subjects, it was found that for getting better accuracy, one should consider the data collected from one person rather than considering the data from a group of people.", "histories": [["v1", "Tue, 23 Jun 2015 00:28:08 GMT  (460kb)", "http://arxiv.org/abs/1506.06832v1", "2nd International Symposium on Computer Vision and the Internet, 2015; to appear in Procedia Computer Science Journal, Elsevier, 2015"]], "COMMENTS": "2nd International Symposium on Computer Vision and the Internet, 2015; to appear in Procedia Computer Science Journal, Elsevier, 2015", "reviews": [], "SUBJECTS": "cs.SD cs.CL cs.HC", "authors": ["assel davletcharova", "sherin sugathan", "bibia abraham", "alex pappachen james"], "accepted": false, "id": "1506.06832"}, "pdf": {"name": "1506.06832.pdf", "metadata": {"source": "CRF", "title": "Detection andAnalysis of Emotion From Speech Signals", "authors": ["Assel Davletcharovaa", "Sherin Sugathanb", "Bibia Abrahamc", "Alex Pappachen Jamesa"], "emails": ["apj@ieee.org"], "sections": [{"heading": null, "text": "This year it has come to the point where it only takes a few days for it to come to a conclusion."}], "references": [{"title": "Emotional speech recognition: Resources, features, and methods, Speech communication", "author": ["D. Ververidis", "C. Kotropoulos"], "venue": null, "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2006}, {"title": "Emotion recognition system using short-term monitoring of physiological signals, Medical and biological engineering and computing", "author": ["K.H. Kim", "S. Bang", "S. Kim"], "venue": null, "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2004}, {"title": "Some effective techniques for naive bayes text classification, Knowledge and Data Engineering, IEEE Transactions on", "author": ["S.-B. Kim", "K.-S. Han", "H.-C. Rim", "S.H. Myaeng"], "venue": null, "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2006}, {"title": "Emotions and heart rate while sitting on a chair", "author": ["J. Anttonen", "V. Surakka"], "venue": "in: Proceedings of the SIGCHI conference on Human factors in computing systems,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2005}, {"title": "The effects of emotions on short-term power spectrum analysis of heart rate variability, The American journal of cardiology", "author": ["R. McCraty", "M. Atkinson", "W.A. Tiller", "G. Rein", "A.D. Watkins"], "venue": null, "citeRegEx": "5", "shortCiteRegEx": "5", "year": 1995}, {"title": "Direct speech feature estimation using an iterative em algorithm for vocal fold pathology", "author": ["L. Gavidia-Ceballos", "J.H. Hansen"], "venue": null, "citeRegEx": "6", "shortCiteRegEx": "6", "year": 1996}, {"title": "Riski, A noninvasive technique for detecting hypernasal speech using a nonlinear operator, Biomedical Engineering, IEEE Transactions on", "author": ["D.A. Cairns", "J.E.J.H. Hansen"], "venue": null, "citeRegEx": "7", "shortCiteRegEx": "7", "year": 1996}, {"title": "Malandraki, Online collaboration environments in telemedicine applications of speech therapy, in: Engineering", "author": ["C. Pierrakeas", "G.V. Georgopoulos"], "venue": "inMedicine and Biology Society,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2005}, {"title": "A novel method for classifying body mass index on the basis of speech signals for future clinical applications: A pilot study, Evidence-Based Complementary andAlternative Medicine", "author": ["B.J. Lee", "B. Ku", "J.-S. Jang", "J.Y. Kim"], "venue": null, "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2013}, {"title": "Ecg based integrated mobile tele medicine system for emergency health tribulations", "author": ["R. Gamasu"], "venue": "Int J Biosci Biotechnol", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2014}, {"title": "Plp and rasta (and mfcc, and inversion) in matlab using melfcc. m and invmelfcc", "author": ["D. Ellis"], "venue": null, "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2006}, {"title": "Whitman,A large-scale evaluation of acoustic and subjective music-similarity measures, Computer Music Journal", "author": ["A. Berenzweig", "B. Logan", "B.D.P. Ellis"], "venue": null, "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2004}, {"title": "Mel frequency cepstral coefficients for music modeling", "author": ["B. Logan"], "venue": "in: ISMIR,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2000}], "referenceMentions": [{"referenceID": 0, "context": "Emotion classification[1] is one of the most challenging tasks in a speech signal processing domain.", "startOffset": 22, "endOffset": 25}, {"referenceID": 1, "context": "So the heart rate of a person can also be measured to get information about the emotional status of person[2, 3].", "startOffset": 106, "endOffset": 112}, {"referenceID": 2, "context": "So the heart rate of a person can also be measured to get information about the emotional status of person[2, 3].", "startOffset": 106, "endOffset": 112}, {"referenceID": 3, "context": "The work in [4] says that if there is a negative stimuli that causes negative emotion the heart rate decelerate more actively than when there is positive stimuli [4].", "startOffset": 12, "endOffset": 15}, {"referenceID": 3, "context": "The work in [4] says that if there is a negative stimuli that causes negative emotion the heart rate decelerate more actively than when there is positive stimuli [4].", "startOffset": 162, "endOffset": 165}, {"referenceID": 4, "context": "In [5], it is argued that heart rate variation (HRV) has some dependency on emotional state of person.", "startOffset": 3, "endOffset": 6}, {"referenceID": 5, "context": "The idea proposed in [6] used speech parameters for detection and analysis of the vocal fold pathology.", "startOffset": 21, "endOffset": 24}, {"referenceID": 6, "context": "In [7], an algorithm was developed for the detection of hypernasal resonance.", "startOffset": 3, "endOffset": 6}, {"referenceID": 1, "context": "The work in [2, 3] studies the dependency of the physiology with human emotions in application to HCI.", "startOffset": 12, "endOffset": 18}, {"referenceID": 2, "context": "The work in [2, 3] studies the dependency of the physiology with human emotions in application to HCI.", "startOffset": 12, "endOffset": 18}, {"referenceID": 1, "context": "The results indicate the possibility of developing an emotional relationship between humans and computers which enables the development of a human-friendly personal robot [2, 3].", "startOffset": 171, "endOffset": 177}, {"referenceID": 2, "context": "The results indicate the possibility of developing an emotional relationship between humans and computers which enables the development of a human-friendly personal robot [2, 3].", "startOffset": 171, "endOffset": 177}, {"referenceID": 3, "context": "In [4], they proposed a method of measuring the heart rate of a patient sitting on a chair.", "startOffset": 3, "endOffset": 6}, {"referenceID": 7, "context": "In [8], they conducted an experiment of speech therapy through telemedicine technology with a group of patients.", "startOffset": 3, "endOffset": 6}, {"referenceID": 7, "context": "The experiment showed that interaction among patients through speech accelerates their recovery and positively influence on the quality of life [8].", "startOffset": 144, "endOffset": 147}, {"referenceID": 8, "context": "In [9], they introduced a remote detection of the Body Mass Index from the speech signal.", "startOffset": 3, "endOffset": 6}, {"referenceID": 8, "context": "The importance of the telemedicine was also discussed in this study, showing its benefits from different aspects such as comfort, low cost, time efficiency, etc [9].", "startOffset": 161, "endOffset": 164}, {"referenceID": 9, "context": "The work in [10] proposed an innovative approach of using mobile phones for measuring the heart rate continuously.", "startOffset": 12, "endOffset": 16}, {"referenceID": 9, "context": "The main advantage of this design is the support for mobility of both patient and doctor[10].", "startOffset": 88, "endOffset": 92}, {"referenceID": 10, "context": "For extracting features from the recorded speech segments, MATLAB functions [11] were used.", "startOffset": 76, "endOffset": 80}, {"referenceID": 11, "context": "The Mel-cepstra takes short-time spectral shape with important data about the quality of voice and production effects [12].", "startOffset": 118, "endOffset": 122}, {"referenceID": 12, "context": "The concept of windowing is based on multiplying the signal frames by window function w[n] [13].", "startOffset": 91, "endOffset": 95}, {"referenceID": 8, "context": "The AUC is a probability of correctly identified classes and it determines quality of a classification model [9].", "startOffset": 109, "endOffset": 112}], "year": 2015, "abstractText": "Recognizing emotion from speech has become one the active research themes in speech processing and in applications based on human-computer interaction. This paper conducts an experimental study on recognizing emotions from human speech. The emotions considered for the experiments include neutral, anger, joy and sadness. The distinuishability of emotional features in speech were studied first followed by emotion classification performed on a custom dataset. The classification was performed for different classifiers. One of the main feature attribute considered in the prepared dataset was the peak-to-peak distance obtained from the graphical representation of the speech signals. After performing the classification tests on a dataset formed from 30 different subjects, it was found that for getting better accuracy, one should consider the data collected from one person rather than considering the data from a group of people.", "creator": "WPS Office"}}}