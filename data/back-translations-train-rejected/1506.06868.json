{"id": "1506.06868", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "23-Jun-2015", "title": "Learning Discriminative Bayesian Networks from High-dimensional Continuous Neuroimaging Data", "abstract": "Due to its causal semantics, Bayesian networks (BN) have been widely employed to discover the underlying data relationship in exploratory studies, such as brain research. Despite its success in modeling the probability distribution of variables, BN is naturally a generative model, which is not necessarily discriminative. This may cause the ignorance of subtle but critical network changes that are of investigation values across populations. In this paper, we propose to improve the discriminative power of BN models for continuous variables from two different perspectives. This brings two general discriminative learning frameworks for Gaussian Bayesian networks (GBN). In the first framework, we employ Fisher kernel to bridge the generative models of GBN and the discriminative classifiers of SVMs, and convert the GBN parameter learning to Fisher kernel learning via minimizing a generalization error bound of SVMs. In the second framework, we employ the max-margin criterion and build it directly upon GBN models to explicitly optimize the classification performance of the GBNs. The advantages and disadvantages of the two frameworks are discussed and experimentally compared. Both of them demonstrate strong power in learning discriminative parameters of GBNs for neuroimaging based brain network analysis, as well as maintaining reasonable representation capacity. The contributions of this paper also include a new Directed Acyclic Graph (DAG) constraint with theoretical guarantee to ensure the graph validity of GBN.", "histories": [["v1", "Tue, 23 Jun 2015 05:39:34 GMT  (464kb)", "http://arxiv.org/abs/1506.06868v1", "16 pages and 5 figures for the article (excluding appendix)"]], "COMMENTS": "16 pages and 5 figures for the article (excluding appendix)", "reviews": [], "SUBJECTS": "cs.CV cs.LG", "authors": ["luping zhou", "lei wang", "lingqiao liu", "philip ogunbona", "dinggang shen"], "accepted": false, "id": "1506.06868"}, "pdf": {"name": "1506.06868.pdf", "metadata": {"source": "CRF", "title": "Learning Discriminative Bayesian Networks from High-dimensional Continuous Neuroimaging Data", "authors": ["Luping Zhou", "Lei Wang", "Lingqiao Liu", "Philip Ogunbona", "Dinggang Shen"], "emails": ["philipo@uow.edu.au."], "sections": [{"heading": null, "text": "ar Xiv: 150 6.06 868v 1 [cs.C V] 23 Jun 2015 1Index Terms - Bayesian Network, Discriminative Learning, Fisher Kernel Learning, Max Margin, Brain Network."}, {"heading": "1 INTRODUCTION", "text": "An important probabilistic graphical model uses a series of conditional independence tests to ensure that the structure is consistent with the conditionality of the observation. [S an important probabilistic graphical model, Bayesian Network (BN) was used to model the probability distribution of a series of random variables for a wide range of applications, e.g. diagnosis, troubleshooting, web mining, meteorology and bioinformatics. It combines the representation of diagrams with Bayesian Analysis and provides an effective way to model the conditional dependence on the variables and derive the parameters of the probability distribution from it. A BN must be a directed acyclic graph (DAG). Two factors characterize a BN, i.e., the structure of the network (the presence / absence of edges in the diagram) and the parameters of the probability distribution."}, {"heading": "2 BACKGROUND", "text": "To complete this work in itself, we present the background to both the methodology and its application to brain network analysis. Please note that the methodology could be generalized to applications beyond the example given in this paper."}, {"heading": "2.1 Sparse Gaussian Bayesian Network (SGBN)", "text": "Since this work is based on the SGBN model, we will first analyze the basics of SGBN in [14]. All symbols are defined in Table 1. A Bayesian network (BN) G is a directed acyclic graph (DAG), i.e. there is no closed path within the graph. It expresses the factorization property of a common distribution p (x) = 1, \u00b7 \u00b7, m p (xi | Pai). The conditional probability p (xi | Pai) is assumed to follow a Gaussian distribution in Gaussian Bayesian Network (GBN). Each node xi is withdrawn via its parent node Pai."}, {"heading": "2.2 Brain Network Analysis", "text": "Neuroimaging modalities and analytical techniques can provide more sensitive and consistent measurements than traditional cognitive assessments for early diagnosis of disease. Many mental disorders are associated with subtle abnormalities throughout the brain, rather than with an individual brain region. The \"distributive\" nature of mental disorders suggests that the interactions between brain regions (neural systems) and hence the need to examine the brain as a complex network are altered. Brain networks are mathematically represented by graphic models that can be constructed from neuroimaging data as follows. Brain images belonging to different subjects are first spatially aligned to a common stereotaxic space, causing affine or deformable transformations, and then divided into regions of interest (ROI), i.e. clusters of imaging voxels that use either data-driven methods or predefined brain atlas. A brain network is then aligned with each region of the brain, corresponding to a node with each region of the node."}, {"heading": "3 PROPOSED DISCRIMINATIVE LEARNING OF GENERATIVE SGBN", "text": "The BN models are by definition generative models that focus on how the data can be generated by an underlying process. In neuroimage analysis, these models represent the effective brain connectivity of the given population. In classification, e.g. to identify AD patients from healthy people, the SGBN models are trained separately for each class. Subsequently, a new sample xi is assigned to the class with the higher likelihood of SGBN. This can ignore some subtle but critical network components that distinguish the classes. Therefore, we argue that the parameters of the generative model should be learned jointly by the two classes in order to preserve the essential discrimination. Integration of generative and discriminatory models is an important research topic in machine learning. [29] The related approaches are roughly divided into three categories: blending, staging and iterative methods. In blending methods, both the discriminatory and the generative terms of the BN method are used."}, {"heading": "3.1 Proposed Fisher-kernel-induced Discriminative Learning (KL-SGBN)", "text": "The algorithm is shown in Figure 1 and presented as follows: In view of two classes in comparison, two SGBN models (with the parameters of \"1\" and \"2\") are learned, each sample being represented by a new feature vector (called Fisher vector), which represents a function of the gradient space of SGBN-2. These sample-specific feature vectors are then fed into an SVM classification to minimize their generalization errors by adapting (Section 3.1.2). The obtained optimal vector of \"1 and 2\" encodes the discriminatory information and thus improves the original SGBNs.5 In this way, we convert the discrimination errors by adapting to Section 3.1.2)."}, {"heading": "3.2 Proposed Max-margin-based Discriminative Learning (MM-SGBN)", "text": "We believe that discrimination against SGBNs can be further improved if we can directly optimize their (instead of SGBNs) classification performance. Therefore, we propose a new learning framework that builds directly on SGBNs. We call this method MM-SGBNs. For binary classification, maximizing the minimum margin between two classes, we can maximize the minimum conditional likelihood ratio (MCLR) [18]: MCLR (nmin i = 1P)."}, {"heading": "3.3 Discussion and Analysis", "text": "The proposed discriminatory learning frameworks produce a series of jointly learned SGBN models, one for each class. Based on these SGBN models, two types of classifiers can be constructed, i.e., the SGBN classifiers are derived from the SGBN models; the SGBN classifiers are derived from the SGBN models; the SVM classifiers are derived from the SGBN models; the two classifiers are coupled from the underlying SGBN models; and the discriminatory SGBN models lead directly to a better SGBN classification and can provide discriminatory vectors for a better classification."}, {"heading": "4 PROPOSED DAG CONSTRAINT", "text": "In this section, we revisit H-SGBN and propose a new DAG restriction that could simplify the optimization problems in the SGBN and its discriminatory learning process, as introduced in Sections 3.1 and 3.2."}, {"heading": "4.1 H-SGBN Revisited", "text": "Remember that the DAG constraint in H-SGBN (Section 2.1) uses matrix P, an implicit function of what makes the optimization problem in Eqn considerably more difficult. In [14], for simplicity's sake, P is first treated as a constant in each optimization iteration as it optimizes iteration, and then recalculated by searching for the binarized new Economy. This hard binarization could introduce a high discontinuity of the Economy into the optimization. In other words, solving these problems by BCD column-by-column could mitigate the problem, since in each iteration only one column of the Economy is changed, resulting in less discontinuity. However, we observe that the solution of BCD depends on which column of the Economy 9 is to be optimized first. In other words, if we arbitrarily mute the order of features (the columns in the GX modality, we will get the BAG's different GNN interpretation, which will affect the SNN."}, {"heading": "4.2 Proposed DAG constraint", "text": "It is well known that a BN is synonymous with a topological order (page 362 in [37]). Therefore, we propose a new DAG restriction that is applicable to continuous variables with GBN based on this equivalence. With a few linear inequalities and variables that differ from others, the new DAG restriction significantly simplifies the positive number applied in [14]. We propose a sufficient and necessary condition for G to be DAG than in Proposition 1. In view of an economical Gaussian Bayesian network parameter that is directed by the AG and its associated graph with m-nodes, the graph G is if and only if there is some oi."}, {"heading": "4.3 Estimation of SGBN from A Single Class", "text": "With our DAG restriction in Eqn. (4.1), we could estimate SGBN from a single class as a starting point for our discriminatory learning of KL-SGBN or MM-SGBN (4.1). (In particular, we can optimize in this area, o, m \u00b2 x:, i \u2212 PA i. (6), i \u00b2), i \u00b2 (6), i \u00b2 (6), i \u00b2 (6), i \u00b2 (6), i \u00b2), i \u00b2 (6), II), II (6), II), II (6), II (6), II), II (6), II), II (6), II), II (6), II), II (6), II), II (6), II), II (6), II (6), II (6), II)."}, {"heading": "5 EXPERIMENT", "text": "In this section, we will examine the characteristics of our proposed methods from three aspects: DAG limitation, discriminatory learning and the resulting connectivity for brain network analysis. Four experiments will be conducted, summarized in Table 2. The data sets and experiments will be elaborated as follows."}, {"heading": "5.1 Neuroimaging Data Sets", "text": "We are conducting our experiment on the publicly available ADNI [42] database to analyze the effective connectivity of the brain to Alzheimer's disease. In addition, three sets of data from two imaging modalities of MRI and FDG-PET downloaded from ADNI are used, which are pre-processed using the typical method of intensity correction, skull stripping and cerebellar removal. We segment the images into grey matter (GM), white matter (WM), anderebrospinal fluid (CSF), which are pre-processed with the standard FSL6 package, with brain activity (ROI) based on an ROI atlas [43] and spatial normalization. The GM volume of each ROI is used as an imaging feature to characterize each network."}, {"heading": "5.2 DAG Constraint", "text": "With our proposed DAG method, the SGBN method has been downloaded by the authors. \"The SGBN learning method for a single class can be learned with all the parameters that are optimized together (OR-SGBN). The SGBN learning method is used to explicitly indicate whether the parameters are optimized together (WHOLE). The SGBN learning method for a single class in Algorithm 3, implemented with the package of CVX. H-SGBN (BCD) is the SGBN learning method for a single class in Algorithm 3, implemented with the package of CVX."}, {"heading": "5.3 Comparison of Discrimination", "text": "We look at two types of classifiers: i) the SGBN classification systems (with two SGBN models, one for each class) and ii) the SVM classification induced by the SGBN models. In this experiment, we test whether our learning methods (KL-SGBN and MM-SGBN) can improve discriminatory performance on both types of classifiers for the real neuroimaging datasets. Initial SGBN models are achieved by our proposed OR-SGBN, as it has been shown to be more robust than H-SGBN as above. For the SGBN classification, we anticipate right away, we associate a test sample to the class with higher probability."}, {"heading": "5.4 Comparison of Connectivity", "text": "We are conducting an investigation to gain some insights into the learned brain networks for the sick and healthy populations, but not for all. In this experiment, we are visualizing the learned brain networks and comparing the connectivity patterns obtained by different methods and from different populations. MRI-II data set is used for this study as it covers regions extending across the four lobes of the brain. Structures of the brain networks recovered from NC and MCI groups are shown in Figure 4, using H-SGBN and OR-SGBN (WHOLE), respectively, the reticular structure is achieved by threshold values of edge weights associated with a cut-off value of 0.01 and MCI groups. Each row i represents the effective connections (dark dots), and each column j represents the effective connections ending at the nodes."}, {"heading": "6 CONCLUSION", "text": "In this paper, we focus on the discriminatory learning of the Bayean network for continuous variables, especially for neuroimaging data. Two discriminatory learning frameworks are proposed to achieve this goal, i.e. KL-SGBN improves the performance of SVM classifiers based on SGBN-induced characteristics, and MM-SGBN explicitly optimizes an SGBN-based criterion for classification. We show how to use Fisher kernels to bridge the generative methods of SGBN and the discriminatory classifiers of SVM, and how to embed the maximum marginal criterion in SGBN for discriminatory learning. Optimization problems are analyzed in detail, and the pros and cons of the proposed methods are discussed. In addition, a new DAG restriction is proposed to ensure the validity of the graph with theoretical guarantee and validation on the benchmark data, and to apply the SVGN benchmark effectiveness to both the proposed models of the early assessment of the SVGAD, while also observing the proposed method of benchmarking the SVGN for the ability of the brain."}, {"heading": "It holds that f(\u0398(t+1),o(t+1),\u03a5(t+1)) \u2264 f(\u0398(t+1),o(t),\u03a5(t)). Also, f(\u0398(t+1),o,\u03a5) is a linear function with respect", "text": "This means that the optimization problem in Eqn. (2) convergence with the alternative optimization strategy is guaranteed because the objective function is lower limited and decreases monotonously with the iteration number. Second, if the optimization problem in Eqn. (2) convergence with the alternative optimization strategy is guaranteed, the results in Eqn. (2) 2 + m3. (2) convergence in Eqn. (2) 2 + m3. (2 \u2212 2) convergence in Eqn. (1 + m)."}], "references": [], "referenceMentions": [], "year": 2015, "abstractText": "Due to its causal semantics, Bayesian networks (BN) have been widely employed to discover the underlying data<lb>relationship in exploratory studies, such as brain research. Despite its success in modeling the probability distribution of variables,<lb>BN is naturally a generative model, which is not necessarily discriminative. This may cause the ignorance of subtle but critical<lb>network changes that are of investigation values across populations. In this paper, we propose to improve the discriminative power<lb>of BN models for continuous variables from two different perspectives. This brings two general discriminative learning frameworks for<lb>Gaussian Bayesian networks (GBN). In the first framework, we employ Fisher kernel to bridge the generative models of GBN and the<lb>discriminative classifiers of SVMs, and convert the GBN parameter learning to Fisher kernel learning via minimizing a generalization<lb>error bound of SVMs. In the second framework, we employ the max-margin criterion and build it directly upon GBN models to explicitly<lb>optimize the classification performance of the GBNs. The advantages and disadvantages of the two frameworks are discussed and<lb>experimentally compared. Both of them demonstrate strong power in learning discriminative parameters of GBNs for neuroimaging<lb>based brain network analysis, as well as maintaining reasonable representation capacity. The contributions of this paper also include<lb>a new Directed Acyclic Graph (DAG) constraint with theoretical guarantee to ensure the graph validity of GBN.", "creator": "LaTeX with hyperref package"}}}