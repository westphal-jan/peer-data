{"id": "1506.07300", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "24-Jun-2015", "title": "Flexible Multi-layer Sparse Approximations of Matrices and Applications", "abstract": "The computational cost of many signal processing and machine learning techniques is often dominated by the cost of applying certain linear operators to high-dimensional vectors. This paper introduces an algorithm aimed at reducing the complexity of applying linear operators in high dimension by approximately factorizing the corresponding matrix into few sparse factors. The approach relies on recent advances in non-convex optimization. It is first explained and analyzed in details and then demonstrated experimentally on various problems including dictionary learning for image denoising, and the approximation of large matrices arising in inverse problems.", "histories": [["v1", "Wed, 24 Jun 2015 10:02:13 GMT  (570kb,D)", "https://arxiv.org/abs/1506.07300v1", null], ["v2", "Tue, 29 Mar 2016 07:56:08 GMT  (491kb,D)", "http://arxiv.org/abs/1506.07300v2", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["luc le magoarou", "r\\'emi gribonval"], "accepted": false, "id": "1506.07300"}, "pdf": {"name": "1506.07300.pdf", "metadata": {"source": "CRF", "title": "Flexible Multi-layer Sparse Approximations of Matrices and Applications", "authors": ["Luc Le Magoarou", "R\u00e9mi Gribonval"], "emails": ["(luc.le-magoarou@inria.fr)", "(remi.gribonval@inria.fr)", "pubs-permissions@ieee.org."], "sections": [{"heading": null, "text": "In fact, most of them are able to set out in search of the new paths they want to take in order to conquer the world."}, {"heading": "II. PROBLEM FORMULATION", "text": "Notation: In this work, matrices are denoted by bold uppercase letters: A; vectors by bold lowercase letters: a; the ith column of a matrix A by: ai; and sentences by calligraphic symbols: A. The default vectorization operator is denoted by vec (\u00b7). The \"0 norm is denoted by the number of non-zero entries."}, {"heading": "A. Objective", "text": "The aim of this thesis is to introduce a method to connect a FA\u00b5ST with an operator of interest. Consider a linear operator corresponding to the matrix A-Rm \u00b7 n. The aim is to find sparse factors Sj-Raj + 1 \u00b7 aj, j-J {1.. J} with a1 = n and aJ + 1 = m, so that A-Rm-J = 1 Sj. This naturally leads to an optimization problem of the form: Minimize S1,..., SJ-J-J-J = 1 Sj-J-2 data fidelity + J-J = 1gj (Sj) economy-inducing penalty, (2) to balance data fidelity and sparseness of the factors."}, {"heading": "B. Expected benefits of FA\u00b5STs", "text": "A multi-layered approximation of an operator A brings several advantages, provided the relative complexity of the factorized form is small in relation to the dimensions of A. For the sake of conciseness, let's introduce the total set of non-zero entries into the Jth factor, and stot = 1 the total number of non-zero entries in the total factorization. Definition III.1. Relative complexity (abbreviated to RC) is the ratio between the total number of non-zero entries in the FA\u00b5ST and the number of non-zero entries in the total factorization. Definition II.3 It is also interesting to introduce the Relative Complexity Gain (RCG), which simply introduces the inversion of the relative complexity into the relative complexity of entries in the FA\u00b5ST and the number of non-zero entries in the RC."}, {"heading": "C. Related work", "text": "Some are very classic tools of numerical linear algebra, such as the truncated SVD, while others have appeared more recently in signal processing and machine learning processes.) The truncated SVD: To reduce the computational complexity of a linear operator, the most classic approach is perhaps to calculate a lower value using the truncated SVD methods. The truncated SVD and four FA\u00b5STs are compared with different configurations (more details in section V) compared with the relative operator norm."}, {"heading": "III. OPTIMIZATION FRAMEWORK", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "A. Objective function", "text": "In this paper, the penalties gj (\u00b7), which appear in the general form of the optimization problem (2), are chosen as indicator functions gj (\u00b7) by stakeholders gj. To avoid the scaling ambiguities that naturally arise when stakeholders are (positively) homogeneous.2 It is customary [15], [32] to normalize the factors and introduce a multiplicative scalar number into the concept of privacy. Therefore, let us normalize the sets of normalized factors Nj = {S-Raj + 1 \u00b7 aj: \"Normalize the total number of stakeholders [S-F = 1} and specify the following form for the constraints: Ej = Nj-SJ-Sj, where Sj imposes thrift explicitly or implicitly, leading to the following optimization problem:\" Minimize, \"S1, SJ-S1, SJ-SJ-SJ-SJ-S1.\""}, {"heading": "B. Algorithm overview", "text": "Problem (4) is highly non-convex, and the parity-inducing penalties are typically not smooth. Nevertheless, based on recent advances in nonconvex optimization, it is possible to propose an algorithm with convergence guarantees up to a stationary point of the problem. In [18], the authors consider cost functions dependent on N blocks of variables of the form: \u03a6 (x1,..., xN): = H (x1,., xN) + N \u2211 j = 1 fj (xj), (5) where the function H is smooth and the penalties are smooth and semi-continuous (the exact assumptions are given below). It should be stressed that no convergence of any kind is assumed. Here, for simplicity, we assume that the penalties Fj's are indicator functions of constraints that set Tj x. To handle this objective function, the authors propose an algorithm called Proximal Lineating Minimization (PALM)."}, {"heading": "C. Algorithm details", "text": "It is quite easy to see that there is a concordance between (4) and (5) by drawing on (4) and (5) the same concordance between (4) and (5). (4) Drawing on the concordance between (4) and (5). (4) Drawing on the concordance between (4) and (5). (4) Drawing on the concordance between (4) and (5). (5) Drawing on the concordance between (6) and (6): (6) and (6): (6). (4) Drawing on the concordance between (4) and (6): 1) and (6). (4) Drawing on the concordance between (4) and (6): 1 and (6): 1. (4) Drawing on the concordance between (4) and (6): 1. (6) and (6)."}, {"heading": "IV. HIERARCHICAL FACTORIZATION", "text": "The algorithm shown in Figure 4 factorises an input matrix corresponding to an operator of interest into sparse J factors and converges to a stationary point of the problem shown in Figure 4. In practice, one is only interested in those stationary points where the data adaptation to the concept of cost function is low, but since there is no general convergence guarantee for such a stationary point for any generic non-convex optimization algorithm, this fact is illustrated by a very simple experiment in which the algorithm palm4MSA is applied to an input operator, the A-Rn \u00b7 n with a known sparse form A = N j = 1 Sj, such as the Hadamard transformation (in this case N = log2n). The naive approach is to set J = N directly in palm4MSA and set the constraints so that the actual scarcity of true factors (as shown in Figure 1) is reflected."}, {"heading": "A. Parallel with deep learning", "text": "Similar problems arose in the neural network community, where it was difficult to optimize the weights of neural networks, which consist of many hidden layers (see [38] for an overview of this topic). Until recently, deep networks were often neglected in favor of flat architectures. However, in the last decade, [39] it was proposed to optimize the network not as a large block, but as a single layer, and then to optimize the entire network globally by means of gradient descent. This heuristics proved to work experimentally well for various tasks [37]. Specifically, it was proposed to first perform a pre-training of the layers (each layer being fed with the characteristics generated by the immediately below layer, and the lowest ones fed with the data) in order to initialize the weights in a good region, and then perform a global fine-tuning of all layers by simple gradient decrease."}, {"heading": "B. Proposed hierarchical algorithm", "text": "We have stated that in every step it can come to a total optimization of all the factors introduced so far in order to enable the adaptation to the real world. (D) It is not only the way in which the real world of the real world of the real world of the real world of the real world of the real world of the real world of the real world of the real world of the real world of the real world of the real world of the real world of the real world of the real world of the real world of the real world of the real world of the real world of the real world of the real world of the real world of the real world of the real world of the real world of the real world of the real world of the real world of the real world of the real world of the world of the world of the world of the world of the world of the world of the world of the world of the world of the real world of the world of the world of the world of the world of the world of the world of the real world of the world of the world of the world of the real world of the world of the real world of the world of the real world of the real world of the real world of the real world of the real world of the real world of the real world of the real world of the real world of the real world of the real world of the real world of the real world of the real world of the real world of the real world of the real world of the real world of the real world of the real world of the real world of the real world of the real world of the real world of the real world of the real world of the real world of the real world of the real world of the real world of the real world of the real world of the real world of the real world of the real world of the real world of the real world of the real world of the real world of the real world of the real world of the real world of the real world of the real world of the real world of the real world of the real world of the real world of the real world of the real world of the real world of the real world of the real world of the real world of the real world of the real world of the real world of the real world of the real world of the real world of the real world of the real world of the real world of the real world of the real world of the real world of the real world of the"}, {"heading": "V. ACCELERATING INVERSE PROBLEMS", "text": "A natural application of FA\u00b5STs are linear inverse problems where a high-dimensional vector \u03b3 must be retrieved from some observed data y \u2248 M\u03b3. As already mentioned in Section II-B, it can be assumed that iterative proximal algorithms will be significantly accelerated if M is well approximated with a FA\u00b5ST of low relative complexity, e.g. using the proposed hierarchical factorization algorithm applied to A = M. In practice, the total number of factors J and the constraints E, E. In our technical report [1], a preliminary study on synthetic data was conducted showing that a flexible trade-off between relative complexity and adaptation to the input matrix can be achieved. Here, we use the rule of thumb presented in Section III-C to deepen the investigation of this question for a matrix M that arises in a real biomedical inverse problem."}, {"heading": "A. Factorization compromise: MEG operator", "text": "In this experiment, we explore the use of FA\u00b5ST in the context of functional brain imaging using magnetoencephalography (MEG) and electroencephalography (EEG). [4] The results of the study. \"[5] It is a high-dimensional regression problem that requires appropriate regulation. Since it is natural to assume that a limited set of brain focus is active during a cognitive task, sparse focal source configurations are often advanced using convex priors. [41] The bottlenecks in optimization algorithms are the dot products with the forward matrix and its transpose. The goal of this experiment is to observe achievable trade-offs between relative complexity and accuracy."}, {"heading": "B. Source localization experiment", "text": "The aforementioned brainconsecrated brainconsecrated brainconsecrated brainconsecrated brainconsecrated brainconsecrated brainconsecrated brainconsecrated brainconsecrated brainconsecrated brainconsecrated brainconsecrated brainconsecrated tecnteeaeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeaeVnln rnnln rf\u00fc the braingeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeee"}, {"heading": "VI. LEARNING FAST DICTIONARIES", "text": "Multi-layered, sparse approximations of operators are particularly suitable for selecting efficient dictionaries for data processing tasks."}, {"heading": "A. Analytic vs. learned dictionaries", "text": "Traditionally, there are two ways to select a dictionary for sparse signal representations [13]. Historically, the only way to invent a dictionary has been to mathematically analyze the data and derive a \"simple\" formula for creating the dictionary. Dictionaries designed in this way are usually called analytical dictionaries [13] (e.g. associated with Fourier, Wavelets, and Hadamard transformations). Due to the relative simplicity of analytical dictionaries, they usually have a familiar sparse form, such as Fast Fourier Transform (FFT) [3] or Discrete Wavelet Transform (DWT) [6]. On the other hand, the development of modern computers has enabled the emergence of automatic methods that learn a dictionary directly from the data [44] - [46]. Given some raw data Y-Rm \u00d7 L, the principle of dictionary learning is to approximate Y through the product of a dictionary."}, {"heading": "B. The best of both worlds", "text": "In practice, we have taken a somewhat hierarchical approach in order to modify terms, and the approach to terms is as simple as it is. (The approach to the terminology of terms is so simple that it is based on the terminology of terms. [13] The approach to the terminology of terms to the terminology of terms is so different that the terminology of terms in the traditional structure of dictatorial learning uses algorithms [13] as shown in Figure 10. (The consequence is that the coefficients can be updated by evaluating the conceptual structure of dictatorial learning.) The procedure described below uses a batch method for dictatorial updates, but the approach is a priority-compatible method with stochastic gradient updates for even more efficient hierarchy.In practice, we are too modifying terms to be a little hierarchical."}, {"heading": "D. Sample complexity of FA\u00b5STs", "text": "The good performance of FA\u00b5ST dictionaries compared to the dense dictionaries observed above may be surprising, since the more restricted structure of such dictionaries (compared to dense dictionaries) may prevent them from providing good approximate values for the areas under consideration. A possible element of the explanation arises from the concept of sampling complexity: As stated in Section II-B, the statistical significance of learned multilayered operators is expected to be improved by a reduced sampling complexity compared to those of dense operators. In the context of dictionary learning, sampling complexity indicates how many training samples should be taken from L so that the empirical risk (with a high probability) is uniformly close to its expectation [50], [51]. In the context of dictionary learning, sampling complexity indicates how many training samples should be taken from L so that the empirical risk (with a high probability) is uniformly close to its expectation."}, {"heading": "VII. CONCLUSION AND FUTURE WORK", "text": "The underlying factorization algorithm is based on recent advances in non-convex optimization and has convergence guarantees; the proposed approach is to hierarchically factorize the input matrix in the hope of achieving better local minima, as is the case, for example, in deep learning; the factorization algorithm used is fairly general and capable of taking into account various constraints; a practical limitation of interest is the scarcity (various forms) that has several interesting properties; in fact, multi-layered, sparsely factorized linear operators have several advantages over classical dense operators, such as increased speed of manipulation, a lighter memory footprint and higher statistical significance when estimated on training data; the interest and versatility of the proposed factorization approach has been demonstrated using various methods, including a localization where the proposed method is compared to the cost."}, {"heading": "APPENDIX A PROJECTION OPERATORS", "text": "In this appendix, the projection operators are given ontoseveral constraint sets of interest for practical applications."}, {"heading": "A. General sparsity constraint", "text": "Let us first consider the following general constraint of thrift: E: = {S,.., HK} forms a division of the index set, si,.. \u2212 N,. \u00b2, K \u00b2, and ST is the matrix whose entries match those of S on T and are set to zero elsewhere. Given a certain matrix U,.. \u00b2, we would like to see their projection on the set E: PE (U),. \u2212 arg minS, and ST is the matrix whose entries match those of S on T and are set to zero elsewhere. Given some matrix U,."}, {"heading": "B. Sparse and piecewise constant constraints", "text": "Given K pairs of divided Ci index matrix entries, we now consider the constraint corresponding to unit standard matrices that are constant over each index, zero outside of these sets, with no more than s non-zero ranges. In other words: Ec: = {S-Rp \u00b7 q: a-a-a-a-a-a-c-a-0-s, SCi = a-i-i-i-i-a-a-a-1,..., K}, S-i-Ci = 0, and-S-F = 1-a-a-Ki = 1-a-i-s, and denote J-a-1,."}, {"heading": "APPENDIX B LIPSCHITZ MODULUS", "text": "To estimate the Lipschitz module of the gradient of the smooth part of the lens, we write: HQ-SijH (L, S1, R, \u03bbi) - HQ-SijH (L, S2, R, \u03bbi) - HQ-F = (\u03bbi) 2 HQ-LTL (S1 \u2212 S2) RRT-HQ-F \u2264 (\u03bbi) 2 HQ-R-22. HQ-L-2 2 HQ-S1 \u2212 S2-F."}, {"heading": "APPENDIX C COVERING DIMENSION", "text": "The coverage number N (A,) of a set A is the minimum number of spheres with a radius that are needed to cover it. The exact definition of the coverage numbers is given in [20]. The coverage dimension of the set, loosely referred to as the coverage dimension in the text, is d (A) = lim \u2192 0 logN (A,) log 1 /. We are interested in the coverage dimension of the set FA\u00b5STs Dspfac. We start with the elementary sets Ej = {A) Raj \u00b7 aj + 1: 0 \u2264 sj, \u0445A \u0432 F = 1}. These quantities can be used as sets of sparsely normalized vectors of the size aj \u2212 spfac + 1. This leads to [20] (with the Frobenius standard): N (Ej,)."}, {"heading": "ACKNOWLEDGMENT", "text": "The authors would like to thank Franc ois Malgouyres and Olivier Chabiron for the discussions that helped to create this work, and Alexandre Gramfort for providing the MEG data and contributing to [17]. Finally, the authors would like to thank the reviewers for their valuable comments."}], "references": [{"title": "Learning computationally efficient dictionaries and their implementation as fast transforms,", "author": ["L. Le Magoarou", "R. Gribonval"], "venue": "CoRR, vol. abs/1406.5388,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2014}, {"title": "Chasing butterflies: In search of efficient dictionaries,", "author": ["L. Le Magoarou", "R. Gribonval"], "venue": "in Acoustics, Speech and Signal Processing (ICASSP),", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2015}, {"title": "An algorithm for the machine calculation of complex Fourier series,", "author": ["J. Cooley", "J. Tukey"], "venue": "Mathematics of Computation,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 1965}, {"title": "Computation of the fast walsh-fourier transform,", "author": ["J. Shanks"], "venue": "Computers, IEEE Transactions on, vol. C-18,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 1969}, {"title": "A fast computational algorithm for the discrete cosine transform,", "author": ["W.-H. Chen", "C. Smith", "S. Fralick"], "venue": "Communications, IEEE Transactions on,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 1977}, {"title": "A theory for multiresolution signal decomposition: the wavelet representation,", "author": ["S. Mallat"], "venue": "Pattern Analysis and Machine Intelligence, IEEE Transactions on,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 1989}, {"title": "The linear complexity of computation,", "author": ["J. Morgenstern"], "venue": "J. ACM,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 1975}, {"title": "Iterative thresholding for sparse approximations,", "author": ["T. Blumensath", "M.E. Davies"], "venue": "Journal of Fourier Analysis and Applications,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2008}, {"title": "A fast iterative shrinkage-thresholding algorithm for linear inverse problems,", "author": ["A. Beck", "M. Teboulle"], "venue": "SIAM Journal on Imaging Science,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2009}, {"title": "Dictionaries for Sparse Representation Modeling,", "author": ["R. Rubinstein", "A. Bruckstein", "M. Elad"], "venue": "Proceedings of the IEEE,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2010}, {"title": "Double sparsity: Learning sparse dictionaries for sparse signal approximation,", "author": ["R. Rubinstein", "M. Zibulevsky", "M. Elad"], "venue": "Signal Processing, IEEE Transactions on,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2010}, {"title": "Toward fast transform learning,", "author": ["O. Chabiron", "F. Malgouyres", "J.-Y. Tourneret", "N. Dobigeon"], "venue": "International Journal of Computer Vision,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2015}, {"title": "The emerging field of signal processing on graphs: Extending high-dimensional data analysis to networks and other irregular domains,", "author": ["D.I. Shuman", "S.K. Narang", "P. Frossard", "A. Ortega", "P. Vandergheynst"], "venue": "Signal Processing Magazine, IEEE,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2013}, {"title": "FA\u03bcST: speeding up linear transforms for tractable inverse problems,", "author": ["L. Le Magoarou", "R. Gribonval", "A. Gramfort"], "venue": null, "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2015}, {"title": "Proximal Alternating Linearized Minimization for nonconvex and nonsmooth problems,", "author": ["J. Bolte", "S. Sabach", "M. Teboulle"], "venue": "Mathematical Programming,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2013}, {"title": "SciPy: Open source scientific tools for Python,", "author": ["E. Jones", "T. Oliphant", "P. Peterson"], "venue": null, "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2001}, {"title": "Sample complexity of dictionary learning and other matrix factorizations,", "author": ["R. Gribonval", "R. Jenatton", "F. Bach", "M. Kleinsteuber", "M. Seibert"], "venue": "Information Theory, IEEE Transactions on,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2015}, {"title": "Rapid solution of integral equations of classical potential theory,", "author": ["V. Rokhlin"], "venue": "Journal of Computational Physics,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 1985}, {"title": "A Sparse Matrix Arithmetic Based on H-matrices. Part I: Introduction to H-matrices,", "author": ["W. Hackbusch"], "venue": "Computing, vol. 62,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 1999}, {"title": "Fast computation of fourier integral operators,", "author": ["E. Cand\u00e8s", "L. Demanet", "L. Ying"], "venue": "SIAM Journal on Scientific Computing,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2007}, {"title": "Fast wavelet transforms and numerical algorithms i,", "author": ["G. Beylkin", "R. Coifman", "V. Rokhlin"], "venue": "Communications on pure and applied mathematics,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 1991}, {"title": "From Sparse Solutions of Systems of Equations to Sparse Modeling of Signals and Images,", "author": ["A.M. Bruckstein", "D.L. Donoho", "M. Elad"], "venue": "SIAM Review,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2009}, {"title": "Learning fast approximations of sparse coding,", "author": ["K. Gregor", "Y. LeCun"], "venue": "Proceedings of the 27th Annual International Conference on Machine Learning, ser. ICML", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2010}, {"title": "k-sparse autoencoders,", "author": ["A. Makhzani", "B. Frey"], "venue": "CoRR, vol. abs/1312.5663,", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2013}, {"title": "Treelets - an adaptive multiscale basis for sparse unordered data,", "author": ["A.B. Lee", "B. Nadler", "L. Wasserman"], "venue": "The Annals of Applied Statistics,", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2008}, {"title": "The sparse matrix transform for covariance estimation and analysis of high dimensional signals,", "author": ["G. Cao", "L. Bachega", "C. Bouman"], "venue": "Image Processing, IEEE Transactions on,", "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2011}, {"title": "On algorithms for sparse multi-factor NMF,", "author": ["S. Lyu", "X. Wang"], "venue": "Advances in Neural Information Processing Systems", "citeRegEx": "32", "shortCiteRegEx": "32", "year": 2013}, {"title": "Sparse matrix factorization,", "author": ["B. Neyshabur", "R. Panigrahy"], "venue": "CoRR, vol. abs/1311.3315,", "citeRegEx": "33", "shortCiteRegEx": "33", "year": 2013}, {"title": "Provable bounds for learning some deep representations,", "author": ["S. Arora", "A. Bhaskara", "R. Ge", "T. Ma"], "venue": "CoRR, vol. abs/1310.6343,", "citeRegEx": "34", "shortCiteRegEx": "34", "year": 2013}, {"title": "Wavelets on graphs via deep learning,", "author": ["R. Rustamov", "L. Guibas"], "venue": "Advances in Neural Information Processing Systems 26. Curran Associates, Inc.,", "citeRegEx": "36", "shortCiteRegEx": "36", "year": 2013}, {"title": "Greedy layerwise training of deep networks,", "author": ["Y. Bengio", "P. Lamblin", "D. Popovici", "H. Larochelle"], "venue": "Advances in neural information processing systems,", "citeRegEx": "37", "shortCiteRegEx": "37", "year": 2007}, {"title": "Learning deep architectures for ai,", "author": ["Y. Bengio"], "venue": "Found. Trends Mach. Learn.,", "citeRegEx": "38", "shortCiteRegEx": "38", "year": 2009}, {"title": "Reducing the dimensionality of data with neural networks,", "author": ["G.E. Hinton", "R.R. Salakhutdinov"], "venue": "Science, vol. 313,", "citeRegEx": "39", "shortCiteRegEx": "39", "year": 2006}, {"title": "Combining sparsity and rotational invariance in EEG/MEG source reconstruction,", "author": ["S. Haufe", "V.V. Nikulin", "A. Ziehe", "K.-R. M\u00fcller", "G. Nolte"], "venue": "NeuroImage, vol. 42,", "citeRegEx": "40", "shortCiteRegEx": "40", "year": 2008}, {"title": "Mixed-norm estimates for the M/EEG inverse problem using accelerated gradient methods,", "author": ["A. Gramfort", "M. Kowalski", "M.S. H\u00e4m\u00e4l\u00e4inen"], "venue": "Phys. Med. Biol.,", "citeRegEx": "41", "shortCiteRegEx": "41", "year": 2012}, {"title": "MNE software for processing MEG and EEG data ,", "author": ["A. Gramfort", "M. Luessi", "E. Larson", "D.A. Engemann", "D. Strohmeier", "C. Brodbeck", "L. Parkkonen", "M.S. H\u00e4m\u00e4l\u00e4inen"], "venue": "NeuroImage, vol. 86,", "citeRegEx": "42", "shortCiteRegEx": "42", "year": 2014}, {"title": "An interiorpoint method for large-scale l1-regularized least squares,", "author": ["S.-J. Kim", "K. Koh", "M. Lustig", "S. Boyd", "D. Gorinevsky"], "venue": "Selected Topics in Signal Processing, IEEE Journal of,", "citeRegEx": "43", "shortCiteRegEx": "43", "year": 2007}, {"title": "Method of optimal directions for frame design,", "author": ["K. Engan", "S. Aase", "J. Hakon Husoy"], "venue": "in Acoustics, Speech, and Signal Processing,", "citeRegEx": "44", "shortCiteRegEx": "44", "year": 1999}, {"title": "Online learning for matrix factorization and sparse coding,", "author": ["J. Mairal", "F. Bach", "J. Ponce", "G. Sapiro"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "46", "shortCiteRegEx": "46", "year": 2010}, {"title": "Efficient Implementation of the K-SVD Algorithm using Batch Orthogonal Matching Pursuit,", "author": ["R. Rubinstein", "M. Zibulevsky", "M. Elad"], "venue": null, "citeRegEx": "47", "shortCiteRegEx": "47", "year": 2008}, {"title": "The sample complexity of dictionary learning,", "author": ["D. Vainsencher", "S. Mannor", "A.M. Bruckstein"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "50", "shortCiteRegEx": "50", "year": 2011}, {"title": "K-Dimensional Coding Schemes in Hilbert Spaces,", "author": ["A. Maurer", "M. Pontil"], "venue": "Information Theory, IEEE Transactions on,", "citeRegEx": "51", "shortCiteRegEx": "51", "year": 2010}], "referenceMentions": [{"referenceID": 2, "context": "The n-dimensional Discrete Fourier Transform (DFT) is certainly the most well known linear operator with an efficient implementation: the Fast Fourier Transform (FFT) [3], allows to apply the operator in O(n log n) arithmetic operations instead of O(n) in its dense form.", "startOffset": 167, "endOffset": 170}, {"referenceID": 3, "context": "Similar complexity savings have been achieved for other widely used operators such as the Hadamard transform [4], the Discrete Cosine Transform (DCT) [5] or the Discrete Wavelet Transform (DWT) [6].", "startOffset": 109, "endOffset": 112}, {"referenceID": 4, "context": "Similar complexity savings have been achieved for other widely used operators such as the Hadamard transform [4], the Discrete Cosine Transform (DCT) [5] or the Discrete Wavelet Transform (DWT) [6].", "startOffset": 150, "endOffset": 153}, {"referenceID": 5, "context": "Similar complexity savings have been achieved for other widely used operators such as the Hadamard transform [4], the Discrete Cosine Transform (DCT) [5] or the Discrete Wavelet Transform (DWT) [6].", "startOffset": 194, "endOffset": 197}, {"referenceID": 0, "context": "Parts of this work have been presented at the conferences ICASSP 2015 [1] and EUSIPCO 2015 [2].", "startOffset": 70, "endOffset": 73}, {"referenceID": 1, "context": "Parts of this work have been presented at the conferences ICASSP 2015 [1] and EUSIPCO 2015 [2].", "startOffset": 91, "endOffset": 94}, {"referenceID": 6, "context": "1The product being taken from right to left: \u220fJ j=1 Sj = SJ \u00b7 \u00b7 \u00b7S1 algorithm given in [7], this multi-layer sparse factorization is actually the natural representation of any fast linear transform.", "startOffset": 87, "endOffset": 90}, {"referenceID": 8, "context": "State of the art algorithms addressing such problems with sparse regularization [8]\u2013[12] are known to heavily rely on matrix-vector products involving both this operator and its adjoint.", "startOffset": 84, "endOffset": 88}, {"referenceID": 9, "context": "To choose a regularizer for inverse problems, dictionary learning is a common method used to learn the domain in which some training data admits a sparse representation [13].", "startOffset": 169, "endOffset": 173}, {"referenceID": 10, "context": "We will see that recent approaches to learn fast dictionaries [14], [15] can be seen as special cases of the FA\u03bcST dictionary learning approach developed in Section VI, where the learned dictionaries are constrained to be FA\u03bcST.", "startOffset": 62, "endOffset": 66}, {"referenceID": 11, "context": "We will see that recent approaches to learn fast dictionaries [14], [15] can be seen as special cases of the FA\u03bcST dictionary learning approach developed in Section VI, where the learned dictionaries are constrained to be FA\u03bcST.", "startOffset": 68, "endOffset": 72}, {"referenceID": 12, "context": "For example, in the emerging area of signal processing on graphs [16], novel definitions of usual operators such as the Fourier or wavelet transforms have been ar X iv :1 50 6.", "startOffset": 65, "endOffset": 69}, {"referenceID": 0, "context": "This paper substantially extends the preliminary work started in [1], [2] and [17], both on the theoretical and experimental sides, with the following contributions:", "startOffset": 65, "endOffset": 68}, {"referenceID": 1, "context": "This paper substantially extends the preliminary work started in [1], [2] and [17], both on the theoretical and experimental sides, with the following contributions:", "startOffset": 70, "endOffset": 73}, {"referenceID": 13, "context": "This paper substantially extends the preliminary work started in [1], [2] and [17], both on the theoretical and experimental sides, with the following contributions:", "startOffset": 78, "endOffset": 82}, {"referenceID": 14, "context": "\u2022 A general framework for multi-layer sparse approximation (MSA) is introduced, that allows to incorporate various constraints on the sought sparse form; \u2022 Recent advances in non-convex optimization [18] are exploited to tackle the resulting non-convex optimization problem with local convergence guarantees; \u2022 A heuristic hierarchical factorization algorithm leveraging these optimization techniques is proposed, that achieves factorizations empirically stable to initialization; \u2022 The versatility of the framework is illustrated with extensive experiments on two showcase applications, linear inverse problems and dictionary learning, demonstrating its practical benefits.", "startOffset": 199, "endOffset": 203}, {"referenceID": 15, "context": "1) Storage cost: Using the Coordinate list (COO) storage paradigm [19], one can store a FA\u03bcST using O(stot) floats and integers.", "startOffset": 66, "endOffset": 70}, {"referenceID": 16, "context": "More specifically, the sample complexity is reduced [20], and better generalization properties are expected.", "startOffset": 52, "endOffset": 56}, {"referenceID": 17, "context": "This general operator compression paradigm encompasses several methods introduced in the last three decades, including the Fast Multipole Method (FMM) [21], H-matrices [22] and others [23].", "startOffset": 151, "endOffset": 155}, {"referenceID": 18, "context": "This general operator compression paradigm encompasses several methods introduced in the last three decades, including the Fast Multipole Method (FMM) [21], H-matrices [22] and others [23].", "startOffset": 168, "endOffset": 172}, {"referenceID": 19, "context": "This general operator compression paradigm encompasses several methods introduced in the last three decades, including the Fast Multipole Method (FMM) [21], H-matrices [22] and others [23].", "startOffset": 184, "endOffset": 188}, {"referenceID": 20, "context": "3) Wavelet-based compression: This operator compression paradigm, introduced in [24], is based on the use of orthogonal", "startOffset": 80, "endOffset": 84}, {"referenceID": 9, "context": "4) Dictionary learning: Given a collection of training vectors y`, 1 \u2264 ` \u2264 L gathered as the columns of a matrix Y, the objective of dictionary learning [13], [25] is to approximate Y by the product of a dictionary D and a coefficients matrix \u0393 with sparse columns, Y \u2248 D\u0393.", "startOffset": 153, "endOffset": 157}, {"referenceID": 10, "context": "In [14], the authors propose the sparse-KSVD algorithm (KSVDS) to learn a dictionary whose atoms are sparse linear combinations of atoms of a so-called base dictionary Dbase.", "startOffset": 3, "endOffset": 7}, {"referenceID": 11, "context": "In [15], the authors propose to learn a dictionary in which each atom is the composition of several circular convolutions using sparse kernels with known supports, so that the dictionary is a sparse operator that is fast to manipulate.", "startOffset": 3, "endOffset": 7}, {"referenceID": 11, "context": "This formulation is powerful, as demonstrated in [15], but limited in nature to the case where the dictionary is well approximated by a product of sparse circulant matrices, and requires knowledge of the supports of the sparse factors.", "startOffset": 49, "endOffset": 53}, {"referenceID": 21, "context": "[26].", "startOffset": 0, "endOffset": 4}, {"referenceID": 22, "context": "This is also likely to bring additional speedups to recent approaches accelerating iterative sparse solvers through learning [27]\u2013[29].", "startOffset": 125, "endOffset": 129}, {"referenceID": 23, "context": "This is also likely to bring additional speedups to recent approaches accelerating iterative sparse solvers through learning [27]\u2013[29].", "startOffset": 130, "endOffset": 134}, {"referenceID": 24, "context": "6) Statistics \u2013 factor analysis: A related problem is to approximately diagonalize a covariance matrix by a unitary matrix in factorized form (1), which can be addressed greedily [30], [31] using a fixed number of elementary Givens rotations.", "startOffset": 179, "endOffset": 183}, {"referenceID": 25, "context": "6) Statistics \u2013 factor analysis: A related problem is to approximately diagonalize a covariance matrix by a unitary matrix in factorized form (1), which can be addressed greedily [30], [31] using a fixed number of elementary Givens rotations.", "startOffset": 185, "endOffset": 189}, {"referenceID": 26, "context": "For example, sparse multi-factor NMF [32] can be seen as solving problem (2) with the Kullback-Leibler divergence as data fidelity", "startOffset": 37, "endOffset": 41}, {"referenceID": 27, "context": "linearities are neglected [33], [34].", "startOffset": 26, "endOffset": 30}, {"referenceID": 28, "context": "linearities are neglected [33], [34].", "startOffset": 32, "endOffset": 36}, {"referenceID": 27, "context": "Apart from its hierarchical flavor, the identification algorithm in [33], [34] has little in common with the proximal method proposed here.", "startOffset": 68, "endOffset": 72}, {"referenceID": 28, "context": "Apart from its hierarchical flavor, the identification algorithm in [33], [34] has little in common with the proximal method proposed here.", "startOffset": 74, "endOffset": 78}, {"referenceID": 25, "context": "The Laplacian ends up being diagonal in all the dimensions corresponding to the wavelets and dense in a small part corresponding to the scaling function (the algorithm ends up being very similar to the one proposed in [31]).", "startOffset": 218, "endOffset": 222}, {"referenceID": 29, "context": "Second, in [36] the authors propose to define data adaptive wavelets by factorizing some training data matrix made of signals on the graph of interest.", "startOffset": 11, "endOffset": 15}, {"referenceID": 30, "context": "The optimization algorithm they propose relies on deep learning techniques, more precisely layer-wise training of stacked autoencoders [37].", "startOffset": 135, "endOffset": 139}, {"referenceID": 11, "context": "To avoid the scaling ambiguities arising naturally when the constraint sets are (positively) homogeneous2, it is common [15], [32] to normalize the factors and introduce a multiplicative scalar \u03bb in the data fidelity term.", "startOffset": 120, "endOffset": 124}, {"referenceID": 26, "context": "To avoid the scaling ambiguities arising naturally when the constraint sets are (positively) homogeneous2, it is common [15], [32] to normalize the factors and introduce a multiplicative scalar \u03bb in the data fidelity term.", "startOffset": 126, "endOffset": 130}, {"referenceID": 11, "context": ", see for example [15].", "startOffset": 18, "endOffset": 22}, {"referenceID": 14, "context": "In [18], the authors consider cost functions depending on N blocks of variables of the form:", "startOffset": 3, "endOffset": 7}, {"referenceID": 14, "context": "To handle this objective function, the authors propose an algorithm called Proximal Alternating Linearized Minimization (PALM) [18], that updates alternatively each block of variable by a proximal (or projected in our case) gradient step.", "startOffset": 127, "endOffset": 131}, {"referenceID": 31, "context": "where it was found difficult to optimize the weights of neural networks comprising many hidden layers (called deep neural networks, see [38] for a survey on the topic).", "startOffset": 136, "endOffset": 140}, {"referenceID": 32, "context": "However in the last decade, it was proposed [39] to optimize the network not as one big block, but one layer at a time, and then globally optimizing the whole network using gradient descent.", "startOffset": 44, "endOffset": 48}, {"referenceID": 30, "context": "This heuristic was shown experimentally to work well on various tasks [37].", "startOffset": 70, "endOffset": 74}, {"referenceID": 0, "context": "A preliminary study on synthetic data was carried out in our technical report [1], showing that a flexible trade-off between relative complexity and adaptation to the input matrix can be achieved.", "startOffset": 78, "endOffset": 81}, {"referenceID": 33, "context": "As it is natural to assume that a limited set of brain foci are active during a cognitive task, sparse focal source configurations are commonly promoted using convex sparse priors [40], [41].", "startOffset": 180, "endOffset": 184}, {"referenceID": 34, "context": "As it is natural to assume that a limited set of brain foci are active during a cognitive task, sparse focal source configurations are commonly promoted using convex sparse priors [40], [41].", "startOffset": 186, "endOffset": 190}, {"referenceID": 35, "context": "To this end, we consider an MEG gain matrix M \u2208 R204\u00d78193 (m = 204 and n = 8193), computed using the MNE software [42] implementing a Boundary Element Method (BEM).", "startOffset": 114, "endOffset": 118}, {"referenceID": 13, "context": "4Compared to a preliminary version of this experiment [17] where the residual was normalized columnwise at the first step, here it is normalized globally.", "startOffset": 54, "endOffset": 58}, {"referenceID": 36, "context": "Three recovery methods were tested: Orthogonal Matching Pursuit (OMP) [10] (choosing 2 atoms), `1-regularized least squares (l1ls) [43] and Iterative Hard Thresholding (IHT) [11]).", "startOffset": 131, "endOffset": 135}, {"referenceID": 7, "context": "Three recovery methods were tested: Orthogonal Matching Pursuit (OMP) [10] (choosing 2 atoms), `1-regularized least squares (l1ls) [43] and Iterative Hard Thresholding (IHT) [11]).", "startOffset": 174, "endOffset": 178}, {"referenceID": 9, "context": "Classically, there are two paths to choose a dictionary for sparse signal representations [13].", "startOffset": 90, "endOffset": 94}, {"referenceID": 9, "context": "Dictionaries designed this way are called analytic dictionaries [13] (e.", "startOffset": 64, "endOffset": 68}, {"referenceID": 2, "context": "Due to the relative simplicity of analytic dictionaries, they usually have a known sparse form such as the Fast Fourier Transform (FFT) [3] or the Discrete Wavelet Transform (DWT) [6].", "startOffset": 136, "endOffset": 139}, {"referenceID": 5, "context": "Due to the relative simplicity of analytic dictionaries, they usually have a known sparse form such as the Fast Fourier Transform (FFT) [3] or the Discrete Wavelet Transform (DWT) [6].", "startOffset": 180, "endOffset": 183}, {"referenceID": 37, "context": "On the other hand, the development of modern computers allowed the surfacing of automatic methods that learn a dictionary directly from the data [44]\u2013[46].", "startOffset": 145, "endOffset": 149}, {"referenceID": 38, "context": "On the other hand, the development of modern computers allowed the surfacing of automatic methods that learn a dictionary directly from the data [44]\u2013[46].", "startOffset": 150, "endOffset": 154}, {"referenceID": 10, "context": "Can one design dictionaries as well adapted to the data as learned dictionaries, while as fast to manipulate and as cheap to store as analytic ones? This question has begun to be explored recently [14], [15], and actually amounts to learning", "startOffset": 197, "endOffset": 201}, {"referenceID": 11, "context": "Can one design dictionaries as well adapted to the data as learned dictionaries, while as fast to manipulate and as cheap to store as analytic ones? This question has begun to be explored recently [14], [15], and actually amounts to learning", "startOffset": 203, "endOffset": 207}, {"referenceID": 9, "context": "This can be done by inserting a dictionary factorization step into the traditional structure of dictionary learning algorithms [13], as illustrated on Figure 10.", "startOffset": 127, "endOffset": 131}, {"referenceID": 38, "context": "DDL, but other algorithms have been tested (such as online dictionary learning [46]), leading to similar qualitative results.", "startOffset": 79, "endOffset": 83}, {"referenceID": 16, "context": "In order to assess the generalization performance and to be as close as possible to the matrix factorization framework studied theoretically in [20], DDL is performed following the", "startOffset": 144, "endOffset": 148}, {"referenceID": 39, "context": "The implementation described in [47] was used, running 50 iterations (empirically sufficient to ensure convergence).", "startOffset": 32, "endOffset": 36}, {"referenceID": 40, "context": "indicates how many training samples L should be taken in order for the empirical risk to be (with high probability) uniformly close to its expectation [50], [51].", "startOffset": 151, "endOffset": 155}, {"referenceID": 41, "context": "indicates how many training samples L should be taken in order for the empirical risk to be (with high probability) uniformly close to its expectation [50], [51].", "startOffset": 157, "endOffset": 161}, {"referenceID": 16, "context": "In [20], a general bound on the deviation between the empirical risk and its expectation is provided, which is proportional to the covering dimension of the dictionary class.", "startOffset": 3, "endOffset": 7}, {"referenceID": 16, "context": "For dense dictionaries the covering dimension is known to be O(mn) [20], [50], [51].", "startOffset": 67, "endOffset": 71}, {"referenceID": 40, "context": "For dense dictionaries the covering dimension is known to be O(mn) [20], [50], [51].", "startOffset": 73, "endOffset": 77}, {"referenceID": 41, "context": "For dense dictionaries the covering dimension is known to be O(mn) [20], [50], [51].", "startOffset": 79, "endOffset": 83}, {"referenceID": 16, "context": "The precise definition of covering numbers is given in [20].", "startOffset": 55, "endOffset": 59}, {"referenceID": 16, "context": "This leads following [20] (with the Frobenius norm) to:", "startOffset": 21, "endOffset": 25}, {"referenceID": 13, "context": "The authors also express their gratitude to Alexandre Gramfort for providing the MEG data and contributing to [17].", "startOffset": 110, "endOffset": 114}], "year": 2016, "abstractText": "The computational cost of many signal processing and machine learning techniques is often dominated by the cost of applying certain linear operators to high-dimensional vectors. This paper introduces an algorithm aimed at reducing the complexity of applying linear operators in high dimension by approximately factorizing the corresponding matrix into few sparse factors. The approach relies on recent advances in non-convex optimization. It is first explained and analyzed in details and then demonstrated experimentally on various problems including dictionary learning for image denoising, and the approximation of large matrices arising in inverse problems.", "creator": "LaTeX with hyperref package"}}}