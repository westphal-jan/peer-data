{"id": "1506.07359", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "24-Jun-2015", "title": "Sequential Extensions of Causal and Evidential Decision Theory", "abstract": "Moving beyond the dualistic view in AI where agent and environment are separated incurs new challenges for decision making, as calculation of expected utility is no longer straightforward. The non-dualistic decision theory literature is split between causal decision theory and evidential decision theory. We extend these decision algorithms to the sequential setting where the agent alternates between taking actions and observing their consequences. We find that evidential decision theory has two natural extensions while causal decision theory only has one.", "histories": [["v1", "Wed, 24 Jun 2015 13:16:16 GMT  (26kb)", "http://arxiv.org/abs/1506.07359v1", "ADT 2015"]], "COMMENTS": "ADT 2015", "reviews": [], "SUBJECTS": "cs.AI", "authors": ["tom everitt", "jan leike", "marcus hutter"], "accepted": false, "id": "1506.07359"}, "pdf": {"name": "1506.07359.pdf", "metadata": {"source": "CRF", "title": "Sequential Extensions of Causal and Evidential Decision Theory", "authors": ["Tom Everitt Jan Leike", "Marcus Hutter"], "emails": [], "sections": [{"heading": null, "text": "ar Xiv: 150 6.07 359v 1 [cs.A I] 2 4Ju n Turning away from the dualistic view in AI, where agent and environment are separated, presents new challenges to decision-making, as the calculation of expected benefits is no longer simple. Non-dualistic decision theory is divided into causal decision theory and evident decision theory. We extend these decision algorithms to the sequential setting in which the agent intervenes between actions and observes their consequences. We find that evident decision theory has two natural extensions, while causal decision theory has only one. Keywords: evidence decision theory, causal decision theory, causal graphic models, planning, dualism, physics."}, {"heading": "1 Introduction", "text": "In fact, the fact is that most of them are able to survive themselves without there being a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process, in which there is a process, in which there is a process, in which there is a process, in which there is a process, in which there is a process, in which there is a process, in which there is a process, in which there is a process, in which there is a process, in which there is a process, in which there is a process, in which there is a process, in which there is a process, in which there is a process, in which there is a process, in which there is a process, in which there is a process, in which there is a process, in which there is a process, in which there is a process, in which there is a process, in which there is a process, in which there is a process, in which there is a process, in which there is a process, in which there is a process, in which there is a process, a process, a process, a process, a process, a process, a process, a process, a process, a process, a process, a process and a process, a process and a process, and a process, and a process and a process, and a process, and a process, and a process, and a process, and it, and it, and it, comes, and it, and it, in which it, and it, and it, and it, and it, respectively."}, {"heading": "2 One-Shot Decision Making", "text": "In the case of a unique decision problem, we assume an action, receive a perception e-E (typically called result in the literature of decision theory) and receive a payout u (e) according to the usage function u: E \u2192 [0, 1]. We assume that the action set A and the set of perceptions E are finite. In addition, the environment contains a hidden state s-S. The hidden state contains information that is not available to the agent at the time of the decision, but can influence the decision and perception. Formally, the environment is given by a probability distribution P about the hidden state, the action and the perception that factors according to a causal diagram [Pea09]. A causal diagram about the random variables x1,... xn is a directed acyclic diagram with the nodes x1,.., xn. Each node xi includes a probability distribution P (xi | pai), where Pai is the set of parents in the diagram xi."}, {"heading": "2.1 Savage Decision Theory", "text": "In the dualistic formulation of decision theory, we have a function P that performs an action a and returns a probability distribution Pa over perceptions. (SDT) In the dualistic model, it is usually conceptually clear what Pa should be. In the physical model, the environmental model takes the form of a causal diagram over a hidden state s, action a and perception e, as in Figure 2. According to this causal graph, the probability distribution P is causal in P (s, a, e) = P (a | s) P (e | s, a). The hidden state is not independent of the action of the decision maker and the savage model is not directly applicable, as we do not have a specification of Pa. How should decisions be made in this context? The literature focuses on two answers to this question: CDT and the savage model."}, {"heading": "2.2 Causal and Evidential Decision Theory", "text": "The question is whether one problem or another is a real problem or a pure problem or a pure problem. (...) The question is whether the other is a real problem or a pure problem. (...) The question is whether one or the other is a pure problem. (...) The question is whether the other is a pure problem. (...) The question is whether the other is a real problem or a pure problem. (...) The question is whether the other is a real problem. (...) The question is whether it is a pure problem. (...) The question is whether it is a real problem. (...) The question is whether it is a real problem or a pure problem. (...) The question is whether it is a real problem. (...) The question is whether it is a real problem. (...) The question is whether it is a real problem. (... The question is a question.) The question is whether it is a real problem. (... The question is a question.) The question is whether it is a real problem."}, {"heading": "3 Sequential Decision Making", "text": "In this section, we extend CDT and EDT to the sequential case. We begin by formally specifying the physical model shown in Figure 1 of the first subsection, and discuss problems with time consistency in Section 3.2, before defining the extensions in Section 3.3 and 3.4, the last subsection dissecting the role of the hidden state."}, {"heading": "3.1 The Physicalistic Model", "text": "For the remainder of this paper, we assume that the agent interacts sequentially with an environment. In due course, the agent selects an action in A and receives a percept et E, which has a benefit from u < < R; the cycle then repeats for t + 1. A story is an element in A \u00b7 E). We use \u00e6 A \u00b7 E to denote an interaction cycle, and \u00e6 < t to denote a story of length t \u2212 1. Perceptions between time t and time m are denoted et: m. A policy is a function that maps a story < t to the next action. We consider only deterministic policies. We assume that the agent is given an environmental model, but knows neither the hidden state nor his own future actions. The unknown hidden state can influence both perceptions and actions."}, {"heading": "3.2 Time Consistency", "text": "When planning for the infinite future, we need to ensure that utilities do not add up to infinity; typically, this is achieved by discounting. Here, we simplify by specifying a finite m \u00b2 N to be the lifespan of the intermediary: the intermediary takes care of the sum of the utilities of all perceptions e1. We plan what we would do for all possible future perceptions et: m by selecting a policy: (A \u00b7 E) \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 in sequential decision theory, we need to plan the next m \u2212 t measures in step by step, and then plan the perception in step by step."}, {"heading": "3.3 Sequential Evidential Decision Theory", "text": "There are two ways to generalize this, depending on whether we only use the next action or the entire future policy as evidence for the next perception. < tat isV aev, \u03c0\u00b5, m (\u00e6 < tat isV aev, \u03c0\u00b5, m (\u00e6 < tat): = \u2211 et\u00b5 (et | \u00e6 < tat) (u (et)) + V aev, \u03c0 (p), m (p). < tat isV aev, m < tat): = 0 for t > m. Sequential Action-Evidential Decision Theory (SAEDT) writes an optimal and time-consistent policy for V aevel.m.it It can be argued that we act consciously (SAEDT)."}, {"heading": "3.4 Sequential Causal Decision Theory", "text": "In the sequential causal decision theory, we ask what would happen if we causally intervene in the nodes of the next action and fix them to \u03c0 (\u00e6 < t) according to politics \u03c0, expressed by the notation do (at: = \u03c0 (\u00e6 < t)), or do (\u043c < t) for short.Definition 6 (sequential causal decision theory).The causal value of a political decision with lifelong m in the given history \u00e6 < tat isV cau, \u03c0\u00b5, m (\u00e6 < tat): = causal causal decision theory (et | causal decision theory).The causal value of a political decision with lifelong m in the given history \u00e6 < tat isV cau, m (\u00e6 < tat)."}, {"heading": "3.5 Expansion over the Hidden State", "text": "The difference between the sequential versions of EDT and CDT is how they update their prediction of a next perception (definitions 3, 4 and 6).The following proposal expands the different beliefs regarding the hidden state.Proposition 10. < For all stories < tatet (A \u00b7 E) * (definitions 3, 4 and 6) < < < < < < tat) < For all stories < tatet (A \u00b7 E) * (E) * (6) < tat) (6) < SPEDT and SCDT respectively: < m) < < s < < ts < t, t: m) \u00b5 (et | S < s < s < tat; tat) \u00b5; tat; tat; tat) \u00b5; t: m; t: m (et | s < t: m (7)."}, {"heading": "4 Discussion", "text": "Our paper is a first stab at the problem of how physical agents should make sequential decisions. CDT and EDT provide an existing basis for non-dualistic decision-making, which we have extended to sequential adjustment. There are two natural ways to make sequential evidential decisions: do I update my beliefs about the hidden state based on my next action (\"what I will do next,\" SAEDT) or my entire policy (\"the kind of agent I am,\" SPEDT)? With Proposition 7, this distinction does not exist for causal decision theory, because with this theory the agent does not consider his own actions at all. Therefore, we have only one version of the sequential causal decision theory, the SCDT. To illustrate the differences between decision theories, we discussed three variants of the Newcomb problem (Example 1, Example 8, and Example 9) and two variants of the toxoplasmosis problem (Example 2 and 5)."}, {"heading": "A Examples", "text": "This section contains the formal calculations for Example 1, Example 2, Example 5, Example 8, and Example 9. These calculations are also available as Python code at http: / / jan.leike.name /.Example 11 (Newcomb's problem).This is a formalization of Example 1. \u2022 S: = {E, F}, where E the opaque box is empty and F \u2212 \u2212 the opaque box is full \u2022 A: = {B1, B2}, where B1 means one-boxes and B2 means two-boxes \u2022 E: = {O0, OT, OMT} \u2022 u (O0), u (OT): = 1.000, u (OM), u (OMT): = 1.000.000, u (OMT): 1.001.000Let us be a small constant indicating the accuracy of the predictor."}], "references": [{"title": "Decision and Causality", "author": ["Arif Ahmed. Evidence"], "venue": "Cambridge University Press,", "citeRegEx": "Ahm14", "shortCiteRegEx": null, "year": 2014}, {"title": "Technical report", "author": ["Alex Altair. A comparison of decision algorithms on Newcomblike problems"], "venue": "Machine Intelligence Research Institute,", "citeRegEx": "Alt13", "shortCiteRegEx": null, "year": 2013}, {"title": "Superintelligence: Paths", "author": ["Nick Bostrom"], "venue": "Dangers, Strategies. Oxford University Press,", "citeRegEx": "Bos14", "shortCiteRegEx": null, "year": 2014}, {"title": "Normative theories of rational choice: Expected utility", "author": ["Rachael Briggs"], "venue": "Edward N. Zalta, editor, The Stanford Encyclopedia of Philosophy. Fall 2014 edition,", "citeRegEx": "Bri14", "shortCiteRegEx": null, "year": 2014}, {"title": "The Philosophical Review", "author": ["Andy Egan. Some counterexamples to causal decision theory"], "venue": "pages 93\u2013114,", "citeRegEx": "Ega07", "shortCiteRegEx": null, "year": 2007}, {"title": "pages 125\u2013162", "author": ["Allan Gibbard andWilliam L Harper. Counterfactuals", "two kinds of expected utility. In Foundations", "Applications of Decision Theory"], "venue": "Springer,", "citeRegEx": "GH78", "shortCiteRegEx": null, "year": 1978}, {"title": "University of Chicago Press", "author": ["Richard C Jeffrey. The Logic of Decision"], "venue": "2nd edition,", "citeRegEx": "Jef83", "shortCiteRegEx": null, "year": 1983}, {"title": "The Foundations of Causal Decision Theory", "author": ["James M Joyce"], "venue": "Cambridge University Press,", "citeRegEx": "Joy99", "shortCiteRegEx": null, "year": 1999}, {"title": "Australasian Journal of Philosophy", "author": ["David Lewis. Causal decision theory"], "venue": "59(1):5\u201330,", "citeRegEx": "Lew81", "shortCiteRegEx": null, "year": 1981}, {"title": "Program equilibrium in the prisoner\u2019s dilemma via L\u00f6b\u2019s theorem", "author": ["Patrick LaVictoire", "Benja Fallenstein", "Eliezer Yudkowsky", "Mihaly Barasz", "Paul Christiano", "Marcello Herreshoff"], "venue": "AAAI Workshop on Multiagent Interaction without Prior Coordination,", "citeRegEx": "LFY14", "shortCiteRegEx": null, "year": 2014}, {"title": "Theoretical Computer Science", "author": ["Tor Lattimore", "Marcus Hutter. General time consistent discounting"], "venue": "519:140\u2013154,", "citeRegEx": "LH14", "shortCiteRegEx": null, "year": 2014}, {"title": "Newcomb\u2019s problem and two principles of choice", "author": ["Robert Nozick"], "venue": "Essays in honor of Carl G. Hempel, pages 114\u2013146. Springer,", "citeRegEx": "Noz69", "shortCiteRegEx": null, "year": 1969}, {"title": "pages 209\u2013218", "author": ["Laurent Orseau", "Mark Ring. Space-time embedded intelligence. In Artificial General Intelligence"], "venue": "Springer,", "citeRegEx": "OR12", "shortCiteRegEx": null, "year": 2012}, {"title": "Cambridge University Press", "author": ["Judea Pearl. Causality"], "venue": "2nd edition,", "citeRegEx": "Pea09", "shortCiteRegEx": null, "year": 2009}, {"title": "Technical report", "author": ["Stuart Russell", "Daniel Dewey", "Max Tegmark", "Janos Kramar", "Richard Mallah. Research priorities for robust", "beneficial artificial intelligence"], "venue": "Future of Life Institute,", "citeRegEx": "RDT15", "shortCiteRegEx": null, "year": 2015}, {"title": "Prentice Hall", "author": ["Stuart J Russell", "Peter Norvig. Artificial Intelligence. A Modern Approach"], "venue": "3rd edition,", "citeRegEx": "RN10", "shortCiteRegEx": null, "year": 2010}, {"title": "The Foundations of Statistics", "author": ["Leonard J Savage"], "venue": "Dover Publications,", "citeRegEx": "Sav72", "shortCiteRegEx": null, "year": 1972}, {"title": "Aligning superintelligence with human interests: A technical research agenda", "author": ["Nate Soares", "Benja Fallenstein"], "venue": "Technical report, Machine Intelligence Research Institute,", "citeRegEx": "SF14a", "shortCiteRegEx": null, "year": 2014}, {"title": "Technical report", "author": ["Nate Soares", "Benja Fallenstein. Toward idealized decision theory"], "venue": "Machine Intelligence Research Institute,", "citeRegEx": "SF14b", "shortCiteRegEx": null, "year": 2014}, {"title": "In Artificial General Intelligence", "author": ["Nate Soares", "Benja Fallenstein. Counterpossibles as necessary for decision theory"], "venue": "Springer,", "citeRegEx": "SF15", "shortCiteRegEx": null, "year": 2015}], "referenceMentions": [{"referenceID": 15, "context": "1 Introduction In artificial-intelligence problems an agent interacts sequentially with an environment by taking actions and receiving percepts [RN10].", "startOffset": 144, "endOffset": 150}, {"referenceID": 12, "context": "But often it is not true: real-world agents are embedded in (and computed by) the environment [OR12], and then a physicalistic model is more appropriate.", "startOffset": 94, "endOffset": 100}, {"referenceID": 9, "context": "This link can be used for uncoordinated cooperation [LFY14].", "startOffset": 52, "endOffset": 59}, {"referenceID": 16, "context": "This is known as Savage decision theory [Sav72].", "startOffset": 40, "endOffset": 47}, {"referenceID": 13, "context": "Formally, the environment is given by a probability distribution P over the hidden state, the action, and the percept that factors according to a causal graph [Pea09].", "startOffset": 159, "endOffset": 166}, {"referenceID": 13, "context": "Causal decision theory has several formulations [GH78, Lew81, Sky82, Joy99]; we use the one given in [Sky82], with Pearl\u2019s calculus of causality [Pea09].", "startOffset": 145, "endOffset": 152}, {"referenceID": 11, "context": "Example 1 (Newcomb\u2019s Problem [Noz69]).", "startOffset": 29, "endOffset": 36}, {"referenceID": 1, "context": "Example 2 (Toxoplasmosis Problem [Alt13]).", "startOffset": 33, "endOffset": 40}, {"referenceID": 4, "context": "Is that the reason there are so few wealthy philosophers? Historically, this problem has been known as the smoking lesion problem [Ega07].", "startOffset": 130, "endOffset": 137}, {"referenceID": 10, "context": "The choice of discounting can lead to time inconsistency: a sliding fixed-size horizon is time inconsistent, but a fixed finite lifetime is time consistent [LH14].", "startOffset": 156, "endOffset": 162}, {"referenceID": 18, "context": "A general principle for how to do this is still an open question [SF14b].", "startOffset": 65, "endOffset": 72}], "year": 2015, "abstractText": "Moving beyond the dualistic view in AI where agent and environment are separated incurs new challenges for decision making, as calculation of expected utility is no longer straightforward. The non-dualistic decision theory literature is split between causal decision theory and evidential decision theory. We extend these decision algorithms to the sequential setting where the agent alternates between taking actions and observing their consequences. We find that evidential decision theory has two natural extensions while causal decision theory only has one.", "creator": "LaTeX with hyperref package"}}}