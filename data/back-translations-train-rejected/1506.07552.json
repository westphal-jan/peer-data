{"id": "1506.07552", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "24-Jun-2015", "title": "Splash: User-friendly Programming Interface for Parallelizing Stochastic Algorithms", "abstract": "Stochastic algorithms are efficient approaches to solving machine learning and optimization problems. In this paper, we propose a general framework called Splash for parallelizing stochastic algorithms on multi-node distributed systems. Splash consists of a programming interface and an execution engine. Using the programming interface, the user develops sequential stochastic algorithms without concerning any detail about distributed computing. The algorithm is then automatically parallelized by a communication-efficient execution engine. We provide theoretical justifications on the optimal rate of convergence for parallelizing stochastic gradient descent. The real-data experiments with stochastic gradient descent, collapsed Gibbs sampling, stochastic variational inference and stochastic collaborative filtering verify that Splash yields order-of-magnitude speedup over single-thread stochastic algorithms and over parallelized batch algorithms. Besides its efficiency, Splash provides a rich collection of interfaces for algorithm implementation. It is built on Apache Spark and is closely integrated with the Spark ecosystem.", "histories": [["v1", "Wed, 24 Jun 2015 20:39:54 GMT  (641kb)", "http://arxiv.org/abs/1506.07552v1", "32 pages"], ["v2", "Wed, 23 Sep 2015 01:11:22 GMT  (286kb)", "http://arxiv.org/abs/1506.07552v2", "redo experiments to learn bigger models; compare Splash with state-of-the-art implementations on Spark"]], "COMMENTS": "32 pages", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["yuchen zhang", "michael i jordan"], "accepted": false, "id": "1506.07552"}, "pdf": {"name": "1506.07552.pdf", "metadata": {"source": "CRF", "title": "Splash: User-friendly Programming Interface for Parallelizing Stochastic Algorithms", "authors": ["Yuchen Zhang", "Michael I. Jordan"], "emails": ["yuczhang@eecs.berkeley.edu", "jordan@eecs.berkeley.edu"], "sections": [{"heading": null, "text": "ar Xiv: 150 6.07 552v 1 [cs.L G] 24 Ju"}, {"heading": "1 Introduction", "text": "This year is the highest in the history of the country."}, {"heading": "2 Background", "text": "In this paper, we will focus on the stochastic algorithms, which take the following general form. At step t, the algorithm receives a data element zt and a vector of common variables vt. Based on these values, the algorithm performs an incremental update of all data on the common variable: vt + 1 \u2190 vt + \u2206 (zt, vt) (1) For example, stochastic gradient descent (SGD) fits within this general framework. If you let x denote a random data element x and let w denote a parameter vector, SGD performs the update: t \u2190 t + 1 and w \u2212 ig \u2190 w \u2212 ig (w; x), which is the loss function associated with the element and the increment t. In this case, both w and t are common variables. In Appendix A, we will discuss additional stochastic algorithms and their implementation via splash."}, {"heading": "3 Programming with Splash", "text": "The goal of the programming interface is to make distributed computing transparent to the user. Splash extends Apache Spark with an abstraction called by Apache Spark. It can be created from a standard RDD object: Val parametrized RDD (rdd). We offer a rich collection of interfaces to convert the components of RDD to facilitate interaction between Splash and Spark."}, {"heading": "4 Strategy for Parallelization", "text": "In this section we first discuss two naive strategies for parallelizing a stochastic algorithm and their respective constraints. These constraints motivate the strategy that Splash applies."}, {"heading": "4.1 Two naive strategies", "text": "Suppose that there are m-threads and each thread processes a subset of Si of S. If the i-th thread increases the common variable by \u2206 (Si), then the accumulation scheme constructs a global update by accumulating local updates: vnew = vold + m \u2211 i = 1 \u0445 (Si). (3) The scheme (3) provides a good approximation of complete update when the stack size is sufficiently small [1]. However, frequent communication is necessary to ensure a small stack size. Distributed systems connected by raw material networks are prohibitively expensive to frequent communication, even if the communication is asynchronous. Applying scheme (3) to a large stack can easily lead to divergence. Take SGD as an example: if all threads proceed from the same vector wold, then after processing a large stack the new vector is."}, {"heading": "4.2 Our strategy", "text": "We will now turn to the description of the splash strategy for combining parallel updates. First, we will introduce the operators that Splash supports for manipulating split variables. Then, we will illustrate how conflicting updates are combined by reweighting. The programming interface allows the user to manipulate split variables through operators within their algorithm implementation. An operator is a function that maps a real number to another real number. Splash supports three types of operators: add, delay, and multiply. The system employs different strategies for parallelizing different types of operators. Adding an operator is the most commonly used operator. If the operation is performed on variable v, the variable of v + B is updated, where a user-specified scalar is applied. SGD update (2) can be implemented using this operator. However, the same operation is not performed by the same mapping; the same mapping is performed by C."}, {"heading": "5 Convergence Analysis", "text": "The goal of the SGD is to minimize an empirical risk."}, {"heading": "6 Experiments", "text": "In this section, we report on the empirical performance of Splash. We solve three machine learning problems: logistic regression, theme modeling and film recommendation. Our implementation of Splash runs on an Amazon EC2 cluster with eight nodes. Each node is powered by an eight-core Intel Xeon E5-2665 with 30GB of memory and is connected to a commodity 1GB network so that the cluster contains 64 cores. For all applications, the system chooses 64 threads as the best level of parallelism.Datasets For logistic regression, we use the Covtype, RCV1 and MNIST accuracy of datasets from the LIBSVM website. Among the three datasets, Covtype and MNIST, we have dense features, RCV1 has sparse features. For topic modeling, we use the NIPS, Enron and NYtimes datasets from the UCI Machine Learning Repository."}, {"heading": "Appendix", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "A More Examples of Stochastic Algorithm", "text": "In this section we describe three additional stochastic algorithms: the collapsed Gibbs sampling algorithm, the stochastic variation inference algorithm, and the stochastic collaborative filter algorithm."}, {"heading": "A.1 Collapsed Gibbs sampling", "text": "Latent Dirichelet Allocation [2] (LDA) is an unattended model for learning topics from documents. LDA's goal is to derive the topic from each word in each document. LDA's Collapsed Gibbs Sampling Algorithm iteratively takes a word w from document d and tries the topic w byP (topic = k | d, w). (nk | d + \u03b1) (nw | k + \u03b2) nk + \u03b2W. (12) Here W is the size of the vocabulary; nk | d is the number of words in document d assigned to the topic k; nw | k is the total number of times the word w is assigned to the topic k and nk: = w nw | k. All counts exclude the current word w. The constants \u03b1 and \u03b2 are hyperparameters of the LDA model. The counts nk | d | k and nk are common variables."}, {"heading": "A.2 Stochastic Variational Inference", "text": "The Statistical Conclusion of Variation (SVI) is another efficient approach to learning the LDA model [10]. SVI models a word w in document d as follows: The probability that the word belongs to the subject k is proportional to a parameter \u03b3dk; the probability that the subject k creates word d is proportional to another parameter \u03bbkw. When iterating t, a document d is randomly drawn from the corpus, and the parameters \u03b3dk and \u03bbkw are recalculated using varying inference (see e.g. [10, Figure 6] for details of the algorithm. Leave the recalculated parameters designated by \u03b3-dk and \u0432-kw, SVI will perform the update of the Update\u03b3dk product."}, {"heading": "A.3 Stochastic Collaborative Filtering", "text": "s choice in the future. Collaborative filtering assumes that there is a latent vector associated with each user, and a latent vector associated with each item. Affinity between a user and an item, or the likelihood that the user will choose the item in the future, is measured by the inner product of its latent vectors. There is a successful stochastic algorithm for learning the latent vectors, which is given implicit user feedback. The algorithm is called Bayesian Personalized Ranking (BPR) [22]. Given a user-item-vector pair (u, i) in which the user has selected the Itevi, the algorithm can randomly select an algorithm, select another vector, and not the Itej \u2212"}, {"heading": "B Constructing Linear Transformation on a Thread", "text": "If elementary operators are applied sequentially, they merge into a single linear transformation. Suppose that after processing a local subset S, the resulting transformation can be represented by v (S) \u00b7 v + \u2206 (S) + T (S), where the factor \"S\" is the scale factor, \"S\" is the term resulting from the operators added elementally, and \"T (S) is the term resulting from the operators added elementally delayed that were declared before the last synchronization. Suppose that P is the set of processed elements. At the beginning, the set of processed elements is empty, so that we have them by\" P \"= 1,\" P \"= 0, and\" P \"= 0 for P = 1. After processing the element z, we assume that the user has performed all kinds of operations, resulting in a transformation that takes the form\" P \"(+ v) (vice versa)."}, {"heading": "C Generalizing Stochastic Algorithms", "text": "In this section, we will demonstrate how Bayesian's collapsed Gibbs sampling method, stochastic variation reasoning method, and personalized ranking algorithm are generalized for processing weighted samples. Suppose the sample weight is. the collapsed Gibbs sampling updates (13) and (14) are generalized by the sampling updates (13) and (24) by the collapsed Gibbs sampling updates (13) and (14). Note that the update (24) should be implemented as a delayed operator - it was declared in the last iteration but executed in the current iteration. For stochastic variation conclusions, we will generalize the updates (15) bybyvi (14) and bybyl)."}, {"heading": "D Example: Reweighting in Parallelized SGD", "text": "We present a simple example that illustrates how the strategy described in section 4.2 creates parallelism. Let us consider the following convex optimization problem: There are N = 3, 000 two-dimensional vectors, which are represented by x1..., xN, so that xi is drawn randomly and independently of the normal distribution x x x x x (0, I2 x 2). The goal is to find a two-dimensional vector w, which minimizes the weighted distance to all samples. Precision, the loss function at the sample x 5 x 5 x 5 x 5 x 5 x 2 x 5 x 5 x 5 x 5 x 5 x 5 x 5 x 5 x 5 x 5 x 3 x 3 x 3 x 3 x 3 x 3 x 4 x 4 x 4 x 4 x 4 x 4 x 4 x 4 x 5 x 5 x 5 x 5 x 5 x 5 x 5 x 5 x 5 x x 5 x x x 5 x x 5 x x x x 5 x 5 x 5 x 5 x 5 x 2 x 5 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 3 x 3 x 3 x 3 x 3 x 3 x 4 x 4 x 4 x 4 x 4 x 4 x 4 x 4 x 4 x 4 x 4 x 4 x 4 x 4 x 4 x 4 x 4 x 4 x 4 x 4 x 4 x 4 x 4 x 4 x 4 x 4 x 4 x 5 x 5 x 5 x 5 x 5 x 5 x 5 x 5 x 5 x 5 x 5 x 5 x 5 x 5 x 5 x x x x x w; x 5 x 5 x 5 x 5 x 5 x 5 x 5 x 5 x 5 x 5 x x x x 5 x 5 x 5 x 2 x 5 x 2 x 5 x 5 x 5 x 5 x 5 x 5 x 3 x 3 x 3 x 3 x 3 x 3 x 3 x 3 x 3 x 3 x 3 x 3 x 3 x 3 x 4 x 4 x 4 x 4 x 4 x 4 x 4 x 4 x 4 x 4 x 4 x 4 x 4 x 4 x 4 x 4 x 4 x 4 x 4 x 4 x 4 x 4 x 4 x 4 x 4 x 4 x 4 x 4 x 4 x 4 x 4 x 4 x 4 x 4 x 4 x 4 x"}, {"heading": "E Determining Thread Number", "text": "Suppose that there are M cores available in the cluster. The executor splits these cores into several groups. Suppose that the i-th group contains mi cores. Group sizes are determined by the following assignment scheme: \u2022 Let 4m0 be the thread number assumed by the last iteration. Let 4m0: = 1 at the first iteration. \u2022 For i = 1, 2,.. if 8mi \u2212 1 \u2264 M \u2212 1 j = 1mj, the let mi: = 4mi \u2212 1. Otherwise, let mi: = 1j = 1mj = 1mj. Finish if I j = 1mj = M. It can be easily verified that the candidate thread numbers (which are the group sizes) in the current iteration are at least as large as those of the last iteration. The candidate thread numbers are 4m0, 16m0 until they consume all available cores."}, {"heading": "F Proof of Theorem 1", "text": "We assume that the following imbalance will occur when the following imbalance occurs: T-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W-W"}, {"heading": "G Proof of Theorem 2", "text": "Remember that wk is the value of the vector w after the iteration k = \u03b2k = \u03b2k = \u03b2k = \u03b2k = \u03b2k = \u03b2k = \u03b2k = \u03b2k = \u03b2k = \u03b2k = \u03b2k = \u03b2k = \u03b2k = \u03b2k = \u03b2k. The set B contains the optimal solution w +. Since the projection on a convex set does not increase the distance to the elements in the set, and because wki (i = 1,., m) are mutually independent, we are both independent of the quality on wk \u2212 1, we haveE [2, wk \u2212 w)."}, {"heading": "G.1 Proof of Lemma 1", "text": "In this proof, we use wj as an abbreviation to express the value of the vector w at the iteration k + 1 when the first thread processes the j-th element. We drop the dependence of the notation on the iteration and the thread index since they are explicitly stated in the context. It is easy to verify that gj = 2 + j (wj; xj) is the gradient of the loss function. We start by checking the expectation of the WK + 11 \u2212 w for the j-th element. By the strong convexity of L and the fact that w is minimized, we have < E [gj], wj \u2212 w > L (wj) that we check the expectation of the WK + 11 \u2212 w for the wk-th element."}, {"heading": "G.2 Proof of Lemma 2", "text": "In this proof, we use wj as an abbreviation to define the value of the vector w at iteration k + 1 when the first thread processes the j-th element. We drop the dependence of the notation on the iteration number and the thread index, as they are explicitly stated in the context. It is easy to verify that this thread (wj + j) will be the gradients of the loss function. Let's capture the neighborhood U2 in Assumption A, and note that the combining to the j-th element (wj \u2212 w) will be the step size parameter when wj is updated. It is easy to verify that the combining (kn + j + j).Let's capture the neighborhood U2 in Assumption A, and the combining thatwj + 1 \u2212 w \u00b2 W (wj \u2212 w) will be the step size parameters when wj is updated."}, {"heading": "H More Experiments", "text": "In this section we will report more about the experiment."}, {"heading": "H.1 Stochastic Gradient Descent", "text": "The target is the minimization of the following objective functionsL (w) = 1nn \"J\" = 1log (1 + exp (\u2212 yi < wi, xi >) + 0 \"0\" 0 \"0\" 0 \"0\" 0 \"0\" 0 \"0\" 0 \"0\" 0 \"0\" 0 \"0\" 0 \"0\" 0 \"0\" 0 \"0\" 0 \"0\" 0 \"0\" 0 \"0\" 0 \"0\" 0 \"0\" 0 \"0\" 0 \"0\" 0 \"0\" 0 \"0\" 0 \"0\" 0 \"0\" 0 \"0\" 0 \"0\" 0 \"0\" 0 \"0\" 0 \"0\" 0 \"0\" 0 \"0\" 0 \"0\" 0 \"0\" 0 \"0\" 0 \"0\" 0 \"0\" 0 \"0\" 0 \"0\" 0 \"0\" 0 \"0\" 0 \"0\" 0 \"0\" 0 \"0\" 0 \"0\" 0 \"0\" 0 \"0\" 0 \"0\" 0 \"0\" 0 \"0\" 0 \"0\" 0 \"0\" 0 \"0\" 0 \"0\" 0 \"0\" 0 \"0\" 0 \"0\" 0 \"0\" 0 \"0\" 0 \"0\" 0 \"0\" 0 \"0\" 0 \"0\" 0 \"0\" 0 \"0\" 0 \"0\" 0 \"0\" 0 \"0\" 0 \"0\" 0 \"0\" 0 \"0\" 0 \"0\" 0 \"0\" 0 \"0\" 0 \"0\" 0 \"0\" 0 \"0\" 0 \"0\" 0 \"0\" 0 \"0\" 0 \"0\" 0 \"0\" 0 \"0\" 0 \"0\" 0 \"0\" 0 \"0\" 0 \"0\" 0 \"0\" 0 \"0\" 0 \"0\" 0 \"0\" 0 \"0\" 0 \"0\" 0 \"0\" 0 \"0\" 0 \"0\" 0 \"0\" 0 \"0\" 0 \"0\" 0 \"0\" 0 \"0\" 0 \"0\" 0 \"0\" 0 \"0\" 0 \"0\" 0 \"0\" 0 \"0\" 0 \"0\" 0 \"0\" 0 \"0\" 0 \"0\" 0 \"0\" 0 \"0\" 0 \"0\" 0 \"0\" 0 \"0\" 0 \"0\" 0 \"0\" 0 \"0\" 0"}, {"heading": "H.2 Collapsed Gibbs Sampling", "text": "We will turn to the Latent Dirichelet Allocation (LDA) model and use collapsed Gibbs sampling (24) - (25). We will conduct experiments with three sets of data: the smaller NIPS paper and Enron e-mail record and the larger New York Times article record. Detailed descriptions of the data sets will be in Table 3.The number of topics will be K = 20. Hyper parameters will be randomly selected \u03b1 = 50 / K and \u03b2 = 0.01. We will randomly divide the words in each document so that half of them will be used for training and the rest for testing. Before the program begins, the word topics will be randomly initialized. For both sets of data, Splash will pass through the data set between two rounds of communication.Learning accuracy will be measured by the perplexity on the test record. Let p (wd) be the probability of testing words in document d under the LDA model."}, {"heading": "H.3 Stochastic Variational Inference", "text": "In this experiment, we compare the parallel variable inference (VI) algorithm, the single thread stochastic variational inference (SVI) algorithm, and the parallel SVI processing implemented by Splash. Note that the VI algorithm is easy to parallelise. To parallelise the VI algorithm, the dataset is divided into m subsets according to the document indexes. Each thread updates the document theme parameters \u03b3dk for documents in its own subset, then all threads perform a reduced operation to calculate the new theme parameters. Since VI is a batch algorithm, there is no conflict in parallelizations. We test the three algorithms on the three datasets described in Table 3. We select the topic K = 20 and select the same hyperparameters, \u03b2 as in collapsed Gibbs sampling. The learning rate is chosen (we)."}, {"heading": "H.4 Stochastic Collaborative Filtering", "text": "We use stochastic collaborative filtering to solve the Netflix movie recommendation problem. Netflix Dataset contains 100 million movie ratings from 480,000 users on 17,000 movies. We split the dataset evenly into a training set and a test set. In the language of personalized recommendations, we say that a user selects a movie when the user reviews the movie. The goal is to predict the number of movies the user selects in the test set. To perform collaborative filtering, users and movies are associated with d-dimensional latent vectors where d = 10. The latent vectors are checked by Bayesian Personalized Ranking (BPR), which the user is described by equation (27). We select learning parameters associated with d-dimensional latent vectors where d = 0.01. The latent vectors are learned by Bayesian Personalized Ranking (BPR)."}], "references": [{"title": "Distributed delayed stochastic optimization", "author": ["A. Agarwal", "J.C. Duchi"], "venue": "NIPS, pages 873\u2013881,", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2011}, {"title": "Latent Dirichlet allocation", "author": ["D.M. Blei", "A.Y. Ng", "M.I. Jordan"], "venue": "the Journal of machine Learning research, 3:993\u20131022,", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2003}, {"title": "Large-scale machine learning with stochastic gradient descent", "author": ["L. Bottou"], "venue": "Proceedings of COMP- STAT\u20192010, pages 177\u2013186. Springer,", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2010}, {"title": "Libsvm: a library for support vector machines", "author": ["C.-C. Chang", "C.-J. Lin"], "venue": "ACM Transactions on Intelligent Systems and Technology (TIST), 2(3):27,", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2011}, {"title": "Adaptive subgradient methods for online learning and stochastic optimization", "author": ["J. Duchi", "E. Hazan", "Y. Singer"], "venue": "The Journal of Machine Learning Research, 12:2121\u20132159,", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2011}, {"title": "Dual averaging for distributed optimization: convergence analysis and network scaling", "author": ["J.C. Duchi", "A. Agarwal", "M.J. Wainwright"], "venue": "Automatic Control, IEEE Transactions on, 57(3):592\u2013606,", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2012}, {"title": "Large-scale matrix factorization with distributed stochastic gradient descent", "author": ["R. Gemulla", "E. Nijkamp", "P.J. Haas", "Y. Sismanis"], "venue": "SIGKDD, pages 69\u201377. ACM,", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2011}, {"title": "Finding scientific topics", "author": ["T.L. Griffiths", "M. Steyvers"], "venue": "Proceedings of the National Academy of Sciences, 101(suppl 1):5228\u20135235,", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2004}, {"title": "More effective distributed ML via a stale synchronous parallel parameter server", "author": ["Q. Ho", "J. Cipar", "H. Cui", "S. Lee", "J.K. Kim", "P.B. Gibbons", "G.A. Gibson", "G. Ganger", "E.P. Xing"], "venue": "NIPS, pages 1223\u20131231,", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2013}, {"title": "Stochastic variational inference", "author": ["M.D. Hoffman", "D.M. Blei", "C. Wang", "J. Paisley"], "venue": "The Journal of Machine Learning Research, 14(1):1303\u20131347,", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2013}, {"title": "Communicationefficient distributed dual coordinate ascent", "author": ["M. Jaggi", "V. Smith", "M. Tak\u00e1c", "J. Terhorst", "S. Krishnan", "T. Hofmann", "M.I. Jordan"], "venue": "NIPS, pages 3068\u20133076,", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2014}, {"title": "Accelerating stochastic gradient descent using predictive variance reduction", "author": ["R. Johnson", "T. Zhang"], "venue": "NIPS, pages 315\u2013323,", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2013}, {"title": "Matrix factorization techniques for recommender systems", "author": ["Y. Koren", "R. Bell", "C. Volinsky"], "venue": "Computer, (8):30\u201337,", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2009}, {"title": "Imagenet classification with deep convolutional neural networks", "author": ["A. Krizhevsky", "I. Sutskever", "G.E. Hinton"], "venue": "NIPS, pages 1097\u20131105,", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2012}, {"title": "Distributed nonnegative matrix factorization for web-scale dyadic data analysis on mapreduce", "author": ["C. Liu", "H.-c. Yang", "J. Fan", "L.-W. He", "Y.-M. Wang"], "venue": "In WWW,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2010}, {"title": "An asynchronous parallel stochastic coordinate descent algorithm", "author": ["J. Liu", "S.J. Wright", "C. R\u00e9", "V. Bittorf", "S. Sridhar"], "venue": "arXiv preprint arXiv:1311.1873,", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2013}, {"title": "Expectation propagation for approximate Bayesian inference", "author": ["T.P. Minka"], "venue": "UAI, pages 362\u2013369. Morgan Kaufmann Publishers Inc.,", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2001}, {"title": "Distributed inference for latent Dirichlet allocation", "author": ["D. Newman", "P. Smyth", "M. Welling", "A.U. Asuncion"], "venue": "NIPS, pages 1081\u20131088,", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2007}, {"title": "Making gradient descent optimal for strongly convex stochastic optimization", "author": ["A. Rakhlin", "O. Shamir", "K. Sridharan"], "venue": "arXiv preprint arXiv:1109.5647,", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2011}, {"title": "Hogwild: A lock-free approach to parallelizing stochastic gradient descent", "author": ["B. Recht", "C. Re", "S. Wright", "F. Niu"], "venue": "NIPS, pages 693\u2013701,", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2011}, {"title": "BPR: Bayesian personalized ranking from implicit feedback", "author": ["S. Rendle", "C. Freudenthaler", "Z. Gantner", "L. Schmidt-Thieme"], "venue": "UAI, pages 452\u2013461. AUAI Press,", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2009}, {"title": "Minimizing finite sums with the stochastic average gradient", "author": ["M. Schmidt", "N.L. Roux", "F. Bach"], "venue": "arXiv preprint arXiv:1309.2388,", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2013}, {"title": "Stochastic dual coordinate ascent methods for regularized loss", "author": ["S. Shalev-Shwartz", "T. Zhang"], "venue": "The Journal of Machine Learning Research, 14(1):567\u2013599,", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2013}, {"title": "Extracting and composing robust features with denoising autoencoders", "author": ["P. Vincent", "H. Larochelle", "Y. Bengio", "P.-A. Manzagol"], "venue": "ICML, pages 1096\u20131103. ACM,", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2008}, {"title": "Dual averaging method for regularized stochastic learning and online optimization", "author": ["L. Xiao"], "venue": "NIPS, pages 2116\u20132124,", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2009}, {"title": "Petuum: A new platform for distributed machine learning on big data", "author": ["E.P. Xing", "Q. Ho", "W. Dai", "J.K. Kim", "J. Wei", "S. Lee", "X. Zheng", "P. Xie", "A. Kumar", "Y. Yu"], "venue": "arXiv preprint arXiv:1312.7651,", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2013}, {"title": "Resilient distributed datasets: A fault-tolerant abstraction for in-memory cluster computing", "author": ["M. Zaharia", "M. Chowdhury", "T. Das", "A. Dave", "J. Ma", "M. McCauley", "M.J. Franklin", "S. Shenker", "I. Stoica"], "venue": "NSDI. USENIX Association,", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2012}, {"title": "Solving large scale linear prediction problems using stochastic gradient descent algorithms", "author": ["T. Zhang"], "venue": "ICML, page 116. ACM,", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2004}, {"title": "Communication-efficient algorithms for statistical optimization", "author": ["Y. Zhang", "M.J. Wainwright", "J.C. Duchi"], "venue": "NIPS, pages 1502\u20131510,", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2012}, {"title": "A fast parallel SGD for matrix factorization in shared memory systems", "author": ["Y. Zhuang", "W.-S. Chin", "Y.-C. Juan", "C.-J. Lin"], "venue": "RecSys, pages 249\u2013256. ACM,", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2013}, {"title": "Parallelized stochastic gradient descent", "author": ["M. Zinkevich", "M. Weimer", "L. Li", "A.J. Smola"], "venue": "NIPS, pages 2595\u20132603,", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2010}], "referenceMentions": [{"referenceID": 27, "context": "Indeed, for convex optimization, the efficiency of stochastic gradient descent (SGD) and its variants has been established both in theory and in practice [29, 3, 26, 5, 23, 12].", "startOffset": 154, "endOffset": 176}, {"referenceID": 2, "context": "Indeed, for convex optimization, the efficiency of stochastic gradient descent (SGD) and its variants has been established both in theory and in practice [29, 3, 26, 5, 23, 12].", "startOffset": 154, "endOffset": 176}, {"referenceID": 24, "context": "Indeed, for convex optimization, the efficiency of stochastic gradient descent (SGD) and its variants has been established both in theory and in practice [29, 3, 26, 5, 23, 12].", "startOffset": 154, "endOffset": 176}, {"referenceID": 4, "context": "Indeed, for convex optimization, the efficiency of stochastic gradient descent (SGD) and its variants has been established both in theory and in practice [29, 3, 26, 5, 23, 12].", "startOffset": 154, "endOffset": 176}, {"referenceID": 21, "context": "Indeed, for convex optimization, the efficiency of stochastic gradient descent (SGD) and its variants has been established both in theory and in practice [29, 3, 26, 5, 23, 12].", "startOffset": 154, "endOffset": 176}, {"referenceID": 11, "context": "Indeed, for convex optimization, the efficiency of stochastic gradient descent (SGD) and its variants has been established both in theory and in practice [29, 3, 26, 5, 23, 12].", "startOffset": 154, "endOffset": 176}, {"referenceID": 12, "context": "For non-convex optimization, stochastic methods achieve state-of-the-art performance on a broad class of problems, including matrix factorization [13], neural networks [14] and representation learning [25].", "startOffset": 146, "endOffset": 150}, {"referenceID": 13, "context": "For non-convex optimization, stochastic methods achieve state-of-the-art performance on a broad class of problems, including matrix factorization [13], neural networks [14] and representation learning [25].", "startOffset": 168, "endOffset": 172}, {"referenceID": 23, "context": "For non-convex optimization, stochastic methods achieve state-of-the-art performance on a broad class of problems, including matrix factorization [13], neural networks [14] and representation learning [25].", "startOffset": 201, "endOffset": 205}, {"referenceID": 16, "context": "Stochastic algorithms are also widely used in the Bayesian setting for finding approximations to posterior distributions; examples include Markov chain Monte Carlo, expectation propagation [18] and stochastic variational inference [10].", "startOffset": 189, "endOffset": 193}, {"referenceID": 9, "context": "Stochastic algorithms are also widely used in the Bayesian setting for finding approximations to posterior distributions; examples include Markov chain Monte Carlo, expectation propagation [18] and stochastic variational inference [10].", "startOffset": 231, "endOffset": 235}, {"referenceID": 19, "context": "One active line of research studies asynchronous parallel updating schemes in the setting of a lock-free shared memory [21, 6, 17, 31, 9, 27].", "startOffset": 119, "endOffset": 141}, {"referenceID": 5, "context": "One active line of research studies asynchronous parallel updating schemes in the setting of a lock-free shared memory [21, 6, 17, 31, 9, 27].", "startOffset": 119, "endOffset": 141}, {"referenceID": 15, "context": "One active line of research studies asynchronous parallel updating schemes in the setting of a lock-free shared memory [21, 6, 17, 31, 9, 27].", "startOffset": 119, "endOffset": 141}, {"referenceID": 29, "context": "One active line of research studies asynchronous parallel updating schemes in the setting of a lock-free shared memory [21, 6, 17, 31, 9, 27].", "startOffset": 119, "endOffset": 141}, {"referenceID": 8, "context": "One active line of research studies asynchronous parallel updating schemes in the setting of a lock-free shared memory [21, 6, 17, 31, 9, 27].", "startOffset": 119, "endOffset": 141}, {"referenceID": 25, "context": "One active line of research studies asynchronous parallel updating schemes in the setting of a lock-free shared memory [21, 6, 17, 31, 9, 27].", "startOffset": 119, "endOffset": 141}, {"referenceID": 0, "context": "When the time delay of concurrent updates are bounded, it is known that such updates preserve statistical correctness [1, 17].", "startOffset": 118, "endOffset": 125}, {"referenceID": 15, "context": "When the time delay of concurrent updates are bounded, it is known that such updates preserve statistical correctness [1, 17].", "startOffset": 118, "endOffset": 125}, {"referenceID": 30, "context": "There has also been a flurry of research studying the implementation of stochastic algorithms in the fully distributed setting [32, 30, 19, 7, 16, 11].", "startOffset": 127, "endOffset": 150}, {"referenceID": 28, "context": "There has also been a flurry of research studying the implementation of stochastic algorithms in the fully distributed setting [32, 30, 19, 7, 16, 11].", "startOffset": 127, "endOffset": 150}, {"referenceID": 17, "context": "There has also been a flurry of research studying the implementation of stochastic algorithms in the fully distributed setting [32, 30, 19, 7, 16, 11].", "startOffset": 127, "endOffset": 150}, {"referenceID": 6, "context": "There has also been a flurry of research studying the implementation of stochastic algorithms in the fully distributed setting [32, 30, 19, 7, 16, 11].", "startOffset": 127, "endOffset": 150}, {"referenceID": 14, "context": "There has also been a flurry of research studying the implementation of stochastic algorithms in the fully distributed setting [32, 30, 19, 7, 16, 11].", "startOffset": 127, "endOffset": 150}, {"referenceID": 10, "context": "There has also been a flurry of research studying the implementation of stochastic algorithms in the fully distributed setting [32, 30, 19, 7, 16, 11].", "startOffset": 127, "endOffset": 150}, {"referenceID": 26, "context": "We build Splash on top of Apache Spark [28], a popular distributed data-processing framework for batch algorithms.", "startOffset": 39, "endOffset": 43}, {"referenceID": 7, "context": "For example, the collapsed Gibbs sampling algorithm for LDA [8] maintains a topic assignment for each word, which is stored as a local variable.", "startOffset": 60, "endOffset": 63}, {"referenceID": 22, "context": "The stochastic dual coordinate ascent (SDCA) algorithm [24] maintains a dual variable for each data element, which is also stored as a local variable.", "startOffset": 55, "endOffset": 59}, {"referenceID": 26, "context": "The Parametrized RDD is based on the Resilient Distributed Dataset (RDD) [28] used by Apache Spark.", "startOffset": 73, "endOffset": 77}, {"referenceID": 0, "context": "The scheme (3) provides a good approximation to the full update if the batch size |Di| is sufficiently small [1].", "startOffset": 109, "endOffset": 112}, {"referenceID": 18, "context": "It is the optimal rate of convergence among all optimization algorithms which relies on noisy gradients [20].", "startOffset": 104, "endOffset": 108}, {"referenceID": 3, "context": "Datasets For logistic regression, we use the Covtype, RCV1 and MNIST 8M datasets from the LIBSVM Data website [4].", "startOffset": 110, "endOffset": 113}, {"referenceID": 7, "context": "The first class are sampling-based algorithms, where we compare collapsed Gibbs sampling [8] and its parallel version under Splash.", "startOffset": 89, "endOffset": 92}, {"referenceID": 1, "context": "The second class are variational inference based algorithms, where we compare batch variational inference (VI) [2], stochastic variational inference (SVI) [10] and the parallel version of SVI under Splash.", "startOffset": 111, "endOffset": 114}, {"referenceID": 9, "context": "The second class are variational inference based algorithms, where we compare batch variational inference (VI) [2], stochastic variational inference (SVI) [10] and the parallel version of SVI under Splash.", "startOffset": 155, "endOffset": 159}, {"referenceID": 20, "context": "For movie recommendation, we compare the batch algorithm based on alternating loss minimization, the stochastic algorithm called Bayesian Personalized Ranking (BPR) [22] and the parallel version of BPR under Splash.", "startOffset": 165, "endOffset": 169}, {"referenceID": 0, "context": "[1] A.", "startOffset": 0, "endOffset": 3}, {"referenceID": 1, "context": "[2] D.", "startOffset": 0, "endOffset": 3}, {"referenceID": 2, "context": "[3] L.", "startOffset": 0, "endOffset": 3}, {"referenceID": 3, "context": "[4] C.", "startOffset": 0, "endOffset": 3}, {"referenceID": 4, "context": "[5] J.", "startOffset": 0, "endOffset": 3}, {"referenceID": 5, "context": "[6] J.", "startOffset": 0, "endOffset": 3}, {"referenceID": 6, "context": "[7] R.", "startOffset": 0, "endOffset": 3}, {"referenceID": 7, "context": "[8] T.", "startOffset": 0, "endOffset": 3}, {"referenceID": 8, "context": "[9] Q.", "startOffset": 0, "endOffset": 3}, {"referenceID": 9, "context": "[10] M.", "startOffset": 0, "endOffset": 4}, {"referenceID": 10, "context": "[11] M.", "startOffset": 0, "endOffset": 4}, {"referenceID": 11, "context": "[12] R.", "startOffset": 0, "endOffset": 4}, {"referenceID": 12, "context": "[13] Y.", "startOffset": 0, "endOffset": 4}, {"referenceID": 13, "context": "[14] A.", "startOffset": 0, "endOffset": 4}, {"referenceID": 14, "context": "[16] C.", "startOffset": 0, "endOffset": 4}, {"referenceID": 15, "context": "[17] J.", "startOffset": 0, "endOffset": 4}, {"referenceID": 16, "context": "[18] T.", "startOffset": 0, "endOffset": 4}, {"referenceID": 17, "context": "[19] D.", "startOffset": 0, "endOffset": 4}, {"referenceID": 18, "context": "[20] A.", "startOffset": 0, "endOffset": 4}, {"referenceID": 19, "context": "[21] B.", "startOffset": 0, "endOffset": 4}, {"referenceID": 20, "context": "[22] S.", "startOffset": 0, "endOffset": 4}, {"referenceID": 21, "context": "[23] M.", "startOffset": 0, "endOffset": 4}, {"referenceID": 22, "context": "[24] S.", "startOffset": 0, "endOffset": 4}, {"referenceID": 23, "context": "[25] P.", "startOffset": 0, "endOffset": 4}, {"referenceID": 24, "context": "[26] L.", "startOffset": 0, "endOffset": 4}, {"referenceID": 25, "context": "[27] E.", "startOffset": 0, "endOffset": 4}, {"referenceID": 26, "context": "[28] M.", "startOffset": 0, "endOffset": 4}, {"referenceID": 27, "context": "[29] T.", "startOffset": 0, "endOffset": 4}, {"referenceID": 28, "context": "[30] Y.", "startOffset": 0, "endOffset": 4}, {"referenceID": 29, "context": "[31] Y.", "startOffset": 0, "endOffset": 4}, {"referenceID": 30, "context": "[32] M.", "startOffset": 0, "endOffset": 4}, {"referenceID": 1, "context": "1 Collapsed Gibbs sampling Latent Dirichelet Allocation [2] (LDA) is an unsupervised model for learning topics from documents.", "startOffset": 56, "endOffset": 59}, {"referenceID": 9, "context": "2 Stochastic Variational Inference Stochastic variational inference (SVI) is another efficient approach to learning the LDA model [10].", "startOffset": 130, "endOffset": 134}, {"referenceID": 20, "context": "The algorithm is called Bayesian Personalized Ranking (BPR) [22].", "startOffset": 60, "endOffset": 64}, {"referenceID": 4, "context": "[5].", "startOffset": 0, "endOffset": 3}, {"referenceID": 0, "context": "where w\u2032 = \u03b1wj + (1\u2212 \u03b1)w\u2217 for some \u03b1 \u2208 [0, 1].", "startOffset": 39, "endOffset": 45}, {"referenceID": 3, "context": "These datasets are obtained from the LIBSVM Data website [4].", "startOffset": 57, "endOffset": 60}, {"referenceID": 1, "context": "3 Stochastic Variational Inference Variational inference [2] and stochastic variational inference [10] are alternative efficient approaches to learning the LDA model.", "startOffset": 57, "endOffset": 60}, {"referenceID": 9, "context": "3 Stochastic Variational Inference Variational inference [2] and stochastic variational inference [10] are alternative efficient approaches to learning the LDA model.", "startOffset": 98, "endOffset": 102}, {"referenceID": 9, "context": "[10], we organize documents into mini-batches, so that SVI processes a mini-batch like processing a document.", "startOffset": 0, "endOffset": 4}, {"referenceID": 9, "context": "[10] for evaluating the SVI algorithm.", "startOffset": 0, "endOffset": 4}, {"referenceID": 19, "context": "In the context of SGD, the accumulation scheme resembles the HOGWILD! update scheme [21].", "startOffset": 84, "endOffset": 88}, {"referenceID": 17, "context": "[19].", "startOffset": 0, "endOffset": 4}], "year": 2017, "abstractText": "Stochastic algorithms are efficient approaches to solving machine learning and optimization problems. In this paper, we propose a general framework called Splash for parallelizing stochastic algorithms on multi-node distributed systems. Splash consists of a programming interface and an execution engine. Using the programming interface, the user develops sequential stochastic algorithms without concerning any detail about distributed computing. The algorithm is then automatically parallelized by a communication-efficient execution engine. We provide theoretical justifications on the optimal rate of convergence for parallelizing stochastic gradient descent. The real-data experiments with stochastic gradient descent, collapsed Gibbs sampling, stochastic variational inference and stochastic collaborative filtering verify that Splash yields order-of-magnitude speedup over single-thread stochastic algorithms and over parallelized batch algorithms. Besides its efficiency, Splash provides a rich collection of interfaces for algorithm implementation. It is built on Apache Spark and is closely integrated with the Spark ecosystem.", "creator": "LaTeX with hyperref package"}}}