{"id": "1506.08230", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "26-Jun-2015", "title": "Convolutional networks and learning invariant to homogeneous multiplicative scalings", "abstract": "The conventional classification schemes -- notably multinomial logistic regression -- used in conjunction with convolutional networks (convnets) are classical in statistics, designed without consideration for the usual coupling with convnets, stochastic gradient descent, and backpropagation. In the specific application to supervised learning for convnets, a simple scale-invariant classification stage turns out to be more robust than multinomial logistic regression, appears to result in slightly lower errors on several standard test sets, has similar computational costs, and features precise control over the actual rate of learning. \"Scale-invariant\" means that multiplying the input values by any nonzero scalar leaves the output unchanged.", "histories": [["v1", "Fri, 26 Jun 2015 22:22:21 GMT  (617kb,D)", "http://arxiv.org/abs/1506.08230v1", "14 pages, 6 figures, 4 tables"], ["v2", "Sun, 13 Sep 2015 18:10:04 GMT  (652kb,D)", "http://arxiv.org/abs/1506.08230v2", "12 pages, 6 figures, 4 tables (ICLR reviewers need only consider the first 8 pages -- the remaining pages are appendices)"], ["v3", "Thu, 24 Dec 2015 20:20:48 GMT  (652kb,D)", "http://arxiv.org/abs/1506.08230v3", "12 pages, 6 figures, 4 tables (ICLR reviewers need only consider the first 9 pages -- the remaining pages are appendices)"], ["v4", "Tue, 16 Feb 2016 22:55:43 GMT  (653kb,D)", "http://arxiv.org/abs/1506.08230v4", "12 pages, 6 figures, 4 tables"]], "COMMENTS": "14 pages, 6 figures, 4 tables", "reviews": [], "SUBJECTS": "cs.LG cs.NE", "authors": ["mark tygert", "arthur szlam", "soumith chintala", "marc'aurelio ranzato", "yuandong tian", "wojciech zaremba"], "accepted": false, "id": "1506.08230"}, "pdf": {"name": "1506.08230.pdf", "metadata": {"source": "CRF", "title": "Scale-invariant learning and convolutional networks", "authors": ["Soumith Chintala", "Marc\u2019Aurelio Ranzato", "Arthur Szlam", "Yuandong Tian", "Mark Tygert"], "emails": [], "sections": [{"heading": "1 Introduction", "text": "The classification of a vector of real numbers (called \"functional activation\") into one of several discrete categories is well established and well studied, with generally satisfactory solutions such as the ubiquitous multinomial logistical regression that is being reviewed by, for example, [2]. Adapting (also known as learning or training) the combination of convexation and the classification phase by minimizing the costs / losses / objective functions associated with the classification suggests that one stage is being developed specifically for use in such a joint arrangement / training. In particular, many convexes are \"uttered\" in terms of scale multiplication and the classification phase."}, {"heading": "2 Notational conventions", "text": "We follow the recommendations of [8]: All vectors are column vectors (with the exception of the gradients of a scalar in relation to a column vector, which are row vectors), and we use \"v\" to denote the Euclidean norm of a vector v; that is, \"v\" is the square root of the sum of the squares of the entries of v. We use \"A\" to denote the spectral norm of a matrix A; that is, \"A\" is the largest single value of A, which is also the maximum of \"Av\" above each vector v, so that \"v\" = 1. The terminology \"Frobenius\" of A refers to the square root of the squares of the entries of A. The spectral norm of a vector, which is regarded as a matrix having a row, or \"Frobenius norm\" is the same norm as the norm of the entries of A."}, {"heading": "3 A scale-invariant classification stage", "text": "\"..\".. \"..\".. \"..\".. \"..\".. \"..\".. \"..\".. \"..\".. \"We study a linear classification, which assigns each one of the classes, which each form a real basis for the classification.\" \"We have the opportunity to explore the classification of the individual classes.\" \"The classification of the individual classes in the class of classes.\" \"\" The classification of the classes in the classes. \"\" \"The classification in the classes.\" \"\" The classification in the classes. \"\" \"\" The classification in the classes. \"\" \"\".. \"\" \"\".. \"\" \"\".. \"\" \"..\" \"\".. \"\" \"..\" \"\" \"..\" \"\" \"\".. \"\" \"\" \"..\" \"\" \"..\" \"\" \"..\" \"\" \"..\""}, {"heading": "4 Robustness", "text": "The combination of (1) and the fact that the Euclidean norm of z of (2) 1 results in the cost of (5) 0 \u2264 c \u2264 4 being satisfactory.7 As investigated in [2], the costs associated with the classification by multinomial logistic regression are isr = ln (m \u2211 '= 1 Exp (y ('))) \u2212 y (j), (8), where j is the index below 1, 2,..., k of the correct class, and y (1), y (2),... y (m) are the entries of the vector y of (3), with m = k for multinomial logistic regression. While the costs c are limited as in (7), the costs r of (8) are limited only for positive values of y (j) and grow linearly for negative y (j)."}, {"heading": "5 Numerical experiments", "text": "The idea behind this is that the people who stay in the city do not feel able to integrate themselves, but that they stay in the city in which they want to live, and that they stay in the city in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they, in which they, in which they live, in which they, in which they, in which they live, in which they, in which they, in which they, in which they live, in which they, in which they, in which they, in which they live, in which they, in which they, in which they live, in which they, in which they live, in which they live, in which they, in which they, in which they, in which they, in which they live, in which they, in which they, in which they live, in which they, in which they, in which they, in which they, in which they live, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they live, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they live, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they live, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which, in which they live, in which"}, {"heading": "A Calculation of the update in (4)", "text": "In these appendices we use the calculation of the matrix as worked out by [8].The chain rule stipulates that the gradient of c in relation to A has the following values: (10) The gradient of c in relation to (5) in relation to z is \u2202 c = 2 (z \u2212 tj) >. (10) The Jacobian of z from (2) in relation to y is \u2202 z = I \u2212 zz > y, (11) where I have the identity matrix and zz > is the orthogonal projection on z - due to (2), \u011az = 1. The derivative of y from (3) in relation to A is \u2202 y = x I, (12) where I have the identity matrix and is the tensor product; x I is a vector of the same height as x, where each entry is the identity matrix I times the corresponding input of x = x I, (12) where I have the identity matrix (b) and (12) is the input of the same height as the input of x; where each input is a vector I; where x is the corresponding to the input of x."}, {"heading": "B Proof of the bound in (6)", "text": "In these appendices, we use the matrix calculation as worked out by [8]. To (6) prove, we calculate \u2202 c / \u2202 x using the chain rule \u2202 c \u2202 x = \u2202 c \u2202 z \u2202 y \u2202 x. (14) The course of c from (5) with respect to z is \u2202 c \u2202 z = 2 (z \u2212 tj) >. (15) The Jacobian of z from (2) with respect to y is \u2202 z \u0445 y = I \u2212 zz > with respect to y, (16) where I designate the identity matrix and zz >, is the orthogonal projection on z - due to (2), \u0445 z = 1. Of course, the Jacobian of y from (3) with respect to x 12th y = A. (17) The fact that I \u2212 zz > is an orthogonal projection that yields I \u2212 zz > with respect to 1. (18) Combine (1) - (3) and (14 - (18) yields (6) as desired."}, {"heading": "Acknowledgements", "text": "We would like to thank Le \u0301 on Bottou and Rob Fergus for their critical contributions to this project."}], "references": [{"title": "Normalization as a canonical neural computation", "author": ["M. Carandini", "D.J. Heeger"], "venue": "Nature Reviews Neurosci., 13 ", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2012}, {"title": "Elements of Statistical Learning: Data Mining", "author": ["T. Hastie", "R. Tibshirani", "J. Friedman"], "venue": "Inference, and Prediction, Springer, 2nd ed.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2009}, {"title": "A framework for kernel-based multi-category classification", "author": ["S.I. Hill", "A. Doucet"], "venue": "J. Artificial Intel. Research, 30 ", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2007}, {"title": "Batch normalization: accelerating deep network training by reducing internal covariate shift", "author": ["S. Ioffe", "C. Szegedy"], "venue": "Tech. Rep. 1502.03167, arXiv", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2015}, {"title": "Learning multiple layers of features from tiny images", "author": ["A. Krizhevsky"], "venue": "Tech. Rep. Master\u2019s Thesis, University of Toronto Department of Computer Science", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2009}, {"title": "An MM algorithm for multicategory vertex discriminant analysis", "author": ["K. Lange", "T.T. Wu"], "venue": "J. Comput. Graph. Statist., 17 ", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2008}, {"title": "Gradient-based learning applied to document recognition", "author": ["Y. LeCun", "L. Bottou", "Y. Bengio", "P. Haffner"], "venue": "Proc. IEEE, 86 ", "citeRegEx": "7", "shortCiteRegEx": null, "year": 1998}, {"title": "Matrix Differential Calculus with Applications in Statistics and Econometrics", "author": ["J.R. Magnus", "H. Neudecker"], "venue": "John Wiley and Sons, 3rd ed.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2007}, {"title": "Multiclass learning with simplex coding", "author": ["Y. Mroueh", "T. Poggio", "L. Rosasco", "J.-J. Slotine"], "venue": "Advances in Neural Information Processing Systems, vol. 25, Curran Associates", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2012}, {"title": "ImageNet large scale visual recognition challenge", "author": ["O. Russakovsky", "J. Deng", "H. Su", "J. Kruse", "S. Satheesh", "S. Ma", "Z. Huang", "A. Karpathy", "A. Khosla", "M. Bernstein", "A.C. Berg", "L. Fei-Fei"], "venue": "Tech. Rep. 1409.0575v3, arXiv", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2015}, {"title": "Multiclass boosting: theory and algorithms", "author": ["M.J. Saberian", "N. Vasconcelos"], "venue": "Advances in Neural Information Processing Systems, vol. 24, Curran Associates", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2011}, {"title": "Multicategory vertex discriminant analysis for high-dimensional data", "author": ["T.T. Wu", "K. Lange"], "venue": "Annals Appl. Statist., 4 ", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2010}, {"title": "Nonlinear vertex discriminant analysis with reproducing kernels", "author": ["T.T. Wu", "Y. Wu"], "venue": "Statist. Anal. Data Mining, 5 ", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2012}], "referenceMentions": [{"referenceID": 1, "context": "1 Introduction Classification of a vector of real numbers (called \u201cfeature activations\u201d) into one of several discrete categories is well established and well studied, with generally satisfactory solutions such as the ubiquitous multinomial logistic regression reviewed, for example, by [2].", "startOffset": 286, "endOffset": 289}, {"referenceID": 6, "context": "However, the canonical classification may not couple well with generation of the feature activations via convolutional networks (convnets) trained using stochastic gradient descent, as discussed, for example, by [7].", "startOffset": 212, "endOffset": 215}, {"referenceID": 2, "context": "Similar classification has been introduced earlier in other contexts by [3], [6], [9], [11], [12], [13], and others.", "startOffset": 72, "endOffset": 75}, {"referenceID": 5, "context": "Similar classification has been introduced earlier in other contexts by [3], [6], [9], [11], [12], [13], and others.", "startOffset": 77, "endOffset": 80}, {"referenceID": 8, "context": "Similar classification has been introduced earlier in other contexts by [3], [6], [9], [11], [12], [13], and others.", "startOffset": 82, "endOffset": 85}, {"referenceID": 10, "context": "Similar classification has been introduced earlier in other contexts by [3], [6], [9], [11], [12], [13], and others.", "startOffset": 87, "endOffset": 91}, {"referenceID": 11, "context": "Similar classification has been introduced earlier in other contexts by [3], [6], [9], [11], [12], [13], and others.", "startOffset": 93, "endOffset": 97}, {"referenceID": 12, "context": "Similar classification has been introduced earlier in other contexts by [3], [6], [9], [11], [12], [13], and others.", "startOffset": 99, "endOffset": 103}, {"referenceID": 0, "context": "Complementary normalization includes the work of [1], [4], and the associated references.", "startOffset": 49, "endOffset": 52}, {"referenceID": 3, "context": "Complementary normalization includes the work of [1], [4], and the associated references.", "startOffset": 54, "endOffset": 57}, {"referenceID": 7, "context": "We follow the recommendations of [8]: all vectors are column vectors (aside from gradients of a scalar with respect to a column vector, which are row vectors), and we use \u2016v\u2016 to denote the Euclidean norm of a vector v; that is, \u2016v\u2016 is the square root of the sum of the squares of the entries of v.", "startOffset": 33, "endOffset": 36}, {"referenceID": 2, "context": ", tk that are the vertices of either a standard simplex or a regular simplex embedded in a Euclidean space of dimension m \u2265 k \u2014 the dimension of the embedding space being strictly greater than the minimum (k \u2212 1) required to contain the simplex will give extra space to help facilitate learning; [3], [6], [9], [11], [12], and [13] (amongst others) discuss these simplices and their applications to classification.", "startOffset": 296, "endOffset": 299}, {"referenceID": 5, "context": ", tk that are the vertices of either a standard simplex or a regular simplex embedded in a Euclidean space of dimension m \u2265 k \u2014 the dimension of the embedding space being strictly greater than the minimum (k \u2212 1) required to contain the simplex will give extra space to help facilitate learning; [3], [6], [9], [11], [12], and [13] (amongst others) discuss these simplices and their applications to classification.", "startOffset": 301, "endOffset": 304}, {"referenceID": 8, "context": ", tk that are the vertices of either a standard simplex or a regular simplex embedded in a Euclidean space of dimension m \u2265 k \u2014 the dimension of the embedding space being strictly greater than the minimum (k \u2212 1) required to contain the simplex will give extra space to help facilitate learning; [3], [6], [9], [11], [12], and [13] (amongst others) discuss these simplices and their applications to classification.", "startOffset": 306, "endOffset": 309}, {"referenceID": 10, "context": ", tk that are the vertices of either a standard simplex or a regular simplex embedded in a Euclidean space of dimension m \u2265 k \u2014 the dimension of the embedding space being strictly greater than the minimum (k \u2212 1) required to contain the simplex will give extra space to help facilitate learning; [3], [6], [9], [11], [12], and [13] (amongst others) discuss these simplices and their applications to classification.", "startOffset": 311, "endOffset": 315}, {"referenceID": 11, "context": ", tk that are the vertices of either a standard simplex or a regular simplex embedded in a Euclidean space of dimension m \u2265 k \u2014 the dimension of the embedding space being strictly greater than the minimum (k \u2212 1) required to contain the simplex will give extra space to help facilitate learning; [3], [6], [9], [11], [12], and [13] (amongst others) discuss these simplices and their applications to classification.", "startOffset": 317, "endOffset": 321}, {"referenceID": 12, "context": ", tk that are the vertices of either a standard simplex or a regular simplex embedded in a Euclidean space of dimension m \u2265 k \u2014 the dimension of the embedding space being strictly greater than the minimum (k \u2212 1) required to contain the simplex will give extra space to help facilitate learning; [3], [6], [9], [11], [12], and [13] (amongst others) discuss these simplices and their applications to classification.", "startOffset": 327, "endOffset": 331}, {"referenceID": 6, "context": "We then conduct iterations of stochastic gradient descent as advocated by [7], updating A to \u00c3 on each iteration via \u00c3 = A\u2212 h ( \u2202c \u2202A )> , (4) where h is a positive real number (known as the \u201clearning rate\u201d or \u201cstep length\u201d) and c is the cost to be minimized that is associated with a vector chosen at random from among the input vectors 2", "startOffset": 74, "endOffset": 77}, {"referenceID": 6, "context": "As elaborated by [7], usually we combine stochastic gradient descent with backpropagation to update the entries of x associated with the chosen input, which requires propagating the gradient \u2202c/\u2202x back into the network generating the feature activations that are the entries of x for the chosen input sample.", "startOffset": 17, "endOffset": 20}, {"referenceID": 1, "context": "(7) As reviewed by [2], the cost associated with classification via multinomial logistic regression is r = ln ( m \u2211 `=1 exp(y) ) \u2212 y, (8) where j is the index among 1, 2, .", "startOffset": 19, "endOffset": 22}, {"referenceID": 6, "context": "Following [7], the architectures for generating the feature activations are convolutional networks (convnets) consisting of series of stages, with each stage feeding its output into the next (except for the last, which feeds into the classification stage).", "startOffset": 10, "endOffset": 13}, {"referenceID": 4, "context": "We consider three data sets whose training properties are reasonably straightforward to investigate, with each set consisting of k = 10 classes of images; the first two are the usual CIFAR-10 and MNIST of [5] and [7].", "startOffset": 205, "endOffset": 208}, {"referenceID": 6, "context": "We consider three data sets whose training properties are reasonably straightforward to investigate, with each set consisting of k = 10 classes of images; the first two are the usual CIFAR-10 and MNIST of [5] and [7].", "startOffset": 213, "endOffset": 216}, {"referenceID": 9, "context": "The third is a subset of the 2012 ImageNet data set of [10], retaining 10 classes of images, representing each class by 100 samples in a training set and 50 per class in a testing set.", "startOffset": 55, "endOffset": 59}, {"referenceID": 6, "context": "As recommended by [7], we learn via (minibatched) stochastic gradient descent, with 100 samples per minibatch; rather than updating the parameters being learned for randomly selected individual images from the training set exactly as in Section 3, we instead randomly permute the training set and partition this permuted set of images into subsets of 100, updating the parameters simultaneously for all 100 images constituting each of the subsets (known as \u201cminibatches\u201d), processing the series of minibatches in series.", "startOffset": 18, "endOffset": 21}, {"referenceID": 7, "context": "A Calculation of the update in (4) In these appendices, we leverage matrix Calculus as elaborated by [8].", "startOffset": 101, "endOffset": 104}, {"referenceID": 7, "context": "B Proof of the bound in (6) In these appendices, we leverage matrix Calculus as elaborated by [8].", "startOffset": 94, "endOffset": 97}], "year": 2017, "abstractText": "The conventional classification schemes \u2014 notably multinomial logistic regression \u2014 used in conjunction with convolutional networks (convnets) are classical in statistics, designed without consideration for the usual coupling with convnets, stochastic gradient descent, and backpropagation. In the specific application to supervised learning for convnets, a simple scale-invariant classification stage turns out to be more robust than multinomial logistic regression, appears to result in slightly lower errors on several standard test sets, has similar computational costs, and features precise control over the actual rate of learning. \u201cScale-invariant\u201d means that multiplying the input values by any nonzero scalar leaves the output unchanged.", "creator": "LaTeX with hyperref package"}}}