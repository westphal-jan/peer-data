{"id": "1506.08251", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "27-Jun-2015", "title": "Occam's Gates", "abstract": "We present a complimentary objective for training recurrent neural networks (RNN) with gating units that helps with regularization and interpretability of the trained model. Attention-based RNN models have shown success in many difficult sequence to sequence classification problems with long and short term dependencies, however these models are prone to overfitting. In this paper, we describe how to regularize these models through an L1 penalty on the activation of the gating units, and show that this technique reduces overfitting on a variety of tasks while also providing to us a human-interpretable visualization of the inputs used by the network. These tasks include sentiment analysis, paraphrase recognition, and question answering.", "histories": [["v1", "Sat, 27 Jun 2015 03:03:10 GMT  (385kb,D)", "http://arxiv.org/abs/1506.08251v1", "In review at NIPS"]], "COMMENTS": "In review at NIPS", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["jonathan raiman", "szymon sidor"], "accepted": false, "id": "1506.08251"}, "pdf": {"name": "1506.08251.pdf", "metadata": {"source": "CRF", "title": "Occam\u2019s Gates", "authors": ["Jonathan Raiman", "Szymon Sidor"], "emails": ["jraiman@mit.edu", "sidor@mit.edu"], "sections": [{"heading": "1 Introduction", "text": "Attention-based recurring neural networks (RNN) have shown great success in a wide range of tasks such as computer vision [1, 2, 3], image generation [4, 5], machine translation [6], speech recognition [7], or even as controllers for memory addressing and retrieval [8, 9]. While there is debate about how biologically plausible these cognitive models are, they are desirable in their ability to allow introspection into the workings and understanding of network errors: in the case of caption [2, 1] and generation [4, 5] or emotion recognition [10], the focus of the system coincides with human intuition. The gates that modulate the network's attention in these networks serve a twofold purpose: firstly, they enable control of the flow of information, and secondly, and perhaps cruelly, the gates communicate problem structure by ensuring that certain groups of neurons are mutually activated or mutually."}, {"heading": "2 Related Work", "text": "The work we present is closely related to two areas of machine learning research: RNN regularization and attention-based models.ar Xiv: 150 6.08 251v 1 [cs.L G] 27 Jun 2015"}, {"heading": "2.1 RNN Regularization", "text": "It has recently been shown that regulation of RNNs with the help of dropout [11] is achievable by regulating a subset of recurring connections in deep RNNs [12, 13]. Previously, it was shown that regulation of weight loss produced only a small improvement [5] and that failure noises were harmful when applied to all connections, as errors accumulated over time [14]. In this paper, we show that this problem can also be solved by a deterministic approach by punishing gate activations from deep RNNs. As a result, RNNs can now benefit from multiple regulation techniques in different architectures."}, {"heading": "2.2 Attention-Based Models", "text": "In recent years, there has been a wealth of evidence that attention-based techniques can improve the performance of machine learning models. Examples include work capturing visual structure through a sequence of images [4, 15, 3, 1, 2, 10] and networks learning how to perceive and control a separate memory [8, 16, 17]. In certain cases, models are trained under surveillance at the gates [1, 16], but in many cases, there is no supervised data for the attentive components. To learn where to focus, several substitute goals have been proposed, including the setting of a prior observation distance that represents a compromise between exploration and exploitation [10], the use of reinforcement [9] to optimize a visual tracking strategy [3], or the partial monitoring of this part by the primary objective. Our work is similar to observation before [10], where we advocate closing inputs and punishing the choice of punishment."}, {"heading": "3 Problem Statement", "text": "A powerful family of models, often called encoder decoders, has opened up many new possibilities for sequence classification [5, 8, 19], including the execution of Python programs [20, 21], drawing images [4], machine translation or syntactic parsing [22, 23]. The main problem we try to solve in this paper is to improve generalization performance in performing this type of classic or structured prediction task using RNNs. In the following sections, we describe three different sequence classification problems that are used to evaluate our approach."}, {"heading": "3.1 Sentiment Analysis", "text": "The central problem in mood analysis is the correct identification and extraction of a speaker's posture or emotional tone in the context of a particular topic or area. Here, we look at the prediction of mood expressed in the Stanford Sentiment Treebank (SST) [24], a collection of 11,855 sentences from movie reviews. This data set is composed of the mood comments of 5 classes: \"terrible, bad, neutral, good, great\" for the 215,154 unique sub-phrases obtained with the Stanford parser after parsing each sentence. In our work, we do not use the parse trees, but treat each sub-phrase as a labeled sequence."}, {"heading": "3.2 Paraphrase Recognition", "text": "In Paraphrase Recognition, the problem is to predict how semantically similar two phrases are from 0 to 1. This task can be considered either regression or binary classification, and the target is measured as a Pearson correlation with human annotations or as an indication of correct paraphrase pairs. Here, we focus on paraphrase recognition of the SemEval 2014 Shared Task 1 dataset [25], which contains 9927 pairs of sentences in a 4500 / 500 / 4927 turn / dev / test split. Each sentence is provided with a Score c value [1,5], with 5 indicating that it is a paraphrase and 1 indicating that the pair is unrelated."}, {"heading": "3.3 Question Answering", "text": "Facebook AI Research recently proposed a set of 20 tasks that are supposed to be \"prerequisites\" for any system \"capable of communicating with humans.\" [27] The data set for each task consists of a series of stories, each consisting of many facts, some of which are marked as relevant, a question and the right answer. Daniel and Sandra went to the office. Then they went to the garden. Sandra and John went to the kitchen. Then they moved into the hallway. Where is Daniel? A: GardenThe football fits in the suitcase. The suitcase fits in the cupboard. The box of chocolates is smaller than the football. Does the box of chocolates fit in the suitcase? A: Yes The tasks are synthetic and have no noisy character of natural processing in the real world, making them easy to solve with handmade systems, but the open question is how to create a model that can solve these tasks without manual feature engineering for specific problems."}, {"heading": "4 Approach", "text": "To improve the performance of RNN against invisible data, we apply Occams Razor to our training data, finding in each example a minimum set of useful inputs over time. To achieve this characteristic, we apply Tor to the different observations of the input sequence, so that the network can maintain or delete the input of a point in time. For example, in a problem of mood classification, Tor would ideally only fire for emotionally charged words and otherwise rest. Because our approach is based on Gates, we assume that vector input is an inseparable unit of information at each step of time, such as a word, image, or fact. If this assumption is true, then when we force the network to reduce its gate usage by penalizing the sum of these activations, we get a solution in a local optima where Gates are less active, which should lead to a generalization. We formalize our approach by describing how we activate the frugality of gate activations for a multitude of minerals."}, {"heading": "4.1 Gated LSTMs", "text": "In our thesis, we use Long-Short Term Memory Networks [28], a popular RNN architecture specifically designed to detect dependencies over long distances and alleviate training difficulties [29]. Since its introduction in 1995, many variants have been proposed [30], but for the purposes of this research, we found that the vanilla version of [30] works best. While LSTMs are able to selectively remember or forget portions of their memory and input, they lack the ability to uniformly transform their input. We add an additional gate to LSTMs, goccam, which multiplies all input simultaneously. In Table 1, we present equations for the gated LSTM, highlighting the differences with the regular LSTM in red. We use the following denotations: \u03c3 (\u00b7) for the logistic sigmoid function, Wi, z, f, o and Ri, o, matrices for the function \u00b7 stxt, \u2212 qf for the logistic sigmoid function."}, {"heading": "4.2 Hierarchical Gated LSTMs", "text": "In this section, we introduce Hierarchical Gated LSTM (HG-LSTM), a gated attention model that uses gated LSTMs as a central building block. In the previous section, we introduced gated LSTMs that are able to selectively ignore or include all input in a timeframe, but for many tasks where the presented information can be divided into larger parts such as sentences, paragraphs or episodes, a similar gating procedure could be applied to these higher levels of abstraction. For example, to find the answer to the question of a story in the bAbI dataset, such a model would benefit from being selective about what words and facts one hears. HG-LSTM consists of two submodels: a fact model and a high-level model (HL model), which are both gated LSTMs. Figure 1 represents the architecture. Each word in a fact sequence is projected by a sequence and by the model."}, {"heading": "4.3 Sparsity Penalty", "text": "The original training target J is extended by the Sparsity penalty and the resulting target is optimised by gradient descent. Penalty is constructed by adding up the activations of the goals shown in 4.1 and scantily weighing them using a parameter \u03bbsparsely selected by hyperparameter search: J \u043a = J + \u03bbsparse \u00b7 n \u2211 i = 1goccam, i."}, {"heading": "4.4 Training Regimens", "text": "The ultimate goal of our approach is to maintain the expressiveness of the network while making it resilient to changes in the input. However, forcing it too early can do more harm than good: A greedy and locally optimal solution forces all gates to close. To prevent this, we encourage early research by gradually increasing the sparsity penalty, and doing so sparingly. We examined two different glow regimes: a linear and a square increase up to \u03bbmax in the training epoch Tmax, as shown below with e of the training epoch: Sparse (e) = \u03bbmax flat min {(e / Tmax) \u00b7 \u03bbmax, \u03bbmax} linear min {(e / Tmax) 2 \u00b7 \u03bbmax, \u03bbmax} square min {(e / Tmax}."}, {"heading": "5 Experiments", "text": "The code needed to conduct the experiments in this work is available online at https: / / www. github.com / JonathanRaiman / Dali 1."}, {"heading": "5.1 Sentiment Analysis", "text": "In this problem, our model is a gated LSTM that reads each word sequentially and uses the last hidden vector as input for a linear Softmax classifier. Our goal is to minimize Kullback-Leibler divergence with the correct designation along with the sparseness penalty. We project each word into a 100-dimensional vector using an embedding matrix, retaining only the words that occur at least twice in our training data, replacing the remaining words with a special unknown word < UNK >. We train 3 different models with hidden sizes 25, 50, 150, and apply dropout [11, 12] with a probability of p = 0.3 to the non-recurring connections of the LSTM. All models are trained with Adadelta [32] with B = 0.95, and we perform an early stop when validation accuracy no longer increases."}, {"heading": "5.2 Paraphrase Detection", "text": "For paraphrase prediction we also use gated LSTMs where the last Softmax layer is removed. Each set of a pair is fed to a separate LSTM and our goal is to minimize the square difference between the true similarity t of the sets and the point product of the last hidden states of the two LSTMs ~ h1, ~ h2: J = min {(~ hT1 ~ \u0121h2 | ~ h1 | | \u0445 ~ h2 | \u2212 t) 2} + \u03bbsparse \u00b7 (\u2211 n i = 1 g1, i + \u2211 n i = 1 g2, i) instead of a Softmax linear classifier, we use the last hidden state of the LSTM."}, {"heading": "5.3 Facebook\u2019s bAbI dataset", "text": "The HG-LSTM takes a question, followed by the sequence of facts, and the final hidden state of the HG-LSTM is fed as input to an LSTM decoder that produces the answer sequentially and terminates its prediction with a < EOS > symbol. We use a separate gated LSTM for questions and facts when creating representations for the HG-LSTM model that produces the answer sequentially and terminates its prediction with a < EOS > symbol that enables the embedding of words in the question and the current hidden state of the high-level model. Our error function is the sum of three separate targets: Eprediction = W-Y-1 (w-S) + s (w-S) + s (w-S), 0) Efact =."}, {"heading": "6 Results", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "6.1 Effects on performance", "text": "The gates of Occam improve the generalization of the sentiment analysis (Fig. 2), the paraphrase prediction (Fig. 4) and the majority of the bAbI questions when answering problems (Fig. 6, Table 2). This effect is particularly visible because the model size increases (Fig. 2, Fig. 4). We note that increasing the model size without a spareness penalty has less effects, but by using spareness an improvement of the sentiment analysis is achieved by 5% and 18% in the paraphrase prediction memory. In addition, the accuracy increases by 14% for three arg. Relationships bAbI problem. We observe greater improvements in this task than for the other two; in particular, this task has longer sentences, and therefore the word gating method is more present. Furthermore, the sparsity annealing methods described in Section 4.4 show improvements over a static objective function (Fig. 3, Fig. 5)."}, {"heading": "6.2 Interpretability", "text": "Particularly for neural network models, there are no well-established methods for less than 0.0 1e-05 0.0001 0.0005 0.001 0.01 0.1 Memory sparsitysq uare linea rfla tsp arsi tyr egim en0.79 0.87 0.86 0.85 0.83 0.830.79 0.81 0.89 0.77 0.86 0.76 0.950.79 0.86 0.78 0.88 0.81 0.84 0.820.800.840.880.92Figure 5: Effect of austerity regimes and road levels on paraphrase prediction. 5. 01. 00. 10. 010. 0Fa ctse lect ion40 38 43 43 33 2344 43 31 2446 42 40 30 2446 44 41 33 2345 45 45 45 47 24, although attempts have been made, e.g. Hinton diagrams [33]."}, {"heading": "6.2.1 Error analysis", "text": "To support this claim, we consider an example from the bAbi dataset in which gates provide a visual indication of progress. In Figure 8, we note that when a validation accuracy of 20% is reached, the model is not yet able to distinguish important information from noise. At an accuracy of 60%, it can now highlight the relevant facts, but the gates for words are not yet mandatory. At an accuracy of 100%, facts and word gates work in harmony: the network actually activates with the relevant person and words that contain location information. We suspect that LSTMs without gates can select the right person and place, but the gates of Occam help them to ignore facts about people that are irrelevant to the question."}, {"heading": "6.2.2 Relevancy detection", "text": "To illustrate this assertion, we show two examples, both of which occurred while training the system on the problem of paraphrase recognition with a Character Model Gated LSTM (Char Gated LSTM). Figure 9 supports the assumption that the model uses word boundaries, and Figure 10 suggests that the network can ignore repetitive or superfluous characters. Figure 9: Char Gated LSTM, Gate Action with yellow highlighter. The model detects tokenization. Figure 10: Char Gated LSTM, Gate Action with yellow highlighter. The model focuses on uppercase and ignores repetitions."}, {"heading": "7 Conclusion", "text": "In this paper, we explored the use of a complementary objective function that forces attention-based RNNs to be selective with regard to their input factors. We demonstrated in three different tasks that our approach improves the generalization and interpretability of the trained models with respect to their non-sparsity penalty counterparts. Finally, we introduced Hierarchical-Gated LSTM, a new model that performs significantly better than traditional stacked LSTMs; this network combines attention-related and hierarchical components, as well as reasons for multiple levels of abstraction. Future work includes studies of this model family that show promising advances in state-of-the-art technology."}], "references": [{"title": "Deep visual-semantic alignments for generating image descriptions", "author": ["Andrej Karpathy", "Li Fei-Fei"], "venue": "arXiv preprint arXiv:1412.2306,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2014}, {"title": "Show and tell: A neural image caption generator", "author": ["Oriol Vinyals", "Alexander Toshev", "Samy Bengio", "Dumitru Erhan"], "venue": "arXiv preprint arXiv:1411.4555,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2014}, {"title": "Recurrent models of visual attention", "author": ["Volodymyr Mnih", "Nicolas Heess", "Alex Graves"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2014}, {"title": "Draw: A recurrent neural network for image generation", "author": ["Karol Gregor", "Ivo Danihelka", "Alex Graves", "Daan Wierstra"], "venue": "arXiv preprint arXiv:1502.04623,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2015}, {"title": "Generating sequences with recurrent neural networks", "author": ["Alex Graves"], "venue": "arXiv preprint arXiv:1308.0850,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2013}, {"title": "Neural machine translation by jointly learning to align and translate", "author": ["Dzmitry Bahdanau", "Kyunghyun Cho", "Yoshua Bengio"], "venue": "arXiv preprint arXiv:1409.0473,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2014}, {"title": "End-to-end continuous speech recognition using attention-based recurrent nn: First results", "author": ["Jan Chorowski", "Dzmitry Bahdanau", "Kyunghyun Cho", "Yoshua Bengio"], "venue": "arXiv preprint arXiv:1412.1602,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2014}, {"title": "Neural turing machines", "author": ["Alex Graves", "Greg Wayne", "Ivo Danihelka"], "venue": "arXiv preprint arXiv:1410.5401,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2014}, {"title": "Reinforcement learning neural turing machines", "author": ["Wojciech Zaremba", "Ilya Sutskever"], "venue": "arXiv preprint arXiv:1505.00521,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2015}, {"title": "A neural autoregressive approach to attention-based recognition", "author": ["Yin Zheng", "Richard S Zemel", "Yu-Jin Zhang", "Hugo Larochelle"], "venue": "International Journal of Computer Vision,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2014}, {"title": "Improving neural networks with dropout", "author": ["Nitish Srivastava"], "venue": "PhD thesis, University of Toronto,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2013}, {"title": "Recurrent neural network regularization", "author": ["Wojciech Zaremba", "Ilya Sutskever", "Oriol Vinyals"], "venue": "arXiv preprint arXiv:1409.2329,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2014}, {"title": "Dropout improves recurrent neural networks for handwriting recognition", "author": ["Vu Pham", "Th\u00e9odore Bluche", "Christopher Kermorvant", "J\u00e9r\u00f4me Louradour"], "venue": "arXiv preprint arXiv:1312.4569,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2013}, {"title": "On fast dropout and its applicability to recurrent networks", "author": ["Justin Bayer", "Christian Osendorfer", "Daniela Korhammer", "Nutan Chen", "Sebastian Urban", "Patrick van der Smagt"], "venue": "arXiv preprint arXiv:1311.0701,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2013}, {"title": "Learning generative models with visual attention", "author": ["Yichuan Tang", "Nitish Srivastava", "Ruslan R Salakhutdinov"], "venue": "Advances in Neural Information Processing Systems", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2014}, {"title": "The ibm 2015 english conversational telephone speech recognition system", "author": ["George Saon", "Hong-Kwang J Kuo", "Steven Rennie", "Michael Picheny"], "venue": "arXiv preprint arXiv:1505.05899,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2015}, {"title": "Sequence to sequence learning with neural networks", "author": ["Ilya Sutskever", "Oriol Vinyals", "Quoc VV Le"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2014}, {"title": "Learning to execute", "author": ["Wojciech Zaremba", "Ilya Sutskever"], "venue": "arXiv preprint arXiv:1410.4615,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2014}, {"title": "Gated feedback recurrent neural networks", "author": ["Junyoung Chung", "Caglar Gulcehre", "Kyunghyun Cho", "Yoshua Bengio"], "venue": "arXiv preprint arXiv:1502.02367,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2015}, {"title": "Grammar as a foreign language", "author": ["Oriol Vinyals", "Lukasz Kaiser", "Terry Koo", "Slav Petrov", "Ilya Sutskever", "Geoffrey Hinton"], "venue": "arXiv preprint arXiv:1412.7449,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2014}, {"title": "Transition-based dependency parsing with stack long short-term memory", "author": ["Chris Dyer", "Miguel Ballesteros", "Wang Ling", "Austin Matthews", "Noah A. Smith"], "venue": "CoRR, abs/1505.08075,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2015}, {"title": "Recursive deep models for semantic compositionality over a sentiment treebank", "author": ["Richard Socher", "Alex Perelygin", "Jean Y Wu", "Jason Chuang", "Christopher D Manning", "Andrew Y Ng", "Christopher Potts"], "venue": "In Proceedings of the conference on empirical methods in natural language processing (EMNLP),", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2013}, {"title": "Semeval-2014 task 1: Evaluation of compositional distributional semantic models on full sentences through semantic relatedness and textual entailment", "author": ["Marco Marelli", "Luisa Bentivogli", "Marco Baroni", "Raffaella Bernardi", "Stefano Menini", "Roberto Zamparelli"], "venue": null, "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2014}, {"title": "Towards AI-complete question answering: A set of prerequisite toy", "author": ["Jason Weston", "Antoine Bordes", "Sumit Chopra", "Tomas Mikolov"], "venue": "tasks. CoRR,", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2015}, {"title": "Long short-term memory", "author": ["Sepp Hochreiter", "J\u00fcrgen Schmidhuber"], "venue": "Neural computation,", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 1997}, {"title": "On the difficulty of training recurrent neural networks", "author": ["Razvan Pascanu", "Tomas Mikolov", "Yoshua Bengio"], "venue": "arXiv preprint arXiv:1211.5063,", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2012}, {"title": "LSTM: A search space odyssey", "author": ["Klaus Greff", "Rupesh Kumar Srivastava", "Jan Koutn\u0131\u0301k", "Bas R. Steunebrink", "J\u00fcrgen Schmidhuber"], "venue": "CoRR, abs/1503.04069,", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2015}, {"title": "Framewise phoneme classification with bidirectional lstm and other neural network architectures", "author": ["Alex Graves", "J\u00fcrgen Schmidhuber"], "venue": "Neural Networks,", "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2005}, {"title": "Adadelta: an adaptive learning rate method", "author": ["Matthew D Zeiler"], "venue": "arXiv preprint arXiv:1212.5701,", "citeRegEx": "32", "shortCiteRegEx": "32", "year": 2012}, {"title": "diagrams: Viewing connection strengths in neural networks", "author": ["Frederick J Bremner", "Stephen J Gotts", "Dina L Denham. Hinton"], "venue": "Behavior Research Methods, Instruments, & Computers,", "citeRegEx": "33", "shortCiteRegEx": "33", "year": 1994}], "referenceMentions": [{"referenceID": 0, "context": "1 Introduction Attention-based recurrent neural networks (RNN) have shown great success in a wide range of tasks such as computer vision [1, 2, 3],image generation [4, 5], machine translation [6], speech recognition [7], or even as controllers for memory addressing and retrieval [8, 9].", "startOffset": 137, "endOffset": 146}, {"referenceID": 1, "context": "1 Introduction Attention-based recurrent neural networks (RNN) have shown great success in a wide range of tasks such as computer vision [1, 2, 3],image generation [4, 5], machine translation [6], speech recognition [7], or even as controllers for memory addressing and retrieval [8, 9].", "startOffset": 137, "endOffset": 146}, {"referenceID": 2, "context": "1 Introduction Attention-based recurrent neural networks (RNN) have shown great success in a wide range of tasks such as computer vision [1, 2, 3],image generation [4, 5], machine translation [6], speech recognition [7], or even as controllers for memory addressing and retrieval [8, 9].", "startOffset": 137, "endOffset": 146}, {"referenceID": 3, "context": "1 Introduction Attention-based recurrent neural networks (RNN) have shown great success in a wide range of tasks such as computer vision [1, 2, 3],image generation [4, 5], machine translation [6], speech recognition [7], or even as controllers for memory addressing and retrieval [8, 9].", "startOffset": 164, "endOffset": 170}, {"referenceID": 4, "context": "1 Introduction Attention-based recurrent neural networks (RNN) have shown great success in a wide range of tasks such as computer vision [1, 2, 3],image generation [4, 5], machine translation [6], speech recognition [7], or even as controllers for memory addressing and retrieval [8, 9].", "startOffset": 164, "endOffset": 170}, {"referenceID": 5, "context": "1 Introduction Attention-based recurrent neural networks (RNN) have shown great success in a wide range of tasks such as computer vision [1, 2, 3],image generation [4, 5], machine translation [6], speech recognition [7], or even as controllers for memory addressing and retrieval [8, 9].", "startOffset": 192, "endOffset": 195}, {"referenceID": 6, "context": "1 Introduction Attention-based recurrent neural networks (RNN) have shown great success in a wide range of tasks such as computer vision [1, 2, 3],image generation [4, 5], machine translation [6], speech recognition [7], or even as controllers for memory addressing and retrieval [8, 9].", "startOffset": 216, "endOffset": 219}, {"referenceID": 7, "context": "1 Introduction Attention-based recurrent neural networks (RNN) have shown great success in a wide range of tasks such as computer vision [1, 2, 3],image generation [4, 5], machine translation [6], speech recognition [7], or even as controllers for memory addressing and retrieval [8, 9].", "startOffset": 280, "endOffset": 286}, {"referenceID": 8, "context": "1 Introduction Attention-based recurrent neural networks (RNN) have shown great success in a wide range of tasks such as computer vision [1, 2, 3],image generation [4, 5], machine translation [6], speech recognition [7], or even as controllers for memory addressing and retrieval [8, 9].", "startOffset": 280, "endOffset": 286}, {"referenceID": 1, "context": "While there is debate as to how biologically plausible these cognition models are, they are desirable in their ability to allow introspection into the network\u2019s workings and understanding failures: in the case of image captioning [2, 1] and generation [4, 5], or emotion detection [10], the system\u2019s focus matches up with human intuition.", "startOffset": 230, "endOffset": 236}, {"referenceID": 0, "context": "While there is debate as to how biologically plausible these cognition models are, they are desirable in their ability to allow introspection into the network\u2019s workings and understanding failures: in the case of image captioning [2, 1] and generation [4, 5], or emotion detection [10], the system\u2019s focus matches up with human intuition.", "startOffset": 230, "endOffset": 236}, {"referenceID": 3, "context": "While there is debate as to how biologically plausible these cognition models are, they are desirable in their ability to allow introspection into the network\u2019s workings and understanding failures: in the case of image captioning [2, 1] and generation [4, 5], or emotion detection [10], the system\u2019s focus matches up with human intuition.", "startOffset": 252, "endOffset": 258}, {"referenceID": 4, "context": "While there is debate as to how biologically plausible these cognition models are, they are desirable in their ability to allow introspection into the network\u2019s workings and understanding failures: in the case of image captioning [2, 1] and generation [4, 5], or emotion detection [10], the system\u2019s focus matches up with human intuition.", "startOffset": 252, "endOffset": 258}, {"referenceID": 9, "context": "While there is debate as to how biologically plausible these cognition models are, they are desirable in their ability to allow introspection into the network\u2019s workings and understanding failures: in the case of image captioning [2, 1] and generation [4, 5], or emotion detection [10], the system\u2019s focus matches up with human intuition.", "startOffset": 281, "endOffset": 285}, {"referenceID": 10, "context": "1 RNN Regularization RNN regularization has recently been shown to be achievable using Dropout [11] by regularizing a subset of the recurrent connections in deep RNNs [12, 13].", "startOffset": 95, "endOffset": 99}, {"referenceID": 11, "context": "1 RNN Regularization RNN regularization has recently been shown to be achievable using Dropout [11] by regularizing a subset of the recurrent connections in deep RNNs [12, 13].", "startOffset": 167, "endOffset": 175}, {"referenceID": 12, "context": "1 RNN Regularization RNN regularization has recently been shown to be achievable using Dropout [11] by regularizing a subset of the recurrent connections in deep RNNs [12, 13].", "startOffset": 167, "endOffset": 175}, {"referenceID": 4, "context": "Previously, it was shown that weight decay regularization only provided small improvement [5] and dropout noise was detrimental when applied to all connections due to the compounding of errors over time [14].", "startOffset": 90, "endOffset": 93}, {"referenceID": 13, "context": "Previously, it was shown that weight decay regularization only provided small improvement [5] and dropout noise was detrimental when applied to all connections due to the compounding of errors over time [14].", "startOffset": 203, "endOffset": 207}, {"referenceID": 3, "context": "Examples of this include work on capturing visual structure through a sequence of glimpses through images [4, 15, 3, 1, 2, 10], and networks that learn how to attend to and control a separate memory [8, 16, 17].", "startOffset": 106, "endOffset": 126}, {"referenceID": 14, "context": "Examples of this include work on capturing visual structure through a sequence of glimpses through images [4, 15, 3, 1, 2, 10], and networks that learn how to attend to and control a separate memory [8, 16, 17].", "startOffset": 106, "endOffset": 126}, {"referenceID": 2, "context": "Examples of this include work on capturing visual structure through a sequence of glimpses through images [4, 15, 3, 1, 2, 10], and networks that learn how to attend to and control a separate memory [8, 16, 17].", "startOffset": 106, "endOffset": 126}, {"referenceID": 0, "context": "Examples of this include work on capturing visual structure through a sequence of glimpses through images [4, 15, 3, 1, 2, 10], and networks that learn how to attend to and control a separate memory [8, 16, 17].", "startOffset": 106, "endOffset": 126}, {"referenceID": 1, "context": "Examples of this include work on capturing visual structure through a sequence of glimpses through images [4, 15, 3, 1, 2, 10], and networks that learn how to attend to and control a separate memory [8, 16, 17].", "startOffset": 106, "endOffset": 126}, {"referenceID": 9, "context": "Examples of this include work on capturing visual structure through a sequence of glimpses through images [4, 15, 3, 1, 2, 10], and networks that learn how to attend to and control a separate memory [8, 16, 17].", "startOffset": 106, "endOffset": 126}, {"referenceID": 7, "context": "Examples of this include work on capturing visual structure through a sequence of glimpses through images [4, 15, 3, 1, 2, 10], and networks that learn how to attend to and control a separate memory [8, 16, 17].", "startOffset": 199, "endOffset": 210}, {"referenceID": 0, "context": "In certain cases the models are trained with supervision on the gates [1, 16], however in many cases there is no supervised data for the attentional component.", "startOffset": 70, "endOffset": 77}, {"referenceID": 9, "context": "Several surrogate objectives have been suggested for learning where to focus, including setting a prior on observation spacing that makes a tradeoff between exploration and exploitation [10], using reinforcement learning [9] to optimize a visual tracking strategy [3], or leaving this part semi-supervised through the primary objective.", "startOffset": 186, "endOffset": 190}, {"referenceID": 8, "context": "Several surrogate objectives have been suggested for learning where to focus, including setting a prior on observation spacing that makes a tradeoff between exploration and exploitation [10], using reinforcement learning [9] to optimize a visual tracking strategy [3], or leaving this part semi-supervised through the primary objective.", "startOffset": 221, "endOffset": 224}, {"referenceID": 2, "context": "Several surrogate objectives have been suggested for learning where to focus, including setting a prior on observation spacing that makes a tradeoff between exploration and exploitation [10], using reinforcement learning [9] to optimize a visual tracking strategy [3], or leaving this part semi-supervised through the primary objective.", "startOffset": 264, "endOffset": 267}, {"referenceID": 9, "context": "Our work resembles the observation prior of [10], where we favor input gates being closed and penalize deviation with a penalty of our choosing.", "startOffset": 44, "endOffset": 48}, {"referenceID": 15, "context": "Similarly to the annealed Dropout from [18], we also consider a gradual increase in the sparsity penalty during training to encourage early exploration.", "startOffset": 39, "endOffset": 43}, {"referenceID": 4, "context": "3 Problem Statement A powerful family of models, often called Encoder-Decoders, have opened many new possibilities for sequence classification [5, 8, 19], including executing Python programs [20, 21], drawing pictures [4], machine translation, or syntactic parsing [22, 23].", "startOffset": 143, "endOffset": 153}, {"referenceID": 7, "context": "3 Problem Statement A powerful family of models, often called Encoder-Decoders, have opened many new possibilities for sequence classification [5, 8, 19], including executing Python programs [20, 21], drawing pictures [4], machine translation, or syntactic parsing [22, 23].", "startOffset": 143, "endOffset": 153}, {"referenceID": 16, "context": "3 Problem Statement A powerful family of models, often called Encoder-Decoders, have opened many new possibilities for sequence classification [5, 8, 19], including executing Python programs [20, 21], drawing pictures [4], machine translation, or syntactic parsing [22, 23].", "startOffset": 143, "endOffset": 153}, {"referenceID": 17, "context": "3 Problem Statement A powerful family of models, often called Encoder-Decoders, have opened many new possibilities for sequence classification [5, 8, 19], including executing Python programs [20, 21], drawing pictures [4], machine translation, or syntactic parsing [22, 23].", "startOffset": 191, "endOffset": 199}, {"referenceID": 18, "context": "3 Problem Statement A powerful family of models, often called Encoder-Decoders, have opened many new possibilities for sequence classification [5, 8, 19], including executing Python programs [20, 21], drawing pictures [4], machine translation, or syntactic parsing [22, 23].", "startOffset": 191, "endOffset": 199}, {"referenceID": 3, "context": "3 Problem Statement A powerful family of models, often called Encoder-Decoders, have opened many new possibilities for sequence classification [5, 8, 19], including executing Python programs [20, 21], drawing pictures [4], machine translation, or syntactic parsing [22, 23].", "startOffset": 218, "endOffset": 221}, {"referenceID": 19, "context": "3 Problem Statement A powerful family of models, often called Encoder-Decoders, have opened many new possibilities for sequence classification [5, 8, 19], including executing Python programs [20, 21], drawing pictures [4], machine translation, or syntactic parsing [22, 23].", "startOffset": 265, "endOffset": 273}, {"referenceID": 20, "context": "3 Problem Statement A powerful family of models, often called Encoder-Decoders, have opened many new possibilities for sequence classification [5, 8, 19], including executing Python programs [20, 21], drawing pictures [4], machine translation, or syntactic parsing [22, 23].", "startOffset": 265, "endOffset": 273}, {"referenceID": 21, "context": "Here we consider predicting the sentiment expressed in the Stanford Sentiment Treebank (SST) [24], a collection of 11,855 sentences extracted from movie reviews.", "startOffset": 93, "endOffset": 97}, {"referenceID": 22, "context": "Here we focus on paraphrase detection on the SemEval 2014 shared task 1 dataset [25] which includes 9927 sentence pairs in a 4500/500/4927 train/dev/test split.", "startOffset": 80, "endOffset": 84}, {"referenceID": 0, "context": "Each sentence is annotated with a score c \u2208 [1, 5], with 5 indicating the pair is a paraphrase, and 1 that the pair is unrelated.", "startOffset": 44, "endOffset": 50}, {"referenceID": 4, "context": "Each sentence is annotated with a score c \u2208 [1, 5], with 5 indicating the pair is a paraphrase, and 1 that the pair is unrelated.", "startOffset": 44, "endOffset": 50}, {"referenceID": 23, "context": "3 Question Answering Facebook AI Research recently proposed a set of 20 tasks designed to be \u201cprerequisites\u201d for any system \u201ccapable of conversing with human\u201d [27].", "startOffset": 159, "endOffset": 163}, {"referenceID": 24, "context": "1 Gated LSTMs In our work we make extensive use of Long-Short Term Memory networks [28], a popular RNN architecture specifically designed to capture long range dependencies and alleviate training difficulties [29].", "startOffset": 83, "endOffset": 87}, {"referenceID": 25, "context": "1 Gated LSTMs In our work we make extensive use of Long-Short Term Memory networks [28], a popular RNN architecture specifically designed to capture long range dependencies and alleviate training difficulties [29].", "startOffset": 209, "endOffset": 213}, {"referenceID": 26, "context": "Since their introduction in 1995, many variants have been proposed [30], however for the purposes of this research we found that the vanilla version from [30] worked best.", "startOffset": 67, "endOffset": 71}, {"referenceID": 26, "context": "Since their introduction in 1995, many variants have been proposed [30], however for the purposes of this research we found that the vanilla version from [30] worked best.", "startOffset": 154, "endOffset": 158}, {"referenceID": 27, "context": "Additionally, we consider Gated Stacked LSTMs, a variant of Stacked LSTMs [31, 5, 20], where the input the lowest LSTM is gated using the hidden state from the topmost LSTM of the previous timestep.", "startOffset": 74, "endOffset": 85}, {"referenceID": 4, "context": "Additionally, we consider Gated Stacked LSTMs, a variant of Stacked LSTMs [31, 5, 20], where the input the lowest LSTM is gated using the hidden state from the topmost LSTM of the previous timestep.", "startOffset": 74, "endOffset": 85}, {"referenceID": 17, "context": "Additionally, we consider Gated Stacked LSTMs, a variant of Stacked LSTMs [31, 5, 20], where the input the lowest LSTM is gated using the hidden state from the topmost LSTM of the previous timestep.", "startOffset": 74, "endOffset": 85}, {"referenceID": 10, "context": "We train 3 different models with hidden sizes 25, 50, 150, and apply Dropout [11, 12] with probability p = 0.", "startOffset": 77, "endOffset": 85}, {"referenceID": 11, "context": "We train 3 different models with hidden sizes 25, 50, 150, and apply Dropout [11, 12] with probability p = 0.", "startOffset": 77, "endOffset": 85}, {"referenceID": 28, "context": "All models are trained using Adadelta [32] with \u03c1 = 0.", "startOffset": 38, "endOffset": 42}, {"referenceID": 4, "context": "The HG-LSTM takes a question, followed by the sequence of facts, and the final hidden state of the HG-LSTM is fed as input to an LSTM decoder that produces the answer sequentially and ends its prediction with an <EOS> symbol [5, 22].", "startOffset": 225, "endOffset": 232}, {"referenceID": 19, "context": "The HG-LSTM takes a question, followed by the sequence of facts, and the final hidden state of the HG-LSTM is fed as input to an LSTM decoder that produces the answer sequentially and ends its prediction with an <EOS> symbol [5, 22].", "startOffset": 225, "endOffset": 232}, {"referenceID": 23, "context": "We use the first 1000 examples for training as suggested in [27], and reserve 20% for validation.", "startOffset": 60, "endOffset": 64}, {"referenceID": 28, "context": "Our model is trained using AdaDelta [32], with \u03c1 = 0.", "startOffset": 36, "endOffset": 40}, {"referenceID": 23, "context": "Finally, we observed that the HG-LSTM model significantly improves performance over the LSTM baseline from [27].", "startOffset": 107, "endOffset": 111}, {"referenceID": 29, "context": "g Hinton Diagrams [33].", "startOffset": 18, "endOffset": 22}, {"referenceID": 23, "context": "Table 2: Comparison of test accuracy on bAbI dataset from [27] with different models.", "startOffset": 58, "endOffset": 62}, {"referenceID": 23, "context": "Models are (left to right): LSTM baseline from [27], followed HG-LSTM with: no penalty, word sparsity penalty only, fact selection penalty only and both.", "startOffset": 47, "endOffset": 51}], "year": 2015, "abstractText": "We present a complimentary objective for training recurrent neural networks (RNN) with gating units that helps with regularization and interpretability of the trained model. Attention-based RNN models have shown success in many difficult sequence to sequence classification problems with long and short term dependencies, however these models are prone to overfitting. In this paper, we describe how to regularize these models through an L1 penalty on the activation of the gating units, and show that this technique reduces overfitting on a variety of tasks while also providing to us a human-interpretable visualization of the inputs used by the network. These tasks include sentiment analysis, paraphrase recognition, and question answering.", "creator": "LaTeX with hyperref package"}}}