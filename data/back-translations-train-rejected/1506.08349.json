{"id": "1506.08349", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "28-Jun-2015", "title": "Improved Deep Speaker Feature Learning for Text-Dependent Speaker Recognition", "abstract": "A deep learning approach has been proposed recently to derive speaker identifies (d-vector) by a deep neural network (DNN). This approach has been applied to text-dependent speaker recognition tasks and shows reasonable performance gains when combined with the conventional i-vector approach. Although promising, the existing d-vector implementation still can not compete with the i-vector baseline. This paper presents two improvements for the deep learning approach: a phonedependent DNN structure to normalize phone variation, and a new scoring approach based on dynamic time warping (DTW). Experiments on a text-dependent speaker recognition task demonstrated that the proposed methods can provide considerable performance improvement over the existing d-vector implementation.", "histories": [["v1", "Sun, 28 Jun 2015 03:32:02 GMT  (1438kb,D)", "http://arxiv.org/abs/1506.08349v1", "arXiv admin note: substantial text overlap witharXiv:1505.06427"]], "COMMENTS": "arXiv admin note: substantial text overlap witharXiv:1505.06427", "reviews": [], "SUBJECTS": "cs.CL cs.LG cs.NE", "authors": ["lantian li", "yiye lin", "zhiyong zhang", "dong wang"], "accepted": false, "id": "1506.08349"}, "pdf": {"name": "1506.08349.pdf", "metadata": {"source": "CRF", "title": "Improved Deep Speaker Feature Learning for Text-Dependent Speaker Recognition", "authors": ["Lantian Li", "Yiye Lin", "Zhiyong Zhang", "Dong Wang"], "emails": ["lilt@cslt.riit.tsinghua.edu.cn;", "lyy@cslt.riit.tsinghua.edu.cn;", "zhangzy@cslt.riit.tsinghua.edu.cn;", "wangdong99@mails.tsinghua.edu.cn"], "sections": [{"heading": null, "text": "This year is the highest in the history of the country."}, {"heading": "II. RELATED WORK", "text": "This paper follows the work in [8] and offers several extensions. In particular, the loudspeaker identity in [8] is represented by a d-vector derived from average pooling, which is quite neat and efficient, but loses a lot of information about the test signal, such as the distribution property and time constraint. One of the most important contributions of this paper is the study of how the time constraint can be used in the DNN-based approach. DNN model has been studied in several respects in the field of loudspeaker recognition, for example, in [10] ASR-trained DNNs have been used to replace the UBM model to derive the acoustic statistics for i-vector models. In [11] a DNN has been used to improve the discriminatory capability of i-vectors, all of which are based on the generative framework, i.e. the i-vector model."}, {"heading": "III. DNN-BASED FEATURE LEARNING", "text": "It is well known that DNNs can learn task-oriented traits from raw input layers after layers. [7] This trait has been used in ASR, where phone discrimination traits are learned from very low levels such as fbanks or even spectra. [7] It has been shown that with a well-trained DNN, variations that are irrelevant to the learning task can be gradually eliminated as the trait spreads through the DNN structure layer after layer. This trait learning is so powerful that in ASR the primary fbank function, which was carefully designed by humans and has been dominated in ASR for several decades, can be defeated. This trait can also be applied to learn talking traits. In fact, researchers have put a lot of effort into finding traits that are more discriminatory for speakers [6], but the efforts are mostly futile and the MFCC is still the most popular choice. The success of DNNs in ASR suggests a new direction that the speaker characteristics can be learned from the speaker discrimination characteristics."}, {"heading": "A. Comparison between i-vectors and d-vectors", "text": "The two types of speaker vectors, the d-vector and the ivector, are fundamentally different. I-vectors are based on a linear Gaussian model in which learning is unattended and the learning criterion is the maximum probability of acoustic characteristics; in contrast, d-vectors are based on neural networks for which learning is supervised, and the learning criterion is the maximum discrimination for speakers. This difference leads to several advantages for d-vectors: first, it is a \"discriminating\" vector that represents speakers by removing speakers of irrelevant variance and is so sensitive to speakers and invariant to other disturbances; second, it is a \"local\" speaker description that uses only the local context, so that it can be inferred from very short utterances; third, it is based on \"universal\" data to learn the DNN model, which allows learning from large amounts of time data that are task-independent."}, {"heading": "A. Phone-dependent training", "text": "One potential problem with the DNN-based feature learning described in the previous section is that it is \"blind learning,\" i.e. the features are learned without prior knowledge from raw data. This means that learning relies solely on the complex depth structure of the DNN model and a large amount of data to detect discriminatory speaker patterns. If the training data is abundant, this is often not a problem; for tasks with limited amounts of data, such as the text-dependent task in our hands, this blind learning tends to be difficult because there are too many speaker-independent variations in the raw data, especially phone content. One possible solution is to provide the DNN model with additional information about which phone is spoken in each frame. This can be achieved simply by adding a telephone indicator in the DNN input."}, {"heading": "B. Segment pooling and dynamic time warping", "text": "Text-dependent loudspeaker recognition is essentially a sequential pattern matching problem, but the current d-vector approach derives loudspeaker identities as individual vectors through average pooling and then formulates loudspeaker recognition as vector matching. This is certainly not ideal, since the time limitation in deriving the loudspeaker vector is completely ignored. A possible solution is to segment an enrollment / test expression into several parts and derive the loudspeaker vector vector vector for each piece from it. The loudspeaker identity of the utterance is then represented by the sequence of piecewise loudspeaker vectors, and the loudspeaker matching occurs by matching the corresponding vector sequences. This paper adopts a simple sequence approach vector matching approach: The two sequences are assumed to be identical in length, and the matching process is carried out piece by piece independently of the loudspeaker vectors. Finally, the piecewise sequencing method takes the point dynamics for all parts because the point matching method is vector vectors."}, {"heading": "V. EXPERIMENTS", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "A. Database", "text": "The experiments are conducted on a database containing a limited number of short phrases. The entire database contains recordings of 10 short phrases from 100 speakers (gender specific), and each phrase contains 2 \u0445 5 Chinese characters. For each speaker, one phrase is recorded 15 times, which is 150 utterances per speaker. The training set includes 80 randomly selected speakers, resulting in a total of 12,000 utterances. To prevent overmatching, a cross-validation set (CV) is selected with 1000 utterances from the training data, and the remaining 11000 utterances are used for model training, including the DNN model in the d-vector approach and the UBM, the T-matrix, the LDA and PLDA model in the i-vector approach. The evaluation set consists of the remaining 20 speakers. The evaluation is done for each phrase. For each phrase, there are 44850 tracks, including 2100 target trails, and 42750 non-target studies to put the short phrases in order."}, {"heading": "B. Baseline", "text": "The acoustic characteristics of the i-vector system are 39-dimensional MFCCs consisting of 13 static components (including C0) and the first and second derivatives; the number of Gaussian components of the UBM is 128; and the dimension of the i-vector is 200; the d-vector baseline uses the DNN structure shown in Fig. 1; the average pooling is used to derive d-vectors; the acoustic characteristics are 40-dimensional fbanks, with the left and right 10 frames interconnected; the characteristics of the frame plane are extracted from the last hidden layer, and the dimension is 200.Table I presents the results with respect to the same error rate (EER); it is evident that the i-vector system significantly exceeds the d-vector system."}, {"heading": "C. phone-dependent learning", "text": "In this experiment, the telephone posteriors are included in the DNN input, as in Fig. 2. The telephone posteriors are generated by a DNN model that was trained for ASR using a Chinese database with 6000 hours of voice data. The telephone set consists of 66 soundless beginning and ending in Chinese as well as the silent telephone. The results are presented in the second line of Table II, which is called \"DNN + PT.\" It can be seen that the telephone-dependent training leads to a marginal but consistent improvement in the performance of the d-vector system."}, {"heading": "D. Segment pooling and DTW", "text": "As shown in Section IV, the segment pooling approach segments an input expression into n pieces and derives a vector for each piece. It is evaluated independently on the basis of the individual d-vectors and the average of the results of these pieces is used as a meaningful value. Results are shown in Fig. 3, where seg-n means that each utterance is segmented into n pieces. It is evident that segment pooling provides a clear performance improvement. To provide a clearer comparison, the EER results are presented with 3 segments in Table II, which is referred to as \"DNN + PT + SEG.\" It is clear that a significant performance is achieved through segmentation. DTW results are presented in the fourth series of Table II, which is referred to as \"DNN + PT + DTW.\" DTW generally exceeds segment pooling."}, {"heading": "E. System combination", "text": "In the following [8] we combine the best i-vector system (PLDA) and the best d-vector system (DNN + PT + DTW) by simply interpolating the values from the two systems: \u03b1siv + (1 \u2212 \u03b1) sdv, where siv and sdv are values from the i-vector and d-vector system, respectively, and \u03b1 is the interpolation factor. The EER results with the optimal \u03b1 are shown in Table III."}, {"heading": "VI. CONCLUSIONS", "text": "This paper presented several improvements to the DNN-based feature learning approach to speaker recognition: We presented a telephone-dependent DNN model for providing phonetic information when learning speaker functions, and proposed two scoring methods based on segment pooling or DTW to take advantage of time constraints, which significantly improved the performance of the d-vector system."}], "references": [{"title": "Speaker verification using adapted gaussian mixture models", "author": ["D. Reynolds", "T. Quatieri", "R. Dunn"], "venue": "Digital Signal Processing, vol. 10, no. 1, pp. 19\u201341, 2000.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2000}, {"title": "Joint factor analysis versus eigenchannels in speaker recognition", "author": ["P. Kenny", "G. Boulianne", "P. Ouellet", "P. Dumouchel"], "venue": "IEEE Transactions on Audio, Speech, and Language Processing, vol. 15, pp. 1435\u20131447, 2007.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2007}, {"title": "Speaker and session variability in gmm-based speaker verification", "author": ["\u2014\u2014"], "venue": "IEEE Transactions on Audio, Speech, and Language Processing, vol. 15, pp. 1448\u20131460, 2007.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2007}, {"title": "Support vector machines using gmm supervectors for speaker verification", "author": ["W. Campbell", "D. Sturim", "D. Reynolds"], "venue": "Signal Processing Letters, IEEE, vol. 13, no. 5, pp. 308\u2013311, 2006.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2006}, {"title": "Probabilistic linear discriminant analysis", "author": ["S. Ioffe"], "venue": "Computer Vision ECCV 2006, Springer Berlin Heidelberg, pp. 531\u2013542, 2006.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2006}, {"title": "An overview of text-independent speaker recognition: From features to supervectors", "author": ["T. Kinnunen", "H. Li"], "venue": "Speech communication, vol. 52, no. 1, pp. 12\u201340, 2010.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2010}, {"title": "Improving wideband speech recognition using mixed-bandwidth training data in cd-dnn-hmm", "author": ["J. Li", "D. Yu", "J. Huang", "Y. Gong"], "venue": "SLT, pp. 131\u2013136, 2012.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2012}, {"title": "Deep neural networks for small footprint text-dependent speaker verification", "author": ["V. Ehsan", "L. Xin", "M. Erik", "L.M. Ignacio", "G.-D. Javier"], "venue": "IEEE International Conference on Acoustic, Speech and Signal Processing (ICASSP), vol. 28, no. 4, pp. 357\u2013366, 2014.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2014}, {"title": "Using dynamic time warping to find patterns in time series", "author": ["D. Berndt", "J. Clifford"], "venue": "KDD workshop, vol. 10, no. 16, pp. 359\u2013370, 1994.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 1994}, {"title": "Deep neural networks for extracting baum-welch statistics for speaker recognition", "author": ["P. Kenny", "V. Gupta", "T. Stafylakis", "P. Ouellet", "J. Alam"], "venue": "Odyssey, 2014.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2014}, {"title": "Discriminative scoring for speaker recognition based on i-vectors", "author": ["J. Wang", "D. Wang", "Z.-W. Zhu", "T. Zheng", "F. Song"], "venue": "APSIPA, 2014.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2014}], "referenceMentions": [{"referenceID": 0, "context": "For example, in the famous Gaussian mixture model-universal background model (GMMUBM) framework [1], the acoustic space is divided into subspaces in the form of Gaussian components, and each subspace roughly represents a phone.", "startOffset": 96, "endOffset": 99}, {"referenceID": 1, "context": "This idea is shared by many advanced techniques derived from GMM-UBM, including the joint factor analysis (JFA) [2] and the i-vector model [3].", "startOffset": 112, "endOffset": 115}, {"referenceID": 2, "context": "This idea is shared by many advanced techniques derived from GMM-UBM, including the joint factor analysis (JFA) [2] and the i-vector model [3].", "startOffset": 139, "endOffset": 142}, {"referenceID": 3, "context": "For example, the SVM approach for GMM-UBMs [4] and the PLDA approach for i-vectors [5].", "startOffset": 43, "endOffset": 46}, {"referenceID": 4, "context": "For example, the SVM approach for GMM-UBMs [4] and the PLDA approach for i-vectors [5].", "startOffset": 83, "endOffset": 86}, {"referenceID": 5, "context": ", features that are more discriminative for speaker recognition [6].", "startOffset": 64, "endOffset": 67}, {"referenceID": 6, "context": "This learned features are very powerful and have defeated the MFCC that has dominated in ASR for several decades [7].", "startOffset": 113, "endOffset": 116}, {"referenceID": 7, "context": "A recent study shows that this is possible at least on text-dependent tasks [8].", "startOffset": 76, "endOffset": 79}, {"referenceID": 7, "context": "This paper follows the work in [8] and provides two enhancements for the DNN-based feature learning: First, phone posteriors are involved in the DNN input so that speakerdiscriminative features can be learned easier by alleviating the impact of phone variation; second, two scoring methods that consider the temporal constraint are proposed: segmentation pooling and dynamic time warping (DTW) [9].", "startOffset": 31, "endOffset": 34}, {"referenceID": 8, "context": "This paper follows the work in [8] and provides two enhancements for the DNN-based feature learning: First, phone posteriors are involved in the DNN input so that speakerdiscriminative features can be learned easier by alleviating the impact of phone variation; second, two scoring methods that consider the temporal constraint are proposed: segmentation pooling and dynamic time warping (DTW) [9].", "startOffset": 394, "endOffset": 397}, {"referenceID": 7, "context": "This paper follows the work in [8] and provides several extensions.", "startOffset": 31, "endOffset": 34}, {"referenceID": 7, "context": "Particularly, the speaker identity in [8] is represented by a d-vector derived by average pooling, which is quite neat and efficient, but loses much information of the test signal, such as the distributional property and the temporal constraint.", "startOffset": 38, "endOffset": 41}, {"referenceID": 9, "context": "For example, in [10], DNNs trained for ASR were used to replace the UBM model to derive the acoustic statistics for i-vector models.", "startOffset": 16, "endOffset": 20}, {"referenceID": 10, "context": "In [11], a DNN was used to replace PLDA to improve discriminative capability of i-vectors.", "startOffset": 3, "endOffset": 7}, {"referenceID": 6, "context": "This property has been employed in ASR where phone-discriminative features are learned from very low-level features such as Fbanks or even spectra [7].", "startOffset": 147, "endOffset": 150}, {"referenceID": 5, "context": "Actually researchers have put much effort in searching for features that are more discriminative for speakers [6], but the effort is mostly vain and the MFCC is still the most popular choice.", "startOffset": 110, "endOffset": 113}, {"referenceID": 7, "context": "To derive utterance-based representations, an average pooling approach was used in [8], where the framelevel features are averaged and the resultant vector is used to represent the speaker.", "startOffset": 83, "endOffset": 86}, {"referenceID": 7, "context": "This vector is called \u2018d-vector\u2019 in [8], Fbank", "startOffset": 36, "endOffset": 39}, {"referenceID": 0, "context": "A more theoretical treatment is based on dynamic time warping (DTW) [1].", "startOffset": 68, "endOffset": 71}, {"referenceID": 7, "context": "Following [8], we combine the best i-vector system (PLDA) and the best d-vector system (DNN+PT+DTW).", "startOffset": 10, "endOffset": 13}], "year": 2015, "abstractText": "A deep learning approach has been proposed recently to derive speaker identifies (d-vector) by a deep neural network (DNN). This approach has been applied to text-dependent speaker recognition tasks and shows reasonable performance gains when combined with the conventional i-vector approach. Although promising, the existing d-vector implementation still can not compete with the i-vector baseline. This paper presents two improvements for the deep learning approach: a phonedependent DNN structure to normalize phone variation, and a new scoring approach based on dynamic time warping (DTW). Experiments on a text-dependent speaker recognition task demonstrated that the proposed methods can provide considerable performance improvement over the existing d-vector implementation.", "creator": "LaTeX with hyperref package"}}}