{"id": "1506.08350", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "28-Jun-2015", "title": "Stochastic Gradient Made Stable: A Manifold Propagation Approach for Large-Scale Optimization", "abstract": "Stochastic gradient descent (SGD) holds as a classical method to build large scale machine learning models over big data. A stochastic gradient is typically calculated from a limited number of samples (known as mini-batch), so it potentially incurs a high variance and causes the estimated parameters bounce around the optimal solution. To improve the stability of stochastic gradient, recent years have witnessed the proposal of several semi-stochastic gradient descent algorithms, which distinguish themselves from standard SGD by incorporating global information into gradient computation. In this paper we contribute a novel stratified semi-stochastic gradient descent (S3GD) algorithm to this nascent research area, accelerating the optimization of a large family of composite convex functions. Though theoretically converging faster, prior semi-stochastic algorithms are found to suffer from high iteration complexity, which makes them even slower than SGD in practice on many datasets. In our proposed S3GD, the semi-stochastic gradient is calculated based on efficient manifold propagation, which can be numerically accomplished by sparse matrix multiplications. This way S3GD is able to generate a highly-accurate estimate of the exact gradient from each mini-batch with largely-reduced computational complexity. Theoretic analysis reveals that the proposed S3GD elegantly balances the geometric algorithmic convergence rate against the space and time complexities during the optimization. The efficacy of S3GD is also experimentally corroborated on several large-scale benchmark datasets.", "histories": [["v1", "Sun, 28 Jun 2015 03:33:38 GMT  (924kb,D)", "http://arxiv.org/abs/1506.08350v1", "12 pages, 7 figures"], ["v2", "Tue, 12 Jan 2016 21:30:08 GMT  (1086kb,D)", "http://arxiv.org/abs/1506.08350v2", "14 pages, 9 figures"]], "COMMENTS": "12 pages, 7 figures", "reviews": [], "SUBJECTS": "cs.LG cs.NA", "authors": ["yadong mu", "wei liu", "wei fan"], "accepted": false, "id": "1506.08350"}, "pdf": {"name": "1506.08350.pdf", "metadata": {"source": "CRF", "title": "Stochastic Gradient Made Stable: A Manifold Propagation Approach for Large-Scale Optimization", "authors": ["Yadong Mu", "Wei Liu", "Wei Fan"], "emails": ["myd@research.att.com", "weiliu@us.ibm.com", "fanwei03@baidu.com"], "sections": [{"heading": null, "text": "Index terms - Large-scale optimization, semi-stochastic gradient descent, multiple propagation. F"}, {"heading": "1 INTRODUCTION", "text": "In fact, it is a matter of a way in which it is a matter of a way in which people move in the most diverse regions of the world. (...) In fact, it is a matter of a way in which they feel able to stand up to themselves and to others. (...) In fact, it is a matter of a way in which they are able to put themselves and themselves at the centre. (...) In fact, it is a matter of a way in which they are able to put themselves and themselves at the centre. (...) It is as if they are able to determine themselves. (...) It is as if they are able to change the world. (...) It is as if they are able to change the world. (...) It is as if they are able to change the world. (...) It is as if they are able to change the world. (...) It is as if they are able to change the world. (...) It is as if they are able to change the world."}, {"heading": "2 THE PROPOSED ALGORITHM", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "2.1 Notations and Assumptions", "text": "The results of the study are: The results of the study show that the number of people who are able, is able to decide whether they are able, are able, are able to learn or are able to solve their problems. The results of the study show that the number of people who are able, is able, is able to solve their problems. The results of the study show that the number of people who are able, is able, is able to identify whether they are able, whether they are able to solve their problems. The results of the study: The number of people who are able to recognize that they are able, is able to identify whether they are able, whether they are able to solve their problems. The results of the study: The number of people who are able to recognize that they are able, that they are able to solve their problems."}, {"heading": "Li respectively. The Lipschitz parameter for their composition", "text": "P (w) is LP \u2264 (1 / n) \u2211 n i = 1 Li. The regularization term R (w) is usually assumed to be indistinguishable and therefore has no lipschitz parameter."}, {"heading": "2.2 Algorithmic Framework", "text": "This year, it is only a matter of time before an agreement is reached."}, {"heading": "2.3 Gradient Approximation by Manifold Propagation", "text": "The main argument is that a universal gradient approach to the data is either impracticable or inaccurate, especially when the loss function P (w) has a large number of conditions LP / \u00b5P. Our proposed approach is abnormally-based gradient approach via data diversity, and the idea has been explored in a different context, but not in a stochastic optimization. For example, it has been shown that any Lipschitz continuous function f (x) can exhibit resistance to low-dimensional manifolds approximated by a linear combination of function values, namelyf (x) = Z (x) f (16), where Z (z) is a collection of pre-specified anchors."}, {"heading": "2.4 Instances of Applications", "text": "This section describes our proposed algorithm based on several representative loss functions and regulations. (> Logistic loss: it applies to real or binary responses. (*) We focus on the binary case in which y \u2212 \u2212 \u2212 \u2212 x (\u2212 x))) (24) 3. To correctly estimate the probability of the class name, we remove the interceptor variable by appending an additional dimension of the constant 1 to each feature vector x.6 The log likelihood function is then expressed as P (w) = 1 (w > xi) = 1 log p (yi | yi | xi). According to the rule of calculation of the sigmoid function, the gradient of the signature (w > xi)."}, {"heading": "2.5 Algorithmic Complexity", "text": "The iteration complexity of the proposed S3GD depends on several variables: the minibatch size p, the number of anchors m, the k-NN parameter in the construction of ASG, the maximum inner loop number kin and the characteristic dimensionality d. The greatest computational effort results from the calculation of H (w) at each outer loop in algorithm 1. Important is that the compact matrix shape in Eqn. (23) largely reduces the complexity of time and space. Most existing semi-stochastic algorithms rely on two nested loops, of which the outer loop requires an exact gradient calculation or covariance matrix estimation. For large data, it entails an enormous O (nd) or O (d2) complexity. For other complex algorithms aiming at improved minibatch construction (such as SSGD [27], the iteration complexity is better than our rule."}, {"heading": "3 CONVERGENCE ANALYSIS", "text": "(P) (P) (P) (P) (P) (P) (P) (P) (P) (P) (P) (P) (P) (P) (P) (P) (P) (P) (P) (P) (P) (P) (P) (P) (P) (P) (P) (P) (P) (P) (P) (P) (P) (P) (P) (P) (P) (P) (P) (P) (P) (P) (P) (P) (P) (P) (P) (P) (P) (P) (P) (P) (P) (P) (P) (P) (P) (P) (P) (P) (P) (P) (P) (P) (P) (P) (P) (P) (P) (P) (P) (P) (P) (P) (P) (P) (P) (P) (P) (P) (P) (P) (P) (P) (P) (P) (P) (P) (P) (P) (P) (P) (P) (P) (P) (P) (P) (P) (P) (P) (P) (P) (P) (P) (P) (P (P) (P) (P) (P) (P) (P) (P) (P) (P) (P) (P) (P) (P) (P) (P) (P) (P) (P) (P) (P) (P) (P (P) (P) (P) (P) () () (P (P) () (P (P) () (P) () (P (P) () () (P (P) (P (P) () () (P () () () (P (P (P) () (P (P) () () () (P (P) () (P (P () (P (P) () (P (P) (P () () () (P (P (P) (P"}, {"heading": "4 EXPERIMENTS", "text": "This section reports on numerical studies between our proposed S3GD and other competing algorithms."}, {"heading": "4.1 Description of Dataset and Applications", "text": "It, it, it, it, it, it, it, it, it, it, it, it, it, \"it,\" it, \"it,\" it, \"it,\" it, \"it,\" it, \"it,\" it, \"it,\" it, \"it,\" it, \"it,\" it, \"it,\" it, \"it,\" it, \"it,\" it, \"it,\" it, \"it,\" it, \"it,\" it, \"it,\" it, \"it,\" it, \"it,\" it, \"it,\" it, \"it,\" it, \"it,\" it, \"it,\" it, \"it,\" it, \"it,\" it, \"it,\" it, \"it,\" it, \"it,\" it, \"it,\" it, \"it,\" it, \"it,\" it, \"it,\" it, \"it,\" it, \"it,\" it, \"it,\" it, \"it,\" it, \"it,\" it, \"it,\" it, \"it,\" it, \"it,\" it, \"it,\" it, \"it,\" it, \"it,\" it, \"it,\" it, \"it,\" it, \"it,\" it, \"it,\" it, \"it,\" it, \"it,\" it, \"it,\" it, \"it,\" it, \"it,\" it, \"it,\" it, \"it,\" it, \"it,\" it, \"it,\" it, \"it,\" it, \"it,\" it, \"it,\" it, \"it,\" it, \"it,\" it, \"it,\" it, \"it,\" it, \"it,\" it, \"it,\" it, \"it,\" it, \"it,\" it, \"it,\" it, \"it,\" it, \"it,\" it, \"it,\" it, \"it,\" it, \"it,\" it, \"it,\" it, \"it,\" it, \"it,\" it, \"it,\" it, \"it,\" it, \"it,\" it, \"it,\" it, \"it,\" it, \"it,\" it, \"it,\""}, {"heading": "4.2 Baseline Algorithms", "text": "We make comparisons between the proposed S3GD and other four competitors, including 11. http: / / www.nist.gov / itl / iad / med11.cfm \u2022 Mini-Batch Stochastic Gradient Descent (SGD): It represents the standard stochastic gradient method. \u2022 At each iteration, the SGD algorithm randomly draws p samples from the training set according to the weight distribution specified in Eqn. (34), calculates their respective stochastic gradient and uniformly forms the average of these stochastic gradients. \u2022 Stratified SGD (SSGD) [27]: This method aims to improve the standard minibatch SGD using data clusters and stratified sampling. SSGD ensures that each iteration draws at least one sample from each data cluster (layer)."}, {"heading": "4.3 Evaluation Settings", "text": "In fact, most of them are able to go in search of a solution worthy of their name, most of them are able to go in search of a solution, most of them are able to go in search of a solution, most of them have set out in search of a solution, most of them have set out in search of a solution, most of them have set out in search of a solution, most of them have set out in search of a solution, most of them have set out in search of a solution, most of them have set out in search of a solution, most of them have set out in search of a solution."}, {"heading": "4.4 Quantitative Investigations", "text": "For all algorithms, the individual parameters are selected based on the criterion in (35). Interestingly, although semi-stochastic gradient methods have been shown to have a faster asymptotic convergence rate, most of them are not as \"economic\" as standard SGD due to the significantly higher iteration complexity. Our proposed S3GD methods outperform all other algorithms in 6 of 9 datasets. SVRG dominates only the small 22-dimensional datasets from IJCNN, and SGD performs best on other datasets KD04 bio and 20newsgroups. SSGD is considered sensitive to unbalanced data partitions, such as MED11 and 20newsgroups, where the positive / negative data ratios are 1. Surprisingly, the standard SGD is among the best."}, {"heading": "5 CONCLUDING REMARKS", "text": "The motivation of the S3GD is to reduce the high iteration complexity in the existing semi-stochastic gradient methods. It uses the stratified gradient constellation as a good cure for the time-consuming, accurate gradation calculation. Our work has significantly advanced the idea of residual minimization correction. 410 40.40.81Optimization IterationC orre lation of (sem i) sto ticg ende xtrac t kagglefaceSGD SSGD SCV S3G10 210 410 500.40.81Optimization IterationC orre lation of (sem i) 5000 IterationC i.80.91Optimization IterationC orre lation of (sem i) sto."}], "references": [{"title": "The proximal average: Basic theory", "author": ["H. Bauschke", "R. Goebel", "Y. Lucet", "X. Wang"], "venue": "SIAM Journal on Optimization, 19(2):766\u2013785", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2008}, {"title": "Stochastic learning", "author": ["L. Bottou"], "venue": "O. Bousquet and U. von Luxburg, editors, Advanced Lectures on Machine Learning. Springer Verlag", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2004}, {"title": "A singular value thresholding algorithm for matrix completion", "author": ["J.-F. Cai", "E.J. Cand\u00e8s", "Z. Shen"], "venue": "SIAM J. on Optimization, 20(4):1956\u20131982", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2010}, {"title": "Training a support vector machine in the primal", "author": ["O. Chapelle"], "venue": "Neural Comput., 19(5):1155\u20131178", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2007}, {"title": "Better minibatch algorithms via accelerated gradient methods", "author": ["A. Cotter", "O. Shamir", "N. Srebro", "K. Sridharan"], "venue": "NIPS", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2011}, {"title": "Optimal distributed online prediction using mini-batches", "author": ["O. Dekel", "R. Gilad-Bachrach", "O. Shamir", "L. Xiao"], "venue": "CoRR, abs/1012.1367", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2010}, {"title": "Deep learning: Methods and applications", "author": ["L. Deng", "D. Yu"], "venue": "Technical Report MSR-TR-2014-21,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2014}, {"title": "Multiclass latent locally linear support vector machines", "author": ["M. Fornoni", "B. Caputo", "F. Orabona"], "venue": "ACML", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2013}, {"title": "Large-scale matrix factorization with distributed stochastic gradient descent", "author": ["R. Gemulla", "E. Nijkamp", "P.J. Haas", "Y. Sismanis"], "venue": "SIGKDD", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2011}, {"title": "Accelerating stochastic gradient descent using predictive variance reduction", "author": ["R. Johnson", "T. Zhang"], "venue": "NIPS", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2013}, {"title": "Semi-stochastic gradient descent methods", "author": ["J. Konecny", "P. Richtarik"], "venue": "CoRR, abs/1312.1666", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2013}, {"title": "Locally linear support vector machines", "author": ["L. Ladicky", "P.H.S. Torr"], "venue": "ICML", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2011}, {"title": "Efficient mini-batch training for stochastic optimization", "author": ["M. Li", "T. Zhang", "Y. Chen", "A.J. Smola"], "venue": "ACM SIGKDD", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2014}, {"title": "Large graph construction for scalable semi-supervised learning", "author": ["W. Liu", "J. He", "S. Chang"], "venue": "ICML", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2010}, {"title": "Constrained stochastic gradient descent for large-scale least squares problem", "author": ["Y. Mu", "W. Ding", "T. Zhou", "D. Tao"], "venue": "ACM SIGKDD", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2013}, {"title": "Introductory Lectures on Convex Optimization: A Basic Course", "author": ["Y. Nesterov"], "venue": "Springer Netherlands", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2004}, {"title": "Minimizing finite sums with the stochastic average gradient", "author": ["M.W. Schmidt", "N.L. Roux", "F. Bach"], "venue": "CoRR, abs/1309.2388", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2013}, {"title": "Accelerated mini-batch stochastic dual coordinate ascent", "author": ["S. Shalev-Shwartz", "T. Zhang"], "venue": "NIPS", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2013}, {"title": "Communication-efficient distributed optimization using an approximate newton-type method", "author": ["O. Shamir", "N. Srebro", "T. Zhang"], "venue": "ICML", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2014}, {"title": "The Nature of Statistical Learning Theory", "author": ["V. Vapnik"], "venue": "Springer-Verlag New York, Inc., New York, NY, USA", "citeRegEx": "20", "shortCiteRegEx": null, "year": 1995}, {"title": "Variance reduction for stochastic gradient optimization", "author": ["C. Wang", "X. Chen", "A.J. Smola", "E.P. Xing"], "venue": "NIPS", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2013}, {"title": "A proximal stochastic gradient method with progressive variance reduction", "author": ["L. Xiao", "T. Zhang"], "venue": "SIAM Journal on Optimization, 24(4):2057\u20132075", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2014}, {"title": "Nonlinear learning using local coordinate coding", "author": ["K. Yu", "T. Zhang", "Y. Gong"], "venue": "NIPS", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2009}, {"title": "NOMAD: nonlocking", "author": ["H. Yun", "H. Yu", "C. Hsieh", "S.V.N. Vishwanathan", "I.S. Dhillon"], "venue": "stochastic multi-machine algorithm for asynchronous and decentralized matrix completion. PVLDB, 7(11):975\u2013986", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2014}, {"title": "Modified logistic regression: An approximation to SVM and its applications in largescale text categorization", "author": ["J. Zhang", "R. Jin", "Y. Yang", "A.G. Hauptmann"], "venue": "ICML", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2003}, {"title": "Text categorization based on regularized linear classification methods", "author": ["T. Zhang", "F.J. Oles"], "venue": "Inf. Retr., 4(1):5\u201331", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2001}, {"title": "Accelerating minibatch stochastic gradient descent using stratified sampling", "author": ["P. Zhao", "T. Zhang"], "venue": "CoRR, abs/1405.3080", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2014}, {"title": "Stochastic optimization with importance sampling", "author": ["P. Zhao", "T. Zhang"], "venue": "CoRR, abs/1401.2753", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2014}, {"title": "Accelerated minibatch randomized block coordinate descent method", "author": ["T. Zhao", "M. Yu", "Y. Wang", "R. Arora", "H. Liu"], "venue": "NIPS", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2014}, {"title": "Gradient descent with proximal average for nonconvex and composite regularization", "author": ["W. Zhong", "J. Kwok"], "venue": "AAAI", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2014}], "referenceMentions": [{"referenceID": 19, "context": "Regularized risk minimization [20] is a fundamental subject in machine learning and statistics, whose formulations typically admit a combination of a loss function and a regularization term.", "startOffset": 30, "endOffset": 34}, {"referenceID": 1, "context": "In such scenarios, stochastic (sub)gradient descent (SGD) [2], [6], [9], [13], [15], [18], [24] is a favored method used by many theorist and practitioners.", "startOffset": 58, "endOffset": 61}, {"referenceID": 5, "context": "In such scenarios, stochastic (sub)gradient descent (SGD) [2], [6], [9], [13], [15], [18], [24] is a favored method used by many theorist and practitioners.", "startOffset": 63, "endOffset": 66}, {"referenceID": 8, "context": "In such scenarios, stochastic (sub)gradient descent (SGD) [2], [6], [9], [13], [15], [18], [24] is a favored method used by many theorist and practitioners.", "startOffset": 68, "endOffset": 71}, {"referenceID": 12, "context": "In such scenarios, stochastic (sub)gradient descent (SGD) [2], [6], [9], [13], [15], [18], [24] is a favored method used by many theorist and practitioners.", "startOffset": 73, "endOffset": 77}, {"referenceID": 14, "context": "In such scenarios, stochastic (sub)gradient descent (SGD) [2], [6], [9], [13], [15], [18], [24] is a favored method used by many theorist and practitioners.", "startOffset": 79, "endOffset": 83}, {"referenceID": 17, "context": "In such scenarios, stochastic (sub)gradient descent (SGD) [2], [6], [9], [13], [15], [18], [24] is a favored method used by many theorist and practitioners.", "startOffset": 85, "endOffset": 89}, {"referenceID": 23, "context": "In such scenarios, stochastic (sub)gradient descent (SGD) [2], [6], [9], [13], [15], [18], [24] is a favored method used by many theorist and practitioners.", "startOffset": 91, "endOffset": 95}, {"referenceID": 4, "context": "Its singlesample or mini-batch [5], [18] updating scheme is a general remedy for the O(n) complexity in exact gradient descent (GD) methods (n represents the number of training samples).", "startOffset": 31, "endOffset": 34}, {"referenceID": 17, "context": "Its singlesample or mini-batch [5], [18] updating scheme is a general remedy for the O(n) complexity in exact gradient descent (GD) methods (n represents the number of training samples).", "startOffset": 36, "endOffset": 40}, {"referenceID": 27, "context": "For example, the work in [28] explicitly expresses the gradient variance and proves that constructing mini-batch using special non-uniform sampling strategy is able to reduce the gradient variance.", "startOffset": 25, "endOffset": 29}, {"referenceID": 16, "context": "Another method named stochastic average gradient (SAG) [17] keeps a record of historic stochastic gradients and adaptively averages them for the use in the current iteration.", "startOffset": 55, "endOffset": 59}, {"referenceID": 9, "context": "We adopt the computational framework of residual-minimizing gradient correction which was originally proposed in stochastic variance-reduced gradient (SVRG) [10] by Johnson and Zhang.", "startOffset": 157, "endOffset": 161}, {"referenceID": 9, "context": "Theoretic analysis in [10], [11], [22] reveals that semistochastic algorithms achieve a geometric rate of convergence.", "startOffset": 22, "endOffset": 26}, {"referenceID": 10, "context": "Theoretic analysis in [10], [11], [22] reveals that semistochastic algorithms achieve a geometric rate of convergence.", "startOffset": 28, "endOffset": 32}, {"referenceID": 21, "context": "Theoretic analysis in [10], [11], [22] reveals that semistochastic algorithms achieve a geometric rate of convergence.", "startOffset": 34, "endOffset": 38}, {"referenceID": 9, "context": "In fact, most existing semi-stochastic algorithms either rely on periodic full gradient computation [10] or use Hessian-like covariance matrix operations [21], which account for their high iteration complexities.", "startOffset": 100, "endOffset": 104}, {"referenceID": 20, "context": "In fact, most existing semi-stochastic algorithms either rely on periodic full gradient computation [10] or use Hessian-like covariance matrix operations [21], which account for their high iteration complexities.", "startOffset": 154, "endOffset": 158}, {"referenceID": 21, "context": "Our theoretic observations are based on the following assumptions, similar to previous semi-stochastic gradient descent methods [22], [27]:", "startOffset": 128, "endOffset": 132}, {"referenceID": 26, "context": "Our theoretic observations are based on the following assumptions, similar to previous semi-stochastic gradient descent methods [22], [27]:", "startOffset": 134, "endOffset": 138}, {"referenceID": 9, "context": "SVRG [10], as introduced in preceding section, obeys the update rule in (4).", "startOffset": 5, "endOffset": 9}, {"referenceID": 20, "context": "Another semi-stochastic algorithm, stochastic control variate (SCV) [21], represents a general approach of using control variate for variance reduction in stochastic gradient.", "startOffset": 68, "endOffset": 72}, {"referenceID": 0, "context": "If R(w) is itself composition of several non-smooth functions, one can resort to the modern proximal average techniques [1], [30].", "startOffset": 120, "endOffset": 123}, {"referenceID": 29, "context": "If R(w) is itself composition of several non-smooth functions, one can resort to the modern proximal average techniques [1], [30].", "startOffset": 125, "endOffset": 129}, {"referenceID": 22, "context": "For example, in [23] Yu et al.", "startOffset": 16, "endOffset": 20}, {"referenceID": 7, "context": "The idea is later generalized in the work of locallylinear support vector machine [8], [12], where each anchor determines a function (rather than a fixed value), namely f(z) in (16) is replaced by an x-varying function fz(x).", "startOffset": 82, "endOffset": 85}, {"referenceID": 11, "context": "The idea is later generalized in the work of locallylinear support vector machine [8], [12], where each anchor determines a function (rather than a fixed value), namely f(z) in (16) is replaced by an x-varying function fz(x).", "startOffset": 87, "endOffset": 91}, {"referenceID": 13, "context": "1) Constructing anchor set: Compared to universal gradient approximation, anchor set [14] has a stronger representation power by establishing locally low-order (such as quadratic or cubic) approximation around each anchor point.", "startOffset": 85, "endOffset": 89}, {"referenceID": 13, "context": "In graph-based propagation methods, it is known that connecting sample with remote anchors potential does harm to the performance [14].", "startOffset": 130, "endOffset": 134}, {"referenceID": 24, "context": "However, as discovered in [25], [26], hinge loss can be smoothed by the loss of \u201cmodified logistic regression\u201d:", "startOffset": 26, "endOffset": 30}, {"referenceID": 25, "context": "However, as discovered in [25], [26], hinge loss can be smoothed by the loss of \u201cmodified logistic regression\u201d:", "startOffset": 32, "endOffset": 36}, {"referenceID": 3, "context": "Another solution of smoothing hinge loss is using squared hinge loss as adopted by L2-SVM [4], namely (1/2)([1 \u2212 ywx]+), which naturally removes the irregular point at the risk of over-penalizing large response.", "startOffset": 90, "endOffset": 93}, {"referenceID": 2, "context": "When parameters w constitute a matrix rather than a vector, regularization terms such as matrix nuclear norm [3] can be applied.", "startOffset": 109, "endOffset": 112}, {"referenceID": 26, "context": "For other sophisticated algorithms that target at improved mini-batch construction (such as SSGD [27]), the iteration complexity is generally better than ours.", "startOffset": 97, "endOffset": 101}, {"referenceID": 15, "context": "5 in [16].", "startOffset": 5, "endOffset": 9}, {"referenceID": 28, "context": "For non-strongly convex functions, adding quadratic perturbation terms can be used to reach similar argument [29].", "startOffset": 109, "endOffset": 113}, {"referenceID": 26, "context": "\u2022 Stratified SGD (SSGD) [27]: This method aims to improve the standard mini-batch SGD using data clustering and stratified sampling.", "startOffset": 24, "endOffset": 28}, {"referenceID": 9, "context": "\u2022 Stochastic Variance Reduction Gradient (SVRG): This original idea work of SVRG is found in [10].", "startOffset": 93, "endOffset": 97}, {"referenceID": 21, "context": "In the comparison we adopt the extension proposed in [22].", "startOffset": 53, "endOffset": 57}, {"referenceID": 21, "context": "Inheriting the two nested loops of SVRG, one of the key parameters in [22] is the the maximal iteration number in the inner loop.", "startOffset": 70, "endOffset": 74}, {"referenceID": 20, "context": "\u2022 Stochastic Control Variate (SCV) [21]: This is another semi-stochastic gradient method that reports state-of-the-art speed and accuracies.", "startOffset": 35, "endOffset": 39}, {"referenceID": 9, "context": "Most of prior works [10], [21] report the performance with respect to iteration counts.", "startOffset": 20, "endOffset": 24}, {"referenceID": 20, "context": "Most of prior works [10], [21] report the performance with respect to iteration counts.", "startOffset": 26, "endOffset": 30}, {"referenceID": 18, "context": "However, we will explore the distributed variants of the proposed S3GD like [19] in the future.", "startOffset": 76, "endOffset": 80}, {"referenceID": 6, "context": "Moreover, extension to non-convex formulations such as deep networks [7] is also a meaningful future direction.", "startOffset": 69, "endOffset": 72}], "year": 2015, "abstractText": "Stochastic gradient descent (SGD) holds as a classical method to build large scale machine learning models over big data. A stochastic gradient is typically calculated from a limited number of samples (known as mini-batch), so it potentially incurs a high variance and causes the estimated parameters bounce around the optimal solution. To improve the stability of stochastic gradient, recent years have witnessed the proposal of several semi-stochastic gradient descent algorithms, which distinguish themselves from standard SGD by incorporating global information into gradient computation. In this paper we contribute a novel stratified semi-stochastic gradient descent (S3GD) algorithm to this nascent research area, accelerating the optimization of a large family of composite convex functions. Though theoretically converging faster, prior semi-stochastic algorithms are found to suffer from high iteration complexity, which makes them even slower than SGD in practice on many datasets. In our proposed S3GD, the semi-stochastic gradient is calculated based on efficient manifold propagation, which can be numerically accomplished by sparse matrix multiplications. This way S3GD is able to generate a highly-accurate estimate of the exact gradient from each mini-batch with largely-reduced computational complexity. Theoretic analysis reveals that the proposed S3GD elegantly balances the geometric algorithmic convergence rate against the space and time complexities during the optimization. The efficacy of S3GD is also experimentally corroborated on several large-scale benchmark datasets.", "creator": "TeX"}}}