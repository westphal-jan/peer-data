{"id": "1506.08422", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "28-Jun-2015", "title": "Topic2Vec: Learning Distributed Representations of Topics", "abstract": "Latent Dirichlet Allocation (LDA) mining thematic structure of documents plays an important role in nature language processing and machine learning areas. However, the probability distribution from LDA only describes the statistical relationship of occurrences in the corpus and usually in practice, probability is not the best choice for feature representations. Recently, embedding methods have been proposed to represent words and documents by learning essential concepts and representations, such as Word2Vec and Doc2Vec. The embedded representations have shown more effectiveness than LDA-style representations in many tasks. In this paper, we propose the Topic2Vec approach which can learn topic representations in the same semantic vector space with words, as an alternative to probability. The experimental results show that Topic2Vec achieves interesting and meaningful results.", "histories": [["v1", "Sun, 28 Jun 2015 16:17:40 GMT  (525kb,D)", "http://arxiv.org/abs/1506.08422v1", "6 pages, 3 figures"]], "COMMENTS": "6 pages, 3 figures", "reviews": [], "SUBJECTS": "cs.CL cs.LG", "authors": ["li-qiang niu", "xin-yu dai"], "accepted": false, "id": "1506.08422"}, "pdf": {"name": "1506.08422.pdf", "metadata": {"source": "CRF", "title": "Topic2Vec: Learning Distributed Representations of Topics", "authors": ["Li-Qiang Niu", "Xin-Yu Dai"], "emails": ["daixinyu}@nlp.nju.edu.cn"], "sections": [{"heading": "1 Introduction", "text": "The aim is to find short and essential descriptions that allow efficient processing of large systems and give rise to basic tasks such as classification, summary and estimation of similarities or relevance. Over the past decades, various models and solutions have been proposed, such as the terminology of words (BOW) (Harris, 1954), TF-IDF, 1983), latent semantic analysis (LSA, 1998) and probabilistic latent semantic analysis (PLSA, 1999). The best known model is Dirichlet Allocation (LDA et al., 2003), which describes the hierarchical relationships between words, topics and documents."}, {"heading": "2 Related Models", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "2.1 Latent Dirichlet Allocation", "text": "The latent Dirichlet Allocation (LDA) (Blei et al., 2003) is a probabilistic generative model that assumes that each document is a mixture of latent topics, with each topic representing a probability distribution across all words in the vocabulary. In short, LDA generates a word sequence as follows: \u2022 For each N-word wn in document d: - Sample of a topic zn \u0445 Multinomial (\u03b8d) - Sample of a word wn \u0445 Multinomial (\u03c6zn). By Gibbs Sample 1 we obtain a probability matrix between document and topicword as well as a probability matrix \u0445. For a new document of any length we can derive the latent topics involved from this and in the meantime we will assign a topic name to each word in the document."}, {"heading": "2.2 Word2Vec", "text": "Inspired by the Neural Probabilistic Language Model (NPLM) (Bengio et al., 2003), Mikolov et al. (2013a) proposed Word2Vec, including CBOW and Skip-gram, to calculate continuous vector representations of words from large data sets. (1a) LCBOW (D) = 1M M M M \u00b2 i = 1 Log p (wi | wcxt), (1b) LSkip \u2212 gram (D) = 1M M M M \u00b2 i = 1 x \u2212 k \u2264 c k, c6 = 0 Log p (wi + c | wi).1http: / / gibbslda.sourforge.net / Here, in Equation (1wa) there is the text of the word (b)."}, {"heading": "3 Topic2Vec", "text": "Inspired by word2vec, we integrate topics and words into the NPLM. We suggest Topic2Vec, as shown in Fig. 1, to learn distributed topics along with word representations. Topic2Vec is also separated in CBOW and Skip-gram situations. CBOW, for example, predicts a word sequence (wt \u2212 2, wt \u2212 1, wt + 1, wt + 2) in which wt is assigned the current word with the topic zt by LDA. CBOW predicts the word wt + 2, wt + 2) predicts the topic zt based on the surrounding words (wt \u2212 2, wt \u2212 1, wt + 2), while Skip-gram predicts surrounding words (wt \u2212 2, wt \u2212 1, wt \u2212 1, wt + 2) predicts the word wt + 2, wt + 2) predicts the current wt and the topic based on the surrounding words (wt \u2212 2, wt \u2212 S \u2212 the word sequence is used in the xolox training."}, {"heading": "4 Experiments", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.1 Dataset", "text": "We use the Gigaword Fifth Edition2 as training data to learn basic word and topic representation. We randomly extract a portion of the documents and construct our training set as follows: We selected 100,000 documents, each of which consists of more than 1,000 characters from the ltw eng subfolder (Los Angeles Times) containing 411,032 documents. We also eliminate words that occur less than 5 times, and the stopwords. In the end, the training set contains about 42 million words and the vocabulary size is 102,644."}, {"heading": "4.2 Evaluation Methods", "text": "In experiments, we run Topic2Vec in Skip-gram and learn about topics along with word representations. We then evaluate the topic repre-2https: / / catalog.ldc.upenn.edu / LDC2011T07presentations by comparing Topic2Vec with LDA in two aspects: (1) we select most related topics or words conditioned to selected topics, and (2) we embed these related words or topics into 2D space using t-SNE (Maaten et al., 2008). During the process, we group words into topics such as: \u2022 LDA: Each topic is a probability distribution over words. We select the topmost N = 10 words with the highest conditional probability. \u2022 Topic2Vec: topics and words are displayed just like the low-dimensional vectors, we can immediately calculate the cosmic similarity between words and topics."}, {"heading": "4.3 Analysis of Results", "text": "As shown in Fig. 2, LDA returns the words \"drug,\" \"drugs,\" \"cancer\" and \"patients\" in topic 19, while Topic2Vec returns the words \"aricept,\" \"memantine,\" \"enbrel\" and \"gabapentin.\" In topic 27, LDA returns the words \"medicine,\" \"hospital,\" \"patients\" and \"physicians,\" while Topic2Vec returns the words \"neonatal,\" \"anesthesiologist,\" \"anesthesia\" and \"coma.\" All we know is that Topic 19 and Topic 27 share the same topic about \"patients\" or \"patients\" and \"physicians,\" but we cannot find out their further difference from the results of LDA. But from the results of Topic2Vec we can easily find that Topic 19 and Topic 27 represent the same topic about \"patients\" or \"medical.\""}, {"heading": "5 Conclusions and Future Work", "text": "In this paper, by integrating NPLM, Word2Vec and LDA, we propose the Topic2Vec, which successfully embeds latent topics in the same semantic vector space with words. In principle, our goal is clearly to learn how Topic2Vec represents new fashion topics. By observing experiments, Topic2Vec shows more differentiated results than LDA and we conclude that Topic2Vec can model topics better. However, for now, we are evaluating only the qualitative performance of Topic2Vec and LDA, and will perform quantitatively more detailed analyses of their differences in the future. Furthermore, we must first run LDA to assign a topic to each word in the corpus before Topic2Vec."}, {"heading": "Acknowledgments", "text": "It is not the first time that the EU Commission has taken such a step."}], "references": [{"title": "A neural probabilistic language model", "author": ["Bengio et al.2003] Yoshua Bengio", "R\u00e9jean Ducharme", "Pascal Vincent", "Christian Jauvin"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Bengio et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Bengio et al\\.", "year": 2003}, {"title": "A unified architecture for natural language processing: deep neural networks with multitask learning", "author": ["Collobert", "Weston2008] Ronan Collobert", "Jason Weston"], "venue": "In Proceedings of the 25th International Conference on Machine learning,", "citeRegEx": "Collobert et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Collobert et al\\.", "year": 2008}, {"title": "An introduction to latent semantic analysis", "author": ["Peter W. Foltz", "Darrell Laham"], "venue": "Discourse processes", "citeRegEx": "Landauer et al\\.,? \\Q1998\\E", "shortCiteRegEx": "Landauer et al\\.", "year": 1998}, {"title": "Latent dirichlet allocation", "author": ["Blei et al.2003] David M. Blei", "Andrew Y. Ng", "Michael I. Jordan"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Blei et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Blei et al\\.", "year": 2003}, {"title": "Efficient estimation of word representations in vector space. CoRR, abs/1301.3781", "author": ["Kai Chen", "Gerg Corrado", "Jeffrey Dean"], "venue": null, "citeRegEx": "Mikolov et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Distributed representations of words and phrases and their compositionality", "author": ["Ilya Sutskever", "Kai Chen", "Greg Corrado", "Jeffrey Dean"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "Mikolov et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Linguistic regularities in continuous space word representations", "author": ["Wen-tau Yih", "Geoffrey Zweig"], "venue": "In Proceedings of the 2013 Conference of the North American Chapter of the Association for Computa-", "citeRegEx": "Mikolov et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Distributed representations of sentences and documents", "author": ["Le", "Mikolov2014] Quoc Le", "Tomas Mikolov"], "venue": "In Proceedings of the 31th International Conference on Machine", "citeRegEx": "Le et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Le et al\\.", "year": 2014}, {"title": "Dynamic pooling and unfolding recursive autoencoders for paraphrase detection", "author": ["Eric H. Huang", "Jeffrey Penniington", "Andrew Y. Ng", "Christopher D. Manning"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "Socher et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Socher et al\\.", "year": 2011}, {"title": "Parsing natural scenes and natural language with recursive neural network", "author": ["Cliff C. Lin", "Andrew Y. Ng", "Christopher D. Manning"], "venue": "In Proceedings of the 26th International Conference on Machine Learning", "citeRegEx": "Socher et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Socher et al\\.", "year": 2011}, {"title": "Improving word representations via global context and multiple word prototypes", "author": ["Huang et al.2012] Eric H. Huang", "Richard Socher", "Christopher D. Manning", "Andrew Y. Ng"], "venue": "In Annual Meeting of the Association for Computational Linguistics", "citeRegEx": "Huang et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Huang et al\\.", "year": 2012}, {"title": "Glove: Global Vectors for Word Representation", "author": ["Richard Socher", "Christopher D. Manning"], "venue": "In Conference on Empirical Methods in Natural Language Processing", "citeRegEx": "Pennington et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Pennington et al\\.", "year": 2014}, {"title": "Learning word vectors for sentiment analysis", "author": ["Mass et al.2011] Andrew L. Mass", "Raymond E. Daly", "Peter T. Pham", "Dan Huang", "Andrew Y. Ng", "Christopher Potts"], "venue": "In Proceedings of the 49th Annual Meeting of the Association", "citeRegEx": "Mass et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Mass et al\\.", "year": 2011}, {"title": "Machine learning in automated text categorization", "author": ["Fabrizio Sebastiani"], "venue": "ACM Computing Survey,", "citeRegEx": "Sebastiani.,? \\Q2002\\E", "shortCiteRegEx": "Sebastiani.", "year": 2002}, {"title": "Learning word representations with hierarchical sparse coding", "author": ["Manaal Faruqui. Chris Dyer", "Noah A. Smith"], "venue": "In NIPS Deep Learning and Representation Learning Workshop,", "citeRegEx": "Yogatama et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Yogatama et al\\.", "year": 2014}, {"title": "Learning word embeddings efficiently with noise-contrastive estimation", "author": ["Mnih", "Kavukcuoglu2013] Andriy Mnih", "Koray Kavukcuoglu"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "Mnih et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mnih et al\\.", "year": 2013}, {"title": "A scalable hierarchical distributed language model", "author": ["Mnih", "Hinton2009] Andriy Mnih", "Geoffrey E. Hinton"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Mnih et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Mnih et al\\.", "year": 2009}, {"title": "Hierarchical probabilistic neural network language model", "author": ["Morin", "Bengio2005] Frederic Morin", "Yoshua Bengio"], "venue": "In Proceedings of the International Workshop on Artificial Intelligence and Statistics,", "citeRegEx": "Morin et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Morin et al\\.", "year": 2005}, {"title": "Word representations: a simple and general method for semi-supervised learning", "author": ["Turian et al.2010] Joseph Turian", "Lev Ratinov", "Yoshua Bengio"], "venue": "In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics,", "citeRegEx": "Turian et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Turian et al\\.", "year": 2010}, {"title": "From frequency to meaning: Vector space models of semantics", "author": ["Turney", "Pantel2010] Peter D. Turney", "Patrick Pantel"], "venue": "Journal of Artificial Intelligence Research", "citeRegEx": "Turney et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Turney et al\\.", "year": 2010}, {"title": "Topic-based language models using EM", "author": ["Gildea", "Hofmann1999] Daniel Gildea", "Thomas Hofmann"], "venue": "In Proceedings of EUROSPEECH,", "citeRegEx": "Gildea et al\\.,? \\Q1999\\E", "shortCiteRegEx": "Gildea et al\\.", "year": 1999}, {"title": "Topical word embeddings", "author": ["Liu et al.2015] Yang Liu", "Zhiyuan Liu", "Tat-Seng Chua", "Maosong Sun"], "venue": "In Association for the Advancement of Artificial Intelligence", "citeRegEx": "Liu et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Liu et al\\.", "year": 2015}, {"title": "Inducing ontological co-occurrence vectors", "author": ["Patrick Pantel", "Marina del Rey"], "venue": "In Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics,", "citeRegEx": "Pantel and Rey.,? \\Q2005\\E", "shortCiteRegEx": "Pantel and Rey.", "year": 2005}, {"title": "The distributional hypothesis", "author": ["Magnus Sahlgren"], "venue": "Italian Journal of Linguistics,", "citeRegEx": "Sahlgren.,? \\Q2008\\E", "shortCiteRegEx": "Sahlgren.", "year": 2008}, {"title": "Placing search in context: the concept revisited", "author": ["Evgeniy Gabrilocivh", "Yossi Matias", "Ehud Rivlin", "Zach Solan", "Gadi Wolfman", "Eytan Ruppin"], "venue": "In Proceedings of the 10th International Conference", "citeRegEx": "Finkelstein et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Finkelstein et al\\.", "year": 2001}, {"title": "A comparison of vector-based representations for semantic composition", "author": ["Blacoe", "Lapata2012] William Blacoe", "Mirella Lapata"], "venue": "In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Com-", "citeRegEx": "Blacoe et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Blacoe et al\\.", "year": 2012}, {"title": "Multi-prototype vector-space models of word meaning", "author": ["Reisinger", "Mooney2010] Joseph Reisinger", "Raymond J. Mooney"], "venue": "In Human Language Technologies: The 2010 Annual Conference of the North America Chapter of the As-", "citeRegEx": "Reisinger et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Reisinger et al\\.", "year": 2010}, {"title": "Re-examining Machine Translation Metrics for Paraphrase Identification", "author": ["Madnani", "Tetreault2012] Nitin Madnani", "Joel Tetreault"], "venue": "In Proceedings of the 2012 Conference of the North American Chapter of the Association", "citeRegEx": "Madnani et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Madnani et al\\.", "year": 2012}, {"title": "Unsupervised Construction of Large Paraphrase Corpora: Exploiting Massively Parallel News Sources", "author": ["Dolan et al.2014] Bill Dolan", "Chris Quirk", "Chris Brockett"], "venue": "In COLING,", "citeRegEx": "Dolan et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Dolan et al\\.", "year": 2004}, {"title": "Co-learning of Word Representations and Morpheme Representations", "author": ["Qiu et al.2014] Siyu Qiu", "Qing Cui", "Jiang Bian", "Bin Gao", "Tie-Yan Liu"], "venue": "In Proceedings of COLING", "citeRegEx": "Qiu et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Qiu et al\\.", "year": 2014}, {"title": "More data usually beats better algorithms. Datawocky Blog", "author": ["Anand Rajaraman"], "venue": null, "citeRegEx": "Rajaraman.,? \\Q2008\\E", "shortCiteRegEx": "Rajaraman.", "year": 2008}, {"title": "A Convolutional Neural Network for Modelling Sentences", "author": ["Edward Grefenstette", "Phil Blunsom"], "venue": "In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics", "citeRegEx": "Kalchbrenner et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Kalchbrenner et al\\.", "year": 2014}, {"title": "A Multiplicative Model for Learning Distributed Text-Based Attribute Representations", "author": ["Kiros et al.2014] Ryan Kiros", "Richard Zemel", "Ruslan Salakhutdinov"], "venue": "In Neural Information Processing Systems (NIPS),", "citeRegEx": "Kiros et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Kiros et al\\.", "year": 2014}, {"title": "Visualizig data using t-SNE", "author": ["Laurens", "Geoffrey Hinton"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Maaten et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Maaten et al\\.", "year": 2008}, {"title": "Introduction to Modern Information Retrieval", "author": ["Salton", "McGill1983] Gerard Salton", "Michael J. McGill"], "venue": null, "citeRegEx": "Salton et al\\.,? \\Q1983\\E", "shortCiteRegEx": "Salton et al\\.", "year": 1983}, {"title": "Probabilistic Latent Semantic Analysis", "author": ["Thomas Hofmann"], "venue": "In Proceedings of the Fifteenth Conference on Uncertainty in Artificial Intelligence", "citeRegEx": "Hofmann.,? \\Q1999\\E", "shortCiteRegEx": "Hofmann.", "year": 1999}, {"title": "Face Recognition Using LDA Based Algorithms", "author": ["Lu et al.2003] Juwei Lu", "K.N. Plataniotis", "A.N. Venetsanopoulos"], "venue": "Neural Networks, IEEE Transactions on 14.1", "citeRegEx": "Lu et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Lu et al\\.", "year": 2003}, {"title": "LDA-Based Document Models for Ad-hoc Retrieval", "author": ["Wei", "Croft2006] Xing Wei", "W. Bruce Croft"], "venue": "In Proceedings of the 29th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval. ACM,", "citeRegEx": "Wei et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Wei et al\\.", "year": 2006}, {"title": "Document Clustering using Locality Preserving Indexing", "author": ["Deng et al.2005] Cai Deng", "Xiaofei He", "Jiawei Han"], "venue": "Knowledge and Data Engineering, IEEE Transactions on 17.12", "citeRegEx": "Deng et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Deng et al\\.", "year": 2005}], "referenceMentions": [{"referenceID": 2, "context": "During the past decades, various models and solutions are proposed, such as Bag-of-Words (BOW) (Harris, 1954), TF-IDF (Salton and McGill, 1983), Latent Semantic Analysis (LSA) (Landauer et al., 1998) and Probabilistic Latent Semantic Analysis (PLSA) (Hofmann, 1999).", "startOffset": 176, "endOffset": 199}, {"referenceID": 35, "context": ", 1998) and Probabilistic Latent Semantic Analysis (PLSA) (Hofmann, 1999).", "startOffset": 58, "endOffset": 73}, {"referenceID": 3, "context": "But the best-known model is Latent Dirichlet Allocation (LDA) (Blei et al., 2003) which describes the hierarchical relationships between words, topics and documents.", "startOffset": 62, "endOffset": 81}, {"referenceID": 0, "context": "Recently, distributed representations with neural probabilistic language models (NPLMs) (Bengio et al., 2003) were proposed to represent words and documents as low-dimensional vectors in one semantic space, and achieved significant results in many NLP and ML tasks (Collobert and Weston, 2008; Mnih and Hinton, 2009; Mikolov et al.", "startOffset": 88, "endOffset": 109}, {"referenceID": 10, "context": ", 2003) were proposed to represent words and documents as low-dimensional vectors in one semantic space, and achieved significant results in many NLP and ML tasks (Collobert and Weston, 2008; Mnih and Hinton, 2009; Mikolov et al., 2013a; Mnih and Kavukcuoglu, 2013; Huang et al., 2012; Le and Mikolov, 2014).", "startOffset": 163, "endOffset": 307}, {"referenceID": 0, "context": "Recently, distributed representations with neural probabilistic language models (NPLMs) (Bengio et al., 2003) were proposed to represent words and documents as low-dimensional vectors in one semantic space, and achieved significant results in many NLP and ML tasks (Collobert and Weston, 2008; Mnih and Hinton, 2009; Mikolov et al., 2013a; Mnih and Kavukcuoglu, 2013; Huang et al., 2012; Le and Mikolov, 2014). In particular, Word2Vec proposed by Mikolov et al. (2013a) could automatically learn concepts and semantic-syntactic relationships between words like vec(\u201cBerlin\u201d) vec(\u201cGermany\u201d) = vec(\u201cParis\u201d) - vec(\u201cFrance\u201d).", "startOffset": 89, "endOffset": 470}, {"referenceID": 0, "context": "Recently, distributed representations with neural probabilistic language models (NPLMs) (Bengio et al., 2003) were proposed to represent words and documents as low-dimensional vectors in one semantic space, and achieved significant results in many NLP and ML tasks (Collobert and Weston, 2008; Mnih and Hinton, 2009; Mikolov et al., 2013a; Mnih and Kavukcuoglu, 2013; Huang et al., 2012; Le and Mikolov, 2014). In particular, Word2Vec proposed by Mikolov et al. (2013a) could automatically learn concepts and semantic-syntactic relationships between words like vec(\u201cBerlin\u201d) vec(\u201cGermany\u201d) = vec(\u201cParis\u201d) - vec(\u201cFrance\u201d). Doc2Vec (Para2Vec) proposed by Le and Mikolov (2014) achieves state-of-the-art performance on sentiment analysis.", "startOffset": 89, "endOffset": 675}, {"referenceID": 3, "context": "Latent Dirichlet allocation (LDA) (Blei et al., 2003) is a probabilistic generative model that assumes each document is a mixture of latent topics, where each topic is a probability distribution over all words in vocabulary.", "startOffset": 34, "endOffset": 53}, {"referenceID": 0, "context": "Inspired by Neural Probabilistic Language Model (NPLM) (Bengio et al., 2003), Mikolov et al.", "startOffset": 55, "endOffset": 76}, {"referenceID": 0, "context": "Inspired by Neural Probabilistic Language Model (NPLM) (Bengio et al., 2003), Mikolov et al. (2013a) proposed Word2Vec including CBOW and Skip-gram for computing continuous vector representations of words from large data sets.", "startOffset": 56, "endOffset": 101}, {"referenceID": 33, "context": "sentations via comparing Topic2Vec with LDA in two aspects: (1) we select most related topics or words conditioned on selected topics and (2) we embed these related words or topics in 2D space using t-SNE (Maaten et al., 2008).", "startOffset": 205, "endOffset": 226}], "year": 2015, "abstractText": "Latent Dirichlet Allocation (LDA) mining thematic structure of documents plays an important role in nature language processing and machine learning areas. However, the probability distribution from LDA only describes the statistical relationship of occurrences in the corpus and usually in practice, probability is not the best choice for feature representations. Recently, embedding methods have been proposed to represent words and documents by learning essential concepts and representations, such as Word2Vec and Doc2Vec. The embedded representations have shown more effectiveness than LDA-style representations in many tasks. In this paper, we propose the Topic2Vec approach which can learn topic representations in the same semantic vector space with words, as an alternative to probability. The experimental results show that Topic2Vec achieves interesting and meaningful results.", "creator": "LaTeX with hyperref package"}}}