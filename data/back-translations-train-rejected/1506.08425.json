{"id": "1506.08425", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "28-Jun-2015", "title": "Deep-Plant: Plant Identification with convolutional neural networks", "abstract": "This paper studies convolutional neural networks (CNN) to learn unsupervised feature representations for 44 different plant species, collected at the Royal Botanic Gardens, Kew, England. To gain intuition on the chosen features from the CNN model (opposed to a 'black box' solution), a visualisation technique based on the deconvolutional networks (DN) is utilized. It is found that venations of different order have been chosen to uniquely represent each of the plant species. Experimental results using these CNN features with different classifiers show consistency and superiority compared to the state-of-the art solutions which rely on hand-crafted features.", "histories": [["v1", "Sun, 28 Jun 2015 16:58:47 GMT  (4084kb,D)", "http://arxiv.org/abs/1506.08425v1", "6 pages, 8 figures, accepted as oral presentation in ICIP2015, Qu\\'ebec City, Canada"]], "COMMENTS": "6 pages, 8 figures, accepted as oral presentation in ICIP2015, Qu\\'ebec City, Canada", "reviews": [], "SUBJECTS": "cs.CV cs.AI cs.NE", "authors": ["sue han lee", "chee seng chan", "paul wilkin", "paolo remagnino"], "accepted": false, "id": "1506.08425"}, "pdf": {"name": "1506.08425.pdf", "metadata": {"source": "CRF", "title": "DEEP-PLANT: PLANT IDENTIFICATION WITH CONVOLUTIONAL NEURAL NETWORKS", "authors": ["Sue Han Lee", "Chee Seng Chan", "Paul Wilkin", "Paolo Remagnino"], "emails": ["{leesuehan@siswa.um.edu.my;", "cs.chan@um.edu.my;", "p.wilkin@kew.org;", "p.remagnino@kingston.ac.uk}"], "sections": [{"heading": null, "text": "Index Terms - Classification of Plants, Deep Learning, Feature Visualization"}, {"heading": "1. INTRODUCTION", "text": "Plants are the backbone of all life on Earth that provides us with food and oxygen. A good understanding of plants is essential to identify new or rare plant species, to improve the pharmaceutical industry, balance the ecosystem, and agricultural productivity and sustainability. Of all those that botanists use variations in leaf characteristics as a comparative tool for their study of plants [1, 2], this is because leaf characteristics are still considered a challenging and unsolvable problem that extends throughout the year. In computer vision, despite much effort [3-8] (using sophisticated computer algorithms), plant identification is still considered a challenging and insolvable task, because a plant in nature has a very similar shape and color representation as shown in the illustration. 1. Kumar et al. [3] proposed an automatic identification of plant species, namely Leafsnap."}, {"heading": "2. PROPOSED APPROACH", "text": "In this section, we will first explain how to use the pre-trained CNN model to identify plants, and then explain how to use a DN model with our new visualization strategy to understand how the CNN model works in identifying different plant species."}, {"heading": "2.1. Convolutional Neural Network", "text": "The CNN model used in this paper is based on the model proposed in [14] with the ILSVRC2012 dataset that was used for pre-training. Instead of training a new CNN architecture, we used the pre-trained network based on a recent work [15] that showed that functions extracted from activating a CNN dataset can be transferred to large-scale object detection work in a fully monitored manner. 2) Our training set is not as extensive as the ILSVRC2012 dataset. The performance of the CNN model depends heavily on the amount and degree of diversity of the training set, and lastly c) training a deep model requires skill and experience. It is also time consuming. For our CNN model, we fine-tune a 44-leaf dataset collected at the Royal Botanic Gardens, Kew, England."}, {"heading": "2.2. Deconvolutional Network", "text": "The CNN model learns and optimizes the filters in each layer through the back-propagation mechanism. These learned filters extract important characteristics that uniquely represent the input-sheet image. Therefore, in order to understand why and how the CNN model works (rather than treating it as a \"black box\"), a filter visualization is required to observe the transformation of the characteristics, as well as to understand the internal operation and characteristics of the CNN model. In addition, we can identify the unique characteristics on the sheet images that are considered important to characterize a plant from this process. [17, 18] introduced multilayer DN, which allows us to observe the transformation of the characteristics by projecting the characteristic maps back into the input-pixel space. Specifically, the characteristic maps from layer n are alternately deconvolved and unpooled layer, which we continuously insert into the input-pixel space."}, {"heading": "3. EXPERIMENTAL RESULTS", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "3.1. Data Preparation", "text": "A new leaf dataset called MalayaKew (MK) Leaf Dataset, which consists of 44 classes collected at the Royal Botanic Gardens in Kew, England, is used in the experiment. Examples of the leaf dataset are shown in Fig. 1, and we could see that this dataset is very difficult because leaves from different classes have a very similar appearance. A dataset (D1) is prepared to compare the performance of the trained CNN. That is, we use leaf images as a whole, where foreground pixels are extracted from each leaf image using HSV color space information. To enlarge the D1 dataset, we rotate the individual leaf images in 7 different orientations, e.g. 45, 90, 135, 180, 225, 270 and 315. We then randomly select 528 leaf images for testing and 2288 images for training."}, {"heading": "3.2. Results and Failure Analysis - D1", "text": "In this section, we present a comparative performance assessment of the CNN plant identification model. Table 1 shows that the use of the traits learned from the CNN model (98.1%) surpasses the most advanced solutions [3,9,11], using carefully selected handmade traits, even when using different classifiers. We performed error analyses and found that most of the misclassified leaves come from Class 2 (4 misclassified), followed by Class 23 (3), Class 9 & 27 (2 each) and Class 38. From our study, as shown in Fig. 5, the Q. robur f. purascens (i.e. Class 2) were misclassified as Q. acutissima (i.e. Class 9), Q. rubra \"Aurea\" (i.e. Class 27) and Q. macranthera (i.e. Class 39)."}, {"heading": "3.3. Results and Failure Analysis - D2", "text": "Here we built a variant dataset (D2) where we manually cut each leaf image in the D1 into patches within the area of the leaf (so that no shape is included).This investigation is twofold. On the one hand, we intend to know what the precision of the plant identification classifier is when the leaf shape is excluded? On the other hand, we want to find out if plant identification could be carried out simply by patching the leaf. Since the original images are selected from 3000 x 3000 to 500 x 500, three different leaf patch sizes (i.e. 500 x 500, 400 x 400 and 256 x 256). Likewise, we increase the variety of leaf patches by rotating them in the same way as in D1. We randomly select 8800 leaf patches for testing and 34672 leaf patches for training. In Table 1, we can see that the classification accuracy of the CNN model is trained."}, {"heading": "4. CONCLUSION", "text": "Based on the experimental results, we have demonstrated that learning the traits by CNN can provide a better representation of the characteristics of leaf images compared to handmade traits. Furthermore, we have shown that the venation structure is an important trait for identifying different plant species with a performance of 99.6%, exceeding conventional solutions, which is confirmed by the analysis of the internal functioning and behavior of the network using DN visualization techniques. In the future, we will extend the work to detection in the wild."}, {"heading": "Acknowledgment", "text": "This research is supported by the Malaysian Ministry of Education's High Impact MoE Grant UM.C / 625 / 1 / HIR / MoE / FCSIT / 08, H-22001-00-B00008."}, {"heading": "5. REFERENCES", "text": "[1] James S Cope, David Corney, Jonathan Y Clark, Paolo Remagnino, and Paul Wilkin, \"Classification of Plant Species Using Digital Morphometries: A review,\" ExpertSystems with Applications, vol. 39, no. 8, pp. Wang-7573, 2012. [2] James Clarke, Sarah Barman, Paolo Remagnino, Ken Bailey, Don Kirkup, Simon Mayo, and Paul Wilkin, \"Venation pattern analysis of leaf images,\" in Advances in Visual Computing, pp. 427-436. Springer, 2006. [3] Neeraj XiXiXiXiar, Peter N Belhumeur, Arijit Biswas, David W Jacobs, W John Kress, Ida C Lopez, and Jo\u00e3o VB Soares, \"Leafsnap: A computer vision system for automatic plant identification,\" in ECCV, pp. 502-516."}], "references": [{"title": "Plant species identification using digital morphometrics: A review", "author": ["James S Cope", "David Corney", "Jonathan Y Clark", "Paolo Remagnino", "Paul Wilkin"], "venue": "Expert  Systems with Applications, vol. 39, no. 8, pp. 7562\u2013 7573, 2012.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2012}, {"title": "Venation pattern analysis of leaf images", "author": ["James Clarke", "Sarah Barman", "Paolo Remagnino", "Ken Bailey", "Don Kirkup", "Simon Mayo", "Paul Wilkin"], "venue": "Advances in Visual Computing, pp. 427\u2013436. Springer, 2006.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2006}, {"title": "Leafsnap: A computer vision system for automatic plant species identification", "author": ["Neeraj Kumar", "Peter N Belhumeur", "Arijit Biswas", "David W Jacobs", "W John Kress", "Ida C Lopez", "Jo\u00e3o VB Soares"], "venue": "ECCV, pp. 502\u2013516. Springer, 2012.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2012}, {"title": "Geometric leaf classification", "author": ["Cem Kalyoncu", "\u00d6nsen Toygar"], "venue": "Computer Vision and Image Understanding, in Press, http://dx.doi.org/10.1016/j.cviu.2014.11.001.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2014}, {"title": "Leaf classification using shape, color, and texture features", "author": ["Abdul Kadir", "Lukito Edi Nugroho", "Adhi Susanto", "Paulus Insap Santosa"], "venue": "arXiv preprint arXiv:1401.4447, 2013.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2013}, {"title": "Shape and texture based plant leaf classification", "author": ["Thibaut Beghin", "James S Cope", "Paolo Remagnino", "Sarah Barman"], "venue": "Advanced Concepts for Intelligent Vision Systems, 2010, pp. 345\u2013353.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2010}, {"title": "Eagle: A novel descriptor for identifying plant species using leaf lamina vascular features", "author": ["James Charters", "Zhiyong Wang", "Zheru Chi", "Ah Chung Tsoi", "David Dagan Feng"], "venue": "ICME-Workshop, 2014, pp. 1\u20136.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2014}, {"title": "The extraction of venation from leaf images by evolved vein classifiers and ant colony algorithms", "author": ["James S Cope", "Paolo Remagnino", "Sarah Barman", "Paul Wilkin"], "venue": "Advanced Concepts for Intelligent Vision Systems. Springer, 2010, pp. 135\u2013144.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2010}, {"title": "Evaluation of features for leaf classification in challenging conditions", "author": ["David Hall", "Chris McCool", "Feras Dayoub", "Niko Sunderhauf", "Ben Upcroft"], "venue": "2015.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2015}, {"title": "Unbiased look at dataset bias", "author": ["Antonio Torralba", "Alexei A Efros"], "venue": "CVPR, 2011, pp. 1521\u20131528.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2011}, {"title": "Linear spatial pyramid matching using sparse coding for image classification", "author": ["Jianchao Yang", "Kai Yu", "Yihong Gong", "Thomas Huang"], "venue": "CVPR, 2009, pp. 1794\u20131801.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2009}, {"title": "Multiscale recognition of legume varieties based on leaf venation images", "author": ["M\u00f3nica G Larese", "Ariel E Bay\u00e1", "Roque M Craviotto", "Miriam R Arango", "Carina Gallo", "Pablo M Granitto"], "venue": "Expert Systems with Applications, vol. 41, no. 10, pp. 4638\u20134647, 2014.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2014}, {"title": "Plant leaf identification using gabor wavelets", "author": ["Dalcimar Casanova", "Jarbas Joaci de Mesquita Sa Junior", "Odemir Martinez Bruno"], "venue": "International Journal of Imaging Systems and Technology, vol. 19, no. 3, pp. 236\u2013243, 2009.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2009}, {"title": "Imagenet classification with deep convolutional neural networks", "author": ["Alex Krizhevsky", "Ilya Sutskever", "Geoffrey E Hinton"], "venue": "NIPS, 2012, pp. 1097\u20131105.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2012}, {"title": "Decaf: A deep convolutional activation feature for generic visual recognition", "author": ["Jeff Donahue", "Yangqing Jia", "Oriol Vinyals", "Judy Hoffman", "Ning Zhang", "Eric Tzeng", "Trevor Darrell"], "venue": "arXiv preprint arXiv:1310.1531, 2013.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2013}, {"title": "Learning a deep convolutional network for image super-resolution", "author": ["Chao Dong", "Chen Change Loy", "Kaiming He", "Xiaoou Tang"], "venue": "ECCV, pp. 184\u2013199. Springer, 2014.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2014}, {"title": "Adaptive deconvolutional networks for mid and high level feature learning", "author": ["Matthew D Zeiler", "Graham W Taylor", "Rob Fergus"], "venue": "ICCV, 2011, pp. 2018\u20132025.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2011}, {"title": "Visualizing and understanding convolutional networks", "author": ["Matthew D Zeiler", "Rob Fergus"], "venue": "ECCV, pp. 818\u2013833. Springer, 2014.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2014}, {"title": "Evolution and function of leaf venation architecture: a review", "author": ["Anita Roth-Nebelsick", "Dieter Uhl", "Volker Mosbrugger", "Hans Kerp"], "venue": "Annals of Botany, vol. 87, no. 5, pp. 553\u2013566, 2001.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2001}, {"title": "Venation pattern formation in arabidopsis thaliana vegetative leaves", "author": ["H\u00e9ctor Candela", "Antonio Mart\u0131nez-Laborda", "Jos\u00e9 Luis Micol"], "venue": "Developmental biology, vol. 205, no. 1, pp. 205\u2013216, 1999.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 1999}, {"title": "Modeling and visualization of leaf venation patterns", "author": ["Adam Runions", "Martin Fuhrer", "Brendan Lane", "Pavol Federl", "Anne-Ga\u00eblle Rolland-Lagan", "Przemyslaw Prusinkiewicz"], "venue": "ACM Transactions on Graphics, vol. 24, no. 3, pp. 702\u2013711, 2005.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2005}, {"title": "Artificial ants to extract leaf outlines and primary venation patterns", "author": ["Robert J Mullen", "Dorothy Monekosso", "Sarah Barman", "Paolo Remagnino", "Paul Wilkin"], "venue": "Ant Colony Optimization and Swarm Intelligence, pp. 251\u2013258. Springer, 2008.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2008}], "referenceMentions": [{"referenceID": 0, "context": "A good understanding of plants is essential to help in identifying new or rare plant species in order to improve the drug industry, balance the ecosystem as well as the agricultural productivity and sustainability [1].", "startOffset": 214, "endOffset": 217}, {"referenceID": 0, "context": "Amongst all, botanists use variations on leaf characteristics as a comparative tool for their study on plants [1, 2].", "startOffset": 110, "endOffset": 116}, {"referenceID": 1, "context": "Amongst all, botanists use variations on leaf characteristics as a comparative tool for their study on plants [1, 2].", "startOffset": 110, "endOffset": 116}, {"referenceID": 2, "context": "This is because leaf characteristics are available to be observed and examined throughout the year in deciduous, annual plants or year-round in evergreen perennials In computer vision, despite many efforts [3\u20138] (i.", "startOffset": 206, "endOffset": 211}, {"referenceID": 3, "context": "This is because leaf characteristics are available to be observed and examined throughout the year in deciduous, annual plants or year-round in evergreen perennials In computer vision, despite many efforts [3\u20138] (i.", "startOffset": 206, "endOffset": 211}, {"referenceID": 4, "context": "This is because leaf characteristics are available to be observed and examined throughout the year in deciduous, annual plants or year-round in evergreen perennials In computer vision, despite many efforts [3\u20138] (i.", "startOffset": 206, "endOffset": 211}, {"referenceID": 5, "context": "This is because leaf characteristics are available to be observed and examined throughout the year in deciduous, annual plants or year-round in evergreen perennials In computer vision, despite many efforts [3\u20138] (i.", "startOffset": 206, "endOffset": 211}, {"referenceID": 6, "context": "This is because leaf characteristics are available to be observed and examined throughout the year in deciduous, annual plants or year-round in evergreen perennials In computer vision, despite many efforts [3\u20138] (i.", "startOffset": 206, "endOffset": 211}, {"referenceID": 7, "context": "This is because leaf characteristics are available to be observed and examined throughout the year in deciduous, annual plants or year-round in evergreen perennials In computer vision, despite many efforts [3\u20138] (i.", "startOffset": 206, "endOffset": 211}, {"referenceID": 2, "context": "[3] proposed an automatic plant species identification system namely Leafsnap.", "startOffset": 0, "endOffset": 3}, {"referenceID": 8, "context": "Other solutions employed geometric [9], multi-scale distance matrix, moment invariants [4], colour, texture [5, 6] and venation features [7, 8] to identify a plant.", "startOffset": 35, "endOffset": 38}, {"referenceID": 3, "context": "Other solutions employed geometric [9], multi-scale distance matrix, moment invariants [4], colour, texture [5, 6] and venation features [7, 8] to identify a plant.", "startOffset": 87, "endOffset": 90}, {"referenceID": 4, "context": "Other solutions employed geometric [9], multi-scale distance matrix, moment invariants [4], colour, texture [5, 6] and venation features [7, 8] to identify a plant.", "startOffset": 108, "endOffset": 114}, {"referenceID": 5, "context": "Other solutions employed geometric [9], multi-scale distance matrix, moment invariants [4], colour, texture [5, 6] and venation features [7, 8] to identify a plant.", "startOffset": 108, "endOffset": 114}, {"referenceID": 6, "context": "Other solutions employed geometric [9], multi-scale distance matrix, moment invariants [4], colour, texture [5, 6] and venation features [7, 8] to identify a plant.", "startOffset": 137, "endOffset": 143}, {"referenceID": 7, "context": "Other solutions employed geometric [9], multi-scale distance matrix, moment invariants [4], colour, texture [5, 6] and venation features [7, 8] to identify a plant.", "startOffset": 137, "endOffset": 143}, {"referenceID": 9, "context": "That is, it may suffer from the dataset bias problem [10].", "startOffset": 53, "endOffset": 57}, {"referenceID": 2, "context": "Empirically, our method outperforms state-of-the-art approaches [3, 9, 11] using the features learned from CNN model in classifying 44 different plant species.", "startOffset": 64, "endOffset": 74}, {"referenceID": 8, "context": "Empirically, our method outperforms state-of-the-art approaches [3, 9, 11] using the features learned from CNN model in classifying 44 different plant species.", "startOffset": 64, "endOffset": 74}, {"referenceID": 10, "context": "Empirically, our method outperforms state-of-the-art approaches [3, 9, 11] using the features learned from CNN model in classifying 44 different plant species.", "startOffset": 64, "endOffset": 74}, {"referenceID": 2, "context": "First, we propose a CNN model to automatically learn the features representation for plant categories, replacing the need of designing hand-crafted features as to previous approaches [3, 9, 12, 13].", "startOffset": 183, "endOffset": 197}, {"referenceID": 8, "context": "First, we propose a CNN model to automatically learn the features representation for plant categories, replacing the need of designing hand-crafted features as to previous approaches [3, 9, 12, 13].", "startOffset": 183, "endOffset": 197}, {"referenceID": 11, "context": "First, we propose a CNN model to automatically learn the features representation for plant categories, replacing the need of designing hand-crafted features as to previous approaches [3, 9, 12, 13].", "startOffset": 183, "endOffset": 197}, {"referenceID": 12, "context": "First, we propose a CNN model to automatically learn the features representation for plant categories, replacing the need of designing hand-crafted features as to previous approaches [3, 9, 12, 13].", "startOffset": 183, "endOffset": 197}, {"referenceID": 13, "context": "The CNN model used in this paper is based on the model proposed in [14] with ILSVRC2012 dataset used for pre-training.", "startOffset": 67, "endOffset": 71}, {"referenceID": 14, "context": "Rather than training a new CNN architecture, we re-used the pre-trained network due to a) recent work [15] reported that features extracted from the activation of a CNN trained in a fully supervised manner on large-scale object recognition works can be re-purposed to a novel generic task; 2) our training set is not large as the ILSVRC2012 dataset.", "startOffset": 102, "endOffset": 106}, {"referenceID": 15, "context": "Indicated in [16], the performance of the CNN model is highly depending on the quantity and the level of diversity of training set, and finally c) training a deep model requires skill and experience.", "startOffset": 13, "endOffset": 17}, {"referenceID": 16, "context": "[17, 18] introduced multi-layered DN that enable us to observe the transformation of the features by projecting the feature maps back to the input pixel space.", "startOffset": 0, "endOffset": 8}, {"referenceID": 17, "context": "[17, 18] introduced multi-layered DN that enable us to observe the transformation of the features by projecting the feature maps back to the input pixel space.", "startOffset": 0, "endOffset": 8}, {"referenceID": 16, "context": "To visualize our CNN model, we employ a strategy named as V1 based on the DN approach [17,18].", "startOffset": 86, "endOffset": 93}, {"referenceID": 17, "context": "To visualize our CNN model, we employ a strategy named as V1 based on the DN approach [17,18].", "startOffset": 86, "endOffset": 93}, {"referenceID": 2, "context": "1%) outperforms state-of-the-art solutions [3,9,11] that employed carefully chosen hand-crafted features even when different classifiers are used.", "startOffset": 43, "endOffset": 51}, {"referenceID": 8, "context": "1%) outperforms state-of-the-art solutions [3,9,11] that employed carefully chosen hand-crafted features even when different classifiers are used.", "startOffset": 43, "endOffset": 51}, {"referenceID": 10, "context": "1%) outperforms state-of-the-art solutions [3,9,11] that employed carefully chosen hand-crafted features even when different classifiers are used.", "startOffset": 43, "endOffset": 51}, {"referenceID": 2, "context": "LeafSnap [3] SVM (RBF) 0.", "startOffset": 9, "endOffset": 12}, {"referenceID": 2, "context": "LeafSnap [3] NN 0.", "startOffset": 9, "endOffset": 12}, {"referenceID": 8, "context": "HCF [9] SVM (RBF) 0.", "startOffset": 4, "endOffset": 7}, {"referenceID": 8, "context": "HCF-ScaleRobust [9] SVM (RBF) 0.", "startOffset": 16, "endOffset": 19}, {"referenceID": 8, "context": "Combine [9] Sum rule (SVM (linear)) 0.", "startOffset": 8, "endOffset": 11}, {"referenceID": 10, "context": "SIFT [11] SVM (linear) 0.", "startOffset": 5, "endOffset": 9}, {"referenceID": 18, "context": "This also agrees with some studies [19, 20] highlighting that quantitative leaf venation data have the potential to revolutionize the plant identification task.", "startOffset": 35, "endOffset": 43}, {"referenceID": 19, "context": "This also agrees with some studies [19, 20] highlighting that quantitative leaf venation data have the potential to revolutionize the plant identification task.", "startOffset": 35, "endOffset": 43}, {"referenceID": 1, "context": "Existing work that had employed venation to perform plant classification are [2, 8, 12, 21, 22].", "startOffset": 77, "endOffset": 95}, {"referenceID": 7, "context": "Existing work that had employed venation to perform plant classification are [2, 8, 12, 21, 22].", "startOffset": 77, "endOffset": 95}, {"referenceID": 11, "context": "Existing work that had employed venation to perform plant classification are [2, 8, 12, 21, 22].", "startOffset": 77, "endOffset": 95}, {"referenceID": 20, "context": "Existing work that had employed venation to perform plant classification are [2, 8, 12, 21, 22].", "startOffset": 77, "endOffset": 95}, {"referenceID": 21, "context": "Existing work that had employed venation to perform plant classification are [2, 8, 12, 21, 22].", "startOffset": 77, "endOffset": 95}], "year": 2015, "abstractText": "This paper studies convolutional neural networks (CNN) to learn unsupervised feature representations for 44 different plant species, collected at the Royal Botanic Gardens, Kew, England. To gain intuition on the chosen features from the CNN model (opposed to a \u2019black box\u2019 solution), a visualisation technique based on the deconvolutional networks (DN) is utilized. It is found that venations of different order have been chosen to uniquely represent each of the plant species. Experimental results using these CNN features with different classifiers show consistency and superiority compared to the state-of-the art solutions which rely on hand-crafted features.", "creator": "LaTeX with hyperref package"}}}