{"id": "1506.09215", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "30-Jun-2015", "title": "Unsupervised Learning from Narrated Instruction Videos", "abstract": "We address the problem of automatically learning the main steps to complete a certain task, such as changing a car tire, from a set of narrated instruction videos. The contributions of this paper are three-fold. First, we develop a joint model for video and natural language narration that takes advantage of the complementary nature of the two signals. Second, we collect an annotated dataset of 57 Internet instruction videos containing more than 350,000 frames for two tasks (changing car tire and CardioPulmonary Resuscitation). Third, we experimentally demonstrate that the proposed model automatically discovers, in an unsupervised manner, the main steps to achieve each task and locate them within the input videos. The results further show that the proposed model outperforms single-modality baselines, demonstrating the benefits of joint modeling video and text.", "histories": [["v1", "Tue, 30 Jun 2015 19:55:37 GMT  (4681kb,D)", "http://arxiv.org/abs/1506.09215v1", "13 pages"], ["v2", "Thu, 2 Jul 2015 16:43:36 GMT  (1200kb,D)", "http://arxiv.org/abs/1506.09215v2", "13 pages - corrected typo in reference [13] + added recently appeared arXiv reference [21]"], ["v3", "Mon, 30 Nov 2015 18:10:53 GMT  (6411kb,D)", "http://arxiv.org/abs/1506.09215v3", "improved NLP method and bigger dataset"], ["v4", "Tue, 28 Jun 2016 18:43:37 GMT  (3190kb,D)", "http://arxiv.org/abs/1506.09215v4", "Appears in: 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR 2016). 21 pages"]], "COMMENTS": "13 pages", "reviews": [], "SUBJECTS": "cs.CV cs.LG", "authors": ["jean-baptiste alayrac", "piotr bojanowski", "nishant agrawal", "josef sivic", "ivan laptev", "simon lacoste-julien"], "accepted": false, "id": "1506.09215"}, "pdf": {"name": "1506.09215.pdf", "metadata": {"source": "CRF", "title": "Learning from narrated instruction videos", "authors": ["Jean-Baptiste Alayrac", "Piotr Bojanowski", "Nishant Agrawal", "IIIT Hyderabad", "Josef Sivic", "Simon Lacoste-Julien"], "emails": [], "sections": [{"heading": "1 Introduction", "text": "Millions of people watch annotated tutorial videos1 to learn new tasks such as assembling IKEA furniture or changing a flat car tire. What if machines could learn these tasks from countless tutorial videos on the Internet? Such an automatic cognitive ability would make it possible to construct virtual assistants and intelligent robots that would help humans accomplish new tasks in unfamiliar situations. In this work, we look at tutorial videos and develop a method that learns a sequence of steps, as well as their textual and visual representations, needed to accomplish a given task. For example, a set of annotated tutorial videos showing how to change a car tire, our method automatically discovers consecutive steps for this task, such as loosening the tutorial steps of the wheel, connecting to the car, removing spare tires, and so on, as illustrated in Figure 1. In addition, the method learns the visual and linguistic variability of these natural steps from video filaments."}, {"heading": "2 Related work", "text": "In fact, most of them will be able to put themselves in a situation where they have to put themselves centre stage."}, {"heading": "3 Modeling narrated instruction videos", "text": "An input video n consists of a video stream of Tn frames (xnt) Tn t = 1 and an audio stream containing a detailed verbal description of the task presented. We assume that the audio description has been transcribed into raw text and then processed into a sequence of Sn bookmarks (dns) Sn s = 1. Given this data, we would like to automatically restore the sequence of K main steps that make up the given task, and locate each step within each input video and text transcription. We formulate the problem as two cluster tasks, one for each input modality, linked by common constraints. This is achieved by solving the following minimization problem: min R-R, Z-Zg (R) + h (Z).t. c (R, Z) \u2264 0, (Z)."}, {"heading": "3.1 Clustering transcribed verbal instructions", "text": "The goal is to turn the transcribed verbal descriptions of each individual video into a sequence of main steps necessary to accomplish the task. We assume that the important steps for many transcripts are common and that the sequence of steps (roughly) is retained in all transcripts. To overcome this challenge, we take advantage of the fact that the execution of a particular task usually involves interactions with objects or persons, and therefore we can stream a structured representation of the input text. More specifically, we present the textual data as a sequence of direct object relationships."}, {"heading": "3.2 Discriminative clustering of videos", "text": "We formulate the problem of clustering N-input video streams (xt) into a sequence of K-steps as a discriminatory clustering task with sequence constraints. We represent each time interval with a d-dimensional feature vector. To simplify notation and implementation, we use an additional constant bias feature on a large value so that it is not regulated. The feature vectors for the n-th video are stacked in a Tn-D design matrix of Xn. The goal is to find an optimal mapping matrix Zn."}, {"heading": "3.3 Joint constraints between verbal descriptions and video", "text": "We want to encode the constraints that link the clustering of transcripts (Section 3.1) to the clustering of videos (Section 3.2). We use two types of constraints to link the two clustering tasks; the first constraint encodes the fact that the clustered execution steps occur simultaneously in both the transcript and the video when they are talking about it; this is a common constraint that strongly links the mapping of indicator matrices R and Z of the two clustering tasks; the second constraint encodes the fact that the clustered execution steps must respect a global sequence order in both the transcript and the video. This is an independent constraint on indicator matrices R and Z of the two clustering tasks. However, since the two mapping matrices are linked, this leads to a single global order common to both clustering. We give details of the types of constraints people say what they do."}, {"heading": "4 Experimental evaluation", "text": "In this section, we first describe our new annotated dataset of instruction videos, followed by the details of the text and video functions, and then we present results divided into three experiments. Sec. 4.1 evaluates the quality of the steps extracted from video arrations. In Sec. 4.2, we evaluate the temporal localization of steps in instruction videos using constraints derived from the text. Finally, we evaluate the usefulness of the visual signal for grouping synonymous text descriptions that represent the same step. We have collected a dataset of instruction videos for two tasks: Changing car tire and Performing cardiopulmonary resuscitation (CPR) by searching videos with relevant keywords. We have selected videos with automatic speech recognition from YouTube (ASR). We focus on the main problem in this paper, we have manually corrected errors and punctuations from ASR."}, {"heading": "4.1 Results of step discovery from text narrations", "text": "The results of discovering the most important steps for each task from text narratives are presented in Table 1. We report the results of the multiple sequence alignment described in paragraph 3.1 using different values of parameter K (the maximum number of steps detected). We illustrate the restored sequences of steps, each step being represented by the most common direct object relationship. The restored sequences are largely correct for both tasks. We also see that by increasing K we restore more complete sequences at the expense of occasional repetitions, e.g. lower wagon and lower jack referring to the same step. For CPR and large K, the method restores fine-grain steps, e.g. tilting head, lifting sense and pinching nose, which were not taken into account in our original definition. To quantify performance, we measure precision as the percentage of correctly recovered steps that appear in correct order. \"We also measure the percentage of the truth of the steps recovered from the table 1."}, {"heading": "4.2 Results of localizing instruction steps in video", "text": "This year, the time has come for a decision to be taken in the next few days."}, {"heading": "4.3 Using visual cues to improve text clustering", "text": "The motivation here is to assess whether we can improve the clustering of texts with the results of the vision. Let's remember that to maximize the precision of our cluster direct objects, we are using the improved model based on Z [4], which does not require a \"background class\" and produces a stronger base model without the narration constraints."}, {"heading": "5 Conclusion", "text": "We present a method to discover the key steps of YouTube annotated tutorial videos. We formulate the method as a common cluster problem of video and annotated text, and demonstrate the benefits of this common approach to two tasks. Next, we plan to expand our method to include more tasks and model the person-object interactions in video and text. Integration of automatic speech recognition into our common framework could also be beneficial."}, {"heading": "6 Acknowledgments", "text": "This research was partially supported by a Google Research Award, and the ERC supports VideoWorld (N \u00b0 267907), Activia (N \u00b0 307574) and LEAP (N \u00b0 336845)."}], "references": [], "referenceMentions": [], "year": 2017, "abstractText": "We address the problem of automatically learning the main steps to complete a<lb>certain task, such as changing a car tire, from a set of narrated instruction videos.<lb>The contributions of this paper are three-fold. First, we develop a joint model for<lb>video and natural language narration that takes advantage of the complementary<lb>nature of the two signals. Second, we collect an annotated dataset of 57 Internet in-<lb>struction videos containing more than 350,000 frames for two tasks (changing car<lb>tire and CardioPulmonary Resuscitation). Third, we experimentally demonstrate<lb>that the proposed model automatically discovers, in an unsupervised manner, the<lb>main steps to achieve each task and locate them within the input videos. The re-<lb>sults further show that the proposed model outperforms single-modality baselines,<lb>demonstrating the benefits of joint modeling video and text.", "creator": "LaTeX with hyperref package"}}}