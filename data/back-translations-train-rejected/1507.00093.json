{"id": "1507.00093", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "1-Jul-2015", "title": "A Study of Gradient Descent Schemes for General-Sum Stochastic Games", "abstract": "Zero-sum stochastic games are easy to solve as they can be cast as simple Markov decision processes. This is however not the case with general-sum stochastic games. A fairly general optimization problem formulation is available for general-sum stochastic games by Filar and Vrieze [2004]. However, the optimization problem there has a non-linear objective and non-linear constraints with special structure. Since gradients of both the objective as well as constraints of this optimization problem are well defined, gradient based schemes seem to be a natural choice. We discuss a gradient scheme tuned for two-player stochastic games. We show in simulations that this scheme indeed converges to a Nash equilibrium, for a simple terrain exploration problem modelled as a general-sum stochastic game. However, it turns out that only global minima of the optimization problem correspond to Nash equilibria of the underlying general-sum stochastic game, while gradient schemes only guarantee convergence to local minima. We then provide important necessary conditions for gradient schemes to converge to Nash equilibria in general-sum stochastic games.", "histories": [["v1", "Wed, 1 Jul 2015 02:37:33 GMT  (33kb)", "http://arxiv.org/abs/1507.00093v1", null]], "reviews": [], "SUBJECTS": "cs.LG cs.GT", "authors": ["h l prasad", "shalabh bhatnagar"], "accepted": false, "id": "1507.00093"}, "pdf": {"name": "1507.00093.pdf", "metadata": {"source": "CRF", "title": "A Study of Gradient Descent Schemes for General-Sum Stochastic Games", "authors": ["H. L. Prasad", "Shalabh Bhatnagar"], "emails": ["hlprasu@csa.iisc.ernet.in", "shalabh@csa.iisc.ernet.in"], "sections": [{"heading": null, "text": "ar Xiv: 150 7.00 093v 1 [cs.L G] 1Zero-sum stochastic games are easy to solve as they can be dropped as simple Markov decision processes, but this is not the case with general stochastic games. For general stochastic games of Filar and Vrieze [2004], a fairly general formulation of the optimization problem is available. However, the optimization problem there has a nonlinear objective and nonlinear constraints with a special structure. Since both the gradients of the lens and the limitations of this optimization problem are well defined, gradient schemes seem to be a natural choice. We are discussing a gradient scheme that is adapted to stochastic games with two players. We show in simulations that this scheme actually converges to a Nash equilibrium, because a simple terrain exploration problem modeled as a general stochastic game turns out to correspond only to a global minimum problem optimization scheme."}, {"heading": "1 Introduction", "text": "In fact, it is not the case that one is able to behave in a way that is common in other countries of the world. (...) It is not the case that one is able to outdo oneself. (...) It is not the case that one is able to outdo oneself. (...) It is not the case that one is able to outdo oneself. (...) It is not the case that one is able to outdo oneself. (...) It is not the case that one is able to outdo oneself. (...) It is not the case that one is able to outdo oneself. (...) It is not the case that one is able to outdo oneself. (...) It is not the case that one is able to outdo oneself. (...) It is not the case that one is able to outdo oneself. \""}, {"heading": "2 The Optimization Problem", "text": "A basic idea of the optimization problem is given in Section 2.2. The complete optimization problem is then formulated in Section 2.3 for setting the infinite horizon with discounted rewards. Subsequently, some important results of Filar and Vrieze [2004] are described, which are applicable here."}, {"heading": "2.1 Stochastic Games", "text": "We assume that the stochastic game ends in a finite but random time. (i) S denotes the discounted value frame in dynamic programming. (ii) Ai (x) denotes the action space for the ith agent, i = 1, 2. A (x) = 2 \u00d7 i = 1Ai (x), the Cartesian product, is the aggregate action space consisting of all possible actions of both agents, if the state of the game is described in the description. (x) = 1Ai (x), the Cartesian product, is the aggregate action space consisting of all possible actions of both agents. (iii) S (x) p = 1Ai (x), is the aggregate action space in which the action space for the ith agent, i = 1Ai (x), the aggregate action space consisting of all possible actions of both agents."}, {"heading": "2.2 The Basic Formulation", "text": "The dynamic programming equation (2) for determining optimal values can now be revised to: vi (x) = max \u03c0i (x) (Ai (x))) {E\u03c0i (x) Q i (x, ai)}, \u0435x-S (i = 1, 2, (3), where Qi (x, ai) = E\u03c0 \u2212 i (x) ri (x, a) + \u03b2-y-U (x) p (y | x, a) vi (y) represents the limit value associated with the pick action ai-Ai (x), in the state x-S for agent i. Furthermore \u0445 (Ai (x) denotes the set of all possible probability distributions over Ai (x). A possible optimization problem is derived (3) in section 2.2.1, followed by a discussion of possible limitations for the feasible solutions in section 2.2.2."}, {"heading": "2.2.1 The objective", "text": "The equation (3) states that vi (x) maps the maximum value of E\u03c0iQi (x, ai) across all possible convex combinations of the policy of agent i, \u03c0i (x). However, neither the optimal value vi (x) nor the optimal policy \u03c0i (x, ai) is known beyond all possible strategies. Thus, a possible optimization goal would bef i (vi, \u03c0i) = vid x x (x) depend on the strategies of all other actors. So, an isolated minimization of f i (vi, i) would really not make sense. Rather, we have to weigh the aggregate objective, f (v, \u03c0) = 2 x (vi, i), against the strategies of all other actors."}, {"heading": "2.2.2 Constraints", "text": "The basic optimization problem (4) has only a set of simple constraints that ensure that \u03c0 remains a valid strategy. As shown in dilemma 2.2, this optimization problem is not sufficient to accurately represent the Nash equilibrium of the underlying general sum discount stochastic game. At this point, we will consider a possible set of additional constraints that could make the optimization problem more useful. Note that the term maximized in Equation (3), i.e. E\u03c0iQ i (x, ai), is a convex combination of the values of Qi (x, ai) across all possible actions ai-Ai (x) in a given state x-S for a given agent i. It implicitly implies that Qi (x, ai) \u2264 vi (x), \u0445ai-Ai (x), x-S, i = 1, 2,."}, {"heading": "2.3 Optimization Problem for two-player Stochastic Games", "text": "One optimization package on similar lines as in section 2.2, for one two-player total sum (discounted) (2), was given by Filar and Vrieze [2004]. The optimization package is as follows: Each player (2) has x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x x-x-x-x-x-x x"}, {"heading": "2.4 Theoretical Results on the Optimization Problem", "text": "The optimization problem described above is applicable to the general sum equilibrium of stochastic games. [Filar and Vrieze, 2004, theorems 3.8.1-3.8.3] are listed below as theorems 2.3-2.5. See [Filar and Vrieze, 2004, pp. 130-132] for proof of these results. Theorem 2.3 In a general, discounted stochastic game, there is a Nash equilibrium in stationary strategies. Theorem 2.4 Let us consider a tuple (v, \u03c0). The strategy \u03c0 forms a Nash equilibrium for the general discount game if and only if (v, \u03c0) the global minimum of the optimization problem is f (v, \u03c0) = 0.Thus, the optimization problem defined in (7) has at least a global optimum that has a value of zero that correlates to the Nash equilibrium. Theorreliem 2.5 (this equals an optimization equilibrium problem smaller than (7)."}, {"heading": "3 A Gradient Descent Scheme", "text": "The optimization problem (7) for two-player general-sum stochastics games has an interesting structure, in which only cross-products between optimization variables appear in both objective function and constraints. As the first naive way to handle this optimization problem, we see whether the same can be divided into minor problems via a uni-variable type scheme [Rao, 1996, Section 5.4, pp. 350]. It is possible to see that the original problem can be divided into two sets of linear optimization problems, with (i) the first set having two optimization problems in v1 and v2 separately. Here, \u03c0 is kept constant; and (ii) the second point, which has one in 1 (x), \u03c02 (x) for each possible state x S. In each of these cases, v is kept constant, so that the original problem can be solved with a uni-variable type of manufactures."}, {"heading": "3.1 Difficulties", "text": "We note that the optimization problem (7) presents the following difficulties: 1. Dimensionality - The number of variables and constraints that accompany the optimization problem is large. In the two-agent scenario, it can be shown that the number of variables is twice as high as the sum of the cardinalities of the state and the spaces of action. For example, the total number of inequality constraints for the same terrain for a simple 4 \u00d7 4 terrain with two actors and two objects can also be calculated as follows: (4169 \u00d7 2) = 16676.2. Non-convexity - The constraint region in the optimization problem is not necessarily convex. In fact, the total number of inequality constraints for the same terrain is also (4169 \u00d7 2) + (4169 \u00d7 2) = 16676.2."}, {"heading": "3.2 The Herskovits Algorithm", "text": "We observed that in the optimization problem (7), the steepest pedigree directions are most often directed against the active boundaries (1) (1). Therefore, a steepest pedigree direction cannot be used because it would be stuck at such a boundary point, which may not be an optimal point. Herskovit's method offers two features that address this problem: (1) The search direction, which is selected at each point while it is a strictly descending direction, uses knowledge of the gradations of the constraints as well as the gradients of the object. (2) The procedure is strictly practicable, i.e., at any point that is currently best practicable, it does not touch any boundaries. (Assumptions) required for the two-step method are as follows: (i) The feasible region has an inner boundary and is equal to closing the boundary. (i.e.) Each point is equal."}, {"heading": "3.3 Initial Feasible Point", "text": "The optimization problem specified in (7) shows a clear separation between strategy probability concepts and value vector concepts. This can be exploited to find an initial workable solution using the following procedure: Firstly, a workable strategy is selected, for example a uniform strategy, \u03c00 = \u03c0i0: i = 1, 2 with\u03c0i0 (x, a) = 1mi (x). \u2212 For the Herskovits algorithm, a strict inner point is sought to begin with. That is, the starting point for the algorithm must (7) be divided into two linear programming problems in v1 and v2, each in (13). \u2212 For the Herskovits algorithm, a strict inner point is sought to begin with."}, {"heading": "3.4 Sparsity", "text": "The two-stage matrix matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix"}, {"heading": "3.5 Computing the Optimal Step-length", "text": "The objective function has previously proved to be cubic and quadratic constraints in the optimization variables, and this structure can be exploited to find the optimal stride length t \u0445 in any desired direction."}, {"heading": "3.5.1 Optimal Step Length, t\u2217", "text": "Let (v0, \u03c00) be the current point and (v, \u03c0) the next point calculated from the previous one by taking a step along the descending direction. Thus, you get, f (v, \u03c0) = f (v0 + tSv, \u03c00 + tSv) = d0 + d1t + d2t 2 + d3t 3 (15) where 0 = 2 x i = 11T | S | {vi0 \u2212 r i (\u03c00) \u2212 \u03b2P (\u03c00) v i 0}, d1 = 2 x i = 11T 3 (15), where 0 = 2 x i = 11T | S | {vi0 \u2212 r i (\u03c00) \u2212 \u03b2P (\u03c00) \u2212 \u03b2P (\u03c00) v i \u00b2 is the descending value."}, {"heading": "3.5.2 Constraints on step-length, t", "text": "The constraints in the optimization problem (7) impose limits on the possible values that t can take (\u03b2). Consider the imbalance constraint (7 (a)) for a certain a1 (x). Let's leave gj (\u00b7) \u2264 0 represent one of these constraints and let \u03b4i calculate the corresponding parameter (in step 6 of the Herskovits method (algorithm 1). Apart from the feasibility of the step size, we also want to make sure that the condition (11), i.e., gj (v, \u03c0) \u2264 jgj (v0), is replaced by ltys (0)."}, {"heading": "3.5.3 Selection of the optimal step length, t\u2217", "text": "Along the chosen downward direction S, the objective function f (v, \u03c0) is a cubic function in the step length. If the extreme points are real and both are positive, then the first partial point in the downward direction is a minimum point and the next a maximum point, as shown in Figure 1. Under this condition, the best point is obtained by finding the best between the minimum point (or two possible points near the minimum point) and the maximum step point determined by the constraints. Otherwise, the cubic curve would be like the dashed curve in Figure 1. In such a case, the optimal step length is simply the maximum possible step length."}, {"heading": "3.5.4 Optimal Step Length Algorithm", "text": "Algorithm 4. Step Length Calculation Parameter: \u03b2: Discount factor Input: (v0, \u03c00): Current value strategy pair Input: S: Selected descent direction Output: t: The best step length1. Calculate d1, d2 and d3 with (16). 2. (t1, t2) \u2190 Roots (d1, 2d2, 3d3) 3. F \u2190 +, the set of all non-negative real numbers for x-S, a1, a1, A1 (x), a2, A2 (x), i = 1, 2 do4. F \u2190 F-Square feasible (bi (x, ai), ci (x, ai), di (x, ai))), where bi (x, ai), ci (x, ai), di (x, ai), di (x, ai), ai (x, ai), di (x, ai), best of (18).5 F-Square (0), feasible."}, {"heading": "3.6 The Complete Algorithm", "text": "Algorithm 5. The complete algorithm parameter: \u03b2: Discounting factor Input: \u03c00: Initial strategy (off (12)) Output: < v *, \u03c0 *: A balanced equilibrium with the revised Simplex method (see Section 3.3). Let's start with loop4. Calculate the feasible direction S with the two-step feasibility direction method (algorithm 1). 5. Terminate the algorithm if S = 0. < v *, if the results of the revised Simplex method (see Section 3.3). Start with loop4. Calculate the feasible direction S with the two-step feasibility direction method (algorithm 1)."}, {"heading": "3.7 Convergence to a KKT point", "text": "The KKT conditions represent a set of necessary and sufficient conditions for a point to be a valid local minimum of an optimization problem. We write down the necessary conditions for a point < v \u0445, \u03c0 \u0445 > to be a local minimum of the optimization problem (7): (a), (v *, \u03c0 *) +, (n), (c) gj (v *, \u03c0 *) = 0, (v *, \u03c0 *) = 0, (j *, \u03c0 *) = 0, j = 1, 2,...., n, (c) gj (v *, \u03c0) \u2265 0, j = 1, 2,.., n, (20), (j = 1, 2,.., n, are the Lagrange multipliers associated with the constraints, gj (v, \u03c0)."}, {"heading": "4 A Simple Terrain Exploration Problem", "text": "A simplified part of the general terrain exploration problem is represented by a part of the object that represents a particular object, whether it is in a particular state or not."}, {"heading": "4.1 Simulation Results", "text": "The simulation results for G = {0, 1, 2, 3} with two objects at (0, 3) and (3, 3) and discount factor \u03b2 = 0.75 are described below. Parameters of the two-stage, feasible direction method are wj (v0, \u03c00) = 1, j = 1, 2,..., n, \u03b1 = 0.5 and \u03c10 = 0.9."}, {"heading": "4.1.1 Objective Value", "text": "The convergence of the objective value using algorithm 5 to a value close to zero is shown in Figure 2. After a first workable solution was found, the objective value \u2248 102.37."}, {"heading": "4.1.2 Strategies", "text": "The convergence behavior of the strategies of both agents with the starting position of the first agent is (2,1) and that of the second being (2,0) is respectively in Figures 3 and 4. The arrows in the various grids in Figures 3 and 4 indicate the feasible actions in each state and their lengths are proportional to the transition probabilities along the corresponding directions. At the given starting positions of the agents and object positions, strategies are plotted that refer only to those positions that an agent can visit with the other agent who is adhering to his own position. Consider, for example, Figure 3. The figure shows the strategy of the first agent with the second agent who is adhering to the position (2,0). At the beginning of the algorithm, all transition probabilities are selected according to the even distribution. In Figures 3 and 4, we show the strategy profile of both agents after the first, 11th and 100th iterations, and in convergence of the algorithm, which do not converge in Figure 3."}, {"heading": "5 Non-Convergence to a Nash Equilibrium", "text": "Theorem 2.4 showed that it is both necessary and sufficient to reach a viable point < v *, p *, p > to correspond to a Nash equilibrium if the objective value, f (v *, p *) = 0. However, for a gradient-based scheme it would be appropriate to have presented conditions relating to gradients of objective and constraints. In this direction, we now present a series of results that ultimately represent the desired number of necessary and sufficient conditions for a minimum point. At a certain point < v, p >, let G = [... gj (v, p): j = 1, 2,. N] represent a matrix whose columns gradients are all constraints (8).Proposition 5.7 At any given point < v, p >, the gradients of the objective function f (v, p), we can be expressed as a linear combination of the gradients of all constraints."}, {"heading": "6 Conclusion", "text": "During the construction of the scheme, we discussed the general nature of the indeterminate objective and non-convex constraints, which illustrate the fact that a simple steepest descent algorithm cannot even converge to a local minimum of the optimization problem. The proposed scheme takes these problems into account when designing both (i) viable search direction and (ii) optimal stride length. We showed that the proposed scheme approaches a KKT point of the optimization problem by appropriately using economy techniques for an associated matrix inversion, which was considered sufficient in simulations carried out for the exemplary problem of terrain sensing. However, in general, in Section 5, we showed that it is not sufficient for a scheme to converge to a KKT point of the optimization problem."}], "references": [{"title": "Multi-agent reinforcement learning: Algorithm converging to Nash equilibrium in general-sum stochastic games", "author": ["N. Akchurina"], "venue": "In 8th International Conference on Autonomous Agents and Multi-agent Systems,", "citeRegEx": "Akchurina.,? \\Q2009\\E", "shortCiteRegEx": "Akchurina.", "year": 2009}, {"title": "Dynamic Programming and Optimal Control", "author": ["D.P. Bertsekas"], "venue": "Athena Scientific,", "citeRegEx": "Bertsekas.,? \\Q1995\\E", "shortCiteRegEx": "Bertsekas.", "year": 1995}, {"title": "Neuro-Dynamic Programming", "author": ["D.P. Bertsekas", "J.N. Tsitsiklis"], "venue": "Athena Scientific,", "citeRegEx": "Bertsekas and Tsitsiklis.,? \\Q1996\\E", "shortCiteRegEx": "Bertsekas and Tsitsiklis.", "year": 1996}, {"title": "A user\u2019s guide to solving dynamic stochastic games using the homotopy method", "author": ["R.N. Borkovsky", "U. Doraszelski", "Y. Kryukov"], "venue": "Operations Research,", "citeRegEx": "Borkovsky et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Borkovsky et al\\.", "year": 2010}, {"title": "On the computation of equilibria in discounted stochastic dynamic games", "author": ["M. Breton", "J.A. Filar", "A. Haurie", "T.A. Schultz"], "venue": "Dynamic Games and Applications in Economics, Springer-Verlag Lecture Notes in Mathematical and Economic Systems,", "citeRegEx": "Breton et al\\.,? \\Q1986\\E", "shortCiteRegEx": "Breton et al\\.", "year": 1986}, {"title": "LDL, a Concise Sparse Cholesky Package", "author": ["T.A. Davis"], "venue": "URL http://www.cise.ufl. edu/research/sparse/ldl", "citeRegEx": "Davis.,? \\Q2007\\E", "shortCiteRegEx": "Davis.", "year": 2007}, {"title": "Competitive Markov Decision Processes. Springer-Verlag, New York, Inc., 1 edition, November", "author": ["J. Filar", "K. Vrieze"], "venue": null, "citeRegEx": "Filar and Vrieze.,? \\Q2004\\E", "shortCiteRegEx": "Filar and Vrieze.", "year": 2004}, {"title": "Stationary equilibria in stochastic games: Structure, selection, and computation", "author": ["P. Herings", "R.J.A.P. Peeters"], "venue": "Journal of Economic Theory,", "citeRegEx": "Herings and Peeters.,? \\Q2004\\E", "shortCiteRegEx": "Herings and Peeters.", "year": 2004}, {"title": "Homotopy methods to compute equilibria in game theory", "author": ["P.J. Herings", "R. Peeters"], "venue": "Research Memoranda 046, Maastricht : METEOR, Maastricht Research School of Economics of Technology and Organization,", "citeRegEx": "Herings and Peeters.,? \\Q2006\\E", "shortCiteRegEx": "Herings and Peeters.", "year": 2006}, {"title": "A two-stage feasible directions algorithm for nonlinear constrained optimization", "author": ["J. Herskovits"], "venue": "In Mathematical Programming", "citeRegEx": "Herskovits.,? \\Q1986\\E", "shortCiteRegEx": "Herskovits.", "year": 1986}, {"title": "Multiagent reinforcement learning: Theoretical framework and an algorithm", "author": ["J. Hu", "M.P. Wellman"], "venue": "In Proc. 15th International Conf. on Machine Learning,", "citeRegEx": "Hu and Wellman.,? \\Q1999\\E", "shortCiteRegEx": "Hu and Wellman.", "year": 1999}, {"title": "Nash q-learning for general-sum stochastic games", "author": ["J. Hu", "M.P. Wellman"], "venue": "In Journal of Machine Learning Research,", "citeRegEx": "Hu and Wellman.,? \\Q2003\\E", "shortCiteRegEx": "Hu and Wellman.", "year": 2003}, {"title": "Friend-or-Foe Q-Learning in General Sum Games", "author": ["M.L. Littman"], "venue": "In Proceedings of the 18th International Conference on Machine Learning,", "citeRegEx": "Littman.,? \\Q2001\\E", "shortCiteRegEx": "Littman.", "year": 2001}, {"title": "Solving stochastic games", "author": ["L. Mac Dermed", "C.L. Isbell"], "venue": "Advances in Neural Information Processing Systems,", "citeRegEx": "Dermed and Isbell.,? \\Q2009\\E", "shortCiteRegEx": "Dermed and Isbell.", "year": 2009}, {"title": "Equilibrium points in n-person games", "author": ["J.F. Nash"], "venue": "Proceedings of the national academy of sciences,", "citeRegEx": "Nash.,? \\Q1950\\E", "shortCiteRegEx": "Nash.", "year": 1950}, {"title": "Engineering Optimization: Theory and Practice", "author": ["S.S. Rao"], "venue": "New Age International (P) Ltd.,", "citeRegEx": "Rao.,? \\Q1996\\E", "shortCiteRegEx": "Rao.", "year": 1996}, {"title": "Stochastic games", "author": ["L.S. Shapley"], "venue": "In Proceedings of the National Academy of Sciences,", "citeRegEx": "Shapley.,? \\Q1953\\E", "shortCiteRegEx": "Shapley.", "year": 1953}, {"title": "Nash convergence of gradient dynamics in general-sum games", "author": ["S. Singh", "M. Kearns", "Y. Mansour"], "venue": "In Proc. of the 16th Conference on Uncertainty in Artificial Intelligence,", "citeRegEx": "Singh et al\\.,? \\Q2000\\E", "shortCiteRegEx": "Singh et al\\.", "year": 2000}], "referenceMentions": [{"referenceID": 6, "context": "A fairly general optimization problem formulation is available for general-sum stochastic games by Filar and Vrieze [2004]. However, the optimization problem there has a non-linear objective and non-linear constraints with special structure.", "startOffset": 99, "endOffset": 123}, {"referenceID": 11, "context": "Since the seminal work of Shapley [1953], stochastic games have been an important class of models for multi-agent systems.", "startOffset": 26, "endOffset": 41}, {"referenceID": 5, "context": "A comprehensive treatment of stochastic games under various payoff criteria is given by Filar and Vrieze [2004]. Many interesting problems like fishery games, advertisement games, etc.", "startOffset": 88, "endOffset": 112}, {"referenceID": 5, "context": "A comprehensive treatment of stochastic games under various payoff criteria is given by Filar and Vrieze [2004]. Many interesting problems like fishery games, advertisement games, etc., can be modelled as stochastic games, see Filar and Vrieze [2004]. One of the significant results is that every stochastic game has a Nash equilibrium and it can be characterised in terms of global minima of a suitable mathematical programming problem.", "startOffset": 88, "endOffset": 251}, {"referenceID": 5, "context": "A comprehensive treatment of stochastic games under various payoff criteria is given by Filar and Vrieze [2004]. Many interesting problems like fishery games, advertisement games, etc., can be modelled as stochastic games, see Filar and Vrieze [2004]. One of the significant results is that every stochastic game has a Nash equilibrium and it can be characterised in terms of global minima of a suitable mathematical programming problem. As an application of general-sum games to the multi-agent scenario, Singh et al. [2000] observed that in a two-agent iterated general-sum game, Nash convergence is assured either in strategies or in the very least in average payoffs.", "startOffset": 88, "endOffset": 526}, {"referenceID": 5, "context": "A comprehensive treatment of stochastic games under various payoff criteria is given by Filar and Vrieze [2004]. Many interesting problems like fishery games, advertisement games, etc., can be modelled as stochastic games, see Filar and Vrieze [2004]. One of the significant results is that every stochastic game has a Nash equilibrium and it can be characterised in terms of global minima of a suitable mathematical programming problem. As an application of general-sum games to the multi-agent scenario, Singh et al. [2000] observed that in a two-agent iterated general-sum game, Nash convergence is assured either in strategies or in the very least in average payoffs. Later by Hu and Wellman [1999], stochastic game theory was observed to be a better framework for multi-agent scenarios as it could be viewed as an extension of the well studied Markov decision theory (see Bertsekas [1995]).", "startOffset": 88, "endOffset": 703}, {"referenceID": 1, "context": "Later by Hu and Wellman [1999], stochastic game theory was observed to be a better framework for multi-agent scenarios as it could be viewed as an extension of the well studied Markov decision theory (see Bertsekas [1995]).", "startOffset": 205, "endOffset": 222}, {"referenceID": 2, "context": "Hu and Wellman [1999] proposed an interesting Q-learning algorithm, that is based on reinforcement learning (see Bertsekas and Tsitsiklis [1996]).", "startOffset": 0, "endOffset": 22}, {"referenceID": 0, "context": "Hu and Wellman [1999] proposed an interesting Q-learning algorithm, that is based on reinforcement learning (see Bertsekas and Tsitsiklis [1996]).", "startOffset": 113, "endOffset": 145}, {"referenceID": 0, "context": "Hu and Wellman [1999] proposed an interesting Q-learning algorithm, that is based on reinforcement learning (see Bertsekas and Tsitsiklis [1996]). However, their algorithm assures convergence only if the game has exactly one Nash equilibrium. An extension to the above algorithm called NashQ was proposed by Hu and Wellman [2003] which showed improvement in performance.", "startOffset": 113, "endOffset": 330}, {"referenceID": 0, "context": "Hu and Wellman [1999] proposed an interesting Q-learning algorithm, that is based on reinforcement learning (see Bertsekas and Tsitsiklis [1996]). However, their algorithm assures convergence only if the game has exactly one Nash equilibrium. An extension to the above algorithm called NashQ was proposed by Hu and Wellman [2003] which showed improvement in performance. However, convergence in a scenario with multiple Nash equilibria was not addressed. Another noteworthy work is of Littman [2001] who proposed a friend-or-foe Q-learning (FFQ) algorithm as an improvement over the NashQ algorithm with assured convergence, though not necessarily to a Nash equilibrium.", "startOffset": 113, "endOffset": 500}, {"referenceID": 0, "context": "Hu and Wellman [1999] proposed an interesting Q-learning algorithm, that is based on reinforcement learning (see Bertsekas and Tsitsiklis [1996]). However, their algorithm assures convergence only if the game has exactly one Nash equilibrium. An extension to the above algorithm called NashQ was proposed by Hu and Wellman [2003] which showed improvement in performance. However, convergence in a scenario with multiple Nash equilibria was not addressed. Another noteworthy work is of Littman [2001] who proposed a friend-or-foe Q-learning (FFQ) algorithm as an improvement over the NashQ algorithm with assured convergence, though not necessarily to a Nash equilibrium. Moreover, the FFQ algorithm is applicable to a restricted class of games where either full co-operation between agents is ensured or the game is zero-sum. Algorithms for some specific cases of stochastic games such as Additive Reward and Additive Transition (AR-AT) games are discussed by Filar and Vrieze [2004] as well as Breton et al.", "startOffset": 113, "endOffset": 984}, {"referenceID": 0, "context": "Hu and Wellman [1999] proposed an interesting Q-learning algorithm, that is based on reinforcement learning (see Bertsekas and Tsitsiklis [1996]). However, their algorithm assures convergence only if the game has exactly one Nash equilibrium. An extension to the above algorithm called NashQ was proposed by Hu and Wellman [2003] which showed improvement in performance. However, convergence in a scenario with multiple Nash equilibria was not addressed. Another noteworthy work is of Littman [2001] who proposed a friend-or-foe Q-learning (FFQ) algorithm as an improvement over the NashQ algorithm with assured convergence, though not necessarily to a Nash equilibrium. Moreover, the FFQ algorithm is applicable to a restricted class of games where either full co-operation between agents is ensured or the game is zero-sum. Algorithms for some specific cases of stochastic games such as Additive Reward and Additive Transition (AR-AT) games are discussed by Filar and Vrieze [2004] as well as Breton et al. [1986]. A new type of approach based on homotopy is proposed by Herings and Peeters [2004].", "startOffset": 113, "endOffset": 1016}, {"referenceID": 0, "context": "Hu and Wellman [1999] proposed an interesting Q-learning algorithm, that is based on reinforcement learning (see Bertsekas and Tsitsiklis [1996]). However, their algorithm assures convergence only if the game has exactly one Nash equilibrium. An extension to the above algorithm called NashQ was proposed by Hu and Wellman [2003] which showed improvement in performance. However, convergence in a scenario with multiple Nash equilibria was not addressed. Another noteworthy work is of Littman [2001] who proposed a friend-or-foe Q-learning (FFQ) algorithm as an improvement over the NashQ algorithm with assured convergence, though not necessarily to a Nash equilibrium. Moreover, the FFQ algorithm is applicable to a restricted class of games where either full co-operation between agents is ensured or the game is zero-sum. Algorithms for some specific cases of stochastic games such as Additive Reward and Additive Transition (AR-AT) games are discussed by Filar and Vrieze [2004] as well as Breton et al. [1986]. A new type of approach based on homotopy is proposed by Herings and Peeters [2004]. In this approach, a homotopic path between equilibrium points of N independent MDPs and the N -player stochastic game in question, is traced numerically giving a Nash equilibrium point of the stochastic game of interest.", "startOffset": 113, "endOffset": 1100}, {"referenceID": 0, "context": "Hu and Wellman [1999] proposed an interesting Q-learning algorithm, that is based on reinforcement learning (see Bertsekas and Tsitsiklis [1996]). However, their algorithm assures convergence only if the game has exactly one Nash equilibrium. An extension to the above algorithm called NashQ was proposed by Hu and Wellman [2003] which showed improvement in performance. However, convergence in a scenario with multiple Nash equilibria was not addressed. Another noteworthy work is of Littman [2001] who proposed a friend-or-foe Q-learning (FFQ) algorithm as an improvement over the NashQ algorithm with assured convergence, though not necessarily to a Nash equilibrium. Moreover, the FFQ algorithm is applicable to a restricted class of games where either full co-operation between agents is ensured or the game is zero-sum. Algorithms for some specific cases of stochastic games such as Additive Reward and Additive Transition (AR-AT) games are discussed by Filar and Vrieze [2004] as well as Breton et al. [1986]. A new type of approach based on homotopy is proposed by Herings and Peeters [2004]. In this approach, a homotopic path between equilibrium points of N independent MDPs and the N -player stochastic game in question, is traced numerically giving a Nash equilibrium point of the stochastic game of interest. Their result applies to both normal form as well as extensive form games. However, this approach has a complexity similar to that of typical gradient descent schemes discussed in this paper. For more recent developments in this direction, see the work by Herings and Peeters [2006] and Borkovsky et al.", "startOffset": 113, "endOffset": 1604}, {"referenceID": 0, "context": "Hu and Wellman [1999] proposed an interesting Q-learning algorithm, that is based on reinforcement learning (see Bertsekas and Tsitsiklis [1996]). However, their algorithm assures convergence only if the game has exactly one Nash equilibrium. An extension to the above algorithm called NashQ was proposed by Hu and Wellman [2003] which showed improvement in performance. However, convergence in a scenario with multiple Nash equilibria was not addressed. Another noteworthy work is of Littman [2001] who proposed a friend-or-foe Q-learning (FFQ) algorithm as an improvement over the NashQ algorithm with assured convergence, though not necessarily to a Nash equilibrium. Moreover, the FFQ algorithm is applicable to a restricted class of games where either full co-operation between agents is ensured or the game is zero-sum. Algorithms for some specific cases of stochastic games such as Additive Reward and Additive Transition (AR-AT) games are discussed by Filar and Vrieze [2004] as well as Breton et al. [1986]. A new type of approach based on homotopy is proposed by Herings and Peeters [2004]. In this approach, a homotopic path between equilibrium points of N independent MDPs and the N -player stochastic game in question, is traced numerically giving a Nash equilibrium point of the stochastic game of interest. Their result applies to both normal form as well as extensive form games. However, this approach has a complexity similar to that of typical gradient descent schemes discussed in this paper. For more recent developments in this direction, see the work by Herings and Peeters [2006] and Borkovsky et al. [2010]. A recent approach for the computation of Nash equilibria is given by Akchurina [2009] in which a reinforcement learning type scheme is proposed.", "startOffset": 113, "endOffset": 1632}, {"referenceID": 0, "context": "A recent approach for the computation of Nash equilibria is given by Akchurina [2009] in which a reinforcement learning type scheme is proposed.", "startOffset": 69, "endOffset": 86}, {"referenceID": 0, "context": "A recent approach for the computation of Nash equilibria is given by Akchurina [2009] in which a reinforcement learning type scheme is proposed. Though their experiments do show convergence in a large group of randomly generated games, a formal proof of convergence has not been provided. For general-sum stochastic games, [Breton et al., 1986, Section 4.3] provides an interesting optimization problem with non-linear objectives and linear constraints whose global minima correspond to Nash equilibria of the underlying general-sum stochastic game. However, since the objective is not guaranteed to be convex, simple gradient descent techniques might not converge to a global minimum. Mac Dermed and Isbell [2009] formulate intermediate optimization problems, called Multi-Objective Linear Programs (MOLPs), to compute Nash equilibria as well as Pareto optimal solutions.", "startOffset": 69, "endOffset": 715}, {"referenceID": 0, "context": "A recent approach for the computation of Nash equilibria is given by Akchurina [2009] in which a reinforcement learning type scheme is proposed. Though their experiments do show convergence in a large group of randomly generated games, a formal proof of convergence has not been provided. For general-sum stochastic games, [Breton et al., 1986, Section 4.3] provides an interesting optimization problem with non-linear objectives and linear constraints whose global minima correspond to Nash equilibria of the underlying general-sum stochastic game. However, since the objective is not guaranteed to be convex, simple gradient descent techniques might not converge to a global minimum. Mac Dermed and Isbell [2009] formulate intermediate optimization problems, called Multi-Objective Linear Programs (MOLPs), to compute Nash equilibria as well as Pareto optimal solutions. However, as mentioned in that paper, the complexity of their algorithm scales exponentially with the problem size. Thus, their algorithm is tractable only for small sized problems with a few tens of states. Another non-linear optimization problem for computing Nash equilibria in general-sum stochastic games has been given by Filar and Vrieze [2004]. We begin with this optimization problem by discussing it in Section 2.", "startOffset": 69, "endOffset": 1224}, {"referenceID": 6, "context": "Some important results by Filar and Vrieze [2004] that are applicable here are then described.", "startOffset": 26, "endOffset": 50}, {"referenceID": 1, "context": "Similar to MDPs [Bertsekas, 1995], one can define the value function as follows:", "startOffset": 16, "endOffset": 33}, {"referenceID": 14, "context": "Like in normal-form games [Nash, 1950], pure strategy Nash equilibria may not exist in the case of stochastic games.", "startOffset": 26, "endOffset": 38}, {"referenceID": 6, "context": "2, for a two-player general-sum discounted stochastic game has been given by Filar and Vrieze [2004]. The optimization problem is as follows:", "startOffset": 77, "endOffset": 101}, {"referenceID": 9, "context": "The algorithm to be discussed is mainly based on an interior-point search algorithm by Herskovits [1986]. We first discuss the difficulties posed by the optimization problem (7).", "startOffset": 87, "endOffset": 105}, {"referenceID": 9, "context": "The algorithm to be discussed is mainly based on an interior-point search algorithm by Herskovits [1986]. We first discuss the difficulties posed by the optimization problem (7). We try to address these issues in the subsequent sections by presenting a suitable gradient-based algorithm. With a suitable initial feasible point, the iterative procedure of Herskovits [1986] converges to a constrained local minimum of a given optimization problem.", "startOffset": 87, "endOffset": 373}, {"referenceID": 9, "context": "The algorithm to be discussed is mainly based on an interior-point search algorithm by Herskovits [1986]. We first discuss the difficulties posed by the optimization problem (7). We try to address these issues in the subsequent sections by presenting a suitable gradient-based algorithm. With a suitable initial feasible point, the iterative procedure of Herskovits [1986] converges to a constrained local minimum of a given optimization problem. The unmodified algorithm of Herskovits [1986] is presented in Section 3.", "startOffset": 87, "endOffset": 493}, {"referenceID": 9, "context": "We present the algorithm of Herskovits [1986] in two parts: First, we provide the two-stage feasible direction method in Algorithm 1 and then in Algorithm 2, we present the full algorithm.", "startOffset": 28, "endOffset": 46}, {"referenceID": 5, "context": "Since H is also sparse, we do sparse LDL decomposition of the matrix H using techniques discussed by Davis et al. [2007], Davis [2007], using publicly available software on the internet.", "startOffset": 101, "endOffset": 121}, {"referenceID": 5, "context": "Since H is also sparse, we do sparse LDL decomposition of the matrix H using techniques discussed by Davis et al. [2007], Davis [2007], using publicly available software on the internet.", "startOffset": 101, "endOffset": 135}], "year": 2015, "abstractText": "Zero-sum stochastic games are easy to solve as they can be cast as simple Markov decision processes. This is however not the case with general-sum stochastic games. A fairly general optimization problem formulation is available for general-sum stochastic games by Filar and Vrieze [2004]. However, the optimization problem there has a non-linear objective and non-linear constraints with special structure. Since gradients of both the objective as well as constraints of this optimization problem are well defined, gradient based schemes seem to be a natural choice. We discuss a gradient scheme tuned for two-player stochastic games. We show in simulations that this scheme indeed converges to a Nash equilibrium, for a simple terrain exploration problem modelled as a general-sum stochastic game. However, it turns out that only global minima of the optimization problem correspond to Nash equilibria of the underlying general-sum stochastic game, while gradient schemes only guarantee convergence to local minima. We then provide important necessary conditions for gradient schemes to converge to Nash equilibria in general-sum stochastic games.", "creator": "dvips(k) 5.991 Copyright 2011 Radical Eye Software"}}}