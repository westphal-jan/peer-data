{"id": "1507.00333", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "30-Jun-2015", "title": "Notes on Low-rank Matrix Factorization", "abstract": "Low-rank matrix factorization (MF) is an important technique in data science. The key idea of MF is that there exists latent structures in the data, by uncovering which we could obtain a compressed representation of the data. By factorizing an original matrix to low-rank matrices, MF provides a unified method for dimesion reduction, clustering, and matrix completion. In this article we review several important variants of MF, including: Basic MF, Non-negative MF, Orthogonal non-negative MF. As can be seen from their names, non-negative MF and orthogonal non-negative MF are variants of basic MF with non-negativity and/or orthogonality constraints. Such constraints are useful in specific senarios. In the first part of this article, we introduce, for each of these models, the application scenarios, the distinctive properties, and the optimizing method. By properly adapting MF, we can go beyond the problem of clustering and matrix completion. In the second part of this article, we will extend MF to sparse matrix compeletion, enhance matrix compeletion using various regularization methods, and make use of MF for (semi-)supervised learning by introducing latent space reinforcement and transformation. We will see that MF is not only a useful model but also as a flexible framework that is applicable for various prediction problems.", "histories": [["v1", "Tue, 30 Jun 2015 20:47:34 GMT  (14kb)", "http://arxiv.org/abs/1507.00333v1", null], ["v2", "Mon, 26 Oct 2015 20:44:46 GMT  (14kb)", "http://arxiv.org/abs/1507.00333v2", null], ["v3", "Fri, 6 May 2016 10:35:36 GMT  (15kb)", "http://arxiv.org/abs/1507.00333v3", null]], "reviews": [], "SUBJECTS": "cs.NA cs.IR cs.LG", "authors": ["yuan lu", "jie yang"], "accepted": false, "id": "1507.00333"}, "pdf": {"name": "1507.00333.pdf", "metadata": {"source": "CRF", "title": "Notes on Low-rank Matrix Factorization", "authors": ["Jie Yang"], "emails": ["j.yang-3@tudelft.nl."], "sections": [{"heading": null, "text": "ar Xiv: 150 7.00 333v 1 [cs.N A] 30 Jun 2015Notes on Matrix Factorization Jie Yang, Faculty of EEMCS, Delft University of Technology, Mekelweg 4, 2628 CD Delft, Netherlands. \u043a Email: j.yang-3 @ tudelft.nl.Dedicated to Yuanyuan, Xiao Baobao and Tu Daye."}, {"heading": "1 Introduction", "text": "The key idea of MF is that there are latent structures in the data that we could obtain by compressing the data. MF has several nice properties: 1) it detects latent structures in the data while addressing the data-saving problem [11]; 2) it has an elegant probabilistic interpretation [15]; it can be easily extended with domain-specific knowledge (e.g. by uncovering linked data), so it is suitable for various real-world problems; 4) many optimization methods such as (stochastic) methods can be applied to find a good solution."}, {"heading": "2 Theory", "text": "This section introduces us to the theory of low-level matrix factorization. As already mentioned, we will go through the following three MF variations: basic MF, non-negative MF, orthogonal non-negative MF."}, {"heading": "2.1 Basic MF", "text": "We start with the basic MF model formulating asmin U, V-X-UVT-L (U, V), (1) where X-Rm-n is the approximate data matrix, and U-Rm-k, V-Rn-k are two low-dimensional matrices (k-min (m, n)). L (U, V) is a regularizing part to avoid overfitting."}, {"heading": "2.1.1 Gradient Descent Optimization", "text": "(2) The reason for using the Frobenius standard is that it has a Guassian sound interpretation and that the objective function can easily be converted into a matrix trace version: min U, VO = Tr (XTX + VUTUVT) + \u03b1Tr (UTU) + \u03b2Tr (VTV). (3) Here, the matrix calculation rule for the transformation is used: min U, VO = Tr (ATA). Note that the trace has many good properties, such as Tr (A) = Tr (AT) and Tr (AB) = Tr (BA) = Tr (BA) used in the following derivatives: TUX \u2212 Gravatives for the Tr (AT) and Tr (VT) = Optimization (BA) = Tr (BA) used in the following derivatives: TUT (V2), VU-Ugoriation for the following TR ()."}, {"heading": "2.1.2 Algorithm Schemes in CF and Others", "text": "An important implementation strategy is that for each weighted entry in the training set, we update a whole set of U and an entire column of VT, since the entire row or column is involved in approaching the weighted entry. The same update mechanism could also be applied in the stochastic algorithm. Meanwhile, this type of update does not fully utilize the data matrix in each update iteration, because not only does a whole set of U (and a column of VT) participate in a single entry in the data matrix X, but also a set of U (and a column of VT) affect an entire row (column) of X. For this reason, we recommend that matrix U and V be updated by making full use of the data matrix X."}, {"heading": "2.2 Non-negative MF", "text": "NonnegativeMF [13] tries to approximate the data matrix with the low-dimensional matrices U, V, all of which entries are not negative, i.e. U, V \u2265 0. The new problem will be: min U, VO = \u0442 X \u2212 UVT-2F + \u03b1-U-2 F + \u03b2-V-2 Fs.t. U \u2265 0, V \u2265 0. (6) The limitation of nonnegativity results from parts of the overall interpretation [13]. As we can imagine, many data from the real world are not negative, e.g. connection strength, favorite strength, etc. Non-negative MF can uncover the important parts that sometimes cannot be reached by unrestricted MF [13]. Apart from the advantage of uncovering parts, non-negative MF has its own computational advantage: There is a relatively fixed method to find a learning rate that is greater than conventional gradient-based methods. To illustrate this, we will first infer the updating of the general rule for the relative convergence 6 as an example."}, {"heading": "2.2.1 Updating Rule Derivation", "text": "The basic idea is the application of KKT supplementary slackness conditions to enforce the non-negativity restriction. On this basis, we can obtain directly updating rules. (7) We have the following KKT condition. (1) We have the following KKT condition. (2) We have V = 0, (8) where \"Hadamard product\" is written. We then have \"L.\" (VUTUVT \u2212 2XTUVT) + \"Tr.\" (UTU). \u2212 Tr. (1). (UVTV \u2212 XV +. (XV +. U). (1). (VU.). (VU.). (VU.). (VU.). (VU.). (VU.). (VU. (VU.). (VU. (.). (VU.). (.). (VU.).). (VU.). (VU.). (............................................. (......................................... (................................................. (.................................). (VU.). (................................................). (.). (........................................).). (VU.). (........................................................)."}, {"heading": "2.2.2 Proof of Convergence", "text": "We prove the convergence of the updating rules in Eq. 12 with the standard auxiliary approach proposed in [14] and extended in [5, 4]. Our proof is mainly based on [5, 4], although the objective function Eq. 6 slightly different.An auxiliary function G (U, Ut) of the function L (U) is a function that (14) we have L (Ut + 1), G (Ut), Ut), L (U). (13) Then if we have Ut + 1 such thatUt + 1 = arg min U (U, Ut), (14) we have L (Ut + 1), Ut + 1 (Ut), Ut)."}, {"heading": "2.3 Orthogonal Non-negative MF", "text": "Another important limitation for MF is orthogonality. First, we formulate the problem asmin U, VO = \u0441X \u2212 UVT \u04412Fs.t. U, V \u2265 0, UTU = I, VTV = I. (23) Note that we do not add regularization here due to the orthogonality limitation. It is proven in [3, 5] ([5]) that this problem is equivalent to K-mean clustering: V is an indication matrix with V \u2032 (i, j) = 0 if xi belongs to the j th (1 \u2264 j \u2264 k) cluster. Here, V = V \u2032 (V \u2032 T) \u2212 1 / 2, i.e. V is a normalized version of V \u2032: V \u2032 is a constant scaling of the corresponding series of V, and vice versa V (: j) \u044522 = 1."}, {"heading": "2.3.1 3-factor MF vs. 2-factor MF", "text": "We refer to Equation 23 as a one-sided 2-factor orthogonal non-negative MF, since only one factored matrix must be orthogonal and there are two factored matrices altogether. It is recommended that in order to bundle rows and columns into X simultaneously, we need a 3-factor bi-orthogonal non-negative MF, i.e. that both U and V are orthogonal: min U, H, VO = VP-X \u2212 UHVT-2Fs.t. U, H, V \u2265 0, UTU = I, VTV = I. (24) It has been proven that in comparison with a 3-factor bi-orthogonal non-negative MF, a 2-factor orthogonal non-negative MF is too restrictive and will lead to a poor approximation [5]."}, {"heading": "2.3.2 Updating Rule Derivation", "text": "We have the following HKT conditions (VHTUHVT-I), we have the following HKT conditions (VHTUHVT-I), we have the following HKT conditions (VHTUHVT-I), we have the following HKT conditions (VHTUHVT-I), we have the following HKT conditions (VHTUHVT-I), we have the following HKT conditions (TVT-I), we have the following HKT conditions, we have the following HKT conditions, we have the following HKT conditions, we have the following HKT conditions, we have the following HKT-I conditions, we have the HKT-I-HKT-I. We have the following HKT conditions."}, {"heading": "3 Adapatations and Applications", "text": "MF has been used for a wide range of social computing applications, including collaborative filtering (CF), link prediction (LP), mood analysis, etc. It can serve not only as a single model for matrix complete solutions or clustering, but also as a framework for solving almost all categories of prediction problems. In this part, we will extend MF to very few cases. In cases where we have additional data, such as connectivity data between users (in CF or additional links in LP) or descriptive data of users and objects, we can incorporate various regulatory techniques to improve matrix completion performance. Furthermore, by properly manipulating latent factors derived from MF, we can adapt MF to (semi) supervised learning."}, {"heading": "3.1 Sparse Matrix Completion", "text": "Here we will deal with the problem of using MF for collborative filtering, link prediction and clustering, starting with a basic assumption that makes the newly introduced models unsuitable: a high proportion of the data is missing, i.e. the data matrix is incomplete. Such an assumption is very common in the real world [12]. The problem is solved by directly modelling the observed data. Equation 1 is modified as follows: min U, VO = 1 when the input (i, j) is observed, and O (i, j) = 0 otherwise.T (Utherwise.T) In this case the objective function is transformed as follows: min U, VO = Tr (OT), i.e. O (i, j) = 1 when the input (i, j) is observed, and O (i, j) = 0 otherwise.T (Utherwise.T)."}, {"heading": "3.1.1 Calculating Memory Occupation", "text": "When completing the matrix, however, sometimes the size of the data matrix is larger than the memory size, which makes the stochasian gradient descendant algorithm more suitable than the matrix-by-matrix method. The question here is how do we calculate the size of a matrix to see if it fits into the memory. Here is a simple method to do such a calculation. Suppose we have a matrix of 10K x 10K, with each input assigned a 32-bit float (e.g. float32 in Python), then the memory allocation for the entire matrix can be roughly calculated as (104 x 104 x 104 x 4) / 106 = 400M. So for a computer with 4G memory, we can build a matrix of 100K x x x x x x x x x x x 10K x 10K into the memory."}, {"heading": "3.2 Enhanced Matrix Completion", "text": "We looked at MF with different constraints, e.g. non-negativity and orthogality, and a type of regulation that prevents entries in subordinate matrices from being too large. This section looks at other types of regulation when external data sources become available, i.e. when they go beyond data matrix X. Usually, this is the real case, as most social media data contains rich data sources. In this subsection, we look at two types of regulation with corresponding additional data: 1. Self-regulation when we have additional linked data between users (in the case of CF or additional link types in the case of LP); 2. 2. Regulation when we have user descriptions and articles. We also refer to two publications [19] and [8] to demonstrate the two types of regulation mentioned above."}, {"heading": "3.2.1 Enhancing Matrix Completion with Self-regularization", "text": "By self-regulation, we are referring to the regularization of lines in the subordinate matrix U = V. Suppose we are now dealing with an LP problem in which we want to predict whether a user trusts another - trust relationships are common in reviews such as Epinions. Normally, there is a different type of connection between users, i.e. social relationships. (Can we use social relationships to enhance the performance of the trust relationship? This is exactly the research question proposed in [19]. It turns out that the answer is yes - as expected, users with social relationships tend to share similar preferences. The basic idea of integrating this into trust relationships is by inserting the regularization term Eq. 44 into the general MF framework."}, {"heading": "3.2.2 Enhancing Matrix Completion with 2-sided regularization", "text": "Here we consider the regulation on both sides of U and V together, which we refer to as 2-sided regulation. (Before we begin, we consider the orthogonal non-negative MF a little. (D) R (D) R (D) R (D) R (D) R (D) R (D) R (D) R (D) R (D) R (D) R (D) R (D) R (D) R (D) R (D) R (D) R (D) R (D) R (D) R (D) R (D) R (D) R (D) R (D) R (D) R) R (D) R (D) R (D) R (D) R (D) R) R (D) R (D) R (D) R) R (D) R (D) R (D) R (D) R (D) R (D) R (D) R (D) R) R (R) R (R) R (R) R (R) R (D) R (D) R (D) R (D) R (D) R (D) R (D) R (D) R (D) R (D) R (D) R (D) R (D) R (D) R (D) R (D) R (D) R (D) R (D) R (D) R (D) R (D) R) R (D) R (D) R (D) R (D) R (D) R) R (D) R (D) R (D) R (D) R (D) R (D) R) R (D) R (D) R (D) R (D) D) R (D) R (D) R (D) R (D) R (D) R (D) R (D) R (D) R (D) R (D) D) R (D) R (D) R (D) R (D) R (D) R (D) R (D) R (D) R (D) R (D) R (D) R (D) R (D) R (D) R (D) R (D) R (D) R (D) R (D)"}, {"heading": "3.3 From Clustering to (Semi-)supervised Learning", "text": "Although other types of additional data are used in advanced MF, the purpose remains matrix completion. However, this subsection looks at other types of machine learning problems, i.e. (semi-) supervised learning. The main assumption of using MF for (semi-) supervised learning is that the latent row (column) is predictable for a particular answer. To use predictability, we need mechanisms to link the latent vectors to the answers. Below, the two mechanisms are listed: 1. Amplification directly forces latent space to become reaction space; 2. Transformation transforms latent space into reaction space. This is similar to machine learning. We refer to publications [10] and [6] to demonstrate the above two methods."}, {"heading": "3.3.1 Enforcing Latent Factor to be Response", "text": "In previous regularizations, we do not force latent space to be an interpretable space. In bilateral regularization, for example, we do not specify the meaning of U, which is used in both X and A factorization. (un, semi-) supervised learning, however, requires that latent space be interpretable. [10] The method is still regularization. [10] The key idea of modeling the post-level emotion indication, for which the authors use 3-factor non-negative orthogonal MF. The input is a post-word matrix X. In addition, in some of the contributions, we get emotion indication. \"The key idea of modeling the post-level emotion indication is to make the sensory polarity of a post as close as possible to the emotion indication of the post,\" asGu, U \u2212 0, we have in the matrix U-2, which is that the matrix is 0."}, {"heading": "3.3.2 Transforming Latent Factor to Response", "text": "To see this, we begin directly with the application of [6]. We would like to model the setting of a user to a controversial topic, which is reflected in his opinion, his sentiment and his retweeting action. We first introduce a retweeting matrix X, which represents the retweeting action of the user to some tweets. We then discuss other alternatives. To train such a model, the authors propose the following model: \"W, VO = X, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 6, 6, 6, 6, 6, 7, 7, 7, 7, 8, 8, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 6, 6, 6, 6, 7, 7, 7, 7, 7, 7, 7, 7, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9"}], "references": [{"title": "The matrix reference manual", "author": ["Mike Brookes"], "venue": "Imperial College London,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2005}, {"title": "On the equivalence of nonnegative matrix factorization and spectral clustering", "author": ["Chris Ding", "Xiaofeng He", "Horst D Simon"], "venue": "In SDM\u201905,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2005}, {"title": "Nonnegative matrix factorization for combinatorial optimization: Spectral clustering, graph matching, and clique finding", "author": ["Chris Ding", "Tao Li", "Michael I Jordan"], "venue": "In ICDM\u201908,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2008}, {"title": "Orthogonal nonnegative matrix t-factorizations for clustering", "author": ["Chris Ding", "Tao Li", "Wei Peng", "Haesun Park"], "venue": "In KDD\u201906,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2006}, {"title": "Modeling user attitude toward controversial topics in online social media", "author": ["Huiji Gao", "Jalal Mahmud", "Jilin Chen", "Jeffrey Nichols", "Michelle Zhou"], "venue": "In ICWSM\u201914,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2014}, {"title": "Exploring temporal effects for location recommendation on location-based social networks", "author": ["Huiji Gao", "Jiliang Tang", "Xia Hu", "Huan Liu"], "venue": "In RecSys\u201913,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2013}, {"title": "Content-aware point of interest recommendation on location-based social networks", "author": ["Huiji Gao", "Jiliang Tang", "Xia Hu", "Huan Liu"], "venue": "In AAAI\u201915,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2015}, {"title": "The elements of statistical learning", "author": ["Trevor Hastie", "Robert Tibshirani", "Jerome Friedman"], "venue": null, "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2009}, {"title": "Unsupervised sentiment analysis with emotional signals", "author": ["Xia Hu", "Jiliang Tang", "Huiji Gao", "Huan Liu"], "venue": "In WWW\u201913,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2013}, {"title": "Factorization meets the neighborhood: a multifaceted collaborative filtering model", "author": ["Yehuda Koren"], "venue": "In KDD\u201908,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2008}, {"title": "Matrix factorization techniques for recommender systems", "author": ["Yehuda Koren", "Robert Bell", "Chris Volinsky"], "venue": null, "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2009}, {"title": "Learning the parts of objects by non-negative matrix factorization", "author": ["Daniel D Lee", "H Sebastian Seung"], "venue": null, "citeRegEx": "13", "shortCiteRegEx": "13", "year": 1999}, {"title": "Algorithms for non-negative matrix factorization", "author": ["Daniel D Lee", "H Sebastian Seung"], "venue": "In NIPS\u201901,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2001}, {"title": "Probabilistic matrix factorization", "author": ["Andriy Mnih", "Ruslan Salakhutdinov"], "venue": "In NIPS\u201907,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2007}, {"title": "Collaborative filtering beyond the user-item matrix: A survey of the state of the art and future challenges", "author": ["Yue Shi", "Martha Larson", "Alan Hanjalic"], "venue": "ACM Computing Surveys (CSUR),", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2014}, {"title": "Relational learning via collective matrix factorization", "author": ["Ajit P Singh", "Geoffrey J Gordon"], "venue": "In KDD\u201908,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2008}, {"title": "Matrix factorization and neighbor based algorithms for the netflix prize problem", "author": ["G\u00e1bor Tak\u00e1cs", "Istv\u00e1n Pil\u00e1szy", "Botty\u00e1n N\u00e9meth", "Domonkos Tikk"], "venue": "In RecSys\u201908,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2008}, {"title": "Exploiting homophily effect for trust prediction", "author": ["Jiliang Tang", "Huiji Gao", "Xia Hu", "Huan Liu"], "venue": "In WSDM\u201913,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2013}, {"title": "Factorization vs. regularization: fusing heterogeneous social relationships in top-n recommendation", "author": ["Quan Yuan", "Li Chen", "Shiwan Zhao"], "venue": "In Recsys\u201911,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2011}], "referenceMentions": [{"referenceID": 9, "context": "MF has several nice properties: 1) it uncovers latent structures in the data, while addressing the data sparseness problem [11]; 2) it has an elegant probabilistic interpretation [15]; 3) it can be easily extended with domain specific prior knowledge (e.", "startOffset": 123, "endOffset": 127}, {"referenceID": 13, "context": "MF has several nice properties: 1) it uncovers latent structures in the data, while addressing the data sparseness problem [11]; 2) it has an elegant probabilistic interpretation [15]; 3) it can be easily extended with domain specific prior knowledge (e.", "startOffset": 179, "endOffset": 183}, {"referenceID": 17, "context": ", homophily in linked data [19]), thus suitable for various real-world problems; 4) many optimization methods such as (stochastic) gradient-based methods can be applied to find a good solution.", "startOffset": 27, "endOffset": 31}, {"referenceID": 2, "context": "Note that for the optimizing method, we mainly use the alternative algorithm, as similar to [4, 19].", "startOffset": 92, "endOffset": 99}, {"referenceID": 17, "context": "Note that for the optimizing method, we mainly use the alternative algorithm, as similar to [4, 19].", "startOffset": 92, "endOffset": 99}, {"referenceID": 0, "context": "For reference, matrix operation and optimization can be referred to [2] and [1] respectively.", "startOffset": 68, "endOffset": 71}, {"referenceID": 7, "context": "Regularization is usually necessary in prediction for bias-variance trade-off [9].", "startOffset": 78, "endOffset": 81}, {"referenceID": 0, "context": "(see more in [2]), we have the following derivatives for U and V,", "startOffset": 13, "endOffset": 16}, {"referenceID": 16, "context": "Detailed algorithm can be find in [18].", "startOffset": 34, "endOffset": 38}, {"referenceID": 2, "context": "As the objective function is non-convex caused by the coupling between U and V, we can choose to alternatively update U and V in each iteration as in [4, 19].", "startOffset": 150, "endOffset": 157}, {"referenceID": 17, "context": "As the objective function is non-convex caused by the coupling between U and V, we can choose to alternatively update U and V in each iteration as in [4, 19].", "startOffset": 150, "endOffset": 157}, {"referenceID": 17, "context": "Detailed algorithm is similar to the one in [19].", "startOffset": 44, "endOffset": 48}, {"referenceID": 11, "context": "Interestingly, the alternative optimization scheme is even more suitable for non-negative MF [13, 14, 5, 4], as we will see in the following subsections.", "startOffset": 93, "endOffset": 107}, {"referenceID": 12, "context": "Interestingly, the alternative optimization scheme is even more suitable for non-negative MF [13, 14, 5, 4], as we will see in the following subsections.", "startOffset": 93, "endOffset": 107}, {"referenceID": 3, "context": "Interestingly, the alternative optimization scheme is even more suitable for non-negative MF [13, 14, 5, 4], as we will see in the following subsections.", "startOffset": 93, "endOffset": 107}, {"referenceID": 2, "context": "Interestingly, the alternative optimization scheme is even more suitable for non-negative MF [13, 14, 5, 4], as we will see in the following subsections.", "startOffset": 93, "endOffset": 107}, {"referenceID": 11, "context": "2 Non-negative MF Non-negativeMF [13] seeks to approximate data matrixXwith low-dimensional matrices U,V whose entries are all non-negative, i.", "startOffset": 33, "endOffset": 37}, {"referenceID": 11, "context": "Non-negativity constaint is originated from parts-of-whole interpretation [13].", "startOffset": 74, "endOffset": 78}, {"referenceID": 11, "context": "Non-negative MF may uncover the important parts, which sometimes can not be achieved by non-constrained MF [13].", "startOffset": 107, "endOffset": 111}, {"referenceID": 17, "context": "Detailed algorithm using these rules is similar to the one in [19].", "startOffset": 62, "endOffset": 66}, {"referenceID": 12, "context": "12 with the standard auxiliary function approach, which is proposed in [14] and extended in [5, 4].", "startOffset": 71, "endOffset": 75}, {"referenceID": 3, "context": "12 with the standard auxiliary function approach, which is proposed in [14] and extended in [5, 4].", "startOffset": 92, "endOffset": 98}, {"referenceID": 2, "context": "12 with the standard auxiliary function approach, which is proposed in [14] and extended in [5, 4].", "startOffset": 92, "endOffset": 98}, {"referenceID": 3, "context": "Our proof is mainly based on [5, 4], although the objective function Eq.", "startOffset": 29, "endOffset": 35}, {"referenceID": 2, "context": "Our proof is mainly based on [5, 4], although the objective function Eq.", "startOffset": 29, "endOffset": 35}, {"referenceID": 3, "context": "17 can be found in [5] (Proposition 6).", "startOffset": 19, "endOffset": 22}, {"referenceID": 1, "context": "It is proved in [3, 5] ([5] gives more mature proof) that this problem is equivalent to K-means clustering: V is an indication matrix with V(i, j) = 0 if xi belongs to the j th (1 \u2264 j \u2264 k) cluster.", "startOffset": 16, "endOffset": 22}, {"referenceID": 3, "context": "It is proved in [3, 5] ([5] gives more mature proof) that this problem is equivalent to K-means clustering: V is an indication matrix with V(i, j) = 0 if xi belongs to the j th (1 \u2264 j \u2264 k) cluster.", "startOffset": 16, "endOffset": 22}, {"referenceID": 3, "context": "It is proved in [3, 5] ([5] gives more mature proof) that this problem is equivalent to K-means clustering: V is an indication matrix with V(i, j) = 0 if xi belongs to the j th (1 \u2264 j \u2264 k) cluster.", "startOffset": 24, "endOffset": 27}, {"referenceID": 3, "context": "It is proved that, compared to 3-factor bi-orthogonal non-negative MF, 2factor bi-orthogonal non-negative MF is too restrictive, and will lead to poor approximation [5].", "startOffset": 165, "endOffset": 168}, {"referenceID": 3, "context": "3-factor bi-orthogonal non-negative MF is useful in document-word clustering [5], outperforming K-means (i.", "startOffset": 77, "endOffset": 80}, {"referenceID": 8, "context": "It has been applied for tasks such as sentiment analysis [10].", "startOffset": 57, "endOffset": 61}, {"referenceID": 10, "context": "Such assumption is very common in real-world cases [12].", "startOffset": 51, "endOffset": 55}, {"referenceID": 5, "context": "For updating rules of non-negative MF and orthogonal non-negative MF, the reader can refer to [7] and [8], respectively.", "startOffset": 94, "endOffset": 97}, {"referenceID": 6, "context": "For updating rules of non-negative MF and orthogonal non-negative MF, the reader can refer to [7] and [8], respectively.", "startOffset": 102, "endOffset": 105}, {"referenceID": 17, "context": "We further point to two publications [19] and [8], to demonstrate the above two types of regularization, respectively.", "startOffset": 37, "endOffset": 41}, {"referenceID": 6, "context": "We further point to two publications [19] and [8], to demonstrate the above two types of regularization, respectively.", "startOffset": 46, "endOffset": 49}, {"referenceID": 17, "context": "Can we use social relation to boost the performance of trust relation prediction? This is exactly the research question proposed in [19].", "startOffset": 132, "endOffset": 136}, {"referenceID": 8, "context": "We will see that this type of regularization is used in a sentiment analysis paper [10], which we will analyze later.", "startOffset": 83, "endOffset": 87}, {"referenceID": 6, "context": "Now let us turn our attention back to 2-sided regularization, basing the example from [8], which considers POI recommendation in location-based social network (LBSN).", "startOffset": 86, "endOffset": 89}, {"referenceID": 15, "context": "Regularization We remark here that the idea of cofactoring two matrices (X,A) with shared factors (U) originates from collective matrix facterization [17], which has many applications in CF [16].", "startOffset": 150, "endOffset": 154}, {"referenceID": 14, "context": "Regularization We remark here that the idea of cofactoring two matrices (X,A) with shared factors (U) originates from collective matrix facterization [17], which has many applications in CF [16].", "startOffset": 190, "endOffset": 194}, {"referenceID": 18, "context": "A interesting comparative study between collective facterization and self-regularization can be found in [20].", "startOffset": 105, "endOffset": 109}, {"referenceID": 8, "context": "We point to publications [10] and [6] for the demonstration of the above two methods, respectively.", "startOffset": 25, "endOffset": 29}, {"referenceID": 4, "context": "We point to publications [10] and [6] for the demonstration of the above two methods, respectively.", "startOffset": 34, "endOffset": 37}, {"referenceID": 8, "context": "[10] deals with the problem of sentiment analysis, for which the authors use 3-factor non-negative orthogonal MF.", "startOffset": 0, "endOffset": 4}, {"referenceID": 3, "context": "Based on Proposition 1 in [5], we know that the answer is no.", "startOffset": 26, "endOffset": 29}, {"referenceID": 8, "context": "However, as we see in this sentiment analysis work [10], regularization is always possible! In fact, the enforcement regularization that we see in this work is the most constrained regularization: it is 2-sided regularization for both U,V, and it is enforcement without any transformation coefficients.", "startOffset": 51, "endOffset": 55}, {"referenceID": 4, "context": "To see this, we start directly with the application of [6].", "startOffset": 55, "endOffset": 58}, {"referenceID": 4, "context": "We first introduce how the model is built in [6], then discuss other alternatives.", "startOffset": 45, "endOffset": 48}], "year": 2017, "abstractText": null, "creator": "LaTeX with hyperref package"}}}