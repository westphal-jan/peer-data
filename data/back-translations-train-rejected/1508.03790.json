{"id": "1508.03790", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "16-Aug-2015", "title": "Depth-Gated LSTM", "abstract": "In this short note, we present an extension of LSTM to use a depth gate to connect memory cells of adjacent layers. Doing so introduces a linear dependence between lower and upper recurrent units. Importantly, the linear dependence is gated through a gating function, which we call forget gate. This gate is a function of lower layer memory cell, its input, and its past memory. We conducted experiments and verified that this new architecture of LSTMs is able to improve machine translation and language modeling performances.", "histories": [["v1", "Sun, 16 Aug 2015 04:31:37 GMT  (44kb,D)", "https://arxiv.org/abs/1508.03790v1", "Content presented in 2015 Jelinek Summer Workshop on Speech and Language Technology on August 14th 2015"], ["v2", "Wed, 19 Aug 2015 19:38:58 GMT  (73kb,D)", "http://arxiv.org/abs/1508.03790v2", "Content presented in 2015 Jelinek Summer Workshop on Speech and Language Technology on August 14th 2015"], ["v3", "Thu, 20 Aug 2015 07:13:04 GMT  (73kb,D)", "http://arxiv.org/abs/1508.03790v3", "Content presented in 2015 Jelinek Summer Workshop on Speech and Language Technology on August 14th 2015"], ["v4", "Tue, 25 Aug 2015 04:24:20 GMT  (101kb,D)", "http://arxiv.org/abs/1508.03790v4", "Content presented in 2015 Jelinek Summer Workshop on Speech and Language Technology on August 14th 2015"]], "COMMENTS": "Content presented in 2015 Jelinek Summer Workshop on Speech and Language Technology on August 14th 2015", "reviews": [], "SUBJECTS": "cs.NE cs.CL", "authors": ["kaisheng yao", "trevor cohn", "katerina vylomova", "kevin duh", "chris dyer"], "accepted": false, "id": "1508.03790"}, "pdf": {"name": "1508.03790.pdf", "metadata": {"source": "CRF", "title": "Depth-Gated LSTM", "authors": ["Kaisheng Yao", "Trevor Cohn", "Katerina Vylomova"], "emails": ["kaisheny@microsoft.com", "tcohn@unimelb.edu.au", "cdyer@cs.cmu.edu"], "sections": [{"heading": null, "text": "In this brief note, we present an extension of long-term neural memory (LSTM) to the use of a depth grid to connect memory cells from adjacent layers, resulting in a linear dependence between recurring units of the lower and upper layer. Importantly, the linear dependence is secured by a gating function called the depth grid, which is a function of the lower layer of memory cell, the input to and the past memory cell of that layer. We conducted experiments and confirmed that this new architecture of LSTMs was able to improve the performance of machine translation and voice modeling."}, {"heading": "1 Introduction", "text": "In natural language processing tasks, recurring neural networks (RNNs) [3-5] are widely used because of their ability to remember long-term dependencies. A typical problem in the formation of deep networks, including RNNs, is the reduction and explosion of gradients. This problem occurs when forming a simple RNN. Long-term short-term memory (LSTM) [6, 7] of neural networks is an extension of simple RNN [3]. In LSTM, a memory cell has a linear dependence on its current activity and its past activity. It is important that a forget factor is used to modulate the flow of information between the past and current activities. LSTMs also have input and output gates to modulate their input and output positions."}, {"heading": "2 Review of recurrent neural networks", "text": "A recursive neural network [3,4] has a hidden state ht, which depends recursively on its past value ht-1; i.e. ht = g (Whhht-1 + Bxhxt) (1), where g (\u00b7) is usually a non-linear function, like tanh. xt is the input. Whh and Bxh are the weight matrices."}, {"heading": "2.1 Long short-term memory (LSTM)", "text": "LSTM was originally proposed in [6, 7] and later modified in [11]. We follow the implementation in [11], which is shown in Fig. 1. LSTM maintains a linear dependence between its memory cells ct and its past ct-1. In addition, LSTM has input and output gates. Specifically, LSTM is under asit = \u03c3 (Wxixt + Whiht-1 + Wcict-1) (2) ft = \u03c3 (Wxfxt + Whfht-1 + Wcfct-1) (3) ct = ft ct-1 + it tanh (Wxcxt + Whcht-1) (4) ot = \u03c3 (Wxoxt + Whoht-1 + Wcoct) (5) ht = ot tanh (ct) (6), where it is, ft and ot = input gate, forget-gate and output gate of LSTM. ht is written from the LSTM gate (\u00b7)."}, {"heading": "2.2 Stacked LSTMs", "text": "Typically, LSTMs are stacked to form deep, recurring neural networks, as shown in the figure on the left. Output of the lower layer LSTM to layer L is h (L). In case of a possible affinity transformation, this output is input x (L + 1) t in the upper layer LSTM to layer L + 1. Except for this output-input connection, there are no further connections between the two layers."}, {"heading": "3 The Depth-gated LSTM", "text": "The deep-gated LSTM (DGLSTM) 1 is shown in the right figure of Figure 2. It has a depth gate that connects the memory cells c (L + 1) t in the upper layer L + 1 and the memory cell cLt in the lower layer L. The depth gate controls how much memory (L + 1) flows directly from the lower memory cell to the upper layer. Gate function on the L + 1 layer at the time t is a logistic function asd (L + 1) l (L + 1) t (L + 1) x x x x x x) d (L x x x x) x x x x x x) d (L + 1) t (L + 1) d (L + 1) ld c (L + 1) l + 1) l (L + 1) l (L + 1) l x x x x x x x x x x) d (L x x x) d) t (L + 1) t (STa t (L + 1) d + d + 1 d (d) d + 1 d (1) x x (l) x x x x (L x) x x x (L x) x x (L x) x x (L x) x x x x x (L x) x x x (L + 1) x (L + 1) x (L + 1) t (L + 1) x (L + 1) x x x (L + 1) x (L + 1) x (L + 1) x x x x x (L + 1) x x x x (L + 1) x (L + 1) x x x (L + 1) x x x x x x (L + 1) x (L + 1) x x x x x x (L + 1) x (L + 1) x x (L + 1) x (L + 1) x (L + 1) x x (L + 1) x (L + 1) x x x x x x x (L + 1) x x x x x (L x x x x (L x) x x x (L + 1) x (L + 1) x x x x (L x x x x x x x (L + 1) x (L + 1) x x x (L x x x (L x"}, {"heading": "4 Experiments", "text": "We applied DGLSTMs to two sets of data. The first is the BTEC machine translation task from Chinese to English. Its training set consists of 44016 set pairs. We use its devset1 and devset 2 for validation, which have a total of 1006 set pairs. We use its devset3 for the test, which has 506 set pairs. The second data set is PennTreeBank (PTB) for language modeling. It consists of 42075 sets for training, 3371 sets for development and 3762 sets for testing."}, {"heading": "4.1 Machine translation results", "text": "We conducted preliminary experiments and found that the attention model [9] performs better than the encoder decoder method [10]. Therefore, we applied the attention model [9] in our experiments. Both encoders and decoders used recursive neural networks in [9]. However, in this experiment, we used only recursive neural networks for decoders. For encoders, we used word embedding that was learned in education.A preliminary experiment showed that the simple RNN [3] performs worst. Therefore, we do not include the simple RNN results in this thesis. We compared DGLSTM with GRU and LSTM. All of these models used a 200-dimensional hidden layer. We varied the depth of the RNNN. The results in Table 1 show that DGLSTM LSTM performs better in all tested layers than LM and we used the transmission model LSTU for the next GRU experiment."}, {"heading": "4.2 Language modeling", "text": "We carried out experiments on the PTB data set. We trained a two-layer DGLSTM. Each layer has a 200-dimensional vector. The results of the test sets are shown in Table 3. Compared to the previously published results on the PTB data set, DGLSTM achieved the least perplexity to our knowledge on the PTB test set."}, {"heading": "5 Related works", "text": "We developed this method independently in a summer workshop and later knew the works in [13,14]. In highway networks in [13], the output of one layer yt is a linear function for input xt, in addition to output of a nonlinear path. Both are gated as follows: yt = H (xt, Whh) T (xt, WxT) + xt C (xt, Wc) (11), where T and C are designated as transformation gates or carry gates. H (\u00b7) is output of a nonlinear path. Whh, WxT, and Wc are matrices. Therefore, output of highway networks has a direct and linear connection, albeit gated, to input. This allows highway networks to train extremely deep networks. DGLSTM is related to networks that are not linear."}, {"heading": "6 Conclusions", "text": "We presented a depth-driven LSTM architecture that uses a depth gate to establish a linear link between the lower and upper memory cells. We observed better performance of this new architecture in machine translation and voice modeling tasks, which is related to the motorway networks [13] and grid LSTM [14], using an additional linear link with gates to regulate the flow of information across layers."}, {"heading": "7 Acknowledgment", "text": "We thank J\u00fcrgen Schmidhuber for pointing out the motorway networks [13] and the LSTM network [14]."}], "references": [{"title": "Deep neural networks for acoustic modeling in speech recognition", "author": ["G. Hinton", "L. Deng", "D. Yu", "G. Dahl", "A. r. Mohamed", "N. Jaitly", "A. Senior", "V. Vanhoucke", "P. Nguyen", "T. Sainath", "B. Kingsbury"], "venue": "2012.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2012}, {"title": "Imagenet classification with deep convolution neural networks", "author": ["A Krizhevsky", "I Sutskever", "G Hinton"], "venue": "NIPS, 2012, vol. 25, pp. 1090\u20131098. 5", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2012}, {"title": "Finding structure in time", "author": ["J. Elman"], "venue": "Cognitive science, vol. 14, no. 2, pp. 179\u2013211, 1990.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 1990}, {"title": "Serial order: a parallel distributed processing approach", "author": ["M. Jordan"], "venue": "Tech. Rep., Univ. of California San Diego, 1997.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 1997}, {"title": "Recurrent neural network based language model", "author": ["T. Mikolov", "M. Karafiat", "L. Burget", "J. Cernocky", "S. Khudanpur"], "venue": "INTERSPEECH, 2010, pp. 1045\u20131048.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2010}, {"title": "Long short-term memory", "author": ["S. Hochreiter", "J. Schmidhuber"], "venue": "Neural Computation, vol. 9, pp. 1735\u20131780, 1997.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 1997}, {"title": "Learning to forget: Continual prediction with LSTM", "author": ["F.A. Gers", "J. Schmidhuber", "F. Cummins"], "venue": "Neural Computation, vol. 12, pp. 2451\u20132471, 1999.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 1999}, {"title": "On the properties of neural machine translation", "author": ["K. Cho", "B. van Merrienboer", "D. Bahdanau", "Y. Bengio"], "venue": "arXiv:1409.125, 2014.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2014}, {"title": "Neural machine translation by jointly learning to align and translate", "author": ["D. Bahdanau", "K. Cho", "Y. Bengio"], "venue": "arXiv:1409.0473 [cs.CL], 2014.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2014}, {"title": "Sequence to sequence learning with neural networks", "author": ["I. Sutskever", "O. Vinyals", "Q.V. Le"], "venue": "NIPS, 2014.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2014}, {"title": "Generating sequences with recurrent neural networks", "author": ["A. Graves"], "venue": "arXiv:1308.0850 [cs.NE], 2013.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2013}, {"title": "How to construct deep recurrent neural networks", "author": ["R. Pascanu", "C. Gulcehre", "K. Cho", "Y. Bengio"], "venue": "arXiv:1312.6026 [cs.NE], 2013.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2013}, {"title": "Highway networks", "author": ["R.K. Srivastava", "K. Greff", "Jurgen Schmidhuber"], "venue": "arxiv:1505.00387v1, May 2015.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2015}, {"title": "Grid long short-term memory", "author": ["Nal Kalchbrenner", "Ivo Danihelka", "Alex Graves"], "venue": "arXiv:1507.01526 [cs.NE], 2015. 6", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2015}], "referenceMentions": [{"referenceID": 0, "context": "Deep neural networks (DNNs) have been successfully applied to many areas, including speech [1] and vision [2].", "startOffset": 91, "endOffset": 94}, {"referenceID": 1, "context": "Deep neural networks (DNNs) have been successfully applied to many areas, including speech [1] and vision [2].", "startOffset": 106, "endOffset": 109}, {"referenceID": 2, "context": "On natural language processing tasks, recurrent neural networks (RNNs) [3\u20135] are widely used because of their ability to memorize long-term dependency.", "startOffset": 71, "endOffset": 76}, {"referenceID": 3, "context": "On natural language processing tasks, recurrent neural networks (RNNs) [3\u20135] are widely used because of their ability to memorize long-term dependency.", "startOffset": 71, "endOffset": 76}, {"referenceID": 4, "context": "On natural language processing tasks, recurrent neural networks (RNNs) [3\u20135] are widely used because of their ability to memorize long-term dependency.", "startOffset": 71, "endOffset": 76}, {"referenceID": 5, "context": "The long short-term memory (LSTM) [6, 7] neural networks is an extension of simple RNN [3].", "startOffset": 34, "endOffset": 40}, {"referenceID": 6, "context": "The long short-term memory (LSTM) [6, 7] neural networks is an extension of simple RNN [3].", "startOffset": 34, "endOffset": 40}, {"referenceID": 2, "context": "The long short-term memory (LSTM) [6, 7] neural networks is an extension of simple RNN [3].", "startOffset": 87, "endOffset": 90}, {"referenceID": 5, "context": "Perhaps the introduction of gating functions in [6, 7] is the most significant improvement to the recurrent neural networks [3].", "startOffset": 48, "endOffset": 54}, {"referenceID": 6, "context": "Perhaps the introduction of gating functions in [6, 7] is the most significant improvement to the recurrent neural networks [3].", "startOffset": 48, "endOffset": 54}, {"referenceID": 2, "context": "Perhaps the introduction of gating functions in [6, 7] is the most significant improvement to the recurrent neural networks [3].", "startOffset": 124, "endOffset": 127}, {"referenceID": 7, "context": "More recently, the Gated Recurrent Unit [8] has also adopted the concept of using gates.", "startOffset": 40, "endOffset": 43}, {"referenceID": 8, "context": "LSTMs and GRUs are widely used in many natural language processing tasks [9, 10].", "startOffset": 73, "endOffset": 80}, {"referenceID": 9, "context": "LSTMs and GRUs are widely used in many natural language processing tasks [9, 10].", "startOffset": 73, "endOffset": 80}, {"referenceID": 2, "context": "A recurrent neural network [3,4] has a hidden state ht that depends on its past value ht-1 recursively; i.", "startOffset": 27, "endOffset": 32}, {"referenceID": 3, "context": "A recurrent neural network [3,4] has a hidden state ht that depends on its past value ht-1 recursively; i.", "startOffset": 27, "endOffset": 32}, {"referenceID": 5, "context": "LSTM was initially proposed in [6, 7] and later modified in [11].", "startOffset": 31, "endOffset": 37}, {"referenceID": 6, "context": "LSTM was initially proposed in [6, 7] and later modified in [11].", "startOffset": 31, "endOffset": 37}, {"referenceID": 10, "context": "LSTM was initially proposed in [6, 7] and later modified in [11].", "startOffset": 60, "endOffset": 64}, {"referenceID": 10, "context": "We follow the implementation in [11], which is illustrated in Fig.", "startOffset": 32, "endOffset": 36}, {"referenceID": 8, "context": "We conducted preliminary experiments and observed that the attention model [9] performed better than the encoder-decoder method [10].", "startOffset": 75, "endOffset": 78}, {"referenceID": 9, "context": "We conducted preliminary experiments and observed that the attention model [9] performed better than the encoder-decoder method [10].", "startOffset": 128, "endOffset": 132}, {"referenceID": 8, "context": "We therefore applied the attention model [9] in our experiments.", "startOffset": 41, "endOffset": 44}, {"referenceID": 8, "context": "Both encoder and decoder used recurrent neural networks in [9].", "startOffset": 59, "endOffset": 62}, {"referenceID": 2, "context": "A preliminary experiment showed that the simple RNN [3] performed the worst.", "startOffset": 52, "endOffset": 55}, {"referenceID": 12, "context": "We developed this method independently in a summer workshop and later knew the works in [13,14].", "startOffset": 88, "endOffset": 95}, {"referenceID": 13, "context": "We developed this method independently in a summer workshop and later knew the works in [13,14].", "startOffset": 88, "endOffset": 95}, {"referenceID": 12, "context": "In highway networks in [13], the output from a layer yt is a linear function to the input xt, in addition", "startOffset": 23, "endOffset": 27}, {"referenceID": 4, "context": "Model Perplexity RNN [5] 123 LSTM [11] 117 sRNN [12] 110 DOT(s)-RNN [12] 108 DGLSTM 96", "startOffset": 21, "endOffset": 24}, {"referenceID": 10, "context": "Model Perplexity RNN [5] 123 LSTM [11] 117 sRNN [12] 110 DOT(s)-RNN [12] 108 DGLSTM 96", "startOffset": 34, "endOffset": 38}, {"referenceID": 11, "context": "Model Perplexity RNN [5] 123 LSTM [11] 117 sRNN [12] 110 DOT(s)-RNN [12] 108 DGLSTM 96", "startOffset": 48, "endOffset": 52}, {"referenceID": 11, "context": "Model Perplexity RNN [5] 123 LSTM [11] 117 sRNN [12] 110 DOT(s)-RNN [12] 108 DGLSTM 96", "startOffset": 68, "endOffset": 72}, {"referenceID": 12, "context": "This might be the biggest difference from the highway networks [13] in its current implementation.", "startOffset": 63, "endOffset": 67}, {"referenceID": 13, "context": "Perhaps the closet work to this research is Grid LSTM [14], which uses LSTMs in different dimensions and connects them using gated linear connections.", "startOffset": 54, "endOffset": 58}, {"referenceID": 12, "context": "This architecture is related to the highway networks [13] and Grid LSTM [14] in using an additional linear connection with gates to regulate information flow across layers.", "startOffset": 53, "endOffset": 57}, {"referenceID": 13, "context": "This architecture is related to the highway networks [13] and Grid LSTM [14] in using an additional linear connection with gates to regulate information flow across layers.", "startOffset": 72, "endOffset": 76}], "year": 2015, "abstractText": "In this short note, we present an extension of long short-term memory (LSTM) neural networks to using a depth gate to connect memory cells of adjacent layers. Doing so introduces a linear dependence between lower and upper layer recurrent units. Importantly, the linear dependence is gated through a gating function, which we call depth gate. This gate is a function of the lower layer memory cell, the input to and the past memory cell of this layer. We conducted experiments and verified that this new architecture of LSTMs was able to improve machine translation and language modeling performances.", "creator": "LaTeX with hyperref package"}}}