{"id": "1508.03812", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "16-Aug-2015", "title": "Causal Decision Trees", "abstract": "Uncovering causal relationships in data is a major objective of data analytics. Causal relationships are normally discovered with designed experiments, e.g. randomised controlled trials, which, however are expensive or infeasible to be conducted in many cases. Causal relationships can also be found using some well designed observational studies, but they require domain experts' knowledge and the process is normally time consuming. Hence there is a need for scalable and automated methods for causal relationship exploration in data. Classification methods are fast and they could be practical substitutes for finding causal signals in data. However, classification methods are not designed for causal discovery and a classification method may find false causal signals and miss the true ones. In this paper, we develop a causal decision tree where nodes have causal interpretations. Our method follows a well established causal inference framework and makes use of a classic statistical test. The method is practical for finding causal signals in large data sets.", "histories": [["v1", "Sun, 16 Aug 2015 11:31:49 GMT  (3399kb,D)", "http://arxiv.org/abs/1508.03812v1", null]], "reviews": [], "SUBJECTS": "cs.AI", "authors": ["jiuyong li", "saisai ma", "thuc duy le", "lin liu", "jixue liu"], "accepted": false, "id": "1508.03812"}, "pdf": {"name": "1508.03812.pdf", "metadata": {"source": "CRF", "title": "Causal Decision Trees", "authors": ["Jiuyong Li", "Saisai Ma", "Thuc Duy Le", "Lin Liu", "Jixue Liu"], "emails": [], "sections": [{"heading": null, "text": "This year, it has reached the point where it will be able to put itself at the top of the group, which is able to put itself at the top of the group."}, {"heading": "II. CAUSE AND EFFECT IN THE POTENTIAL OUTCOME FRAMEWORK", "text": "We aim to find out if there is a causal relationship between X and Y. For a simple discussion, we consider that X = 1 is a treatment and Y = 1 is recovery. We will determine whether the treatment is effective for the treatment, mainly after introduction to [13].The potential outcome or counterfactual model [17] is a well-established framework for a causal conclusion. Here, we present the basic concepts of the model and a principle for estimating the average causal effect, mainly after introduction to [13].With the potential outcome, an individual i in a population has two potential outcomes for treatment. X: Y 1i in taking the treatment and Y 0 in not taking the treatment. We say that Y 1i is the potential outcome in the treatment state and Y 0i is the potential outcome in the control state. Then we have the following definition."}, {"heading": "III. A DECISION TREE MAY NOT ENCODE CAUSAL RELATIONSHIPS", "text": "Decision trees are a popular classification model, with two types of nodes: branching and leaf nodes. A branch node is a predictive attribute, and each of its values represents an election, leading to another branch node or leaf representing a class. Now, we use the potential result model to explain why decision trees cannot encode causalities. Example 1: The dataset and its associated decision tree as shown in Figure 2. The decision tree has explained the data set perfectly, but the tree does not represent causal relationships. Path A = 1: Y = 1, with the difference in probabilities, (prob: Y = 1 | A = 1) \u2212 prob: 0 | A = 1: 1, represents a causal relationship."}, {"heading": "IV. FROM NORMAL DECISION TREES TO CAUSAL DECISION TREES", "text": "Let X = {X1, X2,.., Xm} be a set of predictive attributes, where xi-0, 1} for 1 \u2264 i \u2264 m and Y be a result attribute where y-0, 1}. Dataset D contains n datasets that perform various assignments of values for X and Y, each of which represents the recording of an observation. Suppose X contains all the attributes needed to characterize an individual, and the dataset is large, and therefore there is no bias in the sampling process."}, {"heading": "A. Why a decision tree may not encode causal relationships?", "text": "The construction of a decision tree follows a divide-and-conquer strategy, and the most important decision to be made in construction is to select the attribute as a branch node. Information gain, information gain ratio, or Gini index can be used to select a branch node [9]. These criteria differ slightly, but they all aim to find a discriminatory attribute in context. Definition 3 (discriminatory attribute) In the face of a data set D \u2032, a discriminatory attribute is the attribute Xi, so that | prob (Y = 1) \u2212 prob (Y = 0 | Xi = 1) | is maximized. Note that D \u2032 is a subdata set (of D) defined by the attribute values in the prefix path of the current branch node. It is a context-specific dataset (see Section IV-C for details). In the following profile, we will discuss why Y is not a decision tree."}, {"heading": "B. A measure for causal effect", "text": "Based on the previous discussion to estimate the causal effect of a predictive attribute Q on the result Y, we layer a data set using X\\ {Q} so that within each layer there is no observable difference between the records. The following table summarizes the statistics of the layer sk in which S = sk.sk Y = 1 Y = 0 Total Q = 1 n11k n1k Q = 0 n21k n2ktotal n.2k n.. kThe odds ratio (measurement of the difference of Y between groups Q = 1 Y = 0) of the layer k = 0 Total Q = 1 n11k n1k) / (n12kn21k) or equivalent, ln (n22k).kThe ratio (measurement of the difference of Y between groups Q = 1 and Q = 0) of the layer k is (n11kn22k)."}, {"heading": "C. Causal decision trees", "text": "Our goal is to build a causal decision tree (CDT) in which a non-leaf node = > Q = = 11Q = = causal attribute, an edge denotes an assignment of a value of a causal attribute, and a leaf represents an assignment of a value of the result. A path from the root to a leaf represents a series of assignments of values of the attributes and a highly probable result value. (A CDT differs from a normal decision tree in which each of its non-leaf nodes has a causal interpretation of the result, i.e. a non-leaf causal value and the causal attribute have a context-specific causal relationship as defined below. Definition 4 (Context) Let P-X, then a value assignment of P = p, be called a causal relationship, and (D | P =) becomes a context-specific dataset = 5 in the context of the data specific P."}, {"heading": "V. CAUSAL DECISION TREE ALGORITHM", "text": "Decision trees have the following advantages: (1) The divide-and-conquer strategy of decision tree induction is very efficient; a decision tree construction algorithm is scalable to the dataset size and number of attributes; this is a major advantage in the exploration of big data; (2) Decision trees explore both global and context-specific relationships, and the latter provide detailed explanations for the former. Together, they provide comprehensive explanations for a dataset. In this paper, we use these advantages for exploring causal relationships. However, the challenges of building a CDT are comprehensive: (1) The criterion for selecting a branched attribute for a normal decision tree must be replaced by causality. In our algorithm, we use the Mantel-Haenszel test, a statistically sound method for testing meaningful signals; (2) The time complexity for mantle-hazel testing quadratic datasets is the dataset size on a dataset."}, {"heading": "VI. EXPERIMENTS", "text": "To evaluate CDT, three sets of experiments are conducted: First, we are experimenting with 2 real and 1 synthetic data sets to show that CDT is able to identify more interpretable relationships when compared with normal decision trees. Normal decision trees are built using the C4.5 algorithm implemented in Weka. [9] It is difficult to evaluate discovered causal relationships because we do not have the basic truths (true causal relationships) for most real data sets. It is also impossible to apply a method to evaluate classifiers to evaluate causal discovery results, since a model that does not contain causal relationships can provide an accurate classification, such as the decision tree in Figure 2. Therefore, we are taking a reasonable approach to perform the evaluation by using two sets of data from which the results might be useful for ordinary people. We are examining the results to see if they are reasonable, and putting the boxes of CDT data on top of the normal decision trees are built on the basis of the CDT."}, {"heading": "A. CDT finds more interpretable relationships", "text": "In fact, most of them are able to survive by themselves if they are not able to play by the rules, \"he told the German Press Agency in an interview with\" Welt am Sonntag. \""}, {"heading": "B. CDT identifies causal relationships", "text": "In this context, it should be noted that this project is a project which is, first and foremost, a project which focuses on the needs of the people."}, {"heading": "C. Scalability of the CDT algorithm", "text": "We test the scalability of the CDT algorithm by comparing it with the C4.5 [9] algorithm implemented in Weka [8] and the PC algorithm [21]. We use 12 synthetic data sets created using the same procedure as the data sets in Section VI-B-1. To be fair between the data sets, we chose the nodes to the same extent as the target variables. The comparisons were made with the same desktop computer (quad-core CPU 3.4 GHz and 16 GB of memory).The comparison results are in Figure 7. The runtime of the CDT is almost linear to the size of the data sets and the number of attributes. It is less efficient than C4.5, but more efficient than PC. The results have shown that the proposed CDT is practical for high-dimensional and large data sets."}, {"heading": "VII. DISCUSSIONS", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "A. Difference from other causal trees", "text": "This year, the time has come for only one person to decide whether he or she will be able to save the world or whether he or she will be able to save the world."}, {"heading": "B. Assumptions and practical considerations", "text": "This year, it has reached the stage where it will be able to take the lead."}, {"heading": "VIII. RELATED WORK", "text": "The discovery of causal relationships in passively observed data has attracted enormous research efforts in recent decades due to the high cost, low efficiency, and unknown feasibility of experiments, as well as the increasing availability of observational data. This is due to the theoretical development of a group of statisticians, philosophers, and computer scientists, including Pearl [17], Spiral, Glyphosate, and others that we have seen as graphic causal models in causality determination. These graphic models include causal networks (CBNs), which are the most developed and used. Many algorithms have been developed for learning CBNs, [20] in which there is a complex relationship with causal relationships."}, {"heading": "IX. CONCLUSION", "text": "In this paper, we have proposed Causal Decision Trees (CDTs), a novel model for displaying and discovering causal relationships in data. A CDT provides a compact and accurate graphical representation of the causal relationships between a number of predicate attributes and an outcome attribute. Context-specific causal relationships represented by a CDT are of great practical utility and are not coded by existing causal models. The algorithm developed for constructing a CDT uses the strategy of sharing and conquest to build a normal decision tree and is therefore fast and scalable to large datasets. The criterion used for selecting branched attributes of a CDT is based on the well-established potential outcome model and partial association tests, ensuring the causal semantics of the Trees. Given the increasing availability of big data, we believe that the proposed CDTs will be a more powerful tool for automating decision-making in various areas."}], "references": [{"title": "Local causal and Markov blanket induction for causal discovery and feature selection for classification part I: Algorithms and empirical evaluation", "author": ["C.F. Aliferis", "A. Statnikov", "I. Tsamardinos", "S. Mani", "X.D. Koutsoukos"], "venue": "J. of Machine Learning Research,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2010}, {"title": "The detection of partial association, I: The 2\u00d72 Case", "author": ["M.W. Birch"], "venue": "J. of the R. Stat. Soc.,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 1964}, {"title": "Contextspecific independence in Bayesian networks", "author": ["C. Boutilier", "N. Friedman", "M. Goldszmidt", "D. Koller"], "venue": "In The 12th Conf. on Uncertainty in Artificial Intelligence (UAI),", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 1996}, {"title": "Large-sample learning of Bayesian networks is NP-hard", "author": ["D. Chickering", "D. Heckerman", "C. Meek"], "venue": "J. of Machine Learning Research,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2004}, {"title": "Statistical Methods for Rates and Proportions", "author": ["J.L. Fleiss", "B. Levin", "M.C. Paik"], "venue": null, "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2003}, {"title": "Identifying Markov blankets with decision tree induction", "author": ["L. Frey", "D. Fisher", "I. Tsamardinos", "C. Aliferis", "A. Statnikov"], "venue": "In Data Mining,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2003}, {"title": "The WEKA data mining software: An update", "author": ["Q. Hall", "et. al"], "venue": "SIGKDD Explorations,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2009}, {"title": "Data Mining: Concepts and Techniques", "author": ["J. Han", "M. Kamber", "J. Pei"], "venue": null, "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2011}, {"title": "Mining causal association rules", "author": ["J. Li", "T. Le", "L. Liu", "J. Liu", "Z. Jin", "B. Sun"], "venue": "In Data Mining Workshops (ICDMW),", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2013}, {"title": "Statistical aspects of the analysis of data from retrospective studies of disease", "author": ["N. Mantel", "W. Haenszel"], "venue": "J. of the National Cancer Institute,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 1959}, {"title": "Counterfactuals and Causal Inference: Methods and Principles for Social Research", "author": ["S. Morgan", "C. Winship"], "venue": null, "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2007}, {"title": "Matching estimators of causal effects: Prospects and pitfalls in theory and practice", "author": ["S.L. Morgan", "D.J. Harding"], "venue": "Sociological Methods & Research,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2006}, {"title": "Learning Bayesian Networks", "author": ["R.E. Neapolitan"], "venue": null, "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2003}, {"title": "Explanation Trees for Causal Bayesian Networks", "author": ["U.H. Nielsen", "J. philippe Pellet", "A. Elisseeff"], "venue": "In Uncertainty in Artificial Intelligence,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2008}, {"title": "Variable selection for highdimensional linear models: partially faithful distributions and the PCsimple", "author": ["M.K.P. B\u00fcehlmann", "M. Maathuis"], "venue": "algorithm. Biometrika,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2010}, {"title": "Causality: Models, Reasoning, and Inference", "author": ["J. Pearl"], "venue": null, "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2000}, {"title": "Design of Observational Studies", "author": ["P.R. Rosenbaum"], "venue": null, "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2010}, {"title": "Experimental and Quasi-Experimental Designs for Generalized Causal Inference", "author": ["W.R. Shadish", "T.D. Thomas", "D.T. Campbell"], "venue": null, "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2002}, {"title": "Introduction to causal inference", "author": ["P. Spirtes"], "venue": "J. of Machine Learning Research,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2010}, {"title": "Causation, Predication, and Search", "author": ["P. Spirtes", "C.C. Glymour", "R. Scheines"], "venue": null, "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2001}, {"title": "Discovery of causal rules using partial association", "author": ["Z.Jin", "J. Li", "L. Liu", "T.D. Le", "B. Sun", "R. Wang"], "venue": "In Data Mining (ICDM),", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2012}], "referenceMentions": [{"referenceID": 17, "context": "Causal relationships are normally identified with experiments, such as randomised controlled trials [19], which are effective but expensive and often impossible to be conducted.", "startOffset": 100, "endOffset": 104}, {"referenceID": 16, "context": "Causal relationships can also be found by observational studies, such as cohort studies and case control studies [18].", "startOffset": 113, "endOffset": 117}, {"referenceID": 7, "context": "Decision trees [9] are a good example of classification methods, and they have been widely used in many areas, including social and medical data analyses.", "startOffset": 15, "endOffset": 18}, {"referenceID": 15, "context": "The potential outcome or counterfactual model [17], [12] is a well established framework for causal inference.", "startOffset": 46, "endOffset": 50}, {"referenceID": 10, "context": "The potential outcome or counterfactual model [17], [12] is a well established framework for causal inference.", "startOffset": 52, "endOffset": 56}, {"referenceID": 11, "context": "Here we introduce the basic concepts of the model and a principle for estimating the average causal effect, mainly following the introduction in [13].", "startOffset": 145, "endOffset": 149}, {"referenceID": 7, "context": "Information gain, information gain ratio or Gini index can be used to choose a branching node [9].", "startOffset": 94, "endOffset": 97}, {"referenceID": 4, "context": "For binary outcomes, odds ratio [6] is suitable for measuring the difference of two outcomes.", "startOffset": 32, "endOffset": 35}, {"referenceID": 1, "context": "Partial association test [3] is a means to achieve this.", "startOffset": 25, "endOffset": 28}, {"referenceID": 9, "context": "This is the test statistic of the Mantel-Haenszel test [11], [3].", "startOffset": 55, "endOffset": 59}, {"referenceID": 1, "context": "This is the test statistic of the Mantel-Haenszel test [11], [3].", "startOffset": 61, "endOffset": 64}, {"referenceID": 1, "context": "Such consistency is a strong indication of direct causal relationship between two variables [3].", "startOffset": 92, "endOffset": 95}, {"referenceID": 7, "context": "5 algorithm [9] implemented in Weka [8].", "startOffset": 12, "endOffset": 15}, {"referenceID": 6, "context": "5 algorithm [9] implemented in Weka [8].", "startOffset": 36, "endOffset": 39}, {"referenceID": 19, "context": "Secondly experiments with synthetic data sets are carried out to demonstrate the ability of CDT in finding causal relationships comparing to the commonly used Bayesian network learning method, the PC algorithm [21].", "startOffset": 210, "endOffset": 214}, {"referenceID": 14, "context": "Our solution is to use v1 as the context variable, and apply PC-select [23] (also known as PC-simple [16]) to the two partitions of the data set respectively, one partition containing all the samples with (v1 = 0) and one containing all the samples with (v1 = 1) (while the v1 column is excluded).", "startOffset": 101, "endOffset": 105}, {"referenceID": 7, "context": "5 [9] algorithm implemented in Weka [8] and the PC Algorithm [21].", "startOffset": 2, "endOffset": 5}, {"referenceID": 6, "context": "5 [9] algorithm implemented in Weka [8] and the PC Algorithm [21].", "startOffset": 36, "endOffset": 39}, {"referenceID": 19, "context": "5 [9] algorithm implemented in Weka [8] and the PC Algorithm [21].", "startOffset": 61, "endOffset": 65}, {"referenceID": 2, "context": "Difference from other causal trees In this section, we will differentiate our CDTs from other causal trees derived from causal Bayesian networks, including the conditional probability table tree (CPT-tree) [4] and causal explanation tree [15].", "startOffset": 206, "endOffset": 209}, {"referenceID": 13, "context": "Difference from other causal trees In this section, we will differentiate our CDTs from other causal trees derived from causal Bayesian networks, including the conditional probability table tree (CPT-tree) [4] and causal explanation tree [15].", "startOffset": 238, "endOffset": 242}, {"referenceID": 18, "context": "A causal Bayesian network (CBN) [20] consists of a causal structure of a directed acyclic graph (DAG), with nodes and arcs representing random variables and causal relationships between the variables respectively, and a joint probability distribution of the variables.", "startOffset": 32, "endOffset": 36}, {"referenceID": 2, "context": "The conditional probability table tree (CPT-tree) [4] is designed to summarise the conditional probability tables of a CBN for concise presentation and fast inference.", "startOffset": 50, "endOffset": 53}, {"referenceID": 13, "context": "A causal explanation tree [15] aims at explaining the outcome values using a series of value assignments of a subset of attributes in a CBN.", "startOffset": 26, "endOffset": 30}, {"referenceID": 19, "context": "In the causal Bayesian network discovery framework, some assumptions, such as causal Markov condition, faithfulness and causal sufficiency [21], are used to ensure the causal semantics of the discoveries.", "startOffset": 139, "endOffset": 143}, {"referenceID": 15, "context": "retical development by a group of statisticians, philosophers and computer scientists, including Pearl [17], Spirtes, Glymour [21] and others, we have seen graphical causal models playing dominant role in causality discovery.", "startOffset": 103, "endOffset": 107}, {"referenceID": 19, "context": "retical development by a group of statisticians, philosophers and computer scientists, including Pearl [17], Spirtes, Glymour [21] and others, we have seen graphical causal models playing dominant role in causality discovery.", "startOffset": 126, "endOffset": 130}, {"referenceID": 18, "context": "Among these graphical models, causal Bayesian networks (CBNs) [20] have been the most developed and used one.", "startOffset": 62, "endOffset": 66}, {"referenceID": 12, "context": "Many algorithms have been developed for learning CBNs [14], [20].", "startOffset": 54, "endOffset": 58}, {"referenceID": 18, "context": "Many algorithms have been developed for learning CBNs [14], [20].", "startOffset": 60, "endOffset": 64}, {"referenceID": 3, "context": "However in general learning a complete CBN is NP-hard [5] and the methods are able to handle a CBN with only tens of variables, or hundreds if the causal relationships are sparse [20].", "startOffset": 54, "endOffset": 57}, {"referenceID": 18, "context": "However in general learning a complete CBN is NP-hard [5] and the methods are able to handle a CBN with only tens of variables, or hundreds if the causal relationships are sparse [20].", "startOffset": 179, "endOffset": 183}, {"referenceID": 14, "context": "Existing methods for local causal discovery around a given a target fall into two broad categories: (1) Methods that adapt the algorithms or ideas for learning a complete CBN into local causal discovery, such as PC-Simple [16], [23], a simplified version of the well-known PC algorithm [21] for CBN learning; and HITON-PC [1], which applies the basic idea of PC to find variables strongly (and causally) related to a given target; (2) Methods that are designed to exploit the high efficiency of popular data mining approaches and the causal discovery ability of traditional statistical methods, including the work in [22] and [10], both using association rule mining for identifying causal rules; and the decision tree based approach [7] for finding the Markov blanket of a given variable.", "startOffset": 222, "endOffset": 226}, {"referenceID": 19, "context": "Existing methods for local causal discovery around a given a target fall into two broad categories: (1) Methods that adapt the algorithms or ideas for learning a complete CBN into local causal discovery, such as PC-Simple [16], [23], a simplified version of the well-known PC algorithm [21] for CBN learning; and HITON-PC [1], which applies the basic idea of PC to find variables strongly (and causally) related to a given target; (2) Methods that are designed to exploit the high efficiency of popular data mining approaches and the causal discovery ability of traditional statistical methods, including the work in [22] and [10], both using association rule mining for identifying causal rules; and the decision tree based approach [7] for finding the Markov blanket of a given variable.", "startOffset": 286, "endOffset": 290}, {"referenceID": 0, "context": "Existing methods for local causal discovery around a given a target fall into two broad categories: (1) Methods that adapt the algorithms or ideas for learning a complete CBN into local causal discovery, such as PC-Simple [16], [23], a simplified version of the well-known PC algorithm [21] for CBN learning; and HITON-PC [1], which applies the basic idea of PC to find variables strongly (and causally) related to a given target; (2) Methods that are designed to exploit the high efficiency of popular data mining approaches and the causal discovery ability of traditional statistical methods, including the work in [22] and [10], both using association rule mining for identifying causal rules; and the decision tree based approach [7] for finding the Markov blanket of a given variable.", "startOffset": 322, "endOffset": 325}, {"referenceID": 20, "context": "Existing methods for local causal discovery around a given a target fall into two broad categories: (1) Methods that adapt the algorithms or ideas for learning a complete CBN into local causal discovery, such as PC-Simple [16], [23], a simplified version of the well-known PC algorithm [21] for CBN learning; and HITON-PC [1], which applies the basic idea of PC to find variables strongly (and causally) related to a given target; (2) Methods that are designed to exploit the high efficiency of popular data mining approaches and the causal discovery ability of traditional statistical methods, including the work in [22] and [10], both using association rule mining for identifying causal rules; and the decision tree based approach [7] for finding the Markov blanket of a given variable.", "startOffset": 617, "endOffset": 621}, {"referenceID": 8, "context": "Existing methods for local causal discovery around a given a target fall into two broad categories: (1) Methods that adapt the algorithms or ideas for learning a complete CBN into local causal discovery, such as PC-Simple [16], [23], a simplified version of the well-known PC algorithm [21] for CBN learning; and HITON-PC [1], which applies the basic idea of PC to find variables strongly (and causally) related to a given target; (2) Methods that are designed to exploit the high efficiency of popular data mining approaches and the causal discovery ability of traditional statistical methods, including the work in [22] and [10], both using association rule mining for identifying causal rules; and the decision tree based approach [7] for finding the Markov blanket of a given variable.", "startOffset": 626, "endOffset": 630}, {"referenceID": 5, "context": "Existing methods for local causal discovery around a given a target fall into two broad categories: (1) Methods that adapt the algorithms or ideas for learning a complete CBN into local causal discovery, such as PC-Simple [16], [23], a simplified version of the well-known PC algorithm [21] for CBN learning; and HITON-PC [1], which applies the basic idea of PC to find variables strongly (and causally) related to a given target; (2) Methods that are designed to exploit the high efficiency of popular data mining approaches and the causal discovery ability of traditional statistical methods, including the work in [22] and [10], both using association rule mining for identifying causal rules; and the decision tree based approach [7] for finding the Markov blanket of a given variable.", "startOffset": 734, "endOffset": 737}, {"referenceID": 5, "context": "In terms of using decision trees as a means for causality investigation, except from the above mentioned method for identifying Markov blankets [7], most existing work takes decision trees as a tool for causal relationship representation and/or inference, assuming that the causal relationships are known in advance.", "startOffset": 144, "endOffset": 147}, {"referenceID": 2, "context": "Examples include the CPT-trees [4] and causal explanation tree [15] introduced in the Discussions section, which are both derived from a known causal Bayesian network.", "startOffset": 31, "endOffset": 34}, {"referenceID": 13, "context": "Examples include the CPT-trees [4] and causal explanation tree [15] introduced in the Discussions section, which are both derived from a known causal Bayesian network.", "startOffset": 63, "endOffset": 67}], "year": 2015, "abstractText": "Uncovering causal relationships in data is a major objective of data analytics. Causal relationships are normally discovered with designed experiments, e.g. randomised controlled trials, which, however are expensive or infeasible to be conducted in many cases. Causal relationships can also be found using some well designed observational studies, but they require domain experts\u2019 knowledge and the process is normally time consuming. Hence there is a need for scalable and automated methods for causal relationship exploration in data. Classification methods are fast and they could be practical substitutes for finding causal signals in data. However, classification methods are not designed for causal discovery and a classification method may find false causal signals and miss the true ones. In this paper, we develop a causal decision tree where nodes have causal interpretations. Our method follows a well established causal inference framework and makes use of a classic statistical test. The method is practical for finding causal signals in large data sets. Keywords\u2014Decision tree, Causal relationship, Potential outcome model, Partial association", "creator": "TeX"}}}