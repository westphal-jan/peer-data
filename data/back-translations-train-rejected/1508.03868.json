{"id": "1508.03868", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "16-Aug-2015", "title": "Visual Affect Around the World: A Large-scale Multilingual Visual Sentiment Ontology", "abstract": "Every culture and language is unique. Our work expressly focuses on the uniqueness of culture and language in relation to human affect, specifically sentiment and emotion semantics, and how they manifest in social multimedia. We develop sets of sentiment- and emotion-polarized visual concepts by adapting semantic structures called adjective-noun pairs, originally introduced by Borth et al. (2013), but in a multilingual context. We propose a new language-dependent method for automatic discovery of these adjective-noun constructs. We show how this pipeline can be applied on a social multimedia platform for the creation of a large-scale multilingual visual sentiment concept ontology (MVSO). Unlike the flat structure in Borth et al. (2013), our unified ontology is organized hierarchically by multilingual clusters of visually detectable nouns and subclusters of emotionally biased versions of these nouns. In addition, we present an image-based prediction task to show how generalizable language-specific models are in a multilingual context. A new, publicly available dataset of &gt;15.6K sentiment-biased visual concepts across 12 languages with language-specific detector banks, &gt;7.36M images and their metadata is also released.", "histories": [["v1", "Sun, 16 Aug 2015 21:43:59 GMT  (8952kb,D)", "http://arxiv.org/abs/1508.03868v1", "11 pages, to appear at ACM MM'15"], ["v2", "Sat, 22 Aug 2015 16:33:13 GMT  (8952kb,D)", "http://arxiv.org/abs/1508.03868v2", "11 pages, to appear at ACM MM'15"], ["v3", "Wed, 7 Oct 2015 19:07:14 GMT  (8952kb,D)", "http://arxiv.org/abs/1508.03868v3", "11 pages, to appear at ACM MM'15"]], "COMMENTS": "11 pages, to appear at ACM MM'15", "reviews": [], "SUBJECTS": "cs.MM cs.CL cs.CV cs.IR", "authors": ["brendan jou", "tao chen", "nikolaos pappas", "miriam redi", "mercan topkara", "shih-fu chang"], "accepted": false, "id": "1508.03868"}, "pdf": {"name": "1508.03868.pdf", "metadata": {"source": "CRF", "title": "Visual Affect Around the World: A Large-scale Multilingual Visual Sentiment Ontology", "authors": ["Brendan Jou", "Tao Chen", "Nikolaos Pappas", "Miriam Redi", "Mercan Topkara", "Shih-Fu Chang"], "emails": ["bjou@ee.columbia.edu", "taochen@ee.columbia.edu", "npappas@idiap.ch", "redi@yahoo-inc.com", "mercan@jwplayer.com", "sfchang@ee.columbia.edu", "Permissions@acm.org."], "sections": [{"heading": null, "text": "Categories and Subject Descriptions H.5.4 [Information Interfaces and Presentation]: Hypertext / Hypermedia; I.2.10 [Artificial Intelligence]: Vision and Scene UnderstandingKeywords Multilingual; Language; Cultures; Cross-cultural; Emotion; Sentiment; Ontology; Concept Detection; Social Multimedia * Means equivalent participation. Permission to make digital or printed copies of all or part of this work for personal or commercial use is granted without payment, provided that copies are not made or distributed for profit or commercial purposes, and that copies bear this notice and the full quote on the first page. Copyright for components of this work owned by others than ACM's must be respected. Credit abstraction is permitted."}, {"heading": "1. INTRODUCTION", "text": "In fact, most people who are able to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move and to move."}, {"heading": "2. RELATED WORK", "text": "In fact, it is such that most of them will be able to move to another world, in which they are able to move to another world, in which they are able to move to another world, in which they are able to change the world, in which they are able to move, in which they are able to change the world, in which they live, in which they live, in which they live, in which they live, in which she lives, in which she lives, in which she lives, in which she lives, in which she lives, in which she lives, in which she lives, in which she lives, in which she lives, in which she lives, in which she lives, in which she lives, in which she lives, in which she lives, in which she lives, in which she lives, in which she lives, in which she lives, in which she lives, in which she lives, in which she lives, in which she lives, in which she lives, in which she lives, in which she lives, in which she lives, in which she lives, in which she lives, in which she lives, in which she lives, in which she lives, in which she lives, in which she lives, in which she lives, in which she lives, in which she lives, in which she lives, in which she lives, in which she lives, in which she lives, in which she lives, in which she lives, in which she lives, in which she lives, in which she lives, in which she lives, in which she lives, in which she lives, in which she lives, in which she lives, in which she lives, in which she lives, in which she lives, in which she lives, in which she lives, in which she lives, in which she lives, in which she lives, in which she lives, in which she lives, in which she lives, in which she lives, in which she lives, in which she lives, in which she lives, in which she lives, in which she lives, in which she lives, in which she lives, in which she lives, in which she lives, in which she lives, in which she lives, in which she lives, in which she lives, in which she lives, in which she lives, in which she lives, in which she lives, in which she lives, in which she lives, in which she lives, in which she lives, in which she lives, in"}, {"heading": "3. ONTOLOGY CONSTRUCTION", "text": "An overview of the proposed method for multilingual visual sentiment concepts ontology is shown in 2Note that we make language and culture interchangeable. We define language as the \"lens\" through which we can observe culture. Thus, while the two can be distinguished from each other, they are automatically labeled by a language-specific part of language. In the first phase, we get a set of images and adjective noun combinations selected from words in psychology, such as [34] or [11]."}, {"heading": "3.1 Adjective-Noun Pair Discovery", "text": "This psychology ontology was chosen because it consists of graduated intensities for several basic emotions that offer a richer set of emotional values compared to alternatives such as [11]; it has also been shown that 3www.flickr.com can be useful for VSO [5]. Plutchik emotions are organized by eight basic emotions, each of which has three values: ecstasy > joy > serenity > admiration > confidence > acceptance; terror > fear > fear; astonishment > distraction; grief > sadness > bitterness; disgust > boredom > anger > anger; and, vigilance > expectation > interest; multilingual query construction: To obtain seeds for each language, we recruited 12 native and competent speakers to provide a set of translated or synonymous keywords for those of the 24 plutchik emotions."}, {"heading": "3.2 Filtering Candidate Adjective-Noun Pairs", "text": "This year, the time has come for it to be able to govern the country."}, {"heading": "3.3 Crowdsourcing Validation", "text": "Dre rf\u00fc ide rf\u00fc rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the r"}, {"heading": "4. DATASET ANALYSIS & STATISTICS", "text": "To limit the size of our dataset, we did not download more than 1,000 images per ANP query, and also imposed a limit of no more than 20 images from any uploader on Flickr to increase visual diversity; the 1,000 images selected were selected from the pool of image tag search results found, but in the event that this pool contained less than 1,000 images, we also expanded the pool to include image title and image description searches or metadata searches. Selection from the results pool was always randomized and a small number of images that removed or altered patch or uploader were removed halfway. In total, we downloaded 7,368,364 images in 15,630 ANPs for the 12 languages, with English (4,049,507), Spanish (1,417,781) and Italian (85,464) contributing the most images."}, {"heading": "4.1 Comparison with VSO [5]", "text": "In order to verify and test the effectiveness of our MVSO, we offer a comparison of our extracted English visual sentiment ontology with that of VSO [5] along the dimensions of size (number of ANPs) and variety of nouns and adjectives (Figure 3). Figure 3a compares the overlap of English MVSO with VSO alone after applying all filter criteria except subsampling, which could exclude ANPs belonging to VSO. As mentioned above, about 86% overlap between them. Since we vary a frequency threshold t (as described in Figure 3.2) on image mark counting, the overlaps converge to 100%. This confirms that the popular ANPs covered by VSO are also covered by MVSO, an interesting result given the differences in crawl periods and approaches of Snos. In Figure 3b, we show that there are many more SANPs in our English SANPs compared to our English SANPs."}, {"heading": "4.2 Sentiment Distributions", "text": "Returning to our research motivation from the introduction, an interesting question is to ask which languages tend to be more positive or more negative in terms of visual content. To answer this, we calculated the mean sentiment value of all ANPs of each language and ranked languages as in Fig. 4. To account for the popularity difference between ANPs, we replicated each ANP k times, where k corresponds to the number of images marked with the ANP, up to an upper limit L = \u03b1 \u00d7 Avgi, where Avgi in the ith language is the average number of images per ANP. Varying \u03b1 values result in different medians and distributions, but the trend towards distinguishing positive from negative languages has been quite stable. We show the case where \u03b1 = 3 in Fig. 4 shows a general tendency towards positive sentiment in all languages, with Spanish having the highest positive mood, followed by Italian. This surprising observation is indeed compatible with previous research showing that there is a universal positive tendency towards positive sentiment in all languages, with Spanish being the most strongly followed by Sentiment (where the language is Arabic)."}, {"heading": "4.3 Emotion Distributions", "text": "Another interesting question arises when looking at the occurrence of ANPs with emotions in different languages. \"While our adjective-noun-pair concepts were chosen to be sentimental-tendentious, emotions still represent the root of our framework, as we have built MVSO from the selected emotion concepts. So, apart from emotions that focus only on positivity / negativity, what are likely associations of ANPs with emotions for each language? Which emotions are most commonly counted in different languages? Given the number of keywords E (l) = {e (l) ij = 1. 24, j = 1. ni} that describe each emotion i per language l, where ni is the number of keywords per emotion i that belong to language l, recorded as x (l), and the number of images that are tagged with both ANP x and emotion."}, {"heading": "5. CROSS-LINGUAL MATCHING", "text": "We went looking for the topics commonly mentioned in different cultures and languages, and we analyzed the translations of Chinese translations into different languages that we all used as the basis for our translations.We took two approaches to explore this: the exact translation of ANPs, which uses all of our validation filters in Sec. 3.2 for this analyse.Exact Alignment: We put together ANPs from each language that have exactly the same translation. For example, old books were the translations for one or more ANPs from seven languages, including China, Livres Anciens (French), Vecchi Libri (Italian), Clans (Italian), Clanos (Spanish), eski Kitaplar (Turkish).The translation we covered from the largest number of languages was beautiful girls with ANPs from ten languages."}, {"heading": "6. VISUAL SENTIMENT PREDICTION", "text": "In order to test the effectiveness of a vision-based approach to visual understanding when crossing languages, we designed and built language-specific sentiment predictors using the data collected with MVSO. Inspired by the work in [17], we investigated to what extent the visual sensations of a particular language can be predicted by sentiment models of other languages. We opted for a sentiment prediction task, i.e. predicting whether an image is of positive or negative sensation, because there is a large number of works that explicitly focus on feelings (e.g. [5, 42, 43]) compared with emotion prediction. More importantly, we wanted to reduce the number of variables to be analyzed, since our primary goal was to uncover cross-language differences."}, {"heading": "6.1 Visual Sentiment Concept Detectors", "text": "To build our database of visual concept detectors of ANPs, we specifically used Convolutionary Neural Networks (CNNs), using an AlexNet-like architecture [24] for their good performance in large-scale detection and recognition tasks. To train our detector bank, we refined six models, one for each language, in which network weights were initialized with DeepSentiBank [6], an AlexNet model trained on the VSO [5] dataset. This fine-tuning approach ensures that each network begins with weights that are already somewhat \"affectively distorted,\" the basic learning rates are set at 0.001, and the number of initial neurons in the last fully connected layer is set to the number of ANPs in each language. Steps to reduce the second-level learning rate were set proportionally to the number of training images per language. For a single language, fine-scanning took between 12 and 40 hours for the affinity, the Gaffinity is the highest we expect on a single NVIX GT1, and the highest accuracy in a single class."}, {"heading": "6.2 Sentiment Prediction on Flickr", "text": "We used the CNN-based visual concept models trained for each language to extract image characteristics and use the ANPs \"sentiment scores as monitored labels to learn sentiment prediction models. We compared different levels of CNN models as image characteristics. To simplify the process, we divided the ANP sentiment scores calculated with Eq. (1) into positive and negative classes and learned a binary classifier using linear SVMs, one for each language. The training images are those associated with ANP's strong sentiment scores (absolute values higher than 0.05). Splits of training and test sets were layered in all languages, so that the amount of training and tests for positive and negative sentiment classes were the same for fair linguistic experiment comparisons. We found that the soft-max output features from the previous layer of the CNN language model were the best results for each language, and that CNN's results were the best results for each language."}, {"heading": "7. CONCLUSION & FUTURE WORK", "text": "We have proposed a new multilingual method of discovery for visual sentiment concepts and demonstrated their effectiveness on a social multimedia platform for 12 languages. We have based our approach on the psychology theory that emotions are culture-specific and have an inherent linguistic context, so we have shown how we can use language-specific parts of language labeling along with progressive filtering to achieve coverage and diversity of visual affect concepts in multiple languages. In addition, we have presented a two-tiered hierarchical cluster approach to unify our ontology in different languages. And we are making our multilingual Visual Sentiment Ontology (MVSO), pre-crowdsourcing, as well as post- and image data sets available to the public. A cross-lingual analysis of our large-area MVSO and image sets using semantic mapping and visual sentiment prediction suggest that emotions are not necessarily perceived as cultural commonalities, but rather common knowledge, where our metaphors do show that our metaphors are different."}, {"heading": "8. ACKNOWLEDGMENTS", "text": "The research was sponsored by the U.S. Defense Advanced Research Projects Agency (DARPA) under the Social Media in Strategic Communication (SMISC) program under contract number W911NF-12-C-0028. The views and conclusions contained in this document are those of the author (s) and should not be interpreted as representing official policy of the U.S. Defense Advanced Research Projects Agency or the U.S. Government. The U.S. Government is authorized to reproduce and distribute reproductions for government purposes, notwithstanding copyright notices herein. Co-author B. Jou was supported by the Department of Defense (DoD) through the National Defense Science & Engineering Graduate Fellowship (NDSEG) Program. And Co-author N. Pappas was supported by InEvent (FP7-ICT n. 287872) and MODERN Sinergia (CRSII2 147653)."}, {"heading": "9. REFERENCES", "text": "In WASSA, 2012. [2] C. Banea, R. Mihalcea, J. Wiebe, and S. Hassan. Multilingual Subjectivity Analysis with Machine Translation. In EMNLP, 2008. [3] M. Bautin, L. Vijayarenu, and S. Skiena. International sentimental analysis for news and blogs. In ICWSM, 2013. [5] D. Borth, T. Chen, R. Ji, and S.-F. Chang. SentiBank: Large-scale ontology and classifiers for detecting sentiments and emotion in visual content. In ACM MM, 2013. [D. Borth, T. Chen, T. Breuel, and S.-F. Chang. Large-scale visual sentiments ontology and detectors using adjective noun pairs. In ACM, 2013. T. Borth."}], "references": [{"title": "Multilingual sentiment analysis using machine translation", "author": ["A. Balahur", "M. Turchi"], "venue": "WASSA,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2012}, {"title": "Multilingual subjectivity analysis using machine translation", "author": ["C. Banea", "R. Mihalcea", "J. Wiebe", "S. Hassan"], "venue": "In EMNLP,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2008}, {"title": "International sentiment analysis for news and blogs", "author": ["M. Bautin", "L. Vijayarenu", "S. Skiena"], "venue": "In ICWSM,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2008}, {"title": "SentiBank: Large-scale ontology and classifiers for detecting sentiment and emotions in visual content", "author": ["D. Borth", "T. Chen", "R. Ji", "S.-F. Chang"], "venue": "In ACM MM,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2013}, {"title": "Large-scale visual sentiment ontology and detectors using adjective noun pairs", "author": ["D. Borth", "R. Ji", "T. Chen", "T. Breuel", "S.-F. Chang"], "venue": "In ACM MM,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2013}, {"title": "DeepSentiBank: Visual sentiment concept classification with deep convolutional neural networks", "author": ["T. Chen", "D. Borth", "T. Darrell", "S.-F. Chang"], "venue": "arXiv preprint arXiv:1410.8586,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2014}, {"title": "Predicting viewer affective comments based on image content in social media", "author": ["Y.-Y. Chen", "T. Chen", "W.H. Hsu", "H.-Y.M. Liao", "S.-F. Chang"], "venue": "In ICMR,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2014}, {"title": "The Geneva affective picture database: A new 730-picture database focusing on valence and normative significance", "author": ["E.S. Dan-Glauser", "K. Scherer"], "venue": "Behav. Res. Meth.,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2011}, {"title": "Human language reveals a universal positivity bias", "author": ["S. Dodds"], "venue": "PNAS, 112(8),", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2015}, {"title": "Facial expression and emotion", "author": ["P. Ekman"], "venue": "American Psychologist,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 1993}, {"title": "SENTIWORDNET: A publicly available lexical resource for opinion mining", "author": ["A. Esuli", "F. Sebastiani"], "venue": "In LREC,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2006}, {"title": "Parsing Turkish using the lexical functional grammar formalism", "author": ["Z. G\u00fcng\u00f6rd\u00fc", "K. Oflazer"], "venue": "In ACL,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 1994}, {"title": "The interestingness of images", "author": ["M. Gygli", "H. Grabner", "H. Riemenschneider", "F. Nater", "L.V. Gool"], "venue": "In ICCV,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2013}, {"title": "HunPos: An open source trigram tagger", "author": ["P. Hal\u00e1csy", "A. Kornai", "C. Oravecz"], "venue": "In ACL,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2007}, {"title": "Irrational emotions or emotional wisdom? The evolutionary psychology of affect and social behavior", "author": ["M.G. Haselton", "T. Ketelaar"], "venue": "Affect in Soc. Think. and Behav.,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2006}, {"title": "Cross-cultural mood regression for music digital libraries", "author": ["X. Hu", "Y.-H. Yang"], "venue": "In JCDL,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2014}, {"title": "Can we understand van Gogh\u2019s mood?: Learning to infer affects from images in social networks", "author": ["J. Jia", "S. Wu", "X. Wang", "P. Hu", "L. Cai", "J. Tang"], "venue": "In ACM MM,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2012}, {"title": "Caffe: Convolutional architecture for fast feature embedding", "author": ["Y. Jia", "E. Shelhamer", "J. Donahue", "S. Karayev", "J. Long", "R. Girshick", "S. Guadarrama", "T. Darrell"], "venue": "In ACM MM,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2014}, {"title": "The wisdom of social multimedia: Using Flickr for prediction and forecast", "author": ["X. Jin", "A. Gallagher", "L. Cao", "J. Luo", "J. Han"], "venue": "In ACM MM,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2010}, {"title": "Predicting viewer perceived emotions in animated GIFs", "author": ["B. Jou", "S. Bhattacharya", "S.-F. Chang"], "venue": "In ACM MM,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2014}, {"title": "The design of high-level features for photo quality assessment", "author": ["Y. Ke", "X. Tang", "F. Jing"], "venue": "In CVPR,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2006}, {"title": "What makes an image popular", "author": ["A. Khosla", "A. Das Sarma", "R. Hamid"], "venue": "In WWW,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2014}, {"title": "ImageNet classification with deep convolutional neural networks", "author": ["A. Krizhevsky", "I. Sutskever", "G.E. Hinton"], "venue": "In NIPS,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2012}, {"title": "International Affective Picture System (IAPS): Technical manual and affective ratings", "author": ["P. Lang", "M. Bradley", "B. Cuthbert"], "venue": "Technical report, NIMH CSEA,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 1997}, {"title": "Challenges in cross-cultural/multilingual music information seeking", "author": ["J.H. Lee", "J.S. Downie", "S.J. Cunningham"], "venue": "In ISMIR,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2005}, {"title": "Affective image classification using features inspired by psychology and art theory", "author": ["J. Machajdik", "A. Hanbury"], "venue": "In ACM MM,", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2010}, {"title": "Culture and the self: Implications for cognition, emotion, and motivation", "author": ["H.R. Markus", "S. Kitayama"], "venue": "Psychological Review,", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 1991}, {"title": "The social construction of emotions: New directions from culture theory", "author": ["E.D. McCarthy"], "venue": "Social Perspectives on Emotion,", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 1994}, {"title": "Culture and emotion", "author": ["B. Mesquita", "N.H. Frijda", "K. Scherer"], "venue": "Handbook of Cross-cultural Psychology,", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 1997}, {"title": "Learning multilingual subjective language via cross-lingual projections", "author": ["R. Mihalcea", "C. Banea", "J. Wiebe"], "venue": "In ACL,", "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2007}, {"title": "Distributed representations of words and phrases and their compositionality", "author": ["T. Mikolov", "I. Sutskever", "K. Chen", "G. Corrado", "J. Dean"], "venue": "In NIPS,", "citeRegEx": "32", "shortCiteRegEx": "32", "year": 2013}, {"title": "Affective Computing", "author": ["R.W. Picard"], "venue": null, "citeRegEx": "33", "shortCiteRegEx": "33", "year": 1997}, {"title": "Emotion: A Psychoevolutionary Synthesis", "author": ["R. Plutchik"], "venue": null, "citeRegEx": "34", "shortCiteRegEx": "34", "year": 1980}, {"title": "Seconds of sound and vision: Creativity in micro-videos", "author": ["M. Redi", "N. O\u2019Hare", "R. Schifanella", "M. Trevisiol", "A. Jaimes"], "venue": "In CVPR,", "citeRegEx": "35", "shortCiteRegEx": "35", "year": 2014}, {"title": "Culture and the categorization of emotions", "author": ["J.A. Russell"], "venue": "Psychological Bulletin,", "citeRegEx": "36", "shortCiteRegEx": "36", "year": 1991}, {"title": "Probabilistic part-of-speech tagging using decision trees", "author": ["H. Schmid"], "venue": "In Intl Conf. on New Methods in Language Proc.,", "citeRegEx": "37", "shortCiteRegEx": "37", "year": 1994}, {"title": "Sentiment strength detection in short informal text", "author": ["M. Thelwall", "K. Buckley", "G. Paltoglou", "D. Cai"], "venue": "Jour. Ameri. Soci. for Info. Sci. & Tech.,", "citeRegEx": "38", "shortCiteRegEx": "38", "year": 2010}, {"title": "Feature-rich part-of-speech tagging with a cyclic dependency network", "author": ["K. Toutanova", "D. Klein", "C.D. Manning", "Y. Singer"], "venue": "In NAACL,", "citeRegEx": "39", "shortCiteRegEx": "39", "year": 2003}, {"title": "Visualizing high-dimensional data using t-SNE", "author": ["L. van der Maaten", "G.E. Hinton"], "venue": null, "citeRegEx": "40", "shortCiteRegEx": "40", "year": 2008}, {"title": "Personalized visual aesthetics", "author": ["E.A. Vessel", "J. Stahl", "N. Maurer", "A. Denker", "G.G. Starr"], "venue": "In SPIE-IS&T Electronic Imaging,", "citeRegEx": "41", "shortCiteRegEx": "41", "year": 2014}, {"title": "Emotional valence categorization using holistic image features", "author": ["V. Yanulevskaya", "J. van Gemert", "K. Roth", "A. Herbold", "N. Sebe", "J.M. Geusebroek"], "venue": "In ICIP,", "citeRegEx": "42", "shortCiteRegEx": "42", "year": 2008}, {"title": "Robust image sentiment analysis using progressively trained and domain transferred deep networks", "author": ["Q. You", "J. Luo", "H. Jin", "J. Yang"], "venue": "In AAAI,", "citeRegEx": "43", "shortCiteRegEx": "43", "year": 2014}, {"title": "Feeling and thinking: Preferences need no inferences", "author": ["R.B. Zajonc"], "venue": "American Psychologist,", "citeRegEx": "44", "shortCiteRegEx": "44", "year": 1980}], "referenceMentions": [{"referenceID": 4, "context": "[5], but in a multilingual context.", "startOffset": 0, "endOffset": 3}, {"referenceID": 4, "context": "Unlike the flat structure in [5], our unified ontology is organized hierarchically by multilingual clusters of visually detectable nouns and subclusters of emotionally biased versions of these nouns.", "startOffset": 29, "endOffset": 32}, {"referenceID": 4, "context": ", t5) are labeled with part-of-speech tags, and adjectives and nouns are used to form candidate adjective-noun pair (ANP) combinations [5], while others are ignored (in red).", "startOffset": 135, "endOffset": 138}, {"referenceID": 27, "context": "Some believe emotion to be culture-specific [29], that is, emotion is dependent on one\u2019s cultural context, while others believe emotion to be universal [16], that is, emotion and culture are independent mechanisms.", "startOffset": 44, "endOffset": 48}, {"referenceID": 14, "context": "Some believe emotion to be culture-specific [29], that is, emotion is dependent on one\u2019s cultural context, while others believe emotion to be universal [16], that is, emotion and culture are independent mechanisms.", "startOffset": 152, "endOffset": 156}, {"referenceID": 31, "context": "Do Englishspeakers not feel those same emotions or do they simply refer to them in a different way? Or even if the reference is the same, perhaps the underlying emotion is different? In Affective Computing [33] and Multimedia, we often refer to the affective gap as the conceptual divide between the low-level visual stimuli, like images and features, and the high-level, abstracted semantics of human affect, e.", "startOffset": 206, "endOffset": 210}, {"referenceID": 4, "context": "[5] developed a visual sentiment ontology (VSO), a set of 1,200 mid-level concepts using structured semantics called adjective-noun pairs (ANPs).", "startOffset": 0, "endOffset": 3}, {"referenceID": 20, "context": "Our work is closely related to Multimedia and Vision research that focus on visual aesthetics [22], interestingness [14], popularity [23], and creativity [35].", "startOffset": 94, "endOffset": 98}, {"referenceID": 12, "context": "Our work is closely related to Multimedia and Vision research that focus on visual aesthetics [22], interestingness [14], popularity [23], and creativity [35].", "startOffset": 116, "endOffset": 120}, {"referenceID": 21, "context": "Our work is closely related to Multimedia and Vision research that focus on visual aesthetics [22], interestingness [14], popularity [23], and creativity [35].", "startOffset": 133, "endOffset": 137}, {"referenceID": 33, "context": "Our work is closely related to Multimedia and Vision research that focus on visual aesthetics [22], interestingness [14], popularity [23], and creativity [35].", "startOffset": 154, "endOffset": 158}, {"referenceID": 9, "context": "Our work also relates to research in Cognitive and Social Psychology, especially emotion and culture research [11, 30, 34], but also neuroaesthetics [41], visual preference [41, 44], and social interaction [28].", "startOffset": 110, "endOffset": 122}, {"referenceID": 28, "context": "Our work also relates to research in Cognitive and Social Psychology, especially emotion and culture research [11, 30, 34], but also neuroaesthetics [41], visual preference [41, 44], and social interaction [28].", "startOffset": 110, "endOffset": 122}, {"referenceID": 32, "context": "Our work also relates to research in Cognitive and Social Psychology, especially emotion and culture research [11, 30, 34], but also neuroaesthetics [41], visual preference [41, 44], and social interaction [28].", "startOffset": 110, "endOffset": 122}, {"referenceID": 39, "context": "Our work also relates to research in Cognitive and Social Psychology, especially emotion and culture research [11, 30, 34], but also neuroaesthetics [41], visual preference [41, 44], and social interaction [28].", "startOffset": 149, "endOffset": 153}, {"referenceID": 39, "context": "Our work also relates to research in Cognitive and Social Psychology, especially emotion and culture research [11, 30, 34], but also neuroaesthetics [41], visual preference [41, 44], and social interaction [28].", "startOffset": 173, "endOffset": 181}, {"referenceID": 42, "context": "Our work also relates to research in Cognitive and Social Psychology, especially emotion and culture research [11, 30, 34], but also neuroaesthetics [41], visual preference [41, 44], and social interaction [28].", "startOffset": 173, "endOffset": 181}, {"referenceID": 26, "context": "Our work also relates to research in Cognitive and Social Psychology, especially emotion and culture research [11, 30, 34], but also neuroaesthetics [41], visual preference [41, 44], and social interaction [28].", "startOffset": 206, "endOffset": 210}, {"referenceID": 40, "context": "Progressive research in\u201cvisual affect\u201drecognition was done in [42] and [27] where image features were designed based on art and psychology principles for emotion prediction.", "startOffset": 62, "endOffset": 66}, {"referenceID": 25, "context": "Progressive research in\u201cvisual affect\u201drecognition was done in [42] and [27] where image features were designed based on art and psychology principles for emotion prediction.", "startOffset": 71, "endOffset": 75}, {"referenceID": 16, "context": "And such works were later improved in [18] by adding social media data in semi-supervised frameworks.", "startOffset": 38, "endOffset": 42}, {"referenceID": 23, "context": "The International Affective Picture System (IAPS) dataset [25] is a seminal dataset of \u223c1,000 images, focused on induced emotions in humans for biometric measurement.", "startOffset": 58, "endOffset": 62}, {"referenceID": 7, "context": "The Geneva Affective PicturE Database (GAPED) [8] consists of 730 pictures meant to supplement IAPS and tries to narrow the themes across images.", "startOffset": 46, "endOffset": 49}, {"referenceID": 4, "context": "And recently, in [5], a visual sentiment ontology (VSO) and dataset was created from Flickr image data, resulting in a collection of adjective-noun pairs along with corresponding images, tags and sentiment.", "startOffset": 17, "endOffset": 20}, {"referenceID": 27, "context": "A main contention in the area concerns whether emotions are culturespecific [29], i.", "startOffset": 76, "endOffset": 80}, {"referenceID": 14, "context": "their perception and elicitation varies with the context, or universal [16].", "startOffset": 71, "endOffset": 75}, {"referenceID": 34, "context": "In [36], a survey of crosscultural work on semantics surrounding emotion elicitation and perception is given, showing that there are still competing views as to whether emotion is pan-cultural, culturespecific, or some hybrid of both.", "startOffset": 3, "endOffset": 7}, {"referenceID": 2, "context": "In [3] and [31], they developed multilingual methods for international text sentiment analysis in online blogs and news articles, respectively.", "startOffset": 3, "endOffset": 6}, {"referenceID": 29, "context": "In [3] and [31], they developed multilingual methods for international text sentiment analysis in online blogs and news articles, respectively.", "startOffset": 11, "endOffset": 15}, {"referenceID": 24, "context": "In [26] and [17], they presented approaches to indexing digital music libraries with music from multiple languages.", "startOffset": 3, "endOffset": 7}, {"referenceID": 15, "context": "In [26] and [17], they presented approaches to indexing digital music libraries with music from multiple languages.", "startOffset": 12, "endOffset": 16}, {"referenceID": 15, "context": "Specific to emotion, [17] tried to highlight differences between languages by building models for predicting the musical mood and then cross-predicting in other languages.", "startOffset": 21, "endOffset": 25}, {"referenceID": 4, "context": "on VSO [5] and its associated detector bank, SentiBank [4].", "startOffset": 7, "endOffset": 10}, {"referenceID": 3, "context": "on VSO [5] and its associated detector bank, SentiBank [4].", "startOffset": 55, "endOffset": 58}, {"referenceID": 3, "context": "Their mid-level representation approach has recently proven effective in a wide range of applications in emotion prediction [4, 21], social media commenting [7], etc.", "startOffset": 124, "endOffset": 131}, {"referenceID": 19, "context": "Their mid-level representation approach has recently proven effective in a wide range of applications in emotion prediction [4, 21], social media commenting [7], etc.", "startOffset": 124, "endOffset": 131}, {"referenceID": 6, "context": "Their mid-level representation approach has recently proven effective in a wide range of applications in emotion prediction [4, 21], social media commenting [7], etc.", "startOffset": 157, "endOffset": 160}, {"referenceID": 3, "context": "However, in addition to lack of multilingual support, there are several technical challenges with VSO [4, 5] that we seek to improve on via (1) detection of adjectives and nouns with language-specific part-of-speech taggers, as opposed to a fixed list of adjectives and nouns, (2) automatic discovery of adjective-noun pairs correlated with emotions, as opposed to \u201cconstructed\u201d pairs from top frequent adjectives and nouns, and (3) stronger selection criterion based on image tag frequency, linguistic and semantic filters and crowdsource validation.", "startOffset": 102, "endOffset": 108}, {"referenceID": 4, "context": "However, in addition to lack of multilingual support, there are several technical challenges with VSO [4, 5] that we seek to improve on via (1) detection of adjectives and nouns with language-specific part-of-speech taggers, as opposed to a fixed list of adjectives and nouns, (2) automatic discovery of adjective-noun pairs correlated with emotions, as opposed to \u201cconstructed\u201d pairs from top frequent adjectives and nouns, and (3) stronger selection criterion based on image tag frequency, linguistic and semantic filters and crowdsource validation.", "startOffset": 102, "endOffset": 108}, {"referenceID": 32, "context": "In the first stage, we obtain a set of images and their tags using seed emotion keyword queries, selected according to emotion ontologies from psychology such as [34] or [11].", "startOffset": 162, "endOffset": 166}, {"referenceID": 9, "context": "In the first stage, we obtain a set of images and their tags using seed emotion keyword queries, selected according to emotion ontologies from psychology such as [34] or [11].", "startOffset": 170, "endOffset": 174}, {"referenceID": 18, "context": "We selected Flickr because there is an existing body of multimedia research using it in the past, and in particular, [20] describes how Flickr satisfies two conditions for making use of the \u201cwisdom of the social multimedia\u201d: popularity and availability.", "startOffset": 117, "endOffset": 121}, {"referenceID": 18, "context": "We do not repeat the argument in [20], but note that in addition to those benefits, Flickr has multilingual support and the use of Flickr facilitates a natural comparison to the seminal VSO [5] work.", "startOffset": 33, "endOffset": 37}, {"referenceID": 4, "context": "We do not repeat the argument in [20], but note that in addition to those benefits, Flickr has multilingual support and the use of Flickr facilitates a natural comparison to the seminal VSO [5] work.", "startOffset": 190, "endOffset": 193}, {"referenceID": 32, "context": "As our seed emotion ontology, we selected the Plutchik\u2019s Wheel of Emotions [34].", "startOffset": 75, "endOffset": 79}, {"referenceID": 9, "context": "This psychology ontology was selected because it consists of graded intensities for multiple basic emotions providing a richer set of emotional valences compared to alternatives like [11]; it has also been shown", "startOffset": 183, "endOffset": 187}, {"referenceID": 4, "context": "to be useful for VSO [5].", "startOffset": 21, "endOffset": 24}, {"referenceID": 4, "context": "Like [5], for each emotion, we chose to sample only the top 50K images ranked by Flickr relevance to simply limit the size of our results, but if an emotion had less than 50K images, we extended the search to additional metadata, i.", "startOffset": 5, "endOffset": 8}, {"referenceID": 35, "context": "Part-of-speech Labeling: To identify the type of each word in a Flickr tag, we performed automatic part-of-speech labeling using pre-trained language-specific taggers which achieve high accuracy (>95% for most languages), namely TreeTagger [37], Stanford tagger [39], HunPos tagger [15] and a morphological analyzer for Turkish [13].", "startOffset": 240, "endOffset": 244}, {"referenceID": 37, "context": "Part-of-speech Labeling: To identify the type of each word in a Flickr tag, we performed automatic part-of-speech labeling using pre-trained language-specific taggers which achieve high accuracy (>95% for most languages), namely TreeTagger [37], Stanford tagger [39], HunPos tagger [15] and a morphological analyzer for Turkish [13].", "startOffset": 262, "endOffset": 266}, {"referenceID": 13, "context": "Part-of-speech Labeling: To identify the type of each word in a Flickr tag, we performed automatic part-of-speech labeling using pre-trained language-specific taggers which achieve high accuracy (>95% for most languages), namely TreeTagger [37], Stanford tagger [39], HunPos tagger [15] and a morphological analyzer for Turkish [13].", "startOffset": 282, "endOffset": 286}, {"referenceID": 11, "context": "Part-of-speech Labeling: To identify the type of each word in a Flickr tag, we performed automatic part-of-speech labeling using pre-trained language-specific taggers which achieve high accuracy (>95% for most languages), namely TreeTagger [37], Stanford tagger [39], HunPos tagger [15] and a morphological analyzer for Turkish [13].", "startOffset": 328, "endOffset": 332}, {"referenceID": 4, "context": "To validate the completeness of our strategy we compared with VSO and found that \u223c86% of ANPs discovered by VSO [5] overlap with the English ANPs discovered by our method.", "startOffset": 112, "endOffset": 115}, {"referenceID": 36, "context": "Non-neutral Sentiment: To filter out neutral candidate adjective-noun pairs, each ANP was scored in sentiment using two publicly available sentiment ontologies: SentiStrength [38] and SentiWordnet [12].", "startOffset": 175, "endOffset": 179}, {"referenceID": 10, "context": "Non-neutral Sentiment: To filter out neutral candidate adjective-noun pairs, each ANP was scored in sentiment using two publicly available sentiment ontologies: SentiStrength [38] and SentiWordnet [12].", "startOffset": 197, "endOffset": 201}, {"referenceID": 0, "context": "SentiStrength ontology supported all the languages we considered, but since SentiWordnet could only be used directly for English, we passed in automatic translations in English from all other languages to it, following previous research on multilingual sentiment analysis in machine translation [1, 2].", "startOffset": 295, "endOffset": 301}, {"referenceID": 1, "context": "SentiStrength ontology supported all the languages we considered, but since SentiWordnet could only be used directly for English, we passed in automatic translations in English from all other languages to it, following previous research on multilingual sentiment analysis in machine translation [1, 2].", "startOffset": 295, "endOffset": 301}, {"referenceID": 4, "context": "1 Comparison with VSO [5]", "startOffset": 22, "endOffset": 25}, {"referenceID": 4, "context": "To verify and test the efficacy of our MVSO, we provide a comparison of our extracted English visual sentiment ontology with that of VSO [5] along dimensions of size (numFigure 3: Comparison of our English MVSO and VSO [5] in Figures (a), (b) and (c), in terms of ANP overlap, no.", "startOffset": 137, "endOffset": 140}, {"referenceID": 4, "context": "To verify and test the efficacy of our MVSO, we provide a comparison of our extracted English visual sentiment ontology with that of VSO [5] along dimensions of size (numFigure 3: Comparison of our English MVSO and VSO [5] in Figures (a), (b) and (c), in terms of ANP overlap, no.", "startOffset": 219, "endOffset": 222}, {"referenceID": 8, "context": "This surprising observation is in fact compatible with previous research showing that there is a universal positivity bias over languages with Spanish being the most relatively positive language [9].", "startOffset": 195, "endOffset": 198}, {"referenceID": 0, "context": "i=1 1 ni \u2211ni j=1 c (x) ij \u2208 [0, 1] (2)", "startOffset": 28, "endOffset": 34}, {"referenceID": 0, "context": "i=1 \u2211|X(l)| x=1 emo i(x) \u00b7 count(x) \u2208 [0, 1] (3)", "startOffset": 38, "endOffset": 44}, {"referenceID": 32, "context": "Figure 5 shows these scores per language and Plutchik emotion [34] on a heatmap diagram.", "startOffset": 62, "endOffset": 66}, {"referenceID": 35, "context": "First, we extracted nouns using TreeTagger [37] from the list of translated phrases and discovered 3,099 total nouns.", "startOffset": 43, "endOffset": 47}, {"referenceID": 30, "context": "We then extracted word2vec [32] features, a word representation trained on a Google News corpus, for these translated nouns (188 nouns were out-of-vocabulary), and performed kmeans clustering (k=200) to get groups of nouns with similar meaning.", "startOffset": 27, "endOffset": 31}, {"referenceID": 30, "context": "We extracted word2vec [32] features from the full translated phrase in each cluster and ran another round of k-means clustering (adjusting k based on the number of phrases in each cluster, where phrases in each noun-cluster ranged from 3 to 253).", "startOffset": 22, "endOffset": 26}, {"referenceID": 4, "context": "This two-stage clustering enables us to create a hierarchical organization of our ANPs across languages and form a multilingual ontology over visual sentiment concepts (MVSO), unlike the flat structure in VSO [5].", "startOffset": 209, "endOffset": 212}, {"referenceID": 30, "context": "For visualization, word2vec [32] vectors were projected to R using t-SNE [40].", "startOffset": 28, "endOffset": 32}, {"referenceID": 38, "context": "For visualization, word2vec [32] vectors were projected to R using t-SNE [40].", "startOffset": 73, "endOffset": 77}, {"referenceID": 38, "context": "For Figure 7, we projected data to R using t-SNE dimensionality reduction [40].", "startOffset": 74, "endOffset": 78}, {"referenceID": 15, "context": "Inspired by work in [17], we studied the extent to which the visual sentiments of a given language can be predicted by sentiment models of other languages.", "startOffset": 20, "endOffset": 24}, {"referenceID": 4, "context": "[5, 42, 43]) for its simplicity, compared to emotion prediction.", "startOffset": 0, "endOffset": 11}, {"referenceID": 40, "context": "[5, 42, 43]) for its simplicity, compared to emotion prediction.", "startOffset": 0, "endOffset": 11}, {"referenceID": 41, "context": "[5, 42, 43]) for its simplicity, compared to emotion prediction.", "startOffset": 0, "endOffset": 11}, {"referenceID": 3, "context": "We first constructed a bank of visual concept detectors like in [4] for our final MVSO adjective-noun pairs.", "startOffset": 64, "endOffset": 67}, {"referenceID": 22, "context": "To construct our bank of visual concept detectors of ANPs, we used convolutional neural networks (CNNs), in particular, adopting an AlexNet-styled architecture [24] for its good performance on large-scale vision recognition and detection tasks.", "startOffset": 160, "endOffset": 164}, {"referenceID": 5, "context": "ized with DeepSentiBank [6], an AlexNet model trained on the VSO [5] dataset.", "startOffset": 24, "endOffset": 27}, {"referenceID": 4, "context": "ized with DeepSentiBank [6], an AlexNet model trained on the VSO [5] dataset.", "startOffset": 65, "endOffset": 68}, {"referenceID": 17, "context": "For a single language, finetuning took between 12 and 40 hours for convergence on a single NVIDIA GTX 980 GPU implemented with Caffe [19].", "startOffset": 133, "endOffset": 137}, {"referenceID": 5, "context": "From Table 4, as expected we achieve higher top-1 and top-5 accuracies than DeepSentiBank [6], even when the numbers of output neurons in English and Spanish are higher than those in [6].", "startOffset": 90, "endOffset": 93}, {"referenceID": 5, "context": "From Table 4, as expected we achieve higher top-1 and top-5 accuracies than DeepSentiBank [6], even when the numbers of output neurons in English and Spanish are higher than those in [6].", "startOffset": 183, "endOffset": 186}, {"referenceID": 5, "context": "0% DSB [6] 2,089 826,806 41,113 - - 8.", "startOffset": 7, "endOffset": 10}, {"referenceID": 5, "context": "Table 4: Adjective-noun pair (ANP) classification performance on Flickr images for six major languages in MVSO and compared to DeepSentiBank (DSB) [6].", "startOffset": 147, "endOffset": 150}], "year": 2017, "abstractText": null, "creator": "LaTeX with hyperref package"}}}