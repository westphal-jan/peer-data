{"id": "1508.04028", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "17-Aug-2015", "title": "Owl and Lizard: Patterns of Head Pose and Eye Pose in Driver Gaze Classification", "abstract": "Accurate, robust, inexpensive gaze tracking in the car can help keep a driver safe by facilitating the more effective study of how to improve (1) vehicle interfaces and (2) the design of future Advanced Driver Assistance Systems. In this paper, we estimate head pose and eye pose from monocular video using methods developed extensively in prior work and ask two new interesting questions. First, how much better can we classify driver gaze using head and eye pose versus just using head pose? Second, are there individual-specific gaze strategies that strongly correlate with how much gaze classification improves with the addition of eye pose information? We answer these questions by evaluating data drawn from an on-road study of 40 drivers. The main insight of the paper is conveyed through the analogy of an \"owl\" and \"lizard\" which describes the degree to which the eyes and the head move when shifting gaze. When the head moves a lot (\"owl\"), not much classification improvement is attained by estimating eye pose on top of head pose. On the other hand, when the head stays still and only the eyes move (\"lizard\"), classification accuracy increases significantly from adding in eye pose. We characterize how that accuracy varies between people, gaze strategies, and gaze regions.", "histories": [["v1", "Mon, 17 Aug 2015 13:54:05 GMT  (642kb,D)", "https://arxiv.org/abs/1508.04028v1", "Under review: IET Computer Vision. arXiv admin note: text overlap witharXiv:1507.04760"], ["v2", "Sat, 13 Feb 2016 18:10:49 GMT  (641kb,D)", "http://arxiv.org/abs/1508.04028v2", "Accepted for Publication in IET Computer Vision. arXiv admin note: text overlap witharXiv:1507.04760"], ["v3", "Sun, 20 Nov 2016 04:46:26 GMT  (643kb,D)", "http://arxiv.org/abs/1508.04028v3", "Accepted for Publication in IET Computer Vision. arXiv admin note: text overlap witharXiv:1507.04760"]], "COMMENTS": "Under review: IET Computer Vision. arXiv admin note: text overlap witharXiv:1507.04760", "reviews": [], "SUBJECTS": "cs.CV cs.HC cs.LG", "authors": ["lex fridman", "joonbum lee", "bryan reimer", "trent victor"], "accepted": false, "id": "1508.04028"}, "pdf": {"name": "1508.04028.pdf", "metadata": {"source": "CRF", "title": "Owl and Lizard: Patterns of Head Pose and Eye Pose in Driver Gaze Classification", "authors": ["Lex Fridman", "Joonbum Lee", "Bryan Reimer", "Trent Victor"], "emails": [], "sections": [{"heading": null, "text": "In fact, it is not that it is a real case in which people are able to help themselves by putting themselves in a position to improve the situation. (...) In fact, it is that people are able to survive themselves. (...) It is not that they are able to survive themselves. (...) It is not that they are able to survive themselves. (...) It is not that they are able to survive themselves. \"(...) It is as if they are able to survive themselves.\" (...) It is as if they are able to survive themselves. (...) It is as if they are able to survive themselves. \"(...) It is as if they are able to survive themselves. (...) It is as if they are able to survive themselves.\" (...) It is as if they are able to survive themselves."}, {"heading": "II. RELATED WORK", "text": "The problem of visual tracking of monocular videos has been extensively studied in many areas [10], [11] and we are building on this work to characterize the individual contribution of head movements and eye movements to the classification of glances.The building blocks of our image processing pipeline are: facial alignment, head posture and pupil recognition. We apply topographic algorithms from these areas to answer two questions posed by our work (see \u00a7 I).The algorithm in [12] uses an ensemble of regression steps for super-real facial alignment. Our facial feature is the extraction of algorithms designed for a decade of progress on the face."}, {"heading": "III. DATASET", "text": "In fact, most of them will be able to feel as if they are able to move, as if they are able to move."}, {"heading": "IV. GAZE CLASSIFICATION PIPELINE", "text": "The steps in the pipeline for classifying the viewing region are: (1) Face recognition, (2) Face alignment, (3) Pupil recognition, (4) Extraction and normalization, (5) Classification, (6) Decision circumcision. When the system passes the first three steps, it results in a decision to classify the viewing region for each image fed into the pipeline. In Step 6, this decision may be dropped if it falls below a confidence threshold (see \u00a7 IV-E. The three facial images in Fig. 2 are examples of the result achieved in the first four steps of the pipeline: from raw video frame to extracted facial features and pupil position. As mentioned in Section I, the relative orientation of the facial features serves as a proxy for \"head posture\" and the relative orientation of the pupil position as a proxy for \"eye posture.\" We discuss each of the six steps in the pipeline in the following sections."}, {"heading": "A. Face Detection", "text": "The environment in the car is relatively controlled by the camera position being fixed and the driver's trunk moving in a relatively confined space. Thus, a camera can be positioned so that the driver's face is always fully or almost completely in the field of vision. However, the lighting conditions are sometimes drastically variable (e.g. rapid crossing of a bridge, reflection of the sun on the camera lens, etc.) and therefore there are often cases where the intensity distribution of the image does not allow successful recognition of the face (i.e. false ejection). Each detection step in the pipeline is set to a low False Accept Rate (FAR). A false acceptance error at the beginning of the pipeline spreads and can lead to drastically incorrect head posture and eye evaluation. In the context of the driver's video-based vision classification, a high False Reject Rate (FRR) is more acceptable than a high FAR detector. The face detector in our pipeline uses a pyramid-oriented histogram of the SVM color scheme (SVR)."}, {"heading": "B. Face Alignment and Head Pose", "text": "Both facial alignment and head position estimation are extremely well-researched problems in computer vision [13], [20]. We examined several ground-breaking methods from each area and selected those that worked best for monocular video with vastly different light conditions. Facial alignment in our pipeline is performed using a 68-point marking method in the iBUG 300 W dataset. These marking methods include parts of the nose, the upper edge of the eyebrows, outer and inner lips, jaw and parts in and around the eye. Selected marking methods are presented as black dots in Fig. 2. The algorithm for aligning the 68-point shape to the image data uses a cascade of regressors as described in [12] and implemented in [18]. The two characteristics of this algorithm are most important for localizing the driver's gaze. (1) It is robust, partial upsion is faster than self-assessment (2)."}, {"heading": "C. Pupil Detection", "text": "As described in \u00a7 I, the problem of accurate pupil detection is more difficult than the problem of accurate facial alignment, but both are not always robust to poor light conditions. Therefore, the secondary task of pupil detection is to highlight errors in the previous step of facial alignment. As shown in Table I, the face is detected in 79.4% video frames, but only 61.6% of the original frames pass through the pupil detection step. We use a CDF-based method [15] to extract the pupil from the image of the right eye, and adjust the extracted pupil blob to erosion and dilation using morphological operations. The six steps in this process are as follows: Extract the right eye from the face image, which is compressed as part of the facial alignment."}, {"heading": "D. Feature Extraction and Normalization", "text": "The driver spends more than 90% of his time looking forward to the road, and this fact was used in [7] to normalize the position of the facial features in relation to the average boundary of the visual field associated with the visual field \"street,\" which required an initial 120-second phase of automatic calibration. In this essay, we remove the need for calibration and instead normalize the facial features based on the boundary of the eyes and nose only for the current frame. Fig. 1 shows the deviation of the head and eyes from their \"reference positions.\" The normalization step transforms the facial features linearly so that the boundaries of the eyes and nose fit into a unit square. After this transformation, the relative orientation of the facial features becomes the characteristic vector for the classification step. Experimentally, it was determined that the boundary field of the eyes and nose is the most robust normalization region of the eye and nose."}, {"heading": "E. Classification and Decision Pruning", "text": "The class probability of a single tree is a fraction of the samples of the same class in a leaf. A random forest classifier of depth 25 with a total ensemble of 2,000 trees is used for all experiments in \u00a7 V. The class with the highest probability is the one that the system assigns to the image as a \"decision.\" The ratio of the highest probability to the second highest probability is called the \"confidence\" of the decision. A certainty of 1 is the minimum. There is no maximum. The effect of this threshold is investigated in [7]. For experiments in \u00a7 V a confidence threshold of 10 is used, which means that decisions with a reliability of more than 10 are accepted and the others are ignored. A random forest classifiver was used because it achieved a much higher accuracy than the closest (NKN class), slightly higher costs than the SVRM class and a higher accuracy than fifty times."}, {"heading": "V. RESULTS", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "A. Gaze Region Classification", "text": "In all the experiments and discussions that follow, the key comparison is between the classification with the head alone and the classification that is done with the head and the classification that is done with the eye. (5) The pipeline begins with an annotated frame from the raw video. As previously described, each questionnaire is doubly annotated and meditated to ensure that the lightning region can reliably serve as a cross-validation training. (6) The pipeline begins with a raw frame from the raw video."}, {"heading": "B. User-Specific Classification and Gaze Strategies", "text": "As shown in \u00a7 V-A, adding eye positions to head positions increases the accuracy of eye classification by 5.4%. But that does not tell the whole story, as some user-trained classifiers benefit more from eye postures than others. Fig. 5 shows the differences in user accuracy before and after adding eye positions to the classification characteristics. For many users, an accuracy of 100% is achieved, while for many others, accuracy falls below 80% and even as low as 40%. Using the Pearson correlation coefficient as a guide, we have programmed over one million pairs of variables in search of an answer to the question of what explains this difference in classification performance between users. Some variables correlate with accuracy per region, but not overall. For example, the average size of deviating head movements and pupil movement are good predictors of the classification accuracy of the \"right\" region with head positions alone and together with the eye."}, {"heading": "VI. CONCLUSION", "text": "This paper examines the contribution of head posture and eye posture to the accuracy of vision classification for different viewing strategies. We answer two questions: (1) How much does eye posture contribute and (2) how can the variation in accuracy between users be explained? For the former, we show that eye posture implies a 5.4% increase in average accuracy (from 89.2% to 94.6%) with an effective average rate of 1.3 decisions per second. For the latter, we propose an \"owl number\" measurement that splits the gaze into head movement and eye movement and calculates the relative magnitude of both. This measurement is used to explain the variation in the influence of eye posture between individuals on the accuracy of vision classification."}, {"heading": "ACKNOWLEDGMENT", "text": "This work was supported by the Santos Family Foundation, the New England University Transportation Center, and the Toyota Class Action Settlement Safety Research and Education Program. The views and conclusions expressed are those of the authors and have not been supported or supported by Toyota or plaintiffs. Data are from studies supported by the Insurance Institute for Highway Safety (IIHS)."}], "references": [{"title": "The impact of driver inattention on near-crash/crash risk: An analysis using the 100-car naturalistic driving study data", "author": ["S.G. Klauer", "T.A. Dingus", "V.L. Neale", "J.D. Sudweeks", "D.J. Ramsey"], "venue": "National Highway Traffic Safety Administration, Tech. Rep., 2006.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2006}, {"title": "How dangerous is looking away from the road? algorithms predict crash risk from glance patterns in naturalistic driving", "author": ["Y. Liang", "J.D. Lee", "L. Yekhshatyan"], "venue": "Human Factors: The Journal of the Human Factors and Ergonomics Society, vol. 54, no. 6, pp. 1104\u20131116, 2012.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2012}, {"title": "The attentional demand of automobile driving", "author": ["J.W. Senders", "A. Kristofferson", "W. Levison", "C. Dietrich", "J. Ward"], "venue": "Highway research record, no. 195, 1967.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 1967}, {"title": "Visual-manual nhtsa driver distraction guidelines for in-vehicle electronic devices", "author": ["N.H.T.S. Administration"], "venue": "Washington, DC: National Highway Traffic Safety Administration (NHTSA), Department of Transportation (DOT), 2012.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2012}, {"title": "Statement of principles, criteria and verification procedures on driver interactions with advanced in-vehicle information and communication systems", "author": ["D.F.-T.W. Group"], "venue": "Alliance of Automotive Manufacturers, 2006.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2006}, {"title": "Monitoring, managing, and motivating driver safety and well-being.", "author": ["J.F. Coughlin", "B. Reimer", "B. Mehler"], "venue": "IEEE Pervasive Computing,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2011}, {"title": "Driver gaze estimation without using eye movement", "author": ["L. Fridman", "P. Langhans", "J. Lee", "B. Reimer"], "venue": "IEEE Intelligent Systems, 2016.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2016}, {"title": "Analysis of Naturalistic Driving Study Data: Safer Glances", "author": ["T. Victor", "M. Dozza", "J. B\u00e4rgman", "C.-N. Boda", "J. Engstr\u00f6m", "G. Markkula"], "venue": "Driver Inattention, and Crash Risk,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2014}, {"title": "Investigating drivers\u2019 head and glance correspondence", "author": ["J. Lee", "M. Mu\u00f1oz", "L. Fridman", "T. Victor", "B. Reimer", "B. Mehler"], "venue": "Submitted. 2016.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2016}, {"title": "A survey on methods and models of eye tracking, head pose and gaze estimation", "author": ["R.P. Gaur", "K.N. Jariwala"], "venue": "Journal of Emerging Technologies and Innovative Research, vol. 1, no. 5 (October-2014). JETIR, 2014.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2014}, {"title": "A survey on gaze estimation techniques", "author": ["M. Sireesha", "P. Vijaya", "K. Chellamma"], "venue": "Proceedings of International Conference on VLSI, Communication, Advanced Devices, Signals & Systems and Networking (VCASAN-2013). Springer, 2013, pp. 353\u2013361.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2013}, {"title": "One millisecond face alignment with an ensemble of regression trees", "author": ["V. Kazemi", "J. Sullivan"], "venue": "Computer Vision and Pattern Recognition (CVPR), 2014 IEEE Conference on. IEEE, 2014, pp. 1867\u2013 1874.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2014}, {"title": "Head pose estimation in computer vision: A survey", "author": ["E. Murphy-Chutorian", "M.M. Trivedi"], "venue": "Pattern Analysis and Machine Intelligence, IEEE Transactions on, vol. 31, no. 4, pp. 607\u2013626, 2009.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2009}, {"title": "Eye tracking and head movement detection: A state-of-art survey", "author": ["A. Al-Rahayfeh", "M. Faezipour"], "venue": "Translational Engineering in Health and Medicine, IEEE Journal of, vol. 1, pp. 2 100 212\u20132 100 212, 2013.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2013}, {"title": "Automatic adaptive center of pupil detection using face detection and cdf analysis", "author": ["M. Asadifard", "J. Shanbezadeh"], "venue": "Proceedings of the International MultiConference of Engineers and Computer Scientists, vol. 1, 2010, p. 3.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2010}, {"title": "Multi-modal assessment of on-road demand of voice and manual phone calling and voice navigation entry across two embedded vehicle systems", "author": ["B. Mehler", "D. Kidd", "B. Reimer", "I. Reagan", "J. Dobres", "A. McCartt"], "venue": "Ergonomics, 2015.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2015}, {"title": "Methodology for capturing driver eye glance behavior during in-vehicle secondary tasks", "author": ["D. Smith", "J. Chang", "R. Glassco", "J. Foley", "D. Cohen"], "venue": "Transportation Research Record: Journal of the Transportation Research Board, no. 1937, pp. 61\u201365, 2005.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 1937}, {"title": "Dlib-ml: A machine learning toolkit", "author": ["D.E. King"], "venue": "Journal of Machine Learning Research, vol. 10, pp. 1755\u20131758, 2009.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2009}, {"title": "An extended set of haar-like features for rapid object detection", "author": ["R. Lienhart", "J. Maydt"], "venue": "Image Processing. 2002. Proceedings. 2002 International Conference on, vol. 1. IEEE, 2002, pp. I\u2013900.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2002}, {"title": "Toward a practical face recognition system: Robust alignment and illumination by sparse representation", "author": ["A. Wagner", "J. Wright", "A. Ganesh", "Z. Zhou", "H. Mobahi", "Y. Ma"], "venue": "Pattern Analysis and Machine Intelligence, IEEE Transactions on, vol. 34, no. 2, pp. 372\u2013386, 2012.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2012}, {"title": "300 faces in-the-wild challenge: The first facial landmark localization challenge", "author": ["C. Sagonas", "G. Tzimiropoulos", "S. Zafeiriou", "M. Pantic"], "venue": "Computer Vision Workshops (ICCVW), 2013 IEEE International Conference on. IEEE, 2013, pp. 397\u2013403.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2013}, {"title": "Globally optimal o (n) solution to the pnp problem for general camera models.", "author": ["G. Schweighofer", "A. Pinz"], "venue": "in BMVC,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2008}, {"title": "Learning OpenCV: Computer vision with the OpenCV library", "author": ["G. Bradski", "A. Kaehler"], "venue": null, "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2008}, {"title": "Scikit-learn: Machine learning in Python", "author": ["F. Pedregosa", "G. Varoquaux", "A. Gramfort", "V. Michel", "B. Thirion", "O. Grisel", "M. Blondel", "P. Prettenhofer", "R. Weiss", "V. Dubourg", "J. Vanderplas", "A. Passos", "D. Cournapeau", "M. Brucher", "M. Perrot", "E. Duchesnay"], "venue": "Journal of Machine Learning Research, vol. 12, pp. 2825\u20132830, 2011.", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2011}, {"title": "A study of the behavior of several methods for balancing machine learning training data", "author": ["G.E. Batista", "R.C. Prati", "M.C. Monard"], "venue": "ACM Sigkdd Explorations Newsletter, vol. 6, no. 1, pp. 20\u201329, 2004.", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2004}], "referenceMentions": [{"referenceID": 0, "context": "The allocation of visual attention away from the road has been linked to accident risk [1], [2] and a drop in situational awareness as uncertainty in the environment increases [3].", "startOffset": 87, "endOffset": 90}, {"referenceID": 1, "context": "The allocation of visual attention away from the road has been linked to accident risk [1], [2] and a drop in situational awareness as uncertainty in the environment increases [3].", "startOffset": 92, "endOffset": 95}, {"referenceID": 2, "context": "The allocation of visual attention away from the road has been linked to accident risk [1], [2] and a drop in situational awareness as uncertainty in the environment increases [3].", "startOffset": 176, "endOffset": 179}, {"referenceID": 3, "context": "Driver distraction is often construed as a key source of attention divergence from the roadway and the topic of numerous scientific studies and design guidelines [4], [5].", "startOffset": 162, "endOffset": 165}, {"referenceID": 4, "context": "Driver distraction is often construed as a key source of attention divergence from the roadway and the topic of numerous scientific studies and design guidelines [4], [5].", "startOffset": 167, "endOffset": 170}, {"referenceID": 5, "context": "Such work would suggest that a real-time estimation of drivers gaze could be coupled with an alerting system to enhance safety [6].", "startOffset": 127, "endOffset": 130}, {"referenceID": 6, "context": "In fact, gaze classification performance can be good based on head pose alone [7], because it frequently correlates with eye pose, but not always.", "startOffset": 78, "endOffset": 81}, {"referenceID": 7, "context": "The inter-person classification and gaze strategy variation is discussed using the analogy of an \u201cowl\u201d and \u201clizard\u201d (introduced previously in [8], [9]) which describes the degree to which the eyes and the head move when shifting gaze.", "startOffset": 142, "endOffset": 145}, {"referenceID": 8, "context": "The inter-person classification and gaze strategy variation is discussed using the analogy of an \u201cowl\u201d and \u201clizard\u201d (introduced previously in [8], [9]) which describes the degree to which the eyes and the head move when shifting gaze.", "startOffset": 147, "endOffset": 150}, {"referenceID": 9, "context": "The problem of gaze tracking from monocular video has been investigated extensively across many domains [10], [11].", "startOffset": 104, "endOffset": 108}, {"referenceID": 10, "context": "The problem of gaze tracking from monocular video has been investigated extensively across many domains [10], [11].", "startOffset": 110, "endOffset": 114}, {"referenceID": 11, "context": "The algorithm in [12] uses an ensemble of regression trees for super-real-time face alignment.", "startOffset": 17, "endOffset": 21}, {"referenceID": 11, "context": "algorithm draws upon this method as it is built on a decade of progress on the face alignment problem (see [12] for a detailed review of prior work).", "startOffset": 107, "endOffset": 111}, {"referenceID": 12, "context": "Murphy-Chutorian and Trivedi [13] describe 74 published and tested systems from the last two decades.", "startOffset": 29, "endOffset": 33}, {"referenceID": 6, "context": "Prior work has shown that such a classification set is sufficient for the in-vehicle environment, even under rapidly shifting lighting conditions [7].", "startOffset": 146, "endOffset": 149}, {"referenceID": 13, "context": "Methods usually track corneal reflection, distinct pupil shape in combination with edge-detection, characteristic light intensity of the pupil, or a 3D model of the eye to derive an estimate of an individual\u2019s pupil, iris, or eye position [14].", "startOffset": 239, "endOffset": 243}, {"referenceID": 14, "context": "Our approach uses an adaptive CDF-based method [15] in conjunction with face alignment that significantly narrows the search space.", "startOffset": 47, "endOffset": 51}, {"referenceID": 6, "context": "Studies of the correlation between head and eye movement have shown inter-person variation in the degree to which the head serves as a proxy for gaze [7], [9].", "startOffset": 150, "endOffset": 153}, {"referenceID": 8, "context": "Studies of the correlation between head and eye movement have shown inter-person variation in the degree to which the head serves as a proxy for gaze [7], [9].", "startOffset": 155, "endOffset": 158}, {"referenceID": 8, "context": "For example, a previous work tested drivers\u2019 head movements while looking at the \u201croad\u201d and the \u201ccenter stack\u201d and found that: (1) drivers\u2019 horizontal range of head movements varied (from 5 to 20 degrees) across individuals along with (2) their mean differences of horizontal head angles while looking at the two objects (from 0 to 10 degrees) [9].", "startOffset": 344, "endOffset": 347}, {"referenceID": 15, "context": "Training and evaluation is carried out on a dataset of 40 subjects drawn from a larger driving study of 80 subjects that took place on a local interstate highway (see [16] for detailed experimental methods).", "startOffset": 167, "endOffset": 171}, {"referenceID": 15, "context": "As detailed in [16], any discrepancies between the two annotators were meditated by an arbitrator.", "startOffset": 15, "endOffset": 19}, {"referenceID": 16, "context": "This method of double annotation and mediation of driver gaze has been shown to produce very accurate annotations that can be effectively used as ground truth for supervised learning approaches [17].", "startOffset": 194, "endOffset": 198}, {"referenceID": 17, "context": "The face detector in our pipeline uses a Histogram of Oriented Gradients (HOG) combined with a linear SVM classifier, an image pyramid, and sliding window detection scheme implemented in the DLIB C++ library [18].", "startOffset": 208, "endOffset": 212}, {"referenceID": 18, "context": "The performance of this detector has lower FAR than the widely-used default Haar-feature-based face detector available in OpenCV [19] and thus is more appropriate for our application.", "startOffset": 129, "endOffset": 133}, {"referenceID": 12, "context": "Both face alignment and head pose estimation are extremely well studied problems in computer vision [13], [20].", "startOffset": 100, "endOffset": 104}, {"referenceID": 19, "context": "Both face alignment and head pose estimation are extremely well studied problems in computer vision [13], [20].", "startOffset": 106, "endOffset": 110}, {"referenceID": 20, "context": "Face alignment in our pipeline is performed on a 68-point Multi-PIE facial landmark mark-up used in the iBUG 300-W dataset [21].", "startOffset": 123, "endOffset": 127}, {"referenceID": 11, "context": "The algorithm for aligning the 68-point shape to the image data uses a cascade of regressors as described in [12] and implemented in [18].", "startOffset": 109, "endOffset": 113}, {"referenceID": 17, "context": "The algorithm for aligning the 68-point shape to the image data uses a cascade of regressors as described in [12] and implemented in [18].", "startOffset": 133, "endOffset": 137}, {"referenceID": 12, "context": "These features can be mapped directly to a gaze region using methods that fall under the Nonlinear Regression category defined in [13].", "startOffset": 130, "endOffset": 134}, {"referenceID": 12, "context": "This is categorized under Geometric Methods in [13].", "startOffset": 47, "endOffset": 51}, {"referenceID": 21, "context": "The geometric approach uses OpenCV\u2019s SolvePnP solution of the PnP problem [22].", "startOffset": 74, "endOffset": 78}, {"referenceID": 14, "context": "We use a CDF-based method [15] to extract the pupil from the image of the right eye, and adjust the extracted pupil blob using morphological operations of erosion and dilation.", "startOffset": 26, "endOffset": 30}, {"referenceID": 22, "context": "5) Perform an \u201copening\u201d morphology transformation (described in [23]).", "startOffset": 64, "endOffset": 68}, {"referenceID": 22, "context": "6) Perform a \u201cclosing\u201d morphology transformation (described in [23]).", "startOffset": 63, "endOffset": 67}, {"referenceID": 6, "context": "The driver spends more than 90% of their time looking forward at the road and this fact was used in [7] to normalize the position of facial features relative to the average bounding box of the face associated with the \u201cRoad\u201d gaze region.", "startOffset": 100, "endOffset": 103}, {"referenceID": 23, "context": "Scikit-learn implementation of a random forest classifier [24] is used to generate a set of probabilities for each class from a single feature vector.", "startOffset": 58, "endOffset": 62}, {"referenceID": 6, "context": "The effect of this threshold is explored in [7].", "startOffset": 44, "endOffset": 47}, {"referenceID": 24, "context": "The training data for each of the 6 classes is balanced by random sub-sampling [25].", "startOffset": 79, "endOffset": 83}], "year": 2016, "abstractText": "Accurate, robust, inexpensive gaze tracking in the car can help keep a driver safe by facilitating the more effective study of how to improve (1) vehicle interfaces and (2) the design of future Advanced Driver Assistance Systems. In this paper, we estimate head pose and eye pose from monocular video using methods developed extensively in prior work and ask two new interesting questions. First, how much better can we classify driver gaze using head and eye pose versus just using head pose? Second, are there individual-specific gaze strategies that strongly correlate with how much gaze classification improves with the addition of eye pose information? We answer these questions by evaluating data drawn from an on-road study of 40 drivers. The main insight of the paper is conveyed through the analogy of an \u201cowl\u201d and \u201clizard\u201d which describes the degree to which the eyes and the head move when shifting gaze. When the head moves a lot (\u201cowl\u201d), not much classification improvement is attained by estimating eye pose on top of head pose. On the other hand, when the head stays still and only the eyes move (\u201clizard\u201d), classification accuracy increases significantly from adding in eye pose. We characterize how that accuracy varies between people, gaze strategies, and gaze regions.", "creator": "LaTeX with hyperref package"}}}