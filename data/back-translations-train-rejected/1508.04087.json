{"id": "1508.04087", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "17-Aug-2015", "title": "The SP theory of intelligence: distinctive features and advantages", "abstract": "This paper aims to highlight distinctive features of the \"SP theory of intelligence\" and its apparent advantages compared with some AI-related alternatives. The theory is outlined in an appendix. The distinctive features and advantages are summarised: simplification and integration of observations and concepts, and of structures and processes in computing systems; the SP theory is itself a theory of computing; a central role for information compression via the matching and unification of patterns, and for multiple alignment; transparency in the representation and processing of knowledge; the discovery of 'natural' structures via information compression (DONSVIC); interpretation of aspects of mathematics; interpretation of phenomena in human perception and cognition; realisation of abstract concepts in terms of neurons and their inter-connections (\"SP-neural\"). Distinctive features and advantages of the SP system compared with alternatives in: minimum length encoding and related concepts; deep learning in neural networks; universal search; Bayesian networks and other models for AI; analysis and production of natural language; learning natural language; exact and inexact reasoning; representation and processing of diverse forms of knowledge; IBM's Watson; problems associated with big data, and in the development of intelligence in autonomous robots. In conclusion, a major strength of SP system is that it can provide a firm foundation for the long-term development of AI and, at the same time, it may deliver some benefits and applications on relatively short timescales. It is envisaged that a high-parallel, open-source version of the SP machine will be created, hosted on an existing high-performance computer and derived from the existing SP computer model. This will be a means for researchers everywhere to explore what can be done with the system, and to create new versions of it.", "histories": [["v1", "Mon, 17 Aug 2015 17:15:13 GMT  (61kb,D)", "http://arxiv.org/abs/1508.04087v1", null], ["v2", "Mon, 24 Aug 2015 08:48:08 GMT  (62kb,D)", "http://arxiv.org/abs/1508.04087v2", null], ["v3", "Thu, 17 Sep 2015 10:16:04 GMT  (112kb,D)", "http://arxiv.org/abs/1508.04087v3", null], ["v4", "Fri, 6 Nov 2015 17:59:52 GMT  (109kb,D)", "http://arxiv.org/abs/1508.04087v4", null], ["v5", "Sun, 20 Dec 2015 12:05:50 GMT  (112kb,D)", "http://arxiv.org/abs/1508.04087v5", null], ["v6", "Tue, 15 Mar 2016 16:09:02 GMT  (180kb,D)", "http://arxiv.org/abs/1508.04087v6", null]], "reviews": [], "SUBJECTS": "cs.AI", "authors": ["j g wolff"], "accepted": false, "id": "1508.04087"}, "pdf": {"name": "1508.04087.pdf", "metadata": {"source": "CRF", "title": "The SP theory of intelligence: its distinctive features and advantages", "authors": ["J Gerard Wolff"], "emails": ["jgw@cognitionresearch.org;"], "sections": [{"heading": null, "text": "Dr. Gerry Wolff, BA (Cantab), PhD (Wales), CEng, MBCS (CITP); CognitionResearch.org, Menai Bridge, UK; jgw @ cognitionresearch.org; + 44 (0) 1248 712962; + 44 (0) 7746 290775; Skype: gerry.wolff; Web: www.cognitionresearch.org.ar Xiv: 150 8.04 087v 1 [cs.A I] 1 7A ugget attractive short-term benefits, a great strength of the SP system is that it can provide a solid foundation for the long-term development of many aspects of artificial intelligence, while delivering some benefits and applications in a relatively short time. It is planned that a highly parallel, open source version of the SP machine will be created, hosted on an existing high-performance computer and derived from the existing SP computer model."}, {"heading": "1 Introduction", "text": "The SP Theory of Intelligence outlined in Appendix A. is a unique attempt to simplify and integrate observations and concepts between artificial intelligence, mainstream computing, mathematics, and human perception and cognition.The main objective of this paper is to highlight the characteristics of SP theory and its obvious advantages over some AI-related alternatives. The next section provides a comprehensive overview of the characteristics of SP theory and its strengths. The main objective in the following sections is to highlight the obvious advantages of SP concepts over some alternatives, recognizing the intellectual debt of the SP system and its weaknesses. Given the extensive literature in AI and related fields and the broad spectrum of SP theory, the treatment is necessarily selective."}, {"heading": "2 Overview of distinctive features and strengths", "text": "This section is an extended and revised version of [59, section II-G], which summarizes the main characteristics and strengths of the SP system."}, {"heading": "2.1 Simplification and integration of observations and", "text": "Although the theory is not yet complete (Section A.9), there is now much evidence that the experiment is proving successful - that SP theory combines relative simplicity with descriptive and explanatory power in accordance with Occam's razor (Appendix B) in a wide range of observations and concepts (Appendix A.7) and a wide range of potential benefits and applications (Appendix A.8). Combining relative simplicity with descriptive and explanatory power is perhaps the most characteristic feature of SP theory and a great strength of the theory."}, {"heading": "2.2 Simplification and integration in computing sys-", "text": "The provision of a simple format for knowledge (Appendix A.3) and a framework for knowledge processing (Appendix A.4) promotes a general simplification of computer systems, including hardware and software [62, Section 5]. These two things also promote the seamless integration of different types of knowledge and various aspects of intelligence [62, Section 7], an integration that seems to be necessary if we are to achieve human-like versatility and adaptability in AI [59, Section IV-A]."}, {"heading": "2.3 The SP theory is a theory of computing", "text": "Most other AI-related research is based on the idea that computing can be understood in the sense of the Universal Turing Machine [48] or equivalent models such as Lamda Calculus [3] or Post's Canonical System [38]. [1] In contrast, SP theory itself is a computer theory [55, Chapter 4]. What is special about SP theory as a computer theory is that it provides much of the human-like intelligence missing in earlier models. [10] An apparent exception is the concept of a \"neuronal Turing machine.\""}, {"heading": "2.4 Information compression via the matching and uni-", "text": "In trying to overcome complexities, the SP research program focuses on a simple, \"primitive\" idea: that information compression can be understood as a search for patterns that match, with the merging or \"unification\" of patterns that are the same (\"ICMUP\" means \"information compression by matching and unifying patterns.\" See Appendix A.4.1). The potential advantage of this approach is that it can help us avoid old tram lines and open doors to new ways of thinking."}, {"heading": "2.5 Multiple alignment", "text": "More specifically, ICMUP provides the basis for a concept of multiple alignment borrowed from and adapted to this concept in bioinformatics (Appendix A.4.2). Developing this idea as a framework for the simplification and integration of broad-based concepts was an important undertaking. Multiple alignment is a characteristic and effective idea in the SP research programme."}, {"heading": "2.6 Transparency in the representation and processing", "text": "In contrast to sub-symbolic approaches to artificial intelligence (artificial neural networks, deep learning and related approaches) and despite objections to symbolic AI, 2 knowledge in the SP system is transparent and visible, as is the processing of knowledge."}, {"heading": "2.7 The DONSVIC principle", "text": "A related point is the expectation, confirmed by previous findings, that unsupervised learning in the SP system will correspond to the principle \"DONSVIC\" - The discovery of natural structures through information compression [57, Section 5.2]. In contrast to subsymbolic approaches to artificial intelligence, it is expected that structures that arise through unsupervised learning in the SP system will normally be comprehensible to humans."}, {"heading": "2.8 Mathematics", "text": "Unlike other approaches to artificial intelligence, mainstream computing, or human perception and cognition, SP theory has much to say about the nature of mathematics. In short, it seems that several aspects of mathematics can be understood in the sense of ICMUP and potentially in the sense of multiple orientation [55, Chapter 10], [61]. Although logic has received less attention in the SP research program, it seems likely that similar principles will be applied there [55, Chapter 10]."}, {"heading": "2.9 Perception and cognition", "text": "The SP theory is largely based on research on the perception and cognition of humans and animals, as well as on neuroscience. In particular, an important part of its inspiration is the development of computer models for learning natural language (summarized in [53]).3"}, {"heading": "2.10 SP-neural", "text": "SP-Neuronal Theory contains suggestions - SP-Neuronal - on how abstract concepts can be applied in theory in relation to neurons and neuronal processes. SP-Neuronal Proposals (Appendix A.6) differ significantly from artificial neural networks, as they are usually conceived in computer science, and are probably more plausible in terms of neuroscience."}, {"heading": "3 Minimum length encoding, algorithmic in-", "text": "As mentioned in Appendix A.4, information compression in SP theory can be seen as an example of the minimum length encoding (MLE) principle [44, 51, 39]. Information compression and MLE are also obvious 3See also \"Language Learning\" in www.w.cognitionresearch.org.related to algorithmic information theory (AIT), and Kolmogorov complexity (KC) [28]. These interrelated areas of study include the following characteristics of SP theory: \u2022 Most research on information compression, MLE, AIT and KC assumes that \"compression by Universal Turbine Theory (S4) plays a central role in contrast to most other areas of S4 compression theory.\""}, {"heading": "4 Deep learning in neural networks", "text": "This section on deep learning (DL) in artificial neural networks (ANNs) is mainly based on a report by Schmidhuber [42], who has a track record and many years of experience in this field. Without in any way diminishing the undeniable success of DL in ANNs (both of which are collectively referred to as NNs), the aim here is to highlight the potential benefits of the SP system. This may seem exaggerated, as the SP system, unlike some NNs, has not won any competitions and has not been taken over or promoted by any company or integrated into products. However, for the reasons given in the following subsections, it seems that the SP system is built on firmer foundations than the current generation of NNs and its long-term prospects are better. As mentioned in Appendix A.9, there is also potential for applications in relatively short times.The wide variety of NNs makes it difficult to say things that apply to all."}, {"heading": "4.1 Adaptability of structures", "text": "There is a superficial similarity between NNs and multiple alignments (especially if these are realized as SP neuronal, as described in Appendix A.6), as both have levels or levels and both have connections between the layers. However, NNs are not multiple alignments and offer much less scope for adjustments: \u2022 By default, the number of layers of an NN and the size of each layer are predefined, while the number of rows in a multiple alignment and their size depend entirely on the incoming and stored information from which it is built. \u2022 Normally, there is only a set of layers in a NN with a structure that is fixed, although its behavior can be changed by changing the strength of links within the structure. By contrast, the SP system works by normally building a large variety of multiple alignments, each having an alignment by drawing patterns from a pool of new patterns and what may be a very large pool of old patterns. \u2022 By default, each given layer can have multiple alignments in one system, where only one system is directly connected to another where there is a multiple N)."}, {"heading": "4.2 Biological validity", "text": "It is generally accepted that NNs are loosely related to biological systems. For example: \"In modern software implementations of artificial neural networks, the biology-inspired approach has been largely abandoned in order to find a more practical approach based on statistics and signal processing.\" 5Although there are still large gaps in our knowledge of neural structures in the brain and their functions, it seems that the organization and functioning of the SP system is better supported by available evidence: \u2022 Language Learning Models. \"As mentioned in Section 2.9, the SP research program largely derives from previous research that developed computer models of language learning in children. This research is supported by much relevant empirical evidence contained in the Group Method of Data Handling (GMDH), the number of layers and the number of neurons in each layer depend on the problem being solved [42, Section 5.3]."}, {"heading": "4.3 Learning paradigms", "text": "In this context, supervised learning, unsupervised learning, reinforcement learning - three forms of learning with NNs - are usually treated as equivalent alternatives. 7.In contrast, from an SP point of view, unsupervised learning is seen as the basis for all other forms of learning [59, Sections V-A.1 and V-A.2]. The main reasons for this are, in short: \u2022 Much learning happens without the benefit of labelled examples, help from a teacher or carrots and adhesives. 86 Sometimes it is suggested that the concept of a grandmother's cell or cells is implausible because the death of the cell or cells would mean that one could no longer recognise the grandmother. But this is exactly the kind of thing that can happen in people who have had a stroke or suffer from dementia. 7Although Schmidhuber acknowledges that unsupervised learning can facilitate unsupervised learning and reinforcement learning in relation to the latter point, it is clear that motivations have an impact on learning - if we do things in contrast to theory."}, {"heading": "4.4 Learning from a single occurrence or experience", "text": "Most NGOs incorporate a variant of the idea, proposed by Donald Hebb [13] and known as \"Hebbian learning,\" that neurons firing at about the same time tend to network, or reinforce existing connections between them. This mechanism of learning leads to gradual changes in the behavior of NGOs, in line with the observation that it takes time to learn things like speaking, or how to win competitions in golf. This correspondence between the functioning of NGOs and a familiar trait can reinforce the belief that NGOs are psychologically valid, but this trait seems to conflict with the undeniable fact that we can learn things from a single event or experience."}, {"heading": "4.5 Computational resources, speed of learning, and", "text": "It seems as if an NN with \"16,000 computer processors\" and \"a billion connections\" is led to \"10 million randomly selected YouTube videos that need to be processed over the course of three days.\" Then, after being presented with a list of 20,000 different articles, he began to recognize images of cats. \"Another piece of news is that he lists billions of connections that need to be processed for each image.\""}, {"heading": "4.6 Recognition of images and speech", "text": "With some qualifications (appendices 4,7 and 4,8), NNs perform well in tasks such as the recognition of images (eg [63]) or language (eg [5]). But the SP system has strengths in pattern recognition ([55, chapter 6], [57, section 9]). These include: 11According to an estimate [14], the average human brain contains about 86 billion neurons. \u2022 That it can detect patterns at multiple levels of abstraction, with the integration of class relations and holistic relationships; \u2022 It can model \"familial similarities\" or polythetic categories, meaning that recognition does not depend on the presence of a particular trait or a combination of traits; \u2022 Recognition is robust in the face of errors of omission, commission or substitution; For any given identification or any related inference, the SP system can calculate associated probabilities."}, {"heading": "4.7 Deep neural networks are easily fooled", "text": "A recent report describes how \"we can cause [a deep neural network] to misclassify an image by applying a certain imperceptible disturbance.\" [47, summary] Another report [37] describes how at least some kind of deep neural network can quite easily be tricked into assigning an image with near certainty to a recognizable class of objects such as \"guitar\" or \"penguin,\" when people judge the given image as something like white noise on a television screen or an abstract pattern that contains nothing resembling a guitar or a penguin or other object. Of course, these kinds of errors are a potential problem in any application in which recognition must be reliable, and without a good theory of how NNs work (Section 4.11), they can be difficult."}, {"heading": "4.8 Under-generalisation and over-generalisation", "text": "One problem of any learning system is its ability to generalize based on the data (I) that form the basis of its learning, without generalizing (\"overfitting\") or generalizing (\"underfitting\"). For example, if the system has learned the term \"horse,\" in its subsequent recognition of horses it should not be too closely bound to recognize only horses that are identical or very similar to those in I (undergeneralization), while at the same time it should not make mistakes such as classifying cows, sheep or dogs in the category of \"horse\" (overgeneralization)."}, {"heading": "4.8.1 Under-generalisation in NNs", "text": "It is widely accepted that NNs can suffer from overadaptation, and various solutions have been proposed. Srivastava and colleagues [46] suggest that some neurons in an NN, along with their connections, can be randomly removed from the NN during training to prevent them from adapting too much; while Zeng and colleagues [65] point out that in a multi-level classifier, unsupervised pre-training and specially designed step-by-step supervised training can help to avoid overadaptation; and Wiesler and colleagues [52] say that they have found that a \"factorized structure\" can be effective against overadaptation."}, {"heading": "4.8.2 Over-generalisation in NNs", "text": "The problem of underfit in NNs has also attracted attention. Dauphin and Bengio [6], for example, show how underfit can occur when some large neural networks do not make full use of their computing capacities, and they make suggestions to overcome the problem; and Ganin and Lempitsky [9] describe how a \"two-tiered architecture\" can help overcome problems of underfit."}, {"heading": "4.8.3 Almost certainly, information compression solves both problems", "text": "Without making a detailed comparison with alternative representations of over- and underfit in NNs, it is suggested here that information compression in SP theory provides a simple, elegant solution to both problems.Section 8.0.4 outlines how information compression in connection with learning the first language or language of children can explain how they learn to generalize correctly beyond the language they have heard, while eliminating the exaggerated generalizations that prevail in the early stages. It seems likely that the same principles would also apply to learning grammars to recognize images or language, thereby solving two problems - the problems of overfit and underfit - with an overriding principle. SP system can also help solve the problem of overfit in such a way that it can detect patterns by multiple alignment in the face of errors of omission, picking and substitution (Section 4.6)."}, {"heading": "4.9 Information compression", "text": "Schmidhuber's review [42] contains a short section (4.4) on \"Occams razor: compression and minimum description length (MDL),\" and it mentions information compression in some other sections. Although it implies (in Section 5.10) that \"much of machine learning is essentially about compression,\" the general tenor of the review is that information compression is only one of several \"recurring themes\" in deep learning without much importance. In contrast, information compression is fundamental in SP theory, since it runs like Blackpool in a chunk of rock, in its foundations (Appendix A.1), in the mapping and unification of patterns (Appendix A.4.1), in the construction of multiple arrangements (Appendix A.4.2) and in unattended learning (Appendix A.4.3).Given the evidence of the importance of information compression in intelligence, computer technology and mathematics (Appendix A.1), comparing this status with the design of the P and peripheral information."}, {"heading": "4.10 Transparency in the representation and process-", "text": "In contrast to these uncertainties, the problem of NNs is that there is considerable uncertainty about how they represent knowledge and how they process it:... \"we actually understand surprisingly little of why certain models work and others do not.... One of the challenges of neural networks is to understand exactly what is going on at each level.\" [33]... \"no one knows how neural networks come to their answers.\" [12] A programmer simply needs to adjust the number of nodes and layers in order to optimize how he captures relevant features in the data. However, since it is impossible to say exactly how a neural network works, this fine-tuning is a matter of trial and error. \"With respect to the first quote, it is true that NNNs, as described in the blog, can be made to reveal part of their knowledge. However, while many of the resulting images have artistic appeal, they are not transparent representations of knowledge, and it is not clear how they will be learned to function or how they will function as system objects in such SD-S1 S1-Ss. In contrast to SD-S1, SNs are all kinds of uncertainties."}, {"heading": "4.10.1 Distributed or localist encodings", "text": "The prevailing view is that in neural networks, knowledge of a concept like the \"grandmother\" is encoded in neurons that are widely distributed and have connections between them. To support this \"sub-symbolic\" view, there is no distinction between individual high-level units and random linear combinations of high-level units according to various methods of unit analysis. \"In our experiments, we obtained neurons that, as detectors for faces, human bodies and cat faces, contain the bulk of the semantic information.\" [47, Abstract] But some researchers suggest that it is possible to train neurons to be selective for high-level concepts. \"In our experiments, we obtained neurons that function as detectors for faces, human bodies and cat faces on random frames of YouTube videos.\" [23, Summary] This research seems to support the \"localistic\" or \"symbolic\" empirical \"that these neurons are regarded as very fast neurons.\""}, {"heading": "4.10.2 Class-inclusion hierarchies and Part-whole hierarchies", "text": "With respect to hierarchical structures, there seems to be some uncertainty as to whether NNs would challenge the highest layer of the N hierarchy: \u2022 Class inclusion hierarchies: \"Deep learning allows computational models that consist of multiple processing layers to learn representations of data with multiple levels of abstraction.\" [24, Abstract] \"Hidden layers: these learn more abstract representations of how to stick your head up.\" 13). \u2022 Or partial hierarchies:... \"the first layer may look like edges or corners. Intermediate layers interpret the basic features to search for general shapes or components, like a door or a leaf. The last layers assemble them to complete interpretations... like entire buildings or trees.\" [33] And anyway, the representation is obscure. While it is clear that a human face is part of a human body or components, like a door or a leaf. (Section 4.10.1) It seems reasonable to assume that \"concepts such as\" horse and hierarchy \"are strongest in hierarchy.\""}, {"heading": "4.10.3 Iteration and recursion", "text": "In connection with the questions discussed above, questions are how NNs can encode repeated cases of this or that category (such as the many cases of horses, cows and sheep that one would expect at an agricultural exhibition - Section 4.10.2), and how NNs can encode the types of recursive structures that predominate in natural language, such as this horse, the dog and the horn that belonged to the farmer who sowed his grain, who held the rooster crowing in the morning,.... In contrast to uncertainties about how NNs can deal with such structures, the SP theory provides clear answers: \u2022 Any given SP pattern can appear one, two or more times within a multiple alignment. An example is how the pattern representing a noun phrase (\"NP\") should not arise from any of the preceding four categories shown in [57, Figure 4], once in row 7 and once in row 2."}, {"heading": "4.11 Theoretical foundations", "text": "Problems of transparency in the representation and processing of knowledge (Section 4.10) and errors in recognition (Section 4.7) appear to be symptoms of deeper problems with NNs: weak or missing theoretical foundations. Problems in this area are also suggested by the many different versions of NNs: \"time-delaying\" neural networks, \"gradient-based deepening learners with alternating revolutionary and scanning layers,\" \"weight distribution,\" \"nonlinear automatic feedback with exogenous recursive inputs,\" \"max-pooling convolutional,\" \"multi-column GPU-max-pooling convolutional,\" \"bidirectional short-term memory recursive\" and more [42]. It is true that the creation and testing of many versions of the SP computer model were important for the development of SP theory [57, Section 2.5]. But now the model - the first version of the SP machine - is relatively stable with the NICP and NICP system in parallel development."}, {"heading": "4.12 Symbolic AI", "text": "\"In terms of the representation of knowledge in the brain, one of the most important challenges is to understand how neural activations, which are widespread and sub-symbolic, lead to symbolic behavior, such as language and logical thinking.\" [7, Section 2]. \"[Deep learning techniques] have no obvious way to draw logical conclusions...\" [14] This section briefly discusses aspects of artificial intelligence that seem to be problematic for NNN and where the SP system is relatively strong. By and large, these seem to be areas where the \"symbolic\" tradition of artificial intelligence has proved relatively successful."}, {"heading": "4.12.1 The processing of natural language", "text": "One assumption in the SP research program is that in the search for human capabilities with natural language, we should aim to create a system with human knowledge of the syntactic and semantic structures of language, in contrast to the proposal by Zhang and LeCun [66, Section 1] that... \"text understanding can be managed through a deep learning system without artificially embedding knowledge of words, phrases, sentences, or other syntactic or semantic structures associated with a language.\" Of course, people can gain a superficial understanding of natural language from a few scattered clues, and in some situations that might be all that is needed. However, for a thorough understanding of, say, legal, philosophical, or scientific reasoning, it seems unlikely that we can circumvent the need for good knowledge of syntactic and semantic structures."}, {"heading": "4.12.2 Reasoning and other \u2018symbolic\u2019 aspects of AI", "text": "A few years ago there was quite a lively interest in how NNs could argue (e.g. [27]), but this interest seems to have waned, probably because NNs really are not well suited to much more than the relatively simple types of conclusions that correct errors of omission, commission, or substitution in pattern recognition. In contrast, the SP system shows several types of arguments with clear developmental potential (Section 9), similar things can be said about areas that seem problematic for NNN and where symbolic AI and the SP system are relatively pronounced, such as planning [55, Chapter 8], problem solving (ibid), and grammatical conclusions ([55, Chapter 9], [57, Section 5]. 15Slide 201 in \"Deep Learning for NLP (without magic),\" slide show by R. Socher and C. Manning, dated 2015-07-27, stanford.io / 1bmsBKK."}, {"heading": "4.12.3 Discussion", "text": "In general, the SP system appears to be relatively strong compared to NNs in areas of AI where the \"symbolic\" approach has been successful, for three main reasons: \u2022 Unlike NNs, there is transparency in both the representation and processing of knowledge (Section 2.6). \u2022 Within the multiple alignment framework, it is possible to model concepts from mainstream computing and the symbolic tradition of AI such as \"variable,\" \"value\" and \"type\" (Section 9). \u2022 The multiple alignment framework is much more adaptable than the deep learning framework (Section 4.1)."}, {"heading": "5 Universal search", "text": "Some ideas that can be loosely summarized under the heading \"universal search\" do not, at first glance, seem to offer comprehensive solutions to problems in AI and beyond. Solomonoff [45] has argued that the vast majority of problems in science and mathematics can be regarded either as \"inversion problems\" or as \"time-limited optimization problems,\" and that both types of problems can be solved by inductive conclusions by applying the principle of minimal length coding. In the \"Levin search\" [25, 26] which aims to solve inversion problems, all sorts of programs are interconnected on a universal turing machine, with the computing time equally divided between them until one of the executed programs solves the problem. Ideals of this kind of hutter have been developed (e.g. [17], Schmidhuber (e.g. [41]), and others seek."}, {"heading": "6 Some models for AI", "text": "In this section, some of the systems proposed as artificial intelligence models or aspects of artificial intelligence are briefly considered: Bayesian networks, supporting vector machines, hidden Markov models, Kalman filters, self-organizing maps, petri nets, and cellular automatons. The suggestion here, which admittedly represents a fairly far-reaching generalization, is that while these models are admirably simple and applicable in certain areas, they lack the descriptive and explanatory reach of the SP system. In terms of searching for a far-reaching theory (sections B and 2.1), their scope is too narrow. In addition to its relatively wide scope, the SP system has two main advantages over a Bayesian network: \u2022 simplicity in the representation of statistical knowledge. Each node in a Bayesian network contains a table of conditional probabilities for all possible combinations of inputs, and these tables can be quite large. In contrast, the SP system requires only one set of absolute frameworks for each category of probabilities or the other."}, {"heading": "7 Analysis and production of natural language", "text": "This section considers, with varying degrees of emphasis, the strengths and potential of the SP system in the analysis, understanding and production of natural language, compared to symbolic approaches to these topics. Comparisons with neural networking approaches are discussed in Section 4.The analysis and production of natural language is a relatively mature and successful area of symbolic AI. The SP computer model is not an immediate competitor to existing systems, but its long-term potential is considerable: \u2022 It can model natural language parsing directly and transparently, as illustrated in Figure 2; \u2022 When parsing, it can absorb syntactic ambiguities, resolving ambiguities through the provision of appropriate context and recursive structures in syntax; \u2022 Parsing is robust against errors of omission, commission or substitution; \u2022 A mechanism can achieve both parsing and the production of natural language without any changes; \u2022 The system provides the representation and processing of several non-synthetic forms of knowledge."}, {"heading": "8 Learning natural language", "text": "Although grammatical conclusions have been the subject of research for many years, the automatic learning of the syntax of a natural language remains a major challenge. Even more difficult is the automatic learning of the types of syntactic-semantic structures necessary for things like the interpretation of the meaning of natural language, the production of language or writing from meanings, and, if done at a high level, the translation from one language to another. As mentioned in Appendix A, the SP research program has emerged from previous research that develops computer models of language learning, but requires a radical reorganization of earlier models in order to achieve the goals of the SP program. Now, the SP computer model demonstrates that the unattended learning of plausible generative grammars for the syntax of artificial languages, including the learning of segmental structures, structural and abstract patterns, is consistent with the principle of generalization."}, {"heading": "8.1 Learning the syntax of natural language", "text": "Currently, the SP computer model cannot learn plausible grammar for a natural language, probably due to two weaknesses mentioned in Appendix A.9: its inability to learn intermediate levels of abstraction or discontinuous dependencies on data. However, it seems that these two problems are solvable, and it seems likely that the SP model, with its solution, would become a powerful tool for unattended learning of realistic grammars for natural language syntax, at least for text language."}, {"heading": "8.2 Learning semantic structures and the integration", "text": "The use of a simple format for the presentation of knowledge (Appendix A.3) and a versatile framework for the processing of knowledge (Appendix A.4) means versatility in the presentation and processing of different kinds of knowledge (Appendix A.7). More specifically, it probably means: \u2022 versatility in the learning of different kinds of knowledge, including the meanings or \"semantics\" of natural language. \u2022 Potential for the learning of integrated syntactic-semantic structures - the types of structures that can serve in the interpretation of language [55, Figure 5.18], the production of language from meanings [55, Figure 5.18] and the translation of one language into another [60, Section IIIA.4]."}, {"heading": "9 Exact and inexact forms of reasoning", "text": "In this context, it should be noted that this is a very complex situation, in which both sides are on an equal footing and in which it is important to find a solution to their problems."}, {"heading": "10 Representation and processing of diverse", "text": "Forms of knowledgeOne problem with AI and, more generally, computer technology, as it has evolved to date, is that knowledge can be presented with a large number of different formalities, and that there are a large number of different formats for each one. This complexity is exacerbated by the fact that usually every formalism and format has its own way of processing it. Until recently, this complexity was easy to ignore, but with the advent of big data, the SP system can represent a variety of types of knowledge, including: \u2022 The syntax of natural language; \u2022 Class hierarchies, part-whole hierarchies and their integration; 17 \u2022 Trees and networks, including Bayesian networks; \u2022 Entity relationship structures (tuples).So if rules, systems that support a multiple system; \u2022 Believing that it is a multiple system. \""}, {"heading": "11 IBM\u2019s Watson", "text": "It is known that a research team from IBM developed a computer system called Watson that beat the best human players in the TV quiz Jeopardy in 2011! 18Of course, this is a great achievement, with potential benefits in terms of ideas and, perhaps, applications. But doubts have been expressed about its importance to AI:... \"Systems that seem to master complex language tasks, like IBM Jeopardy! Winner Watson, do it by being super-specialized in a particular format.\" It's nice, but not work that would really apply to any other situation, \"says Yann LeCun] 19and there are possible concerns about how it was developed and will be. Dave Ferrucci, leader of the team that developed Watson, was quoted as saying,\" Did we sit down when we built Watson and try to model human cognition? Absolutely not."}, {"heading": "12 Big data and autonomous robots", "text": "Possible benefits and applications for SP theory are summarized in Appendix A.8. In this section, two potential applications are discussed in more detail: big data and autonomous robots.23 \"IBM pushes deep learning with a Watson upgrade,\" MIT Technology Review, 2015-07-09, bit.ly / 1Nq0bMg."}, {"heading": "12.1 Big data", "text": "The paper \"Big Data and the SP Theory of Intelligence\" [60] describes how SP theory can help solve nine problems with big data: \u2022 Helping overcome the problem of diversity in big data. \u2022 The SP system can serve as a universal framework for the representation and processing of knowledge (UFC), which helps tame the wide variety of formalities and formats for data, each with its own way of processing (Section 10). \u2022 Learning and discovering. In accordance with the DONSVIC principle [57, Section 5.2], the system has strengths in uncontrolled learning or the discovery of \"natural\" structures in data, with potential for further development. \u2022 The SP system has strengths in areas such as pattern recognition, retrieval of information and production of natural language, translation from one representation into another, multiple types of reasoning, planning and problem solving."}, {"heading": "12.2 Autonomous robots", "text": "The paper \"Autonomous Robots and the SP Theory of Intelligence\" [59] describes how the SP theory can help in designing the information-processing \"brains\" of autonomous robots: \u2022 Computer and energy efficiency. This is a revised version of the discussion in [60]. \u2022 Towards human versatility in intelligence. The strengths of the SP system in various areas, summarised in Appendix A.7, can help in the development of human versatility in autonomous robots. \u2022 Towards human adaptability in intelligence. It seems that unattended learning in the SP framework has potential as a key to human adaptability in intelligence, both directly and as a basis for other types of learning. This approach to developing intelligence in autonomous robots differs significantly from others and is more promising."}, {"heading": "13 Conclusion", "text": "Previous sections of this paper aim to highlight the peculiarities of the SP theory of intelligence and its obvious advantages over some AI-related alternatives. \u2022 Section 2 summarises the peculiarities and advantages of the SP system: \u2022 Simplification and integration of observations and concepts; \u2022 Simplification and integration of structures and processes in computer systems; \u2022 SP theory is itself a theory of computer science; \u2022 Theory plays a central role in information compression through matching and unification of patterns; \u2022 More precisely, all processing takes place through a concept of multiple alignment, borrowed and adapted to bioinformatics. This is perhaps the most characteristic feature of theory; \u2022 Transparency in the representation and processing of knowledge; \u2022 Discovering \"natural\" structures through information compression (DONSVIC); \u2022 Interpretation of aspects of mathematics in relation to SP theory; \u2022 Interpretation of phenomena in relation to human perception and cognition systems; \u2022 Interpretation of abstract concepts in relation to real systems; \u2022"}, {"heading": "A Outline of the SP theory", "text": "In fact, it is such that most of them will be able to move into another world, in which they are able to live, in which they are able to move, in which they move, in which they move, in which they move, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they, in which they live, in which they, in which they live, in which they, in which they live, in which they, in which they live, in which they, in which they live, in which they live."}, {"heading": "B Occam\u2019s Razor: simplicity and power", "text": "It is one of the most widespread principles in science - Occam's Razor - that a good theory should combine the simplicity of the concept with an explanatory or descriptive force. Albert Einstein put it this way: \"A theory is the more impressive, the stronger the simplicity of its premise, the stronger its scope.\" In this sense, Alan Turing developed the concept of the \"Universal Turing Machine,\" in the form of models such as the \"Post,\" which are undoubtedly good models of \"Computing.\" But despite the fact that computers could become more intelligent, the concept of a universal Turing Machine is unable to tell us how several decades of research have produced some useful insights and some impressive applications, but I think it is fair that we are suffering from an excess. \""}], "references": [], "referenceMentions": [], "year": 2017, "abstractText": "The main aim of this paper is to highlight distinctive features of the SP theory of intelligence and its apparent advantages compared with some AI-related alternatives. The theory is outlined in an appendix, with pointers to where fuller information may be found. The distinctive features and advantages are summarised: simplification and integration of observations and concepts; simplification and integration of structures and processes in computing systems; the SP theory is itself a theory of computing; a central role for information compression via the matching and unification of patterns, and for multiple alignment; transparency in the representation and processing of knowledge; the discovery of \u2018natural\u2019 structures via information compression (DONSVIC); interpretation of aspects of mathematics; interpretation of phenomena in human perception and cognition; realisation of abstract concepts in terms of neurons and their inter-connections (SP-neural). Distinctive features and advantages of the SP system are highlighted in comparison with alternatives: the concept of minimum length encoding and related concepts; deep learning in neural networks; concepts of universal search; Bayesian networks and other models for AI; the analysis and production of natural language; the learning of natural language; exact and inexact forms of reasoning; representation and processing of diverse forms of knowledge; IBM\u2019s Watson; solving problems associated with big data, and in the development of intelligence in autonomous robots. The main conclusion of the paper is that, while some alternatives to the SP system may \u2217Dr Gerry Wolff, BA (Cantab), PhD (Wales), CEng, MBCS (CITP); CognitionResearch.org, Menai Bridge, UK; jgw@cognitionresearch.org; +44 (0) 1248 712962; +44 (0) 7746 290775; Skype: gerry.wolff; Web: www.cognitionresearch.org. 1 ar X iv :1 50 8. 04 08 7v 1 [ cs .A I] 1 7 A ug 2 01 5 deliver attractive short-term benefits, a major strength of SP system is that it can provide a firm foundation for the long-term development of many aspects of AI and, at the same time, it may deliver some benefits and applications on relatively short timescales. It is envisaged that a high-parallel, open-source version of the SP machine will be created, hosted on an existing high-performance computer and derived from the existing SP computer model. This will be a means for researchers everywhere to explore what can be done with the system, and to create new versions of it.", "creator": "LaTeX with hyperref package"}}}