{"id": "1508.04525", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "19-Aug-2015", "title": "Recognizing Extended Spatiotemporal Expressions by Actively Trained Average Perceptron Ensembles", "abstract": "Precise geocoding and time normalization for text requires that location and time phrases be identified. Many state-of-the-art geoparsers and temporal parsers suffer from low recall. Categories commonly missed by parsers are: nouns used in a non- spatiotemporal sense, adjectival and adverbial phrases, prepositional phrases, and numerical phrases. We collected and annotated data set by querying commercial web searches API with such spatiotemporal expressions as were missed by state-of-the- art parsers. Due to the high cost of sentence annotation, active learning was used to label training data, and a new strategy was designed to better select training examples to reduce labeling cost. For the learning algorithm, we applied an average perceptron trained Featurized Hidden Markov Model (FHMM). Five FHMM instances were used to create an ensemble, with the output phrase selected by voting. Our ensemble model was tested on a range of sequential labeling tasks, and has shown competitive performance. Our contributions include (1) an new dataset annotated with named entities and expanded spatiotemporal expressions; (2) a comparison of inference algorithms for ensemble models showing the superior accuracy of Belief Propagation over Viterbi Decoding; (3) a new example re-weighting method for active ensemble learning that 'memorizes' the latest examples trained; (4) a spatiotemporal parser that jointly recognizes expanded spatiotemporal expressions as well as named entities.", "histories": [["v1", "Wed, 19 Aug 2015 04:17:47 GMT  (5649kb)", "http://arxiv.org/abs/1508.04525v1", "10 pages"]], "COMMENTS": "10 pages", "reviews": [], "SUBJECTS": "cs.CL cs.LG", "authors": ["wei zhang", "yang yu", "osho gupta", "judith gelernter"], "accepted": false, "id": "1508.04525"}, "pdf": {"name": "1508.04525.pdf", "metadata": {"source": "CRF", "title": "Recognizing Extended Spatiotemporal Expressions by Actively Trained Average Perceptron Ensembles", "authors": ["Wei Zhang", "Yang Yu", "Judith Gelernter", "Osho Gupta"], "emails": ["wynnzh@gmail.com", "yu@us.ibm.com", "gelern@cs.cmu.edu", "osho.gupta.ece11@iitbhu.ac.in"], "sections": [{"heading": null, "text": "Our contributions include (1) a new dataset commented on with named entities and extended spatio-temporal expressions; (2) a comparison of inference algorithms for ensemble models that demonstrate the superior accuracy of faith propagation over viterbi decoding; (3) a new example of a method for reweighting active ensemble learning that \"memorizes\" the latest trained examples; (4) a spatio-time parser that recognizes extended spatio-temporal expressions and named entities together. 1Categories and subject descriptions D.3.3 [Artificial intelligence]: Natural Language Processing - Language Analysis and Language Understanding, Text AnalysisGeneral Term Algorithms Keywords temporal parsing, geoparsing, Entity recognition, perceptron algorithm, ensemble learning, active learning, queryby-bagging1 Source at https: / giubb.com / Learn.com / MHtratio1"}, {"heading": "1. INTRODUCTION", "text": "This year, it has reached the stage where it will be able to put itself at the forefront in order to embark on the path to the future."}, {"heading": "2. RELATED WORK", "text": "We join the parsing of time and places in a model (an FHMM ensemble) because we observed linguistic parallels between spatial and temporal expressions, as in Table 2. We believe that this could help distinguish meanings of words that could be used just as frequently in a temporal sense as in a spatial sense. [What does a parser typically find and why?] Geoparsers find terms - especially place terms (or toponyms). However, temporary parsers find months, dates, and holidays. Phrases (which include valuable modifiers of time and places) are not always included, if at all. The PETAR system ([18], which works on tweets, is one of the first to change."}, {"heading": "3. Web DATA for spatiotemporal corpus (EST)", "text": "Dre rf\u00fc nde nlrrfEe\u00fceegnln rf\u00fc ide rf\u00fc ide nlrfEe\u00fceegnln rf\u00fc ide nlrfEe\u00fceegnrrteeeee\u00fceegnln rf\u00fc ide nlrrf\u00fc rf\u00fc ide nlrf\u00fc ide nlrrrrrrrrrrrrrrrrreeeeeerln rf\u00fc rf\u00fc eid nlrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr"}, {"heading": "4. THE AVERAGE PERCEPTRON TRAINING OF FHMM", "text": "In fact, it is such that most of them will be able to survive themselves without there being a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there"}, {"heading": "5. BAG OF AVERAGE PERCEPTRONS \u2013 ENSEMBLE FHMM", "text": "This year, it has come to the point that it has never come as far as it has this year."}, {"heading": "6. ACTIVE LEARNING FOR FHMM ENSEMBLES", "text": "rf\u00fc eid rf\u00fc eid rf\u00fc eid rf\u00fc rf\u00fc eid rf\u00fc nlrf\u00fc nlrf\u00fc rrf\u00fc rf\u00fc rf\u00fc rf\u00fc eid rf\u00fc rf\u00fc eerdteeSrteeeirsrrsrrteeVnlrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrsrsrrrrsrrrrrsrrrsrrrsrrrsrsrrrsrrsrrrsrsrsrsrsrsrsrsrsrsrsrsrsrsrsrsrsrrsrsrsrsrsrsrsrsrsrsrsrsrsrsrsrsrsrsrsrsrsrsrsrsrsrsrsrsrsrsrsrsrsrsrsrsrsrsrsrsrsrsrsrsrsrsrsrsrsrsrsrsrsrsrsrsrsrsrsrsrsrsrsrsrsrsrsrsrsrsrsrsrsrsrsrsrsrsrsrsrsrsrsrsrsrsrsrsrsrsrsrsrsrsrsrsrsrsrsrsrsrsrsrsrsrsrsrsrsrsrsrsrsrsrsrsrsrsrsrsrsrsrsrsrsrsrsrsrsrsrsrsrsrsrsrsrsrsrsrsrsrrsrrsrsrsrrrsrsrrrrrsrrrrrrsrrrrrrrrrrrrrrrrrrrrr"}, {"heading": "7. EXPERIMENTS", "text": "(1) We compare the FHMM model with others based on similar sequence marker data sets - a necessity since our model is the only one used to handle EST data. (2) We look at the active learning process and (3) the degree to which the inference algorithm affects the ensemble FHMM."}, {"heading": "7.1 Corpora to compare models", "text": "We used the CoNLL 2000 chunking dataset, the NLPBA dataset for Biomedical NER, and OntoNotes version 5.0 - the 5 news subgroups it contains, ABC, MNB, NBC, PRI, and VOA. We skipped the CoNLL 2003 NER dataset because we were unable to purchase the dataset at the time of publication. We would be setting out for our future work. We can use some of it, but it seems unnecessary. PRI and VOA are two datasets that resemble EST datasets in size. ABC, MNB, and NBC are three small NER datasets that simulate early stages of active learning. NLPBA is used for testing biomedical NER, and CoNLL chunking is used for testing a similar task."}, {"heading": "7.2 Perceptron FHMM vs. previous work", "text": "This year it is so far that it only takes a few days until it is so far, until it is so far."}, {"heading": "7.3 Evaluation of spatiotemporal parsing", "text": "In Table 10, we used 1700 training sets of the 5-model ensemble at a sampling rate of 0.8. In preliminary experiments, we had created 79 spatio-temporal sets to determine what types of time expressions found temporal parsers. These 79 sets do not contain named entity tags, which can be extremely helpful for spatio-temporal expressions outside the entity, since entities with spatio-temporal expressions occur frequently outside the entity. So, we created an FHMM ensemble model based on the EST dataset with all tags except T and G, namely FHMM-G, to train a universal NER system by location, date, organization, person, etc. Then, FHMM-G is applied to features relevant to the corpus (as shown in Table 4).Our model performed remarkably well in terms of precision (89.27) and time parameters (88.17)."}, {"heading": "7.4 Generalizability and future research", "text": "The algorithm is limited in that it was trained on specific seeds. Space-time expressions that are not included in the data set are still not found, nor numerical sequences with numbers that are different from those in the training data. We could expand the detected expressions by using templates for characteristics, for example, we could find \"the x week of y\" and not just \"the second week of April.\" Nevertheless, larger amounts of data would need to be commented on manually, for example by crowdsourcing to Mechanical Turk, so that the patterns would be tagged. Alternative training algorithms that could be tested include neural networks that have shown excellent results in sequential labeling tasks, especially the short-time memory network [12]. The method is particularly suitable because the sequences we would detect are longer than the standard Named Entity Recognition tasks.Table 9. Comparison of two decoding algorithms on ontoes-NER data set."}, {"heading": "8. CONCLUSION", "text": "We have identified the spatio-temporal expressions overlooked by state-of-the-art time and location parsers, and have developed a tool to identify such expressions; the retrieval of our spatio-temporal parser is high and does not sacrifice precision, demonstrating the strength of our solution. To create our spatio-temporal parser, we have collected and commented on a dataset full of such expressions to train the model; we have used active learning to reduce the required manual annotations, and selected only the most effective examples of training; we have used an average perceptual FHMM as a base model to detect such expressions, and we have used ensemble learning to create a bag of FHMM to reduce the model variance. Our dataset is provided with both designated units and undesignated entities that are spatio-temporal expressions available to the research community; our approach can be widely applied in the sequencing of tasks."}, {"heading": "9. REFERENCES", "text": "[1] Breiman, L.. (1996). Bagging Predictors, Machine Learning, 24 (2): 123-140. [2] Chang, A.X., Manning, C. D. (2012). SUTime: A library for recognizing and normalizing time expressions. LREC. 2012. [3] Collins, M. (2002). Discriminative training methods for Hidden Markov Models: Theory and experiments with perceptron algorithms. Proceedings of the ACL-02 conference on Empirical methods in natural language processing-Volume 10, 1-8. [4] Culotta, A. and McCallum, A. (2005). Reducing labeling effort for structured prediction tasks. AAAAI, vol 2 746-751. [5] Daum\u00e9 III, H., Langford, J., and Marcu, D. (2009)."}], "references": [{"title": "SUTime: A library for recognizing and normalizing time expressions", "author": ["A.X. Chang", "C.D. Manning"], "venue": null, "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2012}, {"title": "Discriminative training methods for Hidden Markov Models: Theory and experiments with perceptron algorithms", "author": ["M. Collins"], "venue": "Proceedings of the ACL-02 conference on Empirical methods in natural language processing-Volume", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2002}, {"title": "Reducing labeling effort for structured prediction", "author": ["A. Culotta", "A. McCallum"], "venue": "tasks. AAAI,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2005}, {"title": "Searchbased structured prediction", "author": ["H. Daum\u00e9 III", "J. Langford", "D. Marcu"], "venue": "Machine learning,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2009}, {"title": "An experimental comparison of three methods for constructing ensembles of decision trees: Bagging, boosting, and randomization", "author": ["T.G. Dietterich"], "venue": "Machine learning,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2000}, {"title": "Regularization networks and support vector machines. Advances in computational mathematics", "author": ["T. Evgeniou", "M. Pontil", "T. Poggio"], "venue": null, "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2000}, {"title": "Joint parsing and named entity recognition", "author": ["J. Finkel", "C. Manning"], "venue": "Proceedings of Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the Association for Computational Linguistics,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2009}, {"title": "The crucial role of semantic discovery and markup in geo-temporal search", "author": ["F.C. Gey", "N. Kando", "R.R. Larson"], "venue": "ESAIR \u201910,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2010}, {"title": "A Viterbi algorithm with soft-decision outputs and its applications", "author": ["J. Hagenauer", "P. Hoeher"], "venue": "In Global Telecommunications Conference and Exhibition  Communications Technology for the 1990s and Beyond'(GLOBECOM),", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 1989}, {"title": "Interpretation and normalization of temporal expressions for question answering. Evaluation of Multilingual and Multi-modal Information Retrieval", "author": ["S. Hartrumpf", "J. Leveling"], "venue": null, "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2007}, {"title": "Long short-term memory", "author": ["S. Hochreiter", "J. Schmidhuber"], "venue": "Neural computation", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 1997}, {"title": "Entropy-based active learning for object recognition", "author": ["A. Holub", "P. Perona", "M.C. Burl"], "venue": "IEEE Computer Society Conference on Computer Vision and Pattern Recognition Workshops,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2008}, {"title": "A study of cross-validation and bootstrap for accuracy estimation and model selection", "author": ["R. Kohavi"], "venue": "Proceedings of the International Joint Conference on Artificial Intelligence IJCAI", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 1995}, {"title": "Factor graphs and the sum-product algorithm", "author": ["F.R. Kschischang", "B.J. Frey", "Loeliger", "H-A"], "venue": "IEEE Transactions on Information Theory", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2001}, {"title": "Conditional random fields: Probabilistic models for segmenting and labeling sequence", "author": ["Lafferty", "John", "Andrew McCallum", "Fernando CN Pereira"], "venue": null, "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2001}, {"title": "Bridging Viterbi and posterior decoding: a generalized risk approach to hidden path inference based on hidden Markov models", "author": ["J. Lember", "A.A. Koloydenko"], "venue": "The Journal of Machine Learning Research", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2014}, {"title": "Fine-grained location extraction from tweets with temporal awareness. SIGIR\u201914", "author": ["C. Li", "A. Sun"], "venue": "July 6\u201311,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2014}, {"title": "Active Learning Through Adaptive Heterogeneous Ensembling", "author": ["Z. Lu", "X. Wu", "J.C. Bongard"], "venue": "Knowledge and Data Engineering, IEEE Transactions on 27(2):", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2015}, {"title": "Maximum Entropy Markov Models for Information Extraction and Segmentation", "author": ["A. McCallum", "D. Freitag", "Pereira", "F. C", "June"], "venue": "In ICML (Vol", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2000}, {"title": "An experimental comparison of active learning strategies for partially labeled sequences", "author": ["D. Marcheggiani", "T. Artieres"], "venue": "Doha Quatar,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2014}, {"title": "Diverse ensembles for active learning", "author": ["P. Melville", "Mooney"], "venue": "Proceedings of the Twenty-first International Conference on Machine Learning", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2004}, {"title": "Creating diversity in ensembles using artificial data", "author": ["P. Melville", "R.J. Mooney"], "venue": "Information Fusion", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2005}, {"title": "Loopy belief propagation for approximate inference: An empirical study", "author": ["K.P. Murphy", "Y. Weiss", "M.I. Jordan"], "venue": "Proceedings of the Fifteenth Conference on Uncertainty in Artificial Intelligence", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 1999}, {"title": "Ensemble learning with active example selection for imbalanced biomedical data classification", "author": ["S. Oh", "M.S. Lee", "Zhang", "B-T"], "venue": "IEEE/ACM Transactions on Computational Biology and Bioinformatics (TCBB)", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2011}, {"title": "Reverend Bayes on inference engines: A distributed hierarchical approach", "author": ["J. Pearl"], "venue": null, "citeRegEx": "26", "shortCiteRegEx": "26", "year": 1982}, {"title": "Automatic early stopping using cross validation: quantifying the criteria", "author": ["L. Prechelt"], "venue": "Neural Networks", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 1998}, {"title": "An introduction to hidden Markov models", "author": ["L.R. Rabiner", "Juang", "B-H"], "venue": "ASSP Magazine,", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 1986}, {"title": "Design challenges and misconceptions in Named Entity Recognition", "author": ["L. Ratinov", "D. Roth"], "venue": null, "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2009}, {"title": "The capacity of low-density parity-check codes under message-passing decoding", "author": ["T.J. Richardson", "R.L. Urbanke"], "venue": "IEEE Transactions on Information Theory", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2001}, {"title": "GikiP at GeoCLEF 2008: Joining GIR and QA forces for querying Wikipedia", "author": ["D. Santos", "N. Cardoso", "Carvalho", "I.P. Dornescu", "Hartfrumpf", "J.S. Leveling", "Y. Skalban"], "venue": "In C. Peters et al. (Eds). Evaluating systems for multilingual and multimodal information access,", "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2009}, {"title": "From temporal expressions to temporal information: Semantic tagging of news messages", "author": ["F. Schilder", "C. Habel"], "venue": "Proceedings of the workshop on Temporal and Spatial Information Processing-Volume 13. Association for Computational Linguistics,", "citeRegEx": "32", "shortCiteRegEx": "32", "year": 2001}, {"title": "Active learning of ensemble classifiers for gesture recognition", "author": ["J Schumacher"], "venue": null, "citeRegEx": "33", "shortCiteRegEx": "33", "year": 2012}, {"title": "Shallow parsing with conditional random fields", "author": ["F. Sha", "F. Pereira"], "venue": "Proceedings of the 2003 Conference of the North American Chapter of the Association for Computational Linguistics on Human Language Technology-Volume", "citeRegEx": "34", "shortCiteRegEx": "34", "year": 2003}, {"title": "Biomedical named entity recognition using conditional random fields and rich feature sets", "author": ["B. Settles"], "venue": "Proceedings of the International Joint Workshop on Natural Language Processing in Biomedicine and its Applications. Association for Computational Linguistics,", "citeRegEx": "35", "shortCiteRegEx": "35", "year": 2004}, {"title": "An analysis of active learning strategies for sequence labeling tasks. Proceedings of the conference on empirical methods in natural language processing", "author": ["B. Settles", "M Craven"], "venue": "Association for Computational Linguistics,", "citeRegEx": "36", "shortCiteRegEx": "36", "year": 2008}, {"title": "Voting between multiple data representations for text chunking", "author": ["H. Shen", "A. Sarkar"], "venue": "B. Ke\u0301gl and G. Lapalme (Eds). AI 2005,", "citeRegEx": "37", "shortCiteRegEx": "37", "year": 2005}, {"title": "Modeling latent-dynamic in shallow parsing: a latent conditional model with improved inference", "author": ["X. Sun", "Morency", "L-P", "D. Okanohara", "J. Tsujii"], "venue": "Proceedings of the 22nd International Conference on Computational Linguistics-Volume", "citeRegEx": "38", "shortCiteRegEx": "38", "year": 2008}, {"title": "Max-margin Markov networks.\" Advances in neural information processing systems", "author": ["Taskar", "Ben. Guestrin", "Carlos. Koller", "Daphne"], "venue": null, "citeRegEx": "39", "shortCiteRegEx": "39", "year": 2004}, {"title": "Regression shrinkage and selection via the lasso", "author": ["R. Tibshirani"], "venue": "Journal of the Royal Statistical Society. Series B (Methodological)", "citeRegEx": "40", "shortCiteRegEx": "40", "year": 1996}, {"title": "Named entity recognition: Exploring features", "author": ["M. Tkachenko", "A. Simanovsky"], "venue": "Proceedings of KONVENS,", "citeRegEx": "41", "shortCiteRegEx": "41", "year": 2012}, {"title": "Large margin methods for structured and interdependent output variables", "author": ["I. Tsochantaridis", "T. Joachims", "T. Hofmann", "Y. Altun"], "venue": "In Journal of Machine Learning Research (pp. 1453-1484)", "citeRegEx": "42", "shortCiteRegEx": "42", "year": 2005}, {"title": "Temporal semantics extraction for improving web search", "author": ["Vicente-Diez", "M.t", "P. Martinez"], "venue": "Database and Expert Systems Application,", "citeRegEx": "43", "shortCiteRegEx": "43", "year": 2009}, {"title": "Extracting spatial relations from document for geographic information retrieval", "author": ["Y. Yuan"], "venue": "International Conference on Geoinformatics,", "citeRegEx": "44", "shortCiteRegEx": "44", "year": 2011}, {"title": "Geocoding location expressions in Twitter messages: A preference learning method", "author": ["W. Zhang", "J. Gelernter"], "venue": "Journal of Spatial Information Science", "citeRegEx": "45", "shortCiteRegEx": "45", "year": 2015}, {"title": "Named entity recognition using an HMM-based chunk tagger", "author": ["G. Zhou", "J. Su"], "venue": "Proceedings of the 40th Annual Meeting on Association for Computational Linguistics. Association for Computational Linguistics,", "citeRegEx": "46", "shortCiteRegEx": "46", "year": 2002}, {"title": "Biomedical named entity recognition based on classifiers ensemble", "author": ["Haochang Wang", "Tiejun Zhao", "Hongye Tan", "Shu Zhang"], "venue": "IJCSA,", "citeRegEx": "47", "shortCiteRegEx": "47", "year": 2008}, {"title": "Biomedical named entity recognition based on classifiers ensemble", "author": ["Haochang Wang", "Tiejun Zhao", "Hongye Tan", "Shu Zhang"], "venue": "IJCSA,", "citeRegEx": "48", "shortCiteRegEx": "48", "year": 2008}, {"title": "Exploiting context for biomedical entity recognition: from syntax to the web", "author": ["J. Finkel", "S. Dingare", "H. Nguyen", "M. Nissim", "C. Manning", "G. Sinclair"], "venue": "In Proceedings of the International Joint Workshop on Natural Language Processing in Biomedicine and its Applications (pp. 88-91)", "citeRegEx": "49", "shortCiteRegEx": "49", "year": 2004}], "referenceMentions": [{"referenceID": 43, "context": "We consider the problem of recognizing unnamed location and time phrases to ease downstream applications such as geocoding [45] and time normalization [2].", "startOffset": 123, "endOffset": 127}, {"referenceID": 0, "context": "We consider the problem of recognizing unnamed location and time phrases to ease downstream applications such as geocoding [45] and time normalization [2].", "startOffset": 151, "endOffset": 154}, {"referenceID": 9, "context": "A wider range of expressions parsed could benefit question and answering [11][9][31], web querying toward the semantic web [43], information retrieval as from news messages [32].", "startOffset": 73, "endOffset": 77}, {"referenceID": 7, "context": "A wider range of expressions parsed could benefit question and answering [11][9][31], web querying toward the semantic web [43], information retrieval as from news messages [32].", "startOffset": 77, "endOffset": 80}, {"referenceID": 29, "context": "A wider range of expressions parsed could benefit question and answering [11][9][31], web querying toward the semantic web [43], information retrieval as from news messages [32].", "startOffset": 80, "endOffset": 84}, {"referenceID": 41, "context": "A wider range of expressions parsed could benefit question and answering [11][9][31], web querying toward the semantic web [43], information retrieval as from news messages [32].", "startOffset": 123, "endOffset": 127}, {"referenceID": 30, "context": "A wider range of expressions parsed could benefit question and answering [11][9][31], web querying toward the semantic web [43], information retrieval as from news messages [32].", "startOffset": 173, "endOffset": 177}, {"referenceID": 26, "context": "Labeling words in sentences automatically is done sequence labeling models such as Hidden Markov Models (HMM) [28] [46] and Conditional Random Fields (CRF) [16], Average Perceptron [3], Max-Margin Markov Networks [39], SVM [42], SEARN algorithm [5], Max Entropy Markov Models [20] and so on.", "startOffset": 110, "endOffset": 114}, {"referenceID": 44, "context": "Labeling words in sentences automatically is done sequence labeling models such as Hidden Markov Models (HMM) [28] [46] and Conditional Random Fields (CRF) [16], Average Perceptron [3], Max-Margin Markov Networks [39], SVM [42], SEARN algorithm [5], Max Entropy Markov Models [20] and so on.", "startOffset": 115, "endOffset": 119}, {"referenceID": 14, "context": "Labeling words in sentences automatically is done sequence labeling models such as Hidden Markov Models (HMM) [28] [46] and Conditional Random Fields (CRF) [16], Average Perceptron [3], Max-Margin Markov Networks [39], SVM [42], SEARN algorithm [5], Max Entropy Markov Models [20] and so on.", "startOffset": 156, "endOffset": 160}, {"referenceID": 1, "context": "Labeling words in sentences automatically is done sequence labeling models such as Hidden Markov Models (HMM) [28] [46] and Conditional Random Fields (CRF) [16], Average Perceptron [3], Max-Margin Markov Networks [39], SVM [42], SEARN algorithm [5], Max Entropy Markov Models [20] and so on.", "startOffset": 181, "endOffset": 184}, {"referenceID": 37, "context": "Labeling words in sentences automatically is done sequence labeling models such as Hidden Markov Models (HMM) [28] [46] and Conditional Random Fields (CRF) [16], Average Perceptron [3], Max-Margin Markov Networks [39], SVM [42], SEARN algorithm [5], Max Entropy Markov Models [20] and so on.", "startOffset": 213, "endOffset": 217}, {"referenceID": 40, "context": "Labeling words in sentences automatically is done sequence labeling models such as Hidden Markov Models (HMM) [28] [46] and Conditional Random Fields (CRF) [16], Average Perceptron [3], Max-Margin Markov Networks [39], SVM [42], SEARN algorithm [5], Max Entropy Markov Models [20] and so on.", "startOffset": 223, "endOffset": 227}, {"referenceID": 3, "context": "Labeling words in sentences automatically is done sequence labeling models such as Hidden Markov Models (HMM) [28] [46] and Conditional Random Fields (CRF) [16], Average Perceptron [3], Max-Margin Markov Networks [39], SVM [42], SEARN algorithm [5], Max Entropy Markov Models [20] and so on.", "startOffset": 245, "endOffset": 248}, {"referenceID": 18, "context": "Labeling words in sentences automatically is done sequence labeling models such as Hidden Markov Models (HMM) [28] [46] and Conditional Random Fields (CRF) [16], Average Perceptron [3], Max-Margin Markov Networks [39], SVM [42], SEARN algorithm [5], Max Entropy Markov Models [20] and so on.", "startOffset": 276, "endOffset": 280}, {"referenceID": 4, "context": "To further reduce overfitting and prediction variance, we used FHMM ensembles, specifically, bagging [6] of FHMM, with the correct result being found by voting among results of the individual models.", "startOffset": 101, "endOffset": 104}, {"referenceID": 16, "context": "The PETAR system ([18] that works on tweets is among the first to change this.", "startOffset": 18, "endOffset": 22}, {"referenceID": 43, "context": "For geoparsing (method to find location entities) results, see [45].", "startOffset": 63, "endOffset": 67}, {"referenceID": 42, "context": "Geocoding research that incorporates natural language understanding of adjectival or prepositional phrases has been termed \u201cspatial relations\u201d [44].", "startOffset": 143, "endOffset": 147}, {"referenceID": 20, "context": "[22] and [23] studied ensemble classifiers for continuous values, [33] for gesture recognition, [25] for biomedical data, and [13] on object recognition for computer vision.", "startOffset": 0, "endOffset": 4}, {"referenceID": 21, "context": "[22] and [23] studied ensemble classifiers for continuous values, [33] for gesture recognition, [25] for biomedical data, and [13] on object recognition for computer vision.", "startOffset": 9, "endOffset": 13}, {"referenceID": 31, "context": "[22] and [23] studied ensemble classifiers for continuous values, [33] for gesture recognition, [25] for biomedical data, and [13] on object recognition for computer vision.", "startOffset": 66, "endOffset": 70}, {"referenceID": 23, "context": "[22] and [23] studied ensemble classifiers for continuous values, [33] for gesture recognition, [25] for biomedical data, and [13] on object recognition for computer vision.", "startOffset": 96, "endOffset": 100}, {"referenceID": 11, "context": "[22] and [23] studied ensemble classifiers for continuous values, [33] for gesture recognition, [25] for biomedical data, and [13] on object recognition for computer vision.", "startOffset": 126, "endOffset": 130}, {"referenceID": 17, "context": "[19] designed ensembles that are heterogeneous rather than homogeneous.", "startOffset": 0, "endOffset": 4}, {"referenceID": 20, "context": "Active learning for ensembles is little studied beyond [22] and [23].", "startOffset": 55, "endOffset": 59}, {"referenceID": 21, "context": "Active learning for ensembles is little studied beyond [22] and [23].", "startOffset": 64, "endOffset": 68}, {"referenceID": 2, "context": "However, active learning for sequence labeling models has been investigated extensively [4] [36] [21].", "startOffset": 88, "endOffset": 91}, {"referenceID": 34, "context": "However, active learning for sequence labeling models has been investigated extensively [4] [36] [21].", "startOffset": 92, "endOffset": 96}, {"referenceID": 19, "context": "However, active learning for sequence labeling models has been investigated extensively [4] [36] [21].", "startOffset": 97, "endOffset": 101}, {"referenceID": 0, "context": "494 SUTime [2] 0.", "startOffset": 11, "endOffset": 14}, {"referenceID": 1, "context": "The model is trained with the average perceptron algorithm [3], which resembles a single linear perceptron in neural networks.", "startOffset": 59, "endOffset": 62}, {"referenceID": 1, "context": "See [3] for details.", "startOffset": 4, "endOffset": 7}, {"referenceID": 0, "context": "In Table 4, the features defined in context window [-2,2] are the \u201cglobal features\u201d that increase non-linearity.", "startOffset": 51, "endOffset": 57}, {"referenceID": 25, "context": "Several general techniques can be used to reduce overfitting in optimization, such as early stopping [27], L2 regularization [7], Lasso regression [40], cross validation [14], and feature selection.", "startOffset": 101, "endOffset": 105}, {"referenceID": 5, "context": "Several general techniques can be used to reduce overfitting in optimization, such as early stopping [27], L2 regularization [7], Lasso regression [40], cross validation [14], and feature selection.", "startOffset": 125, "endOffset": 128}, {"referenceID": 38, "context": "Several general techniques can be used to reduce overfitting in optimization, such as early stopping [27], L2 regularization [7], Lasso regression [40], cross validation [14], and feature selection.", "startOffset": 147, "endOffset": 151}, {"referenceID": 12, "context": "Several general techniques can be used to reduce overfitting in optimization, such as early stopping [27], L2 regularization [7], Lasso regression [40], cross validation [14], and feature selection.", "startOffset": 170, "endOffset": 174}, {"referenceID": 1, "context": "But in a perceptron algorithm, parameter averaging is a simple but effective technique used to control overfitting [3].", "startOffset": 115, "endOffset": 118}, {"referenceID": 8, "context": "We compared two inference (decoding) algorithms: Nbest Viterbi decoding (or Viterbi decoding for short) [10], and the Belief Propagation (BP) algorithm, or the sum-product message passing algorithm [26], [30].", "startOffset": 104, "endOffset": 108}, {"referenceID": 24, "context": "We compared two inference (decoding) algorithms: Nbest Viterbi decoding (or Viterbi decoding for short) [10], and the Belief Propagation (BP) algorithm, or the sum-product message passing algorithm [26], [30].", "startOffset": 198, "endOffset": 202}, {"referenceID": 28, "context": "We compared two inference (decoding) algorithms: Nbest Viterbi decoding (or Viterbi decoding for short) [10], and the Belief Propagation (BP) algorithm, or the sum-product message passing algorithm [26], [30].", "startOffset": 204, "endOffset": 208}, {"referenceID": 13, "context": "The BP algorithm is widely used in graphical models for exact inference in tree graphs [15] and for approximate inference over loopy graphs [24].", "startOffset": 87, "endOffset": 91}, {"referenceID": 22, "context": "The BP algorithm is widely used in graphical models for exact inference in tree graphs [15] and for approximate inference over loopy graphs [24].", "startOffset": 140, "endOffset": 144}, {"referenceID": 15, "context": "Recent work tries to combine the two to achieve the power of both [17].", "startOffset": 66, "endOffset": 70}, {"referenceID": 26, "context": "Viterbi decoding generates the probability for each decoding sequence, whereas BP uses forward-backward algorithm to predict each token individually [28].", "startOffset": 149, "endOffset": 153}, {"referenceID": 0, "context": "[-2,2] in first column means the feature is extracted within token window of 5 for each word.", "startOffset": 0, "endOffset": 6}, {"referenceID": 0, "context": "Lemma in window [-2,2] y y y y", "startOffset": 16, "endOffset": 22}, {"referenceID": 0, "context": "Word form [-2,2] y y", "startOffset": 10, "endOffset": 16}, {"referenceID": 0, "context": "Part-of-speech [-2,2] y", "startOffset": 15, "endOffset": 21}, {"referenceID": 0, "context": "Word suffix [-2,2] y y y", "startOffset": 12, "endOffset": 18}, {"referenceID": 0, "context": "Named entity tag [-2,2] y", "startOffset": 17, "endOffset": 23}, {"referenceID": 0, "context": "Part-of-speech bigram [-2,2] y", "startOffset": 22, "endOffset": 28}, {"referenceID": 0, "context": "Suffix bigram [-2,2] y", "startOffset": 14, "endOffset": 20}, {"referenceID": 0, "context": "Word form bigram [-2,2] y y y", "startOffset": 17, "endOffset": 23}, {"referenceID": 20, "context": "Active learning for classifier ensembles has been studied on classification problems [22] and [23].", "startOffset": 85, "endOffset": 89}, {"referenceID": 21, "context": "Active learning for classifier ensembles has been studied on classification problems [22] and [23].", "startOffset": 94, "endOffset": 98}, {"referenceID": 2, "context": "Active learning for sequence labeling models was extensively investigated in [4] [36] [21].", "startOffset": 77, "endOffset": 80}, {"referenceID": 34, "context": "Active learning for sequence labeling models was extensively investigated in [4] [36] [21].", "startOffset": 81, "endOffset": 85}, {"referenceID": 19, "context": "Active learning for sequence labeling models was extensively investigated in [4] [36] [21].", "startOffset": 86, "endOffset": 90}, {"referenceID": 20, "context": "We introduce the general architecture of a typical active learning routine for ensembles [22] and [23] before we discuss details of active learning.", "startOffset": 89, "endOffset": 93}, {"referenceID": 21, "context": "We introduce the general architecture of a typical active learning routine for ensembles [22] and [23] before we discuss details of active learning.", "startOffset": 98, "endOffset": 102}, {"referenceID": 34, "context": "In this work, we configured algorithm 2 by using (1) a sequence labeling utility function Sequence Vote Entropy [36] for Utility(), and (2) a new Sentence Re-weighting method for creating ensembles from T, which corresponds to learn_Ensemble (Base, T) shown in Algorithm 3.", "startOffset": 112, "endOffset": 116}, {"referenceID": 10, "context": "This is analogous to short-term memory mechanism used in Long-Short Term Memory (LSTM) neural networks [12].", "startOffset": 103, "endOffset": 107}, {"referenceID": 35, "context": "We use the F1 measure from our single FHMM model to be consistent with others\u2019 published results on single models (even though [37] uses an ensemble HMM and voting to achieve 95.", "startOffset": 127, "endOffset": 131}, {"referenceID": 6, "context": "Train test split of those follows [8] to be comparable.", "startOffset": 34, "endOffset": 37}, {"referenceID": 6, "context": "Our score is higher than that of Finkel and Manning\u2019s [8] basic NER model which uses features similar to those that we use.", "startOffset": 54, "endOffset": 57}, {"referenceID": 27, "context": "[29] used Average Perceptron training along with rich semantic features, as do [41].", "startOffset": 0, "endOffset": 4}, {"referenceID": 39, "context": "[29] used Average Perceptron training along with rich semantic features, as do [41].", "startOffset": 79, "endOffset": 83}, {"referenceID": 47, "context": "Still, it is the Wang et al [49] ensemble method that outperforms other single-model methods to a large extent.", "startOffset": 28, "endOffset": 32}, {"referenceID": 6, "context": "Among the results that do not use extended outside knowledge but explore word forms and context only, FHMM is shown to be competitive with [8] and [35].", "startOffset": 139, "endOffset": 142}, {"referenceID": 33, "context": "Among the results that do not use extended outside knowledge but explore word forms and context only, FHMM is shown to be competitive with [8] and [35].", "startOffset": 147, "endOffset": 151}, {"referenceID": 46, "context": "Ensemble of Sequence Labeling algorithms [48] 77.", "startOffset": 41, "endOffset": 45}, {"referenceID": 39, "context": "6 Average Perceptron with knowledge [41] 74.", "startOffset": 36, "endOffset": 40}, {"referenceID": 47, "context": "[49] 70.", "startOffset": 0, "endOffset": 4}, {"referenceID": 33, "context": "[35] 69.", "startOffset": 0, "endOffset": 4}, {"referenceID": 6, "context": "[8] 74.", "startOffset": 0, "endOffset": 3}, {"referenceID": 27, "context": "Average Perceptron by Ratinov [29] 72.", "startOffset": 30, "endOffset": 34}, {"referenceID": 39, "context": "CRF by Tkachenko [41] 76.", "startOffset": 17, "endOffset": 21}, {"referenceID": 32, "context": "Conditional Random Fields [34] 94.", "startOffset": 26, "endOffset": 30}, {"referenceID": 36, "context": "3 Latent Dynamic Conditional Random Fields [38] 94.", "startOffset": 43, "endOffset": 47}, {"referenceID": 34, "context": "This result is consistent with the [36] result.", "startOffset": 35, "endOffset": 39}, {"referenceID": 10, "context": "Alternative algorithms for training that could be tested include neural networks, which have shown excellent results on sequential labeling tasks, especially the long-term short-term memory network [12].", "startOffset": 198, "endOffset": 202}], "year": 2015, "abstractText": "Precise geocoding and time normalization for text requires that location and time phrases be identified. Many state-of-the-art geoparsers and temporal parsers suffer from low recall. Categories commonly missed by parsers are: nouns used in a nonspatiotemporal sense, adjectival and adverbial phrases, prepositional phrases, and numerical phrases. We collected and annotated data set by querying commercial web searches API with such spatiotemporal expressions as were missed by state-of-theart parsers. Due to the high cost of sentence annotation, active learning was used to label training data, and a new strategy was designed to better select training examples to reduce labeling cost. For the learning algorithm, we applied an average perceptron trained Featurized Hidden Markov Model (FHMM). Five FHMM instances were used to create an ensemble, with the output phrase selected by voting. Our ensemble model was tested on a range of sequential labeling tasks, and has shown competitive performance. Our contributions include (1) an new dataset annotated with named entities and expanded spatiotemporal expressions; (2) a comparison of inference algorithms for ensemble models showing the superior accuracy of Belief Propagation over Viterbi Decoding; (3) a new example re-weighting method for active ensemble learning that \u201cmemorizes\u201d the latest examples trained; (4) a spatiotemporal parser that jointly recognizes expanded spatiotemporal expressions as well as named entities.", "creator": "Word"}}}