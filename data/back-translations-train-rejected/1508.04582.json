{"id": "1508.04582", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "19-Aug-2015", "title": "Learning to Predict Independent of Span", "abstract": "We consider how to learn multi-step predictions efficiently. Conventional algorithms wait until observing actual outcomes before performing the computations to update their predictions. If predictions are made at a high rate or span over a large amount of time, substantial computation can be required to store all relevant observations and to update all predictions when the outcome is finally observed. We show that the exact same predictions can be learned in a much more computationally congenial way, with uniform per-step computation that does not depend on the span of the predictions. We apply this idea to various settings of increasing generality, repeatedly adding desired properties and each time deriving an equivalent span-independent algorithm for the conventional algorithm that satisfies these desiderata. Interestingly, along the way several known algorithmic constructs emerge spontaneously from our derivations, including dutch eligibility traces, temporal difference errors, and averaging. This allows us to link these constructs one-to-one to the corresponding desiderata, unambiguously connecting the `how' to the `why'. Each step, we make sure that the derived algorithm subsumes the previous algorithms, thereby retaining their properties. Ultimately we arrive at a single general temporal-difference algorithm that is applicable to the full setting of reinforcement learning.", "histories": [["v1", "Wed, 19 Aug 2015 09:37:25 GMT  (153kb,D)", "http://arxiv.org/abs/1508.04582v1", "32 pages"]], "COMMENTS": "32 pages", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["hado van hasselt", "richard s sutton"], "accepted": false, "id": "1508.04582"}, "pdf": {"name": "1508.04582.pdf", "metadata": {"source": "CRF", "title": "Learning to Predict Independent of Span", "authors": ["Hado van Hasselt", "Richard S. Sutton"], "emails": [], "sections": [{"heading": "1 Learning long-term predictions", "text": "In fact, it is that we are able to assert ourselves, that we are able to change the world, and that we are able to change the world, \"he said in an interview with the Deutsche Presse-Agentur.\" We have to be able to change the world, \"he told the Deutsche Presse-Agentur."}, {"heading": "2 Outline of the paper", "text": "In this section, we briefly describe the high-level narrative of the paper, without going into technical detail. In each of the sections 3 to 8, we describe and formalize one or more desirable properties for our algorithms and derive from them a mathematically congenial algorithm that achieves this precisely. We build up to the last, most general goal an algorithm that is ultimately derived in Section 8 to emphasize the connections between desired properties and algorithmic constructs. Clarifying these connections is one of the main goals of this intermediate goal that achieves this precisely. Specifically, in Section 3, we derive from it a span-independent algorithm to update the predictions for a single end result. The algorithm is offline in the sense that it does not change its predictions until we observe the outcome.The dutch trace arises spontaneously, showing that this track is closely tied to the requirement of the Spanish-independent compilation."}, {"heading": "3 Independence of span and the emergence of traces", "text": "We start with a supervised learning setting - the prediction of the final numerical result of an episodic process = > individual value. An episode of the process begins with time t = 0 and shifts stochastically from state to state in which characteristics are generated \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 until termination with a final numerical result Z lately T. For example, Z could be the price of a certain stock that we want to predict, and each episode can be one year, so that the time T corresponds to the end of the year. We consider the general case of multi-stage predictions (T > 1) in which a prediction is made in each step. The standard supervised learning setting is a special case in which we make only one prediction in each episode (so that without losing universality, T = 1).Our predictions are linear, that is, the prediction in due time is the inner product of the prediction and a learned weight vector."}, {"heading": "4 Online updating and the emergence of TD errors", "text": "The algorithms in the previous section make no changes to the predictions during the episode; these are offline algorithms. However, it would sometimes be useful to update the predictions during an episode, especially if the time span is very long and we don't want to wait that long before starting to learn. However, in this section we introduce an online prediction and derive from it a span-independent algorithm (the backward view) that calculates exactly the same predictions at every step. An online algorithm cannot update the predictions to the end result Z during the episode because Z is not yet available. Instead, if we only make observations up to a horizon h < T we can use the predictions for all previous times t < h towards some informed guesses about what the end result will be. Such a guess plays the role of a target for the updates, such as Z in the forecast view (1), but it is used in front of Z; it is a sub-target."}, {"heading": "5 Unifying online and offline learning and the emer-", "text": "The online algorithms from the previous section are not entirely subject to the offline algorithms from Section 3. Although they all reach the same weights by the end of the episode, \u2212 \u2212 \u2212 if their weights are actually different \u2212 \u03b2\u03b2\u03b2h \u00b7 if the offline algorithm does not change the weights during the episode, and the online algorithm must change them. One might think that the online algorithm is always better because it can use all the relevant incoming information immediately, but it is not. In this case, the intermediate weights are always completely wrong (say due to a bad human \"expert\") and would cause the weights of the online algorithm to be completely wrong for all the steps except the last at the end of the episode. In this case, the weights of the online algorithm would be worse than those of the offline algorithms almost all that we use offline algorithms. Because intermediate goals can sometimes be misleading, we want to reduce their impact to a few steps based on how much we trust these targets."}, {"heading": "6 Bootstrapping", "text": "So far, we have always completely ignored the actual end result. All intermediate targets have been deemed irrelevant until the end of the episode, which has no effect on the final weights calculated. There are cases where we do not want to discard all intermediate targets - for example, if we look at a stock that is about to crash at the end of the year. Certainly, our updated forecasts should include the possibility of such a crash, but we cannot predict whether it will always crash just before the end of the year. Likewise, we assume that it will rain on a certain date for which we want to predict the intermediate target. It would then seem wasteful to ignore the sunny weather on the days leading up to that date. These are examples of cases where the intermediate targets are almost as informative as the final result. In some cases, an intermediate target could even be more informative. For example, it could be due to a highly trusted expert taking into account all possible outcomes from that date."}, {"heading": "7 Combining two notions of trust and the emergence", "text": "It is a strict generalization of the online algorithm (11), but it does not subsumed the offline algorithm (6), or the mean algorithm (19), which smoothly switches between online and offline updates. In this section, we combine the ideas from the last two sections to arrive at an algorithm that generalizes and subsumes all the previous algorithms, unifying all those who have previously entered a single, universally valid algorithm. In this section, we can combine the ideas from the last two sections by using the online algorithm in (27) to update an online weight vector and then define the trusted vector to stay the same with the initial weights until they are the last step to which we replace them with online weights. An algorithm that switches smoothly between offline and online cases can then be achieved similarly to the one before."}, {"heading": "8 Generalizing to cumulative returns and soft termi-", "text": "In fact, it is not so that we are able to assert that it is a pure disinternat, but a pure disinternat, in which it is a pure disinternat. (...) It is not so that it is a pure disinternat. (...) \"It is a disinternat.\" (...) \"It is a disinternat.\" (...) \"It is a disinternat.\" (...) \"(...)\" (...) \"(...)\" (...) \"(...)\" (\") (\") (\"(\") (\") (\") (\") (\") (\") (\") (\") (\") (\") (\") (\") (\" () (\"() (\" () (\"() (\" () () (\"() (\" () (\"() () () (() () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () (() () () () () ((() () () () () () (() () () (() () ((() () () (() () ((() (() () () (() ((() () (() ((() (() (() (() ((() () () (() ((() () (((() (() (() (() () ((() (() ((() ((() (() ((() () () ((() () (((() () (() (() (() ((("}, {"heading": "9 Convergence Analysis", "text": "The algorithm (44) differs from related earlier algorithms such as TD (\u03bb) in a few subtle but important ways. The most notable differences are the updates to the tracks and averaging based on \u03b2t. Known results on convergence are therefore not automatically transferable to this new algorithm and it is appropriate to take a moment to analyze it.The convergence of trusted weights depends on the convergence of online weights, and so we need to examine them jointly. Online weights, in turn, depend on the sequences of parameters and residual predictions that are provided. We want our analysis to be general, which means that we want to be able to handle general sequences of discounts."}, {"heading": "10 Discussion", "text": "In this paper, we considered how to answer predictive questions with algorithms that use constant calculations per time step, proportional to the number of learned weights, and independent of the range of prediction. We looked at both final and cumulative results, among online and offline updates, with and without persistence of the residual predictions we make during an episode, and with hard and soft termination. In the end, we got a single general algorithm that can be used for all these different predictive questions, as shown in (44). This algorithm is guaranteed to be convergent under typical, relatively mild, technical conditions. Some enhancements remain for future work. In particular, we did not take into account how different behavioral strategies can influence our predictions, and as a result, we did not talk about the problem of control, where the goal is to find the optimal policy for a given (reward) signal. Our analysis extends easily to actions that can already be predicted by natural means."}], "references": [{"title": "Non-strongly-convex smooth stochastic approximation with convergence rate O(1/n)", "author": ["F. Bach", "E. Moulines"], "venue": "Advances in Neural Information Processing Systems 26, pp. 773\u2013781.", "citeRegEx": "Bach and Moulines,? 2013", "shortCiteRegEx": "Bach and Moulines", "year": 2013}, {"title": "Dynamic Programming", "author": ["R. Bellman"], "venue": "Princeton University Press.", "citeRegEx": "Bellman,? 1957", "shortCiteRegEx": "Bellman", "year": 1957}, {"title": "Stochastic approximation with two time scales", "author": ["V.S. Borkar"], "venue": "Systems & Control Letters 29(5), pp. 291\u2013294.", "citeRegEx": "Borkar,? 1997", "shortCiteRegEx": "Borkar", "year": 1997}, {"title": "Stochastic approximation", "author": ["V.S. Borkar"], "venue": "Cambridge Books.", "citeRegEx": "Borkar,? 2008", "shortCiteRegEx": "Borkar", "year": 2008}, {"title": "Linear least-squares algorithms for temporal difference learning", "author": ["S.J. Bradtke", "A.G. Barto"], "venue": "Machine Learning 22, pp. 33\u201357.", "citeRegEx": "Bradtke and Barto,? 1996", "shortCiteRegEx": "Bradtke and Barto", "year": 1996}, {"title": "Dynamic programming and Markov processes", "author": ["R.A. Howard"], "venue": "MIT Press.", "citeRegEx": "Howard,? 1960", "shortCiteRegEx": "Howard", "year": 1960}, {"title": "Convergence rate of linear two-time-scale stochastic approximation", "author": ["V.R. Konda", "J.N. Tsitsiklis"], "venue": "Annals of applied probability 14(2), pp. 796\u2013819.", "citeRegEx": "Konda and Tsitsiklis,? 2004", "shortCiteRegEx": "Konda and Tsitsiklis", "year": 2004}, {"title": "Stochastic approximation and recursive algorithms and applications", "author": ["H.J. Kushner", "G. Yin"], "venue": "Vol. 35. Springer Science & Business Media.", "citeRegEx": "Kushner and Yin,? 2003", "shortCiteRegEx": "Kushner and Yin", "year": 2003}, {"title": "Gradient temporal-difference learning algorithms", "author": ["H.R. Maei"], "venue": "PhD thesis. University of Alberta.", "citeRegEx": "Maei,? 2011", "shortCiteRegEx": "Maei", "year": 2011}, {"title": "Weighted importance sampling for off-policy learning with linear function approximation", "author": ["A.R. Mahmood", "H.P. van Hasselt", "R.S. Sutton"], "venue": "Advances in Neural Information Processing Systems", "citeRegEx": "Mahmood et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Mahmood et al\\.", "year": 2014}, {"title": "Human-level control through deep reinforcement learning", "author": ["V. Mnih", "K. Kavukcuoglu", "D. Silver", "A.A. Rusu", "J. Veness", "M.G. Bellemare", "A. Graves", "M. Riedmiller", "A.K. Fidjeland", "G. Ostrovski", "S. Petersen", "C. Beattie", "A. Sadik", "I. Antonoglou", "H. King", "D. Kumaran", "D. Wierstra", "S. Legg", "D. Hassabis"], "venue": "Nature 518(7540), pp. 529\u2013533.", "citeRegEx": "Mnih et al\\.,? 2015", "shortCiteRegEx": "Mnih et al\\.", "year": 2015}, {"title": "Simultaneous localization and mapping with unknown data association using FastSLAM", "author": ["M. Montemerlo", "S. Thrun"], "venue": "In: IEEE International Conference on Robotics and Automation, 2003. Vol. 2. IEEE, pp. 1985\u20131991.", "citeRegEx": "Montemerlo and Thrun,? 2003", "shortCiteRegEx": "Montemerlo and Thrun", "year": 2003}, {"title": "Acceleration of stochastic approximation by averaging", "author": ["B.T. Polyak", "A.B. Juditsky"], "venue": "SIAM Journal on Control and Optimization 30(4), pp. 838\u2013855.", "citeRegEx": "Polyak and Juditsky,? 1992", "shortCiteRegEx": "Polyak and Juditsky", "year": 1992}, {"title": "Off-policy temporal-difference learning with function approximation", "author": ["D. Precup", "R.S. Sutton"], "venue": "In: Proceedings of the eighteenth International Conference on Machine Learning. Morgan Kaufmann, pp. 417\u2013424.", "citeRegEx": "Precup and Sutton,? 2001", "shortCiteRegEx": "Precup and Sutton", "year": 2001}, {"title": "Eligibility traces for off-policy policy evaluation", "author": ["D. Precup", "R.S. Sutton", "S.P. Singh"], "venue": "In: Proceedings of the Seventeenth International Conference on Machine Learning. Morgan Kaufmann, pp. 766\u2013773.", "citeRegEx": "Precup et al\\.,? 2000", "shortCiteRegEx": "Precup et al\\.", "year": 2000}, {"title": "A stochastic approximation method", "author": ["H. Robbins", "S. Monro"], "venue": "The Annals of Mathematical Statistics 22(3), pp. 400\u2013407.", "citeRegEx": "Robbins and Monro,? 1951", "shortCiteRegEx": "Robbins and Monro", "year": 1951}, {"title": "Temporal credit assignment in reinforcement learning", "author": ["R.S. Sutton"], "venue": "PhD thesis. University of Massachusetts.", "citeRegEx": "Sutton,? 1984", "shortCiteRegEx": "Sutton", "year": 1984}, {"title": "Learning to predict by the methods of temporal differences", "author": ["R.S. Sutton"], "venue": "Machine Learning 3, pp. 9\u201344. 31", "citeRegEx": "Sutton,? 1988", "shortCiteRegEx": "Sutton", "year": 1988}, {"title": "Reinforcement Learning: An Introduction", "author": ["R.S. Sutton", "A.G. Barto"], "venue": "The MIT press, Cambridge MA.", "citeRegEx": "Sutton and Barto,? 1998", "shortCiteRegEx": "Sutton and Barto", "year": 1998}, {"title": "A convergent O(n) algorithm for off-policy temporal-difference learning with linear function approximation", "author": ["R.S. Sutton", "Szepesv\u00e1ri", "Cs.", "H.R. Maei"], "venue": "Advances in Neural Information Processing Systems 21, pp. 1609\u20131616.", "citeRegEx": "Sutton et al\\.,? 2008", "shortCiteRegEx": "Sutton et al\\.", "year": 2008}, {"title": "Fast gradient-descent methods for temporal-difference learning with linear function approximation", "author": ["R.S. Sutton", "H.R. Maei", "D. Precup", "S. Bhatnagar", "D. Silver", "Szepesv\u00e1ri", "Cs.", "E. Wiewiora"], "venue": "In: Proceedings of the 26th Annual International Conference on Machine Learning. ACM, pp. 993\u20131000.", "citeRegEx": "Sutton et al\\.,? 2009", "shortCiteRegEx": "Sutton et al\\.", "year": 2009}, {"title": "A new Q(\u03bb) with interim forward view and Monte Carlo equivalence", "author": ["R.S. Sutton", "A.R. Mahmood", "D. Precup", "H.P. van Hasselt"], "venue": "JMLR W&CP", "citeRegEx": "Sutton et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Sutton et al\\.", "year": 2014}, {"title": "Algorithms for reinforcement learning", "author": ["Szepesv\u00e1ri", "Cs."], "venue": "Synthesis Lectures on Artificial Intelligence and Machine Learning 4(1), pp. 1\u2013103.", "citeRegEx": "Szepesv\u00e1ri and Cs.,? 2010", "shortCiteRegEx": "Szepesv\u00e1ri and Cs.", "year": 2010}, {"title": "Off-policy TD(\u03bb) with a true online equivalence", "author": ["H.P. van Hasselt", "A.R. Mahmood", "R.S. Sutton"], "venue": "Proceedings of the 30th Conference on Uncertainty in Artificial Intelligence", "citeRegEx": "Hasselt et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Hasselt et al\\.", "year": 2014}, {"title": "True online TD(\u03bb)", "author": ["H. van Seijen", "R.S. Sutton"], "venue": "JMLR W&CP", "citeRegEx": "Seijen and Sutton,? \\Q2014\\E", "shortCiteRegEx": "Seijen and Sutton", "year": 2014}], "referenceMentions": [{"referenceID": 18, "context": "For example, in reinforcement learning we often learn value functions that are predictions of the discounted sum of all future rewards in the potentially infinite future (Sutton and Barto 1998).", "startOffset": 170, "endOffset": 193}, {"referenceID": 16, "context": "We show that the desire to be online results in the spontaneous emergence of TD errors (Sutton 1984; Sutton 1988).", "startOffset": 87, "endOffset": 113}, {"referenceID": 17, "context": "We show that the desire to be online results in the spontaneous emergence of TD errors (Sutton 1984; Sutton 1988).", "startOffset": 87, "endOffset": 113}, {"referenceID": 12, "context": "This is interesting because such averaging is known to improve the convergence rates of online learning algorithms (Polyak and Juditsky 1992; Bach and Moulines 2013), but seems to only rarely be used in reinforcement learning (as noted, e.", "startOffset": 115, "endOffset": 165}, {"referenceID": 0, "context": "This is interesting because such averaging is known to improve the convergence rates of online learning algorithms (Polyak and Juditsky 1992; Bach and Moulines 2013), but seems to only rarely be used in reinforcement learning (as noted, e.", "startOffset": 115, "endOffset": 165}, {"referenceID": 17, "context": "In Section 6, we formalize these ideas and show they lead naturally to a form of TD(\u03bb) (Sutton 1988; Sutton and Barto 1998).", "startOffset": 87, "endOffset": 123}, {"referenceID": 18, "context": "In Section 6, we formalize these ideas and show they lead naturally to a form of TD(\u03bb) (Sutton 1988; Sutton and Barto 1998).", "startOffset": 87, "endOffset": 123}, {"referenceID": 16, "context": "The et vector is analogous to the conventional eligibility trace (see: Sutton 1988; Sutton and Barto 1998, and references therein) but has a special form as first proposed by van Seijen and Sutton (2014). It is initialized to e\u22121 = 0 (or, equivalently, to e0 = \u03b10\u03c60) and then updated according to", "startOffset": 71, "endOffset": 204}, {"referenceID": 18, "context": "Until recently, exactly equivalences between forward and backward views were only known to exist for algorithms that update their predictions in batch (Sutton and Barto 1998).", "startOffset": 151, "endOffset": 174}, {"referenceID": 16, "context": "Until recently, exactly equivalences between forward and backward views were only known to exist for algorithms that update their predictions in batch (Sutton and Barto 1998). Van Seijen & Sutton (2014) were the first to derive an online backward", "startOffset": 152, "endOffset": 203}, {"referenceID": 18, "context": "This target Z t is known as a \u03bb-return (Sutton and Barto 1998).", "startOffset": 39, "endOffset": 62}, {"referenceID": 16, "context": "This target Z t is known as a \u03bb-return (Sutton and Barto 1998). The version that truncates at the current horizon h was first proposed by van Seijen and Sutton (2014). The total set of updates is", "startOffset": 40, "endOffset": 167}, {"referenceID": 17, "context": "The merits of \u03bb-returns are well known (Sutton 1988; Sutton and Barto 1998) but the \u03b2-weighting of the online weights is novel to this paper, and it is appropriate to discuss it in a little more detail.", "startOffset": 39, "endOffset": 75}, {"referenceID": 18, "context": "The merits of \u03bb-returns are well known (Sutton 1988; Sutton and Barto 1998) but the \u03b2-weighting of the online weights is novel to this paper, and it is appropriate to discuss it in a little more detail.", "startOffset": 39, "endOffset": 75}, {"referenceID": 12, "context": "This specific algorithm is interesting because the predictions according to the averages \u03b8m are known to converge to the optimal predictions faster than the predictions according to any sequence of online weights \u03b8\u0303m (Polyak and Juditsky 1992; Bach and Moulines 2013).", "startOffset": 217, "endOffset": 267}, {"referenceID": 0, "context": "This specific algorithm is interesting because the predictions according to the averages \u03b8m are known to converge to the optimal predictions faster than the predictions according to any sequence of online weights \u03b8\u0303m (Polyak and Juditsky 1992; Bach and Moulines 2013).", "startOffset": 217, "endOffset": 267}, {"referenceID": 16, "context": "Therefore, as in Sutton et al. (2014), we allow the features, discounts and persistency parameters to be stationary functions of an underlying unobserved state, such that \u03c6t .", "startOffset": 17, "endOffset": 38}, {"referenceID": 15, "context": "If the step sizes are suitably chosen, for instance such that \u2211\u221e t=0 \u03b1t =\u221e and \u2211\u221e t=0 \u03b1 2 t <\u221e (Robbins and Monro 1951), and if the means and variances of Z\u221e t and \u03c6t are well-defined and bounded for all t, this update converges to the fixed-point solution \u03b8\u2217 that minimizes the quadratic loss (cf.", "startOffset": 95, "endOffset": 119}, {"referenceID": 0, "context": "Although convergence is already guaranteed when \u03b2t = 1 for all t, recent work has shown that for similar stochastic gradient algorithms the optimal rate of convergence is attained if \u03b2t decreases much faster than \u03b1t, specifically when \u03b2t = O(t \u22121) while \u03b1t = \u03b1 for some constant \u03b1 (Bach and Moulines 2013).", "startOffset": 281, "endOffset": 305}, {"referenceID": 0, "context": "Although convergence is already guaranteed when \u03b2t = 1 for all t, recent work has shown that for similar stochastic gradient algorithms the optimal rate of convergence is attained if \u03b2t decreases much faster than \u03b1t, specifically when \u03b2t = O(t \u22121) while \u03b1t = \u03b1 for some constant \u03b1 (Bach and Moulines 2013). More generally, it seems likely that convergence also holds if \u2211\u221e t=0 \u03b2 2 t <\u221e and \u2211\u221e t=0 \u03b1 2 t =\u221e. The observation that \u03b2t should perhaps decrease over time for faster learning may seem at odds with our introduction of this parameter as a degree of trust. However, these two views are quite compatible if we consider \u03b2t to be the degree of trust we place in the online updates relative to the trust we place in our current predictions due to the trusted weights. When the trust in the predictions increases over time, the relative trust in the inherently noisy online targets should then decrease. Although Theorem 1 is already fairly general, it does not cover the important case when the residual predictions additionally depend on the weights we are updating. It makes sense to use the predictions we trust most and therefore we now consider what happens when Pt . = \u03c6t \u03b8t\u22121. Notice that we have to use \u03b8t\u22121 rather than \u03b8t, because Pt is used in the computation of \u03b8t and so the latter is not yet available when we compute Pt. The analysis of this case is more complex than the previous one, because Pt is no longer a constant function of state. This means the update is no longer a standard gradient-descent update on a quadratic loss, because the target Z t for the online weights itself depends on the trusted weights that we are simultaneously updating. The results by Bach and Moulines (2013) on stochastic gradient descent indicate that perhaps the most interesting case is where \u03b2t decreases faster than \u03b1t, such that limt\u2192\u221e \u03b2t/\u03b1t = 0.", "startOffset": 282, "endOffset": 1709}, {"referenceID": 20, "context": "Then, if \u2211\u221e t=0 \u03b1t = \u2211\u221e t=0 \u03b2t = \u221e, \u2211\u221e t=0 \u03b1 2 t < \u221e, and limt\u2192\u221e \u03b2t \u03b1t = 0, algorithm (44) converges almost surely to the TD fixed-point solution \u03b8\u2217 that minimizes the mean-squared projected Bellman error (Sutton, Szepesv\u00e1ri, and Maei 2008; Sutton et al. 2009), such that", "startOffset": 205, "endOffset": 260}, {"referenceID": 2, "context": "For more detail on analyzing stochastic approximations on two times scales, we refer to Borkar (1997), Borkar (2008), Kushner and Yin (2003), and Konda and Tsitsiklis (2004).", "startOffset": 88, "endOffset": 102}, {"referenceID": 2, "context": "For more detail on analyzing stochastic approximations on two times scales, we refer to Borkar (1997), Borkar (2008), Kushner and Yin (2003), and Konda and Tsitsiklis (2004).", "startOffset": 88, "endOffset": 117}, {"referenceID": 2, "context": "For more detail on analyzing stochastic approximations on two times scales, we refer to Borkar (1997), Borkar (2008), Kushner and Yin (2003), and Konda and Tsitsiklis (2004).", "startOffset": 88, "endOffset": 141}, {"referenceID": 2, "context": "For more detail on analyzing stochastic approximations on two times scales, we refer to Borkar (1997), Borkar (2008), Kushner and Yin (2003), and Konda and Tsitsiklis (2004). We first analyze the convergence of the faster updates to the online weights, where we can assume that the trusted weights are stationary at some value \u03b8.", "startOffset": 88, "endOffset": 174}, {"referenceID": 1, "context": "Then, using a form of policy iteration (Bellman 1957; Howard 1960), we can repeatedly switch between estimating and improving the policy to tackle the problem of optimal control.", "startOffset": 39, "endOffset": 66}, {"referenceID": 5, "context": "Then, using a form of policy iteration (Bellman 1957; Howard 1960), we can repeatedly switch between estimating and improving the policy to tackle the problem of optimal control.", "startOffset": 39, "endOffset": 66}, {"referenceID": 18, "context": "However, to properly and fully include adaptable policies, we would in addition need to carefully consider the problem of learning off-policy, about action-selection policies that differ from the one used to generate the data (Sutton and Barto 1998).", "startOffset": 226, "endOffset": 249}, {"referenceID": 13, "context": "This is consistent but orthogonal to the ideas outlined in this paper, and such off-policy predictions (including those about the greedy and, ultimately, optimal policy) are learnable through a proper use of rejection sampling, as in Q-learning, or importance sampling (Precup, Sutton, and Singh 2000; Precup and Sutton 2001; Maei 2011; Sutton et al. 2014; van Hasselt, Mahmood, and Sutton 2014; Mahmood, van Hasselt, and Sutton 2014).", "startOffset": 269, "endOffset": 434}, {"referenceID": 8, "context": "This is consistent but orthogonal to the ideas outlined in this paper, and such off-policy predictions (including those about the greedy and, ultimately, optimal policy) are learnable through a proper use of rejection sampling, as in Q-learning, or importance sampling (Precup, Sutton, and Singh 2000; Precup and Sutton 2001; Maei 2011; Sutton et al. 2014; van Hasselt, Mahmood, and Sutton 2014; Mahmood, van Hasselt, and Sutton 2014).", "startOffset": 269, "endOffset": 434}, {"referenceID": 21, "context": "This is consistent but orthogonal to the ideas outlined in this paper, and such off-policy predictions (including those about the greedy and, ultimately, optimal policy) are learnable through a proper use of rejection sampling, as in Q-learning, or importance sampling (Precup, Sutton, and Singh 2000; Precup and Sutton 2001; Maei 2011; Sutton et al. 2014; van Hasselt, Mahmood, and Sutton 2014; Mahmood, van Hasselt, and Sutton 2014).", "startOffset": 269, "endOffset": 434}, {"referenceID": 10, "context": "The main idea of span-independent computation is more general and can be applied quite naturally to other settings, including for instance non-linear functions such as deep neural networks (LeCun, Bengio, and Hinton 2015; Mnih et al. 2015) or to quadratic-time linear-function algorithms as in LSTD (Bradtke and Barto 1996).", "startOffset": 189, "endOffset": 239}, {"referenceID": 4, "context": "2015) or to quadratic-time linear-function algorithms as in LSTD (Bradtke and Barto 1996).", "startOffset": 65, "endOffset": 89}], "year": 2015, "abstractText": "We consider how to learn multi-step predictions efficiently. Conventional algorithms wait until observing actual outcomes before performing the computations to update their predictions. If predictions are made at a high rate or span over a large amount of time, substantial computation can be required to store all relevant observations and to update all predictions when the outcome is finally observed. We show that the exact same predictions can be learned in a much more computationally congenial way, with uniform per-step computation that does not depend on the span of the predictions. We apply this idea to various settings of increasing generality, repeatedly adding desired properties and each time deriving an equivalent span-independent algorithm for the conventional algorithm that satisfies these desiderata. Interestingly, along the way several known algorithmic constructs emerge spontaneously from our derivations, including dutch eligibility traces, temporal difference errors, and averaging. This allows us to link these constructs one-to-one to the corresponding desiderata, unambiguously connecting the \u2018how\u2019 to the \u2018why\u2019. Each step, we make sure that the derived algorithm subsumes the previous algorithms, thereby retaining their properties. Ultimately we arrive at a single general temporal-difference algorithm that is applicable to the full setting of reinforcement learning. 1 Learning long-term predictions The span of a multi-step prediction is the number of steps elapsing between when the prediction is made and when its target or ideal value is known. We consider the case in which predictions are made repeatedly, at each of a sequence of discrete time steps. For example, if on each day we predict what a stock market index will be in 30 days, then the span is 30, whereas if we predict at each hour what the stock market index will be in 30 days, then the span is 30\u00d7 24 = 720. The span may vary for individual predictions in a sequence. For example, if we predict on each day what the stock-market index will be at the end of the year, then the span will be much longer for predictions made in January than it is for predictions made in \u2217Google DeepMind \u2020Reinforcement Learning and Artificial Intelligence Laboratory Department of Computing Science, University of Alberta Edmonton, Alberta, Canada T6G 2E8 1 ar X iv :1 50 8. 04 58 2v 1 [ cs .L G ] 1 9 A ug 2 01 5 December. If the span may vary in this way, then we consider the span of the prediction sequence to be the maximum possible span of any individual prediction in the sequence. For example, the span of a daily end-of-year stock-index prediction is 365. Often the span is infinite. For example, in reinforcement learning we often learn value functions that are predictions of the discounted sum of all future rewards in the potentially infinite future (Sutton and Barto 1998). In this paper we consider computational and algorithmic issues in efficiently learning long-term predictions, defined as predictions of large integer span. Predictions could be long term in this sense either because a great deal of clock time passes, as in predicting something at the end of the year, or because predictions are made very often, with a short time between steps (e.g., as in high-frequency financial trading). The per-step computational complexity of some algorithms for learning accurate predictions depends on the span of the predictions, and this can become a significant concern if the span is large. Therefore, we focus on the construction of learning algorithms whose computational complexity per time step (in both time and memory) is constant (does not scale with time) and independent of span. This paper features two recurring themes, the first of which is the repeated spontaneous emergence of, often well-known, algorithmic constructs, directly from our derivations. We start each derivation by formalizing a desired property and constructing an algorithm that fulfills it, without considering computationally efficiency. Then, we derive a spanindependent algorithm that results on each step in exactly the same predictions. Interestingly, each time a specific algorithmic construct emerges, demonstrating a clear connection between the desideratum (the \u2018why\u2019) and the algorithmic construct (the \u2018how\u2019). For instance, the desire to be independent of span leads to a dutch eligibility trace, which was previously derived only in the more specific context of online temporal difference (TD) learning (van Seijen and Sutton 2014). The second theme is that we unify the algorithms at each step. Each time, we make sure to obtain an algorithm that is strictly more general than the previous ones, so that in the end we obtain one single algorithm that can fulfill all the desiderata while remaining computationally congenial. 2 Outline of the paper In this section, we briefly describe the high-level narrative of the paper, without going into technical detail. In each of the Sections 3 to 8, we describe and formalize one or more desirable properties for our algorithms and then derive a computationally congenial algorithm that achieves this exactly. We build up to the final, most general, algorithm that is ultimately derived in Section 8 to highlight the connections between desired properties and algorithmic constructs. Making these connections clear is one of the main goals of this paper. Specifically, in Section 3 we derive a span-independent algorithm to update the predictions for a single final outcome. The algorithm is offline in the sense that does not change its predictions before observing the outcome. The dutch trace emerges spontaneously, which shows that this trace is closely tied to the requirement of span-independent computation. This emergence is surprising and intriguing because it shows that these traces", "creator": "LaTeX with hyperref package"}}}