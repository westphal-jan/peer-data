{"id": "1508.04826", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "19-Aug-2015", "title": "Dither is Better than Dropout for Regularising Deep Neural Networks", "abstract": "Regularisation of deep neural networks (DNN) during training is critical to performance. By far the most popular method is known as dropout. Here, cast through the prism of signal processing theory, we compare and contrast the regularisation effects of dropout with those of dither. We illustrate some serious inherent limitations of dropout and demonstrate that dither provides a far more effective regulariser which does not suffer from the same limitations.", "histories": [["v1", "Wed, 19 Aug 2015 23:02:37 GMT  (202kb)", "http://arxiv.org/abs/1508.04826v1", null], ["v2", "Wed, 26 Aug 2015 12:59:38 GMT  (206kb)", "http://arxiv.org/abs/1508.04826v2", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["andrew j r simpson"], "accepted": false, "id": "1508.04826"}, "pdf": {"name": "1508.04826.pdf", "metadata": {"source": "META", "title": "Dither is Better than Dropout for Regularising Deep Neural Networks", "authors": ["Andrew J.R. Simpson"], "emails": ["Andrew.Simpson@Surrey.ac.uk"], "sections": [{"heading": null, "text": "This year, we have reached the stage where we feel we can put ourselves at the top without being able to put ourselves at the top."}, {"heading": "II. METHOD", "text": "For the case study, we chose the well-known computer vision problem of handwritten number classification using the MNIST dataset [5]. For the input layer, we unzipped the images of 28x28 pixels into vectors of length 784. An example digit is normalized in Fig. 1. Pixel intensities were normalized to mean zero. To replicate Hinton's [6] architecture, but using the biased sigmoid activation function [2], we built a fully connected network of 784x100x10 units, with a 10-unit softmax output layer corresponding to the 10-way classification problem. To place ourselves in the small data regime, we used only the first 256 training examples of the MNIST dataset and tested on the full 10,000 test examples. We trained three versions of the model, with the first dynamics trained without a third unit."}, {"heading": "III. RESULTS", "text": "Fig. 2a records test error rates depending on complete SGD iterations for the unregulated models of different batch sizes. Performance depends on batch size; the model is essentially unable to learn anything useful if the batch size is less than 16 and peaks for batch sizes 32. Fig. 2b records the same values for the regulated models with 50% dropouts. As expected, performance is extremely dependent on batch size; however, performance is significantly worse (than without dropouts) for batch sizes 2 and 4, is improved (relative to no dropouts) for batch size 8 and resembles (the performance without dropouts) for the larger batch sizes. This tends to suggest that the regulation provided by the data itself is relatively well realized (at higher frequencies) by simply distributing the data over larger batches (so there are few obvious benefits for this case)."}, {"heading": "IV. DISCUSSION AND CONCLUSION", "text": "In this paper we have shown that dither is a superior regularizer compared to dropouts and that regularization by dither is more or less independent of batch size as opposed to dropouts. We have argued that dither is superior to dropouts as a regularizer because it is not dependent on batch size and because it is by nature broadband and additive. In addition, we have documented for the first time paradoxical anti-regularization effects of dropouts in small batch sizes."}, {"heading": "ACKNOWLEDGMENT", "text": "The AJRS did this work at weekends and was supported by his wife and children."}], "references": [{"title": "Learning deep architectures for AI\u201d, Foundations and Trends in Machine Learning 2:1\u2013127", "author": ["Y Bengio"], "venue": null, "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2009}, {"title": "Abstract Learning via Demodulation in a Deep Neural Network\u201d, arxiv.org", "author": ["AJR Simpson"], "venue": null, "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2015}, {"title": "Over-Sampling in a Deep Neural Network, arxiv.org", "author": ["AJR Simpson"], "venue": null, "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2015}, {"title": "Improving neural networks by preventing co-adaptation of feature detectors\u201d, The Computing Research Repository (CoRR), abs/1207.0580", "author": ["GE Hinton", "N Srivastava", "A Krizhevsky", "I Sutskever", "R Salakhutdinov"], "venue": null, "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2012}, {"title": "Gradient-based learning applied to document recognition", "author": ["Y LeCun", "L Bottou", "Y Bengio", "P Haffner"], "venue": "Proc. IEEE", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 1998}, {"title": "A fast learning algorithm for deep belief nets", "author": ["GE Hinton", "S Osindero", "Y Teh"], "venue": "Neural Computation", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2006}], "referenceMentions": [{"referenceID": 0, "context": "Deep neural networks [1] may be interpreted as discrete (sampled) systems consisting of linear filters and nonlinear demodulation stages [2] and it has been suggested [3] that the inherent nonlinear distortion and aliasing contribute to problems of overfitting.", "startOffset": 21, "endOffset": 24}, {"referenceID": 1, "context": "Deep neural networks [1] may be interpreted as discrete (sampled) systems consisting of linear filters and nonlinear demodulation stages [2] and it has been suggested [3] that the inherent nonlinear distortion and aliasing contribute to problems of overfitting.", "startOffset": 137, "endOffset": 140}, {"referenceID": 2, "context": "Deep neural networks [1] may be interpreted as discrete (sampled) systems consisting of linear filters and nonlinear demodulation stages [2] and it has been suggested [3] that the inherent nonlinear distortion and aliasing contribute to problems of overfitting.", "startOffset": 167, "endOffset": 170}, {"referenceID": 3, "context": "At face value, dropout [4] appears somewhat compatible with dither and is known to be a useful regulariser.", "startOffset": 23, "endOffset": 26}, {"referenceID": 3, "context": "However, despite the cited motivation of \u2018preventing co-adaptation\u2019 [4], a coherent signal-processing-based rationale for dropout as regulariser has not emerged.", "startOffset": 68, "endOffset": 71}, {"referenceID": 4, "context": "For case study, we chose the wellknown computer vision problem of hand-written digit classification using the MNIST dataset [5].", "startOffset": 124, "endOffset": 127}, {"referenceID": 5, "context": "Replicating Hinton\u2019s [6] architecture, but using the biased sigmoid activation function [2], we built a fully connected network of size 784x100x10", "startOffset": 21, "endOffset": 24}, {"referenceID": 1, "context": "Replicating Hinton\u2019s [6] architecture, but using the biased sigmoid activation function [2], we built a fully connected network of size 784x100x10", "startOffset": 88, "endOffset": 91}], "year": 2015, "abstractText": "Regularisation of deep neural networks (DNN) during training is critical to performance. By far the most popular method is known as dropout. Here, cast through the prism of signal processing theory, we compare and contrast the regularisation effects of dropout with those of dither. We illustrate some serious inherent limitations of dropout and demonstrate that dither provides a far more effective regulariser which does not suffer from the same limitations.", "creator": "PDFCreator Version 1.7.1"}}}