{"id": "1508.04906", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "20-Aug-2015", "title": "Semi-supervised Learning with Regularized Laplacian", "abstract": "We study a semi-supervised learning method based on the similarity graph and RegularizedLaplacian. We give convenient optimization formulation of the Regularized Laplacian method and establishits various properties. In particular, we show that the kernel of the methodcan be interpreted in terms of discrete and continuous time random walks and possesses several importantproperties of proximity measures. Both optimization and linear algebra methods can be used for efficientcomputation of the classification functions. We demonstrate on numerical examples that theRegularized Laplacian method is competitive with respect to the other state of the art semi-supervisedlearning methods.", "histories": [["v1", "Thu, 20 Aug 2015 08:01:42 GMT  (1773kb)", "http://arxiv.org/abs/1508.04906v1", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["konstantin avrachenkov", "pavel chebotarev", "alexey mishenin"], "accepted": false, "id": "1508.04906"}, "pdf": {"name": "1508.04906.pdf", "metadata": {"source": "CRF", "title": "Semi-supervised Learning with Regularized Laplacian", "authors": ["K. Avrachenkov", "P. Chebotarev", "A. Mishenin"], "emails": ["k.avrachenkov@inria.fr"], "sections": [{"heading": null, "text": "ar Xiv: 150 8.04 906v 1 [cs.L G] 20 Aug 201 5IS SN 0249 -639 9IS RN INR IA / R R-- 8765 --F R + EN GRESEARCH REPORTN \u00b0 8765 July 2015Project-Team Maestro"}, {"heading": "Semi-supervised", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "Learning with", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "Regularized Laplacian", "text": "K. Avrachenkov, P. Chebotarev, A. MisheninRESEARCH CENTRE SOPHIA ANTIPOLIS - M\u00c9DITERRAN\u00c92004 route des Lucioles - BP 93 06902 Sophia Antipolis CedexSemi-Supervised Learning withRegularized LaplacianK. Avrachenkov, P. Chebotarev \u2020, A. Mishenin \u2021 Project Team MaestroResearch Report n \u00b0 8765 - July 2015 - 19 pagesAbstract: We study a semi-supervised learning method based on the similarity chart and Regularized Laplacian. We give practical optimization formulation of the Regularized Laplacian method and establish its various properties. In particular, we show that the core of the method can be interpreted in terms of discretion and continuous time."}, {"heading": "L\u2019Apprentissage Semi-supervis\u00e9 avec Laplacian R\u00e9gularis\u00e9", "text": "R\u00e9sum\u00e9: Nous \u00e9tudions une m\u00e9thode d'apprentissage semi-supervis\u00e9, bas\u00e9 sur le graphe de similarit\u00e9 et Laplacian r\u00e9gularis\u00e9. Nous formalisons la m\u00e9thode comme un probl\u00e8me d'optimisation convexe et quadratique et nous \u00e9tablissons ses ses diverse propri\u00e9t\u00e9s. En particulier, nous montrons que le noyau de la m\u00e9thode peut \u00eatre interpr\u00e9t\u00e9 en termes des marches al\u00e9atoires en temps discret et continu et poss\u00e8de plusieurs propri\u00e9t\u00e9s importantes des mesures de proximit\u00e9. Les techniques d'optimisation ainsi que les techniques d'alg\u00e9bre lin\u00e9aire peuvent \u00eatre utilis\u00e9 pour un calcul efficace des fonctions de classification."}, {"heading": "Semi-supervised Learning with Regularized Laplacian 3", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "1 Introduction", "text": "This year it has come to the point that it will only be a matter of time before it will happen, until it does."}, {"heading": "4 K. Avrachenkov & P. Chebotarev & A. Mishenin", "text": "The paper is structured as follows: In the next section, we will formally define the Regularized Laplactic Method. In Section 3, we will discuss several related graph-based semi-monitored methods and graph cores. In Section 4, we will present insightful interpretations and properties of the Regularized Laplactic Method. In Section 5, we will analyze important borderline cases. Subsequently, in Section 6, we will discuss various numerical approaches to calculating the classification functions and show, using numerical examples, that the performance of the Regularized Laplactic Method is better or comparable to the leading semi-monitored methods. Section 7 concludes the paper with instructions for future research."}, {"heading": "2 Notations and method formulation", "text": "Suppose you place N data points (nodes) in K classes and assume that P data points = \u03b2 functions = \u03b2 classification = \u03b2 classification, that is, we know the class to which each labeled point belongs. Denote by Vk denotes the set of labeled points in class k = 1,..., K. Of course, | V1 | +... + | VK | = P. The graph-based semi-supervised learning approach uses a weighted graph G = (V, A) that connects data points, where V, V | = N denotes the set of nodes and A denotes the weight (similarity) matrix. In this work, we assume that A is symmetrical and the underlying graph is connected. Each element aij represents the degree of similarity between data points i and j. Denote by D denotes the diagonal matrix with its (i) element equal to the sum of the matrix."}, {"heading": "Semi-supervised Learning with Regularized Laplacian 5", "text": "(e.g. potential iterations) applied to (2) or by numerical optimization methods applied to (1). In Section 6 numerical methods are performed. As soon as the classification functions are reached, the points are classified according to the rule Fik > Fik \u2032, according to which K \u00b2 6 = K \u00b2 point i into the class k. The bonds can be broken as desired."}, {"heading": "3 Related approaches", "text": "Let's discuss a number of similar approaches: First, we discuss formal relationships, and in the numerical examples section, we compare approaches using some benchmark examples."}, {"heading": "3.1 Relation to heat kernels", "text": "The authors of [17, 18] first introduced and examined the properties of the heat core on the basis of the normalized Laplaker method. Specifically, they introduced the core H (t) = exp (\u2212 tL), (3), where L = D \u2212 1 / 2LD \u2212 1 / 2 is the normalized Laplaker. Let us refer to H (t) as the normalized heat core. Note that the normalized heat core is the solution of the following differential equation H (t) = \u2212 LH (t) with the initial condition H (0) = I. Then, in [19] the PageRank heat core was introduced as the solution (\u2212 t (I \u2212 P), (4), where P = D \u2212 1A, (5) is the transition probability matrix of the standard random path on the graph."}, {"heading": "6 K. Avrachenkov & P. Chebotarev & A. Mishenin", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "3.2 Relation to the generalized semi-supervised learning method", "text": "In [4] the authors proposed a generalized optimization framework for partially supervised learning methods based on graphs based on semi-supervised learning methods, i.e. the input of a weight matrix W = (wij), which is a function of A (in particular one can also take W = A). Especially with \u03c3 = 1 we get the transductive partially supervised learning method [35], with \u03c3 = 1 / 2 we get back the partially supervised learning with local and global consistency [36] and with \u03c3 = 0 we get the PageRank-based method [3]. The classification functions of the generalized partially supervised learning method are D.K = D.K = D.K = D.K (now D.K = D.K = D.K = D.K = D.K = D.K = D.K = D.K = D.K = D.K = D.K = D.K = D.K = D.K = D.K = D.K = D.K = D.K = D.K = D.K = D.K = D.K = D.K = D.K = D.K = D.K = D.K = D.K = D.K = D.K = D.K = D.K = D.K = D.K = D.K = D.K = D.K = D.K = D.K = D.K = D.K = D.K = D.K = D.K = D.K = D.K = D.K = D.K = D.K = D.K = D.K = D.K D.K = D.K = D.K = D.K = D.K = D.K = D.K = D.K D.K = D.K D.K = D.K D.K = D.K D.K = D.K D.K = D.K D.K = D.K D.K = D.K D.K D.K = D.K D.K = D.K D.K = D.K D.K D.K D.K = D.K D.K D.K D.K D.K D"}, {"heading": "4 Properties and interpretations of the Regularized Lapla-", "text": "There are a number of interesting interpretations and characterisations that we can provide for the classification functions (2). These interpretations and characterisations will provide different insights into the regulated laplac core Q\u03b2 and the classification functions (2)."}, {"heading": "4.1 Discrete-time random walk interpretation", "text": "The regulated laplac core Q\u03b2 = (I + \u03b2L) \u2212 1 can be interpreted as the general transition matrix of a random walk on the similarity graph G with a geometrically distributed number of steps. Consider a Markov chain whose states are our data points, and the probability of transitions between different states is proportional to the corresponding entries of the similarity matrix A: p-ij = \u03c4aij, i, j = 1,.., N, i-6 = j, (8), where \u03c4 > 0 is a sufficiently small parameter. Then, the diagonal elements of the transition matrix P-ij = (p-ij) arep-ii = 1-ig 6 = i\u03c4aij, i = 1,..., N (9) Inria result."}, {"heading": "Semi-supervised Learning with Regularized Laplacian 7", "text": "(10) The matrix P (10) determines a random path on G that differs from the \"standard\" defined by (5) and is related to the heat core PageRank (4). In contrast to (5), the transition matrix (10) is symmetrical for each undirected graph; in general, it has an unequal diagonal. It is interesting to note that P \u00b2 corresponds to the weight matrix W, which is used for the transformation of Section 3.2. Consider a sequence of independent Bernoulli experiments indexed by 0, 1, 2,... Suppose that the number of steps, K, in a random gear is equal to the experimental number of the first success. And leave Xk the state of the Markov chain at step k. Then K is distributed geometrically: Pr {K = k} = q."}, {"heading": "4.2 Continuous-time random walk interpretation", "text": "Consider the differential equation H (t) = \u2212 LH (t), (11) with the initial condition H (0) = I. Consider also the standard continuous random path, which exponentially distributes time in the node k with the expected duration 1 / dk and moves to a new node l with the probability akl / dk after the exponentially distributed time progression. Then, the solution hij (t) = exp (\u2212 tL) of the differential equation (11) can be interpreted as the probability to find the default continuous random path in the node j starting from the node i. If we take the laplace transformation of (11), we get H (s) = (sI + L) \u2212 1 = s \u2212 1 (I + s \u2212 1L) \u2212 1. (12) Thus, the classification function (2) can be interpreted as a laplace transformation divided by 1 / s, or equal to the random function of the 8k + the initial component \u2212 1 in the component \u2212 net."}, {"heading": "8 K. Avrachenkov & P. Chebotarev & A. Mishenin", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.3 Proximity and distance properties", "text": "As before, Q\u03b2 = (q \u03b2 ij) N \u00b7 N = \u03b2 q \u03b2 \u03b2 \u03b2 \u03b2 \u03b2 \u03b2 \u03b2 (\u03b2 L) \u2212 1 of (2).Q\u03b2 determines a positive 1 proximity measurement [14] s (i, j): = q \u03b2 ij, i.e., it fulfils [13] the following conditions: (1) for each i-V, \u2211 k-V q \u03b2 ik = 1 and (2) for each i, j, k-V, q\u03b2ji + q \u03b2 \u03b2 - q-ik \u2264 q \u03b2 jj with a strict inequality whenever i = k and i = j (the triangular inequality for proximities).Hence, [14] the following two important properties result: (a) q\u03b2ii > q \u03b2 ij for all i, j for all i, j \u00b2 V for all i = j (egocentric property); (b) the inequality for each d (the inequality for proximites)."}, {"heading": "4.4 Matrix forest characterization", "text": "According to the matrix-forest theorem [13, 1], each entry q\u03b2ij of Q\u03b2 corresponds to the specific weight of the rooted forests connecting the node i to the node j in the weighted diagram G. More precisely, q\u03b2ij = F \u03b2 i j / F \u03b2, where F\u03b2 is the total weight of all rooted forests of G, where F\u03b2i j is the total weight of those who have rooted the node i in a tree in j. Here, the \u03b2-weight of a forest stands for the product of its edge weights, each multiplied by \u03b2. Let us mention a closely related interpretation of the regulated laplac core Q\u03b2 in relation to the dissemination of information [11]. Let us note that an information unit (an idea) must be transmitted by G. A plan of information transmission is a rooted forest F in G: The information unit is first injected into the roots of Q\u03b2; after each choice, the Q\u03b2 is equal to that point."}, {"heading": "Semi-supervised Learning with Regularized Laplacian 9", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.5 Statistical characterization", "text": "Suppose each data point (node) has a value parameter vi and a series of paired comparisons rij between the points is performed. Suppose the result of i in a comparison with j follows the linear statistical scheff\u00e9 model [32] E (rij) = vi \u2212 vj, (13) where E (\u00b7) is the mathematical expectation. The matrix form of (13) is applied to an experiment isE (r) = Xv, where v = (v1,., vN) T, and r is the vector of the comparison results, where X is the incidence matrix (design matrix, in terms of statistics): if the kth element of r is a comparison result of i faced with j, then, in accordance with (13), xki = 1, xkj = 1, and xkl = 0 for l 6."}, {"heading": "10 K. Avrachenkov & P. Chebotarev & A. Mishenin", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "5 Limiting cases", "text": "Let us analyze the formula (2) in two limiting cases: \u03b2 \u2192 0 and \u03b2 \u2192. If \u03b2 \u2192 0, then we have the method k = (I \u2212 \u03b2L) Y \u0445 k + o (\u03b2). Thus, the method for very small values of \u03b2, the method resembles the next neighboring method with the weight matrix W = I \u2212 \u03b2L. If there are many points located more than one jump away from a designated point, the method cannot produce a good classification with very small values of \u03b2. This is demonstrated by the numerical experiments in Section 6. Now let us consider the other case \u03b2 \u00b2. We will use the extension of the Blackwell series [7, 31] for the resolute operator (L + L) \u2212 1 with the very small values of \u03b2. (I + \u03b2L) \u2212 1 = Procedure (I + L) \u2212 1 = Procedure (1N 11T + H \u2212 H2 +...), (16) where H = (L + 1N 11 T) the classes are generalized."}, {"heading": "Semi-supervised Learning with Regularized Laplacian 11", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "6 Numerical methods and examples", "text": "In fact, most of us are able to play by the rules we have set ourselves, \"he said in an interview with\" Welt am Sonntag. \""}, {"heading": "12 K. Avrachenkov & P. Chebotarev & A. Mishenin", "text": "Horizontal line in the figure 1 (a) corresponds to the PageRank method with the best choice of control parameters or the restart probability in the context of PageRank. Since the Regularized Laplacian method is based on the figure 1, we also compare it in the figure 1 (b) with the three thermal core methods which are derived from the variations of the graph. Specifically we look at the three time kernels based on different Laplacian types. (b) With the three thermal core types of the Graphier types types types types types types types types types types types types types types types types types types types types types types types types types types types types types types types types types types types types types types types types types types types types types types types types types types types types types types types types types types types types types types types types types types types types types types types types types types types types types types types types types types types types types types types types types types types types types types types types types types types types types types types types types types types types types types types types types types types types types types types types types types types types types types types types types types types types types types types types types types types types types types types types types types types types types types types types types types types types types types types types types types types types types types types types types types types types types types types types types types types types types types types types types types types types types types types types types types types types types types types types"}, {"heading": "Semi-supervised Learning with Regularized Laplacian 13", "text": "RR n \u00b0 8765"}, {"heading": "14 K. Avrachenkov & P. Chebotarev & A. Mishenin", "text": "Next, we look at the second set of data consisting of Wikipedia mathematical articles. This data set is derived from the English language Wikipedia snapshot (dump) dated January 30, 20102. The similarity curve is constructed by a slight modification of the hypertext graph. Each Wikipedia article typically contains links to other Wikipedia articles used to explain specific terms and concepts. Thus, Wikipedia forms a graph whose nodes represent articles and whose edges represent hypertext interarticle links. Links to specific pages (categories, portals, etc.) were ignored. In the current experiment, we did not use the information about the direction of the links, so the similarity curve in our experiments is unequal."}, {"heading": "Semi-supervised Learning with Regularized Laplacian 15", "text": "It can be argued that \u03c321 is large by nature and the paired comparisons between points can be made with much greater certainty, and that \u03c322 is therefore small. Hence, a statistical explanation is given as to why it is good to take relatively large values for the parameter \u03b2 in the laplac regularized method."}, {"heading": "7 Conclusions", "text": "The method allows for both linear algebraic and optimisation formulas, and the optimisation formulation seems to be particularly suitable for parallel implementation. We have provided various interpretations and removal properties of the state-of-the-art regulated Laplacian Graph Core. We have also shown that the method is related to the linear Scheff\u00e9 model, which has been tested and compared with the other state-of-the-art partially monitored learning methods on two datasets, and the results from the two datasets are consistent. In particular, we can conclude that the performance of the Regularised Laplacian Method is comparable to the PageRank-based method and outperforms the related methods based on heat cores in terms of robustness. Several interesting research directions remain open for investigation. It will be interesting to compare the Laplacian Method with the other semi-monitored methods on a very large dataset."}, {"heading": "16 K. Avrachenkov & P. Chebotarev & A. Mishenin", "text": "[2] In Proceedings of IEEE FOCS 2006, p. 475-486. [3] Avrachenkov, K., Dobrynin, V., Nemirovsky, D., Pham, S. K., and Smirnova, E. (2008). [4] Avrachenkov, D., Pham, S. K., and Smirnova, S., S., S., S., S., S., S., S., S., S., S., S., S., S., S., S., S., S., S., S., S., S., S., S., S., S. K., and Smirnova, S., S., S., S., S., S., S., S., S., S., S., S., S., S., S., S., S., S., S., S., S., S., S., S., S., S., S., S., S., S., S. K., S., S., S., S., S., S., S., S., S., S., S., S., S., S., S., S., S.,.,.,.,.,.,.,.,.,.,.,.,.,.,.,.,.,.,.,.,.,.,.,.,.,.,.,.,.,.,.....,.,.,.,.,.,.,.,.,.,.,.,.,.,.,.,.,.....,.,.,.,.,.,.,.,.,.,.,.,.,.,.,.,.,.,.,.,.,.,.,.,.,.,.,.,.,.,.,.,.,.....,.,.,.,.,.,.,.,.,.,.,.,.,.,.,.,.,.,.,.,.,.,.,.,.,.,.,.,.,.,.,.,.,.,.,.,.,.,.,.,.,.,.,.,.,.,.,.,.,.,.,.,."}, {"heading": "Semi-supervised Learning with Regularized Laplacian 17", "text": "[18] Chung, F., and Yau, S. T. (2000): \"Discrete Green's functions.\" Journal of Combinatorial Theory, Series A, 91 (1), pp. 191-214. [19] Chung, F. (2007): \"The heat kernel as the pagerank of a graph.\" PNAS, 105 (50), pp.19735- 19740. [20] Chung, F. (2009): \"A local graph partitioning algorithm using heat kernel pagerank.\" In the Proceedings of WAW 2009, LNCS 5427, pp.62-740. [21] Critchley, F. (1988): \"The inner-product and squareddistance matrices.\" \"Linear Algebra and its Applications, 105, pp.91-107. Deza, M., Deza."}, {"heading": "18 K. Avrachenkov & P. Chebotarev & A. Mishenin", "text": "[35] Zhou, D., and Burges, C. J. C. (2007) \"Spectral Clustering and Transductive Learning with Multiple Views.\" In Proceedings of ICML 2007, pp. 1159-1166. [36] Zhou, D., Bousquet, O., Navin Lal, T., Weston, J., Sch\u00f6lkopf, B. (2004) \"Learning with Local and Global Consistency.\" In: Advances in Neural Information Processing Systems, 16, pp. 321-328. [37] Zhu, X., Ghahramani, Z., and Lafferty, J. (2003) \"Semi-supervised learning using Gaussian fields and harmonic functions.\" In Proceedings of ICML 2003, Vol. 3, pp. 912-919. [38] Zhu, X. (2005) \"Semi-supervised learning literature survey.\""}, {"heading": "Semi-supervised Learning with Regularized Laplacian 19", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "Contents", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "1 Introduction 3", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "2 Notations and method formulation 4", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "3 Related approaches 5", "text": "3.1 Relationship with heating cores.............................................................................................................."}, {"heading": "4 Properties and interpretations of the Regularized Laplacian method 6", "text": "4.1 Interpretation of random path in discrete time......................................................................................................................................................................................................................................................................................................................................"}, {"heading": "5 Limiting cases 10", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "6 Numerical methods and examples 11", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "7 Conclusions 15", "text": "RR n \u00b0 8765RESEARCH CENTRE SOPHIA ANTIPOLIS - M\u00c9DITERRAN\u00c9E2004 route des Lucioles - BP 93 06902 Sophia Antipolis CedexPublisher Inria Domaine de Voluceau - Rocquencourt BP 105 - 78153 Le Chesnay Cedex inria.frISSN 0249-6399This image \"logo-inria.png\" is available in \"png\" format at: http: / / arxiv.org / ps / 1508.04906v1This image \"pagei.png\" is available in \"png\" format at: http: / / arxiv.org / ps / 1508.04906v1This image \"rrpage1.png\" is available in \"png\" format at: http: / / arxiv.org / ps / 1508.04906v1"}], "references": [{"title": "Spanning forests of a digraph and their applications", "author": ["R.P. Agaev", "P.Y. Chebotarev"], "venue": "Automation and Remote Control,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2001}, {"title": "Local graph partitioning using pagerank vectors", "author": ["R. Andersen", "F. Chung", "K. Lang"], "venue": "In Proceedings of IEEE FOCS", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2006}, {"title": "Pagerank based clustering of hypertext document collections", "author": ["K. Avrachenkov", "V. Dobrynin", "D. Nemirovsky", "S.K. Pham", "E. Smirnova"], "venue": "In Proceedings of ACM SI- GIR", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2008}, {"title": "Generalized optimization framework for graph-based semi-supervised learning", "author": ["K. Avrachenkov", "P. Gon\u00e7alves", "A. Mishenin", "M. Sokol"], "venue": "In Proceedings of SIAM Conference on Data Mining (SDM", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2012}, {"title": "On the choice of kernel and labelled data in semi-supervised learning methods", "author": ["K. Avrachenkov", "P. Gon\u00e7alves", "M. Sokol"], "venue": "WAW 2013, also LNCS,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2013}, {"title": "Beta Current Flow Centrality for Weighted Networks", "author": ["K. Avrachenkov", "V. Mazalov", "B. Tsynguev"], "venue": "In Proceedings of the 4th International Conference on Computational Social Networks (CSoNet 2015),", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2015}, {"title": "Discrete dynamic programming", "author": ["D. Blackwell"], "venue": "The Annals of Mathematical Statistics,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 1962}, {"title": "Semi-supervised classification from discriminative random walks", "author": ["J. Callut", "K. Fran\u00e7oisse", "M. Saerens", "P. Dupont"], "venue": "In Machine Learning and Knowledge Discovery in Databases European Conference,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2008}, {"title": "Aggregation of preferences by the generalized row sum method", "author": ["P.Y. Chebotarev"], "venue": "Mathematical Social Sciences,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 1994}, {"title": "Spanning forests and the golden ratio", "author": ["P. Chebotarev"], "venue": "Discrete Applied Mathematics,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2008}, {"title": "The graph bottleneck identity", "author": ["P. Chebotarev"], "venue": "Advances in Applied Mathematics,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2011}, {"title": "The matrix-forest theorem and measuring relations in small social groups", "author": ["Chebotarev", "P. Yu", "E.V. Shamis"], "venue": "Automation and Remote Control,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 1997}, {"title": "On a duality between metrics and \u03a3proximities", "author": ["Chebotarev", "P. Yu", "E.V. Shamis"], "venue": "Automation and Remote Control,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 1998}, {"title": "On proximity measures for graph vertices", "author": ["Chebotarev", "P. Yu", "E.V. Shamis"], "venue": "Automation and Remote Control,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 1998}, {"title": "The forest metrics of a graph and their properties", "author": ["Chebotarev", "P. Yu", "E.V. Shamis"], "venue": "Automation and Remote Control,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2000}, {"title": "Coverings, heat kernels and spanning trees", "author": ["F. Chung", "S.T. Yau"], "venue": "Electronic Journal of Combinatorics,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 1999}, {"title": "Discrete Green\u2019s functions", "author": ["F. Chung", "S.T. Yau"], "venue": "Journal of Combinatorial Theory, Series A,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2000}, {"title": "The heat kernel as the pagerank of a graph", "author": ["F. Chung"], "venue": "PNAS, 105(50),", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2007}, {"title": "A local graph partitioning algorithm using heat kernel pagerank", "author": ["F. Chung"], "venue": "In Proceedings of WAW 2009,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2009}, {"title": "On certain linear mappings between inner-product and squareddistance matrices", "author": ["F. Critchley"], "venue": "Linear Algebra and its Applications,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 1988}, {"title": "Geometry of Cuts and Metrics, volume 15 of Algorithms and Combinatorics", "author": ["M.M. Deza", "M. Laurent"], "venue": null, "citeRegEx": "22", "shortCiteRegEx": "22", "year": 1997}, {"title": "An experimental investigation of kernels on graphs for collaborative recommendation and semisupervised classification", "author": ["F. Fouss", "K. Francoisse", "L. Yen", "Pirotte A", "M. Saerens"], "venue": "Neural Networks,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2012}, {"title": "New ridge parameters for ridge regression", "author": ["A.V. Dorugade"], "venue": "Journal of the Association of Arab Universities for Basic and Applied Sciences,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2014}, {"title": "An experimental investigation of graph kernels on a collaborative recommendation task", "author": ["F. Fouss", "L. Yen", "A. Pirotte", "M. Saerens"], "venue": "In Sixth International Conference on Data Mining (ICDM\u201906),", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2006}, {"title": "Distances in weighted trees and group inverse of Laplacian matrices", "author": ["S.J. Kirkland", "M. Neumann", "B.L. Shader"], "venue": "SIAM J. Matrix Anal. Appl.,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 1997}, {"title": "The Stanford GraphBase: a platform for combinatorial computing", "author": ["D.E. Knuth"], "venue": null, "citeRegEx": "27", "shortCiteRegEx": "27", "year": 1993}, {"title": "Diffusion kernels on graphs and other discrete input spaces", "author": ["R.I. Kondor", "J. Lafferty"], "venue": "In Proceedings of ICML,", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2002}, {"title": "On some ridge regression estimators: An empirical comparisons", "author": ["G. Muniz", "B.M.G. Kibria"], "venue": "Communications in Statistics \u2013 Simulation and Computation,", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2009}, {"title": "Finding and evaluating community structure in networks", "author": ["M.E.J. Newman", "M. Girvan"], "venue": "Phys. Rev. E,", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2004}, {"title": "Markov decision processes: Discrete stochastic dynamic programming", "author": ["M.L. Puterman"], "venue": null, "citeRegEx": "31", "shortCiteRegEx": "31", "year": 1994}, {"title": "An analysis of variance for paired comparisons", "author": ["H. Scheff\u00e9"], "venue": "Journal of the American Statistical Association,", "citeRegEx": "32", "shortCiteRegEx": "32", "year": 1952}, {"title": "Kernels and regularization of graphs", "author": ["A.J. Smola", "R.I. Kondor"], "venue": "In Proceedings of the 16th Annual Conference on Learning Theory,", "citeRegEx": "33", "shortCiteRegEx": "33", "year": 2003}, {"title": "A family of dissimilarity measures between nodes generalizing both the shortest-path and the commutetime distances", "author": ["L. Yen", "M. Saerens", "A. Mantrach", "M. Shimbo"], "venue": "In 14th ACM SIGKDD Intern. Conf. on Knowledge Discovery and Data Mining, pp. 785\u2013793", "citeRegEx": "34", "shortCiteRegEx": "34", "year": 2008}, {"title": "Spectral clustering and transductive learning with multiple views", "author": ["D. Zhou", "C.J.C. Burges"], "venue": "In Proceedings of ICML", "citeRegEx": "35", "shortCiteRegEx": "35", "year": 2007}, {"title": "Learning with local and global consistency", "author": ["D. Zhou", "O. Bousquet", "T. Navin Lal", "J. Weston", "B. Sch\u00f6lkopf"], "venue": "Advances in Neural Information Processing Systems,", "citeRegEx": "36", "shortCiteRegEx": "36", "year": 2004}, {"title": "Semi-supervised learning using Gaussian fields and harmonic functions", "author": ["X. Zhu", "Z. Ghahramani", "J. Lafferty"], "venue": "In Proceedings of ICML 2003,", "citeRegEx": "37", "shortCiteRegEx": "37", "year": 2003}, {"title": "Semi-supervised learning literature survey", "author": ["X. Zhu"], "venue": "University of Wisconsin- Madison Research Report TR 1530", "citeRegEx": "38", "shortCiteRegEx": "38", "year": 2005}], "referenceMentions": [{"referenceID": 35, "context": "The work [37] seems to be the first work where the graph-based semi-supervised learning was introduced.", "startOffset": 9, "endOffset": 13}, {"referenceID": 35, "context": "The authors of [37] formulated the semi-supervised learning method as a constrained optimization problem involving graph Laplacian.", "startOffset": 15, "endOffset": 19}, {"referenceID": 34, "context": "Then, in [36, 35] the authors proposed optimization formulations based on several variations of the graph Laplacian.", "startOffset": 9, "endOffset": 17}, {"referenceID": 33, "context": "Then, in [36, 35] the authors proposed optimization formulations based on several variations of the graph Laplacian.", "startOffset": 9, "endOffset": 17}, {"referenceID": 3, "context": "In [4] a unifying optimization framework was proposed which gives as particular cases the methods of [35] and [36].", "startOffset": 3, "endOffset": 6}, {"referenceID": 33, "context": "In [4] a unifying optimization framework was proposed which gives as particular cases the methods of [35] and [36].", "startOffset": 101, "endOffset": 105}, {"referenceID": 34, "context": "In [4] a unifying optimization framework was proposed which gives as particular cases the methods of [35] and [36].", "startOffset": 110, "endOffset": 114}, {"referenceID": 3, "context": "In addition, the general framework in [4] gives as a particular case an interesting PageRank based method, which provides robust classification with respect to the choice of the labelled points [3, 5].", "startOffset": 38, "endOffset": 41}, {"referenceID": 2, "context": "In addition, the general framework in [4] gives as a particular case an interesting PageRank based method, which provides robust classification with respect to the choice of the labelled points [3, 5].", "startOffset": 194, "endOffset": 200}, {"referenceID": 4, "context": "In addition, the general framework in [4] gives as a particular case an interesting PageRank based method, which provides robust classification with respect to the choice of the labelled points [3, 5].", "startOffset": 194, "endOffset": 200}, {"referenceID": 1, "context": "We would like to note that the local graph partitioning problem [2, 20] can be related to graph-based semi-supervised learning.", "startOffset": 64, "endOffset": 71}, {"referenceID": 18, "context": "We would like to note that the local graph partitioning problem [2, 20] can be related to graph-based semi-supervised learning.", "startOffset": 64, "endOffset": 71}, {"referenceID": 21, "context": "An interested reader can find more details about various semi-supervised learning methods in the surveys and books [9, 23, 38].", "startOffset": 115, "endOffset": 126}, {"referenceID": 36, "context": "An interested reader can find more details about various semi-supervised learning methods in the surveys and books [9, 23, 38].", "startOffset": 115, "endOffset": 126}, {"referenceID": 11, "context": "To the best of our knowledge, the idea of using Regularized Laplacian and its kernel for measuring proximity in graphs and application to mathematical sociology goes back to the works [13, 15].", "startOffset": 184, "endOffset": 192}, {"referenceID": 13, "context": "To the best of our knowledge, the idea of using Regularized Laplacian and its kernel for measuring proximity in graphs and application to mathematical sociology goes back to the works [13, 15].", "startOffset": 184, "endOffset": 192}, {"referenceID": 21, "context": "In [23] the authors compared experimentally many graph-based semi-supervised learning methods on several datasets and their conclusion was that the semisupervised learning method based on the Regularized Laplacian kernel demonstrates one of the best performances on nearly all datasets.", "startOffset": 3, "endOffset": 7}, {"referenceID": 7, "context": "In [8] the authors studied a semi-supervised learning method based on the Normalized Laplacian graph kernel which also shows good performance.", "startOffset": 3, "endOffset": 6}, {"referenceID": 35, "context": "In fact, the Regularized Laplacian method can be regarded as a Lagrangian relaxation of the method proposed in [37].", "startOffset": 111, "endOffset": 115}, {"referenceID": 35, "context": "Of course, this is a more flexible formulation, since by choosing an appropriate value for the Lagrange multiplier one can always retrieve the method of [37] as a particular case.", "startOffset": 153, "endOffset": 157}, {"referenceID": 26, "context": "(2) The matrix Q\u03b2 = (I + \u03b2L) \u22121 is known as Regularized Laplacian kernel of the graph [28, 33] and can be related to the matrix forest theorems [13, 1] and stochastic matrices [1].", "startOffset": 86, "endOffset": 94}, {"referenceID": 31, "context": "(2) The matrix Q\u03b2 = (I + \u03b2L) \u22121 is known as Regularized Laplacian kernel of the graph [28, 33] and can be related to the matrix forest theorems [13, 1] and stochastic matrices [1].", "startOffset": 86, "endOffset": 94}, {"referenceID": 11, "context": "(2) The matrix Q\u03b2 = (I + \u03b2L) \u22121 is known as Regularized Laplacian kernel of the graph [28, 33] and can be related to the matrix forest theorems [13, 1] and stochastic matrices [1].", "startOffset": 144, "endOffset": 151}, {"referenceID": 0, "context": "(2) The matrix Q\u03b2 = (I + \u03b2L) \u22121 is known as Regularized Laplacian kernel of the graph [28, 33] and can be related to the matrix forest theorems [13, 1] and stochastic matrices [1].", "startOffset": 144, "endOffset": 151}, {"referenceID": 0, "context": "(2) The matrix Q\u03b2 = (I + \u03b2L) \u22121 is known as Regularized Laplacian kernel of the graph [28, 33] and can be related to the matrix forest theorems [13, 1] and stochastic matrices [1].", "startOffset": 176, "endOffset": 179}, {"referenceID": 15, "context": "1 Relation to heat kernels The authors of [17, 18] first introduced and studied the properties of the heat kernel based on the normalized Laplacian.", "startOffset": 42, "endOffset": 50}, {"referenceID": 16, "context": "1 Relation to heat kernels The authors of [17, 18] first introduced and studied the properties of the heat kernel based on the normalized Laplacian.", "startOffset": 42, "endOffset": 50}, {"referenceID": 17, "context": "Then, in [19] the PageRank heat kernel was introduced", "startOffset": 9, "endOffset": 13}, {"referenceID": 18, "context": "In [20] the PageRank heat kernel was applied to local graph partitioning.", "startOffset": 3, "endOffset": 7}, {"referenceID": 26, "context": "In [28] the heat kernel based on the standard Laplacian H(t) = exp(\u2212tL), (6)", "startOffset": 3, "endOffset": 7}, {"referenceID": 35, "context": "Then, in [37] the authors proposed a semi-supervised learning method based on the solution of a heat diffusion equation with Dirichlet boundary conditions.", "startOffset": 9, "endOffset": 13}, {"referenceID": 35, "context": "Equivalently, the method of [37] can be viewed as the minimization of the second term in (1) with the values of the classification functions F\u2217k fixed on the labelled points.", "startOffset": 28, "endOffset": 32}, {"referenceID": 35, "context": "Thus, the proposed approach (1) is more general as it can be viewed as a Lagrangian relaxation of [37].", "startOffset": 98, "endOffset": 102}, {"referenceID": 35, "context": "The results of the method in [37] can be retrieved with a particular choice of the regularization parameter.", "startOffset": 29, "endOffset": 33}, {"referenceID": 3, "context": "2 Relation to the generalized semi-supervised learning method In [4] the authors proposed a generalized optimization framework for graph based semi-supervised learning methods", "startOffset": 65, "endOffset": 68}, {"referenceID": 33, "context": "In particular, with \u03c3 = 1 we retrieve the transductive semi-supervised learning method [35], with \u03c3 = 1/2 we retrieve the semi-supervised learning with local and global consistency [36] and with \u03c3 = 0 we retrieve the PageRank based method [3].", "startOffset": 87, "endOffset": 91}, {"referenceID": 34, "context": "In particular, with \u03c3 = 1 we retrieve the transductive semi-supervised learning method [35], with \u03c3 = 1/2 we retrieve the semi-supervised learning with local and global consistency [36] and with \u03c3 = 0 we retrieve the PageRank based method [3].", "startOffset": 181, "endOffset": 185}, {"referenceID": 2, "context": "In particular, with \u03c3 = 1 we retrieve the transductive semi-supervised learning method [35], with \u03c3 = 1/2 we retrieve the semi-supervised learning with local and global consistency [36] and with \u03c3 = 0 we retrieve the PageRank based method [3].", "startOffset": 239, "endOffset": 242}, {"referenceID": 12, "context": "Q\u03b2 determines a positive 1-proximity measure [14] s(i, j) := q \u03b2 ij , i.", "startOffset": 45, "endOffset": 49}, {"referenceID": 11, "context": ", it satisfies [13] the following conditions: (1) for any i \u2208 V, \u2211 k\u2208V q \u03b2 ik = 1 and (2) for any i, j, k \u2208 V, q ji + q \u03b2 jk \u2212 q \u03b2 ik \u2264 q \u03b2 jj with a strict inequality whenever i = k and i 6= j (the triangle inequality for proximities).", "startOffset": 15, "endOffset": 19}, {"referenceID": 12, "context": "This implies [14] the following two important properties: (a) q ii > q \u03b2 ij for all i, j \u2208 V such that i 6= j (egocentrism property); (b) \u03c1\u03b2ij := \u03b2(q \u03b2 ii + q \u03b2 jj \u2212 q \u03b2 ij \u2212 q \u03b2 ji) is 1 a distance on V.", "startOffset": 13, "endOffset": 17}, {"referenceID": 14, "context": "The distances \u03c1\u03b2ij have a twofold connection with the resistance distance \u03c1\u0303ij on G [16].", "startOffset": 84, "endOffset": 88}, {"referenceID": 11, "context": "An interested reader can find more properties of the proximity measures determined by Q\u03b2 in [13].", "startOffset": 92, "endOffset": 96}, {"referenceID": 10, "context": "Furthermore, every Q\u03b2, \u03b2 > 0 determines a transitional measure on V, which means [12] that: q ij q \u03b2 jk \u2264 q \u03b2 ik q \u03b2 jj for all i, j, k \u2208 V with q \u03b2 ij q \u03b2 jk = q \u03b2 ik q \u03b2 jj if and only if every path in G from i to k visits j.", "startOffset": 81, "endOffset": 85}, {"referenceID": 11, "context": "4 Matrix forest characterization By the matrix forest theorem [13, 1], each entry q ij of Q\u03b2 is equal to the specific weight of the spanning rooted forests that connect node i to node j in the weighted graph G whose combinatorial Laplacian is L.", "startOffset": 62, "endOffset": 69}, {"referenceID": 0, "context": "4 Matrix forest characterization By the matrix forest theorem [13, 1], each entry q ij of Q\u03b2 is equal to the specific weight of the spanning rooted forests that connect node i to node j in the weighted graph G whose combinatorial Laplacian is L.", "startOffset": 62, "endOffset": 69}, {"referenceID": 9, "context": "Let us mention a closely related interpretation of the Regularized Laplacian kernel Q\u03b2 in terms of information dissemination [11].", "startOffset": 125, "endOffset": 129}, {"referenceID": 19, "context": "the cosine law [21] and the inverse covariance mapping [22, Section 5.", "startOffset": 15, "endOffset": 19}, {"referenceID": 30, "context": "Let the result of i in a comparison with j obey the Scheff\u00e9 linear statistical model [32]", "startOffset": 85, "endOffset": 89}, {"referenceID": 22, "context": ", [24, 29] and the references therein.", "startOffset": 2, "endOffset": 10}, {"referenceID": 27, "context": ", [24, 29] and the references therein.", "startOffset": 2, "endOffset": 10}, {"referenceID": 6, "context": "We shall employ the Blackwell series expansion [7, 31] for the resolvent operator (\u03bbI + L) with \u03bb = 1/\u03b2 (I + \u03b2L) = \u03bb(\u03bbI + L) = \u03bb (", "startOffset": 47, "endOffset": 54}, {"referenceID": 29, "context": "We shall employ the Blackwell series expansion [7, 31] for the resolvent operator (\u03bbI + L) with \u03bb = 1/\u03b2 (I + \u03b2L) = \u03bb(\u03bbI + L) = \u03bb (", "startOffset": 47, "endOffset": 54}, {"referenceID": 24, "context": "An interpretation of H in terms of spanning forests can be found in [15, Theorem 3]; see also [26].", "startOffset": 94, "endOffset": 98}, {"referenceID": 5, "context": "Similarly to the power iteration method described in [6], we can write", "startOffset": 53, "endOffset": 56}, {"referenceID": 25, "context": "The network of the interactions of Les Miserables characters has been compiled by Knuth [27].", "startOffset": 88, "endOffset": 92}, {"referenceID": 28, "context": "Using the betweenness based algorithm of Newman and Girvan [30] we obtain 6 clusters which can be identified with the main characters: Valjean (17), Myriel (10), Gavroche (18), Cosette (10), Thenardier (12), Fantine (10), where in brackets we give the number of nodes in the respective cluster.", "startOffset": 59, "endOffset": 63}, {"referenceID": 3, "context": "In [4, 5] it was observed that the PageRank based semi-supervised method (obtained by taking \u03c3 = 0 in (7)) is the only method among a large family of semi-supervised methods which is robust to the choice of the labelled data [3, 4, 5].", "startOffset": 3, "endOffset": 9}, {"referenceID": 4, "context": "In [4, 5] it was observed that the PageRank based semi-supervised method (obtained by taking \u03c3 = 0 in (7)) is the only method among a large family of semi-supervised methods which is robust to the choice of the labelled data [3, 4, 5].", "startOffset": 3, "endOffset": 9}, {"referenceID": 2, "context": "In [4, 5] it was observed that the PageRank based semi-supervised method (obtained by taking \u03c3 = 0 in (7)) is the only method among a large family of semi-supervised methods which is robust to the choice of the labelled data [3, 4, 5].", "startOffset": 225, "endOffset": 234}, {"referenceID": 3, "context": "In [4, 5] it was observed that the PageRank based semi-supervised method (obtained by taking \u03c3 = 0 in (7)) is the only method among a large family of semi-supervised methods which is robust to the choice of the labelled data [3, 4, 5].", "startOffset": 225, "endOffset": 234}, {"referenceID": 4, "context": "In [4, 5] it was observed that the PageRank based semi-supervised method (obtained by taking \u03c3 = 0 in (7)) is the only method among a large family of semi-supervised methods which is robust to the choice of the labelled data [3, 4, 5].", "startOffset": 225, "endOffset": 234}, {"referenceID": 4, "context": "It was observed in [5] that taking labelled data points with large (weighted) degree is typically beneficial for the semi-supervised learning methods.", "startOffset": 19, "endOffset": 22}], "year": 2015, "abstractText": "We study a semi-supervised learning method based on the similarity graph and Regularized Laplacian. We give convenient optimization formulation of the Regularized Laplacian method and establish its various properties. In particular, we show that the kernel of the method can be interpreted in terms of discrete and continuous time random walks and possesses several important properties of proximity measures. Both optimization and linear algebra methods can be used for efficient computation of the classification functions. We demonstrate on numerical examples that the Regularized Laplacian method is competitive with respect to the other state of the art semi-supervised learning methods. Key-words: Semi-supervised learning, Graph-based learning, Regularized Laplacian, Proximity measure, Wikipedia article classification \u2217 Corresponding author. K. Avrachenkov is with Inria Sophia Antipolis, 2004 Route des Lucioles, 06902, Sophia Antipolis, France k.avrachenkov@inria.fr \u2020 P. Chebotarev is with Trapeznikov Institute of Control Sciences of the Russian Academy of Sciences, 65 Profsoyuznaya Str., Moscow, 117997, Russia \u2021 A. Mishenin is with St. Petersburg State University, Faculty of Applied Mathematics and Control Processes, Peterhof, 198504, Russia \u00a7 This work was partially supported by Campus France, Alcatel-Lucent Inria Joint Lab, EU Project Congas FP7-ICT-2011-8-317672, and RFBR grant No. 13-07-00990. L\u2019Apprentissage Semi-supervis\u00e9 avec Laplacian R\u00e9gularis\u00e9 R\u00e9sum\u00e9 : Nous \u00e9tudions une m\u00e9thode d\u2019apprentissage semi-supervis\u00e9, bas\u00e9 sur le graphe de similarit\u00e9 et Laplacian r\u00e9gularis\u00e9. Nous formalisons la m\u00e9thode comme un probl\u00e8me d\u2019optimisation convexe et quadratique et nous \u00e9tablissons ses diverses propri\u00e9t\u00e9s. En particulier, nous montrons que le noyau de la m\u00e9thode peut \u00eatre interpr\u00e9t\u00e9 en termes des marches al\u00e9atoires en temps discret et continu et poss\u00e8de plusieurs propri\u00e9t\u00e9s importantes des mesures de proximit\u00e9. Les techniques d\u2019optimisation ainsi que les techniques d\u2019alg\u00e9bre lin\u00e9aire peuvent \u00eatre utilis\u00e9 pour un calcul efficace des fonctions de classification. Nous d\u00e9montrons sur des exemples num\u00e9riques que la m\u00e9thode de Laplacian r\u00e9gularis\u00e9 est concurrentiel par rapport aux autres \u00e9tat de l\u2019art m\u00e9thodes d\u2019apprentissage semi-supervis\u00e9. Mots-cl\u00e9s : Apprentissage Semi-supervis\u00e9, Apprentissage bas\u00e9 sur le graphe de similarit\u00e9, Laplacian r\u00e9gularis\u00e9, mesure de proximit\u00e9, classification des articles Wikipedia Semi-supervised Learning with Regularized Laplacian 3", "creator": "LaTeX with hyperref package"}}}