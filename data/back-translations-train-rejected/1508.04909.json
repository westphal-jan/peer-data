{"id": "1508.04909", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "20-Aug-2015", "title": "Histogram of gradients of Time-Frequency Representations for Audio scene detection", "abstract": "This paper addresses the problem of audio scenes classification and contributes to the state of the art by proposing a novel feature. We build this feature by considering histogram of gradients (HOG) of time-frequency representation of an audio scene. Contrarily to classical audio features like MFCC, we make the hypothesis that histogram of gradients are able to encode some relevant informations in a time-frequency {representation:} namely, the local direction of variation (in time and frequency) of the signal spectral power. In addition, in order to gain more invariance and robustness, histogram of gradients are locally pooled. We have evaluated the relevance of {the novel feature} by comparing its performances with state-of-the-art competitors, on several datasets, including a novel one that we provide, as part of our contribution. This dataset, that we make publicly available, involves $19$ classes and contains about $900$ minutes of audio scene recording. We thus believe that it may be the next standard dataset for evaluating audio scene classification algorithms. Our comparison results clearly show that our HOG-based features outperform its competitors", "histories": [["v1", "Thu, 20 Aug 2015 08:07:10 GMT  (4726kb,D)", "http://arxiv.org/abs/1508.04909v1", null]], "reviews": [], "SUBJECTS": "cs.SD cs.LG", "authors": ["alain rakotomamonjy", "gilles gasso"], "accepted": false, "id": "1508.04909"}, "pdf": {"name": "1508.04909.pdf", "metadata": {"source": "CRF", "title": "Histogram of Gradients of Time-Frequency Representations for Audio Scene Detection", "authors": ["A. Rakotomamonjy", "G. Gasso"], "emails": ["alain.rakoto@insa-rouen.fr", "gilles.gasso@insa-rouen.fr"], "sections": [{"heading": null, "text": "In fact, most of them are able to survive on their own if they do not put themselves in a position to survive on their own."}, {"heading": "II. FROM SIGNAL TO HISTOGRAM OF GRADIENTS FEATURES", "text": "This section describes the feature extraction pipeline that we propose for the analysis of audio scene signals."}, {"heading": "A. The global feature extraction scheme", "text": "The features we propose for detecting audio scenes are based on some specific information extracted from a time-frequency representation (TFR) of the signal. Once the TFR image has been computed, it is processed to attenuate some false sounds that may impede relevant information related to high-energy time frequency structures. Subsequently, the resulting processed time-frequency representation image is used as an input for our histogram of color gradients for extraction. In short, the idea of the histogram of color gradients is to analyze the direction of energy variation in the time frequency representation locally. As detailed in the sequence, the local HOG information is combined across the entire TF image to generate the final function vector.The dimension of this vector depends on the number of containers in the (local) histogram and on how all local histograms are summarized to form the final function."}, {"heading": "B. From signals to TFR images", "text": "Due to their non-stationary nature, noises are typically represented on a short-term power frequency representation, the idea of which is to capture the power spectrum of the signal in a local window of varying lengths. A large part of the literature on sound detection problems uses such time frequency representations of noises. [3] In this work, we do not deviate from this widespread framework and select the representation of the signal according to a ConstantQ transformation [16], [17], MFCC-based [18] or short-term Fourier-based representation [19]. Unlike a short-term Fourier transformation, this transformation provides a frequency analysis on a protocol scale that adapts it more closely to sound and music representations [20]. Once this TFR has been calculated, we have an image that can be processed as such."}, {"heading": "C. Histogram of gradients", "text": "This year is the highest in the history of the country."}, {"heading": "D. Time-Frequency Histogram Pooling", "text": "Pooling consists in combining the reactions of a feature extraction algorithm calculated in nearby locations, the underlying idea being to combine local features into another feature (of lesser dimensionality) that is expected to store relevant information across the neighborhood. Pooling helps to obtain more robust information, a technique often considered successful in modern visual recognition algorithms [25]. In our case, the pooling of histograms across neighboring cells will aim to create new histograms that capture information about time-frequency structures that may be larger than a cell or that have been slightly translated into time or frequency. In this paper, we will examine several forms of time-frequency pooling (see Figure 3), while the pooling process remains fixed as an average operator. We will consider the following poolings: \u2022 Marginalized pooling throughout the course of this pooling cycle:"}, {"heading": "E. Discussions", "text": "Now that we have explained how to achieve the HOG characteristic of time frequency representation, we want to discuss some of the characteristics of these HOG characteristics and their advantages over characteristics such as MFCC for the characterization of audio scenes. Our original goal was to design characteristics that are able to characterize some time frequency structures that occur in a time-frequency representation. Since we take into account the orientations when counting a given gradient, the histogram of the gradients is invariant to the rotation when this rotation is smaller than the size of the garbage can. In our case, the rotation of a small rotation would correspond to a time-frequency structure that then leads to a change in the orientation of its gradients. Such a situation can occur, for example, in audio scenes that capture a moving object, such as a bus or a car. In fact, variations of the acceleration of a bus will induce variations of the frequency that correlate to the time frequency in that bus."}, {"heading": "III. DATA AND CLASSIFIERS", "text": "In this section, we provide some details about the data sets we have considered for evaluating the function we have proposed, as well as the description of the classifier we use and the experimental protocol."}, {"heading": "A. Toy dataset", "text": "To assess our characteristics, we have created a toy problem that highlights the ability or failure of investigated characteristics (including HOG and competitors) to detect the direction of variation of the power spectrum. Thus, we have created a binary classification problem in which signals from each class consist of a localized linear chord, or increasing or decreasing frequency, defined ass (t) = [t1, t2] (t) cos (2\u03c0 (at + b) t) + n (t) with t [0, 1] and n a centered Gaussian noise of standard deviation 0.4, a = 1200, b = 0 for classes 6y = + 1, a = \u2212 1200, b = 2400 for class y = \u2212 1 and vice versa [t1, t2] (t) is a function whose value is 1 if t1 \u2264 t \u2264 t2 and 0 otherwise. We have set t1 = 0.4 and t2 = 0.6 for both classes of representative samples showing C4-Q transformation."}, {"heading": "B. D-case challenge dataset", "text": "Each example in the dataset consists of a 30-second audio scene recorded at one of the following 10 locations: bus, busy street, office, open-air market, park, quiet street, restaurant, supermarket, tube, tube station. The recording was done at a speed of 44.1 kHz and the number of available examples is 100 with 10 examples per class.Note that the organizers of the challenge provided only the development datas2."}, {"heading": "C. East Anglia (EA) dataset", "text": "This data set was collected in early 2000 by Ma et al. [26] at East Anglia University. It provides environmental sounds from 10 different locations: bar, beach, bus, car, football match, launderette, lecture, office, train station and street. Each recording lasts 4 minutes and was recorded at a frequency of 22100 Hz. Similar to the data set in the D case, we have divided the recording into 30-second audio samples. Therefore, we have only 8 examples per class for this data set."}, {"heading": "D. Litis Rouen dataset", "text": "This data set, which we make publicly available, goes beyond the above in terms of volume and number of locations. Recordings were made using a Galaxy S3 smartphone equipped with Android using the Hi-Q MP3 Recorder application. While such a device can be considered poor, we believe that the resulting recordings would be similar to those obtained for real-world applications where cheap and ubiquitous microphones are more likely to be used. The sampling frequency we used is 44100 Hz and the recording is saved as an MP3 file with a bit rate of 64 Kbit / s. When converted into raw audio signals, they are sampled down to 22050 Hz. In total, approximately 1500 minutes of the audio scene were recorded."}, {"heading": "E. Competing features, classifier and protocols", "text": "This year it is more than ever before in the history of the city."}, {"heading": "IV. EXPERIMENTAL RESULTS", "text": "We have conducted several experiments to demonstrate the advantages of our HOG-based feature over the state of the art and to analyze the influence of the various parameters of the HOG feature extraction pipeline."}, {"heading": "A. Comparison with classical features", "text": "This year it is so far that it will only take one year to move on to the next round."}, {"heading": "B. Analyzing HOG feature parameters", "text": "In the first part of this experiment, we investigated the influence of two parameters of the HOG characteristics on global classification performance. We used the marginalized HOG characteristics, as in the previous experiment, in conjunction with a linear nucleus. The HOG characteristics consist of either signed, unsigned or both histograms, which were ultimately completed with the 4 normalization factors. Therefore, the size of the characteristic for each cell ranges from 8 to 28 = 8 \u00d7 3 + 4. Since we have 64 rows and columns of 8 \u00d7 8 cells in the image, the result is a size vector from 1024 to 3584. The results we get for the different datasets are presented in Table III. We can find that the parameters we evaluate clearly influence performance. Depending on the datasets, the variation of performance ranges between 2% (for East Anglia) and 6% (for D-case and the toy dataset). The most consistent characteristics seem to be those that are composed for histograms."}, {"heading": "C. On the effect of pooling", "text": "In fact, it is well known from computer vision that pooling plays an important role when it comes to pattern recognition, and we believe that proper choice of poolings can improve the performance of our audio scene. Average filter size, number of cells and number of orientations have been set to the values that maximize the performance according to Table IV. Again, the HOG features are used with signed and unsigned histograms and without the normalization factors. Average filter size, number of cells and number of orientations have been set."}, {"heading": "D. More insights on the Rouen\u2019s dataset", "text": "As one of our most important contributions in this paper is the introduction of a novel audio scene dataset, we will subsequently discuss our results regarding this dataset.In Table V we have shown that the average precision obtained as an average over 20 studies, is 0.9170. Table VI presents the normalized sum of all confusion matrices obtained from these 20 training / test splits. They were achieved with the best performance HOG function: namely those with signed and unsigned orientations, without changing the histograms and completely over time, the HOG is equipped with the cell size of 8, 8 containers of orientations and no average filtering. The average precision obtained from this matrix is 0.915 and it is different from the average average average average average average average average average precision in the runs of Table V.VI, which we have obtained with the cell size of 8, 8 containers of orientation and no average filtering."}, {"heading": "V. CONCLUSION", "text": "The problem of the classification of audio scenes is currently a hot topic in the computational analysis of auditory scenes. To address this particular problem, we have introduced in this paper a novel feature that seems to be very promising in the acquisition of relevant discriminatory information. The main block of the feature we have proposed was originally proposed in the field of computer vision, namely the histogram of histograms of histograms. Our novel feature was obtained by a computer histogram of courses of a constant Q transformation followed by a corresponding pooling. We have experimentally proven that these histograms of histograms of histograms of histograms are useful to capture specific characteristics that are present in a time frequency representation that cannot encode classical features such as MFCC, namely the local variation of the power spectrum. Then, our experimental results on real data sets clearly show that our features achieve state-of-the-art classification capabilities on multiple datasets."}], "references": [{"title": "Computational auditory scene analysis: Principles, algorithms and applications", "author": ["D. Wang", "G. Brown", "Eds"], "venue": "Wiley- Interscience,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2006}, {"title": "The bag-of-frame approach to audio pattern recognition: a sufficient model for urban soundscapes but not for polyphonic music", "author": ["J. Aucouturier", "B. Defreville", "F. Pachet"], "venue": "Journal of the Acoustical Society of America, vol. 122, no. 2, pp. 881\u2013891, 2007.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2007}, {"title": "Acoustic environment classification", "author": ["L. Ma", "B. Milner", "D. Smith"], "venue": "ACM Transactions on Speech and Language Processing, vol. 3, 2006.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2006}, {"title": "Non-negative matrix factorization applied to auditory scenes classification", "author": ["B. Cauchi"], "venue": "Master\u2019s thesis, Master ATIAM, Universit\u00e9 Pierre et Marie Curie, 2011.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2011}, {"title": "Combining frame and segment based models for environmental sound classification", "author": ["P. Hu", "W. Liu", "W. Jiang"], "venue": "Proceedings of 13th Annual Conference of the International Speech Communication Association, 2012.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2012}, {"title": "Acoustic scene classification using sparse feature learning and event based pooling", "author": ["K. Lee", "Z. Hyung", "J. Nam"], "venue": "IEEE Workshop on Applications of Signal Processing to Audio and Acoustics, 2013.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2013}, {"title": "Environment sound recognition with time-frequency audio features", "author": ["S. Chu", "S. Narayan", "C.J. Kuo"], "venue": "IEEE Trans. on Audio, Speech and Language Processing, vol. 17, no. 6, pp. 1142\u20131158, 2009.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2009}, {"title": "Large-scale audio feature extraction and svm for acoustic scene classification", "author": ["J. Geiger", "B. Schuller", "G. Rigoll"], "venue": "IEEE Workshop on Applications of Signal Processing to Audio and Acoustics, 2013.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2013}, {"title": "Recurrence quantification analysis features for environmental sound recognition", "author": ["G. Roma", "W. Nogueira", "P. Herrera"], "venue": "IEEE Workshop on Applications of Signal Processing to Audio and Acoustics, 2013.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2013}, {"title": "Detection and classification of acoustic scenes and events: an ieee aasp challenge", "author": ["D. Giannoulis", "E. Benetos", "D. Stowell", "M. Rossignol", "M. Lagrange"], "venue": "IEEE Workshop on Applications of Signal Processing to Audio and Acoustics, 2013.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2013}, {"title": "Spectral vs spectro-temporal features for acoustic event detection", "author": ["C. Cotton", "D. Ellis"], "venue": "IEEE Workshop on applications of Signal processing to audio and acoustics, 2011, pp. 69\u201372.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2011}, {"title": "Characterisation of acoustic scenes using a temporally-constrained shift-invariant model", "author": ["E. Benetos", "M. Lagrange", "S. Dixon"], "venue": "Proceedings of the fifteenth International Conference on Digital Audio effects, 2012.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2012}, {"title": "Audio classification from time-frequency texture", "author": ["G. Yu", "J. Slotine"], "venue": "in Proceedings of IEEE International Conference in Acoustics, Speech and Signal Processing, 2009.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2009}, {"title": "Image feature representation of the subband power distribution for robust sound event classification", "author": ["J. Dennis", "H. Tran", "E. Chng"], "venue": "IEEE Trans on Audio, Speech and Language Processing, vol. 21, no. 2, pp. 367\u2013377, 2013.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2013}, {"title": "Histograms of oriented gradients for human detection", "author": ["N. Dalal", "B. Triggs"], "venue": "Computer Vision and Pattern Recognition, 2005. CVPR 2005. IEEE Computer Society Conference on, vol. 1. IEEE, 2005, pp. 886\u2013893.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2005}, {"title": "Efficient wavelet adaptation for hybrid wavelet-large margin classifiers", "author": ["J. Neumann", "C. Schnorr", "G. Steidl"], "venue": "Pattern Recognition, vol. 38, no. 11, pp. 1815\u20131830, 2005.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 1815}, {"title": "Feature extracted from wavelet decomposition using biorthogonal riesz basis for text-independent speaker recognition", "author": ["S.-Y. Lung"], "venue": "Pattern recognition, vol. 41, no. 10, pp. 3068\u20133070, 2008.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2008}, {"title": "Comparison of techniques for environmental sound recognition", "author": ["M. Cowling", "R. Sitte"], "venue": "Pattern recognition letters, vol. 24, no. 15, pp. 2895\u20132907, 2003.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2003}, {"title": "Calculation of a constant Q spectral transform", "author": ["J. Brown"], "venue": "Journal of Acoustic Society of America, vol. 89, no. 1, pp. 425\u2013 434, 1991.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 1991}, {"title": "Object detection with discriminatively trained part based models", "author": ["P. Felzenszwalb", "R. Grishick", "D. McAllester", "D. Ramanan"], "venue": "IEEE Trans. on Pattern Analysis and Machine Intelligence, vol. 32, no. 9, pp. 1627\u20131645, 2010.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2010}, {"title": "T- HOG: An effective gradient-based descriptor for single line text regions", "author": ["R. Minetto", "N. Thome", "M. Cord", "N. Leite", "J. Stolfi"], "venue": "Pattern recognition, vol. 46, no. 3, pp. 1078\u20131090, 2013.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2013}, {"title": "Efficient additive kernels via explicit feature maps", "author": ["A. Vedaldi", "A. Zisserman"], "venue": "Proceedings of the IEEE Conf. on  Computer Vision and Pattern Recognition (CVPR), 2010.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2010}, {"title": "Distinctive image features from scale-invariant keypoints", "author": ["D.G. Lowe"], "venue": "International journal of computer vision, vol. 60, no. 2, pp. 91\u2013110, 2004.", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2004}, {"title": "A theoretical analysis of feature pooling in vision algorithms", "author": ["Y.-L. Boureau", "J. Ponce", "Y. LeCun"], "venue": "Proceedings of the International Conference on Machine Learning, 2010.", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2010}, {"title": "Context awareness using environmental noise classification", "author": ["L. Ma", "D. Smith", "B. Milner"], "venue": "Proceedings of Eurospeech, 2003, pp. 2237\u20132240.", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2003}, {"title": "PLP and RASTA and MFCC and inversion in matlab", "author": ["D. Ellis"], "venue": "online web resource : available at http://www.ee.columbia.edu/ \u223cdpwe/resources/matlab/rastamat, 2005.", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2005}, {"title": "Constant-Q transform toolbox for music processing", "author": ["C. Schoerkhuber", "A. Klapuri"], "venue": "Proceedings of the Sound and Music 15 Computing Conference, 2010.", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2010}, {"title": "VLFeat: An open and portable library of computer vision algorithms", "author": ["A. Vedaldi", "B. Fulkerson"], "venue": "http://www.vlfeat.org/, 2008.", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2008}], "referenceMentions": [{"referenceID": 0, "context": "The problem of recognizing acoustic environments is known as the problem of audio scene classification and it is one of the most difficult task in the general context of computational auditory scene analysis (CASA) [1].", "startOffset": 215, "endOffset": 218}, {"referenceID": 1, "context": "For instance, following its success in speech recognition, one of the most prominent features that has been considered for audio scene recognition are mel-frequency cepstral coefficients (MFCC) [2], [3], [4], [5].", "startOffset": 194, "endOffset": 197}, {"referenceID": 2, "context": "For instance, following its success in speech recognition, one of the most prominent features that has been considered for audio scene recognition are mel-frequency cepstral coefficients (MFCC) [2], [3], [4], [5].", "startOffset": 199, "endOffset": 202}, {"referenceID": 3, "context": "For instance, following its success in speech recognition, one of the most prominent features that has been considered for audio scene recognition are mel-frequency cepstral coefficients (MFCC) [2], [3], [4], [5].", "startOffset": 204, "endOffset": 207}, {"referenceID": 4, "context": "For instance, following its success in speech recognition, one of the most prominent features that has been considered for audio scene recognition are mel-frequency cepstral coefficients (MFCC) [2], [3], [4], [5].", "startOffset": 209, "endOffset": 212}, {"referenceID": 1, "context": "For instance, [2] consider a Gaussian Mixture Model for estimating the distribution of the MFCC coefficients, while [6] have proposed a sparse feature learning approach for capturing relevant MFCC coefficients.", "startOffset": 14, "endOffset": 17}, {"referenceID": 5, "context": "For instance, [2] consider a Gaussian Mixture Model for estimating the distribution of the MFCC coefficients, while [6] have proposed a sparse feature learning approach for capturing relevant MFCC coefficients.", "startOffset": 116, "endOffset": 119}, {"referenceID": 6, "context": "[7] proposed an ensemble of timefrequency features obtained from a matching pursuit decomposition of the audio signal.", "startOffset": 0, "endOffset": 3}, {"referenceID": 7, "context": "Recently, [8] have considered a large set of features, including spectral, energy-based and voicing-related features.", "startOffset": 10, "endOffset": 13}, {"referenceID": 8, "context": "Another family of relevant features can be obtained from MFCC by considering recursive quantitative analyzing (RQA) as introduced by [9].", "startOffset": 133, "endOffset": 136}, {"referenceID": 9, "context": "According to the recent D-Case challenge on audio scene recognition [10], combining MFCC features with RQA features extracted from MFCC yield to an highly efficient set of features.", "startOffset": 68, "endOffset": 72}, {"referenceID": 10, "context": "In this context, [11] have investigated methods for automatically extracting spatio-temporal patches that are discriminative of the audio scene.", "startOffset": 17, "endOffset": 21}, {"referenceID": 11, "context": "[12] have followed similar ideas but instead of considering NMF they employed a probabilistic model denoted as probabilistic latent component analysis.", "startOffset": 0, "endOffset": 4}, {"referenceID": 12, "context": "Likewise, other works propose features, like texture-based features that are directly computed from time-frequency representations [13], [14].", "startOffset": 131, "endOffset": 135}, {"referenceID": 13, "context": "Likewise, other works propose features, like texture-based features that are directly computed from time-frequency representations [13], [14].", "startOffset": 137, "endOffset": 141}, {"referenceID": 14, "context": "These HOG features have been genuinely introduced for human detection in images [15] but we strongly believe that their properties make them highly valuable for extracting relevant features based on timefrequency representation (TFR).", "startOffset": 80, "endOffset": 84}, {"referenceID": 9, "context": "The recent D-Case challenge [10] is an excellent initiative of this kind although its number of examples is limited (100 for the publicly available examples).", "startOffset": 28, "endOffset": 32}, {"referenceID": 15, "context": "Depending on the task at hand, they either consider wavelet-based [16], [17], MFCC-based [18] or shorttime Fourier-based representations [19].", "startOffset": 66, "endOffset": 70}, {"referenceID": 16, "context": "Depending on the task at hand, they either consider wavelet-based [16], [17], MFCC-based [18] or shorttime Fourier-based representations [19].", "startOffset": 72, "endOffset": 76}, {"referenceID": 17, "context": "Depending on the task at hand, they either consider wavelet-based [16], [17], MFCC-based [18] or shorttime Fourier-based representations [19].", "startOffset": 137, "endOffset": 141}, {"referenceID": 18, "context": "In this work, we do not depart from this widely adopted framework and choose to represent the signal according to a constantQ transform [20].", "startOffset": 136, "endOffset": 140}, {"referenceID": 18, "context": "Contrarily to a short-time Fourier transform, this transform provides a frequency analysis on a log-scale which makes it more adapted to sound and music representations [20].", "startOffset": 169, "endOffset": 173}, {"referenceID": 14, "context": "Histogram of gradients have been originally introduced by [15] for human detection in images.", "startOffset": 58, "endOffset": 62}, {"referenceID": 14, "context": "From works in computer vision [15], [21], [22], it is now well acknowledged that local shape information can be described through gradient intensity and orientations.", "startOffset": 30, "endOffset": 34}, {"referenceID": 19, "context": "From works in computer vision [15], [21], [22], it is now well acknowledged that local shape information can be described through gradient intensity and orientations.", "startOffset": 36, "endOffset": 40}, {"referenceID": 20, "context": "From works in computer vision [15], [21], [22], it is now well acknowledged that local shape information can be described through gradient intensity and orientations.", "startOffset": 42, "endOffset": 46}, {"referenceID": 14, "context": "Two main approaches have been proposed for computing HOG in images [15], [21] and they are both based on the following steps:", "startOffset": 67, "endOffset": 71}, {"referenceID": 19, "context": "Two main approaches have been proposed for computing HOG in images [15], [21] and they are both based on the following steps:", "startOffset": 73, "endOffset": 77}, {"referenceID": 21, "context": "For this work, we have used the implementation in the VLFeat toolbox [23].", "startOffset": 69, "endOffset": 73}, {"referenceID": 22, "context": "In particular, it implements several histogram-based feature extraction algorithms including SIFT [24] and HOG.", "startOffset": 98, "endOffset": 102}, {"referenceID": 14, "context": "For more details, we refer the reader to [15] and [21] and to the VLFeat Hog tutorial1.", "startOffset": 41, "endOffset": 45}, {"referenceID": 19, "context": "For more details, we refer the reader to [15] and [21] and to the VLFeat Hog tutorial1.", "startOffset": 50, "endOffset": 54}, {"referenceID": 23, "context": "This technique is a step commonly considered with success in modern visual recognition algorithms [25].", "startOffset": 98, "endOffset": 102}, {"referenceID": 0, "context": "with t \u2208 [0, 1] and n is a centered Gaussian noise of standard deviation 0.", "startOffset": 9, "endOffset": 15}, {"referenceID": 9, "context": "For the purpose of a challenge, a dataset providing environmental sound recordings has been recently released by [10].", "startOffset": 113, "endOffset": 117}, {"referenceID": 24, "context": "[26] at the East Anglia University.", "startOffset": 0, "endOffset": 4}, {"referenceID": 1, "context": "\u2022 Bag of MFCC: these features are obtained by computing the MFCC features on windowed part of the signals and then in concatenating them all [2].", "startOffset": 141, "endOffset": 144}, {"referenceID": 25, "context": "The toolbox we have employed is the rastamat one with the dithering option on [27].", "startOffset": 78, "endOffset": 82}, {"referenceID": 12, "context": "\u2022 Texture-based TFR analysis: we have also implemented the features extracted from time-frequency representations as proposed in [13].", "startOffset": 129, "endOffset": 133}, {"referenceID": 12, "context": "In agreement with [13], the used local filters are randomly sampled over the CQT representations of the training set.", "startOffset": 18, "endOffset": 22}, {"referenceID": 8, "context": "\u2022 Recurrence plot analysis: these features are those introduced by [9], and they have achieved the best performance on the test set of the D-case audio scene challenge [10].", "startOffset": 67, "endOffset": 70}, {"referenceID": 9, "context": "\u2022 Recurrence plot analysis: these features are those introduced by [9], and they have achieved the best performance on the test set of the D-case audio scene challenge [10].", "startOffset": 168, "endOffset": 172}, {"referenceID": 8, "context": "Interestingly, the final features proposed by [9] are obtained through averaging over time of all time-localized MFCC and recurrence plot features.", "startOffset": 46, "endOffset": 49}, {"referenceID": 8, "context": "frequency of the spectral analysis instead of the 900Hz used by [9].", "startOffset": 64, "endOffset": 67}, {"referenceID": 26, "context": "The CQT transform is computed, by means of the [28]\u2019s toolbox, on the same frequency range as the MFCC features and with 8 bins per octave.", "startOffset": 47, "endOffset": 51}, {"referenceID": 26, "context": "All other parameters have been kept as default as proposed in [28].", "startOffset": 62, "endOffset": 66}, {"referenceID": 27, "context": "As described above, histograms are normalized according to some norms, 4 normalization factors are computed by the vlfeat toolbox [29] and we have considered the possibility of using them as complementary features.", "startOffset": 130, "endOffset": 134}, {"referenceID": 0, "context": "is chosen among [1, 5, 10, 20, 50, 100].", "startOffset": 16, "endOffset": 39}, {"referenceID": 4, "context": "is chosen among [1, 5, 10, 20, 50, 100].", "startOffset": 16, "endOffset": 39}, {"referenceID": 9, "context": "is chosen among [1, 5, 10, 20, 50, 100].", "startOffset": 16, "endOffset": 39}, {"referenceID": 18, "context": "is chosen among [1, 5, 10, 20, 50, 100].", "startOffset": 16, "endOffset": 39}, {"referenceID": 8, "context": "Note that we have also used MFCC and MFCC-RQA features obtained with an upper frequency of 900 Hz as in the paper of [9].", "startOffset": 117, "endOffset": 120}, {"referenceID": 12, "context": "[13] WHILE MFCC-D-DD IS RELATED TO THE MFCC AND DERIVATIVES FEATURES.", "startOffset": 0, "endOffset": 4}, {"referenceID": 8, "context": "We also want to highlight that the performance we report for MFCCRQA-900 is slightly lower than those given in [9] and this is due to the fact our results are averages over 20 splits instead of a 5-fold cross-validation precision.", "startOffset": 111, "endOffset": 114}, {"referenceID": 23, "context": "Indeed, it is well known from the computer vision literature that pooling plays an important role when it comes to pattern recognition [25] and we believe that a proper choice of pooling can also", "startOffset": 135, "endOffset": 139}], "year": 2015, "abstractText": "This paper addresses the problem of audio scenes classification and contributes to the state of the art by proposing a novel feature. We build this feature by considering histogram of gradients (HOG) of an audio scene time-frequency representation. Contrarily to classical audio features like MFCC, we make the hypothesis that histograms of gradients are able to encode some relevant informations in a time-frequency representation: namely, the local direction of variation (in time and frequency) of the signal spectral power. In addition, in order to gain more invariance and robustness, histograms of gradients are locally pooled. We have evaluated the relevance of the novel feature by comparing its performances with stateof-the-art competitors, on several datasets, including a novel one that we provide, as part of our contribution. This dataset, that we make publicly available, involves 19 classes and contains about 1500 minutes of audio scene recordings. We thus believe that it may be the next standard dataset for evaluating audio scene classification algorithms. Our comparison results clearly show that the HOG-based features outperform its competitors.", "creator": "TeX"}}}