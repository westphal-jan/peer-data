{"id": "1508.04924", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "20-Aug-2015", "title": "Distributed Compressive Sensing: A Deep Learning Approach", "abstract": "We address the problem of compressed sensing with Multiple Measurement Vectors (MMVs) when the structure of sparse vectors in different channels depend on each other. \"The sparse vectors are not necessarily joint sparse\". We capture this dependency by computing the conditional probability of each entry of each sparse vector to be non-zero given \"residuals\" of all previous sparse vectors. To compute these probabilities, we propose to use Long Short-Term Memory (LSTM) [1], a bottom up data driven model for sequence modelling. To compute model parameters we minimize a cross entropy cost function. We propose a greedy solver that uses above probabilities at the decoder. By performing extensive experiments on two real world datasets, we show that the proposed method significantly outperforms general MMV solver Simultaneous Orthogonal Matching Pursuit (SOMP) and model based Bayesian methods including Multitask Compressive Sensing [2] and Sparse Bayesian Learning for Temporally Correlated Sources [3]. Nevertheless, we emphasize that the proposed method is a data driven method where availability of training data is important. However, in many applications, train data is indeed available, e.g., recorded images or video.", "histories": [["v1", "Thu, 20 Aug 2015 08:57:29 GMT  (1290kb,D)", "http://arxiv.org/abs/1508.04924v1", null], ["v2", "Mon, 7 Sep 2015 01:15:11 GMT  (1508kb,D)", "http://arxiv.org/abs/1508.04924v2", null], ["v3", "Wed, 11 May 2016 22:18:13 GMT  (2747kb,D)", "http://arxiv.org/abs/1508.04924v3", "To appear in IEEE Transactions on Signal Processing"]], "reviews": [], "SUBJECTS": "cs.LG cs.CV", "authors": ["hamid palangi", "rabab ward", "li deng"], "accepted": false, "id": "1508.04924"}, "pdf": {"name": "1508.04924.pdf", "metadata": {"source": "CRF", "title": "Distributed Compressive Sensing: A Deep Learning Approach", "authors": ["Hamid Palangi", "Rabab Ward", "Li Deng"], "emails": ["hamidp@ece.ubc.ca)", "rababw@ece.ubc.ca)", "deng@microsoft.com)"], "sections": [{"heading": null, "text": "In the general CS framework, where a signal x-1, M-1 random measurements are acquired, where M < N-1 random measurements of M < N random measurements of M < N random measurements of M < N random measurements of M < N random measurements of M < N random measurements of M < N random measurements of M < N is based on the underdetermined system of linear equations (1) H. Palangi are associated with the Department of Electricity and Computer Engineering of the University of British Columbia, Vancouver, Canada, Canada, Canada, Canada, Canada, Canada, etc."}, {"heading": "A. Problem Statement", "text": "As mentioned above, many MMV reconstruction methods do not rely on training data. However, for many applications there is a huge amount of data available that is similar to data compressed by CS. Examples are camera recordings of the same environment, images of the same class (e.g. flowers, buildings,...), electroencephalograms (EEG) of different parts of the brain, etc. In this paper, we address the following questions in the MMV problem that occur when we have training data: 1) Can we use a data-driven bottom-up approach to learn the structure of the sparse vectors in S using the data already available? And then we use this structure to design a better algorithm for the MMV problem that reconstructs the new data? 2) Most MV reconstruction algorithms rely on the shared economy of S. However, in some practical applications, the sparse vectors in S are not exactly economical."}, {"heading": "B. Proposed Method", "text": "In fact, it is that we are able to assert ourselves, that we are able, that we are able to hide ourselves, and that we are able, that we are able to hide ourselves, \"he said."}, {"heading": "C. Related Work", "text": "In fact, we are able to go in search of a solution that is capable of finding a solution that is capable of finding a solution, that is capable of finding a solution that is capable of finding a solution, and that is able to find a solution that is capable of finding a solution, that is able to find a solution that is capable of finding a solution, that is able to find a solution that is capable of finding a solution, that is able to find a solution that is capable of finding a solution, that is able to find a solution that is capable of finding a solution."}, {"heading": "II. RNN WITH LSTM CELLS", "text": "The RNN is a type of deep neural network [21], [22] which is \"deep\" in time dimension and has been widely used in time sequence modeling (v). If we consider the sparse vectors (columns) in S as a sequence (v), the main idea of using RNN for the MMV problem is to predict the sparsity patterns across various sparse vectors in S. Although RNN performs sequence modeling (v) in principle (v), it is generally difficult to learn the long-term dependence within the sequence \u2212 \u2212 \u2212 \u2212 due to the disappearing gradient problem \u2212. One of the effective solutions to this problem in RNNs is the use of memory cells not originally proposed in the Gate MNN, c, as Long ShortTerm Memory (LSTM), and [33] by adding forgotten and gate connections to the architecture."}, {"heading": "III. PROPOSED METHOD", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "A. High Level Picture", "text": "The summary of the proposed method is shown in Fig. 1: We initialize the residual vector, r, for each channel through the measurement vector, y, of the channel. These residual vectors serve as input to the LSTM model, which records their properties using input weight matrices (W1, W2, W3, W4) as well as their dependence on recurrent weight matrices (Wrec1, Wrec2, Wrec4) and central memory units shown in Fig. 2. A transformation matrix U is then used to transform the results of each memory cell after gating, i.e., v."}, {"heading": "B. Training Data Generation", "text": "The main point of view of the proposed method is to consider the sparse reconstruction problem as a two-step task: a classification in the first step and a subsequent smallest squaring in the second step. In the classification step, we want to find the atom of the dictionary, i.e. the column of A, that is most relevant for the given remainder of the current channel and the remnants of the previous channels in the MMV problem. Therefore, we need a series of residual vectors and corresponding sparse vectors for the supervised training. As the offline data and A are given, we can imitate steps explained in the previous section to generate residuals, which means that in the face of a sparse vector with k-equal entries, we calculate y with (3). Then we find the entry with the maximum value in s and set it to zero. Suppose that the index of this entry is zero."}, {"heading": "C. Learning Method", "text": "For the calculation of the proposed model parameters, i.e., the matrices in Figure 2 and U in Figure 1, we have the number of MZ in Figure 1, which we have in Figure 1, 2 and 3 in Figure 1, 3 in Figure 1, 3 in Figure 1, 4 in Figure 1, 4 in Figure 1, 4 in Figure 1, 5 in Figure 1, 5 in Figure 1, 5 in Figure 1, 5 in Figure 1, 5 in Figure 1, 5 in Figure 1, 5 in Figure 1, 5 in Figure 1, 5 in Figure 1, 5 in Figure 1, 5 in Figure 1, 5 in Figure 1, 5 in Figure 1, 5 in Figure 1, 5 in Figure 1, 5 in Figure 1, 5 in Figure 1, 5 in Figure 1, 5 in Figure 1, 5 in Figure 1, 5 in Figure 1, 5 in Figure 1, 5 in Figure 1, 5 in Figure 1, 5 in Figure 1, 5 in Figure 1, 5 in Figure 1, 5 in Figure 1, 5 in Figure 1, 5 in Figure 1, 5 in Figure 1, 5 in Figure 1, 1 in Figure 1 in Figure 1, 5 in Figure 1 in Figure 1, 1 in Figure 1, 1 in Figure 1, 5 in Figure 1 in Figure 1, 1 in Figure 1, 5 in Figure 1 in Figure 1, 1 in Figure 1, 5 in Figure 1 in Figure 1, 5 in Figure 1, 1 in Figure 1 in Figure 1, 1 in Figure 1, 5 in Figure 1 in Figure 1, 5 in Figure 1, 1 in Figure 1 in Figure 1, 5 in Figure 1 in Figure 1, 5 in Figure 1, 1 in Figure 1, 5 in Figure 1 in Figure 1, 5 in Figure 1 in Figure 1, 5 in Figure 1, 5 in Figure 1, 1 in Figure 1 in Figure 1, 5 in Figure 1, 5 in Figure 1 in Figure 1, 5 in Figure 1, 5 in Figure 1 in Figure 1, 5 in Figure 1, 1 in 1 in"}, {"heading": "IV. EXPERIMENTAL RESULTS AND DISCUSSION", "text": "In this section, the results of the different reconstruction algorithms for the MMV problem are presented, including the proposed method for this work. We have conducted experiments with two real data sets, MNIST data sets of handwritten digits [36] and three different classes of images from natural image sets from Microsoft Research in Cambridge [37]. In this section, we would like to answer the following questions: (i) What is the performance of different reconstruction algorithms for the MMV problem, including the proposed method, when different channels, i.e. different columns in S, have different thriftiness patterns? (ii) Does the proposed method work well enough when there is a correlation between different sparse vectors? For example, when sparse vectors are DCT or Wavelet transform of different image blocks? (iii) How fast is the proposed method compared to other reconstruction algorithms for the MV problem?"}, {"heading": "A. MNIST Dataset", "text": "This year it is as far as never before in the history of the Federal Republic of Germany."}, {"heading": "B. Natural Images Dataset", "text": "For experiments on natural images we have used the MSR Cambridge dataset [37]. 10 randomly selected certificates of three classes of this dataset are used for experiments.The images are shown in Fig. 7. We have used 64 \u00d7 64 images. Each image is divided into 8 \u00d7 8 blocks. After reconstructing all blocks of an image in the decoder, the MSE is calculated for the reconstructed image. The task is to encode 4 blocks (L = 4) of an image and reconstruct them in the decoder. This means that S has in (4) 4 columns each with N = 64 entries. We have used 50% measurements, i.e. Y in (4) have 4 columns each with M = 32 centering. We have compared the performance of the proposed algorithm, LSTM-CS, with SOMP, T-SBL, MT and NWSOMP."}, {"heading": "V. CONCLUSIONS AND FUTURE WORK", "text": "In this paper, we presented a method for reconstructing sparse vectors for the MMV problem. We demonstrated that the proposed method is not based on a common split assumption. By experimenting with two real data sets, we were able to show that the proposed method exceeds the general MMV base methods SOMP as well as the Bayesian model-based methods MT-BCS and T-SBL for the MMV problem. Please note that we did not use multi-layered LSTM methods or advanced deep learning methods for training, e.g. drop-out regulation, etc., which will improve the performance of LSTM-CS. This paper is proof that deep learning methods, and in particular sequence modeling methods, e.g. LSTM, can significantly improve the performance of the MMV solvers, especially when the split patterns are more complicated than simple CTT or STS transformations."}, {"heading": "VI. ACKNOWLEDGEMENT", "text": "We would like to thank the authors of [3] and [15] and [2] for providing the code for their work, which helped us a lot to make comparisons."}, {"heading": "APPENDIX A EXPRESSIONS FOR THE GRADIENTS", "text": "In this appendix, we present the final gradient expressions required to train the proposed model for the MMV problem. Since we are limited in space to the representation of the complete derivatives of these gradients. Starting from the cost function in (11), we use the Nesterov method described in (12) to update the parameters of the LSTM-CS model. This is one of the weight matrices or bias vectors in the LSTM-CS architecture. The general format of the gradient of the cost function, Wrec1, Wrec2, Wrec4, Wrec4, Wrec3, Wp1, Wp1, Wp2, Wp2, Wp3, b2, b4} is Wrec1, Wrec4, Wrec4, Wrec1, Wp1, Wp2, Wp3, b3, b3, b3, the general format of the cost function, is the same as the STR-3, b3} of the architecture."}, {"heading": "B. Output Gate", "text": "For recurring connections we have: \u2202 Lt \u2202 Wrec1 = \u03b4rec1 (t).v (t \u2212 1) T (22) Where (t).v (t).t (t).For input connections, W1, and peephole connections, Wp1, we have: axy Lt (t).r (t).t) T (24).c (t) T (t).c (t).t (t).t For recurring connections we have: habi Lt (diag) Wrec3 (t).t Wrec3 (t).t = Wep3 (t).t (t).t (t).t (t).t (t).t (t).t (t).t (t).t (t).t (t) (.t (.t) (.t) (.t (.t).t (t).t (.t (.t).t (t).t (.t (t).t (.t).t (.t (t).t (.t).t (.t (.t).t (.t (.t).t (.t (.t).t (.t (.t).t (.t (.t).t (.t (.t).t (.t (.t).t (.t (.t).t (.t (.t (.t).t (.t (.t).t (.t (.t).t (.t (.t (.t).t (.t (.t).t (.t (.t).t (.t).t (.t (.t (.t).t (.t (.t).t (.t (.t (.t).t (.t).t (.t (.t (.t).t (.t (.t).t (.t).t (.t (.t (.t).t (.t (.t).t (.t).t (.t).t (.t).t (.t (.t).t (."}, {"heading": "D. Forget Gate", "text": "For the recurring connections we will have: \u2202 Lt Wrec2 = diag (\u03b4rec2 = 38 connections) For the recurring connections we will have: \u2202 Lt Wrec2 = diag (\u03b4rec2 = 38 connections). For the recurring connections we will have: \u2202 Lt W2 = diag (\u03b42 = 48 connections). For the recurring connections we will have: \u0445 (t) (6) (6) (6) (8) (8) (8) (8) (8) (8) (8) (8) (8) (8) (8) (8) (8) (8) (8) (8) (8) (8) (8). For the recurring connections we will have. For the recurring connections we will not have. For the recurring connections we will have. For the recurring connections we will have (8) (8) (8) (8) (8) (8). For the recurring connections we will have (8) (8). For the recurring connections we will have (8) (8). For the recurring connections we will have (8). For the recurring connections we will have (8)."}, {"heading": "F. Error signal backpropagation", "text": "Error signals are propagated back over time using the following equations: \u03b4rec1 (t \u2212 1) = [o (t \u2212 1) \u0445 (1 \u2212 o (t \u2212 1) \u0445 h (c (t \u2212 1)) \u0445 [WTrec1.\u03b4rec1 (t) + e (t \u2212 1)] (49) \u03b4reci (t \u2212 1) = [(1 \u2212 h (c (t \u2212 1)) \u0445 (1 + h (c (t \u2212 1)) \u0445 (1 + h (c (t \u2212 1))) \u0445 o (t \u2212 1) \u0441reci (t \u2212 1) \u0445 [WTreci.\u03b4reci (t) + e (t \u2212 1)], for i (2, 3, 4} (50)"}], "references": [{"title": "Long short-term memory", "author": ["S. Hochreiter", "J. Schmidhuber"], "venue": "Neural Comput., vol. 9, no. 8, pp. 1735\u20131780, Nov. 1997.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 1997}, {"title": "Multitask compressive sensing", "author": ["S. Ji", "D. Dunson", "L. Carin"], "venue": "Signal Processing, IEEE Transactions on, vol. 57, no. 1, pp. 92\u2013 106, 2009.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2009}, {"title": "Sparse signal recovery with temporally correlated source vectors using sparse bayesian learning", "author": ["Z. Zhang", "B.D. Rao"], "venue": "Selected Topics in Signal Processing, IEEE Journal of, vol. 5, no. 5, pp. 912\u2013926, 2011.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2011}, {"title": "Compressed sensing", "author": ["D. Donoho"], "venue": "IEEE Transactions on Information Theory, vol. 52, no. 4, pp. 1289 \u20131306, april 2006.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2006}, {"title": "Stable signal recovery from incomplete and inaccurate measurements", "author": ["E. Candes", "J. Romberg", "T. Tao"], "venue": "Communications on Pure and Applied Mathematics, vol. 59, no. 8, pp. 1207\u20131223, 2006.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2006}, {"title": "Compressive sensing [lecture notes", "author": ["R. Baraniuk"], "venue": "IEEE Signal Processing Magazine, vol. 24, no. 4, pp. 118 \u2013121, july 2007.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2007}, {"title": "Compressed sensing with coherent and redundant dictionaries", "author": ["E.J. Cands", "Y.C. Eldar", "D. Needell", "P. Randall"], "venue": "Applied and Computational Harmonic Analysis, vol. 31, no. 1, pp. 59 \u2013 73, 2011.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2011}, {"title": "Structured compressed sensing: From theory to applications", "author": ["M. Duarte", "Y. Eldar"], "venue": "IEEE Transactions on Signal Processing, vol. 59, no. 9, pp. 4053 \u20134085, sept. 2011.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2011}, {"title": "Rank awareness in joint sparse recovery", "author": ["M. Davies", "Y. Eldar"], "venue": "IEEE Transactions on Information Theory, vol. 58, no. 2, pp. 1135 \u20131146, Feb. 2012.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2012}, {"title": "Average case analysis of multichannel sparse recovery using convex relaxation", "author": ["Y. Eldar", "H. Rauhut"], "venue": "IEEE Transactions on Information Theory, vol. 56, no. 1, pp. 505 \u2013519, Jan. 2010.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2010}, {"title": "Algorithms for simultaneous sparse approximation. part I: Greedy pursuit", "author": ["J. Tropp", "A. Gilbert", "M. Strauss"], "venue": "Signal Processing, vol. 86, no. 3, pp. 572 \u2013 588, 2006.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2006}, {"title": "Algorithms for simultaneous sparse approximation. part II: Convex relaxation", "author": ["J. Tropp"], "venue": "Signal Processing, vol. 86, no. 3, pp. 589 \u2013 602, 2006.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2006}, {"title": "An empirical bayesian strategy for solving the simultaneous sparse approximation problem", "author": ["D.P. Wipf", "B.D. Rao"], "venue": "Signal Processing, IEEE Transactions on, vol. 55, no. 7, pp. 3704\u20133716, 2007.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2007}, {"title": "Model-based compressive sensing", "author": ["R. Baraniuk", "V. Cevher", "M. Duarte", "C. Hegde"], "venue": "Information Theory, IEEE Transactions on, vol. 56, no. 4, pp. 1982\u20132001, April 2010.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 1982}, {"title": "Bayesian compressive sensing", "author": ["S. Ji", "Y. Xue", "L. Carin"], "venue": "Signal Processing, IEEE Transactions on, vol. 56, no. 6, pp. 2346\u20132356, 2008.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2008}, {"title": "Embedding prior knowledge within compressed sensing by neural networks", "author": ["D. Merhej", "C. Diab", "M. Khalil", "R. Prost"], "venue": "IEEE Transactions on Neural Networks, vol. 22, no. 10, pp. 1638 \u2013 1649, oct. 2011.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2011}, {"title": "Using deep stacking network to improve structured compressed sensing with multiple measurement vectors", "author": ["H. Palangi", "R. Ward", "L. Deng"], "venue": "IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), 2013.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2013}, {"title": "Sparse bayesian learning and the relevance vector machine", "author": ["M.E. Tipping"], "venue": "J. Mach. Learn. Res., vol. 1, pp. 211\u2013244, Sep. 2001.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2001}, {"title": "Analysis of sparse bayesian learning", "author": ["A.C. Faul", "M.E. Tipping"], "venue": "Advances in Neural Information Processing Systems (NIPS) 14. MIT Press, 2001, pp. 383\u2013389.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2001}, {"title": "Scalable stacking and learning for building deep architectures", "author": ["L. Deng", "D. Yu", "J. Platt"], "venue": "Proc. ICASSP, march 2012, pp. 2133 \u20132136.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2012}, {"title": "Deep neural networks for acoustic modeling in speech recognition: The shared views of four research groups", "author": ["G. Hinton", "L. Deng", "D. Yu", "G.E. Dahl", "A. Mohamed", "N. Jaitly", "A. Senior", "V. Vanhoucke", "P. Nguyen", "T.N. Sainath", "B. Kingsbury"], "venue": "IEEE Signal Processing Magazine, vol. 29, no. 6, pp. 82\u201397, November 2012.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2012}, {"title": "Context-dependent pre-trained deep neural networks for large-vocabulary speech recognition", "author": ["G. Dahl", "D. Yu", "L. Deng", "A. Acero"], "venue": "Audio, Speech, and Language Processing, IEEE Transactions on, vol. 20, no. 1, pp. 30 \u201342, jan. 2012.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2012}, {"title": "Finding structure in time", "author": ["J.L. Elman"], "venue": "Cognitive Science, vol. 14, no. 2, pp. 179\u2013211, 1990.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 1990}, {"title": "An application of recurrent nets to phone probability estimation", "author": ["A.J. Robinson"], "venue": "IEEE Transactions on Neural Networks, vol. 5, no. 2, pp. 298\u2013305, August 1994.", "citeRegEx": "24", "shortCiteRegEx": null, "year": 1994}, {"title": "Analysis of the correlation structure for a neural predictive model with application to speech recognition", "author": ["L. Deng", "K. Hassanein", "M. Elmasry"], "venue": "Neural Networks, vol. 7, no. 2, pp. 331\u2013339, 1994.", "citeRegEx": "25", "shortCiteRegEx": null, "year": 1994}, {"title": "Recurrent neural network based language model.", "author": ["T. Mikolov", "M. Karafi\u00e1t", "L. Burget", "J. Cernock\u1ef3", "S. Khudanpur"], "venue": "in Proc. INTERSPEECH, Makuhari,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2010}, {"title": "Sequence transduction with recurrent neural networks", "author": ["A. Graves"], "venue": "Representation Learning Workshp, ICML, 2012.  13", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2012}, {"title": "Advances in optimizing recurrent networks", "author": ["Y. Bengio", "N. Boulanger-Lewandowski", "R. Pascanu"], "venue": "Proc. ICASSP, Vancouver, Canada, May 2013.", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2013}, {"title": "Investigation of recurrent-neural-network architectures and learning methods for spoken language understanding", "author": ["G. Mesnil", "X. He", "L. Deng", "Y. Bengio"], "venue": "Proc. INTERSPEECH, Lyon, France, August 2013.", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2013}, {"title": "Recurrent deep-stacking networks for sequence classification", "author": ["H. Palangi", "L. Deng", "R. Ward"], "venue": "Signal and Information Processing (ChinaSIP), 2014 IEEE China Summit International Conference on, July 2014, pp. 510\u2013514.", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2014}, {"title": "Learning input and recurrent weight matrices in echo state networks", "author": ["H. Palangi", "L. Deng", "R.K. Ward"], "venue": "NIPS Workshop on Deep Learning, December 2013. [Online]. Available: http://research.microsoft.com/apps/pubs/default.aspx? id=204701", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2013}, {"title": "Learning to forget: Continual prediction with lstm", "author": ["F.A. Gers", "J. Schmidhuber", "F. Cummins"], "venue": "Neural Computation, vol. 12, pp. 2451\u20132471, 1999.", "citeRegEx": "32", "shortCiteRegEx": null, "year": 1999}, {"title": "Learning precise timing with lstm recurrent networks", "author": ["F.A. Gers", "N.N. Schraudolph", "J. Schmidhuber"], "venue": "J. Mach. Learn. Res., vol. 3, pp. 115\u2013143, Mar. 2003.", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2003}, {"title": "A method of solving a convex programming problem with convergence rate o (1/k2)", "author": ["Y. Nesterov"], "venue": "Soviet Mathematics Doklady, vol. 27, pp. 372\u2013376, 1983.", "citeRegEx": "34", "shortCiteRegEx": null, "year": 1983}, {"title": "On the importance of initialization and momentum in deep learning", "author": ["I. Sutskever", "J. Martens", "G.E. Dahl", "G.E. Hinton"], "venue": "ICML (3)\u201913, 2013, pp. 1139\u20131147.", "citeRegEx": "35", "shortCiteRegEx": null, "year": 2013}, {"title": "Gradient-based learning applied to document recognition", "author": ["Y. Lecun", "L. Bottou", "Y. Bengio", "P. Haffner"], "venue": "Proceedings of the IEEE, vol. 86, no. 11, pp. 2278\u20132324, Nov 1998. [Online]. Available: http://yann.lecun.com/exdb/mnist/", "citeRegEx": "36", "shortCiteRegEx": null, "year": 1998}], "referenceMentions": [{"referenceID": 0, "context": "To compute these probabilities, we propose to use Long Short-Term Memory (LSTM) [1], a bottom up data driven model for sequence modelling.", "startOffset": 80, "endOffset": 83}, {"referenceID": 1, "context": "By performing extensive experiments on two real world datasets, we show that the proposed method significantly outperforms general MMV solver Simultaneous Orthogonal Matching Pursuit (SOMP) and model based Bayesian methods including Multitask Compressive Sensing [2] and Sparse Bayesian Learning for Temporally Correlated Sources [3].", "startOffset": 263, "endOffset": 266}, {"referenceID": 2, "context": "By performing extensive experiments on two real world datasets, we show that the proposed method significantly outperforms general MMV solver Simultaneous Orthogonal Matching Pursuit (SOMP) and model based Bayesian methods including Multitask Compressive Sensing [2] and Sparse Bayesian Learning for Temporally Correlated Sources [3].", "startOffset": 330, "endOffset": 333}, {"referenceID": 3, "context": "COMPRESSIVE Sensing (CS) [4],[5],[6] is an effective approach for acquiring sparse signals by which both sensing and compression are performed at the same time.", "startOffset": 25, "endOffset": 28}, {"referenceID": 4, "context": "COMPRESSIVE Sensing (CS) [4],[5],[6] is an effective approach for acquiring sparse signals by which both sensing and compression are performed at the same time.", "startOffset": 29, "endOffset": 32}, {"referenceID": 5, "context": "COMPRESSIVE Sensing (CS) [4],[5],[6] is an effective approach for acquiring sparse signals by which both sensing and compression are performed at the same time.", "startOffset": 33, "endOffset": 36}, {"referenceID": 6, "context": ", \u03a8 \u2208 <N\u00d7N1 where N < N1 (compressed sensing for over-complete dictionaries is introduced in [7]).", "startOffset": 93, "endOffset": 96}, {"referenceID": 7, "context": "Different application areas of MMV include magnetoencephalography, array processing, equalization of sparse communication channels and cognitive radio [8].", "startOffset": 151, "endOffset": 154}, {"referenceID": 8, "context": "Then, the necessary and sufficient condition to obtain a unique S given Y is [9]:", "startOffset": 77, "endOffset": 80}, {"referenceID": 9, "context": "Generally, on average, solving the MMV problem jointly can lead to better uniqueness guarantees than solving the SMV problem for each vector independently [10].", "startOffset": 155, "endOffset": 159}, {"referenceID": 10, "context": "In the current MMV literature, a jointly sparse matrix is recovered typically by one of the following methods: 1) greedy methods [11] like Simultaneous Orthogonal Matching Pursuit (SOMP) which performs non-optimal subset selection, 2) relaxed mixed norm minimization methods [12], or 3) Bayesian methods like [13], [2], [3] where assuming a prior belief, e.", "startOffset": 129, "endOffset": 133}, {"referenceID": 11, "context": "In the current MMV literature, a jointly sparse matrix is recovered typically by one of the following methods: 1) greedy methods [11] like Simultaneous Orthogonal Matching Pursuit (SOMP) which performs non-optimal subset selection, 2) relaxed mixed norm minimization methods [12], or 3) Bayesian methods like [13], [2], [3] where assuming a prior belief, e.", "startOffset": 275, "endOffset": 279}, {"referenceID": 12, "context": "In the current MMV literature, a jointly sparse matrix is recovered typically by one of the following methods: 1) greedy methods [11] like Simultaneous Orthogonal Matching Pursuit (SOMP) which performs non-optimal subset selection, 2) relaxed mixed norm minimization methods [12], or 3) Bayesian methods like [13], [2], [3] where assuming a prior belief, e.", "startOffset": 309, "endOffset": 313}, {"referenceID": 1, "context": "In the current MMV literature, a jointly sparse matrix is recovered typically by one of the following methods: 1) greedy methods [11] like Simultaneous Orthogonal Matching Pursuit (SOMP) which performs non-optimal subset selection, 2) relaxed mixed norm minimization methods [12], or 3) Bayesian methods like [13], [2], [3] where assuming a prior belief, e.", "startOffset": 315, "endOffset": 318}, {"referenceID": 2, "context": "In the current MMV literature, a jointly sparse matrix is recovered typically by one of the following methods: 1) greedy methods [11] like Simultaneous Orthogonal Matching Pursuit (SOMP) which performs non-optimal subset selection, 2) relaxed mixed norm minimization methods [12], or 3) Bayesian methods like [13], [2], [3] where assuming a prior belief, e.", "startOffset": 320, "endOffset": 323}, {"referenceID": 1, "context": ", Multitask Bayesian Compressive Sensing (MTBCS)[2] and Sparse Bayesian Learning for temporally correlated sources (T-SBL)[3]).", "startOffset": 48, "endOffset": 51}, {"referenceID": 2, "context": ", Multitask Bayesian Compressive Sensing (MTBCS)[2] and Sparse Bayesian Learning for temporally correlated sources (T-SBL)[3]).", "startOffset": 122, "endOffset": 125}, {"referenceID": 13, "context": "Related Work Exploiting structures besides sparsity for compressive sensing has been extensively studied in the literature [14], [8], [15], [2], [13], [3], [16], [17].", "startOffset": 123, "endOffset": 127}, {"referenceID": 7, "context": "Related Work Exploiting structures besides sparsity for compressive sensing has been extensively studied in the literature [14], [8], [15], [2], [13], [3], [16], [17].", "startOffset": 129, "endOffset": 132}, {"referenceID": 14, "context": "Related Work Exploiting structures besides sparsity for compressive sensing has been extensively studied in the literature [14], [8], [15], [2], [13], [3], [16], [17].", "startOffset": 134, "endOffset": 138}, {"referenceID": 1, "context": "Related Work Exploiting structures besides sparsity for compressive sensing has been extensively studied in the literature [14], [8], [15], [2], [13], [3], [16], [17].", "startOffset": 140, "endOffset": 143}, {"referenceID": 12, "context": "Related Work Exploiting structures besides sparsity for compressive sensing has been extensively studied in the literature [14], [8], [15], [2], [13], [3], [16], [17].", "startOffset": 145, "endOffset": 149}, {"referenceID": 2, "context": "Related Work Exploiting structures besides sparsity for compressive sensing has been extensively studied in the literature [14], [8], [15], [2], [13], [3], [16], [17].", "startOffset": 151, "endOffset": 154}, {"referenceID": 15, "context": "Related Work Exploiting structures besides sparsity for compressive sensing has been extensively studied in the literature [14], [8], [15], [2], [13], [3], [16], [17].", "startOffset": 156, "endOffset": 160}, {"referenceID": 16, "context": "Related Work Exploiting structures besides sparsity for compressive sensing has been extensively studied in the literature [14], [8], [15], [2], [13], [3], [16], [17].", "startOffset": 162, "endOffset": 166}, {"referenceID": 13, "context": "In [14], it has theoretically been shown that using signal models that exploit these structures will result in a decrease in the number of measurements, i.", "startOffset": 3, "endOffset": 7}, {"referenceID": 7, "context": "In [8], a thorough review on the CS methods that use structure presented in the sparse signal or in the measurements is presented.", "startOffset": 3, "endOffset": 6}, {"referenceID": 14, "context": "In [15], a Bayesian framework for CS is presented.", "startOffset": 3, "endOffset": 7}, {"referenceID": 17, "context": "It then uses a Relevance Vector Machine (RVM) [18] to estimate the entries of the sparse vector.", "startOffset": 46, "endOffset": 50}, {"referenceID": 1, "context": "In [2], a Bayesian framework is presented for the MMV problem.", "startOffset": 3, "endOffset": 6}, {"referenceID": 1, "context": "In [2], it is experimentally shown that the MT-", "startOffset": 3, "endOffset": 6}, {"referenceID": 12, "context": "In [13], the Sparse Bayesian Learning (SBL) [18], [19] is used to solve the MMV problem.", "startOffset": 3, "endOffset": 7}, {"referenceID": 17, "context": "In [13], the Sparse Bayesian Learning (SBL) [18], [19] is used to solve the MMV problem.", "startOffset": 44, "endOffset": 48}, {"referenceID": 18, "context": "In [13], the Sparse Bayesian Learning (SBL) [18], [19] is used to solve the MMV problem.", "startOffset": 50, "endOffset": 54}, {"referenceID": 2, "context": "The authors in [3], address the MMV problem when the entries in each row of S are correlated.", "startOffset": 15, "endOffset": 18}, {"referenceID": 12, "context": "An algorithm based on SBL is proposed and it is shown that the proposed algorithm outperforms mixed norm (`1,2) optimization as well as the method proposed in [13].", "startOffset": 159, "endOffset": 163}, {"referenceID": 15, "context": "In [16], a greedy algorithm aided by a neural network is proposed to address the SMV problem in (3).", "startOffset": 3, "endOffset": 7}, {"referenceID": 16, "context": "In [17], an extension of [16] with a hierarchical Deep Stacking Netowork (DSN) [20] is proposed for the MMV problem.", "startOffset": 3, "endOffset": 7}, {"referenceID": 15, "context": "In [17], an extension of [16] with a hierarchical Deep Stacking Netowork (DSN) [20] is proposed for the MMV problem.", "startOffset": 25, "endOffset": 29}, {"referenceID": 19, "context": "In [17], an extension of [16] with a hierarchical Deep Stacking Netowork (DSN) [20] is proposed for the MMV problem.", "startOffset": 79, "endOffset": 83}, {"referenceID": 20, "context": "RNN WITH LSTM CELLS The RNN is a type of deep neural networks [21], [22] that are \u201cdeep\u201d in temporal dimension and it has been used extensively in time sequence modelling [23], [24], [25], [26], [27], [28], [29], [30], [31].", "startOffset": 62, "endOffset": 66}, {"referenceID": 21, "context": "RNN WITH LSTM CELLS The RNN is a type of deep neural networks [21], [22] that are \u201cdeep\u201d in temporal dimension and it has been used extensively in time sequence modelling [23], [24], [25], [26], [27], [28], [29], [30], [31].", "startOffset": 68, "endOffset": 72}, {"referenceID": 22, "context": "RNN WITH LSTM CELLS The RNN is a type of deep neural networks [21], [22] that are \u201cdeep\u201d in temporal dimension and it has been used extensively in time sequence modelling [23], [24], [25], [26], [27], [28], [29], [30], [31].", "startOffset": 171, "endOffset": 175}, {"referenceID": 23, "context": "RNN WITH LSTM CELLS The RNN is a type of deep neural networks [21], [22] that are \u201cdeep\u201d in temporal dimension and it has been used extensively in time sequence modelling [23], [24], [25], [26], [27], [28], [29], [30], [31].", "startOffset": 177, "endOffset": 181}, {"referenceID": 24, "context": "RNN WITH LSTM CELLS The RNN is a type of deep neural networks [21], [22] that are \u201cdeep\u201d in temporal dimension and it has been used extensively in time sequence modelling [23], [24], [25], [26], [27], [28], [29], [30], [31].", "startOffset": 183, "endOffset": 187}, {"referenceID": 25, "context": "RNN WITH LSTM CELLS The RNN is a type of deep neural networks [21], [22] that are \u201cdeep\u201d in temporal dimension and it has been used extensively in time sequence modelling [23], [24], [25], [26], [27], [28], [29], [30], [31].", "startOffset": 189, "endOffset": 193}, {"referenceID": 26, "context": "RNN WITH LSTM CELLS The RNN is a type of deep neural networks [21], [22] that are \u201cdeep\u201d in temporal dimension and it has been used extensively in time sequence modelling [23], [24], [25], [26], [27], [28], [29], [30], [31].", "startOffset": 195, "endOffset": 199}, {"referenceID": 27, "context": "RNN WITH LSTM CELLS The RNN is a type of deep neural networks [21], [22] that are \u201cdeep\u201d in temporal dimension and it has been used extensively in time sequence modelling [23], [24], [25], [26], [27], [28], [29], [30], [31].", "startOffset": 201, "endOffset": 205}, {"referenceID": 28, "context": "RNN WITH LSTM CELLS The RNN is a type of deep neural networks [21], [22] that are \u201cdeep\u201d in temporal dimension and it has been used extensively in time sequence modelling [23], [24], [25], [26], [27], [28], [29], [30], [31].", "startOffset": 207, "endOffset": 211}, {"referenceID": 29, "context": "RNN WITH LSTM CELLS The RNN is a type of deep neural networks [21], [22] that are \u201cdeep\u201d in temporal dimension and it has been used extensively in time sequence modelling [23], [24], [25], [26], [27], [28], [29], [30], [31].", "startOffset": 213, "endOffset": 217}, {"referenceID": 30, "context": "RNN WITH LSTM CELLS The RNN is a type of deep neural networks [21], [22] that are \u201cdeep\u201d in temporal dimension and it has been used extensively in time sequence modelling [23], [24], [25], [26], [27], [28], [29], [30], [31].", "startOffset": 219, "endOffset": 223}, {"referenceID": 0, "context": "One of the effective solutions for this problem in RNNs is using memory cells instead of neurons originally proposed in [1] as Long ShortTerm Memory (LSTM) and completed in [32] and [33] by adding forget gate and peephole connections to the architecture.", "startOffset": 120, "endOffset": 123}, {"referenceID": 31, "context": "One of the effective solutions for this problem in RNNs is using memory cells instead of neurons originally proposed in [1] as Long ShortTerm Memory (LSTM) and completed in [32] and [33] by adding forget gate and peephole connections to the architecture.", "startOffset": 173, "endOffset": 177}, {"referenceID": 32, "context": "One of the effective solutions for this problem in RNNs is using memory cells instead of neurons originally proposed in [1] as Long ShortTerm Memory (LSTM) and completed in [32] and [33] by adding forget gate and peephole connections to the architecture.", "startOffset": 182, "endOffset": 186}, {"referenceID": 33, "context": "method in [34].", "startOffset": 10, "endOffset": 14}, {"referenceID": 34, "context": "1 of [35] where Nesterov method is derived as a momentum method.", "startOffset": 5, "endOffset": 9}, {"referenceID": 33, "context": "To accelerate the convergence, we have used Nesterov method [34] and found it effective in training the proposed model for the MMV problem.", "startOffset": 60, "endOffset": 64}, {"referenceID": 35, "context": "We have performed experiments on two real world datasets, MNIST dataset of handwritten digits [36] and three different classes of images from natural image dataset of Microsoft Research in Cambridge [37].", "startOffset": 94, "endOffset": 98}, {"referenceID": 14, "context": "We have compared the performance of the proposed reconstruction algorithm (LSTM-CS) with 5 reconstruction algorithms for the MMV problem, Simultaneous Orthogonal Matching Pursuit (SOMP) which is a well known baseline for the MMV problem, Bayesian Compressive Sensing (BCS)[15] applied independently for each channel, Multitask Compressive Sensing (MT-BCS) [2] which takes into account the statistical dependency of different channels, Sparse Bayesian Learning for Temporally correlated sources (T-SBL) [3] which exploits correlation among different sources in the MMV problem and Nonlinear Weighted SOMP (NWSOMP) [17] which solves a regression problem to help SOMP algorithm with prior knowledge from training data.", "startOffset": 272, "endOffset": 276}, {"referenceID": 1, "context": "We have compared the performance of the proposed reconstruction algorithm (LSTM-CS) with 5 reconstruction algorithms for the MMV problem, Simultaneous Orthogonal Matching Pursuit (SOMP) which is a well known baseline for the MMV problem, Bayesian Compressive Sensing (BCS)[15] applied independently for each channel, Multitask Compressive Sensing (MT-BCS) [2] which takes into account the statistical dependency of different channels, Sparse Bayesian Learning for Temporally correlated sources (T-SBL) [3] which exploits correlation among different sources in the MMV problem and Nonlinear Weighted SOMP (NWSOMP) [17] which solves a regression problem to help SOMP algorithm with prior knowledge from training data.", "startOffset": 356, "endOffset": 359}, {"referenceID": 2, "context": "We have compared the performance of the proposed reconstruction algorithm (LSTM-CS) with 5 reconstruction algorithms for the MMV problem, Simultaneous Orthogonal Matching Pursuit (SOMP) which is a well known baseline for the MMV problem, Bayesian Compressive Sensing (BCS)[15] applied independently for each channel, Multitask Compressive Sensing (MT-BCS) [2] which takes into account the statistical dependency of different channels, Sparse Bayesian Learning for Temporally correlated sources (T-SBL) [3] which exploits correlation among different sources in the MMV problem and Nonlinear Weighted SOMP (NWSOMP) [17] which solves a regression problem to help SOMP algorithm with prior knowledge from training data.", "startOffset": 502, "endOffset": 505}, {"referenceID": 16, "context": "We have compared the performance of the proposed reconstruction algorithm (LSTM-CS) with 5 reconstruction algorithms for the MMV problem, Simultaneous Orthogonal Matching Pursuit (SOMP) which is a well known baseline for the MMV problem, Bayesian Compressive Sensing (BCS)[15] applied independently for each channel, Multitask Compressive Sensing (MT-BCS) [2] which takes into account the statistical dependency of different channels, Sparse Bayesian Learning for Temporally correlated sources (T-SBL) [3] which exploits correlation among different sources in the MMV problem and Nonlinear Weighted SOMP (NWSOMP) [17] which solves a regression problem to help SOMP algorithm with prior knowledge from training data.", "startOffset": 613, "endOffset": 617}, {"referenceID": 2, "context": "ACKNOWLEDGEMENT We want to thank the authors of [3] and [15] and [2] for making the code of their work available.", "startOffset": 48, "endOffset": 51}, {"referenceID": 14, "context": "ACKNOWLEDGEMENT We want to thank the authors of [3] and [15] and [2] for making the code of their work available.", "startOffset": 56, "endOffset": 60}, {"referenceID": 1, "context": "ACKNOWLEDGEMENT We want to thank the authors of [3] and [15] and [2] for making the code of their work available.", "startOffset": 65, "endOffset": 68}], "year": 2017, "abstractText": "We address the problem of compressed sensing with Multiple Measurement Vectors (MMVs) when the structure of sparse vectors in different channels depend on each other. The sparse vectors are not necessarily joint sparse. We capture this dependency by computing the conditional probability of each entry of each sparse vector to be non-zero given \u201cresiduals\u201d of all previous sparse vectors. To compute these probabilities, we propose to use Long Short-Term Memory (LSTM) [1], a bottom up data driven model for sequence modelling. To compute model parameters we minimize a cross entropy cost function. We propose a greedy solver that uses above probabilities at the decoder. By performing extensive experiments on two real world datasets, we show that the proposed method significantly outperforms general MMV solver Simultaneous Orthogonal Matching Pursuit (SOMP) and model based Bayesian methods including Multitask Compressive Sensing [2] and Sparse Bayesian Learning for Temporally Correlated Sources [3]. Nevertheless, we emphasize that the proposed method is a data driven method where availability of training data is important. However, in many applications, train data is indeed available, e.g., recorded images or video.", "creator": "LaTeX with hyperref package"}}}