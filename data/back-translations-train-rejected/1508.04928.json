{"id": "1508.04928", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "20-Aug-2015", "title": "Duration and Interval Hidden Markov Model for Sequential Data Analysis", "abstract": "Analysis of sequential event data has been recognized as one of the essential tools in data modeling and analysis field. In this paper, after the examination of its technical requirements and issues to model complex but practical situation, we propose a new sequential data model, dubbed Duration and Interval Hidden Markov Model (DI-HMM), that efficiently represents \"state duration\" and \"state interval\" of data events. This has significant implications to play an important role in representing practical time-series sequential data. This eventually provides an efficient and flexible sequential data retrieval. Numerical experiments on synthetic and real data demonstrate the efficiency and accuracy of the proposed DI-HMM.", "histories": [["v1", "Thu, 20 Aug 2015 09:09:45 GMT  (1420kb,D)", "http://arxiv.org/abs/1508.04928v1", null]], "reviews": [], "SUBJECTS": "cs.AI", "authors": ["hiromi narimatsu", "hiroyuki kasai"], "accepted": false, "id": "1508.04928"}, "pdf": {"name": "1508.04928.pdf", "metadata": {"source": "CRF", "title": "Duration and Interval Hidden Markov Model for Sequential Data Analysis", "authors": ["Hiromi Narimatsu", "Hiroyuki Kasai"], "emails": ["narimatsu@appnet.is.uec.ac.jp", "kasai@is.uec.ac.jp"], "sections": [{"heading": null, "text": "In fact, we are able to go in search of a solution that is capable of finding a solution, that is capable of finding a solution, that is capable of resolving the problems, and that is able to find a solution that is capable of finding a solution, that is capable of finding a solution, that is capable of finding a solution, that is capable of finding a solution, that is capable of finding a solution, that is capable of finding a solution, that is capable of finding a solution, that is capable of finding a solution, that is capable of finding a solution, that is capable of finding a solution, that is capable of finding a solution, that is capable of finding a solution."}, {"heading": "II. RELATED WORK", "text": "This section is about an explanation for sequential data analysis."}, {"heading": "III. SEQUENTIAL DATA ANALYSIS", "text": "This section describes the model requirements for sequential data analysis and compares the requirements and satisfaction of each advanced HMM."}, {"heading": "A. Notations", "text": "The symbols and the markers are defined a priori in this section. First, if we set a certain time as \u00b7 dn = 1, we consider the sequence of what is constitutional 1 \u2264 t \u2264 T. The observation at the time t is represented as ot, and the observation sequence beginning at t = t1 to t = t2 is represented as ot1: t2 = ot1, \u00b7 \u00b7 \u00b7 ot2. Then the target observation sequence to which a significant designation must be assigned, starting at t = 1 and ending at t = T is represented as o1: T = o1, \u00b7 oT, and the set of observable values is V = {v1, \u00b7 \u00b7, vK}. Next, an elementary state is called selm, and each state selm has a different length of time units defined as delm."}, {"heading": "B. Requirement for Model Description", "text": "In this section, the requirements for the sequential data model are discussed on the basis of time series data: representative data of sequential data must be described in several cases, as in Figure 1. Suppose that two different sequences in which successive states, i.e. events, are observed by two different sensors. A state is represented as a block whose width represents its continuous state duration. Furthermore, these events are not continuously observed, i.e. there may be a discontinuous interval time between two observed states. Thus, the length of this unobserved period is represented by the distance between two successive blocks. On the other hand, the grey colored state in Figure 1 illustrates the extracted state sequence, s1: T, which forms a group of state sequences, s1: T, in this figure consists of four states: s1, s2, s3 and s4.Taking the above example, we examine the requirements for the sequential data model Rvalence."}, {"heading": "C. Requirement Assessment for Extended HMM Methods", "text": "This section examines whether HMM and advanced HMM methods meet the requirements analyzed in the previous section. Table I presents a comparison of conventional HMM methods from the perspective of model requirements. The basic HMM methods represent the sequence of states, and all advanced HMM methods inherit this capability. IO-HMM treats the duration of the state, but the objective of the estimation is different. Therefore, it does not meet the requirements of the model. HSMM is proposed to model the remaining duration in order to stay in the same state. Furthermore, HMM-Selftrans and EDM are the advanced models of HSMM. These methods meet the same requirements: the state order and the state duration, but do not support the state interval. As a result of the needs testing, we find that no model can record both the state duration and the state interval at the same time. Nevertheless, HSMM expresses the state duration, and this capability is not supported by any other method closest to the MM, hence we conclude that the MM section is best for the next method."}, {"heading": "IV. HIDDEN SEMI-MARKOV MODEL (HSMM)", "text": "The decisive difference between HMM and HSMM is the number of observations per state. HSMM treats the duration of the stay in a state by introducing an additional parameter specializing in the description of the duration of the transition and the emission probabilities. Assuming that the duration of the state S1 is d1, and {o1, o2, od1} is the concept of the emitted observations of s1 during d1. After the duration of the state d1, s1 is transferred to the next state s1. Assuming that the duration of the state S1 is d1, o2, od1} is the duration of the observations of s1."}, {"heading": "V. DURATION AND INTERVAL HMM (DI-HMM)", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "A. Motivation", "text": "The simplest way to improve HSMM in order to manage both the duration of the state and the state interval is to introduce a special state, the so-called interval state, which describes the time interval between two states. However, it is well known that the introduction of such an interval state between two states reduces the accuracy of discrimination [25]. To address this problem, we extend the conventional HSMM by reintroducing the state interval probability for each transition state between two states. In this paper, this new extended model is called the Duration and Interval Hidden Markov Model (DI-HMM)."}, {"heading": "B. Training Sequential Data Model", "text": "The details of DI-HMM are worked out using sample data, as in Figure 4. The slash-structured blocks represent the data sequence of the training set; ln \u2212 1, n is the time difference between the end of sn \u2212 1 and the beginning of sn. Furthermore, the time interval probability distribution is expressed by the assumption of Gaussian distribution, p (Lm \u2032, m), asp (Lm \u2032, m), asp (Lm \u2032, m) = 1 \u221a 2 e \u2212 (x \u2212 \u00b5) 2 2\u03c32, (5) where the time interval probability distribution is expressed by the assumption of Gaussian distribution, p (Lm \u2032, m), asp (Lm \u2032, m), etc. Then the set of parameters used in DI-HMM is defined as a formation, {a (Sm \u2032, thumb) (SDI \u2032), (Sm \u2032), functions are equal (Sm)."}, {"heading": "C. Probability Estimation for Recognition", "text": "The Viterbi algorithm is used to estimate the probability of a model as [26], then the pair of the model and the described probability are stored as candidates, and finally the maximum probability estimate for each state is calculated for each model. If Lm \u2032, m, i.e. p (Lm \u2032, m), is pre-assigned to the new parameter distribution p (Lm \u2032, m), if Lm \u2032, m is outside the range for the p (Lm \u2032, m), the time interval probability is calculated as asp (Lm \u2032, m) = min 1 \u2264 m \u2264 M, 1 \u2264 Mp (Lm \u2032, m) \u00d7 c (7) Therein, c = 0 \u2264 c \u2264 1. Then the forward variable for the estimation of the maximum probability is calculated as p (Lm \u2032, m)."}, {"heading": "VI. NUMERICAL EXPERIMENTS", "text": "In this section, the following points are compared between DI-HMM and HSMM: Section A describes the discrimination performance, Section B the detection performance, and Section C the calculation time of training and detection of DI-HMM. Figure 5 shows an example of generating synthetic data for the evaluation. First, the starting parameters are the number of states N, the minimum value of the duration dmin, the maximum value of the duration dmax, the minimum value of the state interval lmin, and the maximum value of the state interval lmax. Also, the length of the sequence T is specified in the evaluation. In the example of Figure 5, the number of states and durations N and the number of intervals N \u2212 1. The lengths of each duration and each state interval are incremented one by one, and then all these lengths are combined with round robin."}, {"heading": "A. Discrimination Performance", "text": "First, we create 200 different sequences by using T = 14, N = {3, 4}, dmin = 1, dmax = 10, lmin = 1, and lmax = 4. Figure 6 shows the sample data generated. Each line represents a sequence of data. The gray blocks are observed, and the length of the states represents the duration. To evaluate the discrimination performance, we compare the probabilities calculated using the sequence data shown in Figure 6. Discrimination means that the probability for each training date differs from another. Figure 7 shows the results of the data 5, 10, and 15, extracted from the sequence data shown in Figure 6. The x-axis shows each training date, and the y-axis shows the probability for each test date. From this figure, the probability of HSMM is derived for each training date different."}, {"heading": "B. Recognition Performance", "text": "In fact, it is such that most of them will be able to move to another world, in which they are able to stay in the world, in which they are able to live, in which they are able to live, in which they are able to live, in which they are able to live, in which they are living, in which they are able to live, in which they are living, in which they are living, in which they are living, in which they are living, in which they are living, in which they are living, in which they are living, in which they are living, in which they are living, in which they are living, in which they are living, in which they are living, in which they are living, in which they are living, in which they are living, in which they are living, in which they are living, in which they are living, in which they are living, in which they are living, in which they are living, in which they are living, in which they are living, in which they are living, in which they are living, in which they are living, in which they are living, in which they are living, in which they are living, in which they are living, in which they are living, in which they are living, in which they are living, in which they are living, in which they are living, in which they are living, in which they are living, in which they are living, in which they are living, in which they are living, in which they are living, in which they are living, in which they are living, in which they are living, in which they are living, in which they are living, in which they are living, in which they are living, in which they are living, in which they are living, in which they are living, in which they are living, in which they are living, in which they are living, in which they are living, in which they are living, in which they are living, in which they are living, in which they are living, in which they are living, in which they are living, in which they are living, in which they are living, in which they are living, in which they are living, in which they are living, in which they are living, in which they live in which they live, in which they live, in which they live, in which they live in which they live, in which they live"}, {"heading": "C. Calculation time of Training and Recognition", "text": "For the evaluation of the calculation time, we generate 35 sequences, whereby we do not specify dmin = dmax = 2, lmin = 1, lmax = 10 and T a priori. On the basis of the generated data, we compare training time and detection time with simultaneous changes in the number of training data. The results of the training time and detection time are shown in Figure 12 and Figure 13, respectively. The x-axis shows the number of training data and the y-axis the calculation time for training / detection. The red line is the result of the DI-HMM, the blue line is the result of the HSMM. The slope of the detection time and training time in the DI-HMM are steeper than that of the HSMM. The introduction of the interval probability in the HSMM should therefore cause additional calculation costs. However, it does not seriously affect the total amount of the calculation. Meanwhile, as shown in the preceding sections, the detection performance and the discrimination performance of the DI-MM are higher than the effective results we suggest for this paper, which are very effective for MDI-MDI."}, {"heading": "VII. SUMMARY AND FUTURE WORK", "text": "As described here, we investigated the requirements for sequential data analysis by focusing on the structure and function of sequential data. Subsequently, we proposed Duration and Interval HMM (DI-HMM) by introducing the interval probability to HSMM to handle both the state duration and the state interval. We evaluated the discrimination performance, the detection performance, and measured the computation time for training and detection by computer simulation. To evaluate the discrimination performance, DI-HMM can distinguish between different sequences with less training data. In addition, the error rate of discrimination is less than 0.1 when we selectively train more than two sequences. Therefore, DI-HMM is powerful to find the sequence that is not included in the training data. This feature allows us to easily incorporate new labels into existing databases of training data. Furthermore, the evaluation results obtained using solid data show that we can treat the HMM performance as the additional rhythm pattern, so that we can handle the MDI need for higher rhythm patterns."}], "references": [{"title": "Finding sequential patterns from large sequence data", "author": ["M. Esmaeili", "F. Gabor"], "venue": "Informational Journal of Computer Science Issues (IJSC), vol. 7, no. 1, 2010.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2010}, {"title": "Finding sequential patterns from large sequence data", "author": ["D.D. Lewis", "W.A. Gale"], "venue": "Proc. of ACM the 17th annual international conference on Research and Development in information retrieval (ACM SIGIR), 1994, pp. 3\u201312.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 1994}, {"title": "A sequential clustering algorithm with application to gene expression data", "author": ["J. Song", "D.L. Nicolae"], "venue": "Journal of the Korean Statistical Society, vol. 38, no. 2, pp. 175\u2013184, 2009.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2009}, {"title": "Data mining for wearable sensors in health monitoring systems: A review of recent trends and challenges", "author": ["H. Banaee", "M. Ahmed", "A. Loutfi"], "venue": "Sensors 2013, vol. 13, no. 12, pp. 17 472\u201317 500, Dec. 2013.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2013}, {"title": "Sequential bayesian estimation with censored data for multi-sensor systems", "author": ["Y. Zheng", "R. Niu", "P.K. Varshney"], "venue": "IEEE Trans. on Sygnal Processing, vol. 62, no. 16, May 2014.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2014}, {"title": "Learning and recognizing the hierarchical and sequential structure of human activities", "author": ["H. Cheng"], "venue": "Ph.D. dissertation, Carnegie Mellon University, Dec. 2013.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2013}, {"title": "A dynamic programming approach to continuous speech recognition", "author": ["H. Sakoe", "S. Chiba"], "venue": "Proc. of 7th ICA 1971, vol. 11, no. C13, 1971.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 1971}, {"title": "Support vector machines for pattern classification", "author": ["S. Abe"], "venue": "Springer Science and Business Media, July 2010.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2010}, {"title": "Bayesian updating in recursive graphical models by local computations", "author": ["F.V. Jensen", "S. Lauritzen", "K.G. Olsen"], "venue": "Computational Statistics and Data Analysis, vol. 4, pp. 269\u2013282, 1990.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 1990}, {"title": "Dynamic time-alignment kernel in support vector machine", "author": ["H. Shimodaira", "K. Noma", "M. Nakai", "S. Sagayama"], "venue": "Advances in Neural Information Processing Systems, Cambridge, MA, MIT Press, 2002.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2002}, {"title": "Deep learning for sequential pattern recognition", "author": ["P. Safari"], "venue": "Master\u2019s thesis, Faculty of Electrical Engineering and Information Technology, Technical University Munich, 2013.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2013}, {"title": "Statistical inference for probabilistic functions of finite state markov chains", "author": ["L.E. Baum", "T. Petrie"], "venue": "Ann. Math. Statist, vol. 37, pp. 1554\u20131563, 1966.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 1966}, {"title": "Hidden markov models", "author": ["S.R. Eddy"], "venue": "Elsevier Current Opinion in Structural Biology, vol. 6, no. 3, pp. 277\u2013427, June 1996.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 1996}, {"title": "Hidden morkov models combining discrete symbols and continuous attributes in handwriting recognition", "author": ["H. Xue", "V. Govindaraju"], "venue": "IEEE Trans. on Pattern Analysis and Machine Intelligence, vol. 28, no. 3, pp. 458\u2013462, Mar. 2006.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2006}, {"title": "An input-output HMM architecture", "author": ["Y. Bengio", "P. Frasconi"], "venue": "Advances in Neural Information Processing Systems, 1995.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 1995}, {"title": "Non-stationary fuzzy markov chain", "author": ["F. Salzenstein", "C. Collet", "S. Lecam", "M. Hatt"], "venue": "Pattern Recognition Letters, vol. 28, no. 16, pp. 2201\u2013 2208, Dec. 2007.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2007}, {"title": "An efficient forward-backward algorithm for an explicit duration hidden markov model", "author": ["S.Z. Yu", "H. Kobayashi"], "venue": "IEEE Signal Processing Letters, vol. 10, no. 1, pp. 11\u201314, Jan. 2002.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2002}, {"title": "The infinite hidden markov model", "author": ["M.J. Beal", "Z. Ghahramani", "C.E. Rasmussen"], "venue": "Machine Learning, MIT Press, pp. 29\u2013245, 2002.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2002}, {"title": "Hidden semi-markov models", "author": ["S.Z. Yu"], "venue": "Elsevier Artificial Intelligence, vol. 174, pp. 215\u2013243, 2010.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2010}, {"title": "Hidden semi-markov models (HSMMs)", "author": ["K.P. Murphy"], "venue": "Nov. 2002.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2002}, {"title": "A hidden semi-markov model with missing data and multiple observation sequences for mobility tracking", "author": ["S.Z. Yu", "H. Kobayashi"], "venue": "Elsevier Science B.V. Signal Processing, vol. 83, pp. 235\u2013250, 2003.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2003}, {"title": "Modeling acoustic transitions in speech by modified hidden markov models with state duration and state duration-dependent observation probabilities", "author": ["Y.K. Park", "C.K. UN", "O.W. Kwon"], "venue": "IEEE Trans. on Speech and Audio Processing, vol. 4, no. 5, pp. 289\u2013392, Sep. 1996.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 1996}, {"title": "Map speaker adaptation of state duration distribution for speech recognition", "author": ["Y.B. Yoma", "J.S. Sanchez"], "venue": "Trans. on IEEE Speech and Audio Processing, vol. 10, no. 7, pp. 443\u2013450, Oct. 2002.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2002}, {"title": "The viterbi algorithm", "author": ["G. Forney"], "venue": "Proc. of IEEE, vol. 61, no. 3, pp. 268\u2013278, Mar. 1973.", "citeRegEx": "24", "shortCiteRegEx": null, "year": 1973}, {"title": "Hierarchical multi-channel hidden semi markov models", "author": ["P. Natarajan", "R. Nevaia"], "venue": "Proc. of the 3rd International Joint Conrefence on Artificial Intelligence (IJCAI 07), pp. 2562\u20132567, 2007.", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2007}, {"title": "Dynamic bayesian networks: Representation, inference and learning", "author": ["K. Murphy"], "venue": "Ph.D. dissertation, Dept. Computer Science, UC Berkeley, 2002.", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2002}], "referenceMentions": [{"referenceID": 0, "context": "[1] categorize three types of sequential pattern after theoretical investigation for large amounts of data.", "startOffset": 0, "endOffset": 3}, {"referenceID": 1, "context": "[2] propose a sequential algorithm using special queries to train text classifiers.", "startOffset": 0, "endOffset": 3}, {"referenceID": 2, "context": "[3] propose a sequential clustering algorithm for gene data.", "startOffset": 0, "endOffset": 3}, {"referenceID": 3, "context": "More recently, the studies using sensor data analysis for human behavior recognition and video data understanding have received significant attention because of the significant progress on wearable devices [4][5][6].", "startOffset": 206, "endOffset": 209}, {"referenceID": 4, "context": "More recently, the studies using sensor data analysis for human behavior recognition and video data understanding have received significant attention because of the significant progress on wearable devices [4][5][6].", "startOffset": 209, "endOffset": 212}, {"referenceID": 5, "context": "More recently, the studies using sensor data analysis for human behavior recognition and video data understanding have received significant attention because of the significant progress on wearable devices [4][5][6].", "startOffset": 212, "endOffset": 215}, {"referenceID": 6, "context": "For sequential pattern matching and detection, DP matching [7] extracts similar sequential patterns from different two sequential patterns.", "startOffset": 59, "endOffset": 62}, {"referenceID": 7, "context": "For sequential pattern classification, SVM [8] and Probabilistic Graphical Models [9] are proposed.", "startOffset": 43, "endOffset": 46}, {"referenceID": 8, "context": "For sequential pattern classification, SVM [8] and Probabilistic Graphical Models [9] are proposed.", "startOffset": 82, "endOffset": 85}, {"referenceID": 9, "context": "propose an extended SVM which enables frame-synchronous recognition of sequential pattern [10].", "startOffset": 90, "endOffset": 94}, {"referenceID": 10, "context": "Safari proposes a new model of Deep Learning for sequential pattern recognition [11].", "startOffset": 80, "endOffset": 84}, {"referenceID": 11, "context": "Lastly, HMM [12][13] is a statistical tool for modeling sequence of observations.", "startOffset": 12, "endOffset": 16}, {"referenceID": 12, "context": "Lastly, HMM [12][13] is a statistical tool for modeling sequence of observations.", "startOffset": 16, "endOffset": 20}, {"referenceID": 13, "context": "propose transitionemitting HMMs (TE-HMMs) and state-emitting HMMs (SEHMMs) for treating discontinuous symbols [14].", "startOffset": 110, "endOffset": 114}, {"referenceID": 14, "context": "propose IO-HMM for gesture recognition that maps input sequences to output sequences during learning whereas the original HMM learns only output sequence distributions [15].", "startOffset": 168, "endOffset": 172}, {"referenceID": 15, "context": "deal with a statistical model based on Fuzzy Markov random chains for image segmentation in the context of stationary and non-stationary data [16].", "startOffset": 142, "endOffset": 146}, {"referenceID": 16, "context": "propose Explicit-Duration Hidden Markov Model [17].", "startOffset": 46, "endOffset": 50}, {"referenceID": 17, "context": "propose HMMselftrans that is an extended model of EDM [18].", "startOffset": 54, "endOffset": 58}, {"referenceID": 18, "context": "[19] and Murphy et al.", "startOffset": 0, "endOffset": 4}, {"referenceID": 19, "context": "[20] propose HSMM which is a basic model of EDM.", "startOffset": 0, "endOffset": 4}, {"referenceID": 20, "context": "HSMM is applicable to many applications such as handwriting recognition, human behavior recognition, and other time series data application estimation [21][22][23].", "startOffset": 151, "endOffset": 155}, {"referenceID": 21, "context": "HSMM is applicable to many applications such as handwriting recognition, human behavior recognition, and other time series data application estimation [21][22][23].", "startOffset": 155, "endOffset": 159}, {"referenceID": 22, "context": "HSMM is applicable to many applications such as handwriting recognition, human behavior recognition, and other time series data application estimation [21][22][23].", "startOffset": 159, "endOffset": 163}, {"referenceID": 12, "context": "HMM [13]", "startOffset": 4, "endOffset": 8}, {"referenceID": 14, "context": "IO-HMM [15]", "startOffset": 7, "endOffset": 11}, {"referenceID": 18, "context": "HSMM [19][20] X", "startOffset": 5, "endOffset": 9}, {"referenceID": 19, "context": "HSMM [19][20] X", "startOffset": 9, "endOffset": 13}, {"referenceID": 13, "context": "HMM-selftrans [14] X", "startOffset": 14, "endOffset": 18}, {"referenceID": 16, "context": "EDM [17] X", "startOffset": 4, "endOffset": 8}, {"referenceID": 16, "context": "HSMM is an extended model of conducting HMM using a semi-Markov chain with a variable staying duration for each state [17].", "startOffset": 118, "endOffset": 122}, {"referenceID": 23, "context": "For the estimation of the likelihood probability, we use an extended Viterbi algorithm [24] because it is the most popular algorithm for estimating the maximum likelihood.", "startOffset": 87, "endOffset": 91}, {"referenceID": 24, "context": "However, it is well known that introducing such an interval state between two states degrades the accuracy of discrimination [25].", "startOffset": 125, "endOffset": 129}, {"referenceID": 25, "context": "The Viterbi algorithm is used to estimate the probability of a model as [26].", "startOffset": 72, "endOffset": 76}], "year": 2015, "abstractText": "Analysis of sequential event data has been recognized as one of the essential tools in data modeling and analysis field. In this paper, after the examination of its technical requirements and issues to model complex but practical situation, we propose a new sequential data model, dubbed Duration and Interval Hidden Markov Model (DI-HMM), that efficiently represents \u201cstate duration\u201d and \u201cstate interval\u201d of data events. This has significant implications to play an important role in representing practical time-series sequential data. This eventually provides an efficient and flexible sequential data retrieval. Numerical experiments on synthetic and real data demonstrate the efficiency and accuracy of the proposed DI-HMM. Keywords\u2014Sequential data analysis; HMM; HSMM; State duration; State interval", "creator": "LaTeX with hyperref package"}}}