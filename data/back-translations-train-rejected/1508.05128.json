{"id": "1508.05128", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "20-Aug-2015", "title": "Lifted Relational Neural Networks", "abstract": "We propose a method combining relational-logic representations with deep neural network learning. Domain-specific knowledge is described through relational rules which may be handcrafted or learned. The relational rule-set serves as a template for unfolding possibly deep neural networks whose structures also reflect the structure of given training or testing examples. Different networks corresponding to different examples share their weights, which co-evolve during training by stochastic gradient descend algorithm. Notable relational concepts can be discovered by interpreting shared hidden layer weights corresponding to the rules. Experiments on 78 relational learning benchmarks demonstrate the favorable performance of the method.", "histories": [["v1", "Thu, 20 Aug 2015 21:18:25 GMT  (245kb,D)", "https://arxiv.org/abs/1508.05128v1", "Submitted to ACML'15"], ["v2", "Tue, 13 Oct 2015 12:55:45 GMT  (601kb,D)", "http://arxiv.org/abs/1508.05128v2", "Expanded section on weight learning, added explanation of relationship to convolutional neural networks"]], "COMMENTS": "Submitted to ACML'15", "reviews": [], "SUBJECTS": "cs.AI cs.LG cs.NE", "authors": ["gustav sourek", "vojtech aschenbrenner", "filip zelezny", "ondrej kuzelka"], "accepted": false, "id": "1508.05128"}, "pdf": {"name": "1508.05128.pdf", "metadata": {"source": "CRF", "title": "Lifted Relational Neural Networks", "authors": ["Gustav \u0160ourek", "Vojt\u011bch Aschenbrenner", "Ond\u0159ej Ku\u017eelka"], "emails": ["souregus@fel.cvut.cz", "v@asch.cz", "zelezny@fel.cvut.cz", "KuzelkaO@cardiff.ac.uk"], "sections": [{"heading": null, "text": "Keywords: Relational Learning, Lifting Models, Neural Networks"}, {"heading": "1. Introduction", "text": "This year it is more than ever before."}, {"heading": "2. Preliminaries", "text": "In fact, most of them are able to survive on their own."}, {"heading": "3. Lifted Relational Neural Networks", "text": "We define the primer of the LRNN as N = (h.2). (h.2) We define the primer of the LRNN as N = (h.2). (h.2) We define the primer of the LRNN as N = (h.2). (h.2) We define the primer of the LRNN as N = (h.2). (h.2) We define the primer of the LRNN as N = (h.2). (h.2) We define the primer of the LRNN as N = (h.2). (h.2) We define the primer of the LRNN as N = (h.2). (h.2) We define the primer of the LRNN as N = (h.2)."}, {"heading": "4. Some LRNN Modeling Constructs", "text": "In this section, we will describe several constructs that are simple in LRNNs, but would be difficult or impossible to implement in other existing systems that combine logic and neural networks only because the other systems, unlike LRNNNs, do not allow simultaneous learning of target and auxiliary predicates. Furthermore, while similar constructs are used in principle in probabilistic logical programming systems such as Problog (De Raedt et al., 2007), costly EM algorithms could be executed in learning that repeatedly have to perform mathematically expensive probabilistic conclusions."}, {"heading": "4.1. Implicit Soft Clustering", "text": "In many areas, clusters of specific objects need to be created in order to achieve a good generalization, such as in the prediction of drug side effects, where significant improvements in predictive accuracy have been achieved through methods that have been able to create additional clusters of similar drugs (Davis et al., 2012). However, the existing methods are still more ad hoc, relying on greedy discrete clustering. In LRNNNs, it is easy to define predictors that automate these clusters and use to predict target predictors, as illustrated by the following examples."}, {"heading": "4.2. Soft Matching", "text": "The next example explains the idea of a construct called soft matching and how it can be modeled in LRNNN. Example 9 Let's look again at the example for predicting influenza. Let's assume that we have the reasonable rule that if X is in a group of 4 people who are friends and all have flu symptoms, X fluw (1) 1: HasFlu (X) \u2190 Clique (W, X, Y, Z): Flux symptoms (W): Flux symptoms (Y): Flux symptoms (Z): Flux symptoms (Z): Flux symptoms (Z): Flux symptoms (Z): Flux symptoms (X): Flux symptoms (Z): Flux symptoms (Y): Flux symptoms (Y): Flux symptoms (Z): Flux symptoms (Y): Flux: (Z): Flux: (Z), Flux: (Z), Flux: (W), Flux: (W), Flux: (however:), Flux: (Z), Flux: (Z), Flux: (W), Flux: (W), Flux: (however:), Flux: (Z), Flux: (Z), Flux: (W: (W), Flux: (W:)."}, {"heading": "4.3. Other LRNN Concepts", "text": "While it is possible that we apply the patterns we apply in practice to other areas, there are other models that are easy to implement, such as: \"There are only a limited number of cases that depend on the existence of substructures.\" If the patterns have the same structure, for example, they are all aromatic hexagons with substitutions.3 Let us look at the problem of predicting a property, for example, the way in which this property could be applied to achieve a probability distribution in different places, so that the substitutions that occur jointly in the patterns have a high probability and the other substitutions have a low probability."}, {"heading": "5. Weight Learning", "text": "The aim of the learning process is to find J-J, in which the costs and costs of learning in the bases of learning and learning in the bases of learning and learning in the bases of learning and learning in the bases of learning and learning in the bases of learning and learning in the bases of learning and learning in the bases of learning. (qmkm, t m km) \"We also have a sentence in which the bases of learning and learning in the bases of learning are not necessary, but it simplifies this procedure. (qmkm, t m km)\" Where the bases of learning lie in the bases of learning, we will only simplify the learning processes from the facts, and tji are their target values. For each query, which we have the results of learning in the bases of learning and learning in the bases of learning and learning in the bases of learning and learning in the bases of learning, the bases of learning and learning in the bases of learning of the, and the bases of learning of learning in the bases of learning of the, and the bases of learning of the learning of the, and the"}, {"heading": "6. Related Work", "text": "In fact, most of them are able to play by the rules that they have imposed on themselves, and they are able to play by the rules that they have imposed on themselves."}, {"heading": "7. Experiments", "text": "This year we have to do it with a similar approach as in previous years., \",,\" We, \"he says,\",, \"We,\" \"We,\" \"We,\" \"We,\" \"We,\" \"We,\" \"We,\" \"We,\" \"We,\" \"We,\" \"We,\" \"We,\" \"We,\" \"We,\" \"We,\" \"We,\" \"We,\" \"We,\" \"We,\" We, \"\" We, \"\" We, \"\" We, \"\" We, \"\" We, \"\" We, \"\" We, \"\" We, \"\" We, \"\" We, \"We,\" \"We,\" \"We,\" \"We,\" \"We,\" \"We,\" We, \"We,\" We, \"We,\" We, \"We,\" We, \"We,\" We, \"We,\" We, \"We,\" We, \"We,\" \"We,\" We, \"We,\" We, \"We,\" We, \"\" We, \"We,\" We, \"\" We, \"We,\" We, \"We,\" \"We,\" We, \"\" We, \"We,\" We, \"\" We, \"\" We, \"\" We, \"\" We, \"We,\" \"We,\" We, \"\" We, \"We,\" \"\" We, \"We,\" \"We,\" \"We,\" We, \"\" We, \"We,\" \"We,\" We, \"\" \"We,\" We, \"We,\" \"\" We, \"We,\" We, \"\" \"We,\" We, \"\" We, \"We,\" We, \"We,\" \"\" We, \"We,\" We, \"We,\" \"\" \"We,\" \"We,\" We, \"\" We, \"We,\" \"We,\" We, \"We,\" \"\" We, \"We,\" \"We,\" \"We,\" \"We,\" \"We,\" \"We,\" \"We,\" \"We,\" \"We,\" \"We,\" \"\" \"\" We, \"\" \"\" We, \"We,\" \"\" We, \"we,\" \"we,\" we, \"we,\" we, \"we,\""}, {"heading": "8. Conclusions", "text": "In this paper, we have presented a method that combines relationship logic representations with feedback-forward neural networks, which is similar in spirit to upscale graphical models, as it can be considered an upscale model for the construction of ground neural networks. Experiments conducted indicate that it is possible to achieve state-of-the-art predictive accuracies by weight learning using very generic templates, and that it is capable of inducing remarkable auxiliary concepts. There are many directions for future work, including structural learning, transfer learning, or the study of various collections of activation functions. An important future direction is also the question of expanding LRNNs to support recursion."}, {"heading": "Acknowledgments", "text": "GS and FZ are supported by the Cisco-sponsored research project \"Modelling network traffic with relational properties.\" OK was supported by the Czech Science Foundation through project P202 / 12 / 2032 and now with a grant from the Leverhulme Trust (RPG-2014-164)."}], "references": [{"title": "Dimensions of Neural-symbolic Integration - A Structured Survey", "author": ["Sebastian Bader", "Pascal Hitzler"], "venue": "arXiv preprint,", "citeRegEx": "Bader and Hitzler.,? \\Q2005\\E", "shortCiteRegEx": "Bader and Hitzler.", "year": 2005}, {"title": "Using neural networks for relational learning", "author": ["H Blockeel", "W Uwents"], "venue": "In ICML-2004 Workshop on Statistical Relational Learning and its Connection to Other Fields,", "citeRegEx": "Blockeel and Uwents.,? \\Q2004\\E", "shortCiteRegEx": "Blockeel and Uwents.", "year": 2004}, {"title": "Combining first order logic with connectionist learning", "author": ["M Botta", "Giordana A", "R Piola"], "venue": "In Proceedings of the 14th International Conference on Machine Learning,", "citeRegEx": "Botta et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Botta et al\\.", "year": 1997}, {"title": "On the performance of indirect encoding across the continuum of regularity", "author": ["Jeff Clune", "Kenneth O. Stanley", "Robert T. Pennock", "Charles Ofria"], "venue": "IEEE Trans. Evolutionary Computation,", "citeRegEx": "Clune et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Clune et al\\.", "year": 2011}, {"title": "Neural-Symbolic Learning Systems: Foundations and Applications", "author": ["Artur S. d\u2019Avila Garcez", "Krysia Broda", "Dov M. Gabbay"], "venue": null, "citeRegEx": "Garcez et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Garcez et al\\.", "year": 2012}, {"title": "Demand-driven clustering in relational domains for predicting adverse drug events", "author": ["Jesse Davis", "V\u0131\u0301tor Santos Costa", "Elizabeth Berg", "David Page", "Peggy L. Peissig", "Michael Caldwell"], "venue": "In Proceedings of the 29th International Conference on Machine Learning,", "citeRegEx": "Davis et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Davis et al\\.", "year": 2012}, {"title": "Logical and Relational Learning", "author": ["Luc De Raedt"], "venue": null, "citeRegEx": "Raedt.,? \\Q2008\\E", "shortCiteRegEx": "Raedt.", "year": 2008}, {"title": "Problog: A probabilistic prolog and its application in link discovery", "author": ["Luc De Raedt", "Angelika Kimmig", "Hannu Toivonen"], "venue": "IJCAI", "citeRegEx": "Raedt et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Raedt et al\\.", "year": 2007}, {"title": "Fast relational learning using bottom clause propositionalization with artificial neural networks", "author": ["Manoel VM Fran\u00e7a", "Gerson Zaverucha", "Artur S dAvila Garcez"], "venue": "Machine learning,", "citeRegEx": "Fran\u00e7a et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Fran\u00e7a et al\\.", "year": 2014}, {"title": "The predictive toxicology challenge 2000\u20132001", "author": ["Christoph Helma", "Ross D. King", "Stefan Kramer", "Ashwin Srinivasan"], "venue": null, "citeRegEx": "Helma et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Helma et al\\.", "year": 2001}, {"title": "Mapping part-whole hierarchies into connectionist networks", "author": ["Geoffrey E. Hinton"], "venue": null, "citeRegEx": "Hinton.,? \\Q1990\\E", "shortCiteRegEx": "Hinton.", "year": 1990}, {"title": "Approximating the semantics of logic programs by recurrent neural networks", "author": ["Steffen H\u00f6lldobler", "Yvonne Kalinke", "Hans Peter St\u00f6rr"], "venue": null, "citeRegEx": "H\u00f6lldobler et al\\.,? \\Q1999\\E", "shortCiteRegEx": "H\u00f6lldobler et al\\.", "year": 1999}, {"title": "Towards combining inductive logic programming with bayesian networks", "author": ["Kristian Kersting", "Luc De Raedt"], "venue": "In Inductive Logic Programming, 11th International Conference,", "citeRegEx": "Kersting and Raedt.,? \\Q2001\\E", "shortCiteRegEx": "Kersting and Raedt.", "year": 2001}, {"title": "Lifted graphical models: a survey", "author": ["A Kimmig", "L Mihalkova", "L Getoor"], "venue": "Machine Learning,", "citeRegEx": "Kimmig et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Kimmig et al\\.", "year": 2015}, {"title": "Fuzzy sets and fuzzy logic, volume 4", "author": ["George Klir", "Bo Yuan"], "venue": "Prentice Hall New Jersey,", "citeRegEx": "Klir and Yuan.,? \\Q1995\\E", "shortCiteRegEx": "Klir and Yuan.", "year": 1995}, {"title": "Imagenet classification with deep convolutional neural networks. In Advances in neural information processing", "author": ["Alex Krizhevsky", "Ilya Sutskever", "Geoffrey E Hinton"], "venue": null, "citeRegEx": "Krizhevsky et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Krizhevsky et al\\.", "year": 2012}, {"title": "Comparative evaluation of approaches to propositionalization", "author": ["Mark-A Krogel", "Simon Rawles", "Filip \u017delezn\u00fd", "Peter A Flach", "Nada Lavra\u010d", "Stefan Wrobel"], "venue": null, "citeRegEx": "Krogel et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Krogel et al\\.", "year": 2003}, {"title": "kFOIL: learning simple relational kernels", "author": ["N. Landwehr", "A. Passerini", "L. De Raedt", "P. Frasconi"], "venue": "Proceedings of the 21st national conference on Artificial intelligence,", "citeRegEx": "Landwehr et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Landwehr et al\\.", "year": 2006}, {"title": "Integrating naive bayes and foil", "author": ["Niels Landwehr", "Kristian Kersting", "Luc De Raedt"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "Landwehr et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Landwehr et al\\.", "year": 2007}, {"title": "Gradient-based learning applied to document recognition", "author": ["Yann LeCun", "L\u00e9on Bottou", "Yoshua Bengio", "Patrick Haffner"], "venue": null, "citeRegEx": "LeCun et al\\.,? \\Q1998\\E", "shortCiteRegEx": "LeCun et al\\.", "year": 1998}, {"title": "Is mutagenesis still challenging", "author": ["Huma Lodhi", "Stephen Muggleton"], "venue": "ILP-Late-Breaking Papers,", "citeRegEx": "Lodhi and Muggleton.,? \\Q2005\\E", "shortCiteRegEx": "Lodhi and Muggleton.", "year": 2005}, {"title": "Graph kernels for chemical informatics", "author": ["Liva Ralaivola", "Sanjay J. Swamidass", "Hiroto Saigo", "Pierre Baldi"], "venue": "Neural Netw.,", "citeRegEx": "Ralaivola et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Ralaivola et al\\.", "year": 2005}, {"title": "Multi instance neural networks", "author": ["J Ramon", "L De Raedt"], "venue": "In Proceedings of the ICML Workshop on Attribute-Value and Relational Learning,", "citeRegEx": "Ramon and Raedt.,? \\Q2000\\E", "shortCiteRegEx": "Ramon and Raedt.", "year": 2000}, {"title": "The graph neural network model", "author": ["Franco Scarselli", "Marco Gori", "Ah Chung Tsoi", "Markus Hagenbuchner", "Gabriele Monfardini"], "venue": "IEEE transactions on neural networks / a publication of the IEEE Neural Networks Council,", "citeRegEx": "Scarselli et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Scarselli et al\\.", "year": 2009}, {"title": "Refinement of approximate domain theories by knowledge-based neural networks", "author": ["Geofrey G Towell", "Jude W Shavlik", "Michiel O Noordewier"], "venue": "In Proceedings of the eighth National conference on Artificial intelligence,", "citeRegEx": "Towell et al\\.,? \\Q1990\\E", "shortCiteRegEx": "Towell et al\\.", "year": 1990}, {"title": "The semantics of predicate logic as a programming language", "author": ["Maarten H Van Emden", "Robert A Kowalski"], "venue": "Journal of the ACM (JACM),", "citeRegEx": "Emden and Kowalski.,? \\Q1976\\E", "shortCiteRegEx": "Emden and Kowalski.", "year": 1976}], "referenceMentions": [{"referenceID": 13, "context": "Introduction Lifted models also known as templated models have attracted significant attention recently (Kimmig et al., 2015) in areas such as statistical relational learning.", "startOffset": 104, "endOffset": 125}, {"referenceID": 24, "context": "While there have been several works combining propositional or relational logic with neural networks (Towell et al., 1990; Botta et al., 1997; Fran\u00e7a et al., 2014), none of the existing methods is able to learn weights of latent non-ground relational structures1.", "startOffset": 101, "endOffset": 163}, {"referenceID": 2, "context": "While there have been several works combining propositional or relational logic with neural networks (Towell et al., 1990; Botta et al., 1997; Fran\u00e7a et al., 2014), none of the existing methods is able to learn weights of latent non-ground relational structures1.", "startOffset": 101, "endOffset": 163}, {"referenceID": 8, "context": "While there have been several works combining propositional or relational logic with neural networks (Towell et al., 1990; Botta et al., 1997; Fran\u00e7a et al., 2014), none of the existing methods is able to learn weights of latent non-ground relational structures1.", "startOffset": 101, "endOffset": 163}, {"referenceID": 14, "context": "Logical operators from various fuzzy logics (Klir and Yuan, 1995) may serve as an inspiration for selecting suitable activation functions.", "startOffset": 44, "endOffset": 65}, {"referenceID": 16, "context": "When the LRNN has just one layer, as in this example, one can achieve the same effect using techniques from propositionalization (Krogel et al., 2003) \u2013 treating the bodies of the rules as features and feeding them as attributes to a logistic regression classifier.", "startOffset": 129, "endOffset": 150}, {"referenceID": 5, "context": "in prediction of adverse effects of drugs where significant improvements in predictive accuracy were gained by methods which were able to create auxiliary clusters of similar drugs (Davis et al., 2012).", "startOffset": 181, "endOffset": 201}, {"referenceID": 5, "context": "Example 8 Let us suppose that, similarly to (Davis et al., 2012), we have temporal data about patients, drugs which the patients took and time instants when changes in health occurred.", "startOffset": 44, "endOffset": 64}, {"referenceID": 15, "context": "Exploiting the process of grounding of the lifted template, facilitating weight sharing in the ground networks, LRNNs can also emulate principal structures of convolutional neural networks (Krizhevsky et al., 2012) as the next example shows.", "startOffset": 189, "endOffset": 214}, {"referenceID": 24, "context": "While the KBANN (Towell et al., 1990) also constructs the network structure from given rules, these rules are propositional rather than relational and do not serve as a lifted template.", "startOffset": 16, "endOffset": 37}, {"referenceID": 8, "context": "A more recent system CILP++(Fran\u00e7a et al., 2014) utilizes a relational representation, which is however converted into a propositional form through a propositionalization technique (Krogel et al.", "startOffset": 27, "endOffset": 48}, {"referenceID": 16, "context": ", 2014) utilizes a relational representation, which is however converted into a propositional form through a propositionalization technique (Krogel et al., 2003).", "startOffset": 140, "endOffset": 161}, {"referenceID": 2, "context": "A somewhat more closely related paper on FONN (Botta et al., 1997) also designs a technique forming a network from relational rule set, however this rule set is flat, producing only 1-layer (shallow) networks in which relational patterns are not hierarchically aggregated.", "startOffset": 46, "endOffset": 66}, {"referenceID": 0, "context": "While there are many other approaches of neural-symbolic integration aiming at relational (and first-order) representations (Bader and Hitzler, 2005), e.", "startOffset": 124, "endOffset": 149}, {"referenceID": 11, "context": "based on the CORE method (H\u00f6lldobler et al., 1999), they typically search for a uniform model of the logic program in scope and thus principally differ from the presented lifted modeling approach.", "startOffset": 25, "endOffset": 50}, {"referenceID": 1, "context": "A similarly directed work (Blockeel and Uwents, 2004) facilitated aggregative reasoning to process sets of related tuples from relational database as a sequence through recurrent neural network structure, which was also presented for more general structures in (Scarselli et al.", "startOffset": 26, "endOffset": 53}, {"referenceID": 23, "context": "A similarly directed work (Blockeel and Uwents, 2004) facilitated aggregative reasoning to process sets of related tuples from relational database as a sequence through recurrent neural network structure, which was also presented for more general structures in (Scarselli et al., 2009).", "startOffset": 261, "endOffset": 285}, {"referenceID": 10, "context": "More loosely related works arise also in the neural networks community, where various recursive auto-encoders based on the idea of \u201creduced descriptions\u201d (Hinton, 1990) are trained to encode structured data.", "startOffset": 154, "endOffset": 168}, {"referenceID": 19, "context": "Another line of work are convolutional neural networks (LeCun et al., 1998) and techniques of indirect encoding (Clune et al.", "startOffset": 55, "endOffset": 75}, {"referenceID": 3, "context": ", 1998) and techniques of indirect encoding (Clune et al., 2011), exploiting patterns and regularities in neural connections to create more compressed representations of large neural networks.", "startOffset": 44, "endOffset": 64}, {"referenceID": 20, "context": "Experiments In this section we describe experiments performed on 78 datasets of organic molecules: Mutagenesis dataset (Lodhi and Muggleton, 2005), four datasets from the predictive toxicollogy challenge and 73 NCI-GI datasets (Ralaivola et al.", "startOffset": 119, "endOffset": 146}, {"referenceID": 21, "context": "Experiments In this section we describe experiments performed on 78 datasets of organic molecules: Mutagenesis dataset (Lodhi and Muggleton, 2005), four datasets from the predictive toxicollogy challenge and 73 NCI-GI datasets (Ralaivola et al., 2005).", "startOffset": 227, "endOffset": 251}, {"referenceID": 17, "context": "We compare performance of LRNNs to state-of-the-art relational learners kFOIL (Landwehr et al., 2006) and nFOIL (Landwehr et al.", "startOffset": 78, "endOffset": 101}, {"referenceID": 18, "context": ", 2006) and nFOIL (Landwehr et al., 2007), where kFOIL combines relational rule learninng with support vector machines and nFOIL combines relational rule learning with naive Bayes learning.", "startOffset": 18, "endOffset": 41}, {"referenceID": 8, "context": "We also tried to compare LRNNs with another recent algorithm combining logic and neural networks, called CILP++ (Fran\u00e7a et al., 2014), but we didn\u2019t find it to perform well on our relational datasets as we were not able to obtain, using CILP++, accuracy significantly higher than simple majority class error on any of the datasets8.", "startOffset": 112, "endOffset": 133}, {"referenceID": 8, "context": "While relatively reasonable results for Mutagenesis were reported in (Fran\u00e7a et al., 2014), the expertknowledge attributes were used in the experiments reported therein, which might explain the discrepancy between the results.", "startOffset": 69, "endOffset": 90}], "year": 2015, "abstractText": "We propose a method combining relational-logic representations with neural network learning. A general lifted architecture, possibly reflecting some background domain knowledge, is described through relational rules which may be handcrafted or learned. The relational rule-set serves as a template for unfolding possibly deep neural networks whose structures also reflect the structures of given training or testing relational examples. Different networks corresponding to different examples share their weights, which co-evolve during training by stochastic gradient descent algorithm. The framework allows for hierarchical relational modeling constructs and learning of latent relational concepts through shared hidden layers weights corresponding to the rules. Discovery of notable relational concepts and experiments on 78 relational learning benchmarks demonstrate favorable performance of the method.", "creator": "LaTeX with hyperref package"}}}