{"id": "1508.05133", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "20-Aug-2015", "title": "Steps Toward Deep Kernel Methods from Infinite Neural Networks", "abstract": "Contemporary deep neural networks exhibit impressive results on practical problems. These networks generalize well although their inherent capacity may extend significantly beyond the number of training examples. We analyze this behavior in the context of deep, infinite neural networks. We show that deep infinite layers are naturally aligned with Gaussian processes and kernel methods, and devise stochastic kernels that encode the information of these networks. We show that stability results apply despite the size, offering an explanation for their empirical success.", "histories": [["v1", "Thu, 20 Aug 2015 21:35:52 GMT  (49kb,D)", "http://arxiv.org/abs/1508.05133v1", null], ["v2", "Wed, 2 Sep 2015 18:27:36 GMT  (51kb,D)", "http://arxiv.org/abs/1508.05133v2", null]], "reviews": [], "SUBJECTS": "cs.LG cs.NE", "authors": ["tamir hazan", "tommi jaakkola"], "accepted": false, "id": "1508.05133"}, "pdf": {"name": "1508.05133.pdf", "metadata": {"source": "CRF", "title": "Steps Toward Deep Kernel Methods from Infinite Neural Networks", "authors": ["Tamir Hazan", "Tommi Jaakkola"], "emails": ["tamir.hazan@gmail.com", "tommi@mit.edu"], "sections": [{"heading": "1 Introduction", "text": "The networks in these tasks usually consist of an input layer, several semi-structured intermediate layers, and an output layer. Amazingly, they appear as large, complex models that are much larger than what could be supported by the training examples. Many expect such networks to be overloaded, while in practice they have no limits. Our work suggests that the number of parameters in these models, which are mostly located in the deep layers, is much greater than what is supported by the training examples."}, {"heading": "2 Background", "text": "The neuronal networks form a successful framework for classification that mimics the activation function of neurons (max.), which is successfully used in networks. < < < < < < < < < (p) < (p) < (p). < (p). (p). (p). (p). (p). (p). (p). (p). (p). (p). (p). (p). (p). (p). (p). (p). (p). (p). (p). (p). (p). (p). (p.) p. (p.). (p.). (p.) (p.) (p.) (p.) (p.) (p.) p. (p.). (p.) p. (p.). (p.) p. (p.). (p.). (p.). (p.) p. (p.). (p.). (p.) p.). (p.) p. (p.) p.). (p.). (p.) p.). (p.). (p.) p.). (p.) p.). (p.). (p.) p.) p. (p.). (p.).) p. (p.) p. (p.) p. (p.) p.). (p.) p. (p.). (p.).). (p.) p. (p. (p.) p.). (p. (p.) p.) p.). (p. (p.). (p.) p. (p.) p.). (p.). (p.). (p.) p.). (p.) p.). (p.).). (p.). (p.).). (p.). (p.).). (p. (p.). (p.).).)."}, {"heading": "3 Stochastic kernels for deep and infinitely wide neural networks", "text": "/ / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / /, / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / /"}, {"heading": "4 Deep infinite networks, generalization and experimental validation", "text": "The practice of neural networks proves that they do not overlap, even if the number of learned parameters is in the order of magnitude greater than the number of learned examples. We show that generalization largely depends on the expressiveness of the output plane, which is regulated by its capacity."}, {"heading": "5 Non-linearities", "text": "The infinite layers presented in sections 2 and 3 are limited in their expressiveness < u > q = function. < x, w > Similarly, in the second layer, the u (w) function acts linearly and globally on each x (w) = f (< w, x >). These interactions ignore spatial information in vectors x or function x (w), spatial information important in computer vision and language processing applications. Current deep learning architectures use spatial information using twists. These convolutions are applied to image fields, or correspondingly to overlapping subsets of data instance x, and recursively to their functions x. These operations introduce important aspects of non-linearity and locality."}, {"heading": "6 Related work", "text": "This year, we will be able to put ourselves in a position to take the lead."}, {"heading": "7 Discussion", "text": "In this paper, we explain this behavior using deep, infinite neural networks. We construct stochastic nuclei that rely on Gaussian processes to encode such networks. We also explain how to introduce locality and nonlinearity into such networks, similar to those introduced by evolutionary neural networks. Finally, we provide generalization boundaries and regularity conditions that explain why these networks do not agree. We present our framework mainly for simplicity with only two intermediate layers. It can be extended to any depth, but the higher layers must not use nonlinearity. The problem of finding analytical forms of stochastic nuclei that encode arbitrarily deep layers with nonlinearity is largely open."}], "references": [], "referenceMentions": [], "year": 2017, "abstractText": "Contemporary deep neural networks exhibit impressive results on practical problems. These networks generalize well although their inherent capacity may extend significantly beyond the number of training examples. We analyze this behavior in the context of deep, infinite neural networks. We show that deep infinite layers are naturally aligned with Gaussian processes and kernel methods, and devise stochastic kernels that encode the information of these networks. We show that stability results apply despite the size, offering an explanation for their empir-", "creator": "LaTeX with hyperref package"}}}