{"id": "1508.05328", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "21-Aug-2015", "title": "Multi-agent Reinforcement Learning with Sparse Interactions by Negotiation and Knowledge Transfer", "abstract": "Reinforcement learning has significant applications for multi-agent systems, especially in unknown dynamic environments. However, most multi-agent reinforcement learning (MARL) algorithms suffer from such problems as exponential computation complexity in the joint state-action space, which makes it difficult to scale up to realistic multi-agent problems. In this paper, a novel algorithm named negotiation-based MARL with sparse interactions (NegoSI) is presented. In contrast to traditional sparse-interaction based MARL algorithms, NegoSI adopts the equilibrium concept and makes it possible for agents to select the non-strict Equilibrium Dominating Strategy Profile (non-strict EDSP) or Meta equilibrium for their joint actions. The presented NegoSI algorithm consists of four parts: the equilibrium-based framework for sparse interactions, the negotiation for the equilibrium set, the minimum variance method for selecting one joint action and the knowledge transfer of local Q-values. In this integrated algorithm, three techniques, i.e., unshared value functions, equilibrium solutions and sparse interactions are adopted to achieve privacy protection, better coordination and lower computational complexity, respectively. To evaluate the performance of the presented NegoSI algorithm, two groups of experiments are carried out regarding three criteria: steps of each episode (SEE), rewards of each episode (REE) and average runtime (AR). The first group of experiments is conducted using six grid world games and shows fast convergence and high scalability of the presented algorithm. Then in the second group of experiments NegoSI is applied to an intelligent warehouse problem and simulated results demonstrate the effectiveness of the presented NegoSI algorithm compared with other state-of-the-art MARL algorithms.", "histories": [["v1", "Fri, 21 Aug 2015 16:30:25 GMT  (2055kb)", "http://arxiv.org/abs/1508.05328v1", "12 pages, 15 figures"], ["v2", "Thu, 31 Mar 2016 15:44:26 GMT  (2056kb)", "http://arxiv.org/abs/1508.05328v2", "13 pages, 15 figures"]], "COMMENTS": "12 pages, 15 figures", "reviews": [], "SUBJECTS": "cs.MA cs.AI", "authors": ["luowei zhou", "pei yang", "chunlin chen", "yang gao"], "accepted": false, "id": "1508.05328"}, "pdf": {"name": "1508.05328.pdf", "metadata": {"source": "CRF", "title": "Multi-agent Reinforcement Learning with Sparse Interactions by Negotiation and Knowledge Transfer", "authors": ["Luowei Zhou", "Pei Yang", "Chunlin Chen", "Yang Gao"], "emails": ["luozhou@umich.edu).", "yangpei@nju.edu.cn).", "clchen@nju.edu.cn)."], "sections": [{"heading": null, "text": "This year, it has reached the stage where it will be able to take the lead."}, {"heading": "II. BACKGROUND", "text": "This section presents some important concepts of multi-agent reinforcement learning and typical sparsely interaction-based MARL algorithms."}, {"heading": "A. MDP and Markov Games", "text": "We begin by reviewing two standard decision models that are relevant to our work, i.e., Markov decision process (Q = > Q = Q) and Markov Games, respectively. MDP is the basis of Markov Games, while Markov Games adopt game theory in multi-agent MDPs. MDP describes a sequential decision problem as follows: [22]: Definition 1: (Markov Decision Process, MDP) An MDP is a tuple < S, A, R, T >, where S is the state space, A is the actor's action space, R: S \u00b7 A \u2192 R is the reward function whose action function relates to rewards, T: S \u00b7 A \u00b7 S \u2192 S is the transition function. An agent in an MDP is required to find an optimal policy that maximizes some reward criteria, such as expected discounted reward criteria of rewards."}, {"heading": "B. MAS with Sparse Interactions", "text": "In fact, most of them are able to determine for themselves what they want and what they want."}, {"heading": "III. NEGOTIATION-BASED MARL WITH SPARSE INTERACTIONS", "text": "When people work in a limited environment with potential conflicts, they usually learn first how to perform their individual tasks and then how to coordinate with others. We apply this common sense to our method of sparse interaction, splitting the learning process into two distinct sub-processes. [5] First, each individual agent learns how to coordinate with others in the static environment itself, ignoring the existence of other agents. Second, each agent learns when to coordinate with others according to their immediate reward changes, and then learns how to coordinate with others in game theory. In this section, the negotiation-based framework for MAS with sparse interactions is first presented, followed by a detailed description of related techniques and specific algorithms."}, {"heading": "A. Negotiation-based Framework for Sparse Interactions", "text": "In fact, most people who are able to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to fight, to fight, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to fight, to move, to fight, to move, to fight, to fight, to fight, to move, to fight, to fight, to fight, to move, to fight, to fight, to fight, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to"}, {"heading": "B. Negotiation for Equilibrium Set", "text": "One contribution of this paper is the application of the equilibrium solution concept to traditional Q-learning-based MARL algorithms. Unlike previous work such as CQ-Learning and Learning of Coordination [14], our approach aims to find the equilibrium solution for the \"potentially empty\" Qgorithms played in each \"coordination state.\" As a result, we focus on two pure strategy profiles [8], i.e., non-strict equilibrium and dominant strategy profiles (non-strict EDPS) and meta-equilibrium phrases. The definition of non-strict EDPS is as follows: Definition 3: (Non-strict EDPS measures): In an n-strict equilibrium and normal form game (non-strict EDPS), ~ ei strategies (i = 1.2,., m) are pure strategy Nash balances."}, {"heading": "C. Minimum Variance Method", "text": "In Section III-B, we have outlined the process in which all \"coordinating agents\" negotiate a balance joint action set. However, the received set usually contains many strategy profiles and it is difficult for the actors to choose a suitable one. In this paper, a minimum variance method is proposed to help the \"coordinating agents\" select the joint action with relatively high total benefit and minimum variance to ensure the cooperation and fairness of the learning process. If the balance amount is not empty, the best solution is always the one defined in the minimum variance method. Unlike other sparse interaction algorithms (e.g. CQ learning), actors update their states according to their joint actions and receive immediate rewards, which are used to update local Q values and global Q values."}, {"heading": "D. Local Q-value transfer", "text": "At the beginning of the learning process, we use the transfer knowledge of the agents \"optimal single-agent policy to accelerate the learning process\" Jiai i \"(I). In addition, it is possible to improve algorithm performance through local Q-value transfer. In most previous literature, the initial local Q-values of the newly extended common states are * < I) n (recently, Vrancx et al have proposed a transfer learning method [32] to initialize these Q-values with previously formed Q-values from the source task, which is reasonable in the real world. When people meet with others on the way to their individual goals, they usually have prior knowledge of how to avoid collisions. Based on this earlier common sense and the knowledge of how to finish their individual tasks, the agents learn to negotiate with others and to obtain fixed coordination strategies that are suitable for certain environments. However, Vrancx et al's coordination knowledge is only used to have proven less effective to initialize the environmental information and Q-initiate the local values in our experiments."}, {"heading": "IV. EXPERIMENTS", "text": "In order to test the presented NegoSI algorithm, several groups of simulated experiments are implemented and the results are compared with those of three other state-of-the-art MARL algorithms, namely CQ-Learning [14], NegoQ with Value Function Transfer (NegoQ-VFT) [27] and Independent Learner with Value Function Transfer (IL-VFT). In the next two subsections, the presented NegoSI algorithm is applied to six world grid games and an intelligent warehouse problem, showing that NegoSI is an effective approach to MARL problems compared to other existing MARL algorithms. For all of these experiments, each agent has four actions: top, bottom, left, right. The reward settings are as follows: 1) When an agent reaches his goal / his goals, he receives a reward of 100. His end goal is an absorbing state. An episode is over when all agents reach their negative range of 10 if one)."}, {"heading": "A. Tests on grid world games", "text": "This year it is more than ever before in the history of the city."}, {"heading": "B. A real-world application: intelligent warehouse systems", "text": "This year, it is more than ever before in the history of the city, in which it has come as far as never before in the history of the city."}, {"heading": "V. CONCLUSIONS", "text": "In this thesis, a negotiation-based MARL algorithm with sparse interactions (NegoSI) is proposed for learning and coordination problems in multi-agent systems. In this integrated algorithm, the knowledge transfer mechanism is also used to improve the learning speed and coordination ability of agents. In contrast to traditional sparse interaction-based MARL algorithms, NegoSI adopts the equilibrium concept and allows agents to select non-strict EDSP or meta-balances for their joint actions, making it easy to find near-optimal (or even optimal) strategies and also to avoid collisions. The experimental results show the effectiveness of the presented NegoSI algorithm in terms of characteristics such as fast convergence, low computational complexity and high scalability compared to the most modern MARL algorithms, especially for practical problems. Our future work will focus on further comparing NegoSI algorithms with other functional MMARL algorithms and other general algorithms (draw more problems from MMARI algorithms)."}, {"heading": "ACKNOWLEDGEMENT", "text": "The authors thank Dr. Yujing Hu and Dr. Jiajia Dou for the helpful discussion."}], "references": [{"title": "A comprehensive survey of multi-agent reinforcement learning", "author": ["L. Bu\u015foniu", "R. Babu\u0161ka", "B.D. Schutter"], "venue": "IEEE Transactions on System, Man, and Cybernetics, Part C: Applications and Reviews, vol. 38, no. 2, pp. 156-172, 2008.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2008}, {"title": "Heuristicallyaccelerated multiagent reinforcement learning", "author": ["R. Bianchi", "M.F. Martins", "C.H. Ribeiro", "A.H. Costa"], "venue": "IEEE Transactions on Cybernetics, vol. 44, no. 2, pp. 252-265, 2014.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2014}, {"title": "Cooperative multiagent congestion control for high-speed networks", "author": ["K. Hwang", "S. Tan", "M. Hsiao", "C. Wu"], "venue": "IEEE Transactions on Systems, Man, and Cybernetics, Part B: Cybernetics, vol. 35, no. 2, pp. 255-268, 2005.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2005}, {"title": "Multiagent learning of coordination in loosely coupled multiagent systems", "author": ["C. Yu", "M. Zhang", "F. Ren", "G. Tan"], "venue": "IEEE Transaction on Cybernetics, DOI: 10.1109/TCYB.2014.2387277, 2015.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2014}, {"title": "Markov games as a framework for multi-agent reinforcement learning", "author": ["M.L. Littman"], "venue": "Proceedings of the International Conference on Machine Learning (ICML-94), pp.157-163, New Brunswick, NJ, July 10-13, 1994.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 1994}, {"title": "Decentralized MDPs with sparse interactions", "author": ["F.S. Melo", "M. Veloso"], "venue": "Artificial Intelligence, vol. 175, no. 11, pp. 1757-1789, 2011.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2011}, {"title": "Multiagent reinforcement learning with unshared value functions", "author": ["Y. Hu", "Y. Gao", "B. An"], "venue": "IEEE Transaction on Cybernetics, vol. 45, no. 4, pp. 647-662, 2015.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2015}, {"title": "Accelerating multiagent reinforcement learning by equilibrium transfer", "author": ["Y. Hu", "Y. Gao", "B. An"], "venue": "IEEE Transaction on Cybernetics, vol.45, no. 7, pp. 1289 - 1302, 2015.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2015}, {"title": "Nash Q-learning for general-sum stochastic games", "author": ["J. Hu", "M.P. Wellman"], "venue": "The Journal of Machine Learning Research, vol. 4, pp. 1039C1069, 2003.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2003}, {"title": "Friend-or-foe Q-learning in general-sum games", "author": ["M.L. Littman"], "venue": "Proceedings of the International Conference on Machine Learning (ICML-01), pp. 322-328, Williams College, Williamstown, MA, USA, 2001.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2001}, {"title": "Correlated Q-learning", "author": ["A. Greenwald", "K. Hall", "R. Serrano"], "venue": "Proceedings of the International Conference on Machine Learning (ICML-03), pp. 84-89, Washington, DC, USA, 2003.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2003}, {"title": "Learning of coordination: Exploiting sparse interactions in multiagent systems", "author": ["F.S. Melo", "M. Veloso"], "venue": "Proceedings of the International Conference on Autonomous Agents and Multiagent Systems (AAMAS), pp. 773-780, 2009.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2009}, {"title": "Learning multi-agent state space representations", "author": ["Y.D. Hauwere", "P. Vrancx", "A. Now\u00e9"], "venue": "Proceedings of the International Conference on Autonomous Agents and Multiagent Systems (AAMAS), vol. 1, no. 1, pp. 715-722, 2010.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2010}, {"title": "Generalized learning automata for multi-agent reinforcement learning", "author": ["Y.D. Hauwere", "P. Vrancx", "A. Now\u00e9"], "venue": "AI Communications, vol. 23, no. 4, pp. 311-324, 2010.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2010}, {"title": "Community-aware task allocation for social networked multiagent systems", "author": ["W. Wang", "Y. Jiang"], "venue": "IEEE Transactions on Cybernetics, vol. 44, no. 9, pp. 1529-1543, 2014.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2014}, {"title": "Optimization and coordinated autonomy in mobile fulfillment systems", "author": ["J. Enright", "P.R. Wurman"], "venue": "Automated Action Planning for Autonomous Mobile Robots, pp. 33-38, 2011.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2011}, {"title": "Multiagent systems: a survey from the machine learning perspective", "author": ["P. Stone", "M. Veloso"], "venue": "Autonomous Robots, vol. 8, no. 3, pp. 345-383, 2000.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2000}, {"title": "Equilibrium points in n-person games", "author": ["J.F. Nash"], "venue": "Proceedings of the National Academy of Sciences of the United States of America, vol. 36, no. 1, pp. 48-49, 1950.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 1950}, {"title": "Heuristics for negotiation schedules in multi-plan optimization", "author": ["B. An", "F. Douglis", "F. Ye"], "venue": "Proceedings of the International Conference on Autonomous Agents and Multiagent Aystems (AAMAS), vol. 2, pp. 551-558, 2008.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2008}, {"title": "Agent-mediated multistep optimization for resource allocation in distributed sensor networks", "author": ["B. An", "V. Lesser", "D. Westbrook", "M. Zink"], "venue": "Proceedings of the International Conference on Autonomous Agents and Multiagent Systems (AAMAS), vol. 2, pp. 609-616, 2011.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2011}, {"title": "Reinforcement learning: an introduction", "author": ["R.S. Sutton", "A.G. Barto"], "venue": "MIT press, 1998.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 1998}, {"title": "Learning from delayed rewards", "author": ["C. Watkins"], "venue": "PhD thesis, University of Cambridge, 1989.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 1989}, {"title": "Asynchronous stochastic approximation and Q-learning", "author": ["J. Tsitsiklis"], "venue": "Machine Learning, vol. 16, no. 3, pp. 185-202, 1994.", "citeRegEx": "24", "shortCiteRegEx": null, "year": 1994}, {"title": "Game theory and multiagent reinforcement learning", "author": ["A. Now\u00e9", "P. Vrancx", "Y.D. Hauwere"], "venue": "Reinforcement Learning, Springer Berlin Heidelberg, pp. 441-470, 2012.", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2012}, {"title": "Simple search methods for finding a Nash equilibirum", "author": ["R. Porter", "E. Nudelman", "Y. Shoham"], "venue": "Games and Economic Behavior, vol. 63, no. 2, pp. 642-662, 2008.", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2008}, {"title": "Learning in Multi-agent Systems with Sparse Interactions by Knowledge Transfer and Game Abstraction", "author": ["Y. Hu", "Y. Gao", "B. An"], "venue": "Proceedings of the International Conference on Autonomous Agents and Multiagent Systems (AAMAS), pp. 753-761, Istanbul, Turkey, 4-8 May, 2015.", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2015}, {"title": "Coordinated reinforcement learning", "author": ["C. Guestrin", "M. Lagoudakis", "R. Parr"], "venue": "Proceedings of the International Conference on Machine Learning (ICML-02), vol. 2, pp. 227-234. 2002.", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2002}, {"title": "Context-specific multiagent coordination and planning with factored MDPs", "author": ["C. Guestrin", "S. Venkataraman", "D. Koller"], "venue": "Proceedings of the National Conference on Artificial Intelligence, pp. 253-259. 2002.", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2002}, {"title": "Sparse cooperative Q-learning", "author": ["J.R. Kok", "N.A. Vlassis"], "venue": "Proceedings of the International Conference on Machine Learning (ICML-04), pp. 61-68, Banff, Alberta, Canada, 2004.", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2004}, {"title": "Solving sparse delayed coordination problems in multi-agent reinforcement learning", "author": ["Y.D. Hauwere", "P. Vrancx", "A. Now\u00e9"], "venue": "Adaptive and Learning Agents, Springer Berlin Heidelberg, pp. 114-133, 2012.", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2012}, {"title": "Transfer learning for multiagent coordination", "author": ["P. Vrancx", "Y.D. Hauwere", "A. Now\u00e9"], "venue": "Proceedings of the International Conference on Agents and Artificial Intelligence (ICAART), pp. 263-272, 2011.", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2011}, {"title": "The dynamics of reinforcement learning in cooperative multiagent systems", "author": ["C. Claus", "C. Boutilier"], "venue": "Proceedings of the fifteenth national/tenth conference on Artificial intelligence/Innovative applications of artificial intelligence, pp. 746-752, July 26C30, 1998.", "citeRegEx": "33", "shortCiteRegEx": null, "year": 1998}, {"title": "Cooperative, hybrid agent architecture for real-time traffic signal control", "author": ["M.C. Choy", "D. Srinivasan", "R.L. Cheu"], "venue": "IEEE Transactions on Systems, Man and Cybernetics, Part A: Systems and Humans, vol. 33, no. 5, pp. 597-607, 2003.", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2003}, {"title": "Intercell interference management in ofdma networks: A decentralized approach based onreinforcement learning", "author": ["F. Bernardo", "R.R. Agusti", "J.J. Perez-Romero", "O. Sallent"], "venue": "IEEE Transactions on Systems, Man, and Cybernetics, Part C: Applications and Reviews, vol. 41, no. 6, pp. 968-976, 2011.", "citeRegEx": "35", "shortCiteRegEx": null, "year": 2011}, {"title": "Intelligent Deflection Routing in Buffer- Less Networks", "author": ["S. Haeri", "L. Trajkovic"], "venue": "IEEE Transactions on Cybernetics, vol. 45, no. 2, pp. 316-327, 2015.", "citeRegEx": "36", "shortCiteRegEx": null, "year": 2015}, {"title": "Reinforcement learning versus model predictive control: a comparison on a power system problem", "author": ["D. Ernst", "M. Glavic", "F. Capitanescu", "L. Wehenkel"], "venue": "IEEE Transactions on Systems, Man, and Cybernetics, Part B: Cybernetics, vol. 39, no. 2, pp. 517-529, 2009.", "citeRegEx": "37", "shortCiteRegEx": null, "year": 2009}, {"title": "Improved adaptiveCreinforcement learning control for morphing unmanned air vehicles", "author": ["J. Valasek", "J. Doebbler", "M.D. Tandale", "A.J. Meade"], "venue": "IEEE Transactions on Systems, Man, and Cybernetics, Part B: Cybernetics, vol. 38, no. 4, pp. 1014-1020, 2008.", "citeRegEx": "38", "shortCiteRegEx": null, "year": 2008}, {"title": "A balanced heuristic mechanism for multirobot task allocation of intelligent warehouses", "author": ["L. Zhou", "Y. Shi", "J. Wang", "P. Yang"], "venue": "Mathematical Problems in Engineering, vol. 2014, article ID 380480, 2014.", "citeRegEx": "39", "shortCiteRegEx": null, "year": 2014}, {"title": "Simulation-based evaluations of reinforcement learning algorithms for autonomous mobile robot path planning", "author": ["H.H. Viet", "P.H. Kyaw", "T. Chung"], "venue": "IT Convergence and Services, Springer Netherlands, pp. 467- 476, 2011.", "citeRegEx": "40", "shortCiteRegEx": null, "year": 2011}, {"title": "Coordinating hundreds of cooperative, autonomous vehicles in warehouses", "author": ["P.R. Wurman", "R. D\u2019Andrea", "M. Mountz"], "venue": "AI Magazine, vol. 29, no. 1, pp. 9-20, 2008.", "citeRegEx": "41", "shortCiteRegEx": null, "year": 2008}, {"title": "Empirical evaluation methods for multiobjective reinforcement learning algorithms", "author": ["P. Vamplew", "R. Dazeley", "A. Berry", "R. Issabekov", "E. Dekker"], "venue": "Machine Learning, vol. 84, no. 1-2, pp. 51-80, 2011.", "citeRegEx": "42", "shortCiteRegEx": null, "year": 2011}, {"title": "A reinforcement neuro-fuzzy combiner for multiobjective control", "author": ["C. Lin", "I. Chung"], "venue": "IEEE Transactions on Systems, Man, and Cybernetics, Part B: Cybernetics, vol. 29, no. 6, pp. 726-744, 1999.", "citeRegEx": "43", "shortCiteRegEx": null, "year": 1999}, {"title": "Multiobjective reinforcement learning: A comprehensive overview", "author": ["C. Liu", "X. Xu", "D. Hu"], "venue": "IEEE Transactions on Systems, Man, and Cybernetics: Systems, vol. 45, no. 3, pp. 385-398, 2015.", "citeRegEx": "44", "shortCiteRegEx": null, "year": 2015}], "referenceMentions": [{"referenceID": 0, "context": "INTRODUCTION Multi-agent learning is drawing more and more interests from scientists and engineers in multi-agent systems (MAS) and machine learning communities [1]-[4].", "startOffset": 161, "endOffset": 164}, {"referenceID": 2, "context": "INTRODUCTION Multi-agent learning is drawing more and more interests from scientists and engineers in multi-agent systems (MAS) and machine learning communities [1]-[4].", "startOffset": 165, "endOffset": 168}, {"referenceID": 3, "context": "One key technique for multi-agent learning is multi-agent reinforcement learning (MARL), which is an extension of reinforcement learning in multi-agent domain [5].", "startOffset": 159, "endOffset": 162}, {"referenceID": 4, "context": "been built as frameworks of MARL, such as Markov games (MG) [6] and decentralized sparse-interaction Markov decision processes (Dec-SIMDP) [7].", "startOffset": 60, "endOffset": 63}, {"referenceID": 5, "context": "been built as frameworks of MARL, such as Markov games (MG) [6] and decentralized sparse-interaction Markov decision processes (Dec-SIMDP) [7].", "startOffset": 139, "endOffset": 142}, {"referenceID": 4, "context": "Several well-known equilibrium-based MARL algorithms [6]-[12] are derived from this model.", "startOffset": 53, "endOffset": 56}, {"referenceID": 10, "context": "Several well-known equilibrium-based MARL algorithms [6]-[12] are derived from this model.", "startOffset": 57, "endOffset": 61}, {"referenceID": 11, "context": "Typical Dec-SIMDP based algorithms include LoC [13] and CQ-learning [14].", "startOffset": 47, "endOffset": 51}, {"referenceID": 12, "context": "Typical Dec-SIMDP based algorithms include LoC [13] and CQ-learning [14].", "startOffset": 68, "endOffset": 72}, {"referenceID": 13, "context": "Besides, other models such as learning automata [2] [15] are also valuable tools for designing MARL algorithms.", "startOffset": 52, "endOffset": 56}, {"referenceID": 14, "context": "In spite of the rapid development of MARL theories and algorithms, more efforts are needed for practical applications of MARL when compared with other MAS techniques [16][18] due to some limitations of the existing MARL methods.", "startOffset": 166, "endOffset": 170}, {"referenceID": 16, "context": "In spite of the rapid development of MARL theories and algorithms, more efforts are needed for practical applications of MARL when compared with other MAS techniques [16][18] due to some limitations of the existing MARL methods.", "startOffset": 170, "endOffset": 174}, {"referenceID": 17, "context": ", Nash equilibrium [19]) for each time step and all joint states are computationally expensive [8], even for relatively small scale environments with two or three agents.", "startOffset": 19, "endOffset": 23}, {"referenceID": 6, "context": ", Nash equilibrium [19]) for each time step and all joint states are computationally expensive [8], even for relatively small scale environments with two or three agents.", "startOffset": 95, "endOffset": 98}, {"referenceID": 18, "context": ", streaming processing systems [20], sensor networks [21]) given the agents\u2019 privacy protections and huge real-time communication costs [1].", "startOffset": 31, "endOffset": 35}, {"referenceID": 19, "context": ", streaming processing systems [20], sensor networks [21]) given the agents\u2019 privacy protections and huge real-time communication costs [1].", "startOffset": 53, "endOffset": 57}, {"referenceID": 0, "context": ", streaming processing systems [20], sensor networks [21]) given the agents\u2019 privacy protections and huge real-time communication costs [1].", "startOffset": 136, "endOffset": 139}, {"referenceID": 20, "context": "MDP describes a sequential decision problem as follows [22]: Definition 1: (Markov Decision Process, MDP) A MDP is a tuple \u3008S,A,R,T \u3009, where S is the state space, A is the action space of the agent, R : S\u00d7A\u2192R is the reward function mapping state-action pairs to rewards, T : S\u00d7A\u00d7S \u2192 [0,1] is the transition function.", "startOffset": 55, "endOffset": 59}, {"referenceID": 0, "context": "MDP describes a sequential decision problem as follows [22]: Definition 1: (Markov Decision Process, MDP) A MDP is a tuple \u3008S,A,R,T \u3009, where S is the state space, A is the action space of the agent, R : S\u00d7A\u2192R is the reward function mapping state-action pairs to rewards, T : S\u00d7A\u00d7S \u2192 [0,1] is the transition function.", "startOffset": 283, "endOffset": 288}, {"referenceID": 0, "context": "where V \u2217(s) stands for the value of a state s under the optimal policy, \u03c0 : S\u00d7A\u2192 [0,1] stands for the policy of an agent, E\u03c0 is the expectation under policy \u03c0 , t is any time step, k represents a future time step, rt+k denotes the reward at the time step (t + k) and \u03b3 \u2208 [0,1] is a parameter called the discount factor.", "startOffset": 82, "endOffset": 87}, {"referenceID": 0, "context": "where V \u2217(s) stands for the value of a state s under the optimal policy, \u03c0 : S\u00d7A\u2192 [0,1] stands for the policy of an agent, E\u03c0 is the expectation under policy \u03c0 , t is any time step, k represents a future time step, rt+k denotes the reward at the time step (t + k) and \u03b3 \u2208 [0,1] is a parameter called the discount factor.", "startOffset": 272, "endOffset": 277}, {"referenceID": 21, "context": "One classic RL algorithm for estimating Q(s,a) is Q-learning [23], whose one-step updating rule is as follows: Q(s,a)\u2190 (1\u2212\u03b1)Q(s,a)+\u03b1[r(s,a)+ \u03b3 max a\u2032 Q(s,a)], (3)", "startOffset": 61, "endOffset": 65}, {"referenceID": 0, "context": "where Q(s,a) denotes the state-action value function at a stateaction pair (s,a) and \u03b1 \u2208 [0,1] is a parameter called the learning rate.", "startOffset": 89, "endOffset": 94}, {"referenceID": 22, "context": "Provided that all state-action pairs are visited infinite times with a reasonable learning rate, the estimated Q-value Q(s,a) converges to Q(s,a) [24].", "startOffset": 146, "endOffset": 150}, {"referenceID": 4, "context": "Markov games are widely adopted as a framework for multiagent reinforcement learning (MARL) [6] [10].", "startOffset": 92, "endOffset": 95}, {"referenceID": 8, "context": "Markov games are widely adopted as a framework for multiagent reinforcement learning (MARL) [6] [10].", "startOffset": 96, "endOffset": 100}, {"referenceID": 23, "context": "Therefore, Markov game is a richer framework which generalizes both of the MDP and the repeated game [25].", "startOffset": 101, "endOffset": 105}, {"referenceID": 0, "context": ",n is the set of action spaces for all agents, Ri : S\u00d7A \u2192 R is the reward function of agent i, T : S\u00d7A\u00d7S\u2192 [0,1] is the transition function.", "startOffset": 106, "endOffset": 111}, {"referenceID": 0, "context": "Denote the individual policy of agent i by \u03c0i = S\u00d7Ai \u2192 [0,1] and the joint policy of all agents by \u03c0 = (\u03c01, .", "startOffset": 55, "endOffset": 60}, {"referenceID": 6, "context": "Here, the equilibrium policy concept is usually transferred to finding the equilibrium solution for the one-shot game played in each joint state of a Markov game [8].", "startOffset": 162, "endOffset": 165}, {"referenceID": 8, "context": "Several equilibrium-based MARL algorithms in existing literatures such as NashQ [10] [26] and NegoQ [8] have been proposed, so that the joint state-action pair Q-value can be updated according to the equilibrium:", "startOffset": 80, "endOffset": 84}, {"referenceID": 24, "context": "Several equilibrium-based MARL algorithms in existing literatures such as NashQ [10] [26] and NegoQ [8] have been proposed, so that the joint state-action pair Q-value can be updated according to the equilibrium:", "startOffset": 85, "endOffset": 89}, {"referenceID": 6, "context": "Several equilibrium-based MARL algorithms in existing literatures such as NashQ [10] [26] and NegoQ [8] have been proposed, so that the joint state-action pair Q-value can be updated according to the equilibrium:", "startOffset": 100, "endOffset": 103}, {"referenceID": 8, "context": "MAS with Sparse Interactions The definition of Markov game reveals that all agents need to learn their policies in the full joint state-action space and they are coupled with each other all the time [10] [27].", "startOffset": 199, "endOffset": 203}, {"referenceID": 25, "context": "MAS with Sparse Interactions The definition of Markov game reveals that all agents need to learn their policies in the full joint state-action space and they are coupled with each other all the time [10] [27].", "startOffset": 204, "endOffset": 208}, {"referenceID": 3, "context": "The truth is that the learning agents in many practical multi-agent systems are loosely coupled with some limited interactions in some particular areas [5] [13] [27].", "startOffset": 152, "endOffset": 155}, {"referenceID": 11, "context": "The truth is that the learning agents in many practical multi-agent systems are loosely coupled with some limited interactions in some particular areas [5] [13] [27].", "startOffset": 156, "endOffset": 160}, {"referenceID": 25, "context": "The truth is that the learning agents in many practical multi-agent systems are loosely coupled with some limited interactions in some particular areas [5] [13] [27].", "startOffset": 161, "endOffset": 165}, {"referenceID": 11, "context": "Sparse-interaction based algorithms [13] [14] have recently found wide applications in MAS research.", "startOffset": 36, "endOffset": 40}, {"referenceID": 12, "context": "Sparse-interaction based algorithms [13] [14] have recently found wide applications in MAS research.", "startOffset": 41, "endOffset": 45}, {"referenceID": 23, "context": "An example of MAS with sparse interactions is the intelligent warehouse systems, where autonomous robots only consider other robots when they are close enough to each other [25] (see Fig.", "startOffset": 173, "endOffset": 177}, {"referenceID": 26, "context": "Earlier works such as the coordinated reinforcement learning [28] [29] and sparse cooperative Q-learning [30] used coordination graphs (CGs) to learn interdependencies between agents and decomposed the joint value function to local value functions.", "startOffset": 61, "endOffset": 65}, {"referenceID": 27, "context": "Earlier works such as the coordinated reinforcement learning [28] [29] and sparse cooperative Q-learning [30] used coordination graphs (CGs) to learn interdependencies between agents and decomposed the joint value function to local value functions.", "startOffset": 66, "endOffset": 70}, {"referenceID": 28, "context": "Earlier works such as the coordinated reinforcement learning [28] [29] and sparse cooperative Q-learning [30] used coordination graphs (CGs) to learn interdependencies between agents and decomposed the joint value function to local value functions.", "startOffset": 105, "endOffset": 109}, {"referenceID": 11, "context": "However, these algorithms cannot learn CGs online and only focus on finding specific states where coordination is necessary instead of learning for coordination [13].", "startOffset": 161, "endOffset": 165}, {"referenceID": 11, "context": "Melo and Veloso [13] extended the Q-learning to a two-layer algorithm and made it possible for agents to use an additional COORDINATE action to determine the state when the coordination was necessary.", "startOffset": 16, "endOffset": 20}, {"referenceID": 12, "context": "Hauwere and Vrancx proposed Coordinating Qlearning (CQ-learning) [14] and FCQ-learning [31] that helped agents learn from statistical information of rewards and Qvalues where an agent should take other agents\u2019 states into account.", "startOffset": 65, "endOffset": 69}, {"referenceID": 29, "context": "Hauwere and Vrancx proposed Coordinating Qlearning (CQ-learning) [14] and FCQ-learning [31] that helped agents learn from statistical information of rewards and Qvalues where an agent should take other agents\u2019 states into account.", "startOffset": 87, "endOffset": 91}, {"referenceID": 6, "context": "More recently, Hu et al [8] proposed an efficient equilibrium-based MARL method, called Negotiation-based Q-learning, by which agents can learn in a Markov game with unshared value functions and unshared joint state-actions.", "startOffset": 24, "endOffset": 27}, {"referenceID": 25, "context": "In later work, they applied this method for sparse interactions by knowledge transfer and game abstraction [27], and demonstrated the effectiveness of the equilibrium-based MARL in solving sparse-interaction problems.", "startOffset": 107, "endOffset": 111}, {"referenceID": 3, "context": "We apply this commonsense to our sparse-interaction method and decompose the learning process into two distinct sub-processes [5].", "startOffset": 126, "endOffset": 129}, {"referenceID": 6, "context": "We propose a negotiation mechanism similar to the work in [8] to find this equilibrium policy.", "startOffset": 58, "endOffset": 61}, {"referenceID": 6, "context": "If no non-strict EDSP is found, the agents search for a Meta equilibrium set (See Algorithm 3) instead, which is always nonempty [8].", "startOffset": 129, "endOffset": 132}, {"referenceID": 12, "context": "Different from previous work like CQ-learning and Learning of Coordination [14], our approach aims at finding the equilibrium solution for the one-shot game played in each \u201ccoordination state\u201d.", "startOffset": 75, "endOffset": 79}, {"referenceID": 6, "context": "As a result, we focus on two pure strategy profiles [8], i.", "startOffset": 52, "endOffset": 55}, {"referenceID": 6, "context": "4: end if 5: end for 6: for each ~a \u2208 A do 7: if Ui(~a)\u2265 MinU i PNE then 8: Ji NS \u2190 J i NS \u222a{~a}; 9: end if 10: end for /* Broadcast Ji NS and corresponding utilities*/ 11: JNS \u2190 \u22c2n i=1 J i NS Meta Equilibrium) [8]: In an n-agent (n \u2265 2) normal-form game \u0393, a joint action ~a is called a Meta equilibrium from a metagame k1k2 .", "startOffset": 211, "endOffset": 214}, {"referenceID": 6, "context": "Hu et al [8] used a negotiation-based method to find Meta Equilibrium set and we simplified the method as shown in Algorithm 3.", "startOffset": 9, "endOffset": 12}, {"referenceID": 11, "context": "In most previous literatures [13] [14], the initial local Q-values of the newly expanded joint states are zeros.", "startOffset": 29, "endOffset": 33}, {"referenceID": 12, "context": "In most previous literatures [13] [14], the initial local Q-values of the newly expanded joint states are zeros.", "startOffset": 34, "endOffset": 38}, {"referenceID": 30, "context": "Recently, Vrancx et al proposed a transfer learning method [32] to initialize these Q-values with prior trained Q-value from the source task, which is reasonable in the real world.", "startOffset": 59, "endOffset": 63}, {"referenceID": 31, "context": "In this blank source task, joint action learners (JAL) [33] learn to coordinate with others disregarding the environmental information.", "startOffset": 55, "endOffset": 59}, {"referenceID": 30, "context": "Similar to [32], the joint state ~s in QCT i (~s,~a) is presented as the relative position (\u2206x,\u2206y), horizontally and vertically.", "startOffset": 11, "endOffset": 15}, {"referenceID": 30, "context": "If we initialize the local Q-value with the way used in [32] or just initialize them to zeros, the learning process would be much longer.", "startOffset": 56, "endOffset": 60}, {"referenceID": 12, "context": "EXPERIMENTS To test the presented NegoSI algorithm, several groups of simulated experiments are implemented and the results are compared with those of three other state-of-the-art MARL algorithms, namely, CQ-learning [14], NegoQ with value function transfer (NegoQ-VFT) [27] and independent learners with value function transfer (IL-VFT).", "startOffset": 217, "endOffset": 221}, {"referenceID": 25, "context": "EXPERIMENTS To test the presented NegoSI algorithm, several groups of simulated experiments are implemented and the results are compared with those of three other state-of-the-art MARL algorithms, namely, CQ-learning [14], NegoQ with value function transfer (NegoQ-VFT) [27] and independent learners with value function transfer (IL-VFT).", "startOffset": 270, "endOffset": 274}, {"referenceID": 11, "context": "Tests on grid world games The proposed NegoSI algorithm is evaluated in the grid world games presented by Melo and Veloso [13], which are shown in Fig.", "startOffset": 122, "endOffset": 126}, {"referenceID": 0, "context": "MARL has been widely used in such simulated domains as grid worlds [1], but few applications have been found for realistic problems comparing to single-agent reinforcement learning (RL) algorithms [34]-[38].", "startOffset": 67, "endOffset": 70}, {"referenceID": 32, "context": "MARL has been widely used in such simulated domains as grid worlds [1], but few applications have been found for realistic problems comparing to single-agent reinforcement learning (RL) algorithms [34]-[38].", "startOffset": 197, "endOffset": 201}, {"referenceID": 36, "context": "MARL has been widely used in such simulated domains as grid worlds [1], but few applications have been found for realistic problems comparing to single-agent reinforcement learning (RL) algorithms [34]-[38].", "startOffset": 202, "endOffset": 206}, {"referenceID": 37, "context": "Previously, single agent path planning methods have been successfully used in complex systems [39] [40], however, the intelligent warehouse employs a team of mobile robots to transport objects and single-agent path planning methods frequently cause collisions [41].", "startOffset": 94, "endOffset": 98}, {"referenceID": 38, "context": "Previously, single agent path planning methods have been successfully used in complex systems [39] [40], however, the intelligent warehouse employs a team of mobile robots to transport objects and single-agent path planning methods frequently cause collisions [41].", "startOffset": 99, "endOffset": 103}, {"referenceID": 39, "context": "Previously, single agent path planning methods have been successfully used in complex systems [39] [40], however, the intelligent warehouse employs a team of mobile robots to transport objects and single-agent path planning methods frequently cause collisions [41].", "startOffset": 260, "endOffset": 264}, {"referenceID": 40, "context": "In addition, multi-objective reinforcement learning (MORL) [42][44] will also be considered to further combine environment information and coordination knowledge for local learning.", "startOffset": 59, "endOffset": 63}, {"referenceID": 42, "context": "In addition, multi-objective reinforcement learning (MORL) [42][44] will also be considered to further combine environment information and coordination knowledge for local learning.", "startOffset": 63, "endOffset": 67}], "year": 2017, "abstractText": "Reinforcement learning has significant applications for multi-agent systems, especially in unknown dynamic environments. However, most multi-agent reinforcement learning (MARL) algorithms suffer from such problems as exponential computation complexity in the joint state-action space, which makes it difficult to scale up to realistic multi-agent problems. In this paper, a novel algorithm named negotiation-based MARL with sparse interactions (NegoSI) is presented. In contrast to traditional sparse-interaction based MARL algorithms, NegoSI adopts the equilibrium concept and makes it possible for agents to select the non-strict Equilibrium Dominating Strategy Profile (non-strict EDSP) or Meta equilibrium for their joint actions. The presented NegoSI algorithm consists of four parts: the equilibrium-based framework for sparse interactions, the negotiation for the equilibrium set, the minimum variance method for selecting one joint action and the knowledge transfer of local Q-values. In this integrated algorithm, three techniques, i.e., unshared value functions, equilibrium solutions and sparse interactions are adopted to achieve privacy protection, better coordination and lower computational complexity, respectively. To evaluate the performance of the presented NegoSI algorithm, two groups of experiments are carried out regarding three criteria: steps of each episode (SEE), rewards of each episode (REE) and average runtime (AR). The first group of experiments is conducted using six grid world games and shows fast convergence and high scalability of the presented algorithm. Then in the second group of experiments NegoSI is applied to an intelligent warehouse problem and simulated results demonstrate the effectiveness of the presented NegoSI algorithm compared with other state-of-the-art MARL algorithms.", "creator": "LaTeX with hyperref package"}}}