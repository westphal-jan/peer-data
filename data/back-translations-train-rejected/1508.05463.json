{"id": "1508.05463", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "22-Aug-2015", "title": "StochasticNet: Forming Deep Neural Networks via Stochastic Connectivity", "abstract": "Deep neural networks is a branch in machine learning that has seen a meteoric rise in popularity due to its powerful abilities to represent and model high-level abstractions in highly complex data. One area in deep neural networks that is ripe for exploration is neural connectivity formation. A pivotal study on the brain tissue of rats found that synaptic formation for specific functional connectivity in neocortical neural microcircuits can be surprisingly well modeled and predicted as a random formation. Motivated by this intriguing finding, we introduce the concept of StochasticNet, where deep neural networks are formed via stochastic connectivity between neurons. Such stochastic synaptic formations in a deep neural network architecture can potentially allow for efficient utilization of neurons for performing specific tasks. To evaluate the feasibility of such a deep neural network architecture, we train a StochasticNet using three image datasets. Experimental results show that a StochasticNet can be formed that provides comparable accuracy and reduced overfitting when compared to conventional deep neural networks with more than two times the number of neural connections.", "histories": [["v1", "Sat, 22 Aug 2015 03:36:43 GMT  (521kb)", "https://arxiv.org/abs/1508.05463v1", "8 pages"], ["v2", "Fri, 28 Aug 2015 19:05:03 GMT  (540kb)", "http://arxiv.org/abs/1508.05463v2", "8 pages"], ["v3", "Thu, 3 Sep 2015 01:34:17 GMT  (540kb)", "http://arxiv.org/abs/1508.05463v3", "8 pages"], ["v4", "Tue, 10 Nov 2015 20:30:05 GMT  (541kb)", "http://arxiv.org/abs/1508.05463v4", "8 pages"]], "COMMENTS": "8 pages", "reviews": [], "SUBJECTS": "cs.CV cs.LG cs.NE", "authors": ["mohammad javad shafiee", "parthipan siva", "alexander wong"], "accepted": false, "id": "1508.05463"}, "pdf": {"name": "1508.05463.pdf", "metadata": {"source": "CRF", "title": "StochasticNet: Forming Deep Neural Networks via Stochastic Connectivity", "authors": ["Mohammad Javad Shafiee"], "emails": ["mjshafiee@uwaterloo.ca", "parthipan.siva@aimetis.com", "a28wong@uwaterloo.ca"], "sections": [{"heading": null, "text": "ar Xiv: 150 8.05 463v 4 [cs.C V"}, {"heading": "1 Introduction", "text": "This year, it has reached the stage where it will be able to take the lead."}, {"heading": "2 Review of Random Graph Theory", "text": "In this study, the objective is to use random graph theory [17, 18] to stochastically shape the neural connectivity of deep neural networks. Therefore, it is important to first give a general overview of random graph theory for context. In random graph theory, a random graph can be defined as a probability distribution across graphs [22]. In the literature, a number of different random graph models have been proposed in which a random graph can be expressed by G (n, p), whereby all possible edge connectivity should occur independently with a probability of p, with 0 < p < 1. This random graph model has been generalized by Kovalenko [23], in which the random graph can be expressed by G (V, pij), with V representing a series of nodes and the edge connectivity between two nodes {i, j} in the graph neural graph with a random graph probability of < where a random graph can be represented by < 1; where a random graph of <"}, {"heading": "3 StochasticNets: Deep Neural Networks as Random Graph Realizations", "text": "This year it is so far that it will be able to reete.n the mentioned neume\u00dfe\u00dfeGr."}, {"heading": "4 Experimental Results", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.1 Experimental Setup", "text": "In order to investigate the effectiveness of StochasticNets, we construct StochasticNets with a deep Convolutionary Neural Network Architecture and evaluate the constructed StochasticNets in different ways. First, we investigate the impact of the number of neural connections formed in the constructed StochasticNets on their performance for the task of image object recognition. Second, we investigate the performance of StochasticNets compared to basic deep Convolutionary Neural Networks (which we will simply call ConvNets) with standard neural connectivity for different image object recognition tasks based on different image sets. Third, we investigate the relative speed of StochasticNets in classification in terms of the number of neural connections formed in the constructed StochasticNets. It is important to note that the main objective is to examine the effectiveness of the formation of deep neural networks using chastic connectivity [the influence of STICTY and network parameters]."}, {"heading": "4.1.1 Datasets", "text": "The CIFAR-10 image dataset [24] consists of 50,000 training images categorized into 10 different classes (5,000 images per class) of nature scenes. Each image is an RGB image with a size of 32 x 32. The MNIST image dataset [25] consists of 60,000 training images and 10,000 test images of handwritten digits. Each image is a binary image with a size of 28 x 28, with the handwritten digits normalized in size and centered in each image. The SVHN image dataset [26] consists of 604,388 training images and 26,032 test images of digits in nature scenes. Each image is an RGB image with a size of 32 x 32. The images in the MNIST dataset have been enlarged to 32 x 32 by zero, as the same StochasticNet network configuration is used for all the aforementioned image sets. Finally, the STL image set consists of 800,000 images each, with 800,000 training images labeled in each class."}, {"heading": "4.1.2 StochasticNet Configuration", "text": "The StochasticNets used in this study for all datasets are based on the LeNet-5 Deep Convolutional Neuronal Network [25] architecture and consist of 3 convolutionary layers with 32, 32 and 64 local receptive fields of size 5 x 5 for the first, second and third Convolutionary layer or 1 hidden layer of 64 neurons, with all neuronal connections in the Convolutionary and Hidden layers randomly realized on the basis of probability distributions. While it is possible to use arbitrary distributions to construct StochasticNet findings, for the purpose of this study the neural connectivity probability of the hidden layers follows an even distribution, while for the Convolutionary layers two different spatial distributions were investigated: i) an even distribution and ii) a Gaussian distribution with the mean value at the center of the receptive field and the standard deviation one-third of the receptive size."}, {"heading": "4.2 Number of Neural Connections", "text": "An experiment was conducted to illustrate the influence of the number of neural connections on the modeling accuracy of StochasticNets. Figure 6 shows the training and testing error compared to the number of neural connections in the network for the CIFAR-10 dataset. To achieve the desired number of neural connections to test their effect on modeling accuracy, a StochasticNet with the network configuration described in Section 4.1.2 was provided to train the model.Figure 6 shows the training and testing error compared to the percentage of neural connections relative to the ConvNet baseline, for two different neural connection distributions: i) an even distribution and ii) a Gaussian distribution with the mean value at the center of the receptive field and the standard deviation representing one-third of the receptive field size."}, {"heading": "4.3 Comparisons with ConvNet", "text": "Motivated by the results shown in Figure 6, a comprehensive experiment was conducted to demonstrate the performance of the proposed StochasticNets on various benchmark image sets. StochasticNet insights were formed with 39% neural connectivity via Gaussian distributed connectivity when compared with a conventional ConvNet. StochasticNets and ConvNets were trained on four benchmark image sets (i.e., CIFAR-10, MNIST, SVHN and STL-10) and their training and test error performances are compared with each other. As the neural connectivity of StochasticNets is stochastastastastastatic, the performance of StochasticNets was evaluated on the basis of 25 studies (leading to 25 StochasticNet implementations) and the reported results are based on the average of the 25 studies and test errors of StochasticNets, the connectivity of StochasticNets is shown to be low."}, {"heading": "4.4 Relative Speed vs. Number of Neural Connections", "text": "As the experiments in the previous sections have shown that StochasticNets can perform well compared to conventional ConvNets while exhibiting significantly fewer neural connections, we are now investigating the relative speed of StochasticNets in classification in terms of the number of neural connections formed in the constructed StochasticNets. As in Section 4.2, the probability of neural connections in both the coil layers and the hidden layer is varied in order to achieve the desired number of neural connections in order to test their effect on the classification speed of the StochasticNets formed. Figure 8 shows the relative classification time relative to the percentage of neural connections relative to the baseline ConvNet. The relative time is defined as the time needed during the classification process compared to ConvNet. It can be observed that the relative time decreases with the number of neural connections, which allows the potential for more efficient classification of StochasticNets."}, {"heading": "5 Conclusions", "text": "In this study, we presented a new approach to deep neural network formation inspired by stochastic connectivity, which is expressed in synaptic connectivity between neurons. StochasticNet is a deep neural network that is formed as the realization of a random graph in which synaptic connectivity between neurons is stochastically formed on the basis of a probability distribution.With this approach, neural connectivity within the deep neural network can be constructed in a way that enables efficient neural use, resulting in deep neural networks with much less neural connections, while achieving the same modeling accuracy.The effectiveness and efficiency of the proposed StochasticNet was evaluated using four popular benchmark image sets and compared to a conventional Conventional Neural Network (ConvNet).Experimental results show that the proposed StochasticNet provides a comparable level of accuracy to that of CicNet with much higher Conventional Neural Networks, while CicNet has a much higher level of accuracy."}, {"heading": "Acknowledgments", "text": "This work was supported by the Natural Sciences and Engineering Research Council of Canada, the Canada Research Chairs Program, and the Ontario Ministry of Research and Innovation. Authors also thank Nvidia for the GPU hardware used in this study through the Nvidia Hardware Grant Program. Author contributions M.S. and A.W. conceived and designed the architecture. M.S., P.S., and A.W. worked on the formulation and derivation of the architecture. M.S. implemented the architecture and conducted the experiments. M.S., P.S., and A.W. conducted the data analysis. All authors contributed to writing the paper and editing the paper."}], "references": [{"title": "Deep Speech: Scaling up end-to-end speech recognition", "author": ["A Hannun"], "venue": null, "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2014}, {"title": "Context-dependent pre-trained deep neural networks for large vocabulary speech recognition", "author": ["G Dahl"], "venue": "IEEE Transactions on Audio, Speech, and Language Processing", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2011}, {"title": "Imagenet classification with deep convolutional neural networks", "author": ["A. Krizhevsky", "I. Sutskever", "G. Hinton"], "venue": "NIPS", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2012}, {"title": "Spatial Pyramid Pooling in Deep Convolutional Networks for Visual Recognition", "author": ["K He"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2015}, {"title": "Learning methods for generic object recognition with invariance to pose and lighting", "author": ["Y. LeCun", "F. Huang", "L. Bottou"], "venue": "Conference on Computer Vision and Pattern Recognition", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2014}, {"title": "Very Deep Convolutional Networks for Large-Scale Visual Recognition", "author": ["K. Simonyan", "A. Zisserman"], "venue": null, "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2014}, {"title": "A unified architecture for natural language processing: deep neural networks with multitask learning", "author": ["R. Collobert", "J. Weston"], "venue": null, "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2008}, {"title": "Stochastic Pooling for Regularization of Deep Convolutional Neural Networks", "author": ["M. Zeller", "R. Fergus"], "venue": null, "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2013}, {"title": "Regularization of Neural Networks using DropConnect", "author": ["L Wan"], "venue": null, "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2013}, {"title": "Understanding the difficulty of training deep feedforward neural networks", "author": ["X. Glorot", "Y. Bengio"], "venue": "International Conference on Artificial Intelligence and Statistics", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2010}, {"title": "Deep sparse rectifier networks", "author": ["X. Glorot", "A. Bordes", "Y. Bengio"], "venue": "International Conference on Artificial Intelligence and Statistics", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2011}, {"title": "Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification", "author": ["K He"], "venue": null, "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2015}, {"title": "Going Deeper with Convolutions", "author": ["C Szegedy"], "venue": "Conference on Computer Vision and Pattern Recognition", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2015}, {"title": "Accelerating Very Deep Convolutional Networks for Classification and Detection", "author": ["X Zhang"], "venue": null, "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2015}, {"title": "Statistical connectivity provides a sufficient foundation for specific functional connectivity in neocortical neural microcircuits", "author": ["S Hill"], "venue": "Proceedings of National Academy of Sciences of the United States of America", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2012}, {"title": "On random graphs I", "author": ["P. Erdos", "A. Renyi"], "venue": "Publ. Math. Debrecen", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 1959}, {"title": "Dropout: A Simple Way to Prevent Neural Networks from Overfitting", "author": ["W Chen"], "venue": "Journal of Machine Learning Research", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2014}, {"title": "Regularization of Neural Networks using DropConnect", "author": ["W Li"], "venue": null, "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2013}, {"title": "Compressing Neural Networks with the Hashing Trick", "author": ["W Chen"], "venue": null, "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2015}, {"title": "Probabilistic combinatorics and its applications", "author": ["B. Bollob\u00e1s", "F. Chung"], "venue": "American Mathematical Soc", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 1991}, {"title": "The structure of random directed graph", "author": ["I. Kovalenko"], "venue": "Probab. Math. Statist", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 1975}, {"title": "Learning Multiple Layers of Features from Tiny Images", "author": ["A. Krizhevsky"], "venue": null, "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2009}, {"title": "Gradient-based learning applied to document recognition", "author": ["Y LeCun"], "venue": "Proceedings of the IEEE", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 1998}, {"title": "Reading digits in natural images with unsupervised feature learning", "author": ["Y Netzer"], "venue": "NIPS Workshop", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2011}, {"title": "An Analysis of Single Layer Networks in Unsupervised Feature Learning", "author": ["A Coates"], "venue": "AISTATS", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2011}], "referenceMentions": [{"referenceID": 0, "context": "Deep neural networks have shown considerable capabilities for handling specific complex tasks such as speech recognition [1, 2], object recognition [3\u20136], and natural language processing [7,8].", "startOffset": 121, "endOffset": 127}, {"referenceID": 1, "context": "Deep neural networks have shown considerable capabilities for handling specific complex tasks such as speech recognition [1, 2], object recognition [3\u20136], and natural language processing [7,8].", "startOffset": 121, "endOffset": 127}, {"referenceID": 2, "context": "Deep neural networks have shown considerable capabilities for handling specific complex tasks such as speech recognition [1, 2], object recognition [3\u20136], and natural language processing [7,8].", "startOffset": 148, "endOffset": 153}, {"referenceID": 3, "context": "Deep neural networks have shown considerable capabilities for handling specific complex tasks such as speech recognition [1, 2], object recognition [3\u20136], and natural language processing [7,8].", "startOffset": 148, "endOffset": 153}, {"referenceID": 4, "context": "Deep neural networks have shown considerable capabilities for handling specific complex tasks such as speech recognition [1, 2], object recognition [3\u20136], and natural language processing [7,8].", "startOffset": 148, "endOffset": 153}, {"referenceID": 5, "context": "Deep neural networks have shown considerable capabilities for handling specific complex tasks such as speech recognition [1, 2], object recognition [3\u20136], and natural language processing [7,8].", "startOffset": 148, "endOffset": 153}, {"referenceID": 6, "context": "Deep neural networks have shown considerable capabilities for handling specific complex tasks such as speech recognition [1, 2], object recognition [3\u20136], and natural language processing [7,8].", "startOffset": 187, "endOffset": 192}, {"referenceID": 7, "context": "Recent advances in improving the performance of deep neural networks have focused on areas such as network regularization [9, 10], activation functions [11\u201313], and deeper architectures [6, 14, 15].", "startOffset": 122, "endOffset": 129}, {"referenceID": 8, "context": "Recent advances in improving the performance of deep neural networks have focused on areas such as network regularization [9, 10], activation functions [11\u201313], and deeper architectures [6, 14, 15].", "startOffset": 122, "endOffset": 129}, {"referenceID": 9, "context": "Recent advances in improving the performance of deep neural networks have focused on areas such as network regularization [9, 10], activation functions [11\u201313], and deeper architectures [6, 14, 15].", "startOffset": 152, "endOffset": 159}, {"referenceID": 10, "context": "Recent advances in improving the performance of deep neural networks have focused on areas such as network regularization [9, 10], activation functions [11\u201313], and deeper architectures [6, 14, 15].", "startOffset": 152, "endOffset": 159}, {"referenceID": 11, "context": "Recent advances in improving the performance of deep neural networks have focused on areas such as network regularization [9, 10], activation functions [11\u201313], and deeper architectures [6, 14, 15].", "startOffset": 152, "endOffset": 159}, {"referenceID": 5, "context": "Recent advances in improving the performance of deep neural networks have focused on areas such as network regularization [9, 10], activation functions [11\u201313], and deeper architectures [6, 14, 15].", "startOffset": 186, "endOffset": 197}, {"referenceID": 12, "context": "Recent advances in improving the performance of deep neural networks have focused on areas such as network regularization [9, 10], activation functions [11\u201313], and deeper architectures [6, 14, 15].", "startOffset": 186, "endOffset": 197}, {"referenceID": 13, "context": "Recent advances in improving the performance of deep neural networks have focused on areas such as network regularization [9, 10], activation functions [11\u201313], and deeper architectures [6, 14, 15].", "startOffset": 186, "endOffset": 197}, {"referenceID": 14, "context": "[16], data of living brain tissue from Wistar rats was collected and used to construct a partial map of a rat brain.", "startOffset": 0, "endOffset": 4}, {"referenceID": 15, "context": "To achieve this goal, we introduce the concept of StochasticNet, where the key idea is to leverage random graph theory [17, 18] to form deep neural networks via", "startOffset": 119, "endOffset": 127}, {"referenceID": 16, "context": "Furthermore, since the focus is on neural connectivity, the StochasticNet architecture can be used directly like a conventional deep neural network and benefit from all of the same approaches used for conventional networks such as data augmentation, stochastic pooling, and Dropout [19], and DropConnect [20].", "startOffset": 282, "endOffset": 286}, {"referenceID": 17, "context": "Furthermore, since the focus is on neural connectivity, the StochasticNet architecture can be used directly like a conventional deep neural network and benefit from all of the same approaches used for conventional networks such as data augmentation, stochastic pooling, and Dropout [19], and DropConnect [20].", "startOffset": 304, "endOffset": 308}, {"referenceID": 16, "context": "While a number of stochastic strategies for improving deep neural network performance have been previously introduced [19\u201321], it is very important to note that the proposed StochasticNets is fundamentally different from these existing stochastic strategies in that StochasticNets\u2019 main significant contributions deals primarily with the formation of neural connectivity of individual neurons to construct efficient deep neural networks that are inherently sparse prior to training, while previous stochastic strategies deal with either the grouping of existing neural connections to explicitly enforce sparsity [21], or removal/introduction of neural connectivity for regularization during training.", "startOffset": 118, "endOffset": 125}, {"referenceID": 17, "context": "While a number of stochastic strategies for improving deep neural network performance have been previously introduced [19\u201321], it is very important to note that the proposed StochasticNets is fundamentally different from these existing stochastic strategies in that StochasticNets\u2019 main significant contributions deals primarily with the formation of neural connectivity of individual neurons to construct efficient deep neural networks that are inherently sparse prior to training, while previous stochastic strategies deal with either the grouping of existing neural connections to explicitly enforce sparsity [21], or removal/introduction of neural connectivity for regularization during training.", "startOffset": 118, "endOffset": 125}, {"referenceID": 18, "context": "While a number of stochastic strategies for improving deep neural network performance have been previously introduced [19\u201321], it is very important to note that the proposed StochasticNets is fundamentally different from these existing stochastic strategies in that StochasticNets\u2019 main significant contributions deals primarily with the formation of neural connectivity of individual neurons to construct efficient deep neural networks that are inherently sparse prior to training, while previous stochastic strategies deal with either the grouping of existing neural connections to explicitly enforce sparsity [21], or removal/introduction of neural connectivity for regularization during training.", "startOffset": 118, "endOffset": 125}, {"referenceID": 18, "context": "While a number of stochastic strategies for improving deep neural network performance have been previously introduced [19\u201321], it is very important to note that the proposed StochasticNets is fundamentally different from these existing stochastic strategies in that StochasticNets\u2019 main significant contributions deals primarily with the formation of neural connectivity of individual neurons to construct efficient deep neural networks that are inherently sparse prior to training, while previous stochastic strategies deal with either the grouping of existing neural connections to explicitly enforce sparsity [21], or removal/introduction of neural connectivity for regularization during training.", "startOffset": 612, "endOffset": 616}, {"referenceID": 16, "context": "This is very different from Dropout [19] and DropConnect [20] where the activations and connections are temporarily removed during training and put back during test for regularization purposes only, and as such the resulting neural connectivity of the network remains dense.", "startOffset": 36, "endOffset": 40}, {"referenceID": 17, "context": "This is very different from Dropout [19] and DropConnect [20] where the activations and connections are temporarily removed during training and put back during test for regularization purposes only, and as such the resulting neural connectivity of the network remains dense.", "startOffset": 57, "endOffset": 61}, {"referenceID": 18, "context": "StochasticNets are also very different from HashNets [21], where connection weights are randomly grouped into hash buckets, with each bucket sharing the same weights, to explicitly sparsifying into the network, since there is no notion of grouping/merging in StochasticNets; the formed StochasticNets are naturally sparse due to the formation process.", "startOffset": 53, "endOffset": 57}, {"referenceID": 21, "context": "Experimental results using four image datasets (CIFAR-10 [24], MNIST [25], SVHN [26], and STL-10 [27]) to investigate the efficacy of StochasticNets with respect to different number of neural connections as well as different training set sizes is presented in Section 5.", "startOffset": 57, "endOffset": 61}, {"referenceID": 22, "context": "Experimental results using four image datasets (CIFAR-10 [24], MNIST [25], SVHN [26], and STL-10 [27]) to investigate the efficacy of StochasticNets with respect to different number of neural connections as well as different training set sizes is presented in Section 5.", "startOffset": 69, "endOffset": 73}, {"referenceID": 23, "context": "Experimental results using four image datasets (CIFAR-10 [24], MNIST [25], SVHN [26], and STL-10 [27]) to investigate the efficacy of StochasticNets with respect to different number of neural connections as well as different training set sizes is presented in Section 5.", "startOffset": 80, "endOffset": 84}, {"referenceID": 24, "context": "Experimental results using four image datasets (CIFAR-10 [24], MNIST [25], SVHN [26], and STL-10 [27]) to investigate the efficacy of StochasticNets with respect to different number of neural connections as well as different training set sizes is presented in Section 5.", "startOffset": 97, "endOffset": 101}, {"referenceID": 15, "context": "In this study, the goal is to leverage random graph theory [17, 18] to form the neural connectivity of deep neural networks in a stochastic manner.", "startOffset": 59, "endOffset": 67}, {"referenceID": 19, "context": "In random graph theory, a random graph can be defined as the probability distribution over graphs [22].", "startOffset": 98, "endOffset": 102}, {"referenceID": 20, "context": "This random graph model was generalized by Kovalenko [23], in which the random graph can be expressed by G(V , pij), where V is a set of vertices and the edge connectivity between two vertices {i, j} in the graph is said to occur with a probability of pij , where 0 < pij < 1.", "startOffset": 53, "endOffset": 57}, {"referenceID": 21, "context": "For evaluation purposes, four benchmark image datasets are used: CIFAR-10 [24], MNIST [25], SVHN [26], and STL-10 [27].", "startOffset": 74, "endOffset": 78}, {"referenceID": 22, "context": "For evaluation purposes, four benchmark image datasets are used: CIFAR-10 [24], MNIST [25], SVHN [26], and STL-10 [27].", "startOffset": 86, "endOffset": 90}, {"referenceID": 23, "context": "For evaluation purposes, four benchmark image datasets are used: CIFAR-10 [24], MNIST [25], SVHN [26], and STL-10 [27].", "startOffset": 97, "endOffset": 101}, {"referenceID": 24, "context": "For evaluation purposes, four benchmark image datasets are used: CIFAR-10 [24], MNIST [25], SVHN [26], and STL-10 [27].", "startOffset": 114, "endOffset": 118}, {"referenceID": 21, "context": "The CIFAR-10 image dataset [24] consists of 50,000 training images categorized into 10 different classes (5,000 images per class) of natural scenes.", "startOffset": 27, "endOffset": 31}, {"referenceID": 22, "context": "The MNIST image dataset [25] consists of 60,000 training images and 10,000 test images of handwritten digits.", "startOffset": 24, "endOffset": 28}, {"referenceID": 23, "context": "The SVHN image dataset [26] consists of 604,388 training images and 26,032 test images of digits in natural scenes.", "startOffset": 23, "endOffset": 27}, {"referenceID": 24, "context": "Finally, the STL-10 image dataset [27] consists of 5,000 labeled training images and 8,000 labeled test images categorized into 10 different classes (500 training images and 800 training images per class) of natural scenes.", "startOffset": 34, "endOffset": 38}, {"referenceID": 22, "context": "The StochasticNets used in this study for the all datasets are realized based on the LeNet-5 deep convolutional neural network [25] architecture, and consists of 3 convolutional layers with 32, 32, and 64 local receptive fields of size 5\u00d7 5 for the first, second, and third convolutional layers, respectively, and 1 hidden layer of 64 neurons, with all neural connections in the convolutional and hidden layers being randomly realized based on probability distributions.", "startOffset": 127, "endOffset": 131}], "year": 2015, "abstractText": "Deep neural networks is a branch in machine learning that has seen a meteoric rise in popularity due to its powerful abilities to represent and model high-level abstractions in highly complex data. One area in deep neural networks that is ripe for exploration is neural connectivity formation. A pivotal study on the brain tissue of rats found that synaptic formation for specific functional connectivity in neocortical neural microcircuits can be surprisingly well modeled and predicted as a random formation. Motivated by this intriguing finding, we introduce the concept of StochasticNet, where deep neural networks are formed via stochastic connectivity between neurons. As a result, any type of deep neural networks can be formed as a StochasticNet by allowing the neuron connectivity to be stochastic. Stochastic synaptic formations, in a deep neural network architecture, can allow for efficient utilization of neurons for performing specific tasks. To evaluate the feasibility of such a deep neural network architecture, we train a StochasticNet using four different image datasets (CIFAR-10, MNIST, SVHN, and STL-10). Experimental results show that a StochasticNet, using less than half the number of neural connections as a conventional deep neural network, achieves comparable accuracy and reduces overfitting on the CIFAR-10, MNIST and SVHN dataset. Interestingly, StochasticNet with less than half the number of neural connections, achieved a higher accuracy (relative improvement in test error rate of \u223c6% compared to ConvNet) on the STL-10 dataset than a conventional deep neural network. Finally, StochasticNets have faster operational speeds while achieving better or similar accuracy performances.", "creator": "LaTeX with hyperref package"}}}