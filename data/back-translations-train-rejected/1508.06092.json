{"id": "1508.06092", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "25-Aug-2015", "title": "An analysis of numerical issues in neural training by pseudoinversion", "abstract": "Some novel strategies have recently been proposed for single hidden layer neural network training that set randomly the weights from input to hidden layer, while weights from hidden to output layer are analytically determined by pseudoinversion. These techniques are gaining popularity in spite of their known numerical issues when singular and/or almost singular matrices are involved. In this paper we discuss a critical use of Singular Value Analysis for identification of these drawbacks and we propose an original use of regularisation to determine the output weights, based on the concept of critical hidden layer size. This approach also allows to limit the training computational effort. Besides, we introduce a novel technique which relies an effective determination of input weights to the hidden layer dimension. This approach is tested for both regression and classification tasks, resulting in a significant performance improvement with respect to alternative methods.", "histories": [["v1", "Tue, 25 Aug 2015 09:51:35 GMT  (90kb)", "http://arxiv.org/abs/1508.06092v1", "11 pages, submitted to: Comp. Appl. Math"]], "COMMENTS": "11 pages, submitted to: Comp. Appl. Math", "reviews": [], "SUBJECTS": "cs.LG cs.NE", "authors": ["r cancelliere", "r deluca", "m gai", "p gallinari", "l rubini"], "accepted": false, "id": "1508.06092"}, "pdf": {"name": "1508.06092.pdf", "metadata": {"source": "CRF", "title": null, "authors": [], "emails": ["rossella.cancelliere@unito.it"], "sections": [{"heading": null, "text": "ar Xiv: 150 8.06 092v 1 [cs.L G] 25 Aug 2Keywords Pseudoinverse Matrix \u00b7 Weight setting \u00b7 Regularization \u00b7 Supervised learning"}, {"heading": "1 Introduction", "text": "In fact, the fact is that most of them are able to survive on their own, without there being a process in which there is a process in which there is a process in which there is a process in which there is a process, a process in which there is a process, a process in which there is a process, a process in which there is a process, a process in which there is a process, a process, a process, a process, a process, a process, a process, a process, a process, a process and a process, a process, a process, a process, a process, a process, a process, a process, a process, a process, a process, a process, a process, a process, a process and a process and a process and a process and a process and a process, a process, a process, a process in which, a process and a process and a process and a process and a process come. \""}, {"heading": "2 Input and output weights determination", "text": "Figure 1 shows a standard SLFN with P-input neurons, M-hidden neurons with nonlinear activation functions \u03c6, and Q-output neurons with linear activation functions.When we have training conducted by N-specific training samples of (input, output) pairs (xj, tj), where xj-RP and tj-RQ, the training aims to obtain the matrix of the desired outputs T-RN-Q when the matrix of all input instances X-RN-P is input.We emphasize that in the state of the type pseudoinverse approach, the input weights cij (and hidden neuron distortions) cij (and the hidden neuron inclinations) are randomly selected from a uniform distribution in a fixed interval and not modified any more. Therefore, this step results in the actual input weights that the input weights are actual weights."}, {"heading": "3 Singular value decomposition of regularised problems", "text": "Regularization methods are often used to transform an original problem into a well-established one, i.e. roughly speaking, into a problem that is insensitive to small changes in training conditions (Badeva and Morozov, 1991), and Tikhonov regularization is therefore one of the most common (Tikhonov and Arsenin, 1977; Tikhonov, 1963).The error, which can be functionally minimized, is characterized by a punitive term ER, which depends on the so-called Tikhonov matrix: E-ED + ER = | 22 + | ED-22 + | ED-22, (4) This matrix can be derived, for example, from the choice of high-pass operators (e.g. a difference operator or a weighted Fourier operator) to enforce smoothness. The regularized solution we offer from W-V is now given by: W-HTH = (HTH + TH-TH) -1HT."}, {"heading": "4 Experiments and Results", "text": "In fact, it is a matter of a way in which people are able to put themselves in the world, in which they are able to understand the world. (...) It is not as if they are able to change the world. (...) It is as if they are able to change the world. (...) It is as if they are able to change the world. (...) It is as if they are able to change the world. (...) It is as if they are able to change the world. (...) It is as if they are able to change the world. (...) It is as if they are able to change the world. (...) It is as if they are able to change the world. (...)"}, {"heading": "5 Conclusions", "text": "We have investigated the numerical instability and over-fit problems of individual hidden neural layers trained by pseudo-inversion. We have shown how the analysis of individual values can be used to diagnose numerical instability and how this problem can be solved by identifying a critical region of hidden layers from which the regularization technique benefits. This method also helps to reduce over-fit. Tests have been performed for both regression and classification tasks. In five out of six cases, the proposed regularization has proven necessary and represents a significant performance improvement in terms of uncontrolled techniques; it also enables the construction of slim architectures that achieve near-optimal performance with a reduced number of hidden neurons. Furthermore, the use of sigmoidal activation functions and \"small\" input weights (small because their values are linked to the hidden layer size) allows the MM to maintain the effectiveness of this case, allowing them to characterize the common sign-O, and the ELM to lower it."}, {"heading": "6 Acknowledgment", "text": "The activities were partly carried out within the Visiting Professor Program of the Gruppo Nazionale per il Calcolo Scientifico (GNCS) of the Italian Istituto Nazionale di Alta Matematica (INdAM)."}], "references": [{"title": "Restoration of damaged slices in images using matrix pseudo inversion", "author": ["H. Ajorloo", "M.T. Manzuri-Shalmani", "A. Lakdashti"], "venue": "Proceedings of the 22nd International symposium on computer and information sciences.", "citeRegEx": "Ajorloo et al\\.,? 2007", "shortCiteRegEx": "Ajorloo et al\\.", "year": 2007}, {"title": "Probl\u00e8mes incorrectement pos\u00e9s: Th\u00e9orie et applications en identification , filtrage optimal, contr\u00f4le optimal, analyse et synth\u00e8se de syst\u00e8mes, reconnaissance d\u2019images", "author": ["V. Badeva", "V. Morozov"], "venue": "S\u00e9rie Automatique. Masson.", "citeRegEx": "Badeva and Morozov,? 1991", "shortCiteRegEx": "Badeva and Morozov", "year": 1991}, {"title": "Understanding the difficulty of training deep feedforward neural networks", "author": ["Y. Bengio", "X. Glorot"], "venue": "Proceedings of AISTATS 2010 , volume 9, pages 249\u2013256.", "citeRegEx": "Bengio and Glorot,? 2010", "shortCiteRegEx": "Bengio and Glorot", "year": 2010}, {"title": "Pattern Recognition and Machine Learning (Information Science and Statistics)", "author": ["C.M. Bishop"], "venue": "Springer-Verlag New York, Inc., Secaucus, NJ, USA.", "citeRegEx": "Bishop,? 2006", "shortCiteRegEx": "Bishop", "year": 2006}, {"title": "A high parallel procedure to initialize the output weights of a radial basis function or bp neural network", "author": ["R. Cancelliere"], "venue": "Proceedings of the 5th International Workshop on Applied Parallel Computing, New Paradigms for HPC in Industry and Academia, PARA \u201900, pages 384\u2013390, London, UK, UK. Springer-Verlag.", "citeRegEx": "Cancelliere,? 2001", "shortCiteRegEx": "Cancelliere", "year": 2001}, {"title": "Matrix pseudoinversion for image neural processing", "author": ["R. Cancelliere", "M. Gai", "T. Arti\u00e8res", "P. Gallinari"], "venue": "T. Huang, Z. Zeng, C. Li, and C. Leung, editors, Neural Information Processing, volume 7667 of Lecture Notes in Computer Science, pages 116\u2013125. Springer Berlin Heidelberg.", "citeRegEx": "Cancelliere et al\\.,? 2012", "shortCiteRegEx": "Cancelliere et al\\.", "year": 2012}, {"title": "A new tikhonov regularization method", "author": ["M. Fuhry", "L. Reichel"], "venue": "Numerical Algorithms, 59(3), 433\u2013445.", "citeRegEx": "Fuhry and Reichel,? 2012", "shortCiteRegEx": "Fuhry and Reichel", "year": 2012}, {"title": "Practical complexity control in multilayer perceptrons", "author": ["P. Gallinari", "T. Cibas"], "venue": "Signal Processing, 74, 29\u201346.", "citeRegEx": "Gallinari and Cibas,? 1999", "shortCiteRegEx": "Gallinari and Cibas", "year": 1999}, {"title": "Matrix computations (3rd ed.)", "author": ["G.H. Golub", "C.F. Van Loan"], "venue": null, "citeRegEx": "Golub and Loan,? \\Q1996\\E", "shortCiteRegEx": "Golub and Loan", "year": 1996}, {"title": "Multi-category bioinformatics dataset classification using extreme learning machine", "author": ["T. Helmy", "Z. Rasheed"], "venue": "Proceedings of the Eleventh conference on Congress on Evolutionary Computation, CEC\u201909, pages 3234\u20133240, Piscataway, NJ, USA. IEEE Press.", "citeRegEx": "Helmy and Rasheed,? 2009", "shortCiteRegEx": "Helmy and Rasheed", "year": 2009}, {"title": "Extreme learning machine: theory and applications", "author": ["Huang", "G.-B.", "Zhu", "Q.-Y.", "Siew", "C.-K."], "venue": "Neurocomputing, 70(1), 489\u2013501.", "citeRegEx": "Huang et al\\.,? 2006", "shortCiteRegEx": "Huang et al\\.", "year": 2006}, {"title": "A matrix pseudoinversion lemma and its application to block-based adaptive blind deconvolution for mimo systems", "author": ["K. Kohno", "M. Kawamoto", "Y. Inouye"], "venue": "Trans. Cir. Sys. Part I , 57(7), 1449\u20131462.", "citeRegEx": "Kohno et al\\.,? 2010", "shortCiteRegEx": "Kohno et al\\.", "year": 2010}, {"title": "Efficient backprop", "author": ["Y. LeCun", "L. Bottou", "G. Orr", "Mller", "K.-R."], "venue": "pages 9\u201350. Springer Berlin Heidelberg.", "citeRegEx": "LeCun et al\\.,? 1998", "shortCiteRegEx": "LeCun et al\\.", "year": 1998}, {"title": "Intelligent approaches using support vector machine and extreme learning machine for transmission line protection", "author": ["V. Malathi", "N. Marimuthu", "S. Baskar"], "venue": "Neurocomputing, 73(1012), 2160 \u2013 2167. Subspace Learning / Selected papers from the European Symposium on Time Series Prediction.", "citeRegEx": "Malathi et al\\.,? 2010", "shortCiteRegEx": "Malathi et al\\.", "year": 2010}, {"title": "Tropelm: A double-regularized elm using lars and tikhonov regularization", "author": ["Y. Miche", "M. van Heeswijk", "P. Bas", "O. Simula", "A. Lendasse"], "venue": null, "citeRegEx": "Miche et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Miche et al\\.", "year": 2011}, {"title": "Human action recognition using extreme learning machine based on visual vocabularies", "author": ["R. Minhas", "A. Baradarani", "S. Seifzadeh", "Q.J. Wu"], "venue": "Neurocomputing, 73(1012), 1906 \u2013 1917. Subspace Learning / Selected papers from the European Symposium on Time Series Prediction.", "citeRegEx": "Minhas et al\\.,? 2010", "shortCiteRegEx": "Minhas et al\\.", "year": 2010}, {"title": "An efficient pseudo inverse matrix-based solution for secure auditing", "author": ["T.D. Nguyen", "H.T.B. Pham", "V.H. Dang"], "venue": "Proceedings of the IEEE International Conference on Computing and Communication Technologies, Research, Innovation, and Vision for the Future, IEEE International Conference.", "citeRegEx": "Nguyen et al\\.,? 2010", "shortCiteRegEx": "Nguyen et al\\.", "year": 2010}, {"title": "On best approximate solutions of linear matrix equations", "author": ["R. Penrose", "J.A. Todd"], "venue": "Mathematical Proceedings of the Cambridge Philosophical Society , null, 17\u201319.", "citeRegEx": "Penrose and Todd,? 1956", "shortCiteRegEx": "Penrose and Todd", "year": 1956}, {"title": "Networks for approximation and learning", "author": ["T. Poggio", "F. Girosi"], "venue": "Proceedings of the IEEE , 78(9), 1481\u20131497.", "citeRegEx": "Poggio and Girosi,? 1990", "shortCiteRegEx": "Poggio and Girosi", "year": 1990}, {"title": "Generalized inverse of matrices and its applications", "author": ["C. Rao", "S. Mitra"], "venue": "Wiley series in probability and mathematical statistics: Applied probability and statistics. Wiley.", "citeRegEx": "Rao and Mitra,? 1971", "shortCiteRegEx": "Rao and Mitra", "year": 1971}, {"title": "Parallel distributed processing: explorations in the microstructure of cognition, vol", "author": ["D.E. Rumelhart", "G.E. Hinton", "R.J. Williams"], "venue": "1. chapter Learning internal representations by error propagation, pages 318\u2013362. MIT Press, Cambridge, MA, USA.", "citeRegEx": "Rumelhart et al\\.,? 1986", "shortCiteRegEx": "Rumelhart et al\\.", "year": 1986}, {"title": "Sales forecasting using extreme learning machine with applications in fashion retailing", "author": ["Sun", "Z.-L.", "Choi", "T.-M.", "Au", "K.-F.", "Y. Yu"], "venue": "Decision Support Systems, 46(1), 411 \u2013 419.", "citeRegEx": "Sun et al\\.,? 2008", "shortCiteRegEx": "Sun et al\\.", "year": 2008}, {"title": "Solutions of ill-posed problems", "author": ["A. Tikhonov", "V. Arsenin"], "venue": "Scripta series in mathematics. Winston.", "citeRegEx": "Tikhonov and Arsenin,? 1977", "shortCiteRegEx": "Tikhonov and Arsenin", "year": 1977}, {"title": "Solution of incorrectly formulated problems and the regularization method", "author": ["A.N. Tikhonov"], "venue": "Soviet Math. Dokl., 4, 1035\u20131038.", "citeRegEx": "Tikhonov,? 1963", "shortCiteRegEx": "Tikhonov", "year": 1963}, {"title": "A protein secondary structure prediction framework based on the extreme learning machine", "author": ["G. Wang", "Y. Zhao", "D. Wang"], "venue": "Neurocomputing, 72(13), 262 \u2013 268. Machine Learning for Signal Processing (MLSP 2006) / Life System Modelling, Simulation, and Bio-inspired Computing (LSMS 2007).", "citeRegEx": "Wang et al\\.,? 2008", "shortCiteRegEx": "Wang et al\\.", "year": 2008}, {"title": "Efficient and effective algorithms for training singlehidden-layer neural networks", "author": ["D. Yu", "L. Deng"], "venue": "Pattern Recogn. Lett., 33(5), 554\u2013558.", "citeRegEx": "Yu and Deng,? 2012", "shortCiteRegEx": "Yu and Deng", "year": 2012}], "referenceMentions": [{"referenceID": 20, "context": "The training of one of the most common neural architecture, the single hidden layer feedforward neural network (SLFN) was mainly accomplished in past decades by methods based on gradient descent, and among them the large family of techniques based on backpropagation (Rumelhart et al., 1986).", "startOffset": 267, "endOffset": 291}, {"referenceID": 12, "context": "Some common drawbacks with gradient descent-based learning are anyway the high computational cost because of slow convergence and the relevant risk of converging to poor local minima on the landscape of the error function (LeCun et al., 1998).", "startOffset": 222, "endOffset": 242}, {"referenceID": 18, "context": "The idea of using the simple and efficient training algorithms of radial basis function neural networks, based on matricial pseudoinversion (Poggio and Girosi, 1990), also for SLFN learning was initially suggested in (Cancelliere, 2001); some appealing techniques were than developed (Nguyen et al.", "startOffset": 140, "endOffset": 165}, {"referenceID": 4, "context": "The idea of using the simple and efficient training algorithms of radial basis function neural networks, based on matricial pseudoinversion (Poggio and Girosi, 1990), also for SLFN learning was initially suggested in (Cancelliere, 2001); some appealing techniques were than developed (Nguyen et al.", "startOffset": 217, "endOffset": 236}, {"referenceID": 16, "context": "The idea of using the simple and efficient training algorithms of radial basis function neural networks, based on matricial pseudoinversion (Poggio and Girosi, 1990), also for SLFN learning was initially suggested in (Cancelliere, 2001); some appealing techniques were than developed (Nguyen et al., 2010; Kohno et al., 2010; Ajorloo et al., 2007) and among them the extreme learning machine ELM, (Huang et al.", "startOffset": 284, "endOffset": 347}, {"referenceID": 11, "context": "The idea of using the simple and efficient training algorithms of radial basis function neural networks, based on matricial pseudoinversion (Poggio and Girosi, 1990), also for SLFN learning was initially suggested in (Cancelliere, 2001); some appealing techniques were than developed (Nguyen et al., 2010; Kohno et al., 2010; Ajorloo et al., 2007) and among them the extreme learning machine ELM, (Huang et al.", "startOffset": 284, "endOffset": 347}, {"referenceID": 0, "context": "The idea of using the simple and efficient training algorithms of radial basis function neural networks, based on matricial pseudoinversion (Poggio and Girosi, 1990), also for SLFN learning was initially suggested in (Cancelliere, 2001); some appealing techniques were than developed (Nguyen et al., 2010; Kohno et al., 2010; Ajorloo et al., 2007) and among them the extreme learning machine ELM, (Huang et al.", "startOffset": 284, "endOffset": 347}, {"referenceID": 10, "context": ", 2007) and among them the extreme learning machine ELM, (Huang et al., 2006) which has been successfully applied to a number of real-world applications (Sun et al.", "startOffset": 57, "endOffset": 77}, {"referenceID": 21, "context": ", 2006) which has been successfully applied to a number of real-world applications (Sun et al., 2008; Wang et al., 2008; Malathi et al., 2010; Minhas et al., 2010), showing a good generalization performance with an extremely fast learning speed.", "startOffset": 83, "endOffset": 163}, {"referenceID": 24, "context": ", 2006) which has been successfully applied to a number of real-world applications (Sun et al., 2008; Wang et al., 2008; Malathi et al., 2010; Minhas et al., 2010), showing a good generalization performance with an extremely fast learning speed.", "startOffset": 83, "endOffset": 163}, {"referenceID": 13, "context": ", 2006) which has been successfully applied to a number of real-world applications (Sun et al., 2008; Wang et al., 2008; Malathi et al., 2010; Minhas et al., 2010), showing a good generalization performance with an extremely fast learning speed.", "startOffset": 83, "endOffset": 163}, {"referenceID": 15, "context": ", 2006) which has been successfully applied to a number of real-world applications (Sun et al., 2008; Wang et al., 2008; Malathi et al., 2010; Minhas et al., 2010), showing a good generalization performance with an extremely fast learning speed.", "startOffset": 83, "endOffset": 163}, {"referenceID": 25, "context": "However, such techniques seem to require more hidden units than typical values from backpropagation training to achieve comparable accuracy, as discussed in Yu and Deng (Yu and Deng, 2012).", "startOffset": 169, "endOffset": 188}, {"referenceID": 5, "context": "One aim of this paper is the analysis of these instability issues; a preliminary assessment of the context and our initial results are discussed in (Cancelliere et al., 2012).", "startOffset": 148, "endOffset": 174}, {"referenceID": 17, "context": "in (Penrose and Todd, 1956; Bishop, 2006), is W\u0304 = HT, where H is the Moore-Penrose generalised inverse (or pseudoinverse) of matrixH.", "startOffset": 3, "endOffset": 41}, {"referenceID": 3, "context": "in (Penrose and Todd, 1956; Bishop, 2006), is W\u0304 = HT, where H is the Moore-Penrose generalised inverse (or pseudoinverse) of matrixH.", "startOffset": 3, "endOffset": 41}, {"referenceID": 19, "context": "where \u03a3 is obtained from \u03a3 by taking the reciprocal of each non-zero element \u03c3i, and transposing the resulting matrix (Rao and Mitra, 1971).", "startOffset": 118, "endOffset": 139}, {"referenceID": 1, "context": "roughly speaking into a problem insensitive to small changes in training conditions, regularisation methods are often used (Badeva and Morozov, 1991), and Tikhonov regularisation is one of the most common (Tikhonov and Arsenin, 1977; Tikhonov, 1963).", "startOffset": 123, "endOffset": 149}, {"referenceID": 22, "context": "roughly speaking into a problem insensitive to small changes in training conditions, regularisation methods are often used (Badeva and Morozov, 1991), and Tikhonov regularisation is one of the most common (Tikhonov and Arsenin, 1977; Tikhonov, 1963).", "startOffset": 205, "endOffset": 249}, {"referenceID": 23, "context": "roughly speaking into a problem insensitive to small changes in training conditions, regularisation methods are often used (Badeva and Morozov, 1991), and Tikhonov regularisation is one of the most common (Tikhonov and Arsenin, 1977; Tikhonov, 1963).", "startOffset": 205, "endOffset": 249}, {"referenceID": 7, "context": "The penalty term improves on stability, making the problem less sensitive to initial conditions, and contains model complexity avoiding overfitting, as largely discussed in (Gallinari and Cibas, 1999).", "startOffset": 173, "endOffset": 200}, {"referenceID": 3, "context": "To give preference to solutions \u0174 with smaller norm (Bishop, 2006) a frequent choice is \u0393 = \u221a \u03bbI, so eqs.", "startOffset": 52, "endOffset": 66}, {"referenceID": 6, "context": "(Fuhry and Reichel, 2012)) as: \u0174 = V DU T (8)", "startOffset": 0, "endOffset": 25}, {"referenceID": 5, "context": "Some numerical instability issues have already been evidenced in our previous investigations (Cancelliere et al., 2012); we provided suggestions on possible mitigation techniques like selection of a convenient activation function and normalisation of the input weights.", "startOffset": 93, "endOffset": 119}, {"referenceID": 2, "context": "The use of sigmoidal activation functions has recently been subject of debate because they seem to be more easily driven towards saturation due to their nonzero mean value (Bengio and Glorot, 2010), while hyperbolic tangent seems less sensitive to this problem.", "startOffset": 172, "endOffset": 197}, {"referenceID": 5, "context": "To further mitigate saturation issues, in our previous work (Cancelliere et al., 2012) we selected input weights according to a uniform random distribution in the range (\u22121/ \u221a M , 1/ \u221a M), where M is the number of hidden nodes.", "startOffset": 60, "endOffset": 86}, {"referenceID": 9, "context": "Because of the wide use among researchers belonging to ELM-community (see for instance (Helmy and Rasheed, 2009; Huang et al., 2006; Sun et al., 2008) our performance is also compared with that from unregularised pseudoinversion, input weights selected according to a random uniform distribution in the interval (\u22121,1) and sigmoidal activation functions (hereafter, ELM).", "startOffset": 87, "endOffset": 150}, {"referenceID": 10, "context": "Because of the wide use among researchers belonging to ELM-community (see for instance (Helmy and Rasheed, 2009; Huang et al., 2006; Sun et al., 2008) our performance is also compared with that from unregularised pseudoinversion, input weights selected according to a random uniform distribution in the interval (\u22121,1) and sigmoidal activation functions (hereafter, ELM).", "startOffset": 87, "endOffset": 150}, {"referenceID": 21, "context": "Because of the wide use among researchers belonging to ELM-community (see for instance (Helmy and Rasheed, 2009; Huang et al., 2006; Sun et al., 2008) our performance is also compared with that from unregularised pseudoinversion, input weights selected according to a random uniform distribution in the interval (\u22121,1) and sigmoidal activation functions (hereafter, ELM).", "startOffset": 87, "endOffset": 150}, {"referenceID": 5, "context": "The same trend was detected analysing the astronomical dataset in (Cancelliere et al., 2012).", "startOffset": 66, "endOffset": 92}, {"referenceID": 5, "context": "On the contrary, when weights are randomly selected in the interval (\u22121/ \u221a M, 1/ \u221a M), as for HypT-unreg and Sigm-unreg cases, input weights are automatically kept small when the network size increases, thus exploiting the central part of both activation functions: consequently saturation is avoided (Cancelliere et al., 2012).", "startOffset": 301, "endOffset": 327}, {"referenceID": 25, "context": "(Yu and Deng, 2012)).", "startOffset": 0, "endOffset": 19}, {"referenceID": 10, "context": "The interested reader can find, for the common datatsets, a comparison among training times of ELM and backpropagation in (Huang et al., 2006), and can verify that ELM turns out to be two or three orders of magnitude faster.", "startOffset": 122, "endOffset": 142}, {"referenceID": 14, "context": "(Miche et al., 2011), we note that our technique achieves RMSE values lower than those corresponding to their MSE values, with a", "startOffset": 0, "endOffset": 20}], "year": 2017, "abstractText": "Some novel strategies have recently been proposed for single hidden layer neural network training that set randomly the weights from input to hidden layer, while weights from hidden to output layer are analytically determined by pseudoinversion. These techniques are gaining popularity in spite of their known numerical issues when singular and/or almost singular matrices are involved. In this paper we discuss a critical use of Singular Value Analysis for identification of these drawbacks and we propose an original use of regularisation to determine the output weights, based on the concept of critical hidden layer size. This approach also allows to limit the training computational effort. Besides, we introduce a novel technique which relies an effective determination of input weights to the hidden layer dimension. This approach is tested for both regression and classification tasks, resulting in a significant performance improvement with respect to alternative methods.", "creator": "LaTeX with hyperref package"}}}