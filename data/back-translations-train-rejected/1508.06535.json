{"id": "1508.06535", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "26-Aug-2015", "title": "Deep Convolutional Neural Networks for Smile Recognition", "abstract": "This thesis describes the design and implementation of a smile detector based on deep convolutional neural networks. It starts with a summary of neural networks, the difficulties of training them and new training methods, such as Restricted Boltzmann Machines or autoencoders. It then provides a literature review of convolutional neural networks and recurrent neural networks. In order to select databases for smile recognition, comprehensive statistics of databases popular in the field of facial expression recognition were generated and are summarized in this thesis. It then proposes a model for smile detection, of which the main part is implemented. The experimental results are discussed in this thesis and justified based on a comprehensive model selection performed. All experiments were run on a Tesla K40c GPU benefiting from a speedup of up to factor 10 over the computations on a CPU. A smile detection test accuracy of 99.45% is achieved for the Denver Intensity of Spontaneous Facial Action (DISFA) database, significantly outperforming existing approaches with accuracies ranging from 65.55% to 79.67%. This experiment is re-run under various variations, such as retaining less neutral images or only the low or high intensities, of which the results are extensively compared.", "histories": [["v1", "Wed, 26 Aug 2015 15:39:09 GMT  (3518kb,D)", "http://arxiv.org/abs/1508.06535v1", "MSc thesis"]], "COMMENTS": "MSc thesis", "reviews": [], "SUBJECTS": "cs.CV cs.LG cs.NE", "authors": ["patrick o glauner"], "accepted": false, "id": "1508.06535"}, "pdf": {"name": "1508.06535.pdf", "metadata": {"source": "CRF", "title": "Deep Convolutional Neural Networks for Smile Recognition", "authors": ["Patrick Oliver GLAUNER", "Stavros PETRIDIS", "Sinisa TODOROVIC"], "emails": [], "sections": [{"heading": null, "text": "Imperial College LondonDepartment of ComputingDeep Convolutional Neural Networks for SmileRecognitionPatrick Oliver GLAUNERSeptember 2015Supervised by Professor Maja PANTIK and Dr. Stavros PETRIDISSubmitted in part fulfilling the requirements for the degree of Master of Science in Computing (Machine Learning) of Imperial College Londonar Xiv: 150 8.06 535v 1 [cs.C V] 26 Aug 201 5DeclarationI hereby confirm that all materials in this report, which is not my own work, are duly acknowledged. Patrick Oliver GLAUNERAbstractThis thesis describes the design and implementation of a Smile detector based on deep convolutionary neural networks. It begins with a summary of the neural networks, the difficulties of training them and new training methods, such as Restricted Boltzmann Machines or autoencoders. It then provides a literature review of Convolutionary Neural Networks and Recurrent NETURAL PAGES SITE SITE SITE SITE to be RECOMMMMENTATION SITE."}, {"heading": "I would also like to thank Professor Maja PANTIC for her passion, setting the", "text": "direction of this work and valuable regular feedback. In addition, I am Professor Sinisa TODOROVIC.Table of contents"}, {"heading": "1. Introduction 12", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "2. Background report: neural networks 13", "text": "......................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................"}, {"heading": "3. Selection of databases 24", "text": "3.1. FACS encoding.........................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................."}, {"heading": "4. Model 29", "text": "4.1. Proposed model................................................................................................................................................................."}, {"heading": "5. Towards a static convolutional smile detector 33", "text": "In the second half of the year, the number of infected people in the U.S. will rise again. In the second half of the year, the number of infected infected infected infected infected infected infected infected infected infected infected infected infected infected infected infected infected infected infected infected infected infected infected infected infected infected infected infected infected infected infected infected infected infected infected infected infected infected infected infected infected infected infected infected."}, {"heading": "6. Conclusions and future work 48", "text": "Bibliography 49"}, {"heading": "A. Statistics of all action units 54", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "B. Training time of networks 57", "text": "B.1. Complete data sets......................................................................................................................................"}, {"heading": "C. Result of model selection 60", "text": "C.1. Complete data set..........................................................................................................................."}, {"heading": "D. Performance of selected models 64", "text": "There is no single, unitary, unitary, unitary, unitary, unitary, unitary, unitary, unitary, unitary, unitary, unitary, unitary, unitary, unitary, unitary, unitary, unitary, unitary, unitary, unitary, unitary, unitary, unitary, unitary, unitary, unitary, unitary, unitary, unitary, unitary, unitary, unitary, unitary, unitary, unitary, unitary, unitary, unitary, unitary, unitary, unitary, unitary, unitary, unitary, unitary, unitary, unitary, unitary, unitary, unitary, unitary, unitary, unitary, unitary, unitary, unitary, unitary, unitary, unitary, unitary, unitary, unitary, unitary, unitary, unitary, unitary, unitary, unitary, unitary, unitary, unitary, unitary, unitary, unitary, unitary, unitary, unitary, unitary, unitary, unitary, unitary, unitary, unitary, unitary, unitary, unitary, unitary, unitary, unitary, unitary, unitary, unitary, unitary, unitary, unitary, unitary, unitary, unitary, unitary, unitary, unitary, unitary, unitary, unitary, unitary, unitary, unitary, unitary, unitary, unitary, unitary, unitary, unitary, unitary, unitary, unitary, unitary, unitary, unitary, unitary, unitary, unitary, unitary, unitary, unitary, unitary, un"}, {"heading": "1. Introduction", "text": "Neural networks have been popular in the machine learning community since the 1980s, with their repeated rise and fall in popularity. Their main advantage is their ability to learn complex, non-linear hypotheses from data without having to model complex features, making them of particular interest to computer vision, where the description of features is a long-standing and largely ununderstood issue. Neural networks are difficult to train and have gained enormous fame under the theme of \"deep learning\" over the past decade. Overall, significant advances have been made in the training methods of natural language processing, computer vision and audio processing. Leading IT companies have made significant investments in deep learning for these reasons, such as Baidu, Google, Facebook and Microsoft.Specifically, earlier work by the author on deep learning and pattern recognition in the areas of natural language processing, computer vision and audio processing. Leading IT companies have made significant investments in deep learning for these reasons, making significant investments in these networks for these reasons."}, {"heading": "2. Background report: neural networks", "text": "This chapter gives an overview of different types of neural networks, their capabilities and training challenges, based on [12]. This chapter does not provide an introduction to neural networks, so the reader is referred to [4] and [37] for a comprehensive introduction to neural networks. Neural networks are brain-inspired and consist of several levels of logistic regression units, known as neurons. They experienced different hypes in the 1960s and 1980s / 90s. Neural networks are known to learn complex hypotheses of regression and classification. Conversely, the formation of neural networks is difficult because their cost functions have many local minima. Therefore, education tends to align to a local minimum, leading to a poor generalization of the network. In the last decade, neural networks have made a comeback under the term deep learning and use many hidden layers to build more powerful machine learning algorithms."}, {"heading": "2.1. Feed-forward neural networks", "text": "Forward-facing neural networks are the simplest type of neural networks. They consist of an input layer, one or more hidden layers and an output layer as shown in Figure 2.1. the activation of the unit i of layer j + 1 can be calculated as follows: z (j + 1) i = sj \u2211 k = 0% (j) ik xk (2.1) a (j + 1) i = g (z (j + 1) i) (2.2) g is an activation function for which the sigmoid activation function 1 + e \u2212 x is often used in the hidden layers. The sigmoid function or its generalization, the Softmax function, is used for classification problems in output layer units. For regression problems, the sum of equation 2.1 is used directly in the output layer without using any activation functions. To minimize the better learned functions, the lowest cost and the lowest cost, the unit is referred to as the output unit function 1."}, {"heading": "2.1.1. Difficulty of training", "text": "To learn the weights, the algorithm 2,1 called Backpropagation is used to efficiently calculate the partial derivatives, which are then fed into an optimization algorithm, such as Gradient Descent (Algorithm 2,2) or Stochastic Gradient Descent (Algorithm 2,3), as described in [31]. These three algorithms are based on [40].Algorithm 2,1 Backpropagation: Training quantity m.Equity (l) ij \u2190 rand (\u2212,) (for all l, i, j)."}, {"heading": "2.2. Deep neural networks", "text": "Deep neural networks use many hidden layers, making it possible to learn more and more complex hierarchies of characteristics, as illustrated in Figure 2.2 for the Google Brain [29]. Architectures of this kind are of enormous use, since the long-standing problem of attribute description largely disappears in signal processing. Conversely, the formation of deep neural networks becomes more difficult due to the increased number of parameters. As described in [7] and [8], the return does not scale to deep neural networks: starting from small random initial weights, the backpropagated partial derivatives go to zero. As a result, the formation becomes impracticable and is called the problem of the disappearing gradient."}, {"heading": "2.2.1. Training methods", "text": "For deep neural networks, therefore, the training was divided into two parts: pre-training and fine-tuning. Pre-training enables the initialization of weights to a location in the cost function that can be quickly optimized by regular back-propagation. In the literature, various pre-training methods are described, most prominently unattended methods such as Restricted Boltzmann Machines (RBM) in [18] and [20] or auto-encoders in [41] and [5]. Both methods learn exactly one hidden layer, which is then used as input to the next RBM or auto-encoder to learn the next hidden layer. This process can be repeated many times to more effectively use a so-called Deep Belief Network (DBN) or Stacked Autoencoder, consisting of RBMs or auto-encoders. In addition, there are denoising auto-coders defined in [28], which are trained so that auto-coders are composed of internal RBMs or auto-coders."}, {"heading": "2.2.2. Activation functions", "text": "In the past, sigmoid units were mostly used in the hidden layers, with sigmoid or linear units in the output layer being used for classification or regression, and for classification Softmax activation in the output layer is preferred. As described by Norvig in [44], the output of one specified unit is much stronger than the output of others. Another advantage of Softmax is that it is always differentiable for weight. Recently, the so-called rectified linear unit (ReLU) has been proposed in [38], which is successfully used in many deep learning applications. Figure 2.3 illustrates the sigmoid and ReLU functions.ReLU has a number of advantages over sigmoid reported in [38] and [14]. Firstly, it may be much easier to calculate it as it is either 0 or the input value. In addition, sigmoid has an activation value greater than 0 for input values that are not activated or equal to 0."}, {"heading": "2.2.3. Application to facial expression data", "text": "In this project, deep neural networks were successfully applied to facial expression recognition in [12]. In this study, RBMs, autoencoders and denocializing autoencoders were compared to a noisy dataset of a 2013 Kaggle challenge called \"Emotion and identity detection from face images\" [25], which was won by a neural network presented in [55] with a 52.977% error rate. In [12] a stacked autoencoder was trained with a 39.75% error. In a subsequent project, this error was further reduced to 28% with a stacked denoizing autoencoder [13]. This study also showed that deep neural networks are a promising machine learning method in this context, but not a silver bullet as data pre-processing and intensive model selection are still required."}, {"heading": "2.3. Recurrent neural networks", "text": "Recurrent neural networks (RNNs) are cyclic graphs of neurons as shown in Figure 2.4. They have increased representational power because they generate an internal state of the network that allows them to exhibit dynamic temporal behavior. RNs \"training is more complex because it depends on their structure. Figure 2.4 can be trained using a simple variant of reverse propagation. In practice, relapsing networks are more difficult to train than feedback-forward networks and are less reliable in general."}, {"heading": "2.3.1. Long short-term memory", "text": "A long-term memory (LSTM) defined in [21] is a modular recursive neural network composed of LSTM cells. An LSTM cell is represented in Figure 2.5. Inputs wcji are fed, for which a value g is calculated using the sigmoid function of the point product of input and weight. The second sigmoid unit yinj is the entrance gate. If the output value is close to zero, the product g \u00b7 yinj is also close to zero, which causes the input value to be set to zero. As a result, this blocks the input value and prevents it from penetrating further into the cell. The third sigmoid unit youtj is the exit gate. Its function is to determine when the internal state of the cell should be output. This is the case when the output of this sigmoid unit is close to one. LSTM cells can be assembled in a modular structure, as shown in Figure 2.6 to herald complex predictions."}, {"heading": "2.4. Convolutional neural networks", "text": "Typical deviations from images and videos include translation, rotation, and scaling. Tangent propagation [4] is a method in neural networks to manage transformations by punishing the amount of distortion in the cost function. Convolutionary neural networks (CNNs) are another approach to implementing invariance in neural networks inspired by biological processes. CNNs were originally proposed by LeCun in [30]. They have been successfully applied to computer education problems, such as handwritten digital recognition. In images, nearby pixels are strongly correlated, a property that exploits local characteristics. In a hierarchical approach, local characteristics are used in the first stage of pattern recognition, allowing for the detection of more complex characteristics. The concept of CNNs is described in Figure 2.7 for a layer of evolutionary units, followed by a sampling level."}, {"heading": "2.5. Processing of image sequences", "text": "Recently, it has been reported that CNNs work well at processing image sequences, for example, in [27] for multiple coils, as described in Figure 2.8.A related approach is described in [23]. CNNs are expanded to work on image sequences instead of single images, and the additional weights must be initialized so that they can be easily optimized. [33] offers a comprehensive study and comparison of different initialization methods. [49] describes a deep architecture composed of coils, LSTMs, and regular layers for an NLP problem, starting with several revolutionary layers. Next, a linear layer with fewer units to reduce the dimensionality of the features recognized by the revolutionary layers. Next, the reduced features are fed into an LSTM. The output of the LSTM is then used in regular layers for classification, and the entire architecture is used in Figure 2.9.Similar architectures exist for processing image sequences and are further developed."}, {"heading": "3. Selection of databases", "text": "In this chapter, various popular databases relevant to the detection of action units are presented. Each database includes, among other things, annotations for the respective action units. In addition, statistics on the distribution of action units for each database have been created to select databases full of smiles."}, {"heading": "3.1. FACS coding", "text": "The Facial Action Coding System (FACS) is a system for taxonomizing a person's facial expression based on his facial expression. It was published in 1978 by Paul Ekman and Wallace V. Friesen [6]. Relevant to this thesis are so-called Action Units (AUs), which represent the basic actions of individual facial muscles or muscle groups. Action Units are either set or unset."}, {"heading": "3.2. Available databases", "text": "The Affectiva-MIT Facial Expression Dataset (AMFED) [36] database contains 242 facial videos (168,359 frames) recorded in the wild (under real-life conditions); the Chinese Academy of Sciences Micro-expression (CASME) database [58] was filmed at 60fps and contains 195 micro-expressions of 22 male and 13 female participants; and the Denver Intensity of Spontaneous Facial Action (DISFA) database [35] contains videos of 15 male and 12 female subjects of different ethnicities. Comments of the Action Unit are of varying intensity; and the Geneva Multimodal Emotion Portrayals (GEMEP) [2] contain audio and video recordings of 10 actors representing 18 emotional states."}, {"heading": "3.3. Distribution of action unit intensities", "text": "This task has proved complex as the structure of each database is different and needs to be evaluated1. Comprehensive diagrams and statistics of the individual action units have been compiled. For example, Figure 3.2 shows the binary distribution of AU12, which Smile represents in FACS encoding, the CASME database.Statistics were compiled at different granularity levels.Figure 3.3, for example, contains a selection of action units of the different databases. Due to different terminology, the AMFED database does not use AU12, but a feature called \"Smile,\" as explained in [36].The complete statistics of all action units are in Appendix A.1. Without the use of its own programming language, this task would be easy to accomplish on Pysis alone, as the structure of each database is different and needs to be analysed.Figure 3.4 contains the multi-value intensity distribution of AU12 of the entire DISFAdatabase.Table 3.1 contains a selection of action units of the different databases."}, {"heading": "3.4. Selected databases", "text": "To be selected for the following experiments, a database AU pair must meet two conditions: First, the action unit in the annotations of a database should be set sufficiently frequently to be easier to learn; second, the database images should be in an aligned format; aligned images will be cropped, leaving the actual face in the center; and the availability of annotations on prominent dots in the face."}, {"heading": "3.4.1. DISFA", "text": "For these reasons, the DISFA database was selected to be used for smile detection. Figure 3.5 is a sample image of the aligned version of DISFA. Since a video in DISFA lacks the 4845th image and to avoid dealing with this incidence, only the first 4844 images were used for each video. A total of 130,788 images were used. Further statistics were created for these images. In particular, 30,792 AU12 were set. Table 3.2 contains the distribution of AU12. A total of 82,176 images were set for some action units and 48,612 images did not specify any action units at all. In the original paper on DISFA [35], multi-class SVMs were trained for the different levels 0-5 of the action unit."}, {"heading": "3.4.2. Others", "text": "For the same reasons, the shoulder pain database is of interest for further experiments, e.g. for a multi-database smile detector. In addition, laughter in the MAHNOB laugh database may be of interest for future experiments, since laughter also includes smiles. AMFED was not taken into account further, as \"smile\" is not AU12, but may be of interest for other experiments as well."}, {"heading": "4. Model", "text": "The goal of this project is to detect and predict units of action from videos, especially smiles. A regular deep neural network would not do the job for two reasons: First, deep neural networks do not support the handling of translations or other distortions of input that often occur in facial videos; second, deep neural networks do not have a state, making video processing more difficult since they require the handling of states to detect or predict units of action. In this chapter, the proposed model for smile detection, of which the first part is implemented, is explained in detail; to train it in a reasonable time, a powerful underlying computing infrastructure was used."}, {"heading": "4.1. Proposed model", "text": "Based on the results described in Chapter 2.5, a first model has been defined and refined after discussions with other experts, including Sinisa Todorovic [48]. The model can be summarized as follows: feature extraction in the first stage, followed by the temporal part. For feature extraction, a CNN is trained on images of the entire face or an area suitable for detecting smiles, such as the mouth. This CNN is followed by one or more layers of a regular (dense) neural network to distinguish the features. The exact architecture of the network, such as the number of turns, the number of hidden layers, etc., is subject to the model selection that was carried out in detail in Chapter 5. The size of the input is also subject to model selection, as one input unit is required per input pixel. The larger the input image, the better, as more data and details are available. Conversely, the model becomes more complex and difficult to educate, with excessive or long training times as possible."}, {"heading": "4.2. Implementation", "text": "In this section, the most important implementation decisions are described and explained. In the course of this work, the underlying algorithms of deep learning were not implemented due to time constraints. Therefore, suitable libraries were selected and the results of this evaluation were explained in this section."}, {"heading": "4.2.1. Selection of deep learning library", "text": "The two most important libraries in this area are Theano [3] and Caffe [4], both of which use the GPU for their calculations and have been extensively compared for this project. Results are summarized in this section. Theano is a general purpose of the numerical library for Python, and its instructions are either executed on the CPU or used extensively for this project. Results are summarized in this section."}, {"heading": "4.2.2. Selection of LSTM library", "text": "There is an extension of the lasagna for LSTMs [11], which proves to be effective in evaluation. It is easiest to use it together with the feature detector of the first stage. An end-to-end training of the whole model is also possible with this library. Nevertheless, the project has only one main person in charge, who is unsure whether it will be kept in sync with the lasagna in the future. Support for the use of GPUs for training also offers CURRENNNT [50], a C + + library for recurrent neural networks. This library does not support Python, which makes it difficult to integrate into the existing code of the feature detector. In addition, RNLIB [51] is a popular library for recursive neural networks, including LSTMs. Its Python wrapper allows easy integration into the existing code of the feature detector."}, {"heading": "4.2.3. Progress of implementation", "text": "As already mentioned, the lasagna is still in development, which made implementing the model more time-consuming than originally expected due to changes in the API. In particular, a lot of demo code did not work correctly, leaving the author of this thesis with unexpected behavior and no useful error messages. After these problems were fixed, implementing the training and model selection of the feature detector in Chapter 5 was straightforward due to the abstraction provided by Lasagne. In the course of this project, only the first stage of the model, the feature detector, was implemented. Due to time constraints, the second part could not be implemented. Due to the overall high test accuracy of the feature detector in Chapter 5, there is also less need to add temporal capabilities to this model at this time."}, {"heading": "4.3. Computing infrastructure", "text": "In the first experiments, the GPU acceleration provided by Theano was able to accelerate training by a factor of 3-10 compared to a CPU. Various GPUs were used in these experiments, including a GeForce GTX TITAN Black [10] or an even more powerful Tesla K40c [45]. For the Tesla series, significant acceleration was measured for various applications as collected in Figure 4.2. for the experiments in Chapter 5, a server containing a Tesla K40c with 12 GB of GPU RAM and 64 GB of regular RAM was selected, both of which are large enough to store the model and training data. Tesla would allow multiple experiments to be performed simultaneously, as a single experiment uses only a fraction of the GPU RAM as shown in Figure 4.3."}, {"heading": "5. Towards a static convolutional smile", "text": "DetectorIn this chapter, experiments for the detection of smiles with the coil characteristic detector are performed in the DISFA database. An essential task is the selection of models to select the best architecture from a large permutation of many possible parameters. Starting with the regular detection of smiles, only smiles with low or high intensity are retained for the detection of smiles. Finally, smiles with low intensity are distinguished from smiles with high intensity. In order to perform the experiments in time, preliminary assumptions are made."}, {"heading": "5.1. Selected parameters and assumptions", "text": "Nowadays there is a lack of literature or research on neural networks in terms of sample complexity or general rules for selecting an architecture. Therefore, in order to find good parameter values for the feature detector, model selection must be carried out."}, {"heading": "5.1.1. Candidate parameters to be optimized", "text": "There are many possible parameters that need to be optimized and described in the literature, including: 1. Number of folding pooling pairings2. Architecture of the turns, such as the number of feature cards and their size3. Architecture of the pools, such as the type of pool, the size of the pool or whether pooling should be done at all 4. Type of activation function, such as rectified linear units (ReLU), Softmax or Sigmoid5. Type of regulation, such as dropouts or L26. Number of hidden layers 7. Number of units in the hidden layers 8. Learning rates9. Current parameters 1 to 3 concern the revolutionary part of the network. A number of optimizations are possible, such as the number of folding pooling pairs and how the individual convolutions and poolings should be constructed. Parameters to be optimized include the size and number of feature cards, the pooling part, and the overall question of whether pairing is good."}, {"heading": "5.1.2. Selected parameters and values", "text": "In order to reduce the duration of the model selection to a realistic scale, several assumptions were made in Chapter 2.2. For coils and subsequent poolings, many parameters could be optimized in the model selection, causing the possible search space to explode. Therefore, a number of parameters based on experiments with the same library on MNIST are specified: coils are for ranges of 5 x 5 pixels and 32 characteristic maps are used in each coil layer. Subsequent pooling is provided for ranges of 2 x 2 pixels and only the maximum pooling rate is used, as the specific type of pooling is described as less relevant in the literature [42]. Convolution pooling pairs are used during the experiments, not individual coils that pooling does not follow [52]. For simplicity, a coil and bundling pair is simply called convolution."}, {"heading": "5.1.3. Cost function and performance metrics", "text": "For the following model selection, the cross-entropy loss / cost function for M-examples, hypotheses-hypotheses-hypotheses-hypotheses-hypotheses-hypotheses-hypotheses-hypotheses-hypotheses-hypotheses-hypotheses-hypotheses-hypotheses-hypotheses-hypotheses-hypotheses-hypotheses-hypotheses-hypotheses-hypotheses-hypotheses-hypotheses-hypotheses-hypotheses-hypotheses-hypotheses-hypotheses-hypotheses-hypotheses-hypotheses-hypotheses-hypotheses-hypotheses-hypotheses-hypotheses-hypotheses-hypotheses-hypotheses-hypotheses-hypotheses-hypotheses-hypotheses-hypotheses-hypotheses-hypotheses-hypotheses-hypotheses-hypotheses-hypotheses-hypotheses-hypotheses-hypotheses-hypotheses-hypotheses-hypotheses-hypotheses-hypotheses-hypotheses-hypotheses-hypotheses-hypotheses-hypotheses-hypotheses-hypotheses-hypotheses-hypotheses-hypotheses-hypotheses-hypotheses-hypotheses-hypotheses-hypotheses-hypotheses-hypotheses-hypotheses-hypotheses-hypotheses-hypotheses-hypotheses-hypotheses-hypotheses-hypotheses-hypotheses-hypotheses-hypotheses-hypotheses-hypotheses-hypotheses-hypotheses-hypotheses-hypotheses-hypotheses-hypotheses-hypotheses-hypotheses-hypotheses-hypotheses-hypotheses"}, {"heading": "5.1.4. Input size", "text": "All the experiments are carried out with two different data sources: mouth or whole face, to find out if the mouth alone is as important as the face to detect the smile, see Figure 5.1. The aligned images are 285 x 378 pixels, as described in Chapter 3.4.1. Based on the face markers contained in the aligned images, the position of the mouth can be calculated. A delimiting box has been calculated that includes the mouth of each image. This delimiting box has a size of 128 x 104 pixels, which would require a total of 5,865 input units. To reduce the overmatch and speed up the training, both dimensions have been reduced to 2 / 3 of their original size by a bilateral interpolation. Therefore, the mouth input is 85 x 69 pixels, which would require a total of 5,865 input units. The same considerations have been applied to the input of the entire face. The size reduction factor is stronger for the mouth, in order to possibly perform the training at a significant cost in time."}, {"heading": "5.2. Bottom lines", "text": "In order to evaluate the underlying software and hardware infrastructure, very initial experiments were conducted with much smaller inputs on fixed architectures. Mouth images are of the size 37 x 28, face images of the size 40 x 48. The architectures used are a regular neural network of 2 hidden layers of 800 units each, followed by a softmax output layer of 2 units. The other architecture is a convolutionary neural network of two folding and pooling levels and a fully connected hidden layer in front of the softmax output layer. Details are omitted in these initial experiments. Test losses and accuracy are summarized in Table 5.2. For both inputs, the CNNs outperform the NNs with a margin of 3.18% and 5.44% for mouth and face respectively. Both CNNs achieve an accuracy of over 90%. Nevertheless, given the noise in the data, the NNs intersect well."}, {"heading": "5.3. Model selection for full dataset", "text": "For the following model selection, the entire DISFA database was divided into a training / validation / test ratio of 60% / 20% / 20%. The validation rate was used exclusively for loss validation at each epoch.According to the respective number of epochs, the test rate was used to calculate test loss and test accuracy. Each of the parameters was optimized simultaneously regardless of default values of the other parameters.The model selection was performed for two different types of input: mouth or the entire face and for two different epochs: 10 and 50 epochs. An exhaustive search or more epochs was not possible due to the enormous training times for which statistics of each trained permutation are available in Appendix B. In most examples, the validation loss falls quickly before being slowly converged, as illustrated in Figure 5.2. Therefore, at this stage of the experiment, the maximum number of epochs is set to 50. Each experiment is performed exactly once, but with some significant distortion."}, {"heading": "5.3.1. Mouth", "text": "For mouth input, Tables C.1 and C.2 contain the result of the model selection for 10 and 50 epochs, respectively. The first row of each table is the combination of all the default values and therefore serves as the quintessence in each experiment. For 10 epochs, the default value was not selected for any of the four parameters to be better than the parameters available in the selection. In particular, the lowest losses for 2 turns, 3 hidden layers, 400 hidden units per layer and a failure value of 0.1 are returned. Overall, the test accuracy is at a high level and the individually tested values for the parameters have little impact, as the test accuracy is between 90.99% and 94.54%. A failure value of 0.1 leads to the highest accuracy, while a failure value of 0.7 reduces the test accuracy in the must. For 50 epochs, these selected values remain the same except that in this case 2 hidden layers have the lowest test loss. Overall, the test accuracy at a higher level decreases from 0.7% to the test accuracy of the lowest 954%, from the highest 94.1%."}, {"heading": "5.3.2. Face", "text": "Conversely, Tables C.3 and C.4 contain the result of the model selection for 10 and 50 epochs, respectively, for the entire area input. For 10 epochs, the default setting has been chosen to be better than the one available for the selection in terms of number of coils and number of hidden units. In particular, minimum losses for 1 folding, 2 hidden layers, 100 hidden units per layer and a failure value of 0 are returned. Overall, the test accuracy is at a high level and the individually tested values for the parameters have little impact, as the test accuracy ranges from 94.69% to 96.36%. A failure value of 0 leads to the highest accuracy, while 3 coils reduce the test accuracy in high-speed. For 50 epochs, these selected values remain the same in terms of number of revolutions and downtimes, while the optimal number of hidden layers decreases to 1 and 400 respectively."}, {"heading": "5.3.3. Comparison of mouth vs. face", "text": "For both input parts, mouth and face, the model selection for 50 epochs yielded different optimal parameters, which are collected in Table 5.3. Overall, the test accuracy for entering the entire face is slightly higher than for entering the mouth only for 10 and 50 epochs. Strictly speaking, the test accuracy for the entire face for 10 epochs is about 3% higher. This is logical, since smile is visible not only on the mouth of the human being, but also in other areas, such as the cheeks. This gap decreases to about 2% for 50 epochs. The training time for the entire face is about 20% higher than for the mouth. This is surprisingly less than expected, as the number of input pixels is almost twice as high as for the mouth."}, {"heading": "5.4. Model selection for reduced dataset", "text": "For the model selection in chapter 5.3, the entire DISFA database was used. Measured in chapter 3.4.1, 48,612 of the total 130,788 images are neutral, i.e. no action units are specified. The more neutral the images, the easier the training of the smile detector is. Therefore, a reduced set of DISFA is used in this section. It consists of all 82,176 images that have set an action unit (s) and 30% of the 48,612 remaining neutral images, resulting in a total of 96,759 images. The structure of the experiments remains the same, especially the ratio of training / validation / test examples, the two different input times, mouth and face, the number of eras and the non-exhaustive model selection. Training statistics of each trained permutation are available in Appendix B."}, {"heading": "5.4.1. Mouth", "text": "For mouth input, Tables C.5 and C.6 contain the result of the model selection for 10 and 50 epochs, respectively. For 10 epochs, none of the four parameters were selected for the default value, which is better than the one available in the selection. Specifically, the lowest losses for 2 turns, 2 hidden layers, 400 hidden units per shift and a failure value of 0 are returned. Overall, the test accuracy is at a high level and individually tested values for the parameters have little impact, as the test accuracy ranges from 89.46% to 93.62%. 2 turns result in the highest accuracy, while a failure value of 0.7 most often reduces the test accuracy. For 50 epochs, these selected values remain the same except that in this case 300 hidden units have the lowest test loss. Overall, the test accuracy is at an even higher level and the individually tested values for the parameters have little impact, as the test accuracy is between 95.84% and 97.59%."}, {"heading": "5.4.2. Face", "text": "Conversely, Tables C.7 and C.8 contain the result of the model selection for 10 and 50 epochs respectively for the entire area input. For 10 epochs, the default setting is chosen to be better than the one available in the selection in terms of the number of coils. In particular, minimum losses are returned for 1 turn, 2 hidden layers, 300 hidden units per layer and a failure value of 0. Overall, the test accuracy is at a high level and the individually tested values for the parameters have little impact, as the test accuracy varies between 92.65% and 95.44%. A failure value of 0 leads to the highest accuracy, while 3 turns reduce the test accuracy fastest. For 50 epochs, these selected values remain the same for the number of coils and the number of hidden units. The optimal number of hidden layers and the failure risk change to 1 and 0.1, respectively. Overall, the test accuracy is at an even higher level and the individually tested values for the least toxic parameters."}, {"heading": "5.4.3. Comparison of mouth vs. face", "text": "For both input parts, mouth and face, the model selection for 50 epochs yielded different optimal parameters, which are collected in Table 5.4. Overall, the test accuracy for entering the entire face is slightly higher than for entering the mouth for 10 and 50 epochs only. Strictly speaking, the test accuracy for the entire face for 10 epochs is about 2% higher, reducing this gap to about 1% for 50 epochs. Training time for the entire face is also about 20% higher than for the mouth alone."}, {"heading": "5.5. Repeatability of experiments", "text": "Each experiment has been performed exactly once. Neural network training is subject to random initialization of weights at the beginning of training and random distribution of data into training, validation and test sets, so repetition of an experiment may produce different results. If this difference is large, each experiment must be performed several times to use its median in model selection decisions. To assess whether such a time-consuming process is necessary or not, training of the neural network for 2 hidden layers of oral input in the model selection has been performed ten times for the entire dataset. Results are available in Table 5.5 with a standard deviation of 0.041725% in test accuracy. Due to this small standard deviation, conducting each experiment has a very low bias and is therefore relatively safe for faster training time. The standard deviation of cross entropy loss has been omitted as it is not useful for humans."}, {"heading": "5.6. Evaluation of final models for full and reduced datasets", "text": "In this section, the performance of the final models selected in chapters 5.3 and 5.4 for the complete and reduced data sets is reported. For the complete data set, the final models in Table 5.3 were trained for up to 1000 epochs. However, Table 5.6 contains a selection of test losses and accuracies of both models. The best accuracies are 99.45% and 99.34% for the oral and facial input. Complete results are available in Appendix D, for which the test accuracies in Figure 5.3.For both inputs, the training is close to the best results after 200 epochs after which the training migrates to the maximum. For the mouth and facial input, the best accuracies will be achieved after 700 and 1000 epochs. However, for the test loss, the minimum values will be achieved the minimum data."}, {"heading": "5.7. Comparison of low and high intensities for reduced dataset", "text": "This year, we have reached the point where we see ourselves in a position to put ourselves at the top, in the way that we put ourselves at the top and in the way that we put ourselves at the top, in the way that we put ourselves at the top."}, {"heading": "5.8. Classification of low and high intensities", "text": "The experiments and comparisons in chapter 5.7 yielded interesting observations on the distinction between high and low intensity smiles compared to the remaining (reduced) DISFA dataset. In this section, only low and high intensity smiles are kept for distinction. Based on chapter 3.4.1, there are 20,810 images of low or low intensity smiles and 2,749 images of high intensity for a total of 23,559 images. In this experiment, the same models from table 5.8, which were also trained for 400 epochs, are selected. Table 5.11 contains the results of this experiment. At mouth input, an accuracy of 99.82% is achieved after 200 epochs and then converged, the test loss increases slightly from then on while the test accuracy remains the same. At face input, an accuracy of 99.87% is achieved after 300 epochs. At both inputs, very high accuracies are achieved, due to the lack of general model selection and if there is only random or random input in these experiments, the results are divided into the mouth."}, {"heading": "6. Conclusions and future work", "text": "The main advantage of deep neural networks is their ability to learn complex nonlinear hypotheses without the need for explicitly modelled features, but rather to learn them from data. Convolutional networks allow to handle distortions, such as translation and rotation in input, which are common in computer vision. Applied to the action unit recognition and smile recognition in particular, a deep convolutionary neural network model with an overall accuracy of 99.45% outperforms clearly existing approaches with an accuracy of 65.55% to 79.67%. The network parameter values are subject to extensive model selection. Various variations of this experiment are performed, such as maintaining less neutral images or classification into low or high intensities."}, {"heading": "A. Statistics of all action units", "text": "This Appendix contains in Table A.1 the complete statistics of the action units of the databases taken into account in Chapter 3. - AMFED, CASME, DISFA, GEMEP, MAHNOB - Laughter and shoulder pain. - AMFED CASME DISFA GEMEP MAHNOB - Laughter shoulder PainAU1 - 1976 8778 1584 - AU2 8500 936 7364 1618 - - AU3 - - - 0 - AU4 9078 1937 24595 - 1074 AU5 5478 - 2729 735 - AU6 - 304 19484 - 5557 AU7 - 405 - 2100 - 3364 AU8 - 7 - AU9 76 731 1937 24595 - 1074 - 1074 - AU5 5478 - 2729 - AU5 - 2735 - AU6 - 304 19484 - 5557 - 5557 AU7 - 405 - 2100 - 3364 AU8 - 7 - AU9 731 - 434 - 2008 - 2729 - AU5 - 2735 - AU6 - 304 - 304 AU7 - 5557 AU7 - 5557 AU7 - 5557 AU7 - 405 - 2100 - 3364 AU7 - 3364 AU8 - 7 - 7 - AU9 764 - 1074 - AU9 - 1074 - 1074 - AU5 5478 - AU5 5478 - 2729 - AU6 - 274 - 3057 - 304 AU6 - 5557 AU7 - 5557 AU7 - 5557 AU7 - 5557 AU7 - 5557 AU7 - 405 - 405 - 2100 - 2100 - 2100 - 3364 AU8 - 3364 AU8 - 7 - 7 - 7 - AU9 764 AU8 - 7 - 7 - AU9 731 - AU9 - AU9 731 434 - 2008 - AU9 434 - 2008 423 - AU5 - AU5 - AU5 264 AU26 264 AU26 264 AU264 AU264 AU264 AU264 AU264 AU264 AU264 AU264 AU264 AU304 5264 AU264 AU264 AU264 AU304 AU264 5264 AU264 AU264 AU264 AU264 - AU264 AU264 5"}, {"heading": "B. Training time of networks", "text": "This appendix contains the complete statistics of the median epoch duration of the different convolutions models trained in Chapter 5. No significant dispersion of epoch duration was observed for any of the experiments, so only the median is available in this chapter.B.1. Full data table B.1 shows that the training time largely depends on the number of convolutions and pooling layers. Different dropout levels do not have a significant effect on the duration of an epoch.The training time is slightly shorter if the dropout for p = 0 is completely deactivated.The number of hidden units in the tested area has only a small effect on the training time, but slowly grows with the increased values. Significantly higher units per shift are likely to vary the training time.Table B.2 contains the mean epoch duration for the final models selected for oral and facial input. A comparison of training time is not possible because the selected values vary and the number of input pixels is very different."}, {"heading": "C. Result of model selection", "text": "This appendix contains the complete results of the model selection of the various shaft models trained in chapter 5.C.1. The complete dataset tables C.1 and C.2 contain the results of the mouth input model selection for 10 and 50 epochs, respectively. Tables C.3 and C.4 contain the results of the face input model selection for 10 and 50 epochs, respectively. The reduced dataset tables C.5 and C.6 contain the results of the mouth input model selection for 10 and 50 epochs, respectively. Tables C.7 and C.8 contain the results of the face input model selection for 10 and 50 epochs, respectively."}, {"heading": "D. Performance of selected models", "text": "This appendix contains the performance of the final models based on the values selected in chapter 5.D.1. The complete dataset table D.1 contains the complete collection of test losses and test accuracies for the two selected models trained to input mouth and face datas.D.2. The reduced dataset table D.2 contains the complete collection of test losses and test accuracies for the two selected models trained to input mouth and face data."}], "references": [{"title": "Spatio-Temporal Convolutional Sparse Auto-Encoder for Sequence Classification", "author": ["Moez Baccouche", "Franck Mamalet", "Christian Wolf", "Christophe Garcia", "Atilla Baskurt"], "venue": null, "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2012}, {"title": "Introducing the Geneva Multimodal Expression corpus for experimental research on emotion", "author": ["T. B\u00e4nziger", "M. Mortillaro", "K.R. Scherer"], "venue": "perception. Emotion,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2012}, {"title": "Theano: A CPU and GPU Math Expression Compiler", "author": ["J. Bergstra", "O. Breuleux", "F. Bastien", "P. Lamblin", "R. Pascanu", "G. Desjardins", "J. Turian", "D. Warde-Farley", "Y. Bengio"], "venue": "Proceedings of the Python for Scientific Computing Conference (SciPy) 2010. June", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2010}, {"title": "Pattern Recognition and Machine Learning", "author": ["Bishop", "Christopher M"], "venue": null, "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2007}, {"title": "Deep Learning Methods and Applications", "author": ["Deng", "Li", "Yu", "Dong"], "venue": "Foundations and Trends in Signal Processing,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2014}, {"title": "Facial Action Coding System: A Technique for the Measurement of Facial Movement", "author": ["P. Ekman", "W. Friesen"], "venue": null, "citeRegEx": "6", "shortCiteRegEx": "6", "year": 1978}, {"title": "The Difficulty of Training Deep Architectures and the Effect of Unsupervised Pre-Training", "author": ["Dumitru Erhan", "Pierre-Antoine Manzagol", "Yoshua Bengio", "Samy Bengio", "Pascal Vincent"], "venue": "Twelfth International Conference on Artificial Intelligence and Statistics (AISTATS)", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2009}, {"title": "Why Does Unsupervised Pre-training Help Deep Learning", "author": ["Dumitru Erhan", "Yoshua Bengio", "Aaron Courville", "Pierre-Antoine Manzagol", "Pascal Vincent", "Samy Bengio"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2010}, {"title": "The Shape Boltzmann Machine: A Strong Model of Object Shape", "author": ["Eslami", "S.M. Ali"], "venue": "International Journal of Computer Vision. Volume 107, Issue", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2014}, {"title": "Comparison of Training Methods for Deep Neural Networks", "author": ["Glauner", "Patrick"], "venue": "Imperial College London, London, UK", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2015}, {"title": "Deep Neural Networks for Computer Vision", "author": ["Glauner", "Patrick"], "venue": "Ocado, Hatfield, UK", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2015}, {"title": "Deep Sparse Rectifier", "author": ["Xavier Glorot", "Antoine Bordes", "Yoshua Bengio"], "venue": "Neural Networks. Proceedings of the Fourteenth International Conference on Artificial Intelligence and Statistics (AISTATS-11)", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2011}, {"title": "Towards end-to-end speech recognition with recurrent neural networks", "author": ["A. Graves", "N. Jaitly"], "venue": "In Proc. ICML,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2014}, {"title": "Deep Learning based FACS Action Unit Occurrence and Intensity Estimation. Vicarious Perception Technologies", "author": ["Amogh Gudi", "H. Emrah Tasli", "Tim M. den Uyl", "Andreas Maroulis"], "venue": null, "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2015}, {"title": "Deepspeech: Scaling up end-to-end speech recognition", "author": ["A. Hannun", "C. Case", "Casper", "Catanzaro", "Diamos", "Elsen", "R. Prenger", "S. Satheesh", "S. Sengupta", "A Coates"], "venue": null, "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2014}, {"title": "A Practical Guide to Training Restricted Boltzmann Machines", "author": ["Hinton", "Geoffrey"], "venue": "UTML TR 2010-003,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2010}, {"title": "Deep Neural Networks for Acoustic Modeling in Speech Recognition", "author": ["Hinton", "Geoffrey"], "venue": "IEEE Signal Processing Magazine,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2012}, {"title": "Reducing the dimensionality of data with neural", "author": ["Hinton", "Geoffrey", "R. Salakhutdinov"], "venue": "networks. Science,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2006}, {"title": "Long short-term memory", "author": ["Hochreiter", "Sepp", "Schmidhuber", "Juergen"], "venue": "Neural Computation", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 1997}, {"title": "Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift", "author": ["Ioffe", "Sergey", "Szegedy", "Christian"], "venue": null, "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2015}, {"title": "Caffe: Convolutional Architecture for Fast Feature Embedding", "author": ["Yangqing Jia", "Evan Shelhamer", "Jeff Donahue", "Sergey Karayev", "Jonathan Long", "Ross Girshick", "Sergio Guadarrama", "Trevor Darrell"], "venue": null, "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2014}, {"title": "EmoNets: Multimodal deep learning approaches for emotion recognition", "author": ["Kahou", "Samira Ebrahimi"], "venue": null, "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2015}, {"title": "Large-scale Video Classification with Convolutional Neural Networks", "author": ["Andrej Karpathy", "George Toderici", "Sanketh Shetty", "Thomas Leung", "Rahul Sukthankar", "Li Fei-Fei"], "venue": null, "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2014}, {"title": "Stacked Denoising Autoencoders: Learning Useful Representations in a Deep Network with a Local Denoising Criterion", "author": ["Hugo Larochelle", "Isabelle Lajoie", "Yoshua Bengio"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2010}, {"title": "Building high-level features using large scale unsupervised learning", "author": ["Quoc Le", "Marc\u2019Aurelio Ranzato", "Rajat Monga", "Matthieu Devin", "Kai Chen", "Greg Corrado", "Jeff Dean", "Andrew Ng"], "venue": "International Conference in Machine Learning", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2012}, {"title": "LeNet-5, convolutional neural networks. http://yann.lecun.com/exdb/lenet", "author": ["LeCun", "Yann"], "venue": "Retrieved: April", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2015}, {"title": "Painful data: The UNBC-McMaster Shoulder Pain Expression Archive Database", "author": ["P. Lucy", "J.F. Cohn", "K.M. Prkachin", "P. Solomon", "I. Matthrews"], "venue": "IEEE International Conference on Automatic Face and Gesture Recognition", "citeRegEx": "32", "shortCiteRegEx": "32", "year": 2011}, {"title": "Initialization Strategies of Spatio-Temporal Convolutional Neural Networks", "author": ["Elman Mansimov", "Nitish Srivastava", "Ruslan Salakhutdinov"], "venue": null, "citeRegEx": "33", "shortCiteRegEx": "33", "year": 2015}, {"title": "Blocks and Fuel: Frameworks for deep learning", "author": ["Bart van Merri\u00ebnboer", "Dzmitry Bahdanau", "Vincent Dumoulin", "Dmitriy Serdyuk", "David Warde-Farley", "Jan Chorowski", "Yoshua Bengio"], "venue": null, "citeRegEx": "34", "shortCiteRegEx": "34", "year": 2015}, {"title": "Disfa: A spontaneous facial action intensity database", "author": ["S.M. Mavadati", "Mahoor M.H", "K Bartlett", "P. Trinh", "J.F. Cohn"], "venue": "IEEE Transactions on Affective Computing,", "citeRegEx": "35", "shortCiteRegEx": "35", "year": 2013}, {"title": "Rosalind and Affectiva: Affectiva-MIT Facial Expression Dataset (AM-FED): Naturalistic and Spontaneous Facial Expressions Collected \u201dIn-the-Wild", "author": ["Daniel McDuff", "Rana El Kaliouby", "Thibaud Senechal", "May Amr", "Jeffrey Cohn", "Picard"], "venue": "Proceeding CVPRW \u201913 Proceedings of the 2013 IEEE Conference on Computer Vision and Pattern Recognition Workshops,", "citeRegEx": "36", "shortCiteRegEx": "36", "year": 2013}, {"title": "Machine Learning", "author": ["Mitchell", "Tom"], "venue": null, "citeRegEx": "37", "shortCiteRegEx": "37", "year": 1997}, {"title": "Rectified Linear Units Improve Restricted Boltzmann", "author": ["Nair", "Vinod", "Hinton", "Geoffrey E"], "venue": null, "citeRegEx": "38", "shortCiteRegEx": "38", "year": 2010}, {"title": "Feature selection, L1 vs. L2 regularization, and rotational invariance", "author": ["Ng", "Andrew"], "venue": null, "citeRegEx": "39", "shortCiteRegEx": "39", "year": 2004}, {"title": "Deep Learning Tutorial", "author": ["Ng", "Andrew"], "venue": "http://deeplearning.stanford.edu/tutorial/. Retrieved: February", "citeRegEx": "41", "shortCiteRegEx": "41", "year": 2015}, {"title": "Beyond Short Snippets: Deep Networks for Video Classification", "author": ["Joe Yue-Hei Ng", "Oriol Vinyals", "Matthew Hausknecht", "Rajat Monga", "Sudheendra Vijayanarasimhan", "George Toderici"], "venue": null, "citeRegEx": "42", "shortCiteRegEx": "42", "year": 2015}, {"title": "Dynamic Probabilistic CCA for Analysis of Affective Behavior and Fusion of Continuous Annotations", "author": ["Mihalis A. Nicolaou", "Vladimir Pavlovic", "Maja Pantic"], "venue": "IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE", "citeRegEx": "43", "shortCiteRegEx": "43", "year": 2014}, {"title": "Artificial Intelligence: A Modern Approach", "author": ["Norvig", "Peter", "Russel", "Stuart"], "venue": "Prentice Hall. Third Edition", "citeRegEx": "44", "shortCiteRegEx": "44", "year": 2009}, {"title": "The MAHNOB Laughter Database", "author": ["S. Petridis", "B. Martinez", "M. Pantic"], "venue": "Image and Vision Computing Journal", "citeRegEx": "47", "shortCiteRegEx": "47", "year": 2013}, {"title": "Beyond Temporal Pooling: Recurrence and Temporal Convolutions for Gesture Recognition in Video", "author": ["Lionel Pigou", "A\u00e4ron van den Oord", "Sander Dieleman", "Mieke Van Herreweghe", "Joni Dambre"], "venue": null, "citeRegEx": "48", "shortCiteRegEx": "48", "year": 2015}, {"title": "Convolutional, Long Short-Term Memory", "author": ["T.N. Sainath", "O. Vinyals", "A. Senior", "H. Sak"], "venue": "Fully Connected Deep Neural Networks. Proc. ICASSP", "citeRegEx": "49", "shortCiteRegEx": "49", "year": 2015}, {"title": "Striving for Simplicity: The All Convolutional Net", "author": ["Jost Tobias Springenberg", "Alexey Dosovitskiy", "Thomas Brox", "Martin Riedmiller"], "venue": "University of Freiburg, Germany", "citeRegEx": "52", "shortCiteRegEx": "52", "year": 2015}, {"title": "Dropout: A Simple Way to Prevent Neural Networks from Overfitting", "author": ["Nitish Srivastava", "Geoffrey Hinton", "Alex Krizhevsky", "Ilya Sutskever", "Ruslan Salakhutdinov"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "53", "shortCiteRegEx": "53", "year": 2014}, {"title": "Unsupervised Learning of Video Representations using LSTMs", "author": ["Nitish Srivastava", "Elman Mansimov", "Ruslan Salakhutdinov"], "venue": "University of Toronto", "citeRegEx": "54", "shortCiteRegEx": "54", "year": 2015}, {"title": "Challenges in Representation Learning: Facial Expression Recognition Challenge Implementation", "author": ["Y. Tang"], "venue": "University of Toronto", "citeRegEx": "55", "shortCiteRegEx": "55", "year": 2013}, {"title": "Recent Research Topics", "author": ["Todorovic", "Sinisa"], "venue": "Oregon State University. http://web.engr.oregonstate.edu/~sinisa/. Retrieved: August", "citeRegEx": "57", "shortCiteRegEx": "57", "year": 2015}], "referenceMentions": [{"referenceID": 3, "context": "Neural network with two input and output units and one hidden layer with two units and bias units x0 and z0 [4].", "startOffset": 108, "endOffset": 111}, {"referenceID": 18, "context": "LSTM cell: the integral sign stands for the Sigmoid function, the large filled dot for a multiplication [21].", "startOffset": 104, "endOffset": 108}, {"referenceID": 18, "context": "Example LSTM network: eight input units, four output units, and two memory cell blocks of size two [21].", "startOffset": 99, "endOffset": 103}, {"referenceID": 3, "context": "Illustration of a convolutional neural network [4].", "startOffset": 47, "endOffset": 50}, {"referenceID": 22, "context": "Multiple convolutions to process video input [27].", "startOffset": 45, "endOffset": 49}, {"referenceID": 40, "context": "Deep neural network composed of convolutions, LSTMs, dimensionality reduction and regular layers [49].", "startOffset": 97, "endOffset": 101}, {"referenceID": 22, "context": "Fusion of low-resolution with higher-resolution of the center of the video [27].", "startOffset": 75, "endOffset": 79}, {"referenceID": 35, "context": "Fusion of low-resolution with optical flow [42].", "startOffset": 43, "endOffset": 47}, {"referenceID": 21, "context": "Final stage done by SVM instead of neural network [26].", "startOffset": 50, "endOffset": 54}, {"referenceID": 29, "context": "Sample images of the DISFA database [35].", "startOffset": 36, "endOffset": 40}, {"referenceID": 29, "context": "Sample image of aligned DISFA database of size 285\u00d7 378 pixels [35].", "startOffset": 63, "endOffset": 67}, {"referenceID": 29, "context": "Different input parts: a) mouth, b) face [35].", "startOffset": 41, "endOffset": 45}, {"referenceID": 29, "context": "DISFA examples of video 002 for no smile, low intensity smile and high intensity smile [35].", "startOffset": 87, "endOffset": 91}, {"referenceID": 29, "context": "DISFA examples of video 005 for no smile, low intensity smile and high intensity smile [35].", "startOffset": 87, "endOffset": 91}, {"referenceID": 29, "context": "DISFA examples of video 023 for no smile, low intensity smile and high intensity smile [35].", "startOffset": 87, "endOffset": 91}, {"referenceID": 9, "context": "Concretely, previous work of the author on deep learning for facial expression recognition in [12] resulted in a deep neural network model that significantly outperformed the best contribution to the 2013 Kaggle facial expression competition [25].", "startOffset": 94, "endOffset": 98}, {"referenceID": 13, "context": "Only very few works on this topic have been reported so far, such as in [16].", "startOffset": 72, "endOffset": 76}, {"referenceID": 9, "context": "This chapter provides an overview of different types of neural networks, their capabilities and training challenges, based on [12].", "startOffset": 126, "endOffset": 130}, {"referenceID": 3, "context": "This chapter does not provide an introduction to neural networks, the reader is therefore referred to [4] and [37] for a comprehensive introduction to neural neural networks.", "startOffset": 102, "endOffset": 105}, {"referenceID": 31, "context": "This chapter does not provide an introduction to neural networks, the reader is therefore referred to [4] and [37] for a comprehensive introduction to neural neural networks.", "startOffset": 110, "endOffset": 114}, {"referenceID": 3, "context": ": Neural network with two input and output units and one hidden layer with two units and bias units x0 and z0 [4].", "startOffset": 110, "endOffset": 113}, {"referenceID": 31, "context": "There are different cost functions, such as the least squares or cross-entropy cost function, described in [37].", "startOffset": 107, "endOffset": 111}, {"referenceID": 33, "context": "described in the literature, including L1/L2 regularization [39], early stopping, tangent propagation [4] and dropout [53].", "startOffset": 60, "endOffset": 64}, {"referenceID": 3, "context": "described in the literature, including L1/L2 regularization [39], early stopping, tangent propagation [4] and dropout [53].", "startOffset": 102, "endOffset": 105}, {"referenceID": 42, "context": "described in the literature, including L1/L2 regularization [39], early stopping, tangent propagation [4] and dropout [53].", "startOffset": 118, "endOffset": 122}, {"referenceID": 24, "context": "2 for the Google Brain [29].", "startOffset": 23, "endOffset": 27}, {"referenceID": 6, "context": "As described in [7] and [8], backpropagation does not scale to deep neural networks: starting with small random initial weights, the backpropagated partial derivatives go towards zero.", "startOffset": 16, "endOffset": 19}, {"referenceID": 7, "context": "As described in [7] and [8], backpropagation does not scale to deep neural networks: starting with small random initial weights, the backpropagated partial derivatives go towards zero.", "startOffset": 24, "endOffset": 27}, {"referenceID": 15, "context": "Most prominently, unsupervised methods, such as Restricted Boltzmann Machines (RBM) in [18] and [20] or autoencoders in [41] and [5] are used.", "startOffset": 87, "endOffset": 91}, {"referenceID": 17, "context": "Most prominently, unsupervised methods, such as Restricted Boltzmann Machines (RBM) in [18] and [20] or autoencoders in [41] and [5] are used.", "startOffset": 96, "endOffset": 100}, {"referenceID": 34, "context": "Most prominently, unsupervised methods, such as Restricted Boltzmann Machines (RBM) in [18] and [20] or autoencoders in [41] and [5] are used.", "startOffset": 120, "endOffset": 124}, {"referenceID": 4, "context": "Most prominently, unsupervised methods, such as Restricted Boltzmann Machines (RBM) in [18] and [20] or autoencoders in [41] and [5] are used.", "startOffset": 129, "endOffset": 132}, {"referenceID": 23, "context": "In addition, there are denoising autoencoders defined in [28], which are autoencoders that are trained to denoise corrupted inputs.", "startOffset": 57, "endOffset": 61}, {"referenceID": 16, "context": "Furthermore, other methods such as discriminative pre-training [19] or reduction of internal covariance shift [22] have been reported as effective training methods for deep neural networks.", "startOffset": 63, "endOffset": 67}, {"referenceID": 19, "context": "Furthermore, other methods such as discriminative pre-training [19] or reduction of internal covariance shift [22] have been reported as effective training methods for deep neural networks.", "startOffset": 110, "endOffset": 114}, {"referenceID": 37, "context": "As described by Norvig in [44], the output of a set unit is much stronger than the others.", "startOffset": 26, "endOffset": 30}, {"referenceID": 32, "context": "Recently, the so-called rectified linear unit (ReLU) has been proposed in [38], which has been used successfully in many deep learning applications.", "startOffset": 74, "endOffset": 78}, {"referenceID": 32, "context": "ReLU has a number of advantages over Sigmoid, reported in [38] and [14].", "startOffset": 58, "endOffset": 62}, {"referenceID": 11, "context": "ReLU has a number of advantages over Sigmoid, reported in [38] and [14].", "startOffset": 67, "endOffset": 71}, {"referenceID": 9, "context": "Application to facial expression data In the context of this project, deep neural networks have been successfully applied to facial expression recognition in [12].", "startOffset": 158, "endOffset": 162}, {"referenceID": 44, "context": "This challenge was won by a neural network presented in [55], which achieved an error rate of 52.", "startOffset": 56, "endOffset": 60}, {"referenceID": 9, "context": "In [12], a stacked autoencoder was trained with an error of 39.", "startOffset": 3, "endOffset": 7}, {"referenceID": 10, "context": "In a subsequent project, this error could be reduced further to 28% with a stacked denoising autoencoder [13].", "startOffset": 105, "endOffset": 109}, {"referenceID": 18, "context": "Long short-term memory A long short-term memory (LSTM) defined in [21] is a modular recurrent neural network composed of LSTM cells.", "startOffset": 66, "endOffset": 70}, {"referenceID": 18, "context": ": LSTM cell: the integral sign stands for the Sigmoid function, the large filled dot for a multiplication [21].", "startOffset": 106, "endOffset": 110}, {"referenceID": 18, "context": ": Example LSTM network: eight input units, four output units, and two memory cell blocks of size two [21].", "startOffset": 101, "endOffset": 105}, {"referenceID": 43, "context": "LSTMs have also been reported in [54] to perform well on prediction of image sequences.", "startOffset": 33, "endOffset": 37}, {"referenceID": 3, "context": "Tangent propagation [4] is one method in neural networks to handle transformations by penalizing the amount of distortion in the cost function.", "startOffset": 20, "endOffset": 23}, {"referenceID": 25, "context": "CNNs were initially proposed by LeCun in [30].", "startOffset": 41, "endOffset": 45}, {"referenceID": 3, "context": "7 for a layer of convolutional units, followed by a sub-sampling layer, as described in [4].", "startOffset": 88, "endOffset": 91}, {"referenceID": 3, "context": ": Illustration of a convolutional neural network [4].", "startOffset": 49, "endOffset": 52}, {"referenceID": 22, "context": "Recently, CNNs have been reported to work well on processing of image sequences, for example in [27] for multiple convolutions, as visualized in Figure 2.", "startOffset": 96, "endOffset": 100}, {"referenceID": 22, "context": ": Multiple convolutions to process video input [27].", "startOffset": 47, "endOffset": 51}, {"referenceID": 27, "context": "An extensive study and comparison of different initialization methods is provided in [33].", "startOffset": 85, "endOffset": 89}, {"referenceID": 40, "context": "[49] describes a deep architecture composed of convolutions, LSTMs and regular layers for a NLP problem.", "startOffset": 0, "endOffset": 4}, {"referenceID": 22, "context": "For example, a reported architecture in [27] fuses a low-resolution version of the input with a higher-resolution input of the center of the video.", "startOffset": 40, "endOffset": 44}, {"referenceID": 35, "context": "Conversely, [42] fuses a low-resolution version of the input with the optical flow, as visualized in Figure 2.", "startOffset": 12, "endOffset": 16}, {"referenceID": 21, "context": "This is described in [26] and visualized in Figure 2.", "startOffset": 21, "endOffset": 25}, {"referenceID": 40, "context": ": Deep neural network composed of convolutions, LSTMs, dimensionality reduction and regular layers [49].", "startOffset": 99, "endOffset": 103}, {"referenceID": 0, "context": "scribed in [1].", "startOffset": 11, "endOffset": 14}, {"referenceID": 22, "context": ": Fusion of low-resolution with higher-resolution of the center of the video [27].", "startOffset": 77, "endOffset": 81}, {"referenceID": 35, "context": ": Fusion of low-resolution with optical flow [42].", "startOffset": 45, "endOffset": 49}, {"referenceID": 21, "context": ": Final stage done by SVM instead of neural network [26].", "startOffset": 52, "endOffset": 56}, {"referenceID": 5, "context": "Friesen in 1978 [6].", "startOffset": 16, "endOffset": 19}, {"referenceID": 30, "context": "The Affectiva-MIT Facial Expression Dataset (AMFED) [36] contains 242 facial videos (168,359 frames), which were recorded in the wild (real world conditions).", "startOffset": 52, "endOffset": 56}, {"referenceID": 29, "context": "The Denver Intensity of Spontaneous Facial Action (DISFA) [35] database contains videos of 15 male and 12 female subjects of different ethnicities.", "startOffset": 58, "endOffset": 62}, {"referenceID": 1, "context": "The Geneva Multimodal Emotion Portrayals (GEMEP) [2] contains audio and video recordings of 10 actors which portray 18 affective states.", "startOffset": 49, "endOffset": 52}, {"referenceID": 38, "context": "The MAHNOB Laughter [47] database contains 22 subjects recorded using a video camera, a thermal camera and two microphones.", "startOffset": 20, "endOffset": 24}, {"referenceID": 26, "context": "The UNBC-McMaster Shoulder Pain Expression Archive Database [32] contains 200 video sequences of participants that were suffering from shoulder pain and their corresponding spontaneous facial expressions.", "startOffset": 60, "endOffset": 64}, {"referenceID": 29, "context": ": Sample images of the DISFA database [35].", "startOffset": 38, "endOffset": 42}, {"referenceID": 30, "context": "Due to different terminology, the AMFED database does not use AU12, but a feature called \u201dsmile\u201d as explained in [36].", "startOffset": 113, "endOffset": 117}, {"referenceID": 29, "context": ": Sample image of aligned DISFA database of size 285\u00d7 378 pixels [35].", "startOffset": 65, "endOffset": 69}, {"referenceID": 29, "context": "In the original paper on DISFA [35], multi-class SVMs were trained for the different levels 0-5 of action unit intensity.", "startOffset": 31, "endOffset": 35}, {"referenceID": 45, "context": "5, an initial model has been defined and refined after discussions with other experts, including Sinisa Todorovic [57].", "startOffset": 114, "endOffset": 118}, {"referenceID": 35, "context": "the feature extraction was trained first and used to train the temporal part [42] [48].", "startOffset": 77, "endOffset": 81}, {"referenceID": 39, "context": "the feature extraction was trained first and used to train the temporal part [42] [48].", "startOffset": 82, "endOffset": 86}, {"referenceID": 39, "context": "In contrast, other models that were trained end-to-end are described in the literature, too [48] [15] [17].", "startOffset": 92, "endOffset": 96}, {"referenceID": 12, "context": "In contrast, other models that were trained end-to-end are described in the literature, too [48] [15] [17].", "startOffset": 97, "endOffset": 101}, {"referenceID": 14, "context": "In contrast, other models that were trained end-to-end are described in the literature, too [48] [15] [17].", "startOffset": 102, "endOffset": 106}, {"referenceID": 9, "context": "Selection of deep learning library In [12], the MATLAB Deep Learning Toolbox [46] has been proven to be easy and quick to use for deep learning experiments.", "startOffset": 38, "endOffset": 42}, {"referenceID": 2, "context": "The two main libraries in this domain are Theano [3] and Caffe [24].", "startOffset": 49, "endOffset": 52}, {"referenceID": 20, "context": "The two main libraries in this domain are Theano [3] and Caffe [24].", "startOffset": 63, "endOffset": 67}, {"referenceID": 41, "context": "Another question is whether to use pooling at all, as good results without pooling were reported in [52].", "startOffset": 100, "endOffset": 104}, {"referenceID": 9, "context": "2 and the remaining parameters are described in [12].", "startOffset": 48, "endOffset": 52}, {"referenceID": 35, "context": "Subsequent pooling is for areas of 2\u00d7 2 pixels and only max pooling is used, as the concrete type of pooling is reported to be less relevant in the literature [42].", "startOffset": 159, "endOffset": 163}, {"referenceID": 41, "context": "Convolution-pooling pairs are used throughout the experiments, no single convolutions not followed by pooling [52] [49].", "startOffset": 110, "endOffset": 114}, {"referenceID": 40, "context": "Convolution-pooling pairs are used throughout the experiments, no single convolutions not followed by pooling [52] [49].", "startOffset": 115, "endOffset": 119}, {"referenceID": 29, "context": ": Different input parts: a) mouth, b) face [35].", "startOffset": 43, "endOffset": 47}, {"referenceID": 25, "context": "MNIST [30] is a commonly used toy problem in many deep learning publications.", "startOffset": 6, "endOffset": 10}, {"referenceID": 29, "context": ": DISFA examples of video 002 for no smile, low intensity smile and high intensity smile [35].", "startOffset": 89, "endOffset": 93}, {"referenceID": 29, "context": ": DISFA examples of video 005 for no smile, low intensity smile and high intensity smile [35].", "startOffset": 89, "endOffset": 93}, {"referenceID": 29, "context": ": DISFA examples of video 023 for no smile, low intensity smile and high intensity smile [35].", "startOffset": 89, "endOffset": 93}, {"referenceID": 8, "context": "The CNNs can then be combined using a Shape Boltzmann Machine [9].", "startOffset": 62, "endOffset": 65}, {"referenceID": 36, "context": "In a next step, it can be adopted to regression of action unit intensities or even valence-arousal [43].", "startOffset": 99, "endOffset": 103}], "year": 2015, "abstractText": "This thesis describes the design and implementation of a smile detector based on deep convolutional neural networks. It starts with a summary of neural networks, the difficulties of training them and new training methods, such as Restricted Boltzmann Machines or autoencoders. It then provides a literature review of convolutional neural networks and recurrent neural networks. In order to select databases for smile recognition, comprehensive statistics of databases popular in the field of facial expression recognition were generated and are summarized in this thesis. It then proposes a model for smile detection, of which the main part is implemented. The experimental results are discussed in this thesis and justified based on a comprehensive model selection performed. All experiments were run on a Tesla K40c GPU benefiting from a speedup of up to factor 10 over the computations on a CPU. A smile detection test accuracy of 99.45% is achieved for the Denver Intensity of Spontaneous Facial Action (DISFA) database, significantly outperforming existing approaches with accuracies ranging from 65.55% to 79.67%. This experiment is re-run under various variations, such as retaining less neutral images or only the low or high intensities, of which the results are extensively compared.", "creator": "LaTeX with hyperref package"}}}