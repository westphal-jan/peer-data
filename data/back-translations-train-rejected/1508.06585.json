{"id": "1508.06585", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "26-Aug-2015", "title": "Towards universal neural nets: Gibbs machines and ACE", "abstract": "We study a class of neural nets - Gibbs machines - which are a type of variational auto-encoders, designed for gradual learning. They offer an universal platform for incrementally adding newly learned features, including physical symmetries in space/time. Combining them with classifiers gives rise to a brand of universal generative neural nets - stochastic auto-classifier-encoders (ACE). ACE preserve the non-Gaussian and clustering nature of real-life data and have state-of-the-art performance, both for classification and density estimation for the MNIST data set.", "histories": [["v1", "Wed, 26 Aug 2015 17:43:08 GMT  (658kb,D)", "http://arxiv.org/abs/1508.06585v1", null], ["v2", "Sat, 5 Sep 2015 21:49:06 GMT  (658kb,D)", "http://arxiv.org/abs/1508.06585v2", null], ["v3", "Tue, 10 Nov 2015 03:35:59 GMT  (658kb,D)", "http://arxiv.org/abs/1508.06585v3", null], ["v4", "Fri, 8 Apr 2016 22:11:23 GMT  (658kb,D)", "http://arxiv.org/abs/1508.06585v4", null], ["v5", "Thu, 30 Jun 2016 06:26:34 GMT  (651kb,D)", "http://arxiv.org/abs/1508.06585v5", "v5: added thermodynamic identities and variational error estimation; expanded references"]], "reviews": [], "SUBJECTS": "cs.CV cs.LG cs.NE", "authors": ["galin georgiev"], "accepted": false, "id": "1508.06585"}, "pdf": {"name": "1508.06585.pdf", "metadata": {"source": "META", "title": "Towards universal neural nets: Gibbs machines and ACE", "authors": ["Galin Georgiev"], "emails": ["GALIN.GEORGIEV@GAMMADYNAMICS.COM"], "sections": [{"heading": "1. Introduction.", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "1.1. Universality.", "text": "We are bucking the recent trend of building highly specialized neural networks by exploring networks that perform multiple tasks without compromising performance; a universal network can provisionally be described as one that, among other things: i) works for a variety of applications, i.e. visual recognition / reconstruction, speech recognition / reconstruction, natural language processing, etc.; ii) performs various tasks: classification, generation, probability density estimation, etc.; iii) is self-contained, i.e. does not employ specialized external methods of machine learning; iv) is biologically plausible."}, {"heading": "1.2. Probabilistic and quantum viewpoint.", "text": "The input of a neural network is typically a series of P-observations {x\u00b5} P\u00b5 = 1, which can be represented mathematically as series vectors in the space spanned by observables {xi} Ni = 1, e.g. the N-pixels on a screen. The network is then asked to perform classification, estimation, generation, etc. on it. In generative networks, this is done by randomly generating L-latent observations {z (\u0445) \u00b5} L.1 for each observation x\u00b5. This induced \"uncertainty\" of the \u00b5 state is modelled by a conditional density model p (z | x\u00b5).It is the copy cat, in imaginary time / space, the (squared) wave function of quantum mechanics 1, and completely describes the \u00b5-conditional state (x\u00b5, {z (\u043c) \u00b5} -density z).In statistical mechanics, the latent flux-macrocopic factors are considered as absent (the) physical ones (the)."}, {"heading": "1.3. Equilibrium setting. Gibbs machines.", "text": "Einstein developed the theory of equilibrium, i.e. small fluctuations in the early 20th century (Einstein, 2006), using an exponential model of the density pExp () in space and deriving from it Brownian diffusion, i.e. the Gaussian model density pG () in space and time. They are special cases of a broad class of densities - Gibbs or exponential densities - that form the basis of classical statistical mechanics. Gibbs densities are varying maximum entropy densities and thus optimal for modeling equilibrium. We argue in sub-sections 2.3, 3.2 that Gibbs densities are also optimal for modeling full-generative equilibrium networks, and call the networks they use Gibbs machines. They were inspired by the first fully generative networks - the variable auto encoders (VAE) (Kingma & Welling, 2014), (Recessions, 2014), because they are equal-equilibrium units (3.4)."}, {"heading": "1.4. The curse of Gaussianization.", "text": "In contrast to physics and Brownian particles, human data are clearly not in equilibrium in nature and exhibit large fluctuations and non-Gaussian behavior. Unfortunately, some of the key features of modern neural networks, such as nonlinear activation functions and failures (Srivastava et al., 2014), come with the high price of gaussianization of the data set.Quantifying non-Gaussianity and \"distance\" from equilibrium is not easy when dealing with a large number of observable N and observations P. Fortunately, there is a one-dimensional proxy for the non-Gaussianity of a multidimensional dataset: the non-Gaussianity of negative Gaussian protocol probabilities {\u2212 log pG (z\u00b5)} \u00b5 (log pG (z\u00b5) = 12z\u00b5C (z) \u2212 1z\u00b5 + const, with a multivariate Gaussian nlat (0, C (z), empical Q (Q)."}, {"heading": "1.4.1. NON-GENERATIVE ACE.", "text": "These are precisely the entanglements that - due to their extreme non-gaussianity, see Figure 2 - are ideal candidates for \"feature vectors\" in classification tasks (Hyvarinen et al., 2009), Section 7.9, 7.10. Their conjugates are then the \"receptive fields\" or \"feature detectors\" 3 - see open2If the latent observations {z\u00b5} P\u00b5 = 1 originate from a Nlatdimensional Gaussian distribution, the density of these negative Gaussian log liquefies is proportional to the familiar F (Nlat, P \u2212 Nlat) density (Mardia et al., 1979), Sections 1.8, 3.5. For the typical case P \u2212 Nlat \u2192 Naussi, it is proportional to the negative Gaussian log liquefaction (), which in turn is proportional to a rescaled Gaussian N (0, 1), such as Nlat \u2192 Eure."}, {"heading": "1.4.2. NON-GAUSSIAN DENSITIES.", "text": "A universal mesh will therefore tend to function better if the dimension of the latent layers Nlat \u2265 N, i.e. the so-called supercomplete representation (Coates et al., 2011). If Nlat > > N, for each given N-dimensional observation x\u00b5, only differs significantly from zero by a small number of latents {z\u00b5j} Nlatj = 1, as in the left diagram of Figure 2. For these sparse representations, the sample of highly entropic Gaussian densities, as in the right diagram of Figure 2, is faulty. Sampling instead of \"fat-tail\" densities provides a significant performance improvement for MNIST, Figure 8, right. As in mathematical finance, stochastic volatility and jumps are probably the first natural source of non-causality and almost completely tractable."}, {"heading": "1.4.3. GENERATIVE ACE.", "text": "An even bigger problem for current networks is the spontaneous \"clumping\" or clustering that prevails in real datasets. Statistical mechanics deals with this by introducing higher hierarchical densities linked to low hierarchical densities. Clustering aggregates the sub-balances of lower hierarchy - the observations - to higher hierarchical balances - clusters, subsection 1.2. To imitate this universal phenomenon in its second, generative execution, ACE combines a classifier and a generative auto-encoder in the same space of the observable, in a kind of auto-encoder monitoring, Figure 4. ACE generalizes the classical idea of using separate decoders for separate classes (Hinton et al., 1995). In training, the conditional latent density p (z | x\u00b5) from subsection 1.2 to p (z | x\u00b5, c\u00b5) generalizes the classification, with the A or 1.1 being a fictitious classification."}, {"heading": "1.5. Symmetries in the latent manifold.", "text": "If the dimensionality Nlat of the latent ACE layer is low, the traversing of the latent dimension in a uniform manner is not even a learned layer. Figure 5 shows the dominant dimension for each of the 10 classes in the MNIST. This so-called manifold transformation by modern feed nets was driven by the contractive autoencoders (CAE) (Rifai et al., 2012). A symmetry in our context is, frankly, a one-dimensional parametric transformation that leaves the log probability unchanged (CAE) (CAE). In probabilistic terms, this is equivalent to the existence of a one-parametric density from which \"symmetrical\" observations are sampled in our context, see (1,2)."}, {"heading": "2. Theoretical background.", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "2.1. Conditional densities.", "text": "As shown in subsection 1.2, the conditional density p (z | x\u00b5) is the copy cat of the wave function (\u00b5-th conditional state) in quantum mechanics and thus central in the generative neural network formalism.For discrete data, the minimization of the negative log probability () corresponds to the minimization of the Kullback-Leibler divergence D (r | | p) = x r (x) log r (x) p (x) empirical r () and the model p () density (), da \u2212 logL (r | p) = S (r) + D (r | p), where S (r) = E (\u2212 log r) r is the entropy of r (). Latents are not a priori given but rather sampled from a closed-shaped model p (z). The optimization goal is the mixed empirical density r (x, z)."}, {"heading": "2.2. Conditional independence.", "text": "The hidden / latent observables z = {zj} Nlatj = 1 are conditionally independent if you have x\u00b5 p (z | x\u00b5) = \u0441Nlat j = 1 p (zj | x\u00b5) for a given observation. Conditional independence minimizes the negative entropy concept to the right of (2,2) from the boundary of entropy S (p (z | x\u00b5)) \u2264 \u2211 Nlat j = 1 S (p (zj | x\u00b5)), (Cover & Thomas, 2006), Chapter 2. Everything else is the same, so conditional independence is optimal for networks."}, {"heading": "2.3. Gibbs and q-Gibbs model densities.", "text": "There is a broad class of probability density families - Gibbs or exponential families - that dominate the decisions of model densities (both in physics and in neural networks), which includes a sufficiently large number of density families: Gaussian, Bernoulli, exponential, gamma, etc. Their general closed form is: p\u03bb (z) = p (z) Z e \u2212 p s = 1 \u03bbsMs (z), where p (z) is an arbitrary previous density, \u03bb = {s} are multipliers, Mj (z) are so-called sufficient statistics, and Z = p (z) dz is the normalizing partition function. Sufficient statistics in physics form a complete set of state variables such as energy, momenta, number of particles, etc., comprehensively describe the \u00b5-th conditioned state, subsection 1.2, see (Landau & Lifshitz, 1980) Sections 28,110."}, {"heading": "3. Application to neural nets.", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "3.1. Perturbative nets.", "text": "Networks that rely on decomposition (2,2) for parameter estimation do not contain the purely boundary model density p (z) and are therefore not completely generative, but only disturbing. Boltzmann machines and their multiple reincarnations have been the most successful family of disturbing nets to date. Assuming the completeness of the state variables, Boltzmann machines assume the Boltzmann density as common model density, a special case of the Gibbs equilibrium density from subrange 2.3: pB (x, z) = 1 Z e \u2212 1T E (x, z), with a single sufficient statistic - a bilinear energy function E (,), a trivial precursor and temperature T = 1. It is not traceable because the partition function Z = p (x, z) dxdz density cannot be calculated in closed form."}, {"heading": "3.2. Fully-generative nets. Gibbs machines.", "text": "In order to have a fully generative conditional density model p (z) with observations, it is necessary to select a marginal error which gives the net density p (z) and the sample directly from it, unencumbered by the observations p (z). Therefore, we will distinguish two different error regimes: - Error in closed form p = {zj} Nlatj = 1 - Error in closed form p (z) - Error in closed form p (z) - Error in closed form p (z). A closed model of density p (x) is also selected, which defines a tractable joint density p (x). - Error in closed form p (z) p (z) p (z) p (z) p (z). Unfortunately, the implicit posterior conditional density p (x) / z) p (z) dz is generally intractable. - non-creative observations are sampled by the conditional density p (z)."}, {"heading": "3.3. Latent symmetry statistics. Momenta.", "text": "We will show how to build a translational, scaling and rotational invariance only in a two-dimensional visual recognition model. Generalizations are easy to manage.Each observable, i.e. pixel xi, i = 1,..., N, can be assigned a horizontal and a vertical whole tooth coordinate on the screen (e.g., hi, vi).In these coordinates, a line observation x\u00b5 = {x\u00b5i} Ni = 1 matrix observation {x\u00b5, hi, vi} and a net layer of size N becomes a layer of size N. The center of mass N (h\u00b5, v\u00b5): h\u00b5 N = 1 x\u00b5ihi x\u00b5i, i x\u00b5i: i, v\u00b5: 1 x\u00b5ivi: i = 1 x\u00b5ivi."}, {"heading": "4. Experimental results.", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.1. Non-generative ACE.", "text": "The motivation for the non-generative ACE comes from the Einstein observational entropies {\u2212 log pG (z\u00b5)} \u00b5 in subsection 1,4 and their relationship to the Singular Value Decomposition (SVD). Remember that the SVD is the B \u00b7 N data matrix X with B observations and N-Observable X = V\u0435WT. The B \u00b7 B matrix VVT is i) a projection mapping; ii) its diagonals are up to a constant of negative Gaussian log likelihoods {\u2212 log pG (z\u00b5)} \u00b5; and iii) it is invariant to X, i.e. X = VVTX.Consider a flat auto-encoder, Figure 6, left, and its dual in the space of observations, Figure 6, right. It can be shown that for bound weights V (2) = V (1) the minifixes, in the absence of non-linearity, are the optimal solution to the HV light-5z light."}, {"heading": "4.2. Generative ACE.", "text": "The architecture is shown in Figure 4, minimizing the negative log probability of ACE (1,1), where \u2212 logLAE is replaced by the upper limit (3,4). Laplac sample density is used in training and mixed laplac in testing, using the explicit formulas for the generative error in Appendix B. Generative ACE delivers similarly excellent classification results as non-generative ACE on the regular MNIST dataset, Figure 8, left. Even without fine-tuning hyperparameters, it delivers excellent results for density estimation of the binary MNIST dataset, Figure 8, right. An upper limit for negative log probability in grip 86-87 is located in the ball park of the best non-recurring networks (Gregor et al., 2015), Table 2."}, {"heading": "5. Open problems.", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "Acknowledgments", "text": "We appreciate motivational discussions with Ivaylo Popov and Nikola Toshev. Thanks to Christine Haas for coining the terms complicated and creative / non-creative regimes."}, {"heading": "A. Software and implementation.", "text": "The ACE code used for the cited experiments is in (Georgiev, 2015a).All cited networks are implemented on a single GeForce GTX 970 GPU and on the Theano platform (Bastien et al., 2012) and tap into Theano-Lights standard network implementations (Popov, 2015).Optimizer is ADAM (Kingma & Ba, 2014) stochastic gradient descent-back propagation. Specific hyper parameters are in the text. We have used only two standard sets of hyper parameters, one for the classifier branch and one for the auto encoder branch, and have not performed any hyper-parameter optimizations."}, {"heading": "B. Generative error formulas.", "text": "To have a unit variance in the previous section, we select a previous p (z) = pLap (z; 0, \u221a 0.5) for the independent one-dimensional latents, where pLap (z; \u00b5, b) = exp (\u2212 | z \u2212 \u00b5 | / b) / (2b) is the default laplac density with medium \u00b5 and scale. To have a generative error of zero if (\u00b5, \u03c3) \u2192 (0, 1), we parameterise the conditional posterior as p (z; \u00b5, \u03c3 0.5). The generative error in (3.4) corresponds to: \u2212 log \u03c3 + | \u00b5 | / \u221a 0.5 + \u03c3exp (\u2212 \u00b5 | / (\u03c3 0.5)) \u2212 1, see (Gil et al., 2013), Table 3.For the divergence between a mixture of previous s (z) and a mixture of posterior problem (\u2212 \u00b5 | / (\u03c3 \u221a 0.5)) \u2212 1 (see Gil et al, for the previous table 3.3)."}], "references": [{"title": "Methods of Information Geometry, volume 191 of Translations of Mathematical monographs", "author": ["S. Amari", "H. Nagaoka"], "venue": null, "citeRegEx": "Amari and Nagaoka,? \\Q2000\\E", "shortCiteRegEx": "Amari and Nagaoka", "year": 2000}, {"title": "Geometry of qexponential family of probability distributions. Entropy", "author": ["Amari", "Shun-ichi", "Ohara", "Atsumi"], "venue": null, "citeRegEx": "Amari et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Amari et al\\.", "year": 2011}, {"title": "An analysis of singlelayer networks in unsupervised feature learning", "author": ["A. Coates", "H. Lee", "A.Y. Ng"], "venue": "In AISTATS,", "citeRegEx": "Coates et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Coates et al\\.", "year": 2011}, {"title": "On Boltzmanns principle and some immediate consequences thereof", "author": ["Einstein", "Albert"], "venue": "Einstein, 19052005,", "citeRegEx": "Einstein and Albert.,? \\Q2005\\E", "shortCiteRegEx": "Einstein and Albert.", "year": 2005}, {"title": "ACE, 2015a. URL https://github. com/galinngeorgiev/ACE", "author": ["Georgiev", "Galin"], "venue": null, "citeRegEx": "Georgiev and Galin.,? \\Q2015\\E", "shortCiteRegEx": "Georgiev and Galin.", "year": 2015}, {"title": "Duality in neural nets., 2015b", "author": ["Georgiev", "Galin"], "venue": null, "citeRegEx": "Georgiev and Galin.,? \\Q2015\\E", "shortCiteRegEx": "Georgiev and Galin.", "year": 2015}, {"title": "R\u00e9nyi divergence measures for commonly used univariate continuous distributions", "author": ["M Gil", "F Alajaji", "T. Linder"], "venue": "Information Sciences,", "citeRegEx": "Gil et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Gil et al\\.", "year": 2013}, {"title": "DRAW: A recurrent neural network for image generation", "author": ["Gregor", "Karol", "Danihelka", "Ivo", "Graves", "Alex", "Wierstra", "Daan"], "venue": null, "citeRegEx": "Gregor et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Gregor et al\\.", "year": 2015}, {"title": "Approximating the Kullback Leibler divergence between Gaussian mixture models", "author": ["J.R. Hershey", "P.A. Olsen"], "venue": "In ICASSP,", "citeRegEx": "Hershey and Olsen,? \\Q2007\\E", "shortCiteRegEx": "Hershey and Olsen", "year": 2007}, {"title": "Training products of experts by minimizing contrastive divergence", "author": ["Hinton", "Geoffrey E"], "venue": "Neural Computation,", "citeRegEx": "Hinton and E.,? \\Q2002\\E", "shortCiteRegEx": "Hinton and E.", "year": 2002}, {"title": "Recognizing handwritten digits using mixtures of linear models", "author": ["Hinton", "Geoffrey E", "Revow", "Michael", "Dayan", "Peter"], "venue": "In AINIPS,", "citeRegEx": "Hinton et al\\.,? \\Q1995\\E", "shortCiteRegEx": "Hinton et al\\.", "year": 1995}, {"title": "Natural Image Statistics: A probabilistic approach to early computational vision", "author": ["Hyvarinen", "Aapo", "Hurri", "Jamo", "Hoyer", "Patrick O"], "venue": null, "citeRegEx": "Hyvarinen et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Hyvarinen et al\\.", "year": 2009}, {"title": "Batch normalization: Accelerating deep network training by reducing internal covariate shift", "author": ["Ioffe", "Sergey", "Szegedy", "Christian"], "venue": "URL http://arxiv.org/", "citeRegEx": "Ioffe et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Ioffe et al\\.", "year": 2015}, {"title": "Adam: A method for stochastic optimization", "author": ["Kingma", "Diederik", "Ba", "Jimmy"], "venue": "URL http://arXiv. org/abs/1412.6980", "citeRegEx": "Kingma et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Kingma et al\\.", "year": 2014}, {"title": "Auto-encoding variational bayes", "author": ["Kingma", "Durk P", "Welling", "Max"], "venue": "In ICLR,", "citeRegEx": "Kingma et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Kingma et al\\.", "year": 2014}, {"title": "Recursive Nonlinear Estimation: A Geometric Approach. Lecture Notes in Control And Information Sciences", "author": ["R. Kulhav\u1ef3"], "venue": null, "citeRegEx": "Kulhav\u1ef3,? \\Q1996\\E", "shortCiteRegEx": "Kulhav\u1ef3", "year": 1996}, {"title": "Quantim Mechanics, Nonrelativistic theory, 3rd edition", "author": ["L.D. Landau", "E.M. Lifshitz"], "venue": null, "citeRegEx": "Landau and Lifshitz,? \\Q1977\\E", "shortCiteRegEx": "Landau and Lifshitz", "year": 1977}, {"title": "Statistical Physics, Part 1, 3rd edition", "author": ["L.D. Landau", "E.M. Lifshitz"], "venue": "Elsevier Science,", "citeRegEx": "Landau and Lifshitz,? \\Q1980\\E", "shortCiteRegEx": "Landau and Lifshitz", "year": 1980}, {"title": "Multivariate Analysis", "author": ["K.V. Mardia", "J.T. Kent", "J.M. Bibby"], "venue": null, "citeRegEx": "Mardia et al\\.,? \\Q1979\\E", "shortCiteRegEx": "Mardia et al\\.", "year": 1979}, {"title": "URL https:// github.com/Ivaylo-Popov", "author": ["Popov", "Ivaylo"], "venue": "Theano-Lights,", "citeRegEx": "Popov and Ivaylo.,? \\Q2015\\E", "shortCiteRegEx": "Popov and Ivaylo.", "year": 2015}, {"title": "Stochastic backpropagation and approximate inference in deep generative models", "author": ["Rezende", "Danilo J", "Mohamed", "Shakir", "Wierstra", "Daan"], "venue": "In JMLR,", "citeRegEx": "Rezende et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Rezende et al\\.", "year": 2014}, {"title": "A generative process for sampling contractive auto-encoders", "author": ["Rifai", "Salah", "Bengio", "Yoshua", "Dauphin", "Yann", "Vincent", "Pascal"], "venue": "In ICML,", "citeRegEx": "Rifai et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Rifai et al\\.", "year": 2012}, {"title": "Deep Boltzmann machines", "author": ["Salakhutdinov", "Ruslan", "Hinton", "Geoffrey"], "venue": "In AISTATS,", "citeRegEx": "Salakhutdinov et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Salakhutdinov et al\\.", "year": 2009}, {"title": "On the quantitative analysis of deep belief networks", "author": ["Salakhutdinov", "Ruslan", "Murray", "Iain"], "venue": "In ICML,", "citeRegEx": "Salakhutdinov et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Salakhutdinov et al\\.", "year": 2008}, {"title": "Dropout: A simple way to prevent neural networks from overfitting", "author": ["Srivastava", "Nitish", "Hinton", "Geoffrey", "Krizhevsky", "Alex", "Sutskever", "Ilya", "Salakhutdinov", "Ruslan"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Srivastava et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Srivastava et al\\.", "year": 2014}, {"title": "Introduction to Nonextensive Statistical Mechanics: Approaching a Complex World ", "author": ["Tsallis", "Constantino"], "venue": null, "citeRegEx": "Tsallis and Constantino.,? \\Q2009\\E", "shortCiteRegEx": "Tsallis and Constantino.", "year": 2009}], "referenceMentions": [{"referenceID": 20, "context": "They were inspired by the first fully-generative nets - the variational auto-encoders (VAE) (Kingma & Welling, 2014), (Rezende et al., 2014) - and employ the same upper bound (3.", "startOffset": 118, "endOffset": 140}, {"referenceID": 24, "context": "Unfortunately, some of the key features of modern neural nets, like non-linear activation functions and dropout (Srivastava et al., 2014), come at the high price of Gaussianizing the data set.", "startOffset": 112, "endOffset": 137}, {"referenceID": 11, "context": "It is precisely the intricates, which - because of their extreme non-Gaussianity, see Figure 2 - are ideal candidates for \u201cfeature vectors\u201d in classification tasks (Hyvarinen et al., 2009), section 7.", "startOffset": 164, "endOffset": 188}, {"referenceID": 18, "context": "If the latent observations {z\u03bc}\u03bc=1 come from an Nlatdimensional Gaussian distribution, the density of these negative Gaussian log-likelihoods is proportional to the familiar F (Nlat, P \u2212 Nlat) density (Mardia et al., 1979), sections 1.", "startOffset": 201, "endOffset": 222}, {"referenceID": 2, "context": "have the so-called overcomplete representation (Coates et al., 2011).", "startOffset": 47, "endOffset": 68}, {"referenceID": 10, "context": "ACE generalizes the classic idea of using separate decoders for separate classes, (Hinton et al., 1995).", "startOffset": 82, "endOffset": 103}, {"referenceID": 21, "context": "This so-called manifold learning by modern feed-forward nets was pioneered by the contractive autoencoders (CAE) (Rifai et al., 2012).", "startOffset": 113, "endOffset": 133}, {"referenceID": 15, "context": "The optimization target hence is the mixed empirical density r(x, z) = 1 P \u2211 \u03bc \u03b4(x \u2212x\u03bc)p(z|x\u03bc) (Kulhav\u1ef3, 1996), section 2.", "startOffset": 95, "endOffset": 110}, {"referenceID": 15, "context": "This follows from the variational Pythagorean theorem (Kulhav\u1ef3, 1996), section 3.", "startOffset": 54, "endOffset": 69}, {"referenceID": 24, "context": "Despite their limitations when handling non-binary data, deep Boltzmann machines (Salakhutdinov & Hinton, 2009) are arguably still the dominant universal nets: They perform well both as classifiers (Srivastava et al., 2014) and probability density estimators (Salakhutdinov & Murray, 2008).", "startOffset": 198, "endOffset": 223}, {"referenceID": 20, "context": "4) are universal for fully-generative nets and were first used in the first fully-generative nets, the VAE-s (Kingma & Welling, 2014), (Rezende et al., 2014).", "startOffset": 135, "endOffset": 157}, {"referenceID": 6, "context": "For Gaussian multi-variate sampling, this follows from the explicit form of the generative error (Gil et al., 2013), table 3, and Hadamard\u2019s inequality (Cover & Thomas, 2006), chapter 8.", "startOffset": 97, "endOffset": 115}, {"referenceID": 24, "context": "9-1% handle (Srivastava et al., 2014), table 2.", "startOffset": 12, "endOffset": 37}, {"referenceID": 7, "context": "An upper bound for the negative log-likelihood in the 86-87 handle is in the ballpark of the best non-recurrent nets, (Gregor et al., 2015), table 2.", "startOffset": 118, "endOffset": 139}, {"referenceID": 11, "context": "4, directly as feature detectors, in lieu of artificially computed Independent Component Analysis (ICA) features (Hyvarinen et al., 2009).", "startOffset": 113, "endOffset": 137}, {"referenceID": 6, "context": "5) ) \u22121, see (Gil et al., 2013), table 3.", "startOffset": 13, "endOffset": 31}], "year": 2017, "abstractText": "We study a class of neural nets Gibbs machines which are a type of variational auto-encoders, designed for gradual learning. They offer an universal platform for incrementally adding newly learned features, including physical symmetries in space/time. Combining them with classifiers gives rise to a brand of universal generative neural nets stochastic auto-classifier-encoders (ACE). ACE preserve the non-Gaussian and clustering nature of real-life data and have state-ofthe-art performance, both for classification and density estimation for the MNIST data set.", "creator": "LaTeX with hyperref package"}}}