{"id": "1508.06717", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "27-Aug-2015", "title": "Online Anomaly Detection via Class-Imbalance Learning", "abstract": "Anomaly detection is an important task in many real world applications such as fraud detection, suspicious activity detection, health care monitoring etc. In this paper, we tackle this problem from supervised learning perspective in online learning setting. We maximize well known \\emph{Gmean} metric for class-imbalance learning in online learning framework. Specifically, we show that maximizing \\emph{Gmean} is equivalent to minimizing a convex surrogate loss function and based on that we propose novel online learning algorithm for anomaly detection. We then show, by extensive experiments, that the performance of the proposed algorithm with respect to $sum$ metric is as good as a recently proposed Cost-Sensitive Online Classification(CSOC) algorithm for class-imbalance learning over various benchmarked data sets while keeping running time close to the perception algorithm. Our another conclusion is that other competitive online algorithms do not perform consistently over data sets of varying size. This shows the potential applicability of our proposed approach.", "histories": [["v1", "Thu, 27 Aug 2015 03:39:39 GMT  (1150kb,D)", "http://arxiv.org/abs/1508.06717v1", "This paper is accepted for publication in IC3 2015, Jaypee Noida"]], "COMMENTS": "This paper is accepted for publication in IC3 2015, Jaypee Noida", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["chandresh kumar maurya", "durga toshniwal", "gopalan vijendran venkoparao"], "accepted": false, "id": "1508.06717"}, "pdf": {"name": "1508.06717.pdf", "metadata": {"source": "CRF", "title": "Online Anomaly Detection via Class-Imbalance Learning", "authors": ["Chandresh Kumar Maurya", "Durga Toshniwal", "Gopalan Vijendran Venkoparao"], "emails": ["durgatoshniwal@gmail.com", "GopalanVijendran.Venkoparao@in.bosch.com"], "sections": [{"heading": null, "text": "Index Terms - Class-Imbalance Learning, Online learning, Anomaly detection. INTRODUCTIONAnomaly detection aims to capture behavior in data that does not correspond to normal behavior, as expected by domain experts [1]. Detecting anomalies in the online environment is an important task in many real-world applications. For example, detecting intruders in the computer network, flight navigation system, detecting credit card fraud and so on. It is clear that such a task requires detecting malicious activity in the online environment. However, most existing techniques focus on offline training of the model and then use it to detect anomalies [2], [3], [4], [5]. In this paper, we address anomaly problems from the monitored learning perspective in online setting. In the supervised learning framework, detection of anomalies refers to correctly comparing multiple class examples to the classification of rare examples."}, {"heading": "II. RELATED WORK", "text": "The papers presented in this paper cover two main topics in the field of data mining and machine learning: online learning and classical balance learning. Although there has been much work in both areas separately [6], [7] little work has been done to jointly solve online learning and class imbalance learning. In the following, we briefly describe the work in each area closely related to our work."}, {"heading": "A. Online learning", "text": "Online learning has its origin in Rosenblatt's classic work on the Perceptron algorithm [8]. The Perceptron algorithm is based on the idea of a single neuron. It simply takes an input instance and learns a linear predictor of the form f (w) = wTx, where w is the weight vector and x is the input instance. If it makes a wrong prediction, it updates its parameter vector as follows: wt + 1 = wt + ytxt (1), where wt + 1 is currently t + 1. [9] Proposed online learning with cores. Their algorithm, called NORMA\u03bb, is based on the regulated empirical risk factor IEV, which they solved by regulated stochastic gradient learning."}, {"heading": "B. Class-Imbalance Learning", "text": "In the literature, there are solutions based on either the idea of sensing or the weight scheme. In the first case, either majority examples are evaluated or minority examples are evaluated over-sampled. In the second case, each example is weighted differently and the idea is to learn this weighting optimally. Some examples of sample-based techniques are SMOTE [6], SMOTEBoost [7], AdaBoost.NC [11] and so on. Works that use the weighting scheme include cost-sensitive learning [12], [13], [15] and so on. It is worth mentioning here that there are few papers that jointly solve class imbalance learning and online learning. Below, we will mention some papers that closely correspond to our work."}, {"heading": "III. FRAMEWORK OF CLASS-IMBALANCE LEARNING", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "A. Problem formulation", "text": "& & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & & # 10; & & # 10; & & & # 10; & & & & # 10; & # 10; & & & # 10; & # 10; & & # 10; & & # 10; & & & # 10; & & & # 10; & & & # 10; & & & # 10; & & & # 10; & # 10; & & & # 10; & & # 10; & & # 10; & & # 10; & & # 10; & & # 10; & & # 10; & # 10; & # 10; & & # 10; & & # 10; & # 10; & # 10; & # 10; & & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & &"}, {"heading": "C. Algorithm", "text": "Our proposed algorithm Online G-MEAN (OGMEAN) is specified in algorithm 1. As we can see, OGMEAN only needs one parameter to be matched; \u03c4, the learning rate. Generally, it is set to 1 / \u221a t. However, for the sake of simplicity, we set it to a constant in our experiments. It is clear that OGMEAN takes time that is proportional to O (T \u00b7 n), which is linear in the number of dimensions of the inputs as well as the number of instances received so far."}, {"heading": "D. Relative Loss Bound for OGMEAN", "text": "Following [17], we can limit the regrets of the OGMEAN algorithm. In the following, we simply present the problem without proof. Lemma 2: Let S = {(xt, yt)} t = 1,..., T be the sequence of T examples in which xt-X, yt-Y and | | xt-Y and | | \u2264 1 apply to all. Then, for each w-X variable by setting \u03c4 = | w-T for OGMEAN, the following applies: T-T = 1 Lt (wt) \u2264 T-T = 1 Lt (w) + | w-T."}, {"heading": "IV. EXPERIMENTS", "text": "In this section, we have experimentally validated the accuracy of the proposed OGMEAN algorithm via various benchmark data sets, which can be freely downloaded from the LIBSVM website2. A brief summary of the data set and class imbalance ratio is given in Table I. All algorithms were executed in MATLAB 2012a (64-bit version) on 64-bit Windows 8.1 machine. We compare our algorithm with the recently proposed CSOCsum algorithm [17] in relation to a metric called sum, which is defined as follows: sum = np \u00d7 sensitivity + nn \u00d7 specificity (13), np and nn being weight parameters set manually to 0.5 in [17] each, and sensitivity and specificity are the same as recollection of positive and negative examples. [17] states that the CSOCsum algorithm suggests the state of online class imbalances in relation to the class imbalances."}, {"heading": "A. Evaluation of Weighted Sum Performance", "text": "We performed a comparative study on CSOCsum and OGMEAN algorithms, in which we set the learning rate parameters \u03c4 to 0.2 and the weights for \"sum\" i.e. np and nn to 0.5 for all data sets and both algorithms as in CSOCsum. Online \"sum performance\" of the two algorithms is represented in?????, 2 and Table II over four data sets: \"German,\" \"ijcnn1,\" \"svmguide3\" and \"covtype.\" Based on these results, we can first see that OGMEAN achieves the same or higher \"sum value\" compared to CSOCsum across all data sets."}, {"heading": "B. Comparative Study", "text": "We conducted another experiment to compare the error rate, the cumulative number of updates, and the cumulative time cost of the covtype and german3 datasets. For all algorithms compared, the cost parameter C was set to 1, except ALMA, for which the value of C \u221a 2 is. We kept the values of all other parameters as specified in the LIBOL implementation. From??? we observed that both CSOCsum and OGMEAN did not perform well in terms of the number of errors compared to SCW-I, SCW-II, NHERD, and ALMA. On the other hand, we found that both CSOCsum and OGMEAN did not perform well in terms of the number of errors compared to SCW-I, SCW-II, NHERD, and ALMA."}, {"heading": "V. CONCLUSION", "text": "We have demonstrated that maximizing the Gmean equivalent formulation is not convex and therefore uses convex replacement loss function in empirical risk minimization. We have compared our performance of the OGMEAN algorithm with the recently proposed CSOCsum algorithm across various custom datasets. It has been shown that the direct optimization of the weighted sum of sensitivity and specificity, where weights are learned using laplace estimation techniques, is less efficient than the direct optimization of the equivalent formulation to maximize Gmean. We have also shown error rates, cumulative time costs, and the number of updates to our algorithm with respect to many other online learning algorithms, and have concluded that its performance is as good or better than these recent online algorithms. In our future work we plan to expand the work to multi-class setting. Specifically: How can we optimize the online mean class for this multiclass problem not be required?"}, {"heading": "ACKNOWLEDGMENT", "text": "This research is supported by the Prime Minister's Fellowship for Doctoral Research, a joint initiative of the Confederation of Indian Industry (CII) and industry partner Robert Bosch."}], "references": [{"title": "Anomaly detection: A survey,", "author": ["B.A.V. Chandola", "V. kumar"], "venue": "University of Minnesota, Tech. Rep.,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2009}, {"title": "Lof: Identifying density-based local outliers,", "author": ["M. Breunig", "H.P. Kriegel", "R.T. Ng", "J. Sander"], "venue": "Proceedings of the 2000 ACM SIGMOD international conference on management of data", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2000}, {"title": "Toward supervised anomaly detection.", "author": ["N. Goernitz", "M. Kloft", "K. Rieck", "U. Brefeld"], "venue": "J. Artif. Intell. Res. (JAIR),", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2013}, {"title": "Smote: Synthetic minority over-sampling technique,", "author": ["N.V. Chawla", "K.W. Bowyer", "L.O. Hall", "W.P. Kegelmeyer"], "venue": "J. Artif. Int. Res.,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2002}, {"title": "Smoteboost: Improving prediction of the minority class in boosting.", "author": ["N.V. Chawla", "A. Lazarevic", "L.O. Hall", "K.W. Bowyer"], "venue": "PKDD, ser. Lecture Notes in Computer Science,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2003}, {"title": "Neurocomputing: Foundations of research,", "author": ["F. Rosenblatt"], "venue": "Psychological Review, Nov 1958,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 1958}, {"title": "Online learning with kernels,", "author": ["J. Kivinen", "A. Smola", "R. Williamson"], "venue": "IEEE Transactions on Signal Processing,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2010}, {"title": "Online passive-aggressive algorithms,", "author": ["K. Crammer", "O. Dekel", "J. Keshet", "S. Shalev-Shwartz", "Y. Singer"], "venue": "J. Mach. Learn. Res.,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2006}, {"title": "Negative correlation learning for classification ensembles,", "author": ["S. Wang", "H. Chen", "X. Yao"], "venue": "Neural Networks (IJCNN), The 2010 International Joint Conference on,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2010}, {"title": "The foundations of cost-sensitive learning,", "author": ["C. Elkan"], "venue": "Proceedings of the 17th International Joint Conference on Artificial Intelligence - Volume 2, ser. IJCAI\u201901", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2001}, {"title": "Test strategies for cost-sensitive decision trees,", "author": ["C. Ling", "V. Sheng", "Q. Yang"], "venue": "IEEE Transactions on Knowledge and Data Engineering,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2006}, {"title": "The influence of class imbalance on costsensitive learning: An empirical study,\u201d in ICDM \u201906", "author": ["X.-Y. Liu", "Z.-H. Zhou"], "venue": "Sixth International Conference on Data Mining,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2006}, {"title": "Cost-sensitive multilabel learning for audio tag annotation and retrieval,", "author": ["H.-Y. Lo", "J.-C. Wang", "H.-M. Wang", "S.-D. Lin"], "venue": "Multimedia, IEEE Transactions on,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2011}, {"title": "Online class imbalance learning and its applications in fault detection,", "author": ["S. Wang", "L.L. Minku", "X. Yao"], "venue": "International Journal of Computational Intelligence and Applications,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2013}, {"title": "Cost-sensitive online classification,", "author": ["J. Wang", "P. Zhao", "S. Hoi"], "venue": "IEEE Transactions on Knowledge and Data Engineering,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2014}, {"title": "Addressing the curse of imbalanced training sets: One-sided selection,", "author": ["M. Kubat", "S. Matwin"], "venue": "Proceedings of the Fourteenth International Conference on Machine Learning", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 1997}, {"title": "Online convex programming and generalized infinitesimal gradient ascent,", "author": ["M. Zinkevich"], "venue": "Machine Learning, Proceedings of the Twentieth International Conference (ICML", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2003}, {"title": "The perceptron algorithm with uneven margins,", "author": ["H. Li", "H. Zaragoza", "R. Herbrich", "J. Shawe-Taylor", "J. Kandola"], "venue": "Proceedings of the Nineteenth International Conference on Machine Learning, ser. ICML \u201902,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2002}], "referenceMentions": [{"referenceID": 0, "context": "Anomaly detection aims to capture behavior in data that do not conform to the normal behavior as expected by domain expert [1].", "startOffset": 123, "endOffset": 126}, {"referenceID": 1, "context": "However, most existing techniques focus on offline training of the model and then use it to detect anomalies [2], [3], [4], [5].", "startOffset": 109, "endOffset": 112}, {"referenceID": 2, "context": "However, most existing techniques focus on offline training of the model and then use it to detect anomalies [2], [3], [4], [5].", "startOffset": 114, "endOffset": 117}, {"referenceID": 3, "context": "Although there have been many works in both domain separately [6], [7], little work has been done that jointly solves online learning and class-imbalance learning.", "startOffset": 62, "endOffset": 65}, {"referenceID": 4, "context": "Although there have been many works in both domain separately [6], [7], little work has been done that jointly solves online learning and class-imbalance learning.", "startOffset": 67, "endOffset": 70}, {"referenceID": 5, "context": "Online learning has its origin from classic work of Rosenblatt on perceptron algorithm [8].", "startOffset": 87, "endOffset": 90}, {"referenceID": 6, "context": "[9] proposed online learning with kernels.", "startOffset": 0, "endOffset": 3}, {"referenceID": 7, "context": "Passive-Aggressive (PA) learning [10] is another online learning algorithm based on the idea of maximizing \u201cmargin\u201d in online learning framework.", "startOffset": 33, "endOffset": 37}, {"referenceID": 3, "context": "Some examples of sampling based techniques are SMOTE [6], SMOTEBoost [7], AdaBoost.", "startOffset": 53, "endOffset": 56}, {"referenceID": 4, "context": "Some examples of sampling based techniques are SMOTE [6], SMOTEBoost [7], AdaBoost.", "startOffset": 69, "endOffset": 72}, {"referenceID": 8, "context": "NC [11] and so on.", "startOffset": 3, "endOffset": 7}, {"referenceID": 9, "context": "Work that used weighting scheme include cost-sensitive learning [12], [13], [14], [15] and so on.", "startOffset": 64, "endOffset": 68}, {"referenceID": 10, "context": "Work that used weighting scheme include cost-sensitive learning [12], [13], [14], [15] and so on.", "startOffset": 70, "endOffset": 74}, {"referenceID": 11, "context": "Work that used weighting scheme include cost-sensitive learning [12], [13], [14], [15] and so on.", "startOffset": 76, "endOffset": 80}, {"referenceID": 12, "context": "Work that used weighting scheme include cost-sensitive learning [12], [13], [14], [15] and so on.", "startOffset": 82, "endOffset": 86}, {"referenceID": 13, "context": "In [16], author proposed sampling with online bagging (SOB) for class imbalance detection.", "startOffset": 3, "endOffset": 7}, {"referenceID": 13, "context": "[16] also tries to maximize Gmean, but the way they approached to solve the maximization problem is different from our present work.", "startOffset": 0, "endOffset": 4}, {"referenceID": 14, "context": "[17] proposed online cost sensitive classification of imbalanced data.", "startOffset": 0, "endOffset": 4}, {"referenceID": 14, "context": "[17] work closely matches our work.", "startOffset": 0, "endOffset": 4}, {"referenceID": 15, "context": "Gmean [18] is such a metric that evaluates the degree of inductive bias in terms of a ratio of positive accuracy and negative accuracy.", "startOffset": 6, "endOffset": 10}, {"referenceID": 13, "context": "Online Setting In this subsection, we prove the following lemma due to [16] for the sake of completeness.", "startOffset": 71, "endOffset": 75}, {"referenceID": 6, "context": "In offline learning under ERM framework, measure of quality of f is expected risk [9]", "startOffset": 82, "endOffset": 85}, {"referenceID": 14, "context": "1It can be shown that problem formulation presented in this paper is similar to one presented in [17] but not equivalent since they were minimizing weighted sum of sensitivity and specificity where weights are decided by Laplace estimation.", "startOffset": 97, "endOffset": 101}, {"referenceID": 6, "context": "Since we are interested in online learning, we define an instantaneous risk [9] of using prediction function ft on example (xt, yt) as follows:", "startOffset": 76, "endOffset": 79}, {"referenceID": 6, "context": "Thus, if we can guarantee low cumulative mistake, we can prevent overfitting without the need of regularizer [9].", "startOffset": 109, "endOffset": 112}, {"referenceID": 16, "context": "To this end, we use online gradient descent [19] to optimize (??).", "startOffset": 44, "endOffset": 48}, {"referenceID": 14, "context": "Relative Loss Bound for OGMEAN Following [17], we can bound the regret of OGMEAN algorithm.", "startOffset": 41, "endOffset": 45}, {"referenceID": 14, "context": "We compare our algorithm with recently proposed CSOCsum algorithm [17] with respect to a metric called sum which is defined as:", "startOffset": 66, "endOffset": 70}, {"referenceID": 14, "context": "sum = np \u00d7 sensitivity + nn \u00d7 specificity (13) where np and nn are weight parameter which in [17] are set manually to 0.", "startOffset": 93, "endOffset": 97}, {"referenceID": 14, "context": "It is claimed in [17] that CSOCsum algorithm beats stateof-the-art online algorithms for class-imbalance problem with respect to sum metric.", "startOffset": 17, "endOffset": 21}, {"referenceID": 14, "context": "The algorithms compared in [17] are Perceptron [8], ROMMA, agg-ROMMA, PA-I, PA-II, CPAPB [10] and PAUM [20].", "startOffset": 27, "endOffset": 31}, {"referenceID": 5, "context": "The algorithms compared in [17] are Perceptron [8], ROMMA, agg-ROMMA, PA-I, PA-II, CPAPB [10] and PAUM [20].", "startOffset": 47, "endOffset": 50}, {"referenceID": 7, "context": "The algorithms compared in [17] are Perceptron [8], ROMMA, agg-ROMMA, PA-I, PA-II, CPAPB [10] and PAUM [20].", "startOffset": 89, "endOffset": 93}, {"referenceID": 17, "context": "The algorithms compared in [17] are Perceptron [8], ROMMA, agg-ROMMA, PA-I, PA-II, CPAPB [10] and PAUM [20].", "startOffset": 103, "endOffset": 107}], "year": 2015, "abstractText": "Anomaly detection is an important task in many real world applications such as fraud detection, suspicious activity detection, health care monitoring etc. In this paper, we tackle this problem from supervised learning perspective in online learning setting. We maximize well known Gmean metric for classimbalance learning in online learning framework. Specifically, we show that maximizing Gmean is equivalent to minimizing a convex surrogate loss function and based on that we propose novel online learning algorithm for anomaly detection. We then show, by extensive experiments, that the performance of the proposed algorithm with respect to sum metric is as good as a recently proposed Cost-Sensitive Online Classification(CSOC) algorithm for class-imbalance learning over various benchmarked data sets while keeping running time close to the perception algorithm. Our another conclusion is that other competitive online algorithms do not perform consistently over data sets of varying size. This shows the potential applicability of our proposed approach.", "creator": "LaTeX with hyperref package"}}}