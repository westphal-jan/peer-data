{"id": "1508.06904", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "27-Aug-2015", "title": "Rapid Exact Signal Scanning with Deep Convolutional Neural Networks", "abstract": "We introduce and analyze a rigorous formulation of the dynamics of a signal processing scheme that aims at dense scanning of large input signals. Recently proposed methodologies lack a satisfactory discussion of whether they actually produce the correct results according to their definition, especially in the context of Convolutional Neural Networks. We improve on this through an exact characterization of the requirements for a sound sliding window approach. The tools developed in this paper are especially beneficial if Convolutional Neural Networks are employed, but can also be used as a more general framework to validate related approaches to signal scanning. The contributed theory helps to eliminate redundant computations and renders special case treatment unnecessary, resulting in a dramatic boost in efficiency particularly on massively parallel processors.", "histories": [["v1", "Thu, 27 Aug 2015 15:50:26 GMT  (1818kb,D)", "http://arxiv.org/abs/1508.06904v1", null], ["v2", "Wed, 23 Mar 2016 18:49:52 GMT  (1864kb,D)", "http://arxiv.org/abs/1508.06904v2", null], ["v3", "Sat, 5 Nov 2016 12:18:13 GMT  (2070kb,D)", "http://arxiv.org/abs/1508.06904v3", "Pages 1-16 only: Copyright (c) 2015 IEEE. Personal use of this material is permitted. However, permission to use this material for any other purposes must be obtained from the IEEE by sending a request to pubs-permissions@ieee.org"], ["v4", "Fri, 3 Mar 2017 17:38:00 GMT  (2162kb,D)", "http://arxiv.org/abs/1508.06904v4", "Pages 1-16 only: Copyright (c) 2016 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission"], ["v5", "Wed, 2 Aug 2017 13:23:54 GMT  (2323kb,D)", "http://arxiv.org/abs/1508.06904v5", "Pages 1-16 only: Copyright (c) 2016 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission"]], "reviews": [], "SUBJECTS": "cs.LG cs.CV cs.NE", "authors": ["markus thom", "franz gritschneder"], "accepted": false, "id": "1508.06904"}, "pdf": {"name": "1508.06904.pdf", "metadata": {"source": "CRF", "title": "A Theory for Rapid Exact Signal Scanning with Deep Multi-Scale Convolutional Neural Networks", "authors": ["Markus Thom", "Franz Gritschneder"], "emails": ["markus.thom@uni-ulm.de)."], "sections": [{"heading": null, "text": "In fact, it is so that most of them are able to survive themselves without there being a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is"}, {"heading": "II. PREREQUISITES", "text": "In this section, we begin with a formal introduction to the building blocks of a CNN. Next, we correct the notation used throughout the essay, concluding the section with the formal definition of the subsignal extraction operator and a few statements about its properties."}, {"heading": "A. Convolutional Neural Networks", "text": "Ordinary CNNs process input data using specialized layers [19]. Convolutional layers respect the principle of weight distribution: they interweave their input with a filter bank that is learned during an adjustment process, and add a traceable scalar bias to form the layer output. A mathematical definition is given in Section III-C. Pooling layers give the invariance of a network by small translations of the input data by evaluating a fixed pooling kernel followed by downsampling operation. Effectively, this is the same as applying a function step by step, i.e. only certain equidistant subsignals are taken into account that are not required to start on exactly adjacent samples. This is done in Section IV. In addition to these layers, there are also fully connected layers and non-linear layers. The former are just a special case of Xiv: 150 8.06 904v 1 [cs.L] Aug 27 201 transmitting a single layer independently of a larger one from another, transferring a signal to a larger layer in this resolution."}, {"heading": "B. Notation", "text": "For the sake of simplicity, we limit our analysis to vector-shaped signals. Generalizing our results to more complex signals, such as images with pixels arranged on a two-dimensional grid, is simple by applying the theory to appropriate indexes of the images. We write N1: = N\\ {0} for the positive natural numbers and Z for the integers. The ceiling function, which rounds up their reasoning to the next larger natural number, is called d \u00b7 e. If M is a set and q \u00b2 N1, then Mq denotes the set of all q tuples with entries from M. The elements of Mq are called signals, their q entries are called samples."}, {"heading": "C. Division of a Signal into Subsignals", "text": "A subsignal is a coherent list of samples contained in a larger signal. First, let's formalize the concept of extracting subsignals with a fixed number of samples from a given signal. \u2212 Look at the illustration of the subsignal extraction operator, which is applied to a signal with eight samples for extracting subsignals with four samples each. Definition 1. Let M be a set and let d \u00b2 N1 detect a fixed subsignal dimensionality."}, {"heading": "III. SUBSIGNAL COMPATIBLE TRANSFORMATIONS", "text": "We show that functions that are applied in a sliding fashion can be understood as subsignal-compatible transformations, and that they meet the requirements for subsignal-compatible transformations, and that the composition of subsignal-compatible transformations again is a subsignal-compatible transformation.At the end of this section, we look at non-poolless Convolutionary Neural Networks and show that they meet the requirements for subsignal-compatible transformations. As a consequence, CNNs without pooling layers can be applied to the entire input signal without us handling individual subsignals.We start with the most important definition of this section: Definition 3. M and N are set which are c-compatible transformations."}, {"heading": "A. Relationship between Functions Applied in a Sliding Fashion and Subsignal Compatible Transformations", "text": "Let us now examine functions applied to a signal in a sliding fashion. First, let us define what this means: Definition 5. Let M and N be set, let M and N be a positive natural number, and let f: M and N be a function. (Let's). (Let (Let M and N) be a function (Let M). (Let (Let M) be a function. (Let M) be an operator, let f be arbitrary signals in a sliding manner. The next result is that functions applied in a sliding manner are essentially the same as subsignal-compatible transformations, and that the exchange property could be weakened only in the case where dimensionality reduction is constant."}, {"heading": "B. Composition of Subsignal Compatible Transformations", "text": "The composition of the subsignal compatible transformations is again a subsignal compatible transformation: Theorem 8 (reduction): Let M, N and P (reduction) be set and let c1, c2 and N1. Suppose that T1: + c1 (N) is a subsignal compatible transformation with dimensionality reduction constant c2.Define c: = c1 \u2212 1 (1) compatible transformation with dimensionality reduction constant c2.Define c: = c2 \u2212 1 (1) compatible transformation with dimensionality reduction N1 (1). Then T: + c (M)."}, {"heading": "C. CNNs without Pooling Layers", "text": "The previous parts of this section were quite abstract. We now show how revolutionary neural networks fit into the theory developed so far without any pooling layers. Pooling layers require further preparation and are detailed in Sect. IV.5 The convolution operation is the substance of a CNN. Here, multi-channel input functions are wrapped with previously learned filter banks, and the result is cumulated and a trainable bias is added to the output characteristic map. We start with the introduction of indexing rules to take into account the multi-channel input functions of the occurring signals. Let M be a sentence, a, b, N1 generate positive natural numbers and a multi-channel signal. It is then the introduction of indexing rules for the occurring structures."}, {"heading": "IV. POOLING LAYERS AND FUNCTIONS APPLIED IN A STRIDED FASHION", "text": "This is equivalent to functions that are applied in a sliding fashion, followed by a downsampling operation. Of course, the theory developed here can also be applied to other functions, such as striped multichannel convolutions with the results of Sect. III-C. We will demonstrate how these functions can be converted into subsignal-compatible transformations, using a data structure that was recently introduced as fragmentation [15]. Here, we will greatly generalize the method and rigorously prove the correctness of the approach. As a side effect of our results, we are able to accurately describe the dynamics of the entire execution chain, including the ability to track the position of each processed signal in the data."}, {"heading": "A. Fragmentation", "text": "The fragmentation operator performs a special reordering operation, but for its precise analysis we need to recapitulate some basic number theories."}, {"heading": "B. Relationship between Fragmentation, Functions Applied in a Strided Fashion and Subsignal Compatible Transformations", "text": "The result of a subsignal compatible transformation, which is applied to a fragmented signal, is a fragmented signal with D-N1 samples in each of the s-N1 fragments, in which D-N1 fragments, in which D-N1 fragments, in which D-N1 fragments, in which D-N1 fragments, in which D-N1 fragments, in which D-N1 fragments, in a subsignal compatible transformations with dimensionality constant c-N1,., s) we write: (s) We (s) n (D-c + 1), in which the fragments of each fragment. The output of T is then defined as asT: = [T (1),."}, {"heading": "V. MULTI-SCALE TRANSFORMATIONS", "text": "The previous sections have shown how the Convolutionary Neural Networks can be efficiently evaluated on whole images without extending the theory of subsignal-compatible transformations. We now assume that the number of samples viewed at any given time is fixed at all levels, thus facilitating the design of scale-invariant representations, for example, by using the same classification for all scales of the input signals. [13].A signal is scaled down by applying a low-pass filter to reduce the calibrating artifacts, followed by a subset of equivalent samples. When a subsignal is extracted from a downscaled input signal, it should contain a downscaled copy of the corresponding subsignal from the original input signal."}, {"heading": "VI. CONCLUSIONS", "text": "This paper presented and analyzed the concept of subsignal-compatible transformations, functions that allow subsignal extraction to be exchanged with function evaluation without impacting on the result, rigorously demonstrating how Convolutionary Neural Networks can be applied efficiently and smoothly to large signals, without redundant calculations and special case handling, and the final part demonstrated the versatility of the theory developed by taking detailed account of multiscale transformations. All results have been explicitly proven correct, and the arguments presented in the evidence can be used to verify whether a given implementation is correct, or they can serve as a basis for algorithmic realizations. In both cases, a comprehensive mathematical framework is available that facilitates the analysis of fast, accurate signal scanning schemes."}, {"heading": "ACKNOWLEDGMENT", "text": "This work was supported by Daimler AG, Germany."}], "references": [{"title": "Receptive Fields, Binocular Interaction and Functional Architecture in the Cat\u2019s Visual Cortex", "author": ["D.H. Hubel", "T.N. Wiesel"], "venue": "Journal of Physiology, vol. 160, no. 1, pp. 106\u2013154, 1962.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 1962}, {"title": "Neocognitron: A Self-Organizing Neural Network Model for a Mechanism of Pattern Recognition Unaffected by Shift in Position", "author": ["K. Fukushima"], "venue": "Biological Cybernetics, vol. 36, no. 4, pp. 193\u2013202, 1980.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 1980}, {"title": "Handwritten Digit Recognition with a Back- Propagation Network", "author": ["Y. LeCun", "B. Boser", "J.S. Denker", "D. Henderson", "R.E. Howard", "W. Hubbard", "L.D. Jackel"], "venue": "Advances in Neural Information Processing Systems, vol. 2, 1990, pp. 396\u2013404.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 1990}, {"title": "Multi-Column Deep Neural Networks for Image Classification", "author": ["D.C. Cire\u015fan", "U. Meier", "J. Schmidhuber"], "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2012, pp. 3642\u20133649.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2012}, {"title": "Regularization of Neural Networks using DropConnect", "author": ["L. Wan", "M. Zeiler", "S. Zhang", "Y. LeCun", "R. Fergus"], "venue": "Proceedings of the International Conference on Machine Learning, 2013, pp. 1058\u20131066.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2013}, {"title": "Multi-Column Deep Neural Network for Traffic Sign Classification", "author": ["D. Cire\u015fan", "U. Meier", "J. Masci", "J. Schmidhuber"], "venue": "Neural Networks, vol. 32, pp. 333\u2013338, 2012.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2012}, {"title": "Deeply- Supervised Nets", "author": ["C.-Y. Lee", "S. Xie", "P.W. Gallagher", "Z. Zhang", "Z. Tu"], "venue": "Proceedings of the International Conference on Artificial Intelligence and Statistics, 2015, pp. 562\u2013570.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2015}, {"title": "ImageNet Classification with Deep Convolutional Neural Networks", "author": ["A. Krizhevsky", "I. Sutskever", "G.E. Hinton"], "venue": "Advances in Neural Information Processing Systems, vol. 25, 2013, pp. 1097\u20131105.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2013}, {"title": "Going Deeper with Convolutions", "author": ["C. Szegedy", "W. Liu", "Y. Jia", "P. Sermanet", "S. Reed", "D. Anguelov", "D. Erhan", "V. Vanhoucke", "A. Rabinovich"], "venue": "Tech. Rep. arXiv:1409.4842v1, 2014.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2014}, {"title": "ImageNet Large Scale Visual Recognition Challenge", "author": ["O. Russakovsky", "J. Deng", "H. Su", "J. Krause", "S. Satheesh", "S. Ma", "Z. Huang", "A. Karpathy", "A. Khosla", "M. Bernstein", "A.C. Berg", "L. Fei- Fei"], "venue": "Tech. Rep. arXiv:1409.0575v3, 2015.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2015}, {"title": "Toward Automatic Phenotyping of Developing Embryos From Videos", "author": ["F. Ning", "D. Delhomme", "Y. LeCun", "F. Piano", "L. Bottou", "P.E. Barbano"], "venue": "IEEE Transactions on Image Processing, vol. 14, no. 9, pp. 1360\u20131371, 2005.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2005}, {"title": "Deep Convolutional Networks for Scene Parsing", "author": ["D. Grangier", "L. Bottou", "R. Collobert"], "venue": "International Conference on Machine Learning, Workshop on Learning Feature Hierarchies, 2009.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2009}, {"title": "Learning Hierarchical Features for Scene Labeling", "author": ["C. Farabet", "C. Couprie", "L. Najman", "Y. LeCun"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 35, no. 8, pp. 1915\u20131929, 2013.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 1915}, {"title": "An Original Approach for the Localization of Objects in Images", "author": ["R. Vaillant", "C. Monrocq", "Y. LeCun"], "venue": "Proceedings of the International Conference on Artificial Neural Networks, 1993, pp. 26\u201330.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 1993}, {"title": "Fast Image Scanning with Deep Max-Pooling Convolutional Neural Networks", "author": ["A. Giusti", "D.C. Cire\u015fan", "J. Masci", "L.M. Gambardella", "J. Schmidhuber"], "venue": "IEEE International Conference on Image Processing, 2013, pp. 4034\u20134038.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2013}, {"title": "Highly Efficient Forward and Backward Propagation of Convolutional Neural Networks for Pixelwise Classification", "author": ["H. Li", "R. Zhao", "X. Wang"], "venue": "Tech. Rep. arXiv:1412.4526v2, 2014.  16", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2014}, {"title": "OverFeat: Integrated Recognition, Localization and Detection using Convolutional Networks", "author": ["P. Sermanet", "D. Eigen", "X. Zhang", "M. Mathieu", "R. Fergus", "Y. LeCun"], "venue": "Proceedings of the International Conference on Learning Representations. arXiv:1312.6229v4, 2014.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2014}, {"title": "DenseNet: Implementing Efficient ConvNet Descriptor Pyramids", "author": ["F. Iandola", "M. Moskewicz", "S. Karayev", "R. Girshick", "T. Darrell", "K. Keutzer"], "venue": "Tech. Rep. arXiv:1404.1869v1, 2014.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 1869}, {"title": "Gradient-Based Learning Applied to Document Recognition", "author": ["Y. LeCun", "L. Bottou", "Y. Bengio", "P. Haffner"], "venue": "Proceedings of the IEEE, vol. 86, no. 11, pp. 2278\u20132324, 1998.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 1998}, {"title": "Optimal Unsupervised Learning in Feedforward Neural Networks", "author": ["T.D. Sanger"], "venue": "Master\u2019s thesis, Massachusetts Institute of Technology, 1989.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 1989}, {"title": "Some Theorems on Matrix Differentiation with Special Reference to Kronecker Matrix Products", "author": ["H. Neudecker"], "venue": "Journal of the American Statistical Association, vol. 64, no. 327, pp. 953\u2013963, 1969.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 1969}], "referenceMentions": [{"referenceID": 0, "context": "Inspired by findings on the structure of mammalian visual cortex [1], the concept of spatial weight sharing has been integrated into artificial neural networks to achieve an architecture nowadays broadly known as Convolutional Neural Network (CNN) [2], [3].", "startOffset": 65, "endOffset": 68}, {"referenceID": 1, "context": "Inspired by findings on the structure of mammalian visual cortex [1], the concept of spatial weight sharing has been integrated into artificial neural networks to achieve an architecture nowadays broadly known as Convolutional Neural Network (CNN) [2], [3].", "startOffset": 248, "endOffset": 251}, {"referenceID": 2, "context": "Inspired by findings on the structure of mammalian visual cortex [1], the concept of spatial weight sharing has been integrated into artificial neural networks to achieve an architecture nowadays broadly known as Convolutional Neural Network (CNN) [2], [3].", "startOffset": 253, "endOffset": 256}, {"referenceID": 3, "context": "This particular architecture has proven highly successful in a large variety of pattern recognition tasks, setting the state of the art in handwritten digit recognition [4], [5], traffic sign recognition [4], [6], house number recognition from street view [5], [7], and large-scale object category classification [8], [9], [10] to name just a few.", "startOffset": 169, "endOffset": 172}, {"referenceID": 4, "context": "This particular architecture has proven highly successful in a large variety of pattern recognition tasks, setting the state of the art in handwritten digit recognition [4], [5], traffic sign recognition [4], [6], house number recognition from street view [5], [7], and large-scale object category classification [8], [9], [10] to name just a few.", "startOffset": 174, "endOffset": 177}, {"referenceID": 3, "context": "This particular architecture has proven highly successful in a large variety of pattern recognition tasks, setting the state of the art in handwritten digit recognition [4], [5], traffic sign recognition [4], [6], house number recognition from street view [5], [7], and large-scale object category classification [8], [9], [10] to name just a few.", "startOffset": 204, "endOffset": 207}, {"referenceID": 5, "context": "This particular architecture has proven highly successful in a large variety of pattern recognition tasks, setting the state of the art in handwritten digit recognition [4], [5], traffic sign recognition [4], [6], house number recognition from street view [5], [7], and large-scale object category classification [8], [9], [10] to name just a few.", "startOffset": 209, "endOffset": 212}, {"referenceID": 4, "context": "This particular architecture has proven highly successful in a large variety of pattern recognition tasks, setting the state of the art in handwritten digit recognition [4], [5], traffic sign recognition [4], [6], house number recognition from street view [5], [7], and large-scale object category classification [8], [9], [10] to name just a few.", "startOffset": 256, "endOffset": 259}, {"referenceID": 6, "context": "This particular architecture has proven highly successful in a large variety of pattern recognition tasks, setting the state of the art in handwritten digit recognition [4], [5], traffic sign recognition [4], [6], house number recognition from street view [5], [7], and large-scale object category classification [8], [9], [10] to name just a few.", "startOffset": 261, "endOffset": 264}, {"referenceID": 7, "context": "This particular architecture has proven highly successful in a large variety of pattern recognition tasks, setting the state of the art in handwritten digit recognition [4], [5], traffic sign recognition [4], [6], house number recognition from street view [5], [7], and large-scale object category classification [8], [9], [10] to name just a few.", "startOffset": 313, "endOffset": 316}, {"referenceID": 8, "context": "This particular architecture has proven highly successful in a large variety of pattern recognition tasks, setting the state of the art in handwritten digit recognition [4], [5], traffic sign recognition [4], [6], house number recognition from street view [5], [7], and large-scale object category classification [8], [9], [10] to name just a few.", "startOffset": 318, "endOffset": 321}, {"referenceID": 9, "context": "This particular architecture has proven highly successful in a large variety of pattern recognition tasks, setting the state of the art in handwritten digit recognition [4], [5], traffic sign recognition [4], [6], house number recognition from street view [5], [7], and large-scale object category classification [8], [9], [10] to name just a few.", "startOffset": 323, "endOffset": 327}, {"referenceID": 10, "context": "If this is carried out for each feasible image position, it is possible to assign class membership estimations to all the pixels in an image resulting in a dense description of all objects in a scene [11], [12], [13].", "startOffset": 200, "endOffset": 204}, {"referenceID": 11, "context": "If this is carried out for each feasible image position, it is possible to assign class membership estimations to all the pixels in an image resulting in a dense description of all objects in a scene [11], [12], [13].", "startOffset": 206, "endOffset": 210}, {"referenceID": 12, "context": "If this is carried out for each feasible image position, it is possible to assign class membership estimations to all the pixels in an image resulting in a dense description of all objects in a scene [11], [12], [13].", "startOffset": 212, "endOffset": 216}, {"referenceID": 13, "context": "Although this was already realized for CNNs without pooling layers more than two decades ago [14], approaches that also support pooling layers emerged only recently [15], [16], [17], [18].", "startOffset": 93, "endOffset": 97}, {"referenceID": 14, "context": "Although this was already realized for CNNs without pooling layers more than two decades ago [14], approaches that also support pooling layers emerged only recently [15], [16], [17], [18].", "startOffset": 165, "endOffset": 169}, {"referenceID": 15, "context": "Although this was already realized for CNNs without pooling layers more than two decades ago [14], approaches that also support pooling layers emerged only recently [15], [16], [17], [18].", "startOffset": 171, "endOffset": 175}, {"referenceID": 16, "context": "Although this was already realized for CNNs without pooling layers more than two decades ago [14], approaches that also support pooling layers emerged only recently [15], [16], [17], [18].", "startOffset": 177, "endOffset": 181}, {"referenceID": 17, "context": "Although this was already realized for CNNs without pooling layers more than two decades ago [14], approaches that also support pooling layers emerged only recently [15], [16], [17], [18].", "startOffset": 183, "endOffset": 187}, {"referenceID": 18, "context": "Convolutional Neural Networks Ordinary CNNs process input data by means of specialized layers [19].", "startOffset": 94, "endOffset": 98}, {"referenceID": 19, "context": "The latter send each sample of a larger signal through a scalar transfer function independently, which may for example be a hyperbolic tangent or a rectification nonlinearity [20].", "startOffset": 175, "endOffset": 179}, {"referenceID": 12, "context": "It is further possible to improve performance through incorporation of context information using multi-scale analysis so that the network sees image regions in different spatial resolutions [13].", "startOffset": 190, "endOffset": 194}, {"referenceID": 14, "context": "We will demonstrate how these functions can be turned into subsignal compatible transformations using a data structure recently introduced as fragmentation [15].", "startOffset": 156, "endOffset": 160}, {"referenceID": 14, "context": "Here, we will greatly generalize the method proposed by [15] and rigorously prove the correctness of the approach.", "startOffset": 56, "endOffset": 60}, {"referenceID": 20, "context": "The vectorization operator [21] stacks all the columns of a matrix on top of another: Definition 14.", "startOffset": 27, "endOffset": 31}, {"referenceID": 12, "context": "Since here the context of local regions is incorporated as well, this approach has proven highly effective in classification tasks [13].", "startOffset": 131, "endOffset": 135}, {"referenceID": 12, "context": "This facilitates the design of scale-invariant representations, for example by using the same classifier for all scales of the input [13].", "startOffset": 133, "endOffset": 137}], "year": 2017, "abstractText": "We introduce and analyze a rigorous formulation of the dynamics of a signal processing scheme that aims at dense scanning of large input signals. Recently proposed methodologies lack a satisfactory discussion of whether they actually produce the correct results according to their definition, especially in the context of Convolutional Neural Networks. We improve on this through an exact characterization of the requirements for a sound sliding window approach. The tools developed in this paper are especially beneficial if Convolutional Neural Networks are employed, but can also be used as a more general framework to validate related approaches to signal scanning. The contributed theory helps to eliminate redundant computations and renders special case treatment unnecessary, resulting in a dramatic boost in efficiency particularly on massively parallel processors.", "creator": "LaTeX with hyperref package"}}}