{"id": "1508.06924", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "27-Aug-2015", "title": "Using Thought-Provoking Children's Questions to Drive Artificial Intelligence Research", "abstract": "We propose to use thought-provoking children's questions (TPCQs), namely Highlights BrainPlay questions, to drive artificial intelligence research. These questions are designed to stimulate thought and learning in children, and they can be used to do the same thing in AI systems. We introduce the TPCQ task, which consists of taking a TPCQ question as input and producing as output both (1) answers to the question and (2) learned generalizations. We discuss how BrainPlay questions stimulate learning. We analyze 244 BrainPlay questions, and we report statistics on question type, question class, answer cardinality, answer class, types of knowledge needed, and types of reasoning needed. We find that BrainPlay questions span many aspects of intelligence. We envision an AI system based on the society of mind (Minsky 1986; 2006) consisting of a multilevel architecture with diverse resources that run in parallel to jointly answer and learn from questions. Because the answers to BrainPlay questions and the generalizations learned from them are often highly open-ended, we suggest using human judges for evaluation.", "histories": [["v1", "Thu, 27 Aug 2015 16:23:49 GMT  (23kb)", "https://arxiv.org/abs/1508.06924v1", null], ["v2", "Fri, 11 Sep 2015 13:01:00 GMT  (23kb)", "http://arxiv.org/abs/1508.06924v2", "update Institute name and URL; fix date of Myers and Myers interview; add reference to Turing++ questions; clarify that we developed the set of annotation tags"], ["v3", "Wed, 26 Jul 2017 00:34:24 GMT  (22kb)", "http://arxiv.org/abs/1508.06924v3", "update for EGPAI 2017"]], "reviews": [], "SUBJECTS": "cs.AI", "authors": ["erik t mueller", "henry minsky"], "accepted": false, "id": "1508.06924"}, "pdf": {"name": "1508.06924.pdf", "metadata": {"source": "CRF", "title": null, "authors": [], "emails": [], "sections": [{"heading": null, "text": "ar Xiv: 150 8.06 924v 3 [cs.A I] 2 6Ju l 2"}, {"heading": "Introduction", "text": "Since artificial intelligence tasks such as answering fact-based questions [Ferrucci et al., 2013] and facial recognition [Taigman et al., 2014] are largely solved, there is a need for tougher tasks. Consider the following questions from the children's magazine Highlights: Why does a key not open for each lock? Is a tree or leaf on the tree wider? Could one not sing a song in a dark room? Could one put together a puzzle and the ocean has waves? What happens when a fish is farthest from the head? Is an ice sheet on the ground wider or at the top? Could one sing a song in a dark room? Could one put together a puzzle that is faster than the shadow?"}, {"heading": "Highlights BrainPlay", "text": "The magazine Highlights was founded in 1946 by Garry Cleveland Myers and Caroline Clark Myers and contains a BrainPlay column, which before November 2004 was entitled Headwork. In this essay, we use the term BrainPlay for both brainwork and BrainPlay questions. The editors of the Highlights develop BrainPlay questions with great care. The questions are intended to \"stimulate children from five to twelve years of age to think and think by revising in their minds what is already there and coming up with new ideas that they have not learned from books.\" [Myers, 1968] Take, for example, the BrainPlay question: \"How can you calculate the height of the ceiling on the first floor in a room with a staircase leading to the second floor?\" This question suggests a novel technique: To measure the height of a ceiling when a staircase leads to the next floor, multiply the rise of the stairs by the number of steps leading to the second floor. BrainPlay first appeared in the second issue of Highlights in September 1946, BrainPlay's 1986, not BrainPlay's 1964, around the month."}, {"heading": "Analysis of BrainPlay Questions", "text": "To get an idea of what we are dealing with, we conducted an analysis of the BrainPlay questions in the Highlights issues from January 2000 to December 2000. First, we segmented each top-level question into one or more sub-questions. For example, the top-level question would you rather wear a hood or hat? Why? divided into a first and a second question. Table 1 shows the composition of the sub-questions. Table 2 provides statistics on the length of the sub-questions. The first question tends to be the longest. The second and next question typically provides explanations for the answer to the first question, asks deviations from the first question (often with co-references) or follows in some other way. For the rest of the analysis, we looked at only the first questions. We commented on each first question with exactly one question type, question class, answer category, cardinality and answer class, and commented on each first question with one or more types of knowledge and types of arguments needed. We developed a first set of comments while the overwork is open and if necessary."}, {"heading": "Question Type", "text": "What is your preferred way to travel? Name three uses for bells. Several choices are offered, and the question is not a yes-no question. Would you rather receive a call or letter? Is it harder to ride a bike or run fast? Yes-No The choices are yes and no. Do you know anyone with your initials? Have you ever cried because you were very happy?"}, {"heading": "Question Class", "text": "The question is, what can you do to help someone who is able to live a good life? What can you do to live a good life? What can you do to live a good life? What can you do to live a bad life? What can you do to live a bad life? What can you do to live a bad life? What can you do to live a bad life? What can you do to live a bad life? What can you do to live a bad life? What can you do to live a bad life? What can you do to live a bad life? What can you do to live a bad life? What can you do to live a bad life?"}, {"heading": "Answer Cardinality", "text": "Answer: Who is the biggest person you know? Is it easier to throw or catch a ball? > 1 More than one answer. How do the wings of a bird differ from the wings of a butterfly? Why do people make New Year's resolutions? 2 Two answers: What weather and location are ideal for stargazing? Think of a fruit and a vegetable that start with the letter p. 3 Three answers. Name three ways to have fun on a rainy day. Name three objects shaped like a triangle.5 Five answers. Name the five most important things you like to do with your friends."}, {"heading": "Answer Class", "text": "So the answers to 103 (42.21%) of the 244 BrainPlay questions we analyzed can be automatically evaluated. What about the remaining questions? Human judgment is required to answer the Many, Personal, Open, Debatable and Nontextual questions. More points should be awarded for correct answers to more difficult questions.Many questions have many short, correct answers. When might it be useful to know a few jokes? Where can you find spiders? Exactly one question has one possible right answer. At what time of the year do you usually wear sunglasses? What does it mean to be \"on cloud nine\"? Several questions have short, correct answers. What things do you write in a diary? Name three animals that hatch from eggs? Personally The question can only be answered in relation to your personal experience. Try to name all the people you have spoken to."}, {"heading": "Types of Knowledge Needed", "text": "What is the feeling when you work with a friend? Is it easier to catch a ball? Try to clap your hands behind your back. Properties / attributes of people and things are New Year's resolute. What is the feeling when you work with a friend? Is it easier to catch a ball? Try to clap your hands behind your back? Properties / attributes of people and things are casual?"}, {"heading": "Types of Reasoning Needed", "text": "Again, the percentages are more than 100, because each question is commented on with one or more argumentation.Database Retrieval Database Retrieval Database Retrieval Retrieval Retrieval Retrieval. Who is the biggest person you know? Name three animals that hatch from eggs. Simulating the sequence of events that do not necessarily require physical or three-dimensional thinking. Is it easier to swallow a pill or a spoonful of medicine? Why could a bear with a cub alone be more dangerous than a bear? Planning or generating a sequence of actions to achieve a goal [Ghallab et al., 2004]."}, {"heading": "Reasoning Type % Questions # Questions", "text": "Describe your favorite place for a walk. Comparison Quantitative or qualitative comparison. Who is the biggest person you know? What do elbows and knees have in common? Retrieving episodic memory or retrieving personal experiences from episodic memory. Have you ever been so busy that you forgot to eat a meal? What mistakes have you made from which you have learned? Visualization visualization and images. How do the wings of a bird differ from the wings of a butterfly? From the stars, the moon and the sun you can see during the day? 3D simulation Physical or three-dimensional simulation. Try to clap with your hands on your back. Why don't we wear clocks on our ankles? Invent or create something. Describe a toy you want to invent. Invent a word that means \"so funny you can't stop laughing.\" How many arithmetic operations have grown over the past year? How many arithmetic centimeters have you grown to choose?"}, {"heading": "Correlation with Question Position", "text": "The correlation of various annotations with the position in the BrainPlay column is shown in Table 9, with only correlations greater than 0.1. Publishers of the highlights present the BrainPlay questions in increasing order of difficulty [Myers and Myers, 1964], so that these correlations give a rough idea of difficulty. High positive correlations correspond to high difficulty, while high negative correlations correspond to low difficulty."}, {"heading": "BrainPlay\u2019s Coverage of Intelligence", "text": "We can use the key sections of the fifth issue of The Cognitive Neurosciences [Gazzaniga and Mangun, 2014] as a guide to the many areas of human intelligence. A rough correspondence between these sections and BrainPlay is shown in Table 10. (\"VI Memory\" includes prediction and imagination.) We see that BrainPlay questions encompass many aspects of intelligence. Intentionally and intentionally, many of the thought-provoking questions asked by children are designed to push the system into generating new knowledge, because many of the answers are open and are most often unlikely to be previously seen and explicitly stored. It is a hallmark of human intelligence that new knowledge can and must be generated from existing knowledge when needed in order to achieve a new goal, and these questions are designed to exercise and uncover these mechanisms."}, {"heading": "Related Work", "text": "In Aristo [Clark, 2015], a multiple choice question of the primary school science exam is taken as input, and an answer is produced as output. While Aristo investigates scientific knowledge, the BrainPlay / TPCQ task explores the knowledge that each child simply acquires through experience. Basic science questions evaluate the understanding of the connections learned in school, while TPCQs encourage the creation of new connections. In the bAbI tasks [Weston et al., 2015] a simple story and question about history are taken as input, and an answer is produced as output. Stories are created using a simulator that contains characters and objects. Questions are very simple and limited compared to TPCQs. The MCTest output [Richardson et al., 2013] consists of short stories, multiple choice questions about the stories and correct answers to the questions. The questions are designed so that answering PCs (1) understand information from two sets of questions or more than one set of questions (StoryQ) are provided as a basis for the Q."}, {"heading": "Conclusion", "text": "If today's artificial intelligence systems cannot even answer these questions, how can we really say that they are intelligent? We believe that systems that can answer and learn from BrainPlay will increase progress in artificial intelligence."}, {"heading": "Acknowledgments", "text": "We thank Kent Johnson, CEO of Highlights for Children, Inc., for allowing us to use the BrainPlay questions. We also thank Patricia M. Mikelson and Sharon M. Umnik of Highlights for providing us with the BrainPlay material."}], "references": [{"title": "VQA: Visual question answering", "author": ["Stanislaw Antol", "Aishwarya Agrawal", "Jiasen Lu", "Margaret Mitchell", "Dhruv Batra", "C. Lawrence Zitnick", "Devi Parikh"], "venue": "CoRR, abs/1505.00468,", "citeRegEx": "Antol et al.. 2015", "shortCiteRegEx": null, "year": 2015}, {"title": "Elementary school science and math tests as a driver for AI: Take the Aristo Challenge! In Blai Bonet and Sven Koenig", "author": ["Peter Clark"], "venue": "editors, Proceedings of the Twenty-Ninth AAAI Conference on Artificial Intelligence, pages 4019\u20134021, Palo Alto, CA,", "citeRegEx": "Clark. 2015", "shortCiteRegEx": null, "year": 2015}, {"title": "Recognizing textual entailment: Models and applications", "author": ["Ido Dagan", "Dan Roth", "Mark Sammons", "Fabio Massimo Zanzotto"], "venue": "Morgan & Claypool, San Rafael, CA,", "citeRegEx": "Dagan et al.. 2013", "shortCiteRegEx": null, "year": 2013}, {"title": "Theory of Mind: How Children Understand Others\u2019 Thoughts and Feelings", "author": ["Martin J. Doherty"], "venue": "Psychology Press, East Sussex,", "citeRegEx": "Doherty. 2009", "shortCiteRegEx": null, "year": 2009}, {"title": "Watson: Beyond Jeopardy! Artificial Intelligence", "author": ["David Ferrucci", "Anthony Levas", "Sugato Bagchi", "David Gondek", "Erik T. Mueller"], "venue": "199\u2013200:93\u2013 105,", "citeRegEx": "Ferrucci et al.. 2013", "shortCiteRegEx": null, "year": 2013}, {"title": "editors", "author": ["Michael S. Gazzaniga", "George R. Mangun"], "venue": "The Cognitive Neurosciences. MIT Press, Cambridge, MA, fifth edition,", "citeRegEx": "Gazzaniga and Mangun. 2014", "shortCiteRegEx": null, "year": 2014}, {"title": "Automated Planning: Theory and Practice", "author": ["Malik Ghallab", "Dana Nau", "Paolo Traverso"], "venue": "Morgan Kaufmann, San Francisco,", "citeRegEx": "Ghallab et al.. 2004", "shortCiteRegEx": null, "year": 2004}, {"title": "How We Remember: Brain Mechanisms of Episodic Memory", "author": ["Michael E. Hasselmo"], "venue": "MIT Press, Cambridge, MA,", "citeRegEx": "Hasselmo. 2012", "shortCiteRegEx": null, "year": 2012}, {"title": "Hillsdale", "author": ["Fritz Heider. The Psychology of Interpersonal Relations. Lawrence Erlbaum"], "venue": "NJ,", "citeRegEx": "Heider. 1958", "shortCiteRegEx": null, "year": 1958}, {"title": "The Winograd schema challenge", "author": ["Levesque et al", "2012] Hector J. Levesque", "Ernest Davis", "Leora Morgenstern"], "venue": "Principles of Knowledge Representation and Reasoning: Proceedings of the Thirteenth International", "citeRegEx": "al. et al\\.,? \\Q2012\\E", "shortCiteRegEx": "al. et al\\.", "year": 2012}, {"title": "Courtesy of Patricia M", "author": ["Garry Cleveland Myers", "Caroline Clark Myers. Unpublished interview with Garry Cleveland Myers", "Caroline Clark Myers"], "venue": "Mikelson,", "citeRegEx": "Myers and Myers. 1964", "shortCiteRegEx": null, "year": 1964}, {"title": "Columbus", "author": ["Garry Cleveland Myers. Headwork for elementary school children. Highlights for Children"], "venue": "Ohio,", "citeRegEx": "Myers. 1968", "shortCiteRegEx": null, "year": 1968}, {"title": "Turing++ questions: A test for the science of (human) intelligence", "author": ["Tomaso Poggio", "Ethan Meyers"], "venue": "AI Magazine, 37(1):73\u201377,", "citeRegEx": "Poggio and Meyers. 2016", "shortCiteRegEx": null, "year": 2016}, {"title": "MCTest: A challenge dataset for the open-domain machine comprehension of text", "author": ["Richardson et al", "2013] Matthew Richardson", "Christopher J.C. Burges", "Erin Renshaw"], "venue": "In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing,", "citeRegEx": "al. et al\\.,? \\Q2013\\E", "shortCiteRegEx": "al. et al\\.", "year": 2013}, {"title": "and Understanding: An Inquiry into Human Knowledge Structures", "author": ["Roger C. Schank", "Robert P. Abelson. Scripts", "Plans", "Goals"], "venue": "Lawrence Erlbaum, Hillsdale, NJ,", "citeRegEx": "Schank and Abelson. 1977", "shortCiteRegEx": null, "year": 1977}, {"title": "DeepFace: Closing the gap to human-level performance in face verification", "author": ["Taigman et al", "2014] Yaniv Taigman", "Ming Yang", "Marc\u2019Aurelio Ranzato", "Lior Wolf"], "venue": "IEEE Conference on Computer Vision and Pattern Recognition,", "citeRegEx": "al. et al\\.,? \\Q2014\\E", "shortCiteRegEx": "al. et al\\.", "year": 2014}, {"title": "Oxford University Press", "author": ["Endel Tulving. Elements of episodic memory"], "venue": "New York,", "citeRegEx": "Tulving. 1983", "shortCiteRegEx": null, "year": 1983}, {"title": "Mind", "author": ["Alan M. Turing. Computing machinery", "intelligence"], "venue": "59(236):433\u2013460,", "citeRegEx": "Turing. 1950", "shortCiteRegEx": null, "year": 1950}, {"title": "Towards AI-complete question answering: A set of prerequisite toy tasks", "author": ["Jason Weston", "Antoine Bordes", "Sumit Chopra", "Tomas Mikolov"], "venue": "CoRR, abs/1502.05698,", "citeRegEx": "Weston et al.. 2015", "shortCiteRegEx": null, "year": 2015}, {"title": "Headwork: Open ended questions except a few for the very young", "author": ["Jean Wood"], "venue": "Courtesy of Sharon M. Umnik,", "citeRegEx": "Wood. 1986", "shortCiteRegEx": null, "year": 1986}], "referenceMentions": [{"referenceID": 4, "context": "As artificial intelligence tasks like fact-based question answering [Ferrucci et al., 2013] and face recognition [Taigman et al.", "startOffset": 68, "endOffset": 91}, {"referenceID": 11, "context": "The questions are designed to \u201c[stimulate] children from five to twelve to think and reason by working over in their heads what is already there, arriving at new ideas not learned from books\u201d [Myers, 1968].", "startOffset": 192, "endOffset": 205}, {"referenceID": 19, "context": "BrainPlay first appeared in the second issue of Highlights in September 1946 [Wood, 1986].", "startOffset": 77, "endOffset": 89}, {"referenceID": 10, "context": "Each month, BrainPlay presents around 20 questions arranged by age level [Myers and Myers, 1964].", "startOffset": 73, "endOffset": 96}, {"referenceID": 3, "context": "Theory of Mind Evaluates theory of mind [Doherty, 2009].", "startOffset": 40, "endOffset": 55}, {"referenceID": 17, "context": "Because there is no gold standard answer key for them, answers can be judged for plausibility by human judges, as in the Turing test [Turing, 1950].", "startOffset": 133, "endOffset": 147}, {"referenceID": 14, "context": "Scripts Stereotypical situations and scripts [Schank and Abelson, 1977].", "startOffset": 45, "endOffset": 71}, {"referenceID": 14, "context": "Plans/Goals Goals and plans [Schank and Abelson, 1977].", "startOffset": 28, "endOffset": 54}, {"referenceID": 8, "context": "Interpersonal Relations Interpersonal relations [Heider, 1958].", "startOffset": 48, "endOffset": 62}, {"referenceID": 16, "context": "Episodic Memory Episodic memory [Tulving, 1983; Hasselmo, 2012].", "startOffset": 32, "endOffset": 63}, {"referenceID": 7, "context": "Episodic Memory Episodic memory [Tulving, 1983; Hasselmo, 2012].", "startOffset": 32, "endOffset": 63}, {"referenceID": 6, "context": "Planning Planning or generating a sequence of actions to achieve a goal [Ghallab et al., 2004].", "startOffset": 72, "endOffset": 94}, {"referenceID": 10, "context": "The Highlights editors present the BrainPlay questions in increasing order of difficulty [Myers and Myers, 1964], so these correlations give a rough idea of difficulty.", "startOffset": 89, "endOffset": 112}, {"referenceID": 5, "context": "We can use the major sections of the fifth edition of The Cognitive Neurosciences [Gazzaniga and Mangun, 2014] as a guide to the many areas of human intelligence.", "startOffset": 82, "endOffset": 110}, {"referenceID": 1, "context": "In Aristo [Clark, 2015], a multiple choice elementary school science exam question is taken as input, and an answer is produced as output.", "startOffset": 10, "endOffset": 23}, {"referenceID": 18, "context": "In the bAbI tasks [Weston et al., 2015], a simple story and question about the story are taken as input, and an answer is produced as output.", "startOffset": 18, "endOffset": 39}, {"referenceID": 2, "context": "In the recognizing textual entailment (RTE) task [Dagan et al., 2013], a text T and a hypothesis H are taken as input, and a label T entails H , H contradicts T , or unknown is produced as output.", "startOffset": 49, "endOffset": 69}, {"referenceID": 0, "context": "In the VQA task [Antol et al., 2015], an image and a multiple choice or open-ended question about the image are taken as input, and an answer is produced as output.", "startOffset": 16, "endOffset": 36}, {"referenceID": 12, "context": "At the Center for Brains, Minds and Machines, the Turing questions on images [Poggio and Meyers, 2016] will be used to evaluate not only a system\u2019s responses to questions, but also how accurately the system matches human behavior and neural physiology.", "startOffset": 77, "endOffset": 102}], "year": 2017, "abstractText": "We propose to use thought-provoking children\u2019s questions (TPCQs), namely Highlights BrainPlay questions, as a new method to drive artificial intelligence research and to evaluate the capabilities of general-purpose AI systems. These questions are designed to stimulate thought and learning in children, and they can be used to do the same thing in AI systems, while demonstrating the system\u2019s reasoning capabilities to the evaluator. We introduce the TPCQ task, which which takes a TPCQ question as input and produces as output (1) answers to the question and (2) learned generalizations. We discuss how BrainPlay questions stimulate learning. We analyze 244 BrainPlay questions, and we report statistics on question type, question class, answer cardinality, answer class, types of knowledge needed, and types of reasoning needed. We find that BrainPlay questions span many aspects of intelligence. Because the answers to BrainPlay questions and the generalizations learned from them are often highly open-ended, we suggest using human judges for evaluation.", "creator": "LaTeX with hyperref package"}}}