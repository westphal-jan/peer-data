{"id": "1508.07551", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "30-Aug-2015", "title": "X-TREPAN: a multi class regression and adapted extraction of comprehensible decision tree in artificial neural networks", "abstract": "In this work, the TREPAN algorithm is enhanced and extended for extracting decision trees from neural networks. We empirically evaluated the performance of the algorithm on a set of databases from real world events. This benchmark enhancement was achieved by adapting Single-test TREPAN and C4.5 decision tree induction algorithms to analyze the datasets. The models are then compared with X-TREPAN for comprehensibility and classification accuracy. Furthermore, we validate the experimentations by applying statistical methods. Finally, the modified algorithm is extended to work with multi-class regression problems and the ability to comprehend generalized feed forward networks is achieved.", "histories": [["v1", "Sun, 30 Aug 2015 10:14:48 GMT  (415kb)", "http://arxiv.org/abs/1508.07551v1", "17 Pages, 8 Tables, 8 Figures, 6 Equations"]], "COMMENTS": "17 Pages, 8 Tables, 8 Figures, 6 Equations", "reviews": [], "SUBJECTS": "cs.LG cs.NE", "authors": ["awudu karim", "shangbo zhou"], "accepted": false, "id": "1508.07551"}, "pdf": {"name": "1508.07551.pdf", "metadata": {"source": "META", "title": "X-TREPAN", "authors": ["Awudu Karim", "Shangbo Zhou"], "emails": ["awudubody@yahoo.com"], "sections": [{"heading": null, "text": "KEYWORDS: Neural Network, Feed Forward, Decision Tree, Extraction, Classification, Comprehensibility This work expands and expands the TREPAN algorithm to extract decision trees from neural networks. Empirically, the performance of the algorithm was evaluated using a series of real-world databases. This benchmark improvement was achieved by adapting individual TREPAN and C4.5 decision tree induction algorithms to analyze the data sets. Subsequently, the models are compared with X-TREPAN to ensure comprehensibility and classification accuracy. Furthermore, we validate the experiments using statistical methods. Finally, the modified algorithm is extended to work with regression problems of multiple classes and the ability to capture generalized feed networks. KEYWORDS: Neural Network, Feed Forward, Decision Tree, Extraction, Classification, Comprehensibility."}, {"heading": "1. INTRODUCTION", "text": "Artificial neural networks are modelled on the architecture of the human brain, providing a means of efficiently modelling large and complex problems where there are hundreds of independent variables that have many interactions. Neural networks generate their own implicit rules by learning from examples. Artificial neural networks have been applied to a wide range of problem areas [1], such as medical diagnostics [2], games [3], robotics [4], speech generation [5] and speech recognition [6]. The generalisation capability of neural networks has proved superior to other learning systems across a wide range of applications [7]. Despite their relative success, the continued adoption of neural networks in some areas may be hampered by their inability to explain in an understandable way how a decision was made. This lack of transparency in neural network reasoning has been referred to as a \"black box problem.\""}, {"heading": "2. BACKGROUND AND LITERATURE REVIEW", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "2.1 Artificial Neural Network", "text": "Artificial neural networks, as the name suggests, are modelled on the architecture of the human brain and provide a means of efficiently modelling large and complex problems, where there can be hundreds of independent variables with many interactions. Neural networks learn from examples by generating their own implicit rules."}, {"heading": "2.2 Neural Network Architecture", "text": "A neural network consists of a large number of units, referred to as processing elements or nodes or neurons connected to each other on a parallel scale; the network begins with an input layer, each node corresponding to an independent variable; input nodes are connected to a number of nodes in a hidden layer; there can be more than one hidden layer and one output layer; each node in the hidden layer receives a series of inputs (X1, X2,..., Xm), multiplies them by a connection weight (W1, W2,..., Wm), then applies a function, f (WTX) to them, and passes the output to the nodes in the next layer. \u2212 The connection weights are the unknown parameters estimated by an iterative training method to indicate the strength and excitation of the connection; the final outputs of the network are calculated layer by layer [11]."}, {"heading": "2.3 Multilayer Perceptrons", "text": "Multilayer Perceptrons (MLP) are multi-layer feed-forward networks that are typically trained with back propagation, and have been used in numerous applications, their main advantage being that they are easy to use and can approach any input / output card. A major drawback is that they train slowly, require a lot of training data (typically three times more training samples than network weights) [14]. Figure 3. A schematic Multilayered Perceptron NetworkA Generalized Feed Forward (GFF) network is a special case of multi-layer perception where connections can jump over one or more layers. Although an MLP can solve any problem that a GFF can solve, a GFF network can solve the problem more efficiently in practice [14]. Figure 4 shows a general scheme of a generalized feed-forward network."}, {"heading": "2.4 Neural Networks for Classification and Regression", "text": "Neural networks are one of the most commonly used algorithms for classification problems. The output layer is indicative of the classifier's decision. Cross-entropy error function is most commonly used for classification problems in combination with logistical or soft-max activation functions. Cross-entropy assumes that the probability of predicate values in a classification problem is between 0 and 1. In a classification problem, each starting node of a neural network represents a different hypothesis and the node activations represent the probability that any hypothesis can be true. Each starting node represents a probability distribution and the cross-entropy measures calculate the difference between the network distribution and the actual distribution [15]. Assigning credit risks (good or bad) is an example of a neural network classification problem."}, {"heading": "2.5 Neural Network Training", "text": "The neural network approach is a two-stage process. In the first stage, a generalized network is derived that maps the input data to the desired output using a training algorithm. In the next stage, the network is tested for its generalization capability against a new dataset. Often, the neural network tends to train too much and store the data. To avoid this possibility, a cross-validation dataset is used. The cross-validation dataset is a part of the dataset that is set aside before training and used to determine the generalization level produced by the training set. As the training process progresses, the training error gradually decreases. First, the cross-validation error decreases, but then increases with the network over trains. The best generalization capability of the network can be tapped by stopping the algorithm in which the error on the cross-validation approach increases, but then increases with the network over trains."}, {"heading": "2.6 Rule Extraction from Neural Networks", "text": "Although neural networks are known as robust classifiers, they have limited use in decision-critical applications such as medical systems. Trained neural networks act as black boxes and are often difficult to interpret. [16] The availability of a system that would provide an explanation for the input / output mapping of a neural network in the form of rules would therefore be very useful. Rule extraction is such a system that attempts to explain to the user how the neural network arrived at its decision in the form of ifs-then-rules. So far, two explicit approaches have been defined to transform the knowledge contained in a neural network and the weights contained in it into a set of symbolic, decompositional and pedagogical rules. [17] In the de-compositional approach, the emphasis is on extracting rules at the individual hidden and / or output level into a binary result. It is about analyzing the weight factors and bipartition associations in general."}, {"heading": "2.7 Decision Trees", "text": "A decision tree is a special graph type drawn in the form of a tree structure. It consists of internal nodes, each associated with a logical test and its possible consequences. Decision trees are probably the most commonly used symbolic learning algorithms, as are neural networks in the non-symbolic category."}, {"heading": "2.8 Decision Tree Classification", "text": "Decision trees classify data by recursively dividing the data set into mutually exclusive subsets that best explain the variation in the dependent variable under observation [22]. Decision trees classify instances (data points) by sorting them from the root node to a leaf node. This lead node specifies the classification of the instance. Each branch of the decision tree represents a possible decision scenario and its output. Decision tree algorithms represent concept descriptions in the form of a tree structure. They begin with a series of instances to learn and generate a tree structure that is used to classify new instances. An instance in a data set is described by a set of characteristic values called attributes that can have either continuous or nominal values. Decision tree induction is best suited for data where each example in the data set is described by a fixed number of attributes node for all examples of this data set using a video tree approach and a decision di- method."}, {"heading": "3. TREPAN ALGORITHM", "text": "The TREPAN [24] and [25] algorithms developed by Craven et al are novel rule extraction algorithms that mimic the behavior of a neural network. Faced with a trained neural network, TREPAN extracts decision trees that offer a close approximation to the function represented by the network. This work also explores its application to a variety of learned models. TREPAN uses a concept of recursive partitioning that is similar to other decision tree induction algorithms. In contrast to the depth growth used by other decision tree algorithms, TREPAN expands with the best first principle, making the node that increases tree fidelity as it expands the best.In conventional decision tree induction algorithms, the amount of training data decreases as the tree is traversed downwards by selecting splitting tests."}, {"heading": "3.1 M-of-N Splitting tests", "text": "TREPAN uses the m-of-n test to partition the portion of the instance space covered by a particular internal node. An m-of-n expression (a Boolean expression) is fulfilled if at least one integer threshold m of its n literals matches. For example, consider four features a, b, c, and d; the m-of-n test: 3-of- {a, b > 3,3, c, d} on a node means that if one of the 3 conditions of the given set of 4 is met, then an example will pass through that node. TREPAN uses a bar search method with the bar width as the custom parameter to find the best m-of-n test. Bar search is heuristic best-first of any algorithm evaluating that first n node (where n is a fixed value designated as the \"bar width\") at each tree depth, and then selects the best of them for both the local tree division and the following TREPAN is used."}, {"heading": "3.1 Single Test TREPAN and Disjunctive TREPAN", "text": "In addition to the TREPAN algorithm, Craven has also developed two of its most important variants: the TREPAN algorithm for single tests is similar to the TREPAN algorithm in all respects, except that, as the name suggests, it uses individual function tests on the internal node. Disjunctive TREPAN, on the other hand, uses disjunctive \"OR\" tests on the internal node of the tree instead of m-of-n tests. A more detailed explanation of the TREPAN algorithm can be found in Craven's dissertation [27]. Baesens et al [28] have applied TREPAN to credit risk assessment and reported that it provides very good classification accuracy compared to the logistic regression classifier and the popular C4.5 algorithm."}, {"heading": "4. C4.5 ALGORITHM", "text": "The C4.5 algorithm [29] is one of the most commonly used decision tree learning algorithms. It is an advanced and incremental software extension of the basic ID3 algorithm [30], which is designed to address problems not addressed by ID3. The C4.5 algorithm derived from these instances originates in Hunt's Concept Learning Systems (CLS) [31]. It is a non-incremental algorithm, which means that it derives its classes from an initial series of training instances. Classes derived from these instances are expected to work for all future test instances. The algorithm uses the greedy search approach to select the best attribute, and never thinks back to reconsider previous decisions. The C4.5 algorithm searches the attributes of the training instances and finds the attribute that best separates the data from the other attribute (if this attribute remains perfectly classified in the subset)."}, {"heading": "4.1 Information Gain, Entropy Measure and Gain Ratio", "text": "This year it is more than ever before."}, {"heading": "5. EXPERIMENTATION AND RESULT ANALYSIS", "text": "We analyze three sets of data with classes greater than two and compare the results of the single test TREPAN and C4.5 with those of X-TREPAN in terms of comprehensibility and classification accuracy. To test the ability of XTREPAN to understand GFF networks, a general feed-forward network was trained, and the traditional command \"using-network\" was used to confirm that X-TREPAN provides correct results for the network. In all experiments, we used the single-test TREPAN as the best variant for comparison with the new model."}, {"heading": "5.1 Body Fat", "text": "Body Fat is a regression problem in the category of simple machine learning data sets, attempting to predict body fat percentage by body characteristics. A 14-4-1 MLP with hyperbolic tangent function was used to train the network for 1500 epochs, achieving an r (correlation coefficient) of 0.9882. Figure 6 shows the comparison of the classification accuracy of body fat using the three models. Eq.5Eq.6TREPAN achieves a classification accuracy of 94% and C4.5 a classification accuracy of 91%, while X-TREPAN achieves a comparatively much higher accuracy of 96%. Furthermore, both X-TREPAN and TREPAN generate similar trees in terms of size, but accuracy and comprehensibility achieved by X-TREPAN are comparatively higher. The tables below show the confusion matrix of classification accuracy achieved by comparing TREPAN to TREPAN."}, {"heading": "5.2 Outages", "text": "Failures represent a database of the category of small datasets. A12-3-1 MLP network with hyperbolic tangents and bias-axon transfer functions in the first and second hidden layer respectively provided the best accuracy. The model was designed for 12,000 epochs and achieved an r (correlation coefficient) value of 0.985 (or an r2 of (0.985) 2). Figure 7 shows the comparison of the classification accuracy of failures by the three algorithms. In terms of classification accuracy, TREPAN reaches 84%, C4.5 91%, while X-TREPAN reaches 92%. However, here TREPAN, C4.5, and X-TREPAN all generate very different trees in terms of size, with C4.5 producing the largest and most complex decision tree, while X-TREPAN produces the simplest and smallest decision structure with comparatively higher accuracy and comprehensibility."}, {"heading": "5.3 Admissions", "text": "A typical university admissions database model based on a 22-15-10-2 MLP network, using two hidden layers with hyperbolic tangent transfer functions, the best model was achieved by X-TREPAN with a minimum sample size of 1000, a tree size of 50 and a classification accuracy of 74%. Figure 8 shows the comparison of the classification accuracy of the approvals by the three models. On the other hand, TREPAN achieved an accuracy of 71.97% (not rounded), which was close to that of TREPAN of 72%, but produced a significantly large and complex decision tree. In terms of the confusion matrix, TREPAN achieved an accuracy of 71.6%, which is very close to that of XTREPAN. The confusion matrix is represented in the following Tables.Yes 379 190 no. 259 774Classification accuracy (%) 59.40% 80.867% (total accuracy) 1.2967%"}, {"heading": "6. PERFORMANCE ASSESSMENT", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "6.1 Classification Accuracy", "text": "A confusion matrix is a matrix diagram of predicted versus actual classes with all correct classifications displayed along the diagonal of the matrix. It indicates the number of correctly classified instances, incorrectly classified instances, and general classification accuracy. The accuracy of the classifier is given by the formula () () 100) (% + + + + + = TNFPFNTP AccuracyWhere true positive = (TP), true negative = (TN) false positive = (FP) and false negative = (FN). A false positive (FP) is when a negative instance is incorrectly classified as positive and false negative (FN) when a positive instance is classified as negative."}, {"heading": "6.2 Comprehensibility", "text": "The comprehensibility of the tree structure decreases with increasing size and complexity. The principle of Occam's Razors states: \"If you have two competing theories that make exactly the same predictions, the simpler the better.\" [33] Therefore, X-TREPAN is better among the three algorithms, as it produces smaller and simpler trees than single test TREPAN and C4.5 in most scenarios."}, {"heading": "7. CONCLUSION", "text": "The TREPAN algorithm code has been modified (X-TREPAN) to deal with multi-level regression problems. Various experiments have been conducted to test its compatibility with generalized feed networks. Weights and network files have been restructured to represent GFF networks in a format recognized by X-TREPAN. Neural network models have been trained on each data set with different parameters such as network architecture and transfer functions. Weights and distortions derived from the trained models of the three data sets have been transmitted to X-TREPAN to learn from neural networks. To evaluate performance, the classification accuracy of single-test TREPAN, C4.5 and X-TREPAN have been compared. In the scenarios discussed in the paper, X-TREPAN model performed significantly better than the single-test TREPAN and C4.5 algorithms in terms of classification accuracy and decision validity, but also in terms of the accuracy of decision validation and decision validation results."}], "references": [{"title": "Artificial Neural Networks for the Diagnosis of Aggressive Periodontitis Trained by Immunologic Parameters", "author": ["G. Papantonopoulos", "K. Takahashi", "T. Bountis", "B.G. Loos"], "venue": "PLoS ONE 9(3):", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2014}, {"title": "Behavior Analysis through Games Using Artificial Neural Networks", "author": ["D. Puzenat"], "venue": "Third International Conference on Advances in Computer-Human Interactions (IEEE),", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2010}, {"title": "An Artificial Neural Network Based Robot Controller that Uses Rat\u2019s", "author": ["M. Mano", "G. Capi", "N. Tanaka", "S. Kawahara"], "venue": "Brain Signals, Robotics,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2020}, {"title": "Prahallad, speech generation", "author": ["E.V. Raghavendra", "K.P. Vijayaditya"], "venue": "National Conference on Communications (NCC),", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2010}, {"title": "Implementation Of Back-Propagation Neural Network For Isolated Bangla Speech Recognition, International Journal of Information Sciences and Techniques", "author": ["A. Hossain", "M. Rahman", "U.K.Prodhan", "F. Khan"], "venue": "(IJIST) vol", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2013}, {"title": "Comparison between Artificial Neural Network Learning Algorithms for Prediction of Student Average considering Effective Factors in Learning and Educational Progress", "author": ["S. Ayat", "Z.A. Pour"], "venue": "Journal of mathematics and computer science", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2014}, {"title": "The truth will come to light: Directions and challenges in extracting the knowledge embedded within trained artificial neural networks", "author": ["A.B. Ticke", "R. Andrews", "M. Golea", "J. Diederich"], "venue": "IEEE Trans. Neural Network,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 1998}, {"title": "Lessons from past current issues and future research directions in extracting knowledge embedded in artificial neural networks in Hybrid", "author": ["A. Ticke", "F. Maire", "G. Bologna", "R. Andrews", "J. Diederich"], "venue": "Neural Systems,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2000}, {"title": "Teach/Me Data Analysis, Springer-Verlag, Berlin-New York-Tokyo", "author": ["H. Lohminger"], "venue": null, "citeRegEx": "11", "shortCiteRegEx": "11", "year": 1999}, {"title": "Exploratory analysis of metallurgical process data with neural networks and related methods", "author": ["C. Aldrich"], "venue": null, "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2002}, {"title": "Exercises in Rethinking Innateness: A handbook for connectionist simulations", "author": ["K. Plunkeet", "J.L. Elman"], "venue": null, "citeRegEx": "15", "shortCiteRegEx": "15", "year": 1997}, {"title": "Shavli, Extracting Refined Rules from Knowledge-based Neural Networks", "author": ["J.W.G.G. Towell"], "venue": "Machine Learning,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 1993}, {"title": "Using sampling and queries to extract rules from trained neural networks, Machine Learning", "author": ["M.W Craven", "J.W. Shavlik"], "venue": "Proceedings of the Eleventh International Conference,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 1995}, {"title": "Rule Learning by searching on adopted nets", "author": ["L. Fu"], "venue": "In Proceedings of the 9th National Conference on Artificial Intelligence, Anaheim, CA,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 1991}, {"title": "Extracting rules from artificial neural networks with distributed representations, In Advances in Neural Information Processing Systems 7, Cambridge, MA:Mit press, pp", "author": ["S. Thrums"], "venue": null, "citeRegEx": "19", "shortCiteRegEx": "19", "year": 1995}, {"title": "Extracting Comprehensible models from trained Neural Network, PhD Thesis, Computer Science Department", "author": ["M.W. Craven"], "venue": null, "citeRegEx": "20", "shortCiteRegEx": "20", "year": 1996}, {"title": "A survey and critique of techniques for extracting rules from trained neural networks", "author": ["R. Andrews", "J. Diederich", "A.B. Tickle"], "venue": "Knowledge Based Systems,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 1995}, {"title": "A method of choosing multiway partitions for classification and decision tree", "author": ["D. Biggs", "B. de Ville", "E. Suen"], "venue": "Journal of Applied Statistics", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 1991}, {"title": "Machine Learning techniques for natural resource data analysis", "author": ["G. Liepins", "R. Goeltz", "R. Rush"], "venue": "AI Applications 4(3):9-18,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 1990}, {"title": "Using sampling and queries to extract rules from trained neural networks, Machine Learning", "author": ["M.W. Craven", "J.W. Shavlik"], "venue": "Proceedings of the Eleventh Inter-national Conference,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 1995}, {"title": "Extracting tree-structured representations of trained networks", "author": ["M.W. Craven", "J.W. Shavlik"], "venue": "In Advance in Neural Information Procession Systems,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 1996}, {"title": "Learning accurate and understandable rules from SVM classifiers, M", "author": ["F. Chen"], "venue": "Sc. Thesis, School of computing science,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2004}, {"title": "Extracting Comprehensible models from trained Neural Network", "author": ["M.W. Craven"], "venue": "PhD Thesis, Computer Science Department,", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 1996}, {"title": "Using Neural Network Rule Extraction and Decision Tables for Credit-Risk Evaluation management", "author": ["B. Baesens", "R. Setiono", "C. Mues", "J. Vanthienen"], "venue": "Science, vol. 49,", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2003}, {"title": "Programs for Machine Learning", "author": ["J.R. Quinlan"], "venue": null, "citeRegEx": "29", "shortCiteRegEx": "29", "year": 1993}, {"title": "ID3 Algorithm, Machine Learning, University of Sydney, book", "author": ["J. Ross Quinlan"], "venue": null, "citeRegEx": "30", "shortCiteRegEx": "30", "year": 1975}, {"title": "Classification and regression trees", "author": ["L.J. Breiman", "R. Friedman", "C. Olshen"], "venue": null, "citeRegEx": "32", "shortCiteRegEx": "32", "year": 1984}, {"title": "Classification and regression trees", "author": ["L. Breiman", "J. Friedman", "R. Olshen", "C. Stone"], "venue": null, "citeRegEx": "33", "shortCiteRegEx": "33", "year": 1984}], "referenceMentions": [{"referenceID": 0, "context": "Artificial neural networks have been applied to a variety of problem domains [1] such as medical diagnostics [2], games [3], robotics [4], speech generation [5] and speech recognition [6].", "startOffset": 109, "endOffset": 112}, {"referenceID": 1, "context": "Artificial neural networks have been applied to a variety of problem domains [1] such as medical diagnostics [2], games [3], robotics [4], speech generation [5] and speech recognition [6].", "startOffset": 120, "endOffset": 123}, {"referenceID": 2, "context": "Artificial neural networks have been applied to a variety of problem domains [1] such as medical diagnostics [2], games [3], robotics [4], speech generation [5] and speech recognition [6].", "startOffset": 134, "endOffset": 137}, {"referenceID": 3, "context": "Artificial neural networks have been applied to a variety of problem domains [1] such as medical diagnostics [2], games [3], robotics [4], speech generation [5] and speech recognition [6].", "startOffset": 157, "endOffset": 160}, {"referenceID": 4, "context": "Artificial neural networks have been applied to a variety of problem domains [1] such as medical diagnostics [2], games [3], robotics [4], speech generation [5] and speech recognition [6].", "startOffset": 184, "endOffset": 187}, {"referenceID": 5, "context": "The generalization ability of neural networks has proved to be superior to other learning systems over a wide range of applications [7].", "startOffset": 132, "endOffset": 135}, {"referenceID": 6, "context": "[9], [10] summarize several proposed approaches to rule extraction.", "startOffset": 0, "endOffset": 3}, {"referenceID": 7, "context": "[9], [10] summarize several proposed approaches to rule extraction.", "startOffset": 5, "endOffset": 9}, {"referenceID": 8, "context": "The calculation of the final outputs of the network proceeds layer by layer [11].", "startOffset": 76, "endOffset": 80}, {"referenceID": 9, "context": "Figure 1 shows the model of a single neuron [12]", "startOffset": 44, "endOffset": 48}, {"referenceID": 10, "context": "Each output node represents a probability distribution and the cross entropy measures calculate the difference between the network distribution and the actual distribution [15].", "startOffset": 172, "endOffset": 176}, {"referenceID": 11, "context": "Trained neural networks act like black boxes and are often difficult to interpret [16].", "startOffset": 82, "endOffset": 86}, {"referenceID": 12, "context": "Two explicit approaches have been defined to date for transforming the knowledge and weights contained in a neural network into a set of symbolic rules de-compositional and pedagogical [17].", "startOffset": 185, "endOffset": 189}, {"referenceID": 13, "context": "The subset [18] algorithm is an example of this category.", "startOffset": 11, "endOffset": 15}, {"referenceID": 14, "context": "The Validity Interval Analysis (VIA) [19] proposed by Thrum and TREPAN [20] is an example of one such technique .", "startOffset": 37, "endOffset": 41}, {"referenceID": 15, "context": "The Validity Interval Analysis (VIA) [19] proposed by Thrum and TREPAN [20] is an example of one such technique .", "startOffset": 71, "endOffset": 75}, {"referenceID": 16, "context": "Andrews et al [21] proposed a third category called eclectic which combines the elements of the basic categories.", "startOffset": 14, "endOffset": 18}, {"referenceID": 17, "context": "8 Decision Tree Classification Decision trees classify data through recursive partitioning of the data set into mutually exclusive subsets which best explain the variation in the dependent variable under observation[22][23] .", "startOffset": 215, "endOffset": 219}, {"referenceID": 18, "context": "8 Decision Tree Classification Decision trees classify data through recursive partitioning of the data set into mutually exclusive subsets which best explain the variation in the dependent variable under observation[22][23] .", "startOffset": 219, "endOffset": 223}, {"referenceID": 19, "context": "TREPAN ALGORITHM The TREPAN [24] and [25] algorithms developed by Craven et al are novel rule-extraction algorithms that mimic the behavior of a neural network.", "startOffset": 28, "endOffset": 32}, {"referenceID": 20, "context": "TREPAN ALGORITHM The TREPAN [24] and [25] algorithms developed by Craven et al are novel rule-extraction algorithms that mimic the behavior of a neural network.", "startOffset": 37, "endOffset": 41}, {"referenceID": 21, "context": "The following illustrates a pseudocode of the TREPAN algorithm [26].", "startOffset": 63, "endOffset": 67}, {"referenceID": 22, "context": "A more detailed explanation of the TREPAN algorithm can be found in Craven\u2019s dissertation [27].", "startOffset": 90, "endOffset": 94}, {"referenceID": 23, "context": "Baesens et al [28] have applied TREPAN to credit risk evaluation and reported that it yields very good classification accuracy as compared to the logistic regression classifier and the popular C4.", "startOffset": 14, "endOffset": 18}, {"referenceID": 24, "context": "5 algorithm [29] is one of the most widely used decision tree learning algorithms.", "startOffset": 12, "endOffset": 16}, {"referenceID": 25, "context": "It is an advanced and incremental software extension of the basic ID3 algorithm [30] designed to address the issues that were not dealt with by ID3.", "startOffset": 80, "endOffset": 84}, {"referenceID": 26, "context": "ID3 uses the concept of Information Gain which is based on Information theory [32] to select the best attributes.", "startOffset": 78, "endOffset": 82}, {"referenceID": 27, "context": "The principle of Occam\u2019s Razors says \u201cwhen you have two competing theories which make exactly the same projections, the one that is simpler is the better\u201d [33].", "startOffset": 155, "endOffset": 159}], "year": 2015, "abstractText": "In this work, the TREPAN algorithm is enhanced and extended for extracting decision trees from neural networks. We empirically evaluated the performance of the algorithm on a set of databases from real world events. This benchmark enhancement was achieved by adapting Single-test TREPAN and C4.5 decision tree induction algorithms to analyze the datasets. The models are then compared with X-TREPAN for comprehensibility and classification accuracy. Furthermore, we validate the experimentations by applying statistical methods. Finally, the modified algorithm is extended to work with multi-class regression problems and the ability to comprehend generalized feed forward networks is achieved.", "creator": "PDF24 Creator"}}}