{"id": "1509.00913", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "3-Sep-2015", "title": "On-the-Fly Learning in a Perpetual Learning Machine", "abstract": "Despite the promise of brain-inspired machine learning, deep neural networks (DNN) have frustratingly failed to bridge the deceptively large gap between learning and memory. Here, we introduce a Perpetual Learning Machine; a new type of DNN that is capable of brain-like dynamic 'on the fly' learning because it exists in a self-supervised state of Perpetual Stochastic Gradient Descent. Thus, we provide the means to unify learning and memory within a machine learning framework.", "histories": [["v1", "Thu, 3 Sep 2015 01:30:29 GMT  (473kb)", "http://arxiv.org/abs/1509.00913v1", null], ["v2", "Tue, 8 Sep 2015 07:54:41 GMT  (467kb)", "http://arxiv.org/abs/1509.00913v2", null], ["v3", "Tue, 29 Sep 2015 16:57:42 GMT  (590kb)", "http://arxiv.org/abs/1509.00913v3", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["andrew j r simpson"], "accepted": false, "id": "1509.00913"}, "pdf": {"name": "1509.00913.pdf", "metadata": {"source": "CRF", "title": "On-the-Fly Learning in a Perpetual Learning Machine", "authors": ["Andrew J.R. Simpson"], "emails": ["Andrew.Simpson@Surrey.ac.uk"], "sections": [{"heading": null, "text": "This year is the highest in the history of the country."}], "references": [{"title": "Long short-term memory", "author": ["S Hochreiter", "J Schmidhuber"], "venue": "Neural computation", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 1997}, {"title": "Gradient-based learning applied to document recognition", "author": ["Y LeCun", "L Bottou", "Y Bengio", "P Haffner"], "venue": "Proc. IEEE", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 1998}, {"title": "Abstract Learning via Demodulation in a Deep Neural Network\u201d, arxiv.org abs/1502.04042", "author": ["AJR Simpson"], "venue": null, "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2015}, {"title": "Dither is Better than Dropout for Regularising Deep Neural Networks\u201d, arxiv.org abs/1508.04826", "author": ["AJR Simpson"], "venue": null, "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2015}, {"title": "Parallel Dither and Dropout for Regularising Deep Neural Networks\u201d, arxiv.org abs/1508.07130", "author": ["AJR Simpson"], "venue": null, "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2015}, {"title": "Improving neural networks by preventing co-adaptation of feature detectors\u201d, The Computing Research Repository (CoRR), abs/1207.0580", "author": ["GE Hinton", "N Srivastava", "A Krizhevsky", "I Sutskever", "R Salakhutdinov"], "venue": null, "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2012}], "referenceMentions": [{"referenceID": 1, "context": "We chose the well-known MNIST hand-written digit dataset [2].", "startOffset": 57, "endOffset": 60}, {"referenceID": 2, "context": "Both DNNs used biased sigmoids [3] throughout (with zero bias in the output layer).", "startOffset": 31, "endOffset": 34}, {"referenceID": 3, "context": "The storage and recall DNNs were trained, independently, using only the first 50 images for 100 full-sweep iterations of typical non-batch SGD [4,5].", "startOffset": 143, "endOffset": 148}, {"referenceID": 4, "context": "The storage and recall DNNs were trained, independently, using only the first 50 images for 100 full-sweep iterations of typical non-batch SGD [4,5].", "startOffset": 143, "endOffset": 148}, {"referenceID": 4, "context": "This synthetic image was then combined with the random class and used together to train both DNNs in parallel (via non-batch SGD [5]).", "startOffset": 129, "endOffset": 132}, {"referenceID": 3, "context": "As in [4,5], all dither was random noise of zero mean and unit scale and dropout [6,5] was 50%.", "startOffset": 6, "endOffset": 11}, {"referenceID": 4, "context": "As in [4,5], all dither was random noise of zero mean and unit scale and dropout [6,5] was 50%.", "startOffset": 6, "endOffset": 11}, {"referenceID": 5, "context": "As in [4,5], all dither was random noise of zero mean and unit scale and dropout [6,5] was 50%.", "startOffset": 81, "endOffset": 86}, {"referenceID": 4, "context": "As in [4,5], all dither was random noise of zero mean and unit scale and dropout [6,5] was 50%.", "startOffset": 81, "endOffset": 86}, {"referenceID": 4, "context": "We note that (data not shown) no aspect of the current model trains successfully without regularisation via parallel dither (w/ dropout) [5].", "startOffset": 137, "endOffset": 140}, {"referenceID": 0, "context": ", LSTM [1]), this new class of Perpetual Learning Machine is able to store, retain, recall and add memories in perpetuity.", "startOffset": 7, "endOffset": 10}, {"referenceID": 0, "context": ", [1]), the present unified architecture represents both learning and memory through the states of its weights.", "startOffset": 2, "endOffset": 5}], "year": 2015, "abstractText": "Despite the promise of brain-inspired machine learning, deep neural networks (DNN) have frustratingly failed to bridge the deceptively large gap between learning and memory. Here, we introduce a Perpetual Learning Machine; a new type of DNN that is capable of brain-like dynamic \u2018on the fly\u2019 learning because it exists in a self-supervised state of Perpetual Stochastic Gradient Descent. Thus, we provide the means to unify learning and memory within a machine learning framework.", "creator": "PDFCreator Version 1.7.1"}}}