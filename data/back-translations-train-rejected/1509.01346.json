{"id": "1509.01346", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "4-Sep-2015", "title": "Deep Broad Learning - Big Models for Big Data", "abstract": "Deep learning has demonstrated the power of detailed modeling of complex high-order (multivariate) interactions in data. For some learning tasks there is power in learning models that are not only Deep but also Broad. By Broad, we mean models that incorporate evidence from large numbers of features. This is of especial value in applications where many different features and combinations of features all carry small amounts of information about the class. The most accurate models will integrate all that information. In this paper, we propose an algorithm for Deep Broad Learning called DBL. The proposed algorithm has a tunable parameter $n$, that specifies the depth of the model. It provides straightforward paths towards out-of-core learning for large data. We demonstrate that DBL learns models from large quantities of data with accuracy that is highly competitive with the state-of-the-art.", "histories": [["v1", "Fri, 4 Sep 2015 06:01:11 GMT  (3259kb,D)", "http://arxiv.org/abs/1509.01346v1", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["nayyar a zaidi", "geoffrey i webb", "mark j carman", "francois petitjean"], "accepted": false, "id": "1509.01346"}, "pdf": {"name": "1509.01346.pdf", "metadata": {"source": "CRF", "title": "Deep Broad Learning - Big Models for Big Data", "authors": ["Nayyar A. Zaidi", "Geoffrey I. Webb", "Mark J. Carman", "Francois Petitjean"], "emails": ["nayyar.zaidi@monash.edu", "geoff.webb@monash.edu", "mark.carman@monash.edu", "francois.petitjean@monash.edu"], "sections": [{"heading": null, "text": "Keywords: Classification, Big Data, Deep Learning, Broad Learning, DiscriminativeGenerative Learning, Logistic Regression, Extended Logistic Regression"}, {"heading": "1. Introduction", "text": "The rapid growth of the amount of data (Ganz und Reinsel, 2012) makes it increasingly difficult for machine learning to extract the maximum value from current data repositories. Most state-of-the-art learning algorithms have been developed in the context of small datasets, but the amount of information present in big data is typically much greater than the amount present in small data sets. As a result, big data can support the creation of very detailed models that encode complex higher-class distributions. We know that the error of most classifiers decreases as they are supplied with more data. This can be observed in Figure 1, where the variation in the error rate of two classifiers is shown, with increasing quantity of training data (Frank and Asuncion, 2010)."}, {"heading": "2. Notation", "text": "We try to assign to a given example x = (x1,.., xa) a value Y = {y1,.. yC} of the class variables Y, where the xi are value assignments for the a attributes A = {X1,..., Xa}. We define (A n) as the set of all subsets of the A-size n, each subset in the set denoted2. logistical regression taking into account all n-level attributes is denoted by LRn, e.g. LR2, LR3, LR4, etc. taking into account all square, cubic, quartile, etc. attributes as \u03b1: (A n) = {\u03b1 A: | \u03b1 | = n}. We use x\u03b1 to denote the set of values determined by attributes in the subset \u03b1 for each data object x.LR for categorical data."}, {"heading": "3. Using generative models to precondition discriminative learning", "text": "There is a direct equivalent between a weighted NB and a LR (Zaidi et al., 2013, 2014). We write LR for categorical features such as: PLR (y | x) = exp (\u03b2y + a \u2211 i = 1 \u03b2y, i, xi \u2212 log \u2211 c, Y exp (\u03b2c + a \u2211 j = 1 \u03b2c, j, xj)) (1) and NB as: PNB (y | x) = P (y), i = 1 P (xi | y). You can add the weights to NB in order to mitigate the attribute independence assumption, which leads to the WANBIA-C formulation, which can be written as follows: PW (y | x) = P (y) wy (wy) wy (wy) wy (wy) wy (i = 1 P (xi | y) wy, i wc) wy, i wy, i wy, i, xi \u00b2 wc (c) wc (c) wc (c), wc = 1 P (c), j c (c, j c, j c (c), j c (c, j c, j c, j c (c), j c (c), j c (c c (c), j c (c c c), j c (c c c (c), j c (c c), j c (c (c) c (c wy c, j c (c) wy c (c, j c, j c, j c (c), j c (c (c), j c (c, j c (c) c (c, j c (c) c, j c (c) c (c, j c (c) c (j c (c) c, j c (c (c) c, j c (c) c (c, j c (c) c (c, j c (c) c (c, j c (c) c (c (c) c, j c (c) c (c, j c (c) c (c) c, j c (c) c (c, j c (c (c) c (c) c (c) c, j c"}, {"heading": "4. Deep Broad Learner (DBL)", "text": "In order to create an efficient and effective learner with a low bias, we want to apply the same trick used by WANBIA-C for LR with categorical features of higher order. We define LRn as: PLRn (y | x) = exp (\u03b2y + \u2211 \u03b1 (An) \u03b2y, \u03b1, x\u03b1) \u2211 c-Y-exp (\u03b2c + \u03bd (An) \u03b2c, \u03b1, x\u03b1). (3) For example, if n = 2 does not include the terms \u03b2y, i, xi and \u03b2y, i, j, xj, because it does not increase the space of different distributions that can be modelled, but increases the number of parameters that need to be optimized. To advance this model using generative learning, we need a generative model of the form P (y | x) = P (y)."}, {"heading": "4.1 Averaged n-Join Estimators (AnJE)", "text": "Let P be a division of the attributes A. By assuming independence only between the sets of attributes A-P, we obtain an n-common estimator: PAnJE (x-y) = 2-4. For example, if there are four attributes X1, X2, X3, and X4 divided into sentences X1, X2, and X4, then by assuming a conditional independence between the sentences we obtain PAnJE (x1, x2, x4, y) = P (x1, x2, x4, y), let the division of all divisions of A be such that we obtain PAnJE (x1, x2, x4, y) = n. For convenience, let us assume that a multiple is a multiple of n. Let us be a division of YES that includes each set of n attributes."}, {"heading": "4.3 Alternative Parameterization", "text": "Let us fix DBLn in such a way that: \u03b2y = wy log \u03c0y, and \u03b2y, \u03b1, x\u03b1 = wy, \u03b1, x\u03b1 log \u03b8x\u03b1 | y. (14) Now we can rewrite Equation 11 as follows: Log PLR (y) = \u03b2y + \u2211 \u03b1. (An) \u03b2y, \u03b1, x\u03b1 \u2212 Log, c).Y exp (An) \u03b2c, \u03b1. \"However, it can be seen that this leads to Equation 3. We call these parameters LRn. Like DBLn, LRn also leads to a convex optimization problem, and therefore its parameters can be optimized by simple gradients of decent based algorithms. Let us calculate the gradients of the objective function in Equation 15 with respect to \u03b2y."}, {"heading": "5. Related Work", "text": "In fact, most of us will be able to abide by the rules we have set ourselves to make them a reality, \"he said in an interview with the Deutsche Presse-Agentur.\" I am very satisfied, \"he added,\" but it is too early to say that we will be able to, that we will be able to change the rules. \""}, {"heading": "6. Experiments", "text": "In this section, we compare and analyze the performance of our proposed algorithms and related methods in relation to 77 natural domains from the UCI repository of machine learning (Frank and Asuncion, 2010). Experiments are performed on the datasets described in the table. There are a total of 77 datasets with less than 1000 instances, 21 datasets each with more than 1000 datasets, and 16 datasets with more than 10,000 instances. There are 8 datasets shown in bold font. Each algorithm is tested in each dataset, using 5 rounds of 2x cross-validation. We compare four different metrics to Loss, RMSE, Bias and Variance5 variations. We report on the Win-Draw-Loss-Loss-Loss-Loss-Loss-Loss-Loss-Loss-Loss-Loss-Loss variations."}, {"heading": "No. of Iterations", "text": "10 0 10 1 10 2 10 3 10 4 10 5N eg ati ve Lo g-L ike lih oo d # 10 4-10-8-6-4-202468 Census-incomeDBL 2 LR2No. of Iterations10 0 10 1 10 2 10 3 10 4N eg ati ve Lo g-L ike lih oo d # 10 5-6-5-4-3-2-10 CovtypeDBL 2 LR2No. of Iterations10 010 110 2N eg ati ve Lo g-L ike lih oo d # 10 4-3,5-3-2,5-2-1,5-1-0.50 Letter-recogDBL 2 LR 2No. of Iterations10 010 110 210-L ike lih oo d # 10 4-18-16-12-10-8-4-20 LocalizationDBL 2 LR 2Nr."}, {"heading": "No. of Iterations", "text": "10 0 10 1 10 2N eg ati ve Lo g-L ike lih oo d-7000-6000-5000-4000-3000-2000-10000 OptdigitsDBL 2 LR2No. of Iterations10 010 110 2N eg ati ve Lo g-L ike lih oo d-14000-12000-10000-8000-6000-4000-20000 PendigitsDBL 2 LR 2No. of Iterations10 010 110 2N eg ati ve Lo g-L ike lih oo d-4500-4000-3500-3000-2500-2000-1500-1000-5000 SideblockDBL 2 LR 2No. of Iterations10 010 110 210 3N eg ati ve Lo g-L ike lih oo-10-8-6-4-20 Poker-handDBL 2 LR 2No. of Iterations10 0 10 110 210 3N eg ati ve Lo g-L ik-10-8-4-4000-4000-BB3000-2000-23000-LN"}, {"heading": "No. of Iterations", "text": "It can be seen that the improvement DBLn offers over LRn gets better the deeper we go to deeper structures, i.e. the bigger n becomes. Similar behavior has been observed in several datasets, and although we studied the rates10 010 110 210 310 4N, e.g. ati ve Lo g-L ike lih oo d # 10 4-1.5-1-0.500.511.52 AdultDBL 3 LR 3"}, {"heading": "No. of Iterations", "text": "10 0 10 1 10 2N eg ati ve Lo g-L ike lih oo d # 10 5-101234 Census-incomeDBL 3 LR3No. of Iterations10 0 10 1 10 2 10 3N eg ati ve Lo g-L ike lih oo d # 10 5-6-5-4-3-2-10 CovtypeDBL 3 LR3No. of Iterations10 010 110 2N eg ati ve Lo g-L ike lih oo d # 10 5-2-1.5-1-0.50 LocalizationDBL 3 LR 3No. of Iterations10 110 210 3N eg ati ve Lo g-L ike lih oo d # 10 5-2-1.5-1-0.50 LocalizationDBL 3 LR 3No. of Iterations10 110 210 210 210 3N eg ati ve Lo g-L ike lih oo d # 10 5-2-1.5-1-0.50 Localization10-2000000 BL 2000000 B00000-20001.5-2000L i003 BB3 210 310 210 210 3N eg ati ve Lo g-L 2000000"}, {"heading": "No. of Iterations", "text": "10 0 10 1 10 2N eg ati ve Lo g-L ike lih oo d-7000-6000-5000-4000-3000-2000-10000 OptdigitsDBL 3 LR3No. of Iterations10 010 110 2N eg ati ve Lo g-L ike lih oo d-14000-12000-10000-8000-6000-4000-20000 PendigitsDBL 3 LR 3No. of Iterations10 010 110 2N eg ati ve Lo g-L ike lih oo-10-8-6-4-20 Poker-handDBL 3 LR 3No. of Iterations10 0 10 110 210 3N eg ati ve Lo g-L ike lih oo-10-6-4-4-20 Poker-handDBL 3 LR 3No. of Iterations10 0 10 110 210 3N eg ati ve Lo g-L ik-10-8-4-4000-3000-BB3000-3N"}, {"heading": "No. of Iterations", "text": "Since the issue of convergence is a complicated matter and is outside the scope of this work, we expect that this phenomenon will be an interesting place to study in the future."}, {"heading": "No. of Iterations", "text": "6.4 DBLn vs. Random ForestThe two DBLn models are compared in terms of W-D-L of 0-1 loss, RMSE, bias and variance with Random Forest in Table 4. On small datasets, it can be seen that DBLn has a significantly lower bias than RF. DBL3 variance is significantly higher than RF, while the difference in variance for DBL2 and RF is not significant. 0-1 loss results for DBLn and RF are similar. However, RF has better RMSE results than DBLn on small datasets. On large datasets, DBLn gains in terms of 0-1 loss and RMSE. The averaged 0-1 loss results and RMSE results are shown in Figure 18. It can be seen that DBL2, DBL3 and RF have similar 0-1 losses and RMSE in terms of small datasets."}, {"heading": "7. Conclusion and Future Work", "text": "DBL consists of parameters that are learned with both generative and discriminatory training. In order to obtain the generative parameterization for the DB, we first developed AnJE, a generative counterpart to the higher order regression. We have shown that DBLn and LRn can learn equivalent models, but DBLN is able to use the information gained to effectively drive the optimization process forward. DBLN converts in fewer iterations, which leads to faster training. We also compared DBLN with the equivalent AnJE and AnDE models and showed that DBLN has lower presets than AnJE and AnDE models. We have DBLN with the state of the art classification much faster, which leads to faster training time."}, {"heading": "8. Code and Datasets", "text": "Code with running instructions can be downloaded from https: / / www.dropbox.com / sh / iw33mgcku9m2quc / AABXwYewVtm0mVE6KoyMPEVFa? dl = 0."}, {"heading": "9. Acknowledgments", "text": "This research was supported by the Australian Research Council (ARC) under funding programmes DP140100087, DP120100553, DP140100087 and the Asian Office of Aerospace Research and Development, the Air Force Office of Scientific Research under contracts FA2386-12-1-4030, FA2386-15-1-4017 and FA2386-15-1-4007."}, {"heading": "Appendix A. Detailed Results", "text": "In this appendix we compare the 0-1 loss and RMSE results of DBLn, AnDE and RF. The aim is to assess the performance of each model on Big Datasets. Therefore, the results on 8 Big Datasets are only given in Tables 5 and 6 for 0-1 Loss and RMSE respectively. We also compare the results with AnJE. Note: A1JE is naive Bayes. DBL1 results are also compared. Note: DBL1 is WANBIA-C (Zaidi et al., 2013). The best results are presented in bold font."}], "references": [{"title": "Convex Optimization", "author": ["S Boyd", "L Vandenberghe"], "venue": null, "citeRegEx": "Boyd and Vandenberghe.,? \\Q2008\\E", "shortCiteRegEx": "Boyd and Vandenberghe.", "year": 2008}, {"title": "Structural extension to logistic regression: Discriminative", "author": ["ics.uci.edu/ml. J. Ganz", "D. Reinsel"], "venue": "The Digital Universe Study,", "citeRegEx": "Ganz and Reinsel.,? \\Q2012\\E", "shortCiteRegEx": "Ganz and Reinsel.", "year": 2012}, {"title": "Discriminative versus generative parameter and structure learning of bayesian network classifiers", "author": ["F. Pernkopf", "J. Bilmes"], "venue": "In ICML,", "citeRegEx": "Pernkopf and Bilmes.,? \\Q2005\\E", "shortCiteRegEx": "Pernkopf and Bilmes.", "year": 2005}, {"title": "On discriminative parameter learning of bayesian network classifiers", "author": ["F. Pernkopf", "M. Wohlmayr"], "venue": "In ECML PKDD,", "citeRegEx": "Pernkopf and Wohlmayr.,? \\Q2009\\E", "shortCiteRegEx": "Pernkopf and Wohlmayr.", "year": 2009}, {"title": "On discriminative Bayesian network classifiers and logistic regression", "author": ["T Roos", "H Wettig", "P Gr\u00fcnwald", "P Myllym\u00e4ki", "H Tirri"], "venue": "Machine Learning,", "citeRegEx": "Roos et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Roos et al\\.", "year": 2005}, {"title": "Learning limited dependence Bayesian classifiers", "author": ["M Sahami"], "venue": "In Proceedings of the Second International Conference on Knowledge Discovery and Data Mining,", "citeRegEx": "Sahami.,? \\Q1996\\E", "shortCiteRegEx": "Sahami.", "year": 1996}, {"title": "Discriminative parameter learning for bayesian networks", "author": ["J. Su", "H. Zhang", "C. Ling", "S. Matwin"], "venue": "In ICML,", "citeRegEx": "Su et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Su et al\\.", "year": 2008}, {"title": "Not so naive Bayes: Averaged one-dependence estimators", "author": ["G I. Webb", "J Boughton", "Z Wang"], "venue": "Machine Learning,", "citeRegEx": "Webb et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Webb et al\\.", "year": 2005}, {"title": "Learning by extrapolation from marginal to full-multivariate probability distributions: decreasingly naive Bayesian classification", "author": ["Geoffrey I. Webb", "Janice Boughton", "Fei Zheng", "Kai Ming Ting", "Houssam Salem"], "venue": "Machine Learning,", "citeRegEx": "Webb et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Webb et al\\.", "year": 2011}, {"title": "Fast and efficient single pass Bayesian learning", "author": ["N.A. Zaidi", "G.I. Webb"], "venue": "In Proceedings of the 17th Pacific-Asia Conference on Knowledge Discovery and Data Mining (PAKDD),", "citeRegEx": "Zaidi and Webb.,? \\Q2012\\E", "shortCiteRegEx": "Zaidi and Webb.", "year": 2012}, {"title": "Alleviating naive Bayes attribute independence assumption by attribute weighting", "author": ["N.A. Zaidi", "J. Cerquides", "M. J Carman", "G.I. Webb"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Zaidi et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Zaidi et al\\.", "year": 2013}, {"title": "Naive-bayes inspired effective pre-conditioners for speeding-up logistic regression", "author": ["N.A. Zaidi", "M. J Carman", "J. Cerquides", "G.I. Webb"], "venue": "In IEEE International Conference on Data Mining,", "citeRegEx": "Zaidi et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Zaidi et al\\.", "year": 2014}, {"title": "LBFGSB, fortran routines for large scale bound constrained optimization", "author": ["C. Zhu", "R.H. Byrd", "J. Nocedal"], "venue": "ACM Transactions on Mathematical Software,", "citeRegEx": "Zhu et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Zhu et al\\.", "year": 1997}], "referenceMentions": [{"referenceID": 1, "context": "Introduction The rapid growth in data quantity (Ganz and Reinsel, 2012) makes it increasingly difficult for machine learning to extract maximum value from current data stores.", "startOffset": 47, "endOffset": 71}, {"referenceID": 5, "context": "One is a low-bias high-variance learner (KDB k = 5, taking into account quintic features, (Sahami, 1996)) and the other is a low-variance high-bias learner (naive Bayes, a linear classifier).", "startOffset": 90, "endOffset": 104}, {"referenceID": 8, "context": ", 2007) and Averaged 2-Dependence Estimators (A2DE) (Webb et al., 2011; Zaidi and Webb, 2012), both of which consider all combinations of 3 variables, are both Deep and Broad.", "startOffset": 52, "endOffset": 93}, {"referenceID": 9, "context": ", 2007) and Averaged 2-Dependence Estimators (A2DE) (Webb et al., 2011; Zaidi and Webb, 2012), both of which consider all combinations of 3 variables, are both Deep and Broad.", "startOffset": 52, "endOffset": 93}, {"referenceID": 0, "context": "Since we do not want to be stuck in local minimums, a natural question to ask is whether the resulting objective function is convex Boyd and Vandenberghe (2008). It turns out that the objective function of DBL is indeed convex.", "startOffset": 132, "endOffset": 161}, {"referenceID": 0, "context": "Since we do not want to be stuck in local minimums, a natural question to ask is whether the resulting objective function is convex Boyd and Vandenberghe (2008). It turns out that the objective function of DBL is indeed convex. Roos et al. (2005) proved that an objective function of the form \u2211 x\u2208D log PB(y|x), optimized by any conditional Bayesian network model is convex if and only if the structure G of the Bayesian network B is perfect, that is, all the nodes in G are moral nodes.", "startOffset": 132, "endOffset": 247}, {"referenceID": 10, "context": "Zaidi et al. (2014) show that for NB, such DBL style parameterization with generative-discriminative learning can greatly speed-up convergence relative to only discriminative training.", "startOffset": 0, "endOffset": 20}, {"referenceID": 2, "context": "Some related ideas to ELR are also explored in Pernkopf and Bilmes (2005); Pernkopf and Wohlmayr (2009); Su et al.", "startOffset": 47, "endOffset": 74}, {"referenceID": 2, "context": "Some related ideas to ELR are also explored in Pernkopf and Bilmes (2005); Pernkopf and Wohlmayr (2009); Su et al.", "startOffset": 47, "endOffset": 104}, {"referenceID": 2, "context": "Some related ideas to ELR are also explored in Pernkopf and Bilmes (2005); Pernkopf and Wohlmayr (2009); Su et al. (2008). Several", "startOffset": 47, "endOffset": 122}, {"referenceID": 12, "context": "We employed L-BFGS quasi-Newton methods (Zhu et al., 1997) for solving the optimization6.", "startOffset": 40, "endOffset": 58}, {"referenceID": 12, "context": "We employed L-BFGS quasi-Newton methods (Zhu et al., 1997) for solving the optimization6. We used a Random Forest that is an ensemble of 100 decision trees Breiman (2001). Both DBL and LR are L2 regularized.", "startOffset": 41, "endOffset": 171}, {"referenceID": 9, "context": "Recently, AnDE models have been proposed as a fast and effective Bayesian classifiers when learning from large quantities of data (Zaidi and Webb, 2012).", "startOffset": 130, "endOffset": 152}], "year": 2015, "abstractText": "Deep learning has demonstrated the power of detailed modeling of complex high-order (multivariate) interactions in data. For some learning tasks there is power in learning models that are not only Deep but also Broad. By Broad, we mean models that incorporate evidence from large numbers of features. This is of especial value in applications where many different features and combinations of features all carry small amounts of information about the class. The most accurate models will integrate all that information. In this paper, we propose an algorithm for Deep Broad Learning called DBL. The proposed algorithm has a tunable parameter n, that specifies the depth of the model. It provides straightforward paths towards out-of-core learning for large data. We demonstrate that DBL learns models from large quantities of data with accuracy that is highly competitive with the state-ofthe-art.", "creator": "LaTeX with hyperref package"}}}