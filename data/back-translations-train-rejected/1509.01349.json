{"id": "1509.01349", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "4-Sep-2015", "title": "Parallel and Distributed Approaches for Graph Based Semi-supervised Learning", "abstract": "Two approaches for graph based semi-supervised learning are proposed. The firstapproach is based on iteration of an affine map. A key element of the affine map iteration is sparsematrix-vector multiplication, which has several very efficient parallel implementations. The secondapproach belongs to the class of Markov Chain Monte Carlo (MCMC) algorithms. It is based onsampling of nodes by performing a random walk on the graph. The latter approach is distributedby its nature and can be easily implemented on several processors or over the network. Boththeoretical and practical evaluations are provided. It is found that the nodes are classified intotheir class with very small error. The sampling algorithm's ability to track new incoming nodesand to classify them is also demonstrated.", "histories": [["v1", "Fri, 4 Sep 2015 06:35:55 GMT  (3325kb,D)", "http://arxiv.org/abs/1509.01349v1", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["konstantin avrachenkov", "vivek borkar", "krishnakant saboo"], "accepted": false, "id": "1509.01349"}, "pdf": {"name": "1509.01349.pdf", "metadata": {"source": "CRF", "title": "Parallel and Distributed Approaches for Graph Based Semi-supervised Learning", "authors": ["K. Avrachenkov", "V.S. Borkar", "K. Saboo"], "emails": ["k.avrachenkov@inria.fr", "borkar.vs@gmail.com", "kvsaboo.2004@ee.iitb.ac.in"], "sections": [{"heading": null, "text": "IS SN 0249 -639 9IS RN INR IA / R R-- 8767 --FR + EN GRESEARCH REPORT N \u00b0 8767 August 2015Project Team Maestro"}, {"heading": "Parallel and Distributed", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "Approaches for", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "Graph Based", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "Semi-supervised", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "Learning", "text": "K. Avrachenkov, V.S. Borkar, K. Sabooar Xiv: 150 9,01 349v 1 [cs.L G] 4S ep2 015RESEARCH CENTRE SOPHIA ANTIPOLIS - M\u00c9DITERRAN\u00c92004 route des Lucioles - BP 93 06902 Sophia Antipolis CedexParallel and Distributed Approaches for Graph Based Semi-Supervised LearningK. Avrachenkov, V.S. Borkar \u2020, K. Saboo \u2021 Project-Team MaestroResearch Report n \u00b0 8767 - August 2015 - 21 pagesAbstract: Two approaches for Graph Based Semi-Supervised LearningK are proposed. The first approach is based on the iteration of an affine map. A key element of the affine map iteration is the iteration of Bomrix-Vector-Vector Samplify, which has several very efficient parallel implementations. The second approach belongs to the Markov class of systems distributed on multiple Monte Carlo chains (MMC systems are very simple)."}, {"heading": "Les Approches Parall\u00e8les et Distribu\u00e9s pour l\u2019Apprentissage", "text": "Semi-supervis\u00e9R\u00e9sum\u00e9: Deux approches pour l'apprentissage semi-supervis\u00e9 bas\u00e9 sur le graphe de similarit\u00e9 sont propos\u00e9s. La premi\u00e8re approche est bas\u00e9e sur l'it\u00e9ration d'un op\u00e9rateur affine. Un \u00e9l\u00e9ment cl\u00e9 de l'it\u00e9ration de l'op\u00e9rateur affine est la multiplication vecteur par matrice de type sparse, qui a plusieurs impl\u00e9mentations parall\u00e8les tr\u00e8s efficaces. La seconde approche appartient \u00e0 la classe des algorithmes de Monte-Carlo par cha\u00eenes de Markov (MCMC). Elle est bas\u00e9 sur un \u00e9chantillonnage de noeuds en effectuant une marche al\u00e9atoire sur le graphe de similarit\u00e9."}, {"heading": "Distributed Semi-supervised Learning 3", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "Contents", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "1 Introduction 4", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "2 Optimization Problem and Algorithms 4", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "3 Convergence Analysis 6", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4 Experiments and Results 8", "text": "4.1 Les Miserables..........................................................................................................................."}, {"heading": "5 Conclusion 15", "text": "RR n \u00b0 8767"}, {"heading": "4 K. Avrachenkov & V.S. Borkar & K. Saboo", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "1 Introduction", "text": "In fact, it is the case that it will be able to take the lead."}, {"heading": "2 Optimization Problem and Algorithms", "text": "Consider a graph with adjacence matrix A that has N nodes, each of which belongs to one of the K classes. The class information of some of the nodes is also given, which are called nodes. Let's define D as a diagonal matrix with Dii = d (i), where d (i) is the degree of the node i. Y is a N \u00b7 K matrix that contains the information about the designated nodes."}, {"heading": "Distributed Semi-supervised Learning 5", "text": "Yij = {1 if the described node i belongs to class j, 0 otherwise. F is a N \u00b7 K matrix with each element as a real value. Fik represents the \"affiliation\" of the node i to class K. It is called a \"classification function\" or \"attribute vector.\" The goal of the semi-monitored learning problem is to find F.k so that it is close to the labeling function, and it varies smoothly across the graph. The class of the node i is calculated by F using the following relation. Node i belongs to class k ifFik > Fij \u0432\u0430j \u0432\u0430j 6 = k. The optimization problem associated with the above requirements can be written as follows: MinimizeQ (F) = 2 K \u00b2 k = 1 FT.kD \u03c3 \u2212 k = 1LD\u03c3k \u00b2 K \u00b2 k = 1 (F.k \u2212 Y.k)."}, {"heading": "6 K. Avrachenkov & V.S. Borkar & K. Saboo", "text": "Algorithm 2: F t + 1ij = F t ij + \u03b7tI {Xt = i} p (i, Xt + 1) q (i, Xt + 1) (HiiF t Xt + 1j \u2212 F t ij + \u03b1Yij), where {\u03b7t} t \u2265 0 is a positive step size sequence that satisfies the result, t \u2265 0 \u03b7t = \u221e and \u2211 t \u2265 0 \u03b7 2 t < \u221e. Here p (i, j), q (i, j) are the elements of P and Q and the ratio p (i, Xt + 1) / q (i, Xt + 1) is the (conditional) probability ratio that corrects the fact that we simulate the chain of transitions determined by Q and not by P. This is a form of sampling meaning. We perform this update step: j, 1, 2,.., K} henceforth. In each iteration, the property value of a node is only updated by this next step, before we clearly designate it as the next node in this sequence."}, {"heading": "3 Convergence Analysis", "text": "We now present proof of the convergence of the two algorithms. Let us consider the problem of finding a unique solution x \u00b2 of Systemx = G (x) = B (x) = B (0, 1) and the corresponding (according to normalized) positive eigenvector w = [w1,... wd] T. Let us define the weighted Norm x \u00b2 w = max i \u00b2 w = max i \u00b2 w = max i \u00b2 w (0, 1) and the corresponding (according to normalized) positive eigenvector w = [w1,... wd] T. Let us define the weighted Norm x \u00b2 w = max i \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p p \u00b2 p p \u00b2 p p p \u00b2 p p p p p p p \u00b2 p p p p p p p \u00b2 p p p p p p p \u00b2 p (j \u00b2 p) p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p (2) p p \u00b2 p p p \u00b2 p p p p) p \u00b2 p (1) p \u00b2 p \u00b2 p p (p) p \u00b2 p \u00b2 p (p) p \u00b2 p \u00b2 p (p) p \u00b2 p) p (p) p (p) p \u00b2 p (p) p \u00b2 p (p) p) p \u00b2 p \u00b2 p (p) p (p) p) p (p) p (p) p) p (p) p) p (p) p) p (p) p) p (p) p) p (p) p) p) p (p) p) p (p) p) p) p (p) p) p) p (p) p) p) p) p (p) p) p (p) p) p) p) p (p) p) p) p) p (p) p) p) p) p) p (p) p) p) p) p) p) p) p) p) p) p) p) p)"}, {"heading": "Distributed Semi-supervised Learning 7", "text": "x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x"}, {"heading": "8 K. Avrachenkov & V.S. Borkar & K. Saboo", "text": "Now \"G\" (x) - \"G\" (y) - \"b\" (i, j) (xj - yj) - \"maxi\" (i) - \"xi\" (i) - \"xi\" (i) - \"xi\" (i) - \"xi\" (i) - \"xi\" (i) - \"xi\" (i) - \"xi\" (i) - \"xi\" (i) - \"xi\" (i) - \"y\" (i) - \"w\" (i) - \"xi\" - \"-\" - \"-\" - \"-\" - \"-\" - \"-\" - \"-\" - \"-\" - \"-\" - \"-\" - \"-\" - \"-\" - \"-\" - \"-\" - \"-\" - \"-\" - \"-\" - \"-\" - \"-\" - \"-\" - \"-\" - \"-\" - \"-\" - \"-\" - \"-\" - \"-\" - \"-\" - \"-\" - \"-\" - \"-\" - \"-\" - \"-\" - \"-\" - \"-\" - \"-\" - \"-\" - \"-\" - \"-\" - \"-\" - \"-\" - \"-\" - \"-\" - \"-\" - \"-\" - \"-\" - \"-\" - \"-\" - \"-\" - \"-\" - \"-\" - \"-\" - \"-\" - \"-\" - \"-\" - \"-\" - \"-\" - \"-\" - \"-\" - \"-\" - \"-\" - \"-\" - \"-\" - \"-\" - \"-\" - \"-\" - \"-\" - \"-\" - \"-\" - \"-\" - \"-\" - \"-\" - \"-\" - \"-\" - \"-\" - \"-\" - \"-\" - \"-\" - \"-\" - \"-\" - \"-\" - \"-\" - \"-\" - \"-\" - \"-\" - \"-\" - \"-\" - \"-\" - \"-\" - \"-\" - \"-\" - \"-\" - \"-\" - \"-\" - \"-\" - \"-\" - \"-\" - \"-\" - \"-\" - \"-"}, {"heading": "4 Experiments and Results", "text": "To evaluate the classification accuracy of the two algorithms, we have used the following (real and synthetic) networks: the graph based on the French classic Les Miserables by Victor Hugo [11], the graphs of the web site records of universities such as WebKB [20], Gaussian blend graphs, and finally the dynamic stochastic block model graph based on the M / M / K / K queue. The number of updates of the power method in an iteration is N, while the classification function of only one node is updated at each step of the sampling algorithm. Therefore, we will refer to the N steps of the sampling algorithm as an iteration to keep the comparison fair."}, {"heading": "4.1 Les Miserables graph", "text": "Characters of the novel Les Miserables form the nodes of the graph [11]. The nodes appearing in the novel on the same page are connected by a weighted directed edge. We look at the undirected version of this graph with the weight of all nodes set to 1. This graph has 77 nodes Inria."}, {"heading": "Distributed Semi-supervised Learning 9", "text": "and 6 clusters, namely Myriel (10), Valjean (17), Fantine (10), Cosette (10), Thenardier (12), Gavroche (18), where the class name is the name of a character and in brackets is the number of nodes in this cluster. Class information of the above nodes is available to the algorithms. We use the decreasing step size 12 + bt / 100c. Algorithms 1 and 2 provide \u00b5 and \u03c3 as two parameters that can be varied. for different values of these parameters, algorithm 1 was executed for 500 iterations and then the class of each node was identified as a label node. This was done 3 times for each value of \u00b5 and \u03c3 and then the average error was calculated over the 3 classifications. The average error is shown in Figure 1 (a). Figure 1 (b) shows the same information for the dose values of the dose that corresponds to the standard Laplacian."}, {"heading": "10 K. Avrachenkov & V.S. Borkar & K. Saboo", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.2 WebKB Data", "text": "Next, we will look at the classification of the web pages of 4 universities - Cornell, Texas, Washington and Wisconsin - according to the popular WebKB dataset [20]. We will look at the graph formed by the hyperlinks linking these pages. Websites in the original dataset that do not have hyperlinks to web pages within the dataset will not be taken into account, resulting in the following clusters Cornell (677), Texas (591), Washington (983) and Wisconsin (616). The node with the highest error level is selected from each class (university) as a labeled node. We will use the decreasing step size 12 + bt / 1000c. We will use this decreasing step size for the other experiments unless otherwise stated. Figure 6 shows the error development in%. The majority of this error is based on the fact that the nodes belonging to Wisconsin are classified as nodes belonging to Cornell. The grade labeled Cornell is equal to these class Cornell nodes at an average of 52, approximately three times the number of Wisconsin and Wisconsin at an average of 46."}, {"heading": "Distributed Semi-supervised Learning 11", "text": "Degree is used as x-axis for the sampling algorithm in Figure 6, while iterations are used for the power siteration algorithm. Basis for the division by average degree is that the sampling algorithm receives information from only one neighbor in one step, while the power siteration gets information from all neighbors in an iteration.The division by average degree makes the two methods comparable in terms of calculation costs.As you can see, the two methods are comparable both in terms of accuracy and the computational cost of achieving a certain accuracy.It is interesting to observe that if you want to get a very rough classification quickly, the sampling algorithm may be preferable. This situation is typical of methods of type MCMC."}, {"heading": "4.3 Gaussian Mixture Models", "text": "A node belongs to class 1 with the probability of 0.33, to class 2 with the probability of 0.33 and to class 3 with the probability of 0.34. A node shares an edge with all nodes within a given radius of itself. Two nodes with the highest degree of each class were selected as labeled nodes, while the remaining nodes were not labeled. It is known that selecting high-degree nodes as labeled nodes is advantageous [3]. A Gausser blending diagram with 500 nodes was created using the above parameters. Figure 7 (a) shows the graph with the nodes that are colored according to their class - Class 1 nodes are shown in pink, Class 2 in green, and Class 3 in red. Figure 7 (b) shows the classes in which the nodes were classified using the sampling-based algorithm. Nodes classified in class 1 are shown in microssa, Class 2 in green, and Class 3 are classified in completely yellow, while the nodes are classified in class 3."}, {"heading": "12 K. Avrachenkov & V.S. Borkar & K. Saboo", "text": "On the X axis, the iteration / average degree log is used for the scanning method, while the iteration log is used for the power iteration; the intention is the same as in Figure 6. The scanning algorithm exceeds the method of performance iteration in terms of the computational costs required to achieve a given accuracy, the number of iterations for the two methods is of the same order of magnitude (5-8 for PI, while 15-20 for scanning), and the average node degree is between 180-570. Convergence is faster in the chart with higher cluster density, and the final error in charts with higher cluster density is lower compared to others in both methods. The incorrectly classified nodes are at the boundary of two classes. Instead of the MCMC-based selection, a method for selecting the round robin node has also been tried."}, {"heading": "4.4 Tracking of new incoming nodes", "text": "After the sampling algorithm had performed 200 iterations on the original graph with 500 nodes, a new node with the same parameters was introduced into the graphs. The algorithm continued the classification function, which was found after 200 iterations for a further 20 iterations, this time with 501 nodes. The class of all nodes was recalculated, and the development of the characteristic vector of the new node is shown in Figure 11. In the example presented here, the new node belonged to class 2. Figure 11 shows that the value of the classification function for the new node increases with increasing number of iterations, and therefore the new node is correctly classified, i.e. it was traced correctly. Instead of the MCMC-based sampling, the class of the new incoming node can also be determined by using an alternative selection scheme. In this case, we first select the incoming node and then the neighbors from the neighboring sample that we select again."}, {"heading": "Distributed Semi-supervised Learning 13", "text": "The idea behind how it works is that an unwritten incoming node will not greatly alter the classification function of the other nodes. In fact, it will hardly change the values for nodes that are several steps away. So to get a good estimate of the classification function, selecting nodes as described in this experiment is sufficient. In fact, this technique is much faster than the MCMC technique, because we do not select nodes from the whole graph, but only from 1-hop neighbors."}, {"heading": "4.5 Stochastic Block Model", "text": "We are now looking at a dynamic graph in which nodes can enter and exit the graph. It has been shown that the sampling algorithm can classify new incoming nodes. A dynamic stochastic block model with several classes is taken. Stochastic block models [8, 12] have proven to be more realistic in applications such as social networks than classic models such as preferred attachments, and there is a growing interest in them. Intuitively, they have relatively denser subnetworks connected by weak bonds. A node is connected to another node of the same class, with probability pin and nodes from another class with probability pin; pin > > pout. In the present work, we are introducing an extension of the stochastic block model to the dynamic setting. Specifically, there is a Poisson arrival of nodes in the graph with the rate pin in the graph, with the class of the incoming node selected according to a predetermined probability distribution, each of the nodes remaining random for a period of time."}, {"heading": "14 K. Avrachenkov & V.S. Borkar & K. Saboo", "text": "(a) Graph with class of node, which is indicated by the color of the node < b) Graph with class of node, as found by the sampling algorithm, which is indicated by the color of the node. Misclassified nodes are shown in yellow. Figure 7: Gaussian blend diagrams during initialization. The graph had 3 classes and an incoming node could belong to one of the classes with equal probability. K = 1000, \u03bbarr = 1 / (2 \u00d7 104) and \u00b5dep = 1 / 107 were selected during the initialization of error diagrams. Therefore, \u03bbarr\u00b5dep = 500. Two nodes with the maximum degree from each class were selected as the designated nodes during initialization. K steps were considered as an iteration. Figure 14 and Figure 15 show the evolution of the% error and the size of the graph for the original graph size of 500, so that the size of the 600 and the 400 step size of the algorithm was kept constant."}, {"heading": "Distributed Semi-supervised Learning 15", "text": "Similarly, Figure 17 shows the effect of decreasing the exit rate on Figure 1 / 108 while maintaining Figure 1 / (2 \u00b7 104). In both cases, the number of nodes in the stable state should be 5000. It can be seen that the decrease in the exit rate is much smoother in this case than in Figure 13 (b), Figure 14 (b) and Figure 15 (b), indicating the slowly varying size of the diagram. During the experiment, the number of marked nodes from each cluster was kept constant. In the case of a marked node on the left, a neighbor of the remaining node belonging to the same cluster was randomly selected as the marked node."}, {"heading": "5 Conclusion", "text": "Two iterative algorithms for graph-based semi-supervised learning were proposed: the first algorithm is based on the iteration of affine contraction (a weighted norm), while the second algorithm is based on a sample scheme inspired by amplification learning; the classification accuracy of graph nodes in classes for two algorithms was evaluated and confirmed as equal; and the performance of the sample algorithm was also evaluated using GaussianRR n \u00b0 8767."}, {"heading": "16 K. Avrachenkov & V.S. Borkar & K. Saboo", "text": "The sampling algorithm correctly classifies the new incoming node in significantly fewer iterations. This capability was then applied to a dynamic stochastic block model graph modeled on the basis of M / M / K / K queue. It has been shown that the sampling algorithm can be applied to dynamically changing systems and still achieves a small error. Unlike the power iteration algorithm, the sampling algorithm can be implemented in a distributed manner and has very little iteration, so it should be the preferred method for very large graphs."}, {"heading": "Distributed Semi-supervised Learning 17", "text": "[5] Borkar, V.S., and Mathkar, A.S. \"Reinforcement Learning for Matrix Computations: PageRank as an Example.\" Distributed Computing and Internet Technology, Proceedings of the 10th ICDCIT, Bhubaneshwar, India (R. Natarajan, ed.) Springer Lecture Notes in Computer Science No. 8337, Springer International Publishing, Switzerland, 2014. [6] Borkar, V.S. Stochastic Approximation: A Dynamical Systems Viewpoint. Hindustan Publishing Agency, New Delhi, and Cambridge Uni. Press, Cambridge, UK, 2008. [7] Chapelle, O., Sch\u00f6lkopf, B. and Zien A. Semi-Supervised Learning, MIT Press, 2006. [8] Condon, A., and Karp, R. \"Algorithms for graph partitioning on the planted partition model."}, {"heading": "18 K. Avrachenkov & V.S. Borkar & K. Saboo", "text": "[15] Zhou, D., Bousquet, O., Navin Lal, T., Weston, J., Sch\u00f6lkopf, B. \"Learning with Local and Global Consistency.\" In: Advances in Neural Information Processing Systems, 16, pp. 321-328, 2004. [16] Zhou D., Hofmann T., Sch\u00f6lkopf B. \"Semi-Supervised Learning on Directed Graphs,\" In Advances in Neural Information Processing Systems, pp. 1633-1640, 2004. [17] Zhou, D., and Burges, C. J. C. \"Spectral Clustering and Transductive Learning with Multiple Views.\" In Proceedings of ICML 2007, pp. 1159-1166, 2007. [18] Zhu, X. \"Semi-Supervised Learning Literature Survey.\" University of Wisconsin-Madison Research Report TR 1530, 2005. [19] NVIDIA CUDA Programming Guide, available at http: / / docns.ndiWorld S. Carviven / M20 - Unigical Science 12V."}, {"heading": "Distributed Semi-supervised Learning 19", "text": "RR n \u00b0 8767"}, {"heading": "20 K. Avrachenkov & V.S. Borkar & K. Saboo", "text": ""}, {"heading": "Distributed Semi-supervised Learning 21", "text": "RR n \u00b0 8767RESEARCH CENTRE SOPHIA ANTIPOLIS - M\u00c9DITERRAN\u00c9E2004 route des Lucioles - BP 93 06902 Sophia Antipolis CedexPublisher Inria Domaine de Voluceau - Rocquencourt BP 105 - 78153 Le Chesnay Cedex inria.frISSN 0249-6399"}], "references": [{"title": "Pagerank based clustering of hypertext document collections", "author": ["K. Avrachenkov", "V. Dobrynin", "D. Nemirovsky", "S.K. Pham", "E. Smirnova"], "venue": "In Proceedings of ACM SIGIR 2008,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2008}, {"title": "Generalized optimization framework for graph-based semi-supervised learning.", "author": ["K. Avrachenkov", "P. Gon\u00e7alves", "A. Mishenin", "M. Sokol"], "venue": "Proceedings of SIAM Conference on Data Mining (SDM 2012)", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2012}, {"title": "On the choice of kernel and labelled data in semi-supervised learning methods", "author": ["K. Avrachenkov", "P. Gon\u00e7alves", "M. Sokol"], "venue": "In Proceedings of Algorithms and Models for the Web Graph (WAW 2013),", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2013}, {"title": "Implementing sparse matrix-vector multiplication on throughputoriented processors", "author": ["N. Bell", "M. Garland"], "venue": "Proceedings of the ACM Conference on High Performance Computing Networking, Storage and Analysis", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2009}, {"title": "Reinforcement Learning for Matrix Computations: PageRank as an Example.", "author": ["V.S. Borkar", "A.S. Mathkar"], "venue": "Distributed Computing and Internet Technology, Proceedings of the 10th ICDCIT, Bhubaneshwar, India (R. Natarajan,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2014}, {"title": "Stochastic approximation: A Dynamical Systems Viewpoint", "author": ["V.S. Borkar"], "venue": "Hindustan Publishing Agency,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2008}, {"title": "Algorithms for graph partitioning on the planted partition model", "author": ["A. Condon", "R. Karp"], "venue": "Random Structures and Algorithms, v.18,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2001}, {"title": "An experimental investigation of graph kernels on a collaborative recommendation task", "author": ["F. Fouss", "L. Yen", "A. Pirotte", "M. Saerens"], "venue": "In Sixth International Conference on Data Mining (ICDM\u201906),", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2006}, {"title": "An experimental investigation of kernels on graphs for collaborative recommendation and semisupervised classification", "author": ["F. Fouss", "K. Francoisse", "L. Yen", "Pirotte A", "M. Saerens"], "venue": "Neural Networks, 31,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2012}, {"title": "The Stanford GraphBase: A Platform for Combinatorial Computing, Addison- Wesley", "author": ["D.E. Knuth"], "venue": null, "citeRegEx": "11", "shortCiteRegEx": "11", "year": 1993}, {"title": "Community detection in the labelled stochastic block model", "author": ["S. Heimlicher", "M. Lelarge", "L. Massouli\u00e9"], "venue": "ArXiv preprint arXiv:1209.2910,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2012}, {"title": "Scalable parallel programming with CUDA", "author": ["J. John Nickolls", "I. Ian Buck", "M. Michael Garland", "K. Kevin Skadron"], "venue": "Queue, v.6(2),", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2008}, {"title": "Using Web Graph Structure for Person Name Disambiguation", "author": ["E. Smirnova", "K. Avrachenkov", "B. Trousse"], "venue": "In Proceedings of CLEF,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2010}, {"title": "Learning with local and global consistency", "author": ["D. Zhou", "O. Bousquet", "T. Navin Lal", "J. Weston", "B. Sch\u00f6lkopf"], "venue": "Advances in Neural Information Processing Systems,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2004}, {"title": "Semi-supervised learning on directed graphs", "author": ["D. Zhou", "T. Hofmann", "B. Sch\u00f6lkopf"], "venue": "In Advances in neural information processing systems.,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2004}, {"title": "Spectral clustering and transductive learning with multiple views", "author": ["D. Zhou", "C.J.C. Burges"], "venue": "In Proceedings of ICML", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2007}, {"title": "Semi-supervised learning literature survey", "author": ["X. Zhu"], "venue": "University of Wisconsin-Madison Research Report TR 1530,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2005}, {"title": "Learning to extract symbolic knowledge from the World", "author": ["Craven", "Mark"], "venue": "Wide Web. No. CMU-CS-98-122. Carnegie-Mellon Univ Pittsburgh pa School of Computer Science,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 1998}], "referenceMentions": [{"referenceID": 7, "context": "The present work focuses on graph based semi-supervised learning [7, 9, 10, 18].", "startOffset": 65, "endOffset": 79}, {"referenceID": 8, "context": "The present work focuses on graph based semi-supervised learning [7, 9, 10, 18].", "startOffset": 65, "endOffset": 79}, {"referenceID": 16, "context": "The present work focuses on graph based semi-supervised learning [7, 9, 10, 18].", "startOffset": 65, "endOffset": 79}, {"referenceID": 7, "context": "Graph based semi-supervised learning finds application in recommendation systems [9], classification of web pages into categories [16], person name disambiguation [14], etc.", "startOffset": 81, "endOffset": 84}, {"referenceID": 14, "context": "Graph based semi-supervised learning finds application in recommendation systems [9], classification of web pages into categories [16], person name disambiguation [14], etc.", "startOffset": 130, "endOffset": 134}, {"referenceID": 12, "context": "Graph based semi-supervised learning finds application in recommendation systems [9], classification of web pages into categories [16], person name disambiguation [14], etc.", "startOffset": 163, "endOffset": 167}, {"referenceID": 1, "context": "In [2] the authors have formulated a generalized optimization problem which gives as important particular cases: Standard Laplacian [17], Normalised Laplacian [15] and PageRank [1] algorithms.", "startOffset": 3, "endOffset": 6}, {"referenceID": 15, "context": "In [2] the authors have formulated a generalized optimization problem which gives as important particular cases: Standard Laplacian [17], Normalised Laplacian [15] and PageRank [1] algorithms.", "startOffset": 132, "endOffset": 136}, {"referenceID": 13, "context": "In [2] the authors have formulated a generalized optimization problem which gives as important particular cases: Standard Laplacian [17], Normalised Laplacian [15] and PageRank [1] algorithms.", "startOffset": 159, "endOffset": 163}, {"referenceID": 0, "context": "In [2] the authors have formulated a generalized optimization problem which gives as important particular cases: Standard Laplacian [17], Normalised Laplacian [15] and PageRank [1] algorithms.", "startOffset": 177, "endOffset": 180}, {"referenceID": 1, "context": "In the present work, starting from the generalized formulation of [2], we propose two approaches for the computation of the feature vectors.", "startOffset": 66, "endOffset": 69}, {"referenceID": 4, "context": "[5] have developed a sampling based reinforcement learning scheme for the PageRank algorithm.", "startOffset": 0, "endOffset": 3}, {"referenceID": 1, "context": "k, where B = D\u2212\u03c3AD\u03c3\u22121 and \u03b1 = 2 2+\u03bc for \u03bc > 0 [2].", "startOffset": 46, "endOffset": 49}, {"referenceID": 3, "context": "One of the state of the art implementations is provided by NVIDIA CUDA platform [4, 13, 19].", "startOffset": 80, "endOffset": 91}, {"referenceID": 11, "context": "One of the state of the art implementations is provided by NVIDIA CUDA platform [4, 13, 19].", "startOffset": 80, "endOffset": 91}, {"referenceID": 5, "context": "Iteration (2) is a stochastic approximation algorithm that can be analyzed using the \u2018ODE\u2019 (for Ordinary Differential Equation\u2019) approach which treats the discrete time algorithm as a noisy discretization (or \u2018Euler scheme\u2019) of a limiting ODE with slowly decreasing step-size, decremented at just the right rate so that the errors due to discretization and noise are asymptotically negligible and the correct asymptotic behavior of the ODE is replicated [6].", "startOffset": 454, "endOffset": 457}, {"referenceID": 5, "context": "The limiting ODE in this case can be derived as in Chapter 6, [6] and is \u1e8b(t) = \u0393(G(x(t))\u2212 x(t)) = G\u0303(x(t))\u2212 x(t), where G\u0303(x) = (I \u2212 \u0393)x+ \u0393G(x).", "startOffset": 62, "endOffset": 65}, {"referenceID": 5, "context": "126-127 of [6], the ODE converges to the unique fixed point of G\u0303, equivalently of G, i.", "startOffset": 11, "endOffset": 14}, {"referenceID": 5, "context": "75, [6], supt \u2016x\u2016 < \u221e with probability one.", "startOffset": 4, "endOffset": 7}, {"referenceID": 5, "context": "If we consider round robin sampling, the results of [6], Section 4.", "startOffset": 52, "endOffset": 55}, {"referenceID": 9, "context": "To evaluate the classification accuracy of the two algorithms, we have used the following networks (real and synthetic): the graph based on the French classic Les Miserables written by Victor Hugo [11], the graph of university webpages dataset as found on WebKB [20], Gaussian mixture graphs, and finally dynamic stochastic block model graph based on M/M/K/K queue.", "startOffset": 197, "endOffset": 201}, {"referenceID": 17, "context": "To evaluate the classification accuracy of the two algorithms, we have used the following networks (real and synthetic): the graph based on the French classic Les Miserables written by Victor Hugo [11], the graph of university webpages dataset as found on WebKB [20], Gaussian mixture graphs, and finally dynamic stochastic block model graph based on M/M/K/K queue.", "startOffset": 262, "endOffset": 266}, {"referenceID": 9, "context": "1 Les Miserables graph Characters of the novel Les Miserables form the nodes of the graph [11].", "startOffset": 90, "endOffset": 94}, {"referenceID": 2, "context": "This choice of the parameters can also be backed up by the results from [3] and [15].", "startOffset": 72, "endOffset": 75}, {"referenceID": 13, "context": "This choice of the parameters can also be backed up by the results from [3] and [15].", "startOffset": 80, "endOffset": 84}, {"referenceID": 2, "context": "The difference in performance for \u03c3\u2019s can be understood by taking the analogy of a random walk on the graph starting from the labelled node of the cluster [3].", "startOffset": 155, "endOffset": 158}, {"referenceID": 17, "context": "2 WebKB Data We next see the classification of webpages of 4 universities - Cornell, Texas, Washington and Wisconsin - corresponding to the popular WebKB dataset [20].", "startOffset": 162, "endOffset": 166}, {"referenceID": 2, "context": "It is known that choosing high degree nodes as labelled nodes is beneficial [3].", "startOffset": 76, "endOffset": 79}, {"referenceID": 6, "context": "Stochastic block models [8, 12] have been found to be closer to reality than classical models such as preferential attachment, in applications such as social networks and there is a growing interest in them.", "startOffset": 24, "endOffset": 31}, {"referenceID": 10, "context": "Stochastic block models [8, 12] have been found to be closer to reality than classical models such as preferential attachment, in applications such as social networks and there is a growing interest in them.", "startOffset": 24, "endOffset": 31}, {"referenceID": 5, "context": ", O(\u03b7) ([6], Chapter 9).", "startOffset": 8, "endOffset": 11}], "year": 2015, "abstractText": "Two approaches for graph based semi-supervised learning are proposed. The first approach is based on iteration of an affine map. A key element of the affine map iteration is sparse matrix-vector multiplication, which has several very efficient parallel implementations. The second approach belongs to the class of Markov Chain Monte Carlo (MCMC) algorithms. It is based on sampling of nodes by performing a random walk on the graph. The latter approach is distributed by its nature and can be easily implemented on several processors or over the network. Both theoretical and practical evaluations are provided. It is found that the nodes are classified into their class with very small error. The sampling algorithm\u2019s ability to track new incoming nodes and to classify them is also demonstrated. Key-words: Semi-supervised learning, Graph-based learning, Distributed algorithms, Parallel algorithms \u2217 K. Avrachenkov is with Inria Sophia Antipolis, France, k.avrachenkov@inria.fr \u2020 V.S. Borkar is with Department of Electrical Engineering, IIT Bombay, India, borkar.vs@gmail.com \u2021 K. Saboo is with Department of Electrical Engineering, IIT Bombay, India, kvsaboo.2004@ee.iitb.ac.in \u00a7 The work of KA and VSB was supported in part by grant no. 5100-ITA from the Indo-French Centre for the Promotion of Advanced Research (IFCPAR) and Alcatel-Lucent Inria Joint Lab. Les Approches Parall\u00e8les et Distribu\u00e9s pour l\u2019Apprentissage Semi-supervis\u00e9 R\u00e9sum\u00e9 : Deux approches pour l\u2019apprentissage semi-supervis\u00e9 bas\u00e9 sur le graphe de similarit\u00e9 sont propos\u00e9s. La premi\u00e8re approche est bas\u00e9e sur l\u2019it\u00e9ration d\u2019un op\u00e9rateur affine. Un \u00e9l\u00e9ment cl\u00e9 de l\u2019it\u00e9ration de l\u2019op\u00e9rateur affine est la multiplication vecteur par matrice de type sparse, qui a plusieurs impl\u00e9mentations parall\u00e8les tr\u00e8s efficaces. La seconde approche appartient \u00e0 la classe des algorithmes de Monte-Carlo par cha\u00eenes de Markov (MCMC). Elle est bas\u00e9 sur un \u00e9chantillonnage de noeuds en effectuant une marche al\u00e9atoire sur le graphe de similarit\u00e9. Cette derni\u00e8re approche est distribu\u00e9 par sa nature et peut \u00eatre facilement mis en oeuvre sur plusieurs processeurs ou sur un r\u00e9seau. \u00c9valuations th\u00e9oriques ainsi que pratiques sont fournis. On constate que les noeuds sont class\u00e9s dans leurs classes avec tr\u00e8s petite erreur. La capacit\u00e9 de l\u2019algorithme MCMC de suivre les nouveaux noeuds arrivants online et de les classer est \u00e9galement d\u00e9montr\u00e9. Mots-cl\u00e9s : Apprentissage semi-supervis\u00e9, Apprentissage bas\u00e9 sur le graphe de similarit\u00e9, Algorithmes distribu\u00e9s, Algorithmes parall\u00e8les Distributed Semi-supervised Learning 3", "creator": "LaTeX with hyperref package"}}}