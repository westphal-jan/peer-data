{"id": "1509.01549", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "4-Sep-2015", "title": "Giraffe: Using Deep Reinforcement Learning to Play Chess", "abstract": "This report presents Giraffe, a chess engine that uses self-play to discover all its domain-specific knowledge, with minimal hand-crafted knowledge given by the programmer. Unlike previous attempts using machine learning only to perform parameter-tuning on hand-crafted evaluation functions, Giraffe's learning system also performs automatic feature extraction and pattern recognition. The trained evaluation function performs comparably to the evaluation functions of state-of-the-art chess engines - all of which containing thousands of lines of carefully hand-crafted pattern recognizers, tuned over many years by both computer chess experts and human chess masters. Giraffe is the most successful attempt thus far at using end-to-end machine learning to play chess.", "histories": [["v1", "Fri, 4 Sep 2015 18:21:52 GMT  (393kb,D)", "http://arxiv.org/abs/1509.01549v1", "MSc Dissertation"], ["v2", "Mon, 14 Sep 2015 15:42:35 GMT  (393kb,D)", "http://arxiv.org/abs/1509.01549v2", "MSc Dissertation"]], "COMMENTS": "MSc Dissertation", "reviews": [], "SUBJECTS": "cs.AI cs.LG cs.NE", "authors": ["matthew lai"], "accepted": false, "id": "1509.01549"}, "pdf": {"name": "1509.01549.pdf", "metadata": {"source": "CRF", "title": "Giraffe: Using Deep Reinforcement Learning to Play Chess", "authors": ["Matthew Lai"], "emails": [], "sections": [{"heading": null, "text": "This report introduces Giraffe, a chess machine that plays itself to discover all of its domain-specific knowledge, with minimal craftsmanship provided by the programmer. In contrast to previous attempts to use machine learning only to parameterise handmade evaluation functions, Giraffe's learning system also performs automatic feature extraction and pattern recognition. The trained evaluation function is comparable to the evaluation functions of modern chess machines - which all contain thousands of lines of carefully crafted pattern recognition tuned over many years by both computer chess experts and human chess masters. Giraffe is the most successful attempt yet to use machine learning to play chess. We also explored the possibility of using probability thresholds instead of depth to form search trees. Depth-based searches form the backbone of virtually all existing chess machines today and are an algorithm that has become well established in the last half century."}, {"heading": "Acknowledgements", "text": "This project would not have been possible without the guidance, encouragement, insights and inspiration of my supervisor Professor Duncan Gillies. I would also like to thank the computer chess community for their support, in particular - Gerd Isenberg, who maintained and wrote most of the Chess Programming Wiki, the most comprehensive reference for everything to do with computer chess, and Graham Banks, who tested giraffe against a variety of opponents. I am also grateful for the hundreds of people who have played thousands of games against giraffes on the Internet Chess Club, including Grandmaster Tahir Vakhidov, International Master Alejandro Bofill Mas, IM Alejandro Montalvo, IM Alex Domont, IM Ulrich Schulze, IM William Hartston, FIDE Master \"Renium,\" \"solsjenitsyn,\" \"arnav311004,\" wmm72 \"and\" DoctorNick. \"Watching their games on the ICC allowed me to make many potential improvements to giraffes, which would have been much less successful for my friends than this project would have been."}, {"heading": "1 Introduction 8", "text": "1.1 Computers and chess................................................."}, {"heading": "2 Background and Related Work 9", "text": "For this purpose, the foundations for training were laid. For this purpose, the foundations for training were laid. For this purpose, the foundations for training were laid. For this purpose, the foundations for training were laid. For this purpose, the foundations for training were laid. For this purpose, the foundations for training were laid. For this purpose, the foundations for training were laid. For this purpose, the foundations for training were laid. For this purpose, the foundations for training were laid. For this purpose, the foundations for training were laid. For this purpose, the foundations for training were laid. For this purpose, the foundations for training were laid."}, {"heading": "3 Supporting Work 16", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4 Neural Network-Based Evaluation 17", "text": "4.1. Feature Representation.........................................................................................................................................................................................................................................................................................................................................."}, {"heading": "5 Probabilistic Search 26", "text": "5.1 Redefining the search problem............................................. 265.2 Conventional search with limited depth....................................... 265.3 Search with limited probability.............................................................................................................................................................."}, {"heading": "6 Neural Network-Based Probability Estimation 29", "text": "6.1 Feature Representation......................................................................................................................................................................................................................................................................"}, {"heading": "7 Conclusion 32", "text": "......................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................"}, {"heading": "1 Introduction", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "1.1 Computers and Chess", "text": "It is a game that requires more creativity than ever before, and it is a game that is about more than just how you yourself, how you yourself, how you yourself, how you yourself, and how you, how you do it, how you do it, how you do it, how you do it, how you do it, how you do it, and how you do it, how you do it, how you do it, and how you do it, how you do it, and how you do it, how you do it, how you do it, and how you do it, how you do it, and how do you do it, and how do you do it, and how do you do it, how do you do it, how do you do it, how do you do it, how do you do it."}, {"heading": "2 Background and Related Work", "text": "Due to the complex but well-defined nature of the game, chess is a well-researched problem in terms of artificial intelligence. Thousands of essays have been written on all aspects of computer chess since the 1950s, and at least hundreds of chess engines have been written by researchers, professionals and amateurs in the same years. [3] The chess problem is this: at a chess position, we want to find a move that maximizes our chance of winning the game. That means we also need to have a model for the opponent how she chooses her moves. In high chess, we usually model the opponent so that he is the same as we are - that is, we assume that he chooses moves the way we do and never makes mistakes. This assumption may not apply in games against much weaker players, where, for example, we have two possible moves A and B, where the move A gives a better position than when the opponent sees and plays a better one."}, {"heading": "2.1 Conventional Chess Engines", "text": "Although they differ in implementation, almost all existing chess engines (and all top competitors) implement largely the same algorithms. They are all based on the idea of the Minimax fixed-depth algorithm developed by John von Neumann in 1928 [4] and adapted for the chess problem by Claude E. Shannon in 1950 [5]."}, {"heading": "2.1.1 Minimax", "text": "This year it has come to the point where it will be able to retaliate, \"he said in an interview with the\" Welt am Sonntag. \""}, {"heading": "2.1.2 \u03b1-\u03b2 Pruning", "text": "Without the introduction of any heuristics and without loss of information, this algorithm can be optimized by introducing a \"window\" for each call to minimax (). The idea is that if the actual score is below the lower limit, this means that the caller already has a better turn and therefore does not care about the exact value of that node (only that it is lower than the lower limit). Conversely, if the actual score is higher than the upper limit, it means that the caller does not care about the exact value either, just the fact that it is practically higher than the upper limit. It may at first seem counterproductive to have an upper limit, but the reason is that chess is a zero sum, and the upper limit is that the switch is good for every position we are looking for. This optimization is called \u03b1-\u03b2-pruning [8], where Alpha and \u03b2 are the lower and upper limit. Since Alpha and awful decisions are based on the top, and we will differ from them."}, {"heading": "2.1.3 Evaluation", "text": "As mentioned above, the evaluation function is a very important part of a complex game. The evaluation functions include most domains that are kept in check. In this project, we are developing an evaluation function based on a machine approach. However, before we do that, we will take a look at the evaluation functions of Stockfisch."}, {"heading": "2.2 Machine Learning in Zero-Sum Games", "text": "The use of machine learning to play zero-sum games (including chess) has been tried countless times. In this section, such attempts are summarized and explained how they relate to this project."}, {"heading": "2.2.1 Mate Solvers", "text": "There have been many attempts to solve chess endgames by machine learning. Endgames are usually defined as positions in which only a few pieces (usually less than 6 in total) are left. These situations are much simpler than average chess positions, and relatively simple rules for optimal play can often be derived. Most experiments in this area use genetic programming [11] [12] and inductive learning [13]. Unfortunately, these approaches do not scale to much more complicated positions, as found in interludes and openings where there are no simple rules for optimal play."}, {"heading": "2.2.2 Learning Based on Hand-Selected Features", "text": "In a typical chess evaluation function, there are hundreds of parameters that need to be coordinated with each other. As it is very time consuming to perform the manual evaluation, some attempted to either automatically optimize these parameters using machine learning or to use the hard coded characteristics as input for machine learning to generate final evaluation results. NeuroChess [14] is one such experiment. The Falcon project [15] is an attempt to optimize the evaluation parameters using genetic programming with the help of a mentor that has a much more complicated evaluation function. Although it is possible to achieve good results with this method, the fact that the characteristics are hand-picked means that the systems are limited by the creativity of their developers (and the performance of the mentor in the case of Falcon). A more recent experiment is the Meep project by Veness et al. [16], whose evaluation function is a linear combination of hand-designed characteristics and where the weights are trained."}, {"heading": "2.2.3 Temporal-Difference Learning", "text": "While the results of gameplay are the most direct way to measure the performance of models, it is often impractical due to the very large number of games that are necessary to achieve statistically significant results for small changes. [17] It is mathematically inefficient to perform all the calculations required to play a game just to get a result with less than 2 bits of information. [17] The most successful approach so far has been time-differentiated learning, where the system is not tested for its ability to predict the outcome of the game from a position, but for its ability to predict its own score in the near future, and thus achieve temporary consistency. As long as some positions have fixed results, lost, or drawn positions), models with higher predictive power would have better temporal consistency (this is not true if no position has fixed score, as the model can always produce a constant value)."}, {"heading": "2.3 Deep Learning", "text": "In order to go beyond the weight, we have to deal with the hand-picked characteristics and actually have a system that has deep, non-linear functionality based on complex functions such as the evaluation function in the chess world. They are not able to build hierarchical structures, but they also require very large amounts of training to achieve complex functions in which the input maps are very large by nature. Models have to be extremely large. Not only do they require high computing power, but they also require very large amounts of training to prevent them from interfering with the model."}, {"heading": "2.4 Project Overview and Contributions", "text": "This project aims to create a new chess engine based on machine learning, using both algorithms that have already proven successful in past experiments such as TDLeaf (\u03bb), as well as new algorithms developed for this project.The first part of the project is essentially a modern iteration of the TDLeaf (\u03bb) experiment [21] to learn new evaluation functions, taking advantage of discoveries and advances in neural networks over the last 15 years, as well as the much higher computing power available today. Our goal is to create a system that can automatically perform high-level functional extraction so as not to be constrained by human creativity - something that was impracticable in the 1990s due to the limitations of computing power. In the second part of the project, we are developing a novel probability-based search framework to enable highly selective searches that use neural networks to provide guidance for the time each must spend from a subtree."}, {"heading": "3 Supporting Work", "text": "Most of the popular techniques in computer chess have been implemented in this chess engine. With a basic linear evaluation function that takes into account material, figure square tables, mobility and some royal security, it plays at something below the strength of the FIDE candidate. The search routine is also fairly standard, although aggressive heuristic pruning techniques have been deliberately omitted because we want all heuristic decisions to be made by learned systems. An implementation of neural networks has also been written. The decision was made to write from scratch rather than use an existing framework, as it needs to be comprehensively adapted for our application, so that features such as limited connectivity and the use of an existing framework would not save much work, and it would involve extensive modifications to an unknown code."}, {"heading": "4 Neural Network-Based Evaluation", "text": "The first problem we are tackling is the evaluation function, because other systems (e.g. train sequence) require us to already have a good evaluation function. As mentioned in Section 2.1.3 above, the function of the evaluation function is to take a position as input and estimate the probability of winning for the players without looking forward. This function is called at the leaf node of the search when we have to stop the search due to time constraints."}, {"heading": "4.1 Feature Representation", "text": "This year, the time has come for us to be able to leave the country, to leave it, to take it back in our hands."}, {"heading": "4.2 Network Architecture", "text": "The evaluation network is a 4-layer network (three hidden layers plus output layer) in which all hidden nodes use Rectified Linear Activation (ReLU) [31], which is a modern choice that is much more efficient than traditional hyperbolic tangents and logistical activation functions, especially for networks with more than two hidden layers. To limit output between -1 and 1, the output node uses hyperbolic tangent activation. If the representation of features includes features with different modalities, it is often advantageous to mix them only in higher, more abstract layers. This is because at very low levels data from different modalities cannot be mixed meaningfully and the additional connections do not provide any benefit (and in fact make the model more susceptible to retrofits). In this case, there are three modalities - piecentrally, square-centrically and centrically-layered. In the first two modalities, the first two modalities are kept separate from each other."}, {"heading": "4.3 Training Set Generation", "text": "Before we can start training the network, we need a way to generate a corpus of positions on which to train the system. Generating a good corpus is not an easy task, because it must simultaneously fulfill a few potentially conflicting goals - \u2022 Correct distribution - However, the positions should have roughly the same distribution as those that actually appear in the game. For example, it does not make sense to train the system in positions with three queens per side, because these positions practically never appear in actual games. \u2022 Variety - Although we want the positions to be representative of those that appear in actual games, we also want some variety on top of it so that the system can learn to play in highly unequal positions, for example. This is important because they do not often appear in real games, they are encountered in internal nodes of search, and the machine needs to know how to rate them correctly. \u2022 High volume - To train a highly flexible model, we need a large number of training positions."}, {"heading": "4.4 Network Initialization", "text": "Although in principle TD-Leaf can train a system from a random initialization, a random initialization on a complex problem like chess would lead to extremely long training times. Therefore, we use a very simple evaluation function that contains only very basic material knowledge to start the training process. The bootstrapping process takes only a few seconds and brings the neural network to a point in the parameter space that is much closer to a good minimum. It is also possible to boot with a much more precise function, such as one of the evaluations of the top chess engine, but since the goal of this project is to investigate how much chess knowledge can be learned entirely by self-discovery, this approach was not chosen."}, {"heading": "4.5 TD-Leaf", "text": "It is possible to create another system in which people are able to develop and develop their own abilities. (...) It is not that they are able to surpass themselves. \"(...)\" It is not that they do it. \"(...)\" It is not that they do it. \"(...)\" It is not that they do it. \"(...)\" It is that they do it. \"(...)\" It is that they do it. \"(...)\" It is that way. \"(...)\" It is that way. \"(...)\" It is that way. \"(...)\" It is that way. \"(...)\" It is that way. \"(...)\" It is that way. \"(...\" It is that way. \"(...)\" It is that way. \"(...).\" It is that way. \"(...\" It is that. \"(...) It is that.\" (... \"it is that.\" It is. \"It is that.\" It is. \"It is.\" It is. \"It is that.\" It is. \"It is.\" It is. \"It is.\" It is. (... \"It is.\" It is. \"It is that.\" It is. \"It is.\" It is. \"(...\" It is. \"It is.\" It is. \"It is.\" It is. \"(...\" It is. \"It is.\" It is. \"It is.\""}, {"heading": "4.6 Results and Evaluation", "text": "To test the assessor's understanding of position, the engine is guided through the Strategic Test Suite [34]. The Strategic Test Suite is a collection of 1500 positions, divided into 15 themes of 100 positions each. Each group of 100 positions tests the engine's understanding of a strategic idea. Unlike most other test subjects, who do not have deep tactical lines for the engine, the understanding of the opponent's values is tested in different situations."}, {"heading": "5 Probabilistic Search", "text": "Before we introduce the work on the formation of neural networks for traction and time allocation, we must describe how giraffe performs searches. Although it is not the main topic of the study of this project, it represents a significant departure from the way searches are performed in traditional chess engines. Although it offers a small advantage in terms of playing strength per se, the main advantage is that it provides us with a much more theoretically solid foundation on which to build the machine learning system."}, {"heading": "5.1 The Search Problem Redefined", "text": "As mentioned in section 2, there are several ways to define the search problem, and they are all equivalent. In any case, the goal of the search is to predict which position will be reached if both sides play a theoretically optimal game, and a sequence of moves leading to that position, because if we have the list of moves that make up the theoretically optimal game from the starting position of the search, the first move in this list is obviously the move we should play, assuming that the opponent is at least as skillful as we are. We can apply this goal recursively until we reach all possible end positions in chess. We can then define a main theoretical variation (PV) of any position that should be the sequence of theoretically optimal moves that take us from the position under consideration to the end of the game (checkmate or tie by stalemate / insufficient repetition / no progress). Some positions may have more than one theoretical PV, but all have at least one."}, {"heading": "5.2 Conventional Depth-Limited Search", "text": "When von Neumann introduced the Minimax algorithm in 1928, he focused on cases in which the entire game tree can be explored [4]. Although this is an important achievement, it does not apply to games such as chess, where the game tree is too large to be fully explored. Norbert's 1948 rendition of Minimax is the first known application of incomplete Minimax to use a heuristic function on search tree leaves [35]. He used the distance from the root as a simple metric to determine which part of the sub-tree should be searched for. While modern chess engines have improved this by introducing various types of enhancements and reductions [36] [37] [38], the basic idea of using depth to define search trees has not changed. Practically all existing chess engines perform depth-limited searches. Pseudo-code for a simplified depth-limited search: Minimax (Position, Depth) function {if depth = submax: return = max = best possible score for each turn."}, {"heading": "5.3 Probability-Limited Search", "text": "Instead of \"searching all nodes with less than 10 movements from the root position,\" we look for \"all nodes with more than 0,000001 chances of being part of the theoretical PV from the root position.\" At each position in the search, we divide the probability estimate for the node into probability estimates for children of the node. Probability estimates for all children of a node must add to the probability estimate of the parent, because if P (parent) is the probability that a parent position is part of the theoretical PV and the parent n has children, P (child1 | parent) + P (child2 | parent) +... + P (child2 | parent) = 1, assuming that it is exactly a theoretical PV. From elementary probability theory, we also know that for every child i, P (Childi & parent) have the best type code."}, {"heading": "5.4 Depth-Limited Search vs Probability-Limited Search", "text": "This is because, in most cases, the branching factor is roughly constant and the knot probabilities decrease at a consistent rate, and the search will end at a roughly uniform depth. However, it is different when there are positions with sub-trees with different branching factors. An extreme example is verification - when verification is given, the opponent usually has few evasive movements available. Another example is when a branch exchanges high mobility pieces (e.g. queens). In these cases, branches would have much lower branching factors after replacement. Figure 5 below shows what the search tree would look like in such a situation, and which nodes require a depth-limited search and a probability-limited search."}, {"heading": "6 Neural Network-Based Probability Estimation", "text": "In the previous section, we explained how, instead of searching all branches to the same depth, we can search all branches to the same probability threshold. We divided the probability evenly between each child beforehand, so: 3Please see Appendix A for a description of the Elo rating system. P (childi | parent) = 1 nwhere n is the number of children for the parent. In this section, we explore the possibility of using a neural network to estimate P (childi | parent), which should theoretically give us a more selective search. Specifically, we train a network to generate a probability distribution P (childi | parent) specifying a parent position, as well as information about a move (which leads to the child). Afterwards, we would use it during the search for each position to evaluate and normalize each legal move to generate a probability distribution, which we then multiply by the probability of the parent to generate a probability for each child."}, {"heading": "6.1 Feature Representation", "text": "The characteristic representation for this function consists of two parts - characteristics that describe the parent position, and characteristics that describe the move taking into account. The part that describes the parent position is exactly the same as the characteristic representation that we use for the position evaluation, as described in Section 4.1. Additional characteristics are as follows: \u2022 Figure type - What kind of figure is it? \u2022 Square - \"From the square of the train to the square -\" To the square of the train \"\u2022 Carriage type - For promotions depends on the way in which the pawn advances \u2022 Rank - How does the move compare with other legal moves? Most additional characteristics are self-explanatory, but the ranking function is a little curious. The probability that a move is the best in a given situation depends on other legal moves for that position. For example, if there is a four-way equivalence for the most likely best moves for a position, any move that is significantly worse than the four is likely to be considered at least that some of these moves must not be evaluated at all."}, {"heading": "6.2 Network Architecture", "text": "The network architecture for the probability estimation network is very similar to the architecture for the evaluation network described in Section 4.2. The only difference is that the output node has logistical activation instead of hyperbolic tangential activation, which shifts the output to the range (0, 1) for arithmetical reasons (easier calculation of gradients with entropy loss), and has rectified linear activation for hidden nodes and separately connected layers for features from the three different modalities. See Section 4.2 for a more detailed explanation of this connectivity scheme."}, {"heading": "6.3 Training Positions Generation", "text": "For the training of the probability estimation network, we also need a corpus of blank positions. However, we do not use the same corpus as for the training of the evaluation network, because the distributions are different. In order to train the evaluation network with TD-Leaf (\u03bb), the training places must be representative of the root positions that will occur in the actual games. This means, for example, that there will be missing positions with severe material imbalance, as these positions do not normally occur in games. However, these positions very often occur in internal nodes of search trees. As the probability estimation network is used on internal nodes, we would like to have a training set distributed in the same way. In order to generate this training set, we perform time-limited searches at the root positions from the evaluation network training set (see section 4.3), and randomly selected samples from the positions encountered in these searches as internal nodes. The training set consists of 5 million positions."}, {"heading": "6.4 Network Training", "text": "Unlike the evaluation network, in this case there is no need for iterative learning of the time difference, since we already have a trained evaluator and can already find the best move for each training position simply by searching through a static equality probability estimator. The first step of the training process is to mark each training position with the best move by performing a time-limited search at each training position. Once we have the designated positions, we generate the training set for the gradient descent by combining each position with all legal movements from that position, which are characterized by a binary training target indicating whether the move is the best or not. Then, we perform a stochastic gradient descent using the AdaDelta updating rules, using the function of cross entropy loss."}, {"heading": "6.5 Results and Evaluation", "text": "The trained network proved to be very effective. Table 4 below shows the predictive power of the network. 46% of the time, the actual best move turned out to be the predicted best move. 70% of the time, it turned out that the actual best move was among the three best predicted moves. Table 5 shows the result on another test set, in which all positions were filtered out where the best move was a winning capture (simple positions). Even among these difficult calm positions, the correct best move is predicted 28% of the time, and the best move is among the three predicted moves 55% of the time.In terms of actual game strength, the giraffe is much stronger with the neural network probability estimator. After 3000 games, the difference is 48 \u00b1 12 Elo points, which is a score distribution of 57% versus 43%. However, in terms of actual game strength, the neural network is too slow to be used anywhere in a search tree."}, {"heading": "7 Conclusion", "text": "The results show that the learned system at least4Please see Appendix A for a description of the Elo rating system.comparable to the best expert-designed counters in existence today, many of which have fine tuned over the course of decades.The beauty of this approach lies in its generality. While it has not been researched in this project due to lack of time, it is likely that this approach can easily be ported to other zero-sum games, and that it achieves state-of-the-art performance quickly, especially in games where there has been decades of intensive research into creating a strong AI player. In addition to the machine learning aspects of the project, we have introduced and tested an alternative variant of the decades-long Minimax algorithm, in which we apply probability limits to limit the search tree."}, {"heading": "7.1 Future Work", "text": "Many interesting ideas and potential improvements for giraffe have not been implemented due to time constraints. In this section some of these ideas are listed."}, {"heading": "7.1.1 Neural Network-Based Time Management", "text": "Time management is an important part of chess. Different tournaments use different time controls (for example, the usual time control in online play is 5 minutes per page during the entire game or 2 minutes per page, with 12 seconds added after each turn).However, they all require that players split the allotted time and decide how much time they spend on each turn. There are many considerations - the complexity of the position, the expected remaining playing time and the confidence in the currently chosen turn (as the thinking goes on).Giraffe and practically all other chess engines today use simple heuristics to decide how much time to spend per turn."}, {"heading": "7.1.2 More Fine-Grained Caching, and Model Compression", "text": "One of the greatest weaknesses of giraffes is the search speed. Although it is competitive against many motors even at very low speeds, it would be much stronger if it could close the speed gap. Giraffe already stores outputs of neural networks. However, the cache hit rate is low as the exact same positions are rarely evaluated twice. However, most positions that occur in a search tree will be similar, and with clever designs of network connectivity it might be possible to cache intermediate activations within networks. Some recent discoveries in the formation of neural networks by Ba and Caruana suggest that it may be possible to compress large neural networks by using them as mentors to form smaller networks [42]. This is another potential way to speed up giraffes."}, {"heading": "7.1.3 Evaluation Error Estimation", "text": "When the static evaluation function is called to evaluate a position, it only needs to give an accurate evaluation if it is within a range set by the caller. Due to the functionality of the \u03b1-\u03b2 section, it may be advantageous to be able to quickly estimate the upper and lower limit of a position's score, because if the possible score range is completely outside the \u03b1-\u03b2 window, there is no need to perform the costly full evaluation. One possible way to make a bound estimate is to train a network with two outputs - the two limits - both outputs can be trained to predict the evaluation results, but with different error functions. For example, asymmetric cost functions, which punish positive errors much more than negative errors, can be used to train the performance for lower limits, and vice versa."}, {"heading": "7.1.4 Similarity Pruning", "text": "One of the reasons for the difference is how accurately positions are evaluated. If positions are not evaluated accurately, deeper and broader searches are needed to compensate for them. Closing this gap was the main focus of this project. Another reason for the high search efficiency of humans is the concept of position similarity. People can often decide that some movements are effectively equivalent in some situations and avoid searching them all. This reduces the average branching factor of search readers.One possible way to do this using machine learning is to use a neural network that takes positions as inputs and prints sequences of numbers that can function as \"signatures\" for each position. Unattended learning (for example, clusters) can then be applied to signatures to measure the degree of similarity between positions. However, it is unclear how such networks can be traced back."}, {"heading": "Appendix A The Elo Rating System", "text": "The Elo rating system is a mathematical model that assigns ratings to players based on game outcomes, ensuring that a stronger player would receive a higher rating than a weaker player. Ratings are only comparable within the same pool of players. A score difference of 200 equals a score gap of about 75% / 25%, with the stronger player expecting to receive 75% of the points. In this work, unless otherwise stated, all ratings are on the FIDE scale. On the FIDE scale, a beginner who knows the rules of the game would have a rating of about 1000. A serious tournament player of low level would have a rating of about 1800. Magnus Carlsen, the reigning world chess champion, has a rating of 2853 [43] (list as of September 2015). The best chess engines are estimated to have ratings above 3200. However, there are not enough games between computers and people with official FIDE ratings to establish an exact relationship between human and computer ratings."}], "references": [{"title": "Deep blue system overview", "author": ["Feng-hsiung Hsu", "Murray S Campbell", "A Joseph Hoane Jr."], "venue": "In Proceedings of the 9th international conference on Supercomputing,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 1995}, {"title": "Behind Deep Blue: Building the computer that defeated the world chess champion", "author": ["Feng-Hsiung Hsu"], "venue": null, "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2002}, {"title": "Zur theorie der gesellschaftsspiele", "author": ["J v Neumann"], "venue": "Mathematische Annalen,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 1928}, {"title": "Xxii. programming a computer for playing chess", "author": ["Claude E Shannon"], "venue": "The London, Edinburgh, and Dublin Philosophical Magazine and Journal of Science,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 1950}, {"title": "Searching for solutions in games and artificial intelligence", "author": ["Louis Victor Allis"], "venue": "Ponsen & Looijen,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 1994}, {"title": "An upper bound for the number of reachable positions", "author": ["Shirish Chinchalkar"], "venue": "ICCA JOURNAL,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 1996}, {"title": "Computer science as empirical inquiry", "author": ["Allen Newell", "Herbert A Simon"], "venue": "Symbols and search. Communications of the ACM,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 1976}, {"title": "GP-endchess: Using genetic programming to evolve chess endgame", "author": ["Ami Hauptman", "Moshe Sipper"], "venue": null, "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2005}, {"title": "Genetically programmed strategies for chess endgame", "author": ["Nicolas Lassabe", "St\u00e9phane Sanchez", "Herv\u00e9e Luga", "Yves Duthen"], "venue": "In Proceedings of the 8th annual conference on genetic and evolutionary computation,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2006}, {"title": "A comparative study of inductive learning systems aqllp and id-3 using a chess endgame test problem", "author": ["Paul O\u2019Rorke", "File No UIUCDCS-F"], "venue": "Technical Dep. Comput. Sci., Univ. Illinois,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 1982}, {"title": "Experiments in parameter learning using temporal differences", "author": ["Jonathan Baxter", "Andrew Tridgell", "Lex Weaver"], "venue": "International Computer Chess Association Journal,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 1998}, {"title": "Genetic algorithms for mentorassisted evaluation function optimization", "author": ["Omid David-Tabibi", "Moshe Koppel", "Nathan S Netanyahu"], "venue": "In Proceedings of the 10th annual conference on Genetic and evolutionary computation,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2008}, {"title": "Bootstrapping from game tree search. In Advances in neural information processing", "author": ["Joel Veness", "David Silver", "Alan Blair", "William W Cohen"], "venue": null, "citeRegEx": "16", "shortCiteRegEx": "16", "year": 1937}, {"title": "Learning to play the game of chess", "author": ["Sebastian Thrun"], "venue": "Advances in neural information processing systems,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 1995}, {"title": "Temporal difference learning and td-gammon", "author": ["Gerald Tesauro"], "venue": "Communications of the ACM,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 1995}, {"title": "Temporal difference learning of position evaluation in the game of go", "author": ["Nicol N Schraudolph", "Peter Dayan", "Terrence J Sejnowski"], "venue": "Advances in Neural Information Processing Systems,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 1994}, {"title": "Tdleaf (lambda): Combining temporal difference learning with game-tree search", "author": ["Jonathan Baxter", "Andrew Tridgell", "Lex Weaver"], "venue": "arXiv preprint cs/9901001,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 1999}, {"title": "Learning multiple layers of representation", "author": ["Geoffrey E Hinton"], "venue": "Trends in cognitive sciences,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2007}, {"title": "Imagenet classification with deep convolutional neural networks. In Advances in neural information processing", "author": ["Alex Krizhevsky", "Ilya Sutskever", "Geoffrey E Hinton"], "venue": null, "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2012}, {"title": "Playing atari with deep reinforcement learning", "author": ["Volodymyr Mnih", "Koray Kavukcuoglu", "David Silver", "Alex Graves", "Ioannis Antonoglou", "Daan Wierstra", "Martin Riedmiller"], "venue": "arXiv preprint arXiv:1312.5602,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2013}, {"title": "Efficiency of three forward-pruning techniques in shogi: Futility pruning, null-move pruning, and late move reduction (lmr)", "author": ["Kunihito Hoki", "Masakazu Muramatsu"], "venue": "Entertainment Computing,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2012}, {"title": "Adaptive null-move pruning", "author": ["Ernst A Heinz"], "venue": null, "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2000}, {"title": "On the importance of initialization and momentum in deep learning", "author": ["Ilya Sutskever", "James Martens", "George Dahl", "Geoffrey Hinton"], "venue": "In Proceedings of the 30th International Conference on Machine Learning", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2013}, {"title": "Adaptive subgradient methods for online learning and stochastic optimization", "author": ["John Duchi", "Elad Hazan", "Yoram Singer"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2011}, {"title": "Adadelta: an adaptive learning rate method", "author": ["Matthew D Zeiler"], "venue": "arXiv preprint arXiv: 1212.5701,", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2012}, {"title": "Deep sparse rectifier networks", "author": ["Xavier Glorot", "Antoine Bordes", "Yoshua Bengio"], "venue": "In Proceedings of the 14th International Conference on Artificial Intelligence and Statistics. JMLR W&CP Volume,", "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2011}, {"title": "A method of solving a convex programming problem with convergence rate o (1/k2)", "author": ["Yurii Nesterov"], "venue": "In Soviet Mathematics Doklady,", "citeRegEx": "32", "shortCiteRegEx": "32", "year": 1983}, {"title": "Rprop-a fast adaptive learning algorithm", "author": ["Martin Riedmiller", "Heinrich Braun"], "venue": "In Proc. of ISCIS VII), Universitat. Citeseer,", "citeRegEx": "33", "shortCiteRegEx": "33", "year": 1992}, {"title": "Cybernetics, or communication and control in the animal and the machine", "author": ["Norbert Wiener"], "venue": null, "citeRegEx": "35", "shortCiteRegEx": "35", "year": 1948}, {"title": "Experiments with the null-move heuristic", "author": ["Gordon Goetsch", "Murray S Campbell"], "venue": "In Computers, Chess, and Cognition,", "citeRegEx": "36", "shortCiteRegEx": "36", "year": 1990}, {"title": "An analysis of forward pruning", "author": ["Stephen JJ Smith", "Dana S Nau"], "venue": "In AAAI,", "citeRegEx": "37", "shortCiteRegEx": "37", "year": 1994}, {"title": "Extended futility pruning", "author": ["Ernst A Heinz"], "venue": null, "citeRegEx": "38", "shortCiteRegEx": "38", "year": 2000}, {"title": "Searching to variable depth in computer chess", "author": ["Hermann Kaindl"], "venue": "In IJCAI,", "citeRegEx": "39", "shortCiteRegEx": "39", "year": 1983}, {"title": "How darkthought plays chess", "author": ["Ernst A Heinz"], "venue": null, "citeRegEx": "40", "shortCiteRegEx": "40", "year": 2000}, {"title": "Do deep nets really need to be deep", "author": ["Jimmy Ba", "Rich Caruana"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "42", "shortCiteRegEx": "42", "year": 2014}], "referenceMentions": [{"referenceID": 0, "context": "In a sense, the way humans play chess is much more computationally efficient - using Garry Kasparov vs Deep Blue as an example, Kasparov could not have been searching more than 3-5 positions per second, while Deep Blue, a supercomputer with 480 custom \u201dchess processors\u201d, searched about 200 million positions per second [1][2] to play at approximately equal strength (Deep Blue won the 6-game match with 2 wins, 3 draws, and 1 loss).", "startOffset": 320, "endOffset": 323}, {"referenceID": 1, "context": "In a sense, the way humans play chess is much more computationally efficient - using Garry Kasparov vs Deep Blue as an example, Kasparov could not have been searching more than 3-5 positions per second, while Deep Blue, a supercomputer with 480 custom \u201dchess processors\u201d, searched about 200 million positions per second [1][2] to play at approximately equal strength (Deep Blue won the 6-game match with 2 wins, 3 draws, and 1 loss).", "startOffset": 323, "endOffset": 326}, {"referenceID": 2, "context": "They are all based on the idea of the fixed-depth minimax algorithm first developed by John von Neumann in 1928 [4], and adapted for the problem of chess by Claude E.", "startOffset": 112, "endOffset": 115}, {"referenceID": 3, "context": "Shannon in 1950 [5].", "startOffset": 16, "endOffset": 19}, {"referenceID": 4, "context": "The search tree size for chess is estimated to be about 10123 [6] (1046 without repetitions [7]) which is more than one hundred orders of magnitudes higher than what is computationally feasible using modern computers.", "startOffset": 62, "endOffset": 65}, {"referenceID": 5, "context": "The search tree size for chess is estimated to be about 10123 [6] (1046 without repetitions [7]) which is more than one hundred orders of magnitudes higher than what is computationally feasible using modern computers.", "startOffset": 92, "endOffset": 95}, {"referenceID": 6, "context": "This optimization is called \u03b1-\u03b2 pruning [8], where \u03b1 and \u03b2 are the lower and upper bounds.", "startOffset": 40, "endOffset": 43}, {"referenceID": 7, "context": "Most attempts in this area use genetic programming [11] [12], and inductive learning [13].", "startOffset": 51, "endOffset": 55}, {"referenceID": 8, "context": "Most attempts in this area use genetic programming [11] [12], and inductive learning [13].", "startOffset": 56, "endOffset": 60}, {"referenceID": 9, "context": "Most attempts in this area use genetic programming [11] [12], and inductive learning [13].", "startOffset": 85, "endOffset": 89}, {"referenceID": 10, "context": "NeuroChess [14] is one such attempt.", "startOffset": 11, "endOffset": 15}, {"referenceID": 11, "context": "The Falcon project [15] is an attempt to tune evaluation parameters using genetic programming, with the help of a mentor, which has a much more complicated evaluation function.", "startOffset": 19, "endOffset": 23}, {"referenceID": 12, "context": "[16], whose evaluation function is a linear combination of hand-designed features, and where the weights are trained using reinforcement learning.", "startOffset": 0, "endOffset": 4}, {"referenceID": 13, "context": "won, lost, or drawn positions), models with higher predictive power would have better temporal consistency (this is not true if no position has fixed score, since the model can always produce a constant value for example, and achieve temporal consistency that way [18]).", "startOffset": 264, "endOffset": 268}, {"referenceID": 14, "context": "The most famous study in using temporal-difference reinforcement learning to play zero-sum games is probably Tesauro\u2019s backgammon program TD-Gammon [19] from 1995.", "startOffset": 148, "endOffset": 152}, {"referenceID": 15, "context": "[20], but they were only able to achieve \u201da low playing level\u201d [20], due to the complexity of the game, and the necessity to look ahead.", "startOffset": 0, "endOffset": 4}, {"referenceID": 15, "context": "[20], but they were only able to achieve \u201da low playing level\u201d [20], due to the complexity of the game, and the necessity to look ahead.", "startOffset": 63, "endOffset": 67}, {"referenceID": 13, "context": "A similar approach was used by Thrun in his NeuroChess program [18], which, when used with GNU Chess\u2019s search routine, wins approximately 13% of all games against GNU Chess.", "startOffset": 63, "endOffset": 67}, {"referenceID": 13, "context": "The author attributed the disappointing performance to inefficiency in implementation, as neural network evaluation, as implemented in NeuroChess, takes \u201dtwo orders of magnitudes longer than evaluating an optimized linear evaluation function (like that of GNU-Chess)\u201d [18].", "startOffset": 268, "endOffset": 272}, {"referenceID": 16, "context": "adapted the temporal-difference learning algorithm to be used in conjunction with minimax searches [21].", "startOffset": 99, "endOffset": 103}, {"referenceID": 16, "context": "Although not explored in their study, they also postulate that without significantly simplifying (and speeding up) the evaluation function, the best way to solve tactical myopia is by finding \u201dan effective algorithm for [learning] to search [selectively],\u201d which would have \u201dpotential for far greater improvement\u201d [21].", "startOffset": 314, "endOffset": 318}, {"referenceID": 17, "context": "[22].", "startOffset": 0, "endOffset": 4}, {"referenceID": 18, "context": "2% [23].", "startOffset": 3, "endOffset": 7}, {"referenceID": 18, "context": "3% error [23].", "startOffset": 9, "endOffset": 13}, {"referenceID": 19, "context": "The networks they trained were able to surpass the performance of previous game-specific AIs on six of the seven games, and exceeded human expert performance on three of them [24].", "startOffset": 175, "endOffset": 179}, {"referenceID": 16, "context": "The first part of the project is essentially a modern iteration of the TDLeaf(\u03bb) experiment [21] on learning new evaluation functions, taking advantage of discoveries and advances in neural networks in the past 15 years, as well as the much higher computational power available today.", "startOffset": 92, "endOffset": 96}, {"referenceID": 20, "context": "It is already well-established that strategic extensions, reductions, and pruning have huge benefits on the performance of chess engines [25] [26], however, existing heuristics for making those decisions are mechanical and quite crude, and must be done very conservatively to not miss tactical details.", "startOffset": 137, "endOffset": 141}, {"referenceID": 21, "context": "It is already well-established that strategic extensions, reductions, and pruning have huge benefits on the performance of chess engines [25] [26], however, existing heuristics for making those decisions are mechanical and quite crude, and must be done very conservatively to not miss tactical details.", "startOffset": 142, "endOffset": 146}, {"referenceID": 22, "context": "A few state-of-art optimization algorithms have also been implemented - mini-batch stochastic gradient descent with Nesterov\u2019s accelerated momentum [28], the AdaGrad adaptive subgradient method [29], and an improved version of the AdaGrad method known as AdaDelta [30].", "startOffset": 148, "endOffset": 152}, {"referenceID": 23, "context": "A few state-of-art optimization algorithms have also been implemented - mini-batch stochastic gradient descent with Nesterov\u2019s accelerated momentum [28], the AdaGrad adaptive subgradient method [29], and an improved version of the AdaGrad method known as AdaDelta [30].", "startOffset": 194, "endOffset": 198}, {"referenceID": 24, "context": "A few state-of-art optimization algorithms have also been implemented - mini-batch stochastic gradient descent with Nesterov\u2019s accelerated momentum [28], the AdaGrad adaptive subgradient method [29], and an improved version of the AdaGrad method known as AdaDelta [30].", "startOffset": 264, "endOffset": 268}, {"referenceID": 25, "context": "The evaluator network is a 4-layer network (three hidden layers plus output layer), where all hidden nodes use Rectified Linear activation (ReLU) [31], which is a modern choice that is much more efficient than the traditional hyperbolic tangent and logistic activation functions, especially for networks with more than two hidden layers.", "startOffset": 146, "endOffset": 150}, {"referenceID": 11, "context": "For Falcon (an engine based on genetic programming), a 10,000-games database of grandmaster games is used, with 1 position drawn from each game [15].", "startOffset": 144, "endOffset": 148}, {"referenceID": 11, "context": "This gives them more positions with material imbalances than would typically appear in grandmaster-level games [15].", "startOffset": 111, "endOffset": 115}, {"referenceID": 16, "context": "For KnightCap (an engine based on using TD-Leaf to tune the parameters of a hand-coded evaluation function), online play on the Free Internet Chess Server is used [21].", "startOffset": 163, "endOffset": 167}, {"referenceID": 11, "context": "It is possible to use another stronger chess engine as a mentor to generate ground-truth labels (as has been done for the Falcon project [15]), but for this project, we are aiming to create a system that can learn to play chess with no external help, and as little built-in chess knowledge as possible.", "startOffset": 137, "endOffset": 141}, {"referenceID": 16, "context": "In this project, we use a modified version of the algorithm known as TD-Leaf(\u03bb), which allows faster learning by exploiting the fact that the local gradient of minimax is simply the gradient of the evaluation function at the leaf node of the search [21].", "startOffset": 249, "endOffset": 253}, {"referenceID": 16, "context": "where w is the set of weights of the model, \u03b1 is the learning rate (set to 1 in our case), \u2207J(xt, w) is the gradient of the model at each time point, and dt is the temporal differences from each time point dt to the next time point [21].", "startOffset": 232, "endOffset": 236}, {"referenceID": 24, "context": "The gradients are then used to train the network using stochastic gradient descent with AdaDelta update rules [30].", "startOffset": 110, "endOffset": 114}, {"referenceID": 26, "context": "Many different optimizations were implemented for this project - SGD with momentum, SGD with Nesterov\u2019s accelerated gradient [32], and adaptive gradient (AdaGrad) [29].", "startOffset": 125, "endOffset": 129}, {"referenceID": 23, "context": "Many different optimizations were implemented for this project - SGD with momentum, SGD with Nesterov\u2019s accelerated gradient [32], and adaptive gradient (AdaGrad) [29].", "startOffset": 163, "endOffset": 167}, {"referenceID": 24, "context": "It allows weights for neurons that are rarely activated to retain higher learning rates to make best use of limited activations, while allowing weights for neurons that are often activated to decrease over time so that they can converge to better minimums [30].", "startOffset": 256, "endOffset": 260}, {"referenceID": 27, "context": "This is similar in idea to the much older resilient back-propagation (RPROP) algorithm [33], which does not work in a stochastic setting due to gradients frequently changing signs.", "startOffset": 87, "endOffset": 91}, {"referenceID": 2, "context": "When von Neumann introduced the minimax algorithm in 1928, he focused on cases where the entire game tree can be explored [4].", "startOffset": 122, "endOffset": 125}, {"referenceID": 28, "context": "Norbert\u2019s rendition of minimax in 1948 is the first known application of incomplete minimax that uses a heuristic function at leaves of the search tree [35].", "startOffset": 152, "endOffset": 156}, {"referenceID": 29, "context": "While modern chess engines have improved on that by introducing various types of extensions and reductions [36] [37] [38], the fundamental idea of using depth to define search trees has not changed.", "startOffset": 107, "endOffset": 111}, {"referenceID": 30, "context": "While modern chess engines have improved on that by introducing various types of extensions and reductions [36] [37] [38], the fundamental idea of using depth to define search trees has not changed.", "startOffset": 112, "endOffset": 116}, {"referenceID": 31, "context": "While modern chess engines have improved on that by introducing various types of extensions and reductions [36] [37] [38], the fundamental idea of using depth to define search trees has not changed.", "startOffset": 117, "endOffset": 121}, {"referenceID": 32, "context": "Interestingly, in depth-limited searches, it has been found that extending the search in cases of checks [39] and situations where there is only a single reply [40] is beneficial.", "startOffset": 105, "endOffset": 109}, {"referenceID": 33, "context": "Interestingly, in depth-limited searches, it has been found that extending the search in cases of checks [39] and situations where there is only a single reply [40] is beneficial.", "startOffset": 160, "endOffset": 164}, {"referenceID": 20, "context": "This technique is known as late move reduction [25].", "startOffset": 47, "endOffset": 51}, {"referenceID": 13, "context": "For example, Thrun reported a difference of 2 orders of magnitude between his implementation of a neural-network based chess engine and a state of art chess engine of that time [18].", "startOffset": 177, "endOffset": 181}, {"referenceID": 34, "context": "Some recent discoveries in neural network training by Ba and Caruana suggest that it may be possible to compress large neural networks by using them as mentors to train smaller networks [42].", "startOffset": 186, "endOffset": 190}], "year": 2015, "abstractText": "This report presents Giraffe, a chess engine that uses self-play to discover all its domain-specific knowledge, with minimal hand-crafted knowledge given by the programmer. Unlike previous attempts using machine learning only to perform parametertuning on hand-crafted evaluation functions, Giraffe\u2019s learning system also performs automatic feature extraction and pattern recognition. The trained evaluation function performs comparably to the evaluation functions of state-of-the-art chess engines all of which containing thousands of lines of carefully hand-crafted pattern recognizers, tuned over many years by both computer chess experts and human chess masters. Giraffe is the most successful attempt thus far at using end-to-end machine learning to play chess. We also investigated the possibility of using probability thresholds instead of depth to shape search trees. Depth-based searches form the backbone of virtually all chess engines in existence today, and is an algorithm that has become well-established over the past half century. Preliminary comparisons between a basic implementation of probability-based search and a basic implementation of depth-based search showed that our new probability-based approach performs moderately better than the established approach. There are also evidences suggesting that many successful ad-hoc add-ons to depth-based searches are generalized by switching to a probability-based search. We believe the probability-based search to be a more fundamentally correct way to perform minimax. Finally, we designed another machine learning system to shape search trees within the probability-based search framework. Given any position, this system estimates the probability of each of the moves being the best move without looking ahead. The system is highly effective the actual best move is within the top 3 ranked moves 70% of the time, out of an average of approximately 35 legal moves from each position. This also resulted in a significant increase in playing strength. With the move evaluator guiding a probability-based search using the learned evaluator, Giraffe plays at approximately the level of an FIDE International Master (top 2.2% of tournament chess players with an official rating)12. F\u00e9d\u00e9ration Internationale des \u00c9checs, or the World Chess Federation, is the international organisation that governs all major international chess competitions. Please see Appendix A for a description of the Elo rating system.", "creator": "LaTeX with hyperref package"}}}