{"id": "1509.01618", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "4-Sep-2015", "title": "Efficient Sampling for k-Determinantal Point Processes", "abstract": "Determinantal Point Processes (DPPs) provide probabilistic models over discrete sets of items that help model repulsion and diversity. Applicability of DPPs to large sets of data is, however, hindered by the expensive matrix operations involved, especially when sampling. We therefore propose a new efficient approximate two-stage sampling algorithm for discrete k-DPPs. As opposed to previous approximations, our algorithm aims at minimizing the variational distance to the original distribution. Experiments indicate that the resulting sampling algorithm works well on large data and yields more accurate samples than previous approaches.", "histories": [["v1", "Fri, 4 Sep 2015 21:38:17 GMT  (140kb,D)", "https://arxiv.org/abs/1509.01618v1", null], ["v2", "Sat, 28 May 2016 00:37:56 GMT  (176kb,D)", "http://arxiv.org/abs/1509.01618v2", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["chengtao li", "stefanie jegelka", "suvrit sra"], "accepted": false, "id": "1509.01618"}, "pdf": {"name": "1509.01618.pdf", "metadata": {"source": "CRF", "title": "Efficient Sampling for k-Determinantal Point Processes", "authors": ["Chengtao Li", "Stefanie Jegelka", "Suvrit Sra"], "emails": ["ctli@mit.edu", "stefje@csail.mit.edu", "suvrit@mit.edu"], "sections": [{"heading": "1 Introduction", "text": "This year, more than ever before in the history of the country in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is not a country, but in which it is a country, a country, a city and a country."}, {"heading": "2 Setup and basic definitions", "text": "A determinant point process Dpp (L) is a distribution over all subsets of a fundamental set Y of cardinality N. It is determined by a positive semidefinitive kernel. LY is the submatrix of L, consisting of the entries Lij with i, j and Y Y. Then the probability PL (Y) of observing Y Y is proportional to det (LY); consequently PL (Y) = det (LY) / det (L + I)."}, {"heading": "3 Coreset sampling", "text": "Let us ask whether this is a division of Y, i.e. a division of Y, i.e. a division of Y, i.e. a division of Y, i.e. a division of Y, i.e. a division of Y, i.e. a division of Y, i.e. a division of Y, i.e. a division of Y, i.e. a division of Y, i.e. a division of Y, i.e. a division of Y, i.e. a division of Y, i.e. a division of Y, i.e. a division of Y, i.e. a division of Y, i.e. a division of Y, i.e. a division of Y, i.e. a division of Y, i.e. a division of Y, i.e. a division of Y, i.e. a division of Y, i.e. a division of Y, i.e. a division of Y, i.e. a division of Y, i.e. a division of Y, i.e. a division of Y, i.e. a division of Y, i.e. a division of Y, i.e. a division of Y, i.e. a division of Y, i.e."}, {"heading": "4 Partition, distortion and approximation error", "text": "Let us give some insights into quantities that affect the distance between the individual elements, k \u2212 Pk \u2012 tv when we stomp with algo (1). In a nutshell, this distance depends on three key variables (defined below): the probability of non-singularity is then not singular in relation to any other factors, and the normalization factor. For a partition, we define non-singularity as a partition-dependent quantity, so that the probability of non-singularity dppk (L) is not singular in relation to any other factors. In the face of a coreset C, we define the distortion factor 1 + LS \u00b2 as a partition-dependent quantity, so that for all c \u00b2 C, for all u \u00b2 Yc, and for all (k \u2212 1) singular elements we set in relation to Yc."}, {"heading": "5 Efficient construction", "text": "We have the option of setting up such a partition with a low probability under Dppk (L). The optimal such partition minimizes the probability that we are aiming for such a partition with a low probability. A small value also means that the parts of the system are dense and compact, i.e., the diameter of Yc in Equation (4.4) is small.Finding such a partition is optimal, so we resort to a local search."}, {"heading": "6 Experiments", "text": "Next, we explore the effectiveness and effectiveness of three competing approaches: - The division of data on the one hand, on the other, on the other, on the third, on the third, on the third, on the fourth, on the fourth, on the fourth, on the fourth, on the fourth, on the fourth, on the fourth, on the fourth, on the fourth, on the fourth, on the fourth, on the fourth, on the fourth, on the fourth, on the fourth, on the fourth, on the fourth, on the fourth, on the fourth, on the fourth, on the fourth, on the fourth, on the fourth, on the fourth, on the fourth, on the fourth, on the fourth, on the fourth, on the fourth, on the fourth, on the fourth, on the fourth, on the fourth, on the fourth, on the fourth, on the fourth, on the fourth, on the fourth, on the, on the, on the, on the, on the, on the, on the, on the, on the, on the, on the, on the, on the fourth, on the, on the, on the, on the, on the, on the, on the, on the, on the, on the, on the, on the, on the, on the, on the, on the, on the, on the, on the, on the, on the, on the, on the, on the, on the, on the, on the, on the, on the, on the, on the, on the, on the, on the, on the, on the, on the, on the, on the, on the, on the, on the, on the, on the, on the, on the, on the, on the, on the, on the, on the, on the, on the, on the, on the, on the, on the, on the, on the, on the, on the, on the, on the, on the, on the, on the, on the, on the, on the, on the, on the, on the, on the, on the, on the, on the, on the, on the, on the, on the, on the, on the, on the, on the, on the, on the, on the, on the, on the, on the, on the,"}, {"heading": "7 Conclusion", "text": "In contrast to other approaches, our algorithm aims directly to minimize the total deviation between the approximate and the original probability distribution. Our experiments demonstrate the effectiveness and efficiency of our approach: Our design not only exhibits lower errors in the total deviation compared to other methods, but also produces these more precise samples efficiently, at comparable or faster speeds than other methods. Recognition. In part, this research was accepted by an NSF CAREER Award 1553284 and a Google Research Award."}], "references": [{"title": "Markov Determinantal Point Processes", "author": ["R.H. Affandi", "A. Kulesza", "E.B. Fox"], "venue": "Uncertainty in Artificial Intelligence (UAI),", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2012}, {"title": "Approximate inference in continuous determinantal processes", "author": ["R.H. Affandi", "E. Fox", "B. Taskar"], "venue": "Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2013}, {"title": "Nystrom Approximation for Large-Scale Determinantal Processes", "author": ["R.H. Affandi", "A. Kulesza", "E.B. Fox", "B. Taskar"], "venue": "Proc. Int. Conference on Artificial Intelligence and Statistics (AISTATS),", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2013}, {"title": "Learning the Parameters of Determinantal Point Process Kernels", "author": ["R.H. Affandi", "E.B. Fox", "R.P. Adams", "B. Taskar"], "venue": "Int. Conference on Machine Learning (ICML),", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2014}, {"title": "Notes on using determinantal point processes for clustering with applications to text clustering", "author": ["A. Agarwal", "A. Choromanska", "K. Choromanski"], "venue": "arXiv preprint arXiv:1410.6975,", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2014}, {"title": "Monte Carlo Markov Chain algorithms for sampling strongly Rayleigh distributions and determinantal point processes", "author": ["N. Anari", "S.O. Gharan", "A. Rezaei"], "venue": "arXiv preprint arXiv:1602.05242,", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2016}, {"title": "k-means++: The advantages of careful seeding", "author": ["D. Arthur", "S. Vassilvitskii"], "venue": "SIAM-ACM Symposium on Discrete Algorithms (SODA),", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2007}, {"title": "Coresets for nonparametric estimation-the case of dp-means", "author": ["O. Bachem", "M. Lucic", "A. Krause"], "venue": "Int. Conference on Machine Learning (ICML), pages 209\u2013217,", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2015}, {"title": "Diversifying sparsity using variational determinantal point processes", "author": ["N.K. Batmanghelich", "G. Quon", "A. Kulesza", "M. Kellis", "P. Golland", "L. Bornn"], "venue": "arXiv preprint arXiv:1411.6307,", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2014}, {"title": "Spectral methods in machine learning and new strategies for very large datasets", "author": ["M.-A. Belabbas", "P.J. Wolfe"], "venue": "Proceedings of the National Academy of Sciences, pages 369\u2013374,", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2009}, {"title": "Determinantal point processes", "author": ["A. Borodin"], "venue": "arXiv preprint arXiv:0911.1153,", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2009}, {"title": "Eynard-Mehta Theorem, Schur Process, and Their Pfaffian Analogs", "author": ["A. Borodin", "E.M. Rains"], "venue": "Journal of Statistical Physics, 121(3-4):291\u2013317,", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2005}, {"title": "Algebraic complexity theory", "author": ["P. B\u00fcrgisser", "M. Clausen", "M.A. Shokrollahi"], "venue": "Grundlehren der mathematischen Wissenschaften, 315,", "citeRegEx": "13", "shortCiteRegEx": null, "year": 1997}, {"title": "Perfect simulation of determinantal point processes", "author": ["L. Decreusefond", "I. Flint", "K.C. Low"], "venue": "arXiv preprint arXiv:1311.1027,", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2013}, {"title": "Efficient Volume Sampling for Row/Column Subset Selection", "author": ["A. Deshpande", "L. Rademacher"], "venue": "IEEE Symposium on Foundations of Computer Science (FOCS),", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2010}, {"title": "Matrix approximation and projective clustering via volume sampling", "author": ["A. Deshpande", "L. Rademacher", "S. Vempala", "G. Wang"], "venue": "SIAM-ACM Symposium on Discrete Algorithms (SODA),", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2006}, {"title": "Coresets for weighted facilities and their applications", "author": ["D. Feldman", "A. Fiat", "M. Sharir"], "venue": "IEEE Symposium on Foundations of Computer Science (FOCS),", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2006}, {"title": "Turning big data into tiny data: Constant-size coresets for k-means, pca and projective clustering", "author": ["D. Feldman", "M. Schmidt", "C. Sohler"], "venue": "SIAM-ACM Symposium on Discrete Algorithms (SODA), pages 1434\u20131453,", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2013}, {"title": "Inference from iterative simulation using multiple sequences", "author": ["A. Gelman", "D.B. Rubin"], "venue": "Statistical science, pages 457\u2013472,", "citeRegEx": "19", "shortCiteRegEx": null, "year": 1992}, {"title": "Near-Optimal MAP Inference for Determinantal Point Processes", "author": ["J. Gillenwater", "A. Kulesza", "B. Taskar"], "venue": "Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2012}, {"title": "Discovering diverse and salient threads in document collections", "author": ["J. Gillenwater", "A. Kulesza", "B. Taskar"], "venue": "Conference on Empirical Methods in Natural Language Processing (EMNLP),", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2012}, {"title": "Expectation-Maximization for Learning Determinantal Point Processes", "author": ["J. Gillenwater", "E. Fox", "A. Kulesza", "B. Taskar"], "venue": "Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2014}, {"title": "Diverse Sequential Subset Selection for Supervised Video Summarization", "author": ["B. Gong", "W.-L. Chao", "K. Grauman", "S. Fei"], "venue": "Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2014}, {"title": "Smaller coresets for k-median and k-means clustering", "author": ["S. Har-Peled", "A. Kushal"], "venue": "Proceedings of the twenty-first annual symposium on Computational geometry, pages 126\u2013134,", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2005}, {"title": "On coresets for k-means and k-median clustering", "author": ["S. Har-Peled", "S. Mazumdar"], "venue": "Symposium on Theory of Computing (STOC), pages 291\u2013300,", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2004}, {"title": "On coresets for k-means and k-median clustering", "author": ["S. Har-Peled", "S. Mazumdar"], "venue": "Symposium on Theory of Computing (STOC), pages 291\u2013300,", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2004}, {"title": "Determinantal processes and independence", "author": ["J.B. Hough", "M. Krishnapur", "Y. Peres", "B. Vir\u00e1g"], "venue": "Probability Surveys,", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2006}, {"title": "Fast Determinantal Point Process Sampling with Application to Clustering", "author": ["B. Kang"], "venue": "Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2013}, {"title": "Near-optimal sensor placements in gaussian processes: Theory, efficient algorithms and empirical studies", "author": ["A. Krause", "A. Singh", "C. Guestrin"], "venue": "Journal of Machine Learning Research, 9: 235\u2013284,", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2008}, {"title": "Structured Determinantal Point Processes", "author": ["A. Kulesza", "B. Taskar"], "venue": "Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2010}, {"title": "Learning Determinantal Point Processes", "author": ["A. Kulesza", "B. Taskar"], "venue": "Uncertainty in Artificial Intelligence (UAI),", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2011}, {"title": "k-DPPs: Fixed-Size Determinantal Point Processes", "author": ["A. Kulesza", "B. Taskar"], "venue": "Int. Conference on Machine Learning (ICML),", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2011}, {"title": "Determinantal point processes for machine learning", "author": ["A. Kulesza", "B. Taskar"], "venue": "arXiv preprint arXiv:1207.6083,", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2012}, {"title": "Priors for diversity in generative latent variable models", "author": ["J.T. Kwok", "R.P. Adams"], "venue": "Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2012}, {"title": "Gradient-based learning applied to document recognition", "author": ["Y. LeCun", "L. Bottou", "Y. Bengio", "P. Haffner"], "venue": "Proceedings of the IEEE, 86(11):2278\u20132324,", "citeRegEx": "35", "shortCiteRegEx": null, "year": 1998}, {"title": "A class of submodular functions for document summarization", "author": ["H. Lin", "J. Bilmes"], "venue": "Anal. Meeting of the Association for Computational Linguistics (ACL),", "citeRegEx": "36", "shortCiteRegEx": null, "year": 2011}, {"title": "Near optimal dimensionality reductions that preserve volumes", "author": ["A. Magen", "A. Zouzias"], "venue": "Approximation, Randomization and Combinatorial Optimization. Algorithms and Techniques,", "citeRegEx": "37", "shortCiteRegEx": null, "year": 2008}, {"title": "Learning Determinantal Point Processes", "author": ["Z. Mariet", "S. Sra"], "venue": "Int. Conference on Machine Learning (ICML),", "citeRegEx": "38", "shortCiteRegEx": null, "year": 2015}, {"title": "Core-sets for canonical correlation analysis", "author": ["S. Paul"], "venue": "Int. Conference on Information and Knowledge Management (CIKM), pages 1887\u20131890,", "citeRegEx": "39", "shortCiteRegEx": null, "year": 2015}, {"title": "Coda: Convergence diagnosis and output analysis for mcmc", "author": ["M. Plummer", "N. Best", "K. Cowles", "K. Vines"], "venue": "R News, 6(1):7\u201311,", "citeRegEx": "40", "shortCiteRegEx": null, "year": 2006}, {"title": "Random Features for Large-scale Kernel Machines", "author": ["A. Rahimi", "B. Recht"], "venue": "Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "41", "shortCiteRegEx": null, "year": 2007}, {"title": "Coresets for k-segmentation of streaming data", "author": ["G. Rosman", "M. Volkov", "D. Feldman", "J.W. Fisher III", "D. Rus"], "venue": "Advances in Neural Information Processing Systems (NIPS), pages 559\u2013567,", "citeRegEx": "42", "shortCiteRegEx": null, "year": 2014}, {"title": "Determinantal clustering processes - a nonparametric bayesian approach to kernel based semi-supervised clustering", "author": ["A. Shah", "Z. Ghahramani"], "venue": "arXiv preprint arXiv:1309.6862,", "citeRegEx": "43", "shortCiteRegEx": null, "year": 2013}, {"title": "A Determinantal Point Process Latent Variable Model for Inhibition in Neural Spiking Data", "author": ["J. Snoek", "R. Zemel", "R.P. Adams"], "venue": "Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "44", "shortCiteRegEx": null, "year": 2013}, {"title": "Using The Matrix Ridge Approximation to Speedup Determinantal Point Processes Sampling Algorithms", "author": ["S. Wang", "C. Zhang", "H. Qian", "Z. Zhang"], "venue": "Proc. AAAI Conference on Artificial Intelligence,", "citeRegEx": "45", "shortCiteRegEx": null, "year": 2014}, {"title": "The Matrix Ridge Approximation: Algorithms", "author": ["Z. Zhang"], "venue": "Machine Learning, 97:227\u2013258,", "citeRegEx": "46", "shortCiteRegEx": null, "year": 2014}, {"title": "Solving the apparent diversity-accuracy dilemma of recommender systems", "author": ["T. Zhou", "Z. Kuscsik", "J.-G. Liu", "M. Medo", "J.R. Wakeling", "Y.-C. Zhang"], "venue": "Proceedings of the National Academy of Sciences, 107(10):4511\u20134515,", "citeRegEx": "47", "shortCiteRegEx": null, "year": 2010}], "referenceMentions": [{"referenceID": 8, "context": "Our focus is on diversity, a criterion that plays a key role in a variety of applications, such as gene network subsampling [9], document summarization [36], video summarization [23], content driven search [4], recommender systems [47], sensor placement [29], among many others [1, 5, 21, 30, 33, 43, 44].", "startOffset": 124, "endOffset": 127}, {"referenceID": 35, "context": "Our focus is on diversity, a criterion that plays a key role in a variety of applications, such as gene network subsampling [9], document summarization [36], video summarization [23], content driven search [4], recommender systems [47], sensor placement [29], among many others [1, 5, 21, 30, 33, 43, 44].", "startOffset": 152, "endOffset": 156}, {"referenceID": 22, "context": "Our focus is on diversity, a criterion that plays a key role in a variety of applications, such as gene network subsampling [9], document summarization [36], video summarization [23], content driven search [4], recommender systems [47], sensor placement [29], among many others [1, 5, 21, 30, 33, 43, 44].", "startOffset": 178, "endOffset": 182}, {"referenceID": 3, "context": "Our focus is on diversity, a criterion that plays a key role in a variety of applications, such as gene network subsampling [9], document summarization [36], video summarization [23], content driven search [4], recommender systems [47], sensor placement [29], among many others [1, 5, 21, 30, 33, 43, 44].", "startOffset": 206, "endOffset": 209}, {"referenceID": 46, "context": "Our focus is on diversity, a criterion that plays a key role in a variety of applications, such as gene network subsampling [9], document summarization [36], video summarization [23], content driven search [4], recommender systems [47], sensor placement [29], among many others [1, 5, 21, 30, 33, 43, 44].", "startOffset": 231, "endOffset": 235}, {"referenceID": 28, "context": "Our focus is on diversity, a criterion that plays a key role in a variety of applications, such as gene network subsampling [9], document summarization [36], video summarization [23], content driven search [4], recommender systems [47], sensor placement [29], among many others [1, 5, 21, 30, 33, 43, 44].", "startOffset": 254, "endOffset": 258}, {"referenceID": 0, "context": "Our focus is on diversity, a criterion that plays a key role in a variety of applications, such as gene network subsampling [9], document summarization [36], video summarization [23], content driven search [4], recommender systems [47], sensor placement [29], among many others [1, 5, 21, 30, 33, 43, 44].", "startOffset": 278, "endOffset": 304}, {"referenceID": 4, "context": "Our focus is on diversity, a criterion that plays a key role in a variety of applications, such as gene network subsampling [9], document summarization [36], video summarization [23], content driven search [4], recommender systems [47], sensor placement [29], among many others [1, 5, 21, 30, 33, 43, 44].", "startOffset": 278, "endOffset": 304}, {"referenceID": 20, "context": "Our focus is on diversity, a criterion that plays a key role in a variety of applications, such as gene network subsampling [9], document summarization [36], video summarization [23], content driven search [4], recommender systems [47], sensor placement [29], among many others [1, 5, 21, 30, 33, 43, 44].", "startOffset": 278, "endOffset": 304}, {"referenceID": 29, "context": "Our focus is on diversity, a criterion that plays a key role in a variety of applications, such as gene network subsampling [9], document summarization [36], video summarization [23], content driven search [4], recommender systems [47], sensor placement [29], among many others [1, 5, 21, 30, 33, 43, 44].", "startOffset": 278, "endOffset": 304}, {"referenceID": 32, "context": "Our focus is on diversity, a criterion that plays a key role in a variety of applications, such as gene network subsampling [9], document summarization [36], video summarization [23], content driven search [4], recommender systems [47], sensor placement [29], among many others [1, 5, 21, 30, 33, 43, 44].", "startOffset": 278, "endOffset": 304}, {"referenceID": 42, "context": "Our focus is on diversity, a criterion that plays a key role in a variety of applications, such as gene network subsampling [9], document summarization [36], video summarization [23], content driven search [4], recommender systems [47], sensor placement [29], among many others [1, 5, 21, 30, 33, 43, 44].", "startOffset": 278, "endOffset": 304}, {"referenceID": 43, "context": "Our focus is on diversity, a criterion that plays a key role in a variety of applications, such as gene network subsampling [9], document summarization [36], video summarization [23], content driven search [4], recommender systems [47], sensor placement [29], among many others [1, 5, 21, 30, 33, 43, 44].", "startOffset": 278, "endOffset": 304}, {"referenceID": 3, "context": "Dpps enjoy rising interest in machine learning [4, 23, 28, 30, 32, 34, 38]; a part of their appeal can be attributed to computational tractability of basic tasks such as computing partition functions, sampling, and extracting marginals [27, 33].", "startOffset": 47, "endOffset": 74}, {"referenceID": 22, "context": "Dpps enjoy rising interest in machine learning [4, 23, 28, 30, 32, 34, 38]; a part of their appeal can be attributed to computational tractability of basic tasks such as computing partition functions, sampling, and extracting marginals [27, 33].", "startOffset": 47, "endOffset": 74}, {"referenceID": 27, "context": "Dpps enjoy rising interest in machine learning [4, 23, 28, 30, 32, 34, 38]; a part of their appeal can be attributed to computational tractability of basic tasks such as computing partition functions, sampling, and extracting marginals [27, 33].", "startOffset": 47, "endOffset": 74}, {"referenceID": 29, "context": "Dpps enjoy rising interest in machine learning [4, 23, 28, 30, 32, 34, 38]; a part of their appeal can be attributed to computational tractability of basic tasks such as computing partition functions, sampling, and extracting marginals [27, 33].", "startOffset": 47, "endOffset": 74}, {"referenceID": 31, "context": "Dpps enjoy rising interest in machine learning [4, 23, 28, 30, 32, 34, 38]; a part of their appeal can be attributed to computational tractability of basic tasks such as computing partition functions, sampling, and extracting marginals [27, 33].", "startOffset": 47, "endOffset": 74}, {"referenceID": 33, "context": "Dpps enjoy rising interest in machine learning [4, 23, 28, 30, 32, 34, 38]; a part of their appeal can be attributed to computational tractability of basic tasks such as computing partition functions, sampling, and extracting marginals [27, 33].", "startOffset": 47, "endOffset": 74}, {"referenceID": 37, "context": "Dpps enjoy rising interest in machine learning [4, 23, 28, 30, 32, 34, 38]; a part of their appeal can be attributed to computational tractability of basic tasks such as computing partition functions, sampling, and extracting marginals [27, 33].", "startOffset": 47, "endOffset": 74}, {"referenceID": 26, "context": "Dpps enjoy rising interest in machine learning [4, 23, 28, 30, 32, 34, 38]; a part of their appeal can be attributed to computational tractability of basic tasks such as computing partition functions, sampling, and extracting marginals [27, 33].", "startOffset": 236, "endOffset": 244}, {"referenceID": 32, "context": "Dpps enjoy rising interest in machine learning [4, 23, 28, 30, 32, 34, 38]; a part of their appeal can be attributed to computational tractability of basic tasks such as computing partition functions, sampling, and extracting marginals [27, 33].", "startOffset": 236, "endOffset": 244}, {"referenceID": 31, "context": "Cubic preprocessing costs also impede wider use of the cardinality constrained variant k-Dpp [32].", "startOffset": 93, "endOffset": 97}, {"referenceID": 2, "context": "Much work has been devoted to approximately sample from a Dpp by first approximating its kernel via algorithms such as the Nystr\u00f6m method [3], Random Kitchen Sinks [2, 41], or matrix ridge approximations [45, 46], and then sampling based on this approximation.", "startOffset": 138, "endOffset": 141}, {"referenceID": 1, "context": "Much work has been devoted to approximately sample from a Dpp by first approximating its kernel via algorithms such as the Nystr\u00f6m method [3], Random Kitchen Sinks [2, 41], or matrix ridge approximations [45, 46], and then sampling based on this approximation.", "startOffset": 164, "endOffset": 171}, {"referenceID": 40, "context": "Much work has been devoted to approximately sample from a Dpp by first approximating its kernel via algorithms such as the Nystr\u00f6m method [3], Random Kitchen Sinks [2, 41], or matrix ridge approximations [45, 46], and then sampling based on this approximation.", "startOffset": 164, "endOffset": 171}, {"referenceID": 44, "context": "Much work has been devoted to approximately sample from a Dpp by first approximating its kernel via algorithms such as the Nystr\u00f6m method [3], Random Kitchen Sinks [2, 41], or matrix ridge approximations [45, 46], and then sampling based on this approximation.", "startOffset": 204, "endOffset": 212}, {"referenceID": 45, "context": "Much work has been devoted to approximately sample from a Dpp by first approximating its kernel via algorithms such as the Nystr\u00f6m method [3], Random Kitchen Sinks [2, 41], or matrix ridge approximations [45, 46], and then sampling based on this approximation.", "startOffset": 204, "endOffset": 212}, {"referenceID": 29, "context": "Alternative methods use a dual formulation [30], which however presupposes a decomposition L = XX> of the DPP kernel, which may be unavailable and inefficient to compute in practice.", "startOffset": 43, "endOffset": 47}, {"referenceID": 5, "context": "Finally, MCMC [6, 10, 14, 28] offers a potentially attractive avenue different from the above approaches that all rely on the same spectral technique.", "startOffset": 14, "endOffset": 29}, {"referenceID": 9, "context": "Finally, MCMC [6, 10, 14, 28] offers a potentially attractive avenue different from the above approaches that all rely on the same spectral technique.", "startOffset": 14, "endOffset": 29}, {"referenceID": 13, "context": "Finally, MCMC [6, 10, 14, 28] offers a potentially attractive avenue different from the above approaches that all rely on the same spectral technique.", "startOffset": 14, "endOffset": 29}, {"referenceID": 27, "context": "Finally, MCMC [6, 10, 14, 28] offers a potentially attractive avenue different from the above approaches that all rely on the same spectral technique.", "startOffset": 14, "endOffset": 29}, {"referenceID": 16, "context": "We do this via the idea of coresets [17, 25], small subsets of the data that capture function values of interest almost as well as the full dataset.", "startOffset": 36, "endOffset": 44}, {"referenceID": 24, "context": "We do this via the idea of coresets [17, 25], small subsets of the data that capture function values of interest almost as well as the full dataset.", "startOffset": 36, "endOffset": 44}, {"referenceID": 31, "context": "For a coreset of size M, our sampling time is O(k2M), which is independent of N since we are using k-Dpps [32].", "startOffset": 106, "endOffset": 110}, {"referenceID": 10, "context": "Dpps have been studied in statistical physics and probability [11, 12, 27]; they have witnessed rising interest in machine learning [21, 23, 30, 32\u201334, 38, 44].", "startOffset": 62, "endOffset": 74}, {"referenceID": 11, "context": "Dpps have been studied in statistical physics and probability [11, 12, 27]; they have witnessed rising interest in machine learning [21, 23, 30, 32\u201334, 38, 44].", "startOffset": 62, "endOffset": 74}, {"referenceID": 26, "context": "Dpps have been studied in statistical physics and probability [11, 12, 27]; they have witnessed rising interest in machine learning [21, 23, 30, 32\u201334, 38, 44].", "startOffset": 62, "endOffset": 74}, {"referenceID": 20, "context": "Dpps have been studied in statistical physics and probability [11, 12, 27]; they have witnessed rising interest in machine learning [21, 23, 30, 32\u201334, 38, 44].", "startOffset": 132, "endOffset": 159}, {"referenceID": 22, "context": "Dpps have been studied in statistical physics and probability [11, 12, 27]; they have witnessed rising interest in machine learning [21, 23, 30, 32\u201334, 38, 44].", "startOffset": 132, "endOffset": 159}, {"referenceID": 29, "context": "Dpps have been studied in statistical physics and probability [11, 12, 27]; they have witnessed rising interest in machine learning [21, 23, 30, 32\u201334, 38, 44].", "startOffset": 132, "endOffset": 159}, {"referenceID": 31, "context": "Dpps have been studied in statistical physics and probability [11, 12, 27]; they have witnessed rising interest in machine learning [21, 23, 30, 32\u201334, 38, 44].", "startOffset": 132, "endOffset": 159}, {"referenceID": 32, "context": "Dpps have been studied in statistical physics and probability [11, 12, 27]; they have witnessed rising interest in machine learning [21, 23, 30, 32\u201334, 38, 44].", "startOffset": 132, "endOffset": 159}, {"referenceID": 33, "context": "Dpps have been studied in statistical physics and probability [11, 12, 27]; they have witnessed rising interest in machine learning [21, 23, 30, 32\u201334, 38, 44].", "startOffset": 132, "endOffset": 159}, {"referenceID": 37, "context": "Dpps have been studied in statistical physics and probability [11, 12, 27]; they have witnessed rising interest in machine learning [21, 23, 30, 32\u201334, 38, 44].", "startOffset": 132, "endOffset": 159}, {"referenceID": 43, "context": "Dpps have been studied in statistical physics and probability [11, 12, 27]; they have witnessed rising interest in machine learning [21, 23, 30, 32\u201334, 38, 44].", "startOffset": 132, "endOffset": 159}, {"referenceID": 14, "context": "Cardinality-conditioned Dpp sampling is also referred to as \u201cvolume sampling\u201d, which has been used for matrix approximations [15, 16].", "startOffset": 125, "endOffset": 133}, {"referenceID": 15, "context": "Cardinality-conditioned Dpp sampling is also referred to as \u201cvolume sampling\u201d, which has been used for matrix approximations [15, 16].", "startOffset": 125, "endOffset": 133}, {"referenceID": 2, "context": "Several works address faster Dpp sampling via matrix approximations [3, 15, 30, 37] or MCMC [10, 28].", "startOffset": 68, "endOffset": 83}, {"referenceID": 14, "context": "Several works address faster Dpp sampling via matrix approximations [3, 15, 30, 37] or MCMC [10, 28].", "startOffset": 68, "endOffset": 83}, {"referenceID": 29, "context": "Several works address faster Dpp sampling via matrix approximations [3, 15, 30, 37] or MCMC [10, 28].", "startOffset": 68, "endOffset": 83}, {"referenceID": 36, "context": "Several works address faster Dpp sampling via matrix approximations [3, 15, 30, 37] or MCMC [10, 28].", "startOffset": 68, "endOffset": 83}, {"referenceID": 9, "context": "Several works address faster Dpp sampling via matrix approximations [3, 15, 30, 37] or MCMC [10, 28].", "startOffset": 92, "endOffset": 100}, {"referenceID": 27, "context": "Several works address faster Dpp sampling via matrix approximations [3, 15, 30, 37] or MCMC [10, 28].", "startOffset": 92, "endOffset": 100}, {"referenceID": 3, "context": "Finally, different lines of work address learning DPPs [4, 22, 31, 38] and MAP estimation [20].", "startOffset": 55, "endOffset": 70}, {"referenceID": 21, "context": "Finally, different lines of work address learning DPPs [4, 22, 31, 38] and MAP estimation [20].", "startOffset": 55, "endOffset": 70}, {"referenceID": 30, "context": "Finally, different lines of work address learning DPPs [4, 22, 31, 38] and MAP estimation [20].", "startOffset": 55, "endOffset": 70}, {"referenceID": 37, "context": "Finally, different lines of work address learning DPPs [4, 22, 31, 38] and MAP estimation [20].", "startOffset": 55, "endOffset": 70}, {"referenceID": 19, "context": "Finally, different lines of work address learning DPPs [4, 22, 31, 38] and MAP estimation [20].", "startOffset": 90, "endOffset": 94}, {"referenceID": 7, "context": "Coresets have been applied to large-scale clustering [8, 18, 24, 26], PCA and CCA [18, 39], and segmentation of streaming data [42].", "startOffset": 53, "endOffset": 68}, {"referenceID": 17, "context": "Coresets have been applied to large-scale clustering [8, 18, 24, 26], PCA and CCA [18, 39], and segmentation of streaming data [42].", "startOffset": 53, "endOffset": 68}, {"referenceID": 23, "context": "Coresets have been applied to large-scale clustering [8, 18, 24, 26], PCA and CCA [18, 39], and segmentation of streaming data [42].", "startOffset": 53, "endOffset": 68}, {"referenceID": 25, "context": "Coresets have been applied to large-scale clustering [8, 18, 24, 26], PCA and CCA [18, 39], and segmentation of streaming data [42].", "startOffset": 53, "endOffset": 68}, {"referenceID": 17, "context": "Coresets have been applied to large-scale clustering [8, 18, 24, 26], PCA and CCA [18, 39], and segmentation of streaming data [42].", "startOffset": 82, "endOffset": 90}, {"referenceID": 38, "context": "Coresets have been applied to large-scale clustering [8, 18, 24, 26], PCA and CCA [18, 39], and segmentation of streaming data [42].", "startOffset": 82, "endOffset": 90}, {"referenceID": 41, "context": "Coresets have been applied to large-scale clustering [8, 18, 24, 26], PCA and CCA [18, 39], and segmentation of streaming data [42].", "startOffset": 127, "endOffset": 131}, {"referenceID": 31, "context": "Conditioning on sampling sets of fixed cardinality k, one obtains a k-Dpp [32]: PL,k(Y) : = PL(Y | |Y| = k) = det(LY)ek(L)J |Y| = kK, where ek(L) is the k-th coefficient of the characteristic polynomial det(\u03bbI\u2212 L) = \u2211 k=0(\u22121)ek(L)\u03bb.", "startOffset": 74, "endOffset": 78}, {"referenceID": 12, "context": "2In theory, this can be computed in O(N\u03c9 log(N)) time [13], but the eigendecompositions and dynamic programming used in practice typically take cubic time.", "startOffset": 54, "endOffset": 58}, {"referenceID": 6, "context": "6 Experiments We next evaluate CoreDpp, and compare its efficiency and effectiveness against three competing approaches: - Partitioning using k-means (with kmeans++ initialization [7]), with C chosen as the centers of the clusters; referred to as K++ in the results.", "startOffset": 180, "endOffset": 183}, {"referenceID": 2, "context": "- The adaptive, stochastic Nystr\u00f6m sampler of [3] (NysStoch).", "startOffset": 46, "endOffset": 49}, {"referenceID": 27, "context": "- The Metropolis-Hastings DPP sampler MCDPP [28].", "startOffset": 44, "endOffset": 48}, {"referenceID": 18, "context": "We use the well-known Gelman and Rubin multiple sequence diagnostic [19] to empirically judge mixing.", "startOffset": 68, "endOffset": 72}, {"referenceID": 34, "context": "MNIST [35].", "startOffset": 6, "endOffset": 10}, {"referenceID": 8, "context": "GENES [9].", "startOffset": 6, "endOffset": 9}, {"referenceID": 27, "context": "3 Running Time on Large Datasets Lastly, we address running times for CoreDpp, NysStoch and the Markov chain k-DPP (MCDPP [28]).", "startOffset": 122, "endOffset": 126}, {"referenceID": 18, "context": "For the latter, we evaluate convergence via the Gelman and Rubin multiple sequence diagnostic [19]; we run 10 chains simultaneously and use the CODA [40] package to calculate the potential scale reduction factor (PSRF), and set the number of iterations to the point when PSRF drops below 1.", "startOffset": 94, "endOffset": 98}, {"referenceID": 39, "context": "For the latter, we evaluate convergence via the Gelman and Rubin multiple sequence diagnostic [19]; we run 10 chains simultaneously and use the CODA [40] package to calculate the potential scale reduction factor (PSRF), and set the number of iterations to the point when PSRF drops below 1.", "startOffset": 149, "endOffset": 153}, {"referenceID": 29, "context": "1, and NysStoch uses the dual form of k-Dpp sampling [30].", "startOffset": 53, "endOffset": 57}], "year": 2016, "abstractText": "Determinantal Point Processes (Dpps) are elegant probabilistic models of repulsion and diversity over discrete sets of items. But their applicability to large sets is hindered by expensive cubic-complexity matrix operations for basic tasks such as sampling. In light of this, we propose a new method for approximate sampling from discrete k-Dpps. Our method takes advantage of the diversity property of subsets sampled from a Dpp, and proceeds in two stages: first it constructs coresets for the ground set of items; thereafter, it efficiently samples subsets based on the constructed coresets. As opposed to previous approaches, our algorithm aims to minimize the total variation distance to the original distribution. Experiments on both synthetic and real datasets indicate that our sampling algorithm works efficiently on large data sets, and yields more accurate samples than previous approaches.", "creator": "LaTeX with hyperref package"}}}