{"id": "1509.01659", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "5-Sep-2015", "title": "Gravitational Clustering", "abstract": "The downfall of many supervised learning algorithms, such as neural networks, is the inherent need for a large amount of training data. Although there is a lot of buzz about big data, there is still the problem of doing classification from a small dataset. Other methods such as support vector machines, although capable of dealing with few samples, are inherently binary classifiers, and are in need of learning strategies such as One vs All in the case of multi-classification. In the presence of a large number of classes this can become problematic. In this paper we present, a novel approach to supervised learning through the method of clustering. Unlike traditional methods such as K-Means, Gravitational Clustering does not require the initial number of clusters, and automatically builds the clusters, individual samples can be arbitrarily weighted and it requires only few samples while staying resilient to over-fitting.", "histories": [["v1", "Sat, 5 Sep 2015 03:37:50 GMT  (57kb)", "http://arxiv.org/abs/1509.01659v1", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["armen aghajanyan"], "accepted": false, "id": "1509.01659"}, "pdf": {"name": "1509.01659.pdf", "metadata": {"source": "CRF", "title": "Introduction to Gravitational Clustering", "authors": ["Armen Aghajanyan"], "emails": [], "sections": [{"heading": null, "text": "ar Xiv: 150 9.01 659v 1 [cs.L G] 5S ep2 015 PATTERN ANALYSIS AND MACHINE INTELLIGENCE, VOL. X, NO. Y, FEBRUARY 2015 1Keywords - Machine Learning, Classification, Clustering.I. Introduction The name of this algorithm is derived from the metaphor on which the algorithm was built. Each cluster is symbolic of a planet, and each planet has a mass and a radius, as well as the class it represents. However, unlike real planets, our planets are static in relation to other planets. The process of training can be conceptually conceived as constructing a universe. The process of predicting is simply to place a mass in the universe and trace which planet it will appear. This algorithm has three beautiful properties: 1) Ability to learn from a few samples. 2) Ability to learn the importance of vectors to tract the weight.3) Ability to learn the importance of the element system."}, {"heading": "II. Definition", "text": "First, let us define mathematically what each of our symbolic structures will look like, the most important structure being our cluster or planet. We define the planet as a planet containing a dynamic mass m, a dynamic radius r, a dynamic position \u2212 \u2192 x and a static class \u03b8. Mathematically speaking: m-Rr-R \u2212 \u2192 x-Rn\u00da-ZP = {m, r, \u2212 \u2192 x, \u03b8} (1) Our universe will simply consist of a series of planets, the universe will also contain a few global constants, the initial radius of a newly formed planet, which we will call r \u00b2. The so-called percentage step, which represents the amount that a test mass moves before recalculating the new forces to the test mass. We call this the Greek \u03b1. The number of steps or iterations undertaken is called \u03b2. The distance between the planets is calculated using the function D (\u2212 \u2192 x, \u2212 \u2192 y)."}, {"heading": "III. Training Model", "text": "One of the better aspects of the model is its ability to evaluate your feature vectors. To do this, we define a hybrid feature vector h.h = {\u2212 \u2192 x, m, \u03b8} (2) The m variable allows us to evaluate the value of the feature vector. For example, if you have a probability diagnosis, each feature vector contains the class of diagnosis and the probability of diagnosis represented by the mass. Training is fairly simple. Below, you will find the pseudocode.nearplanets \u2190 planets within a radius of \u2212 \u2192 h x; nearplanets \u2190 near planets where P\u03b8 = h\u03b8; if nearplanets is Empty thenUniverse Add Planet {m = hm, r \u2032, \u2212 \u2192 p = \u2212 h x, \u03b8 = h\u03b8} else p \u2190 planet that generates the most force, nearplanets; Universe update p \u2012 {m = pm + hm, r = m pr pm, \u2212 \u2192 p + hm \u00b2."}, {"heading": "A. Asymptotic Analysis", "text": "Say N is the number of planets and D is the dimensionality of our attribute vectors. Assuming that the planet exists, we get O (D \u0445 N) = O (N) (3) If we use a KD tree (Bentley, 1975), we can with the average asymptotic O (D \u0445 logN) = O (logN) (4) On the other hand, provided we have to add the planet: O (D \u0445 N + Nnear) = O (N + Nnear) (5) KD tree (Bentley, 1975) O (D \u0445 logN + Nnear) = O (logN + Nnear) (6) This is the asymptotic addition of a single traction vector."}, {"heading": "B. Comparison of Training Times", "text": "Gravitational clusteringK means SVM decision treesBig O (Ns (log N + Nn)) O (nDk + 1 log n) O (n3) O (nsD log (ns)) Online training Yes Yes No Partial variant meaning Yes No No No \u2022 Nn is equivalent to Nnear \u2022 Ns is equivalent to Nsamples"}, {"heading": "IV. Simulation Testing Model", "text": "Metaphorically speaking, predicting the class of a new point means throwing a piece of mass into the universe and tracking the mass until it collides with a planet. In this metaphor, we assume that the planets are infinitely small and therefore there will be no interference. Our test point is simply defined as l = [\u2212 \u2192 x]. First, let's define how we obtain the normalized direction force vector. Remember physics that the gravitational force between two planets is F = G m1m2r2 (8). In our case, we assume that the mass of each individual test point is equal, so we can disregard mass. We can also remove the G constant. Our hybrid force equation per planet p is now: F = pmr2 (9) Where r D is (\u2212 \u2192 p x, \u2212 \u2192 l x x). We define the total normalized force on our test mass using the equation used."}, {"heading": "A. Asymptotic Analysis of Simulation Testing Model", "text": "Let us say that N is the number of planets and D is the dimensionality of our characteristic vector. Calculation of the force requires O (4D \u0445 N) = O (N) (11) The 4 results from the vector arithmetic to be done. A subtraction, a multiplication, a distance to the square, a division. The N term results from the summation. The total simulation follows next. O (7D \u0445 N \u0445 \u03b2 + N) (12) The 3 other D terms result from: determination of the order of magnitude, multiplication by force (simultaneously multiplied by \u03b1) and the updated summation. The next N results from the search for the planets with the radius containing pos. We can ignore the final statement as they do not directly affect N. We get: O (7D \u0445 \u03b2 + N) = O (N (7D \u0445 \u03b2 + 1) = O (N) (13)."}, {"heading": "V. Probabilistic Non-Simulating Model", "text": "We propose another method to calculate the class of the test point, without the need for simulation and by purely statistical methods. We first assume that a planet or cluster is normally distributed from the center and that the standard deviation is a function of the radius of the planet \u03c3 (pr). Therefore, let us define the probability density function. PDFp = 12\u03c0 \u043c (pr) e \u2212 D (\u2212 \u2192 p x, \u2212 \u2192 l x) 22\u03c3 (pr) 2 (15) To take into account the fact that different classes have different sets of planets, we will transform this function into: MAX\u03b8 [Universe] p | p\u03b8 = \u03b8n12\u03c0 \u043c \u043c \u043c \u03c3 (pr) e \u2212 D (\u2212 \u2192 p x, \u2212 l x) 22\u03c3 (pr) 2] (15)."}, {"heading": "VI. Testing Results", "text": "We tested the algorithm using the Wisconsin Breast Cancer Data Set (Wolberg and Mangasarian, 1990) (Lichman, 2013). Below are the results. Gravitational clustering r \u2032 = 50 \u03b1 = 0.01 \u03b2 = 100r \u2032 = 5000 \u03b1 = 0.001 \u03b2 = 1000Simulated model 89.65% 90.59% Probabilistic model 92.78% 72.41% It is interesting to note that the larger the clusters and the smaller the number of clusters, the less accurate the probability model will be. Of course, the clusters model the data they encapsulate perfectly. We continued our tests by comparing the results of some popular out-of-the-box methods. All the other algorithms were implemented in the Scikit Learn library (Pedregosa et al., 2011). The datasets we used were the popular iris datasets (Lichman, 2013), the digital datasets of some popular out-of-the-the-box algorithms were implemented in the Scileosa library."}, {"heading": "VII. Conclusion", "text": "In this paper, we introduced a novel technique of clustering and supervised learning that can learn from a few samples while maintaining a low asymptotic runtime and inherently allowing random weighting of samples. We compared it to current classification techniques and showed both the strengths of the algorithm and the weaknesses. From the test results, we can conclude that our algorithm operates consistently in both low and high dimensional data and remains consistent across a range of multi-class datasets. All the code, including the tests and the algorithm itself, can be found at https: / / github.com / ArmenAg / GravitationalClustering / Thanks for reading."}], "references": [{"title": "Conjugate-gradient neural networks in classification of multisource and very-high-dimensional remote sensing data", "author": ["J.A. Benediktsson", "P.H. SWAIN", "O.K. ERSOY"], "venue": "International Journal of Remote Sensing, 14(15):2883\u20132903.", "citeRegEx": "Benediktsson et al\\.,? 1993", "shortCiteRegEx": "Benediktsson et al\\.", "year": 1993}, {"title": "Multidimensional binary search trees used for associative searching", "author": ["J.L. Bentley"], "venue": "Commun. ACM, 18(9):509\u2013517.", "citeRegEx": "Bentley,? 1975", "shortCiteRegEx": "Bentley", "year": 1975}, {"title": "Hidden markov models for recognition using artificial neural networks", "author": ["V. Bevilacqua", "G. Mastronardi", "A. Pedone", "G. Romanazzi", "D. Daleno"], "venue": "4113:126\u2013 134.", "citeRegEx": "Bevilacqua et al\\.,? 2006", "shortCiteRegEx": "Bevilacqua et al\\.", "year": 2006}, {"title": "Support-vector networks", "author": ["C. Cortes", "V. Vapnik"], "venue": "Machine Learning, 20(3):273\u2013297.", "citeRegEx": "Cortes and Vapnik,? 1995", "shortCiteRegEx": "Cortes and Vapnik", "year": 1995}, {"title": "Women, Fire, and Dangerous Things", "author": ["G. Lakoff"], "venue": "The University of Chicago Press.", "citeRegEx": "Lakoff,? 1987", "shortCiteRegEx": "Lakoff", "year": 1987}, {"title": "Some methods for classification and analysis of multivariate observations", "author": ["J. MacQueen"], "venue": "pages 281\u2013297.", "citeRegEx": "MacQueen,? 1967", "shortCiteRegEx": "MacQueen", "year": 1967}, {"title": "Scikit-learn: Machine learning in Python", "author": ["F. Pedregosa", "G. Varoquaux", "A. Gramfort", "V. Michel", "B. Thirion", "O. Grisel", "M. Blondel", "P. Prettenhofer", "R. Weiss", "V. Dubourg", "J. Vanderplas", "A. Passos", "D. Cournapeau", "M. Brucher", "M. Perrot", "E. Duchesnay"], "venue": null, "citeRegEx": "Pedregosa et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Pedregosa et al\\.", "year": 2011}, {"title": "Multisurface method of pattern separation for medical diagnosis applied to breast cytology", "author": ["W. Wolberg", "O. Mangasarian"], "venue": "pages 9193\u20139196.", "citeRegEx": "Wolberg and Mangasarian,? 1990", "shortCiteRegEx": "Wolberg and Mangasarian", "year": 1990}], "referenceMentions": [{"referenceID": 0, "context": "Abstract\u2014The downfall of many supervised learning algorithms, such as neural networks, is the inherent need for a large amount of training data (Benediktsson et al., 1993).", "startOffset": 144, "endOffset": 171}, {"referenceID": 3, "context": "Other methods such as support vector machines, although capable of dealing with few samples, are inherently binary classifiers (Cortes and Vapnik, 1995), and are in need of learning strategies such as One vs All in the case of multi-classification.", "startOffset": 127, "endOffset": 152}, {"referenceID": 5, "context": "Unlike traditional methods such as K-Means (MacQueen, 1967), Gravitational Clustering does not require the initial number of clusters, and automatically builds the clusters, individual samples can be arbitrarily weighted and it requires only few samples while staying resilient to over-fitting.", "startOffset": 43, "endOffset": 59}, {"referenceID": 4, "context": "Eleanor Rosch (Lakoff, 1987)(P.", "startOffset": 14, "endOffset": 28}, {"referenceID": 1, "context": "Using a KD-Tree (Bentley, 1975) will allow us to train with the average asymptotic of", "startOffset": 16, "endOffset": 31}, {"referenceID": 1, "context": "KD-Tree (Bentley, 1975)", "startOffset": 8, "endOffset": 23}, {"referenceID": 7, "context": "We tested the algorithm out on the Wisconsin breast cancer data-set (Wolberg and Mangasarian, 1990) (Lichman, 2013).", "startOffset": 68, "endOffset": 99}, {"referenceID": 6, "context": "All the other algorithms were implemented in the scikit-learn library (Pedregosa et al., 2011).", "startOffset": 70, "endOffset": 94}, {"referenceID": 6, "context": "The data-sets we used were the popular Iris data-set (Lichman, 2013), digits data-set(Pedregosa et al., 2011), Ollivetti data-set (Bevilacqua et al.", "startOffset": 85, "endOffset": 109}, {"referenceID": 2, "context": ", 2011), Ollivetti data-set (Bevilacqua et al., 2006).", "startOffset": 28, "endOffset": 53}], "year": 2015, "abstractText": "The downfall of many supervised learning algorithms, such as neural networks, is the inherent need for a large amount of training data (Benediktsson et al., 1993). Although there is a lot of buzz about big data, there is still the problem of doing classification from a small data-set. Other methods such as support vector machines, although capable of dealing with few samples, are inherently binary classifiers (Cortes and Vapnik, 1995), and are in need of learning strategies such as One vs All in the case of multi-classification. In the presence of a large number of classes this can become problematic. In this paper we present, a novel approach to supervised learning through the method of clustering. Unlike traditional methods such as K-Means (MacQueen, 1967), Gravitational Clustering does not require the initial number of clusters, and automatically builds the clusters, individual samples can be arbitrarily weighted and it requires only few samples while staying resilient to over-fitting. Keywords\u2014Machine Learning, Classification, Clustering.", "creator": "LaTeX with hyperref package"}}}