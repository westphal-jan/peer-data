{"id": "1509.01771", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "6-Sep-2015", "title": "Sampled Weighted Min-Hashing for Large-Scale Topic Mining", "abstract": "We present Sampled Weighted Min-Hashing (SWMH), a randomized approach to automatically mine topics from large-scale corpora. SWMH generates multiple random partitions of the corpus vocabulary based on term co-occurrence and agglomerates highly overlapping inter-partition cells to produce the mined topics. While other approaches define a topic as a probabilistic distribution over a vocabulary, SWMH topics are ordered subsets of such vocabulary. Interestingly, the topics mined by SWMH underlie themes from the corpus at different levels of granularity. We extensively evaluate the meaningfulness of the mined topics both qualitatively and quantitatively on the NIPS (1.7 K documents), 20 Newsgroups (20 K), Reuters (800 K) and Wikipedia (4 M) corpora. Additionally, we compare the quality of SWMH with Online LDA topics for document representation in classification.", "histories": [["v1", "Sun, 6 Sep 2015 06:09:28 GMT  (295kb,D)", "http://arxiv.org/abs/1509.01771v1", "10 pages, Proceedings of the Mexican Conference on Pattern Recognition 2015"], ["v2", "Tue, 8 Sep 2015 03:14:12 GMT  (262kb,D)", "http://arxiv.org/abs/1509.01771v2", "10 pages, Proceedings of the Mexican Conference on Pattern Recognition 2015"]], "COMMENTS": "10 pages, Proceedings of the Mexican Conference on Pattern Recognition 2015", "reviews": [], "SUBJECTS": "cs.LG cs.CL cs.IR", "authors": ["gibran fuentes-pineda", "ivan vladimir meza-ruiz"], "accepted": false, "id": "1509.01771"}, "pdf": {"name": "1509.01771.pdf", "metadata": {"source": "CRF", "title": "Sampled Weighted Min-Hashing for Large-Scale Topic Mining", "authors": ["Gibran Fuentes-Pineda", "Ivan Vladimir Meza-Ru\u0131\u0301z"], "emails": [], "sections": [{"heading": null, "text": "Keywords: large-scale mining, min-hashing, contiguous terms"}, {"heading": "1 Introduction", "text": "Automatic topic extraction has become very important in recent years, as it provides a useful way to organize, execute and display large-scale collections of documents. Among the most successful approaches to topic discovery are topic models such as Latin Dirichlet Allocation (LDA) and Hierarchy Dirichlet Processes (HDP), which are associated with latent thematic variables. More recently, undirected graphical models have also been applied to modeling (e.g. Boltzmann machines and Neuron Autoregressive Distribution Estimators)."}, {"heading": "2 Min-Hashing for Pairwise Similarity Search", "text": "The basic idea is the definition of Minhash functions h with the property that the probability of two sets A1, A2, A2, A2, A2, A2, A2, A2, A2, A2, A2, A2, A2, A2, A2, A2, A2, A2, A2, A2, A2, A2, A2, A2, A3, A3, A3, A3, A3, A3, A3, A3, A2, A2, A2, A2, A2, A2, A2, A2, A3, A3, A3, A3, A3, A2, A2, A3, A3, A3, A3, A3, A2, A2, A2, A2, A2, A2, A2, A2, A2, A2, A2, A2, A2, A2, A2, A2, A3, A3, A3, A3, A3, A3, A3, A3, A3, A3, A2, A6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6,"}, {"heading": "3 Sampled Min-Hashing for Topic Mining", "text": "It is a question of the extent to which the terms of the individual terms in the individual terms can be generalized. (It is a question of the extent to which the terms of the individual terms can be generalized. (It is a question of the extent to which the terms of the terms, topics and documents can be generalized, although they can be generalized to any kind of algorithm.) (It is a question of the extent to which the terms of the terms, topics and documents can be generalized, although they can be generalized to any kind of algorithm. (It is a question of the extent to which the terms of the terms, topics and documents can be generalized.) (It is a question of the extent to which the terms, topics and documents can be generalized to any kind of algorithms.)"}, {"heading": "4 Experimental Results", "text": "In this section, we will evaluate various aspects of the mined topics. First, we will present a comparison between the topics mined by SWMH and SMH. Second, we will evaluate the scalability of the proposed approach. Third, we will use the mined topics to classify documents. Finally, we will compare SWMH topics with online LDA topics. The corporations used in our experiments were: NIPS, 20 newsgroups, Reuters and Wikipedia1. NIPS is a small collection of articles (3,649 documents), 20 newsgroups is a larger collection of mail newsgroups (34,891 documents), Reuters is a medium collection of news (137,589 documents) and Wikipedia is a large collection of encyclopedia articles (1,265, 756 documents) 2. All experiments presented in this work were conducted on an Intel (R) Xeon (R) -Xeon (R) workstation computer with 8GB of memory and 8 processors. However, we would like to point out that the processor of the current version of the code will not be comparable to the one of the current one."}, {"heading": "4.1 Comparison between SMH and SWMH", "text": "For these experiments, we used the corpora of NIPS and Reuters and various values of the parameters s * and r, which define the number of MinHash tables. We set the similarity parameters to 0.15, 0.13, and 0.10, and the tuple size (r) to 3 and 4. These parameters yielded the following table sizes: 205, 315, 693, 1369, 2427, 6931. Figure 2 shows the effect of the weighting on the set of removed topics. First, note the fraction of both numbers as you move from 1369 to 2427 tables. This effect corresponds to 1 Wikipedia dump from 2013-09-04. 2 All corpora were pre-processed to crop terms that appeared less than 6 times in the overall corpus. Reset the s * to.10 when changing r from 3 to 4. Lower values in s * are stricter, and therefore fewer topics are degraded. Figure 2 also shows that the set of degraded topics is significantly reduced by the number of degraded topics than the MH portion of similar SWH."}, {"heading": "4.2 Scalability evaluation", "text": "To test the scalability of SWMH, we have measured the time and memory required to remove topics in the Reuters corpus while increasing the number of documents to be analyzed. In particular, we are conducting 10 experiments with SWMH, each increasing the number of documents by 10%. Figure 3 illustrates the time needed to remove topics while increasing the number of documents and increasing a complexity index, which is achieved by a combination of the size of the vocabulary and the average number of times a term appears in a document. As can be seen, in both cases, the time is almost linear and runs into thousands of seconds. The removal times for the companies were: NIPS, 43; 20 Newsgroups, 70; Reuters, 4, 446 and Wikipedia, 45, 834. These times are in contrast to the time required by Online LDA to model 100 topics 4: NIPS, 60; 20 Newsgroups, 154 and Reuters, 25, 997 In addition, we have 2 MB, 400 MB, 1 MB, 1 MB, 1 MB, 1 MB, 1 MB, 1 MB, 1 MB, 1 MB, 1 MB, 1 MB, 1 MB, 1 MB, 1 MB, 1 MB, 1 MB, 1 MB."}, {"heading": "4.3 Document classification", "text": "In this evaluation, we used the degraded topics to create a document representation based on the similarity between topics and documents, which was used for training.3 The parameters were set to s * = 0.1, r = 3 and overlap threshold of 0.7. 4 https: / / github.com / qpleple / online-lda-vb was adjusted to use our file for-mats.an SVM classifier with the class of the document. In this experiment, we focused in particular on the 20 newsgroups corpus. We used the typical setting of this corpus for document classification (60% training, 40% testing).Table 2 shows the performance for different variants of SWMH and online LDA topics. The results show that the number of topics is relevant for the task: online LDA with 400 topics is better than 100 topics. A similar behavior is discernible for SWMH, but the parameter r affects the content of the topics and hence the performance."}, {"heading": "4.4 Comparison between mined and modeled topics", "text": "In this evaluation, we compare the quality of the topics covered by SWMH with online LDA topics for the 20 newsgroups and Reuters corporations. To this end, we measure the topic coherence, which asC (t) = M \u2211 m = 2 m \u2212 1 \u2211 l = 1 log D (vm, vl) D (vl) is defined, where D (vl) is the document frequency of the term vl, and D (vm, vl) is the co-document frequency of the terms vm and vl [10]. However, this metric depends on the first M elements of the topics. For our evaluations, we have set M to 10. However, we note that the comparison is not direct, as both the SWMH and the online LDA topics are of different nature: SWMH topics are subsets of the vocabulary with uniform M elements of the topics, while online LDA topics represent distributions across the entire vocabulary."}, {"heading": "5 Discussion and Future Work", "text": "In this paper, we presented a large-scale approach to automatically decompose topics in a particular body, based on Sampled Weighted Min-Hashing. The decomposed topics consist of subsets of highly correlated terms from the vocabulary. The proposed approach is capable of decomposing topics into corpus, ranging from thousands of documents (about 1 minute) to millions of documents (about 7 hours), including topics similar to those of the online LDA. We found that the decomposed topics can be used to represent a document for classification purposes. We also showed that the complexity of the proposed approach increases linearly with the volume of documents. Interestingly, some of the topics intercepted by SWMH relate to the structure of the documents (e.g. in NIPS, the words in the first topic correspond to parts of an article) and others to specific groups (e.g. team sports in 20 newsgroups and Reuters or the Transformers universe in Wikipedia). These examples suggest that SWMH is capable of generating certain levels of meaning most."}], "references": [{"title": "Latent Dirichlet allocation", "author": ["David M. Blei", "Andrew Y. Ng", "Michael I. Jordan"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2003}, {"title": "On the resemblance and containment of documents", "author": ["Andrei Z. Broder"], "venue": null, "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2000}, {"title": "The importance of proper weighting methods", "author": ["Christopher Buckley"], "venue": "In Proceedings of the Workshop on Human Language Technology,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 1993}, {"title": "Large-scale discovery of spatially related images", "author": ["Ondrej Chum", "Jiri Matas"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2010}, {"title": "Near duplicate image detection: min-hash and tf-idf weighting", "author": ["Ondrej Chum", "James Philbin", "Andrew Zisserman"], "venue": "In Proceedings of the British Machine Vision Conference,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2008}, {"title": "Scalable object discovery: A hash-based approach to clustering co-occurring visual words", "author": ["Gibran Fuentes Pineda", "Hisashi Koga", "Toshinori Watanabe"], "venue": "IEICE Transactions on Information and Systems,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2024}, {"title": "Training products of experts by minimizing contrastive divergence", "author": ["Geoffrey E. Hinton"], "venue": "Neural Computation,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2002}, {"title": "Online learning for latent Dirichlet allocation", "author": ["Matthew D. Hoffman", "David M. Blei", "Francis Bach"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2010}, {"title": "A neural autoregressive topic model", "author": ["Hugo Larochelle", "Lauly Stanislas"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2012}, {"title": "Optimizing semantic coherence in topic models", "author": ["David Mimno", "Hanna M. Wallach", "Edmund Talley", "Miriam Leenders", "Andrew McCallum"], "venue": "In Proceedings of the Conference on Empirical Methods in Natural Language Processing,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2011}, {"title": "Sparse stochastic inference for latent Dirichlet allocation", "author": ["David Mimno", "Matthew D. Hoffman", "David M. Blei"], "venue": "In International Conference on Machine Learning,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2012}, {"title": "Modeling documents with a deep Boltzmann machine", "author": ["Geoffrey Hinton"], "venue": "In Proceedings of the Conference on Uncertainty in Artificial Intelligence,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2013}, {"title": "Replicated softmax: An undirected topic model", "author": ["Ruslan Salakhutdinov", "Geoffrey E. Hinton"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2009}, {"title": "Term-weighting approaches in automatic text retrieval", "author": ["Gerard Salton", "Christopher Buckley"], "venue": "Information Processing & Management,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 1988}, {"title": "Hierarchical Dirichlet processes", "author": ["Yee Whye Teh", "Michael I. Jordan", "Matthew J. Beal", "David M. Blei"], "venue": "Journal of the American Statistical Association,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2004}], "referenceMentions": [{"referenceID": 0, "context": "Among the most successful approaches to topic discovery are directed topic models such as Latent Dirichlet Allocation (LDA) [1] and Hierarchical Dirichlet Processes (HDP) [15] which are Directed Graphical Models with latent topic variables.", "startOffset": 124, "endOffset": 127}, {"referenceID": 14, "context": "Among the most successful approaches to topic discovery are directed topic models such as Latent Dirichlet Allocation (LDA) [1] and Hierarchical Dirichlet Processes (HDP) [15] which are Directed Graphical Models with latent topic variables.", "startOffset": 171, "endOffset": 175}, {"referenceID": 12, "context": ", Boltzmann Machines [13, 12] and Neural Autoregressive Distribution Estimators [9]).", "startOffset": 21, "endOffset": 29}, {"referenceID": 11, "context": ", Boltzmann Machines [13, 12] and Neural Autoregressive Distribution Estimators [9]).", "startOffset": 21, "endOffset": 29}, {"referenceID": 8, "context": ", Boltzmann Machines [13, 12] and Neural Autoregressive Distribution Estimators [9]).", "startOffset": 80, "endOffset": 83}, {"referenceID": 7, "context": ", Online LDA [8] and stochastic variational inference [11]).", "startOffset": 13, "endOffset": 16}, {"referenceID": 10, "context": ", Online LDA [8] and stochastic variational inference [11]).", "startOffset": 54, "endOffset": 58}, {"referenceID": 6, "context": "Undirected Topic Models can be also trained efficiently using approximate strategies such as Contrastive Divergence [7].", "startOffset": 116, "endOffset": 119}, {"referenceID": 5, "context": "co-occurring terms is a combinatorial problem that lies in a large search space, we propose Sampled Weighted Min-Hashing (SWMH), an extended version of Sampled MinHashing (SMH) [6].", "startOffset": 177, "endOffset": 180}, {"referenceID": 1, "context": "SMH partitions the vocabulary into sets of highly co-occurring terms by applying Min-Hashing [2] to the inverted file entries of the corpus.", "startOffset": 93, "endOffset": 96}, {"referenceID": 13, "context": "This setting is not ideal for information retrieval applications where weighting have a positive impact on the quality of the retrieved documents [14, 3].", "startOffset": 146, "endOffset": 153}, {"referenceID": 2, "context": "This setting is not ideal for information retrieval applications where weighting have a positive impact on the quality of the retrieved documents [14, 3].", "startOffset": 146, "endOffset": 153}, {"referenceID": 0, "context": "P [h(A1) = h(A2)] = | A1 \u2229A2 | | A1 \u222aA2 | \u2208 [0, 1].", "startOffset": 44, "endOffset": 50}, {"referenceID": 1, "context": "Remarkably, it has been shown that the portion of identical MinHash values between two sets is an unbiased estimator of their Jaccard similarity [2].", "startOffset": 145, "endOffset": 148}, {"referenceID": 4, "context": "[5] to weighted set similarity, defined as", "startOffset": 0, "endOffset": 3}, {"referenceID": 0, "context": "simhist(H1, H2) = \u2211 i wi min(H i 1, H i 2) \u2211 i wi max(H i 1, H i 2) \u2208 [0, 1], (2)", "startOffset": 70, "endOffset": 76}, {"referenceID": 4, "context": "This extension allows the use of popular document representations based on weighting schemes such as tf-idf and has been applied to image retrieval [5] and clustering [4].", "startOffset": 148, "endOffset": 151}, {"referenceID": 3, "context": "This extension allows the use of popular document representations based on weighting schemes such as tf-idf and has been applied to image retrieval [5] and clustering [4].", "startOffset": 167, "endOffset": 170}, {"referenceID": 4, "context": "Recently, it was also successfully applied to retrieving co-occurring terms by hashing the inverted file lists instead of the documents [5, 6].", "startOffset": 136, "endOffset": 142}, {"referenceID": 5, "context": "Recently, it was also successfully applied to retrieving co-occurring terms by hashing the inverted file lists instead of the documents [5, 6].", "startOffset": 136, "endOffset": 142}, {"referenceID": 5, "context": "[6] proposed Sampled Min-Hashing (SMH), a simple strategy based on Min-Hashing to discover objects from large-scale image collections.", "startOffset": 0, "endOffset": 3}, {"referenceID": 0, "context": "ovr(C1, C2) = | C1 \u2229 C2 | min(| C1 |, | C2 |) \u2208 [0, 1].", "startOffset": 48, "endOffset": 54}, {"referenceID": 0, "context": ", Tk) = \u2211 i wi min (T i 1, \u00b7 \u00b7 \u00b7 , T i k) \u2211 i wi max (T i 1, \u00b7 \u00b7 \u00b7 , T i k) \u2208 [0, 1], (5)", "startOffset": 78, "endOffset": 84}, {"referenceID": 4, "context": "[5] to efficiently find such co-occurring terms.", "startOffset": 0, "endOffset": 3}, {"referenceID": 9, "context": "whereD(vl) is the document frequency of the term vl, andD(vm, vl) is the co-document frequency of the terms vm and vl [10].", "startOffset": 118, "endOffset": 122}], "year": 2017, "abstractText": "We present Sampled Weighted Min-Hashing (SWMH), a randomized approach to automatically mine topics from large-scale corpora. SWMH generates multiple random partitions of the corpus vocabulary based on term cooccurrence and agglomerates highly overlapping inter-partition cells to produce the mined topics. While other approaches define a topic as a probabilistic distribution over a vocabulary, SWMH topics are ordered subsets of such vocabulary. Interestingly, the topics mined by SWMH underlie themes from the corpus at different levels of granularity. We extensively evaluate the meaningfulness of the mined topics both qualitatively and quantitatively on the NIPS (1.7K documents), 20 Newsgroups (20K), Reuters (800K) and Wikipedia (4M) corpora. Additionally, we compare the quality of SWMH with Online LDA topics for document representation in classification.", "creator": "LaTeX with hyperref package"}}}