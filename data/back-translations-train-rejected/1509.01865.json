{"id": "1509.01865", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "6-Sep-2015", "title": "A Hybrid Approach to Domain-Specific Entity Linking", "abstract": "The current state-of-the-art Entity Linking (EL) systems are geared towards corpora that are as heterogeneous as the Web, and therefore perform sub-optimally on domain-specific corpora. A key open problem is how to construct effective EL systems for specific domains, as knowledge of the local context should in principle increase, rather than decrease, effectiveness. In this paper we propose the hybrid use of simple specialist linkers in combination with an existing generalist system to address this problem. Our main findings are the following. First, we construct a new reusable benchmark for EL on a corpus of domain-specific conversations. Second, we test the performance of a range of approaches under the same conditions, and show that specialist linkers obtain high precision in isolation, and high recall when combined with generalist linkers. Hence, we can effectively exploit local context and get the best of both worlds.", "histories": [["v1", "Sun, 6 Sep 2015 23:16:45 GMT  (304kb)", "http://arxiv.org/abs/1509.01865v1", "SEM'15"]], "COMMENTS": "SEM'15", "reviews": [], "SUBJECTS": "cs.IR cs.CL", "authors": ["alex olieman", "jaap kamps", "maarten marx", "arjan nusselder"], "accepted": false, "id": "1509.01865"}, "pdf": {"name": "1509.01865.pdf", "metadata": {"source": "META", "title": "A Hybrid Approach to Domain-Specific Entity Linking", "authors": ["Alex Olieman", "Jaap Kamps", "Maarten Marx", "Arjan Nusselder"], "emails": ["olieman@uva.nl", "kamps@uva.nl", "maartenmarx@uva.nl", "arjan@nusselder.eu"], "sections": [{"heading": null, "text": "A key open problem is how to build effective EL systems for specific areas, since knowledge of the local context should in principle increase, not decrease, the effectiveness of EL. In this paper, we propose the hybrid use of simple specialized linkers in combination with an existing generalist system to solve this problem. Our key findings are the following: First, we construct a new, reusable benchmark for EL on a corpus of domain-specific conversations. Second, we test the performance of a number of approaches under the same conditions and show that specialized linkers achieve high precision in isolation and high recall when combined with generalist linkers. In this way, we can effectively exploit the local context and get the best of both worlds. Categories and subject descriptions H.3.1 [Information Systems]: Content Analysis and Indexing - abstracting methods, indexing methods, linguistic processing. Keywords, Extraction Linking, Information, Attachment of Textixing - Linking."}, {"heading": "1. INTRODUCTION", "text": "In fact, most of them are able to determine for themselves what they want and what they don't want."}, {"heading": "2. RELATED WORK", "text": "However, progress in the field of supervised machine learning and the availability of encyclopedic resources has led to the use of Open Domain KBs in recent years. IE's domain-specific character is no longer expressed in CBs, but in training data [9]. One promising direction is transfer learning, which is known for classification tasks [5], whereas this has not been sufficiently demonstrated for EL. Alternatively, the need for domain experts has been established through the semi-supervised adaptation of generalistic models to a target corpus [10]. A promising direction is transfer learning, which is suitable for classification tasks."}, {"heading": "3. DOMAIN-SPECIFIC LINKING", "text": "Our approach is to develop specialized linkers for entity types that are frequently mentioned in the target corpus. These linkers use a small amount of background knowledge and achieve entity recognition and uniqueness through pattern recognition, string matching and structured queries against the body. We selected the Dutch parliamentary procedures as the target corpus for an experiment that is available in an XML format with rich (structural) annotations and extends to 1814 to date. Automated analysis of parliamentary procedures is part of a larger international effort and has been facilitated by previous work within the PoliticalMashup project [7]. Two commercially available EL systems are used as base systems and also as components for our combination approach. The first is DBpedia Spotlight v0.7, which uses raw text as input and produces links to generative models based on DBpedia and Wikipedia [4]. It differs from other state-of-the-art links to Wikipedia systems by using them as end-of-the-7."}, {"heading": "3.1 Domain-specific candidate entities", "text": "The simplest way we have considered commenting on entities of a particular type begins with collecting names for the entities in question, including acronyms, which are stored in a dictionary that assigns them to canonical URIs. Subsequently, a state machine that encodes all names is constructed by the Aho-Corasick algorithm [1], which allows names to be assigned to any input string, and the URI of the mentioned entities can be found in the dictionary. This approach of minimal effort is basically limited to entity types in which there is no ambiguity. Many-to-one mapping of names on URIs is synonymous, but does not allow a single name to be associated with multiple entities. Therefore, such a linker must address a type with few instances or in which ambiguous names are already considered to be congruous."}, {"heading": "3.2 Genre-specific characteristics", "text": "We focus on characteristics that relate to the mentioned persons; a prominent entity in many kinds of conversations. Conversations have a temporal aspect, i.e. all words were spoken or written at some point in time, and it is likely that the frequency with which a particular person is mentioned varies over time. Conversations are also situated: they occur in a (virtual) space in which a person may or may not be present. These characteristics can be used for disamusement. If a corpus focuses on a particular domain of discourse, there may be characteristic ways in which names are used, such as in etiquette and / or jargon. Government members and parliamentarians (in short: members) abide by guidelines that address each other during a parliamentary debate."}, {"heading": "4. EVALUATION", "text": "In this section we describe the development of a reusable EL benchmark on a corpus of domain specific conversations. Our approach - the use of special linkers for outstanding entity types and their combination with universal EL systems - is tested against this benchmark and we report the results."}, {"heading": "4.1 Benchmark", "text": "Given the uneven distribution of current content in the various debates, we have stratified the sample into government departments, with which we assume that the current content is strongly linked. There is no formal one-to-one relationship between debates and departments, and therefore we have used speakers with a government position as an indicator. The size of the sample is limited to the approximate length of a three-hour debate to limit the amount of time that our voluntary commentators have to spend on manual comments. From this general limit, we have set quotas per department in proportion to the number of debates associated with the department. For each department, a random debate is selected and taken from the pool. A random scene - the speaking time of a single member with optional interruptions and repetitions - is selected from this debate and included in the sample."}, {"heading": "4.2 Combination of system annotations", "text": "We have taken a simple approach to combining the output of multiple systems in order to achieve the goal of linking the aforementioned domain-specific units and other units, and this approach is aimed at not using training data. Previous work on how to combine the outputs of multiple generalist EL systems has used a tuning method [6] and shows that it is reasonably effective. However, linkage voting seems less promising when systems are specialized in certain entity types. Let's take the analogy of asking a question in a room full of specialists that plays a large role. We therefore apply a preferential sequence: the most specialized (i.e. highly accurate) system is asked to link a sentence first, and only if it is not the second system in the sequence 1 mentioning phrases commented by at least one system1, and so on. By adding an EL at the end of the generalist system, the phrases are also not randomly linked to each other in the chain."}, {"heading": "4.3 Results", "text": "We have used the developed benchmarks to assess the accuracy of the annotations generated by the professionals linkers and the baseline systems. To this end, we calculate precision and callback between the system and the gold annotations (n = 639). Figure 1 shows the performance of the specialized linkers (PM), DBpedia Spotlight (DBpS), Frog + Semanticizer (F + S) and preference combinations thereof. For the individual systems, the performance of the individuals and organizations is also shown separately. These results show that the specialized linkers were able to generate a greater number of precise annotations for the corpus than both of the baseline systems, while they are based on two specific entity activity type type type type type type type type type type type type type type type type type type type type type type type type type type type type type type type type type type type type type type type type type type type type type type type type type type type type type type type type type type type type type type type type type type type type type type type type type type type type type type type type type type type type type type type type type type type type type type type type type type type type type type type type type type type type type type type type type type type type type type type type type type type type type type type type type type type type type type type type type type type type type type type type type type type type type type type type type type type type."}, {"heading": "5. APPLICATIONS", "text": "The potential for semantic annotations to improve access to information becomes clear when we focus on users who have a deep interest in the domain of the corpus. An obvious application is semantic search [2], where linking entities can help address issues of homonymy and synonymy in retrieving documents. Specifically, entity linkages can simplify the type of questions used in corpus analysis for which the desired answer is not a list of questions. Consider this example of the genre of conversation text: give an overview of all questions addressed to Person X. This information could be answered at a high level, for example by displaying a timeline showing the frequency of questions asked, and for a selected period of time, who are the top questioners and what other entities are frequently mentioned in the context of these questions."}, {"heading": "6. CONCLUSIONS", "text": "An unfortunate effect of this goal is that such generalistic EL systems often disappoint when used on domain-specific corpora. We have proposed and evaluated a solution that is very cost-effective compared to existing alternative approaches. We have outlined the prerequisites for and the development of a lightweight linking system that targets outstanding entity types in a specific corpus. In our approach, the output of such specialized linkers is easily combined with that of a commercially available EL system that is responsible for linking mentioned entities of non-prominent types that are also of interest to users of the corpus.The specialized system, two generalistic base systems and hybrid combinations thereof, show that the evaluation of a gold standard carefully constructed by two human annotators who have experience in using the selected corpus system, two generalistic systems containing Nexio benchmark data, are both very competitive."}, {"heading": "7. REFERENCES", "text": "[1] Aho, A. V. and Corasick, M.J. 1975. Efficient stringmatching: an aid to bibliographic search. Communications of the ACM. 18, (1975), 333-340. [2] Berlanga, R., Nebot, V. and P\u00e9rez, M. 2014. Tailoredsemantic annotation for semantic search. Journal of Web Semantics. (2014), 1-13. [3] Van den Bosch, A., Busser, B., Canisius, S. and Daelemans, W. 2007. An efficient memory-based morphosyntactic tagger and parser for Dutch. (Selected Papers of CLIN, Belgium, 2007), 99-114. [4] Daiber, J., Jakob, M., Hokamp, C. and Mendes, P.N. 2013.Improving Efficiency and Accuracy in Multilingual Entity Extraction."}], "references": [{"title": "Efficient string matching: an aid to bibliographic search", "author": ["A.V. Aho", "M.J. Corasick"], "venue": "Communications of the ACM", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 1975}, {"title": "Tailored semantic annotation for semantic search", "author": ["R. Berlanga", "V. Nebot", "M. P\u00e9rez"], "venue": "Journal of Web Semantics", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2014}, {"title": "An efficient memory-based morphosyntactic tagger and parser for Dutch", "author": ["A. Van den Bosch", "B. Busser", "S. Canisius", "W. Daelemans"], "venue": "Selected Papers of CLIN 2007 (Leuven,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2007}, {"title": "Improving Efficiency and Accuracy in Multilingual Entity Extraction", "author": ["J. Daiber", "M. Jakob", "C. Hokamp", "P.N. Mendes"], "venue": "Proc. of I-Semantics 2013 (Austria,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2013}, {"title": "Frustratingly Easy Semi-Supervised Domain Adaptation", "author": ["H. Daum\u00e9 III", "A. Kumar", "A. Saha"], "venue": "Proceedings of DANLP", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2010}, {"title": "Can we use linked data semantic annotators for the extraction of domainrelevant expressions", "author": ["M. Gagnon", "A. Zouaq", "L. Jean-Louis"], "venue": "Proc. of WWW 2013 companion", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2013}, {"title": "Succinct summaries of narrative events using social networks", "author": ["B. De Goede", "M. Marx", "A. Nusselder", "J. van Wees"], "venue": "Proc. of HT", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2011}, {"title": "Feeding the Second Screen: Semantic Linking based on Subtitles", "author": ["D. Odijk", "E. Meij", "M. de Rijke"], "venue": "OAIR", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2013}, {"title": "Information Extraction: Past, Present, and Future. Multi-Source, Multilingual Information Extraction and Summarization", "author": ["J. Piskorski", "R. Yangarber"], "venue": null, "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2013}, {"title": "Entity Linking with a Knowledge Base: Issues, Techniques, and Solutions", "author": ["W. Shen", "J. Wang", "J. Han"], "venue": "IEEE Transactions on Knowledge and Data Engineering. 4347,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2014}, {"title": "GERBIL \u2013 General Entity Annotator Benchmarking Framework", "author": ["R Usbeck"], "venue": "Proc. of WWW 2015 (Florence,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2015}], "referenceMentions": [{"referenceID": 9, "context": "The majority of state-of-the-art EL systems utilize one or more open-domain KBs, such as Wikipedia, DBpedia, Freebase, or YAGO, as basis for learning their entity recognition and disambiguation models [10].", "startOffset": 201, "endOffset": 205}, {"referenceID": 8, "context": "This approach shows definite merit when the target corpus consists of texts with heterogeneous topical contents [9], e.", "startOffset": 112, "endOffset": 115}, {"referenceID": 9, "context": "This problem has been identified as one of three promising research directions in this area [10].", "startOffset": 92, "endOffset": 96}, {"referenceID": 8, "context": "Until the beginning of the 21st century, it was common to collect the domain knowledge that was needed for an IE task in a KB [9].", "startOffset": 126, "endOffset": 129}, {"referenceID": 8, "context": "The domain-specific nature of IE is no longer expressed in the KB, but instead in the training data [9].", "startOffset": 100, "endOffset": 103}, {"referenceID": 9, "context": "Efforts to reduce the need for domain experts have been made by semi-supervised adaptation of generalist models to a target corpus [10].", "startOffset": 131, "endOffset": 135}, {"referenceID": 4, "context": "One promising direction is Transfer Learning, which is known to work for classification tasks [5], whereas this has not been demonstrated sufficiently for EL.", "startOffset": 94, "endOffset": 97}, {"referenceID": 1, "context": "Alternatively, a domainrelevant part of the KB can be selected by excluding KB-entries that are more likely to be generated by a parsimonious unigram model of the KB (with the corpus as background), than by the unigram corpus model [2].", "startOffset": 232, "endOffset": 235}, {"referenceID": 1, "context": "[2], KB-entries are also tailored by basing entity-specific language models on both the corpus and the KB.", "startOffset": 0, "endOffset": 3}, {"referenceID": 10, "context": "The recently presented GERBIL [11] is a KB-agnostic EL benchmarking framework, which addresses issues with the comparability and reproducibility of EL systems and experiments.", "startOffset": 30, "endOffset": 34}, {"referenceID": 6, "context": "The automated analysis of parliamentary proceedings is part of a larger international effort, and has been facilitated by previous work in the PoliticalMashup project [7].", "startOffset": 167, "endOffset": 170}, {"referenceID": 3, "context": "7, which takes raw text as input and produces links with generative models based on DBpedia and Wikipedia [4].", "startOffset": 106, "endOffset": 109}, {"referenceID": 2, "context": "Frog is an NLP workbench for Dutch [3], from which the phrase chunking module is used to identify noun phrases.", "startOffset": 35, "endOffset": 38}, {"referenceID": 7, "context": "The identified phrases are subsequently passed on to the UvA Semanticizer, which takes a learning to rerank approach to disambiguation [8].", "startOffset": 135, "endOffset": 138}, {"referenceID": 0, "context": "Subsequently, a state machine that encodes all names is constructed by the Aho-Corasick algorithm [1].", "startOffset": 98, "endOffset": 101}, {"referenceID": 5, "context": "Earlier work on how to combine the output of multiple generalist EL systems has used a voting method [6], and shows it to be somewhat effective.", "startOffset": 101, "endOffset": 104}, {"referenceID": 1, "context": "An obvious application is in semantic search [2], where entity linking can help address issues with homonymy and synonymy in document retrieval.", "startOffset": 45, "endOffset": 48}, {"referenceID": 6, "context": ", it is possible to derive a graph of who is interrupted by whom from the structure of the proceedings [7].", "startOffset": 103, "endOffset": 106}], "year": 2015, "abstractText": "The current state-of-the-art Entity Linking (EL) systems are geared towards corpora that are as heterogeneous as the Web, and therefore perform sub-optimally on domain-specific corpora. A key open problem is how to construct effective EL systems for specific domains, as knowledge of the local context should in principle increase, rather than decrease, effectiveness. In this paper we propose the hybrid use of simple specialist linkers in combination with an existing generalist system to address this problem. Our main findings are the following. First, we construct a new reusable benchmark for EL on a corpus of domain-specific conversations. Second, we test the performance of a range of approaches under the same conditions, and show that specialist linkers obtain high precision in isolation, and high recall when combined with generalist linkers. Hence, we can effectively exploit local context and get the best of both worlds.", "creator": "Microsoft\u00ae Office Word 2007"}}}