{"id": "1509.02213", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "7-Sep-2015", "title": "Unsupervised Spoken Term Detection with Spoken Queries by Multi-level Acoustic Patterns with Varying Model Granularity", "abstract": "This paper presents a new approach for unsupervised Spoken Term Detection with spoken queries using multiple sets of acoustic patterns automatically discovered from the target corpus. The different pattern HMM configurations(number of states per model, number of distinct models, number of Gaussians per state)form a three-dimensional model granularity space. Different sets of acoustic patterns automatically discovered on different points properly distributed over this three-dimensional space are complementary to one another, thus can jointly capture the characteristics of the spoken terms. By representing the spoken content and spoken query as sequences of acoustic patterns, a series of approaches for matching the pattern index sequences while considering the signal variations are developed. In this way, not only the on-line computation load can be reduced, but the signal distributions caused by different speakers and acoustic conditions can be reasonably taken care of. The results indicate that this approach significantly outperformed the unsupervised feature-based DTW baseline by 16.16\\% in mean average precision on the TIMIT corpus.", "histories": [["v1", "Mon, 7 Sep 2015 22:40:31 GMT  (1759kb,D)", "http://arxiv.org/abs/1509.02213v1", "Accepted by ICASSP 2014"]], "COMMENTS": "Accepted by ICASSP 2014", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["cheng-tao chung", "chun-an chan", "lin-shan lee"], "accepted": false, "id": "1509.02213"}, "pdf": {"name": "1509.02213.pdf", "metadata": {"source": "CRF", "title": "UNSUPERVISED SPOKEN TERM DETECTION WITH SPOKEN QUERIES BY MULTI-LEVEL ACOUSTIC PATTERNS WITH VARYING MODEL GRANULARITY", "authors": ["Cheng-Tao Chung", "Chun-an Chan", "Lin-shan Lee"], "emails": ["b97901182@gmail.com,", "chunanchan@gmail.com,", "lslee@gate.sinica.edu.tw"], "sections": [{"heading": null, "text": "Index Terms - Zero Resource Speech Recognition, Unattended Learning, Dynamic Time Warping, Hidden Markov Models, Speech Recognition"}, {"heading": "1. INTRODUCTION", "text": "In fact, most of them are not purely political parties, but a party that is able to establish itself."}, {"heading": "2. ACOUSTIC PATTERNS WITH VARYING MODEL GRANULARITY", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "2.1. Pattern Discovery for a Given Model Configuration", "text": "In view of an unlabeled speech corpus, it is not difficult for the unsupervised discovery of the desired acoustic patterns from the corpus for a selected hyperparameter set determining the topology of the HMMs [19] [21]. This can be achieved by first finding an initial label \u03c90 based on a set of assumed patterns for all observations in the corpus, as in (1) [17]. Then, in each iteration t the defined HMM parameter set can be trained with the label \u03c9t \u2212 1 obtained in the previous iteration, and the new label \u03c9t can be freely decoded with the obtained parameter set \u041a\u0430t as in (3).\u03c90 = Initialization (1), (1) \u0432\u0430g max."}, {"heading": "2.2. Model Granularity Space", "text": "The above process can be performed with many different HMM configurations, each characterized by three hyperparameters: the number of states m in each HMM acoustic pattern, the total number of acoustic patterns n during initialization, and the number of Gaussians l in each HMM state, \u0432 = (m, n, l). The transcription of a speech signal decoded with these acoustic patterns HMMs can be considered as a temporal segmentation of the signal, so that the HMM length (or number of states in each HMM) m represents the temporal granularity. The set of all acoustic patterns HMMs can be considered as a segmentation of the phonetic space, so that the total number of acoustic patterns HFCMs represents the phonetic granularity in each HMM state. The different Gaussians in each state then collectively determine the distributions of signals in the acoustic feature space, which are represented by MFCs."}, {"heading": "3. SPOKEN TERM DETECTION AND SEARCH METHODS", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "3.1. Off-line Processing", "text": "All spoken documents in the archive are initially decoded offline in sequences of acoustic patterns, each level of the acoustic pattern HMM being set. Let {pr, r = 1, 2, 3,.., nk} denote the nk acoustic patterns in the set of HMMs pi and pj in the set of HMMs k. We will then construct a similarity matrix S of the size nk \u00b7 nk for each individual component k, for which the component S (i, j) denotes the similarity between any two patterns HMMs pi and pj in the set of HMMs pi and pj in the set of HMM k. Two similarity matrices used for this work are in (4).S (i, j) = {\u03b4 (i, j), for hard similarity or (4a) exp (\u2212 KL (i, j) / \u03b2, for soft similarity. (4b) The matrix (4a) is the simple matrix (DiM-4M) which is determined by the identification KL in the 4M."}, {"heading": "3.2. On-line Matching Matrix Construction", "text": "In the online phase, for each spoken query q and each document d entered in the archive, we perform the following: Suppose that a document d is decrypted into a sequence of D-acoustic patterns with indices (d1, d2,..., dD) and the query q into a sequence of Q-patterns with indices (q1,..., qQ). Thus, for each document query pair, we construct a matching matrix W of size D \u00b7 Q, in which each entry (i, j) represents the similarity between acoustic patterns with indices di and qj as in (5a).W (i, j) = {S (di, qj) for 1-best sequences or (5a) PTi SPj for N-best sequences. (5b) Alternatively, we can also extend the N-best sequences of documents with N-best sequences of queries as in Figure (2) and (5b) PTSPi for the best sequence (5b)."}, {"heading": "3.3. On-line Matching Policy", "text": "There can be two methods to calculate the relevance value between document d and query q. In the Sub-Sequence Matching (SUB) method in (6a), we sum up the elements in the matrixW along the diagonal direction and generate the collected similarities for all sub-sequences starting at all pattern positions in d, as shown in Figure (3) (a). The maximum is selected to represent the relevance between document d and query q on the matrix specified in (6a). R (d, q) = max i: Index Q \u2211 j = 1 W (i + j, j), for SUB, or (6a) max u: path | u | \u2211 j = 1 W (ud (j), for DTW. (6b) To alleviate the problem of insertion / deletion, we can also base dynamic time distortion (DTW) on the matrix as in W (version 6b) and DFig-W (3)."}, {"heading": "3.4. Overall Relevance Score", "text": "In each of (4) (5) (6) there are two options that result in a total of 8 q search methods. Therefore, we use three binary digits to specify these methods when reporting experimental results below: \u03b3 = (Soft, Nbest, DTW), i.e. Soft = 1 for soft similarity in (4b) and 0 for (4a); Nbest = 1 for the N-best sequence in (5b) and 0 for (5a); DTW = 1 for DTW in (6b) and 0 for (6a). These search methods, {\u03b3s, s = 1,..., 8} give different relevance values for each pattern in k, R, Q) as in (6). The general relevance of R (d, q) between d and q is then simply the weighted sum of (6) over all K-different sets of acoustic patterns."}, {"heading": "4. EXPERIMENTS", "text": "The proposed approach was tested in preliminary experiments conducted on the TIMIT corpus. 20 sets of acoustic patterns with the number of states m = 3, 5, 7, 9, 11 and the number of HMMs n = 50, 100, 200, 300 were initially generated with l = 1 gauss per state on the TIMIT training set. Then, we increased the number of Gaussians per state by 1 and performed (2) and (3) until the sentences converge. We repeated the process until we had l = 1, 2, 3, 4 for all 20 sample sets and ended up with K = 80 pattern HMM sets. Using the 8 search methods mentioned in section 3, this resulted in a total of 640 points for each pair of query documents in (7). The TIMIT training set was also taken as a spoken archive from which we would like to recognize the spoken terms. The set of queries consisted of 32 spoken words randomly selected from the TIMIT test set."}, {"heading": "4.1. Feature Selection and Achievable Performance", "text": "In this experiment, our goal was to learn the 640 weights \u03bb (\u0432, \u03b3) in (7) to be either 1 or 0 to optimize the MAP. We randomly divided the query into 2 disjointed groups A and B, each containing 16 queries. We use Set B as a development set for learning these weights and focused our discussion on Set A. Starting from each group (\u0432, \u03b3) = 0, we eagerly select the next group that would provide the best MAP and set it to 1. This process was repeated until 20 pairs of (\u0432, \u03b3) were selected. The results are shown in Figure 4, in which the oracle results are also shown in cyan by learning on Set A. The baseline of framebased DTW was 10.16% for Set A. It was probably low because most queries unexpectedly had less than 10 relevant documents in the training group, and the different dialects of TIMIT made it even more difficult."}, {"heading": "4.2. Performance Analysis without a Development Set", "text": "We consider the case as the only part of the search that could be performed offline, it is certainly an attractive conclusion (one). We first consider the different search method \u03b3 by summing up all 80 results for all combinations of m, n, l, but not \u03b3, as shown in part (A) of Figure (5). Several conclusions can be drawn: (a) Soft similarity brought massive improvements (Soft = 1 > Soft = 0 for all combinations of Soft and DTW), (c) DTW degraded performance in general (DTW = 1 < DTW = 0 for all combinations of Soft and Nbest). Summary (a) is consistent with the conclusion derived from the 20 results in Figure 4. Since generating a soft similarity metric is the only part of the search that could be performed, it is certainly attractive."}, {"heading": "5. CONCLUSION", "text": "In this paper, we present a new approach to unattended detection of spoken terms, using multi-level acoustic patterns from the target corpus.The different sample sets with different model configurations complement each other and can thus collectively collect the information for the spoken terms. Significantly better performance than frame-based DTW on the received TIMIT corpus."}, {"heading": "6. REFERENCES", "text": "In recent years, it has become clear that these two countries are a country where it is a country, where it is not a country, but a country where it is a country. [1] David RH Miller, Michael Kleber, Chia-Lin Kao, Owen Kimball, Thomas Colthurst, Stephen A Lowe, Richard M Schwartz, and Herbert Gish, \"Quick and Precise Spoken Terms.\" [2] Jonathan Mamou, Bhuvana Ramabhadran, and Olivier Siohan, and Olivier Siohan, \"Vocabulary spoken term detection,\" in which it is a term \"in which it is a country,\" in which it is a country. \""}], "references": [{"title": "Rapid and accurate spoken term detection", "author": ["David RH Miller", "Michael Kleber", "Chia-Lin Kao", "Owen Kimball", "Thomas Colthurst", "Stephen A Lowe", "Richard M Schwartz", "Herbert Gish"], "venue": "INTERSPEECH, 2007, pp. 314\u2013317.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2007}, {"title": "Vocabulary independent spoken term detection", "author": ["Jonathan Mamou", "Bhuvana Ramabhadran", "Olivier Siohan"], "venue": "Proceedings of the 30th annual international ACM SIGIR conference on Research and development in information retrieval. ACM, 2007, pp. 615\u2013622.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2007}, {"title": "A phonetic search approach to the 2006 nist spoken term detection evaluation", "author": ["Roy G Wallace", "Robert J Vogt", "Sridha Sridharan"], "venue": "2007.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2007}, {"title": "Performance analysis for lattice-based speech indexing approaches using words and subword units", "author": ["Yi-cheng Pan", "Lin-shan Lee"], "venue": "Audio, Speech, and Language Processing, IEEE Transactions on, vol. 18, no. 6, pp. 1562\u20131574, 2010.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2010}, {"title": "Lattice-based search for spoken utterance retrieval", "author": ["Murat Saraclar", "Richard Sproat"], "venue": "Urbana, vol. 51, pp. 61801, 2004.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 1801}, {"title": "Resources for speech research: present and future infrastructure needs", "author": ["Lou Boves", "Rolf Carlson", "Erhard W Hinrichs", "David House", "Steven Krauwer", "Lothar Lemnitzer", "Martti Vainio", "Peter Wittenburg"], "venue": "INTERSPEECH. Citeseer, 2009, pp. 1803\u20131806.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2009}, {"title": "Wwtw: the world wide telecom web", "author": ["Arun Kumar", "Nitendra Rajput", "Dipanjan Chakraborty", "Sheetal K Agarwal", "Amit A Nanavati"], "venue": "Proceedings of the 2007 workshop on Networked systems for developing regions. ACM, 2007, p. 7.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2007}, {"title": "The spoken web search task at mediaeval 2011", "author": ["Florian Metze", "Nitendra Rajput", "Xavier Anguera", "Marelie Davel", "Guillaume Gravier", "Charl Van Heerden", "Gautam V Mantena", "Armando Muscariello", "Kishore Prahallad", "Igor Szoke"], "venue": "Acoustics, Speech and Signal Processing (ICASSP), 2012 IEEE International Conference on. IEEE, 2012, pp. 5165\u20135168.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2012}, {"title": "Unsupervised hidden markov modeling of spoken queries for spoken term detection without speech recognition", "author": ["Chun-An Chan", "Lin-Shan Lee"], "venue": "INTERSPEECH, 2011, pp. 2141\u20132144.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2011}, {"title": "Rapid evaluation of speech representations for spoken term discovery", "author": ["Michael A Carlin", "Samuel Thomas", "Aren Jansen", "Hynek Hermansky"], "venue": "INTERSPEECH, 2011, pp. 821\u2013824.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2011}, {"title": "Unsupervised spoken keyword spotting via segmental dtw on gaussian posteriorgrams", "author": ["Yaodong Zhang", "James R Glass"], "venue": "Automatic Speech Recognition & Understanding, 2009. ASRU 2009. IEEE Workshop on. IEEE, 2009, pp. 398\u2013403.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2009}, {"title": "Unsupervised acoustic sub-word unit detection for query-byexample spoken term detection", "author": ["Marijn Huijbregts", "Mitchell McLaren", "David van Leeuwen"], "venue": "Acoustics, Speech and Signal Processing (ICASSP), 2011 IEEE International Conference on. IEEE, 2011, pp. 4436\u20134439.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2011}, {"title": "An acoustic segment modeling approach to query-by-example spoken term detection", "author": ["Haipeng Wang", "Cheung-Chi Leung", "Tan Lee", "Bin Ma", "Haizhou Li"], "venue": "Acoustics, Speech and Signal Processing (ICASSP), 2012 IEEE International Conference on. IEEE, 2012, pp. 5157\u20135160.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2012}, {"title": "A piecewise aggregate approximation lower-bound estimate for posteriorgram-based dynamic time warping", "author": ["Yaodong Zhang", "James R Glass"], "venue": "INTERSPEECH, 2011, pp. 1909\u20131912.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2011}, {"title": "Fast spoken query detection using lower-bound dynamic time warping on graphical processing units", "author": ["Yaodong Zhang", "Kiarash Adl", "James Glass"], "venue": "Acoustics, Speech and Signal Processing (ICASSP), 2012 IEEE International Conference on. IEEE, 2012, pp. 5173\u20135176.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2012}, {"title": "Indexing raw acoustic features for scalable zero resource search", "author": ["Aren Jansen", "Benjamin Van Durme"], "venue": "INTERSPEECH, 2012.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2012}, {"title": "Unsupervised discovery of linguistic structure including two-level acoustic patterns using three cascaded stages of iterative optimization", "author": ["Cheng-Tao Chung", "Chun-an Chan", "Lin-shan Lee"], "venue": "Acoustics, Speech and Signal Processing (ICASSP), 2013 IEEE International Conference on. IEEE, 2013, pp. 8081\u20138085.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2013}, {"title": "Towards spoken term discovery at scale with zero resources", "author": ["Aren Jansen", "Kenneth Church", "Hynek Hermansky"], "venue": "INTER- SPEECH, 2010, pp. 1676\u20131679.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2010}, {"title": "Towards unsupervised training of speaker independent acoustic models", "author": ["Aren Jansen", "Kenneth Church"], "venue": "INTERSPEECH, 2011, pp. 1693\u20131692.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2011}, {"title": "Unsupervised training of an hmm-based speech recognizer for topic classification", "author": ["Herbert Gish", "Man-hung Siu", "Arthur Chan", "William Belfield"], "venue": "INTERSPEECH, 2009, pp. 1935\u20131938.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2009}, {"title": "Unsupervised models for morpheme segmentation and morphology learning", "author": ["Mathias Creutz", "Krista Lagus"], "venue": "ACM Transactions on Speech and Language Processing (TSLP), vol. 4, no. 1, pp. 3, 2007.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2007}, {"title": "Enhancing query expansion for semantic retrieval of spoken content with automatically discovered acoustic patterns", "author": ["Hung-yi Lee", "Yun-Chiao Li", "Cheng-Tao Chung", "Lin-shan Lee"], "venue": "Acoustics, Speech and Signal Processing (ICASSP), 2013 IEEE International Conference on. IEEE, 2013, pp. 8297\u20138301.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2013}, {"title": "Approximating the kullback leibler divergence between gaussian mixture models", "author": ["John R Hershey", "Peder A Olsen"], "venue": "Acoustics, Speech and Signal Processing, 2007. ICASSP 2007. IEEE International Conference on. IEEE, 2007, vol. 4, pp. IV\u2013317.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2007}, {"title": "Constructing category hierarchies for visual recognition", "author": ["Marcin Marsza\u0142ek", "Cordelia Schmid"], "venue": "Computer Vision\u2013ECCV 2008, pp. 479\u2013491. Springer, 2008.", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2008}, {"title": "Query chains: learning to rank from implicit feedback", "author": ["Filip Radlinski", "Thorsten Joachims"], "venue": "Proceedings of the eleventh ACM SIGKDD international conference on Knowledge discovery in data mining. ACM, 2005, pp. 239\u2013248.", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2005}, {"title": "Learning to rank for information retrieval", "author": ["Tie-Yan Liu"], "venue": "Foundations and Trends in Information Retrieval, vol. 3, no. 3, pp. 225\u2013 331, 2009.", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2009}], "referenceMentions": [{"referenceID": 0, "context": "Spoken term detection (STD) usually refers to the task of finding all occurrences of the text query term from a large spoken archive [1].", "startOffset": 133, "endOffset": 136}, {"referenceID": 0, "context": "Most STD approaches were based on automatic speech recognition (ASR), transforming speech into words or subwords for token matching [1][2][3][4], with performance relying heavily on the ASR accuracy [5].", "startOffset": 132, "endOffset": 135}, {"referenceID": 1, "context": "Most STD approaches were based on automatic speech recognition (ASR), transforming speech into words or subwords for token matching [1][2][3][4], with performance relying heavily on the ASR accuracy [5].", "startOffset": 135, "endOffset": 138}, {"referenceID": 2, "context": "Most STD approaches were based on automatic speech recognition (ASR), transforming speech into words or subwords for token matching [1][2][3][4], with performance relying heavily on the ASR accuracy [5].", "startOffset": 138, "endOffset": 141}, {"referenceID": 3, "context": "Most STD approaches were based on automatic speech recognition (ASR), transforming speech into words or subwords for token matching [1][2][3][4], with performance relying heavily on the ASR accuracy [5].", "startOffset": 141, "endOffset": 144}, {"referenceID": 4, "context": "Most STD approaches were based on automatic speech recognition (ASR), transforming speech into words or subwords for token matching [1][2][3][4], with performance relying heavily on the ASR accuracy [5].", "startOffset": 199, "endOffset": 202}, {"referenceID": 5, "context": "In this way, the difficulties in conventional ASR such as recognition errors and need for annotated training data may be bypassed, which is especially attractive for languages with very limited annotated data [6][7] or spoken content with unknown languages.", "startOffset": 209, "endOffset": 212}, {"referenceID": 6, "context": "In this way, the difficulties in conventional ASR such as recognition errors and need for annotated training data may be bypassed, which is especially attractive for languages with very limited annotated data [6][7] or spoken content with unknown languages.", "startOffset": 212, "endOffset": 215}, {"referenceID": 7, "context": "This leads to recent efforts in unsupervised STD with spoken queries without using annotated data during training [8] [9], which is also the focus of this work.", "startOffset": 114, "endOffset": 117}, {"referenceID": 8, "context": "This leads to recent efforts in unsupervised STD with spoken queries without using annotated data during training [8] [9], which is also the focus of this work.", "startOffset": 118, "endOffset": 121}, {"referenceID": 9, "context": "Many related works focused on feature representations and distance measures within the DTW framework that are more robust to speaker and acoustic condition variations[10], including using the posteriorgrams from a universal Gaussian mixture model [11], and the acoustic segment models [12][13].", "startOffset": 166, "endOffset": 170}, {"referenceID": 10, "context": "Many related works focused on feature representations and distance measures within the DTW framework that are more robust to speaker and acoustic condition variations[10], including using the posteriorgrams from a universal Gaussian mixture model [11], and the acoustic segment models [12][13].", "startOffset": 247, "endOffset": 251}, {"referenceID": 11, "context": "Many related works focused on feature representations and distance measures within the DTW framework that are more robust to speaker and acoustic condition variations[10], including using the posteriorgrams from a universal Gaussian mixture model [11], and the acoustic segment models [12][13].", "startOffset": 285, "endOffset": 289}, {"referenceID": 12, "context": "Many related works focused on feature representations and distance measures within the DTW framework that are more robust to speaker and acoustic condition variations[10], including using the posteriorgrams from a universal Gaussian mixture model [11], and the acoustic segment models [12][13].", "startOffset": 289, "endOffset": 293}, {"referenceID": 8, "context": "Substantial efforts were devoted to reducing this computation load, such as segment-based DTW [9], lowerbound estimation for DTW [14][15], and a locality sensitive hashing technique for indexing speech frames [16].", "startOffset": 94, "endOffset": 97}, {"referenceID": 13, "context": "Substantial efforts were devoted to reducing this computation load, such as segment-based DTW [9], lowerbound estimation for DTW [14][15], and a locality sensitive hashing technique for indexing speech frames [16].", "startOffset": 129, "endOffset": 133}, {"referenceID": 14, "context": "Substantial efforts were devoted to reducing this computation load, such as segment-based DTW [9], lowerbound estimation for DTW [14][15], and a locality sensitive hashing technique for indexing speech frames [16].", "startOffset": 133, "endOffset": 137}, {"referenceID": 15, "context": "Substantial efforts were devoted to reducing this computation load, such as segment-based DTW [9], lowerbound estimation for DTW [14][15], and a locality sensitive hashing technique for indexing speech frames [16].", "startOffset": 209, "endOffset": 213}, {"referenceID": 16, "context": "In recent years substantial effort has been made for unsupervised model based discovery of acoustic patterns from corpora without manual annotation[17][18][19][20][21][22].", "startOffset": 147, "endOffset": 151}, {"referenceID": 17, "context": "In recent years substantial effort has been made for unsupervised model based discovery of acoustic patterns from corpora without manual annotation[17][18][19][20][21][22].", "startOffset": 151, "endOffset": 155}, {"referenceID": 18, "context": "In recent years substantial effort has been made for unsupervised model based discovery of acoustic patterns from corpora without manual annotation[17][18][19][20][21][22].", "startOffset": 155, "endOffset": 159}, {"referenceID": 19, "context": "In recent years substantial effort has been made for unsupervised model based discovery of acoustic patterns from corpora without manual annotation[17][18][19][20][21][22].", "startOffset": 159, "endOffset": 163}, {"referenceID": 20, "context": "In recent years substantial effort has been made for unsupervised model based discovery of acoustic patterns from corpora without manual annotation[17][18][19][20][21][22].", "startOffset": 163, "endOffset": 167}, {"referenceID": 21, "context": "In recent years substantial effort has been made for unsupervised model based discovery of acoustic patterns from corpora without manual annotation[17][18][19][20][21][22].", "startOffset": 167, "endOffset": 171}, {"referenceID": 18, "context": "Given an unlabelled speech corpus, it is not difficult for unsupervised discovery of the desired acoustic patterns from the corpus for a chosen hyperparameter set \u03c8 that determines the topology of the HMMs [19][20][21].", "startOffset": 206, "endOffset": 210}, {"referenceID": 19, "context": "Given an unlabelled speech corpus, it is not difficult for unsupervised discovery of the desired acoustic patterns from the corpus for a chosen hyperparameter set \u03c8 that determines the topology of the HMMs [19][20][21].", "startOffset": 210, "endOffset": 214}, {"referenceID": 20, "context": "Given an unlabelled speech corpus, it is not difficult for unsupervised discovery of the desired acoustic patterns from the corpus for a chosen hyperparameter set \u03c8 that determines the topology of the HMMs [19][20][21].", "startOffset": 214, "endOffset": 218}, {"referenceID": 16, "context": "This can be achieved by first finding an initial label \u03c90 based on a set of assumed patterns for all observations in the corpus \u03c7 as in (1)[17].", "startOffset": 139, "endOffset": 143}, {"referenceID": 22, "context": "The KL-divergence KL(i, j) between two pattern HMMs in (4b) is defined as the KLdivergence between the states based on the variational approximation [23] summed over the states.", "startOffset": 149, "endOffset": 153}, {"referenceID": 23, "context": "To transform the KL divergence into a similarity measure between 0 and 1, a negative exponential was applied [24] with a scaling factor \u03b2.", "startOffset": 109, "endOffset": 113}, {"referenceID": 24, "context": "Note that although choices of \u03bb(\u03c8k, \u03b3s) can be based on the optimization with respect to an evaluation metric [25][26], the preliminary experiments in this work were to verify the feasibility of the proposed frameworks instead.", "startOffset": 110, "endOffset": 114}, {"referenceID": 25, "context": "Note that although choices of \u03bb(\u03c8k, \u03b3s) can be based on the optimization with respect to an evaluation metric [25][26], the preliminary experiments in this work were to verify the feasibility of the proposed frameworks instead.", "startOffset": 114, "endOffset": 118}], "year": 2015, "abstractText": "This paper presents a new approach for unsupervised Spoken Term Detection with spoken queries using multiple sets of acoustic patterns automatically discovered from the target corpus. The different pattern HMM configurations(number of states per model, number of distinct models, number of Gaussians per state)form a three-dimensional model granularity space. Different sets of acoustic patterns automatically discovered on different points properly distributed over this three-dimensional space are complementary to one another, thus can jointly capture the characteristics of the spoken terms. By representing the spoken content and spoken query as sequences of acoustic patterns, a series of approaches for matching the pattern index sequences while considering the signal variations are developed. In this way, not only the on-line computation load can be reduced, but the signal distributions caused by different speakers and acoustic conditions can be reasonably taken care of. The results indicate that this approach significantly outperformed the unsupervised feature-based DTW baseline by 16.16% in mean average precision on the TIMIT corpus.", "creator": "LaTeX with hyperref package"}}}