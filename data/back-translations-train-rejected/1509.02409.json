{"id": "1509.02409", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "8-Sep-2015", "title": "Data-selective Transfer Learning for Multi-Domain Speech Recognition", "abstract": "Negative transfer in training of acoustic models for automatic speech recognition has been reported in several contexts such as domain change or speaker characteristics. This paper proposes a novel technique to overcome negative transfer by efficient selection of speech data for acoustic model training. Here data is chosen on relevance for a specific target. A submodular function based on likelihood ratios is used to determine how acoustically similar each training utterance is to a target test set. The approach is evaluated on a wide-domain data set, covering speech from radio and TV broadcasts, telephone conversations, meetings, lectures and read speech. Experiments demonstrate that the proposed technique both finds relevant data and limits negative transfer. Results on a 6--hour test set show a relative improvement of 4% with data selection over using all data in PLP based models, and 2% with DNN features.", "histories": [["v1", "Tue, 8 Sep 2015 15:20:12 GMT  (71kb,D)", "http://arxiv.org/abs/1509.02409v1", null]], "reviews": [], "SUBJECTS": "cs.LG cs.CL cs.SD", "authors": ["mortaza doulaty", "oscar saz", "thomas hain"], "accepted": false, "id": "1509.02409"}, "pdf": {"name": "1509.02409.pdf", "metadata": {"source": "CRF", "title": "Data\u2013selective Transfer Learning for Multi\u2013Domain Speech Recognition", "authors": ["Mortaza Doulaty", "Oscar Saz", "Thomas Hain"], "emails": ["t.hain}@sheffield.ac.uk"], "sections": [{"heading": null, "text": "The results of a 6-hour test show a relative improvement of 4% in data selection about the use of all data in PLP-based models and 2% in DNN functions. Index terms: data selection, transfer learning, negative transfer, speech recognition. The results of a 6-hour test show a relative improvement of 4% in data selection about the use of all data in PLP-based models and 2% in DNN functions."}, {"heading": "1. Introduction", "text": "As if that were not already enough, one has to ask oneself to what extent it is even possible for a solution to be found at all."}, {"heading": "2. Data selection for ASR", "text": "Data selection for ASR has been largely studied for minimal representative data selection [5, 8, 9, 7, 10, 11, 4, 12, 13]. Here, given a large pool of training data, the goal is to find a subset of data so that a model set trained with these data achieves performance comparable to a model set trained with all data. (This line of work refers to active learning, with the goal of selecting a subset for manual transcription with the lowest budget [14, 15, 16], and with unattended and semi-monitored learning techniques, the general goal being to select a subset of the training set with the most reliable transcripts available [17, 18, 19].Two techniques are typically used for data selection: uncertainty sampling [20], where the results from an existing model are used to select or reject data; and query committee [21], where clearly trained models are used."}, {"heading": "3. Likelihood ratio data selection", "text": "To determine whether data is similar to a training set, one can opt for a classification approach that identifies a suitable object or not. In this case, we use the likelihood ratio (LR) between a GMM trained on the basis of the target data and a GMM trained on the basis of the entire training set (LR). The total LR of an expression in the training set LR (O), O \u00b2 of the length of T-frames is defined as the geometric mean of the frame-based LR value of the target data model. (4) One can define a modular function [22] based on the cumulative LRs of all expressions contained in a subset S \u00b2."}, {"heading": "4. Experimental setup", "text": "To evaluate the proposed approach in a multi-domain ASR task, a data set was selected that combines 6 different types of data from the following sources: \u2022 Radio (RD): BBC Radio4 broadcasts in February 2009. \u2022 Television (TV): broadcasts from BBC in May 2008. \u2022 Telephone Speech (CT): From Fisher Corpus1 [25]. \u2022 Meetings (MT): From AMI [26] and ICSI [27] Corpora. \u2022 Lectures (TK): From TedTalks [28]. \u2022 Language Reading (RS): From the WSJCAM0 corpus [29]. A subset of 10h from each domain was selected to form the training set (60 h in each domain), and 1h from each domain was used for testing (6h in total). The selection of domains aims to cover the most common and distinctive types of audio recordings."}, {"heading": "4.1. Baseline results", "text": "Table 1 shows results with both types of acoustic characteristics. These results show the wide range of performance in different areas, from 17-18% for reading and radio broadcasts to 51% for television broadcasts. The use of DNN frontends results in a relative 25% improvement in performance over PLP characteristics; this is consistent across all areas and follows results previously observed in the literature [33]."}, {"heading": "5. Results", "text": "A first series of experiments was conducted to identify and measure negative transmissions in ASR tasks, and an evaluation of the proposed data selection technique was conducted. (1) All telephone call data was extrapolated to 16 kHz to match the sampling rate of the remaining data."}, {"heading": "5.1. Evaluation of negative transfer", "text": "Six different domain-dependent MLE models were trained from the 10 hours of training data for each domain (in all experiments PLP functions were used unless otherwise stated) and each of these models was then used to decipher the complete test set. Results in Table 2 show that the results in the domain (if the train and test data on manually labeled domains do not match) are not significantly different from those obtained with a model trained on 60 hours of training. Instead, cross-domain results (train and test do not match) result in significant performance decreases everywhere. A second series of experiments was conducted with models trained on 20 hours of data, using data from all possible domain pairs, for a total of 30 new acoustic models. Figure 1 shows the results in terms of improvement and degradation compared to the results of the 10-hour in-domain models. Figure 1 series represent the test domain and the columns representing the domain that were added to the data when reading the domain."}, {"heading": "5.2. Data selection based on budget", "text": "Next, the data selection technique proposed in Section 3 was evaluated; for each of the six target test domains, Gaussian Mixture Models (GMM) were trained with 512 mixtures (Tgt1: 6), and a background mixture of 512 GMM mixtures (Tgt1: 6) was trained from the entire 60-hour training set. These GMMs were used to calculate the LR value for each training expression (LR (O) to select training data according to acoustic similarity; the initial evaluation was based on the data selection based on the budget. Five possible budgets of 10, 20, 30, 40 and 50 hours were designed for each test domain and the corresponding training data were selected using the submodular function fLR (S). Figure 2 shows a relative improvement for each domain and the budget compared to the results with the 60-hour model. The graphs show that all domains improve performance until a certain limit is reached."}, {"heading": "5.3. Automatic decision on budget", "text": "One problem that may arise with the evaluated budget proposal is the fact that a decision on a budget amount has to be made, and as the results in Figure 2 suggest, the optimal budget varies in different areas. A method for determining a budget for a particular target area was proposed by selecting only statements with a probability ratio above a threshold defined as the mean of the highest-weighted mixture of a GMM adapted to the distribution of probability ratios. Using the mixture with the highest weight avoids the influence of outliers in the distribution of the KR values. Experiments with an automatic budget decision were conducted for both types of characteristics, PLP and PLP + BN. Table 3 presents the results for these experiments and compares them with the result of data selection based on a 30-hour budget that was the best fixed budget from Figure 2. The results showed that using an automatically derived threshold meant the results for both types of data were less improved, suggesting that the desired amount of data could be selected for each table (suggesting the desired amount of data for the proposed method) from Figure 2."}, {"heading": "6. Conclusion", "text": "We confirmed that the use of more data in MLE-based acoustic models does not always lead to increased performance. A submodular function based on the likelihood ratio was proposed and used to perform an informed and efficient selection of data for different target test sets. Evaluation of selection techniques based on budget and automatic budget decision has resulted in increases of 4% over a 60-hour MLE model for PLP characteristics and 2% for PLP + BN characteristics. Previous work has shown that data selection techniques can lead to datasets that tend toward specific groups of phones or triphons [19]. Phonetic analysis of the datasets provided by the likelihood ratio function used in this paper showed no bias for phones in these datasets. The 60-hour training data used in this work were phonetically well-balanced, reducing the risk of distorted phonetic distortions in the limited data."}, {"heading": "7. Acknowledgements", "text": "This work was supported by the EPSRC Programme Grant EP / I031022 / 1 Natural Speech Technology (NST)."}, {"heading": "8. Data Access Statement", "text": "The language data used in this paper comes from the following sources: Fisher Corpus (LDC catalog number LDC2004T19), ICSI Meetings corpus (LDC catalog number LDC2004S02), WSJCAM0 (LDC catalog number LDC95S24), AMI corpus (DOI number 10.1007 / 11677482 3), TedTalks data (freely available as part of IWSLT evaluations), BBC radio and television data (this data has been distributed to NST project partners under an agreement with BBC F & E and is not yet publicly available), the specific file lists used for training and testing in this paper, and the results files can be downloaded from http: / / mini.dcs.shef.ac.uk / publications / papers / is15-doulaty."}, {"heading": "9. References", "text": "[1] H. Hermansky, \"Perceptual linear predictive (PLP) analysis ofspeech,\" The Journal of the Acoustical Society of America, vol. 87, no. 4, pp. 1738-1752, 1990. [2] F. Grezl, M. Karafia \u0301 t, S. Konta \u0301 r, and J. Cernocky, \"Probabilistic and bottle-neck features for LVCSR of meetings.\" [4] Proceedings of ICASSP, Hawaii, USA, 2007, pp. 757-760. [3] X. Huang, A. Acero, and H. Hon, Spoken language processing. Prentice Hall: Englewood Cliffs, 2001, O. Kapralova, E. Weinstein, P. Moreno, and O. Siohan \"A big data approach to acoustic model training corpus selection,\" in Proceedings of Interspeech, Singapore, 2014, pp. 2083-2087. [5] K. Weohan, Wehoff J. Libler, K. \""}], "references": [{"title": "Perceptual linear predictive (PLP) analysis of speech", "author": ["H. Hermansky"], "venue": "The Journal of the Acoustical Society of America, vol. 87, no. 4, pp. 1738\u20131752, 1990.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 1990}, {"title": "Cernock\u1ef3, \u201cProbabilistic and bottle-neck features for LVCSR of meetings.", "author": ["F. Grezl", "M. Karafi\u00e1t", "S. Kont\u00e1r"], "venue": "Proceedings of ICASSP, Hawaii,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2007}, {"title": "A big data approach to acoustic model training corpus selection", "author": ["O. Kapralova", "J. Alex", "E. Weinstein", "P. Moreno", "O. Siohan"], "venue": "Proceedings of Interspeech, Singapore, 2014, pp. 2083\u20132087.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2014}, {"title": "Unsupervised submodular subset selection for speech data", "author": ["K. Wei", "Y. Liu", "K. Kirchhoff", "J. Bilmes"], "venue": "Proceedings of ICASSP, Florence, Italy, 2014.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2014}, {"title": "To transfer or not to transfer", "author": ["M.T. Rosenstein", "Z. Marx", "L.P. Kaelbling", "T.G. Dietterich"], "venue": "NIPS 2005 Workshop on Transfer Learning, vol. 898, 2005.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2005}, {"title": "How to select a good training-data subset for transcription: Submodular active selection for sequences", "author": ["H. Lin", "J. Bilmes"], "venue": "Proceedings of Interspeech, Brighton, UK, 2009.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2009}, {"title": "Submodular subset selection for large-scale speech training data", "author": ["K. Wei", "Y. Liu", "K. Kirchhoff", "C. Bartels", "J. Bilmes"], "venue": "Proceedings of ICASSP, Florence, Italy, 2014.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2014}, {"title": "Data selection for speech recognition", "author": ["Y. Wu", "R. Zhang", "A. Rudnicky"], "venue": "Proceedings of ASRU, Kyoto, Japan, 2007, pp. 562\u2013565.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2007}, {"title": "A new data selection approach for semi-supervised acoustic modeling", "author": ["R. Zhang", "A. Rudnicky"], "venue": "Proceedings of ICASSP, Toulouse, France, 2006.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2006}, {"title": "In search of optimal data selection for training of automatic speech recognition systems", "author": ["A. Nagroski", "L. Boves", "H. Steeneken"], "venue": "Proceedings of ASRU, St. Thomas, US Virgin Islands, 2003, pp. 67\u201372.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2003}, {"title": "Kullback-Leibler divergence-based ASR training data selection", "author": ["E. Gouvea", "M.H. Davel"], "venue": "Proceedings of Interspeech, Florence, Italy, 2011, pp. 2297\u20132300.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2011}, {"title": "iVector-based acoustic data selection.", "author": ["O. Siohan", "M. Bacchiani"], "venue": "Proceedings of Interspeech, Lyon, France,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2013}, {"title": "Active learning for spoken language understanding", "author": ["G. Tur", "R. Schapire", "D. Hakkani-T\u00fcr"], "venue": "Proceedings of ICASSP, Hong Kong, 2003.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2003}, {"title": "Active learning literature survey", "author": ["B. Settles"], "venue": "University of Wisconsin, Madison, WI, USA, Tech. Rep., 2010.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2010}, {"title": "Unsupervised training of acoustic models for large vocabulary continuous speech recognition", "author": ["F. Wessel", "H. Ney"], "venue": "IEEE Transactions on Speech and Audio Processing, vol. 13, no. 1, pp. 23\u201331, 2005.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2005}, {"title": "Automatic transcription of multi-genre media archives", "author": ["P. Lanchantin", "P.J. Bell", "M.J. Gales", "T. Hain", "X. Liu", "Y. Long", "J. Quinnell", "S. Renals", "O. Saz", "M.S. Seigel"], "venue": "Proceedings of SLAM Workshop, Marseille, France, 2013.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2013}, {"title": "Training data selection based on context-dependent state matching", "author": ["O. Siohan"], "venue": "Proceedings of ICASSP, Florence, Italy, 2014, pp. 3316\u20133319.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2014}, {"title": "Semi-supervised learning literature survey", "author": ["X. Zhu"], "venue": "University of Wisconsin, Madison, WI, USA, Tech. Rep., 2005.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2005}, {"title": "Query by committee", "author": ["H.S. Seung", "M. Opper", "H. Sompolinsky"], "venue": "Proceedings of COLT Workshop, Pittsburgh, PA, USA, 1992, pp. 287\u2013294.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 1992}, {"title": "Submodular function maximization", "author": ["A. Krause", "D. Golovin"], "venue": "Tractability: Practical Approaches to Hard Problems, 2014.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2014}, {"title": "An analysis of approximations for maximizing submodular set functions I", "author": ["G. Nemhauser", "L. Wolsey", "M. Fisher"], "venue": "Mathematical Programming, vol. 14, no. 1, pp. 265\u2013294, 1978.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 1978}, {"title": "Using document summarization techniques for speech data subset selection.", "author": ["K. Wei", "Y. Liu", "K. Kirchhoff", "J. Bilmes"], "venue": "Proceedings of HLT-NAACL, Atlanta,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2013}, {"title": "The Fisher corpus: A resource for the next generations of speech-to-text.", "author": ["C. Cieri", "D. Miller", "K. Walker"], "venue": "Proceedings of LREC, Lisbon, Portugal,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2004}, {"title": "The AMI meeting corpus: A preannouncement", "author": ["J. Carletta", "S. Ashby", "S. Bourban", "M. Flynn", "M. Guillemot", "T. Hain", "J. Kadlec", "W. Karaiskos", "Vasilis Kraaij", "M. Kronenthal", "G. Lathoud", "M. Lincoln", "A. Lisowska", "I. McCowan", "W. Post", "D. Reidsma", "P. Wellner"], "venue": "Proceedings of MLMI, Bethesda, USA, 2006, pp. 28\u201339.", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2006}, {"title": "The ICSI meeting corpus", "author": ["A. Janin", "D. Baron", "J. Edwards", "D. Ellis", "D. Gelbart", "N. Morgan", "B. Peskin", "T. Pfau", "E. Shriberg", "A. Stolcke", "C. Wooters"], "venue": "Proceedings of ICASSP, Hong Kong, 2003.", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2003}, {"title": "The USFD spoken language translation system for IWSLT 2014", "author": ["R.W.N. Ng", "M. Doulaty", "R. Doddipatla", "O. Saz", "M. Hasan", "T. Hain", "W. Aziz", "K. Shaf", "L. Specia"], "venue": "Lake Tahoe, USA, 2014.", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2014}, {"title": "WSJ- CAM0: A british english speech corpus for large vocabulary continuous speech recognition", "author": ["T. Robinson", "J. Fransen", "D. Pye", "J. Foote", "S. Renals"], "venue": "Proceedings of ICASSP, Detroit, USA, 1995.", "citeRegEx": "29", "shortCiteRegEx": null, "year": 1995}, {"title": "Parallel training of neural networks for speech recognition", "author": ["K. Vesely", "L. Burget", "F. Grezl"], "venue": "Proceedings of Interspeech, Makuhari, Japan, 2010.", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2010}, {"title": "Using neural network front-ends on far field multiple microphones based speech recognition", "author": ["Y. Liu", "P. Zhang", "T. Hain"], "venue": "Proceedings of ICASSP, Florence, Italy, 2014.", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2014}, {"title": "The HTK book (for HTK version 3.4)", "author": ["S. Young", "G. Evermann", "M. Gales", "T. Hain", "D. Kershaw", "X. Liu", "G. Moore", "J. Odell", "D. Ollason", "D. Povey"], "venue": "Cambridge university engineering department, vol. 2, no. 2, 2006.", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2006}, {"title": "A scalable approach to using dnnderived features in gmm-hmm based acoustic modeling for lvcsr.", "author": ["Z.-J. Yan", "Q. Huo", "J. Xu"], "venue": "Proceedings of Interspeech,", "citeRegEx": "33", "shortCiteRegEx": "33", "year": 2013}, {"title": "Background\u2013tracking acoustic features for genre identification of broadcast shows", "author": ["O. Saz", "M. Doulaty", "T. Hain"], "venue": "Lake Tahoe NV, USA, 2014.", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2014}], "referenceMentions": [{"referenceID": 0, "context": "Maximum Likelihood Estimation (MLE) of Gaussian Mixture Model (GMM) parameters of a Hidden Markov Model (HMM) is still a standard approach to train acoustic models in ASR, either with perceptually\u2013based features like Perceptual Linear Prediction (PLP) features [1], or with Deep Neural Network (DNN) based features [2] in tandem configuration.", "startOffset": 261, "endOffset": 264}, {"referenceID": 1, "context": "Maximum Likelihood Estimation (MLE) of Gaussian Mixture Model (GMM) parameters of a Hidden Markov Model (HMM) is still a standard approach to train acoustic models in ASR, either with perceptually\u2013based features like Perceptual Linear Prediction (PLP) features [1], or with Deep Neural Network (DNN) based features [2] in tandem configuration.", "startOffset": 315, "endOffset": 318}, {"referenceID": 2, "context": "g [4]).", "startOffset": 2, "endOffset": 5}, {"referenceID": 3, "context": "However, adding more data does not guarantee that the performance of the system will improve, and even if it does, the gains become smaller and smaller [5].", "startOffset": 152, "endOffset": 155}, {"referenceID": 4, "context": "A further effect, negative transfer, is found in several examples, which indicates that knowledge acquired for a task can have a negative performance effect in another task [6].", "startOffset": 173, "endOffset": 176}, {"referenceID": 5, "context": "Submodular functions have been successfully used before to select data in semisupervised training and active learning for ASR tasks [7, 8].", "startOffset": 132, "endOffset": 138}, {"referenceID": 6, "context": "Submodular functions have been successfully used before to select data in semisupervised training and active learning for ASR tasks [7, 8].", "startOffset": 132, "endOffset": 138}, {"referenceID": 3, "context": "Data selection for ASR has mostly been studied for minimal representative data selection [5, 8, 9, 7, 10, 11, 4, 12, 13].", "startOffset": 89, "endOffset": 120}, {"referenceID": 6, "context": "Data selection for ASR has mostly been studied for minimal representative data selection [5, 8, 9, 7, 10, 11, 4, 12, 13].", "startOffset": 89, "endOffset": 120}, {"referenceID": 7, "context": "Data selection for ASR has mostly been studied for minimal representative data selection [5, 8, 9, 7, 10, 11, 4, 12, 13].", "startOffset": 89, "endOffset": 120}, {"referenceID": 5, "context": "Data selection for ASR has mostly been studied for minimal representative data selection [5, 8, 9, 7, 10, 11, 4, 12, 13].", "startOffset": 89, "endOffset": 120}, {"referenceID": 8, "context": "Data selection for ASR has mostly been studied for minimal representative data selection [5, 8, 9, 7, 10, 11, 4, 12, 13].", "startOffset": 89, "endOffset": 120}, {"referenceID": 9, "context": "Data selection for ASR has mostly been studied for minimal representative data selection [5, 8, 9, 7, 10, 11, 4, 12, 13].", "startOffset": 89, "endOffset": 120}, {"referenceID": 2, "context": "Data selection for ASR has mostly been studied for minimal representative data selection [5, 8, 9, 7, 10, 11, 4, 12, 13].", "startOffset": 89, "endOffset": 120}, {"referenceID": 10, "context": "Data selection for ASR has mostly been studied for minimal representative data selection [5, 8, 9, 7, 10, 11, 4, 12, 13].", "startOffset": 89, "endOffset": 120}, {"referenceID": 11, "context": "Data selection for ASR has mostly been studied for minimal representative data selection [5, 8, 9, 7, 10, 11, 4, 12, 13].", "startOffset": 89, "endOffset": 120}, {"referenceID": 12, "context": "This line of work is related to active learning, where the aim is to select a subset for manual transcription with the least budget [14, 15, 16], and with unsupervised and semi\u2013 supervised learning techniques, where the overall objective is to select a subset of the training set with the most reliable available transcripts [17, 18, 19].", "startOffset": 132, "endOffset": 144}, {"referenceID": 13, "context": "This line of work is related to active learning, where the aim is to select a subset for manual transcription with the least budget [14, 15, 16], and with unsupervised and semi\u2013 supervised learning techniques, where the overall objective is to select a subset of the training set with the most reliable available transcripts [17, 18, 19].", "startOffset": 132, "endOffset": 144}, {"referenceID": 14, "context": "This line of work is related to active learning, where the aim is to select a subset for manual transcription with the least budget [14, 15, 16], and with unsupervised and semi\u2013 supervised learning techniques, where the overall objective is to select a subset of the training set with the most reliable available transcripts [17, 18, 19].", "startOffset": 325, "endOffset": 337}, {"referenceID": 15, "context": "This line of work is related to active learning, where the aim is to select a subset for manual transcription with the least budget [14, 15, 16], and with unsupervised and semi\u2013 supervised learning techniques, where the overall objective is to select a subset of the training set with the most reliable available transcripts [17, 18, 19].", "startOffset": 325, "endOffset": 337}, {"referenceID": 16, "context": "This line of work is related to active learning, where the aim is to select a subset for manual transcription with the least budget [14, 15, 16], and with unsupervised and semi\u2013 supervised learning techniques, where the overall objective is to select a subset of the training set with the most reliable available transcripts [17, 18, 19].", "startOffset": 325, "endOffset": 337}, {"referenceID": 17, "context": "Two techniques are typically used for selecting data: uncertainty sampling [20], where the scores from an existing model are used to choose or reject data; and query by committee [21], where votes of distinctly trained models are used [7].", "startOffset": 75, "endOffset": 79}, {"referenceID": 18, "context": "Two techniques are typically used for selecting data: uncertainty sampling [20], where the scores from an existing model are used to choose or reject data; and query by committee [21], where votes of distinctly trained models are used [7].", "startOffset": 179, "endOffset": 183}, {"referenceID": 5, "context": "Two techniques are typically used for selecting data: uncertainty sampling [20], where the scores from an existing model are used to choose or reject data; and query by committee [21], where votes of distinctly trained models are used [7].", "startOffset": 235, "endOffset": 238}, {"referenceID": 14, "context": "Confidence scores are used to select data with the most reliable transcriptions, as in semi\u2013supervised training [17, 4], or to select data for manual transcription in active learning [15, 14].", "startOffset": 112, "endOffset": 119}, {"referenceID": 2, "context": "Confidence scores are used to select data with the most reliable transcriptions, as in semi\u2013supervised training [17, 4], or to select data for manual transcription in active learning [15, 14].", "startOffset": 112, "endOffset": 119}, {"referenceID": 12, "context": "Confidence scores are used to select data with the most reliable transcriptions, as in semi\u2013supervised training [17, 4], or to select data for manual transcription in active learning [15, 14].", "startOffset": 183, "endOffset": 191}, {"referenceID": 5, "context": "Entropy\u2013based methods aim to pick data that, for instance, fits a uniform distribution of target units (phonemes, words, etc), resulting in maximum entropy [7, 10, 9] or having a similar distribution to a target set [12, 13, 19].", "startOffset": 156, "endOffset": 166}, {"referenceID": 8, "context": "Entropy\u2013based methods aim to pick data that, for instance, fits a uniform distribution of target units (phonemes, words, etc), resulting in maximum entropy [7, 10, 9] or having a similar distribution to a target set [12, 13, 19].", "startOffset": 156, "endOffset": 166}, {"referenceID": 7, "context": "Entropy\u2013based methods aim to pick data that, for instance, fits a uniform distribution of target units (phonemes, words, etc), resulting in maximum entropy [7, 10, 9] or having a similar distribution to a target set [12, 13, 19].", "startOffset": 156, "endOffset": 166}, {"referenceID": 10, "context": "Entropy\u2013based methods aim to pick data that, for instance, fits a uniform distribution of target units (phonemes, words, etc), resulting in maximum entropy [7, 10, 9] or having a similar distribution to a target set [12, 13, 19].", "startOffset": 216, "endOffset": 228}, {"referenceID": 11, "context": "Entropy\u2013based methods aim to pick data that, for instance, fits a uniform distribution of target units (phonemes, words, etc), resulting in maximum entropy [7, 10, 9] or having a similar distribution to a target set [12, 13, 19].", "startOffset": 216, "endOffset": 228}, {"referenceID": 16, "context": "Entropy\u2013based methods aim to pick data that, for instance, fits a uniform distribution of target units (phonemes, words, etc), resulting in maximum entropy [7, 10, 9] or having a similar distribution to a target set [12, 13, 19].", "startOffset": 216, "endOffset": 228}, {"referenceID": 6, "context": "The use of submodular functions has been proposed to tackle the effect of the diminishing returns, when adding more data to a training set [8, 5, 7].", "startOffset": 139, "endOffset": 148}, {"referenceID": 3, "context": "The use of submodular functions has been proposed to tackle the effect of the diminishing returns, when adding more data to a training set [8, 5, 7].", "startOffset": 139, "endOffset": 148}, {"referenceID": 5, "context": "The use of submodular functions has been proposed to tackle the effect of the diminishing returns, when adding more data to a training set [8, 5, 7].", "startOffset": 139, "endOffset": 148}, {"referenceID": 19, "context": "Finding S is an NP\u2013hard problem [22, 8] and greedy solutions are proposed where the subset S is increased iteratively by the item s \u2208 \u03a9 that maximises the value of f when added to S as in Equation 3.", "startOffset": 32, "endOffset": 39}, {"referenceID": 6, "context": "Finding S is an NP\u2013hard problem [22, 8] and greedy solutions are proposed where the subset S is increased iteratively by the item s \u2208 \u03a9 that maximises the value of f when added to S as in Equation 3.", "startOffset": 32, "endOffset": 39}, {"referenceID": 20, "context": "If the function f is a normalised monotone submodular function, then the simple greedy algorithm provides a good approximation of the optimal solution [23, 22, 7] Several functions f can be found in the literature to perform data selection for ASR tasks, including facility location functions, saturated coverage functions [24, 8], diversity reward functions [5] or graph cut functions [7].", "startOffset": 151, "endOffset": 162}, {"referenceID": 19, "context": "If the function f is a normalised monotone submodular function, then the simple greedy algorithm provides a good approximation of the optimal solution [23, 22, 7] Several functions f can be found in the literature to perform data selection for ASR tasks, including facility location functions, saturated coverage functions [24, 8], diversity reward functions [5] or graph cut functions [7].", "startOffset": 151, "endOffset": 162}, {"referenceID": 5, "context": "If the function f is a normalised monotone submodular function, then the simple greedy algorithm provides a good approximation of the optimal solution [23, 22, 7] Several functions f can be found in the literature to perform data selection for ASR tasks, including facility location functions, saturated coverage functions [24, 8], diversity reward functions [5] or graph cut functions [7].", "startOffset": 151, "endOffset": 162}, {"referenceID": 21, "context": "If the function f is a normalised monotone submodular function, then the simple greedy algorithm provides a good approximation of the optimal solution [23, 22, 7] Several functions f can be found in the literature to perform data selection for ASR tasks, including facility location functions, saturated coverage functions [24, 8], diversity reward functions [5] or graph cut functions [7].", "startOffset": 323, "endOffset": 330}, {"referenceID": 6, "context": "If the function f is a normalised monotone submodular function, then the simple greedy algorithm provides a good approximation of the optimal solution [23, 22, 7] Several functions f can be found in the literature to perform data selection for ASR tasks, including facility location functions, saturated coverage functions [24, 8], diversity reward functions [5] or graph cut functions [7].", "startOffset": 323, "endOffset": 330}, {"referenceID": 3, "context": "If the function f is a normalised monotone submodular function, then the simple greedy algorithm provides a good approximation of the optimal solution [23, 22, 7] Several functions f can be found in the literature to perform data selection for ASR tasks, including facility location functions, saturated coverage functions [24, 8], diversity reward functions [5] or graph cut functions [7].", "startOffset": 359, "endOffset": 362}, {"referenceID": 5, "context": "If the function f is a normalised monotone submodular function, then the simple greedy algorithm provides a good approximation of the optimal solution [23, 22, 7] Several functions f can be found in the literature to perform data selection for ASR tasks, including facility location functions, saturated coverage functions [24, 8], diversity reward functions [5] or graph cut functions [7].", "startOffset": 386, "endOffset": 389}, {"referenceID": 19, "context": "One can define a modular function [22] based on the accumulated LRs of all utterances included in a subset S \u2286 \u03a9 in the following form:", "startOffset": 34, "endOffset": 38}, {"referenceID": 19, "context": "Modular functions are a special case of submodular functions [22] where the greater than or equal sign in Equation 1 changes to the equal sign.", "startOffset": 61, "endOffset": 65}, {"referenceID": 20, "context": "If a submodular function is non\u2013decreasing and normalised (f(\u2205) = 0), then the greedy solution obtained by Equation 3 is no worse than the optimal value by a constant fraction (1\u2212 1/e) [23].", "startOffset": 185, "endOffset": 189}, {"referenceID": 22, "context": "\u2022 Telephone speech (CT): From the Fisher corpus [25].", "startOffset": 48, "endOffset": 52}, {"referenceID": 23, "context": "\u2022 Meetings (MT): From AMI [26] and ICSI [27] corpora.", "startOffset": 26, "endOffset": 30}, {"referenceID": 24, "context": "\u2022 Meetings (MT): From AMI [26] and ICSI [27] corpora.", "startOffset": 40, "endOffset": 44}, {"referenceID": 25, "context": "\u2022 Lectures (TK): From TedTalks [28].", "startOffset": 31, "endOffset": 35}, {"referenceID": 26, "context": "\u2022 Read speech (RS): From the WSJCAM0 corpus [29].", "startOffset": 44, "endOffset": 48}, {"referenceID": 27, "context": "DNN training was performed with the TNet toolkit [30] and more details can be found at [31].", "startOffset": 49, "endOffset": 53}, {"referenceID": 28, "context": "DNN training was performed with the TNet toolkit [30] and more details can be found at [31].", "startOffset": 87, "endOffset": 91}, {"referenceID": 29, "context": "For both types of features, MLE\u2013based GMM\u2013 HMM models were trained using HTK [32] with 5\u2013state crossword triphones and 16 gaussians per state.", "startOffset": 77, "endOffset": 81}, {"referenceID": 30, "context": "The use of DNN front\u2013ends provides a 25% relative improvement in performance against PLP features; which is consistent across domains and follows results previously seen in the literature [33].", "startOffset": 188, "endOffset": 192}, {"referenceID": 16, "context": "Previous works have shown that data selection techniques can result in data sets biased towards specific groups of phones or triphones [19].", "startOffset": 135, "endOffset": 139}, {"referenceID": 31, "context": "Further work should also investigate data selection techniques for datasets larger than the one studied here, and in completely mismatched conditions and using different features that better describe the background\u2019s acoustic characteristics [34].", "startOffset": 242, "endOffset": 246}], "year": 2015, "abstractText": "Negative transfer in training of acoustic models for automatic speech recognition has been reported in several contexts such as domain change or speaker characteristics. This paper proposes a novel technique to overcome negative transfer by efficient selection of speech data for acoustic model training. Here data is chosen on relevance for a specific target. A submodular function based on likelihood ratios is used to determine how acoustically similar each training utterance is to a target test set. The approach is evaluated on a wide\u2013domain data set, covering speech from radio and TV broadcasts, telephone conversations, meetings, lectures and read speech. Experiments demonstrate that the proposed technique both finds relevant data and limits negative transfer. Results on a 6\u2013hour test set show a relative improvement of 4% with data selection over using all data in PLP based models, and 2% with DNN features.", "creator": "LaTeX with hyperref package"}}}