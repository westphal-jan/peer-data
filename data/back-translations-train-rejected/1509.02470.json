{"id": "1509.02470", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "8-Sep-2015", "title": "Deep Attributes from Context-Aware Regional Neural Codes", "abstract": "Recently, many researches employ middle-layer output of convolutional neural network models (CNN) as features for different visual recognition tasks. Although promising results have been achieved in some empirical studies, such type of representations still suffer from the well-known issue of semantic gap. This paper proposes so-called deep attribute framework to alleviate this issue from three aspects. First, we introduce object region proposals as intermedia to represent target images, and extract features from region proposals. Second, we study aggregating features from different CNN layers for all region proposals. The aggregation yields a holistic yet compact representation of input images. Results show that cross-region max-pooling of soft-max layer output outperform all other layers. As soft-max layer directly corresponds to semantic concepts, this representation is named \"deep attributes\". Third, we observe that only a small portion of generated regions by object proposals algorithm are correlated to classification target. Therefore, we introduce context-aware region refining algorithm to pick out contextual regions and build context-aware classifiers.", "histories": [["v1", "Tue, 8 Sep 2015 17:53:54 GMT  (1789kb,D)", "http://arxiv.org/abs/1509.02470v1", "10 pages, 8 figures"]], "COMMENTS": "10 pages, 8 figures", "reviews": [], "SUBJECTS": "cs.CV cs.LG cs.NE", "authors": ["jianwei luo", "jianguo li", "jun wang", "zhiguo jiang", "yurong chen"], "accepted": false, "id": "1509.02470"}, "pdf": {"name": "1509.02470.pdf", "metadata": {"source": "CRF", "title": "Deep Attributes from Context-Aware Regional Neural Codes", "authors": ["Jianwei Luo", "Jianguo Li", "Jun Wang", "Zhiguo Jiang", "Yurong Chen"], "emails": [], "sections": [{"heading": null, "text": "We apply the proposed Deep Attributes Framework to various visual tasks. Extensive experiments are conducted using standard benchmarks for three visual recognition tasks, i.e. image classification, fine-grained recognition and visual instance search. The results show that Deep Attributes approaches achieve state-of-the-art results and significantly exceed existing peer methods, although some benchmarks have little overlap with pre-trained CNN models."}, {"heading": "1. Introduction", "text": "In fact, most people who are able to identify themselves, identify themselves and understand, identify themselves, identify and identify, identify and identify, identify and identify, identify and identify, identify and identify, identify and identify, identify and identify, identify, identify, identify and identify. In fact, it is the case that most of them are people who are able to identify and identify themselves. In fact, it is the case that they are people who are able to identify themselves."}, {"heading": "2. Related Works", "text": "In this section, we will briefly refer to related works from the following four aspects:. Since the breakthrough of CNN models on ImageNet Large Scale Recognition Challenge (ILSVRC) 2012 [20], the use of CNN models has become popular in the computer vision community. [25] The performance of CNN features is evaluated on several levels, including object recognition, object fine-tuning and image retrieval. In addition, CNN features work surprisingly well in image division."}, {"heading": "3. Deep Attribute with Regional Neural Codes", "text": "The proposed method of deep attributes consists of the following four steps. (1) Extraction of regional suggestions: We use advanced techniques such as selective search [31] or marginal boxes [35] to extract semantic regions, as both perform satisfactorily in benchmarks [16]. (2) Calculation of neural codes for regional suggestions: We use CNN models trained from 1000 categories of ILSVRC 2012 to achieve 1000-dimensional semantic results for each extracted region. Such computed neural codes serve as semantic input for the next level of pooling. (3) Cross-region pooling: We perform pooling per dimension between extracted regions to obtain a 1000-dimensional holistic representation. Different pooling layout schemes are applicable for further possible performance improvements. (4) Context-sistent region visualization and classification depth can be trained via classification builder."}, {"heading": "3.1. Cross Region Pooling (CRP)", "text": "Since the CNN models are based on a scale of 1000 people, they are extracted by region recognition algorithms such as [31, 35], then the regions are wrapped and fed into CNN models. Each region Ri is then represented by the output of soft-max layers such as Fi = (fi1, \u00b7 \u00b7, fiK), where fik is the k-th dimensional neural code of Fi. The calculated neural codes of all regions are then aggregated with a pooling operation to construct a holistic representation of the input image I. The pooling schemes could be either maxpooling or average-pooling. In our practice, we find maxpooling works better than the average pooling schematization across all region suggestions. Thus, the final code for the k-th dimension f-k is obtained as a pyrax scheme."}, {"heading": "3.2. Context Aware Region Refining (CARR)", "text": "For a particular classification task, we observed that extracted regions could be divided into three categories. First, some regions are directly related to the classification target. Second, some regions can be considered useful context information, while the auto region is certainly background noise. To improve accuracy, we should make maximum use of the context information while suppressing the clutter information. Note that the background clutter regions are category-specific. Again, in Figure 3, for auto-classification, the horse region is background noise. This inspires us to select category-specific contexts."}, {"heading": "4. Implementation and Performance Study", "text": "Through this section, we study the performance on PASCAL VOC 2007 Image Classification Benchmark with Standard Protocol. PASCAL VOC [10] is a very demanding benchmark for object recognition. It contains images of 20 categories, including animals, handmade objects and natural objects. The objects are located at different locations and scale algorithm 2: Prediction with CARR Classifier Data: Input test image I and regional suggestions {Rk} for each classifier Hc (x) dofor t = 1: T do for each region Rk doCompute Score Sck = H c t (Fk); End Sck in descending order; Select Top K regions from all N regions; Cross region pooling on Top K regions; Get new representation FI; Predict Image Score with ScI = H c t (FI); End Sort Sck in descending order; Select Top K regions from all N regions; Select Top Region Pooling on Top K regions from all N regions; Do not split the top regions from all N regions;"}, {"heading": "4.1. CNN Models: Alex\u2019s vs VGG\u2019s", "text": "We first compare two CNN models: Alex's network [20] and VGG's network [28]. There are several different VGG CNN models available, where we used the 16-layer VGG model. In this study, we only apply the single-scale crossproposal max pooling without context refinement. We use selective search as algorithms to create regional proposals. Alex's network reaches 80.8% mAP on 20 categories of VOC 2007, while VGG's network reaches 85.6% mAP. It is obvious that VGG's network outperforms Alex's network by a large margin, which is consistent with the factor that VGG's network performs better than Alex's network on ImageNet. Therefore, we adopt VGG's network in all remaining studies."}, {"heading": "4.2. Selective Search vs Edge Box", "text": "In Figure 5 (a), the retrieval rate in relation to the number of top regions on PASCAL VOC 2007 is represented by both the selective search and the edge field. This diagram is generated as follows: Since the dataset of PASCAL VOC 2007 contains annotations on object delimitation fields, we use selective search or edge fields to generate a series of regional suggestions and sort them by regional values. We then select TopK regions and calculate the overlap rate (IoU) using top K regions with the basic truth. A region is counted as a reminder if the IoU is greater than 0.5. Changing the value of K resulted in the diagram. From this diagram, we can see that the edge field works slightly better than the selective search."}, {"heading": "4.3. Different Pooling Schemes", "text": "We also compare different pooling schemes. First, we compare pooling layout schemes as part of cross-proposal maxipooling. Single-scale does cross-region pooling (CRP) over the entire image. Spatial pyramid pooling splits the entire image into 1x1 grid, 2x2 grid plus a center grid. It leads to CRP in each grid and then concatenates features from each grid. Multi-scale pooling results in CRP in five different regional scales according to the ratio of region size to image size. Characteristics from different scales are combined into a holistic representation. In this study, we adopt VGG net with max pooling. Experiments show that single-scale reaches 86.1% mAP, spatial pyramid-pooling reaches 87.2% mAP, and multi-scale pooling 89.7%."}, {"heading": "4.4. Different CNN Layers", "text": "In this study, we compare 5 different CNN layers (pool5, fc1, fc2, fc3, soft-max) with respect to their performance in cross-region pooling. Figure 4 illustrates the results both in individual cases and in multiple cases. We can conclude that (1) in multiple cases, maximum pooling is consistently better than average pooling on different layers, while the DA (Soft-Max) layer reaches 89.7% MAP, which outperforms all other layers in both maximum and average pooling. (2) In individual cases, maximum pooling outperforms average pooling on all layers except the Soft-Max layer. Soft-Max layer with average pooling achieves the best performance over all other layers in this case."}, {"heading": "4.5. How Many Regions Are Required?", "text": "This study will answer these two questions: We use Edge-Box to create regional proposals; we select Top-K regions by their regional value for multi-scale cross-region-pooling; for different K-values, we get the accuracy on VOC 2007; the results are shown in Figure 5 (b); it shows that more regions are better, and if the number of regions exceeds 500, the accuracy is saturated; that means we can only feed Top-500 regions to CNN instead of all 1500 + regions, which can save a lot of computing costs."}, {"heading": "4.6. Parameters in CARR", "text": "In context-sensitive region refinement, we need to determine the parameter K and the number of iterations. Instead of directly specifying K, we define the context region ratio \u03b2 = KN, where N is the total number of extracted region suggestions. Figure 6 illustrates the MAP curves on VOC 2007 with different \u03b2 and different iteration number. It shows that \u03b2 = 0.025 gives the best results. As the number of total regions N is usually greater than 1500, this means that for each image and category, normally, about 40 context regions are selected. Furthermore, we observed that only one iteration of context region refinement yields 3.6% improvement in accuracy for single-scale deep attributes (from 86.1% to 89.7%), while it yields 1.1% improvement in accuracy for the multi-scale case (from 89.7% to 90.8%). Further iterations do not yield any numerical advantages, so we set Iteration value = 0.2 for all remaining experiments."}, {"heading": "4.7. How About using Other Layers in Refining?", "text": "In Section 4.4, we have shown that the Soft Max Layer (DA) is the best layer for cross-region bundling, so we have fixed the first step (the global CRP step) with DA characteristics. In Section 4.6, we have included DA characteristics in the finishing stage of the region. In this section, we will examine other CNN layers in the finishing stage. We have compared DA + DA (also known as DA2) with DA + Pool5, DA + FC1, DA + FC2, DA + FC3 in the VOC 2007, in which the first DA indicates the global CRP step, while the second point (after \"+\" characters) indicates the layer used in the finishing stage of the region. We will also examine the effects of max pooling and average pooling in the finishing stage. Figure 7 illustrates the comparison results. It is obvious that average pooling works better than maxpooling in the finishing stage of the region. We will also examine the effects of max pooling and average pooling in the finishing stage. Figure 7 illustrates the comparison results. It is clear that average pooling works better than maxpooling in the region of the finishing stage of the region."}, {"heading": "5. Experimental Results", "text": "We thoroughly evaluate the proposed approach based on three vision tasks, i.e. image classification, fine-grained object detection, and visual instance evaluation. According to the previous study, we defined parameters of the proposed framework as a VGG-16 CNN model, edge box for regional extraction, multi-scale pooling layout, and a CARR step with \u03b2 = 0.025."}, {"heading": "5.1. Image Classification Task", "text": "First, we report on the mean average precision (MAP) for the PASCAL VOC 2007 dataset with results per category. Table 1 shows the results per category compared to the state-of-the-art methods w.r.t for each category on VOC 2007, while Table 2 shows the results per category on VOC 2012. Note that we have listed one of the results according to DA + FC1, which is trained on a combination of VOC 2007 and 2012. We have listed CNN-related methods for their training configurations on two benchmarks in Table 3. We can see that our method is relatively simple, without fine-tuning and data augmentation, and by far surpasses current state-of-the-art methods. Two methods are noticeable here: First, the very deep CNN method (also known as VGG-16-19 fusion)."}, {"heading": "5.2. Fine-Grained Recognition Task", "text": "The fine-grained recognition task is evaluated using the Oxford Flower Data Set, which contains 102 categories of flowers. Each category contains 40 to 258 images. The flowers appear in different scales, poses and lighting conditions. The data set provides segmentation for all images. However, we do not use this information in our experiment. Our evaluation follows the standard protocol of this benchmark. We report on the average accuracy of the Oxford 102 Flower Data Set in Table 4. DA2 achieves an accuracy of 90.1%, which is significantly higher than all existing methods. When replacing the grafting level DA function with FC1, it reached an accuracy of 95.1% (DA + FC1). Note that these results are obtained without using segmentation information, fine-tuning CNN networks. Interestingly, the flower data set hardly overlaps with the presented CNN models of the 1000 category."}, {"heading": "5.3. Visual Instance Retrieval Task", "text": "The task of the visual instance query is evaluated on the basis of two sets of data: (1) Holidays [17] consists of 1491 holiday photos divided into 500 groups based on the same object or scene. An image is selected from each group and used as a query image. This data set is challenging due to variations in view and scale. (2) The data set from the University of Kentucky Benchmark [24] (UKB) includes 10,200 indoor images taken uniformly from 2550 objects, and each image is used to query the rest. It is difficult due to variations in the viewing angle. In this experiment, we use cosmic similarity as a measurement metric. We specify the mean average precision (mAP) for the Holidays dataset, and the accuracy of the top 4 query results for the UKB dataset according to the standard evaluation protocol for these two datasets. Note that this task is not intended to require the image query."}, {"heading": "6. Conclusions and Discussions", "text": "In fact, most people who are able to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to"}], "references": [{"title": "Efficient object detection and segmentation for fine-grained recognition", "author": ["A. Angelova", "S. Zhu"], "venue": "CVPR, pages 811\u2013 818", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2013}, {"title": "Three things everyone should know to improve object retrieval", "author": ["R. Arandjelovi\u0107", "A. Zisserman"], "venue": "CVPR", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2012}, {"title": "From contours to regions: An empirical evaluation", "author": ["P. Arbelaez", "M. Maire", "C. Fowlkes", "J. Malik"], "venue": "CVPR, pages 2294\u20132301", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2009}, {"title": "and et al", "author": ["P. Arbelaez", "J. Pont-Tuset"], "venue": "Multiscale combinatorial grouping. In CVPR", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2014}, {"title": "Neural codes for image retrieval", "author": ["A. Babenko", "A. Slesarev", "A. Chigorin", "V. Lempitsky"], "venue": "arXiv:1404.1777", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2014}, {"title": "Return of the devil in the details: Delving deep into convolutional nets", "author": ["K. Chatfield", "K. Simonyan", "A. Vedaldi", "A. Zisserman"], "venue": "BMVC", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2014}, {"title": "Bing: Binarized normed gradients for objectness estimation at 300fps", "author": ["M.-M. Cheng", "Z. Zhang", "W.-Y. Lin", "P. Torr"], "venue": "CVPR", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2014}, {"title": "Imagenet: A large-scale hierarchical image database", "author": ["J. Deng", "W. Dong", "R. Socher", "L.-J. Li", "K. Li", "L. Fei- Fei"], "venue": "CVPR, pages 248\u2013255", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2009}, {"title": "and et al", "author": ["J. Donahue", "Y. Jia", "O. Vinyals"], "venue": "Decaf: A deep convolutional activation feature for generic visual recognition. In ICML", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2014}, {"title": "The pascal visual object classes (voc) challenge", "author": ["M. Everingham", "L. Van Gool", "C.K. Williams", "J. Winn", "A. Zisserman"], "venue": "IJCV, 88(2):303\u2013338", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2010}, {"title": "Describing objects by their attributes", "author": ["A. Farhadi", "I. Endres", "D. Hoiem", "D. Forsyth"], "venue": "CVPR, pages 1778\u20131785", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2009}, {"title": "Rich feature hierarchies for accurate object detection and semantic segmentation", "author": ["R. Girshick", "J. Donahue", "T. Darrell", "J. Malik"], "venue": "arXiv:1311.2524", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2013}, {"title": "Multi-scale orderless pooling of deep convolutional activation features", "author": ["Y. Gong", "L. Wang", "R. Guo", "S. Lazebnik"], "venue": "arXiv:1403.1840", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2014}, {"title": "Recognition using regions", "author": ["C. Gu", "J.J. Lim", "P. Arbelaez", "J. Malik"], "venue": "CVPR, pages 1030\u20131037", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2009}, {"title": "Combining efficient object localization and image classification", "author": ["H. Harzallah", "F. Jurie", "C. Schmid"], "venue": "CVPR, pages 237\u2013244", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2009}, {"title": "How good are detection proposals", "author": ["J. Hosang", "R. Benenson", "B. Schiele"], "venue": "really? BMVC", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2014}, {"title": "Hamming embedding and weak geometric consistency for large scale image search", "author": ["H. Jegou", "M. Douze", "C. Schmid"], "venue": "In ECCV,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2008}, {"title": "Aggregating local descriptors into a compact image representation", "author": ["H. J\u00e9gou", "M. Douze", "C. Schmid", "P. P\u00e9rez"], "venue": "In CVPR,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2010}, {"title": "Caffe: Convolutional architecture for fast feature embedding", "author": ["Y. Jia", "E. Shelhamer", "J. Donahue", "S. Karayev", "J. Long", "R. Girshick", "S. Guadarrama", "T. Darrell"], "venue": "arXiv:1408.5093", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2014}, {"title": "Imagenet classification with deep convolutional neural networks", "author": ["A. Krizhevsky", "I. Sutskever", "G.E. Hinton"], "venue": "NIPS, pages 1097\u20131105", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2012}, {"title": "Attribute and simile classifiers for face verification", "author": ["N. Kumar", "A.C. Berg", "P.N. Belhumeur", "S.K. Nayar"], "venue": "CVPR, pages 365\u2013372", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2009}, {"title": "Beyond bags of features: Spatial pyramid matching for recognizing natural scene categories", "author": ["S. Lazebnik", "C. Schmid", "J. Ponce"], "venue": "CVPR, volume 2, pages 2169\u20132178", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2006}, {"title": "Attributes make sense on segmented objects", "author": ["Z. Li", "E. Gavves", "T. Mensink", "C.G. Snoek"], "venue": "In ECCV,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2014}, {"title": "Scalable recognition with a vocabulary tree", "author": ["D. Nister", "H. Stewenius"], "venue": "CVPR, volume 2, pages 2161\u20132168", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2006}, {"title": "Cnn features off-the-shelf: an astounding baseline for recognition", "author": ["A.S. Razavian", "H. Azizpour", "J. Sullivan", "S. Carlsson"], "venue": "arXiv:1403.6382", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2014}, {"title": "ImageNet Large Scale Visual Recognition Challenge", "author": ["O. Russakovsky", "J. Deng", "H. Su", "J. Krause", "S. Satheesh", "S. Ma", "Z. Huang", "A. Karpathy", "A. Khosla", "M. Bernstein", "A.C. Berg", "L. Fei-Fei"], "venue": "CoRR, abs/1409.0575", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2014}, {"title": "Overfeat: Integrated recognition", "author": ["P. Sermanet", "D. Eigen", "X. Zhang", "M. Mathieu", "R. Fergus", "Y. LeCun"], "venue": "localization and detection using convolutional networks. In International Conference on Learning Representations", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2014}, {"title": "Very deep convolutional networks for large-scale image recognition", "author": ["K. Simonyan", "A. Zisserman"], "venue": "CoRR, abs/1409.1556", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2014}, {"title": "Video google: A text retrieval approach to object matching in videos", "author": ["J. Sivic", "A. Zisserman"], "venue": "CVPR, pages 1470\u2013 1477. IEEE", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2003}, {"title": "Content-based image retrieval at the end of the early years", "author": ["A.W. Smeulders", "M. Worring", "S. Santini", "A. Gupta", "R. Jain"], "venue": "Pattern Analysis and Machine Intelligence, IEEE Transactions on, 22(12):1349\u20131380", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2000}, {"title": "K", "author": ["J.R. Uijlings"], "venue": "E. van de Sande, T. Gevers, and A. W. Smeulders. Selective search for object recognition. IJCV, 104(2):154\u2013171", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2013}, {"title": "CNN: single-label to multi-label", "author": ["Y. Wei", "W. Xia", "J. Huang", "B. Ni", "J. Dong", "Y. Zhao", "S. Yan"], "venue": "CoRR, abs/1406.5726", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2014}, {"title": "and J", "author": ["H. Yang", "J.T. Zhou", "Y. Zhang", "B.-B. Gao", "J. Wu"], "venue": "Cai. Can partial strong labels boost multi-label object recognition? arXiv preprint arXiv:1504.05843", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2015}, {"title": "Linear spatial pyramid matching using sparse coding for image classification", "author": ["J. Yang", "K. Yu", "Y. Gong", "T. Huang"], "venue": "CVPR, pages 1794\u20131801", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2009}, {"title": "Edge boxes: Locating object proposals from edges", "author": ["C.L. Zitnick", "P. Dollar"], "venue": "ECCV", "citeRegEx": "35", "shortCiteRegEx": null, "year": 2014}], "referenceMentions": [{"referenceID": 19, "context": "[20] on ImageNet [8], researches on convolutional neural networks (CNN) have been exploding.", "startOffset": 0, "endOffset": 4}, {"referenceID": 7, "context": "[20] on ImageNet [8], researches on convolutional neural networks (CNN) have been exploding.", "startOffset": 17, "endOffset": 20}, {"referenceID": 11, "context": "researches adopt pre-trained CNN models as feature extractor for various visual recognition tasks like object detection [12], object recognition [9, 25, 6], image retrieval [13, 25], etc.", "startOffset": 120, "endOffset": 124}, {"referenceID": 8, "context": "researches adopt pre-trained CNN models as feature extractor for various visual recognition tasks like object detection [12], object recognition [9, 25, 6], image retrieval [13, 25], etc.", "startOffset": 145, "endOffset": 155}, {"referenceID": 24, "context": "researches adopt pre-trained CNN models as feature extractor for various visual recognition tasks like object detection [12], object recognition [9, 25, 6], image retrieval [13, 25], etc.", "startOffset": 145, "endOffset": 155}, {"referenceID": 5, "context": "researches adopt pre-trained CNN models as feature extractor for various visual recognition tasks like object detection [12], object recognition [9, 25, 6], image retrieval [13, 25], etc.", "startOffset": 145, "endOffset": 155}, {"referenceID": 12, "context": "researches adopt pre-trained CNN models as feature extractor for various visual recognition tasks like object detection [12], object recognition [9, 25, 6], image retrieval [13, 25], etc.", "startOffset": 173, "endOffset": 181}, {"referenceID": 24, "context": "researches adopt pre-trained CNN models as feature extractor for various visual recognition tasks like object detection [12], object recognition [9, 25, 6], image retrieval [13, 25], etc.", "startOffset": 173, "endOffset": 181}, {"referenceID": 28, "context": "These developed techniques have shown promising results in comparison to conventional methods using standard feature representations like bag-of-words [29], sparse-coding [34], etc.", "startOffset": 151, "endOffset": 155}, {"referenceID": 33, "context": "These developed techniques have shown promising results in comparison to conventional methods using standard feature representations like bag-of-words [29], sparse-coding [34], etc.", "startOffset": 171, "endOffset": 175}, {"referenceID": 29, "context": "These two are summarized to be the well-known semantic gap [30].", "startOffset": 59, "endOffset": 63}, {"referenceID": 13, "context": "[14] employ mid-level features like contour shape, edge shape, color and texture to describe each region for visual recognition tasks.", "startOffset": 0, "endOffset": 4}, {"referenceID": 10, "context": "It is well known that region features can naturally preserve more mid-level semantic information like materials, textures, shapes, etc of objects [11].", "startOffset": 146, "endOffset": 150}, {"referenceID": 2, "context": "However, traditional region representations either highly depend on segmentation algorithm [3, 23], or lack of a generic semantic representation for regions for various visual recognition tasks.", "startOffset": 91, "endOffset": 98}, {"referenceID": 22, "context": "However, traditional region representations either highly depend on segmentation algorithm [3, 23], or lack of a generic semantic representation for regions for various visual recognition tasks.", "startOffset": 91, "endOffset": 98}, {"referenceID": 30, "context": "(1) We introduce region proposals using algorithm like selective search [31] or edge-box [35] from each input image.", "startOffset": 72, "endOffset": 76}, {"referenceID": 34, "context": "(1) We introduce region proposals using algorithm like selective search [31] or edge-box [35] from each input image.", "startOffset": 89, "endOffset": 93}, {"referenceID": 19, "context": "Since the breakthrough success of CNN models on ImageNet Large Scale Visual Recognition Challenge (ILSVRC) 2012 [20], employing CNN models to other vision tasks becomes popular in the computer vision community.", "startOffset": 112, "endOffset": 116}, {"referenceID": 24, "context": "[25] evaluate the performance of CNN features on several vision tasks, including object recognition, fine-grained object recognition, and image retrieval.", "startOffset": 0, "endOffset": 4}, {"referenceID": 8, "context": "Meanwhile, DeCAF [9] also shows that CNN features work surprisingly well on image classification.", "startOffset": 17, "endOffset": 20}, {"referenceID": 4, "context": "Subsequently, Babenko et al [5] present a similar idea on im-", "startOffset": 28, "endOffset": 31}, {"referenceID": 20, "context": "[21] consider the labels of reference faces and facecomponents as attributes to describe other faces.", "startOffset": 0, "endOffset": 4}, {"referenceID": 10, "context": "[11] describe objects using 64 explicitly semantic attribute classifiers.", "startOffset": 0, "endOffset": 4}, {"referenceID": 21, "context": "As one of the most well known work, spatial pyramid matching performs pooling over pyramid of regular grids [22, 34].", "startOffset": 108, "endOffset": 116}, {"referenceID": 33, "context": "As one of the most well known work, spatial pyramid matching performs pooling over pyramid of regular grids [22, 34].", "startOffset": 108, "endOffset": 116}, {"referenceID": 12, "context": "[13] encodes the activations of CNN fully connected layer by VLAD [18], and then concatenates the encoded features over windows at three scale levels.", "startOffset": 0, "endOffset": 4}, {"referenceID": 17, "context": "[13] encodes the activations of CNN fully connected layer by VLAD [18], and then concatenates the encoded features over windows at three scale levels.", "startOffset": 66, "endOffset": 70}, {"referenceID": 26, "context": "On the contrary, decision-level cross-region pooling has been applied when there are multiple region/patch candidates [27, 32].", "startOffset": 118, "endOffset": 126}, {"referenceID": 31, "context": "On the contrary, decision-level cross-region pooling has been applied when there are multiple region/patch candidates [27, 32].", "startOffset": 118, "endOffset": 126}, {"referenceID": 11, "context": "Region proposals: Methods for detecting region proposal are used in object detection to avoid exhaustive sliding window search across images and speed up the detection without noticeable loss of recall rates [12].", "startOffset": 208, "endOffset": 212}, {"referenceID": 30, "context": "In the past few years, there have been extensive studies on this topic and many techniques are invented, including selective search [31], edge-boxes [35], BING [7], multiscale combinatorial grouping (MCG) [4], and so on.", "startOffset": 132, "endOffset": 136}, {"referenceID": 34, "context": "In the past few years, there have been extensive studies on this topic and many techniques are invented, including selective search [31], edge-boxes [35], BING [7], multiscale combinatorial grouping (MCG) [4], and so on.", "startOffset": 149, "endOffset": 153}, {"referenceID": 6, "context": "In the past few years, there have been extensive studies on this topic and many techniques are invented, including selective search [31], edge-boxes [35], BING [7], multiscale combinatorial grouping (MCG) [4], and so on.", "startOffset": 160, "endOffset": 163}, {"referenceID": 3, "context": "In the past few years, there have been extensive studies on this topic and many techniques are invented, including selective search [31], edge-boxes [35], BING [7], multiscale combinatorial grouping (MCG) [4], and so on.", "startOffset": 205, "endOffset": 208}, {"referenceID": 15, "context": "[16] evaluates ten region proposal methods, in which selective search and edge-boxes achieved consistently better performance in terms of ground truth recall, repeatability, and detection speed.", "startOffset": 0, "endOffset": 4}, {"referenceID": 30, "context": "(1) Region proposals extraction: We use advanced techniques like selective search [31] or edge-boxes [35] to extract semantical regions, as both of which show satisfactory performance in benchmarks [16].", "startOffset": 82, "endOffset": 86}, {"referenceID": 34, "context": "(1) Region proposals extraction: We use advanced techniques like selective search [31] or edge-boxes [35] to extract semantical regions, as both of which show satisfactory performance in benchmarks [16].", "startOffset": 101, "endOffset": 105}, {"referenceID": 15, "context": "(1) Region proposals extraction: We use advanced techniques like selective search [31] or edge-boxes [35] to extract semantical regions, as both of which show satisfactory performance in benchmarks [16].", "startOffset": 198, "endOffset": 202}, {"referenceID": 30, "context": ", RN} are extracted by region detection algorithms like [31, 35].", "startOffset": 56, "endOffset": 64}, {"referenceID": 34, "context": ", RN} are extracted by region detection algorithms like [31, 35].", "startOffset": 56, "endOffset": 64}, {"referenceID": 15, "context": "Region proposal algorithms usually produce thousands of regions for input images to ensure high recall rate [16].", "startOffset": 108, "endOffset": 112}, {"referenceID": 9, "context": "PASCAL VOC [10] is a very challenging benchmark for object recognition.", "startOffset": 11, "endOffset": 15}, {"referenceID": 25, "context": "In this paper, we adopt the CNN models trained on ImageNet dataset [26], which contains 1.", "startOffset": 67, "endOffset": 71}, {"referenceID": 18, "context": "We make experiments based on the Caffe deep learning framework [19].", "startOffset": 63, "endOffset": 67}, {"referenceID": 1, "context": "After deep attribute feature is extracted, we process the feature with a RootSIFT trick normalization as in [2].", "startOffset": 108, "endOffset": 111}, {"referenceID": 19, "context": "We first compare two CNN models: Alex\u2019s net [20] vs VGG\u2019s net [28].", "startOffset": 44, "endOffset": 48}, {"referenceID": 27, "context": "We first compare two CNN models: Alex\u2019s net [20] vs VGG\u2019s net [28].", "startOffset": 62, "endOffset": 66}, {"referenceID": 15, "context": "We then compare two best region proposal generation algorithms according to [16], i.", "startOffset": 76, "endOffset": 80}, {"referenceID": 30, "context": ", selective search [31] and edge-box [35].", "startOffset": 19, "endOffset": 23}, {"referenceID": 34, "context": ", selective search [31] and edge-box [35].", "startOffset": 37, "endOffset": 41}, {"referenceID": 14, "context": "INRIA [15] 77.", "startOffset": 6, "endOffset": 10}, {"referenceID": 5, "context": "5 CNN S\u2217 [6] 95.", "startOffset": 9, "endOffset": 12}, {"referenceID": 24, "context": "4 CNNaug-SVM [25] 90.", "startOffset": 13, "endOffset": 17}, {"referenceID": 31, "context": "2 HCP-1000C\u2217 [32] 95.", "startOffset": 13, "endOffset": 17}, {"referenceID": 31, "context": "5 HCP-2000C\u2217 [32] 96.", "startOffset": 13, "endOffset": 17}, {"referenceID": 27, "context": "2 VGG-16-19-Fusion [28] 98.", "startOffset": 19, "endOffset": 23}, {"referenceID": 32, "context": "7 FV+LV-20-VD [33] 97.", "startOffset": 14, "endOffset": 18}, {"referenceID": 31, "context": "HCP-1000C [32] 97.", "startOffset": 10, "endOffset": 14}, {"referenceID": 31, "context": "7 HCP-2000C [32] 97.", "startOffset": 12, "endOffset": 16}, {"referenceID": 27, "context": "2 VGG-16 [28] 99.", "startOffset": 9, "endOffset": 13}, {"referenceID": 27, "context": "0 VGG-16-19-Fusion [28] 99.", "startOffset": 19, "endOffset": 23}, {"referenceID": 32, "context": "3 FV+LV-20-VD [33] 98.", "startOffset": 14, "endOffset": 18}, {"referenceID": 32, "context": "Second is the multiview multi-instance framework by [33] (FV+LV-20-VD), which takes FC layers output from region proposals as feature view and ground truth bounding box as label view, and combine them under a Fisher-vector framework.", "startOffset": 52, "endOffset": 56}, {"referenceID": 24, "context": "CNNaug-SVM [25] OverFeat [27] No Yes FC 77.", "startOffset": 11, "endOffset": 15}, {"referenceID": 26, "context": "CNNaug-SVM [25] OverFeat [27] No Yes FC 77.", "startOffset": 25, "endOffset": 29}, {"referenceID": 5, "context": "2 NA CNN S [6] CNN-S [6] Yes Yes FC 82.", "startOffset": 11, "endOffset": 14}, {"referenceID": 5, "context": "2 NA CNN S [6] CNN-S [6] Yes Yes FC 82.", "startOffset": 21, "endOffset": 24}, {"referenceID": 31, "context": "2 HCP-1000C [32] Alex\u2019s [20] Yes No FC 81.", "startOffset": 12, "endOffset": 16}, {"referenceID": 19, "context": "2 HCP-1000C [32] Alex\u2019s [20] Yes No FC 81.", "startOffset": 24, "endOffset": 28}, {"referenceID": 31, "context": "7 HCP-2000C [32] Alex\u2019s [20] Yes Yes FC 85.", "startOffset": 12, "endOffset": 16}, {"referenceID": 19, "context": "7 HCP-2000C [32] Alex\u2019s [20] Yes Yes FC 85.", "startOffset": 24, "endOffset": 28}, {"referenceID": 27, "context": "2 VGG-16-19-Fusion [28] VGG-16+19 [28] No Yes FC 89.", "startOffset": 19, "endOffset": 23}, {"referenceID": 27, "context": "2 VGG-16-19-Fusion [28] VGG-16+19 [28] No Yes FC 89.", "startOffset": 34, "endOffset": 38}, {"referenceID": 32, "context": "3 FV+LV-20-VD [33] CNN-S/M [6] Yes Yes FC 90.", "startOffset": 14, "endOffset": 18}, {"referenceID": 5, "context": "3 FV+LV-20-VD [33] CNN-S/M [6] Yes Yes FC 90.", "startOffset": 27, "endOffset": 30}, {"referenceID": 27, "context": "7 DA VGG-16 [28] No No Soft-max 90.", "startOffset": 12, "endOffset": 16}, {"referenceID": 27, "context": "2 DA+FC1 VGG-16 [28] No No Soft-max+FC 91.", "startOffset": 16, "endOffset": 20}, {"referenceID": 27, "context": "2 DA+FC1\u2217 VGG-16 [28] No No Soft-max+FC 92.", "startOffset": 17, "endOffset": 21}, {"referenceID": 0, "context": "Dense HOG+Coding+Pooling w/o seg [1] 76.", "startOffset": 33, "endOffset": 36}, {"referenceID": 0, "context": "7 Seg+Dense HOG+Coding+Pooling [1] 80.", "startOffset": 31, "endOffset": 34}, {"referenceID": 24, "context": "7 CNN-SVM w/o seg [25] 74.", "startOffset": 18, "endOffset": 22}, {"referenceID": 24, "context": "7 CNNaug-SVM w/o seg [25] 86.", "startOffset": 21, "endOffset": 25}, {"referenceID": 16, "context": "The visual instance retrieval task is evaluated on two datasets: (1) Holidays [17] consists of 1491 vacation photographs distributed among 500 groups based on same object or scene.", "startOffset": 78, "endOffset": 82}, {"referenceID": 23, "context": "(2) University of Kentucky Benchmark dataset [24] (UKB) includes 10,200 indoor photographs uniformly from 2550 objects, and each image is used to query the rest.", "startOffset": 45, "endOffset": 49}, {"referenceID": 12, "context": "MOP-CNN [13] 80.", "startOffset": 8, "endOffset": 12}, {"referenceID": 4, "context": "2 Neural Codes [5] 74.", "startOffset": 15, "endOffset": 18}, {"referenceID": 4, "context": "8 Neural Codes+ retrain [5] 79.", "startOffset": 24, "endOffset": 27}, {"referenceID": 24, "context": "3 CNNaug-ss [25] 84.", "startOffset": 12, "endOffset": 16}], "year": 2015, "abstractText": "Recently, many researches employ middle-layer output of convolutional neural network models (CNN) as features for different visual recognition tasks. Although promising results have been achieved in some empirical studies, such type of representations still suffer from the well-known issue of semantic gap. This paper proposes so-called deep attribute framework to alleviate this issue from three aspects. First, we introduce object region proposals as intermedia to represent target images, and extract features from region proposals. Second, we study aggregating features from different CNN layers for all region proposals. The aggregation yields a holistic yet compact representation of input images. Results show that cross-region max-pooling of soft-max layer output outperform all other layers. As softmax layer directly corresponds to semantic concepts, this representation is named \u201cdeep attributes\u201d. Third, we observe that only a small portion of generated regions by object proposals algorithm are correlated to classification target. Therefore, we introduce context-aware region refining algorithm to pick out contextual regions and build contextaware classifiers. We apply the proposed deep attributes framework for various vision tasks. Extensive experiments are conducted on standard benchmarks for three visual recognition tasks, i.e., image classification, fine-grained recognition and visual instance retrieval. Results show that deep attribute approaches achieve state-of-the-art results, and outperforms existing peer methods with a significant margin, even though some benchmarks have little overlap of concepts with the pre-trained CNN models.", "creator": "LaTeX with hyperref package"}}}