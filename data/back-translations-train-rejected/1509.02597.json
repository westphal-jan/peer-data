{"id": "1509.02597", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "9-Sep-2015", "title": "Asynchronous Distributed ADMM for Large-Scale Optimization- Part I: Algorithm and Convergence Analysis", "abstract": "Aiming at solving large-scale learning problems, this paper studies distributed optimization methods based on the alternating direction method of multipliers (ADMM). By formulating the learning problem as a consensus problem, the ADMM can be used to solve the consensus problem in a fully parallel fashion over a computer network with a star topology. However, traditional synchronized computation does not scale well with the problem size, as the speed of the algorithm is limited by the slowest workers. This is particularly true in a heterogeneous network where the computing nodes experience different computation and communication delays. In this paper, we propose an asynchronous distributed ADMM (AD-AMM) which can effectively improve the time efficiency of distributed optimization. Our main interest lies in analyzing the convergence conditions of the AD-ADMM, under the popular partially asynchronous model, which is defined based on a maximum tolerable delay of the network. Specifically, by considering general and possibly non-convex cost functions, we show that the AD-ADMM is guaranteed to converge to the set of Karush-Kuhn-Tucker (KKT) points as long as the algorithm parameters are chosen appropriately according to the network delay. We further illustrate that the asynchrony of the ADMM has to be handled with care, as slightly modifying the implementation of the AD-ADMM can jeopardize the algorithm convergence, even under a standard convex setting.", "histories": [["v1", "Wed, 9 Sep 2015 01:45:08 GMT  (396kb)", "https://arxiv.org/abs/1509.02597v1", "28 pages, submitted for publication"], ["v2", "Fri, 19 Feb 2016 06:02:38 GMT  (399kb)", "http://arxiv.org/abs/1509.02597v2", "37 pages"]], "COMMENTS": "28 pages, submitted for publication", "reviews": [], "SUBJECTS": "cs.DC cs.LG cs.SY", "authors": ["tsung-hui chang", "mingyi hong", "wei-cheng liao", "xiangfeng wang"], "accepted": false, "id": "1509.02597"}, "pdf": {"name": "1509.02597.pdf", "metadata": {"source": "CRF", "title": "Asynchronous Distributed ADMM for Large-Scale Optimization- Part I: Algorithm and Convergence Analysis", "authors": ["Tsung-Hui Chang", "Mingyi Hong", "Wei-Cheng Liao", "Xiangfeng Wang"], "emails": ["tsunghui.chang@ieee.org.", "mingyi@iastate.edu", "liaox146@umn.edu", "xfwang@sei.ecnu.edu.cn"], "sections": [{"heading": null, "text": "ar Xiv: 150 9.02 597v 2 [cs.D C] February 19 20Keywords \u2212 Distributed optimization, ADMM, Asynchronous, Consensus optimizationPart of this work was submitted to IEEE ICASSP, Shanghai, China, March 20-25, 2016 [1]. Tsung-Hui Chang is supported by NSFC, China, grant number 61571385. Mingyi Hong is supported by NFS grant number CCF-1526078 and AFOSR grant number 15RT0767. Xiangfeng Wang is supported by Shanghai YangFan number 15YF1403400 and NSFC number 11501210."}, {"heading": "A. Background", "text": "In fact, most of them will be able to move to a different world, in which they will be able to move to a different world than they are able to move to a different world."}, {"heading": "B. Related Works", "text": "Another approach to distributed and parallel optimization is based on the method of alternative alignment of multipliers (ADMM) [9, Section 7.1.1]. In distributed ADMM, the original learning problem is divided into N sub-problems, each containing a subset of learning samples or learning parameters. In each iteration, the workers solve the sub-problems and send the most up-to-date information to the master, who summarizes this information and passes the result on to the workers. In this way, a major learning problem can be solved in a parallel and distributed way."}, {"heading": "C. Contributions", "text": "In this paper, we generalize the problems with the synchronously distributed ADMM [9], [18] with the asynchronous setting. Like [10] - [19], [20], the asynchronously distributed ADMM algorithm developed in this paper, the freedom to update exists only on the basis of a number of workers who further improve the calculation of the efficiency of the distributed ADMM."}, {"heading": "A. Applications", "text": "We aim to solve the problem (1) via a star network (cluster) with a master node and N workers / slaves as shown in Figure 1. Such a distributed optimization approach is extremely useful in modern big data applications. [3] For example, consider the following empirical risk mitigation problems [7], which depend on the number of samples (aTj w, yj), (2) where m is the number of training samples and (aTj w, yj) represents a loss function (e.g. regression or classification error) depending on the training sample."}, {"heading": "B. Distributed ADMM", "text": "In this section, we present the distributed ADMM [4], [9] to solve the problem (1). Consider the following consensus formulation of the problem (1) min x0, xi-Rn, i = 1,..., NN \u2211 i = 1fi (xi) + h (x0) (4a) s.t. xi = x0, 0 = 1,., N. (4b) In (4), the N + 1 variable xi, i = 0, 1,., N, is subject to the consensus constraint in (4b), i.e., x0 = x1 = xxi. Thus, the problem (4) is equivalent to (1). It has been shown that such a consensus problem can be solved efficiently by the ADMM [9]. To describe this method, we let the Lagrange be dual variably associated (4b) and define the following augmented rangian functionLLC (x)."}, {"heading": "A. Algorithm Description", "text": "In this section, we represent an AD-ADMM as an index from which the Master information is derived (i.e., the number of Master updates x0). (i.e.) (i.e.) (i.e.) (i.e.) (i.e.) (i.e.) (i.e.) (i.e.) (i.e.) (i.) (i.) (i.) (i.) (i.) (i.) (i.) (i.) (i.) (i.) (i.) (i.) (i.) (i.) (i.) (i.) (i.) (i.) (i.) (i.) (i.) (i.) (i.) (i.) (i.) (i.) (i.) (i.) (i. (i.) (i.) (i.) (i. (i.) (i.) (i.) (i. (i.) (i.) (i.) (i.) (i.) (i.) (i.) (i.) (i.) (i.) (i.) (i.) (i.) (i.) (i.) (i.) (i.) (i.) (i.) (i.) (i. (i.) (i.) (i.) (i.) (i.) (i.) (i.) (i.) (i.) (i.) (i.) (i.) (i.) (i.) (i.) (i.) (i.) (i.) (i.) (i.) (i.) (i.) (i.) (i. (i.) (i.) (i.) (i.) (i.) (i.) (i.) (i. (i.) (i.) (i.) (i.) (i.) (i.) (i.) (i. (i.) (i.) (i.) (i.) (i.) (i.) (i.) (i.) (i.) (i.) (i."}, {"heading": "B. Convergence Analysis", "text": "In this sub-section, we analyze the convergence conditions of Algorithmus 2. In addition, we assume that there is a constant constant. < / p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p"}, {"heading": "C. Proof of Theorem 1 and Corollary 1", "text": "Let's write Algorithm 2 from the Master's point of view."}, {"heading": "A. Example 1: Sparse PCA", "text": "Theorem 1 has shown that the AD-ADMM can converge for non-convex problems = 1.5 AD problems. To verify this point, we consider the following sparse PCA problem [8] min w-Rn-N-J = 1w T-T-J Bjw + 2001, (50) where Bj-ADMM for the solution (50). In the simulations, each matrix Bj-Rn suggests a regularization parameter. The sparse PCA problem above is not a convex problem. In Figure 3, we show the convergence performance of the AD-ADMM for the solution (50). In the simulations, each matrix Bj-Rn suggests a 1000-500-sparse random matrix with about 5000 non-zero entries. In Figure 3, the convergence performance of the AD-ADMM for the solution (50) is represented differently."}, {"heading": "B. Example 2: LASSO", "text": "In this example, we compare the convergence performance of algorithm 4 with algorithm 2. We consider the following LASSO behavior less likely than 4 (February).We compare the convergence performance of algorithm 4 with algorithm 2. We consider the following LASSO behavior indispensable (February).We consider the following LASSO problems indispensable (February).We consider the behavior of algorithm 4 indispensable (February).The elements of algorithm 2 are randomly generated after the Gaussian distribution with zero mean and unit variance, i.e. We consider each of algorithms indispensable after Aiw0 + 1 + 1, where w0 + empirical accuracy is indispensable with chronic zero scenario."}], "references": [{"title": "Asynchronous distributed alternating direction method of multipliers: Algorithm and convergence analysis", "author": ["T.-H. Chang", "M. Hong", "W.-C. Liao", "X. Wang"], "venue": "submitted to IEEE ICASSP, Shanghai, China, March 20-25, 2016.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2016}, {"title": "Convex optimization for big data", "author": ["V. Cevher", "S. Becker", "M. Schmidt"], "venue": "IEEE Signal Process. Mag., pp. 32\u201343, Sept. 2014.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2014}, {"title": "Scaling up Machine Learning- Parallel and Distributed Approaches", "author": ["R. Bekkerman", "M. Bilenko", "J. Langford"], "venue": null, "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2012}, {"title": "Parallel and distributed computation: Numerical methods", "author": ["D.P. Bertsekas", "J.N. Tsitsiklis"], "venue": "Upper Saddle River, NJ, USA: Prentice-Hall, Inc.,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 1989}, {"title": "Regression shrinkage and selection via the LASSO", "author": ["R. Tibshirani"], "venue": "J. Roy. Stat. Soc. B, vol. 58, pp. 267\u2013288, 1996.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 1996}, {"title": "Large-scale sparse logistic regression", "author": ["J. Liu", "J. Chen", "J. Ye"], "venue": "Proc. ACM Int. Conf. on Knowledge Discovery and Data Mining, New York, NY, USA, June 28 - July 1, 2009, pp. 547\u2013556.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2009}, {"title": "The Elements of Statistical Learning: Data Mining, Inference, and Prediction", "author": ["T. Hastie", "R. Tibshirani", "J. Friedman"], "venue": null, "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2001}, {"title": "Alternating maximization: Unifying framework for 8 sparse PCA formulations and efficient parallel codes", "author": ["P. Richt\u00e1rik", "M. Tak\u00e1\u010d", "S.D. Ahipasaoglu"], "venue": "[Online] http://arxiv.org/abs/1212.4137.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 1212}, {"title": "Distributed optimization and statistical learning via the alternating direction method of multipliers", "author": ["S. Boyd", "N. Parikh", "E. Chu", "B. Peleato", "J. Eckstein"], "venue": "Foundations and Trends in Machine Learning, vol. 3, no. 1, pp. 1\u2013122, 2011.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2011}, {"title": "Hogwild!: A lock-free approach to parallelizing stochastic gradient descent", "author": ["F. Niu", "B. Recht", "C. Re", "S.J. Wright"], "venue": "Proc. Advances in Neural Information Processing Systems (NIPS), vol. 24, pp. 693-701, 2011, [Online] http://arxiv.org/ abs/1106.5730.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2011}, {"title": "Distributed delayed stochastic optimization", "author": ["A. Agarwal", "J.C. Duchi"], "venue": "Proc. Advances in Neural Information Processing Systems (NIPS), vol. 24, pp. 873-881, 2011, [Online] http://arxiv.org/abs/1104.5525.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2011}, {"title": "Parameter server for distributed machine learning", "author": ["M. Li", "L. Zhou", "Z. Yang", "A. Li", "F. Xia", "D.G. Andersen", "A. Smola"], "venue": "[Online] http://www.cs.cmu.edu/\u223cmuli/file/ps.pdf. February 22, 2016  DRAFT  34", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2016}, {"title": "Distributed delayed proximal gradient methods", "author": ["M. Li", "D.G. Andersen", "A. Smola"], "venue": "[Online] http://www.cs.cmu.edu/ \u223cmuli/file/ddp.pdf.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 0}, {"title": "Asynchronous stochastic coordinate descent: Parallelism and convergence properties", "author": ["J. Liu", "S.J. Wright"], "venue": "SIAM J. Optim.,, vol. 25, no. 1, pp. 351\u2013376, Feb. 2015.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2015}, {"title": "Parallel successive convex approximation for nonsmooth nonconvex optimization", "author": ["M. Razaviyayn", "M. Hong", "Z.-Q. Luo", "J.S. Pang"], "venue": "the Proceedings of the Neural Information Processing (NIPS), 2014.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2014}, {"title": "Decomposition by partial linearization: Parallel optimization of multi-agent systems", "author": ["G. Scutari", "F. Facchinei", "P. Song", "D.P. Palomar", "J.-S. Pang"], "venue": "IEEE Transactions on Signal Processing, vol. 63, no. 3, pp. 641\u2013656, 2014.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2014}, {"title": "Hybrid random/deterministic parallel algorithms for nonconvex big data optimization", "author": ["A. Daneshmand", "F. Facchinei", "V. Kungurtsev", "G. Scutari"], "venue": "to appear in IEEE Trans. on Signal Processing [Online] http://www.eng.buffalo.edu/ \u223cgesualdo/Papers/DanFaccKungTSPsub14.pdf.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 0}, {"title": "Convergence analysis of alternating direction method of multipliers for a family of nonconvex problems", "author": ["M. Hong", "Z.-Q. Luo", "M. Razaviyayn"], "venue": "to appear in SIAM J. Opt.; available on http://arxiv.org/pdf/1410.1390.pdf.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 1390}, {"title": "Asynchronous distributed ADMM for consensus optimization", "author": ["R. Zhang", "J.T. Kwok"], "venue": "Proc. 31th ICML, , 2014., Beijing, China, June 21-26, 2014, pp. 1\u20139.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2014}, {"title": "A distributed, asynchronous and incremental algorithm for nonconvex optimization: An ADMM based approach", "author": ["M. Hong"], "venue": "technical report; available on http://arxiv.org/pdf/1412.6058.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 1412}, {"title": "Asynchronous distributed optimization using a randomized alternating direction method of multipliers", "author": ["F. Iutzeler", "P. Bianchi", "P. Ciblat", "W. Hachem"], "venue": "Proc. IEEE CDC, Florence, Italy, Dec. 10-13, 2013, pp. 3671\u20133676.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2013}, {"title": "On the O(1/K) convergence of asynchronous distributed alternating direction method of multipliers", "author": ["E. Wei", "A. Ozdaglar"], "venue": "available on arxiv.org.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 0}, {"title": "D-ADMM: A communication-efficient distributed algorithm for separable optimization", "author": ["J.F.C. Mota", "J.M.F. Xavier", "P.M.Q. Aguiar", "M. Puschel"], "venue": "IEEE. Trans. Signal Process., vol. 60, no. 10, pp. 2718\u20132723, May 2013.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2013}, {"title": "Decentralized low-rank matrix completion", "author": ["Q. Ling", "Y. Xu", "W. Yin", "Z. Wen"], "venue": "Proc. IEEE ICASSP, Kyoto, Japan, March 25-30, 2012, pp. 2925\u20132928.", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2012}, {"title": "Asynchronous distributed ADMM for large-scale optimization- Part II: Linear convergence analysis and numerical performance", "author": ["T.-H. Chang", "W.-C. Liao", "M. Hong", "X. Wang"], "venue": "submitted for publication.", "citeRegEx": "25", "shortCiteRegEx": null, "year": 0}, {"title": "Sparisty and smoothness via the fused lasso", "author": ["R. Tibshirani", "M. Saunders"], "venue": "J. R. Statist. Soc. B, vol. 67, no. 1, pp. 91\u2013108, 2005.", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2005}, {"title": "Convergence analysis of ADMM based power system mode estimation under asynchronous wide-area communication delays", "author": ["J. Zhang", "S. Nabavi", "A. Chakrabortty", "Y. Xin"], "venue": "Proc. IEEE PES General Meeting, Denver, CO, USA, July 26-30, 2015, pp. 1\u20135.", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2015}, {"title": "Distributed constrained optimization by consensus-based primal-dual perturbation method", "author": ["T.-H. Chang", "A. Nedi\u0107", "A. Scaglione"], "venue": "IEEE. Trans. Auto. Control., vol. 59, no. 6, pp. 1524\u20131538, June 2014.", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2014}, {"title": "Multi-layered optimization of demand resources using Lagrange dual decomposition", "author": ["J.-Y. Joo", "M. Ilic"], "venue": "IEEE Trans. Smart Grid, vol. 4, no. 4, pp. 2081\u20132088, Dec 2013.", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2013}, {"title": "Distributed optimal power flow for smart microgrids", "author": ["E. Dall\u2019Anese", "H. Zhu", "G.B. Giannakis"], "venue": "IEEE Trans. Smart Grid, vol. 4, no. 3, pp. 1464\u20131475, Sept. 2013.", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2013}, {"title": "On the o(1/n) convergence rate of Douglas-Rachford alternating direction method", "author": ["B. He", "X. Yuan"], "venue": "SIAM J. Num. Anal., vol. 50, 2012. February 22, 2016  DRAFT  35", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2012}, {"title": "On the global and linear convergence of the generalized alternating direction method of multipliers", "author": ["W. Deng", "W. Yin"], "venue": "Rice CAAM technical report 12-14, 2012.", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2012}, {"title": "On the linear convergence of the alternating direction method of multipliers", "author": ["M. Hong", "Z.-Q. Luo"], "venue": "available on arxiv.org.", "citeRegEx": "33", "shortCiteRegEx": null, "year": 0}, {"title": "Introductory lectures on convex optimization: A basic course", "author": ["Y. Nesterov"], "venue": "Kluwer Academic Publishers,", "citeRegEx": "34", "shortCiteRegEx": "34", "year": 2004}, {"title": "Electronic companion for \u201cAsynchronous distributed ADMM for large-scale optimization- Part I: Algorithm and convergence analysis", "author": ["T.-H. Chang", "M. Hong", "W.-C. Liao", "X. Wang"], "venue": "available on http://arxiv.org.", "citeRegEx": "35", "shortCiteRegEx": null, "year": 0}, {"title": "Nonlinear Programming: 2nd Ed", "author": ["D.P. Bertsekas"], "venue": "Athena Scientific,", "citeRegEx": "36", "shortCiteRegEx": "36", "year": 2003}], "referenceMentions": [{"referenceID": 0, "context": "Part of this work was submitted to IEEE ICASSP, Shanghai, China, March 20-25, 2016 [1].", "startOffset": 83, "endOffset": 86}, {"referenceID": 1, "context": "Background Scaling up optimization algorithms for future data-intensive applications calls for efficient distributed and parallel implementations, so that modern multi-core high performance computing technologies can be fully utilized [2]\u2013[4].", "startOffset": 235, "endOffset": 238}, {"referenceID": 3, "context": "Background Scaling up optimization algorithms for future data-intensive applications calls for efficient distributed and parallel implementations, so that modern multi-core high performance computing technologies can be fully utilized [2]\u2013[4].", "startOffset": 239, "endOffset": 242}, {"referenceID": 4, "context": "Problem (1) includes as special cases many important statistical learning problems such as the LASSO problem [5], logistic regression (LR) problem [6], support vector machine (SVM) [7] and the sparse principal component analysis (PCA) problem [8].", "startOffset": 109, "endOffset": 112}, {"referenceID": 5, "context": "Problem (1) includes as special cases many important statistical learning problems such as the LASSO problem [5], logistic regression (LR) problem [6], support vector machine (SVM) [7] and the sparse principal component analysis (PCA) problem [8].", "startOffset": 147, "endOffset": 150}, {"referenceID": 6, "context": "Problem (1) includes as special cases many important statistical learning problems such as the LASSO problem [5], logistic regression (LR) problem [6], support vector machine (SVM) [7] and the sparse principal component analysis (PCA) problem [8].", "startOffset": 181, "endOffset": 184}, {"referenceID": 7, "context": "Problem (1) includes as special cases many important statistical learning problems such as the LASSO problem [5], logistic regression (LR) problem [6], support vector machine (SVM) [7] and the sparse principal component analysis (PCA) problem [8].", "startOffset": 243, "endOffset": 246}, {"referenceID": 2, "context": "In this paper, we focus on solving large-scale instances of these learning problems with either a large number of training samples or a large number of features (n is large) [3].", "startOffset": 174, "endOffset": 177}, {"referenceID": 3, "context": "Such star topology represents a common architecture for distributed computing, therefore it has been used widely in distributed optimization [4], [9]\u2013[16].", "startOffset": 141, "endOffset": 144}, {"referenceID": 8, "context": "Such star topology represents a common architecture for distributed computing, therefore it has been used widely in distributed optimization [4], [9]\u2013[16].", "startOffset": 146, "endOffset": 149}, {"referenceID": 15, "context": "Such star topology represents a common architecture for distributed computing, therefore it has been used widely in distributed optimization [4], [9]\u2013[16].", "startOffset": 150, "endOffset": 154}, {"referenceID": 9, "context": "For example, under the star topology, references [10], [11] presented distributed stochastic gradient descent (SGD) methods, references [12], [13] parallelized the proximal gradient (PG) methods, while references [14]\u2013[17] parallelized the block coordinate descent (BCD) method.", "startOffset": 49, "endOffset": 53}, {"referenceID": 10, "context": "For example, under the star topology, references [10], [11] presented distributed stochastic gradient descent (SGD) methods, references [12], [13] parallelized the proximal gradient (PG) methods, while references [14]\u2013[17] parallelized the block coordinate descent (BCD) method.", "startOffset": 55, "endOffset": 59}, {"referenceID": 11, "context": "For example, under the star topology, references [10], [11] presented distributed stochastic gradient descent (SGD) methods, references [12], [13] parallelized the proximal gradient (PG) methods, while references [14]\u2013[17] parallelized the block coordinate descent (BCD) method.", "startOffset": 136, "endOffset": 140}, {"referenceID": 12, "context": "For example, under the star topology, references [10], [11] presented distributed stochastic gradient descent (SGD) methods, references [12], [13] parallelized the proximal gradient (PG) methods, while references [14]\u2013[17] parallelized the block coordinate descent (BCD) method.", "startOffset": 142, "endOffset": 146}, {"referenceID": 13, "context": "For example, under the star topology, references [10], [11] presented distributed stochastic gradient descent (SGD) methods, references [12], [13] parallelized the proximal gradient (PG) methods, while references [14]\u2013[17] parallelized the block coordinate descent (BCD) method.", "startOffset": 213, "endOffset": 217}, {"referenceID": 16, "context": "For example, under the star topology, references [10], [11] presented distributed stochastic gradient descent (SGD) methods, references [12], [13] parallelized the proximal gradient (PG) methods, while references [14]\u2013[17] parallelized the block coordinate descent (BCD) method.", "startOffset": 218, "endOffset": 222}, {"referenceID": 9, "context": "To address such dilemma, a few recent works [10]\u2013[14] have introduced \u201casynchrony\u201d into the distributed algorithms, which allows the master to perform updates when not all, but a small subset of workers have returned their gradient information.", "startOffset": 44, "endOffset": 48}, {"referenceID": 13, "context": "To address such dilemma, a few recent works [10]\u2013[14] have introduced \u201casynchrony\u201d into the distributed algorithms, which allows the master to perform updates when not all, but a small subset of workers have returned their gradient information.", "startOffset": 49, "endOffset": 53}, {"referenceID": 9, "context": "As has been consistently reported in [10]\u2013[14], under such an asynchronous protocol, the computation time can decrease almost linearly with the number of workers.", "startOffset": 37, "endOffset": 41}, {"referenceID": 13, "context": "As has been consistently reported in [10]\u2013[14], under such an asynchronous protocol, the computation time can decrease almost linearly with the number of workers.", "startOffset": 42, "endOffset": 46}, {"referenceID": 8, "context": "Notably, other than the standard convex setting [9], the recent analysis in [18] has shown that such distributed ADMM is provably convergent to a Karush-Kuhn-Tucker (KKT) point even for non-convex problems.", "startOffset": 48, "endOffset": 51}, {"referenceID": 17, "context": "Notably, other than the standard convex setting [9], the recent analysis in [18] has shown that such distributed ADMM is provably convergent to a Karush-Kuhn-Tucker (KKT) point even for non-convex problems.", "startOffset": 76, "endOffset": 80}, {"referenceID": 8, "context": "Recently, the synchronous distributed ADMM [9], [18] has been extended to the asynchronous setting, similar to [10]\u2013[14].", "startOffset": 43, "endOffset": 46}, {"referenceID": 17, "context": "Recently, the synchronous distributed ADMM [9], [18] has been extended to the asynchronous setting, similar to [10]\u2013[14].", "startOffset": 48, "endOffset": 52}, {"referenceID": 9, "context": "Recently, the synchronous distributed ADMM [9], [18] has been extended to the asynchronous setting, similar to [10]\u2013[14].", "startOffset": 111, "endOffset": 115}, {"referenceID": 13, "context": "Recently, the synchronous distributed ADMM [9], [18] has been extended to the asynchronous setting, similar to [10]\u2013[14].", "startOffset": 116, "endOffset": 120}, {"referenceID": 18, "context": "Specifically, reference [19] has considered a version of AD-ADMM with bounded delay assumption and studied its theoretical and numerical performances.", "startOffset": 24, "endOffset": 28}, {"referenceID": 18, "context": "However, only convex cases are considered in [19].", "startOffset": 45, "endOffset": 49}, {"referenceID": 19, "context": "Reference [20] has studied another version of AD-ADMM for non-convex problems, which considers inexact subproblem updates and, similar to [10]\u2013[14], the workers compute gradient information only.", "startOffset": 10, "endOffset": 14}, {"referenceID": 9, "context": "Reference [20] has studied another version of AD-ADMM for non-convex problems, which considers inexact subproblem updates and, similar to [10]\u2013[14], the workers compute gradient information only.", "startOffset": 138, "endOffset": 142}, {"referenceID": 13, "context": "Reference [20] has studied another version of AD-ADMM for non-convex problems, which considers inexact subproblem updates and, similar to [10]\u2013[14], the workers compute gradient information only.", "startOffset": 143, "endOffset": 147}, {"referenceID": 20, "context": "References [21]\u2013[23] have respectively considered asynchronous ADMM methods for decentralized optimization over networks.", "startOffset": 11, "endOffset": 15}, {"referenceID": 22, "context": "References [21]\u2013[23] have respectively considered asynchronous ADMM methods for decentralized optimization over networks.", "startOffset": 16, "endOffset": 20}, {"referenceID": 20, "context": "Specifically, the asynchrony in [21] lies in that, at each iteration, the nodes are randomly activated to perform variable update.", "startOffset": 32, "endOffset": 36}, {"referenceID": 21, "context": "The method presented in [22] further allows that the communications between nodes can succeed or fail randomly.", "startOffset": 24, "endOffset": 28}, {"referenceID": 21, "context": "It is shown in [22] that such asynchronous ADMM can converge in a probability-one sense, provided that the nodes and communication links satisfy certain statistical assumption.", "startOffset": 15, "endOffset": 19}, {"referenceID": 22, "context": "Reference [23] has considered an asynchronous dual ADMM method.", "startOffset": 10, "endOffset": 14}, {"referenceID": 8, "context": "Contributions In this paper1, we generalize the state-of-the-art synchronous distributed ADMM [9], [18] to the asynchronous setting.", "startOffset": 94, "endOffset": 97}, {"referenceID": 17, "context": "Contributions In this paper1, we generalize the state-of-the-art synchronous distributed ADMM [9], [18] to the asynchronous setting.", "startOffset": 99, "endOffset": 103}, {"referenceID": 9, "context": "Like [10]\u2013[14], [19], [20], the asynchronous distributed ADMM (AD-ADMM) algorithm developed in this paper gives the master the freedom of making updates only based on variable information from a partial set of workers, which further improves the computation efficiency of the distributed ADMM.", "startOffset": 5, "endOffset": 9}, {"referenceID": 13, "context": "Like [10]\u2013[14], [19], [20], the asynchronous distributed ADMM (AD-ADMM) algorithm developed in this paper gives the master the freedom of making updates only based on variable information from a partial set of workers, which further improves the computation efficiency of the distributed ADMM.", "startOffset": 10, "endOffset": 14}, {"referenceID": 18, "context": "Like [10]\u2013[14], [19], [20], the asynchronous distributed ADMM (AD-ADMM) algorithm developed in this paper gives the master the freedom of making updates only based on variable information from a partial set of workers, which further improves the computation efficiency of the distributed ADMM.", "startOffset": 16, "endOffset": 20}, {"referenceID": 19, "context": "Like [10]\u2013[14], [19], [20], the asynchronous distributed ADMM (AD-ADMM) algorithm developed in this paper gives the master the freedom of making updates only based on variable information from a partial set of workers, which further improves the computation efficiency of the distributed ADMM.", "startOffset": 22, "endOffset": 26}, {"referenceID": 18, "context": "Our results differ significantly from the existing works [19], [21], [22] which are all developed for convex problems.", "startOffset": 57, "endOffset": 61}, {"referenceID": 20, "context": "Our results differ significantly from the existing works [19], [21], [22] which are all developed for convex problems.", "startOffset": 63, "endOffset": 67}, {"referenceID": 21, "context": "Our results differ significantly from the existing works [19], [21], [22] which are all developed for convex problems.", "startOffset": 69, "endOffset": 73}, {"referenceID": 7, "context": "Therefore, the analysis and algorithm proposed here are applicable not only to standard convex learning problems but also to important non-convex problems such as the sparse PCA problem [8] and matrix factorization problems [24].", "startOffset": 186, "endOffset": 189}, {"referenceID": 23, "context": "Therefore, the analysis and algorithm proposed here are applicable not only to standard convex learning problems but also to important non-convex problems such as the sparse PCA problem [8] and matrix factorization problems [24].", "startOffset": 224, "endOffset": 228}, {"referenceID": 19, "context": "To the best of our knowledge, except the inexact version in [20], this is the first time that the distributed ADMM is rigorously shown to be convergent for non-convex problems under the asynchronous protocol.", "startOffset": 60, "endOffset": 64}, {"referenceID": 18, "context": "Moreover, unlike [19], [21], [22] where the convergence analyses all rely on certain statistical assumption on the nodes/workers, our convergence analysis is deterministic and characterizes the worst-case convergence conditions of the AD-ADMM under a bounded delay assumption only.", "startOffset": 17, "endOffset": 21}, {"referenceID": 20, "context": "Moreover, unlike [19], [21], [22] where the convergence analyses all rely on certain statistical assumption on the nodes/workers, our convergence analysis is deterministic and characterizes the worst-case convergence conditions of the AD-ADMM under a bounded delay assumption only.", "startOffset": 23, "endOffset": 27}, {"referenceID": 21, "context": "Moreover, unlike [19], [21], [22] where the convergence analyses all rely on certain statistical assumption on the nodes/workers, our convergence analysis is deterministic and characterizes the worst-case convergence conditions of the AD-ADMM under a bounded delay assumption only.", "startOffset": 29, "endOffset": 33}, {"referenceID": 0, "context": "Furthermore, we demonstrate that the asynchrony of ADMM has to be handled with care \u2013 as a slight modification of the algorithm may In contrast to the conference paper [1], the current paper presents detailed proofs of theorems and more simulation results.", "startOffset": 168, "endOffset": 171}, {"referenceID": 24, "context": "In the companion paper [25], the linear convergence conditions of the AD-ADMM is further analyzed.", "startOffset": 23, "endOffset": 27}, {"referenceID": 8, "context": "Synopsis: Section II presents the applications of problem (1) and reviews the distributed ADMM in [9].", "startOffset": 98, "endOffset": 101}, {"referenceID": 2, "context": "Such distributed optimization approach is extremely useful in modern big data applications [3].", "startOffset": 91, "endOffset": 94}, {"referenceID": 6, "context": "For example, let us consider the following regularized empirical risk minimization problem [7] min w\u2208R m \u2211", "startOffset": 91, "endOffset": 94}, {"referenceID": 25, "context": "Problem (2) is one of the most important problems in signal processing and statistical learning, which includes the LASSO problem [26], LR [6], SVM [7] and the sparse PCA problem [8], to name a few.", "startOffset": 130, "endOffset": 134}, {"referenceID": 5, "context": "Problem (2) is one of the most important problems in signal processing and statistical learning, which includes the LASSO problem [26], LR [6], SVM [7] and the sparse PCA problem [8], to name a few.", "startOffset": 139, "endOffset": 142}, {"referenceID": 6, "context": "Problem (2) is one of the most important problems in signal processing and statistical learning, which includes the LASSO problem [26], LR [6], SVM [7] and the sparse PCA problem [8], to name a few.", "startOffset": 148, "endOffset": 151}, {"referenceID": 7, "context": "Problem (2) is one of the most important problems in signal processing and statistical learning, which includes the LASSO problem [26], LR [6], SVM [7] and the sparse PCA problem [8], to name a few.", "startOffset": 179, "endOffset": 182}, {"referenceID": 26, "context": "It is interesting to mention that many emerging problems in smart power grid can also be formulated as problem (1); see, for example, the power state estimation problem considered in [27] is solved by employing the distributed ADMM.", "startOffset": 183, "endOffset": 187}, {"referenceID": 27, "context": ", demand response) in [28]\u2013[30] can potentially be handled by the distributed ADMM as well.", "startOffset": 22, "endOffset": 26}, {"referenceID": 29, "context": ", demand response) in [28]\u2013[30] can potentially be handled by the distributed ADMM as well.", "startOffset": 27, "endOffset": 31}, {"referenceID": 3, "context": "Distributed ADMM In this section, we present the distributed ADMM [4], [9] for solving problem (1).", "startOffset": 66, "endOffset": 69}, {"referenceID": 8, "context": "Distributed ADMM In this section, we present the distributed ADMM [4], [9] for solving problem (1).", "startOffset": 71, "endOffset": 74}, {"referenceID": 8, "context": "It has been shown that such a consensus problem can be efficiently solved by the ADMM [9].", "startOffset": 86, "endOffset": 89}, {"referenceID": 3, "context": "According to [4], the standard synchronous ADMM iteratively updates the primal variables xi, i = 0, 1, .", "startOffset": 13, "endOffset": 16}, {"referenceID": 8, "context": "Algorithm 1 (Synchronous) Distributed ADMM for (4) [9] 1: Given initial variables x0 and \u03bb0; set x0 = x 0 and k = 0.", "startOffset": 51, "endOffset": 54}, {"referenceID": 8, "context": ", [9], [18], [31]\u2013[33].", "startOffset": 2, "endOffset": 5}, {"referenceID": 17, "context": ", [9], [18], [31]\u2013[33].", "startOffset": 7, "endOffset": 11}, {"referenceID": 30, "context": ", [9], [18], [31]\u2013[33].", "startOffset": 13, "endOffset": 17}, {"referenceID": 32, "context": ", [9], [18], [31]\u2013[33].", "startOffset": 18, "endOffset": 22}, {"referenceID": 30, "context": "Specifically, [31] shows that the ADMM, under general convex assumptions, has a worst-case O(1/k) convergence rate; while [32] shows that the ADMM can have a linear convergence rate given strong convexity and smoothness conditions on fi\u2019s.", "startOffset": 14, "endOffset": 18}, {"referenceID": 31, "context": "Specifically, [31] shows that the ADMM, under general convex assumptions, has a worst-case O(1/k) convergence rate; while [32] shows that the ADMM can have a linear convergence rate given strong convexity and smoothness conditions on fi\u2019s.", "startOffset": 122, "endOffset": 126}, {"referenceID": 17, "context": "For non-convex and smooth fi\u2019s, the work [18] shows that Algorithm 1 can converge to the set of KKT points with a O(1/ \u221a k) rate as long as \u03c1 is large enough.", "startOffset": 41, "endOffset": 45}, {"referenceID": 9, "context": "The asynchronism we consider is in the same spirit of [10]\u2013[14], [19], [20], where the master does not wait for all the workers.", "startOffset": 54, "endOffset": 58}, {"referenceID": 13, "context": "The asynchronism we consider is in the same spirit of [10]\u2013[14], [19], [20], where the master does not wait for all the workers.", "startOffset": 59, "endOffset": 63}, {"referenceID": 18, "context": "The asynchronism we consider is in the same spirit of [10]\u2013[14], [19], [20], where the master does not wait for all the workers.", "startOffset": 65, "endOffset": 69}, {"referenceID": 19, "context": "The asynchronism we consider is in the same spirit of [10]\u2013[14], [19], [20], where the master does not wait for all the workers.", "startOffset": 71, "endOffset": 75}, {"referenceID": 3, "context": "In particular, we follow the popular partially asynchronous model [4] and assume: Assumption 1 (Bounded delay) Let \u03c4 \u2265 1 be a maximum tolerable delay.", "startOffset": 66, "endOffset": 69}, {"referenceID": 18, "context": ", |Ak| \u2265 A for all k [19].", "startOffset": 21, "endOffset": 25}, {"referenceID": 24, "context": "Detailed numerical results will be reported in Section V of the companion paper [25].", "startOffset": 80, "endOffset": 84}, {"referenceID": 18, "context": "Let us compare Theorem 1 with the results in [19], [22].", "startOffset": 45, "endOffset": 49}, {"referenceID": 21, "context": "Let us compare Theorem 1 with the results in [19], [22].", "startOffset": 51, "endOffset": 55}, {"referenceID": 18, "context": "First, the convergence conditions in [19], [22] are only applicable for convex problems, whereas our results hold for both convex and non-convex problems.", "startOffset": 37, "endOffset": 41}, {"referenceID": 21, "context": "First, the convergence conditions in [19], [22] are only applicable for convex problems, whereas our results hold for both convex and non-convex problems.", "startOffset": 43, "endOffset": 47}, {"referenceID": 18, "context": "Second, [19], [22] have made specific statistical assumptions on the behavior of the workers, and the convergence results presented therein are in an expectation sense.", "startOffset": 8, "endOffset": 12}, {"referenceID": 21, "context": "Second, [19], [22] have made specific statistical assumptions on the behavior of the workers, and the convergence results presented therein are in an expectation sense.", "startOffset": 14, "endOffset": 18}, {"referenceID": 18, "context": "Therefore it is possible, at least theoretically, that a realization of the algorithm fails to converge despite satisfying the conditions given in [19].", "startOffset": 147, "endOffset": 151}, {"referenceID": 17, "context": "Inspired by [18], our analysis for Theorem 1 investigates how the augmented Lagrangian function, i.", "startOffset": 12, "endOffset": 16}, {"referenceID": 32, "context": "Such insight is reminiscent of the recent convergence results for multi-block ADMM in [33].", "startOffset": 86, "endOffset": 90}, {"referenceID": 7, "context": "To verify this point, let us consider the following sparse PCA problem [8] min w\u2208R \u2212 N \u2211", "startOffset": 71, "endOffset": 74}, {"referenceID": 24, "context": "The conditions under which linear convergence can be achieved are presented in the companion paper [25].", "startOffset": 99, "endOffset": 103}, {"referenceID": 24, "context": "Numerical results which demonstrate the time efficiency of the proposed AD-ADMM on a high performance computer cluster are also presented in [25].", "startOffset": 141, "endOffset": 145}], "year": 2016, "abstractText": "Aiming at solving large-scale optimization problems, this paper studies distributed optimization methods based on the alternating direction method of multipliers (ADMM). By formulating the optimization problem as a consensus problem, the ADMM can be used to solve the consensus problem in a fully parallel fashion over a computer network with a star topology. However, traditional synchronized computation does not scale well with the problem size, as the speed of the algorithm is limited by the slowest workers. This is particularly true in a heterogeneous network where the computing nodes experience different computation and communication delays. In this paper, we propose an asynchronous distributed ADMM (AD-ADMM) which can effectively improve the time efficiency of distributed optimization. Our main interest lies in analyzing the convergence conditions of the AD-ADMM, under the popular partially asynchronous model, which is defined based on a maximum tolerable delay of the network. Specifically, by considering general and possibly non-convex cost functions, we show that the AD-ADMM is guaranteed to converge to the set of Karush-Kuhn-Tucker (KKT) points as long as the algorithm parameters are chosen appropriately according to the network delay. We further illustrate that the asynchrony of the ADMM has to be handled with care, as slightly modifying the implementation of the AD-ADMM can jeopardize the algorithm convergence, even under the standard convex setting. Keywords\u2212 Distributed optimization, ADMM, Asynchronous, Consensus optimization Part of this work was submitted to IEEE ICASSP, Shanghai, China, March 20-25, 2016 [1]. Tsung-Hui Chang is supported by NSFC, China, Grant No. 61571385. Mingyi Hong is supported by NFS Grant No. CCF-1526078 , and AFOSR, Grant No. 15RT0767. Xiangfeng Wang is supported by Shanghai YangFan No. 15YF1403400 and NSFC No. 11501210. Tsung-Hui Chang is the corresponding author. Address: School of Science and Engineering, The Chinese University of Hong Kong, Shenzhen, China 518172. E-mail: tsunghui.chang@ieee.org. Mingyi Hong is with Department of Industrial and Manufacturing Systems Engineering, Iowa State University, Ames, 50011, USA, E-mail: mingyi@iastate.edu Wei-Cheng Liao is with Department of Electrical and Computer Engineering, University of Minnesota, Minneapolis, MN 55455, USA, E-mail: liaox146@umn.edu Xiangfeng Wang is with Shanghai Key Lab for Trustworthy Computing, School of Computer Science and Software Engineering, East China Normal University, Shanghai, 200062, China, E-mail: xfwang@sei.ecnu.edu.cn February 22, 2016 DRAFT", "creator": "dvips(k) 5.991 Copyright 2011 Radical Eye Software"}}}