{"id": "1509.02604", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "9-Sep-2015", "title": "Asynchronous Distributed ADMM for Large-Scale Optimization- Part II: Linear Convergence Analysis and Numerical Performance", "abstract": "The alternating direction method of multipliers (ADMM) has been recognized as a versatile approach for solving modern large-scale machine learning and signal processing problems efficiently. When the data size and/or the problem dimension is large, a distributed version of ADMM can be used, which is capable of distributing the computation load and the data set to a network of computing nodes. Unfortunately, a direct synchronous implementation of such algorithm does not scale well with the problem size, as the algorithm speed is limited by the slowest computing nodes. To address this issue, in a companion paper, we have proposed an asynchronous distributed ADMM (AD-ADMM) and studied its worst-case convergence conditions. In this paper, we further the study by characterizing the conditions under which the AD-ADMM achieves linear convergence. Our conditions as well as the resulting linear rates reveal the impact that various algorithm parameters, network delay and network size have on the algorithm performance. To demonstrate the superior time efficiency of the proposed AD-ADMM, we test the AD-ADMM on a high-performance computer cluster by solving a large-scale logistic regression problem.", "histories": [["v1", "Wed, 9 Sep 2015 02:07:27 GMT  (118kb)", "http://arxiv.org/abs/1509.02604v1", "submitted for publication, 28 pages"]], "COMMENTS": "submitted for publication, 28 pages", "reviews": [], "SUBJECTS": "cs.DC cs.LG cs.SY", "authors": ["tsung-hui chang", "wei-cheng liao", "mingyi hong", "xiangfeng wang"], "accepted": false, "id": "1509.02604"}, "pdf": {"name": "1509.02604.pdf", "metadata": {"source": "CRF", "title": "Asynchronous Distributed ADMM for Large-Scale Optimization- Part II: Linear Convergence Analysis and Numerical Performance", "authors": ["Tsung-Hui Chang", "Wei-Cheng Liao", "Mingyi Hong", "Xiangfeng Wang"], "emails": ["tsunghui.chang@ieee.org.", "mhong@umn.edu", "mingyi@iastate.edu", "xfwang@sei.ecnu.edu.cn"], "sections": [{"heading": null, "text": "In fact, most of them will be able to abide by the rules that they have imposed on themselves, and they will be able to understand the rules that they have imposed on themselves."}], "references": [{"title": "Asynchronous distributed alternating direction method of multipliers: Algorithm and convergence analysis", "author": ["T.-H. Chang", "M. Hong", "W.-C. Liao", "X. Wang"], "venue": "submitted to NIPS, Montreal, Canada, Dec. 7-12, 2015.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2015}, {"title": "Distributed optimization and statistical learning via the alternating direction method of multipliers", "author": ["S. Boyd", "N. Parikh", "E. Chu", "B. Peleato", "J. Eckstein"], "venue": "Foundations and Trends in Machine Learning, vol. 3, no. 1, pp. 1\u2013122, 2011.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2011}, {"title": "Sparisty and smoothness via the fused lasso", "author": ["R. Tibshirani", "M. Saunders"], "venue": "J. R. Statist. Soc. B, vol. 67, no. 1, pp. 91\u2013108, 2005.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2005}, {"title": "Large-scale sparse logistic regression", "author": ["J. Liu", "J. Chen", "J. Ye"], "venue": "Proc. ACM Int. Conf. on Knowledge Discovery and Data Mining, New York, NY, USA, June 28 - July 1, 2009, pp. 547\u2013556.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2009}, {"title": "The Elements of Statistical Learning: Data Mining, Inference, and Prediction", "author": ["T. Hastie", "R. Tibshirani", "J. Friedman"], "venue": null, "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2001}, {"title": "Alternating maximization: Unifying framework for 8 sparse PCA formulations and efficient parallel codes", "author": ["P. Richt\u00e1rik", "M. Tak\u00e1\u010d", "S.D. Ahipasaoglu"], "venue": "[Online] http://arxiv.org/abs/1212.4137.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 1212}, {"title": "Parallel and distributed computation: Numerical methods", "author": ["D.P. Bertsekas", "J.N. Tsitsiklis"], "venue": "Upper Saddle River, NJ, USA: Prentice-Hall, Inc.,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 1989}, {"title": "Hogwild!: A lock-free approach to parallelizing stochastic gradient descent", "author": ["F. Niu", "B. Recht", "C. Re", "S.J. Wright"], "venue": "Proc. Advances in Neural Information Processing Systems (NIPS), vol. 24, pp. 693-701, 2011, [Online] http://arxiv.org/ abs/1106.5730.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2011}, {"title": "Distributed delayed stochastic optimization", "author": ["A. Agarwal", "J.C. Duchi"], "venue": "Proc. Advances in Neural Information Processing Systems (NIPS), vol. 24, pp. 873-881, 2011, [Online] http://arxiv.org/abs/1104.5525.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2011}, {"title": "Parameter server for distributed machine learning", "author": ["M. Li", "L. Zhou", "Z. Yang", "A. Li", "F. Xia", "D.G. Andersen", "A. Smola"], "venue": "[Online] http://www.cs.cmu.edu/\u223cmuli/file/ps.pdf.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 0}, {"title": "Distributed delayed proximal gradient methods", "author": ["M. Li", "D.G. Andersen", "A. Smola"], "venue": "[Online] http://www.cs.cmu.edu/ \u223cmuli/file/ddp.pdf.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 0}, {"title": "Asynchronous stochastic coordinate descent: Parallelism and convergence properties", "author": ["J. Liu", "S.J. Wright"], "venue": "SIAM J. Optim.,, vol. 25, no. 1, pp. 351\u2013376, Feb. 2015.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2015}, {"title": "Parallel successive convex approximation for nonsmooth nonconvex optimization", "author": ["M. Razaviyayn", "M. Hong", "Z.-Q. Luo", "J.S. Pang"], "venue": "the Proceedings of the Neural Information Processing (NIPS), 2014.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2014}, {"title": "Decomposition by partial linearization: Parallel optimization of multi-agent systems", "author": ["G. Scutari", "F. Facchinei", "P. Song", "D.P. Palomar", "J.-S. Pang"], "venue": "IEEE Transactions on Signal Processing, vol. 63, no. 3, pp. 641\u2013656, 2014.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2014}, {"title": "On the o(1/n) convergence rate of Douglas-Rachford alternating direction method", "author": ["B. He", "X. Yuan"], "venue": "SIAM J. Num. Anal., vol. 50, 2012.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2012}, {"title": "Convergence analysis of alternating direction method of multipliers for a family of nonconvex problems", "author": ["M. Hong", "Z.-Q. Luo", "M. Razaviyayn"], "venue": "technical report; available on http://arxiv.org/pdf/1410.1390.pdf. September 10, 2015  DRAFT  25", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2015}, {"title": "On the global and linear convergence of the generalized alternating direction method of multipliers", "author": ["W. Deng", "W. Yin"], "venue": "Rice CAAM technical report 12-14, 2012.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2012}, {"title": "On the linear convergence of the ADMM in decentralized consensus optimization", "author": ["W. Shi", "Q. Ling", "K. Yuan", "G. Wu", "W. Yin"], "venue": "IEEE Trans. Signal Process., vol. 62, no. 7, pp. 1750\u20131761, April 2014.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2014}, {"title": "Multi-agent distributed optimization via inexact consensus ADMM", "author": ["T.-H. Chang", "M. Hong", "X. Wang"], "venue": "IEEE Trans. Signal Process., vol. 63, no. 2, pp. 482\u2013497, Jan. 2015.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2015}, {"title": "Linear convergence rate of class of distributed augmented lagrangian algorithms", "author": ["D. Jakoveti\u0107", "J.M.F. Moura", "J. Xavier"], "venue": "to appear in IEEE Trans. Automatic Control.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 0}, {"title": "On the linear convergence of the alternating direction method of multipliers", "author": ["M. Hong", "Z.-Q. Luo"], "venue": "available on arxiv.org.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 0}, {"title": "Asynchronous distributed ADMM for consensus optimization", "author": ["R. Zhang", "J.T. Kwok"], "venue": "Proc. 31th ICML, , 2014., Beijing, China, June 21-26, 2014, pp. 1\u20139.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2014}, {"title": "Asynchronous distributed ADMM for large-scale optimization- Part I: Algorithm and convergence analysis", "author": ["T.-H. Chang", "M. Hong", "W.-C. Liao", "X. Wang"], "venue": "submitted for publication.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 0}, {"title": "A fast iterative shrinkage-thresholding algorithm for linear inverse problems", "author": ["A. Beck", "M. Teboulle"], "venue": "SIAM J. Imaging Sci., vol. 2, no. 1, pp. 183\u2013202, 2009.", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2009}], "referenceMentions": [{"referenceID": 2, "context": "Many important statistical learning problems can be formulated as problem (1), including, for example, the LASSO problem [3], logistic regression (LR) problem [4], support vector machine (SVM) [5] and the sparse principal component analysis (PCA) problem [6], to name a few.", "startOffset": 121, "endOffset": 124}, {"referenceID": 3, "context": "Many important statistical learning problems can be formulated as problem (1), including, for example, the LASSO problem [3], logistic regression (LR) problem [4], support vector machine (SVM) [5] and the sparse principal component analysis (PCA) problem [6], to name a few.", "startOffset": 159, "endOffset": 162}, {"referenceID": 4, "context": "Many important statistical learning problems can be formulated as problem (1), including, for example, the LASSO problem [3], logistic regression (LR) problem [4], support vector machine (SVM) [5] and the sparse principal component analysis (PCA) problem [6], to name a few.", "startOffset": 193, "endOffset": 196}, {"referenceID": 5, "context": "Many important statistical learning problems can be formulated as problem (1), including, for example, the LASSO problem [3], logistic regression (LR) problem [4], support vector machine (SVM) [5] and the sparse principal component analysis (PCA) problem [6], to name a few.", "startOffset": 255, "endOffset": 258}, {"referenceID": 1, "context": "Distributed optimization algorithms that can scale well with large-scale instances of (1) have drawn significant attention in recent years [2], [7]\u2013[14].", "startOffset": 139, "endOffset": 142}, {"referenceID": 6, "context": "Distributed optimization algorithms that can scale well with large-scale instances of (1) have drawn significant attention in recent years [2], [7]\u2013[14].", "startOffset": 144, "endOffset": 147}, {"referenceID": 13, "context": "Distributed optimization algorithms that can scale well with large-scale instances of (1) have drawn significant attention in recent years [2], [7]\u2013[14].", "startOffset": 148, "endOffset": 152}, {"referenceID": 1, "context": "The convergence conditions of the distributed ADMM have been extensively studied; see [2], [7], [15]\u2013[20].", "startOffset": 86, "endOffset": 89}, {"referenceID": 6, "context": "The convergence conditions of the distributed ADMM have been extensively studied; see [2], [7], [15]\u2013[20].", "startOffset": 91, "endOffset": 94}, {"referenceID": 14, "context": "The convergence conditions of the distributed ADMM have been extensively studied; see [2], [7], [15]\u2013[20].", "startOffset": 96, "endOffset": 100}, {"referenceID": 19, "context": "The convergence conditions of the distributed ADMM have been extensively studied; see [2], [7], [15]\u2013[20].", "startOffset": 101, "endOffset": 105}, {"referenceID": 1, "context": "For example, for general convex problems, references [2], [7] showed that the ADMM is guaranteed to converge to an optimal solution and [15] showed that the ADMM has a worst-case O(1/k) convergence rate, where k is the iteration number.", "startOffset": 53, "endOffset": 56}, {"referenceID": 6, "context": "For example, for general convex problems, references [2], [7] showed that the ADMM is guaranteed to converge to an optimal solution and [15] showed that the ADMM has a worst-case O(1/k) convergence rate, where k is the iteration number.", "startOffset": 58, "endOffset": 61}, {"referenceID": 14, "context": "For example, for general convex problems, references [2], [7] showed that the ADMM is guaranteed to converge to an optimal solution and [15] showed that the ADMM has a worst-case O(1/k) convergence rate, where k is the iteration number.", "startOffset": 136, "endOffset": 140}, {"referenceID": 15, "context": "Considering non-convex problems with smooth fi\u2019s, reference [16] presented conditions for which the distributed ADMM converges to the set of Karush-KuhnTucker (KKT) points.", "startOffset": 60, "endOffset": 64}, {"referenceID": 16, "context": "For problems with strongly convex and smooth fi\u2019s or problems satisfying certain error bound condition, references [17] and [21] respectively showed that the ADMM can even exhibit a linear convergence rate.", "startOffset": 115, "endOffset": 119}, {"referenceID": 20, "context": "For problems with strongly convex and smooth fi\u2019s or problems satisfying certain error bound condition, references [17] and [21] respectively showed that the ADMM can even exhibit a linear convergence rate.", "startOffset": 124, "endOffset": 128}, {"referenceID": 17, "context": "References [18]\u2013[20] also showed similar linear convergence conditions for some variants of distributed ADMM in a network with a general topology.", "startOffset": 11, "endOffset": 15}, {"referenceID": 19, "context": "References [18]\u2013[20] also showed similar linear convergence conditions for some variants of distributed ADMM in a network with a general topology.", "startOffset": 16, "endOffset": 20}, {"referenceID": 1, "context": "However, the distributed ADMM in [2], [16] have assumed a synchronous network, where at each iteration, the master always waits until all", "startOffset": 33, "endOffset": 36}, {"referenceID": 15, "context": "However, the distributed ADMM in [2], [16] have assumed a synchronous network, where at each iteration, the master always waits until all", "startOffset": 38, "endOffset": 42}, {"referenceID": 21, "context": "To improve the time efficiency, the works [22], [23] have generalized the distributed ADMM to an asynchronous network.", "startOffset": 42, "endOffset": 46}, {"referenceID": 22, "context": "To improve the time efficiency, the works [22], [23] have generalized the distributed ADMM to an asynchronous network.", "startOffset": 48, "endOffset": 52}, {"referenceID": 21, "context": "Specifically, in the asynchronous distributed ADMM (AD-ADMM) proposed in [22], [23], the master does not necessarily wait for all the workers.", "startOffset": 73, "endOffset": 77}, {"referenceID": 22, "context": "Specifically, in the asynchronous distributed ADMM (AD-ADMM) proposed in [22], [23], the master does not necessarily wait for all the workers.", "startOffset": 79, "endOffset": 83}, {"referenceID": 22, "context": "Theoretically, it has been shown in [23] that the AD-ADMM is guaranteed to converge (to a KKT point) even for non-convex problem (1), under a bounded delay assumption only.", "startOffset": 36, "endOffset": 40}, {"referenceID": 22, "context": "Firstly, beyond the convergence analysis in [23], we further present the conditions for which the AD-ADMM can exhibit a linear convergence rate.", "startOffset": 44, "endOffset": 48}, {"referenceID": 16, "context": "To the best of our knowledge, our results are novel, and are by no means extensions of the existing analyses [17]\u2013[21] for synchronous ADMM.", "startOffset": 109, "endOffset": 113}, {"referenceID": 20, "context": "To the best of our knowledge, our results are novel, and are by no means extensions of the existing analyses [17]\u2013[21] for synchronous ADMM.", "startOffset": 114, "endOffset": 118}, {"referenceID": 22, "context": "Synopsis: Section II reviews the AD-ADMM in [23].", "startOffset": 44, "endOffset": 48}, {"referenceID": 22, "context": "ASYNCHRONOUS DISTRIBUTED ADMM In this section, we review the AD-ADMM proposed in [23].", "startOffset": 81, "endOffset": 85}, {"referenceID": 6, "context": "By applying the standard ADMM [7] to problem (2), one obtains the following three simple steps: for iteration k = 0, 1, .", "startOffset": 30, "endOffset": 33}, {"referenceID": 22, "context": "1 in [23]).", "startOffset": 5, "endOffset": 9}, {"referenceID": 21, "context": "The distributed ADMM has been extended to an asynchronous network in [22], [23].", "startOffset": 69, "endOffset": 73}, {"referenceID": 22, "context": "The distributed ADMM has been extended to an asynchronous network in [22], [23].", "startOffset": 75, "endOffset": 79}, {"referenceID": 6, "context": "This condition guarantees that the variable information is at most \u03c4 iterations old, and is known as the partially asynchronous model [7]: Assumption 1 (Bounded delay) Let \u03c4 \u2265 1 be a maximum tolerable delay.", "startOffset": 134, "endOffset": 137}, {"referenceID": 4, "context": "Interestingly, such structured cost function appears in many machine learning problems, for example, the least squared problem and the logistic regression problem [5].", "startOffset": 163, "endOffset": 166}, {"referenceID": 16, "context": "Since it has been known that the (synchronous) distributed ADMM [17]\u2013[21] can converge linearly given the same structured cost functions in Assumption 3 and Assumption 4, the convergence results presented above demonstrate that the linear convergence property can be preserved in the asynchronous network.", "startOffset": 64, "endOffset": 68}, {"referenceID": 20, "context": "Since it has been known that the (synchronous) distributed ADMM [17]\u2013[21] can converge linearly given the same structured cost functions in Assumption 3 and Assumption 4, the convergence results presented above demonstrate that the linear convergence property can be preserved in the asynchronous network.", "startOffset": 69, "endOffset": 73}, {"referenceID": 23, "context": "For each worker, we employed the fast iterative shrinkage thresholding algorithm (FISTA) [25] to solve the corresponding subproblem (10).", "startOffset": 89, "endOffset": 93}, {"referenceID": 21, "context": "In our experiments, analogous to [22], we further constrained the minimum size of the active set Ak by |Ak| \u2265 A where A \u2208 [1, N ] is an integer.", "startOffset": 33, "endOffset": 37}, {"referenceID": 22, "context": "CONCLUSIONS In this paper, we have analytically studied the linear convergence conditions of the AD-ADMM proposed in [23].", "startOffset": 117, "endOffset": 121}], "year": 2015, "abstractText": "The alternating direction method of multipliers (ADMM) has been recognized as a versatile approach for solving modern large-scale machine learning and signal processing problems efficiently. When the data size and/or the problem dimension is large, a distributed version of ADMM can be used, which is capable of distributing the computation load and the data set to a network of computing nodes. Unfortunately, a direct synchronous implementation of such algorithm does not scale well with the problem size, as the algorithm speed is limited by the slowest computing nodes. To address this issue, in a companion paper, we have proposed an asynchronous distributed ADMM (AD-ADMM) and studied its worst-case convergence conditions. In this paper, we further the study by characterizing the conditions under which the AD-ADMM achieves linear convergence. Our conditions as well as the resulting linear rates reveal the impact that various algorithm parameters, network delay and network size have on the algorithm performance. To demonstrate the superior time efficiency of the proposed AD-ADMM, we test the ADADMM on a high-performance computer cluster by solving a large-scale logistic regression problem. Keywords\u2212 Distributed optimization, ADMM, Asynchronous, Consensus optimization \u22c6Tsung-Hui Chang is the corresponding author. Address: School of Science and Engineering, The Chinese University of Hong Kong, Shenzhen, China 518172, E-mail: tsunghui.chang@ieee.org. Wei-Cheng Liao is with Department of Electrical and Computer Engineering, University of Minnesota, Minneapolis, MN 55455, USA, E-mail: mhong@umn.edu Mingyi Hong is with Department of Industrial and Manufacturing Systems Engineering, Iowa State University, Ames, 50011, USA, E-mail: mingyi@iastate.edu Xiangfeng Wang is with Shanghai Key Lab for Trustworthy Computing, Software Engineering Institute, East China Normal University, Shanghai, 200062, China, E-mail: xfwang@sei.ecnu.edu.cn September 10, 2015 DRAFT", "creator": "dvips(k) 5.991 Copyright 2011 Radical Eye Software"}}}