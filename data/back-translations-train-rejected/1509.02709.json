{"id": "1509.02709", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "9-Sep-2015", "title": "A Topological Approach to Meta-heuristics: Analytical Results on the BFS vs. DFS Algorithm Selection Problem", "abstract": "Search is a central problem in artificial intelligence, and BFS and DFS the two most fundamental ways to search. In this report we derive results for average BFS and DFS runtime: For tree search, we employ a probabilistic model of goal distribution; for graph search, the analysis depends on an additional statistic of path redundancy and average branching factor. As an application, we use the results on two concrete grammar problems. The runtime estimates can be used to select the faster out of BFS and DFS for a given problem, and may form the basis for further analysis of more advanced search methods. Finally, we verify our results experimentally; the analytical approximations come surprisingly close to empirical reality.", "histories": [["v1", "Wed, 9 Sep 2015 10:30:48 GMT  (344kb,D)", "http://arxiv.org/abs/1509.02709v1", "Main results published in 28th Australian Joint Conference on Artificial Intelligence, 2015"]], "COMMENTS": "Main results published in 28th Australian Joint Conference on Artificial Intelligence, 2015", "reviews": [], "SUBJECTS": "cs.AI", "authors": ["tom everitt", "marcus hutter"], "accepted": false, "id": "1509.02709"}, "pdf": {"name": "1509.02709.pdf", "metadata": {"source": "CRF", "title": "A Topological Approach to Meta-heuristics: Analytical Results on the BFS vs. DFS Algorithm Selection Problem", "authors": ["Tom Everitt", "Marcus Hutter"], "emails": [], "sections": [{"heading": null, "text": "KeywordsSFSO, DFS, Analytical algorithm selection, Average runtime, Metaheuristics, Tree search, Graph search, Probable target distributionContent"}, {"heading": "1 Introduction 2", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "2 Graph search problems 3", "text": "2.1 Performance of algorithms........................ 5"}, {"heading": "3 Basic Search Algorithms 6", "text": "3.1 Trees...................................... 6 3.2 Uninformed search methods.................... 6 3.3 Informed constructive methods....................."}, {"heading": "4 Literature review 8", "text": "4.1 Characteristic meta-heuristics............... 8 4.2 Learning search policy............................... 9ar Xiv: 150 9.02 709v 1 [cs.A I] 9 S"}, {"heading": "5 Complete Binary Tree 10", "text": "................................................................................................................................"}, {"heading": "6 Colliding Branches 17", "text": "6.1 DFS analysis................................. 18 SFSO analysis..........................................."}, {"heading": "7 Grammar Problems 20", "text": "..................................................................................."}, {"heading": "8 Experimental verification 24", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "9 Empirical Predictions 27", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "10 Discussion 29", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "A List of notation 33", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "B List of search problems 34", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "C List of Topological features 35", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "1 Introduction", "text": "In fact, the fact is that most of them are able to assert themselves, that they are able to assert themselves, that they are able to assert themselves, and that they are able to assert themselves, and that they are able to assert themselves."}, {"heading": "2 Graph search problems", "text": "It is in fact the case that one is able to live in a country in which people are able to live, to live, to live, to live, to live, to live, to live, to live, to live, to live, to live, to live, to live, to live, to live, to live, to live, to live, to live, to live, to live, to live, to live, to live, to live, to live, to live, to live, to live, to live, to live and to live."}, {"heading": "2.1 Algorithm performance", "text": "A search algorithm is an algorithm that returns a solution (a state or path) to a search problem with diagrams, giving oracle access to the functions N and C, and possibly either EC and h or Q (depending on the type of search problem). The performance of a single problem can be defined in the following terms: 1. Quality of solution. 2. The number of states examined; a state is considered researched when either N (s) or C (s) is called. 3. The runtime of the algorithm is typically subject to a worst-case analysis (is the procedure complete / optimal) and 3 and 4 an asymptotic worst-case analysis. We focus on constructive search and measure performance by the average number of states researched."}, {"heading": "3 Basic Search Algorithms", "text": "A wealth of search methods has been studied for both the constructive and the local search problems. We will discuss only a subset of the more important ones here and refer to the books (Russell and Norvig, 2010; Edelkamp and Schro \u00bc dl, 2012) for more details. First, some preliminary work on trees, a basic structure in search analysis."}, {"heading": "3.1 Trees", "text": "A root tree is a (directed) graph with a root s0 where each pair of nodes is connected by exactly one path. A node v is the distance from root s0 to v. The depth d is the length of a longest path starting from s0. If each node at level smaller than D-N has exactly b children and nodes at level D are leaves (have no children), then the tree is complete with branch factor b and depth D. Such a tree has bD leaves and (bD + 1 \u2212 1) / (b \u2212 1) nodes. In particular, complete binary trees (with branch factor 2) have 2D leaves and 2D + 1 \u2212 1 nodes. A node v is a descendant of a node u if there is a path from u to v (i.e. if v is a child of a child of u)."}, {"heading": "3.2 Uninformed search methods", "text": "In this case, it is a question of a single path, which is as long as possible, and of a path which must be adhered to in order to take it."}, {"heading": "3.3 Informed Constructive Methods", "text": "The most popular method for an informed constructive search is A *. It can be seen as a generalization of BFS. A * combines the heuristic information h (v) of a node v with the accumulated edge costs g (v) of the shortest path found from the start node to v. A * always extends the detected node with the smallest g (v) + h (v). But if the heuristic function for nodes is smaller closer to the destination, A * will prioritize more promising nodes and find the destination much faster than BFS. In particular, if the heuristic strategy is consistent (never overestimate the distance and respect the triangular inequality), then A * is guaranteed to find an optimal solution. The tree search version of A * requires only the heuristic way to be permissible (never overestimate the search distance)."}, {"heading": "3.4 Informed Local Search", "text": "Most informed local search algorithms strive to combine an exploiting component of mountaineering with an exploration component.The simplest is mountaineering, which always goes to the neighbor with the highest objective value and is randomly restarted when stuck. More advanced methods include simulated glow, which adds a random motion to mountaineering, and the random component decays over time. Genetic algorithms use a population of search nodes and try to find new search points by combining features from detected search nodes."}, {"heading": "4 Literature review", "text": "The work in the first subsection assumes that there is a portfolio of predefined algorithms, and only tries to predict which algorithm in the portfolio is better for which problem. In the second subsection, approaches are discussed that attempt to build new search policies, possibly using a number of basic algorithms as building blocks."}, {"heading": "4.1 Feature-based Meta-heuristics", "text": "The problem of algorithm selection is the question of which algorithm should best be applied to a particular problem (Rice, 1975; Kotthoff, 2014). Closely related is the question of how to infer the search time of different search algorithms for the problem, since this information can be used to select the fastest algorithm. Both analytical studies and machine learning techniques applied to empirical data have been tried, the latter sometimes known as empirical performance models. The most comprehensive surveys are by Hutter et al. (2014) and Kotthoff (2011), and the doctoral theses Thompson (2011) and Arbelaez Rodriguez (2011). For DFS, Knuth (1975) has made a simple but important observation on how the branching factor seen during the search can be used to estimate the size of the search tree and the runtime. Despite the simplicity of the system, the estimates work amazingly well in practice. Several generisations were developed by Chdom et (1978)."}, {"heading": "4.2 Learning the search policy", "text": "In fact, it is not as if it were an attempt to behave in a way that was the case in the past. (...) In fact, it is not as if it had been an attempt to behave in a way that was the case in the past. (...) It is not as if it had been an attempt. (...) It is not as if it had been an attempt. (...) It is as if it had been an attempt. (...) It is as if it had been an attempt. (...). (...) It is not as if it had been an attempt. (...) It is not as if it had been an attempt. (...) It is not as if it had been an attempt. (...) It is not as if it had been an attempt. (...) It is not as if it had not been an attempt. (...) It is not as if it had been an attempt. (...)"}, {"heading": "5 Complete Binary Tree", "text": "In a search diagram, the neighborhood relation N leads to a topology of the state space S. The following two sections analytically examine how the structure and depth of the diagram and the distribution of the targets can be used to predict the search performance of BFS and DFS. Figure 1 gives the intuition for the different search strategies BFS and DFS and how they initially focus the search on different areas of the tree. Consider, as a concrete example, the search problem of solving a Rubik's Cube. There is an upper limit D = 20 to how many steps it can take to reach the target (Rokicki and Kociemba, 2013). However, what would be the expected BFS and DFS search time for this problem? Which search algorithms that cannot remember where they were, the search space will be directed to a complete tree with a specified branch factor 9. What would be the expected BFS and DFS search time for this problem? Which would be faster? After the first level of the first section of the Section 6, several objectives will be examined in the background of this first section."}, {"heading": "5.1 Preliminaries", "text": "For simplicity, let us say that the runtime or search time of a search method (FSO or DFS) = k = is the number of nodes to be examined until a first target is found (5 and 6 respectively in Figure 1). This simplistic assumption is based on the fact that the node expansion is the dominant operation consuming similar time throughout the tree. However, if no target exists, the search method will examine all nodes before stopping. In this case, we define runtime as the number of nodes in the search problem plus 1 (i.e., 2D + 1 in the case of a binary tree of depth D).2Let the event be that a target exists, and the event that a target exists at level k, and the number of its complementarities. Let us let Fk = k (k \u2212 i = 0) be the event that the level k has the first target."}, {"heading": "5.2 Complete Binary Tree with a Single Goal Level", "text": "D D D D D (D D D D D D D (D D D D D D D D D D D D (D D D D D D D D D D D D D D D D (D D D D D D D D D D D D D D D D D D D D D D D D D D D (D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D (D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D (D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D"}, {"heading": "5.3 Complete Binary Tree with Multiple Goal Levels", "text": "We now generalize the model developed in the previous section to problems that can have targets on any number of levels. Let's let pk be the associated target probability for each level k {0,.., D}. Not every pk should be equal to 0. Nodes at level k have an ideal probability pk to be a target. We will refer to these types of problems as (multi-target level) complete binary trees with depth D and target probability p."}, {"heading": "5.3.1 DFS Analysis", "text": "Our approximation of DFS performance in the case of multiple target levels approaches the geometric distribution used in Proposition 6 approximating k = k with an exponential distribution (its continuous approximation by Lemma 3).Proposition 9 (Expected multi-target level DFS performance).If we look at a complete binary tree of depth D with approximate target probabilities p = [p0,.., pD]. [0, 1) D \u2212 \u2212 p \u2212 p \u2212 p \u2212 p value, for all k, pk 1, then the expected number of DFS nodes with approximate minimum target probabilities p = [p0,., pD]: = 1 / D \u2211 k = 0 ln (1 \u2212 pk) \u2212 j 2 \u2212 j, and for all k, pk 1, then the expected number of DFS variables with approximate minimum target probabilities p = [p0,., pD]: = 1 / D \u2211 k = 0 ln (1 \u2212 pk) \u2212 j 2 \u2212 j, and for all k, pk 1, then the expected number of DFS variables with approximate minimum target probabilities is sought. The proof constructs for each level k an exponential random variable Xk, which is found at one level before the target is searched."}, {"heading": "5.3.2 BFS Analysis", "text": "The corresponding expected search time tBFSMGL (D, p) for BFS requires less insight and can be calculated precisely by conditioning at which level the first target is located. However, the resulting formula is less elegant. The same technique cannot be used for DFS because DFS does not exhaust the levels one by one. The expected BFS search time gets a more uniform expression by introducing an additional hypothetical level D + 1, where all nodes are targets. That is, level D + 1 has target probability pD + 1 = 1 andP (1 \u2212 pi) 2 i). The expected BFS search time is determined by introducing an additional hypothetical level D + 1, where all nodes are targets."}, {"heading": "6 Colliding Branches", "text": "The last section predicted the runtime of tree search algorithms that do not remember which nodes they visit, meaning that the search graph always has the shape of a tree (the same node may occur in several places). In this section, we examine the performance of graph search algorithms that avoid revisiting previously explored nodes by tracking which nodes have already been seen. Figure 3 gives an idea of the difference between BFS and DFS in multiplied linked graphs (with limited search depth). Definition 11. For a specific search problem: Have the level of a node v, level (v), the length of a shortest path from the start node to v. Let D = maxv level (v) be the (generalized) depth of the search diagram. Let the first node at level n before reaching DFS, 0 \u2264 n \u2264 D. The tracking counter L plays a central role in the analysis."}, {"heading": "6.1 DFS Analysis", "text": "The nodes \u03b40, \u03b4D play a central role in the analysis of the DFS sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub"}, {"heading": "6.2 BFS Analysis", "text": "The SFSO analysis requires only the descending counter L (0, \u00b7) with the first argument to 0, and follows the same structure at level 5.3.2. Unlike the DFS holes above, this analysis gives a precise expression for the expected runtime. The idea is to count the number of nodes in the upper k levels of the tree (0, 0),., L (0, k)) and calculate the probability that they contain a target. The upper subgraph Uk = 0 L (0, i) be the number of nodes at level k,."}, {"heading": "7 Grammar Problems", "text": "We will now show how to apply the general theory of Section 6 to two specific grammar problems: A grammar problem is a constructive search problem where nodes are strings above a finite alphabet B and the neighborhood relationship is given by a set of manufacturing rules. Manufacturing rules are mappings x \u2192 y, x, y, y, B * that define how strings can be transformed. For example, the manufacturing rule S \u2192 Sa allows the conversion of the string aSa to aSaa. A grammar problem is defined by a set of manufacturing rules, along with a starting rule and a set of target strings. One solution is a sequence of manufacturing direction applications that turns the initial rule into a target rule. Many search problems can be formulated as grammar problems, with state string representations modified by manufacturing rules. Their generality makes it predictable whether a particular grammar problem has a solution or not."}, {"heading": "7.1 Binary Grammar", "text": "Leave the empty string (# a + 1) = d + 2 children. Nodes further to the right will detect the number of parents."}, {"heading": "7.2 Random Grammar", "text": "The random grammar problem has the alphabet B = {S, a, b} and the initial string S, the production rules always contain S \u2192 (indicating the empty string) plus a random subset of the addition S \u2192 Sa, S \u2192 Sb, S \u2192 aS, S \u2192 bS and a random subset of the motion rules Sa \u2192 aS, Sb \u2192 bS, aS \u2192 Sa and bS \u2192 Sb. Only strings that do not contain S can be target nodes. As usual, a maximum depth D and a target probability vector p = [p0,., pD] are given. For simplified analysis, we misuse the notation as follows. We consider S-less nodes to be one level higher than they actually are. For example, we will consider a section to be at level 1, although it is technically at level 2 or lower (e.g. reached by the path S \u2192 binaries, S \u2192). The minor modifications of BS are always a general problem, leaving only the first DFS and DFS rules smaller."}, {"heading": "7.3 Full Grammar", "text": "The search for the full grammar problem is illustrated in Figure 7 (edges caused by moving rules are not shown).Since there are four additional rules that can be applied to each node, each node will have four children. Typically, if we move further to the right into the tree, more children will have already been detected.The full grammar problem can be analysed by reducing it to a binary grammar problem with the same parameters D and p."}, {"heading": "8 Experimental verification", "text": "To verify the analytical results, we have implemented the models Sections 5-7 in Python 3. < The first number in each field is the empirical average, \u2022 the second number is the analytical estimate, and \u2022 the third number is the percentage error of the analytical estimate. For certain parameter settings, there is only a small chance (< 10 \u2212 3) that there are no targets. Under such circumstances, every 1000 generated search plots typically inhabit a target, and so the empirical search time is comparatively short. However, since a tree of depth 14 is about 215 \u2248 3 \u2022 105 nodes (and a search algorithm must search through all of them in the case that there is no target), the rarely occurring event without a target can significantly affect the expected search time."}, {"heading": "9 Empirical Predictions", "text": "The Random Grammar Model of Section 7.2 has a rich variety of topological features such as branching factor distribution properties. It is an interesting BG method, which is largely accurate. Each field contains empirical average / analysis expectation / error percentages. However, the extent to which such characteristics can be used to predict whether BFS or DFS is the better search method is not the least analytical approach due to its many cases. Therefore, we approached this problem empirically. \u2022 We created a data set with 1827 randomly sampled random grammars. The sample was uniformly conducted from the following groups: First, a random sample of a number of rules r [4, 8], then a random sample r subset of the 8 possible production rules. \u2022 Also sample maximum search depth D [11, 15]."}, {"heading": "10 Discussion", "text": "It is even possible to equate intelligence with (Bayesian expectimax) optimization techniques (Legg and Hutter, 2007).In this report, we derived analytical results for the expected runtime performance. Section 5 focuses on BFS and DFS tree search, where the explored nodes are not remembered. A vector p = (P1,., pD) describes a priority target probability for the different levels of the tree. This concrete but general model of target distribution allowed us to calculate the closed form of BFS and DFS average runtime."}, {"heading": "A List of notation", "text": "P Probability X, Y Random variables E [\u00b7] Expectation of a random variable O Big-O Notation EC Edge cost h Heuristic function g Accumulated path cost from start node Q Objective function D Maximum depth of search space pg Goal probability on a single target plane g pk Goal probability on a plane k Vector of probabilities on multiple target planes \u00b5, \u03c32 Target peak and target propagation in Gaussian binary tree structure. Probability that a target exists. Probability that level k has a target Fk. Probability that level k has the first target tBFSSGL."}, {"heading": "B List of search problems", "text": "The following is an incomplete list of problems that were naturally modeled as search problems, ordered by type. \u2022 Riddles - N Riddles - Instant Madness (Knuth 1975) - Eternity II (Assembly Riddles) \u2022 Infinite Grammar (also known as Production System; simpler PSVN) - STRIPS Planning (PDDL Language) - Root Factorial (Knuth) \u2022 Real World Problems - MDP-SAT-VLSI Chip Design (Cell Layout, Channel Guidance) - Robot Navigation (continuous) - Route Finding - Tour Finding - Montage Sequence - Protein Design - Jobshop (who does what when) \u2022 Other - Towers of Hanoi-Cannibal Mission - Sokoban-Rubiks Cube - Sudoku Knight Jumped - N Queens - Belief State (in a deterministic, partially observable world) - Quagroup Problem, Sukuankuk's Cube (Rudoobit)."}, {"heading": "C List of Topological features", "text": "The neighbourhood relationship N induces a topology to the state space S. The aim of this study is to examine in more detail how topological features of the search graph affect search performance. Below is a list of potentially useful features, with +,?, - classifying the features with a prioritization probability. Please refer to (Diestel, 2006) for a standard reference to graph theory and for explanations and definitions of lower terms. \u2022 Problem type + directed / undirected graph \u2022 State space + number of nodes (finite or infinite) - Number of edges \u2022 Graph structure - split / k partite + clip size distribution + clip size coverage number (how many cliques are required to cover the graph?) - Accordance (each cycle of length \u2265 4 has a \"chord\" x) - Stability number (largest number of non-connected nodes) + maximum number of nodes (maximum circumference + xx) x (maximum circumference + distance) x (maximum circumference + y) x) - Stability number of stability (greatest number of non-connected nodes + maximum number of nodes (maximum circumference + y) x (maximum distance)."}], "references": [{"title": "Learning During Search", "author": ["A. Arbelaez Rodriguez"], "venue": "Phd thesis, University of Paris-Sud", "citeRegEx": "Rodriguez,? \\Q2011\\E", "shortCiteRegEx": "Rodriguez", "year": 2011}, {"title": "Metaheuristics in Combinatorial Optimization: Overview and Conceptual Comparison", "author": ["C. Blum", "A. Roli"], "venue": "ACM Computing Surveys,", "citeRegEx": "Blum and Roli,? \\Q2003\\E", "shortCiteRegEx": "Blum and Roli", "year": 2003}, {"title": "Hyper Heuristics: an emerging direction in modern search technology", "author": ["E. Burke", "E. Hart", "G. Kendall", "J. Newall", "P. Ross", "S. Schulenburg"], "venue": "Handbook of Metaheuristics,", "citeRegEx": "Burke et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Burke et al\\.", "year": 2003}, {"title": "Hyper-heuristics: a survey of the state of the art", "author": ["E.K. Burke", "M. Gendreau", "M. Hyde", "G. Kendall", "G. Ochoa 1\u00e3", "E. Zcan", "R. Qu"], "venue": "Journal of the Operational Research Society,", "citeRegEx": "Burke et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Burke et al\\.", "year": 2013}, {"title": "Heuristic Sampling: A Method for Predicting the Performance of Tree Searching Programs", "author": ["P.C. Chen"], "venue": "SIAM Journal on Computing,", "citeRegEx": "Chen,? \\Q1992\\E", "shortCiteRegEx": "Chen", "year": 1992}, {"title": "Explanation-based Learning: An Alternative View", "author": ["G. Dejong", "R. Mooney"], "venue": "Machine Learning,", "citeRegEx": "Dejong and Mooney,? \\Q1986\\E", "shortCiteRegEx": "Dejong and Mooney", "year": 1986}, {"title": "Graph Theory (Graduate Texts in Mathematics)", "author": ["R. Diestel"], "venue": null, "citeRegEx": "Diestel,? \\Q2006\\E", "shortCiteRegEx": "Diestel", "year": 2006}, {"title": "Acquiring search-control knowledge via static analysis", "author": ["O. Etzioni"], "venue": "Artificial Intelligence,", "citeRegEx": "Etzioni,? \\Q1993\\E", "shortCiteRegEx": "Etzioni", "year": 1993}, {"title": "Analytical Results on the BFS vs. DFS Algorithm Selection Problem", "author": ["T. Everitt", "M. Hutter"], "venue": "Part I: Tree Search. In 28th Australian Joint Conference on Artificial Intelligence", "citeRegEx": "Everitt and Hutter,? \\Q2015\\E", "shortCiteRegEx": "Everitt and Hutter", "year": 2015}, {"title": "Analytical Results on the BFS vs. DFS Algorithm Selection Problem. Part II: Graph Search", "author": ["T. Everitt", "M. Hutter"], "venue": "In 28th Australian Joint Conference on Artificial Intelligence", "citeRegEx": "Everitt and Hutter,? \\Q2015\\E", "shortCiteRegEx": "Everitt and Hutter", "year": 2015}, {"title": "How to Solve It Automatically: Selection Among ProblemSolving Methods", "author": ["E. Fink"], "venue": "In Proceedings of the Fourth International Conference on Artificial Intelligence Planning Systems,", "citeRegEx": "Fink,? \\Q1998\\E", "shortCiteRegEx": "Fink", "year": 1998}, {"title": "Heavy-tailed distributions in combinatorial search", "author": ["C.P. Gomes", "B. Selman", "N. Crato"], "venue": "Principles and Practice of Constraint", "citeRegEx": "Gomes et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Gomes et al\\.", "year": 1997}, {"title": "Online estimation of SAT solving runtime", "author": ["S. Haim", "T. Walsh"], "venue": "In Theory and Applications of Satisfiability Testing,", "citeRegEx": "Haim and Walsh,? \\Q2008\\E", "shortCiteRegEx": "Haim and Walsh", "year": 2008}, {"title": "Programming by optimization", "author": ["H.H. Hoos"], "venue": "Communications of the ACM,", "citeRegEx": "Hoos,? \\Q2012\\E", "shortCiteRegEx": "Hoos", "year": 2012}, {"title": "Algorithm runtime prediction: Methods & evaluation", "author": ["F. Hutter", "L. Xu", "H.H. Hoos", "K. Leyton-Brown"], "venue": "Artificial Intelligence,", "citeRegEx": "Hutter et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Hutter et al\\.", "year": 2014}, {"title": "Estimating Search Tree Size", "author": ["P. Kilby", "J. Slaney", "S. Thi\u00e9baux", "T. Walsh"], "venue": "In Proc. of the 21st National Conf. of Artificial Intelligence,", "citeRegEx": "Kilby et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Kilby et al\\.", "year": 2006}, {"title": "Estimating the efficiency of backtrack programs", "author": ["D.E. Knuth"], "venue": "Mathematics of Computation,", "citeRegEx": "Knuth,? \\Q1975\\E", "shortCiteRegEx": "Knuth", "year": 1975}, {"title": "Time complexity of iterativedeepening-A", "author": ["R.E. Korf", "M. Reid", "S. Edelkamp"], "venue": "Artificial Intelligence,", "citeRegEx": "Korf et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Korf et al\\.", "year": 2001}, {"title": "Algorithm Selection for Combinatorial Search Problems: A Survey", "author": ["L. Kotthoff"], "venue": "AI Magazine,", "citeRegEx": "Kotthoff,? \\Q2014\\E", "shortCiteRegEx": "Kotthoff", "year": 2014}, {"title": "Fundaments of Branching Heuristics: Theory and Examples", "author": ["O. Kullmann"], "venue": "Technical report,", "citeRegEx": "Kullmann,? \\Q2008\\E", "shortCiteRegEx": "Kullmann", "year": 2008}, {"title": "Inductive learning of search control rules for planning", "author": ["C. Leckie", "I. Zukerman"], "venue": "Artificial Intelligence,", "citeRegEx": "Leckie and Zukerman,? \\Q1998\\E", "shortCiteRegEx": "Leckie and Zukerman", "year": 1998}, {"title": "Predicting the size of Depthfirst Branch and Bound search trees", "author": ["L.H.S. Lelis", "L. Otten", "R. Dechter"], "venue": "IJCAI International Joint Conference on Artificial Intelligence,", "citeRegEx": "Lelis et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Lelis et al\\.", "year": 2013}, {"title": "Learning Search Control Knowledge: An Explanation-Based Approach", "author": ["S. Minton"], "venue": null, "citeRegEx": "Minton,? \\Q1988\\E", "shortCiteRegEx": "Minton", "year": 1988}, {"title": "Quantitative results concerning the utility of explanationbased learning", "author": ["S. Minton"], "venue": "Artificial Intelligence,", "citeRegEx": "Minton,? \\Q1990\\E", "shortCiteRegEx": "Minton", "year": 1990}, {"title": "Explanation-based generalization: A unifying view", "author": ["T. Mitchell", "R. Keller", "S. Kedar-CabeUi"], "venue": "Machine Learning,", "citeRegEx": "Mitchell et al\\.,? \\Q1986\\E", "shortCiteRegEx": "Mitchell et al\\.", "year": 1986}, {"title": "Heuristics: Intelligent Search Strategies for Computer Problem Solving", "author": ["J. Pearl"], "venue": null, "citeRegEx": "Pearl,? \\Q1984\\E", "shortCiteRegEx": "Pearl", "year": 1984}, {"title": "Probabilistic Reasoning in Intelligent Systems: Networks of Plausible Inference", "author": ["J. Pearl"], "venue": null, "citeRegEx": "Pearl,? \\Q1988\\E", "shortCiteRegEx": "Pearl", "year": 1988}, {"title": "The graph-tool python library. figshare", "author": ["T.P. Peixoto"], "venue": null, "citeRegEx": "Peixoto,? \\Q2015\\E", "shortCiteRegEx": "Peixoto", "year": 2015}, {"title": "Tree Size by Partial Backtracking", "author": ["P.W. Purdom"], "venue": "SIAM Journal on Computing,", "citeRegEx": "Purdom,? \\Q1978\\E", "shortCiteRegEx": "Purdom", "year": 1978}, {"title": "The algorithm selection problem", "author": ["J.R. Rice"], "venue": "Advances in Computers,", "citeRegEx": "Rice,? \\Q1975\\E", "shortCiteRegEx": "Rice", "year": 1975}, {"title": "The diameter of the rubiks cube group is twenty", "author": ["T. Rokicki", "H. Kociemba"], "venue": "SIAM Journal on Discrete Mathematics,", "citeRegEx": "Rokicki and Kociemba,? \\Q2013\\E", "shortCiteRegEx": "Rokicki and Kociemba", "year": 2013}, {"title": "Hyperheuristics: learning to combine simple heuristics in bin-packing problems", "author": ["P. Ross", "S. Schulenburg", "J.G. Marin-Blazquez", "E. Hart"], "venue": null, "citeRegEx": "Ross et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Ross et al\\.", "year": 2002}, {"title": "Artificial intelligence: a modern approach", "author": ["S.J. Russell", "P. Norvig"], "venue": null, "citeRegEx": "Russell and Norvig,? \\Q2010\\E", "shortCiteRegEx": "Russell and Norvig", "year": 2010}, {"title": "Measuring instance difficulty for combinatorial optimization problems", "author": ["K. Smith-Miles", "L. Lopes"], "venue": "Computers and Operations Research,", "citeRegEx": "Smith.Miles and Lopes,? \\Q2012\\E", "shortCiteRegEx": "Smith.Miles and Lopes", "year": 2012}, {"title": "Metareasoning about propagators for constraint satisfaction", "author": ["C. Thompson"], "venue": "Phd thesis, University of Saskatchewan", "citeRegEx": "Thompson,? \\Q2011\\E", "shortCiteRegEx": "Thompson", "year": 2011}, {"title": "Laplacian Spectral Properties of Graphs from Random Local Samples", "author": ["Z. Wu", "V.M. Preciado"], "venue": null, "citeRegEx": "Wu and Preciado,? \\Q2013\\E", "shortCiteRegEx": "Wu and Preciado", "year": 2013}, {"title": "Predicting the performance of IDA* using conditional distributions", "author": ["U. Zahavi", "A. Felner", "N. Burch", "R.C. Holte"], "venue": "Journal of Artificial Intelligence Research,", "citeRegEx": "Zahavi et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Zahavi et al\\.", "year": 2010}], "referenceMentions": [{"referenceID": 32, "context": "A wide range of problems in artificial intelligence can be naturally formulated as search problems (Russell and Norvig, 2010; Edelkamp and Schr\u00f6dl, 2012).", "startOffset": 99, "endOffset": 153}, {"referenceID": 29, "context": "Predicting the best algorithm is sometimes known as the algorithm selection problem (Rice, 1975).", "startOffset": 84, "endOffset": 96}, {"referenceID": 18, "context": "A number of studies have approached the algorithm selection problem with machine learning techniques (Kotthoff, 2014; Hutter et al., 2014).", "startOffset": 101, "endOffset": 138}, {"referenceID": 14, "context": "A number of studies have approached the algorithm selection problem with machine learning techniques (Kotthoff, 2014; Hutter et al., 2014).", "startOffset": 101, "endOffset": 138}, {"referenceID": 14, "context": "A number of studies have approached the algorithm selection problem with machine learning techniques (Kotthoff, 2014; Hutter et al., 2014). While demonstrably a feasible path, machine learning tend to be used as a black box, offering little insight into why a certain method works better on a given problem. On the other hand, most existing analytical results focus on worst-case big-O analysis, which is often less useful than average-case analysis when selecting algorithm. An important worst-case result is Knuth\u2019s (1975) simple but useful technique", "startOffset": 118, "endOffset": 525}, {"referenceID": 12, "context": "Kilby et al. (2006) used it for algorithm selection in the SAT problem.", "startOffset": 0, "endOffset": 20}, {"referenceID": 12, "context": "Kilby et al. (2006) used it for algorithm selection in the SAT problem. See also the extensions by Purdom (1978), Chen (1992), and Lelis et al.", "startOffset": 0, "endOffset": 113}, {"referenceID": 4, "context": "See also the extensions by Purdom (1978), Chen (1992), and Lelis et al.", "startOffset": 42, "endOffset": 54}, {"referenceID": 4, "context": "See also the extensions by Purdom (1978), Chen (1992), and Lelis et al. (2013). Analytical IDA* runtime predictions based on problem features was obtained by Korf et al.", "startOffset": 42, "endOffset": 79}, {"referenceID": 4, "context": "See also the extensions by Purdom (1978), Chen (1992), and Lelis et al. (2013). Analytical IDA* runtime predictions based on problem features was obtained by Korf et al. (2001) and Zahavi et al.", "startOffset": 42, "endOffset": 177}, {"referenceID": 4, "context": "See also the extensions by Purdom (1978), Chen (1992), and Lelis et al. (2013). Analytical IDA* runtime predictions based on problem features was obtained by Korf et al. (2001) and Zahavi et al. (2010). In this study we focus on theoretical analysis of average runtime of BFS and DFS.", "startOffset": 42, "endOffset": 202}, {"referenceID": 1, "context": "Blum and Roli (2003) suggest that this offers a unified view of many search problems.", "startOffset": 0, "endOffset": 21}, {"referenceID": 1, "context": "Blum and Roli (2003) suggest that this offers a unified view of many search problems. A CSP formulation also seems largely in line with what Pearl (1988) has in mind, although Pearl is less specific.", "startOffset": 0, "endOffset": 154}, {"referenceID": 32, "context": "We here review only a subset of the more important ones, and refer to the books (Russell and Norvig, 2010; Edelkamp and Schr\u00f6dl, 2012) for more details.", "startOffset": 80, "endOffset": 134}, {"referenceID": 29, "context": "The algorithm selection problem asks what algorithm best to use on a given problem (Rice, 1975; Kotthoff, 2014).", "startOffset": 83, "endOffset": 111}, {"referenceID": 18, "context": "The algorithm selection problem asks what algorithm best to use on a given problem (Rice, 1975; Kotthoff, 2014).", "startOffset": 83, "endOffset": 111}, {"referenceID": 28, "context": "Several generalisations have been developed (Purdom, 1978; Chen, 1992).", "startOffset": 44, "endOffset": 70}, {"referenceID": 4, "context": "Several generalisations have been developed (Purdom, 1978; Chen, 1992).", "startOffset": 44, "endOffset": 70}, {"referenceID": 34, "context": "Schemes using much wider ranges of problem properties are applied to CSPs in (Thompson, 2011; Arbelaez Rodriguez, 2011), and to the NP-complete problems SAT, TSP and Mixed integer programming in (Hutter et al.", "startOffset": 77, "endOffset": 119}, {"referenceID": 14, "context": "Schemes using much wider ranges of problem properties are applied to CSPs in (Thompson, 2011; Arbelaez Rodriguez, 2011), and to the NP-complete problems SAT, TSP and Mixed integer programming in (Hutter et al., 2014).", "startOffset": 195, "endOffset": 216}, {"referenceID": 10, "context": "The most comprehensive surveys are given by Hutter et al. (2014) and Kotthoff (2014), and the PhD theses Thompson (2011) and Arbelaez Rodriguez (2011).", "startOffset": 44, "endOffset": 65}, {"referenceID": 10, "context": "The most comprehensive surveys are given by Hutter et al. (2014) and Kotthoff (2014), and the PhD theses Thompson (2011) and Arbelaez Rodriguez (2011).", "startOffset": 44, "endOffset": 85}, {"referenceID": 10, "context": "The most comprehensive surveys are given by Hutter et al. (2014) and Kotthoff (2014), and the PhD theses Thompson (2011) and Arbelaez Rodriguez (2011).", "startOffset": 44, "endOffset": 121}, {"referenceID": 0, "context": "(2014) and Kotthoff (2014), and the PhD theses Thompson (2011) and Arbelaez Rodriguez (2011). For DFS, Knuth (1975) made a simple but important observation how the branching factor seen during search can be used to estimate the size of the search tree and the runtime.", "startOffset": 76, "endOffset": 93}, {"referenceID": 0, "context": "(2014) and Kotthoff (2014), and the PhD theses Thompson (2011) and Arbelaez Rodriguez (2011). For DFS, Knuth (1975) made a simple but important observation how the branching factor seen during search can be used to estimate the size of the search tree and the runtime.", "startOffset": 76, "endOffset": 116}, {"referenceID": 0, "context": "(2014) and Kotthoff (2014), and the PhD theses Thompson (2011) and Arbelaez Rodriguez (2011). For DFS, Knuth (1975) made a simple but important observation how the branching factor seen during search can be used to estimate the size of the search tree and the runtime. Despite the simplicity of the scheme, the estimates work surprisingly well in practice. Several generalisations have been developed (Purdom, 1978; Chen, 1992). Kilby et al. (2006) generalise Knuth\u2019s method, and also use it to select search policy for the SAT problem based on which search policy has the lowest estimated runtime.", "startOffset": 76, "endOffset": 449}, {"referenceID": 0, "context": "(2014) and Kotthoff (2014), and the PhD theses Thompson (2011) and Arbelaez Rodriguez (2011). For DFS, Knuth (1975) made a simple but important observation how the branching factor seen during search can be used to estimate the size of the search tree and the runtime. Despite the simplicity of the scheme, the estimates work surprisingly well in practice. Several generalisations have been developed (Purdom, 1978; Chen, 1992). Kilby et al. (2006) generalise Knuth\u2019s method, and also use it to select search policy for the SAT problem based on which search policy has the lowest estimated runtime. Kullmann (2008) and Lelis et al.", "startOffset": 76, "endOffset": 615}, {"referenceID": 0, "context": "(2014) and Kotthoff (2014), and the PhD theses Thompson (2011) and Arbelaez Rodriguez (2011). For DFS, Knuth (1975) made a simple but important observation how the branching factor seen during search can be used to estimate the size of the search tree and the runtime. Despite the simplicity of the scheme, the estimates work surprisingly well in practice. Several generalisations have been developed (Purdom, 1978; Chen, 1992). Kilby et al. (2006) generalise Knuth\u2019s method, and also use it to select search policy for the SAT problem based on which search policy has the lowest estimated runtime. Kullmann (2008) and Lelis et al. (2013) both develop estimation schemes for branch-and-bound algorithms.", "startOffset": 76, "endOffset": 639}, {"referenceID": 0, "context": "(2014) and Kotthoff (2014), and the PhD theses Thompson (2011) and Arbelaez Rodriguez (2011). For DFS, Knuth (1975) made a simple but important observation how the branching factor seen during search can be used to estimate the size of the search tree and the runtime. Despite the simplicity of the scheme, the estimates work surprisingly well in practice. Several generalisations have been developed (Purdom, 1978; Chen, 1992). Kilby et al. (2006) generalise Knuth\u2019s method, and also use it to select search policy for the SAT problem based on which search policy has the lowest estimated runtime. Kullmann (2008) and Lelis et al. (2013) both develop estimation schemes for branch-and-bound algorithms. Haim and Walsh (2008) approach the SAT problem, and instead of branching factor use properties of the given formula (such as the number and the size of clauses) to be predict search time and best search policy.", "startOffset": 76, "endOffset": 726}, {"referenceID": 0, "context": "(2014) and Kotthoff (2014), and the PhD theses Thompson (2011) and Arbelaez Rodriguez (2011). For DFS, Knuth (1975) made a simple but important observation how the branching factor seen during search can be used to estimate the size of the search tree and the runtime. Despite the simplicity of the scheme, the estimates work surprisingly well in practice. Several generalisations have been developed (Purdom, 1978; Chen, 1992). Kilby et al. (2006) generalise Knuth\u2019s method, and also use it to select search policy for the SAT problem based on which search policy has the lowest estimated runtime. Kullmann (2008) and Lelis et al. (2013) both develop estimation schemes for branch-and-bound algorithms. Haim and Walsh (2008) approach the SAT problem, and instead of branching factor use properties of the given formula (such as the number and the size of clauses) to be predict search time and best search policy. In the case of informed search, Korf et al. (2001) developed an interesting analytic technique for estimating the search time of IDA*.", "startOffset": 76, "endOffset": 966}, {"referenceID": 0, "context": "(2014) and Kotthoff (2014), and the PhD theses Thompson (2011) and Arbelaez Rodriguez (2011). For DFS, Knuth (1975) made a simple but important observation how the branching factor seen during search can be used to estimate the size of the search tree and the runtime. Despite the simplicity of the scheme, the estimates work surprisingly well in practice. Several generalisations have been developed (Purdom, 1978; Chen, 1992). Kilby et al. (2006) generalise Knuth\u2019s method, and also use it to select search policy for the SAT problem based on which search policy has the lowest estimated runtime. Kullmann (2008) and Lelis et al. (2013) both develop estimation schemes for branch-and-bound algorithms. Haim and Walsh (2008) approach the SAT problem, and instead of branching factor use properties of the given formula (such as the number and the size of clauses) to be predict search time and best search policy. In the case of informed search, Korf et al. (2001) developed an interesting analytic technique for estimating the search time of IDA*. Assuming a consistent heuristic function, the estimate is based on the distribution of heuristic function values at different depths of the search tree, rather than heuristic accuracy. Intuitively, the scheme works because the number of nodes expanded in each iteration of IDA* depends on the number of nodes with heuristic value less than the threshold. The distribution of heuristic values is also easy to estimate in practice. Zahavi et al. (2010) generalise the work of Korf et al.", "startOffset": 76, "endOffset": 1501}, {"referenceID": 0, "context": "(2014) and Kotthoff (2014), and the PhD theses Thompson (2011) and Arbelaez Rodriguez (2011). For DFS, Knuth (1975) made a simple but important observation how the branching factor seen during search can be used to estimate the size of the search tree and the runtime. Despite the simplicity of the scheme, the estimates work surprisingly well in practice. Several generalisations have been developed (Purdom, 1978; Chen, 1992). Kilby et al. (2006) generalise Knuth\u2019s method, and also use it to select search policy for the SAT problem based on which search policy has the lowest estimated runtime. Kullmann (2008) and Lelis et al. (2013) both develop estimation schemes for branch-and-bound algorithms. Haim and Walsh (2008) approach the SAT problem, and instead of branching factor use properties of the given formula (such as the number and the size of clauses) to be predict search time and best search policy. In the case of informed search, Korf et al. (2001) developed an interesting analytic technique for estimating the search time of IDA*. Assuming a consistent heuristic function, the estimate is based on the distribution of heuristic function values at different depths of the search tree, rather than heuristic accuracy. Intuitively, the scheme works because the number of nodes expanded in each iteration of IDA* depends on the number of nodes with heuristic value less than the threshold. The distribution of heuristic values is also easy to estimate in practice. Zahavi et al. (2010) generalise the work of Korf et al. to non-consistent heuristics. Many other approaches instead try to directly infer the best search policy, without the intermediate step of estimating runtime. Fink (1998) does this for STRIPS-like learning using only the problem size to infer which method is likely to be more efficient.", "startOffset": 76, "endOffset": 1707}, {"referenceID": 0, "context": "(2014) and Kotthoff (2014), and the PhD theses Thompson (2011) and Arbelaez Rodriguez (2011). For DFS, Knuth (1975) made a simple but important observation how the branching factor seen during search can be used to estimate the size of the search tree and the runtime. Despite the simplicity of the scheme, the estimates work surprisingly well in practice. Several generalisations have been developed (Purdom, 1978; Chen, 1992). Kilby et al. (2006) generalise Knuth\u2019s method, and also use it to select search policy for the SAT problem based on which search policy has the lowest estimated runtime. Kullmann (2008) and Lelis et al. (2013) both develop estimation schemes for branch-and-bound algorithms. Haim and Walsh (2008) approach the SAT problem, and instead of branching factor use properties of the given formula (such as the number and the size of clauses) to be predict search time and best search policy. In the case of informed search, Korf et al. (2001) developed an interesting analytic technique for estimating the search time of IDA*. Assuming a consistent heuristic function, the estimate is based on the distribution of heuristic function values at different depths of the search tree, rather than heuristic accuracy. Intuitively, the scheme works because the number of nodes expanded in each iteration of IDA* depends on the number of nodes with heuristic value less than the threshold. The distribution of heuristic values is also easy to estimate in practice. Zahavi et al. (2010) generalise the work of Korf et al. to non-consistent heuristics. Many other approaches instead try to directly infer the best search policy, without the intermediate step of estimating runtime. Fink (1998) does this for STRIPS-like learning using only the problem size to infer which method is likely to be more efficient. Schemes using much wider ranges of problem properties are applied to CSPs in (Thompson, 2011; Arbelaez Rodriguez, 2011), and to the NP-complete problems SAT, TSP and Mixed integer programming in (Hutter et al., 2014). Smith-Miles and Lopes (2012) review and discuss commonly used features for the algorithm selection problem, mainly applied to the local search scenario.", "startOffset": 76, "endOffset": 2071}, {"referenceID": 5, "context": "Explanation-based Learning (EBL) (Dejong and Mooney, 1986; Mitchell et al., 1986; Minton, 1988) is a general method for learning from examples and domain knowledge.", "startOffset": 33, "endOffset": 95}, {"referenceID": 24, "context": "Explanation-based Learning (EBL) (Dejong and Mooney, 1986; Mitchell et al., 1986; Minton, 1988) is a general method for learning from examples and domain knowledge.", "startOffset": 33, "endOffset": 95}, {"referenceID": 22, "context": "Explanation-based Learning (EBL) (Dejong and Mooney, 1986; Mitchell et al., 1986; Minton, 1988) is a general method for learning from examples and domain knowledge.", "startOffset": 33, "endOffset": 95}, {"referenceID": 22, "context": "While attractive, it can also lead to overspecific learning (Minton, 1988).", "startOffset": 60, "endOffset": 74}, {"referenceID": 7, "context": "Partial Evaluation (PE) is an alternative learning method that is more robust in this respect, with less dependency on examples (Etzioni, 1993).", "startOffset": 128, "endOffset": 143}, {"referenceID": 3, "context": "Some research is also being done on automatic construction of low-level heuristics (see (Burke et al., 2013) for references).", "startOffset": 88, "endOffset": 108}, {"referenceID": 13, "context": "A related approach directed at programming in general is programming by optimisation (Hoos, 2012), where machine learning techniques are used to find the best algorithm in a space of programs delineated by the human programmer.", "startOffset": 85, "endOffset": 97}, {"referenceID": 5, "context": "Partial Evaluation (PE) is an alternative learning method that is more robust in this respect, with less dependency on examples (Etzioni, 1993). Leckie and Zukerman (1998) develops a more inductive way to learn search control knowledge (in contrast to the deductive generalisations performed by EBL and PE), where plenty of training examples substitute for domain knowledge.", "startOffset": 129, "endOffset": 172}, {"referenceID": 2, "context": "A more modern approach is known as hyper heuristics (Burke et al., 2003, 2013). It views the problem of inferring good search policies more abstractly. Rather than interacting with the neighbourhood structure/graph problem directly, the hyper heuristic only has access to a set of search policies for the original graph problem. The search policies are known as low-level heuristics in this literature (not to be confused with heuristic functions). The goal of the hyper heuristic is to find a good policy for when to apply which low-level heuristic. For example, Ross et al. (2002) used Genetic Algorithms to learn which binpacking heuristic to apply in which type of state in a bin-packing problem.", "startOffset": 53, "endOffset": 583}, {"referenceID": 30, "context": "There is an upper bound D = 20 to how many moves it can take to reach the goal (Rokicki and Kociemba, 2013).", "startOffset": 79, "endOffset": 107}, {"referenceID": 17, "context": "Korf et al. (2001) study", "startOffset": 0, "endOffset": 19}, {"referenceID": 27, "context": "To verify the analytical results, we have implemented the models Sections 5\u20137 in Python 3 using the graph-tool package (Peixoto, 2015).", "startOffset": 119, "endOffset": 134}, {"referenceID": 16, "context": "Earlier studies have only addressed worst-case runtimes: Knuth (1975) and followers for DFS; Korf et al.", "startOffset": 57, "endOffset": 70}, {"referenceID": 16, "context": "Earlier studies have only addressed worst-case runtimes: Knuth (1975) and followers for DFS; Korf et al. (2001) and followers for IDA*, effectively a generalised version of BFS.", "startOffset": 57, "endOffset": 112}, {"referenceID": 16, "context": ") There is good hope that the descendant counter L can be estimated online from the local sample obtained during search, similar to (Knuth, 1975).", "startOffset": 132, "endOffset": 145}, {"referenceID": 25, "context": "The goal distribution is likely to prove more challenging, but resembles the automatic creation of heuristic functions, so techniques such as relaxed problems could well prove useful (Pearl, 1984).", "startOffset": 183, "endOffset": 196}], "year": 2015, "abstractText": "Search is a central problem in artificial intelligence, and BFS and DFS the two most fundamental ways to search. In this report we derive results for average BFS and DFS runtime: For tree search, we employ a probabilistic model of goal distribution; for graph search, the analysis depends on an additional statistic of path redundancy and average branching factor. As an application, we use the results on two concrete grammar problems. The runtime estimates can be used to select the faster out of BFS and DFS for a given problem, and may form the basis for further analysis of more advanced search methods. Finally, we verify our results experimentally; the analytical approximations come surprisingly close to empirical reality.", "creator": "LaTeX with hyperref package"}}}