{"id": "1509.03044", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "10-Sep-2015", "title": "Recurrent Reinforcement Learning: A Hybrid Approach", "abstract": "Successful applications of reinforcement learning in real-world problems often require dealing with partially observable states. It is in general very challenging to construct and infer hidden states as they often depend on the agent's entire interaction history and may require substantial domain knowledge. In this work, we investigate a deep-learning approach to learning the representation of states in partially observable tasks, with minimal prior knowledge of the domain. In particular, we study reinforcement learning with deep neural networks, including RNN and LSTM, which are equipped with the desired property of being able to capture long-term dependency on history, and thus providing an effective way of learning the representation of hidden states. We further develop a hybrid approach that combines the strength of both supervised learning (for representing hidden states) and reinforcement learning (for optimizing control) with joint training. Extensive experiments based on a KDD Cup 1998 direct mailing campaign problem demonstrate the effectiveness and advantages of the proposed approach, which performs the best across the board.", "histories": [["v1", "Thu, 10 Sep 2015 07:45:30 GMT  (129kb,D)", "http://arxiv.org/abs/1509.03044v1", "8 pages, 3 figures"], ["v2", "Thu, 19 Nov 2015 19:32:08 GMT  (2648kb,D)", "http://arxiv.org/abs/1509.03044v2", "11 pages, 6 figures"]], "COMMENTS": "8 pages, 3 figures", "reviews": [], "SUBJECTS": "cs.LG cs.AI cs.SY", "authors": ["xiujun li", "lihong li", "jianfeng gao", "xiaodong he", "jianshu chen", "li deng", "ji he"], "accepted": false, "id": "1509.03044"}, "pdf": {"name": "1509.03044.pdf", "metadata": {"source": "CRF", "title": "Recurrent Reinforcement Learning: A Hybrid Approach", "authors": ["Xiujun Li", "Lihong Li", "Jianfeng Gao", "Xiaodong He", "Jianshu Chen", "Li Deng", "Ji He"], "emails": ["lixiujun@cs.wisc.edu", "deng}@microsoft.com", "jvking@uw.edu"], "sections": [{"heading": "1 Introduction", "text": "In fact, it is such that most people will be able to move into another world, in which they are able to live, in which they are able to live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they, in which they, in which they, in which they live, in which they, in which they, in which they live, in which they, in which they, in which they, in which they live, in which they, in which they, in which they live, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, live, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, live, in which they, in which they, in which they, in which they, live, in which they, in which they, in which they, live, in which they, live, in which they, in which they, in which they, live, in which they, in which they, in which they, in which they, in which they, live, in fact, in which they, in which they, in fact, in which they, in which they, live, in which they, live, in which they, in which they, in fact, are able to be able to put themselves, in a different world,"}, {"heading": "2 Background and Related Work", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "2.1 Reinforcement Learning", "text": "In Amplification Learning, an agent uses observation and rewards to learn an (approximate) optimal policy for an environment that maximizes the expected total reward. Formally, in discrete steps t = 1, 2, 3,.. the agent obtains an observation ot-O, takes an action at A, and receives a real reward rt-R, where O, A, and R are the sets of observations, actions, and rewards, respectively. Let-ht = (o1, a1, r1,.) ot-1, at-1, rt-1, ot) an interaction story rt-R, where O and R are the sets of observations, actions, and rewards. The task of amplification learning is for the agent to learn an optimal action selection policy, so that the discounted cumulative reward, R = compensated cumulative reward t-1rt, is maximized, for a given discount factor."}, {"heading": "2.2 Deep (Reinforcement) Learning", "text": "Most well known is the recent use of a deep Q network (DQN) in Q-Learning to solve a large number of Atari games (Mnih et al. 2015), although neural networks were used in some of the classic RL applications such as TD-Gammon (Tesauro 1995). For partially observable environments, deep learning can also be used to construct and track hidden states, even if the hidden states in a deep network are not readily interpretable. For example, Deng et al. (2013) applied a deep network to track beliefs in a spoken dialogue system. Previous examples include applications of recursive neural networks (LSTM and RNN) to control problems (Bakker 2002; Lin 1993). More recently, DQN has expanded to include deep recurrent QM learning for POMDPs in Atari games."}, {"heading": "2.3 Customer Relationship Management", "text": "In general, Customer Relationship Management (CRM) refers to \"the practice of analyzing and using marketing databases and using communications technologies to determine business practices and methods that maximize the value of each individual customer's life\" (Kumar and Reinartz 2012). At the heart of the definition of CRM is the notion of a customer's life value (LTV) (Dwyer 1997), as opposed to short-term, or short-term, measures of customer value. In the past, data mining has been applied to CRM (Berry and Linoff 2004) in a number of important applications, such as segmenting customers according to their behavior, identifying potentially risky customers, etc. In these cases, data mining provides valuable insights (such as clustering and classification) to support business decisions."}, {"heading": "3 Models", "text": "Remember that our goal is to learn the optimal Q function from a sequence (or sequence of) interaction stories in the form of (o1, a1, r1, o2, a2, r2,..). Once a good approximation of the optimal Q function is achieved, it is easy to define a near-optimal policy that greedily selects actions. At this point, we describe three families of approaches: the first is a baseline for supervised learning (SL) that ignores long-term impacts on customers and thus effectively optimizes short-term (short-sighted) customer value; the second uses neural networks to represent the Q function, including DQN, RLRNN, and RL-LSTM. They represent state-of-the-art baseline lines that have found many successes in a variety of problems recently."}, {"heading": "3.1 Supervised Learning", "text": "In our experiments we formulated the problem as regression with the raw reward signal as the target. This reduction leads to standard deep learning models with the mean square error as the loss function for the training. Multiple network architectures are used, with and without built-in modelling of long-term dependence: \u2022 Multilayer (deep) neural networks (DNN) break into the interaction history of individual transitions, {(ot, at, rt, ot + 1)} t = 1,2,.... The network is learned to predict rt based on (ot, at), for rt > prognostics. In our experiments we found that the interaction history can be divided into individual transitions {(ot, rt, ot + 1)."}, {"heading": "3.2 Reinforcement Learning", "text": "In contrast, reinforcement learning takes into account future rewards and aims to directly optimize the overall long-term reward, making reinforcement learning particularly suitable for tasks such as CRM when trying to optimize customers \"LTV.Once a good Q network is learned, it can be used to greedily select actions: E-Qs: = argmaxaQ (s, a) and optimize network parameters to obtain an approximate Q function, Q (s, a). Once a good Q network is learned, it can be used to greedily select actions: E-Qs: = argmaxaQ (s, a)."}, {"heading": "3.3 SL+RL Hybrid Models", "text": "By contrast, SL models can be optimized to predict observations and immediate rewards, and thus have the potential to better represent and derive hidden states. With such complementary strengths, it is beneficial to take a hybrid approach that allows the RL component to maximize long-term rewards. 1Contextual window-based method can be used to capture a fixed window interaction story for ot, but does not work well in our simulation settings. To this end, we propose a new family of hybrid models that combine verified learning and reinforcement learning, where the SL component can be an RNN or LSTM; the RL component is a DQN component."}, {"heading": "4 Experiments", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.1 DataSet", "text": "In this paper, we use the 1998 KDD Cup Direct Mailing dataset2, which was used in the RL literature (Marivate 2015; Pednault, Abe, and Zadrozny 2002) It was collected by a non-profit organization, PVA, which provides programs and services for U.S. veterans with spinal cord injuries or diseases. PVA raises money through direct mailing campaigns. The record contains one record for each donor who received the mailing and did not make a donation in the previous 12 months. For each of them, it records whether and how much he has donated in response to the campaigns, apart from providing data on the previous and current mailing campaign as well as personal information and donation history of each expired donor. Training data is collected for 23 different periods for 95, 412 donors, resulting in more than 2M transitional stuples."}, {"heading": "4.2 Evaluation Methodology", "text": "This year, it has come to the point where it will be able to find a solution that is capable of finding a solution that is capable of finding a solution that is capable of finding a solution and that is able to find a solution that is capable of finding a solution that is capable of finding a solution that is capable of finding a solution that is capable of finding a solution that is capable of finding a solution that is capable of finding a solution that is capable of finding a solution. \""}, {"heading": "4.3 Experiment Setup", "text": "We found that smaller data is sufficient to take strong action, so we used only a random subset of donors of the total data for experiments. We tested four data sizes of different numbers of donors, each with 23 steps, so that the total number of transitions is {50K, 100K, 200K, 500K}. Subsequently, the data was divided into training, validation, and test kits with proportions of 4: 1: 1. To generate training data, we started with the initial donor observation vector in the training set and implemented one of the following data collection guidelines for selecting measures: \u2022 Uniformal random policy (U): each step uniformly randomly selected the measures from the measures. \u2022 Probability-Matching Policy (M): at each step, we evaluated the measures with the probability p (a), the frequency with which a measure occurs in raw data. \u2022 Real Policy (R): The measure is the real measure that is recorded in the raw data."}, {"heading": "4.4 Results", "text": "This year, it is closer than ever before in the history of the country."}, {"heading": "5 Conclusions", "text": "In this paper, we have investigated how to use deep learning models for both supervised learning and reinforcement learning to solve a CRM task typical of non-Markovian problems in the real world. Specifically, we have investigated how to use monitored signals in training data to learn hidden state representations, and then jointly train a deep Q network (using reinforcement learning) to optimize control to maximize long-term rewards. By conducting large-scale experimental analysis under various conditions, we have shown that: (1) Deep RL is more effective than SL for optimizing lifelong values; (2) RL using RNN / LSTM models is a promising approach to solving non-Markovian tasks with long-term dependencies; (3) It is promising to use memory network models to learn hidden-state representations in a supervised learning mode, with QN being jointly trained for non-Markovian tasks with long-term dependencies."}], "references": [{"title": "G", "author": ["M.J. Berry", "Linoff"], "venue": "S.", "citeRegEx": "Berry and Linoff 2004", "shortCiteRegEx": null, "year": 2004}, {"title": "and Yu", "author": ["L. Deng"], "venue": "D.", "citeRegEx": "Deng and Yu 2014", "shortCiteRegEx": null, "year": 2014}, {"title": "Recent advances in deep learning for speech research at Microsoft", "author": ["Deng"], "venue": "In In Acoustics, Speech and Signal Processing 2013 (ICASSP-13)", "citeRegEx": "Deng,? \\Q2013\\E", "shortCiteRegEx": "Deng", "year": 2013}, {"title": "F", "author": ["Dwyer"], "venue": "R.", "citeRegEx": "Dwyer 1997", "shortCiteRegEx": null, "year": 1997}, {"title": "and Stone", "author": ["M. Hausknecht"], "venue": "P.", "citeRegEx": "Hausknecht and Stone 2015", "shortCiteRegEx": null, "year": 2015}, {"title": "and Schmidhuber", "author": ["S. Hochreiter"], "venue": "J.", "citeRegEx": "Hochreiter and Schmidhuber 1997", "shortCiteRegEx": null, "year": 1997}, {"title": "A", "author": ["L.P. Kaelbling", "M.L. Littman", "Cassandra"], "venue": "R.", "citeRegEx": "Kaelbling. Littman. and Cassandra 1998", "shortCiteRegEx": null, "year": 1998}, {"title": "and Reinartz", "author": ["V. Kumar"], "venue": "W.", "citeRegEx": "Kumar and Reinartz 2012", "shortCiteRegEx": null, "year": 2012}, {"title": "and Parr", "author": ["M.G. Lagoudakis"], "venue": "R.", "citeRegEx": "Lagoudakis and Parr 2003", "shortCiteRegEx": null, "year": 2003}, {"title": "Reinforcement Learning for Robots using Neural Networks", "author": ["Lin", "L.-J"], "venue": "Ph.D. Dissertation,", "citeRegEx": "Lin and L..J.,? \\Q1993\\E", "shortCiteRegEx": "Lin and L..J.", "year": 1993}, {"title": "V", "author": ["Marivate"], "venue": "N.", "citeRegEx": "Marivate 2015", "shortCiteRegEx": null, "year": 2015}, {"title": "A", "author": ["V. Mnih", "K. Kavukcuoglu", "D. Silver", "A.A. Rusu", "J. Veness", "M.G. Bellemare", "A. Graves", "M. Riedmiller", "Fidjeland"], "venue": "K.; Ostrovski, G.; Petersen, S.; Beattie, C.; Sadik, A.; Antonoglou, I.; King, H.; Kumaran, D.; Wierstra, D.; Legg, S.; and Hassabis, D.", "citeRegEx": "Mnih et al. 2015", "shortCiteRegEx": null, "year": 2015}, {"title": "Language understanding for text-based games using deep reinforcement learning", "author": ["Kulkarni Narasimhan", "K. Barzilay 2015] Narasimhan", "T. Kulkarni", "R. Barzilay"], "venue": "In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP-15)", "citeRegEx": "Narasimhan et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Narasimhan et al\\.", "year": 2015}, {"title": "J", "author": ["Netzer, O.", "Lattin"], "venue": "M.; and Srinivasan, V.", "citeRegEx": "Netzer. Lattin. and Srinivasan 2008", "shortCiteRegEx": null, "year": 2008}, {"title": "Action-conditional video prediction using deep networks in Atari games. arXiv preprint arXiv:1507.06527", "author": ["Oh"], "venue": null, "citeRegEx": "Oh,? \\Q2015\\E", "shortCiteRegEx": "Oh", "year": 2015}, {"title": "E", "author": ["Pednault"], "venue": "P. D.; Abe, N.; and Zadrozny, B.", "citeRegEx": "Pednault. Abe. and Zadrozny 2002", "shortCiteRegEx": null, "year": 2002}, {"title": "G", "author": ["Pineau, J.", "Gordon"], "venue": "J.; and Thrun, S.", "citeRegEx": "Pineau. Gordon. and Thrun 2003", "shortCiteRegEx": null, "year": 2003}, {"title": "Concurrent reinforcement learning from customer interactions", "author": ["Silver"], "venue": "In Proceedings of the Thirtieth International Conference on Machine Learning", "citeRegEx": "Silver,? \\Q2013\\E", "shortCiteRegEx": "Silver", "year": 2013}, {"title": "A", "author": ["R.S. Sutton", "Barto"], "venue": "G.", "citeRegEx": "Sutton and Barto 1998", "shortCiteRegEx": null, "year": 1998}, {"title": "P", "author": ["Theocharous, G.", "Thomas"], "venue": "S.; and Ghavamzadeh, M.", "citeRegEx": "Theocharous. Thomas. and Ghavamzadeh 2015", "shortCiteRegEx": null, "year": 2015}, {"title": "S", "author": ["J.D. Williams", "Young"], "venue": "J.", "citeRegEx": "Williams and Young 2007", "shortCiteRegEx": null, "year": 2007}], "referenceMentions": [], "year": 2015, "abstractText": "Successful applications of reinforcement learning in realworld problems often require dealing with partially observable states. It is in general very challenging to construct and infer hidden states as they often depend on the agent\u2019s entire interaction history and may require substantial domain knowledge. In this work, we investigate a deep-learning approach to learning the representation of states in partially observable tasks, with minimal prior knowledge of the domain. In particular, we study reinforcement learning with deep neural networks, including RNN and LSTM, which are equipped with the desired property of being able to capture long-term dependency on history, and thus providing an effective way of learning the representation of hidden states. We further develop a hybrid approach that combines the strength of both supervised learning (for representing hidden states) and reinforcement learning (for optimizing control) with joint training. Extensive experiments based on a KDD Cup 1998 direct mailing campaign problem demonstrate the effectiveness and advantages of the proposed approach, which performs the best across the board.", "creator": "LaTeX with hyperref package"}}}