{"id": "1509.03200", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "19-Jun-2015", "title": "A new Initial Centroid finding Method based on Dissimilarity Tree for K-means Algorithm", "abstract": "Cluster analysis is one of the primary data analysis technique in data mining and K-means is one of the commonly used partitioning clustering algorithm. In K-means algorithm, resulting set of clusters depend on the choice of initial centroids. If we can find initial centroids which are coherent with the arrangement of data, the better set of clusters can be obtained. This paper proposes a method based on the Dissimilarity Tree to find, the better initial centroid as well as every bit more accurate cluster with less computational time. Theory analysis and experimental results indicate that the proposed method can effectively improve the accuracy of clusters and reduce the computational complexity of the K-means algorithm.", "histories": [["v1", "Fri, 19 Jun 2015 11:43:14 GMT  (663kb)", "http://arxiv.org/abs/1509.03200v1", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["abhishek kumar", "suresh chandra gupta"], "accepted": false, "id": "1509.03200"}, "pdf": {"name": "1509.03200.pdf", "metadata": {"source": "CRF", "title": null, "authors": [], "emails": [], "sections": [{"heading": null, "text": "In fact, the fact is that most of them will be able to be in a position to be in a position, and that they will be able to be in a position to be in a position to put themselves in a position, to put themselves in a position, to put themselves in a position, to put themselves in a position, to put themselves in a position, to put themselves in a position, to put themselves in a position, to put themselves in a position where they are."}, {"heading": "II. RELATED WORK", "text": "The standard K mean algorithm is really sensitive to initial centric algorithms [5]. Several methods have been proposed to find the better initial centric algorithm [7] [8] [9]. Some methods also aimed to improve both the efficiency and accuracy of the K mean cluster technique [10]. M. Fahim et al. [7] proposed an algorithm that requires less execution time compared to the K mean cluster technique. In [7] the author proposed to maintain the distance to the next cluster of the previous iteration and to use it to compare it with the distance of new centric objects in the next iteration. If the current distance is smaller or equal to the previous one, the data object remains in its cluster and there is no need to recalculate its distances from the other cluster centroids."}, {"heading": "III. K-MEAN ALGORITHM", "text": "K-Means is a partitioning type cluster algorithm used in data mining, and it is one of the most popular, simplest, and most unattended learning algorithms [11]. The basic concept of this algorithm is to group the given records D into k Number of fragmented clusters. Step 2 consists of two basic steps [5]. The first step is to randomly select the K-initial centroids for each cluster. Where k is the number of clusters. Step 2 is to bring all the data objects in the dataset to the nearby centroids [5]. Euclidean distance is largely used to calculate the length between all data objects and the centroids. When each of the data objects is inserted into some clusters, the initial grouping takes place. Afterwards, the centroid of all clusters is estimated again by updating the average value of all data objects of all clusters. Some data objects may update to their clusters."}, {"heading": "IV. PROPOSED ALGORITHM", "text": "The dissimilarity between the objects a and b is defined as: m ff abm ff ab fab d badist11 2) (whereby 0fab, if either xf or yf is missing or xf = yf = 0; otherwise, 1fxxy. f xyd is defined as: fhfhfffab 5.0525.0525x2525x25x25x25x25x25x25x25x25x25x25x25x25x25x25x25x25x25x25x25x25x25x25x25x25x25x25x25x25x25x25x25x25x25x25x25x25x25x25x25x25x25x25x25x25x25x25x25x25x25x25x25x25x25x25x25x25x25x25x25x25x25x25x25x25x25x25x25x25x25x25x25x25x25x25x25x25x25x25x25x25x25x25x25x25x25x25x25x25x25x25x25x25x25x25x25x25x25x25x25x25x25x25x25x25x25x25x25x25x25x25x25x25x25x25x25x25x25x25x25x25x25x25x25x25x25x25x25x25x25x25x25x25x25x25x25x25x25x25x25x25x25x25x25x25x25x25x25x25x25x25x25x25x25x25x25x25x25x25x25x25x25x25x25x25x25x25x25x25x25x25x25x25x25x25x25x25x25x25x25x25x25x25x25x25x25x25x25x25x25x25x25x25x25x25x25x25x25x25x25x25x25x25x25x25x25x25x25x25x25x25x25x"}, {"heading": "V. EXPERIMENTAL RESULTS", "text": "The experimental environment in this context was AMD CPU, 2G EMS Memory, 500 GB hard disk and Windows 8.1 OS. MATLAB 2013a is used as a tool to verify the validity of an improved algorithm. In this paper, the multivariate datasets selected from the UCI Machine Learning Repository are selected, databases [12] used to test both the efficiency and accuracy of the proposed K-mean algorithm. The same datasets are used as input for the standard algorithm and modified algorithm. Experiment compares the accuracy and total runtime of the cluster sets of both algorithms. Both algorithms require the number of desired clusters as input. Standard K-mean algorithm, the initial centering will be random, but modified algorithms will automatically calculate the initial centering and show optimal cluster execution by the program."}, {"heading": "VI. CONCLUSION AND FUTURE WORK", "text": "The K-mean algorithm is widely used in many fields of application, but in this method the quality of the final clusters depends heavily on the initial centrifuge chosen at random. The modified algorithm produces cluster results more accurately and efficiently than the standard K-mean algorithm. This paper presents a dissimilarity tree algorithm that calculates the initial centrifuges that reduce the number of iterations in the K-mean algorithm. One limitation of this algorithm is that it still uses the desired number of clusters as input. Automatic determination of cluster numbers according to the distribution of data objects in data sets is proposed as future work."}], "references": [{"title": "Fine particles, thin films andex change anisotropy", "author": ["Sun Shibao", "Qin Keyun", "\u201cResearch on Modified k-means Data Cluster Algorithm\u201d I.S. Jacobs", "C.P. Bean"], "venue": "Computer Engineering, vol.33, No.13, pp.200\u2013201,July 2007.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2007}, {"title": "Enhancing K-means Clustering Algorithm with Improved Initial Center", "author": ["Madhu Yedla", "Srinivasa Rao Pathakota", "T M Srinivasa"], "venue": "International Journal of Computer Science and Information Technologies (IJCSIT), vol. 1(2), 2010, 121-125.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2010}, {"title": "Convergence Properties of the K-means Algorithms", "author": ["Leon Bottou", "Neuristique", "Yoshua Bengio"], "venue": "Dept, IRO, IESI- CNR.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 0}, {"title": "Data Mining Concepts and Technique", "author": ["J. Han", "M. Kamber"], "venue": null, "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2001}, {"title": "An Improved PAM Algorithm for Optimizing Initial Cluster Centre", "author": ["Feng Bo", "Hao Wenning", "Chen Gang", "Jin Dawei", "Zhao Shuining"], "venue": "IEEE, 2012, 978-1-4673-2008- 5/12.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2012}, {"title": "An Efficient Enhanced K-means Clustering Algorithm", "author": ["A.M. Fahim", "A.M. Salem", "F.A. Torkey", "M.A. Ranadan"], "venue": "Journel of Zejiang University, 10(7): 16261633, 2006.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2006}, {"title": "Divisive Correlation Clustering Algorithm (DCCA) for Grouping of Genes: detecting varying patterns in expression profiles", "author": ["A. Bhattacharya", "R.K. De"], "venue": "Bioinformatics, vol. 24, pp. 1359-1366, 2008.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2008}, {"title": "A New Algorithm to Get the Initial Centroids", "author": ["F. Yuan", "Z.H. Meng", "H.X. Zhangz", "C.R. Dong"], "venue": "proceedings of the 3 International Conference on Machine Learning and Cypernetics, pp. 26-29, August 2004.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2004}, {"title": "Improvement of K-means Clustering Algorithm with Better Initial Centroids based on Weighted Average", "author": ["S. Mahmud", "M. Rahman", "N. Akhtar"], "venue": "7 International Conference on Electrical and Computer Engineering, Dhaka, PP. 647-650, 20-222 December 2012,", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2012}, {"title": "Clustering Algorithm Research", "author": ["Sun Jigui", "Liu Jie", "Zhao Lianyu"], "venue": "Journal of software, vol. 19, no 1, pp. 48-61, January 2008.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2008}], "referenceMentions": [{"referenceID": 0, "context": "Clustering is widely practiced in numerous arenas, for example image processing, machine learning, marketing, medicines, data compression, information retrieval and so on [1].", "startOffset": 171, "endOffset": 174}, {"referenceID": 1, "context": "Numerous methods are available to solve clustering based problems [3].", "startOffset": 66, "endOffset": 69}, {"referenceID": 2, "context": "[4].", "startOffset": 0, "endOffset": 3}, {"referenceID": 3, "context": "Number of iterations needed while executing the K-means clustering algorithms and efficiency are also depends on the initial centroids [5].", "startOffset": 135, "endOffset": 138}, {"referenceID": 4, "context": "This study is motivated by [6] in which a TP (Tree Pruning) algorithm is presented to tackle the problem of the initial medoids of PAM algorithm.", "startOffset": 27, "endOffset": 30}, {"referenceID": 3, "context": "The standard K-means algorithm is really sensitive to initial centroid [5].", "startOffset": 71, "endOffset": 74}, {"referenceID": 5, "context": "Several methods have been proposed for finding the better initial centroid [7] [8] [9].", "startOffset": 75, "endOffset": 78}, {"referenceID": 6, "context": "Several methods have been proposed for finding the better initial centroid [7] [8] [9].", "startOffset": 79, "endOffset": 82}, {"referenceID": 7, "context": "Several methods have been proposed for finding the better initial centroid [7] [8] [9].", "startOffset": 83, "endOffset": 86}, {"referenceID": 8, "context": "Some methods were also aimed to amend both the efficiency and accuracy of K-means clustering technique [10].", "startOffset": 103, "endOffset": 107}, {"referenceID": 5, "context": "[7] proposed an algorithm that require less execution time compared to K-means clustering technique.", "startOffset": 0, "endOffset": 3}, {"referenceID": 5, "context": "In [7], the author proposed to maintain the distance to the closest cluster of previous iteration and use it to compare with distance from new centroid in the next iteration.", "startOffset": 3, "endOffset": 6}, {"referenceID": 6, "context": "[8] proposed a advanced clustering algorithm, called \u201cDivisive Correlation Clustering Algorithm (DCCA)\u201d for clustering of genes.", "startOffset": 0, "endOffset": 3}, {"referenceID": 7, "context": "[9] presented a method to find the initial centroid.", "startOffset": 0, "endOffset": 3}, {"referenceID": 8, "context": "[10] proposed an algorithm to find the initial centroid.", "startOffset": 0, "endOffset": 4}, {"referenceID": 9, "context": "K-MEAN ALGORITHM K-means is a partitioning type clustering algorithm used in data-mining and it is one of the most popular, simple and unsupervised learning algorithm [11].", "startOffset": 167, "endOffset": 171}, {"referenceID": 3, "context": "The algorithm consists two basic steps [5].", "startOffset": 39, "endOffset": 42}, {"referenceID": 3, "context": "The second step is to take all data objects of dataset to the nearby centroids [5].", "startOffset": 79, "endOffset": 82}, {"referenceID": 3, "context": "This procedure goes on yet the convergence criteria have not been satisfied or the centroids have not become similar for two consecutive iterations [5].", "startOffset": 148, "endOffset": 151}, {"referenceID": 8, "context": "Pseudo code for the K-means clustering algorithm is described below [10].", "startOffset": 68, "endOffset": 72}, {"referenceID": 4, "context": "The steps of dissimilarity tree based algorithm borrow the steps of TP (Tree Pruning) algorithm from [6].", "startOffset": 101, "endOffset": 104}], "year": 2014, "abstractText": "Cluster analysis is one of the primary data analysis technique in data mining and K-means is one of the commonly used partitioning clustering algorithm. In K-means algorithm, resulting set of clusters depend on the choice of initial centroids. If we can find initial centroids which are coherent with the arrangement of data, the better set of clusters can be obtained. This paper proposes a method based on the Dissimilarity Tree to find, the better initial centroid as well as every bit more accurate cluster with less computational time. Theory analysis and experimental results indicate that the proposed method can effectively improve the accuracy of clusters and reduce the computational complexity of the K-means algorithm.", "creator": "Microsoft\u00ae Word 2013"}}}