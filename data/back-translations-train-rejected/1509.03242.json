{"id": "1509.03242", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "10-Sep-2015", "title": "Gibbs Sampling Strategies for Semantic Perception of Streaming Video Data", "abstract": "Topic modeling of streaming sensor data can be used for high level perception of the environment by a mobile robot. In this paper we compare various Gibbs sampling strategies for topic modeling of streaming spatiotemporal data, such as video captured by a mobile robot. Compared to previous work on online topic modeling, such as o-LDA and incremental LDA, we show that the proposed technique results in lower online and final perplexity, given the realtime constraints.", "histories": [["v1", "Thu, 10 Sep 2015 17:25:50 GMT  (2859kb,D)", "http://arxiv.org/abs/1509.03242v1", null]], "reviews": [], "SUBJECTS": "cs.RO cs.LG", "authors": ["yogesh girdhar", "gregory dudek"], "accepted": false, "id": "1509.03242"}, "pdf": {"name": "1509.03242.pdf", "metadata": {"source": "CRF", "title": "Gibbs Sampling Strategies for Semantic Perception of Streaming Video Data", "authors": ["Yogesh Girdhar", "Gregory Dudek"], "emails": ["yogi@whoi.edu", "dudek@cim.mcgill.ca"], "sections": [{"heading": null, "text": "I. INTRODUCTIONMaking decisions based on the environmental context of a robot requires that we first model the context of the robot observations, which in turn could correspond to different semantic or conceptually higher units that compose the world. If we get an observation model of these units that compose the world, then it is easy to describe a given scene in relation to these units with this model; likewise, if we get a description of the world in relation to these units, then it is easy to calculate the observation model for each individual unit. The challenge is to accomplish these two tasks together, unattended, and without prior information. ROST [1], a real-time online spatiotemporal subject that attempts to solve this problem of assigning high-level labels to low-level observations. Topic modeling techniques were originally developed for unattended semantic modeling of text documents that can then be developed with these main algorithms [2] algorithms that automatically detect themes."}, {"heading": "II. PREVIOUS WORK", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "A. Topic Modeling of Spatiotemporal Data", "text": "Given the images of scenes with multiple objects, theme modeling was used to detect objects in these images in an unattended manner. Bosch et al. [6] used PLSA and a SIFT-based [7] visual vocabulary to model the content of far Xiv: 150 9.03 242v 1 [cs.R O] images dated September 10, 2015, and used an adjacent classifier to classify the images. Fei-Fei et al. [8] demonstrated the use of LDA to provide an intermediate representation of images, which was then used to learn an image classifier across multiple categories.Instead of modeling the entire image as a document, spatial LDA (SLDA) [9] models a subset of words that come close to each other in an image as a document, resulting in a better coding of the spatial structure."}, {"heading": "III. SPATIOTEMPORAL TOPIC MODEL", "text": "An observation word is a discrete observation by a robot. In view of the observation words and their location, we would like to calculate the posterior distribution of the topics at this place. Let's be the observed word at this place. We assume the following probabilistic model for the observation words: 1) Word distribution for each topic k: \u03c6k \u0445 Dirichlet (\u03b2), 2) Topic distribution for words at this place x: \u03b8x \u0445 Dirichlet (\u03b1 + H (x)), 3) Topic name for w: z \u0445 Discrete (\u03b8x), 4) Word name: w \u0445 Discrete (\u03c6z), where y \u0445 Y implies that random variable y is sampled from the distribution Y, z is the topic name for the word observation w, and H (x) is the distribution of the topics in the neighborhood of location x. Each topic is modelled by the distribution of location x."}, {"heading": "IV. APPROXIMATING NEIGHBORHOODS USING CELLS", "text": "The generation process defined above models the clustering behavior of observations from a natural scene well, but is difficult to implement because it requires us to keep an eye on the distribution of topics in every place in the world, which is mathematically unfeasible for any large dataset. In the special case where the nucleus is a uniform distribution over a finite region, we can assume a cell division of the world and approximate the distribution of topics around a place by adding up the distribution of topics of cells in and around the place. Let us divide the world into C cells, in which each cell c-C with its neighboring cells G (c) C. Let c (x) be the cell that contains points x. In this paper, we experiment only with a grid decomposition of the world in which each cell is connected to its six closest neighbors, 4 spatial and 2 temporal properties."}, {"heading": "V. REALTIME INFERENCE USING GIBBS SAMPLING", "text": "In view of a word observation wi, its location xi, and its neighborhood Gi = G (c (xi)), we use a Gibbs sampler to assign a new topic name to the word by starting from the back of the topic distribution: P (zi = k Gi, \u2212 i + \u03b1), (4), where nwk, \u2212 i counts the number of words of type w in the topic k, without the current word wi, \u2212 i + \u03b2), \u2212 i is the number of words with the topic k in the neighborhood Gi, \u2212 i + \u03b1), where nwk, \u2212 i counts the number of words of type w in the theme k, without the current word wi, nkGi, \u2212 i is the number of words with the topic k in the neighborhood Gi, \u2212 i + \u03b1, \u03b2 are the dirichlet hyper parameters. Note that for a neighborhood size of 0, the above Gibbs samplers are equivalent to the LDA samplers proposed."}, {"heading": "A. Now Gibbs Sampling", "text": "The simplest way to process streaming observation data to ensure that the subject names of the last observation match is to refine subjects only from the last observation to the arrival of the next observation. P (t | T) = {1, if t = T 0, otherwise (5) We call this the Now Gibbs sampler. This is analogous to the o-LDA approach of Banerjee and Basu [13]. If R is our calculation budget, defined as the expected number of observation time steps our system can refine between the arrival times of two consecutive observations, and r (t) the number of observations in Mt after the time T, then this approach gives every observation R resources. E {r (t) = R (6) Although this sounds fair, the problem is that no information from the future is used to improve the understanding of the past data."}, {"heading": "B. Uniform Gibbs Sampling", "text": "A conceptually opposite strategy is to randomly select an observation from all previous observations and refine the subject names for all words in that observation. P (t | T) = 1 / T (7) This is analogous to the incremental Gibbs sampler for LDA proposed by Canini et al. [14]. The expected value of r (t) is then: E {r (t) = R (1t + 1 t + 1 + \u00b7 \u00b7 \u00b7 \u00b7 + 1 T) (8) \u2248 R (log T \u2212 log t). (9) We see that older observations are labeled disproportionately higher than more recent observations, and subject names of new observations may take a long time to approximate."}, {"heading": "C. Age Proportional Gibbs Sampling", "text": "A seemingly good intermediate approach could be to skew the random sample of observations in favor of selecting current observations, with the probability being proportional to their timestamp. P (t | T) = t \u2211 T i = 1 i (10) Then the expected number of times this observation is refined is indicated by: E {r (t)} = R (t \u2211 t i = 1 i + \u00b7 \u00b7 + t \u2211 T i = 1 i) (11) \u2248 2R (T \u2212 t) T. (12) When a new observation is made, the expected number of refinements it will receive before the arrival of the next observation is Rt / \u0445 2R / t, implying that while this strategy is better than a uniform sample (according to which we will not have a sufficient number of refinements for which we are still not suitable)."}, {"heading": "D. Exponential Gibbs Sampling", "text": "Using a geometric distribution, we can define the probability of a refinement of the time style t, at the present time TP (t | T) = q (1 \u2212 q) T \u2212 t, (15) where 0 < q < 1 is a parameter. Using this distribution to pick refinement patterns ensures that on average qR number of refinements are spent on refining the most recent observations and the remaining (q \u2212 1) R refinement siterations are spent on refining other current observations. At the T \u2192 \u221e limit, observations are refined in each time step E {r (t) = R number of observations, similar to the Gibbs sampler now. However, this approach allows new information to influence some of the most recent observations of the past, resulting in a lower global perplexity of the learned model."}, {"heading": "E. Mixed Gibbs Sampling", "text": "We expect both Now and Exponential Gibbs samplers to be good at ensuring that the subject labels for the last observation converge quickly (to a locally optimal solution) before the next observation arrives, whereas uniform and age-based Gibbs samplers are better at finding globally optimal outcomes. One way to balance these two performance goals is to combine these global and local strategies. We consider four such approaches in this paper: Uniform + Now: P (t | T) = {\u03b7, if t = T (1 \u2212 \u03b7) / (T \u2212 1), otherwise (16) AgeProportional + Now: P (t | T) = {\u03b7, if t = T (1 \u2212 \u03b7) t \u0445 T \u2212 1i = i, otherwise (17) Uniform + Exp: P (t | T) = throuq (1 \u2212 q) T (1 \u2212 q) T \u2212 \u03b7 (\u2264 1) and the ratio between the respective strategies."}, {"heading": "VI. EXPERIMENTS", "text": "1) Dataset: We evaluated the performance of ROST in analyzing videos using three different datasets with millions of visual words. We used a mixed vocabulary to describe each frame with 5000 ORB words, 256 intensity words (pixel intensity) and 180 color words (pixel hue), for a total vocabulary size of 5436. Although it is difficult to prove the optimality of the vocabulary, our experiments showed that once the vocabulary size is sufficiently large, a limited sensitivity to its exact value exists [15]. Some key statistics for these datasets are in Table I. The dataset with two objects shows a simple scenario in which two different objects on a textured (wood) background randomly come together first and finally. The aerial dataset was collected using Unicorn UAV over a coastal region. The UAV provides a zig-zag dataset with a variety of ocean cover structures and forest patterns."}, {"heading": "A. Realtime Gibbs Samplers", "text": "To evaluate the proposed real-time Gibbs samplers against real data, we performed the following experiment: For each video dataset and each Gibbs sampler, we calculated the theme names and perplexity online with 10 random restarts. We then compared the mean confusion of the words, one time step after their arrival (instantaneous), and after all the observations were (final), with the perplexity of the theme names calculated in stacks. To make a fair comparison, we used the same refinement time per time step (TR) for both stack and online cases. The resulting perplexity diagrams are shown in Figures 3, 4, and 5. The mean confusion values for the entire datasets are represented in Tables III (instantaneous confusion) and II (final confusion), with the confusion caused by confusion."}, {"heading": "VII. CONCLUSION", "text": "Thematic modeling techniques such as ROST model the latent context of spatio-temporal streaming observation, such as image and other sensor data captured by a robot. In this paper, we compared the performance of multiple Gibbs samplers for spatio-temporal real-time modeling of topics, including o-LDA and incremental LDA. We measured how well the topic labels converge, globally for the entire data and individually for an observation, one step after their observation time. The latter measurement criterion is useful to evaluate the performance of the proposed technique in the context of robotics, where we need to make immediate decisions. We showed that the proposed mixed Gibbs samplers such as Uniform + Nowconsistently perform better than other samplers that focus only on recent observations or refine all observations with the same probability."}, {"heading": "ACKNOWLEDGMENT", "text": "This work was supported by the Natural Sciences and Engineering Research Council (NSERC) through the NSERC Canadian Field Robotics Network (NCFRN). Yogesh Girdhar is currently supported by the Woods Hole Oceanographic Institution's postdoctoral program funded by the Devonshire Foundation and the J. Seward Johnson Fund. Authors would like to thank Julian Straub at MIT for the helpful discussion."}], "references": [{"title": "Autonomous adaptive exploration using realtime online spatiotemporal topic modeling,", "author": ["Y. Girdhar", "P. Giguere", "G. Dudek"], "venue": "The International Journal of Robotics Research,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2013}, {"title": "Latent dirichlet allocation,", "author": ["D.M. Blei", "A.Y. Ng", "M.I. Jordan"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2003}, {"title": "Finding scientific topics,", "author": ["T.L. Griffiths", "M. Steyvers"], "venue": "Proceedings of the National Academy of Sciences of the United States of America,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2004}, {"title": "Curiosity Based Exploration for Learning Terrain Models,", "author": ["Y. Girdhar", "D. Whitney", "G. Dudek"], "venue": "IEEE International Conference on Robotics and Automation,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2014}, {"title": "Exploring Underwater Environments with Curiosity,", "author": ["Y. Girdhar", "G. Dudek"], "venue": "Canadian Conference on Computer and Robot Vision. Montreal: IEEE,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2014}, {"title": "Scene Classification Via pLSA,", "author": ["A. Bosch", "A. Zisserman", "X. Mu\u00f1oz"], "venue": "ser. Lecture Notes in Computer Science,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2006}, {"title": "Distinctive image features from scale-invariant keypoints,", "author": ["D.G. Lowe"], "venue": "International Journal of Computer Vision,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2004}, {"title": "A Bayesian Hierarchical Model for Learning Natural Scene Categories,", "author": ["L. Fei-Fei", "P. Perona"], "venue": "IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR\u201905),", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2005}, {"title": "Spatial Latent Dirichlet Allocation,", "author": ["X. Wang", "E. Grimson"], "venue": "Advances in Neural Information Processing Systems,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2007}, {"title": "Geometric LDA: A Generative Model for Particular Object Discovery,", "author": ["J. Philbin", "J. Sivic", "A. Zisserman"], "venue": "Proceedings of the British Machine Vision Conference,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2008}, {"title": "Unsupervised discovery of visual object class hierarchies,", "author": ["J. Sivic", "B.C. Russell", "A. Zisserman", "W.T. Freeman", "A.A. Efros"], "venue": "IEEE Conference on Computer Vision and Pattern Recognition", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2008}, {"title": "Hierarchical topic models and the nested Chinese restaurant process,", "author": ["D.M. Blei", "T.L. Griffiths", "M.I. Jordan", "J.B. Tenenbaum"], "venue": "Advances in neural information processing systems (NIPS),", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2004}, {"title": "Topic Models over Text Streams: A Study of Batch and Online Unsupervised Learning,", "author": ["A. Banerjee", "S. Basu"], "venue": "SIAM International Conference on Data Mining,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2007}, {"title": "Online Inference of Topics with Latent Dirichlet Allocation,", "author": ["K.R. Canini", "L. Shi", "T.L. Griffiths"], "venue": "Proceedings of the International Conference on Artificial Intelligence and Statistics,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 1999}, {"title": "Online Visual Vocabularies,", "author": ["Y. Girdhar", "G. Dudek"], "venue": "CRV", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2011}], "referenceMentions": [{"referenceID": 0, "context": "ROST [1] , a realtime online spatiotemporal topic modeling framework attempt to solve this problem of assigning high level labels to low level streaming observations.", "startOffset": 5, "endOffset": 8}, {"referenceID": 1, "context": "Topic modeling techniques were originally developed for unsupervised semantic modeling of text documents [2] [3].", "startOffset": 105, "endOffset": 108}, {"referenceID": 2, "context": "Topic modeling techniques were originally developed for unsupervised semantic modeling of text documents [2] [3].", "startOffset": 109, "endOffset": 112}, {"referenceID": 0, "context": "ROST[1] extends previous work on text and image topic modeling to make it suitable for processing streaming sensor data such as video and audio observed by a robot, and presents approximations for posterior inferencing that work in realtime.", "startOffset": 4, "endOffset": 7}, {"referenceID": 3, "context": "ROST has been used for building semantic maps [4] and for modeling curiosity in a mobile robot, for the purpose of information theoretic exploration [5].", "startOffset": 46, "endOffset": 49}, {"referenceID": 4, "context": "ROST has been used for building semantic maps [4] and for modeling curiosity in a mobile robot, for the purpose of information theoretic exploration [5].", "startOffset": 149, "endOffset": 152}, {"referenceID": 5, "context": "[6] used PLSA and a SIFT based [7] visual vocabulary to model the content of ar X iv :1 50 9.", "startOffset": 0, "endOffset": 3}, {"referenceID": 6, "context": "[6] used PLSA and a SIFT based [7] visual vocabulary to model the content of ar X iv :1 50 9.", "startOffset": 31, "endOffset": 34}, {"referenceID": 7, "context": "[8] have demonstrated the use of LDA to provide an intermediate representation of images, which was then used to learn an image classifier over multiple categories.", "startOffset": 0, "endOffset": 3}, {"referenceID": 8, "context": "Instead of modeling the entire image as a document, Spatial LDA (SLDA) [9] models a subset of words, close to each other in an image as a document, resulting in a better encoding of the spatial structure.", "startOffset": 71, "endOffset": 74}, {"referenceID": 9, "context": "Geometric LDA (gLDA) [10] models the LDA topics using words that are augmented with spatial position.", "startOffset": 21, "endOffset": 25}, {"referenceID": 10, "context": "[11] used hierarchical LDA (hLDA) [12] for automatic generation of meaningful object hierarchies.", "startOffset": 0, "endOffset": 4}, {"referenceID": 11, "context": "[11] used hierarchical LDA (hLDA) [12] for automatic generation of meaningful object hierarchies.", "startOffset": 34, "endOffset": 38}, {"referenceID": 1, "context": "The main difference between this generative process and the generative process of words in a text document as proposed by LDA [2], [3] is in step 2.", "startOffset": 126, "endOffset": 129}, {"referenceID": 2, "context": "The main difference between this generative process and the generative process of words in a text document as proposed by LDA [2], [3] is in step 2.", "startOffset": 131, "endOffset": 134}, {"referenceID": 2, "context": "[3], where each cell corresponds to a document.", "startOffset": 0, "endOffset": 3}, {"referenceID": 12, "context": "This is analogous to o-LDA approach by Banerjee and Basu [13].", "startOffset": 57, "endOffset": 61}, {"referenceID": 13, "context": "[14].", "startOffset": 0, "endOffset": 4}, {"referenceID": 14, "context": "Although it is difficult to substantiate the optimality of the vocabulary, our experiments have suggested that once the vocabulary size is sufficiently large, there is limited sensitivity to its precise value [15].", "startOffset": 209, "endOffset": 213}], "year": 2015, "abstractText": "Topic modeling of streaming sensor data can be used for high level perception of the environment by a mobile robot. In this paper we compare various Gibbs sampling strategies for topic modeling of streaming spatiotemporal data, such as video captured by a mobile robot. Compared to previous work on online topic modeling, such as o-LDA and incremental LDA, we show that the proposed technique results in lower online and final perplexity, given the realtime constraints.", "creator": "LaTeX with hyperref package"}}}