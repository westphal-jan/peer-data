{"id": "1509.03371", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "11-Sep-2015", "title": "Efficient Convolutional Neural Networks for Pixelwise Classification on Heterogeneous Hardware Systems", "abstract": "This work presents and analyzes three convolutional neural network (CNN) models for efficient pixelwise classification of images. When using convolutional neural networks to classify single pixels in patches of a whole image, a lot of redundant computations are carried out when using sliding window networks. This set of new architectures solve this issue by either removing redundant computations or using fully convolutional architectures that inherently predict many pixels at once.", "histories": [["v1", "Fri, 11 Sep 2015 01:20:46 GMT  (3683kb,D)", "http://arxiv.org/abs/1509.03371v1", "92 pages, project source code available atthis https URL, technical report written at ETH Z\\\"urich, in collaboration with AMD, UZH INI and HHMI Janelia"]], "COMMENTS": "92 pages, project source code available atthis https URL, technical report written at ETH Z\\\"urich, in collaboration with AMD, UZH INI and HHMI Janelia", "reviews": [], "SUBJECTS": "cs.CV cs.AI", "authors": ["fabian tschopp"], "accepted": false, "id": "1509.03371"}, "pdf": {"name": "1509.03371.pdf", "metadata": {"source": "CRF", "title": "Efficient Convolutional Neural Networks for Pixelwise Classification on Heterogeneous Hardware Systems", "authors": ["Fabian Tschopp"], "emails": [], "sections": [{"heading": null, "text": "The implementation of the three models takes place via a new utility above the Caffe library. The utility supports a wide range of image input and output formats, pre-processing parameters and methods for balancing the label histogram during training. The Caffe library has been expanded to include new layers and a new backend for availability on a wider range of hardware such as CPUs and GPUs via OpenCl. AMD GPUs have observed accelerations of 54 x (SK-Net), 437 x (U-Net) and 320 x (USKNet), using the SK equivalent SW (sliding window) network.The label throughput is up to one megapixel per second.The analysed neural networks exhibit characteristics that apply during training or processing, and not every dataset is suitable for every architecture.The quality of the prediction of the damage prediction on the entire Caffe 2012 project is assessed by the ISBI damage pipeline. The three models are implemented via a new utility above the Caffe library. The utility supports a wide range of image input and output formats, preprocessing parameters and methods for balancing the label histogram during training."}, {"heading": "Acknowledgements", "text": "University of Zurich, Institute of NeuroinformaticFirst of all, I would like to thank my supervisor Dr. Jan Funke for his guidance, motivation and the opportunity to visit the HHMI Janelia in Ashburn, Virginia, USA. I would also like to thank my supervisor Prof. Dr. Angelika Steger for the collaboration with the Institute of Neuroinformatics that made this research project possible. I would like to thank Stephan Gerhard and Julien Martel for interesting discussions about neural networks and this technical report. Howard Hughes Medical Institute, JaneliaIn addition to my advisors, I would like to thank Dr. Srinivas Turaga and Dr. Stephan Saalfeld for their collaboration at the HMI Janelia, which inspired me to expand the scope of my research and gave me insights into the applications of neural networks for image segmentation in Connectomics. In addition, Janelia has the nicest campus of all research institutes I have seen so far."}, {"heading": "1 Introduction 1", "text": "1.1 Convolutional Neural Networks..................... 1. 1.2 Caffe Library......................................... 2. 1.3 Pixelwise Classification......................................................................................................"}, {"heading": "2 Datasets 7", "text": "2.1 DS1 - Segmented neural anisotropic ssTEM dataset..... 7 2.2 DS2 - ISBI 2012 dataset of neural tissue........... 9"}, {"heading": "3 Models 11", "text": "3.1 Introduction......................................................................................................................................................................................................................................................................................................................................................................."}, {"heading": "4 Caffe Neural Tool 24", "text": "4.1 Functionality........................................ 24 4.2 Pre-processing.................................... 25 4.3 Histogram compensation......................................."}, {"heading": "5 Caffe Library 29", "text": ".)......................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................."}, {"heading": "6 Benchmarks 40", "text": ".)......................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................."}, {"heading": "7 Results 56", "text": "7.1. Introduction......................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................"}, {"heading": "8 Conclusion 66", "text": "8. 1 Research Time Line....................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................."}, {"heading": "A Network Architectures 72", "text": "A.1 SK-Net.............................................................................................................................................................................................................................................................................................................................................................................................................................................................."}, {"heading": "1.1 Convolutional Neural Networks", "text": "It is possible for the networks to be used for various image classification and object recognition tasks. A prominent example is the ImageNet / AlexNet [1] for object recognition. Recent networks [2] can have many, in this case over 20, layers and millions of learnable parameters. This work focuses on the classification of biomedical data, especially neural tissue microscopy images (see Chapter 2). The challenge is that the training data is scarcer than that of everyday images, such as handwritten letters or online image collections."}, {"heading": "1.2 Caffe Library", "text": "Caffe stands for Convolutional Architecture for Fast Feature Embedding [4]. It is a state-of-the-art neural network library that has been greatly optimized for use with nVidia's CUDA technology, so in many cases the library is already very efficient using certain GPUs. What has been missing [5] is fast CPU support (the current CPU backend is predominantly single thread) and support for GPUs and accelerator devices from AMD and Intel. The library is still under development and has a large community [6]. Network models and trained weights (usually referred to as model zoo) can be shared in Google's prototxt (network and learning configurations) and protocol buffers (trained weights and solver states). The library is typically used on the command line with the Caffe binary library or via a Python interface (see Figure 1.3) and protocol buffers (trained weights and soles)."}, {"heading": "1.3 Pixelwise Classification", "text": "The work of Hongsheng Li et al. [7] makes it possible to make existing SW networks more efficient and at the same time provide identical prediction results (see Section 3.3.1). Alternatively, fully revolutionary models (U and sometimes USK) directly issue a larger patch, as shown in Figure 1.2. This method of training and processing is called patch-based (n = 1, w > 1), as opposed to minibatch-based (n > 1, w = 1). A combination of the two (n > 1, w > 1) is only useful if the images in the dataset are not dependent on each other."}, {"heading": "1.4 Existing Work", "text": "This technical report is based on the following existing work: \u2022 SW (sliding window) network designed by Julien Martel [10], not published. Architecture is geared to the segmentation of the DS1 dataset. \u2022 Strided kernel convolution and pooling kerels by Hongsheng Li et al. [7]. This is the basic approach to accelerate existing SW networks. \u2022 Malis criterion, first introduced by Srinivas Turaga et al. [3], which supports an alternative method for forming neural networks by affinity diagrams, which is very specific and useful for biomedical data where areas are separated by background boundaries. \u2022 The Open Source Caffe Library, developed by the Berkeley Vision and Learning Center [6], [4]. \u2022 U network designed by Ronneberger et al. [2] This model is also optimized for biomedical images and in particular the DS2 BI dataset (ISBI 2012 Secrivolution of Jeff Scriue 11]."}, {"heading": "1.5 New Contributions", "text": "An overview of new contributions to the caffe landscape in terms of models, utilities and library changes is given in Figure 1.3. This work addresses the given problem at all levels - from the use of efficient BLAS libraries to backend development and easy-to-use frontends to new network models. However, on the side of neural network models, two new neural network architectures are introduced, the SK-Net (Section 3.3) and USK-Net (Section 3.5). A meta-analysis of the three efficient networks (SK, U, USK) based on: \u2022 Differences and characteristics of network designs (Chapter 3). \u2022 Calculation costs and efficiency (Chapter 6). \u2022 Image segmentation quality, which is evaluated both numerically and visually (Chapter 7) for the typical foreground background, two label classifieds.In order to be able to easily train the network models on different data sets, Caffe Neural Tool (Chapter 4) was developed."}, {"heading": "1.6 Terminology", "text": "The most commonly used symbols and abbreviations in the report: \u2022 Forwarding, processing: \u2022 Calculation of data through a neural network from input to output. \u2022 Backwarding, training, backpropagation: Calculation of neural network gradients (diff maps) in the reverse direction and updating of network weights. \u2022 Data block: Memory block containing function cards for forward processing in the neural network. \u2022 Diff blob: Memory block containing the differential / error signal card during backward propagation. \u2022 BLAS: Basic Linear Algebra subprograms. Contains functions such as efficient matrix multiplications. \u2022 DS1: Dataset 1, see Section 2.1. \u2022 DS2: Dataset 2, see Section 2.2. \u2022 SW: Slide window networks for pixel-wise classification, see Section 2.2."}, {"heading": "2.1 DS1 - Segmented anisotropic ssTEM dataset of neural", "text": "tissueThis dataset shows neural tissue from a Drosophila larva ventral nerve cord and was then recorded with the serial section of transmission electron microscopy at the HHMI Janelia Research Campus [8]. Training data consists of 20 images of 1024 by 1024 pixels raw ssTEM and the corresponding segmentation. It is divided into nine different labels, which are consolidated in foreground and background for two labels Training and evaluation (see Section 7.2): 72.1. DS1 - Segmented anisotropic ssTEM dataset of neural tissue \u2022 # 0: Horizontal cell membranes - background \u2022 # 1: + 45 vertical cell membranes - background \u2022 # 2: Vertical cell membranes - background \u2022 # 3: -45 Vertical cell membranes - background, which can be woven on vertical cell membranes - background \u2022 # 4: + 45 vertical cell membranes - background - # 7 - background - gliographic connections - #: 7 - background."}, {"heading": "2.2 DS2 - ISBI 2012 dataset of neural tissue", "text": "This dataset is from the ISBI 2012 Challenge [11], [14], [15]. Raw images and corresponding segmentation images for the training are 512 x 512 pixels; the neural tissue characteristics are of a similar scale to the DS1 dataset; the raw images have slightly less contrast and are blurred; the training set consists of 30 images, giving a total of 30 \u00d7 5122 \u2248 7.8 MPixels, which is about half as much as DS1. The test data used on DS2 is a separate batch of 30 images, 512 x 512 pixels in size. For these images, the truth of the reason for the segmentation is not available for public download, so the evaluation given in Section 7.3 is based solely on the reports of the official ISBI 2012 evaluation [11], which is still open to new results.For both stacks, the data extends beyond 2 x 2 x 1.5 microns with a resolution of 4 x 50 nm / pixel [11].The data set with the corresponding data set in the 3and the corresponding models [9.]"}, {"heading": "3.1 Introduction", "text": "This chapter describes how the network architectures were set up to train and process the DS1 and DS2 datasets. They were configured for two label classifications, but nine labels were also tested, and even more could be learned from the Softmax loss. Mali's loss (Section 5.3.2) implements only a pre- and background separation, and no special striding or padding was used for all network architectures."}, {"heading": "3.2 Sliding Window (SW-Net)", "text": "Sliding window networks classify an image by using a pixel and a border v of any size around it as input, and classify the middle pixel by running the patch through a neural network. Then, the next pixel is labeled by shifting the window patch by one pixel, classifying the adjacent pixel of the first pixel. Pixels can also be processed in a minibatch to increase GPU usage and amortize the transfer times of direct memory access (from host to device memory), which is still very inefficient, since most of the context of two adjacent pixels overlaps and the same filters are applied across the entire context. Redundant calculations can be reduced for a patch of input pixels by using SK networks. The sliding network described here was developed by Julien Martel [10], using the baseline achieved for SK and SK networks."}, {"heading": "3.3 SK-Net", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "3.3.1 Converting SW Networks to SK", "text": "Hongsheng Li et al. [7] provides a pseudo-code (page 4) on how to convert a sliding-window network into a striped kernel network (= 1), but it is incomplete in terms of consistency checking, kernel sizes and output sizes, and the theory of converting inner product levels (fully connected) is not described either. Therefore, I can provide a more complete version (see algorithm 1), even if this is done without considering consistency and output dimensions (w), forcing s = 1 and p = 0 in all layers of the SK network. Each data dimension (width, height, depth) can be processed separately for the kernel size (k), kernel stride (d) and output dimension (w). The algorithm is able to transform networks and find consistency that is fully automated.For caffe network configurations, only the network configurations and strictable kernel sizes are provided."}, {"heading": "3.3.2 SK Network", "text": "When processing the network in Table 3.1, the condition w (i \u2212 1) SW mod (i) SW 6 = 102 pixels = 0 of algorithm 1 is actually violated by the second pooling layer, with 43 mod 2 6 = 0. It is easy to fix this by starting from the last layer of the network and calculating (i \u2212 1) SW = k (i) SWw (i) SW (i) SW (i) SW (3.4) for inner product and convolution layers. The corrected network is given in Table 3.2. the conversion of these into SK results in the network in Table 3.3.16The last three inner product layers from SW \u2212 1) + w (i) SW-SW (3.4) SW (3.4) for inner product and convolution layers."}, {"heading": "3.4 U-Net", "text": "The U-Net configuration presented here is the network configuration as described in Ronneberger et al. paper [2]. Table 3.4 describes the network in the same style as Table 3.3 for the SK network to compare it. Layer names are selected in the same style as for SW and SK networks. U-Net has contracting and expanding sections: \u2022 Contracting: Two convolutions followed by a maximum pooling layer. \u2022 Expanding: Deconvolution followed by a conversion to reduce the number of feature cards, one mergecrop and two convoluton layers. The source code for running U-Net and the prototype configuration files were not available for download at the time of this project, so the network presented here, which is my own interpretation, may differ from the original design. The paper does not give all the details, such as how the MergeCrop layer and upconvolution work."}, {"heading": "3.5 USK-Net", "text": "The majority of the convolutions, and thus the free parameters, can be trained on downsampled feature cards by using one or more contracting path steps (and their expanding counterparts) from the U-Net. After a contracting step, the network was configured to: \u2022 Network outputs 512 by 512 pixel labels. \u2022 The context considered for each pixel classification is 180 by 180 pixels. \u2022 As a result, network input is 692 by 692 pixels. \u2022 The SK network unit (Conv3 to ip2) is required to accept 344 by 344 pixels as input and outputs 258 by 258 pixels."}, {"heading": "4.1 Functionality", "text": "@ @ - @ - @ - @ - @ - @ - @ - @ - @ - @ - @ - @ - @ - @ - @ - @ - @ - @ - @ - to-go-to-go-to-go-to-go-to-go-to-go-to-go-to-go-to-go-to-go-to-go-to-go-to-go-to-go-to-go-to-go-to-go-to-go-to-go-to-go-to-go-to-go-to-go-to-to-go-to-to-go-to-go-to-to-go-to-to-go-to-to-go-to-to-go-to-to-go-to-to-to-go-to-to-to-to-to-go-to-to-to-to-go-to-to-to-to-to-go-to-to-to-to-to-to-go-to-to-to-to-to-to-go-to-to-to-to-to-go-to-to-to-to-to-to-to-to-to-to-go-to-to-to-to-to-to-to-to-to-to-to-to-to-to-go-to-to-to-to-to-to-to-to-to-to-to-to-to-to-to-to-to-to-to-to-to-to-to-to-to-to-to-to-to-to-go-to-to-to-to-to-to-to-to-to-to-to-to-to-to-to-to-to-to-go-to-to-to-to-to-to-to-to-to-to-to-to-to-to-to-to-to-to-to-to-to-to-to-to-to-to-to-to-to-to-to-to-to-to-to-to-to-to-to-to-to-to-to-to-to-to-to-to-to-to"}, {"heading": "4.2 Preprocessing", "text": "This technique has been applied to the DS1 dataset (see section 2.1). Consolidation occurs after histogram compensation, which allows difficult and rare labels to be compensated for before they are consolidated only on background and foreground. This also allows important small, difficult features to be highlighted in the training data. \u2022 Rotation of training fields to a random multiple of 90. \u2022 Random reflection of training fields. \u2022 Blur of training fields with a Gaussian core of any size. The blur has an average of zero and the variance is randomly selected from a normal distribution. \u2022 Mean and variance of distribution can be selected. \u2022 CLAHE (contrast limited adaptive histogram compensation), with a clipping parameter. The function is integrated with OpenCV. \u2022 Patch normalization to [\u2212 1.0, 1.0] in the flow point before feeding the biological network.254.3. This histogram equalization method would not cause a multiple rotation."}, {"heading": "4.3 Histogram Equalization", "text": "This year, it will be able to fix and fix the mentioned bugs."}, {"heading": "5.1 Introduction", "text": "Customizing the Caffe library for efficient pixel-by-pixel classification on heterogeneous hardware involves the largest programming effort of the project and changes 20,000 lines of code compared to the BVLC master branch [5], [6]. Changes can be grouped into customizations at different levels: \u2022 Modified solver and network code to support the Caffe Neural Tool C + + interface. \u2022 Modified existing layers to customize pixel-wise inputs and outputs. \u2022 Additional layers for SK-, U- and USK architectures. \u2022 Additional layers for Mali loss. \u2022 Redesign N-dimensional layers to support up to 6D rounds and maximum pooling with striped cores. \u2022 OpenCL- and OpenCL-hybrid code to support a wide range of GPUs and USK architectures. \u2022 Backend adjustments to allow macro-dynamic shifting of the Caffe and backend to allow for running time and non-matching of the existing Caffe architectures."}, {"heading": "5.2 Modified Layers", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "5.2.1 SK Layers", "text": "SK (strided kernel) layers are layers with a kernel size k > 1 and an inner ride in the kernel (d > 1).The result is a kernel that considers a feature map in the context of (k \u2212 1) \u00b7 d + 1 pixels, also referred to as external kernel size.The motivation to have such kernels is to be able to convert single-pixel sliding window (SW) networks into patch prediction networks (SK).How this works is shown in Figures 1.1 and 1.2 as well as in the striped representation in Figure 3.1. For SK convolutions, the matrix multiplication (see Section 5.5) remains the same as for normal convolutions. Only the im2col and col2in memory Copy Kernel Copy Copy Copy Copy Cerning Cerning Cerning Cernsion (I have the existing Hongshal kernel codes closely."}, {"heading": "5.2.2 N-Dimensional Layers", "text": "The Caffe library is able to specify any amount of dimensions for the blob storage infrastructure used to transfer data between layers. However, not all layers automatically operate in higher dimensions. For most elementary kernels, nothing needs to be changed. Convolutions and pooling operations require a slight redesign. For convolutions, matrix multiplication remains the same as for normal and striped kernel convolutions (see Section 5.5). There were existing cores for normal N-dimensional convolutions by Jeff Donahue [12]. However, these cores only support the standard kernel confrontation d = 1. In this research project, I combined the existing striped kernel code and N-dimensional convolutions to obtain the most generalized form of convolutions reaching up to 6 dimensions and core steps."}, {"heading": "5.3 New Layers", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "5.3.1 Merge Crop", "text": "The MergeCrop layer is required in U and USK-Net architectures (see Sections 3.4 and 3.5). The layer accepts two input blobs: \u2022 Blob A of size w (i \u2212 1) A. \u2022 Blob B of size w (i \u2212 1) B \u2265 w (i \u2212 1) A. The layer prints a blob that contains all feature maps of A and B that may have a different number of feature maps. Input B must be truncated to size A. The output feature maps are size w (i) = w (i \u2212 1) A. During backpropagation, the error maps are passed by copying in the opposite direction. On U and USK-Net, backwarding is only enabled for input A, because Input B receives the differential data on a different path (from the down sampling pooling layer) in the neural network."}, {"heading": "5.3.2 Malis Loss", "text": "The implementation in Caffe [5] that I provide is based on existing codes to calculate the Malian criterion for Matlab (17) and Torch (18), which are implemented only as CPU code and do not run on OpenCL or CUDA.Figure 5.1 describes how the Malian criterion loss is used with the network models presented in this report (see Chapter 3), and the layers are implemented only as CPU code and not on OpenCL or CUDA.Figure 5.1 describing how the Malian criterion loss is presented along with the network models."}, {"heading": "5.3.3 Affinity", "text": "The affinity layer is an additional layer that must be used in connection with the loss of Mali (see Figure 5.1).The idea is that not only affinity graphs but also pixel-by-pixel classifications can be used to 345.3. New layerslearned with the loss of Mali. During forwarding, the affinity layer must look at adjacent pixels in the horizontal and vertical direction of entering and calculating their connectivity (affinity). A (x) x, y = min z = x, y (5.3) A (y) x, y = min z = y, y + 1 I + x, z (5.4) M (x) x, y = arg z, x + z, y = z, y (5.5) M (y) x, y = arg z, z (5.6)."}, {"heading": "5.3.4 Connected Components", "text": "The connected component layer is a small layer based on the OpenCV flood filling algorithm that separates the connected components from a background called Ground Truth (Figure 5.4). On the basis of this map, the Malian loss knows which areas need to be separated and which ones need to be connected. In this case, the cell membrane, which is considered as the background, is not assigned to any component."}, {"heading": "5.4 OpenCL Backend", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "5.4.1 Implementation", "text": "In my version of the Caffe library, an additional versatile backend is available for various computer devices based on OpenCL and ViennaCL [19]. The backend is called Greentea and is part of the Greentea project, consisting of frontend, models and modified Caffe libraries (see Figure 1.3).This section gives an overview of interesting aspects of how the OpenCL backend works and how the Caffe library changed365.4. OpenCL backendis are given. Further details and complete documentation are available within the source code available for download [5]. A key feature is that the OpenCL backend feature is equivalent to the CUDA backend. All GPU layers can be used on both backends."}, {"heading": "5.4.2 OpenCL Hybrid", "text": "The OpenCL hybrid implementation describes how the OpenCL backend is used when selecting a CPU device instead of a GPU device, but the two basic differences are memory allocation and BLAS library calls. However, when a SyncedMem object is instantiated, the memory is allocated as host memory rather than device memory. Unlike the CPU backend, the memory is allocated via OpenCL memory objects, which allows access to the underlying memory pointer of OpenCL memory objects and can map the memory to a new CPU pointer. Convolution MethodsFor BLAS calls, the following steps are performed: 1. For all involved cl-mem memory objects, the underlying host pointer is restored and mapped to a new CPU pointer. At this point, OpenCL ensures that all backends are optimized."}, {"heading": "5.5 Convolution Methods", "text": "This year it has come to the point where it is a reactionary project, which is a reactionary project, which is primarily a reactionary project."}, {"heading": "6.1 Introduction", "text": "This chapter evaluates the performance (or efficiency) of different models on a variety of hardware devices. This is critical for accelerating neural networks by identifying and eliminating computing and memory bottlenecks. All theoretical calculations in this chapter are based on two-dimensional square computing cores and function boards, which is consistent with the way the network architectures are structured (see chapter 3)."}, {"heading": "6.2 Hardware", "text": "The device specifications in Table 6.1 are taken from the official white papers (AMD [22], nVidia [23], Intel ARK [24]).The stated FLOP / s performance is based on the FMA functionality. The two AMD W9100 graphics cards were kindly sponsored by AMD [25], as noted in the receipts. The card has special features such as high 64-bit precision and error correction memory. These were not used for caffe, only the OpenCL 2.0 driver and the large amount of memory was important. The W9100 is a workstation, but consumer cards from nVidia and AMD can also be used without limitations, as long as there is sufficient 406.3. Software device memory (R9 290X, R9 390X, Titan, Titan X).This is available when creating low-cost, high-throughput systems for neural networks.The 97-Jan7 GTU and JantesPU devices are only available in the JantesP1 table."}, {"heading": "6.3 Software", "text": "The modified version of Caffe [5] in Project Greentea supports a variety of configurations that run differently depending on the computing device. Table 6.2 represents the setup for benchmarking. It is the best possible setup for any combination of backend and device for the models in Chapter 3. OpenBLAS is compiled to use all CPU cores via OpenMP and supports all vector extensions available on the CPUs used. Alternatively, the cBLAS header interface also supports Intel MKL and ATLAS as a replacement for OpenBLAS. The CPU could also be used with clBLAS. This is not advisable as clBLAS is optimized for GPUs that have a different memory architecture than CPUs. Although there is a cache hierarchy on the CPU, the GPU must use fast local memory to temporarily cache blocks of the matrix (from global memory). Local memory does not exist on the PU."}, {"heading": "6.4 Device Memory", "text": "Since all networks are run with almost all output sizes (up to the limitations given by the layers, such as the divisibility of the input functions, see Chapter 3), the networks can be configured to adjust the memory and computing constraints given by each device, so the networks do not need to be retrained in this case and the results are numerically identical. A second goal may be to use the output sizes of the memory that correspond to the dataset: non-square outputs such as 256 by 32 pixels are also possible, and then the sizes can be set so that the image sizes are divisible to the process by the output sizes of the network. Thus, it is also important that no output sizes are taken into account, that there are no simple rules of scaling how the storage requirements change, as this depends on the number of output types and their sizes at all levels, as well as the maximum conversion."}, {"heading": "6.5 Labeling Throughput", "text": "The Labeling Throughput (Table 6.5) is a general performance measure for neural networks and also shows how different devices and backends perform. Even on the CPU, using the fastest network (U) results in an acceleration of 447 compared to what was achievable with Caffe [6] before Project Greentea [5]. Using a network (SK) that delivers identical results to the original SW network, the CPU acceleration is still a factor of 48 x. AMD GPUs allow acceleration of 54 x (SK-Net), 437 x (U-Net) and 320 x (USK-Net) compared to SW-Net. Similarly, the nVidia GPU scales to OpenCL (SK: 33 x, U: 459 x, USK: 315 x) and CUDA (SK: 57 x, U: 711 x, USK: 442) as the fastest execution on the OpenCU."}, {"heading": "SW SK USK U", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "SW SK USK U", "text": "Network performance during training was not evaluated in this report, as training the models based on available data was not a limiting goal.466.6. Layer performance analysis is generally slower than forwarding, as differential maps, gradients and weight updates need to be calculated."}, {"heading": "6.6 Layer Performance Analysis", "text": "This section breaks down neural networks into layers to assess why certain networks are faster than others and find the potential to optimize models."}, {"heading": "6.6.1 SK-Net", "text": "Looking at Figure 6.5, the folding layers, and in particular the IP1 layer (83%), are responsible for the overall performance. There are many input (192) and output (1024) functions on the ip1 layer (see Section 3.3) and therefore high computing complexity (O > w2 \u00b7 (2 \u00b7 fin \u00b7 k2 \u2212 1)) (see Section 5.5) on the ip1 layer. Since the folding layers consist only of a fast memory copying operation to arrange the data so that matrix multiplications are possible by an optimized BLAS, and the SGEMM (single precision matrix layer multi-476.6) consist only of a fast memory copying operation so that matrix multiplications are possible by an optimized BLAS."}, {"heading": "6.6.2 U-Net", "text": "49In U-Net, most coils have the same order of complexity (see Table 6.7).The result is a very balanced network in terms of forward times (see Figure 6.7).The trade-off is between the size of the feature map and the number of feature map in the middle of the network.The upconvolution layers each contribute up to 5.2% of the network forwarding time.This is less efficient than the optimum, as only the 4-next neighbor interpolation is calculated with constant weight.This would be no more effort than a memory copying process when forwarding and accumulating the four closest neighbors during the backward computation. Currently, it is implemented with a caffe deconvolution, which is a reverse convolution layer. This layer already existed in caffe, while no direct upsampling of feature maps is impled.The evolution cores are groupgroupgrouped, i.e. each upampling kernel is considered only one."}, {"heading": "6.6.3 USK-Net", "text": "516.6. Layer Performance AnalysisLayer GFLOP AMD OCL nV OCL nV CUDA Intel OCL Conv1 0.64 2.51% 4.97% 7.08% 5.43% Conv2 13.75 21.55% 19.25% 42.11% 8.04% Conv3 26.25 27.06% 20.14% 62.58% 11.73% Conv4 21.81 26.99% 21.78% 62.61% 12.57% Conv5 18.92% 24.93% 27.93% 63.93% 13.06% IP1 141.76 45.93% 31.25% 86.64% 27.82% IP2 4.43 23.53% 28.14% 72.69% 42.59% Conv6 4.42 23.56% 21.29% 63.39% 31.44% Conv7 29.44 27.31% 27.88% 61.29% 17.26% Conv8 9.66 18.44% 20.45% 46.17% ip3 0.53% ipcio ip3% 36% Confin 1 0.73% Concio 1 0.36%"}, {"heading": "6.7 NUMA Issues", "text": "In fact, most of them will be able to play by the rules they have established in the past."}, {"heading": "6.8 Alexnet", "text": "To compare how backends and devices behave in a widely used image classification network using minibatches (with n = 10, w = 227, fin = 3) and multiple OpenCL queues (q = 8), the Alexnet [1] included in the Caffe library was also evaluated.The CUDA backend has an advantage over the OpenCL backend, but is less versatile; the AMD GPU seems to be less efficient than the nVidia GPU with minibatches and smaller matrix multiplications than the nVidia GPU, so the AMD GPU performs worse on the same backend than the nVidia GPU. With the SK-, USK- and U-Net (Section 6,5), the AMD GPU works better using the same (OpenCL) backend (OpenCL)."}, {"heading": "7.1 Introduction", "text": "This chapter presents the results of the training of the models in Chapter 3.The evaluation is based on: \u2022 Two datasets (DS1 and DS2: see Chapter 2). \u2022 Three models (SK, U, USK: see Chapter 3). \u2022 Two training functions (Softmax and Maliis). \u2022 One processing loss function (Softmax). \u2022 Three training configurations (Softmax, Maliis and Softmax + Maliis). \u2022 10,000 training sequences per configuration, or 20,000 for combined training. \u2022 Three error targets (margin, distortion and pixel error). The number of training sequences has been selected so that training of all 18 combinations was possible within one week on two AMD W9100 GPUs, resulting in a total of 10 TFLOP / s. [22] The training data in both cases is not very large (see Chapter 2), and therefore the loss for each training method should be converted."}, {"heading": "7.2 Analysis on DS1", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "7.2.1 Training", "text": "The training parameters used on DS1 were set so that they did not use the error masking functionality. Masking typically gives thicker membranes (background) labels when 567.2. Analysis on DS1 used with Softmax, weighing the amount of background errors against the foreground (cell interior). Mali's loss was carried out without first using a patch to prefer training patches based on her label histogram. Softmax loss on the other hand was used with the previously enabled patch, justified by the different character of the loss functions: Softmax calculates a per-pixel error, while Mali's error is only reported for problematic pixels that can be very focused on a few pixels (see Section 5.3.2). In the patch pre-processing step, the images were improved with CLAHE (Contrast-limited adaptive Histogram Qualification) and normalized in the range of 1.0, [\u2212 1.0]."}, {"heading": "7.2.2 Numerical", "text": "577.2. Analysis on DS1Explanations on Tables 7.1 and 7.2: \u2022 Rank: The internal ranking, as indicated by the margin error. \u2022 Loss function: Softmax displays 10,000 iterations with the Softmax loss function. Malian indicates 10,000 iterations with the Malian loss function. If both are specified, the training was performed with 10,000 Softmax and then 10,000 Malian training iterations. \u2022 Rand, Warping and Pixel: Error metrics, as proposed by Jain et al. [27] and used in the ISBI 2012 challenge [11]. An evaluation script is available for Fiji [13] in the Caffe Neural Models Repository. [9] As expected, the ranking on DS1 dataset is: Taking the average rank, USK-Net performs better than SK-Net, which is more accurate than U-Net. The same applies to training methods using Softmax + Malpixel to minimize the margin error than U-Net."}, {"heading": "7.2.3 Visual", "text": "In fact, most of us are able to play by the rules they have imposed on ourselves. (...) It's not as if they play by the rules. (...) It's not as if they play by the rules. (...) It's not as if they play by the rules. (...) It's not as if they play by the rules. (...) It's not as if they play by the rules. (...) It's not as if they play by the rules. (...) (...) It's as if they play by the rules. \"(...) It's not as if they play by the rules.\" (...)"}, {"heading": "7.3 Analysis on DS2", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "7.3.1 Training", "text": "The training on DS2 was similar to the training on DS1. The main difference is that the patch was not previously used on Softmax and Mali. Instead, the error masking was activated when using Softmax, which leads to thicker edges. This is motivated by the fact that the input images are somewhat blurred and therefore the cell membranes are less sharp and harder to distinguish from the cell interior. Here, the SK network already converged through 2000 Softmax training processes before switching to Mali.617.3. Analysis on DS2 was not possible either."}, {"heading": "7.3.2 Numerical", "text": "Interestingly, the USK-Net with Softmax + Malian loss training unexpectedly performs worse on the DS2 dataset than on the DS1, where it performed best. DS1 and DS2 have in common that the USK network, in combination with Softmax training, performs best on pixel error. Overall, this dataset is much more difficult to explain than on DS1. A visual check shows that the USK-Net was often too confident in labeling the cell interior that connected cells that were to be separated, but this did not happen on the DS1 dataset."}, {"heading": "U 0.0382 0.000353 0.0611", "text": "Finally, the results of U-Net by Ronneberger et al. [2] (Table 7.3) could not be achieved, probably because fewer transformations were used to augment training data. It is not per se clear whether the U-network would perform better than SK and USK given the larger training data set. Ideas for improving training include: \u2022 Weight maps to scale the loss rather than just mask it. \u2022 Add elastic deformations, shifting and scaling instead of just rotation and mirroring to increase the amount of training data. \u2022 Experiment with loss functions other than Maliis and Softmax or alternate them during training."}, {"heading": "7.3.3 Visual", "text": "The visual analysis is based on image number 16 of the DS2 test stack (see Section 2.2).62 Explanation of the labels in Figure 7.2: The visual results of Softmax training (a to c) have thicker membrane labels than on DS1. This is because error masking has been activated here and the network has consequently seen the same amount of error pixels both in the foreground and in the background. (A) A bright spot that is an error in the electron microscopy image. The affected membrane is misclassified by all networks and training sessions. Local contrast improvement with CLAHE has not helped here. (B) Mitochondrion close to a cell membrane. The USK network removes the membrane with all training and training (c, f and i)."}, {"heading": "8.1 Research Time Line", "text": "An overview of the research timeframe to give a context of what influenced the goals and decisions taken during the project: From To Activity / Event 05.11.2014 05.11.2014 Proposal for cooperation at UZH INI. 09.11.2014 22.01.2014 Idea discussion with Dr. Jan Funke. 14.12.2014 14.12.2014 Hongsheng Li et al. Publication (SK cores) [7]. 07.02.2015 07.02.2015 Discussion of the project proposal with AMD [25]. 23.02.2015 23.02.2015 Start of research. 26.02.2015 06.03.2015 Getting the sliding window network up and running [10]. 08.03.2015 18.04.2015 OpenCL backend development [5]. 10.04.2015 21.04.2015 Discussion of the project proposal with AMD [25]. 22.04.2015 22.04.2015 Arrival of the AMD-W9100 GPU (hardware sponsorship). 19.04.2015 19.04.2015 Pull application of the modified Caffe for BVLC [4]."}, {"heading": "8.2 Implications", "text": "The first idea for the research project was the implementation of broken cores. However, with the publication of Hongsheng Li et al. [7] paper the problem was already solved. We got their source code and I was able to translate existing sliding window networks into advanced kernel networks. These events lead to a shift of focus to the OpenCL backend and support a variety of hardware. This was important to see how existing CPU clusters and AMD GPUs work instead of just nVidia GPUs.A nice side effect of completely rewriting the entire Caffe library to OpenCL was a complete understanding of the library, the bottlenecks, how all layers work and what are the main goals for optimization and network design. It turned out that operating networks across devices gives no advantage in the case of SK, U and USK architectures, as perfect scaling is possible when independent instances of the network are running on each device."}, {"heading": "8.3 Difficulties Encountered", "text": "The obvious difficulty was to keep up with general research in the area of pixel-by-pixel classification of images, as important papers [7], [2] were published during the project research. It was also a lot of work to keep up with the changes in the Caffe library, as they changed many core aspects such as the network file format and shape specifications for memory blobs, breaking compatibility with the existing code of Honghsheng Li et al.'s approach [7] as well as the existing sliding window network [10], [9]. Constantly drawing new changes from the BVLC master branch [6] and adapting my own branch to these changes was necessary, the advantage being that backward compatibility with the official version is always ensured and that my own branch had some scope throughout the scope of the project."}, {"heading": "8.4 Reproducibility of Results", "text": "The results obtained in this report are guaranteed to be reproducible by using the following software pipeline, using CUDA or OpenCL hardware equivalent to the hardware used in this report: \u2022 Caffe [5] URL: https: / / github.com / naibaf7 / caffeCommit Checksum: f84c2a4fb8d633bc7d8fc9771eb06a3cf2215212 \u2022 Caffe Neural Tool [16] URL: https: / / github.com / naibaf7 / caffe _ neural _ DDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDhttps: / / / gibDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDdddddddddddddddddd"}, {"heading": "8.5 Outlook", "text": "In this perspective, I will give a brief introduction to future plans for the improved Caffe version [5], extended use cases for the models [9], missing features for the Caffe Neural Tool [16], and ideas that did not fit within the time frame of the project."}, {"heading": "8.5.1 Device Abstracted Backend", "text": "In order to support future multi-device training methods and eliminate redundancy, the backend should be further standardized so that the only remaining code duplication remains with the computing cores actually used in the layers, which will minimize bugs occurring on a single backend and make software verification much easier, and will reduce the time it takes for newly developed layers to be available on all devices. At the time of the project, the improved caffe library [5] will restrict full support for the old CPU backend to CPUs in favor of an OpenCL hybrid solution (see Section 5.4.2), and some tuning to work properly on NUMA processors (see Section 6.7) is still required."}, {"heading": "8.5.2 Improving Training Data", "text": "As a first step to improving results, all network architectures should be evaluated on the basis of more training data obtained artificially or from more basic truths. It is advisable to test all models on the basis of a given data set, as there is no clear winner among the networks. Results can vary widely, as the numerical analysis showed (see Section 7.2 versus 7.3)."}, {"heading": "8.5.3 Parameter Grid Search", "text": "The network architectures presented here are only examples of a whole class of possible networks. Many parameters such as kernel sizes and how SK networks can be combined with U-like networks can be evaluated. Especially deep multi-path networks can be useful by merging the feature maps of different architectures before making the output label predictions."}, {"heading": "8.5.4 Testing of Volumetric Architectures", "text": "Depending on the dataset, SK, U, and USK networks can be configured for 3D in much more ways than for 2D. For example, the depth direction is likely to have a lower physical resolution than width and height due to the way the data is captured with sections and electron microscopy. This should be taken into account when selecting the core size, kernel step, and span of the depth dimension. An example is the ISBI 2012 dataset, which includes 2 x 2 x 1.5 microns with a resolution of 4 x 4 x 50 nm / pixel [14], [11], [15].698.6."}, {"heading": "8.5.5 Improving Test Metrics", "text": "The visual inspection and error metrics used in this report can only tell you how accurately the label predictions are compared to the truth on the ground. In Connectomics, this may not be the most important goal. The tools that process the segmentations may be able to correct certain errors in the predictions by testing how likely the end result is, while other merge and split errors lead to uncorrectable errors, a goal that could be more useful in selecting the network architecture, loss function, and training method."}, {"heading": "8.6 Final Words", "text": "This research project brings together many disciplines, such as high performance computing, machine learning, visual computing and a bit of connectomics. I was able to implement most of the features originally planned and find substitutes for ideas that have proven to be poor. Finally, the research results include a useful, versatile stack of open source software (Project Greentea) that can be expanded in the future. A growing user base has already been built up during the work on the project [29]. The models and tools introduced with this project can be efficiently used on a wide variety of data sets and hardware, making it very flexible. Also, the collaboration at HMI Janelia has been a great experience. Hardware sponsorship by AMD shows that programming the Project Greentea and efficient machine learning libraries in general are also of great interest to hardware builders.70"}, {"heading": "Appendix A", "text": "Network Architecture 71A.1. SK-NetA.1 SK-Net (0.1) pool2 (MAX PoolingSK) Kernel Size: 2 Step: 1 Step: 1 Step: 1 Step: 1 Step: 1 Step: 0 Step: 0 Step: 1 Step: 0 Step: 1 Step: 1 Step: 1 Step: 1 Step: 1 Step: 1 Step: 1 Step: 1 Step: 0 Step: 0 Step: 1 Step: 1 Step: 1 Step: 1 Step: 1 Step: 1 Step: 1 Step: 1 Step: 1 Step: 1 Step: 1 Step: 1 Step: 1 Step: 1 Step: 1 Step: 1 Step: 1 Step: 1 Step: 1 Step: 1 Step: 1 Step: 1 Step: 1 Step: 1 Step: 1 Step: 1 Step: 1 Step: 1 Step: 1 Step: 1 Step: 1 Step: 1 Step: 1 Step: 1 Step: 1 Step: 1 Step: 1 Step: 1 Step: 1 Step: 1 Step: 1 Step: 1 Step: 1 Step: 1 Step: 1 Step: 1 Step: 1 Step: 1: 1 Step: 1 Step: 1: 1 Step: 1 Step: 1: 1 Step: 1: 1 Step: 1: 1 Step: 1: 1 Step: 1 Step: 1: 1 Step: 1 Step: 1: 1: 1 Step: 1: 1 Step: 1: 1 Step: 1: 1 Step: 1: 1: 1 Step: 1 Step: 1 Step: 1 Step: 1: 1: 1: 1 Step: 1 Step: 1: 1: 1: 1 Step: 1 Step 1: 1: 1 Step: 1: 1 Step: 1: 1: 1: 1: 1 Step: 1: 1 Step 1: 1: 1: 1: 1 Step: 1: 1: 1: 1: 1 Step: 1: 1: 1 Step: 1: 1: 1: 1: 1"}], "references": [{"title": "ImageNet Classification with Deep Convolutional Neural Networks", "author": ["Alex Krizhevsky", "Ilya Sutskever", "Geoffrey E. Hinton"], "venue": "Advances in Neural Information Processing Systems 25", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2012}, {"title": "U-Net: Convolutional Networks for Biomedical Image Segmentation", "author": ["O. Ronneberger", "P. Fischer", "T. Brox"], "venue": "ArXiv e-prints (May 2015)", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2015}, {"title": "Maximin affinity learning of image segmentation", "author": ["S.C. Turaga"], "venue": "ArXiv e-prints (Nov", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2009}, {"title": "Caffe: Convolutional Architecture for Fast Feature Embedding", "author": ["Yangqing Jia"], "venue": null, "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2014}, {"title": "Highly Efficient Forward and Backward Propagation of Convolutional Neural Networks for Pixelwise Classification", "author": ["H. Li", "R. Zhao", "X. Wang"], "venue": "ArXiv e-prints (Dec", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2014}, {"title": "Segmented anisotropic ssTEM dataset of neural tissue", "author": ["Stephan Gerhard"], "venue": "url: http://dx.doi.org/10.6084/m9.figshare.856713 (visited on 20th Aug", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2013}, {"title": "Sliding Window Network", "author": ["Julien Martel"], "venue": "url: https://www.ini.uzh.ch/ people/jmartel (visited on 20th Aug", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2015}, {"title": "Caffe ND convolutions. url: https", "author": ["Jeff Donahue"], "venue": "BVLC / caffe/pull/2049 (visited on 20th Aug. 2015)", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2015}, {"title": "An Integrated Micro- and Macroarchitectural Analysis of the Drosophila Brain by Computer-Assisted Serial Section Electron Microscopy", "author": ["Albert Cardona"], "venue": "PLoS Biology", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2010}, {"title": "Malis criterion for Matlab. url: https", "author": ["Srinivas Turaga"], "venue": "github . com / srinituraga/malis/ (visited on 20th Aug", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2015}, {"title": "Malis criterion for Torch", "author": ["Srinivas Turaga"], "venue": "url: https://github.com/srinituraga/ lua---imgraph/ (visited on 20th Aug", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2015}, {"title": "ViennaCL - A High Level Linear Algebra Library for GPUs and Multi-Core CPUs", "author": ["K. Rupp", "F. Rudolf", "J. Weinbub"], "venue": "In: Intl. Workshop on GPUs and Scientific Applications", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2010}, {"title": "cuDNN: Efficient Primitives for Deep Learning", "author": ["S. Chetlur"], "venue": "ArXiv e-prints (Oct", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2014}, {"title": "Fast Convolutional Nets With fbfft: A GPU Performance Evaluation", "author": ["Nicolas Vasilache"], "venue": null, "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2014}, {"title": "Boundary Learning by Optimization with Topological Constraints", "author": ["Viren Jain"], "venue": "IEEE Computer Society Conference on Computer Vision and Pattern Recognition. Institute of Electrical & Electronics Engineers (IEEE),", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2010}, {"title": "Caffe Pull Request", "author": ["Fabian Tschopp"], "venue": "url: https://github.com/BVLC/caffe/ pull/2610 (visited on 20th Aug", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2015}], "referenceMentions": [{"referenceID": 0, "context": "A prominent example is the ImageNet / AlexNet [1] for object recognition.", "startOffset": 46, "endOffset": 49}, {"referenceID": 1, "context": "Recent networks [2] can have very many, in this case over 20, layers and millions of learnable parameters.", "startOffset": 16, "endOffset": 19}, {"referenceID": 2, "context": "Therefore, this research also considers different training methods including the Malis [3] criterion.", "startOffset": 87, "endOffset": 90}, {"referenceID": 3, "context": "Caffe stands for Convolutional Architecture for Fast Feature Embedding [4].", "startOffset": 71, "endOffset": 74}, {"referenceID": 4, "context": "[7] allows to make existing SW networks more efficient while giving identical prediction results (see Section 3.", "startOffset": 0, "endOffset": 3}, {"referenceID": 5, "context": "(Raw image source: ssTEM [8], [9]).", "startOffset": 25, "endOffset": 28}, {"referenceID": 5, "context": "(Raw image source: ssTEM [8], [9]).", "startOffset": 25, "endOffset": 28}, {"referenceID": 6, "context": "This technical report is based on the following existing work: \u2022 SW (sliding window) network designed by Julien Martel [10], not published.", "startOffset": 119, "endOffset": 123}, {"referenceID": 4, "context": "[7].", "startOffset": 0, "endOffset": 3}, {"referenceID": 2, "context": "[3].", "startOffset": 0, "endOffset": 3}, {"referenceID": 3, "context": "\u2022 The Open Source Caffe library maintained by the Berkeley Vision and Learning Center [6], [4].", "startOffset": 91, "endOffset": 94}, {"referenceID": 1, "context": "[2].", "startOffset": 0, "endOffset": 3}, {"referenceID": 7, "context": "\u2022 N-dimensional convolution kernels by Jeff Donahue [12].", "startOffset": 52, "endOffset": 56}, {"referenceID": 1, "context": "[2] network architecture, see Section 3.", "startOffset": 0, "endOffset": 3}, {"referenceID": 5, "context": "1: DS1 ssTEM raw image, 512 by 512 pixels of image 2 (right upper corner) (Source: ssTEM [8], [9]).", "startOffset": 89, "endOffset": 92}, {"referenceID": 5, "context": "This data set shows neural tissue from a Drosophila larva ventral nerve cord and was acquired using serial section Transmission Electron Microscopy at HHMI Janelia Research Campus [8].", "startOffset": 180, "endOffset": 183}, {"referenceID": 5, "context": "1 (Source: ssTEM [8], [9]).", "startOffset": 17, "endOffset": 20}, {"referenceID": 8, "context": "This data set is from the ISBI 2012 challenge [11], [14], [15].", "startOffset": 58, "endOffset": 62}, {"referenceID": 6, "context": "The sliding window network described here was developed by Julien Martel [10].", "startOffset": 73, "endOffset": 77}, {"referenceID": 4, "context": "[7].", "startOffset": 0, "endOffset": 3}, {"referenceID": 4, "context": "[7] provide a pseudo code (page 4) on how to convert a sliding window network to a strided kernel network.", "startOffset": 0, "endOffset": 3}, {"referenceID": 1, "context": "paper [2].", "startOffset": 6, "endOffset": 9}, {"referenceID": 1, "context": "The configurations of this U-Net included in the Caffe Neural Models [9] are therefore incompatible to the original work [2].", "startOffset": 121, "endOffset": 124}, {"referenceID": 1, "context": "This is the same as used in the original paper [2].", "startOffset": 47, "endOffset": 50}, {"referenceID": 1, "context": "[2].", "startOffset": 0, "endOffset": 3}, {"referenceID": 0, "context": "3, p \u2208 [0, 1] is a random value picked at uniform and ci \u2208 [0, 1] the label frequency for label i = 0, .", "startOffset": 7, "endOffset": 13}, {"referenceID": 0, "context": "3, p \u2208 [0, 1] is a random value picked at uniform and ci \u2208 [0, 1] the label frequency for label i = 0, .", "startOffset": 59, "endOffset": 65}, {"referenceID": 4, "context": "[7].", "startOffset": 0, "endOffset": 3}, {"referenceID": 7, "context": "There were existing kernels for normal N-dimensional convolutions by Jeff Donahue [12].", "startOffset": 82, "endOffset": 86}, {"referenceID": 9, "context": "The implementation in Caffe [5] that I provide is based on existing code to compute the Malis criterion for Matlab [17] and Torch [18] by Srinivas Turaga et al.", "startOffset": 115, "endOffset": 119}, {"referenceID": 10, "context": "The implementation in Caffe [5] that I provide is based on existing code to compute the Malis criterion for Matlab [17] and Torch [18] by Srinivas Turaga et al.", "startOffset": 130, "endOffset": 134}, {"referenceID": 2, "context": "[3].", "startOffset": 0, "endOffset": 3}, {"referenceID": 2, "context": "[3].", "startOffset": 0, "endOffset": 3}, {"referenceID": 11, "context": "1 Implementation In my version of the Caffe library, an additional versatile backend for various compute devices, based on OpenCL and ViennaCL [19], is available.", "startOffset": 143, "endOffset": 147}, {"referenceID": 12, "context": "The cuDNN library also implements a modified form of GEMM convolutions, and they also evaluated FFT and direct convolution as options [20].", "startOffset": 134, "endOffset": 138}, {"referenceID": 13, "context": "[21]), but managing the device memory remains difficult.", "startOffset": 0, "endOffset": 4}, {"referenceID": 11, "context": "As ViennaCL [19] was used as a part of the OpenCL backend, ViennaCL-BLAS is also available as an alternative to clBLAS.", "startOffset": 12, "endOffset": 16}, {"referenceID": 12, "context": "For CUDA, the solution in this case is to use cuDNN, which streams the convolutions in batches to be more efficient [20].", "startOffset": 116, "endOffset": 120}, {"referenceID": 12, "context": "Using minibatches (n > 1) can increase the GPU utilization on OpenCL when using multiple queues (q > 1) and the efficiency on CUDA when using cuDNN [20].", "startOffset": 148, "endOffset": 152}, {"referenceID": 0, "context": "For comparison how the backends and devices perform on a widely used network for image classification that uses minibatches (with n = 10, w = 227, fin = 3) and multiple OpenCL queues (q = 8), the Alexnet [1] included in the Caffe library was also evaluated.", "startOffset": 204, "endOffset": 207}, {"referenceID": 14, "context": "[27] and used in the ISBI 2012 challenge [11].", "startOffset": 0, "endOffset": 4}, {"referenceID": 5, "context": "1: DS1 ssTEM raw and corresponding ground truth, 1024 by 1024 pixels (Source: ssTEM [8], [9]).", "startOffset": 84, "endOffset": 87}, {"referenceID": 1, "context": "[2]).", "startOffset": 0, "endOffset": 3}, {"referenceID": 1, "context": "[2] (Table 7.", "startOffset": 0, "endOffset": 3}, {"referenceID": 4, "context": "paper released (SK kernels) [7].", "startOffset": 28, "endOffset": 31}, {"referenceID": 6, "context": "2015 Getting the sliding window network to work [10].", "startOffset": 48, "endOffset": 52}, {"referenceID": 3, "context": "2015 Pull request of the modified Caffe to BVLC [4].", "startOffset": 48, "endOffset": 51}, {"referenceID": 1, "context": "paper released (U-Net) [2].", "startOffset": 23, "endOffset": 26}, {"referenceID": 4, "context": "[7] paper, the problem already got solved.", "startOffset": 0, "endOffset": 3}, {"referenceID": 1, "context": "paper [2], the research focus was shifted to analyzing the U-Net approach, which is able to classify up to one megapixel per second.", "startOffset": 6, "endOffset": 9}, {"referenceID": 2, "context": "Looking at ISBI 2012 results [11] and their test metrics, as well as the fact that one of the authors of the Malis criterion [3], Dr.", "startOffset": 125, "endOffset": 128}, {"referenceID": 4, "context": "The obvious difficulty was to keep up with the general research in pixelwise classification of images, as important papers [7], [2] were released during the project research.", "startOffset": 123, "endOffset": 126}, {"referenceID": 1, "context": "The obvious difficulty was to keep up with the general research in pixelwise classification of images, as important papers [7], [2] were released during the project research.", "startOffset": 128, "endOffset": 131}, {"referenceID": 3, "context": "It was also a lot of work to keep up with the changes of the Caffe library [4], as they changed many core aspects such as network file format and shape specifications for memory blobs.", "startOffset": 75, "endOffset": 78}, {"referenceID": 4, "context": "\u2019s approach [7] as well as the existing sliding window network [10], [9].", "startOffset": 12, "endOffset": 15}, {"referenceID": 6, "context": "\u2019s approach [7] as well as the existing sliding window network [10], [9].", "startOffset": 63, "endOffset": 67}, {"referenceID": 8, "context": "5 microns with a resolution of 4 by 4 by 50 nm/pixel [14], [11], [15].", "startOffset": 65, "endOffset": 69}, {"referenceID": 15, "context": "During working on the project, it was already possible to establish a growing user base [29].", "startOffset": 88, "endOffset": 92}], "year": 2015, "abstractText": "This work presents and analyzes three convolutional neural network (CNN) models for efficient pixelwise classification of images. When using convolutional neural networks to classify single pixels in patches of a whole image, a lot of redundant computations are carried out when using sliding window networks. This set of new architectures solve this issue by either removing redundant computations or using fully convolutional architectures that inherently predict many pixels at once. The implementations of the three models are accessible through a new utility on top of the Caffe library. The utility provides support for a wide range of image input and output formats, pre-processing parameters and methods to equalize the label histogram during training. The Caffe library has been extended by new layers and a new backend for availability on a wider range of hardware such as CPUs and GPUs through OpenCL. On AMD GPUs, speedups of 54\u00d7 (SK-Net), 437\u00d7 (U-Net) and 320\u00d7 (USKNet) have been observed, taking the SK equivalent SW (sliding window) network as the baseline. The label throughput is up to one megapixel per second. The analyzed neural networks have distinctive characteristics that apply during training or processing, and not every data set is suitable to every architecture. The quality of the predictions is assessed on two neural tissue data sets, of which one is the ISBI 2012 challenge data set. Two different loss functions, Malis loss and Softmax loss, were used during training. The whole pipeline, consisting of models, interface and modified Caffe library, is available as Open Source software under the working title Project Greentea.", "creator": "LaTeX with hyperref package"}}}