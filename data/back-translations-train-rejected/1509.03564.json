{"id": "1509.03564", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "11-Sep-2015", "title": "Lazy Factored Inference for Functional Probabilistic Programming", "abstract": "Probabilistic programming provides the means to represent and reason about complex probabilistic models using programming language constructs. Even simple probabilistic programs can produce models with infinitely many variables. Factored inference algorithms are widely used for probabilistic graphical models, but cannot be applied to these programs because all the variables and factors have to be enumerated. In this paper, we present a new inference framework, lazy factored inference (LFI), that enables factored algorithms to be used for models with infinitely many variables. LFI expands the model to a bounded depth and uses the structure of the program to precisely quantify the effect of the unexpanded part of the model, producing lower and upper bounds to the probability of the query.", "histories": [["v1", "Fri, 11 Sep 2015 15:45:39 GMT  (306kb)", "http://arxiv.org/abs/1509.03564v1", null]], "reviews": [], "SUBJECTS": "cs.AI", "authors": ["avi pfeffer", "brian ruttenberg", "amy sliva", "michael howard", "glenn takata"], "accepted": false, "id": "1509.03564"}, "pdf": {"name": "1509.03564.pdf", "metadata": {"source": "CRF", "title": "Lazy Factored Inference for Functional Probabilistic Programming", "authors": ["Avi Pfeffer", "Brian Ruttenberg", "Amy Sliva", "Michael Howard"], "emails": ["gtakata}@cra.com"], "sections": [{"heading": null, "text": "Probabilistic programming provides the means to visualize and rationalize complex probabilistic models using programming language constructs. Even simple probabilistic programs can generate models with an infinite number of variables. Factored inference algorithms are often used for probabilistic graphical models, but cannot be applied to these programs because all variables and factors must be enumerated. In this paper, we introduce a new inference framework, the lazy factored inference (LFI), which makes it possible to use factored algorithms for models with an infinite number of variables. LFI extends the model to a limited depth and uses the structure of the program to precisely quantify the effect of the unexpanded part of the model, generating lower and upper limits to the probability of the query."}, {"heading": "1. INTRODUCTION", "text": "This year, it is closer than ever before in the history of the country."}, {"heading": "2. RANDOM LISTS EXAMPLE", "text": "As a simple running example, we use a model that generates random lists of unlimited length. Each list contains the symbol \"a\" or the symbol \"b.\" The lists are generated by a generator function that leads the list of random lists. This list of random generators can be defined in terms of both probability and number of random lists."}, {"heading": "3. THE LFI ALGORITHM FOR PROBABILISTIC PROGRAMMING", "text": "The main intuition is that variables that are far from the query and evidence have little impact on the query. In other words, these variables can be contextually independent of the values of the extended variables. LFI extends the model to a limited depth, exploring only relevant parts of the model. For example, from a partial expansion of our random list model to the first n elements of a list l, we can calculate: \u2022 p1 = P has the length \u2264 n and does not contain \"b.\""}, {"heading": "4. THE STEPS OF LFI", "text": "We will now give details of the four steps of the LFI algorithm for PP and its implementation by means of Figaro."}, {"heading": "4.1 STEP 1: EXPAND THE MODEL", "text": "The first step of the LFI algorithm is to expand the model to a depth starting with the query and the evidence. This step must determine which variables are relevant when the model is extended to that depth and the range of each relevant variable, which is a set of extended values (possibly including *).We present two approaches to expansion, a basic algorithm (Section 4.1.1) that is suitable for simple queries, and a backtracking version (Section 4.1.2) that can be used to calculate more complex queries and evidences.Section 4.1.3 deals specifically with the lazy expansion of evidence. We explain these algorithms with Figaro constructs, but they are all generalizable to other functional PP languages.Explain from Section 2 that in Figaro a random variable is represented by an element. Some elements are atomic, which means that they do not depend on any arguments (e.g. > - 0.b) (> - 0.6)."}, {"heading": "4.1.1 Basic expansion algorithm", "text": "The basic expansion begins with a list of relevant elements, which consist of the query and the evidence, each of which are presented as Figaro elements, and goes recursively into the depth as following.For a relevant element E: 1 If d < 0, 2, 3, 4, 5, 5, 6, 6, 7, 7, 7, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,"}, {"heading": "4.1.2 Backtracking Expansion", "text": "The above algorithm is sufficient if we expand only a single query without evidence, and if the expansion forms a tree so that no element occurs in more than one path. However, if the same element is used both by the query and by some evidence, or is reachable from the query by more than one path, this basic expansion algorithm encounters a subtle problem where it can calculate inconsistent ranges for the same elements. Suppose we have a query element X and a evidential element Y, and the target depth is 1. Suppose that Y is an argument for an argument of X. If we expand X first, we will eventually extend Y to depth -1, which results in a range of {*}, because Y is an evidence element, we will eventually expand it to depth 1, which leads to another range of elements, which leads to another range of Y."}, {"heading": "4.1.3 Lazily Expanding Evidence", "text": "There is an additional optimization we can make in the expansion phase of LFI. Let's look at a large model with many evidence object elements and a single query. Implementing the above expansion algorithm requires that we extend all evidence object variables regardless of their distance from the query, resulting in a large number of elements. However, as with irrelevant parts of the model represented by *, remote evidence object variables may not be relevant to the query (i.e. there will be no path from the evidence object variables to a query variable within the depth d of the query). Ideally, we expand evidence that is close to the query and can actually contribute to calculating the probability limits. We can achieve this by modifying the basic expansion algorithm so that it expands slowly in multiple iterations, starting only with the query elements Q.1. Set Expandery List = Q Depth with Lazd 2. Expand expanding List for each ExpandE in ExpandE depth."}, {"heading": "4.2 STEP 2: PRODUCE FACTORS FOR THE RELEVANT ELEMENTS", "text": "In fact, most of them are able to play by the rules that they have imposed on themselves, and they are able to play by the rules that they have imposed on themselves."}, {"heading": "4.3 STEP 3: APPLY A FACTORED ALGORITHM", "text": "Based on the factors generated in step 2, we can now determine a response to the query, which is defined as the sum of product expressions over these factors. The goal is to reduce this sum of product expressions over the query variables to a single factor. Factor algorithms such as VE and BP generate solutions or approximations to this factor. In LFI, standard factor algorithms can be applied without modification, but they are only calculated using factors that are representative of the relevant parts of the calculation to answer the query to the desired depth. The default algorithm is called once using the lower limits and once using the upper limits specified in the factors."}, {"heading": "4.4 STEP 4: FINALIZE THE RESULT", "text": "By using a factor algorithm in the previous step, we acquire through the query two factors, one for the lower and one for the upper limits. These factors will generally be unnormalized and * could have a positive probability mass. In this finalization step, we must normalize the results and absorb the probability mass of * into the regular values. Let the unnormalized lower limit of the value i (regular or *) be the query li and let the unnormalized upper limit be ui. Standard normalization takes a series of unnormalized probabilities qi, calculates their sum Z = qi and then calculates pi = qi / Z to obtain the normalized probabilities. In our case, U = Educui is an upper limit of the normalization factor. Therefore, Li = li / U is a lower limit to the normalized probability of the value i. Meanwhile, for a regular value j a probability associated with the regular value i is assigned to a regular probability, so that the upper probability cannot be associated with the regular mass."}, {"heading": "5. ANALYSIS", "text": "Our analysis assumes that there is a single variable, and in fact, multiple variables can break the result if query variables are joined together only after a certain depth. If multiple variables are desired, this can easily be achieved by defining a single variable that can be a tuple of the query variable, making the query variable unsteady. Our result assumes that all variables have already been included before the boundaries begin to converge."}, {"heading": "6. EXPERIMENTATION", "text": "We carried out two initial implementations of the LFI algorithm in Figaro, using VE and BP. We carried out two series of experiments: the first with an infinitely hidden Markov model (HMM) and the second with unlimited and infinitely probable context-free grammars (PCFGs)."}, {"heading": "6.1 INFINITE HMM", "text": "Our first experiments were carried out using a time-limited Markov model (HMM), which has 15 states ranging from 0 to 14. At each step, the state transitions to an adjacent state upwards or downwards are designed in such a way that the probability of a transition downwards or upwards is roughly the same, while the state tends to be near the endpoints to the nearby endpoint. Especially in state s, where 1 \u2264 s \u2264 13 is, the state transitions to state s + 1 show the probability s / 14. The effect of this is that when the state is near the center, there is a significant uncertainty about which endpoint is reached, while closer to the endpoints there is less uncertainty. Each state produces a Boolean endpoint (the emission probabilities are designed to have ambiguity near the center and less ambiguity near the endpoints)."}, {"heading": "6.2 UNBOUNDED AND INFINITE PCFGS", "text": "In this context, it should be noted that this is a mere formality."}, {"heading": "7. DISCUSSION AND RELATED WORK", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "7.1 CONNECTION TO LOGICAL PROBABILISTIC PROGRAMMING", "text": "Some similar ideas have become logical programming languages in the literature. Poole introduced the main ideology of abduction (PHA). Poole's algorithm maintains a priority list of partial declarations that accompanies the query, and the algorithm can be terminated at any time without agreement."}, {"heading": "7.2 OTHER RELATED WORK", "text": "It is as if most of them will be able to move, to be able to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance"}, {"heading": "8. CONCLUSION AND FUTURE WORK", "text": "In this paper, we have presented an algorithm for LFI in PP that makes the factored conclusion a workable framework for full-fledged PP. LFI takes advantage of the fact that not all variables in a probable model are relevant to a particular query, and limits the query probability by examining only the most relevant parts of the model. We have provided a basic algorithm and several optimizations to improve efficiency and accuracy. Experimental results using an implementation of LFI in Figaro show the potential of this approach to provide tractable, factored algorithms for PP. Our main goal for future work is to further develop and analyze a number of specific lazy factored algorithms. Specifically, we would like to investigate the interaction between BP's approximate calculations and the lazy limits. Furthermore, we plan intelligent expansion strategies that expand parts of the program to improve the response to the query most likely."}, {"heading": "57, 97-109.", "text": "Herbrich, R., Minka, T., and Graepel, T. (2006). Trueskill: A Bayesian skill rating system. Advances in Neural Information Processing Systems (NIPS), 569-576. Hughes, J. (1989). Why functional programming matters. The Computer Journal, 32, 98-107.Ihler, A. T. (2012). Accuracy bounds for faith propagation. arXiv Preprint: 1206.5277.Kimmig, A., Demoen, B., De Raedt, L., Costa, V. S., and Rocha, R. (2011). On the implementation of the probabilistic logic programming language ProbLog. Theory and Practice of Logic Programming, 11, 235-262. Kiselyov, O. and Shan, C.-c. (2009). Embedded probabilistic programming."}], "references": [{"title": "Pruning bayesian networks for efficient computation", "author": ["M. Baker", "T.E. Boult"], "venue": "UAI, 225-232.", "citeRegEx": "Baker and Boult,? 1990", "shortCiteRegEx": "Baker and Boult", "year": 1990}, {"title": "Lazy evaluation in Penniless propagation over join trees", "author": ["A. Cano", "S. Moral", "A. Salmer\u00f3n"], "venue": "Networks, 39, 175-185.", "citeRegEx": "Cano et al\\.,? 2002", "shortCiteRegEx": "Cano et al\\.", "year": 2002}, {"title": "Anytime Lifted Belief Propagation", "author": ["R. de Salvo Braz", "S. Natarajan", "H. Bui", "J. Shavlik", "S. Russell"], "venue": "ILP-MLG-SRL", "citeRegEx": "Braz et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Braz et al\\.", "year": 2009}, {"title": "Bucket elimination: A unifying framework for reasoning", "author": ["R. Dechter"], "venue": "Artificial Intelligence, 113, 41-", "citeRegEx": "Dechter,? 1999", "shortCiteRegEx": "Dechter", "year": 1999}, {"title": "Dynamic construction of belief networks", "author": ["R.P. Goldman", "E. Charniak"], "venue": "UAI, 90-97.", "citeRegEx": "Goldman and Charniak,? 1990", "shortCiteRegEx": "Goldman and Charniak", "year": 1990}, {"title": "Church: A language for generative models", "author": ["N.D. Goodman", "V.K. Mansinghka", "D. Roy", "K. Bonawitz", "J.B. Tenenbaum"], "venue": "UAI.", "citeRegEx": "Goodman et al\\.,? 2008", "shortCiteRegEx": "Goodman et al\\.", "year": 2008}, {"title": "Generating Bayesian networks from probability logic knowledge bases", "author": ["P. Haddawy"], "venue": "UAI, 262-269.", "citeRegEx": "Haddawy,? 1994", "shortCiteRegEx": "Haddawy", "year": 1994}, {"title": "Monte Carlo sampling methods using Markov chains and their applications", "author": ["W.K. Hastings"], "venue": "Biometrika, 57, 97-109.", "citeRegEx": "Hastings,? 1970", "shortCiteRegEx": "Hastings", "year": 1970}, {"title": "Trueskill: A Bayesian skill rating system", "author": ["R. Herbrich", "T. Minka", "T. Graepel"], "venue": "Advances in Neural Information Processing Systems (NIPS), 569-576.", "citeRegEx": "Herbrich et al\\.,? 2006", "shortCiteRegEx": "Herbrich et al\\.", "year": 2006}, {"title": "Why functional programming matters", "author": ["J. Hughes"], "venue": "The Computer Journal, 32, 98-107.", "citeRegEx": "Hughes,? 1989", "shortCiteRegEx": "Hughes", "year": 1989}, {"title": "Accuracy bounds for belief propagation", "author": ["A.T. Ihler"], "venue": "arXiv Preprint:1206.5277.", "citeRegEx": "Ihler,? 2012", "shortCiteRegEx": "Ihler", "year": 2012}, {"title": "On the implementation of the probabilistic logic programming language ProbLog", "author": ["A. Kimmig", "B. Demoen", "L. De Raedt", "V.S. Costa", "R. Rocha"], "venue": "Theory and Practice of Logic Programming, 11, 235-262.", "citeRegEx": "Kimmig et al\\.,? 2011", "shortCiteRegEx": "Kimmig et al\\.", "year": 2011}, {"title": "Embedded probabilistic programming", "author": ["O. Kiselyov", "Shan", "C.-c."], "venue": "Domain-Specific Languages, 360-384.", "citeRegEx": "Kiselyov et al\\.,? 2009", "shortCiteRegEx": "Kiselyov et al\\.", "year": 2009}, {"title": "Effective Bayesian inference for stochastic programs", "author": ["D. Koller", "D. McAllester", "A. Pfeffer"], "venue": "AAAI.", "citeRegEx": "Koller et al\\.,? 1997", "shortCiteRegEx": "Koller et al\\.", "year": 1997}, {"title": "Local Computation with Probabilities on Graphical Structures and Their Applications to Expert Systems", "author": ["S.L. Lauritzen", "D.J. Spiegelhalter"], "venue": "Journal of the Royal Statistical Society B, 50.", "citeRegEx": "Lauritzen and Spiegelhalter,? 1988", "shortCiteRegEx": "Lauritzen and Spiegelhalter", "year": 1988}, {"title": "An empirical evaluation of possible variations of lazy propagation", "author": ["A.L. Madsen"], "venue": "UAI, 366-373.", "citeRegEx": "Madsen,? 2004", "shortCiteRegEx": "Madsen", "year": 2004}, {"title": "Turbo decoding as an instance of Pearl's belief propagation algorithm", "author": ["R.J. McEliece", "D.J. Mackay", "J.F. Cheng"], "venue": "IEEE Journal on Selected Areas in Communication, 16, 140-152.", "citeRegEx": "McEliece et al\\.,? 1998", "shortCiteRegEx": "McEliece et al\\.", "year": 1998}, {"title": "Equations of state calculations by fast computing machines", "author": ["N. Metropolis", "A.W. Rosenbluth", "M.N. Rosenbluth"], "venue": "Chemistry and Physics, 21.", "citeRegEx": "Metropolis et al\\.,? 1953", "shortCiteRegEx": "Metropolis et al\\.", "year": 1953}, {"title": "BLOG: Probabilistic models with unknown objects", "author": ["B. Milch", "B. Marthi", "S. Russell", "D. Sontag", "D.L. Ong", "A. Kolobov"], "venue": "IJCAI, 1352-1359.", "citeRegEx": "Milch et al\\.,? 2005", "shortCiteRegEx": "Milch et al\\.", "year": 2005}, {"title": "Expectation propagation for approximate Bayesian inference", "author": ["T.P. Minka"], "venue": "UAI, 362-369.", "citeRegEx": "Minka,? 2001", "shortCiteRegEx": "Minka", "year": 2001}, {"title": "Bounds on marginal probability distributions", "author": ["J.M. Mooij", "H.J. Kappen"], "venue": "NIPS, 4, 3.", "citeRegEx": "Mooij and Kappen,? 2008", "shortCiteRegEx": "Mooij and Kappen", "year": 2008}, {"title": "Probabilistic Reasoning in Intelligent Systems: Networks of Plausible Inference", "author": ["J. Pearl"], "venue": "San Mateo, CA: Morgan Kaufmann.", "citeRegEx": "Pearl,? 1988", "shortCiteRegEx": "Pearl", "year": 1988}, {"title": "The Design and Implementation of IBAL: A General-Purpose Probabilistic Language", "author": ["A. Pfeffer"], "venue": "Statistical Relational Learning, Getoor, L., Taskar, B., eds. MIT Press.", "citeRegEx": "Pfeffer,? 2007", "shortCiteRegEx": "Pfeffer", "year": 2007}, {"title": "Creating and Manipulating Probabilistic Programs with Figaro", "author": ["A. Pfeffer"], "venue": "2nd International Workshop on Statistical Relational AI.", "citeRegEx": "Pfeffer,? 2012", "shortCiteRegEx": "Pfeffer", "year": 2012}, {"title": "Semantics and inference for recursive probability models", "author": ["A. Pfeffer", "D. Koller"], "venue": "AAAI.", "citeRegEx": "Pfeffer and Koller,? 2000", "shortCiteRegEx": "Pfeffer and Koller", "year": 2000}, {"title": "Logic programming, abduction and probability", "author": ["D. Poole"], "venue": "New Generation Computing, 11, 377-400.", "citeRegEx": "Poole,? 1993", "shortCiteRegEx": "Poole", "year": 1993}, {"title": "Explanation-based approximate weighted model counting for probabilistic logics", "author": ["J. Renkens", "A. Kimmig", "G. Van den Broeck", "L. De Raedt"], "venue": "Workshops at the Twenty-Eighth AAAI Conference on Artificial Intelligence.", "citeRegEx": "Renkens et al\\.,? 2014", "shortCiteRegEx": "Renkens et al\\.", "year": 2014}, {"title": "k-Optimal: a novel approximate inference algorithm for ProbLog", "author": ["J. Renkens", "G. Van den Broeck", "S. Nijssen"], "venue": "Machine Learning, 89, 215-231.", "citeRegEx": "Renkens et al\\.,? 2012", "shortCiteRegEx": "Renkens et al\\.", "year": 2012}, {"title": "Probabilistic inference and influence diagrams", "author": ["R.D. Shachter"], "venue": "Operations Research, 36, 589-604.", "citeRegEx": "Shachter,? 1988", "shortCiteRegEx": "Shachter", "year": 1988}, {"title": "Infer.NET and CSOFT", "author": ["J. Winn"], "venue": "In NIPS 2008 Workshop on Probabilistic Programming", "citeRegEx": "Winn,? \\Q2008\\E", "shortCiteRegEx": "Winn", "year": 2008}, {"title": "A simple approach to Bayesian network computations", "author": ["N.L. Zhang", "D. Poole"], "venue": "The 10th Canadian Conference on Artificial Intelligence, 171-178.", "citeRegEx": "Zhang and Poole,? 1994", "shortCiteRegEx": "Zhang and Poole", "year": 1994}], "referenceMentions": [{"referenceID": 7, "context": "Currently, the typical method for performing inference in PP systems based on functional programming is to use MetropolisHastings (MH) (Metropolis, Rosenbluth, and Rosenbluth, 1953; Hastings, 1970), which has become a standard algorithm in languages such as BLOG (Milch et al.", "startOffset": 135, "endOffset": 197}, {"referenceID": 18, "context": "Currently, the typical method for performing inference in PP systems based on functional programming is to use MetropolisHastings (MH) (Metropolis, Rosenbluth, and Rosenbluth, 1953; Hastings, 1970), which has become a standard algorithm in languages such as BLOG (Milch et al., 2005), Church (Goodman et al.", "startOffset": 263, "endOffset": 283}, {"referenceID": 5, "context": ", 2005), Church (Goodman et al., 2008), and Figaro (Pfeffer, 2012).", "startOffset": 16, "endOffset": 38}, {"referenceID": 23, "context": ", 2008), and Figaro (Pfeffer, 2012).", "startOffset": 20, "endOffset": 35}, {"referenceID": 14, "context": "Factored algorithms, such as junction tree (Lauritzen and Spiegelhalter, 1988), variable elimination (VE) (Zhang and Poole, 1994; Dechter, 1999) and belief propagation (BP) (Pearl, 1988; McEliece, Mackay, and Cheng, 1998), are widely used inference algorithms and are generally preferred to MH.", "startOffset": 43, "endOffset": 78}, {"referenceID": 30, "context": "Factored algorithms, such as junction tree (Lauritzen and Spiegelhalter, 1988), variable elimination (VE) (Zhang and Poole, 1994; Dechter, 1999) and belief propagation (BP) (Pearl, 1988; McEliece, Mackay, and Cheng, 1998), are widely used inference algorithms and are generally preferred to MH.", "startOffset": 106, "endOffset": 144}, {"referenceID": 3, "context": "Factored algorithms, such as junction tree (Lauritzen and Spiegelhalter, 1988), variable elimination (VE) (Zhang and Poole, 1994; Dechter, 1999) and belief propagation (BP) (Pearl, 1988; McEliece, Mackay, and Cheng, 1998), are widely used inference algorithms and are generally preferred to MH.", "startOffset": 106, "endOffset": 144}, {"referenceID": 21, "context": "Factored algorithms, such as junction tree (Lauritzen and Spiegelhalter, 1988), variable elimination (VE) (Zhang and Poole, 1994; Dechter, 1999) and belief propagation (BP) (Pearl, 1988; McEliece, Mackay, and Cheng, 1998), are widely used inference algorithms and are generally preferred to MH.", "startOffset": 173, "endOffset": 221}, {"referenceID": 29, "context": "NET (Winn, 2008) has achieved excellent results on real-world inference tasks (Herbrich, Minka, and Graepel, 2006) using expectation propagation (Minka, 2001), a factored algorithm, at the cost of severely restricting the expressivity of the language to avoid recursion and eliminate infinite models.", "startOffset": 4, "endOffset": 16}, {"referenceID": 19, "context": "NET (Winn, 2008) has achieved excellent results on real-world inference tasks (Herbrich, Minka, and Graepel, 2006) using expectation propagation (Minka, 2001), a factored algorithm, at the cost of severely restricting the expressivity of the language to avoid recursion and eliminate infinite models.", "startOffset": 145, "endOffset": 158}, {"referenceID": 23, "context": "We then describe an implementation of two lazy factored algorithms\u2014VE and BP\u2014in the open source Figaro PP language (Pfeffer, 2012) and present experimental results on reasoning with infinite hidden Markov models and probabilistic context-free grammars, which would otherwise be intractable using standard factored algorithms.", "startOffset": 115, "endOffset": 130}, {"referenceID": 25, "context": "Poole introduced the main ideas for probabilistic Horn abduction (PHA) (Poole, 1993).", "startOffset": 71, "endOffset": 84}, {"referenceID": 11, "context": "A similar idea underlies inference in ProbLog (Kimmig et al., 2011).", "startOffset": 46, "endOffset": 67}, {"referenceID": 26, "context": "Finally, these ideas have also been applied by compiling the search for explanations into weighted partial MAXSAT (Renkens et al., 2014).", "startOffset": 114, "endOffset": 136}, {"referenceID": 26, "context": "This fact resulted in the PHA-style algorithms having very weak upper bounds, although this was addressed in (Renkens et al., 2014) by allowing negation.", "startOffset": 109, "endOffset": 131}, {"referenceID": 9, "context": "For example, lazy evaluation of game trees using the alpha-beta algorithm allows computation of a potentially infinite search space (Hughes, 1989).", "startOffset": 132, "endOffset": 146}, {"referenceID": 22, "context": "IBAL (Pfeffer, 2007) provides an algorithm for solving infinite probabilistic models in PP with finite observations and also makes use of laziness to evaluate queries on infinitely large models.", "startOffset": 5, "endOffset": 20}, {"referenceID": 24, "context": "In (Pfeffer and Koller, 2000), the authors propose a scheme for inference with recursive probabilistic models, but it is not computationally expressed.", "startOffset": 3, "endOffset": 29}, {"referenceID": 15, "context": "Finally, (Madsen, 2004) and (Cano, Moral, and Salmer\u00f3n, 2002) use laziness to simplify the computation, but nevertheless take into account all the relevant parts of the network.", "startOffset": 9, "endOffset": 23}, {"referenceID": 30, "context": "There is also a body of work related to achieving more efficient inference in Bayesian networks by exploiting the structure of the graphical models to prune irrelevant nodes and manipulate the possible factorizations (Zhang and Poole, 1994; Pearl, 1988; Shachter, 1988; Baker and Boult, 1990).", "startOffset": 217, "endOffset": 292}, {"referenceID": 21, "context": "There is also a body of work related to achieving more efficient inference in Bayesian networks by exploiting the structure of the graphical models to prune irrelevant nodes and manipulate the possible factorizations (Zhang and Poole, 1994; Pearl, 1988; Shachter, 1988; Baker and Boult, 1990).", "startOffset": 217, "endOffset": 292}, {"referenceID": 28, "context": "There is also a body of work related to achieving more efficient inference in Bayesian networks by exploiting the structure of the graphical models to prune irrelevant nodes and manipulate the possible factorizations (Zhang and Poole, 1994; Pearl, 1988; Shachter, 1988; Baker and Boult, 1990).", "startOffset": 217, "endOffset": 292}, {"referenceID": 0, "context": "There is also a body of work related to achieving more efficient inference in Bayesian networks by exploiting the structure of the graphical models to prune irrelevant nodes and manipulate the possible factorizations (Zhang and Poole, 1994; Pearl, 1988; Shachter, 1988; Baker and Boult, 1990).", "startOffset": 217, "endOffset": 292}, {"referenceID": 4, "context": "Step 1 also extends ideas from knowledge-based model construction (Goldman and Charniak, 1990; Ngo and Haddawy, 1996; Haddawy, 1994).", "startOffset": 66, "endOffset": 132}, {"referenceID": 6, "context": "Step 1 also extends ideas from knowledge-based model construction (Goldman and Charniak, 1990; Ngo and Haddawy, 1996; Haddawy, 1994).", "startOffset": 66, "endOffset": 132}, {"referenceID": 20, "context": "Our work is also related to box propagation methods for providing bounds to BP algorithms (Mooij and Kappen, 2008; Ihler, 2012).", "startOffset": 90, "endOffset": 127}, {"referenceID": 10, "context": "Our work is also related to box propagation methods for providing bounds to BP algorithms (Mooij and Kappen, 2008; Ihler, 2012).", "startOffset": 90, "endOffset": 127}], "year": 2015, "abstractText": "Probabilistic programming provides the means to represent and reason about complex probabilistic models using programming language constructs. Even simple probabilistic programs can produce models with infinitely many variables. Factored inference algorithms are widely used for probabilistic graphical models, but cannot be applied to these programs because all the variables and factors have to be enumerated. In this paper, we present a new inference framework, lazy factored inference (LFI), that enables factored algorithms to be used for models with infinitely many variables. LFI expands the model to a bounded depth and uses the structure of the program to precisely quantify the effect of the unexpanded part of the model, producing lower and upper bounds to the probability of the query.", "creator": "Acrobat PDFMaker 11 for Word"}}}