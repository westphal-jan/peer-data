{"id": "1509.03755", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "12-Sep-2015", "title": "Toward better feature weighting algorithms: a focus on Relief", "abstract": "Feature weighting algorithms try to solve a problem of great importance nowadays in machine learning: The search of a relevance measure for the features of a given domain. This relevance is primarily used for feature selection as feature weighting can be seen as a generalization of it, but it is also useful to better understand a problem's domain or to guide an inductor in its learning process. Relief family of algorithms are proven to be very effective in this task. Some other feature weighting methods are reviewed in order to give some context and then the different existing extensions to the original algorithm are explained.", "histories": [["v1", "Sat, 12 Sep 2015 15:10:15 GMT  (46kb)", "http://arxiv.org/abs/1509.03755v1", null], ["v2", "Wed, 16 Sep 2015 11:58:32 GMT  (47kb)", "http://arxiv.org/abs/1509.03755v2", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["gabriel prat masramon", "llu\\'is a belanche mu\\~noz"], "accepted": false, "id": "1509.03755"}, "pdf": {"name": "1509.03755.pdf", "metadata": {"source": "CRF", "title": "Toward better feature weighting algorithms: a focus on Relief", "authors": ["Gabriel Prat Masramon"], "emails": ["gprat@lsi.upc.edu"], "sections": [{"heading": null, "text": "ar Xiv: 150 9.03 755v 1 [cs.L GFeature weighting algorithms try to solve a problem of great importance to the properties of a particular area. This relevance is used primarily for feature selection, since feature weighting can be considered a generalization of the problem, but it is also useful to better understand the domain of a problem or guide an inductor in its learning process. Relief's family of algorithms proves very effective in this task. Some other feature weighting methods are reviewed to give some context and then the different existing tensions with the original algorithm are explained. One of the known problems of Relief is the deterioration of its performance when redundant features are present."}, {"heading": "1 Overview", "text": "A trait selection algorithm is undoubtedly one of the most important problems in machine learning, pattern recognition and retrieval of information, among other things. The generic purpose that is pursued is to improve inductive learning, either in terms of learning speed, generalization capacity or simplicity of presentation. It is then possible to better understand the results obtained, reduce the volume of storage generated by irrelevant or redundant traits, and eliminate useless knowledge. On the other hand, trait weight algorithms attempt to assess relevance (in the form of weights to the traits) rather than binding on whether a trait is either relevant or not. This is a much harder problem, but also a more flexible framework from an inductive learning perspective."}, {"heading": "2 State of the art", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "2.1 Introduction", "text": "This year, it is closer than ever before in the history of the country."}, {"heading": "2.2 Feature weighting", "text": "The first group of methods that we will look at are those that are based on the most commonly used weight classes. Although the section focuses on the weighting of characteristics, most of the methods described below can also be used for the weight selection of characteristics. (1) The second group of characteristics (2) represents the probability of possible class values. (2) The third group of characteristics (2) represents the probability of possible class values. (3) The third group of characteristics (2) represents the probability of possible class values. (2) The third group of characteristics (2) represents the probability of characteristics. (2) The third group of characteristics (2) represents the probability of characteristics. (3) The third group of characteristics that we use to express the conditional probability of the class of values. (2) The first group of methods that we will look at is the first group of methods that we will look at, those that we will apply to the group of values (2)."}, {"heading": "2.3 Relief", "text": "A common feature of the methods cited above is that they treat characteristics individually under the assumption of conditional independence from characteristics according to class. In the other hand, relief takes care of all other characteristics when judging a particular characteristic. Another interesting feature of relief is that it is aware of the contextual information that is capable of recognizing local correlations of characteristic values and their ability to distinguish from an instance of another class. The word \"close\" in the previous sentence is of crucial importance, as we have mentioned that one of the main differences between relief and the other methods cited is the ability to take into account the local context, which helps to separate (connect) characteristics that differ from different (same) classes in general, but characteristics that do so. In Figure. 2 we can see the original algorithms used by Kira and Rendell in [Kira and Rendell] 1992."}, {"heading": "2.4 Extensions of Relief", "text": "This year, it has reached the point where it will be able to put itself at the top of the list."}, {"heading": "3 New apportations", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "3.1 Redundancy analysis", "text": "To start with the redundancy analysis of Relief, we must first define exactly the meaning of redundancy before introducing anything else. In general, the definitions of redundancy that we find in the literature are based on correlation of attributes, i.e. two attributes are not fully correlated if their values are correlated. An interesting special case is when one characteristic is an exact copy of another, so that its values are fully correlated, i.e. one characteristic is obviously redundant. In such a case, it is not easy to determine redundancy. We can take as an example the characteristics shown in Table 2. The characteristic fr is intuitively redundant with the sentence {f2}, but not with any of them, so it would not be redundant based on the definition of redundancy."}, {"heading": "3.2 Double Relief", "text": "When more and more irrelevant characteristics are added to a dataset (Ascheri) diff (Ascheri) that the distance calculation of the relief = 3.5. In such cases, the algorithm may lose its context of locality and in the end it may not recognize the relevant characteristics if they are actually far from each other, if we calculate the difference between the values of the feature Ai for two distances I1 and I2. The sum of the differences over all the characteristics is used to determine the distance between two distances in the next calculation (see Eq.2.16). As in the k-next neighbor classification algorithm (kNN) many weight schemes that assign different weight schemes to the characteristics in the calculation of the distance between distances (see Eq.3.3)."}, {"heading": "4 Empirical results", "text": "To begin with the empirical results, we must define a measure of success for the weight estimates. - First, we must have a success criterion. - The problems where we know which of the traits are important (e.g., artificial datasets), we can use this knowledge to evaluate estimates. - The ability of the weight estimates to distinguish between important and unimportant traits is very important. - Positive separability (s > 0) means that important traits are correctly separated from unimportant ones.s = WIworst \u2212 WRbest [\u2212 2] usability shows the ability of the weight estimates to distinguish between important and unimportant traits. Positive separability (s > 0) means that important traits are separated from unimportant traits. - WRbest [\u2212 2] usability indicates the ability of the weight estimates to distinguish from unimportant traits. Positive separability from unimportant traits (u > 0) means that an important trait is almost separated from an important trait."}, {"heading": "5 Conclusions and future work", "text": "In fact, most of them will be able to go in search of a solution."}], "references": [{"title": "Classification and Regression Trees", "author": ["Breiman et al", "L. 1984] Breiman", "J.H. Friedman", "R.A. Olshen", "C.J. Stone"], "venue": null, "citeRegEx": "al. et al\\.,? \\Q1984\\E", "shortCiteRegEx": "al. et al\\.", "year": 1984}, {"title": "Trading mips and memory for knowledge engineering", "author": ["Creecy et al", "R.H. 1992] Creecy", "B.M. Masand", "S.J. Smith", "D.L. Waltz"], "venue": "Commun. ACM,", "citeRegEx": "al. et al\\.,? \\Q1992\\E", "shortCiteRegEx": "al. et al\\.", "year": 1992}, {"title": "Experiments in Induction", "author": ["Hunt et al", "E.B. 1966] Hunt", "J. Marin", "P.J. Stone"], "venue": null, "citeRegEx": "al. et al\\.,? \\Q1966\\E", "shortCiteRegEx": "al. et al\\.", "year": 1966}, {"title": "The feature selection problem: Traditional methods and a new algorithm", "author": ["Kira", "Rendell", "K. 1992] Kira", "L.A. Rendell"], "venue": "In AAAI,", "citeRegEx": "Kira et al\\.,? \\Q1992\\E", "shortCiteRegEx": "Kira et al\\.", "year": 1992}, {"title": "Wrappers for feature subset selection", "author": ["Kohavi", "John", "R. 1997] Kohavi", "G.H. John"], "venue": "Artif. Intell.,", "citeRegEx": "Kohavi et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Kohavi et al\\.", "year": 1997}, {"title": "Toward optimal feature selection", "author": ["Koller", "Sahami", "D. 1996] Koller", "M. Sahami"], "venue": null, "citeRegEx": "Koller et al\\.,? \\Q1996\\E", "shortCiteRegEx": "Koller et al\\.", "year": 1996}, {"title": "Information Theory, Inference, and Learning Algorithms", "author": ["MacKay", "D.J.C. 2003] MacKay"], "venue": null, "citeRegEx": "MacKay and MacKay,? \\Q2003\\E", "shortCiteRegEx": "MacKay and MacKay", "year": 2003}, {"title": "An optimal weighting criterion of case indexing for both numeric and symbolic attributes", "author": ["Mohri", "Tanaka", "T. 1994] Mohri", "H. Tanaka"], "venue": null, "citeRegEx": "Mohri et al\\.,? \\Q1994\\E", "shortCiteRegEx": "Mohri et al\\.", "year": 1994}, {"title": "Theoretical and empirical analysis of relieff and rrelieff", "author": ["Robnik-\u0160ikonja", "Kononenko", "M. 2003] Robnik-\u0160ikonja", "I. Kononenko"], "venue": "Machine Learning,", "citeRegEx": "Robnik.\u0160ikonja et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Robnik.\u0160ikonja et al\\.", "year": 2003}, {"title": "A mathematical theory of communication", "author": ["Shannon", "C.E. 1948] Shannon"], "venue": "Bell System Tech. J.,", "citeRegEx": "Shannon and Shannon,? \\Q1948\\E", "shortCiteRegEx": "Shannon and Shannon", "year": 1948}, {"title": "A review and empirical evaluation of feature weighting methods for a class of lazy learning", "author": ["Wettschereck et al", "D. 1997] Wettschereck", "D.W. Aha", "T. Mohri"], "venue": null, "citeRegEx": "al. et al\\.,? \\Q1997\\E", "shortCiteRegEx": "al. et al\\.", "year": 1997}, {"title": "Improved heterogeneous distance functions", "author": ["Wilson", "Martinez", "D.R. 1997] Wilson", "T.R. Martinez"], "venue": "J. Artif. Intell. Res. (JAIR),", "citeRegEx": "Wilson et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Wilson et al\\.", "year": 1997}], "referenceMentions": [], "year": 2017, "abstractText": "Feature weighting algorithms try to solve a problem of great importance nowadays in machine learning: The search of a relevance measure for the features of a given domain. This relevance is primarily used for feature selection as feature weighting can be seen as a generalization of it, but it is also useful to better understand a problem\u2019s domain or to guide an inductor in its learning process. Relief family of algorithms are proven to be very effective in this task. Some other feature weighting methods are reviewed in order to give some context and then the different existing extensions to the original algorithm are explained. One of Relief\u2019s known issues is the performance degradation of its estimates when redundant features are present. A novel theoretical definition of redundancy level is given in order to guide the work towards an extension of the algorithm that is more robust against redundancy. A new extension is presented that aims for improving the algorithms performance. Some experiments were driven to test this new extension against the existing ones with a set of artificial and real datasets and denoted that in certain cases it improves the weight\u2019s estimation accuracy.", "creator": "LaTeX with hyperref package"}}}