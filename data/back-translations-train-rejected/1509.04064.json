{"id": "1509.04064", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "14-Sep-2015", "title": "Benchmarking for Bayesian Reinforcement Learning", "abstract": "In the Bayesian Reinforcement Learning (BRL) setting, agents try to maximise the collected rewards while interacting with their environment while using some prior knowledge that is accessed beforehand. Many BRL algorithms have already been proposed, but even though a few toy examples exist in the literature, there are still no extensive or rigorous benchmarks to compare them. The paper addresses this problem, and provides a new BRL comparison methodology along with the corresponding open source library. In this methodology, a comparison criterion that measures the performance of algorithms on large sets of Markov Decision Processes (MDPs) drawn from some probability distributions is defined. In order to enable the comparison of non-anytime algorithms, our methodology also includes a detailed analysis of the computation time requirement of each algorithm. Our library is released with all source code and documentation: it includes three test problems, each of which has two different prior distributions, and seven state-of-the-art RL algorithms. Finally, our library is illustrated by comparing all the available algorithms and the results are discussed.", "histories": [["v1", "Mon, 14 Sep 2015 12:47:52 GMT  (224kb,D)", "http://arxiv.org/abs/1509.04064v1", "37 pages"]], "COMMENTS": "37 pages", "reviews": [], "SUBJECTS": "cs.AI", "authors": ["michael castronovo", "damien ernst", "adrien couetoux", "raphael fonteneau"], "accepted": false, "id": "1509.04064"}, "pdf": {"name": "1509.04064.pdf", "metadata": {"source": "CRF", "title": "Benchmarking for Bayesian Reinforcement Learning Benchmarking for Bayesian Reinforcement Learning", "authors": ["Micha\u00ebl Castronovo", "Damien Ernst", "Adrien Cou\u00ebtoux"], "emails": ["m.castronovo@ulg.ac.be", "dernst@ulg.ac.be", "acouetoux@ulg.ac.be", "raphael.fonteneau@ulg.ac.be"], "sections": [{"heading": null, "text": "Keywords: Bayesian Reinforcement Learning, Benchmarking, BBRL Library, Offline Learning, Reinforcement Learning"}, {"heading": "1. Introduction", "text": "In fact, it is the case that most of them are able to assert themselves, that they are able to assert themselves, that they are able to assert themselves, and that they are able to assert themselves; in fact, it is the case that they are able to assert themselves."}, {"heading": "2. Problem Statement", "text": "This section is dedicated to formalizing the various tools and concepts discussed in this essay."}, {"heading": "2.1 Reinforcement Learning", "text": "Let M = (X, U, f (\u00b7 \u03c0ote), \u03c1M, pM, 0 (\u00b7), \u03b3) be a given unknown MDP, where X = {x (1),.., x (nX)} denotes its finite state space and U = {u (1),.., u (nU)} denotes its finite action space. If the MDP is in state t at the time t and action ut is selected, the actor immediately moves to the next state xt + 1 with a probability of P (xt + 1 | xt, ut) = f (xt, ut, xt + 1). An instantaneous deterministic, limited reward system rt = \u03c1M (xt, ut, xt + 1) is observed."}, {"heading": "2.2 Prior Knowledge", "text": "Model-based Bayesian Reinforcement Learning (BRL) proposes uncertainty to the model, using a probability distribution p0M (\u00b7) across a number of MDPs candidates; such a probability distribution is referred to as a prior distribution and can be used to encode specific knowledge available prior to interaction. Given a prior distribution p0M (\u00b7), the expected yield of a given E / E strategy \u03c0 is defined as: J\u03c0p0M (\u00b7) = E M \u0445 p0M (\u00b7) [J\u03c0M] [J\u03c0M]. Within the BRL framework, the goal is to maximize J\u03c0 p0M (\u00b7) by finding a \"Bayesian optimal policy\" defined as follows:"}, {"heading": "2.3 Computation time characterisation", "text": "Most BRL algorithms rely on some characteristics that, with sufficient computational time, ensure that their agents converge to optimum behavior, but it is not clear beforehand whether an algorithm meets certain computational time limitations and performs well at the same time. Parameterization of the algorithms makes selection even more complex. Most BRL algorithms depend on parameters (number of transitions simulated in each iteration, etc.) that can influence computational time in some way. In addition, the computational time for a given algorithm and defined parameters often varies from one simulation to another. These characteristics make it almost impossible to compare BRL algorithms under strict computational time limitations. To solve this problem, algorithms are executed with several options of parameters, and we analyze their time performance subsequently. In addition, a distinction between offline and online computation time is made."}, {"heading": "3. A new Bayesian Reinforcement Learning benchmark protocol", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "3.1 A comparison criterion for BRL", "text": "In this paper, a real Bayesian evaluation is proposed, in the sense that the various algorithms are compared on a large number of problems drawn according to a test probability distribution, in contrast to Bayesian literature (Guez et al. (2012); Castro and Precup (2010); Asmuth and Littman (2011), where authors select a fixed number of MDPs on which to evaluate their algorithms. Our criterion for comparing algorithms is to measure their average rewards based on a given random distribution of MDPs, using a different distribution of MDPs as prior knowledge. In our experimental protocol, an experiment is defined based on a previous distribution p0M (\u00b7) and a test distribution pM (\u00b7), both of which are random distributions across the set of possible MDPs, not stochastic transition functions. To illustrate the difference, let's take an example."}, {"heading": "3.2 The experimental protocol", "text": "This section describes our experimental protocol, which is based on our comparison criterion for BRL and provides a detailed calculation time analysis. An experiment is defined by (i) a prior distribution p0M and (ii) a test distribution pM. Based on these results, an agent is evaluated as follows: 1. Use these values to estimate p0M.2. Example N MDPs from the test distribution pM.3. For each sampled MDP M, calculate an estimate J (p0M) M of J \u03c0 (p0M) M.4 Use these values to calculate an estimate J (p0M) p.To J (p0M) M, the expected return of the agents to p 0 M, a trajectory yis sampled on the MDP M, and the cumulative return is calculated."}, {"heading": "4. BBRL library", "text": "BBRL2 is a C + + open source library for Bayesian Reinforcement Learning (discrete state / action spaces) > < < This library provides high-level features while remaining as flexible and documented as possible to address the needs of each researcher in this field. To this end, we have developed a full command line interface, along with a comprehensive website: & ltps: / / github.com / mcastron / BBRLBBRL-Agent focuses on the core operations required to apply the benchmark shown in this paper. To conduct a full experiment with the BBRL library, follow these five steps: 1. We create a test and a previous distribution. These distributions are represented by Flat Dirichlet Multinomial Distributions (FDM)."}, {"heading": "5. Illustration", "text": "This section illustrates this in Section 3. We first describe the algorithms that are eligible for comparison in Section 5.1, followed by a description of the benchmarks in Section 5.2. Section 5.3 shows and analyzes the results obtained."}, {"heading": "5.1 Compared algorithms", "text": "In this section, we present the list of algorithms included in this study. The pseudo code of each algorithm can be found in Appendix A. For each algorithm, a list of \"reasonable\" values is provided to test each of its parameters. If an algorithm has more than one parameter, all possible parameter combinations are tested."}, {"heading": "5.1.1 Random", "text": "At each time step t, the plot sut is uniformly pulled out of U."}, {"heading": "5.1.2 -Greedy", "text": "The -Greedy agent maintains an approximation to the current MDP and calculates its associated Q function in each time step. The selected action is either randomly selected (with a probability of (1 \u2265 0) or greedily (with a probability of 1 \u2212) in relation to the approximate model. Values tested: \u2022 0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0}."}, {"heading": "5.1.3 Soft-max", "text": "The Soft-max agent maintains an approximation to the current MDP and calculates its associated Q function in each time step. The selected action is selected at random, with the probability of drawing an action being u proportional to Q (xt, u). The temperature parameter \u03c4 allows to control the influence of the Q function on these probabilities (\u03c4 \u2192 0 +: greedy selection; \u03c4 \u2192 + \u221e: random selection). Values tested: \u2022 \u03c4 = 0.05, 0.10, 0.20, 0.33, 0.50, 1.0, 2.0, 3.0, 5.0, 25.0}."}, {"heading": "5.1.4 OPPS", "text": "Given a prior distribution p0M (.) and an E / E strategy space S (either discrete or continuous), the offline discrete strategy space algorithm (OPPS) identifies a strategy that maximizes the expected discounted sum of returns relative to the MDPs extracted from the priority. OPPS discrete strategy space algorithm (OPPS-DS) (Castronovo et al. (2012, 2014) formalizes the strategy selection problem as a k-armed bandit problem where k = | S |. Dragging an arm amounts to pulling an MDP from p0M (.) and playing the associated E / E strategy for a single track. The discounted sum of observed returns is the return on that arm. This multi-armed bandit problem was solved by using the UCB1 algorithm (Audit et al. (2002); Audiovo et al (2007)."}, {"heading": "5.1.5 BAMCP", "text": "Bayes-adaptive Monte Carlo Planning (BAMCP) (Guez et al. (2012)) is an evolution of the Upper Confidence Tree (UCT) algorithm (Kocsis and Szepesva \u0301 ri (2006), which scans each transition according to the history of the observed transitions. The principle of this algorithm is the adaptation of the UCT principle for planning in a Bayes adaptive MDP, also known as \"believe-augmented MDP,\" an MDP that is derived from the concatenation of the actual state and the rear, taking into account the augmented states. The BAMCP algorithm is made computationally traceable by using a sparse sampling strategy that avoids sampling a model from the rear distribution at each node of the planning tree."}, {"heading": "5.1.6 BFS3", "text": "The Bayesian Forward Search Sparse Sampling (BFS3) (Asmuth and Littman (2011)) is a Bayesian RL algorithm whose principle is to apply the principle of the FSSS (Forward Search Sparse Sampling, see Kearns et al. (2002)) algorithm to belief-enlarged MDPs. It first tries out a model from the posterior model, which is then used for sample transitions. In practice, the parameters of the BFS3 are used to control how much computing power is allowed. K defines the number of nodes to be developed in each time step, C defines the branching factor of the tree and the depth controls its maximum depth.Tested values: \u2022 K {1, 500, 1250, 1250, 2500, 5000, 5000, {2}, 15 \u00b7 15."}, {"heading": "5.1.7 SBOSS", "text": "The Smarter Best of Sampled Set (SBOSS) (Castro and Precup (2010)) is a Bayesian RL algorithm based on the assumption that the model is sampled from a Dirichlet distribution. It derives uncertainty limits for the value of state pairs of actions. On the basis of these limits, it then decides how many models should be sampled from the back and how often the back one should be sampled \u2212 in order to reduce the computational costs of Bayesian updates. The sampling technique is then used to build a grouped MDP, as in Asmuth et al. (2009), and to derive the corresponding optimal measures with respect to this MDP. In practice, the number of sampled models is determined dynamically based on a parameter. The re-sampling frequency depends on a parameter: \u2022 1.0, 1e \u2212 1, 1e \u2212 3, 1e \u2212 4, 1e, 1e \u2212 5 \u2212 1 g, 1g, 1g, 1, 15, 1g, 1g, 15, 1g, 1, 1, 15, 1, 1g, 1, 1, 1, 1g, 1, 1g, 1, 1e, 1e, 1e, 1e, 1e, 1e, 1e, 1e, 1e, 1e, 1e, 1e, 1e, 1e, 1e, 1e, 1e, 1e, 1e, 1e, 1e, 1e, 1e, 1e, 1e, 1e, 1e, 1e, 1e, 1e, 1e, 1e, 1e, 1e, 1e, 1e, 1e, 1e, 1e, 1e, 1e, 1e, 1e, 1e, 1e, 1e, 1e, 1e, 1e, 1e, 1g, 1e, 1e, 1e, 1e, 1e, 1e, 1e, 1e, 1e, 1e, 1e, 1e, 1e, 1e, 1e, 1e, 1e, 1e, 1e, 1e, 1e, 1e, 1e, 1e, 1e, 1e,"}, {"heading": "5.1.8 BEB", "text": "The Bayesian Exploration Bonus (BEB) (Kolter and Ng (2009)) is a Bayesian RL algorithm that builds the expected MDP according to the current background at each timestep t. Prior to solving this MDP, it calculates a new reward function \u03c1 (t) BEB (x, u, y) = \u03c1M (x, u, y) + \u03b2c (t) < x, u, y >, where c (t) < x, u, y > the number of time transitions < x, u, y > in timestep t. This algorithm solves the mean MDP of the current background in which we have replaced Economy M (\u00b7, \u00b7, \u00b7) with Rap (t) BEB (\u00b7, \u00b7, \u00b7) and applies its optimal policy to the current MDP for one step. The Bonus \u03b2 is a parameter that controls the E / E balance. B comes with theoretical \u00b7 \u00b7 guarantees, \u00b7 \u00b7, and \u00b7 () policies."}, {"heading": "5.1.9 Computation times variance", "text": "Each algorithm has one or more parameters that can affect the number of sampled transitions from a given state or the length of each simulation, which in turn affects the calculation time requirements at each step. Therefore, for some algorithms, no selection of parameters can bring the calculation time below or above certain values. In other words, each algorithm has its own calculation time range. Note that in some methods, the calculation time is affected by several parameters at the same time. We present a qualitative description of how the calculation time changes depending on parameters in Table 1.4. If a random decision is made, the model is not resolved. 5.K defines the number of nodes to be developed in each step, and the depth defines the maximum depth of the tree. 6.K defines the number of nodes to be developed in each step, C the branching factor of the tree and the maximum depth. 7.The number of models to be sampled is proportional to the frequency of the sample, while the sample is sampled proportional to the frequency."}, {"heading": "5.2 Benchmarks", "text": "In our setting, the transition matrix is the only element that differs between two MDPs drawn from the same distribution. For each < state, action > pair < x, u >, we define a dirichlet distribution that represents uncertainty about the transitions occurring from < x, u >. A dirichlet distribution is parameterized by a set of concentration parameters \u03b1 (1) < x, u >, \u00b7 \u00b7, \u03b1 (nX) < x, u >. We have collected all concentration parameters in a single vector \u03b8. Consequently, our MDP distributions are parameterized by \u03c1M (the reward function) and multiple dirichlet distributions by \u03b8."}, {"heading": "5.2.1 Generalised Chain distribution (p\u03c1", "text": "The distribution of the general chain (GC) is inspired by the problem of the five-state chain (5 states, 3 actions) (Dearden et al. (1998)).The agent starts from State 1 and must pass through State 2, 3 and 4 to reach the last State (State 5) in which the best rewards are available.The agent has 3 actions at his disposal.An action can either make the agent move from State x (n) to State x (n + 1) or force him to go back to State x (1).The transition matrix is drawn from an FDM parameterized by the State GC, and the reward function is designated by the State GC. Further details can be found in Appendix B. 1."}, {"heading": "5.2.2 Generalised Double-Loop distribution (p\u03c1", "text": "A loop is a trap: if the agent enters this loop, he has no choice but to leave it without a reward. Leaving this loop offers a small reward. The other loop leads to a good reward. However, any action of this loop can either drag the agent into the next state of the loop or force him to return to state 1 without a reward. The transition matrix is based on an FDM parameterized by the successGDL, and the reward function is characterized by the ECG. Further details can be found in Appendix B. 2."}, {"heading": "5.2.3 Grid distribution (p\u03c1", "text": "The agent is placed at a corner of a 5x5 grid (the S cell) and must reach the opposite corner (the G cell). If he succeeds, he returns to his initial state and receives a reward. The agent can perform 4 different actions according to the 4 directions (top, bottom, left, right). However, depending on the cell on which the agent stands, each action has a certain probability of failure and can prevent the agent from moving in the chosen direction. The transition matrix is taken from an FDM parameterized by the previous grid, and the reward function is indicated by the grid. Further details can be found in Appendix B.3."}, {"heading": "5.3 Discussion of the results", "text": "In the last few years, it has been shown that you are able to outdo yourself in all areas where you can outdo each other. In the last few years, it has been shown that you are able to outdo yourself in all areas where you can outdo each other. In the last few years, it has been shown that you can outdo each other. In the second half of the last decade, it has been shown that you can outdo yourself. In the last few years, it has been shown that you can outdo each other. In the second half of the last decade, you can outdo each other. In the second half of the last decade, you have outdone yourself. In the second half of the last decade, you have outdone yourself. In the second half of the last decade, you have outdone yourself. In the second half of the last decade, you have outdone yourself. In the second half of the last decade, you have outdone yourself. In the second half of the last decade, you have outdone yourself. In the second half of the last decade, you have outdone yourself. In the second half of the last decade, you have outdone yourself."}, {"heading": "5.3.3 Summary", "text": "If the offline time budget for OPPS-DS was too limited, different algorithms were suitable depending on the online time budget: \u2022 Low online time budget: SBOSS was the fastest algorithm that made better decisions than random policy. \u2022 Median online time budget: BEB achieved similar performance to OPPS-DS in each experiment. \u2022 High online time budget: In the first experiment, BFS3, BEB and OPPS-DS succeeded in catching up if given enough time. In the second experiment, it was BAMCP that achieved this result. Neither BFS3 nor BAMCP could keep up with BEB and OPPS-DS in the last experiment. The results obtained in the inaccurate case were very interesting. BEB was not as good as it seemed to be in the precise experiment, while SBOSS improved significantly compared to the others."}, {"heading": "6. Conclusion", "text": "Specifically, our benchmarking protocol shows that no single algorithm dominates all other algorithms in all scenarios, and the protocol we have introduced can compare any time algorithm with non-time algorithms, while measuring the impact of inaccurate offline training. By comparing algorithms to large problem complexes, we avoid them matching a single problem. Our methodology is linked to an open source library, BBRL, and we hope it will help other researchers design algorithms whose performance is compared with computation times that can be critical in many applications. This library is specifically designed for easy handling of new algorithms, and has a complete and comprehensive documentation website."}, {"heading": "Acknowledgments", "text": "Michae \ufffd l Castronovo appreciates the financial support of the FRIA. Raphael Fonteneau is a postdoctoral fellow of the F.R.S.-FNRS (Belgian Funds for Scientifique Research).8. \u00b1 100 times more than the low online time budget 9. \u00b1 100 times more than the medium online time budget."}, {"heading": "Appendix A. Pseudo-code of the algorithms", "text": "Algorithm 1 -Greedy1: Procedure offline learning (\u03c0 (.) > Q function (\"Q function\") 2: M function (\"Build an initial model based on p0M (.)\" 3: end procedure 4: 5: function search (x, h) 6: {Draw a random value in [0; 1] 7: r \u2190 U (0, 1) 8: 9: if r < then {Random case} 10: return \"A randomly selected action\" 11: 12: else {Greedy case} 13: \u03c0 M-value iteration (M-value) (M-value) 14: return \u03c0 M (x) 15: end if 16: end function online-learning (x, u, y, r) 19: \"Update model M-iteration.\" r.t. Transition < x: u, y, y, r > DP-value) (M-value) (M-function) (M-function) M-end (M-end): 14-Iteration (M-end) x: x-17: M-end (M-end)."}, {"heading": "Appendix B. MDP distributions in detail", "text": "In this section, we describe in more detail the MDPs drawn from the distributions considered. In addition, we also offer a formal description of the corresponding \u03b8 (parameterization of the FDM used to draw the transition matrix) and \u03c1M (reward function).B.1 Generalized chain distributionOn these MDPs, we can identify two potentially optimal behaviors: \u2022 The agent tries to move along the chain, reaches the last state and collects as many rewards as possible before returning to state 1; \u2022 The agent gives up in order to reach state 5 and tries to return to state 1 as often as possible.B.1.1 Formal description X = {1, 2, 4, 5}, U = {1, 3}."}], "references": [{"title": "Approaching Bayes-optimalilty using Monte-Carlo tree search", "author": ["J. Asmuth", "M. Littman"], "venue": "In Proceedings of the 21st International Conference on Automated Planning and Scheduling,", "citeRegEx": "Asmuth and Littman.,? \\Q2011\\E", "shortCiteRegEx": "Asmuth and Littman.", "year": 2011}, {"title": "A Bayesian sampling approach to exploration in Reinforcement Learning", "author": ["J. Asmuth", "L. Li", "M.L. Littman", "A. Nouri", "D. Wingate"], "venue": "In Proceedings of the Twenty-Fifth Conference on Uncertainty in Artificial Intelligence (UAI),", "citeRegEx": "Asmuth et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Asmuth et al\\.", "year": 2009}, {"title": "Tuning bandit algorithms in stochastic environments", "author": ["J.Y. Audibert", "R. Munos", "C. Szepesv\u00e1ri"], "venue": "In Algorithmic Learning Theory,", "citeRegEx": "Audibert et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Audibert et al\\.", "year": 2007}, {"title": "Finite-time analysis of the multiarmed bandit problem", "author": ["P. Auer", "N. Cesa-Bianchi", "P. Fischer"], "venue": "Machine learning,", "citeRegEx": "Auer et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Auer et al\\.", "year": 2002}, {"title": "Smarter sampling in model-based bayesian reinforcement learning", "author": ["P.S. Castro", "D. Precup"], "venue": "In Machine Learning and Knowledge Discovery in Databases,", "citeRegEx": "Castro and Precup.,? \\Q2010\\E", "shortCiteRegEx": "Castro and Precup.", "year": 2010}, {"title": "Learning exploration/exploitation strategies for single trajectory Reinforcement Learning", "author": ["M. Castronovo", "F. Maes", "R. Fonteneau", "D. Ernst"], "venue": "Journal of Machine Learning Research (JMLR),", "citeRegEx": "Castronovo et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Castronovo et al\\.", "year": 2012}, {"title": "Bayes Adaptive Reinforcement Learning versus Off-line Prior-based Policy Search: an Empirical Comparison. 23rd annual machine learning conference of Belgium and the Netherlands (BENELEARN", "author": ["M. Castronovo", "R. Fonteneau", "D. Ernst"], "venue": null, "citeRegEx": "Castronovo et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Castronovo et al\\.", "year": 2014}, {"title": "Bayesian Q-learning", "author": ["R. Dearden", "N. Friedman", "S. Russell"], "venue": "In Proceedings of Fifteenth National Conference on Artificial Intelligence (AAAI),", "citeRegEx": "Dearden et al\\.,? \\Q1998\\E", "shortCiteRegEx": "Dearden et al\\.", "year": 1998}, {"title": "Model based Bayesian exploration", "author": ["R. Dearden", "N. Friedman", "D. Andre"], "venue": "In Proceedings of the Fifteenth Conference on Uncertainty in Artificial Intelligence (UAI),", "citeRegEx": "Dearden et al\\.,? \\Q1999\\E", "shortCiteRegEx": "Dearden et al\\.", "year": 1999}, {"title": "Efficient Bayes-adaptive Reinforcement Learning using sample-based search", "author": ["A. Guez", "D. Silver", "P. Dayan"], "venue": "In Neural Information Processing Systems (NIPS),", "citeRegEx": "Guez et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Guez et al\\.", "year": 2012}, {"title": "A sparse sampling algorithm for near-optimal planning in large Markov decision processes", "author": ["M. Kearns", "Y. Mansour", "A.Y. Ng"], "venue": "Machine Learning,", "citeRegEx": "Kearns et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Kearns et al\\.", "year": 2002}, {"title": "Bandit based Monte-Carlo planning", "author": ["L. Kocsis", "C. Szepesv\u00e1ri"], "venue": "European Conference on Machine Learning (ECML),", "citeRegEx": "Kocsis and Szepesv\u00e1ri.,? \\Q2006\\E", "shortCiteRegEx": "Kocsis and Szepesv\u00e1ri.", "year": 2006}, {"title": "Near-Bayesian exploration in polynomial time", "author": ["J. Zico Kolter", "Andrew Y. Ng"], "venue": "In Proceedings of the 26th Annual International Conference on Machine Learning,", "citeRegEx": "Kolter and Ng.,? \\Q2009\\E", "shortCiteRegEx": "Kolter and Ng.", "year": 2009}, {"title": "A Bayesian framework for Reinforcement Learning", "author": ["M. Strens"], "venue": "In Proceedings of the Seventeenth International Conference on Machine Learning (ICML),", "citeRegEx": "Strens.,? \\Q2000\\E", "shortCiteRegEx": "Strens.", "year": 2000}], "referenceMentions": [{"referenceID": 7, "context": "Model-based Bayesian Reinforcement Learning (BRL) (Dearden et al. (1999); Strens (2000)) specifically targets RL problems for which such a prior knowledge is encoded in the form of a probability distribution (the \u201cprior\u201d) over possible models of the environment.", "startOffset": 51, "endOffset": 73}, {"referenceID": 7, "context": "Model-based Bayesian Reinforcement Learning (BRL) (Dearden et al. (1999); Strens (2000)) specifically targets RL problems for which such a prior knowledge is encoded in the form of a probability distribution (the \u201cprior\u201d) over possible models of the environment.", "startOffset": 51, "endOffset": 88}, {"referenceID": 3, "context": "To properly compare Bayesian algorithms, the first comprehensive BRL benchmarking protocol is designed, following the foundations of Castronovo et al. (2014). \u201cComprehensive BRL benchmark\u201d refers to a tool which assesses the performance of BRL algorithms over a large set of problems that are actually drawn according to a prior distribution.", "startOffset": 133, "endOffset": 158}, {"referenceID": 3, "context": "To properly compare Bayesian algorithms, the first comprehensive BRL benchmarking protocol is designed, following the foundations of Castronovo et al. (2014). \u201cComprehensive BRL benchmark\u201d refers to a tool which assesses the performance of BRL algorithms over a large set of problems that are actually drawn according to a prior distribution. In previous papers addressing BRL, authors usually validate their algorithm by testing it on a few test problems, defined by a small set of predefined MDPs. For instance, BAMCP (Guez et al. (2012)), SBOSS (Castro and Precup (2010)), and BFS3 (Asmuth and Littman (2011)) are all validated on a fixed number of MDPs.", "startOffset": 133, "endOffset": 540}, {"referenceID": 3, "context": "(2012)), SBOSS (Castro and Precup (2010)), and BFS3 (Asmuth and Littman (2011)) are all validated on a fixed number of MDPs.", "startOffset": 16, "endOffset": 41}, {"referenceID": 0, "context": "(2012)), SBOSS (Castro and Precup (2010)), and BFS3 (Asmuth and Littman (2011)) are all validated on a fixed number of MDPs.", "startOffset": 53, "endOffset": 79}, {"referenceID": 7, "context": "This is in contrast with the Bayesian literature (Guez et al. (2012); Castro and Precup (2010); Asmuth and Littman (2011)), where authors pick a fixed number of MDPs on which they evaluate their algorithm.", "startOffset": 50, "endOffset": 69}, {"referenceID": 3, "context": "(2012); Castro and Precup (2010); Asmuth and Littman (2011)), where authors pick a fixed number of MDPs on which they evaluate their algorithm.", "startOffset": 8, "endOffset": 33}, {"referenceID": 0, "context": "(2012); Castro and Precup (2010); Asmuth and Littman (2011)), where authors pick a fixed number of MDPs on which they evaluate their algorithm.", "startOffset": 34, "endOffset": 60}, {"referenceID": 5, "context": "This protocol is an extension of the one presented in Castronovo et al. (2014).", "startOffset": 54, "endOffset": 79}, {"referenceID": 2, "context": "This multi-armed bandit problem has been solved by using the UCB1 algorithm (Auer et al. (2002); Audibert et al.", "startOffset": 77, "endOffset": 96}, {"referenceID": 2, "context": "(2002); Audibert et al. (2007)).", "startOffset": 8, "endOffset": 31}, {"referenceID": 9, "context": "5 BAMCP Bayes-adaptive Monte Carlo Planning (BAMCP) (Guez et al. (2012)) is an evolution of the Upper Confidence Tree (UCT) algorithm (Kocsis and Szepesv\u00e1ri (2006)), where each transition is sampled according to the history of observed transitions.", "startOffset": 53, "endOffset": 72}, {"referenceID": 9, "context": "5 BAMCP Bayes-adaptive Monte Carlo Planning (BAMCP) (Guez et al. (2012)) is an evolution of the Upper Confidence Tree (UCT) algorithm (Kocsis and Szepesv\u00e1ri (2006)), where each transition is sampled according to the history of observed transitions.", "startOffset": 53, "endOffset": 164}, {"referenceID": 0, "context": "6 BFS3 The Bayesian Forward Search Sparse Sampling (BFS3) (Asmuth and Littman (2011)) is a Bayesian RL algorithm whose principle is to apply the principle of the FSSS (Forward Search Sparse Sampling, see Kearns et al.", "startOffset": 59, "endOffset": 85}, {"referenceID": 0, "context": "6 BFS3 The Bayesian Forward Search Sparse Sampling (BFS3) (Asmuth and Littman (2011)) is a Bayesian RL algorithm whose principle is to apply the principle of the FSSS (Forward Search Sparse Sampling, see Kearns et al. (2002)) algorithm to belief-augmented MDPs.", "startOffset": 59, "endOffset": 225}, {"referenceID": 3, "context": "7 SBOSS The Smarter Best of Sampled Set (SBOSS) (Castro and Precup (2010)) is a Bayesian RL algorithm which relies on the assumption that the model is sampled from a Dirichlet distribution.", "startOffset": 49, "endOffset": 74}, {"referenceID": 1, "context": "The sampling technique is then used to build a merged MDP, as in Asmuth et al. (2009), and to derive the corresponding optimal action with respect to that MDP.", "startOffset": 65, "endOffset": 86}, {"referenceID": 12, "context": "8 BEB The Bayesian Exploration Bonus (BEB) (Kolter and Ng (2009)) is a Bayesian RL algorithm which builds, at each time-step t, the expected MDP given the current posterior.", "startOffset": 44, "endOffset": 65}, {"referenceID": 7, "context": "1 Generalised Chain distribution (p\u03c1 GC ,\u03b8 (\u00b7)) The Generalised Chain (GC) distribution is inspired from the five-state chain problem (5 states, 3 actions) (Dearden et al. (1998)).", "startOffset": 157, "endOffset": 179}, {"referenceID": 7, "context": "2 Generalised Double-Loop distribution (p\u03c1 GDL,\u03b8GDL(\u00b7)) The Generalised Double-Loop (GDL) distribution is inspired from the double-loop problem (9 states, 2 actions) (Dearden et al. (1998)).", "startOffset": 167, "endOffset": 189}, {"referenceID": 7, "context": "3 Grid distribution (p\u03c1 Grid,\u03b8Grid(\u00b7)) The Grid distribution is inspired from the Dearden\u2019s maze problem (25 states, 4 actions) (Dearden et al. (1998)).", "startOffset": 129, "endOffset": 151}], "year": 2015, "abstractText": "In the Bayesian Reinforcement Learning (BRL) setting, agents try to maximise the collected rewards while interacting with their environment while using some prior knowledge that is accessed beforehand. Many BRL algorithms have already been proposed, but even though a few toy examples exist in the literature, there are still no extensive or rigorous benchmarks to compare them. The paper addresses this problem, and provides a new BRL comparison methodology along with the corresponding open source library. In this methodology, a comparison criterion that measures the performance of algorithms on large sets of Markov Decision Processes (MDPs) drawn from some probability distributions is defined. In order to enable the comparison of non-anytime algorithms, our methodology also includes a detailed analysis of the computation time requirement of each algorithm. Our library is released with all source code and documentation: it includes three test problems, each of which has two different prior distributions, and seven state-of-the-art RL algorithms. Finally, our library is illustrated by comparing all the available algorithms and the results are discussed.", "creator": "LaTeX with hyperref package"}}}