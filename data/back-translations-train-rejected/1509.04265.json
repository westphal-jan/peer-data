{"id": "1509.04265", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "12-Sep-2015", "title": "Double Relief with progressive weighting function", "abstract": "Feature weighting algorithms try to solve a problem of great importance nowadays in machine learning: The search of a relevance measure for the features of a given domain. This relevance is primarily used for feature selection as feature weighting can be seen as a generalization of it, but it is also useful to better understand a problem's domain or to guide an inductor in its learning process. Relief family of algorithms are proven to be very effective in this task.", "histories": [["v1", "Sat, 12 Sep 2015 15:28:08 GMT  (38kb)", "https://arxiv.org/abs/1509.04265v1", "arXiv admin note: substantial text overlap witharXiv:1509.03755"], ["v2", "Wed, 16 Sep 2015 12:09:28 GMT  (38kb)", "http://arxiv.org/abs/1509.04265v2", "arXiv admin note: substantial text overlap witharXiv:1509.03755"]], "COMMENTS": "arXiv admin note: substantial text overlap witharXiv:1509.03755", "reviews": [], "SUBJECTS": "cs.LG cs.AI", "authors": ["gabriel prat masramon", "llu\\'is a belanche mu\\~noz"], "accepted": false, "id": "1509.04265"}, "pdf": {"name": "1509.04265.pdf", "metadata": {"source": "CRF", "title": "Double Relief with progressive weighting function", "authors": ["Gabriel Prat"], "emails": ["gprat@lsi.upc.edu", "belanche@lsi.upc.edu"], "sections": [{"heading": null, "text": "ar Xiv: 150 9.04 265v 2 [csFeature weighting algorithms attempt to solve a problem that is of great importance in machine learning today: the search for a relevance measurement for the properties of a particular area. This relevance is primarily used for feature weighting, but can also be seen as a generalization of a problem, but it is also useful to better understand the domain of a problem or guide an inductor in its learning process. Relief family of algorithms proves to be very effective in this task. In previous work, a new extension has been proposed aimed at testing the performance of the algorithm, and it has been shown to improve weight estimation accuracy in certain cases."}, {"heading": "1 Overview", "text": "Feature selection is undoubtedly one of the most important problems when machine learning, pattern recognition and retrieving information, among other things. A feature selection algorithm is a computational solution that is motivated by a certain definition of relevance. However, the relevance of a feature can have several definitions depending on the goal being considered thereafter. On the other hand, feature weighting algorithms try to estimate relevance (in the form of weighting to features) rather than binary deciding whether a feature is relevant or not. This is a much more difficult problem, but also a more flexible framework from an inductive learning perspective. These types of ofalgorithms are confronted with the downweighting of irrelevant features, the overweighting of relevant features, and the problem of relevance assignment when redundancy is a problem. In this work, we review Relief, one of the most popular feature weighting algorithms that weights algorithms."}, {"heading": "2 Relief", "text": "Most of them deal with characteristics that require a conditional independence of the characteristics from the class. On the other hand, when they evaluate a particular characteristic, relief has all the other characteristics in care. Another interesting characteristic of relief is that it is contextual information that is able to detect local correlations of characteristics and their ability to distinguish from an instance of another class. The basic idea behind relief is to assign large weights to characteristics that help them to be close to distances of different class and close to distances that belong to the same class. The word \"near\" in the previous sentence is of crucial importance, as we have mentioned that one of the main differences between relief and the other methods cited is the ability to take into account the local context. We reward characteristics that we do not reward that are separated from each other (the same) classes in general, but characteristics that do."}, {"heading": "2.1 Extensions of Relief", "text": "In fact, the fact is that most of them are able to move to another world in which they are able to find themselves."}, {"heading": "3 Double Relief", "text": "When more and more irrelevant characteristics are added to a dataset, the distance calculation of Relief degrades its performance, because instances can be considered neighbors if they are indeed far apart, if we calculate their distance only with the relevant characteristics. In such cases, the algorithm may lose its context of locality and in the end fail to recognize relevant characteristics. The difference function (Ai, I1, I2) calculates the difference between the values of the feature Ai for two instances I1 and I2. The sum of the differences across all characteristics is used to determine the distance between two instances in the closest hit calculation (see Eq.2.1). As seen in the k-next neighbor classification algorithm (kNN), many weighting schemes will assign different weights to the characteristics in the calculation of the distance between instances (see Equation 3.1)."}, {"heading": "3.1 Progressively weighted double Relief", "text": "\"For the first time in my life, I have been able to do this for the first time in my life. I have been able to do it for the first time. I have been able to do it for the first time. I have been able to do it for the second time. I have been able to do it for the first time. I have been able to do it for the second time. I have been able to do it for the second time. I have been able to do it for the last few years. I have been able to do it for the first time. I have been able to do it for the first time. I have been able to do it for the last few years. I have been able to do it for the first time. I have been able to do it for the second time. I have been able to do it for the first time. I have been able to do it for the first time."}, {"heading": "4 Experimental design", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.1 Objective", "text": "The above sections present three algorithms: ReliefF The algorithm introduced by Kononenko in [Kononenko, 1994] dReliefF The above algorithm uses its own partial weights to compare the performance of the three algorithms in relation to the factor of irrelevant attributes The above use of a function to progressively increase the weight consideration effect in the distance calculation The aim of the experiments presented is to compare the performance of the three algorithms in relation to the factor of irrelevant attributes. Hypothesis is that the performance of the unmodified algorithm is more strongly influenced by the number of irrelevant attributes that increase due to their influence in the distance calculation."}, {"heading": "4.2 Factors", "text": "As already mentioned before the key factor of the experiments, the ratio of irrelevant attributes to 150 is interesting, but there are some disturbing factors that have an effect on the results of the experiments. Factors that will be taken into account in the experiments are: \u2022 Problem solving \u2022 Numerical vs. categorical attributes \u2022 Number of relevant attributes \u2022 Number of irrelevant attributes \u2022 Data randomization The most important factor that will have an effect on the performance results will be the problem that we want to solve, and additionally it will be the most difficult to reduce. To eliminate its influence, all possible attributes would have to be tried, which is obviously impossible. Another factor that can clearly affect the performance is the type of attributes that Relief has a heterogeneous function for the distance calculation, which depends on whether the attributes are numerical or categorical. To reduce the effect of these two attributes, the same experiments will be performed on six different problems, with three relevant attributes that are generated by both, and three with sufficient attributes."}, {"heading": "4.3 Design", "text": "Here we have to decide which of the possible combinations of factors will be tried in the experiments. A better way to reduce or eliminate the contribution to the experimental errors of each factor would be to treat them as blocking factors, i.e. to create homogeneous blocks in which the factors are kept constant while the target factor takes all possible values. If blocking is not possible due to limited resources, a random subset of each block can be executed. In the ranges described above, there are a total of 3 \u00d7 I \u00b7 N \u00b7 (N \u2212 1) different factor combinations for each problem, as shown in Equation 4.1, where N is the number of relevant attributes and I \u00b7 N \u00b7 (N \u2212 1) is the number of iterations (i.e. random row generations) for each combination of relevant and irrelevant attribute numbers."}, {"heading": "4.4 Problems", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.4.1 RDG1NamedContinuous", "text": "A data generator that generates data randomly with numerical attributes by creating a decision list consisting of rules. The rules are in the form cx: = 1 t, where t is an inequality term (i.e. x < y or x \u2265 y) between any attribute and a random value. For each rule, the number n will be a random number in the range [1.. 10]. An example set of rules can be generated randomly one by one under Equation 4.2.RULE 0: c0: = a1 < 0.986: a0 > = 0.65 RULE 1: = a1 < 0.95: a2 < 0.129 RULE 2: c2: = a1 > = 0.562 (4.2) Instances are randomly generated one by one. The class is determined by the first rule that applies to the current instance."}, {"heading": "4.4.2 RandomRBFRandRed1", "text": "Radial basic functions (RBF) are functions whose characteristic feature is that their response decreases (or increases) monotonously with distance from a central point. There are various formulas to describe the specific form of the function, and they usually have parameters to control the center and the distance scale. In this particular case, the function f (x) is the Gaussian one described by Equation 4.3 and shown in Fig. 6. Their parameters are their mean \u00b5 and their standard deviation. A Gaussian RBF decreases monotonously with distance from the centre. f (x) = 1\u03c3 2\u03c0 exp (\u2212 (x \u2212 \u00b5) 22\u03c32) (4.3) Random RBF data are generated by first generating a random set of centers for each class. Each center is randomly assigned to one weight, one central point per attribute and one standard deviation. To generate new instances, a center is selected taking into account each center's weight."}, {"heading": "4.4.3 NonMonotonic", "text": "Now I create for each instance a random value ri in rage [0.. N], where N is the number of important attributes. For example, the value ai of the attribute a will be the one in Equation 4.4.ai = {ra \u00b7 ri, if i mod 2 6 = 0 ra \u00b7 \u221a ri, if i mod 2 = 0 (4.4) For example, the class i will be the integer part of ri. Irrelevant attributes will be randomly generated after an even distribution in the range [0, 1]."}, {"heading": "4.4.4 MajorityN", "text": "Generates n binary attributes and i irrelevant attributes. The class attribute is 1 if the instance has a majority of 1s in the relevant attributes and 0 otherwise."}, {"heading": "4.4.5 ModuloP", "text": "Each modulo-p problem is described by a set of relevant and irrelevant attributes, both with integer values in the range [0, p). Class c can be defined as in Equation 4.5.c = \u2211 r \u0435Rr mod p (4.5)."}, {"heading": "4.4.6 RDG1NamedCategoric", "text": "The same data generator as for RDG1NamedContinuous, but this time Boolean attributes instead of numeric Boolean attributes, so that the rules are now Boolean predicates."}, {"heading": "5 Results", "text": "In this section, the results of the experiments described above are presented. Six diagrams are presented in Fig. 7. To clearly understand what the axes represent, a notation must be introduced. Let R = r1, r2,.., rn be the relevant attributes set and I = i1, i2,.., in the set of irrelevant with | R | = n and | I | = m. And let (a) be the weight that the algorithm has assigned to an irrelevant formula. Now, the x axis represents the total number of attributes (m + n) and the y axis represents the separability s (i.e. the maximum weight assigned to a relevant attribute, minus the maximum weight assigned to an irrelevant formula). Formulas are shown in Equation 5.1.x axis: m + ny axis: s = (max-R w (ar)) \u2212 (max ai-I w (ai)) Now, to highlight the differences between the three algorithms."}, {"heading": "6 Conclusions", "text": "This year it is more than ever before."}], "references": [{"title": "The feature selection problem: Traditional methods and a new algorithm", "author": ["Kira", "Rendell", "K. 1992] Kira", "L.A. Rendell"], "venue": "In AAAI,", "citeRegEx": "Kira et al\\.,? \\Q1992\\E", "shortCiteRegEx": "Kira et al\\.", "year": 1992}, {"title": "Wrappers for feature subset selection", "author": ["Kohavi", "John", "R. 1997] Kohavi", "G.H. John"], "venue": "Artif. Intell.,", "citeRegEx": "Kohavi et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Kohavi et al\\.", "year": 1997}, {"title": "Theoretical and empirical analysis of relieff and rrelieff", "author": ["Robnik-\u0160ikonja", "Kononenko", "M. 2003] Robnik-\u0160ikonja", "I. Kononenko"], "venue": "Machine Learning,", "citeRegEx": "Robnik.\u0160ikonja et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Robnik.\u0160ikonja et al\\.", "year": 2003}, {"title": "A review and empirical evaluation of feature weighting methods for a class of lazy learning", "author": ["Wettschereck et al", "D. 1997] Wettschereck", "D.W. Aha", "T. Mohri"], "venue": null, "citeRegEx": "al. et al\\.,? \\Q1997\\E", "shortCiteRegEx": "al. et al\\.", "year": 1997}, {"title": "Improved heterogeneous distance functions", "author": ["Wilson", "Martinez", "D.R. 1997] Wilson", "T.R. Martinez"], "venue": "J. Artif. Intell. Res. (JAIR),", "citeRegEx": "Wilson et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Wilson et al\\.", "year": 1997}], "referenceMentions": [], "year": 2017, "abstractText": "Feature weighting algorithms try to solve a problem of great importance nowadays in machine learning: The search of a relevance measure for the features of a given domain. This relevance is primarily used for feature selection as feature weighting can be seen as a generalization of it, but it is also useful to better understand a problem\u2019s domain or to guide an inductor in its learning process. Relief family of algorithms are proven to be very effective in this task. On previous work, a new extension was proposed that aimed for improving the algorithm\u2019s performance and it was shown that in certain cases it improved the weights\u2019 estimation accuracy. However, it also seemed to be sensible to some characteristics of the data. An improvement of that previously presented extension is presented in this work that aims to make it more robust to problem specific characteristics. An experimental design is proposed to test its performance. Results of the tests prove that it indeed increase the robustness of the previously proposed extension. 1 Overview Feature selection is undoubtedly one of the most important problems in machine learning, pattern recognition and information retrieval, among others. A feature selection algorithm is a computational solution that is motivated by a certain definition of relevance. However, the relevance of a feature may have several definitions depending on the objective that is looked after. On the other hand, feature weighting algorithms try to estimate relevance (in the form of weights to the features) rather than binarily deciding whether a feature is either relevant or not. This is a much harder problem, but also a more flexible framework from an inductive learning perspective. This kind of 1 algorithms are confronted with the down-weighting of irrelevant features, the up-weighting of relevant ones and the problem of relevance assignment when redundancy is an issue. In this work we review Relief, one of the most popular feature weighting algorithms. Original Relief and some of its variants are presented on section 2 drawing heavily on own earlier material. Next, we revisit a \"double\" or feedback extension of the algorithm, that was firstly introduced in an own previous work, that takes its own estimations into account in order to improve general performance. Finally a new version of the algorithm is presented on section 3 that uses its own estimations in a progressive manner, it initially behaves like the traditional algorithm and gradually increases the importance of its estimates to behave at the end as the \"double\" version. An experimental design is presenten in secion 4 to test the performance of the original algorithm versus the two proposed ones. Finally some results and conclusions are presented. 2 Relief Relief is a feature weighting algorithm that doesn\u2019t share one common characteristic of the feature selection and weighting methods. Most of them treat features individually assuming conditional independence of features upon the class. In the other hand, Relief takes all other features in care when evaluating a specific feature. Another interesting characteristic of Relief is that it is aware of contextual information being able to detect local correlations of feature values and their ability to discriminate from an instance of a different class. The main idea behind Relief is to assign large weights to features that contribute in separating near instances of different class and joining near instances belonging to the same class. The word \"near\" in the previous sentence is of crucial importance since we mentioned that one of the main differences between Relief and the other cited methods is the ability to take local context into account. Relief does not reward features that separate (join) instances of different (same) classes in general but features that do so for near instances. In Fig. 1 we can see the original algorithm presented by Kira and Rendell in [Kira and Rendell, 1992]. We maintained the original notation that slightly differs from the used above as now features (attributes) are labeled A. There we can see that in the aim of detecting whether the feature is useful to discriminate near instances it selects two nearest neighbors of the current instance Ri. One from the same class H called the nearest hit and one from the different class M (the original Relief algorithm only dealt with two class problems) called the nearest miss. With these two nearest neighbors it increases the weight ofthe feature if it has the same value for both Ri and H and decreases it otherwise. The opposite occurs with the nearest miss, Relief increases the weight of a feature if it has opposite values for Ri and M and decreases it otherwise. One of the central parts of Relief is the difference function diff which is also", "creator": "LaTeX with hyperref package"}}}