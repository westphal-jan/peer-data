{"id": "1509.04340", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "14-Sep-2015", "title": "Voted Kernel Regularization", "abstract": "This paper presents an algorithm, Voted Kernel Regularization , that provides the flexibility of using potentially very complex kernel functions such as predictors based on much higher-degree polynomial kernels, while benefitting from strong learning guarantees. The success of our algorithm arises from derived bounds that suggest a new regularization penalty in terms of the Rademacher complexities of the corresponding families of kernel maps. In a series of experiments we demonstrate the improved performance of our algorithm as compared to baselines. Furthermore, the algorithm enjoys several favorable properties. The optimization problem is convex, it allows for learning with non-PDS kernels, and the solutions are highly sparse, resulting in improved classification speed and memory requirements.", "histories": [["v1", "Mon, 14 Sep 2015 21:58:43 GMT  (21kb)", "http://arxiv.org/abs/1509.04340v1", "16 pages"]], "COMMENTS": "16 pages", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["corinna cortes", "prasoon goyal", "vitaly kuznetsov", "mehryar mohri"], "accepted": false, "id": "1509.04340"}, "pdf": {"name": "1509.04340.pdf", "metadata": {"source": "CRF", "title": null, "authors": [], "emails": ["corinna@google.com,", "pg1338@nyu.edu,", "vitaly@cims.nyu.edu,", "mohri@cs.nyu.edu."], "sections": [{"heading": null, "text": "ar Xiv: 150 9.04 340v 1 [cs.L G] 14 Sep 2015"}, {"heading": "1. INTRODUCTION", "text": "The idea behind it is not new, but it is about the question of what the future of mankind is like. \"It is about the future,\" he says, \"it is about the future.\" \"It is about the future.\" \"It is about the future.\" \"It is about the future.\" \"It is about the future.\" \"It is about the future.\" \"It is about the future.\" \"It is about the future.\" \"It is about the future.\" \"It is about the future.\" \"\" It is about the future. \"\" It is about the future. \"\" It is about the future. \"It is about the future.\" It is about the future. \"\" It is about the future. \"\" It is about the future. \"\" It is about the future. \"It is about the future.\" It is about the future. \"It is about the future.\" It is about the future. \"It is about the future.\" It is about the future. \"It is about the future.\" It is about the future. \"It is about the future.\" It is about the future. \"It is about the future.\" It is about the future."}, {"heading": "2. PRELIMINARIES", "text": "We assume that training and examination points i.i.d. are drawn according to some distribution D over X \u00b7 {\u2212 1, + 1} and that S = (x1, y1),.., (xm, ym) is a training sample of size m drawn according to Dm. For a function f that includes values in R, let us call R (f) its binary classification error, R \u00b2 S (f) its empirical error, and R \u00b2 S (f) its empirical marginal error for the sample S: R (f) = E (x, y). D [1yf (x) \u2264 0], R \u00b2 S (f) = E (x, y)."}, {"heading": "3. THE VOTED KERNEL REGULARIZATION ALGORITHM", "text": "In this section we will introduce the Voted Kernel Regularization Algorithm. Let K1, \u03b2., Kp be p positive semi-definite (PSD) functions with modified (PSD) function with modified (PSD) function with modified (PSD) function., Hp, defined by Hk = 7 \u2192 Kk (x, x): x, where the drawing accounts for two possible ways of classifying a point x. \"The general form of a hypothesis f returned by the algorithm is the following: f = m, j = 1p, jKk (\u00b7, xj), where the drawing accounts for two possible ways of classification x.,\" f is a linear combination of hypotheses in Hks."}, {"heading": "4. LEARNING GUARANTEES", "text": "In this section, we offer strong data-dependent learning guarantees for the Voting Kernel Regularization Algorithms = 2.0 m = 2.0 log = 2.0 log = 2.0 log = 2.0 log = 2.0 log = 2.0 log = 2.0 log = 2.0 log = 2.0 log = 2.0 log = 2.0 log = 2.0 log = 2.0 log = 2.0 log = 2.0 log = 2.0 log = 2.0 log = 2.0 log = 2.0 log = 2.0 log = 2.0 log = 2.0 log = 2.0 log = 2.0 log = 2.0 log = 2.0 log = 2.0 log = 2.0 log = 2.0 log = 2.0 log = 2.0 log = 2.0 log = 2.0 log = 2.0 log = 2.0 log = 2.0 log = 2.0 log = 2.0 log = 2.0 log = 2.0 log = 2.0 log = 2.0 log = 2.0 log = 2.0 log = 2.0 log = 2.0 log = 2.0 log = 2.0 log = 2.0 log = 2.0 log = 2.0 log = 2.0 log 2.0 log = 2.0 log = 2.0 log 2.0 log = 2.0 log = 2.0 log 2.0 log = 2.0 log = 2.0 log 2.0 log = 2.0 log = 2.0 log = 2.0 log = 2.0 log 2.0 log = 2.0 log = 2.0 log = 2.0 log = 2.0 log = 2.0 log = 2.0 log = 2.0 log = 2.0 log = 2.0 log = 2.0 log = 2.0 log = 2.0 log = 2.0 log = 2.0 log = 2.0 log = 2.0 log = 2.0 log = 2.0 log = 2.0 log = 2.0 log = 2.0 log = 2.0 log = 2.0 log = 2.0 log = 2.0 log = 2.0 log = 2.0 log = 2.0 log = 2.0 log = 2.0 log = 2.0 log = 2.0 log = 2.0 log = 2.0 log = 2.0 log = 2.0 log = 2.0 log = 2.0 log = 2.0 log = 2.0 log = 2.0 log = 2.0 log = 2.0 log = 2.0 log = 2.0 log = 2.0 log = 2.0 log = 2.0 log = 2.0 log = 2.0 log = 2.0 log = 2.0 log = 2.0 log = 2.0 log = 2.0 log = 2.0 log = 2.0 log = 2.0 log = 2.0 log = 2.0 log = 2.0 log = 2.0 log = 2.0 log = 2.0 log = 2.0 log = 2.0 log 2.0 log = 2.0 log = 2.0 log = 2.0 log = 2.0 log 2.0 log = 2.0 log 2.0 log = 2.0 log = 2.0 log 2.0 log = 2.0 log 2.0 log = 2.0 log 2.0 log = 2.0 log 2.0 log = 2.0 log 2.0 log 2.0 log = 2.0 log 2.0 log 2.0 log = 2.0 log = 2.0 log 2.0 log = 2.0 log 2.0 log = 2.0 log 2.0 log 2.0 log = 2.0 log 2.0 log 2.0 log 2.0 log"}, {"heading": "5. OPTIMIZATION SOLUTIONS", "text": "In this section we propose two different algorithmic approaches to solve the optimization problem (1): Linear Programming (LP) and Coordinative Descendancy (CD)."}, {"heading": "6. EXPERIMENTS", "text": "We experimented with several benchmark datasets from the UCI repository, specifically breast cancer, climate, diabetes, german (numeric), ionosphere, musk, ocr49, phishing, retinopathy, vertebrl, and waveform01. Here, ocr49 refers to the subset of OCR datasets with classes 4 and 9, and similarly waveform01 refers to the subset of waveform dataset with classes 0 and 1. Further details on all datasets are given in Table 2 in Appendix D. Our experiments compared voted kernel regularization to regular SVM, which we refer to as L2-SVM, and norm-1 SVM dataset with classes 0 and 1. In all of our experiments, we used lp solve, an-the-shelf LP solver to solve the voted kernel regularization and L1-SVM optimization problems."}, {"heading": "7. CONCLUSION", "text": "Our algorithm benefits from strong data-dependent learning guarantees that allow learning with highly complex feature maps, but do not overlap. We have further improved these learning guarantees through local complexity analyses that lead to an extension of the Voted Kernel Regularization Algorithm. The key ingredient of our algorithm is a new regularization term that takes advantage of the wheel-making complexity of various families of core functions used by the Voted Kernel Regularization Algorithm. We offer a thorough analysis of several different alternatives that can be used for this approximation. In addition, we offer two practical implementations of our algorithm based on linear programming and coordinate derivation. Finally, we presented results of extensive experiments that show that our algorithm always finds solutions that are much more economical than those of the other supporting vector algorithms, while frequently outperforming other formulations."}, {"heading": "APPENDIX A. PROOFS OF LEARNING GUARANTEES", "text": "Theorem 2: Proof. for a fixed h = (h1, h1, h1, h2, h2, h3, h3, h3, h3, h3, h3, h3, h3, h3, h3, h3, h3, h3, h3, h3, h3, h3, h3, h3, h3, h3, h3, h3, h3, h3, h3, h3, h3, h3, h3, h3, h3, h3, h3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, h3, h3, h3, h3, h3, h3, h3, h3, h3, h3, h3, h3, h3, h3, h3, h3, h3, h3, h3, h3, h3, h3, h3, h3, h3, h3, h3, h3, h3, h3, h3, h3, h3, h3, h3, h3, h3, h3, h3, h3, h3, h3, h3, h3, h3, h3, h3, h3, h3, h3, h3, h3, h3, h3, h3, h3, h3, h3, h3, h3, h3, h3, h3, h3, h3, h3, h3, h3, h3, h3, h3, h3, h3, h3, h3, h3, h3, h3, h3, h3, h3, h3, h3, h3, h3, h3, h3, h3, h3, h3, h3,"}, {"heading": "APPENDIX B. OPTIMIZATION PROBLEM", "text": "This section returns the derivation for the VKR optimization problem. We assume that H1 = \u03b2 = \u03b2 = 11 \u00b0 i = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p"}, {"heading": "APPENDIX C. COORDINATE DESCENT (CD) FORMULATION", "text": "An alternative approach to solving the Voted Kernel Regularization Optimization Problem (1) consists of using a coordinate descent method (1). A coordinate descent method runs in rounds. In each round, it receives a parameter vector \u03b1. Leave \u03b1t = (\u03b1t, k, j) k, j denote the vector that is achieved after t \u2265 1 iterations, and leave \u03b10 = 0. Leave ek, j the unit vector in the direction (k, j) in Rp \u00b7 m. Then the direction ek, j and the step length selected in the tth round are those minimizing F (\u03b1t \u2212 1 + TABLE 2 \u2212 \u2212.Dataset statistics.Examples Features breastcancer 699 9 Klima 540 18 Diabetes 768 8 German 1000 24 ionosphere 351 34 musk 476 166 ocr49 2000 phil."}, {"heading": "APPENDIX D. DATASET STATISTICS", "text": "The statistics of the data sets can be found in Table 2."}], "references": [{"title": "Ensembles of kernel predictors", "author": ["C. Cortes", "M. Mohri", "A. Rostamizadeh"], "venue": "In UAI,", "citeRegEx": "2010", "shortCiteRegEx": "2010", "year": 2011}, {"title": "Multi-task feature and kernel selection for SVMs", "author": ["T. Jebara"], "venue": "In ICML,", "citeRegEx": "Jebara.,? \\Q2004\\E", "shortCiteRegEx": "Jebara.", "year": 2004}, {"title": "Nonstationary kernel combination", "author": ["D.P. Lewis", "T. Jebara", "W.S. Noble"], "venue": "In ICML,", "citeRegEx": "Lewis et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Lewis et al\\.", "year": 2004}, {"title": "Statistical Learning Theory", "author": ["V.N. Vapnik"], "venue": "In COLT,", "citeRegEx": "Vapnik.,? \\Q2006\\E", "shortCiteRegEx": "Vapnik.", "year": 2006}, {"title": "Multiclass multiple kernel learning", "author": ["A. Zien", "C.S. Ong"], "venue": "ICML", "citeRegEx": "Zien and Ong.,? \\Q2007\\E", "shortCiteRegEx": "Zien and Ong.", "year": 2007}, {"title": "2005], for any \u03b4 > 0 with probability at least 1 \u2212 \u03b4", "author": ["Bartlett"], "venue": null, "citeRegEx": "Bartlett,? \\Q2005\\E", "shortCiteRegEx": "Bartlett", "year": 2005}], "referenceMentions": [{"referenceID": 3, "context": "In the absence of any regularization, that is \u03bb = 0 and \u03b2 = 0, it reduces to the minimization of the Hinge loss and is therefore of course close to the SVM algorithm [Cortes and Vapnik, 1995]. For \u03bb = 0, that is when discarding our regularization based on the different complexity of the hypothesis sets, the algorithm coincides with an algorithm originally described by Vapnik [1998][pp.", "startOffset": 178, "endOffset": 385}], "year": 2015, "abstractText": "This paper presents an algorithm, Voted Kernel Regularization , that provides the flexibility of using potentially very complex kernel functions such as predictors based on much higher-degree polynomial kernels, while benefitting from strong learning guarantees. The success of our algorithm arises from derived bounds that suggest a new regularization penalty in terms of the Rademacher complexities of the corresponding families of kernel maps. In a series of experiments we demonstrate the improved performance of our algorithm as compared to baselines. Furthermore, the algorithm enjoys several favorable properties. The optimization problem is convex, it allows for learning with non-PDS kernels, and the solutions are highly sparse, resulting in improved classification speed and memory requirements.", "creator": "LaTeX with hyperref package"}}}