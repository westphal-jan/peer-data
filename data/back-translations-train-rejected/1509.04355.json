{"id": "1509.04355", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "15-Sep-2015", "title": "Towards Making High Dimensional Distance Metric Learning Practical", "abstract": "In this work, we study distance metric learning (DML) for high dimensional data. A typical approach for DML with high dimensional data is to perform the dimensionality reduction first before learning the distance metric. The main shortcoming of this approach is that it may result in a suboptimal solution due to the subspace removed by the dimensionality reduction method. In this work, we present a dual random projection frame for DML with high dimensional data that explicitly addresses the limitation of dimensionality reduction for DML. The key idea is to first project all the data points into a low dimensional space by random projection, and compute the dual variables using the projected vectors. It then reconstructs the distance metric in the original space using the estimated dual variables. The proposed method, on one hand, enjoys the light computation of random projection, and on the other hand, alleviates the limitation of most dimensionality reduction methods. We verify both empirically and theoretically the effectiveness of the proposed algorithm for high dimensional DML.", "histories": [["v1", "Tue, 15 Sep 2015 00:04:24 GMT  (43kb)", "http://arxiv.org/abs/1509.04355v1", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["qi qian", "rong jin", "lijun zhang", "shenghuo zhu"], "accepted": false, "id": "1509.04355"}, "pdf": {"name": "1509.04355.pdf", "metadata": {"source": "CRF", "title": "Towards Making High Dimensional Distance Metric Learning Practical", "authors": ["Qi Qian", "Rong Jin", "Lijun Zhang", "Shenghuo Zhu", "QI QIAN", "RONG JIN", "LIJUN ZHANG", "SHENGHUO ZHU"], "emails": ["QIANQI@CSE.MSU.EDU", "RONGJIN@CSE.MSU.EDU", "ZLJ@CSE.MSU.EDU", "ZSH@NEC-LABS.COM"], "sections": [{"heading": null, "text": "ar Xiv: 150 9.04 355v 1 [cs.L GKeywords: Distance Metric Learning, Dual Random Projection"}, {"heading": "1. Introduction", "text": "This year it is more than ever before."}, {"heading": "2. Related Work", "text": "Many algorithms have been developed for DML (Xing et al., 2002; Globerson and Roweis, 2005; Davis et al., 2007; Weinberger and Saul, 2009). Examples of DML algorithms are MCML (Globerson and Roweis, 2005), ITML (Jin et al., 2009), LMNN and OASIS (Chechik et al., 2010). In addition to algorithms, several studies have been conducted to analyze the generalization of DML (Jin et al., 2012), Bellet and Habe, 2012). Survey papers (Yang and Jin et al., 2013) provide detailed studies on the subject. Although numerous studies have been devoted to DML, only a great deal of progress is being made to address the large dimensions in DML."}, {"heading": "3. Dual Random Projection for Distance Metric Learning", "text": "Let's say X = (x1, \u00b7 \u00b7, xn) - Rd \u00b7 n stands for the collection of training examples. Given a PSD matrix M, the distance between two examples xi and xj is given as (xi, xj) = (xi \u2212 xj) - M (xi \u2212 xj) - M (xi \u2212 xj) - M (xi \u2212 xj) -M (xi \u2212 xj) -M (xi \u2212 xj) -M (xi \u2212 xj) -M (xi \u2212 xj) -M (xi \u2212 xj).The proposed framework for DML will be based on triplet constraints, not on color constraints. Let D = (x1i, x1k), xNj, xNj, xNj, xNk) - we will apply the set of triplet constraints used for training where xxtj is expected to be more similar to x. Our goal is to learn a metric function that is consistent with most problems."}, {"heading": "3.1 Dual Random Projection for Distance Metric Learning", "text": "The direct solution of the primary problem in (1) or the dual problem in (2) could be mathematically expensive (2) if the data is of high dimension and the number of triplets is very large. We address this challenge by using a random matrix R, Ri and Ri, j and N (0, 1 / m), and project all data points into the random space using the random matrix, i.e., x and Ri, j and Ri, j and N (0, 1 / m), and project all data points into the random space using the random matrix, i.e., x and Ri = R. As a result, after random projection, A and Ri, j and R. \"A typical approach of random projection for the DML is to obtain a random projection, m and mby solving the random problem with the random projected vectors."}, {"heading": "3.2 Main Theoretical Results", "text": "First: (Zhang et al., 2013) Let's consider the case when the data matrix X is of low rank. (The theorem below shows that under the low rank assumption, with a high probability, the distance metric recovered by algorithm 1 is nearly optimal. (DuRP) The theorem below shows that the triplet condition D and the number of random projections m. (2) In contrast to a random matrix R: Rd \u00b7 m and Ri, j \u00b2 N (0, 1 / m). 3: Project each example as x = R x. 4: Solving the optimization problem (5) and achieving the optimal solution in the original space of M + 1: 1 Let's assume that we find the optimal solution of At6: Output: MSD (M \u00b2) Theorem 1 Let's be the optimal solution (1)."}, {"heading": "4. Experiments", "text": "We will first describe the experimental framework and then present our empirical study for ranking and classification tasks on different datasets."}, {"heading": "4.1 Experimental Setting", "text": "In fact, it is in such a way that most of us are able to hide ourselves in the position, namely in the way in which they are able, in which they are able, in which they are able, in which they are able, in which they are able, in which they are able to assert themselves, in which they are able, in which they are able to exist, in which they are able to exist, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live in which they live, in which they live, in which they live in which they live, in which they live in which they live, in which they live, in which they live in which they live, in which they live, in which they live in which they live, in which they live, in which they live in which they live, in which they live, in which they live, in which they live in which they live, in which they live, in which they live in which they live, in which they live, in which they live in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live,"}, {"heading": "4.2 Efficiency of the Proposed Method", "text": "In this experiment, we set the number of random projections to 10, which, according to the experimental results in sections 4.3 and 4.4, gives almost the optimal performance for the proposed algorithm. To be fair, the number of reduced dimensions will also be compared for LMN.Table. 2 compares the CPU time (in minutes) of different methods.Note that the time of sampling of triplets is not taken into account, as it is consumed by all methods, and all other operators (e.g. random projection and PCA) are included. It is not surprising to find that DuRP, SRP and SPCA have similar CPUtimes and are significantly more efficient than the other methods due to the effect of dimensionality reduction. Since DuRP and SRP share the same method for calculating the dual variables in subspace, the only difference between them lies in the method of reconstructing the distance metric from the estimated variables."}, {"heading": "4.3 Evaluation by Ranking", "text": "In the first experiment, we set the number of random projections used by SRP, SPCA and the proposed DuRP algorithm to 10, which represents about 1% of the dimensionality of the original space. For a fair comparison, the number of reduced dimensions for LMNN is also set to 10. We measure the quality of the acquired metrics using the metrics of the mAPP. 3 summarizes the performance of the different methods for DML."}, {"heading": "4.4 Evaluation by Classification", "text": "In this experiment, we evaluate the learned metric based on its classification accuracy using the k-NN (k = 5) classifier. We emphasize that the purpose of this experiment is to evaluate the metrics learned through various DML algorithms, not to show that the learned metric leads to the current classification performance2. Similar to the ranking assessment, all experiments are performed five times and the results are averaged over five attempts with standard deviation in Figure 3. We have essentially the same observations as in the ranking experiments reported in Section 4.3, except that the three methods DuRP, DuOri, and OASIS perform very similarly for most datasets. Note the main concern of this work is time efficiency, and the size of the metric learned is d \u00b7 d \u00b7 d. It is easy to store the learned metric efficiently by maintaining a slight approximation to it."}, {"heading": "5. Conclusion", "text": "In this paper, we propose a dual random projection method to learn the distance metric for large-format high-dimensional datasets, the main idea being to solve the dual problem in the random-projection subspace, and then restore the distance metric in the original space using the estimated dual variables. We develop the theoretical guarantee that the proposed method is highly likely to restore the optimal solution with little error if the data matrix is low-ranking, and the optimal dual variables, even if the data matrix cannot be approached well by a low-ranking matrix. Our empirical study confirms both the effectiveness and efficiency of method 2. Many studies (e.g. (Weinberger and Saul, 2009; Xu et al., 2012) have shown that metric learning provides no better classification accuracy than the standard classification algorithms (e.g. SVM) with a sufficient number of training data."}, {"heading": "Appendix A. Proof of Theorem 1", "text": "First, we want to prove that G \u2212 s is a good estimate for G. \u2212 Z \u2212 Z \u2212 Z \u2212 Z \u2212 Z \u2212 Z \u2212 Z S (xai \u2212 xaj) (xai \u2212 xak) (xai \u2212 xak) (xai \u2212 xak) (xai \u2212 xak) (xai \u2212 xaj) (xai \u2212 xaj) (xbi \u2212 xbk) (xbi \u2212 xbj) (xbi \u2212 xbj) (xbj) > (xai \u2212 xak) \u2212 Z s (xai \u2212 xtk) s (xai \u2212 xtk), (xbi \u2212 xbj) \u2212 xbj) (xbi \u2212 xbj) > = < za, zb > where zt = (xti \u2212 xtk) Z \u2212 Z \u2212 Z s (xbj)."}, {"heading": "Appendix B. Proof of Theorem 2", "text": "Our analysis is based on the following two theorems: Theorem 4 (Theorem 2 (Blum, 2005) Let x (Alpha) Rd, and x (Alpha) x (Alpha) x (Alpha) x (Alpha) x (Alpha) x (Alpha) x (Alpha) x (Alpha) x (Alpha) x (Alpha) x (Alpha) x (Alpha) x (Alpha) x (Alpha) x (Alpha) x (Alpha) x (Alpha) x (Alpha) x (Alpha) x (Alpha) x (Alpha) x (Alpha) x (Alpha) x (Alpha) x (Alpha) x (Alpha) x (Alpha) x (Alpha) x (Alpha) x (Alpha) x (Alpha) x (Alpha) x (Alpha) x (Alpha) x (Alpha) x (Alpha) x (Alpha) x (Alpha) x (Alpha) x (Alpha) x (Alpha) x (Alpha) x (Alpha) x (Alpha) x (Alpha) x (Alpha) x (Alpha) x (Alpha) x (Alpha) x (Alpha) x (Alpha) x (Alpha) x (Alpha) x (Alpha) x (Alpha) x (Alpha) x (Alpha) x (Alpha) x (Alpha) x (Alpha) x (Alpha) x (Alpha) x (Alpha) x (Alpha) x (Alpha) x (Alpha) x (Alpha) x (Alpha) x (Alpha) x (Alpha) x (Alpha) x (Alpha) x (Alpha) x (Alpha) x (Alpha) x (Alpha) x (Alpha) x (Alpha) x (Alpha) x (Alpha) x (Alpha) x (Alpha) x (Alpha) x (Alpha) x (Alpha) x (Alpha) x (Alpha) x (Alpha) x (Alpha) x (Alpha) x (Alpha) x (Alpha) x (Alpha) x (Alpha) x (Alph"}], "references": [{"title": "Robustness and generalization for metric learning", "author": ["Aur\u00e9lien Bellet", "Amaury Habrard"], "venue": "CoRR, abs/1209.1086,", "citeRegEx": "Bellet and Habrard.,? \\Q2012\\E", "shortCiteRegEx": "Bellet and Habrard.", "year": 2012}, {"title": "Random projection, margins, kernels, and feature-selection", "author": ["Avrim Blum"], "venue": "In SLSFS, pages 52\u201368,", "citeRegEx": "Blum.,? \\Q2005\\E", "shortCiteRegEx": "Blum.", "year": 2005}, {"title": "Random projections for k-means clustering", "author": ["Christos Boutsidis", "Anastasios Zouzias", "Petros Drineas"], "venue": "In NIPS,", "citeRegEx": "Boutsidis et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Boutsidis et al\\.", "year": 2010}, {"title": "Probabilistic dyadic data analysis with local and global consistency", "author": ["Deng Cai", "Xuanhui Wang", "Xiaofei He"], "venue": "In ICML,", "citeRegEx": "Cai et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Cai et al\\.", "year": 2009}, {"title": "Libsvm: A library for support vector machines", "author": ["Chih-Chung Chang", "Chih-Jen Lin"], "venue": "ACM TIST,", "citeRegEx": "Chang and Lin.,? \\Q2011\\E", "shortCiteRegEx": "Chang and Lin.", "year": 2011}, {"title": "Large scale online learning of image similarity through ranking", "author": ["Gal Chechik", "Varun Sharma", "Uri Shalit", "Samy Bengio"], "venue": "JMLR, 11:1109\u20131135,", "citeRegEx": "Chechik et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Chechik et al\\.", "year": 2010}, {"title": "Structured metric learning for high dimensional problems", "author": ["Jason V. Davis", "Inderjit S. Dhillon"], "venue": "In KDD,", "citeRegEx": "Davis and Dhillon.,? \\Q2008\\E", "shortCiteRegEx": "Davis and Dhillon.", "year": 2008}, {"title": "Information-theoretic metric learning", "author": ["Jason V. Davis", "Brian Kulis", "Prateek Jain", "Suvrit Sra", "Inderjit S. Dhillon"], "venue": "In ICML,", "citeRegEx": "Davis et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Davis et al\\.", "year": 2007}, {"title": "Experiments with random projections for machine learning", "author": ["Dmitriy Fradkin", "David Madigan"], "venue": "In KDD,", "citeRegEx": "Fradkin and Madigan.,? \\Q2003\\E", "shortCiteRegEx": "Fradkin and Madigan.", "year": 2003}, {"title": "Metric learning by collapsing classes", "author": ["Amir Globerson", "Sam T. Roweis"], "venue": "In NIPS,", "citeRegEx": "Globerson and Roweis.,? \\Q2005\\E", "shortCiteRegEx": "Globerson and Roweis.", "year": 2005}, {"title": "Projection-free online learning", "author": ["Elad Hazan", "Satyen Kale"], "venue": "In ICML,", "citeRegEx": "Hazan and Kale.,? \\Q2012\\E", "shortCiteRegEx": "Hazan and Kale.", "year": 2012}, {"title": "Regularized distance metric learning: Theory and algorithm", "author": ["Rong Jin", "Shijun Wang", "Yang Zhou"], "venue": "In NIPS,", "citeRegEx": "Jin et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Jin et al\\.", "year": 2009}, {"title": "The spectrum of kernel random matrices", "author": ["Noureddine El Karoui"], "venue": "In The Annals of Statistics,", "citeRegEx": "Karoui.,? \\Q2010\\E", "shortCiteRegEx": "Karoui.", "year": 2010}, {"title": "Metric learning: A survey", "author": ["Brian Kulis"], "venue": "Foundations and Trends in Machine Learning,", "citeRegEx": "Kulis.,? \\Q2013\\E", "shortCiteRegEx": "Kulis.", "year": 2013}, {"title": "Robust structural metric learning", "author": ["Daryl Lim", "Gert Lanckriet", "Brian McFee"], "venue": "In ICML,", "citeRegEx": "Lim et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Lim et al\\.", "year": 2013}, {"title": "Stochastic gradient descent with only one projection", "author": ["M. Mahdavi", "T. Yang", "R. Jin", "S. Zhu", "J. Yi"], "venue": "In NIPS,", "citeRegEx": "Mahdavi et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Mahdavi et al\\.", "year": 2012}, {"title": "Linear regression with random projections", "author": ["Odalric-Ambrym Maillard", "Remi Munos"], "venue": "In JMLR,", "citeRegEx": "Maillard and Munos.,? \\Q2012\\E", "shortCiteRegEx": "Maillard and Munos.", "year": 2012}, {"title": "An efficient sparse metric learning in high-dimensional space via l1-penalized log-determinant regularization", "author": ["Guo-Jun Qi", "Jinhui Tang", "Zheng-Jun Zha", "Tat-Seng Chua", "Hong-Jiang Zhang"], "venue": null, "citeRegEx": "Qi et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Qi et al\\.", "year": 2009}, {"title": "Random features for large-scale kernel machines", "author": ["Ali Rahimi", "Benjamin Recht"], "venue": "In NIPS,", "citeRegEx": "Rahimi and Recht.,? \\Q2007\\E", "shortCiteRegEx": "Rahimi and Recht.", "year": 2007}, {"title": "Stochastic dual coordinate ascent methods for regularized loss minimization", "author": ["Shai Shalev-Shwartz", "Tong Zhang"], "venue": "CoRR, abs/1209.1873,", "citeRegEx": "Shalev.Shwartz and Zhang.,? \\Q2012\\E", "shortCiteRegEx": "Shalev.Shwartz and Zhang.", "year": 2012}, {"title": "Learning a distance metric from a network", "author": ["Blake Shaw", "Bert C. Huang", "Tony Jebara"], "venue": "In NIPS, pages 1899\u20131907,", "citeRegEx": "Shaw et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Shaw et al\\.", "year": 2011}, {"title": "Vector space projections: a numerical approach to signal and image processing, neural nets, and optics", "author": ["Henry Stark", "Yongi Yang", "Yongyi Yang"], "venue": null, "citeRegEx": "Stark et al\\.,? \\Q1998\\E", "shortCiteRegEx": "Stark et al\\.", "year": 1998}, {"title": "Manifold modeling with learned distance in random projection space for face recognition", "author": ["Grigorios Tsagkatakis", "Andreas E. Savakis"], "venue": "In ICPR,", "citeRegEx": "Tsagkatakis and Savakis.,? \\Q2010\\E", "shortCiteRegEx": "Tsagkatakis and Savakis.", "year": 2010}, {"title": "Distance metric learning for large margin nearest neighbor classification", "author": ["Kilian Q. Weinberger", "Lawrence K. Saul"], "venue": "JMLR, 10:207\u2013244,", "citeRegEx": "Weinberger and Saul.,? \\Q2009\\E", "shortCiteRegEx": "Weinberger and Saul.", "year": 2009}, {"title": "Distance metric learning with application to clustering with side-information", "author": ["Eric P. Xing", "Andrew Y. Ng", "Michael I. Jordan", "Stuart J. Russell"], "venue": "In NIPS,", "citeRegEx": "Xing et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Xing et al\\.", "year": 2002}, {"title": "Distance metric learning for kernel machines", "author": ["Zhixiang Eddie Xu", "Kilian Q. Weinberger", "Olivier Chapelle"], "venue": "CoRR, abs/1208.3422,", "citeRegEx": "Xu et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Xu et al\\.", "year": 2012}, {"title": "Distance metric learning: a comprehensive survery", "author": ["Liu Yang", "Rong Jin"], "venue": null, "citeRegEx": "Yang and Jin.,? \\Q2006\\E", "shortCiteRegEx": "Yang and Jin.", "year": 2006}, {"title": "Recovering optimal solution by dual random projection", "author": ["Lijun Zhang", "Mehrdad Mah-davi", "Rong Jin", "Tian-Bao Yang", "Shenghuo Zhu"], "venue": "In arXiv:1211.3046,", "citeRegEx": "Zhang et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Zhang et al\\.", "year": 2013}], "referenceMentions": [{"referenceID": 5, "context": "Introduction Distance metric learning (DML) is essential to many machine learning tasks, including ranking (Chechik et al., 2010; Lim et al., 2013), k-nearest neighbor (k-NN) classification (Weinberger and Saul, 2009) and k-means clustering (Xing et al.", "startOffset": 107, "endOffset": 147}, {"referenceID": 14, "context": "Introduction Distance metric learning (DML) is essential to many machine learning tasks, including ranking (Chechik et al., 2010; Lim et al., 2013), k-nearest neighbor (k-NN) classification (Weinberger and Saul, 2009) and k-means clustering (Xing et al.", "startOffset": 107, "endOffset": 147}, {"referenceID": 23, "context": ", 2013), k-nearest neighbor (k-NN) classification (Weinberger and Saul, 2009) and k-means clustering (Xing et al.", "startOffset": 50, "endOffset": 77}, {"referenceID": 24, "context": ", 2013), k-nearest neighbor (k-NN) classification (Weinberger and Saul, 2009) and k-means clustering (Xing et al., 2002).", "startOffset": 101, "endOffset": 120}, {"referenceID": 24, "context": "It finds a good metric by minimizing the distance between data pairs in the same classes and maximizing the distance between data pairs from different classes (Xing et al., 2002; Globerson and Roweis, 2005; Yang and Jin, 2006; Davis et al., 2007; Weinberger and Saul, 2009; Shaw et al., 2011).", "startOffset": 159, "endOffset": 292}, {"referenceID": 9, "context": "It finds a good metric by minimizing the distance between data pairs in the same classes and maximizing the distance between data pairs from different classes (Xing et al., 2002; Globerson and Roweis, 2005; Yang and Jin, 2006; Davis et al., 2007; Weinberger and Saul, 2009; Shaw et al., 2011).", "startOffset": 159, "endOffset": 292}, {"referenceID": 26, "context": "It finds a good metric by minimizing the distance between data pairs in the same classes and maximizing the distance between data pairs from different classes (Xing et al., 2002; Globerson and Roweis, 2005; Yang and Jin, 2006; Davis et al., 2007; Weinberger and Saul, 2009; Shaw et al., 2011).", "startOffset": 159, "endOffset": 292}, {"referenceID": 7, "context": "It finds a good metric by minimizing the distance between data pairs in the same classes and maximizing the distance between data pairs from different classes (Xing et al., 2002; Globerson and Roweis, 2005; Yang and Jin, 2006; Davis et al., 2007; Weinberger and Saul, 2009; Shaw et al., 2011).", "startOffset": 159, "endOffset": 292}, {"referenceID": 23, "context": "It finds a good metric by minimizing the distance between data pairs in the same classes and maximizing the distance between data pairs from different classes (Xing et al., 2002; Globerson and Roweis, 2005; Yang and Jin, 2006; Davis et al., 2007; Weinberger and Saul, 2009; Shaw et al., 2011).", "startOffset": 159, "endOffset": 292}, {"referenceID": 20, "context": "It finds a good metric by minimizing the distance between data pairs in the same classes and maximizing the distance between data pairs from different classes (Xing et al., 2002; Globerson and Roweis, 2005; Yang and Jin, 2006; Davis et al., 2007; Weinberger and Saul, 2009; Shaw et al., 2011).", "startOffset": 159, "endOffset": 292}, {"referenceID": 5, "context": "In a recent study (Chechik et al., 2010), the authors show empirically that it is possible to learn a good distance metric using online learning without having to perform the projection at each iteration.", "startOffset": 18, "endOffset": 40}, {"referenceID": 6, "context": "In (Davis and Dhillon, 2008), the authors assume that the learned metric M is of low rank, and write it as M = LL\u22a4, where L \u2208 Rd\u00d7r with r \u226a d.", "startOffset": 3, "endOffset": 28}, {"referenceID": 23, "context": "A similar idea was studied in (Weinberger and Saul, 2009).", "startOffset": 30, "endOffset": 57}, {"referenceID": 23, "context": "An alternative approach is to reduce the dimensionality of data using dimensionality reduction methods such as principal component analysis (PCA) (Weinberger and Saul, 2009) or random projection (RP) (Tsagkatakis and Savakis, 2010).", "startOffset": 146, "endOffset": 173}, {"referenceID": 22, "context": "An alternative approach is to reduce the dimensionality of data using dimensionality reduction methods such as principal component analysis (PCA) (Weinberger and Saul, 2009) or random projection (RP) (Tsagkatakis and Savakis, 2010).", "startOffset": 200, "endOffset": 231}, {"referenceID": 8, "context": "Although RP is computationally more efficient than PCA, it often yields significantly worse performance than PCA unless the number of random projections is sufficiently large (Fradkin and Madigan, 2003).", "startOffset": 175, "endOffset": 202}, {"referenceID": 18, "context": ", classification (Rahimi and Recht, 2007), clustering (Boutsidis et al.", "startOffset": 17, "endOffset": 41}, {"referenceID": 2, "context": ", classification (Rahimi and Recht, 2007), clustering (Boutsidis et al., 2010) and regression (Maillard and Munos, 2012), only a few studies examined the application of RP to DML, and most of them with limited success.", "startOffset": 54, "endOffset": 78}, {"referenceID": 16, "context": ", 2010) and regression (Maillard and Munos, 2012), only a few studies examined the application of RP to DML, and most of them with limited success.", "startOffset": 23, "endOffset": 49}, {"referenceID": 27, "context": "We finally note that our work is built upon the recent work (Zhang et al., 2013) on random projection where a dual random projection algorithm is developed for linear classification.", "startOffset": 60, "endOffset": 80}, {"referenceID": 27, "context": "Our work differs from (Zhang et al., 2013) in that we apply the theory of dual random projection to DML.", "startOffset": 22, "endOffset": 42}, {"referenceID": 27, "context": "Unlike the theory in (Zhang et al., 2013) where the data matrix is assumed to be low rank or approximately low rank, our new theory of dual random projection is applicable to any", "startOffset": 21, "endOffset": 41}, {"referenceID": 10, "context": "We note that this is different from the algorithms presented in (Hazan and Kale, 2012; Mahdavi et al., 2012).", "startOffset": 64, "endOffset": 108}, {"referenceID": 15, "context": "We note that this is different from the algorithms presented in (Hazan and Kale, 2012; Mahdavi et al., 2012).", "startOffset": 64, "endOffset": 108}, {"referenceID": 24, "context": "Many algorithms have been developed for DML (Xing et al., 2002; Globerson and Roweis, 2005; Davis et al., 2007; Weinberger and Saul, 2009).", "startOffset": 44, "endOffset": 138}, {"referenceID": 9, "context": "Many algorithms have been developed for DML (Xing et al., 2002; Globerson and Roweis, 2005; Davis et al., 2007; Weinberger and Saul, 2009).", "startOffset": 44, "endOffset": 138}, {"referenceID": 7, "context": "Many algorithms have been developed for DML (Xing et al., 2002; Globerson and Roweis, 2005; Davis et al., 2007; Weinberger and Saul, 2009).", "startOffset": 44, "endOffset": 138}, {"referenceID": 23, "context": "Many algorithms have been developed for DML (Xing et al., 2002; Globerson and Roweis, 2005; Davis et al., 2007; Weinberger and Saul, 2009).", "startOffset": 44, "endOffset": 138}, {"referenceID": 9, "context": "Exemplar DML algorithms are MCML (Globerson and Roweis, 2005), ITML (Davis et al.", "startOffset": 33, "endOffset": 61}, {"referenceID": 7, "context": "Exemplar DML algorithms are MCML (Globerson and Roweis, 2005), ITML (Davis et al., 2007), LMNN (Weinberger and Saul, 2009) and OASIS (Chechik et al.", "startOffset": 68, "endOffset": 88}, {"referenceID": 23, "context": ", 2007), LMNN (Weinberger and Saul, 2009) and OASIS (Chechik et al.", "startOffset": 14, "endOffset": 41}, {"referenceID": 5, "context": ", 2007), LMNN (Weinberger and Saul, 2009) and OASIS (Chechik et al., 2010).", "startOffset": 52, "endOffset": 74}, {"referenceID": 11, "context": "Besides algorithms, several studies were devoted to analyzing the generalization performance of DML (Jin et al., 2009; Bellet and Habrard, 2012).", "startOffset": 100, "endOffset": 144}, {"referenceID": 0, "context": "Besides algorithms, several studies were devoted to analyzing the generalization performance of DML (Jin et al., 2009; Bellet and Habrard, 2012).", "startOffset": 100, "endOffset": 144}, {"referenceID": 26, "context": "Survey papers (Yang and Jin, 2006; Kulis, 2013) provide detailed investigation about the topic.", "startOffset": 14, "endOffset": 47}, {"referenceID": 13, "context": "Survey papers (Yang and Jin, 2006; Kulis, 2013) provide detailed investigation about the topic.", "startOffset": 14, "endOffset": 47}, {"referenceID": 6, "context": "Although numerous studies were devoted to DML, only a limited progress is made to address the high dimensional challenge in DML (Davis and Dhillon, 2008; Weinberger and Saul, 2009; Qi et al., 2009; Lim et al., 2013).", "startOffset": 128, "endOffset": 215}, {"referenceID": 23, "context": "Although numerous studies were devoted to DML, only a limited progress is made to address the high dimensional challenge in DML (Davis and Dhillon, 2008; Weinberger and Saul, 2009; Qi et al., 2009; Lim et al., 2013).", "startOffset": 128, "endOffset": 215}, {"referenceID": 17, "context": "Although numerous studies were devoted to DML, only a limited progress is made to address the high dimensional challenge in DML (Davis and Dhillon, 2008; Weinberger and Saul, 2009; Qi et al., 2009; Lim et al., 2013).", "startOffset": 128, "endOffset": 215}, {"referenceID": 14, "context": "Although numerous studies were devoted to DML, only a limited progress is made to address the high dimensional challenge in DML (Davis and Dhillon, 2008; Weinberger and Saul, 2009; Qi et al., 2009; Lim et al., 2013).", "startOffset": 128, "endOffset": 215}, {"referenceID": 6, "context": "In (Davis and Dhillon, 2008; Weinberger and Saul, 2009), the authors address the challenge of high dimensionality by enforcing the distance metric to be a low rank matrix.", "startOffset": 3, "endOffset": 55}, {"referenceID": 23, "context": "In (Davis and Dhillon, 2008; Weinberger and Saul, 2009), the authors address the challenge of high dimensionality by enforcing the distance metric to be a low rank matrix.", "startOffset": 3, "endOffset": 55}, {"referenceID": 17, "context": "(Qi et al., 2009; Lim et al., 2013) alleviate the challenge of learning a distance metric M from high dimensional data by assuming M to be a sparse matrix.", "startOffset": 0, "endOffset": 35}, {"referenceID": 14, "context": "(Qi et al., 2009; Lim et al., 2013) alleviate the challenge of learning a distance metric M from high dimensional data by assuming M to be a sparse matrix.", "startOffset": 0, "endOffset": 35}, {"referenceID": 18, "context": "Random projection is widely used for dimension reduction in various learning tasks (Rahimi and Recht, 2007; Boutsidis et al., 2010; Maillard and Munos, 2012).", "startOffset": 83, "endOffset": 157}, {"referenceID": 2, "context": "Random projection is widely used for dimension reduction in various learning tasks (Rahimi and Recht, 2007; Boutsidis et al., 2010; Maillard and Munos, 2012).", "startOffset": 83, "endOffset": 157}, {"referenceID": 16, "context": "Random projection is widely used for dimension reduction in various learning tasks (Rahimi and Recht, 2007; Boutsidis et al., 2010; Maillard and Munos, 2012).", "startOffset": 83, "endOffset": 157}, {"referenceID": 8, "context": "Unfortunately, it requires a large amount of random projections for the desired result (Fradkin and Madigan, 2003), and this limits its application in DML, where the computational cost is proportion to the square of dimensions.", "startOffset": 87, "endOffset": 114}, {"referenceID": 27, "context": "Dual random projection is first introduced for linear classification task (Zhang et al., 2013) and following aspects make our work significantly different from the initial study (Zhang et al.", "startOffset": 74, "endOffset": 94}, {"referenceID": 27, "context": ", 2013) and following aspects make our work significantly different from the initial study (Zhang et al., 2013): First, we apply dual random projection for DML, where the number of variables is quadratic to the dimension and the dimension crisis is more serious than linear classifier.", "startOffset": 91, "endOffset": 111}, {"referenceID": 27, "context": "Last, we give the theoretical guarantee when the dataset is not low rank, which is an important assumption for the study (Zhang et al., 2013).", "startOffset": 121, "endOffset": 141}, {"referenceID": 23, "context": "This is because several previous studies have suggested that triplet constraints are more effective than pairwise constraints (Weinberger and Saul, 2009; Chechik et al., 2010; Shaw et al., 2011).", "startOffset": 126, "endOffset": 194}, {"referenceID": 5, "context": "This is because several previous studies have suggested that triplet constraints are more effective than pairwise constraints (Weinberger and Saul, 2009; Chechik et al., 2010; Shaw et al., 2011).", "startOffset": 126, "endOffset": 194}, {"referenceID": 20, "context": "This is because several previous studies have suggested that triplet constraints are more effective than pairwise constraints (Weinberger and Saul, 2009; Chechik et al., 2010; Shaw et al., 2011).", "startOffset": 126, "endOffset": 194}, {"referenceID": 5, "context": "We note that we did not enforce M in (1) to be PSD because we follow the one-projection paradigm proposed in (Chechik et al., 2010) that first learns a symmetric matrix M by solving the optimization problem in (1) and then projects the learned matrix M onto the PSD cone.", "startOffset": 109, "endOffset": 131}, {"referenceID": 27, "context": "We emphasize that unlike (Zhang et al., 2013), we did not assume l(\u00b7) to be smooth, making it possible to apply the proposed approach to the hinge loss.", "startOffset": 25, "endOffset": 45}, {"referenceID": 5, "context": "Following one-projection paradigm (Chechik et al., 2010), we project the learned symmetric matrix M onto the PSD cone at the end of the algorithm.", "startOffset": 34, "endOffset": 56}, {"referenceID": 19, "context": "We choose stochastic dual coordinate ascent (SDCA) method for solving the dual problem (5) because it enjoys a linear convergence when the loss function is smooth, and is shown empirically to be significantly faster than the other stochastic optimization methods (Shalev-Shwartz and Zhang, 2012).", "startOffset": 263, "endOffset": 295}, {"referenceID": 19, "context": "We use the combination strategy recommended in (Shalev-Shwartz and Zhang, 2012), denoted by CSDCA, which uses SGD for the first epoch and then applies SDCA for the rest epochs.", "startOffset": 47, "endOffset": 79}, {"referenceID": 27, "context": "2 Main Theoretical Results First, similar to (Zhang et al., 2013), we consider the case when the data matrix X is of low rank.", "startOffset": 45, "endOffset": 65}, {"referenceID": 27, "context": "It is important to note that our analysis, unlike (Zhang et al., 2013), can be applied to non-smooth loss such as the hinge loss.", "startOffset": 50, "endOffset": 70}, {"referenceID": 5, "context": ", 2007) and we use the version pre-processed by (Chechik et al., 2010).", "startOffset": 48, "endOffset": 70}, {"referenceID": 3, "context": "tdt30 is a subset of tdt2 dataset (Cai et al., 2009).", "startOffset": 34, "endOffset": 52}, {"referenceID": 4, "context": "All the other datasets are downloaded from LIBSVM (Chang and Lin, 2011), where rcv30 is a subset of the original dataset consisted of documents from the 30 most popular categories.", "startOffset": 50, "endOffset": 71}, {"referenceID": 5, "context": "Since it is expensive to compute and maintain a matrix of 50, 000 \u00d7 50, 000, for these three datasets, we follow the procedure in (Chechik et al., 2010) that maps all documents to a space of 1, 000 dimension.", "startOffset": 130, "endOffset": 152}, {"referenceID": 5, "context": "First, we follow the evaluation protocol in (Chechik et al., 2010) and evaluate the learned metric by its ranking performance.", "startOffset": 44, "endOffset": 66}, {"referenceID": 5, "context": "\u2022 OASIS (Chechik et al., 2010): A state-of-art online learning algorithm for DML that learns the optimal distance metric directly from the original space without any dimensionality reduction.", "startOffset": 8, "endOffset": 30}, {"referenceID": 23, "context": "\u2022 LMNN (Weinberger and Saul, 2009): A state-of-art batch learning algorithm for DML.", "startOffset": 7, "endOffset": 34}, {"referenceID": 19, "context": ", DuOri, DuRP, SRP, SPCA and OASIS), which yields sufficiently accurate solutions in our experiments and is also consistent with the observation in (Shalev-Shwartz and Zhang, 2012).", "startOffset": 148, "endOffset": 180}, {"referenceID": 19, "context": "The step size of CSDCA is set according to the analysis in (Shalev-Shwartz and Zhang, 2012).", "startOffset": 59, "endOffset": 91}, {"referenceID": 23, "context": ", (Weinberger and Saul, 2009; Xu et al., 2012)) have shown that metric learning do not yield better classification accuracy than the standard classification algorithms (e.", "startOffset": 2, "endOffset": 46}, {"referenceID": 25, "context": ", (Weinberger and Saul, 2009; Xu et al., 2012)) have shown that metric learning do not yield better classification accuracy than the standard classification algorithms (e.", "startOffset": 2, "endOffset": 46}, {"referenceID": 27, "context": "Corollary 3 (Zhang et al., 2013) Let S \u2208 Rr\u00d7m be a standard Gaussian random matrix.", "startOffset": 12, "endOffset": 32}, {"referenceID": 21, "context": "Combining the inequalities in (11) and (12), we have 1 \u03bbN (\u03b1\u0302\u2212\u03b1\u2217)(G\u2212 \u011c)\u03b1\u2217 \u2265 1 \u03bbN (\u03b1\u0302\u2212\u03b1\u2217)\u011c(\u03b1\u0302\u2212\u03b1\u2217) or (\u03b1\u2217 \u2212 \u03b1\u0302\u2217)(\u011c\u2212G)\u03b1\u2217 \u2265 (\u03b1\u0302\u2217 \u2212\u03b1\u2217)G(\u03b1\u0302\u2217 \u2212\u03b1\u2217) + (\u03b1\u0302\u2217 \u2212\u03b1\u2217)(\u011c\u2212G)(\u03b1\u0302\u2217 \u2212\u03b1\u2217) Define p\u2217 = Z\u0303\u03b1\u2217, p\u0302\u2217 = Z\u0303\u03b1\u0302\u2217, we have: (p\u2217 \u2212 p\u0302\u2217)\u0393p\u2217 \u2265 \u2016p\u0302\u2217 \u2212 p\u2217\u20162 + (p\u0302\u2217 \u2212 p\u2217)\u0393(p\u0302\u2217 \u2212 p\u2217) Using the bound given in (8), with a probability 1\u2212 \u03b4, we have \u2016p\u0302\u2217 \u2212 p\u2217\u20162 \u2264 3\u03b5 1\u2212 3\u03b5\u2016p\u2217\u20162 We complete the proof by using the fact \u2016M\u2217 \u2212 M\u0302\u2217\u2016F = 1 \u03bbN \u2016p\u2217 \u2212 p\u0302\u2217\u20162, \u2016M\u2217\u2016F = 1 \u03bbN \u2016p\u2217\u20162 and (Stark et al., 1998) \u2016\u03a0PSD(M\u2217)\u2212\u03a0PSD(M\u0302\u2217)\u2016F \u2264 \u2016M\u2217 \u2212 M\u0302\u2217\u2016F", "startOffset": 444, "endOffset": 464}, {"referenceID": 1, "context": "Theorem 4 (Theorem 2 (Blum, 2005)) Let x \u2208 Rd, and x\u0302 = R\u22a4x/\u221am, where R \u2208 Rd\u00d7m is a random matrix whose entries are chosen independently from N (0, 1).", "startOffset": 21, "endOffset": 33}, {"referenceID": 12, "context": "Theorem 5 (Lemma B-1 (Karoui, 2010)) Suppose M is a real symmetric matrix with non-negative entries, and E is a real symmetric matrix such that maxi,j |Ei,j| \u2264 \u03be.", "startOffset": 21, "endOffset": 35}], "year": 2015, "abstractText": "In this work, we study distance metric learning (DML) for high dimensional data. A typical approach for DML with high dimensional data is to perform the dimensionality reduction first before learning the distance metric. The main shortcoming of this approach is that it may result in a suboptimal solution due to the subspace removed by the dimensionality reduction method. In this work, we present a dual random projection frame for DML with high dimensional data that explicitly addresses the limitation of dimensionality reduction for DML. The key idea is to first project all the data points into a low dimensional space by random projection, and compute the dual variables using the projected vectors. It then reconstructs the distance metric in the original space using the estimated dual variables. The proposed method, on one hand, enjoys the light computation of random projection, and on the other hand, alleviates the limitation of most dimensionality reduction methods. We verify both empirically and theoretically the effectiveness of the proposed algorithm for high dimensional DML.", "creator": "dvips(k) 5.991 Copyright 2011 Radical Eye Software"}}}