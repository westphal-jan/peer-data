{"id": "1509.04473", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "15-Sep-2015", "title": "Splitting Compounds by Semantic Analogy", "abstract": "Compounding is a highly productive word-formation process in some languages that is often problematic for natural language processing applications. In this paper, we investigate whether distributional semantics in the form of word embeddings can enable a deeper, i.e., more knowledge-rich, processing of compounds than the standard string-based methods. We present an unsupervised approach that exploits regularities in the semantic vector space (based on analogies such as \"bookshop is to shop as bookshelf is to shelf\") to produce compound analyses of high quality. A subsequent compound splitting algorithm based on these analyses is highly effective, particularly for ambiguous compounds. German to English machine translation experiments show that this semantic analogy-based compound splitter leads to better translations than a commonly used frequency-based method.", "histories": [["v1", "Tue, 15 Sep 2015 10:03:35 GMT  (495kb,D)", "http://arxiv.org/abs/1509.04473v1", null]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["joachim daiber", "lautaro quiroz", "roger wechsler", "stella frank"], "accepted": false, "id": "1509.04473"}, "pdf": {"name": "1509.04473.pdf", "metadata": {"source": "CRF", "title": "Splitting Compounds by Semantic Analogy", "authors": ["Joachim Daiber", "Lautaro Quiroz", "Roger Wechsler", "Stella Frank"], "emails": ["J.Daiber@uva.nl", "S.C.Frank@uva.nl", "first.last@student.uva.nl"], "sections": [{"heading": "1 Introduction", "text": "Composite words are a common occurrence in languages such as German, leading to difficulties in processing natural language, especially machine translation, and several methods for dealing with this problem - from flat counting methods to deeper but more complex neural processing methods - have been suggested. In this paper, we investigate whether similar methods can be applied to perform deeper, i.e. more knowledgeable, processing of connections, and a major advantage of word embedding is the regularities that indicate their multidimensional vector spaces. Mikolov et al showed that regularities such as \"King is for man what Queen is for woman\" can be expressed in the form of basic linear algebra operations."}, {"heading": "2 Related work", "text": "Our methodology follows recent work on morphology induction (Soricut and Och, 2015), which combines string edits with distribution semantics to split words into morphemes. In this model, morphemes are represented as string edits plus vectors and linked to derivative diagrams. Authors consider morphemes with prefix and suffix lengths up to six characters, while our approach to noun-compound splitting only takes into account components with at least four characters in length."}, {"heading": "2.1 Splitting compounds for SMT", "text": "Dealing with word composition in statistical machine translation (SMT) is essential to mitigate the sparse data problems that productive word generation causes. There are several problems that need to be addressed: splitting composite words into their correct components (i.e. the ambiguity between splitting points), deciding whether a composite word should be split at all, and, when translated into a composite language, merging components into a composite word (something we do not address, but see Fraser et al. (2012) and Cap et al. (2014) for systems that do so. Koehn and Knight (2003) address German splitting using a simple approach based on the frequency of components (something we do not address, but see Fraser et al. (2012) and Cap et al. (2014) for systems that do so)."}, {"heading": "2.2 Semantic compositionality", "text": "Reddy et al. (2011) examined English noun combinations and found that distributional relationships can capture the relationship between compound parts and the whole as judged by humans as \"letters.\" Schulte im Walde et al. (2013) replicate this result for German and also show that simple window-based distribution vectors surpass syntax-based vectors. 1In vector algebra: \u2191 dMain target = v (main target) \u2212 v (main target) and v (main vein) \u2248 v (main vein) + \u2191 dMain target. The combinations translate into main target (main target) and main artery (main vein). As a standalone noun, main means head."}, {"heading": "3 Towards deeper processing of compound words", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "3.1 Unsupervised morphology induction from word embeddings", "text": "Our approach is based on the work of Soricut and Och (2015), which exploit vector space regularities to induce morphological transformations. Authors extract morphological transformations in the form of prefixes and suffix replacement rules up to a maximum length of 6 characters. The method requires an initial candidate set that contains all possible prefix and suffix rules that occur in the monolingual corpus. For English, the candidate set contains rules such as Suffix: ed: ing, which replaces the suffix with (e.g. went \u2192 ging).This candidate set also contains overgenerated rules that do not reflect actual morphological transformations; for example, Prefix: S: 2 in Scream \u2192 cream.The goal is to filter the initial candidate vector vector vector to remove false rules while adhering to useful rules. A rule applies to all word pairings, word entries are used to vector vectors that represent the vector, for example, the vector vector that represents the directive vector of the vector vector."}, {"heading": "3.2 Compound words and the semantic vector space", "text": "In this paper we focus on endocentric compounds, which are also the most common type in Germany. Endocentric compounds consist of a modifier and a semantic head. The semantic head is a modifier and a semantic head. The semantic head specializes in the basic meaning of the word and the modifier."}, {"heading": "4 Compound induction from word embeddings", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.1 Compound extraction", "text": "We retain a modifier as a candidate when both the modifier and the rest of the word, i.e. the potential header of the connection, appear in the vocabulary (i.e., the set of all connections to which the modifier applies) is 13.5 words. Table 1a shows the ten modifiers of the candidates with the largest support rates. At this time, the candidate set contains all modifier header splitters that can be observed in the data, including candidates that do not reflect real compound splits. 7 compound splits are not applied recursively here, as we assume that internal splitters from the appearance of the heads as individual words.8Prototype To find the composite we support the composite the validators."}, {"heading": "4.2 Implementation considerations", "text": "It is about the question of whether it is about a way and a way, in which it is about a way and a way, in which it is about a way and a way, in which it is about a way and a way, in which it is about a way and a way, in which it is about a way and a way, in which it is about a way and a way, in which it is about a way and a way, in which it is about a way and a way, in which it is about a way and a way, in which it is about a way and a way, in which it is about a way and a way, in which it is about a way and a way, in which it is about a way and a way, in which it is about a way and a way, in which it is about a way and a way, in which it is about a way and a way, in which it is about a way and a way, in which it is about a way, in which it is about a way and a way in which it is about a way and a way in which it is about a way and a way in which it is about a way and a way in which it is about a way and a way in which it is about a way and a way in which it is about a way and a way in which it is about a way and a way in which it is about a way and a way in which it is about a way and a way in which it is about a way and a way in which it is about a way and a way in which it is about a way and a way in which it is about a way and a way in which it is about a way and a way and a way it is about a way and a way in which it is about a way and a way in which it is about a way and a way and a way it is about a way and a way and a way in which it is about a way and a way in which it is about a way and a way and a way it is about a way and a way and a way in which it is about a way and a way in which it is about a way and a way in which it is about a way and a way and a way in which it is about a way in which it is about a way and a way and a way and a way it is about a way and a way and a way and a way"}, {"heading": "4.3 Compound splitting", "text": "This year it is more than ever before."}, {"heading": "5 Compound splitting for machine translation", "text": "In fact, we will be able to leave the country to save the world, \"he said."}, {"heading": "6 Conclusion", "text": "In this paper, we investigated whether regularities in the semantic word embedding space can be used to model the composition of composite words based on analogy. To get closer to this question, we wrote the following papers: First, we investigated whether properties of compounds can be found in semantic vector space. We found that this space is suitable for modelling compounds based on their semantic head. On this basis, we discussed how to extract compound transformations and prototypes using the Soricut and Och (2015) method, and proposed an algorithm for applying these structures to compound splitting. Our experiments show that the analogy-based compound splitter outperforms a commonly used compound splitter in a gold standard task. Our novel compound splitter is particularly adept at splitting highly ambiguous compounds. Finally, we applied the analogy-based compound splitter to a common compound splitter used in a translation machine, and found that it is common in a small number of compounds."}], "references": [{"title": "How to produce unseen teddy bears: Improved morphological processing of compounds in SMT", "author": ["Cap et al.2014] Fabienne Cap", "Alexander Fraser", "Marion Weller", "Aoife Cahill"], "venue": "In Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics (EACL)", "citeRegEx": "Cap et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Cap et al\\.", "year": 2014}, {"title": "Meteor universal: Language specific translation evaluation for any target language", "author": ["Denkowski", "Lavie2014] Michael Denkowski", "Alon Lavie"], "venue": "In Proceedings of the Ninth Workshop on Statistical Machine Translation", "citeRegEx": "Denkowski et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Denkowski et al\\.", "year": 2014}, {"title": "IRSTLM: An open source toolkit for handling large scale language models", "author": ["Nicola Bertoldi", "Mauro Cettolo"], "venue": "In Proceedings of Interspeech 2008 - 9th Annual Conference of the International Speech Communication Association", "citeRegEx": "Federico et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Federico et al\\.", "year": 2008}, {"title": "Modeling inflection and word-formation in SMT", "author": ["Marion Weller", "Aoife Cahill", "Fabienne Cap"], "venue": "In Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics (EACL)", "citeRegEx": "Fraser et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Fraser et al\\.", "year": 2012}, {"title": "How to avoid burning ducks: Combining linguistic analysis and corpus statistics for German compound processing", "author": ["Fritzinger", "Fraser2010] Fabienne Fritzinger", "Alexander Fraser"], "venue": "In Proceedings of the ACL 2010 Joint Fifth Workshop on Statistical Machine Translation and Metrics (MATR)", "citeRegEx": "Fritzinger et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Fritzinger et al\\.", "year": 2010}, {"title": "Determining immediate constituents of compounds in GermaNet", "author": ["Henrich", "Hinrichs2011] Verena Henrich", "Erhard W. Hinrichs"], "venue": "In Proceedings of the International Conference on Recent Advances in Natural Language Processing", "citeRegEx": "Henrich et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Henrich et al\\.", "year": 2011}, {"title": "Empirical methods for compound splitting", "author": ["Koehn", "Knight2003] Philipp Koehn", "Kevin Knight"], "venue": "In Proceedings of the 10th Conference of the European Chapter of the Association for Computational Linguistics (EACL)", "citeRegEx": "Koehn et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Koehn et al\\.", "year": 2003}, {"title": "Moses: Open source toolkit for statistical machine translation", "author": ["Koehn et al.2007] Philipp Koehn", "Hieu Hoang", "Alexandra Birch", "Chris Callison-Burch", "Marcello Federico", "Nicola Bertoldi", "Brooke Cowan", "Wade Shen", "Christine Moran", "Richard Zens", "Chris Dyer", "Ondrej Bojar", "Alexandra Constantin", "Evan Herbst"], "venue": "In Proceedings of the 45th Annual Meeting of the Association for Computational Linguistics (ACL)", "citeRegEx": "Koehn et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Koehn et al\\.", "year": 2007}, {"title": "Statistical significance tests for machine translation evaluation", "author": ["Philipp Koehn"], "venue": "In Proceedings of the 9th Conference on Empirical Methods in Natural Language Processing (EMNLP)", "citeRegEx": "Koehn.,? \\Q2004\\E", "shortCiteRegEx": "Koehn.", "year": 2004}, {"title": "Linguistic regularities in continuous space word representations", "author": ["Wen-tau Yih", "Geoffrey Zweig"], "venue": "In Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics (NAACL)", "citeRegEx": "Mikolov et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Improving SMT quality with morpho-syntactic analysis", "author": ["Nie\u00dfen", "Ney2000] Sonja Nie\u00dfen", "Hermann Ney"], "venue": "In Proceedings of the 18th International Conference on Computational Linguistics (COLING)", "citeRegEx": "Nie\u00dfen et al\\.,? \\Q2000\\E", "shortCiteRegEx": "Nie\u00dfen et al\\.", "year": 2000}, {"title": "A systematic comparison of various statistical alignment models", "author": ["Och", "Ney2003] Franz Josef Och", "Hermann Ney"], "venue": "Computational Linguistics,", "citeRegEx": "Och et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Och et al\\.", "year": 2003}, {"title": "BLEU: A method for automatic evaluation of machine translation", "author": ["Salim Roukos", "Todd Ward", "Wei-Jing Zhu"], "venue": "In Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics (ACL)", "citeRegEx": "Papineni et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Papineni et al\\.", "year": 2002}, {"title": "Statistical machine translation of German compound words", "author": ["Popovi\u0107 et al.2006] Maja Popovi\u0107", "Daniel Stein", "Hermann Ney"], "venue": "In Proceedings of FinTal - 5th International Conference on Natural Language Processing", "citeRegEx": "Popovi\u0107 et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Popovi\u0107 et al\\.", "year": 2006}, {"title": "An empirical study on compositionality in compound nouns", "author": ["Reddy et al.2011] Siva Reddy", "Diana McCarthy", "Suresh Manandhar"], "venue": "In Proceedings of the 5th International Joint Conference on Natural Language Processing", "citeRegEx": "Reddy et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Reddy et al\\.", "year": 2011}, {"title": "Exploring vector space models to predict the compositionality of German noun-noun compounds", "author": ["Stefan M\u00fcller", "Stephen Roller"], "venue": "In Proceedings of the 2nd Joint Conference on Lexical and Computational Semantics (*SEM)", "citeRegEx": "Walde et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Walde et al\\.", "year": 2013}, {"title": "Unsupervised morphology induction using word embeddings", "author": ["Soricut", "Och2015] Radu Soricut", "Franz Och"], "venue": "In Proceedings of the 2015 Conference of the North American Chapter of the Association for Computational Linguistics (NAACL)", "citeRegEx": "Soricut et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Soricut et al\\.", "year": 2015}, {"title": "Distinguishing degrees of compositionality in compound splitting for statistical machine translation", "author": ["Weller et al.2014] Marion Weller", "Fabienne Cap", "Stefan M\u00fcller", "Sabine Schulte im Walde", "Alexander Fraser"], "venue": "In Proceedings of the First Workshop on Computational Approaches", "citeRegEx": "Weller et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Weller et al\\.", "year": 2014}], "referenceMentions": [{"referenceID": 9, "context": "Mikolov et al. (2013) showed that regularities such as \u201cking is to man what queen is to woman\u201d can be expressed and exploited in the form of basic linear algebra operations on the vectors produced by their method.", "startOffset": 0, "endOffset": 22}, {"referenceID": 9, "context": "Mikolov et al. (2013) showed that regularities such as \u201cking is to man what queen is to woman\u201d can be expressed and exploited in the form of basic linear algebra operations on the vectors produced by their method. This often-cited example can be expressed as follows: v(king)\u2212 v(man) + v(woman) \u2248 v(queen), where v(.) maps a word into its word embedding in vector space. In a very recent approach, Soricut and Och (2015) exploit these regularities for unsupervised morphology induction.", "startOffset": 0, "endOffset": 421}, {"referenceID": 2, "context": "disambiguating between split points), deciding whether to split a compound word at all, and, if translating into a compounding language, merging components into a compound word (something we do not address, but see Fraser et al. (2012) and Cap et al.", "startOffset": 215, "endOffset": 236}, {"referenceID": 0, "context": "(2012) and Cap et al. (2014) for systems that do).", "startOffset": 11, "endOffset": 29}, {"referenceID": 0, "context": "(2012) and Cap et al. (2014) for systems that do). Koehn and Knight (2003) address German compound splitting using a straightforward approach based on component frequency.", "startOffset": 11, "endOffset": 75}, {"referenceID": 0, "context": "(2012) and Cap et al. (2014) for systems that do). Koehn and Knight (2003) address German compound splitting using a straightforward approach based on component frequency. They also present splitting approaches based on word alignments and POS tag information, but find that while the more resourceintensive approaches give better splitting performance (measured by gold-standard segmentations) the frequency-based method results in the best SMT performance (measured by BLEU). This is attributed to the fact that phrase-based MT system do not penalize the frequency-based method for over-splitting, since it can handle components as a phrase. Nie\u00dfen and Ney (2000), Popovi\u0107 et al.", "startOffset": 11, "endOffset": 666}, {"referenceID": 0, "context": "(2012) and Cap et al. (2014) for systems that do). Koehn and Knight (2003) address German compound splitting using a straightforward approach based on component frequency. They also present splitting approaches based on word alignments and POS tag information, but find that while the more resourceintensive approaches give better splitting performance (measured by gold-standard segmentations) the frequency-based method results in the best SMT performance (measured by BLEU). This is attributed to the fact that phrase-based MT system do not penalize the frequency-based method for over-splitting, since it can handle components as a phrase. Nie\u00dfen and Ney (2000), Popovi\u0107 et al. (2006) and Fritzinger and Fraser (2010) explore using morphological analyzers for German compound splitting, with mixed results.", "startOffset": 11, "endOffset": 689}, {"referenceID": 0, "context": "(2012) and Cap et al. (2014) for systems that do). Koehn and Knight (2003) address German compound splitting using a straightforward approach based on component frequency. They also present splitting approaches based on word alignments and POS tag information, but find that while the more resourceintensive approaches give better splitting performance (measured by gold-standard segmentations) the frequency-based method results in the best SMT performance (measured by BLEU). This is attributed to the fact that phrase-based MT system do not penalize the frequency-based method for over-splitting, since it can handle components as a phrase. Nie\u00dfen and Ney (2000), Popovi\u0107 et al. (2006) and Fritzinger and Fraser (2010) explore using morphological analyzers for German compound splitting, with mixed results.", "startOffset": 11, "endOffset": 722}, {"referenceID": 0, "context": "(2012) and Cap et al. (2014) for systems that do). Koehn and Knight (2003) address German compound splitting using a straightforward approach based on component frequency. They also present splitting approaches based on word alignments and POS tag information, but find that while the more resourceintensive approaches give better splitting performance (measured by gold-standard segmentations) the frequency-based method results in the best SMT performance (measured by BLEU). This is attributed to the fact that phrase-based MT system do not penalize the frequency-based method for over-splitting, since it can handle components as a phrase. Nie\u00dfen and Ney (2000), Popovi\u0107 et al. (2006) and Fritzinger and Fraser (2010) explore using morphological analyzers for German compound splitting, with mixed results. Since these approaches use heavy supervision within the morphological analyzer, they are orthogonal to our unsupervised approach. It may be advantageous to split only compositional compounds, and leave lexicalized compounds whole. Weller et al. (2014) investigate this question by using distributional similarity to split only words that pass a certain threshold (i.", "startOffset": 11, "endOffset": 1063}, {"referenceID": 14, "context": "Reddy et al. (2011) examine English noun compounds and find that distributional co-occurrence can capture the relationship between compound parts and whole, as judged by humans in terms of \u2018literalness\u2019.", "startOffset": 0, "endOffset": 20}, {"referenceID": 14, "context": "Reddy et al. (2011) examine English noun compounds and find that distributional co-occurrence can capture the relationship between compound parts and whole, as judged by humans in terms of \u2018literalness\u2019. Schulte im Walde et al. (2013) replicate this result for German, and also show that simple window-based distributional vectors outperform syntax-based vectors.", "startOffset": 0, "endOffset": 235}, {"referenceID": 7, "context": "Moses (Koehn et al., 2007) offers a compound splitter that splits a word if the geometric average of the frequencies of its components is higher than the frequency of the compound.", "startOffset": 6, "endOffset": 26}, {"referenceID": 7, "context": "Translation setup We use the Moses decoder (Koehn et al., 2007) to train a phrase-based MT system on the English\u2013German Common crawl parallel corpus and WMT news test 2010 (tuning).", "startOffset": 43, "endOffset": 63}, {"referenceID": 2, "context": "We use a 3rd order language model estimated using IRSTLM (Federico et al., 2008), as well as lexicalized reordering.", "startOffset": 57, "endOffset": 80}, {"referenceID": 12, "context": "For each experiment, we report BLEU (Papineni et al., 2002), METEOR (Denkowski and Lavie, 2014), and the number of compound splits performed on the test set.", "startOffset": 36, "endOffset": 59}, {"referenceID": 8, "context": "Statistical significance tests are performed using bootstrap resampling (Koehn, 2004).", "startOffset": 72, "endOffset": 85}], "year": 2015, "abstractText": "Compounding is a highly productive word-formation process in some languages that is often problematic for natural language processing applications. In this paper, we investigate whether distributional semantics in the form of word embeddings can enable a deeper, i.e., more knowledge-rich, processing of compounds than the standard string-based methods. We present an unsupervised approach that exploits regularities in the semantic vector space (based on analogies such as \u201cbookshop is to shop as bookshelf is to shelf\u201d) to produce compound analyses of high quality. A subsequent compound splitting algorithm based on these analyses is highly effective, particularly for ambiguous compounds. German to English machine translation experiments show that this semantic analogy-based compound splitter leads to better translations than a commonly used frequency-based method.", "creator": "LaTeX with hyperref package"}}}