{"id": "1509.04581", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "15-Sep-2015", "title": "Kernelized Deep Convolutional Neural Network for Describing Complex Images", "abstract": "With the impressive capability to capture visual content, deep convolutional neural networks (CNN) have demon- strated promising performance in various vision-based ap- plications, such as classification, recognition, and objec- t detection. However, due to the intrinsic structure design of CNN, for images with complex content, it achieves lim- ited capability on invariance to translation, rotation, and re-sizing changes, which is strongly emphasized in the s- cenario of content-based image retrieval. In this paper, to address this problem, we proposed a new kernelized deep convolutional neural network. We first discuss our motiva- tion by an experimental study to demonstrate the sensitivi- ty of the global CNN feature to the basic geometric trans- formations. Then, we propose to represent visual content with approximate invariance to the above geometric trans- formations from a kernelized perspective. We extract CNN features on the detected object-like patches and aggregate these patch-level CNN features to form a vectorial repre- sentation with the Fisher vector model. The effectiveness of our proposed algorithm is demonstrated on image search application with three benchmark datasets.", "histories": [["v1", "Tue, 15 Sep 2015 14:35:11 GMT  (3687kb,D)", "http://arxiv.org/abs/1509.04581v1", "9 pages"]], "COMMENTS": "9 pages", "reviews": [], "SUBJECTS": "cs.CV cs.AI cs.IR cs.MM", "authors": ["zhen liu"], "accepted": false, "id": "1509.04581"}, "pdf": {"name": "1509.04581.pdf", "metadata": {"source": "CRF", "title": "Kernelized Deep Convolutional Neural Network for Describing Complex Images", "authors": ["Zhen Liu"], "emails": ["liuzheng@mail.ustc.edu.cn"], "sections": [{"heading": "1. Introduction", "text": "In many visual analysis systems, the visual content in an image is usually represented in a fixed vector 32 to ensure the convenience of subsequent processing. In recent years, much effort has been put into first designing the visual characteristics of CNN and second, aggregating the visual characteristics into a single vector [38, 39, 32, 20, 22].The visual characteristics of the visual terminology (BoVW) model is one of the famous methods of constructing the visual characteristics of CNN. In the BoVW model, a number of local invariant visual characteristics are first worked out based on the recognized image patches or densely sampled visual terms. Then, an image is presented in a visual word histogram based on the quantization results of local characteristics with an off-line trained visual vocabulary."}, {"heading": "2. Sensitivity of Global CNN Feature", "text": "In this section, we will examine the sensitivity of CNN's global function to geometric transformations, i.e., translation, scaling and rotation in detail. The study is based on Holidays data [20], which is a benchmark dataset for image search with 1491 high-resolution images. We will use the Caffe-based CNN implementation [23] to extract our CNN function. Below, we will show the effects of geometric transformations on CNN's global function independently to ensure that each image undergoes only one type of geometric transformation. Generally, a translation can be made in vertical and horizontal directions. To simplify the study, we will consider only the general transformation in direction."}, {"heading": "3. Kernelized Convolutional Neural Network", "text": "In this section we introduce our algorithm to construct the vectorial representations on the rough content-oriented level. < p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p >"}, {"heading": "4. Experimental Results", "text": "In this section, we evaluate our algorithm on the image repetition application. We use three publicly available benchmark datasets, i.e. Holidays [20] and UKBench [29] and Oxford Building [3], to demonstrate the effects of the parameters in our algorithm. We also compare our algorithm with some other methods for the image repetition application. Holidays dataset [20] contains 1491 high-resolution images of different scenes and objects with 500 queries. To evaluate the performance, we use the average precision measurement calculated as an area below the precision curve for a query. We calculate the mean value of the average precision for all queries to obtain an average precision (mAP), which is used to evaluate the overall performance [34]. UKBench dataset [29] contains 2550 objects or scenes, each with four images recorded under different views or image conditions, resulting in a total query of 200 images [29]."}, {"heading": "4.1. Impact of Parameters", "text": "In this section we will examine the effects of parameters. There are three parameters in our algorithm. The first is the number of image fields x detected by BING detector [8], which can be designated by N. The second is the dimension of the vector representation of image fields, \u03b3 (x). We adopt the CNN model to construct the vector representation of x, which leads to a 4096 D graph (x) [23]. The last parameter is the visual representation of x used in Fisher vector [32], which corresponds to the \u03b2 (\u00b7) in Eq. 6, which can be cited by V. The results are shown in Fig. 6. It can be seen that better accuracy is achieved when more image fields (greater N) are used."}, {"heading": "4.2. Comparisons", "text": "In this section we give some comparisons with the results of other research papers. As shown in Table 3, you can see that the proposed KCNN method achieves the best result on both holiday and UKBench records. However, at the Oxford Building record, SIFT-based methods can achieve a better result, namely [32] [3] [22]. The reason for this is that the Oxford Building record consists of image data and the retrieval on this record more resembles a fine-grained problem [28]. On the other hand, a deep revolutionary novel network is designed for constructing the general classification problem [11, 24] and fine-tuning is required for the fine-grained visual tasks. There are also some work on performing image searches with CNN. However, our work shows significant differences to them. Compared with [36], our goal is completely different [36] to construct a complicatory representation for a sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub-sub"}, {"heading": "5. Conclusion", "text": "In this article, we analyzed the sensitivity of CNN's global feature to geometric transformations of images such as translation, scaling, and rotation. Based on our analysis, inspired by the well-studied methods of displaying local feature base images, we proposed our Kernelized Convolutional Network (KCNN) algorithm to describe the content of complex images. Our KCNN method provides a more robust vector representation. In addition to the CNN structure implemented in the Caffe library, there are several other emerging CNN structures [37, 16]. In the future, we would like to investigate the potential of these different CNN models in image recovery and examine the performance of our KCNN model, which is integrated with these CNN models."}], "references": [{"title": "What is an object", "author": ["B. Alexe", "T. Deselaers", "V. Ferrari"], "venue": "In Proceedings of IEEE International Conference on Computer Vision and Patteren Recognation,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2010}, {"title": "Measuring the objectness of image windows", "author": ["B. Alexe", "T. Deselaers", "V. Ferrari"], "venue": "Proceedings of IEEE Trans-  actions on Pattern Analysis and Machine Intelligence, volume 34, pages 2189\u20132202,", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2012}, {"title": "All about vlad", "author": ["R. Arandjelovic", "A. Zisserman"], "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2013}, {"title": "Neural codes for image retrieval", "author": ["A. Babenko", "A. Slesarev", "A. Chigorin", "V. Lempitsky"], "venue": "Proceedings of European Conference on Computer Vision, pages 584\u2013599.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2014}, {"title": "Surf: Speeded up robust features", "author": ["H. Bay", "T. Tuytelaars", "L. Van Gool"], "venue": "Proceedings of the European Conference on Computer Vision, pages 404\u2013417,", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2006}, {"title": "Object recognition with hierarchical kernel descriptors", "author": ["L. Bo", "K. Lai", "X. Ren", "D. Fox"], "venue": "Proceedings of IEEE International Conference on Computer Vision and Pattern Recognition, June", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2011}, {"title": "Efficient Match Kernel between Sets of Features for Visual Recognition", "author": ["L. Bo", "C. Sminchisescu"], "venue": "Proceedings of Advances in Neural Information Processing Systems, December", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2009}, {"title": "BING: Binarized normed gradients for objectness estimation at 300fps", "author": ["M.-M. Cheng", "Z. Zhang", "W.-Y. Lin", "P.H.S. Torr"], "venue": "Proceedings of IEEE International Conference on Computer Vision and Pattern Recognition,", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2014}, {"title": "Deep filter banks for texture recognition and segmentation", "author": ["M. Cimpoi", "S. Maji", "A. Vedaldi"], "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 3828\u20133836,", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2015}, {"title": "Histograms of oriented gradients for human detection", "author": ["N. Dalal", "B. Triggs"], "venue": "Proceedings of IEEE Conference on Computer Vision and Pattern Recognition, volume 1, pages 886\u2013893,", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2005}, {"title": "Imagenet: A large-scale hierarchical image database", "author": ["J. Deng", "W. Dong", "R. Socher", "L. Li", "K. Li", "L. Fei-Fei"], "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 248\u2013255,", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2009}, {"title": "Imagenet: A large-scale hierarchical image database", "author": ["J. Deng", "W. Dong", "R. Socher", "L.-J. Li", "K. Li", "L. Fei- Fei"], "venue": "Proceedings of IEEE Conference on Computer Vision and Pattern Recognition, pages 248\u2013255,", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2009}, {"title": "Category independent object proposals", "author": ["I. Endres", "D. Hoiem"], "venue": "Proceedings of European Conference on Computer Vision, pages 575\u2013588,", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2010}, {"title": "Multi-scale orderless pooling of deep convolutional activation features", "author": ["Y. Gong", "L. Wang", "R. Guo", "S. Lazebnik"], "venue": "Proceedings of European Conference on Computer Vision, pages 392\u2013407.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2014}, {"title": "Measuring invariances in deep networks", "author": ["I. Goodfellow", "H. Lee", "Q.V. Le", "A. Saxe", "A.Y. Ng"], "venue": "Proceedings of Advances in neural information processing systems, pages 646\u2013654,", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2009}, {"title": "Densenet: Implementing efficient convnet descriptor pyramids", "author": ["F. Iandola", "M. Moskewicz", "S. Karayev", "R. Girshick", "T. Darrell", "K. Keutzer"], "venue": "arXiv preprint arXiv:1404.1869,", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2014}, {"title": "Exploiting generative models in discriminative classifiers", "author": ["T. Jaakkola", "D. Haussler"], "venue": "Proceedings of Advances in Neural Information Processing Systems,", "citeRegEx": "17", "shortCiteRegEx": null, "year": 1998}, {"title": "Negative evidences and cooccurences in image retrieval: The benefit of pca and whitening", "author": ["H. J\u00e9gou", "O. Chum"], "venue": "Proceedings of the European Conference on Computer Vision, pages 774\u2013787.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2012}, {"title": "Hamming embedding and weak geometric consistency for large scale image search", "author": ["H. Jegou", "M. Douze", "C. Schmid"], "venue": "Proceedings of the European Conference on Computer Vision, pages 304\u2013317,", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2008}, {"title": "Aggregating local descriptors into a compact image representation", "author": ["H. J\u00e9gou", "M. Douze", "C. Schmid", "P. P\u00e9rez"], "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 3304\u20133311,", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2010}, {"title": "Aggregating local image descriptors into compact codes", "author": ["H. J\u00e9gou", "F. Perronnin", "M. Douze", "C. Schmid"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2012}, {"title": "Triangulation embedding and democratic aggregation for image search", "author": ["H. J\u00e9gou", "A. Zisserman"], "venue": "CVPR - International Conference on Computer Vision and Pattern Recognition, Columbus, United States, June", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2014}, {"title": "Caffe: Convolutional architecture for fast feature embedding", "author": ["Y. Jia", "E. Shelhamer", "J. Donahue", "S. Karayev", "J. Long", "R. Girshick", "S. Guadarrama", "T. Darrell"], "venue": "arXiv preprint arXiv:1408.5093,", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2014}, {"title": "Imagenet classification with deep convolutional neural networks", "author": ["A. Krizhevsky", "I. Sutskever", "G.E. Hinton"], "venue": "Advances in neural information processing systems, pages 1097\u20131105,", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2012}, {"title": "Understanding image representations by measuring their equivariance and equivalence", "author": ["K. Lenc", "A. Vedaldi"], "venue": "arXiv preprint arXiv:1411.5908,", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2014}, {"title": "Do convnets learn correspondence", "author": ["J.L. Long", "N. Zhang", "T. Darrell"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2014}, {"title": "Distinctive image features from scale-invariant keypoints", "author": ["D.G. Lowe"], "venue": "International Journal of Computer Vision, 60(2):91\u2013110,", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2004}, {"title": "Automated flower classification over a large number of classes", "author": ["M.-E. Nilsback", "A. Zisserman"], "venue": "Computer Vision, Graphics & Image Processing, 2008. ICVGIP\u201908. Sixth Indian Conference on, pages 722\u2013729,", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2008}, {"title": "Scalable recognition with a vocabulary tree", "author": ["D. Nister", "H. Stewenius"], "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 2161\u20132168,", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2006}, {"title": "Efficient representation of local geometry for large scale object retrieval", "author": ["M. Perd\u2019och", "O. Chum", "J. Matas"], "venue": "In Proceedings of IEEE Conference on Computer Vision and Pattern Recognition,", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2009}, {"title": "Fisher kernels on visual vocabularies for image categorization", "author": ["F. Perronnin", "C. Dance"], "venue": "Proceedings of IEEE International Conference on Computer Vision and Pattern Recognition,", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2007}, {"title": "Large-scale image retrieval with compressed fisher vectors", "author": ["F. Perronnin", "Y. Liu", "J. S\u00e1nchez", "H. Poirier"], "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 3384\u20133391,", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2010}, {"title": "Improving the fisher kernel for large-scale image classification", "author": ["F. Perronnin", "J. S\u00e1nchez", "T. Mensink"], "venue": "Proceedings of European Conference on Computer Vision, pages 143\u2013156.", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2010}, {"title": "Object retrieval with large vocabularies and fast spatial matching", "author": ["J. Philbin", "O. Chum", "M. Isard", "J. Sivic", "A. Zisserman"], "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 1\u20138,", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2007}, {"title": "Persistent evidence of local image properties in generic convnets", "author": ["A.S. Razavian", "H. Azizpour", "A. Maki", "J. Sullivan", "C.H. Ek", "S. Carlsson"], "venue": "Image Analysis, pages 249\u2013262. Springer,", "citeRegEx": "35", "shortCiteRegEx": null, "year": 2015}, {"title": "Cnn features off-the-shelf: an astounding baseline for recognition", "author": ["A.S. Razavian", "H. Azizpour", "J. Sullivan", "S. Carlsson"], "venue": "Proceedings of IEEE International Conference on Computer Vision and Pattern Recognition Workshops (CVPRW), pages 512\u2013519,", "citeRegEx": "36", "shortCiteRegEx": null, "year": 2014}, {"title": "Very deep convolutional networks for large-scale image recognition", "author": ["K. Simonyan", "A. Zisserman"], "venue": "arXiv preprint arXiv:1409.1556,", "citeRegEx": "37", "shortCiteRegEx": null, "year": 2014}, {"title": "Video google: A text retrieval approach to object matching in videos", "author": ["J. Sivic", "A. Zisserman"], "venue": "Proceedings of the IEEE International Conference on Computer Vision, pages 1470\u20131477,", "citeRegEx": "38", "shortCiteRegEx": null, "year": 2003}, {"title": "Locality-constrained linear coding for image classification", "author": ["J. Wang", "J. Yang", "K. Yu", "F. Lv", "T. Huang", "Y. Gong"], "venue": "Proceedings of IEEE Conference on Computer Vision and Pattern Recognition, pages 3360\u20133367,", "citeRegEx": "39", "shortCiteRegEx": null, "year": 2010}, {"title": "Visualizing and understanding convolutional networks", "author": ["M.D. Zeiler", "R. Fergus"], "venue": "Proceedings of European Conference on Computer Vision, pages 818\u2013833.", "citeRegEx": "40", "shortCiteRegEx": null, "year": 2014}], "referenceMentions": [{"referenceID": 26, "context": "In recent years a lot of effort has been made on first designing the handcraft visual features [27, 10, 5] and then aggregating the visual features into a single vector [38, 39, 32, 20, 22].", "startOffset": 95, "endOffset": 106}, {"referenceID": 9, "context": "In recent years a lot of effort has been made on first designing the handcraft visual features [27, 10, 5] and then aggregating the visual features into a single vector [38, 39, 32, 20, 22].", "startOffset": 95, "endOffset": 106}, {"referenceID": 4, "context": "In recent years a lot of effort has been made on first designing the handcraft visual features [27, 10, 5] and then aggregating the visual features into a single vector [38, 39, 32, 20, 22].", "startOffset": 95, "endOffset": 106}, {"referenceID": 37, "context": "In recent years a lot of effort has been made on first designing the handcraft visual features [27, 10, 5] and then aggregating the visual features into a single vector [38, 39, 32, 20, 22].", "startOffset": 169, "endOffset": 189}, {"referenceID": 38, "context": "In recent years a lot of effort has been made on first designing the handcraft visual features [27, 10, 5] and then aggregating the visual features into a single vector [38, 39, 32, 20, 22].", "startOffset": 169, "endOffset": 189}, {"referenceID": 31, "context": "In recent years a lot of effort has been made on first designing the handcraft visual features [27, 10, 5] and then aggregating the visual features into a single vector [38, 39, 32, 20, 22].", "startOffset": 169, "endOffset": 189}, {"referenceID": 19, "context": "In recent years a lot of effort has been made on first designing the handcraft visual features [27, 10, 5] and then aggregating the visual features into a single vector [38, 39, 32, 20, 22].", "startOffset": 169, "endOffset": 189}, {"referenceID": 21, "context": "In recent years a lot of effort has been made on first designing the handcraft visual features [27, 10, 5] and then aggregating the visual features into a single vector [38, 39, 32, 20, 22].", "startOffset": 169, "endOffset": 189}, {"referenceID": 28, "context": "The visual vocabulary is usually trained with the unsupervised clustering algorithm, such as the standard k-means, hierarchical k-means [29], approximate k-means [34].", "startOffset": 136, "endOffset": 140}, {"referenceID": 33, "context": "The visual vocabulary is usually trained with the unsupervised clustering algorithm, such as the standard k-means, hierarchical k-means [29], approximate k-means [34].", "startOffset": 162, "endOffset": 166}, {"referenceID": 38, "context": "Instead of the hard vector quantization, in [39], Wang et al.", "startOffset": 44, "endOffset": 48}, {"referenceID": 31, "context": "Kernel method is another alternative to transform a set of features into a vectorial representation, such as Fisher kernel [32], and democratic kernel [22].", "startOffset": 123, "endOffset": 127}, {"referenceID": 21, "context": "Kernel method is another alternative to transform a set of features into a vectorial representation, such as Fisher kernel [32], and democratic kernel [22].", "startOffset": 151, "endOffset": 155}, {"referenceID": 20, "context": "Besides the quantization results in the BoVW model, Fisher kernel also includes the residual information between the local visual features and their visual words [21].", "startOffset": 162, "endOffset": 166}, {"referenceID": 31, "context": "Fisher kernel is demonstrated to be more efficient than the BoVW model in image classification and image search applications [32, 22, 20, 18, 33].", "startOffset": 125, "endOffset": 145}, {"referenceID": 21, "context": "Fisher kernel is demonstrated to be more efficient than the BoVW model in image classification and image search applications [32, 22, 20, 18, 33].", "startOffset": 125, "endOffset": 145}, {"referenceID": 19, "context": "Fisher kernel is demonstrated to be more efficient than the BoVW model in image classification and image search applications [32, 22, 20, 18, 33].", "startOffset": 125, "endOffset": 145}, {"referenceID": 17, "context": "Fisher kernel is demonstrated to be more efficient than the BoVW model in image classification and image search applications [32, 22, 20, 18, 33].", "startOffset": 125, "endOffset": 145}, {"referenceID": 32, "context": "Fisher kernel is demonstrated to be more efficient than the BoVW model in image classification and image search applications [32, 22, 20, 18, 33].", "startOffset": 125, "endOffset": 145}, {"referenceID": 19, "context": "One non probabilistic version of Fisher kernel is carefully investigated in [20, 21], which is named as vector of locally aggregated descriptors (VLAD).", "startOffset": 76, "endOffset": 84}, {"referenceID": 20, "context": "One non probabilistic version of Fisher kernel is carefully investigated in [20, 21], which is named as vector of locally aggregated descriptors (VLAD).", "startOffset": 76, "endOffset": 84}, {"referenceID": 26, "context": "Instead of designing the handcraft visual features, such as SIFT [27], SURF [5], and HOG [10], deep convolutional neural network (CNN) [24] learns a non-linear transformation model from large-scale well organized semantic dataset, namely ImageNet [12].", "startOffset": 65, "endOffset": 69}, {"referenceID": 4, "context": "Instead of designing the handcraft visual features, such as SIFT [27], SURF [5], and HOG [10], deep convolutional neural network (CNN) [24] learns a non-linear transformation model from large-scale well organized semantic dataset, namely ImageNet [12].", "startOffset": 76, "endOffset": 79}, {"referenceID": 9, "context": "Instead of designing the handcraft visual features, such as SIFT [27], SURF [5], and HOG [10], deep convolutional neural network (CNN) [24] learns a non-linear transformation model from large-scale well organized semantic dataset, namely ImageNet [12].", "startOffset": 89, "endOffset": 93}, {"referenceID": 23, "context": "Instead of designing the handcraft visual features, such as SIFT [27], SURF [5], and HOG [10], deep convolutional neural network (CNN) [24] learns a non-linear transformation model from large-scale well organized semantic dataset, namely ImageNet [12].", "startOffset": 135, "endOffset": 139}, {"referenceID": 11, "context": "Instead of designing the handcraft visual features, such as SIFT [27], SURF [5], and HOG [10], deep convolutional neural network (CNN) [24] learns a non-linear transformation model from large-scale well organized semantic dataset, namely ImageNet [12].", "startOffset": 247, "endOffset": 251}, {"referenceID": 22, "context": "With the learned non-linear transformation model, each image can be transformed to a feature vector [23].", "startOffset": 100, "endOffset": 104}, {"referenceID": 35, "context": "With breakthrough in many computer vision tasks, the CNN model has made a milestone in visual representation and become a new benchmark baseline [36].", "startOffset": 145, "endOffset": 149}, {"referenceID": 14, "context": "A lot of efforts have been made to understand the representation ability of convolutional neural network [15, 40, 25, 9, 26, 35].", "startOffset": 105, "endOffset": 128}, {"referenceID": 39, "context": "A lot of efforts have been made to understand the representation ability of convolutional neural network [15, 40, 25, 9, 26, 35].", "startOffset": 105, "endOffset": 128}, {"referenceID": 24, "context": "A lot of efforts have been made to understand the representation ability of convolutional neural network [15, 40, 25, 9, 26, 35].", "startOffset": 105, "endOffset": 128}, {"referenceID": 8, "context": "A lot of efforts have been made to understand the representation ability of convolutional neural network [15, 40, 25, 9, 26, 35].", "startOffset": 105, "endOffset": 128}, {"referenceID": 25, "context": "A lot of efforts have been made to understand the representation ability of convolutional neural network [15, 40, 25, 9, 26, 35].", "startOffset": 105, "endOffset": 128}, {"referenceID": 34, "context": "A lot of efforts have been made to understand the representation ability of convolutional neural network [15, 40, 25, 9, 26, 35].", "startOffset": 105, "endOffset": 128}, {"referenceID": 14, "context": "In [15], Goodfellow et al.", "startOffset": 3, "endOffset": 7}, {"referenceID": 23, "context": "There should not be substantial differences from the original CNN model in [24].", "startOffset": 75, "endOffset": 79}, {"referenceID": 39, "context": "In [40], Zeiler and Fergus try to understand why deep convolutional neural network works very well.", "startOffset": 3, "endOffset": 7}, {"referenceID": 24, "context": "In [25], Lenc et al.", "startOffset": 3, "endOffset": 7}, {"referenceID": 8, "context": "In [9], Cimpoi et al.", "startOffset": 3, "endOffset": 6}, {"referenceID": 25, "context": "In [26], Long et al.", "startOffset": 3, "endOffset": 7}, {"referenceID": 34, "context": "More specifically, in [35], Razavian et al.", "startOffset": 22, "endOffset": 26}, {"referenceID": 19, "context": "The study is made on the Holidays [20] dataset which is a benchmark dataset for image search with 1491 high resolution images.", "startOffset": 34, "endOffset": 38}, {"referenceID": 22, "context": "We use the Caffe-based CNN implementation [23] to extract our CNN feature.", "startOffset": 42, "endOffset": 46}, {"referenceID": 16, "context": "Let\u2019s consider using match kernel K(\u00b7, \u00b7) [17, 7, 22] to measure the similarity between X and Y , hence we have", "startOffset": 42, "endOffset": 53}, {"referenceID": 6, "context": "Let\u2019s consider using match kernel K(\u00b7, \u00b7) [17, 7, 22] to measure the similarity between X and Y , hence we have", "startOffset": 42, "endOffset": 53}, {"referenceID": 21, "context": "Let\u2019s consider using match kernel K(\u00b7, \u00b7) [17, 7, 22] to measure the similarity between X and Y , hence we have", "startOffset": 42, "endOffset": 53}, {"referenceID": 26, "context": "There are many classic works on it [27, 10, 5].", "startOffset": 35, "endOffset": 46}, {"referenceID": 9, "context": "There are many classic works on it [27, 10, 5].", "startOffset": 35, "endOffset": 46}, {"referenceID": 4, "context": "There are many classic works on it [27, 10, 5].", "startOffset": 35, "endOffset": 46}, {"referenceID": 26, "context": "For example, in SIFT [27] algorithm, the spatially constrained gradient histogram is used to represent the image patch.", "startOffset": 21, "endOffset": 25}, {"referenceID": 35, "context": "The recently research works revealed that the deep convolutional neural network (CNN) is very powerful for many computer vision tasks [36].", "startOffset": 134, "endOffset": 138}, {"referenceID": 22, "context": "In this paper, we adopt the CNN model [23] to transform the image patch into its vectorial representation.", "startOffset": 38, "endOffset": 42}, {"referenceID": 22, "context": "In [23], a pre-trained CNN model and well organized code are provided to be publicly available for academic uses.", "startOffset": 3, "endOffset": 7}, {"referenceID": 16, "context": "There are also a lot of works devoted to kernel methods [17, 7, 22, 33, 6, 32].", "startOffset": 56, "endOffset": 78}, {"referenceID": 6, "context": "There are also a lot of works devoted to kernel methods [17, 7, 22, 33, 6, 32].", "startOffset": 56, "endOffset": 78}, {"referenceID": 21, "context": "There are also a lot of works devoted to kernel methods [17, 7, 22, 33, 6, 32].", "startOffset": 56, "endOffset": 78}, {"referenceID": 32, "context": "There are also a lot of works devoted to kernel methods [17, 7, 22, 33, 6, 32].", "startOffset": 56, "endOffset": 78}, {"referenceID": 5, "context": "There are also a lot of works devoted to kernel methods [17, 7, 22, 33, 6, 32].", "startOffset": 56, "endOffset": 78}, {"referenceID": 31, "context": "There are also a lot of works devoted to kernel methods [17, 7, 22, 33, 6, 32].", "startOffset": 56, "endOffset": 78}, {"referenceID": 32, "context": "One classic separable kernel is the Fisher kernel which models the joint probability distribution of a set of features [33, 32, 31, 20].", "startOffset": 119, "endOffset": 135}, {"referenceID": 31, "context": "One classic separable kernel is the Fisher kernel which models the joint probability distribution of a set of features [33, 32, 31, 20].", "startOffset": 119, "endOffset": 135}, {"referenceID": 30, "context": "One classic separable kernel is the Fisher kernel which models the joint probability distribution of a set of features [33, 32, 31, 20].", "startOffset": 119, "endOffset": 135}, {"referenceID": 19, "context": "One classic separable kernel is the Fisher kernel which models the joint probability distribution of a set of features [33, 32, 31, 20].", "startOffset": 119, "endOffset": 135}, {"referenceID": 30, "context": "[31, 33] applied Fisher kernel to image classification and image retrieval applications.", "startOffset": 0, "endOffset": 8}, {"referenceID": 32, "context": "[31, 33] applied Fisher kernel to image classification and image retrieval applications.", "startOffset": 0, "endOffset": 8}, {"referenceID": 26, "context": "For example, in SIFT algorithm [27], the image patches are detected with different of Gaussian (DoG) method to obtain the scale invariant property.", "startOffset": 31, "endOffset": 35}, {"referenceID": 7, "context": "In this paper, we use the object detector [8, 2, 1, 13] to extract some object-like patches from the image.", "startOffset": 42, "endOffset": 55}, {"referenceID": 1, "context": "In this paper, we use the object detector [8, 2, 1, 13] to extract some object-like patches from the image.", "startOffset": 42, "endOffset": 55}, {"referenceID": 0, "context": "In this paper, we use the object detector [8, 2, 1, 13] to extract some object-like patches from the image.", "startOffset": 42, "endOffset": 55}, {"referenceID": 12, "context": "In this paper, we use the object detector [8, 2, 1, 13] to extract some object-like patches from the image.", "startOffset": 42, "endOffset": 55}, {"referenceID": 7, "context": "In a most recently published work named BING object detector [8], Cheng et al.", "startOffset": 61, "endOffset": 64}, {"referenceID": 7, "context": "BING object detection algorithm [8] output a real value for each patch to indicate how the detected image patch is like to be an object.", "startOffset": 32, "endOffset": 35}, {"referenceID": 7, "context": "Considering the excellent speed of BING algorithm, we adopt it [8] to extract our image patches.", "startOffset": 63, "endOffset": 66}, {"referenceID": 19, "context": "e, Holidays [20] and UKBench [29] and Oxford Building [3], to demonstrate the impact of the parameters in our algorithm.", "startOffset": 12, "endOffset": 16}, {"referenceID": 28, "context": "e, Holidays [20] and UKBench [29] and Oxford Building [3], to demonstrate the impact of the parameters in our algorithm.", "startOffset": 29, "endOffset": 33}, {"referenceID": 2, "context": "e, Holidays [20] and UKBench [29] and Oxford Building [3], to demonstrate the impact of the parameters in our algorithm.", "startOffset": 54, "endOffset": 57}, {"referenceID": 19, "context": "Holidays dataset [20] contains 1491 high-resolution images of different scenes and objects with 500 queries.", "startOffset": 17, "endOffset": 21}, {"referenceID": 33, "context": "We compute the mean of the average precision for all queries to obtain a mean Average Precision (mAP) score, which is used to evaluate the overall performance [34].", "startOffset": 159, "endOffset": 163}, {"referenceID": 28, "context": "UKBench dataset [29] contains 2550 objects or scenes, each with four images taken under different views or imaging conditions, resulting in 10200 images in total.", "startOffset": 16, "endOffset": 20}, {"referenceID": 28, "context": "In terms of accuracy measurement, the top-4 accuracy [29] is used as evaluation metric.", "startOffset": 53, "endOffset": 57}, {"referenceID": 2, "context": "Oxford Building dataset [3, 34] consists of 5062 images of buildings and 55 query images corresponding to 11 distinct buildings in Oxford.", "startOffset": 24, "endOffset": 31}, {"referenceID": 33, "context": "Oxford Building dataset [3, 34] consists of 5062 images of buildings and 55 query images corresponding to 11 distinct buildings in Oxford.", "startOffset": 24, "endOffset": 31}, {"referenceID": 7, "context": "The first one is the number of image patches x detected by BING detector [8], which can be denoted by N .", "startOffset": 73, "endOffset": 76}, {"referenceID": 22, "context": "We adopt the CNN model to construct the vectorial representation of x resulting in a 4096-D \u03b3(x) [23].", "startOffset": 97, "endOffset": 101}, {"referenceID": 7, "context": "N is the number of object detected with BING detector [8].", "startOffset": 54, "endOffset": 57}, {"referenceID": 31, "context": "V is the number of Gaussian functions used in Fisher vector model [32].", "startOffset": 66, "endOffset": 70}, {"referenceID": 22, "context": "D is the dimension of the CNN features [23] after performing the PCA dimension reduction.", "startOffset": 39, "endOffset": 43}, {"referenceID": 18, "context": "The performance of the proposed kernelized convolutional neural network (KCNN) algorithm on three benchmark datasets, namely Holidays [19], Oxford Building [34], and UKBench [29].", "startOffset": 134, "endOffset": 138}, {"referenceID": 33, "context": "The performance of the proposed kernelized convolutional neural network (KCNN) algorithm on three benchmark datasets, namely Holidays [19], Oxford Building [34], and UKBench [29].", "startOffset": 156, "endOffset": 160}, {"referenceID": 28, "context": "The performance of the proposed kernelized convolutional neural network (KCNN) algorithm on three benchmark datasets, namely Holidays [19], Oxford Building [34], and UKBench [29].", "startOffset": 174, "endOffset": 178}, {"referenceID": 31, "context": "The last parameter is the visual vocabulary size used in Fisher vector [32] corresponding to the \u03b2(\u00b7) in Eq.", "startOffset": 71, "endOffset": 75}, {"referenceID": 29, "context": "Similar result has also been observed when SIFT features are used to perform retrieval on this dataset [30] [21].", "startOffset": 103, "endOffset": 107}, {"referenceID": 20, "context": "Similar result has also been observed when SIFT features are used to perform retrieval on this dataset [30] [21].", "startOffset": 108, "endOffset": 112}, {"referenceID": 26, "context": "That is, in the construction of SIFT descriptor, better retrieval performance is obtained with the orientation selected as the gravity orientation instead of the traditional dominant gradient orientation [27] [21], since there is very few rotation transformations for the building images, as demonstrated in Fig.", "startOffset": 204, "endOffset": 208}, {"referenceID": 20, "context": "That is, in the construction of SIFT descriptor, better retrieval performance is obtained with the orientation selected as the gravity orientation instead of the traditional dominant gradient orientation [27] [21], since there is very few rotation transformations for the building images, as demonstrated in Fig.", "startOffset": 209, "endOffset": 213}, {"referenceID": 26, "context": "However on Oxford Building dataset SIFT [27] based methods can get better result namely [32] [3] [22].", "startOffset": 40, "endOffset": 44}, {"referenceID": 31, "context": "However on Oxford Building dataset SIFT [27] based methods can get better result namely [32] [3] [22].", "startOffset": 88, "endOffset": 92}, {"referenceID": 2, "context": "However on Oxford Building dataset SIFT [27] based methods can get better result namely [32] [3] [22].", "startOffset": 93, "endOffset": 96}, {"referenceID": 21, "context": "However on Oxford Building dataset SIFT [27] based methods can get better result namely [32] [3] [22].", "startOffset": 97, "endOffset": 101}, {"referenceID": 27, "context": "The reason is that the Oxford Building dataset consists of building images and the retrieval on this dataset is more like a fine-grained problem [28].", "startOffset": 145, "endOffset": 149}, {"referenceID": 10, "context": "ral network is designed to tackle the generic classification problem [11, 24] and fine-tune is usually required for the fine-grained vision tasks.", "startOffset": 69, "endOffset": 77}, {"referenceID": 23, "context": "ral network is designed to tackle the generic classification problem [11, 24] and fine-tune is usually required for the fine-grained vision tasks.", "startOffset": 69, "endOffset": 77}, {"referenceID": 35, "context": "Comparing with [36], our goal is totally different.", "startOffset": 15, "endOffset": 19}, {"referenceID": 35, "context": "Our goal is to construct a vectorial representation for an image while [36] use the Spatial Search that is not a vectorial image representation.", "startOffset": 71, "endOffset": 75}, {"referenceID": 13, "context": "Comparing with [14], on Holidays, we get 0.", "startOffset": 15, "endOffset": 19}, {"referenceID": 13, "context": "829 mAP while [14] gets 0.", "startOffset": 14, "endOffset": 18}, {"referenceID": 13, "context": "Besides the higher accuracy, we also address the rotation transformation while [14] not.", "startOffset": 79, "endOffset": 83}, {"referenceID": 3, "context": "Comparing with [4], they focus on construct compressed codes of image representation with the retrained regular CNN while we focus on addressing the object transformations in the vectorial representation of complex images without retraining.", "startOffset": 15, "endOffset": 18}, {"referenceID": 31, "context": "Dataset [32] [3] [22] CNN KCNN", "startOffset": 8, "endOffset": 12}, {"referenceID": 2, "context": "Dataset [32] [3] [22] CNN KCNN", "startOffset": 13, "endOffset": 16}, {"referenceID": 21, "context": "Dataset [32] [3] [22] CNN KCNN", "startOffset": 17, "endOffset": 21}, {"referenceID": 36, "context": "Besides the CNN structure implemented in Caffe library, there are also some other emerging CNN structures [37, 16].", "startOffset": 106, "endOffset": 114}, {"referenceID": 15, "context": "Besides the CNN structure implemented in Caffe library, there are also some other emerging CNN structures [37, 16].", "startOffset": 106, "endOffset": 114}], "year": 2015, "abstractText": "With the impressive capability to capture visual content, deep convolutional neural networks (CNN) have demonstrated promising performance in various vision-based applications, such as classification, recognition, and object detection. However, due to the intrinsic structure design of CNN, for images with complex content, it achieves limited capability on invariance to translation, rotation, and re-sizing changes, which is strongly emphasized in the scenario of content-based image retrieval. In this paper, to address this problem, we proposed a new kernelized deep convolutional neural network. We first discuss our motivation by an experimental study to demonstrate the sensitivity of the global CNN feature to the basic geometric transformations. Then, we propose to represent visual content with approximate invariance to the above geometric transformations from a kernelized perspective. We extract CNN features on the detected object-like patches and aggregate these patch-level CNN features to form a vectorial representation with the Fisher vector model. The effectiveness of our proposed algorithm is demonstrated on image search application with three benchmark datasets.", "creator": "LaTeX with hyperref package"}}}