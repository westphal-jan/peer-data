{"id": "1509.04904", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "16-Sep-2015", "title": "Causal Model Analysis using Collider v-structure with Negative Percentage Mapping", "abstract": "A major problem of causal inference is the arrangement of dependent nodes in a directed acyclic graph (DAG) with path coefficients and observed confounders. Path coefficients do not provide the units to measure the strength of information flowing from one node to the other. Here we proposed the method of causal structure learning using collider v-structures (CVS) with Negative Percentage Mapping (NPM) to get selective thresholds of information strength, to direct the edges and subjective confounders in a DAG. The NPM is used to scale the strength of information passed through nodes in units of percentage from interval from 0 to 1. The causal structures are constructed by bottom up approach using path coefficients, causal directions and confounders, derived implementing collider v-structure and NPM. The method is self-sufficient to observe all the latent confounders present in the causal model and capable of detecting every responsible causal direction. The results are tested for simulated datasets of non-Gaussian distributions and compared with DirectLiNGAM and ICA-LiNGAM to check efficiency of the proposed method.", "histories": [["v1", "Wed, 16 Sep 2015 12:37:30 GMT  (2060kb,D)", "http://arxiv.org/abs/1509.04904v1", null]], "reviews": [], "SUBJECTS": "cs.AI", "authors": ["pramod kumar parida", "tshilidzi marwala", "snehashish chakraverty"], "accepted": false, "id": "1509.04904"}, "pdf": {"name": "1509.04904.pdf", "metadata": {"source": "CRF", "title": "Causal Model Analysis using Collider v-structure with Negative Percentage Mapping", "authors": ["Pramod Kumar Parida", "Tshilidzi Marwala"], "emails": ["pramodsstyle@gmail.com", "tmarwala@gmail.com", "chak@yahoo.com"], "sections": [{"heading": "1 Background Study: Markov Model, d-separable, v-structures", "text": "The Markov model is a directional graph that does not contain bidirectional edges [5]. This criterion does not provide the acyclicity of the model. However, with the Markov model we can find the conditional dependent and independent nodes, provided we have information about previous observations. Sometimes, it is very difficult to know about previous observations or any of the previous information. Suppose the observed variables are present, the later ones are conditionally dependent on the previous ones. Now, we need a criterion to observe the posterior effects. In this scenario, d-separable provides the criteria of substructural learning for conditional dependence and independence of the characteristics. To implement d-separable structures, we need v-structures. The primary definition of the v-structure is provided for the undirected graphic models in which two nodes are connected only by an undirected edge."}, {"heading": "2 Proposed Structural Learning Method", "text": "Any orthodox and modern approach to causal construction delays the definition of the measure of information flowing through directed edges, while the path coefficient / connectivity strength is the information transmitted. Depending on the information (path coefficient) from either nodes or external confounders, we cannot derive the directions in the causal model. This problem becomes more complicated when it comes to the multivariate structure. So we need a measurement for these entropies, through which we will be able to complete our instructions.Before scaling the flow of information, we need a model we can work on for causal constructions.The easiest way to build a larger model is to first build the substructures of the larger model and then arrange them to complete the required structure.The directed acyclic causal graph can be studied as the decomposition of multiple V structures under the lights of the Markov model [10], in our case of multiple bottom structures we used to form a bottom order."}, {"heading": "2.1 Collider v-structure with NPM (CVS with NPM):", "text": "Dre rf\u00fc eeisrVnree\u00fcgr rf\u00fc ide rf\u00fc ide rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf"}, {"heading": "3 Mathematical model: Collider v-structure, NPM", "text": "In order to provide a mathematical model, we have to simplify the causal structures shown in Figure 1 and make a generalization of the possible permutations in the V structure of the accelerator."}, {"heading": "3.1 Model for Collider v-structure:", "text": "Consider a series of observations where m is the number of instances and n is the number of variables or observations. Now, we draw three different types of observations (i, j, k) to construct the collider v structure with the fully observed confounders; the lower equation is a generalized form of the linear model of the collider v structure, m, i = 1vi = cjim, j = 1vk + ei (1) In equation 1, vi is the mean collision, and vj, vk are the extreme collisions, (cji, cki) represents the path coefficients of (vj, vk) or the subscripts (ji, ki) in coefficients represent the statements of (j, k, k, i) is the mean collision and vk, and we are the confounditions of vii."}, {"heading": "3.2 Negative Percentage Mapping (NPM):", "text": "To calculate the thresholds for selecting the best possible directions in the collider v structure, we need to find the percentage contribution for each of the observed path coefficients and confounders estimated in Equation 3.Theorem 1. Consider the linear model for two variable propositions (x, y), since y = ax + b, where y are estimated as a function of x and a, b estimate parameters for the liner model. The percentage contribution (% c) for (ax, b), c < is (ax, b)% c = (ax, b) / y always applies to each {ax)% c = (ax), (b)% c), c), c), the causal parameters [0, 1] such that (ax)% c + (b)% c = 1. If (b)% c = 1 (ax)%, c = 1%, c), c (b), c), (b), c), c)."}, {"heading": "4 Experimental Setup", "text": "For the best implementation of our method, an optimised and fully randomised experimental set-up is required."}, {"heading": "5 Simulations", "text": "This year, it is closer than ever before in the history of the country."}, {"heading": "6 Conclusion", "text": "The proposed model provides solutions for how much information is passed from the parent node to the child and how strongly a child node is influenced by external confusion, which remains unresolved to this day.The novelty of the method is to provide yardsticks for the path coefficient in percentage contribution units from a ring of [0, 1].The model provides strong criteria for the directed acyclic formation of causal graphs, as shown in the figures above.The ability of the causal DAG estimate is fully verified under synthetic and real conditions compared to other methods and outcomes described in Section 5. The method is very useful in loud data sets to generate causal DAGs. Further details about the loud data results and reversible systems are provided in complementary material.A full R code is available for further use and development and all data sets used are provided in supplementary material.The results of the proposed method may be used to provide a larger problem and a reversible structure for the future system."}], "references": [{"title": "Kernel independent component analysis", "author": ["F.R. Bach", "M.I. Jordan"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2002}, {"title": "Active Learning of Causal Networks with Intervention Experiments and Optimal Designs", "author": ["He", "Yang-Bo", "Geng", "Zhi"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2008}, {"title": "UCI repository of machine learning databases. Available at http: //www.ics.uci.edu/ \u0303mlearn/MLRepository.html", "author": ["P.M. Murphy", "D.W. Aha"], "venue": null, "citeRegEx": "4", "shortCiteRegEx": "4", "year": 1995}, {"title": "Causality: Models, Reasoning, and Inference", "author": ["J. Pearl"], "venue": null, "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2000}, {"title": "Using Markov Blanket for Causal Structure Learning", "author": ["J.P. Pellet"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2008}, {"title": "A Linear non-Gaussian Acyclic Model for Causal Discovery", "author": ["S. Shimizu", "P.O. Hoyer", "A. Hyv\u00e4rinen", "A. Kerminen"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2006}, {"title": "DirectLiNGAM: A Direct Method for Learning a Linear non-Gaussian Structural Equation Model", "author": ["S. Shimizu", "T. Inazumi", "Y. Sogawa", "A. Hyv\u00e4rinen", "Y. Kawahara", "T. Washio", "P.O. Hoyer", "K. Bollen"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2011}, {"title": "Geometry of the Faithfulness Assumption in Causal Inference", "author": ["C. Uhler", "G. Raskutti", "P. Buhlmann", "B. Yu"], "venue": "The Annals of Statistics,Vol", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2013}, {"title": "A Recursiv Method for Structural Learning of Diirected Acyclic Graphs. Journal of Machine Learning Research, 9:459-483", "author": ["X. Xie", "Z. Geng"], "venue": null, "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2008}], "referenceMentions": [{"referenceID": 0, "context": "The NPM is used to scale the strength of information passed through nodes in units of percentage from interval [0, 1].", "startOffset": 111, "endOffset": 117}, {"referenceID": 3, "context": "The Markov model is a directed graph which does not contains any bi-directed edges [5].", "startOffset": 83, "endOffset": 86}, {"referenceID": 1, "context": "He and Geng [2] applied intervention technique on markov equivalence classes to produce subgraphs with directed edges.", "startOffset": 12, "endOffset": 15}, {"referenceID": 4, "context": "The structural learning of causal model can be shown through faithfulness and relevance of connections using d-separable and v-structures [6].", "startOffset": 138, "endOffset": 141}, {"referenceID": 0, "context": "The faithfulness for markov equivalent classes are beneficial and weak or strong faithful criteria can be derived for Gaussian and uniform distributions, where path coefficients and errors are drawn from intervals [\u22121, 1] and [0, 1] respectively [9].", "startOffset": 226, "endOffset": 232}, {"referenceID": 7, "context": "The faithfulness for markov equivalent classes are beneficial and weak or strong faithful criteria can be derived for Gaussian and uniform distributions, where path coefficients and errors are drawn from intervals [\u22121, 1] and [0, 1] respectively [9].", "startOffset": 246, "endOffset": 249}, {"referenceID": 8, "context": "The directed acyclic causal graph can be studied as a decomposition of multiple v-structures under the lights of markov model [10].", "startOffset": 126, "endOffset": 130}, {"referenceID": 0, "context": "The percentage contribution (%) for (ax, b) \u2208 < is (ax, b)% = (ax, b)/y always holds, for each {(ax)%c, (b)%c} \u2208 [0, 1] such that (ax)% + (b)% = 1.", "startOffset": 113, "endOffset": 119}, {"referenceID": 0, "context": "If (ax)% + (b)% = 1 for any {(ax)%c, (b)%c} \u2208 S, where set S contains the elements from {[\u2212\u221e,\u22121], [\u22121, 0], [0, 1], [1,\u221e]} then for every over margin percentage there exist a negative percentage for wich (ax)%+(b)% = 1 holds.", "startOffset": 107, "endOffset": 113}, {"referenceID": 0, "context": "These percentages can be mapped to interval [0, 1] by negative percentage mapping defined as (ax, b)% = {|(ax)%|, |(b)%|}/{|(ax)%|+ |(b)%|}.", "startOffset": 44, "endOffset": 50}, {"referenceID": 0, "context": "So if we will observe all the negative percentage contributions, then through negative percentage mapping we can easily compute all the parameter contributions in [0, 1] interval.", "startOffset": 163, "endOffset": 169}, {"referenceID": 0, "context": "Bach and Jordan [1] provides the framework to construct the sparse matrices with known probabilities.", "startOffset": 16, "endOffset": 19}, {"referenceID": 5, "context": "The non-Gaussianity of data is more useful than Gaussian for identification of causal DAGS [7].", "startOffset": 91, "endOffset": 94}, {"referenceID": 5, "context": "The model is tested for effective computational time with ICA-LiNGAM [7] and DirectLiNGAM [8] methods.", "startOffset": 69, "endOffset": 72}, {"referenceID": 6, "context": "The model is tested for effective computational time with ICA-LiNGAM [7] and DirectLiNGAM [8] methods.", "startOffset": 90, "endOffset": 93}, {"referenceID": 2, "context": "We used the regression, multivariate and categorical dataset of Boston Housing, available in UCI Repository [4].", "startOffset": 108, "endOffset": 111}, {"referenceID": 0, "context": "The novelty of the method is to provide measures for path coefficient in units of percentage contributions from a rang of [0, 1].", "startOffset": 122, "endOffset": 128}], "year": 2015, "abstractText": "A major problem of causal inference is the arrangement of dependent nodes in a directed acyclic graph (DAG) with path coefficients and observed confounders. Path coefficients do not provide the units to measure the strength of information flowing from one node to the other. Here we proposed the method of causal structure learning using collider v-structures (CVS) with Negative Percentage Mapping (NPM) to get selective thresholds of information strength, to direct the edges and subjective confounders in a DAG. The NPM is used to scale the strength of information passed through nodes in units of percentage from interval [0, 1]. The causal structures are constructed by bottom up approach using path coefficients, causal directions and confounders, derived implementing collider v-structure and NPM. The method is self-sufficient to observe all the latent confounders present in the causal model and capable of detecting every responsible causal direction. The results are tested for simulated datasets of non-Gaussian distributions and compared with DirectLiNGAM and ICA-LiNGAM to check efficiency of the proposed method. 1 Background Study: Markov Model, d-separable, v-structures In Bayesian Learning the Markov properties play an important role, which set the criteria for structure analysis. The Markov model is a directed graph which does not contains any bi-directed edges [5]. This criteria does not provides the acyclicity of the model. But using markov model we can find the conditional dependent and independent nodes, provided we have information on prior observations. Sometime it happens to be very difficult to know about prior observations or any of the prior information. Assuming the observed variables are prior, the later ones will be conditionally dependent on the prior ones. Now we need a criteria to observe the posterior effects. In this scenario d-separable provides the criteria of sub-structural learning for conditional dependence and independence of the features. To implement d-separable we need v-structures. The primary definition of v-structure is provided for the undirected graphical models, where two nodes are only connected through an undirected edge [3]. The v-structures in a markov model are the sub-structures where each three of the observed nodes are only connected through two directed edges. For the choice of any three nodes we can permute them in three possible ways using two 1 ar X iv :1 50 9. 04 90 4v 1 [ cs .A I] 1 6 Se p 20 15 directed edges irrespective of their positions. The d-separable criteria is used to construct a set of nodes such that conditioning on which, the paths/edges can be separated between other two nodes in the v-structure. Consider v-structures of a \u2192 b \u2192 c , a \u2190 b \u2192 c and a \u2192 b \u2190 c. In first and second types a and c can be d-separated by observing/conditioning on b, while in the third one conditioning on b makes a and c becomes dependent on each other as ancestor nodes of b remains unobserved.. The third type of v-structure is called a collider. He and Geng [2] applied intervention technique on markov equivalence classes to produce subgraphs with directed edges. They generated v-structures of subgraphs with markov properties to find directions, using conditional probabilities. Markov Blankets can reveal the directions in the nodes based on their relevance of connection inside the blanket. The structural learning of causal model can be shown through faithfulness and relevance of connections using d-separable and v-structures [6]. The faithfulness for markov equivalent classes are beneficial and weak or strong faithful criteria can be derived for Gaussian and uniform distributions, where path coefficients and errors are drawn from intervals [\u22121, 1] and [0, 1] respectively [9]. In following Sections 2 and 3, we provided complete explanation for using collider v-structures as a substructural model and NPM for measuring strength of path coefficients. 2 Proposed Structural Learning Method Every orthodox and modern approach of causal construction lag to define the measure for information passing through directed edges, while path coefficient/connection strength is the information passed on. Depending on the information (path coefficient) either from nodes or external confounders we cannot conclude the directions in causal model. This problem becomes more complicated when it comes to multivariate structure. So we need a measure for these entropies through which we will be able conclude our directions. Before scaling the flow of information, we need a model to work on for causal constructions. The easiest way to build a larger model is, first to build the sub-structures of the bigger model and then arrange these small ones to complete the required structure. The directed acyclic causal graph can be studied as a decomposition of multiple v-structures under the lights of markov model [10]. In our case we used a bottom up approach of arranging multiple v-structures to form a causal DAG. For this type of approach we don\u2019t need any prior information regarding the structure or contributing variables. After deriving all the direction, path coefficients and confounders for the considered vstructures, these can be combined together to provide a complete causal structure. The model of collider v-structure provides the necessary sub-structural analysis in a very detailed way. 2.1 Collider v-structure with NPM (CVS with NPM): To analyze a larger causal structure in a smaller sub-structural level consider the v-structure as defied in the markov model, that is a set of three nodes connected through two directed edges. The collider is a v-structure where two extreme nodes have directed edges towards the middle node. We referred it as collider v-structure whenever an external confounder is present or added to the collider system and we modeled our algorithm suitable to analyze these types of DAG. Below Figure 1 provides the collider v-structures.", "creator": "LaTeX with hyperref package"}}}