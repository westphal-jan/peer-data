{"id": "1509.05173", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "17-Sep-2015", "title": "Taming the ReLU with Parallel Dither in a Deep Neural Network", "abstract": "Rectified Linear Units (ReLU) seem to have displaced traditional 'smooth' nonlinearities as activation-function-du-jour in many - but not all - deep neural network (DNN) applications. However, nobody seems to know why. In this article, we argue that ReLU are useful because they are ideal demodulators - this helps them perform fast abstract learning. However, this fast learning comes at the expense of serious nonlinear distortion products - decoy features. We show that Parallel Dither acts to suppress the decoy features, preventing overfitting and leaving the true features cleanly demodulated for rapid, reliable learning.", "histories": [["v1", "Thu, 17 Sep 2015 09:04:30 GMT  (271kb)", "http://arxiv.org/abs/1509.05173v1", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["andrew j r simpson"], "accepted": false, "id": "1509.05173"}, "pdf": {"name": "1509.05173.pdf", "metadata": {"source": "META", "title": "Taming the ReLU with Parallel Dither in a Deep Neural Network", "authors": ["Andrew J.R. Simpson"], "emails": ["Andrew.Simpson@Surrey.ac.uk"], "sections": [{"heading": null, "text": "In fact, it is the case that most people who are in a position to put themselves into another world, to put themselves into another world, in which they can no longer move, feel put back into another world, in which they can no longer move, in which they can no longer move, in which they can no longer live, in which they can no longer live, in which they can no longer live, in which they can no longer live, in which they can no longer live, in which they can no longer live, in which they can no longer live, in which they can no longer live, in which they can no longer live, in which they can no longer live, in which they can no longer live, in which they can no longer live, in which they can no longer live, in which they can not be able to live, in which they can no longer live, in which they can no longer live, in which they cannot live, in which they do not want to be able to be able to be, in which they do not want to be able to live, in which they can no longer live, in which they do not want to be able to be able to live, in which they do not want to live, in which they can no longer live, in which they do not want to be able to be able to live, in which they do not want to be able to live, in which they can no longer live, in which they do not want to be able to live, in which they can no longer live, in which they do not want to be able to be able to be able to live, in which they do not want to live, in which they can no longer want to live, in which they can no longer live, in which they can no longer want to be able to live, in which they do not want to live, in which they do not want to be able to be able to live, in which they can no longer live, in which they can no longer live, in which they can no longer want to be able to live, in which they can no longer want to be able to live, in which they can no longer live, in which they do not want to be able to live, in which they can no longer want to be able to live, in which they can no longer want to live in which they can live, in which they can no longer want to be able to be able to live, in which they can"}, {"heading": "II. METHOD", "text": "This year it is more than ever before."}, {"heading": "III. RESULTS", "text": "The model developed without regularization performs poorly (as in [6]). Here too (Fig. 3) we draw equivalent data (from [6]) from a model with the same architecture, but with the biased signification function [1], which was trained on the same data and the same random initial weights. The same 100-fold parallel dither w / dropout was also applied in [6]. This provides an interesting comparison, since the biased signaling function of this model (1) is identical in terms of the signing rate."}, {"heading": "IV. DISCUSSION AND CONCLUSION", "text": "Following discrete signal processing of deep neural networks [1,4-6], we have interpreted ReLU in terms of demodulation and introduced the concept of decoy features to capture the effect of a DNN learning artifact feature caused by abrupt nonlinearity of the activation function. We have demonstrated the ability of parallel densities [6,5] to suppress these decoy features and thus regulate a ReLU DNN. We have also shown that the demodulation advantage of the ReLU over the traditional smooth activation functions is inverse compared to the distorted sigmoid [1] (which is explicitly optimized for demodulation)."}, {"heading": "ACKNOWLEDGMENT", "text": "The AJRS did this work at weekends and was supported by his wife and children."}], "references": [{"title": "Abstract Learning via Demodulation in a Deep Neural Network\u201d, arxiv.org abs/1502.04042", "author": ["AJR Simpson"], "venue": null, "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2015}, {"title": "Deep sparse rectifier neural networks", "author": ["X Glorot", "A Bordes", "Y Bengio"], "venue": "In Int. Conf. Artificial Intelligence and Statistics,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2011}, {"title": "Over-Sampling in a Deep Neural Network\u201d, arxiv.org abs/1502.03648", "author": ["AJR Simpson"], "venue": null, "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2015}, {"title": "Dither is Better than Dropout for Regularising Deep Neural Networks\u201d, arxiv.org abs/1508.04826", "author": ["AJR Simpson"], "venue": null, "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2015}, {"title": "Parallel Dither and Dropout for Regularising Deep Neural Networks\u201d, arxiv.org abs/1508.07130", "author": ["AJR Simpson"], "venue": null, "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2015}, {"title": "Gradient-based learning applied to document recognition", "author": ["Y LeCun", "L Bottou", "Y Bengio", "P Haffner"], "venue": "Proc. IEEE", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 1998}, {"title": "A fast learning algorithm for deep belief nets", "author": ["GE Hinton", "S Osindero", "Y Teh"], "venue": "Neural Computation", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2006}, {"title": "Improving neural networks by preventing co-adaptation of feature detectors\u201d, The Computing Research Repository (CoRR), abs/1207.0580", "author": ["GE Hinton", "N Srivastava", "A Krizhevsky", "I Sutskever", "R Salakhutdinov"], "venue": null, "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2012}, {"title": "On-the-Fly Learning in a Perpetual Learning Machine\u201d, arxiv.org abs/1509.00913", "author": ["AJR Simpson"], "venue": null, "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2015}, {"title": "Use it or Lose it: Selective Memory and Forgetting in a Perpetual Learning Machine\u201d, arxiv.org abs/1509.03185", "author": ["AJR Simpson"], "venue": null, "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2015}], "referenceMentions": [{"referenceID": 0, "context": "Deep neural networks are able to learn abstract, hierarchical representations through a process of demodulation [1].", "startOffset": 112, "endOffset": 115}, {"referenceID": 0, "context": "For symmetrical carrier signals, demodulation is the result of rectification [1].", "startOffset": 77, "endOffset": 80}, {"referenceID": 1, "context": "In this context, it is not surprising that the Rectified Linear Unit (ReLU) [2] has found success in DNNs [3] \u2013 it is, in principle, a nearly ideal demodulator function.", "startOffset": 76, "endOffset": 79}, {"referenceID": 0, "context": "The discrete signal processing interpretation of DNN [1,4] views each neuron as a step of abstraction.", "startOffset": 53, "endOffset": 58}, {"referenceID": 2, "context": "The discrete signal processing interpretation of DNN [1,4] views each neuron as a step of abstraction.", "startOffset": 53, "endOffset": 58}, {"referenceID": 2, "context": "In this context, through gradient descent, the DNN is looking for those filters [4] which most easily allow discrimination.", "startOffset": 80, "endOffset": 83}, {"referenceID": 2, "context": "Following this line of reasoning, even for artefact-free data, nonlinear distortion introduced by the activation function might provide decoy features [4].", "startOffset": 151, "endOffset": 154}, {"referenceID": 2, "context": "Secondly, the decoy features may not necessarily be reliably demodulated further up the hierarchy, especially if they are either high-order or the result of aliasing [4].", "startOffset": 166, "endOffset": 169}, {"referenceID": 4, "context": "Dither also acts to regularise DNN by the same means [6,7].", "startOffset": 53, "endOffset": 58}, {"referenceID": 5, "context": "Dither also acts to regularise DNN by the same means [6,7].", "startOffset": 53, "endOffset": 58}, {"referenceID": 5, "context": "In order to illustrate how the process of dithering improves the cause for ReLU in the DNN context, we use the well-known computer vision problem of handwritten digit classification using the MNIST dataset [7].", "startOffset": 206, "endOffset": 209}, {"referenceID": 6, "context": "Hinton\u2019s [8] architecture, but using ReLU, we built a fully connected network of size 784x100x10 units, where the 10unit softmax output layer corresponds to the 10-way digit classification problem.", "startOffset": 9, "endOffset": 12}, {"referenceID": 3, "context": "Operating within the so-called \u2018small-data regime\u2019 (as in [5,6]), we used only the first 256 training examples of the MNIST dataset and tested on the full 10,000 test examples.", "startOffset": 58, "endOffset": 63}, {"referenceID": 4, "context": "Operating within the so-called \u2018small-data regime\u2019 (as in [5,6]), we used only the first 256 training examples of the MNIST dataset and tested on the full 10,000 test examples.", "startOffset": 58, "endOffset": 63}, {"referenceID": 4, "context": "dither [6].", "startOffset": 7, "endOffset": 10}, {"referenceID": 4, "context": "The fourth was the baseline model regularised using 100x parallel dither w/ dropout [6,9].", "startOffset": 84, "endOffset": 89}, {"referenceID": 7, "context": "The fourth was the baseline model regularised using 100x parallel dither w/ dropout [6,9].", "startOffset": 84, "endOffset": 89}, {"referenceID": 0, "context": "These parameter choices allow useful comparison with the equivalent optimally-biased-sigmoid [1] results of [6] (which were trained from the exact same random starting weights on the same problem).", "startOffset": 93, "endOffset": 96}, {"referenceID": 4, "context": "These parameter choices allow useful comparison with the equivalent optimally-biased-sigmoid [1] results of [6] (which were trained from the exact same random starting weights on the same problem).", "startOffset": 108, "endOffset": 111}, {"referenceID": 4, "context": "The biased-sigmoid data of [6], for the model regularised with 100x parallel dither w/ dropout, is included both for reference and because it is explicitly optimised for demodulation [1].", "startOffset": 27, "endOffset": 30}, {"referenceID": 0, "context": "The biased-sigmoid data of [6], for the model regularised with 100x parallel dither w/ dropout, is included both for reference and because it is explicitly optimised for demodulation [1].", "startOffset": 183, "endOffset": 186}, {"referenceID": 0, "context": "A audio-domain test signal was constructed with a sinusoidal carrier signal (10000 cycles/s) modulated by (multiplied with) a 100 cycle/s sinusoidal modulator (similar to [1]).", "startOffset": 171, "endOffset": 174}, {"referenceID": 4, "context": "Test error functions of (non-batch) SGD iterations, for the various ReLU models, including the equivalent biased-sigmoid data of [6] for comparison.", "startOffset": 129, "endOffset": 132}, {"referenceID": 4, "context": "The model trained with parallel dither w/ dropout performs best (as in [6]).", "startOffset": 71, "endOffset": 74}, {"referenceID": 4, "context": "3) equivalent data (from [6]) from a model with the same architecture but featuring the biased-sigmoid [1] activation function, trained on the same data and from the same random starting weights.", "startOffset": 25, "endOffset": 28}, {"referenceID": 0, "context": "3) equivalent data (from [6]) from a model with the same architecture but featuring the biased-sigmoid [1] activation function, trained on the same data and from the same random starting weights.", "startOffset": 103, "endOffset": 106}, {"referenceID": 4, "context": "The same 100x parallel dither w/ dropout was also applied in [6].", "startOffset": 61, "endOffset": 64}, {"referenceID": 0, "context": "This provides an interesting comparison because the biased sigmoid [1] activation function of this model was explicitly optimised for demodulation but is much smoother than the ReLU.", "startOffset": 67, "endOffset": 70}, {"referenceID": 4, "context": "(though parallel dither [6] might help as in [10,11]).", "startOffset": 24, "endOffset": 27}, {"referenceID": 8, "context": "(though parallel dither [6] might help as in [10,11]).", "startOffset": 45, "endOffset": 52}, {"referenceID": 9, "context": "(though parallel dither [6] might help as in [10,11]).", "startOffset": 45, "endOffset": 52}, {"referenceID": 0, "context": "In this paper, following the discrete signal processing interpretation of deep neural networks [1,4-6], we have interpreted ReLU in terms of demodulation and we have introduced the concept of decoy features to capture the action of a DNN learning artefactual features produced by abrupt activation-function nonlinearities.", "startOffset": 95, "endOffset": 102}, {"referenceID": 2, "context": "In this paper, following the discrete signal processing interpretation of deep neural networks [1,4-6], we have interpreted ReLU in terms of demodulation and we have introduced the concept of decoy features to capture the action of a DNN learning artefactual features produced by abrupt activation-function nonlinearities.", "startOffset": 95, "endOffset": 102}, {"referenceID": 3, "context": "In this paper, following the discrete signal processing interpretation of deep neural networks [1,4-6], we have interpreted ReLU in terms of demodulation and we have introduced the concept of decoy features to capture the action of a DNN learning artefactual features produced by abrupt activation-function nonlinearities.", "startOffset": 95, "endOffset": 102}, {"referenceID": 4, "context": "In this paper, following the discrete signal processing interpretation of deep neural networks [1,4-6], we have interpreted ReLU in terms of demodulation and we have introduced the concept of decoy features to capture the action of a DNN learning artefactual features produced by abrupt activation-function nonlinearities.", "startOffset": 95, "endOffset": 102}, {"referenceID": 4, "context": "We have illustrated the ability of parallel dither [6,5] to suppress these decoy features and hence to regularise a ReLU DNN.", "startOffset": 51, "endOffset": 56}, {"referenceID": 3, "context": "We have illustrated the ability of parallel dither [6,5] to suppress these decoy features and hence to regularise a ReLU DNN.", "startOffset": 51, "endOffset": 56}, {"referenceID": 0, "context": "We have also shown that the demodulation advantge of the ReLU over the traditional smooth activation functions is reversed when compared with the biased-sigmoid [1] (which is explicitly optimised for demodulation).", "startOffset": 161, "endOffset": 164}], "year": 2015, "abstractText": "Rectified Linear Units (ReLU) seem to have displaced traditional \u2018smooth\u2019 nonlinearities as activationfunction-du-jour in many \u2013 but not all deep neural network (DNN) applications. However, nobody seems to know why. In this article, we argue that ReLU are useful because they are ideal demodulators \u2013 this helps them perform fast abstract learning. However, this fast learning comes at the expense of serious nonlinear distortion products decoy features. We show that Parallel Dither acts to suppress the decoy features, preventing overfitting and leaving the true features cleanly demodulated for rapid, reliable learning.", "creator": "PDFCreator Version 1.7.1"}}}