{"id": "1509.05371", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "17-Sep-2015", "title": "DeXpression: Deep Convolutional Neural Network for Expression Recognition", "abstract": "We propose a convolutional neural network (CNN) architecture for facial expression recognition. The proposed architecture is independent of any hand-crafted feature extraction and performs better than the earlier proposed convolutional neural network based approaches. We visualize the automatically extracted features which have been learned by the network in order to provide a better understanding. The standard datasets, i.e. Extended Cohn-Kanade (CKP) and MMI Facial Expression Databse are used for the quantitative evaluation. On the CKP set the current state of the art approach, using CNNs, achieves an accuracy of 99.2%. For the MMI dataset, currently the best accuracy for emotion recognition is 93.33%. The proposed architecture achieves 99.6% for CKP and 98.63% for MMI, therefore performing better than the state of the art using CNNs. Automatic facial expression recognition has a broad spectrum of applications such as human-computer interaction and safety systems. This is due to the fact that non-verbal cues are important forms of communication and play a pivotal role in interpersonal communication. The performance of the proposed architecture endorses the efficacy and reliable usage of the proposed work for real world applications.", "histories": [["v1", "Thu, 17 Sep 2015 18:49:10 GMT  (1905kb,D)", "http://arxiv.org/abs/1509.05371v1", "Under consideration for publication in Pattern Recognition Letters"], ["v2", "Wed, 17 Aug 2016 19:34:55 GMT  (1905kb,D)", "http://arxiv.org/abs/1509.05371v2", "Under consideration for publication in Pattern Recognition Letters"]], "COMMENTS": "Under consideration for publication in Pattern Recognition Letters", "reviews": [], "SUBJECTS": "cs.CV cs.LG", "authors": ["peter burkert", "felix trier", "muhammad zeshan afzal", "reas dengel", "marcus liwicki"], "accepted": false, "id": "1509.05371"}, "pdf": {"name": "1509.05371.pdf", "metadata": {"source": "CRF", "title": "DeXpression: Deep Convolutional Neural Network for Expression Recognition", "authors": ["Peter Burkert", "Felix Trier", "Muhammad Zeshan Afzal", "Andreas Dengel", "Marcus Liwicki"], "emails": ["burkert11@cs.uni-kl.de,", "trier10@cs.uni-kl.de,", "afzal@iupr.com,", "andreas.dengel@dfki.de,", "liwicki@dfki.uni-kl.de"], "sections": [{"heading": "1 INTRODUCTION", "text": "This year, it will be able to put itself at the top of the group."}, {"heading": "2 RELATED WORK", "text": "In fact, it is so that most of them are able to obey the rules which they have imposed on themselves. (...) Indeed, it is so that they are able to determine for themselves what they want to do. (...) It is as if they do not do it. (...) It is as if they do not. (...) It is as if they do. (...) It is as if they want to do it. (...). (...). (...). (...). (.). (.). (.). (.). (.). (.). (.). (.). (.). (.). (.). (.). (.). (.). (.). (.). (.). (.). (.). (.). (.). (.). (.). (.). (.). (.). (.). (.). (.). (.). (.). (.). (.).). (.). (.). (.). (.).). (.). (.).). (.).). (.).). (.). (.).). (.).). (.). (.).).). (.). (.).).). (.).). (.). (.). (.).). (.). (.). (.).). (.). (.).). (.). (.).). (.).). (.). (.). (.).). (.). (.).). (. (.).).). (.). (.).). (.).).).). (. (.).).). (.).). (.).).). (.).).). (.).).).).). (). (). ().). ().).). ().).). ().)"}, {"heading": "3 CONVOLUTIONAL NEURAL NETWORKS", "text": "Let fk apply the filter with a core size of n \u00b7 m to the input x. n \u00b7 m is the number of input connections each CNN neuron has. The resulting output of the plane is calculated as follows: C (xu, v) = n 2 \u2211 i = \u2212 n2m 2 \u2211 j = \u2212 m2fk (i, j) xu \u2212 i, v \u2212 j (1) To calculate a richer and more varied representation of the input, several filters fk can be applied to the input using k \u00b2 N. The filters fk are realized by dividing the weights of adjacent x x cells. This has the positive effect that lower weights need to be trained as opposed to standard multi-layer perceptrons, since multiple weights are bound to the input weights.Max Pooling: Max Pooling: Max Pooling: Max Pooling reduces the input by applying the maximum function via the input xi, then type the output as follows:"}, {"heading": "4 DATASETS", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.1 MMI Dataset", "text": "The MMI data set introduced by Pantic et al. [16] contains over 2900 videos and images of 75 people. The annotations contain action units and emotions. The database contains a web interface with an integrated search for scanning the database. The videos / images are coloured. People are of different ages, genders and ethnicities. The emotions studied are the six basic emotions: anger, disgust, fear, happiness, sadness, surprise."}, {"heading": "4.2 CKP Dataset", "text": "This data set was introduced by Lucey et al. [12]. 210 persons aged 18 to 50 years were recorded representing emotions. This data set contains records of emotions of 210 persons aged 18 to 50 years. Both female and male persons are present and from different backgrounds. 81% are European Americans and 13% African Americans. The images are of the size 640 x 490 px and 640 x 480 px. They are both gray and colored. In total, this set has 593 emotionally marked sequences. Emotions consist of anger, disgust, fear, happiness, sadness, surprise and contempt."}, {"heading": "4.3 Comparison", "text": "In the MMI data set (fig. 2), the emotion of anger is represented in different ways, as can be seen in the eyebrows, forehead and mouth. The mouth in the lower picture is tightly closed, while the mouth is open in the upper picture. In disgust, the differences are also visible, as the woman in the upper picture shows a much stronger reaction. The man who represents fear has eyebrows that slightly cover the eyes. Such differences also occur in the CKP set (fig. 3). In anger, both people smile strongly. In the lower picture, the woman who shows sadness shows a stronger reaction to the lips and chin. The last emotion surprise also has differences, such as the openness of the mouth. in anger, the eyebrows and cheeks differ greatly. In disgust, greater differences can be seen. In the upper picture, not only the curvature of the mouth is stronger, but the nose is also more involved."}, {"heading": "5 PROPOSED ARCHITECTURE", "text": "The proposed deep Convolutional Neural Network Architecture (shown in Figure 4) consists of four parts. The first part automatically pre-processes the data, starting with Convolution 1, which applies 64 different filters; the next layer is Pooling 1, which projects the images downwards and then normalizes them by LRN 1; the next steps are the two FeatEx (Parallel Feature Extraction Block) blocks, highlighted in Figure 4. They are the core of the proposed architecture and described later in this section. The features extracted from these blocks are forwarded to a fully connected layer, which it uses to classify the input into the different emotions; the architecture described is compact, which makes them not only quick to train, but also suitable for real-time applications; this is also important because the network with resource usage of at least FeatEx was built; the key structure in our architecture is the parallel Feature Extraction Block (FeatEx)."}, {"heading": "6 EXPERIMENTS AND RESULTS", "text": "As an implementation, Caffe [17] was used, which is a deep learning framework maintained by the Berkeley Vision and Learning Center (BVLC).CKP: The CKP database has been analyzed many times and many different approaches have been evaluated to \"solve\" this sentence. To determine whether the architecture is competitive, it was evaluated on the CKP dataset. For the experiments, all 5,870 annotated images were used to perform a 10-fold cross-validation; the proposed architecture has proven to be very effective with this dataset, with an average accuracy of 99.6%. InTable 2 different state-of-the-art results are listed as a comparison; the 100% accuracy reported by Zafar [14] is based on hand-picked images; the results are not validated by cross-validation; the confusion matrix in Fig. 6a shows the results and shows that some emotions are perfectly recognisable."}, {"heading": "7 DISCUSSION", "text": "The accuracy on the CKP set shows that the chosen approach is robust, misclassification usually occurs on images that are the first few examples of a emotion sequence. Often, these images depict a neutral facial expression. Therefore, these misclassifications are not necessarily an error in the approach, but in the data selection. Otherwise, no major problem could be found. Emotion surprise is often confused with disgust at a rate of 0.045%, which is highest. Of these images, in which emotion is present, only a few are misclassified. As there is no consent for the misclassified images, they cannot be displayed here. However, some unique names are provided: Image S119 001 00000010 is classified as fear, while the commented emotion corresponds to Surprise. The image shows a person with wide open mouth and open eyes. Images representing surprise are often very similar, as the persons have wide open mouth and eyes."}, {"heading": "8 CONCLUSION AND FUTURE WORK", "text": "This article introduces DeXpression, which works fully automatically. It is a neural network that requires little computational effort compared to current CNN architectures. To create it, the newly assembled structure FeatEx was introduced. It consists of several revolutionary layers of different sizes as well as Max Pooling and ReLU layers. FeatEx creates a rich feature representation of the input. The results of the 10-fold cross-validation result in an average detection accuracy of 99.6% on the CKP dataset and 98.36% on the MMI dataset. This shows that the proposed architecture is capable of competing with the current state of the art in the field of emotion detection. In Section 7, the analysis has shown that DeXpression works without major errors. Most of the misclassifications occurred during the first few images of an emotion sequence. Often, emotions are not repressible in these images."}, {"heading": "ACKNOWLEDGMENTS", "text": "We thank the Affect Analysis Group of the University of Pittsburgh for providing the Extended CohnKanade database and Prof. Pantic and Dr. Valstar for providing the MMI database."}], "references": [{"title": "A real-time automated system for recognition of human facial expressions", "author": ["K. Anderson", "P.W. Mcowan"], "venue": "IEEE Trans. Syst., Man, Cybern. B, Cybern, pp. 96\u2013105, 2006.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2006}, {"title": "Facial expression recognition in image sequences using geometric deformation features and support vector machines", "author": ["I. Kotsia", "I. Pitas"], "venue": "Image Processing, IEEE Transactions on, vol. 16, no. 1, pp. 172\u2013187, Jan 2007.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2007}, {"title": "Face expression recognition and analysis: the state of the art", "author": ["B.V. Kumar"], "venue": "Course Paper, Visual Interfaces to Computer, 2009.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2009}, {"title": "Automatic facial expression recognition using features of salient facial patches", "author": ["S. Happy", "A. Routray"], "venue": "Affective Computing, IEEE Transactions on, vol. 6, no. 1, pp. 1\u201312, Jan 2015.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2015}, {"title": "Decaf: A deep convolutional activation feature for generic visual recognition", "author": ["J. Donahue", "Y. Jia", "O. Vinyals", "J. Hoffman", "N. Zhang", "E. Tzeng", "T. Darrell"], "venue": "arXiv preprint arXiv:1310.1531, 2013.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2013}, {"title": "Going deeper with convolutions", "author": ["C. Szegedy", "W. Liu", "Y. Jia", "P. Sermanet", "S. Reed", "D. Anguelov", "D. Erhan", "V. Vanhoucke", "A. Rabinovich"], "venue": "CoRR, vol. abs/1409.4842, 2014. [Online]. Available: http://arxiv.org/abs/1409.4842", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2014}, {"title": "Face expression recognition: A brief overview of the last decade", "author": ["C.-D. Caleanu"], "venue": "Applied Computational Intelligence and Informatics (SACI), 2013 IEEE 8th International Symposium on. IEEE, 2013, pp. 157\u2013161.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2013}, {"title": "Face expression recognition and analysis: the state of the art", "author": ["V. Bettadapura"], "venue": "arXiv preprint arXiv:1203.6722, 2012.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2012}, {"title": "Deep learning for realtime robust facial expression recognition on a smartphone", "author": ["I. Song", "H.-J. Kim", "P.B. Jeon"], "venue": "Consumer Electronics (ICCE), 2014 IEEE International Conference on. IEEE, 2014, pp. 564\u2013567.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2014}, {"title": "The extended cohn-kanade dataset (ck+): A complete dataset for action unit and emotion-specified expression", "author": ["P. Lucey", "J. Cohn", "T. Kanade", "J. Saragih", "Z. Ambadar", "I. Matthews"], "venue": "Computer Vision and Pattern Recognition Workshops (CVPRW), 2010 IEEE Computer Society Conference on, June 2010, pp. 94\u2013101.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2010}, {"title": "Facial expression recognition based on local binary patterns: A comprehensive study", "author": ["C. Shan", "S. Gong", "P.W. McOwan"], "venue": "Image and Vision Computing, vol. 27, no. 6, pp. 803 \u2013 816, 2009.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2009}, {"title": "Face recognition with expression variation via robust ncc", "author": ["A. Zafer", "R. Nawaz", "J. Iqbal"], "venue": "Emerging Technologies (ICET), 2013 IEEE 9th International Conference on, Dec 2013, pp. 1\u20135.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2013}, {"title": "Deep sparse rectifier neural networks", "author": ["X. Glorot", "A. Bordes", "Y. Bengio"], "venue": "Proceedings of the Fourteenth International Conference on Artificial Intelligence and Statistics (AISTATS-11), G. J. Gordon and D. B. Dunson, Eds., vol. 15. Journal of Machine Learning Research - Workshop and Conference Proceedings, 2011, pp. 315\u2013 323.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2011}, {"title": "Webbased database for facial expression analysis", "author": ["M. Pantic", "M.F. Valstar", "R. Rademaker", "L. Maat"], "venue": "Proceedings of IEEE Int\u2019l Conf. Multimedia and Expo (ICME\u201905), Amsterdam, The Netherlands, July 2005, pp. 317\u2013321.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2005}, {"title": "Caffe: Convolutional architecture for fast feature embedding", "author": ["Y. Jia", "E. Shelhamer", "J. Donahue", "S. Karayev", "J. Long", "R. Girshick", "S. Guadarrama", "T. Darrell"], "venue": "arXiv preprint arXiv:1408.5093, 2014.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2014}, {"title": "Deep learning for real-time robust facial expression recognition on a smartphone", "author": ["I. Song", "H.-J. Kim", "P. Jeon"], "venue": "Consumer Electronics (ICCE), 2014 IEEE International Conference on, Jan 2014, pp. 564\u2013567.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2014}, {"title": "Static topographic modeling for facial expression recognition and analysis", "author": ["J. Wang", "L. Yin"], "venue": "Comput. Vis. Image Underst., vol. 108, no. 1-2, pp. 19\u201334, Oct. 2007.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2007}], "referenceMentions": [{"referenceID": 0, "context": "One example is the Software called EmotiChat [1].", "startOffset": 45, "endOffset": 48}, {"referenceID": 1, "context": "This can be realized through manual inference [2] or an automatic detection approach [1].", "startOffset": 46, "endOffset": 49}, {"referenceID": 0, "context": "This can be realized through manual inference [2] or an automatic detection approach [1].", "startOffset": 85, "endOffset": 88}, {"referenceID": 2, "context": "Multiple activations of AUs describe the facial expression [3].", "startOffset": 59, "endOffset": 62}, {"referenceID": 1, "context": "[2].", "startOffset": 0, "endOffset": 3}, {"referenceID": 3, "context": "Detecting such landmarks can be hard, as the distance between them differs depending on the person [4].", "startOffset": 99, "endOffset": 102}, {"referenceID": 3, "context": "When a face shows an emotion the structure changes and different filters can be applied to detect this [4].", "startOffset": 103, "endOffset": 106}, {"referenceID": 4, "context": "ANN and have been shown to work well as feature extractor when using images as input [5] and are real-time capable.", "startOffset": 85, "endOffset": 88}, {"referenceID": 5, "context": "GoogleNet [6] is a deep neural network architecture that relies on CNNs.", "startOffset": 10, "endOffset": 13}, {"referenceID": 6, "context": "A detailed overview for expression recognition was given by C\u0103leanu [8] and Bettadapura [9].", "startOffset": 68, "endOffset": 71}, {"referenceID": 7, "context": "A detailed overview for expression recognition was given by C\u0103leanu [8] and Bettadapura [9].", "startOffset": 88, "endOffset": 91}, {"referenceID": 5, "context": "[6] have proposed an architecture called GoogLeNet.", "startOffset": 0, "endOffset": 3}, {"referenceID": 3, "context": "2), Happy and Routray [4] classify between six basic emotions.", "startOffset": 22, "endOffset": 25}, {"referenceID": 8, "context": "[11] have used a deep convolutional neural network for learning facial expressions.", "startOffset": 0, "endOffset": 4}, {"referenceID": 9, "context": "[12] have created the Extended CohnKanade dataset.", "startOffset": 0, "endOffset": 4}, {"referenceID": 0, "context": "[1] have developed a face expression system, which is capable of recognizing the six basic emotions.", "startOffset": 0, "endOffset": 3}, {"referenceID": 1, "context": "Kotsia and Pitas [2] detect emotions by mapping a Candide grid, a face mask with a low number of polygons, onto a person\u2019s face.", "startOffset": 17, "endOffset": 20}, {"referenceID": 10, "context": "[13] have created an emotion recognition", "startOffset": 0, "endOffset": 4}, {"referenceID": 11, "context": "[14] proposed an emotion recognition system using Robust Normalized Cross Correlation (NCC).", "startOffset": 0, "endOffset": 4}, {"referenceID": 12, "context": "This function also prevents the vanishing gradient error, since the gradients are linear functions or zero but in no case non-linear functions [15].", "startOffset": 143, "endOffset": 147}, {"referenceID": 0, "context": "Let N be the dimension of the input vector, then Softmax calculates a mapping such that: S(x) : R \u2192 [0, 1] For each component 1 \u2264 j \u2264 N , the output is calculated as follows:", "startOffset": 100, "endOffset": 106}, {"referenceID": 13, "context": "[16] contains over 2900 videos and images of 75 persons.", "startOffset": 0, "endOffset": 4}, {"referenceID": 9, "context": "[12].", "startOffset": 0, "endOffset": 4}, {"referenceID": 14, "context": "As implementation Caffe [17] was used.", "startOffset": 24, "endOffset": 28}, {"referenceID": 11, "context": "The 100% accuracy reported by Zafar [14] is based on hand picked images.", "startOffset": 36, "endOffset": 40}, {"referenceID": 16, "context": "This is better than the accuracies achieved by Wang and Yin [19] (Table 3).", "startOffset": 60, "endOffset": 64}, {"referenceID": 11, "context": "Author Method Accuracy Aliya Zafar [14] NCC 100% Happy et al.", "startOffset": 35, "endOffset": 39}, {"referenceID": 3, "context": "[4] Facial Patches + SVM 94.", "startOffset": 0, "endOffset": 3}, {"referenceID": 9, "context": "[12] AAM + SVM \u2265 80% Song et al.", "startOffset": 0, "endOffset": 4}, {"referenceID": 15, "context": "[18] ANN (CNN) 99.", "startOffset": 0, "endOffset": 4}, {"referenceID": 16, "context": "Author Method Accuracy Wang and Yin [19] LDA 93.", "startOffset": 36, "endOffset": 40}, {"referenceID": 16, "context": "33% Wang and Yin [19] QDC 92.", "startOffset": 17, "endOffset": 21}, {"referenceID": 16, "context": "78% Wang and Yin [19] NBC 85.", "startOffset": 17, "endOffset": 21}], "year": 2017, "abstractText": "We propose a convolutional neural network (CNN) architecture for facial expression recognition. The proposed architecture is independent of any hand-crafted feature extraction and performs better than the earlier proposed convolutional neural network based approaches. We visualize the automatically extracted features which have been learned by the network in order to provide a better understanding. The standard datasets, i.e. Extended Cohn-Kanade (CKP) and MMI Facial Expression Databse are used for the quantitative evaluation. On the CKP set the current state of the art approach, using CNNs, achieves an accuracy of 99.2%. For the MMI dataset, currently the best accuracy for emotion recognition is 93.33%. The proposed architecture achieves 99.6% for CKP and 98.63% for MMI, therefore performing better than the state of the art using CNNs. Automatic facial expression recognition has a broad spectrum of applications such as human-computer interaction and safety systems. This is due to the fact that non-verbal cues are important forms of communication and play a pivotal role in interpersonal communication. The performance of the proposed architecture endorses the efficacy and reliable usage of the proposed work for real", "creator": "LaTeX with hyperref package"}}}