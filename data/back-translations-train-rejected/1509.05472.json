{"id": "1509.05472", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "17-Sep-2015", "title": "Learning to Hash for Indexing Big Data - A Survey", "abstract": "The explosive growth in big data has attracted much attention in designing efficient indexing and search methods recently. In many critical applications such as large-scale search and pattern matching, finding the nearest neighbors to a query is a fundamental research problem. However, the straightforward solution using exhaustive comparison is infeasible due to the prohibitive computational complexity and memory requirement. In response, Approximate Nearest Neighbor (ANN) search based on hashing techniques has become popular due to its promising performance in both efficiency and accuracy. Prior randomized hashing methods, e.g., Locality-Sensitive Hashing (LSH), explore data-independent hash functions with random projections or permutations. Although having elegant theoretic guarantees on the search quality in certain metric spaces, performance of randomized hashing has been shown insufficient in many real-world applications. As a remedy, new approaches incorporating data-driven learning methods in development of advanced hash functions have emerged. Such learning to hash methods exploit information such as data distributions or class labels when optimizing the hash codes or functions. Importantly, the learned hash codes are able to preserve the proximity of neighboring data in the original feature spaces in the hash code spaces. The goal of this paper is to provide readers with systematic understanding of insights, pros and cons of the emerging techniques. We provide a comprehensive survey of the learning to hash framework and representative techniques of various types, including unsupervised, semi-supervised, and supervised. In addition, we also summarize recent hashing approaches utilizing the deep learning models. Finally, we discuss the future direction and trends of research in this area.", "histories": [["v1", "Thu, 17 Sep 2015 23:19:07 GMT  (2730kb)", "http://arxiv.org/abs/1509.05472v1", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["jun wang", "wei liu", "sanjiv kumar", "shih-fu chang"], "accepted": false, "id": "1509.05472"}, "pdf": {"name": "1509.05472.pdf", "metadata": {"source": "CRF", "title": "Learning to Hash for Indexing Big Data - A Survey", "authors": ["Jun Wang"], "emails": ["j.wang@alibaba-inc.com.", "weiliu@us.ibm.com.", "sanjivk@google.com."], "sections": [{"heading": null, "text": "The question that has arisen in recent years is whether this is a way in which the Internet has been massively overloaded in recent decades, a way in which the World Wide Web has 366 million accessible websites, containing more than 100 million tweets per day, and in which Yahoo spreads more than 3 billion messages per day. In addition to the overwhelming textual data that the photo-sharing site Flickr has to offer, images are also being uploaded at the rate of over 3, 000Jun Wang with the Institute of Data Science and Technology at Alibaba Group, Seattle, 98101, USA."}, {"heading": "II. Notations and Background", "text": "In this section, we will first present the notations as summarized in Table I. Then, we will briefly present the conceptual paradigm of hash-based ANN search. Finally, we will present some background information on hashing methods, including the introduction of two well-known randomized hashing techniques."}, {"heading": "A. Notations", "text": "Considering an example x P RD, one can use a series of hash functions H \"th1, \u00a8 \u00a8, hKu to calculate a K-bit binary code y\" ty1, \u00a8 \u00a8 \u00a8, yKu for x asy \"th1pxq, \u00a8 \u00a8 \u00a8, h2pxq, \u00a8 \u00a8, hKpxqu,\" (1) in which the kth bit is calculated as yk \"hkpxq.\" The hash function performs the mapping as hk: RD \"B. Such a binary encoding process can also be regarded as mapping the original data to a binary estimated space, namely as\" hamming space: H: x\u00d1th1pxq, \"\u00a8 \u00a8,\" hKpxqu. \"(2) In view of a series of hash functions, we can search all elements in the database\" txnu N, \"hammhammhkq,\" \"1qqu,\" Xyu, \"Xyqu,\" Xyu, \"Xyqu,\" Xyu, \"Xyqu,\" Xyu, \"Xyu,\" Xyqu, \"Xyu,\" Xyqu, \"Xyu,\" Xyu, \"Xyu,\" Xyu, \"Xyu,\" Xyu, \"Xyu."}, {"heading": "B. Pipeline of Hashing-based ANN Search", "text": "This year, it is closer than ever before to being able to take the lead."}, {"heading": "C. Randomized Hashing Methods", "text": "In this section we briefly review two categories of randomized methods, i.e. random projection based on random permutation-based approaches. C.1 Random Projection Based HashingAs is a representative member of the LSH family, random projection of RPH functions near the room where the cards are located is a representative of the LSH family, random projection based on hash functions in different application areas. \"Random projection based hashingAs is a representative member of the LSH family, random projection based hash functions have been used in different application areas.\" Random projection based hashingAs is a representative member of the LSH family."}, {"heading": "III. Categories of Learning Based Hashing Methods", "text": "Among the three key steps in hash-based ANN search, the design of improved data-dependent hash functions is at the center of learning about the hash paradigm. Since LSH's proposal in [58], many new hash techniques have been developed. Note that most of the emerging hash methods focus on improving search performance using a single hash table. This is because these techniques expect to learn compact discriminatory codes, so that searching within a small hamming ball of the query, or even an exhaustive scan in the hamming space, is both fast and precise. Therefore, we will focus primarily on various techniques and algorithms for designing a single hash table. In particular, we offer different perspectives such as the learning paradigm and the properties of the hash function to categorize the recently developed hash approaches. It is worth noting that some recent studies have shown that exploring the power of multiple hash tables can sometimes produce a hash reading."}, {"heading": "A. Data-Dependent vs. Data-Independent", "text": "Depending on whether the design of hash functions requires analysis of a given data set, there are two high-level categories of hash techniques: data-independent and data-dependent. As one of the most popular data-independent approaches, random projection is widely used for designing data-independent hash techniques such as LSH and SIKH. LSH is arguably the most popular hash method and has been applied to a variety of problem areas, including information gathering and computer vision. Both LSH and SIKH randomly sampled the projection vector w and intersection b, as defined in Eq.4. Although these methods provide strict performance guarantees, they are less efficient because the hash functions are not specifically designed for a particular data set or search task. Based on the random projection scheme, several efforts have been made to improve the performance of the LSH method. [37] [39] [41] Restricting the data-based functions to a certain data set type below three may be more efficient."}, {"heading": "B. Unsupervised, Supervised, and Semi-Supervised", "text": "Many emerging hashing techniques are designed by exploiting various machine learning paradigms, ranging from unattended and monitored to semi-monitored. Representative unsupervised methods include spectral hashing [32], graph hashing [28], manifold hashing [60], iterative quantization hashing [61], kernel location-sensitive hashing [41] [21], isotropic hashing [62], angular quantization hashing [63], and spherical hashing [64]. Under these approaches, spectral hashing examines data distribution and graph hashing uses the underlying manifold structure of data captured by a graphical representation. Furthermore, supervised learning paradigms ranging from kernel learning to deep learning are exploited to learn binary codes."}, {"heading": "C. Pointwise, Pairwise, Triplet-wise and Listwise", "text": "Based on the degree of monitoring, the monitored or semi-monitored hash methods q can be grouped into several subcategories, including pointwise, pairwise, tripletwise and listwise approaches. For example, some existing approaches use semantic attributes or labels at the instance level to design the hash functions [71] [72]. In addition, learning methods based on pairwise monitoring have been extensively investigated, and many hash techniques have been proposed [19] [22] [65] [66] [14] [69] [70] [73]. As demonstrated in Figure 4 (a), the pair px2, x3q contains similar points and the other two pairs px1, x2q and px3q contain different points. Such relationships are considered in the learning process to obtain the pairwise label information in the learned hammering space x."}, {"heading": "D. Linear vs. Nonlinear", "text": "This year is the highest in the history of the country."}, {"heading": "E. Single-Shot Learning vs. Multiple-Shot Learning", "text": "In learning-based hash methods, one first formulates an objective function that reflects the desired properties of the hash codes. In a single-line learning paradigm, the optimal solution is derived by optimizing the objective function in a single shot. In such a hash learning framework, the hash functions are learned simultaneously. In contrast, the multi-line learning method considers a global goal, but optimizes a hash function taking into account the bias generated by the previous hash functions. Such a procedure trains hash functions one bit at a time [83] [25] [84]. Multi-line hash learning is often used in supervised or semi-supervised settings, since the given label information can be used to assess the quality of hash functions learned in previous steps. Thus, for example, the sequential hash-based hashing aims to integrate the bit correlations by iteratively updating the characteristic weights of higher matrix."}, {"heading": "F. Non-Weighted vs. Weighted Hashing", "text": "In view of the hamming embedding defined in Eq.2, traditional hashing-based indexing schemes map the original data into an unweighted hamming space, to which each bit contributes equally. In view of such mapping, the hamming distance is calculated by counting the number of different bits. However, it is easy to observe that different bits often behave differently [14]. Generally, the binary code generated from large variance projections was developed for weighted embedding hash codes in order to improve the differentiation between hash codes. Techniques were developed to learn weighted hamming embedding of hash codes."}, {"heading": "IV. Methodology Review and Analysis", "text": "In this section, we will focus on reviewing several representative hashing methods that explore various machine learning techniques to design data-specific indexing schemes, which consist of unattended, semi-supervised, and supervised hashing, including spectral hashing, anchor chart hashing, angle quantization, binary reconstructive embedding based on hashing, metric learning-based hashing, semi-supervised hashing, column generation hashing, and ranking-supervised hashing. Table II summarizes the hashing techniques studied and their technical merits. Note that this section mainly focuses on describing the intuition and formulation of each method, as well as discussing its advantages and disadvantages. The performance of each method strongly depends on practical settings, including learning parameters and datasets themselves. Generally, non-linear and supervised techniques tend to perform better than linear and unsupervised methods [more cost-supervised methods] [21, they are more technical]."}, {"heading": "A. Spectral Hashing", "text": "In the formulation of spectral hashing, the desired property is seen as keeping neighbors in the entrance area as neighbors in the mutton space and making the codes balanced and uncorrelated [32]. Therefore, the goal of spectral hashing is formulated as follows: min \u00ffij1 2 Aij} yi'j} 2 \"2 trpYJLYq (15) is subjected to: Y P t '1,1uN\u0445 K1Jyk \u00a8\" 0, k \"1,\" KYJY \"nIK\u0445 K, where A\" tAiju N i, j \"1 is a matrix of similarities in pairs and the Laplacian matrix is calculated as L\" diagpA1q \"A. The constraint 1Jyk\" 0 ensures that the hash bit yk achieves a balanced partitioning of the data and the constraint YJY \"nIK.K imposes orthogonality between hash bits to minimize the problem."}, {"heading": "B. Anchor Graph Hashing", "text": "Following a similar objective to spectral hashing, the anchor diagram hashing was designed to solve the problem from a different perspective, without assuming a uniform distribution [28]. Note that the decisive bottleneck in solving Eq. 15 is the cost of building a pariwsie similarity diagram A, the calculation of the accompanying Laplacian diagram, and the solution of its own system, which has at least a square complexity. The key idea is to use a small set of MpM! Nq anchor techniques to approach the graph structure represented by Matrix A in such a way that the similarity between any pair of dots can be approximated using point-to-anchor similarities."}, {"heading": "C. Angular Quantization Based Hashing", "text": "Since similarity is often measured by the cosine of the angle between sample pairs, angular quantization is therefore proposed to map non-negative feature vectors to a vertex of the binary hypercube with the smallest angle [63]. In such a setting, the vertices of the hypercube are treated as quantization markers that grow exponentially with the data dimension D. As shown in Figure 7, the next binary vertex b in a hypercube up to the data point x by b \u02da \"argmax bbJx} b} 2 (17) can be found subject to: b P t0.1uK, although it is a holistic programming problem, its global maximum can be found with a complexity of OpD logDq. The optimal binary binary codes are used as binary hash codes for data points as y\" b \"formulas combinary numbers. Based on this canulary quantization framework, a number jumper is designed to follow the number matrix of RD with a number matrix dependent on R\u02da D, whereby a number matrix to R\u02da D is calculated."}, {"heading": "D. Binary Reconstructive Embedding", "text": "Instead of data-independent random projections such as in LSH or major components such as in SH, Kulis and Darrell [27], proposed data-dependent and bit-correlated hash functions such as: hkpxq \"sgn\" s \u00ffq \"1Wkq\u03bapxkq,\" xq \"(19) The sample sentence txkqu,\" q \"1,\" \"s is the training data for the learning hash function hk\" and \"p\" q is a core function, and W is a weight matrix. Based on the above formulation, a method called Binary Reconstructive Embedding (BRE) has been developed to minimize a cost function for measuring the difference between the metric and reconstructed distance in the hamming space. Euclidean metric dM and the binary reconstruction distance dR are defined as: dMpxi, xi \"xi\" xi \"xj\" 2 (20) Rdpxi \",\" qqKK \"1phKK."}, {"heading": "E. Metric Learning based Hashing", "text": "The key idea for the metric learning method is based on learning a parameterized Mahalanoq metric with pairwise identification information. Such learned metrics are then used for standard random projection based on hash functions [19]. The goal is to preserve the pairwise relationship in the binary code space, where similar data pairs more or less collide in the same hash buck and unequal pairs are less likely to share the same hash codes as in Figure 8. The parameterized inner product is defined as assimpxi, xjq \"x J i Mxj, where M is a positively defined dump-d matrix to be learned from the labeled data. Note that this similarity measurement corresponds to the parameterized square Mahalanobis distance dM. Assume that M is equal as the factored distance between JG and JG."}, {"heading": "F. Semi-Supervised Hashing", "text": "Monitored hashing techniques have been shown to be better than uncontrolled approaches because they use the supervisory information to design task-specific hash codes. However, the human annotation process is often costly for a typical setting of large-scale problems, and the labels can be loud and sparse, which could easily lead to overfitting. However, when looking at a small set of paired labels and a large amount of unlabeled data, semi-monitored hashing is designed to conceive hash functions with minimal empirical loss while maintaining maximum entropy across the dataset. Suppose that paired labels are two types of labels M and C. A pair of data points pxi and xj P M indicates that the strategies are similar and pxi, and phasqq P C means that xi and xj Hence, the empirical functions of the H-labeled family, is one of hash functions."}, {"heading": "G. Column Generation Hashing", "text": "Beyond the pair relationship, complex methods such as ranking triplets and ranking lists are used to learn q q functions with the property of maintaining rank. In many real-world applications such as image retrieval and recommendation systems, it is often easier to obtain relative comparison rather than incessant or pari-wise labels q.For a general assertion, such relative comparison information is presented in a threefold form. Formally, a set of triplets is presented as: E \"tpqi, x\" i, x \"i q | simpqi, x\" i qu, where the function simp \u00a8 q could be an unknown unit of similarity. Therefore, the triplet pqi, x \"i\" i q indicates that the sample point x'i-simpqi is semantically more similar or closer to a query point q than the point x'i weighting, as shown in Figure 4 (b)."}, {"heading": "H. Ranking Supervised Hashing", "text": "Unlike other methods that examine the triplet relationship [74] [75] [65], the ranking of the hashing method is an attempt to obtain the ranking of a set of database points that correspond to the query point [76]. Suppose that the training point X \"txnu hasq q q q q q points with xn P RD. Furthermore, a query set is called Q\" tqm, and qm P RD, m \"1, r m,\" M. \"Each element rmn falls within the integer range r1, N\" qq \"s and no two elements share the same value for the exact ranking case."}, {"heading": "I. Circulant Binary Embedding", "text": "Recognizing that most of the current hash techniques are based on linear projections, which could suffer from very high computing and storage costs for high-dimensional data, the circular binary embedding has recently been developed to meet such a challenge using circular projection. [81] In short, we can generate the corresponding circular matrix R \"circprq [101] with a vector r\" tr0, \"\u00a8, rd '1u. Therefore, the binary embedding with circular projection is defined as: hpxq\" sgnpRxq \"sgnpcircprq\" xq. \"(32) Since the circular projection circprqx corresponds to the circular convolution rgx, the calculation of linear projection can finally be reduced to any number of linear uses using fast Fourier transformations ascircprqx\" rgx \"F '1 pFprq \u02dd Fpxqq\" (33)."}, {"heading": "V. Deep Learning for Hashing", "text": "This year is the highest in the history of the country."}, {"heading": "VI. Advanced Methods and Related Applications", "text": "In this section, we expand the scope of the survey with a few more advanced hashing methods designed for specific settings and applications, such as Pointto hyperplane hashing, subspace hashing, and multimodality hashing."}, {"heading": "A. Hyperplane Hashing", "text": "Distinguishable from the conventional hashing techniques previously recorded, hyperashing is actually quite important for many tried-and-tested machine learning methods. \"The problem with fast point-to-point hashing methods is the fast point-to-point search for the next neighborhood level (see Figure 12 (b)), where the query is a hyperplane that is difficult to achieve because point-to-hyperplane distances are quite different from routine point-to-point distances. Despite much research on point-to-point hashing, this particular hashing paradigm is rarely touched upon. For convenience, we call point-to-hyperplane hashing the hyperplane."}, {"heading": "B. Subspace Hashing", "text": "Beyond the conventional hashing mentioned above, which deals with the search in a database of vectors, both traditional spatial concepts and subspace hashing [119], which has rarely been explored in literature, attempts are made to efficiently search a large database of subspaces. Subspace representation is widely used in many computer visions, pattern recognition and statistical learning problems, such as subspace representations of image fields, image sets, video clips, etc. For example, facial images of the same subject with fixed poses but different illuminations are often assumed to be near linear subspaces. A common application scenario is the use of a single-area image to find the subspace (and the associated subject ID) closest to the query image [120]. In the face of a query in the form of vector or subspace, the search for a nearest subspace in a subspace database is often encountered in a subspace database of subspatial elements, including the subspatial motions and multiplicity."}, {"heading": "C. MultiModality Hashing", "text": "Note that the majority of hash learning methods for the construction of hamming embedding are designed for a single modality or representation. Some more recent advanced methods are proposed to design hash functions for more complex settings, such as for data to be represented by multimodal features or for data to be formed in heterogeneous ways [121]. Such types of hashing methods are closely related to social network applications, whether multimodality and heterogeneity are commonly observed. In the following, we will examine several representative methods that have been recently proposed. Realizing that data items like webpage can be described from multiple information sources, composing hashing was recently proposed to design hashing snams using multiple information sources [122]. Besides the intuitive way of concatenating multiple features to derive hash functions, the author also presented an iterative weighting scheme and formulated modulvative modulvative modulvity to achieve modulvative modulvative modulvity-modulvity similarity between the hamming functions]."}, {"heading": "D. Applications with Hashing", "text": "Indexing massive multimedia data such as images and videos are the natural applications for learning-based hashing. Particularly due to the known semantic gap, monitored and semi-monitored hashing methods have been extensively studied for image search and restoration [69] [29] [41] [129] [130] [131], mobile product search [132], and other closely related computer vision applications include image patch matching [133], image classification [118], face recognition [135], estimation [71], object tracking [136], and data dual recognition [127] [52] [138]. In addition, this emerging hash learning framework can be used for some general machine learning and data mining tasks, including cross-modality data fusion [139], large-scale data optimization [140], large-scale classification and regression [141], collaborative filtering [142] and uncomplicated recommendations [143]."}, {"heading": "VII. Open Issues and Future Directions", "text": "Despite enormous progress in developing a variety of hash techniques, several important questions remain. First, unlike the location-sensitive hash family, most learning-based hash techniques are not based on theoretical guarantees for the quality of returned neighbors. Although several newer techniques have provided a theoretical analysis of collision probability, they are largely based on randomized hash functions [116] [117]. Therefore, it is highly desirable to further investigate such theoretical properties. Second, compact hash codes have been largely studied for large-scale retrieval problems. Due to their compact form, the hash codes also have great potential in many other large-scale data modeling tasks, such as efficient nonlinear kernel SVM classifiers [148] and fast kernel approximation [149]. A larger question is: Instead of using the original data, one can use compact code processes directly to perform unsupervised learning or unsupervised learning."}], "references": [{"title": "Image retrieval: Ideas, influences, and trends of the new age", "author": ["R. Datta", "D. Joshi", "J. Li", "J.Z. Wang"], "venue": "ACM Computing Surveys, vol. 40, no. 2, pp. 1\u201360, 2008.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2008}, {"title": "Nearest-neighbor methods in learning and vision: theory and practice", "author": ["G. Shakhnarovich", "T. Darrell", "P. Indyk"], "venue": null, "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2006}, {"title": "Dynamic programming, ser. Rand Corporation research study", "author": ["R. Bellman"], "venue": null, "citeRegEx": "3", "shortCiteRegEx": "3", "year": 1957}, {"title": "Multidimensional binary search trees used for associative searching", "author": ["J. Bentley"], "venue": "Communications of the ACM, vol. 18, no. 9, p. 517, 1975.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 1975}, {"title": "Efficient algorithms with neural network behavior", "author": ["S. Omohundro"], "venue": "Complex Systems, vol. 1, no. 2, pp. 273\u2013347, 1987.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 1987}, {"title": "Satisfying general proximity/similarity queries with metric trees", "author": ["J. Uhlmann"], "venue": "Information Processing Letters, vol. 40, no. 4, pp. 175\u2013179, 1991.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 1991}, {"title": "Data structures and algorithms for nearest neighbor search in general metric spaces", "author": ["P. Yianilos"], "venue": "Proc. of the fourth annual ACM-SIAM Symposium on Discrete algorithms, 1993, pp. 311\u2013321.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 1993}, {"title": "Nearest-neighbor searching in high dimensions", "author": ["P. Indyk"], "venue": "Handbook of discrete and computational geometry, J. E. Goodman and J. O\u2019Rourke, Eds. Boca Raton, FL: CRC Press LLC, 2004.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2004}, {"title": "Product quantization for nearest neighbor search", "author": ["H. Jegou", "M. Douze", "C. Schmid"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 33, no. 1, pp. 117\u2013128, 2011.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2011}, {"title": "Optimized product quantization for approximate nearest neighbor search", "author": ["T. Ge", "K. He", "Q. Ke", "J. Sun"], "venue": "Proceedings of the IEEE International Conference on Computer Vision and Pattern Recognition, Portland, Oregon, USA, 2013, pp. 2946\u20132953.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2013}, {"title": "80 million tiny images: A large data set for nonparametric object and scene recognition", "author": ["A. Torralba", "R. Fergus", "W. Freeman"], "venue": "IEEE Trans. on Pattern Analysis and Machine Intelligence, vol. 30, no. 11, pp. 1958\u20131970, 2008.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 1958}, {"title": "Similarity search in high dimensions via hashing", "author": ["A. Gionis", "P. Indyk", "R. Motwani"], "venue": "Proc. of 25th International Conference on Very Large Data Bases, 1999, pp. 518\u2013529.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 1999}, {"title": "Semi-supervised hashing for large scale search", "author": ["J. Wang", "S. Kumar", "S.-F. Chang"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence, 2012.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2012}, {"title": "Content-based image retrieval at the end of the early years", "author": ["A. Smeulders", "M. Worring", "S. Santini", "A. Gupta", "R. Jain"], "venue": "IEEE Trans. on Pattern Analysis and Machine Intelligence, vol. 22, no. 12, pp. 1349\u20131380, 2000.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2000}, {"title": "A learning framework for nearest neighbor search", "author": ["L. Cayton", "S. Dasgupta"], "venue": "Advances in Neural Information Processing Systems 20, J. Platt, D. Koller, Y. Singer, and S. Roweis, Eds. Cambridge, MA: MIT Press, 2008, pp. 233\u2013240.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2008}, {"title": "Compact hashing with joint optimization of search accuracy and time", "author": ["J. He", "S.-F. Chang", "R. Radhakrishnan", "C. Bauer"], "venue": "IEEE Conference on Computer Vision and Pattern Recognition, Colorado Springs, CO, USA, 2011, pp. 753\u2013760.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2011}, {"title": "Learning task-specific similarity", "author": ["G. Shakhnarovich"], "venue": "Ph.D. dissertation, Massachusetts Institute of Technology, 2005.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2005}, {"title": "Fast similarity search for learned metrics", "author": ["B. Kulis", "P. Jain", "K. Grauman"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 31, no. 12, pp. 2143\u20132157, 2009.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2009}, {"title": "Asymmetric distances for binary embeddings", "author": ["A. Gordo", "F. Perronnin"], "venue": "IEEE Conference on Computer Vision and Pattern Recognition, Colorado Springs, CO, USA, 2011, pp. 729\u2013736.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2011}, {"title": "Kernelized locality-sensitive hashing", "author": ["B. Kulis", "K. Grauman"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence, 2011.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2011}, {"title": "Supervised hashing with kernels", "author": ["W. Liu", "J. Wang", "R. Ji", "Y.-G. Jiang", "S.-F. Chang"], "venue": "Proceedings of the IEEE International Conference on Computer Vision and Pattern Recognition, Providence, RI, USA, 2012, pp. 2074\u20132081.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2012}, {"title": "Compressed hashing", "author": ["Y. Lin", "R. Jin", "D. Cai", "S. Yan", "X. Li"], "venue": "Proceedings of the IEEE International Conference on Computer Vision and Pattern Recognition, Portland, Oregon, USA, 2013, pp. 446\u2013451.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2013}, {"title": "Random maximum margin hashing", "author": ["A. Joly", "O. Buisson"], "venue": "IEEE Conference on Computer Vision and Pattern Recognition, Colorado Springs, CO, USA, 2011, pp. 873\u2013880.", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2011}, {"title": "Sequential projection learning for hashing with compact codes", "author": ["J. Wang", "S. Kumar", "S.-F. Chang"], "venue": "Proceedings of the 27th International Conference on Machine Learning, 2010, pp. 1127\u20131134.", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2010}, {"title": "K-means hashing: An affinitypreserving quantization method for learning binary compact codes", "author": ["K. He", "F. Wen", "J. Sun"], "venue": "Proceedings of the IEEE International Conference on Computer Vision and Pattern Recognition, Portland, Oregon, USA, 2013, pp. 2938\u20132945.", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2013}, {"title": "Learning to Hash with Binary Reconstructive Embeddings", "author": ["B. Kulis", "T. Darrell"], "venue": "Proc. of Advances in Neural Information Processing Systems, Y. Bengio, D. Schuurmans, J. Lafferty, C. K. I. Williams, and A. Culotta, Eds., 2009, vol. 20, pp. 1042\u20131050.", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2009}, {"title": "Hashing with graphs", "author": ["W. Liu", "J. Wang", "S. Kumar", "S.-F. Chang"], "venue": "Proc. of ICML, Bellevue, Washington, USA, 2011.", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2011}, {"title": "Semi-supervised hashing for scalable image retrieval", "author": ["J. Wang", "S. Kumar", "S.-F. Chang"], "venue": "IEEE Computer Society Conference on Computer Vision and Pattern Recognition, San Francisco, USA, 2010, pp. 3424\u20133431.", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2010}, {"title": "Minimal loss hashing for compact binary codes", "author": ["M. Norouzi", "D. Fleet"], "venue": "Proceedings of the 27th International Conference on Machine Learning, 2011.", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2011}, {"title": "Complementary hashing for approximate nearest neighbor search", "author": ["H. Xu", "J. Wang", "Z. Li", "G. Zeng", "S. Li", "N. Yu"], "venue": " 20  PROCEEDINGS OF THE IEEE Proceedings of the IEEE International Conference on Computer Vision, 2011, pp. 1631\u20131638.", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2011}, {"title": "Spectral hashing", "author": ["Y. Weiss", "A. Torralba", "R. Fergus"], "venue": "Proc. of Advances in Neural Information Processing Systems, D. Koller, D. Schuurmans, Y. Bengio, and L. Bottou, Eds. MIT Press, 2008, vol. 21, pp. 1753\u20131760.", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2008}, {"title": "Locality-sensitive binary codes from shift-invariant kernels", "author": ["M. Raginsky", "S. Lazebnik"], "venue": "Advances in Neural Information Processing Systems 22, Y. Bengio, D. Schuurmans, J. Lafferty, C. K. I. Williams, and A. Culotta, Eds. MIT Press, 2009, pp. 1509\u20131517.", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2009}, {"title": "Similarity estimation techniques from rounding algorithms", "author": ["M.S. Charikar"], "venue": "Proceedings of the thiry-fourth annual ACM symposium on Theory of computing. ACM, 2002, pp. 380\u2013388.", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2002}, {"title": "Localitysensitive hashing scheme based on p-stable distributions", "author": ["M. Datar", "N. Immorlica", "P. Indyk", "V. Mirrokni"], "venue": "Proceedings of the twentieth annual Symposium on Computational Geometry, 2004, pp. 253\u2013262.", "citeRegEx": "35", "shortCiteRegEx": null, "year": 2004}, {"title": "LSH forest: self-tuning indexes for similarity search", "author": ["M. Bawa", "T. Condie", "P. Ganesan"], "venue": "Proceedings of the 14th international conference on World Wide Web, Chiba, Japan, 2005, pp. 651\u2013660.", "citeRegEx": "36", "shortCiteRegEx": null, "year": 2005}, {"title": "Multiprobe LSH: efficient indexing for high-dimensional similarity search", "author": ["Q. Lv", "W. Josephson", "Z. Wang", "M. Charikar", "K. Li"], "venue": "Proceedings of the 33rd international conference on Very large data bases, 2007, pp. 950\u2013961.", "citeRegEx": "37", "shortCiteRegEx": null, "year": 2007}, {"title": "Modeling lsh for performance tuning", "author": ["W. Dong", "Z. Wang", "W. Josephson", "M. Charikar", "K. Li"], "venue": "Proceedings of the 17th ACM Conference on Information and Knowledge Management, 2008, pp. 669\u2013678.", "citeRegEx": "38", "shortCiteRegEx": null, "year": 2008}, {"title": "Fast locality-sensitive hashing", "author": ["A. Dasgupta", "R. Kumar", "T. Sarlos"], "venue": "Proceedings of the 17th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, 2011, pp. 1073\u20131081.", "citeRegEx": "39", "shortCiteRegEx": null, "year": 2011}, {"title": "Bayesian locality sensitive hashing for fast similarity search", "author": ["V. Satuluri", "S. Parthasarathy"], "venue": "Proceedings of the VLDB Endowment, vol. 5, no. 5, pp. 430\u2013441, 2012.", "citeRegEx": "40", "shortCiteRegEx": null, "year": 2012}, {"title": "Kernelized locality-sensitive hashing for scalable image search", "author": ["B. Kulis", "K. Grauman"], "venue": "IEEE International Conference on Computer Vision, kyoto, Japan, 2009, pp. 2130 \u2013 2137.", "citeRegEx": "41", "shortCiteRegEx": null, "year": 2009}, {"title": "Weakly-supervised hashing in kernel space", "author": ["Y. Mu", "J. Shen", "S. Yan"], "venue": "IEEE Computer Society Conference on Computer Vision and Pattern Recognition, San Francisco, USA, 2010, pp. 3344\u20133351.", "citeRegEx": "42", "shortCiteRegEx": null, "year": 2010}, {"title": "Super-bit localitysensitive hashing", "author": ["J. Ji", "J. Li", "S. Yan", "B. Zhang", "Q. Tian"], "venue": "inAdvances in Neural Information Processing Systems 25, 2012, pp. 108\u2013116.", "citeRegEx": "43", "shortCiteRegEx": null, "year": 2012}, {"title": "Non-metric locality-sensitive hashing.", "author": ["Y. Mu", "S. Yan"], "venue": "Proceedings of the Twenty-Fourth AAAI Conference on Artificial Intelligence,", "citeRegEx": "44", "shortCiteRegEx": "44", "year": 2010}, {"title": "On the resemblance and containment of documents", "author": ["A. Broder"], "venue": "Compression and Complexity of Sequences Proceedings, 1997, pp. 21\u201329.", "citeRegEx": "45", "shortCiteRegEx": null, "year": 1997}, {"title": "Min-wise independent permutations", "author": ["A.Z. Broder", "M. Charikar", "A.M. Frieze", "M. Mitzenmacher"], "venue": "Proceedings of the Thirtieth Annual ACM Symposium on Theory of Computing, 1998, pp. 327\u2013336.", "citeRegEx": "46", "shortCiteRegEx": null, "year": 1998}, {"title": "Mining of massive datasets", "author": ["A. Rajaraman", "J.D. Ullman"], "venue": null, "citeRegEx": "47", "shortCiteRegEx": "47", "year": 2012}, {"title": "Improved consistent sampling, weighted minhash and l1 sketching", "author": ["S. Ioffe"], "venue": "IEEE International Conference on Data Mining. IEEE, 2010, pp. 246\u2013255.", "citeRegEx": "48", "shortCiteRegEx": null, "year": 2010}, {"title": "Finding near-duplicate web pages: a large-scale evaluation of algorithms", "author": ["M. Henzinger"], "venue": "Proceedings of the 29th annual international ACM SIGIR conference on Research and development in information retrieval, 2006, pp. 284\u2013291.", "citeRegEx": "49", "shortCiteRegEx": null, "year": 2006}, {"title": "Google news personalization: scalable online collaborative filtering", "author": ["A.S. Das", "M. Datar", "A. Garg", "S. Rajaram"], "venue": "Proceedings of the 16th international conference on World Wide Web, 2007, pp. 271\u2013280.", "citeRegEx": "50", "shortCiteRegEx": null, "year": 2007}, {"title": "Near duplicate image detection: min-hash and tf-idf weighting.", "author": ["O. Chum", "J. Philbin", "A. Zisserman"], "venue": "Proceedings of the the British Machine Vision Conference,", "citeRegEx": "51", "shortCiteRegEx": "51", "year": 2008}, {"title": "Partition min-hash for partial duplicate image discovery", "author": ["D.C. Lee", "Q. Ke", "M. Isard"], "venue": "European Conference on Computer Vision, Heraklion, Crete, Greece, 2012, pp. 648\u2013662.", "citeRegEx": "52", "shortCiteRegEx": null, "year": 2012}, {"title": "b-bit minwise hashing", "author": ["P. Li", "C. K\u00f6nig"], "venue": "Proceedings of the 19th International Conference on World Wide Web, 2010, pp. 671\u2013680.", "citeRegEx": "53", "shortCiteRegEx": null, "year": 2010}, {"title": "b-bit minwise hashing for estimating three-way similarities", "author": ["P. Li", "A. Konig", "W. Gui"], "venue": "Advances in Neural Information Processing Systems 23, J. Lafferty, C. K. I. Williams, J. Shawe-Taylor, R. Zemel, and A. Culotta, Eds., 2010, pp. 1387\u20131395.", "citeRegEx": "54", "shortCiteRegEx": null, "year": 2010}, {"title": "One permutation hashing", "author": ["P. Li", "A. Owen", "C.-H. Zhang"], "venue": "Advances in Neural Information Processing Systems 25, P. Bartlett, F. Pereira, C. Burges, L. Bottou, and K. Weinberger, Eds., 2012, pp. 3122\u20133130.", "citeRegEx": "55", "shortCiteRegEx": null, "year": 2012}, {"title": "Geometric min-hashing: Finding a (thick) needle in a haystack", "author": ["O. Chum", "M. Perdoch", "J. Matas"], "venue": "IEEE Computer Society Conference on Computer Vision and Pattern Recognition, Miami, Florida, USA, 2009, pp. 17\u201324.", "citeRegEx": "56", "shortCiteRegEx": null, "year": 2009}, {"title": "Fast computation of min-hash signatures for image collections", "author": ["O. Chum", "J. Matas"], "venue": "Proceedings of the IEEE International Conference on Computer Vision and Pattern Recognition, Providence, RI, USA, 2012, pp. 3077\u20133084.", "citeRegEx": "57", "shortCiteRegEx": null, "year": 2012}, {"title": "Approximate nearest neighbors: towards removing the curse of dimensionality", "author": ["P. Indyk", "R. Motwani"], "venue": "Proc. of 30th ACM Symposium on Theory of Computing, 1998, pp. 604\u2013613.", "citeRegEx": "58", "shortCiteRegEx": null, "year": 1998}, {"title": "Fast search in hamming space with multi-index hashing", "author": ["M. Norouzi", "A. Punjani", "D.J. Fleet"], "venue": "IEEE Conference on Computer Vision and Pattern Recognition, Providence, RI, USA, 2012, pp. 3108\u20133115.", "citeRegEx": "59", "shortCiteRegEx": null, "year": 2012}, {"title": "Inductive hashing on manifolds", "author": ["F. Shen", "C. Shen", "Q. Shi", "A. van den Hengel", "Z. Tang"], "venue": "Proceedings of the IEEE International Conference on Computer Vision and Pattern Recognition, Portland, Oregon, USA, 2013, pp. 1562\u20131569.", "citeRegEx": "60", "shortCiteRegEx": null, "year": 2013}, {"title": "Iterative quantization: A procrustean approach to learning binary codes", "author": ["Y. Gong", "S. Lazebnik"], "venue": "IEEE Conference on Computer Vision and Pattern Recognition, Colorado Springs, CO, USA, 2011, pp. 817\u2013824.", "citeRegEx": "61", "shortCiteRegEx": null, "year": 2011}, {"title": "Isotropic hashing", "author": ["W. Kong", "W.-J. Li"], "venue": "Advances in Neural Information Processing Systems 25, 2012, pp. 1655\u2013 1663.", "citeRegEx": "62", "shortCiteRegEx": null, "year": 2012}, {"title": "Angular quantization based binary codes for fast similarity search", "author": ["Y. Gong", "S. Kumar", "V. Verma", "S. Lazebnik"], "venue": "Advances in Neural Information Processing Systems 25, 2012, pp. 1205\u20131213.", "citeRegEx": "63", "shortCiteRegEx": null, "year": 2012}, {"title": "Spherical hashing", "author": ["J.-P. Heo", "Y. Lee", "J. He", "S.-F. Chang", "S.-E. Yoon"], "venue": "IEEE Conference on Computer Vision and Pattern Recognition, Providence, RI, USA, 2012, pp. 2957\u2013 2964.", "citeRegEx": "64", "shortCiteRegEx": null, "year": 2012}, {"title": "Hamming distance metric learning", "author": ["M. Norouzi", "D. Fleet", "R. Salakhutdinov"], "venue": "Advances in Neural Information Processing Systems, 2012, pp. 1070\u20131078.", "citeRegEx": "65", "shortCiteRegEx": null, "year": 2012}, {"title": "Small codes and large image databases for recognition", "author": ["A. Torralba", "R. Fergus", "Y. Weiss"], "venue": "IEEE Conference on Computer Vision and Pattern Recognition, Anchorage, Alaska, USA, 2008, pp. 1\u20138.", "citeRegEx": "66", "shortCiteRegEx": null, "year": 2008}, {"title": "Supervise binary hash code learning with jensen shannon divergence", "author": ["L. Fan"], "venue": "Proceedings of the IEEE International Conference on Computer Vision, 2013.", "citeRegEx": "67", "shortCiteRegEx": null, "year": 2013}, {"title": "Supervised discrete hashing", "author": ["F. Shen", "C. Shen", "W. Liu", "H.T. Shen"], "venue": "Proceedings of the IEEE International Conference on Computer Vision and Pattern Recognition, Boston, Massachusetts, USA, 2015.", "citeRegEx": "68", "shortCiteRegEx": null, "year": 2015}, {"title": "Fast image search for learned metrics", "author": ["P. Jain", "B. Kulis", "K. Grauman"], "venue": "IEEE Conference on Computer Vision and Pattern Recognition, Anchorage, Alaska, USA, 2008, pp. 1\u20138.", "citeRegEx": "69", "shortCiteRegEx": null, "year": 2008}, {"title": "Online metric learning and fast similarity search", "author": ["P. Jain", "B. Kulis", "I.S. Dhillon", "K. Grauman"], "venue": "Advances in neural information processing systems, 2008, pp. 761\u2013768.", "citeRegEx": "70", "shortCiteRegEx": null, "year": 2008}, {"title": "Fast pose estimation with parameter-sensitive hashing", "author": ["G. Shakhnarovich", "P. Viola", "T. Darrell"], "venue": "IEEE International Conference on Computer Vision, Nice, France, 2003, pp. 750\u2013 757.", "citeRegEx": "71", "shortCiteRegEx": null, "year": 2003}, {"title": "Attribute discovery via predictable discriminative binary codes", "author": ["M. Rastegari", "A. Farhadi", "D.A. Forsyth"], "venue": "European Conference on Computer Vision, Florence, Italy, 2012, pp. 876\u2013889.", "citeRegEx": "72", "shortCiteRegEx": null, "year": 2012}, {"title": "Non-transitive hashing with latent similarity components", "author": ["M. Ou", "P. Cui", "F. Wang", "J. Wang", "W. Zhu"], "venue": "Proceedings of  WANG, LIU, KUMAR, AND CHANG: LEARNING TO HASH FOR INDEXING BIG DATA - A SURVEY  21 the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, 2015, pp. 895\u2013904.", "citeRegEx": "73", "shortCiteRegEx": null, "year": 2015}, {"title": "Learning hash functions using column generation", "author": ["X. Li", "G. Lin", "C. Shen", "A.V. den Hengel", "A. Dick"], "venue": "Proceedings of the 30th International Conference on Machine Learning, 2013, pp. 142\u2013150.", "citeRegEx": "74", "shortCiteRegEx": null, "year": 2013}, {"title": "Order preserving hashing for approximate nearest neighbor search", "author": ["J. Wang", "J. Wang", "N. Yu", "S. Li"], "venue": "Proceedings of the 21st ACM international conference on Multimedia, 2013, pp. 133\u2013142.", "citeRegEx": "75", "shortCiteRegEx": null, "year": 2013}, {"title": "Learning hash codes with listwise supervision", "author": ["J. Wang", "W. Liu", "A.X. Sun", "Y.-G. Jiang"], "venue": "Proceedings of the IEEE International Conference on Computer Vision, 2013.", "citeRegEx": "76", "shortCiteRegEx": null, "year": 2013}, {"title": "Aggregating local descriptors into a compact image representation", "author": ["H. J\u00e9gou", "M. Douze", "C. Schmid", "P. P\u00e9rez"], "venue": "IEEE Conference on Computer Vision and Pattern Recognition, 2010, pp. 3304\u20133311.", "citeRegEx": "77", "shortCiteRegEx": null, "year": 2010}, {"title": "On the difficulty of nearest neighbor search", "author": ["J. He", "S. Kumar", "S.-F. Chang"], "venue": "Proceedings of the 29th international conference on Machine learning, 2012.", "citeRegEx": "78", "shortCiteRegEx": null, "year": 2012}, {"title": "Ldahash: Improved matching with smaller descriptors", "author": ["C. Strecha", "A.M. Bronstein", "M.M. Bronstein", "P. Fua"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 34, no. 1, pp. 66\u201378, 2012.", "citeRegEx": "79", "shortCiteRegEx": null, "year": 2012}, {"title": "Efficient discriminative projections for compact binary descriptors", "author": ["T. Trzcinski", "V. Lepetit"], "venue": "European Conference on Computer Vision, Florence, Italy, 2012, pp. 228\u2013242.", "citeRegEx": "80", "shortCiteRegEx": null, "year": 2012}, {"title": "Circulant binary embedding", "author": ["F. Yu", "S. Kumar", "Y. Gong", "S.-F. Chang"], "venue": "Proc. of ICML, Beijing, China, 2014, pp. 946\u2013 954.", "citeRegEx": "81", "shortCiteRegEx": null, "year": 2014}, {"title": "Scalable similarity search with optimized kernel hashing", "author": ["J. He", "W. Liu", "S.-F. Chang"], "venue": "Proceedings of the 16th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, 2010, pp. 1129\u20131138.", "citeRegEx": "82", "shortCiteRegEx": null, "year": 2010}, {"title": "Spec hashing: Similarity preserving algorithm for entropy-based coding", "author": ["R.-S. Lin", "D.A. Ross", "J. Yagnik"], "venue": "IEEE Computer Society Conference on Computer Vision and Pattern Recognition, San Francisco, USA, 2010, pp. 848\u2013854.", "citeRegEx": "83", "shortCiteRegEx": null, "year": 2010}, {"title": "Complementary projection hashing", "author": ["Z. Jin", "Y. Hu", "Y. Lin", "D. Zhang", "S. Lin", "D. Cai", "X. Li"], "venue": "Proceedings of the IEEE International Conference on Computer Vision, 2013.", "citeRegEx": "84", "shortCiteRegEx": null, "year": 2013}, {"title": "A general two-step approach to learning-based hashing", "author": ["G. Lin", "C. Shen", "A.V. den Hengel", "D. Suter"], "venue": "Proceedings of the IEEE International Conference on Computer Vision, 2013.", "citeRegEx": "85", "shortCiteRegEx": null, "year": 2013}, {"title": "Self-taught hashing for fast similarity search", "author": ["D. Zhang", "J. Wang", "D. Cai", "J. Lu"], "venue": "Proceedings of the 33rd International ACM SIGIR Conference on Research and Development in Information Retrieval, 2010, pp. 18\u201325.", "citeRegEx": "86", "shortCiteRegEx": null, "year": 2010}, {"title": "A desicion-theoretic generalization of on-line learning and an application to boosting", "author": ["Y. Freund", "R. Schapire"], "venue": "Computational Learning Theory, 1995, pp. 23\u201337.", "citeRegEx": "87", "shortCiteRegEx": null, "year": 1995}, {"title": "Boostmap: An embedding method for efficient nearest neighbor retrieval", "author": ["V. Athitsos", "J. Alon", "S. Sclaroff", "G. Kollios"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 30, no. 1, pp. 89\u2013104, Jan. 2008.", "citeRegEx": "88", "shortCiteRegEx": null, "year": 2008}, {"title": "Queryadaptive image search with hash codes", "author": ["Y.-G. Jiang", "J. Wang", "X. Xue", "S.-F. Chang"], "venue": "IEEE Transactions on Multimedia, vol. 15, no. 2, pp. 442\u2013453, 2013.", "citeRegEx": "89", "shortCiteRegEx": null, "year": 2013}, {"title": "Lost in binarization: Query-adaptive ranking for similar image search with compact codes", "author": ["Y.-G. Jiang", "J. Wang", "S.-F. Chang"], "venue": "Proceedings of ACM International Conference on Multimedia Retrieval, 2011.", "citeRegEx": "90", "shortCiteRegEx": null, "year": 2011}, {"title": "Weighted hashing for fast large scale similarity search", "author": ["Q. Wang", "D. Zhang", "L. Si"], "venue": "Proceedings of the 22nd ACM Conference on information and knowledge management, 2013, pp. 1185\u20131188.", "citeRegEx": "91", "shortCiteRegEx": null, "year": 2013}, {"title": "Binary code ranking with weighted hamming distance", "author": ["L. Zhang", "Y. Zhang", "X. Gu", "Q. Tian"], "venue": "Proceedings of the IEEE International Conference on Computer Vision and Pattern Recognition, Portland, Oregon, USA, 2013, pp. 1586\u2013159.", "citeRegEx": "92", "shortCiteRegEx": null, "year": 2013}, {"title": "Hash bit selection: a unified solution for selection problems in hashing", "author": ["X. Liu", "J. He", "B. Lang", "S.-F. Chang"], "venue": "Proceedings of the IEEE International Conference on Computer Vision and Pattern Recognition, Portland, Oregon, USA, 2013, pp. 1570\u20131577.", "citeRegEx": "93", "shortCiteRegEx": null, "year": 2013}, {"title": "Qsrank: Querysensitive hash code ranking for efficient \u01eb-neighbor search", "author": ["X. Zhang", "L. Zhang", "H.-Y. Shum"], "venue": " IEEE Conference on Computer Vision and Pattern Recognition, Providence, RI, USA, 2012, pp. 2058\u20132065.", "citeRegEx": "94", "shortCiteRegEx": null, "year": 2012}, {"title": "Spectral grouping using the Nystr\u00f6m method", "author": ["C. Fowlkes", "S. Belongie", "F. Chung", "J. Malik"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence, no. 2, pp. 214\u2013225, 2004.", "citeRegEx": "95", "shortCiteRegEx": null, "year": 2004}, {"title": "Multidimensional spectral hashing", "author": ["Y. Weiss", "R. Fergus", "A. Torralba"], "venue": "European Conference on Computer Vision, Florence, Italy, 2012, pp. 340\u2013353.", "citeRegEx": "96", "shortCiteRegEx": null, "year": 2012}, {"title": "Large graph construction for scalable semi-supervised learning", "author": ["W. Liu", "J. He", "S.-F. Chang"], "venue": "Proceedings of the 27th International Conference on Machine Learning, 2010, pp. 679\u2013 686.", "citeRegEx": "97", "shortCiteRegEx": null, "year": 2010}, {"title": "Discrete graph hashing", "author": ["W. Liu", "C. Mu", "S. Kumar", "S.-F. Chang"], "venue": "Advances in Neural Information Processing Systems, 2014, pp. 3419\u20133427.", "citeRegEx": "98", "shortCiteRegEx": null, "year": 2014}, {"title": "Sparse similarity-preserving hashing", "author": ["J. Masci", "A. Bronstein", "M. Bronstein", "P. Sprechmann", "G. Sapiro"], "venue": "Proc. International Conference on Learning Representations, 2014.", "citeRegEx": "99", "shortCiteRegEx": null, "year": 2014}, {"title": "Information-theoretic metric learning", "author": ["J.V. Davis", "B. Kulis", "P. Jain", "S. Sra", "I.S. Dhillon"], "venue": "Proceedings of the 24th international conference on Machine learning, 2007, pp. 209\u2013216.", "citeRegEx": "100", "shortCiteRegEx": null, "year": 2007}, {"title": "Toeplitz and circulant matrices: A review", "author": ["R.M. Gray"], "venue": "now publishers inc,", "citeRegEx": "101", "shortCiteRegEx": "101", "year": 2006}, {"title": "Deep learning", "author": ["Y. LeCun", "Y. Bengio", "G. Hinton"], "venue": "Nature, vol. 521, pp. 436\u2013444, 2015.", "citeRegEx": "102", "shortCiteRegEx": null, "year": 2015}, {"title": "Semantic hashing", "author": ["R. Salakhutdinov", "G. Hinton"], "venue": "International Journal of Approximate Reasoning, vol. 50, no. 7, pp. 969\u2013978, 2009.", "citeRegEx": "103", "shortCiteRegEx": null, "year": 2009}, {"title": "Reducing the dimensionality of data with neural networks", "author": ["G. Hinton", "R. Salakhutdinov"], "venue": "Science, vol. 313, no. 5786, pp. 504\u2013507, 2006.", "citeRegEx": "104", "shortCiteRegEx": null, "year": 2006}, {"title": "Learning a nonlinear embedding by preserving class neighbourhood structure", "author": ["R. Salakhutdinov", "G. Hinton"], "venue": "Proc. International Conference on Artificial Intelligence and Statistics, 2007, pp. 412\u2013419.", "citeRegEx": "105", "shortCiteRegEx": null, "year": 2007}, {"title": "Deep hashing for compact binary codes learning", "author": ["V.E. Liong", "J. Lu", "G. Wang", "P. Moulin", "J. Zhou"], "venue": "Proc. IEEE Conference on Computer Vision and Pattern Recognition, 2015.", "citeRegEx": "106", "shortCiteRegEx": null, "year": 2015}, {"title": "Supervised hashing for image retrieval via image representation learning", "author": ["R. Xia", "Y. Pan", "H. Lai", "C. Liu", "S. Yan"], "venue": "Proc. AAAI Conference on Artificial Intelligence, 2014, pp. 2156\u2013 2162.", "citeRegEx": "107", "shortCiteRegEx": null, "year": 2014}, {"title": "Deep semantic ranking based hashing for multi-label image retrieval", "author": ["F. Zhao", "Y. Huang", "L. Wang", "T. Tan"], "venue": "Proc. IEEE Conference on Computer Vision and Pattern Recognition, 2015.", "citeRegEx": "108", "shortCiteRegEx": null, "year": 2015}, {"title": "Simultaneous feature learning and hash coding with deep neural networks", "author": ["H. Lai", "Y. Pan", "Y. Liu", "S. Yan"], "venue": "Proc. IEEE Conference on Computer Vision and Pattern Recognition, 2015.", "citeRegEx": "109", "shortCiteRegEx": null, "year": 2015}, {"title": "Learning fast approximations of sparse coding", "author": ["K. Gregor", "Y. LeCun"], "venue": "Proc. International Conference on Machine Learning, 2010, pp. 399\u2013406.", "citeRegEx": "110", "shortCiteRegEx": null, "year": 2010}, {"title": "Iterative quantization: A procrustean approach to learning binary codes for large-scale image retrieval", "author": ["Y. Gong", "S. Lazebnik", "A. Gordo", "F. Perronnin"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 35, no. 12, pp. 2916\u2013 2929, 2013.", "citeRegEx": "111", "shortCiteRegEx": null, "year": 2013}, {"title": "Imagenet classification with deep convolutional neural networks", "author": ["A. Krizhevsky", "I. Sutskever", "G. Hinton"], "venue": "Proc. Advances in Neural Information Processing Systems, vol. 25, 2012, pp. 1106\u20131114.", "citeRegEx": "112", "shortCiteRegEx": null, "year": 2012}, {"title": "Compressing neural networks with the hashing trick", "author": ["W. Chen", "J.T. Wilson", "S. Tyree", "K.Q. Weinberger", "Y. Chen"], "venue": "Proc. International Conference on Machine Learning, 2015, pp. 2285\u20132294.", "citeRegEx": "113", "shortCiteRegEx": null, "year": 2015}, {"title": "Hashing hyperplane queries to near points with applications to large-scale active learning", "author": ["S. Vijayanarasimhan", "P. Jain", "K. Grauman"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence, 2013.", "citeRegEx": "114", "shortCiteRegEx": null, "year": 2013}, {"title": "Support vector machine active learning with applications to text classification", "author": ["S. Tong", "D. Koller"], "venue": "Journal of Machine Learning Research, vol. 2, pp. 45\u201366, 2001.", "citeRegEx": "115", "shortCiteRegEx": null, "year": 2001}, {"title": "Hashing hyperplane queries to near points with applications to large-scale  22  PROCEEDINGS OF THE IEEE active learning", "author": ["P. Jain", "S. Vijayanarasimhan", "K. Grauman"], "venue": "Advances in Neural Information Processing Systems 23, J. Lafferty, C. K. I. Williams, J. Shawe-Taylor, R. Zemel, and A. Culotta, Eds. MIT Press, 2010, pp. 928\u2013936.", "citeRegEx": "116", "shortCiteRegEx": null, "year": 2010}, {"title": "Compact hyperplane hashing with bilinear functions", "author": ["W. Liu", "J. Wang", "Y. Mu", "S. Kumar", "S.-F. Chang"], "venue": "Proceedings of the 29th international conference on Machine learning, 2012.", "citeRegEx": "117", "shortCiteRegEx": null, "year": 2012}, {"title": "Learning binary codes for high-dimensional data using bilinear projections", "author": ["Y. Gong", "S. Kumar", "H. Rowley", "S. Lazebnik"], "venue": "Proceedings of the IEEE International Conference on Computer Vision and Pattern Recognition, Portland, Oregon, USA, 2013, pp. 484\u2013491.", "citeRegEx": "118", "shortCiteRegEx": null, "year": 2013}, {"title": "Approximate nearest subspace search", "author": ["R. Basri", "T. Hassner", "L. Zelnik-Manor"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 33, no. 2, pp. 266\u2013278, 2011.", "citeRegEx": "119", "shortCiteRegEx": null, "year": 2011}, {"title": "Fast subspace search via grassmannian based hashing", "author": ["X. Wang", "S. Atev", "J. Wright", "G. Lerman"], "venue": "Proceedings of the IEEE International Conference on Computer Vision, 2013.", "citeRegEx": "120", "shortCiteRegEx": null, "year": 2013}, {"title": "Sequential spectral learning to hash with multiple representations", "author": ["S. Kim", "Y. Kang", "S. Choi"], "venue": "European Conference on Computer Vision, Florence, Italy, 2012, pp. 538\u2013551.", "citeRegEx": "121", "shortCiteRegEx": null, "year": 2012}, {"title": "Composite hashing with multiple information sources", "author": ["D. Zhang", "F. Wang", "L. Si"], "venue": "Proceedings of the 34th international ACM SIGIR conference on Research and development in Information Retrieval, 2011, pp. 225\u2013234.", "citeRegEx": "122", "shortCiteRegEx": null, "year": 2011}, {"title": "Co-regularized hashing for multimodal data", "author": ["Y. Zhen", "D.-Y. Yeung"], "venue": "Advances in Neural Information Processing Systems 25, P. Bartlett, F. Pereira, C. Burges, L. Bottou, and K. Weinberger, Eds. MIT Press, 2012, pp. 1385\u20131393.", "citeRegEx": "123", "shortCiteRegEx": null, "year": 2012}, {"title": "Predictable dual-view hashing", "author": ["M. Rastegari", "J. Choi", "S. Fakhraei", "D. Hal", "L. Davis"], "venue": "Proceedings of the 30th International Conference on Machine Learning, 2013, pp. 1328\u2013 1336.", "citeRegEx": "124", "shortCiteRegEx": null, "year": 2013}, {"title": "A probabilistic model for multimodal hash function learning", "author": ["Y. Zhen", "D.-Y. Yeung"], "venue": "Proceedings of the 18th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, 2012, pp. 940\u2013948.", "citeRegEx": "125", "shortCiteRegEx": null, "year": 2012}, {"title": "Multiple feature hashing for real-time large scale near-duplicate video retrieval", "author": ["J. Song", "Y. Yang", "Z. Huang", "H.T. Shen", "R. Hong"], "venue": "Proceedings of the 19th ACM international conference on Multimedia, 2011, pp. 423\u2013432.", "citeRegEx": "126", "shortCiteRegEx": null, "year": 2011}, {"title": "Submodular video hashing: a unified framework towards video pooling and indexing", "author": ["L. Cao", "Z. Li", "Y. Mu", "S.-F. Chang"], "venue": "Proceedings of the 20th ACM international conference on Multimedia, 2012, pp. 299\u2013308.", "citeRegEx": "127", "shortCiteRegEx": null, "year": 2012}, {"title": "Probabilistic attributed hashing", "author": ["M. Ou", "P. Cui", "J. Wang", "F. Wang", "W. Zhu"], "venue": "Twenty-Ninth AAAI Conference on Artificial Intelligence, 2015, pp. 2894\u20132900.", "citeRegEx": "128", "shortCiteRegEx": null, "year": 2015}, {"title": "Learning binary hash codes for large-scale image search", "author": ["K. Grauman", "R. Fergus"], "venue": "Machine Learning for Computer Vision. Springer, 2013, pp. 49\u201387.", "citeRegEx": "129", "shortCiteRegEx": null, "year": 2013}, {"title": "Efficiently searching for similar images", "author": ["K. Grauman"], "venue": "Commun. ACM, vol. 53, no. 6, 2010.", "citeRegEx": "130", "shortCiteRegEx": null, "year": 2010}, {"title": "Manhattan hashing for largescale image retrieval", "author": ["W. Kong", "W.-J. Li", "M. Guo"], "venue": "Proceedings of the 35th international ACM SIGIR conference on Research and development in information retrieval, 2012, pp. 45\u201354.", "citeRegEx": "131", "shortCiteRegEx": null, "year": 2012}, {"title": "Mobile product search with bag of hash bits and boundary reranking", "author": ["J. He", "J. Feng", "X. Liu", "T. Cheng", "T.-H. Lin", "H. Chung", "S.-F. Chang"], "venue": "IEEE Conference on Computer Vision and Pattern Recognition, Providence, RI, USA, 2012, pp. 3005\u20133012.", "citeRegEx": "132", "shortCiteRegEx": null, "year": 2012}, {"title": "Coherency sensitive hashing", "author": ["S. Korman", "S. Avidan"], "venue": "Proceedings of the IEEE International Conference on Computer Vision, 2011, pp. 1607\u20131614.", "citeRegEx": "133", "shortCiteRegEx": null, "year": 2011}, {"title": "Rapid face recognition using hashing", "author": ["Q. Shi", "H. Li", "C. Shen"], "venue": "IEEE Computer Society Conference on Computer Vision and Pattern Recognition, San Francisco, USA, 2010, pp. 2753\u20132760.", "citeRegEx": "134", "shortCiteRegEx": null, "year": 2010}, {"title": "Optimal bayesian hashing for efficient face recognition", "author": ["Q. Dai", "J. Li", "J. Wang", "Y. Chen", "Y.-G. Jiang"], "venue": "Proceedings of the Twenty-Fourth International Joint Conference on Artificial Intellige, 2015, pp. 3430\u20133437.", "citeRegEx": "135", "shortCiteRegEx": null, "year": 2015}, {"title": "Learning compact binary codes for visual tracking", "author": ["X. Li", "C. Shen", "A. Dick", "A. van den Hengel"], "venue": "Proceedings of the IEEE International Conference on Computer Vision and Pattern Recognition, Portland, Oregon, USA, 2013, pp. 2419\u2013 2426.", "citeRegEx": "136", "shortCiteRegEx": null, "year": 2013}, {"title": "Efficient mining of repetitions in large-scale tv streams with product quantization hashing", "author": ["J. Yuan", "G. Gravier", "S. Campion", "X. Liu", "H. Jgou"], "venue": "European Conference on Computer Vision, Florence, Italy, 2012, pp. 271\u2013280.", "citeRegEx": "137", "shortCiteRegEx": null, "year": 2012}, {"title": "Detecting nearduplicates for web crawling", "author": ["G.S. Manku", "A. Jain", "A. Das Sarma"], "venue": "Proceedings of the 16th international conference on World Wide Web, 2007, pp. 141\u2013150.", "citeRegEx": "138", "shortCiteRegEx": null, "year": 2007}, {"title": "Data fusion through cross-modality metric learning using similarity-sensitive hashing", "author": ["M.M. Bronstein", "A.M. Bronstein", "F. Michel", "N. Paragios"], "venue": "IEEE Computer Society Conference on Computer Vision and Pattern Recognition, San Francisco, USA, 2010, pp. 3594\u20133601.", "citeRegEx": "139", "shortCiteRegEx": null, "year": 2010}, {"title": "Accelerated large scale optimization by concomitant hashing", "author": ["Y. Mu", "J. Wright", "S.-F. Chang"], "venue": "European Conference on Computer Vision, Florence, Italy, 2012, pp. 414\u2013427.", "citeRegEx": "140", "shortCiteRegEx": null, "year": 2012}, {"title": "Hashing algorithms for large-scale learning", "author": ["P. Li", "A. Shrivastava", "J.L. Moore", "A.C. Knig"], "venue": "Advances in Neural Information Processing Systems 24, J. Shawe-Taylor, R. Zemel, P. Bartlett, F. Pereira, and K. Weinberger, Eds., 2011, pp. 2672\u20132680.", "citeRegEx": "141", "shortCiteRegEx": null, "year": 2011}, {"title": "Learning binary codes for collaborative filtering", "author": ["K. Zhou", "H. Zha"], "venue": "Proceedings of the 18th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, 2012, pp. 498\u2013506.", "citeRegEx": "142", "shortCiteRegEx": null, "year": 2012}, {"title": "Comparing apples to oranges: A scalable solution with heterogeneous hashing", "author": ["M. Ou", "P. Cui", "F. Wang", "J. Wang", "W. Zhu", "S. Yang"], "venue": "Proceedings of the 19th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, 2013, pp. 230\u2013238.", "citeRegEx": "143", "shortCiteRegEx": null, "year": 2013}, {"title": "Large-scale video hashing via structure learning", "author": ["G. Ye", "D. Liu", "J. Wang", "S.-F. Chang"], "venue": "Proceedings of the IEEE International Conference on Computer Vision, 2013.", "citeRegEx": "144", "shortCiteRegEx": null, "year": 2013}, {"title": "Semantic hashing using tags and topic modeling", "author": ["Q. Wang", "D. Zhang", "L. Si"], "venue": "Proceedings of the 36th International ACM SIGIR Conference on Research and Development in Information Retrieval, 2013, pp. 213\u2013222.", "citeRegEx": "145", "shortCiteRegEx": null, "year": 2013}, {"title": "Two-stage hashing for fast document retrieval", "author": ["H. Li", "W. Liu", "H. Ji"], "venue": "Proceedings of the Annual Meeting of the Association for Computational Linguistics, Baltimore, Maryland, USA, 2014.", "citeRegEx": "146", "shortCiteRegEx": null, "year": 2014}, {"title": "Fast pairwise query selection for large-scale active learning to rank", "author": ["B. Qian", "X. Wang", "J. Wang", "WeifengZhi", "H. Li", "I. Davidson"], "venue": "Proceedings of the IEEE International Conference on Data Mining, 2013.", "citeRegEx": "147", "shortCiteRegEx": null, "year": 2013}, {"title": "Hash-svm: Scalable kernel machines for large-scale visual classification", "author": ["Y. Mu", "G. Hua", "W. Fan", "S.-F. Chang"], "venue": "Proceedings of the IEEE International Conference on Computer Vision and Pattern Recognition, Columbus, Ohio, USA, 2014, pp. 446\u2013451.", "citeRegEx": "148", "shortCiteRegEx": null, "year": 2014}, {"title": "Hash kernels for structured data", "author": ["Q. Shi", "J. Petterson", "G. Dror", "J. Langford", "A. Smola", "S. Vishwanathan"], "venue": "The Journal of Machine Learning Research, vol. 10, pp. 2615\u20132637, 2009.", "citeRegEx": "149", "shortCiteRegEx": null, "year": 2009}], "referenceMentions": [{"referenceID": 0, "context": "Besides the widely used text-based commercial search engines such as Google and Bing, content-based image retrieval (CBIR) has attracted substantial attention in the past decade [1].", "startOffset": 178, "endOffset": 181}, {"referenceID": 1, "context": "Searching for similar data samples in a given database essentially relates to the fundamental problem of nearest neighbor search [2].", "startOffset": 129, "endOffset": 132}, {"referenceID": 2, "context": "Besides the scalability issue, most practical large-scale applications also suffer from the curse of dimensionality [3], since data under modern analytics usually contains thousands or even tens of thousands of dimensions, e.", "startOffset": 116, "endOffset": 119}, {"referenceID": 3, "context": "Tree-based indexing approaches, such as KD tree [4], ball tree [5], metric tree [6], and vantage point tree [7], have been popular during the past several decades.", "startOffset": 48, "endOffset": 51}, {"referenceID": 4, "context": "Tree-based indexing approaches, such as KD tree [4], ball tree [5], metric tree [6], and vantage point tree [7], have been popular during the past several decades.", "startOffset": 63, "endOffset": 66}, {"referenceID": 5, "context": "Tree-based indexing approaches, such as KD tree [4], ball tree [5], metric tree [6], and vantage point tree [7], have been popular during the past several decades.", "startOffset": 80, "endOffset": 83}, {"referenceID": 6, "context": "Tree-based indexing approaches, such as KD tree [4], ball tree [5], metric tree [6], and vantage point tree [7], have been popular during the past several decades.", "startOffset": 108, "endOffset": 111}, {"referenceID": 7, "context": "In addition, the performance of tree-based indexing methods dramatically degrades when handling high-dimensional data [8].", "startOffset": 118, "endOffset": 121}, {"referenceID": 8, "context": "More recently, product quantization techniques have been proposed to encode high-dimensional data vectors via subspace decomposition for efficient ANN search [9][10].", "startOffset": 158, "endOffset": 161}, {"referenceID": 9, "context": "More recently, product quantization techniques have been proposed to encode high-dimensional data vectors via subspace decomposition for efficient ANN search [9][10].", "startOffset": 161, "endOffset": 165}, {"referenceID": 10, "context": "For instance, 80 million tiny images (32\u02c6 32 pixels, double type) cost around 600G bytes [11], but can be compressed into 64-bit binary codes requiring only 600M bytes! In many cases, hash codes are organized into a hash table for inverse table lookup, as shown in Figure 1.", "startOffset": 89, "endOffset": 93}, {"referenceID": 11, "context": "Among these methods, the randomized scheme of LocalitySensitive Hashing (LSH) is one of the most popular choices [13].", "startOffset": 113, "endOffset": 117}, {"referenceID": 12, "context": "Second, the theoretical guarantees of LSH only apply to certain metrics such as lp (p P p0,2s) and Jaccard [14].", "startOffset": 107, "endOffset": 111}, {"referenceID": 13, "context": "This discrepancy between semantic and metric spaces has been recognized in the computer vision and machine learning communities, namely as semantic gap [15].", "startOffset": 152, "endOffset": 156}, {"referenceID": 14, "context": "To tackle the aforementioned issues, many hashing methods have been proposed recently to leverage machine learning techniques to produce more effective hash codes [16].", "startOffset": 163, "endOffset": 167}, {"referenceID": 15, "context": "The goal of learning to hash is to learn datadependent and task-specific hash functions that yield compact binary codes to achieve good search accuracy [17].", "startOffset": 152, "endOffset": 156}, {"referenceID": 16, "context": "In order to achieve this goal, sophisticated machine learning tools and algorithms have been adapted to the procedure of hash function design, including the boosting algorithm [18], distance metric learning [19], asymmetric binary embedding [20], kernel methods [21][22], compressed sensing [23], maximum margin learning [24], sequential", "startOffset": 176, "endOffset": 180}, {"referenceID": 17, "context": "In order to achieve this goal, sophisticated machine learning tools and algorithms have been adapted to the procedure of hash function design, including the boosting algorithm [18], distance metric learning [19], asymmetric binary embedding [20], kernel methods [21][22], compressed sensing [23], maximum margin learning [24], sequential", "startOffset": 207, "endOffset": 211}, {"referenceID": 18, "context": "In order to achieve this goal, sophisticated machine learning tools and algorithms have been adapted to the procedure of hash function design, including the boosting algorithm [18], distance metric learning [19], asymmetric binary embedding [20], kernel methods [21][22], compressed sensing [23], maximum margin learning [24], sequential", "startOffset": 241, "endOffset": 245}, {"referenceID": 19, "context": "In order to achieve this goal, sophisticated machine learning tools and algorithms have been adapted to the procedure of hash function design, including the boosting algorithm [18], distance metric learning [19], asymmetric binary embedding [20], kernel methods [21][22], compressed sensing [23], maximum margin learning [24], sequential", "startOffset": 262, "endOffset": 266}, {"referenceID": 20, "context": "In order to achieve this goal, sophisticated machine learning tools and algorithms have been adapted to the procedure of hash function design, including the boosting algorithm [18], distance metric learning [19], asymmetric binary embedding [20], kernel methods [21][22], compressed sensing [23], maximum margin learning [24], sequential", "startOffset": 266, "endOffset": 270}, {"referenceID": 21, "context": "In order to achieve this goal, sophisticated machine learning tools and algorithms have been adapted to the procedure of hash function design, including the boosting algorithm [18], distance metric learning [19], asymmetric binary embedding [20], kernel methods [21][22], compressed sensing [23], maximum margin learning [24], sequential", "startOffset": 291, "endOffset": 295}, {"referenceID": 22, "context": "In order to achieve this goal, sophisticated machine learning tools and algorithms have been adapted to the procedure of hash function design, including the boosting algorithm [18], distance metric learning [19], asymmetric binary embedding [20], kernel methods [21][22], compressed sensing [23], maximum margin learning [24], sequential", "startOffset": 321, "endOffset": 325}, {"referenceID": 23, "context": "learning [25], clustering analysis [26], semi-supervised learning [14], supervised learning [27][22], graph learning [28], and so on.", "startOffset": 9, "endOffset": 13}, {"referenceID": 24, "context": "learning [25], clustering analysis [26], semi-supervised learning [14], supervised learning [27][22], graph learning [28], and so on.", "startOffset": 35, "endOffset": 39}, {"referenceID": 12, "context": "learning [25], clustering analysis [26], semi-supervised learning [14], supervised learning [27][22], graph learning [28], and so on.", "startOffset": 66, "endOffset": 70}, {"referenceID": 25, "context": "learning [25], clustering analysis [26], semi-supervised learning [14], supervised learning [27][22], graph learning [28], and so on.", "startOffset": 92, "endOffset": 96}, {"referenceID": 20, "context": "learning [25], clustering analysis [26], semi-supervised learning [14], supervised learning [27][22], graph learning [28], and so on.", "startOffset": 96, "endOffset": 100}, {"referenceID": 26, "context": "learning [25], clustering analysis [26], semi-supervised learning [14], supervised learning [27][22], graph learning [28], and so on.", "startOffset": 117, "endOffset": 121}, {"referenceID": 10, "context": "Both the supervised and semi-supervised learning paradigms have been explored using such pairwise semantic relationships to learn semantically relevant hash functions [11][29][30][31].", "startOffset": 167, "endOffset": 171}, {"referenceID": 27, "context": "Both the supervised and semi-supervised learning paradigms have been explored using such pairwise semantic relationships to learn semantically relevant hash functions [11][29][30][31].", "startOffset": 171, "endOffset": 175}, {"referenceID": 28, "context": "Both the supervised and semi-supervised learning paradigms have been explored using such pairwise semantic relationships to learn semantically relevant hash functions [11][29][30][31].", "startOffset": 175, "endOffset": 179}, {"referenceID": 29, "context": "Both the supervised and semi-supervised learning paradigms have been explored using such pairwise semantic relationships to learn semantically relevant hash functions [11][29][30][31].", "startOffset": 179, "endOffset": 183}, {"referenceID": 30, "context": "For example, LSH keeps fp \u0308q to be an identity function, while shift-invariant kernel-based hashing and spectral hashing choose fp \u0308q to be a shifted cosine or sinusoidal function [32][33].", "startOffset": 180, "endOffset": 184}, {"referenceID": 31, "context": "For example, LSH keeps fp \u0308q to be an identity function, while shift-invariant kernel-based hashing and spectral hashing choose fp \u0308q to be a shifted cosine or sinusoidal function [32][33].", "startOffset": 184, "endOffset": 188}, {"referenceID": 32, "context": ", cosine similarity or Jaccard similarity [34].", "startOffset": 42, "endOffset": 46}, {"referenceID": 32, "context": "The random vectorw is constructed by sampling each component of w randomly from a standard Gaussian distribution for cosine distance [34].", "startOffset": 133, "endOffset": 137}, {"referenceID": 33, "context": "In particular, with hash codes of length K, it is required to construct a sufficient number of hash tables to ensure the desired performance bound [35].", "startOffset": 147, "endOffset": 151}, {"referenceID": 34, "context": "For instance, a self-tuning indexing technique, called LSH forest was proposed in [36], which aims at improving the performance without additional storage and query overhead.", "startOffset": 82, "endOffset": 86}, {"referenceID": 35, "context": "In [37][38], a technique called MultiProbe LSH was developed to reduce the number of required hash tables through intelligently probing multiple buckets in each hash table.", "startOffset": 3, "endOffset": 7}, {"referenceID": 36, "context": "In [37][38], a technique called MultiProbe LSH was developed to reduce the number of required hash tables through intelligently probing multiple buckets in each hash table.", "startOffset": 7, "endOffset": 11}, {"referenceID": 37, "context": "In [39], nonlinear randomized Hadamard transforms were explored to speed up the LSH based ANN search for Euclidean distance.", "startOffset": 3, "endOffset": 7}, {"referenceID": 38, "context": "In [40], BayesLSH was proposed to combine Bayesian inference with LSH in a principled manner, which has probabilistic guarantees on the quality of the search results in terms of accuracy and recall.", "startOffset": 3, "endOffset": 7}, {"referenceID": 14, "context": "In machine learning and data mining community, recent methods tend to leverage data-dependent and taskspecific information to improve the efficiency of random projection based hash functions [16].", "startOffset": 191, "endOffset": 195}, {"referenceID": 39, "context": "For example, incorporating kernel learning with LSH can help generalize ANN search from a standard metric space to a wide range of similarity functions [41][42].", "startOffset": 152, "endOffset": 156}, {"referenceID": 40, "context": "For example, incorporating kernel learning with LSH can help generalize ANN search from a standard metric space to a wide range of similarity functions [41][42].", "startOffset": 156, "endOffset": 160}, {"referenceID": 17, "context": "Furthermore, metric learning has been combined with randomized LSH functions to explore a set of pairwise similarity and dissimilarity constraints [19].", "startOffset": 147, "endOffset": 151}, {"referenceID": 41, "context": "Other variants of locality sensitive hashing techniques include super-bit LSH [43], boosted LSH [18], as well as non-metric LSH [44]", "startOffset": 78, "endOffset": 82}, {"referenceID": 16, "context": "Other variants of locality sensitive hashing techniques include super-bit LSH [43], boosted LSH [18], as well as non-metric LSH [44]", "startOffset": 96, "endOffset": 100}, {"referenceID": 42, "context": "Other variants of locality sensitive hashing techniques include super-bit LSH [43], boosted LSH [18], as well as non-metric LSH [44]", "startOffset": 128, "endOffset": 132}, {"referenceID": 43, "context": "A typical application is to index documents and then identify near-duplicate samples from a corpus of documents [45][46].", "startOffset": 112, "endOffset": 116}, {"referenceID": 44, "context": "A typical application is to index documents and then identify near-duplicate samples from a corpus of documents [45][46].", "startOffset": 116, "endOffset": 120}, {"referenceID": 45, "context": "Note that such a hash function holds a property that the chance of two sets having the same MinHash values is equal to the Jaccard similarity between them [47]", "startOffset": 155, "endOffset": 159}, {"referenceID": 46, "context": "12 still holds [48].", "startOffset": 15, "endOffset": 19}, {"referenceID": 47, "context": ", the min-hash approach outperforms other competing methods for the application of webpage duplicate detection [49].", "startOffset": 111, "endOffset": 115}, {"referenceID": 48, "context": "In addition, the minhash scheme is also applied for Google news personalization [50] and near duplicate image detection [51] [52].", "startOffset": 80, "endOffset": 84}, {"referenceID": 49, "context": "In addition, the minhash scheme is also applied for Google news personalization [50] and near duplicate image detection [51] [52].", "startOffset": 120, "endOffset": 124}, {"referenceID": 50, "context": "In addition, the minhash scheme is also applied for Google news personalization [50] and near duplicate image detection [51] [52].", "startOffset": 125, "endOffset": 129}, {"referenceID": 51, "context": "Some recent efforts have been made to further improve the min-hash technique, including b-bit minwise hashing [53] [54], one permutation approach [55], geometric min-Hashing [56], and a fast computing technique for image data [57].", "startOffset": 110, "endOffset": 114}, {"referenceID": 52, "context": "Some recent efforts have been made to further improve the min-hash technique, including b-bit minwise hashing [53] [54], one permutation approach [55], geometric min-Hashing [56], and a fast computing technique for image data [57].", "startOffset": 115, "endOffset": 119}, {"referenceID": 53, "context": "Some recent efforts have been made to further improve the min-hash technique, including b-bit minwise hashing [53] [54], one permutation approach [55], geometric min-Hashing [56], and a fast computing technique for image data [57].", "startOffset": 146, "endOffset": 150}, {"referenceID": 54, "context": "Some recent efforts have been made to further improve the min-hash technique, including b-bit minwise hashing [53] [54], one permutation approach [55], geometric min-Hashing [56], and a fast computing technique for image data [57].", "startOffset": 174, "endOffset": 178}, {"referenceID": 55, "context": "Some recent efforts have been made to further improve the min-hash technique, including b-bit minwise hashing [53] [54], one permutation approach [55], geometric min-Hashing [56], and a fast computing technique for image data [57].", "startOffset": 226, "endOffset": 230}, {"referenceID": 56, "context": "Since the proposal of LSH in [58], many new hashing techniques have been developed.", "startOffset": 29, "endOffset": 33}, {"referenceID": 29, "context": ", developed multiple complementary hash tables that are sequentially learned using a boosting-style algorithm [31].", "startOffset": 110, "endOffset": 114}, {"referenceID": 57, "context": "[59].", "startOffset": 0, "endOffset": 4}, {"referenceID": 35, "context": "Based on the random projection scheme, there have been several efforts to improve the performance of the LSH method [37] [39] [41].", "startOffset": 116, "endOffset": 120}, {"referenceID": 37, "context": "Based on the random projection scheme, there have been several efforts to improve the performance of the LSH method [37] [39] [41].", "startOffset": 121, "endOffset": 125}, {"referenceID": 39, "context": "Based on the random projection scheme, there have been several efforts to improve the performance of the LSH method [37] [39] [41].", "startOffset": 126, "endOffset": 130}, {"referenceID": 30, "context": "Representative unsupervised methods include spectral hashing [32], graph hashing [28], manifold hashing [60], iterative quantization hashing [61], kernalized locality sensitive hashing [41][21], isotropic hashing [62], angular quantization hashing [63], and spherical hashing [64].", "startOffset": 61, "endOffset": 65}, {"referenceID": 26, "context": "Representative unsupervised methods include spectral hashing [32], graph hashing [28], manifold hashing [60], iterative quantization hashing [61], kernalized locality sensitive hashing [41][21], isotropic hashing [62], angular quantization hashing [63], and spherical hashing [64].", "startOffset": 81, "endOffset": 85}, {"referenceID": 58, "context": "Representative unsupervised methods include spectral hashing [32], graph hashing [28], manifold hashing [60], iterative quantization hashing [61], kernalized locality sensitive hashing [41][21], isotropic hashing [62], angular quantization hashing [63], and spherical hashing [64].", "startOffset": 104, "endOffset": 108}, {"referenceID": 59, "context": "Representative unsupervised methods include spectral hashing [32], graph hashing [28], manifold hashing [60], iterative quantization hashing [61], kernalized locality sensitive hashing [41][21], isotropic hashing [62], angular quantization hashing [63], and spherical hashing [64].", "startOffset": 141, "endOffset": 145}, {"referenceID": 39, "context": "Representative unsupervised methods include spectral hashing [32], graph hashing [28], manifold hashing [60], iterative quantization hashing [61], kernalized locality sensitive hashing [41][21], isotropic hashing [62], angular quantization hashing [63], and spherical hashing [64].", "startOffset": 185, "endOffset": 189}, {"referenceID": 19, "context": "Representative unsupervised methods include spectral hashing [32], graph hashing [28], manifold hashing [60], iterative quantization hashing [61], kernalized locality sensitive hashing [41][21], isotropic hashing [62], angular quantization hashing [63], and spherical hashing [64].", "startOffset": 189, "endOffset": 193}, {"referenceID": 60, "context": "Representative unsupervised methods include spectral hashing [32], graph hashing [28], manifold hashing [60], iterative quantization hashing [61], kernalized locality sensitive hashing [41][21], isotropic hashing [62], angular quantization hashing [63], and spherical hashing [64].", "startOffset": 213, "endOffset": 217}, {"referenceID": 61, "context": "Representative unsupervised methods include spectral hashing [32], graph hashing [28], manifold hashing [60], iterative quantization hashing [61], kernalized locality sensitive hashing [41][21], isotropic hashing [62], angular quantization hashing [63], and spherical hashing [64].", "startOffset": 248, "endOffset": 252}, {"referenceID": 62, "context": "Representative unsupervised methods include spectral hashing [32], graph hashing [28], manifold hashing [60], iterative quantization hashing [61], kernalized locality sensitive hashing [41][21], isotropic hashing [62], angular quantization hashing [63], and spherical hashing [64].", "startOffset": 276, "endOffset": 280}, {"referenceID": 17, "context": "In addition, supervised learning paradigms ranging from kernel learning to metric learning to deep learning have been exploited to learn binary codes, and many supervised hashing methods have been proposed recently [19][22][65][66][67][68].", "startOffset": 215, "endOffset": 219}, {"referenceID": 20, "context": "In addition, supervised learning paradigms ranging from kernel learning to metric learning to deep learning have been exploited to learn binary codes, and many supervised hashing methods have been proposed recently [19][22][65][66][67][68].", "startOffset": 219, "endOffset": 223}, {"referenceID": 63, "context": "In addition, supervised learning paradigms ranging from kernel learning to metric learning to deep learning have been exploited to learn binary codes, and many supervised hashing methods have been proposed recently [19][22][65][66][67][68].", "startOffset": 223, "endOffset": 227}, {"referenceID": 64, "context": "In addition, supervised learning paradigms ranging from kernel learning to metric learning to deep learning have been exploited to learn binary codes, and many supervised hashing methods have been proposed recently [19][22][65][66][67][68].", "startOffset": 227, "endOffset": 231}, {"referenceID": 65, "context": "In addition, supervised learning paradigms ranging from kernel learning to metric learning to deep learning have been exploited to learn binary codes, and many supervised hashing methods have been proposed recently [19][22][65][66][67][68].", "startOffset": 231, "endOffset": 235}, {"referenceID": 66, "context": "In addition, supervised learning paradigms ranging from kernel learning to metric learning to deep learning have been exploited to learn binary codes, and many supervised hashing methods have been proposed recently [19][22][65][66][67][68].", "startOffset": 235, "endOffset": 239}, {"referenceID": 12, "context": "al proposed a regularized objective to achieve accurate yet balanced hash codes to avoid overfitting [14].", "startOffset": 101, "endOffset": 105}, {"referenceID": 67, "context": "In [69][70], authors proposed to exploit the metric learning and locality sensitive hashing to achieve fast similarity based search.", "startOffset": 3, "endOffset": 7}, {"referenceID": 68, "context": "In [69][70], authors proposed to exploit the metric learning and locality sensitive hashing to achieve fast similarity based search.", "startOffset": 7, "endOffset": 11}, {"referenceID": 69, "context": "For example, a few existing approaches utilize the instance level semantic attributes or labels to design the hash functions [71][18][72].", "startOffset": 125, "endOffset": 129}, {"referenceID": 16, "context": "For example, a few existing approaches utilize the instance level semantic attributes or labels to design the hash functions [71][18][72].", "startOffset": 129, "endOffset": 133}, {"referenceID": 70, "context": "For example, a few existing approaches utilize the instance level semantic attributes or labels to design the hash functions [71][18][72].", "startOffset": 133, "endOffset": 137}, {"referenceID": 17, "context": "Additionally, learning methods based on pairwise supervision have been extensively studied, and many hashing techniques have been proposed [19][22][65][66][14][69][70][73].", "startOffset": 139, "endOffset": 143}, {"referenceID": 20, "context": "Additionally, learning methods based on pairwise supervision have been extensively studied, and many hashing techniques have been proposed [19][22][65][66][14][69][70][73].", "startOffset": 143, "endOffset": 147}, {"referenceID": 63, "context": "Additionally, learning methods based on pairwise supervision have been extensively studied, and many hashing techniques have been proposed [19][22][65][66][14][69][70][73].", "startOffset": 147, "endOffset": 151}, {"referenceID": 64, "context": "Additionally, learning methods based on pairwise supervision have been extensively studied, and many hashing techniques have been proposed [19][22][65][66][14][69][70][73].", "startOffset": 151, "endOffset": 155}, {"referenceID": 12, "context": "Additionally, learning methods based on pairwise supervision have been extensively studied, and many hashing techniques have been proposed [19][22][65][66][14][69][70][73].", "startOffset": 155, "endOffset": 159}, {"referenceID": 67, "context": "Additionally, learning methods based on pairwise supervision have been extensively studied, and many hashing techniques have been proposed [19][22][65][66][14][69][70][73].", "startOffset": 159, "endOffset": 163}, {"referenceID": 68, "context": "Additionally, learning methods based on pairwise supervision have been extensively studied, and many hashing techniques have been proposed [19][22][65][66][14][69][70][73].", "startOffset": 163, "endOffset": 167}, {"referenceID": 71, "context": "Additionally, learning methods based on pairwise supervision have been extensively studied, and many hashing techniques have been proposed [19][22][65][66][14][69][70][73].", "startOffset": 167, "endOffset": 171}, {"referenceID": 72, "context": "More recently, a triplet ranking that encodes the pairwise proximity comparison among three data points is exploited to design hash codes [74][65][75].", "startOffset": 138, "endOffset": 142}, {"referenceID": 63, "context": "More recently, a triplet ranking that encodes the pairwise proximity comparison among three data points is exploited to design hash codes [74][65][75].", "startOffset": 142, "endOffset": 146}, {"referenceID": 73, "context": "More recently, a triplet ranking that encodes the pairwise proximity comparison among three data points is exploited to design hash codes [74][65][75].", "startOffset": 146, "endOffset": 150}, {"referenceID": 74, "context": "By converting rank lists to a triplet tensor matrix, listwise hashing is designed to preserve the ranking in the Hamming space [76].", "startOffset": 127, "endOffset": 131}, {"referenceID": 61, "context": "For instance, PCA hashing performs principal component analysis on the data to derive large variance projections [63][77][78], as shown in Figure 5(a).", "startOffset": 113, "endOffset": 117}, {"referenceID": 75, "context": "For instance, PCA hashing performs principal component analysis on the data to derive large variance projections [63][77][78], as shown in Figure 5(a).", "startOffset": 117, "endOffset": 121}, {"referenceID": 76, "context": "For instance, PCA hashing performs principal component analysis on the data to derive large variance projections [63][77][78], as shown in Figure 5(a).", "startOffset": 121, "endOffset": 125}, {"referenceID": 77, "context": "In the same league, supervised methods have used Linear Discriminant Analysis to design more discriminative hash codes [79][80].", "startOffset": 119, "endOffset": 123}, {"referenceID": 78, "context": "In the same league, supervised methods have used Linear Discriminant Analysis to design more discriminative hash codes [79][80].", "startOffset": 123, "endOffset": 127}, {"referenceID": 12, "context": "Semi-supervised hashing methods estimate the projections that have minimum empirical loss on pair-wise labels while partitioning the unlabeled data in a balanced way [14].", "startOffset": 166, "endOffset": 170}, {"referenceID": 12, "context": "Two types of solutions based on relaxation of the orthogonality constraints or random/learned rotation of the data have been proposed in the literature to address these issues [14][61].", "startOffset": 176, "endOffset": 180}, {"referenceID": 59, "context": "Two types of solutions based on relaxation of the orthogonality constraints or random/learned rotation of the data have been proposed in the literature to address these issues [14][61].", "startOffset": 180, "endOffset": 184}, {"referenceID": 60, "context": "Isotropic hashing is proposed to derive projections with equal variances and is shown to be superior to anisotropic variances based projections [62].", "startOffset": 144, "endOffset": 148}, {"referenceID": 23, "context": "Instead of performing one-shot learning, sequential projection learning derives correlated projections with the goal of correcting errors from previous hash bits [25].", "startOffset": 162, "endOffset": 166}, {"referenceID": 79, "context": "Finally, to reduce the computational complexity of full projection, circulant binary embedding was recently proposed to significantly speed up the encoding process using the circulant convolution [81].", "startOffset": 196, "endOffset": 200}, {"referenceID": 31, "context": "In addition, shift-invariant kernel-based hashing chooses fp \u0308q to be a shifted cosine function and samples the projection vector in the same way as standard LSH does [33].", "startOffset": 167, "endOffset": 171}, {"referenceID": 19, "context": "Another category of nonlinear hashing techniques employs kernel functions [21][22][28][82].", "startOffset": 74, "endOffset": 78}, {"referenceID": 20, "context": "Another category of nonlinear hashing techniques employs kernel functions [21][22][28][82].", "startOffset": 78, "endOffset": 82}, {"referenceID": 26, "context": "Another category of nonlinear hashing techniques employs kernel functions [21][22][28][82].", "startOffset": 82, "endOffset": 86}, {"referenceID": 80, "context": "Another category of nonlinear hashing techniques employs kernel functions [21][22][28][82].", "startOffset": 86, "endOffset": 90}, {"referenceID": 26, "context": "[28] uses a kernel function to measure similarity of each points with a set of anchors resulting in nonlinear hashing.", "startOffset": 0, "endOffset": 4}, {"referenceID": 19, "context": "Kernerlized LSH uses a sparse set of datapoints to compute a kernel matrix and preform random projection in the kernel space to compute binary codes [21].", "startOffset": 149, "endOffset": 153}, {"referenceID": 25, "context": "Based on similar representation of kernel metric, Kulis and Darrell propose learning of hash functions by explicitly minimizing the reconstruction error in the kernel space and Hamming space [27].", "startOffset": 191, "endOffset": 195}, {"referenceID": 20, "context": "applies kernel representation but optimizes the hash functions by exploring the equivalence between optimizing the code inner products and the Hamming distances to achieve scale invariance [22].", "startOffset": 189, "endOffset": 193}, {"referenceID": 81, "context": "Such a procedure sequentially trains hash functions one bit at a time [83][25][84].", "startOffset": 70, "endOffset": 74}, {"referenceID": 23, "context": "Such a procedure sequentially trains hash functions one bit at a time [83][25][84].", "startOffset": 74, "endOffset": 78}, {"referenceID": 82, "context": "Such a procedure sequentially trains hash functions one bit at a time [83][25][84].", "startOffset": 78, "endOffset": 82}, {"referenceID": 23, "context": "For instance, the sequential projection based hashing aims to incorporate the bit correlations by iteratively updating the pairwise label matrix, where higher weights are imposed on point pairs violated by the previous hash functions [25].", "startOffset": 234, "endOffset": 238}, {"referenceID": 82, "context": "In the complementary projection learning approach [84], the authors present a sequential learning procedure to obtain a series of hash functions that cross the sparse data region, as well as generate balanced hash buckets.", "startOffset": 50, "endOffset": 54}, {"referenceID": 83, "context": "Other interesting learning ideas include twostep learning methods which treat hash bit learning and hash function learning separately [85][86].", "startOffset": 134, "endOffset": 138}, {"referenceID": 84, "context": "Other interesting learning ideas include twostep learning methods which treat hash bit learning and hash function learning separately [85][86].", "startOffset": 138, "endOffset": 142}, {"referenceID": 12, "context": "However, it is easy to observe that different bits often behave differently [14][32].", "startOffset": 76, "endOffset": 80}, {"referenceID": 30, "context": "However, it is easy to observe that different bits often behave differently [14][32].", "startOffset": 80, "endOffset": 84}, {"referenceID": 16, "context": "One of the representative approaches is Boosted Similarity Sensitive Coding (BSSC) [18].", "startOffset": 83, "endOffset": 87}, {"referenceID": 85, "context": "If one treats each hash function as a decision stump, the straightforward way of learning the weights is to directly apply adaptive boosting algorithm [87] as described in [18].", "startOffset": 151, "endOffset": 155}, {"referenceID": 16, "context": "If one treats each hash function as a decision stump, the straightforward way of learning the weights is to directly apply adaptive boosting algorithm [87] as described in [18].", "startOffset": 172, "endOffset": 176}, {"referenceID": 86, "context": "In [88], a boosting-style method called BoostMAP is proposed to map data points to weighted binary vectors that can leverage both metric and semantic similarity measures.", "startOffset": 3, "endOffset": 7}, {"referenceID": 72, "context": "Other weighted hashing methods include designing specific bit-level weighting schemes to improve the search accuracy [74][89][90][91][92].", "startOffset": 117, "endOffset": 121}, {"referenceID": 87, "context": "Other weighted hashing methods include designing specific bit-level weighting schemes to improve the search accuracy [74][89][90][91][92].", "startOffset": 121, "endOffset": 125}, {"referenceID": 88, "context": "Other weighted hashing methods include designing specific bit-level weighting schemes to improve the search accuracy [74][89][90][91][92].", "startOffset": 125, "endOffset": 129}, {"referenceID": 89, "context": "Other weighted hashing methods include designing specific bit-level weighting schemes to improve the search accuracy [74][89][90][91][92].", "startOffset": 129, "endOffset": 133}, {"referenceID": 90, "context": "Other weighted hashing methods include designing specific bit-level weighting schemes to improve the search accuracy [74][89][90][91][92].", "startOffset": 133, "endOffset": 137}, {"referenceID": 91, "context": "In addition, a recent work about designing a unified bit selection framework can be regarded as a special case of weighted hashing approach, where the weights of hash bits are binary [93].", "startOffset": 183, "endOffset": 187}, {"referenceID": 92, "context": "Another effective hash code ranking method is the query-sensitive hashing, which explores the raw feature of the query sample and learns query-specific weights of hash bits to achieve accurate \u01ebnearest neighbor search [94].", "startOffset": 218, "endOffset": 222}, {"referenceID": 12, "context": "In general, the nonlinear and supervised techniques tend to generate better performance than linear and unsupervised methods, while being more computationally costly [14][19][21][22][27].", "startOffset": 166, "endOffset": 170}, {"referenceID": 17, "context": "In general, the nonlinear and supervised techniques tend to generate better performance than linear and unsupervised methods, while being more computationally costly [14][19][21][22][27].", "startOffset": 170, "endOffset": 174}, {"referenceID": 19, "context": "In general, the nonlinear and supervised techniques tend to generate better performance than linear and unsupervised methods, while being more computationally costly [14][19][21][22][27].", "startOffset": 174, "endOffset": 178}, {"referenceID": 20, "context": "In general, the nonlinear and supervised techniques tend to generate better performance than linear and unsupervised methods, while being more computationally costly [14][19][21][22][27].", "startOffset": 178, "endOffset": 182}, {"referenceID": 25, "context": "In general, the nonlinear and supervised techniques tend to generate better performance than linear and unsupervised methods, while being more computationally costly [14][19][21][22][27].", "startOffset": 182, "endOffset": 186}, {"referenceID": 30, "context": "In the formulation of spectral hashing, the desired properties include keeping neighbors in input space as neighbors in the hamming space and requiring the codes to be balanced and uncorrelated [32].", "startOffset": 194, "endOffset": 198}, {"referenceID": 93, "context": "Motivated by the wellknown spectral graph analysis [95], the authors suggest to minimize the cost function with relaxed constraints.", "startOffset": 51, "endOffset": 55}, {"referenceID": 30, "context": "In particular, with the assumption of uniform data distribution, the spectral solution can be efficiently computed using 1D-Laplacian eigenfunctions [32].", "startOffset": 149, "endOffset": 153}, {"referenceID": 94, "context": "Hence, a \u201ckernel trick\u201d is used to alleviate the degraded performance when using long hash bits [96].", "startOffset": 96, "endOffset": 100}, {"referenceID": 26, "context": "Following the similar objective as spectral hashing, anchor graph hashing was designed to solve the problem from a different perspective without the assumption of uniform distribution [28].", "startOffset": 184, "endOffset": 188}, {"referenceID": 95, "context": "The key idea is to use a small set of MpM !Nq anchor points to approximate the graph structure represented by the matrix A such that the similarity between any pair of points can be approximated using point-to-anchor similarities [97].", "startOffset": 230, "endOffset": 234}, {"referenceID": 26, "context": "R M\u02c6K and \u03a3 \u201c diagp\u03c31,  \u0308  \u0308  \u0308 , \u03c3k,  \u0308  \u0308  \u0308 , \u03c3Kq P R K\u02c6K , where tvk,\u03c3ku are the eigenvector-eigenvalue pairs [28].", "startOffset": 114, "endOffset": 118}, {"referenceID": 96, "context": "The anchor graph hashing approach was recently further improved by leveraging a discrete optimization technique to directly solve binary hash codes without any relaxation [98].", "startOffset": 171, "endOffset": 175}, {"referenceID": 61, "context": "Since similarity is often measured by the cosine of the angle between pairs of samples, angular quantization is thus proposed to map non-negative feature vectors onto a vertex of the binary hypercube with the smallest angle [63].", "startOffset": 224, "endOffset": 228}, {"referenceID": 61, "context": "method [63].", "startOffset": 7, "endOffset": 11}, {"referenceID": 61, "context": "b4 \u201c r0 1 1sJ in the illustrated example [63].", "startOffset": 41, "endOffset": 45}, {"referenceID": 12, "context": "Note that the above formulation still generates a D-bit binary code for each data point, while compact codes are often desired in many real-world applications [14].", "startOffset": 159, "endOffset": 163}, {"referenceID": 61, "context": "To generate a K=bit code, a projection matrix S P R with orthogonal columns can be used to replace the rotation matrix R in the above objective with additional normalization, as discussed in [63].", "startOffset": 191, "endOffset": 195}, {"referenceID": 25, "context": "Instead of using data-independent random projections as in LSH or principal components as in SH, Kulis and Darrell [27] proposed data-dependent and bit-correlated hash functions as:", "startOffset": 115, "endOffset": 119}, {"referenceID": 17, "context": "figure in [19]).", "startOffset": 10, "endOffset": 14}, {"referenceID": 20, "context": "proposed to utilize the equivalence between code inner products and the Hamming distances to design supervised and kernel-based hash functions [22].", "startOffset": 143, "endOffset": 147}, {"referenceID": 20, "context": "Such a strategy of optimizing the hash code inner product in KSH rather than the Hamming distance like what\u2019s done in BRE pays off nicely and leads to major performance gains in similarity-based retrieval consistently confirmed in extensive experiments reported in [22] and recent studies [99].", "startOffset": 265, "endOffset": 269}, {"referenceID": 97, "context": "Such a strategy of optimizing the hash code inner product in KSH rather than the Hamming distance like what\u2019s done in BRE pays off nicely and leads to major performance gains in similarity-based retrieval consistently confirmed in extensive experiments reported in [22] and recent studies [99].", "startOffset": 289, "endOffset": 293}, {"referenceID": 17, "context": "Such learned metrics are then employed to the standard random projection based hash functions [19].", "startOffset": 94, "endOffset": 98}, {"referenceID": 98, "context": "Note that the matrix M can be learned through various metric learning method such as information-theoretic metric learning [100].", "startOffset": 123, "endOffset": 128}, {"referenceID": 68, "context": "Realizing that the pairwise constraints often come to be available incrementally, Jain et al exploit an efficient online locality-sensitive hashing with gradually learned distance metrics [70].", "startOffset": 188, "endOffset": 192}, {"referenceID": 12, "context": "The above objective can be solved using various optimization strategies, resulting in orthogonal or correlated binary codes, as described in [14][25].", "startOffset": 141, "endOffset": 145}, {"referenceID": 23, "context": "The above objective can be solved using various optimization strategies, resulting in orthogonal or correlated binary codes, as described in [14][25].", "startOffset": 145, "endOffset": 149}, {"referenceID": 29, "context": "employs similar semi-supervised formulation to sequentially learn multiple complementary hash tables to further improve the performance [31].", "startOffset": 136, "endOffset": 140}, {"referenceID": 72, "context": "As one of the representative methods falling into this category, column generation hashing explores the largemargin framework to leverage such type of proximity comparison information to design weighted hash functions [74].", "startOffset": 218, "endOffset": 222}, {"referenceID": 72, "context": "Different from other methods that explore the triplet relationship [74][75][65], the ranking supervised hashing method attempt to preserve the ranking order of a set of database points corresponding to the query point [76].", "startOffset": 67, "endOffset": 71}, {"referenceID": 73, "context": "Different from other methods that explore the triplet relationship [74][75][65], the ranking supervised hashing method attempt to preserve the ranking order of a set of database points corresponding to the query point [76].", "startOffset": 71, "endOffset": 75}, {"referenceID": 63, "context": "Different from other methods that explore the triplet relationship [74][75][65], the ranking supervised hashing method attempt to preserve the ranking order of a set of database points corresponding to the query point [76].", "startOffset": 75, "endOffset": 79}, {"referenceID": 74, "context": "Different from other methods that explore the triplet relationship [74][75][65], the ranking supervised hashing method attempt to preserve the ranking order of a set of database points corresponding to the query point [76].", "startOffset": 218, "endOffset": 222}, {"referenceID": 74, "context": "The augmented Lagrangian multiplier method was introduced to derive feasible solutions for the above constrained problem, as discussed in [76].", "startOffset": 138, "endOffset": 142}, {"referenceID": 79, "context": "Realizing that most of the current hashing techniques rely on linear projections, which could suffer from very high computational and storage costs for high-dimensional data, circulant binary embedding was recently developed to handle such a challenge using the circulant projection [81].", "startOffset": 283, "endOffset": 287}, {"referenceID": 99, "context": "Briefly, given a vector r \u201c tr0,  \u0308  \u0308  \u0308 , rd \u03011u, we can generate its corresponding circulant matrix R \u201c circprq [101].", "startOffset": 115, "endOffset": 120}, {"referenceID": 100, "context": "During the past decade (since around 2006), Deep Learning [102], also known as Deep Neural Networks, has drawn increasing attention and research efforts in a variety of artificial intelligence areas including speech recognition, computer vision, machine learning, text mining, etc.", "startOffset": 58, "endOffset": 63}, {"referenceID": 101, "context": "The earliest work in deep learning based hashing may be Semantic Hashing [103].", "startOffset": 73, "endOffset": 78}, {"referenceID": 102, "context": "Such a deep model is made as a stack of Restricted Boltzmann Machines (RBMs) [104].", "startOffset": 77, "endOffset": 82}, {"referenceID": 64, "context": "To enhance the performance of deep RBMs, a supervised version was proposed in [66], which borrows the idea of nonlinear Neighbourhood Component Analysis (NCA) embedding [105].", "startOffset": 78, "endOffset": 82}, {"referenceID": 103, "context": "To enhance the performance of deep RBMs, a supervised version was proposed in [66], which borrows the idea of nonlinear Neighbourhood Component Analysis (NCA) embedding [105].", "startOffset": 169, "endOffset": 174}, {"referenceID": 64, "context": "In [66], supervised deep RBMs using a Gaussian distribution to model visible units in the first layer were successfully applied to handle massive image data.", "startOffset": 3, "endOffset": 7}, {"referenceID": 97, "context": "A recent work named Sparse Similarity-Preserving Hashing [99] tried to address the low recall issue pertaining to relatively long hash codes, which affect most of previous hashing techniques.", "startOffset": 57, "endOffset": 61}, {"referenceID": 108, "context": "Within this architecture, two ISTA-type networks [110] that share the same set of parameters and conduct fast approximations of sparse coding are coupled in the training phase.", "startOffset": 49, "endOffset": 54}, {"referenceID": 97, "context": "In [99], an extension to hashing multimodal data, e.", "startOffset": 3, "endOffset": 7}, {"referenceID": 104, "context": "Another work named Deep Hashing [106] developed a deep neural network to learn a multiple hierarchical nonlinear transformation which maps original images to compact binary hash codes and hence supports large-scale image retrieval with the learned binary image representation.", "startOffset": 32, "endOffset": 37}, {"referenceID": 109, "context": "Similar constraints have been adopted in prior unsupervised hashing or binary coding methods such as Iterative Quantization (ITQ) [111].", "startOffset": 130, "endOffset": 135}, {"referenceID": 104, "context": "A supervised version called Supervised Deep Hashing was also presented in [106], where a discriminative term incorporating pairwise supervised information is added to the objective function of the deep hashing model.", "startOffset": 74, "endOffset": 79}, {"referenceID": 104, "context": "The authors of [106] showed the superiority of the supervised deep hashing model over its unsupervised counterpart.", "startOffset": 15, "endOffset": 20}, {"referenceID": 20, "context": "Specifically, the Sparse Similarity-Preserving Hashing method is found to be inferior to the state-of-the-art supervised hashing method Kernel-Based Supervised Hashing (KSH) [22] in terms of search accuracy on some image datasets [99]; the Deep Hashing method and its supervised version are slightly better than ITQ and its supervised version CCA+ITQ, respectively [111][106].", "startOffset": 174, "endOffset": 178}, {"referenceID": 97, "context": "Specifically, the Sparse Similarity-Preserving Hashing method is found to be inferior to the state-of-the-art supervised hashing method Kernel-Based Supervised Hashing (KSH) [22] in terms of search accuracy on some image datasets [99]; the Deep Hashing method and its supervised version are slightly better than ITQ and its supervised version CCA+ITQ, respectively [111][106].", "startOffset": 230, "endOffset": 234}, {"referenceID": 109, "context": "Specifically, the Sparse Similarity-Preserving Hashing method is found to be inferior to the state-of-the-art supervised hashing method Kernel-Based Supervised Hashing (KSH) [22] in terms of search accuracy on some image datasets [99]; the Deep Hashing method and its supervised version are slightly better than ITQ and its supervised version CCA+ITQ, respectively [111][106].", "startOffset": 365, "endOffset": 370}, {"referenceID": 104, "context": "Specifically, the Sparse Similarity-Preserving Hashing method is found to be inferior to the state-of-the-art supervised hashing method Kernel-Based Supervised Hashing (KSH) [22] in terms of search accuracy on some image datasets [99]; the Deep Hashing method and its supervised version are slightly better than ITQ and its supervised version CCA+ITQ, respectively [111][106].", "startOffset": 370, "endOffset": 375}, {"referenceID": 105, "context": "To remove this barrier, a recent method called Convolutional Neural Network Hashing [107] was developed to integrate image feature learning and hash value learning into a joint learning model.", "startOffset": 84, "endOffset": 89}, {"referenceID": 110, "context": "Given pairwise supervised information, this model consists of a stage of learning approximate hash codes and a stage of training a deep Convolutional Neural Network (CNN) [112] that outputs continuous hash values.", "startOffset": 171, "endOffset": 176}, {"referenceID": 106, "context": "Also based on CNNs, a latest method called as Deep Semantic Ranking Hashing [108] was presented to learn hash values such that multilevel semantic similarities among multi-labeled images are preserved.", "startOffset": 76, "endOffset": 81}, {"referenceID": 101, "context": "Semantic Hashing [103] text unsupervised no 4", "startOffset": 17, "endOffset": 22}, {"referenceID": 64, "context": "Restricted Boltzmann Machine [66] text and image supervised no 4 and 5", "startOffset": 29, "endOffset": 33}, {"referenceID": 97, "context": "Tailored Feed-Forward Neural Network [99] text and image supervised no 6", "startOffset": 37, "endOffset": 41}, {"referenceID": 104, "context": "Deep Hashing [106] image unsupervised no 3", "startOffset": 13, "endOffset": 18}, {"referenceID": 104, "context": "Supervised Deep Hashing [106] image supervised no 3", "startOffset": 24, "endOffset": 29}, {"referenceID": 105, "context": "Convolutional Neural Network Hashing [107] image supervised yes 5", "startOffset": 37, "endOffset": 42}, {"referenceID": 106, "context": "Deep Semantic Ranking Hashing [108] image supervised yes 8", "startOffset": 30, "endOffset": 35}, {"referenceID": 107, "context": "Deep Neural Network Hashing [109] image supervised yes 10", "startOffset": 28, "endOffset": 33}, {"referenceID": 105, "context": "The above Convolutional Neural Network Hashing method [107] requires separately learning approximate hash codes to guide the subsequent learning of image representation and finer hash values.", "startOffset": 54, "endOffset": 59}, {"referenceID": 107, "context": "A latest method called Deep Neural Network Hashing [109] goes beyond, in which the image representation and hash values are learned in one stage so that representation learning and hash learning are tightly coupled to benefit each other.", "startOffset": 51, "endOffset": 56}, {"referenceID": 106, "context": "Similar to the Deep Semantic Ranking Hashing method [108], the Deep Neural Network Hashing method incorporates listwise supervised information to train a deep CNN, giving rise to a currently deepest architecture for supervised hashing.", "startOffset": 52, "endOffset": 57}, {"referenceID": 107, "context": "In [109], the Deep Neural Network Hashing method was shown to surpass the Convolutional Neural Network Hashing method as well as several shallow learning based supervised hashing methods in terms of image search accuracy.", "startOffset": 3, "endOffset": 8}, {"referenceID": 111, "context": "The latest work [113] presented a hashing trick named HashedNets, which shrinks the storage costs of neural networks significantly while mostly preserving the generalization performance in image classification tasks.", "startOffset": 16, "endOffset": 21}, {"referenceID": 112, "context": "Hyperplane hashing is actually fairly important for many machine learning applications such as large-scale active learning with SVMs [114].", "startOffset": 133, "endOffset": 138}, {"referenceID": 113, "context": "In SVM-based active learning [115], the well proven sample selection strategy is to search in the unlabeled sample pool to identify the sample closest to the current hyperplane decision boundary, thus providing the most useful information for improving the learning model.", "startOffset": 29, "endOffset": 34}, {"referenceID": 114, "context": "The existing hyperplane hashing methods [116][117] all attempt to minimize a slightly modified \u201cdistance\u201d |wx| }w}}x} , i.", "startOffset": 40, "endOffset": 45}, {"referenceID": 115, "context": "The existing hyperplane hashing methods [116][117] all attempt to minimize a slightly modified \u201cdistance\u201d |wx| }w}}x} , i.", "startOffset": 45, "endOffset": 50}, {"referenceID": 115, "context": "[117] devised two different families of randomized hash functions to attack the hyperplane hashing problem.", "startOffset": 0, "endOffset": 5}, {"referenceID": 115, "context": "[117] designed a randomized function family with bilinear Bilinear-Hyperplane Hash (BH-Hash) as:", "startOffset": 0, "endOffset": 5}, {"referenceID": 115, "context": "proved in [117] that the probability of collision for a hyperplane query Pw and a database point x under h is", "startOffset": 10, "endOffset": 15}, {"referenceID": 116, "context": "extended the bilinear formulation to the conventional point-to-point hashing scheme through designing compact binary codes for highdimensional visual descriptors [118].", "startOffset": 162, "endOffset": 167}, {"referenceID": 117, "context": "Beyond the aforementioned conventional hashing which tackles searching in a database of vectors, subspace hashing [119], which has been rarely explored in the literature, attempts to efficiently search through a large database of subspaces.", "startOffset": 114, "endOffset": 119}, {"referenceID": 118, "context": "A common use scenario is to use a single face image to find the subspace (and the corresponding subject ID) closest to the query image [120].", "startOffset": 135, "endOffset": 140}, {"referenceID": 118, "context": "Given a query in the form of vector or subspace, searching for a nearest subspace in a subspace database is frequently encountered in a variety of practical applications including example-based image synthesis, scene classification, speaker recognition, face recognition, and motion-based action recognition [120].", "startOffset": 308, "endOffset": 313}, {"referenceID": 117, "context": "[119] presented a general framework to the problem of Approximate Nearest Subspace (ANS) search, which uniformly deals with the cases that query is a vector or subspace, query and database elements are subspaces of fixed dimension, query and database elements are subspaces of different dimension, and database elements are subspaces of varying di-", "startOffset": 0, "endOffset": 5}, {"referenceID": 117, "context": "The critical technique exploited by [119] is twostep: 1) a simple mapping that maps both query and database elements to \u201cpoints\u201d in a new vector space, and 2) doing approximate nearest neighbor search using conventional vector hashing algorithms in the new space.", "startOffset": 36, "endOffset": 41}, {"referenceID": 117, "context": "Consequently, the main contribution of [119] is reducing the difficult subspace hashing problem to a regular vector hashing task.", "startOffset": 39, "endOffset": 44}, {"referenceID": 117, "context": "[119] used LSH for the vector hashing task.", "startOffset": 0, "endOffset": 5}, {"referenceID": 117, "context": "While simple, the hashing technique (mapping + LSH) of [119] perhaps suffers from the high dimensionality of the constructed new vector space.", "startOffset": 55, "endOffset": 60}, {"referenceID": 118, "context": "More recently, [120] exclusively addressed the point-tosubspace query where query is a vector and database items are subspaces of arbitrary dimension.", "startOffset": 15, "endOffset": 20}, {"referenceID": 118, "context": "[120] proposed a rigorously faster hashing technique than that of [119].", "startOffset": 0, "endOffset": 5}, {"referenceID": 117, "context": "[120] proposed a rigorously faster hashing technique than that of [119].", "startOffset": 66, "endOffset": 71}, {"referenceID": 117, "context": "Its hash function can hash D-dimensional vectors (D is the ambient dimension of the query) or D\u02c6r-dimensional subspaces (r is arbitrary) in a linear time complexity OpDq, which is computationally more efficient than the hash functions devised in [119].", "startOffset": 246, "endOffset": 251}, {"referenceID": 118, "context": "[120] further proved the search time under the OpDq hashes to be sublinear in the database size.", "startOffset": 0, "endOffset": 5}, {"referenceID": 118, "context": "Based on the nice finding of [120], we would like to achieve faster hashing for the subspace-to-subspace query by means of crafted novel hash functions to handle subspaces in varying dimension.", "startOffset": 29, "endOffset": 34}, {"referenceID": 119, "context": "Some recent advanced methods are proposed to design the hash functions for more complex settings, such as that the data are represented by multimodal features or the data are formed in a heterogeneous way [121].", "startOffset": 205, "endOffset": 210}, {"referenceID": 120, "context": "Realizing that data items like webpage can be described from multiple information sources, composing hashing was recently proposed to design hashing schme using several information sources [122].", "startOffset": 189, "endOffset": 194}, {"referenceID": 121, "context": "Co-regularized hashing was proposed to investigate the hashing learning across multiparity data in a supervised setting, where similar and dissimilar pairs of intra-modality points are given as supervision information [123].", "startOffset": 218, "endOffset": 223}, {"referenceID": 122, "context": "Dual-view hashing attempts to derive a hidden common Hamming embedding of data from two views, while maintaining the predictability of the binary codes [124].", "startOffset": 152, "endOffset": 157}, {"referenceID": 123, "context": "A probabilistic model called multimodal latent binary embedding was recently presented to derive binary latent factors in a common Hamming space for indexing multimodal data [125].", "startOffset": 174, "endOffset": 179}, {"referenceID": 124, "context": "Other closely related hashing methods include the design of multiple feature hashing for near-duplicate duplicate detection [126], submodular hashing for video indexing [127], and Probabilistic Attributed Hashing for integrating low-level features and semantic attributes [128].", "startOffset": 124, "endOffset": 129}, {"referenceID": 125, "context": "Other closely related hashing methods include the design of multiple feature hashing for near-duplicate duplicate detection [126], submodular hashing for video indexing [127], and Probabilistic Attributed Hashing for integrating low-level features and semantic attributes [128].", "startOffset": 169, "endOffset": 174}, {"referenceID": 126, "context": "Other closely related hashing methods include the design of multiple feature hashing for near-duplicate duplicate detection [126], submodular hashing for video indexing [127], and Probabilistic Attributed Hashing for integrating low-level features and semantic attributes [128].", "startOffset": 272, "endOffset": 277}, {"referenceID": 67, "context": "Especially, due to the well-known semantic gap, supervised and semi-supervised hashing methods have been extensively studied for image search and retrieval [69][29][41][129][130][131], mobile product search[132].", "startOffset": 156, "endOffset": 160}, {"referenceID": 27, "context": "Especially, due to the well-known semantic gap, supervised and semi-supervised hashing methods have been extensively studied for image search and retrieval [69][29][41][129][130][131], mobile product search[132].", "startOffset": 160, "endOffset": 164}, {"referenceID": 39, "context": "Especially, due to the well-known semantic gap, supervised and semi-supervised hashing methods have been extensively studied for image search and retrieval [69][29][41][129][130][131], mobile product search[132].", "startOffset": 164, "endOffset": 168}, {"referenceID": 127, "context": "Especially, due to the well-known semantic gap, supervised and semi-supervised hashing methods have been extensively studied for image search and retrieval [69][29][41][129][130][131], mobile product search[132].", "startOffset": 168, "endOffset": 173}, {"referenceID": 128, "context": "Especially, due to the well-known semantic gap, supervised and semi-supervised hashing methods have been extensively studied for image search and retrieval [69][29][41][129][130][131], mobile product search[132].", "startOffset": 173, "endOffset": 178}, {"referenceID": 129, "context": "Especially, due to the well-known semantic gap, supervised and semi-supervised hashing methods have been extensively studied for image search and retrieval [69][29][41][129][130][131], mobile product search[132].", "startOffset": 178, "endOffset": 183}, {"referenceID": 130, "context": "Especially, due to the well-known semantic gap, supervised and semi-supervised hashing methods have been extensively studied for image search and retrieval [69][29][41][129][130][131], mobile product search[132].", "startOffset": 206, "endOffset": 211}, {"referenceID": 131, "context": "Other closely related computer vision applications include image patch matching [133], image classification [118], face recognition [134][135], pose estimation [71], object tracking [136], and duplicate detection [126][137][52][51][138].", "startOffset": 80, "endOffset": 85}, {"referenceID": 116, "context": "Other closely related computer vision applications include image patch matching [133], image classification [118], face recognition [134][135], pose estimation [71], object tracking [136], and duplicate detection [126][137][52][51][138].", "startOffset": 108, "endOffset": 113}, {"referenceID": 132, "context": "Other closely related computer vision applications include image patch matching [133], image classification [118], face recognition [134][135], pose estimation [71], object tracking [136], and duplicate detection [126][137][52][51][138].", "startOffset": 132, "endOffset": 137}, {"referenceID": 133, "context": "Other closely related computer vision applications include image patch matching [133], image classification [118], face recognition [134][135], pose estimation [71], object tracking [136], and duplicate detection [126][137][52][51][138].", "startOffset": 137, "endOffset": 142}, {"referenceID": 69, "context": "Other closely related computer vision applications include image patch matching [133], image classification [118], face recognition [134][135], pose estimation [71], object tracking [136], and duplicate detection [126][137][52][51][138].", "startOffset": 160, "endOffset": 164}, {"referenceID": 134, "context": "Other closely related computer vision applications include image patch matching [133], image classification [118], face recognition [134][135], pose estimation [71], object tracking [136], and duplicate detection [126][137][52][51][138].", "startOffset": 182, "endOffset": 187}, {"referenceID": 124, "context": "Other closely related computer vision applications include image patch matching [133], image classification [118], face recognition [134][135], pose estimation [71], object tracking [136], and duplicate detection [126][137][52][51][138].", "startOffset": 213, "endOffset": 218}, {"referenceID": 135, "context": "Other closely related computer vision applications include image patch matching [133], image classification [118], face recognition [134][135], pose estimation [71], object tracking [136], and duplicate detection [126][137][52][51][138].", "startOffset": 218, "endOffset": 223}, {"referenceID": 50, "context": "Other closely related computer vision applications include image patch matching [133], image classification [118], face recognition [134][135], pose estimation [71], object tracking [136], and duplicate detection [126][137][52][51][138].", "startOffset": 223, "endOffset": 227}, {"referenceID": 49, "context": "Other closely related computer vision applications include image patch matching [133], image classification [118], face recognition [134][135], pose estimation [71], object tracking [136], and duplicate detection [126][137][52][51][138].", "startOffset": 227, "endOffset": 231}, {"referenceID": 136, "context": "Other closely related computer vision applications include image patch matching [133], image classification [118], face recognition [134][135], pose estimation [71], object tracking [136], and duplicate detection [126][137][52][51][138].", "startOffset": 231, "endOffset": 236}, {"referenceID": 137, "context": "In addition, this emerging hash learning framework can be exploited for some general machine learning and data mining tasks, including crossmodality data fusion [139], large scale optimization [140], large scale classification and regression [141], collaborative filtering [142], and recommendation [143].", "startOffset": 161, "endOffset": 166}, {"referenceID": 138, "context": "In addition, this emerging hash learning framework can be exploited for some general machine learning and data mining tasks, including crossmodality data fusion [139], large scale optimization [140], large scale classification and regression [141], collaborative filtering [142], and recommendation [143].", "startOffset": 193, "endOffset": 198}, {"referenceID": 139, "context": "In addition, this emerging hash learning framework can be exploited for some general machine learning and data mining tasks, including crossmodality data fusion [139], large scale optimization [140], large scale classification and regression [141], collaborative filtering [142], and recommendation [143].", "startOffset": 242, "endOffset": 247}, {"referenceID": 140, "context": "In addition, this emerging hash learning framework can be exploited for some general machine learning and data mining tasks, including crossmodality data fusion [139], large scale optimization [140], large scale classification and regression [141], collaborative filtering [142], and recommendation [143].", "startOffset": 273, "endOffset": 278}, {"referenceID": 141, "context": "In addition, this emerging hash learning framework can be exploited for some general machine learning and data mining tasks, including crossmodality data fusion [139], large scale optimization [140], large scale classification and regression [141], collaborative filtering [142], and recommendation [143].", "startOffset": 299, "endOffset": 304}, {"referenceID": 142, "context": "proposed a structure learning framework to derive a video hashing technique that incorporates both temporal and spatial structure information [144].", "startOffset": 142, "endOffset": 147}, {"referenceID": 143, "context": "proposed to leverage both tag information and semantic topic modeling to achieve more accurate hash codes [145].", "startOffset": 106, "endOffset": 111}, {"referenceID": 144, "context": "designed a two-stage unsupervised hashing framework for fast document retrieval [146].", "startOffset": 80, "endOffset": 85}, {"referenceID": 112, "context": "Without performing exhaustive test on all the data points, hyperplane hashing can help significantly speed up the interactive training sample selection procedure [114][117][116].", "startOffset": 162, "endOffset": 167}, {"referenceID": 115, "context": "Without performing exhaustive test on all the data points, hyperplane hashing can help significantly speed up the interactive training sample selection procedure [114][117][116].", "startOffset": 167, "endOffset": 172}, {"referenceID": 114, "context": "Without performing exhaustive test on all the data points, hyperplane hashing can help significantly speed up the interactive training sample selection procedure [114][117][116].", "startOffset": 172, "endOffset": 177}, {"referenceID": 145, "context": "In addition, a two-stage hashing scheme is developed to achieve fast query pair selection for large scale active learning to rank [147].", "startOffset": 130, "endOffset": 135}, {"referenceID": 114, "context": "Although several recent techniques have presented theoretical analysis of the collision probability, they are mostly based on randomized hash functions [116][117][19].", "startOffset": 152, "endOffset": 157}, {"referenceID": 115, "context": "Although several recent techniques have presented theoretical analysis of the collision probability, they are mostly based on randomized hash functions [116][117][19].", "startOffset": 157, "endOffset": 162}, {"referenceID": 17, "context": "Although several recent techniques have presented theoretical analysis of the collision probability, they are mostly based on randomized hash functions [116][117][19].", "startOffset": 162, "endOffset": 166}, {"referenceID": 146, "context": "Due to their compact form, the hash codes also have great potential in many other large scale data modeling tasks such as efficient nonlinear kernel SVM classifiers [148] and rapid kernel approximation [149].", "startOffset": 165, "endOffset": 170}, {"referenceID": 147, "context": "Due to their compact form, the hash codes also have great potential in many other large scale data modeling tasks such as efficient nonlinear kernel SVM classifiers [148] and rapid kernel approximation [149].", "startOffset": 202, "endOffset": 207}], "year": 2015, "abstractText": "The explosive growth in big data has attracted much attention in designing efficient indexing and search methods recently. In many critical applications such as large-scale search and pattern matching, finding the nearest neighbors to a query is a fundamental research problem. However, the straightforward solution using exhaustive comparison is infeasible due to the prohibitive computational complexity and memory requirement. In response, Approximate Nearest Neighbor (ANN) search based on hashing techniques has become popular due to its promising performance in both efficiency and accuracy. Prior randomized hashing methods, e.g., Locality-Sensitive Hashing (LSH), explore data-independent hash functions with random projections or permutations. Although having elegant theoretic guarantees on the search quality in certain metric spaces, performance of randomized hashing has been shown insufficient in many real-world applications. As a remedy, new approaches incorporating data-driven learning methods in development of advanced hash functions have emerged. Such learning to hash methods exploit information such as data distributions or class labels when optimizing the hash codes or functions. Importantly, the learned hash codes are able to preserve the proximity of neighboring data in the original feature spaces in the hash code spaces. The goal of this paper is to provide readers with systematic understanding of insights, pros and cons of the emerging techniques. We provide a comprehensive survey of the learning to hash framework and representative techniques of various types, including unsupervised, semi-supervised, and supervised. In addition, we also summarize recent hashing approaches utilizing the deep learning models. Finally, we discuss the future direction and trends of research in this area.", "creator": "LaTeX with hyperref package"}}}