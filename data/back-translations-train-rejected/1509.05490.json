{"id": "1509.05490", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "18-Sep-2015", "title": "TransA: An Adaptive Approach for Knowledge Graph Embedding", "abstract": "Knowledge representation is a major topic in AI, and many studies attempt to represent entities and relations of knowledge base in a continuous vector space. Among these attempts, translation-based methods build entity and relation vectors by minimizing the translation loss from a head entity to a tail one. In spite of the success of these methods, translation-based methods also suffer from the oversimplified loss metric, and are not competitive enough to model various and complex entities/relations in knowledge bases. To address this issue, we propose \\textbf{TransA}, an adaptive metric approach for embedding, utilizing the metric learning ideas to provide a more flexible embedding method. Experiments are conducted on the benchmark datasets and our proposed method makes significant and consistent improvements over the state-of-the-art baselines.", "histories": [["v1", "Fri, 18 Sep 2015 02:40:07 GMT  (365kb)", "http://arxiv.org/abs/1509.05490v1", null], ["v2", "Mon, 28 Sep 2015 02:21:20 GMT  (158kb,D)", "http://arxiv.org/abs/1509.05490v2", null]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["han xiao", "minlie huang", "yu hao", "xiaoyan zhu"], "accepted": false, "id": "1509.05490"}, "pdf": {"name": "1509.05490.pdf", "metadata": {"source": "CRF", "title": "TransA: An Adaptive Approach for Knowledge Graph Embedding", "authors": [], "emails": [], "sections": [{"heading": null, "text": "ar Xiv: 150 9.05 490v 1 [cs.C L] 18 Sep 20"}, {"heading": "Introduction", "text": "It is a question of whether and to what extent the people in the USA, in Europe, in Europe, in Europe, in the USA, in Europe, in the world, in the USA, in Europe, in the USA, in the USA, in the USA, in Europe, in the USA, in the USA, in Europe, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in"}, {"heading": "Related Work", "text": "Preliminary studies are divided into two lines: one is the translation-based embedding method and the other includes many other embedding methods."}, {"heading": "Translation-Based Embedding Methods", "text": "All translation-based methods share a common principle h + r \u2248 t, but differ in the definition of the relationship-based space in which a head unit h connects to a tail unit t. This principle suggests that t should be the closest neighbor of (h + r). Therefore, the translation-based methods all have the same form of the score function that is applied to the Euclidean distance to measure the loss as follows: \u2022 TransE (h, t) = | hr + r \u2212 tr | 2 2where hr, tr are the entity that embeds vectors projected into the relationship-specific space. Note that this branch of methods maintains state-of-the-art performance. \u2022 TransE (Bordes et al. 2013) creates the units in the original space, say hr = h, tr = t. \u2022 TransH (Wang et al. 2014) projects the units into a hyperplane to solve the problem of the complex relation, we say, w-hr-situations-hr."}, {"heading": "Other Embedding Methods", "text": "There are also many other models of knowledge diagrams that are embedded. Unstructured models (UM) have been used in this approach. UM (Bordes et al. 2012) is a simplified version of TransE by setting all relation vectors to zero r = 0. Obviously, the relation is not taken into account in this model. SE model (Bordes et al. 2011) applies two relationship-related matrices, one for head and the other for tail. The score function is defined as fr (h, t) = | Mh \u2212 Mt, rt | 2. According to (Socher et al. 2013), this model cannot capture the relationship between entities and relations. Single Layer Model (SLM) applies neural network to knowledge diagram horizontal embedding. The score function is defined asfr (h, t) = u r g (Mr, 1h + Mr, 2t)."}, {"heading": "Adaptive Metric Approach", "text": "In this section, we will present the TransA adaptive metric approach and the theoretical analysis from two perspectives."}, {"heading": "Adaptive Metric Score Function", "text": "As mentioned in the \"Introduction,\" all translation-based methods follow the same principle h + r \u2248 t, but they differ in the relation-specific spaces into which entities are projected. Therefore, such methods have a similar score function. fr (h, t) = | h + r \u2212 t | 2 = (h + r \u2212 t) (1) This score function is actually a Euclidean measurement. The disadvantages of overly simplified measurement were discussed in the \"Introduction.\" Consequently, the proposed TransA replaces the inflexible euclidean distance with an adaptive Mahalanobis distance of absolute loss, because the distance from Mahalanobis is more flexible and adaptable (Wang and Sun 2014). Therefore, our score function is as follows: fr (h, t) = (h + r \u2212 t |) Wr \u2212 t (2)."}, {"heading": "Perspective from Equipotential Surfaces", "text": "TransA shares almost the same geometric explanations with other translation-based methods, but they differ in loss metrics. In other translation-based methods, the equivalent potential hyper-surfaces are spheres as defined by Euclidean distance: | | (t \u2212 h) \u2212 r | | 22 = C (3), where C is the threshold or equivalent potential value. In TransA, however, the equivalent potential hyper-surfaces are elliptical surfaces such as the Mahalanobis distance of absolute loss states (Kulis 2012): | (t \u2212 h) \u2212 r | (t \u2212 h) \u2212 r | = C (4) Note that the elliptical hyper-surfaces would be a bit distorted as the absolute operator uses them, but that makes no difference to the analysis of TransA's performance. As we know, different equivalent potential hyper-surfaces correspond to different thresholds and different thresholds that determine whether the triple metrics are correct or not."}, {"heading": "Perspective from Feature Weighting", "text": "For the weight matrix Wr, which is symmetrical, we obtain the equivalent unique form by LDL decomposition (Golub and Van Loan 2012) as follows: Wr = L'r DrLr (5) fr = (Lr | h + r \u2212 t |) Dr (Lr | h + r \u2212 t |) (6) In the above equations, Lr can be considered a transformation matrix that transforms the loss vector | h + r \u2212 t | into another space. Furthermore, Dr = diag (w1, w2, w3....) is a diagonal matrix and different embedding dimensions are weighted by wiring. As analyzed in \"Introduction,\" a relationship could only be influenced by several specific dimensions, while the other dimensions would be noisy. Treating different dimensions identically in current translation-based methods can hardly suppress noise, thus achieving unsatisfactory performance."}, {"heading": "Connection to Previous Works", "text": "With regard to TransR, which rotates and scales the embedding spaces, TransA has two advantages over TransM. Firstly, we weight features to avoid noise. Secondly, we relax the PSD condition for flexible representation. With regard to TransM, which has dimensions in the weighting using pre-calculated coefficients, TransA has two advantages over TransM. Firstly, we learn the weights from the data, which makes the score function more adaptable. Secondly, we apply the feature transformation, which makes embedding more effective."}, {"heading": "Training Algorithm", "text": "To train the model, we use the margin-based ranking error. Taking other limitations into account, the target function can be defined as follows: Min. (h, r, t), Min. (h, r, t), Min. (h, t), Min. (h, t), Min. (h, t), Min. (h, t), Min. (h, r, t), Min. (h), Min. (h), Min. (h), Min. (h), Min. (h), Min. (h), Min. (h), Min. (h), Min. (h), Min. (h), Min. (h), Min. (h), Min. (h), Min. (h), Min. (h), Min. (h). (h)."}, {"heading": "Link Prediction", "text": "There are two metrics for the rating: the average Rank-Rank-Rank-Rank-Rating-Rating-Rating-Rating-Rating-Rating-Rating-Rating-Rating-Rating-Rating-Rating-Rating-Rating-Rating-Rating-Rating-Rating-Rating-Rating-Rating-Rating-Rating-Rating-Rating-Rating-Rating-Rating-Rating-Rating-Rating-Rating-Rating-Rating-Rating-Rating-Rating-Rating-Rating-Rating-Rating-Rating-Rating-Rating-Rating-Rating-Rating-Rating-Rating-Rating-Rating-Rating-Rating-Rating-Rating-Rating-Rating-Rating-Rating-Rating-Rating-Rating-Rating-Rating-Rating-Rating-Rating-Rating-Rating-Rating-Rating-Rating-Rating-Rating-Rating-Rating-Rating-Rating-Rating-Rating-Rating-Rating-Rating-Rating-Rating-Rating-Rating-Rating-Rating-Rating-Rating-Rating-Rating-Rating-Rating-Rating-Rating-Rating-Rating-Rating-Rating-Rating-Rating-Rating-Rating-Rating-Rating-Rating-Rating-Rating-Rating-Rating-Rating-Rating-Rating-Rating-Rating-Rating-Rating-Rating-Rating-Rating-Rating-Rating-Rating-Rating-Rating-Rating-Rating-Rating-Rating-Rating-Rating-Rating-Rating-Rating-Rating-Rating-Rating-Rating-Rating-Rating-Rating-Rating-Rating-Rating-Rating-Rating-Rating-Rating-Rating-Rating-Rating-Rating-Rating-Rating-Rating-Rating-Rating-Rating-Rating-Rating-R"}, {"heading": "Triples Classification", "text": "The Triples classification is a classical task in the knowledge base, which aims to predict whether a given triple (h, r, t) is correct or not. Our evaluation protocol is the same as previous studies. In addition, WN11 and FB13 are the benchmark data sets for this task. The evaluation of the classification requires negative labels. The data sets have already been created with negative triples, each correct triple being corrected to obtain a negative triple. The decision is as follows: for a triple (h, r, t), if fr (h, t) is below a threshold, then positive; otherwise negative. The thresholds {\u03c3r} are determined using the validation data sets. The final accuracy is based on how many triples are correctly classified. Since all methods use the same data sets, we directly copy the results of different methods from the literature."}, {"heading": "Conclusion", "text": "In this paper, we propose TransA, a translation-based knowledge base for embedding methods with an adaptive and flexible metric. TransA uses elliptical equipotential hypersurfaces to characterize the embedding of topologies and several specific characteristic dimensions for a relationship to avoid much noise. Thus, our adaptive metric approach models different and complex entities / relationships in the knowledge base. Experiments are performed with two benchmark tasks and the results show that TransA achieves consistent and significant improvements over the current state of artificial baselines. To reproduce our results, our codes and data are put into github.References Bao, J.; Duan, N.; Zhou, M.; and Zhao, A. Kettledgebased question as machine translation."}, {"heading": "Asia Conference on Language, Information, and Computa-", "text": "In the second half of the last decade, the number of unemployed in the US has multiplied. (In the second half of the last decade, the number of unemployed in the US has doubled. (In the second half of the last decade, the number of unemployed in the US has multiplied.) (In the third half of the last decade, the number of unemployed in the US has doubled. (In the third half of the last decade, the number of unemployed in the US has doubled.). (In the third half of the last decade, the number of unemployed in the US has doubled.). (In the second half of the last decade, the number of unemployed in the US has multiplied.). (In the second half of the last decade, the number of unemployed in the US has multiplied."}], "references": [{"title": "Knowledgebased question answering as machine translation", "author": ["J. Bao", "N. Duan", "M. Zhou", "T. Zhao"], "venue": "Cell 2:6.", "citeRegEx": "Bao et al\\.,? 2014", "shortCiteRegEx": "Bao et al\\.", "year": 2014}, {"title": "Freebase: a collaboratively created graph database for structuring human knowledge", "author": ["K. Bollacker", "C. Evans", "P. Paritosh", "T. Sturge", "J. Taylor"], "venue": "Proceedings of the 2008 ACM SIGMOD international conference on Management of data, 1247\u20131250. ACM.", "citeRegEx": "Bollacker et al\\.,? 2008", "shortCiteRegEx": "Bollacker et al\\.", "year": 2008}, {"title": "Learning structured embeddings of knowledge bases", "author": ["A. Bordes", "J. Weston", "R. Collobert", "Y Bengio"], "venue": "In Proceedings of the Twenty-fifth AAAI Conference on Artificial Intelligence", "citeRegEx": "Bordes et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Bordes et al\\.", "year": 2011}, {"title": "Joint learning of words and meaning representations for open-text semantic parsing", "author": ["A. Bordes", "X. Glorot", "J. Weston", "Y. Bengio"], "venue": "International Conference on Artificial Intelligence and Statistics, 127\u2013135.", "citeRegEx": "Bordes et al\\.,? 2012", "shortCiteRegEx": "Bordes et al\\.", "year": 2012}, {"title": "Translating embeddings for modeling multi-relational data", "author": ["A. Bordes", "N. Usunier", "A. Garcia-Duran", "J. Weston", "O. Yakhnenko"], "venue": "Advances in Neural Information Processing Systems, 2787\u20132795.", "citeRegEx": "Bordes et al\\.,? 2013", "shortCiteRegEx": "Bordes et al\\.", "year": 2013}, {"title": "A semantic matching energy function for learning with multirelational data", "author": ["A. Bordes", "X. Glorot", "J. Weston", "Y. Bengio"], "venue": "Machine Learning 94(2):233\u2013259.", "citeRegEx": "Bordes et al\\.,? 2014", "shortCiteRegEx": "Bordes et al\\.", "year": 2014}, {"title": "A unified architecture for natural language processing: Deep neural networks with multitask learning", "author": ["R. Collobert", "J. Weston"], "venue": "Proceedings of the 25th international conference on Machine learning, 160\u2013167. ACM.", "citeRegEx": "Collobert and Weston,? 2008", "shortCiteRegEx": "Collobert and Weston", "year": 2008}, {"title": "Open question answering over curated and extracted knowledge bases", "author": ["A. Fader", "L. Zettlemoyer", "O. Etzioni"], "venue": "Proceedings of the 20th ACM SIGKDD international conference on Knowledge discovery and data mining, 1156\u20131165. ACM.", "citeRegEx": "Fader et al\\.,? 2014", "shortCiteRegEx": "Fader et al\\.", "year": 2014}, {"title": "Transition-based knowledge graph embedding with relational mapping properties", "author": ["M. Fan", "Q. Zhou", "E. Chang", "T.F. Zheng"], "venue": "Proceedings of the 28th Pacific Asia Conference on Language, Information, and Computation, 328\u2013337.", "citeRegEx": "Fan et al\\.,? 2014", "shortCiteRegEx": "Fan et al\\.", "year": 2014}, {"title": "Matrix computations, volume 3", "author": ["G.H. Golub", "C.F. Van Loan"], "venue": "JHU Press.", "citeRegEx": "Golub and Loan,? 2012", "shortCiteRegEx": "Golub and Loan", "year": 2012}, {"title": "Semantically smooth knowledge graph embedding", "author": ["S. Guo", "Q. Wang", "B. Wang", "L. Wang", "L. Guo"], "venue": "Proceedings of ACL.", "citeRegEx": "Guo et al\\.,? 2015", "shortCiteRegEx": "Guo et al\\.", "year": 2015}, {"title": "A latent factor model for highly multi-relational data", "author": ["R. Jenatton", "N.L. Roux", "A. Bordes", "G.R. Obozinski"], "venue": "Advances in Neural Information Processing Systems, 3167\u20133175.", "citeRegEx": "Jenatton et al\\.,? 2012", "shortCiteRegEx": "Jenatton et al\\.", "year": 2012}, {"title": "Metric learning: A survey", "author": ["B. Kulis"], "venue": "Foundations & Trends in Machine Learning 5(4):287\u2013364.", "citeRegEx": "Kulis,? 2012", "shortCiteRegEx": "Kulis", "year": 2012}, {"title": "Learning entity and relation embeddings for knowledge graph completion", "author": ["Y. Lin", "Z. Liu", "M. Sun", "Y. Liu", "X. Zhu"], "venue": "Proceedings of the Twenty-Ninth AAAI Conference on Artificial Intelligence.", "citeRegEx": "Lin et al\\.,? 2015", "shortCiteRegEx": "Lin et al\\.", "year": 2015}, {"title": "Modeling relation paths for representation learning of knowledge bases", "author": ["Y. Lin", "Z. Liu", "M. Sun"], "venue": "Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing (EMNLP). Association for Computational Linguistics.", "citeRegEx": "Lin et al\\.,? 2015", "shortCiteRegEx": "Lin et al\\.", "year": 2015}, {"title": "Wordnet: a lexical database for english", "author": ["G.A. Miller"], "venue": "Communications of the ACM 38(11):39\u201341.", "citeRegEx": "Miller,? 1995", "shortCiteRegEx": "Miller", "year": 1995}, {"title": "A threeway model for collective learning on multi-relational data", "author": ["M. Nickel", "V. Tresp", "H.-P. Kriegel"], "venue": "Proceedings of the 28th international conference on machine learning (ICML-11), 809\u2013816.", "citeRegEx": "Nickel et al\\.,? 2011", "shortCiteRegEx": "Nickel et al\\.", "year": 2011}, {"title": "Factorizing yago: scalable machine learning for linked data", "author": ["M. Nickel", "V. Tresp", "H.-P. Kriegel"], "venue": "Proceedings of the 21st international conference on World Wide Web, 271\u2013280. ACM.", "citeRegEx": "Nickel et al\\.,? 2012", "shortCiteRegEx": "Nickel et al\\.", "year": 2012}, {"title": "Reasoning with neural tensor networks for knowledge base completion", "author": ["R. Socher", "D. Chen", "C.D. Manning", "A. Ng"], "venue": "Advances in Neural Information Processing Systems, 926\u2013934.", "citeRegEx": "Socher et al\\.,? 2013", "shortCiteRegEx": "Socher et al\\.", "year": 2013}, {"title": "Survey on distance metric learning and dimensionality reduction in data mining", "author": ["F. Wang", "J. Sun"], "venue": "Data Mining and Knowledge Discovery 1\u201331.", "citeRegEx": "Wang and Sun,? 2014", "shortCiteRegEx": "Wang and Sun", "year": 2014}, {"title": "Knowledge graph embedding by translating on hyperplanes", "author": ["Z. Wang", "J. Zhang", "J. Feng", "Z. Chen"], "venue": "Proceedings of the Twenty-Eighth AAAI Conference on Artificial Intelligence, 1112\u20131119.", "citeRegEx": "Wang et al\\.,? 2014", "shortCiteRegEx": "Wang et al\\.", "year": 2014}, {"title": "Knowledge base completion using embeddings and rules", "author": ["Q. Wang", "B. Wang", "L. Guo"], "venue": "Proceedings of the 24th International Joint Conference on Artificial Intelligence.", "citeRegEx": "Wang et al\\.,? 2015", "shortCiteRegEx": "Wang et al\\.", "year": 2015}], "referenceMentions": [{"referenceID": 15, "context": "Knowledge graphs such as Wordnet (Miller 1995) and Freebase (Bollacker et al.", "startOffset": 33, "endOffset": 46}, {"referenceID": 1, "context": "Knowledge graphs such as Wordnet (Miller 1995) and Freebase (Bollacker et al. 2008) play an important role in AI researches and applications.", "startOffset": 60, "endOffset": 83}, {"referenceID": 0, "context": "Recent researches such as query expansion prefer involving knowledge graphs (Bao et al. 2014) while some industrial applications such as question answering robots are also powered by knowledge graphs (Fader, Zettlemoyer, and Etzioni 2014).", "startOffset": 76, "endOffset": 93}, {"referenceID": 4, "context": "To provide a general paradigm to support computing on knowledge graph, various knowledge graph embedding methods have been proposed, such as TransE (Bordes et al. 2013), TransH (Wang et al.", "startOffset": 148, "endOffset": 168}, {"referenceID": 20, "context": "2013), TransH (Wang et al. 2014) and TransR (Lin et al.", "startOffset": 14, "endOffset": 32}, {"referenceID": 13, "context": "2014) and TransR (Lin et al. 2015).", "startOffset": 17, "endOffset": 34}, {"referenceID": 4, "context": "\u2022 TransE (Bordes et al. 2013) lays the entities in the original space, say hr = h, tr = t.", "startOffset": 9, "endOffset": 29}, {"referenceID": 20, "context": "\u2022 TransH (Wang et al. 2014) projects the entities into a hyperplane for addressing the issue of complex relation embedding, say hr = h\u2212w r hwr, tr = t\u2212w \u22a4 r twr.", "startOffset": 9, "endOffset": 27}, {"referenceID": 13, "context": "\u2022 TransR (Lin et al. 2015) transforms the entities by the same matrix to also address the issue of complex relation embedding, as: hr = Mrh, tr = Mrt.", "startOffset": 9, "endOffset": 26}, {"referenceID": 8, "context": "TransM (Fan et al. 2014) pre-calculates the distinct weight for each training triple to perform better.", "startOffset": 7, "endOffset": 24}, {"referenceID": 3, "context": "The UM (Bordes et al. 2012) is a simplified version of TransE by setting all the relation vectors to zero r = 0.", "startOffset": 7, "endOffset": 27}, {"referenceID": 2, "context": "The SE model (Bordes et al. 2011) applies two relation-related matrices, one for head and the other for tail.", "startOffset": 13, "endOffset": 33}, {"referenceID": 18, "context": "According to (Socher et al. 2013), this model cannot capture the relationship among entities and relations.", "startOffset": 13, "endOffset": 33}, {"referenceID": 6, "context": "(Collobert and Weston 2008) had proposed a similar method but applied this approach into the language model.", "startOffset": 0, "endOffset": 27}, {"referenceID": 3, "context": "The SME model (Bordes et al. 2012) (Bordes et al.", "startOffset": 14, "endOffset": 34}, {"referenceID": 5, "context": "2012) (Bordes et al. 2014) attempts to capture the correlations between entities and relations by matrix product and Hadamard product.", "startOffset": 6, "endOffset": 26}, {"referenceID": 5, "context": "In some recent work (Bordes et al. 2014), the second form of score function is re-defined with 3-way tensors instead of matrices.", "startOffset": 20, "endOffset": 40}, {"referenceID": 11, "context": "The LFM (Jenatton et al. 2012) uses the second-order correlations between entities by a quadratic form, defined as fr(h, t) = hWrt.", "startOffset": 8, "endOffset": 30}, {"referenceID": 18, "context": "The NTN model (Socher et al. 2013) defines an expressive score function for graph embedding to joint the SLM and LFM.", "startOffset": 14, "endOffset": 34}, {"referenceID": 10, "context": "(Guo et al. 2015) aims at leveraging the geometric structure of embedding space to make entity representations semantically smooth.", "startOffset": 0, "endOffset": 17}, {"referenceID": 20, "context": "(Wang et al. 2014) jointly embeds knowledge and texts.", "startOffset": 0, "endOffset": 18}, {"referenceID": 19, "context": "As a consequence, the proposed TransA replaces inflexible Euclidean distance with adaptive Mahalanobis distance of absolute loss, because Mahalanobis distance is more flexible and more adaptive (Wang and Sun 2014).", "startOffset": 194, "endOffset": 213}, {"referenceID": 12, "context": "A welldefined norm is necessary for most metric learning scenes (Kulis 2012), and the non-negative condition could be achieved more easily than PSD, so it generalises the common metric learning algebraic form for better rendering the knowledge topologies.", "startOffset": 64, "endOffset": 76}, {"referenceID": 12, "context": "However, for TransA, the equipotential hyper-surfaces are elliptical surfaces as the Mahalanobis distance of absolute loss states (Kulis 2012):", "startOffset": 130, "endOffset": 142}, {"referenceID": 4, "context": "We follow the same protocol as used in TransE (Bordes et al. 2013), TransH (Wang et al.", "startOffset": 46, "endOffset": 66}, {"referenceID": 20, "context": "2013), TransH (Wang et al. 2014) and TransR (Lin et al.", "startOffset": 14, "endOffset": 32}, {"referenceID": 13, "context": "2014) and TransR (Lin et al. 2015).", "startOffset": 17, "endOffset": 34}, {"referenceID": 4, "context": "As the datasets are the same, we directly copy the experimental results of several baselines from the literature, as in (Bordes et al. 2013), (Wang et al.", "startOffset": 120, "endOffset": 140}, {"referenceID": 2, "context": "SE(Bordes et al. 2011) 1,011 985 68.", "startOffset": 2, "endOffset": 22}, {"referenceID": 3, "context": "8 SME (Bordes et al. 2012) 545 533 65.", "startOffset": 6, "endOffset": 26}, {"referenceID": 11, "context": "8 LFM (Jenatton et al. 2012) 469 456 71.", "startOffset": 6, "endOffset": 28}, {"referenceID": 4, "context": "TransE (Bordes et al. 2013) 263 251 75.", "startOffset": 7, "endOffset": 27}, {"referenceID": 20, "context": "1 TransH (Wang et al. 2014) 401 388 73.", "startOffset": 9, "endOffset": 27}, {"referenceID": 13, "context": "4 TransR (Lin et al. 2015) 238 225 79.", "startOffset": 9, "endOffset": 26}, {"referenceID": 13, "context": "2014) and (Lin et al. 2015).", "startOffset": 10, "endOffset": 27}, {"referenceID": 4, "context": "Mapping properties of relations follow the same rules in (Bordes et al. 2013).", "startOffset": 57, "endOffset": 77}, {"referenceID": 2, "context": "SE(Bordes et al. 2011) 35.", "startOffset": 2, "endOffset": 22}, {"referenceID": 3, "context": "3 SME (Bordes et al. 2012) 35.", "startOffset": 6, "endOffset": 26}, {"referenceID": 4, "context": "3 TransE (Bordes et al. 2013) 43.", "startOffset": 9, "endOffset": 29}, {"referenceID": 20, "context": "0 TransH (Wang et al. 2014) 66.", "startOffset": 9, "endOffset": 27}, {"referenceID": 13, "context": "TransR (Lin et al. 2015) 78.", "startOffset": 7, "endOffset": 24}], "year": 2017, "abstractText": "Knowledge representation is a major topic in AI, and many studies attempt to represent entities and relations of knowledge base in a continuous vector space. Among these attempts, translation-based methods build entity and relation vectors by minimizing the translation loss from a head entity to a tail one. In spite of the success of these methods, translation-based methods also suffer from the oversimplified loss metric, and are not competitive enough to model various and complex entities/relations in knowledge bases. To address this issue, we propose TransA, an adaptive metric approach for embedding, utilizing the metric learning ideas to provide a more flexible embedding method. Experiments are conducted on the benchmark datasets and our proposed method makes significant and consistent improvements over the state-of-the-art baselines.", "creator": "dvips(k) 5.991 Copyright 2011 Radical Eye Software"}}}