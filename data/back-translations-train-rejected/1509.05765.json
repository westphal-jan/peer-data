{"id": "1509.05765", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "18-Sep-2015", "title": "\"Oddball SGD\": Novelty Driven Stochastic Gradient Descent for Training Deep Neural Networks", "abstract": "Stochastic Gradient Descent (SGD) is arguably the most popular of the machine learning methods applied to training deep neural networks (DNN) today. It has recently been demonstrated that SGD can be statistically biased so that certain elements of the training set are learned more rapidly than others. In this article, we place SGD into a feedback loop whereby the probability of selection is proportional to error magnitude. This provides a novelty-driven oddball SGD process that learns more rapidly than traditional SGD by prioritising those elements of the training set with the largest novelty (error). In our DNN example, oddball SGD trains some 50x faster than regular SGD.", "histories": [["v1", "Fri, 18 Sep 2015 19:58:24 GMT  (299kb)", "http://arxiv.org/abs/1509.05765v1", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["andrew j r simpson"], "accepted": false, "id": "1509.05765"}, "pdf": {"name": "1509.05765.pdf", "metadata": {"source": "CRF", "title": "\u201cOddball SGD\u201d: Novelty Driven Stochastic Gradient Descent for Training Deep Neural Networks", "authors": ["Andrew J.R. Simpson"], "emails": ["Andrew.Simpson@Surrey.ac.uk"], "sections": [{"heading": null, "text": "This year, it is closer than ever before in the history of the country."}], "references": [{"title": "Use it or Lose it: Selective Memory and Forgetting in a Perpetual Learning Machine\u201d, arxiv.org abs/1509.03185", "author": ["AJR Simpson"], "venue": null, "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2015}, {"title": "Dither is Better than Dropout for Regularising Deep Neural Networks\u201d, arxiv.org abs/1508.04826", "author": ["AJR Simpson"], "venue": null, "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2015}, {"title": "Parallel Dither and Dropout for Regularising Deep Neural Networks\u201d, arxiv.org abs/1508.07130", "author": ["AJR Simpson"], "venue": null, "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2015}, {"title": "Gradient-based learning applied to document recognition", "author": ["Y LeCun", "L Bottou", "Y Bengio", "P Haffner"], "venue": "Proc. IEEE", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 1998}, {"title": "A fast learning algorithm for deep belief nets", "author": ["GE Hinton", "S Osindero", "Y Teh"], "venue": "Neural Computation", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2006}, {"title": "Abstract Learning via Demodulation in a Deep Neural Network\u201d, arxiv.org abs/1502.04042", "author": ["AJR Simpson"], "venue": null, "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2015}, {"title": "Improving neural networks by preventing co-adaptation of feature detectors\u201d, The Computing Research Repository (CoRR), abs/1207.0580", "author": ["GE Hinton", "N Srivastava", "A Krizhevsky", "I Sutskever", "R Salakhutdinov"], "venue": null, "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2012}, {"title": "Taming the ReLU with Parallel Dither in a Deep Neural Network\u201d, arxiv.org abs/1509.05173", "author": ["AJR Simpson"], "venue": null, "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2015}, {"title": "Selective Adaptation to \u201cOddball\u201d Sounds by the Human Auditory System", "author": ["AJR Simpson", "NS Harper", "JD Reiss", "D McAlpine"], "venue": "J Neurosci 34:1963-1969", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2014}], "referenceMentions": [{"referenceID": 0, "context": "This error might be interpreted as a measure of novelty \u2013 what the network cannot predict it does not know, so unknown can be interpreted as novel in the context of prior learning (or forgetting [1]).", "startOffset": 195, "endOffset": 198}, {"referenceID": 0, "context": "It has been demonstrated [1] that SGD may be biased towards selective learning of specific elements of a training set by frequentist statistical biases representing the probability of a given training element being selected for an SGD update step.", "startOffset": 25, "endOffset": 28}, {"referenceID": 1, "context": "Using parallel dither [2,3] to enable non-batch SGD, we show that oddball SGD speeds up learning by around 50x.", "startOffset": 22, "endOffset": 27}, {"referenceID": 2, "context": "Using parallel dither [2,3] to enable non-batch SGD, we show that oddball SGD speeds up learning by around 50x.", "startOffset": 22, "endOffset": 27}, {"referenceID": 3, "context": "As example case, we use the well-known computer vision problem of hand-written digit classification using the MNIST dataset [4].", "startOffset": 124, "endOffset": 127}, {"referenceID": 4, "context": "Replicating Hinton\u2019s [5] architecture, but using the biasedsigmoid [6] activation function (which is optimised for demodulation), we built a fully connected network of size 784x100x10 units, where the 10-unit softmax output layer corresponds to the 10-way digit classification problem.", "startOffset": 21, "endOffset": 24}, {"referenceID": 5, "context": "Replicating Hinton\u2019s [5] architecture, but using the biasedsigmoid [6] activation function (which is optimised for demodulation), we built a fully connected network of size 784x100x10 units, where the 10-unit softmax output layer corresponds to the 10-way digit classification problem.", "startOffset": 67, "endOffset": 70}, {"referenceID": 2, "context": "Operating within the so-called \u2018small-data regime\u2019 (as in [3]), we used only the first 256 training examples of the MNIST dataset and tested on the full 10,000 test examples.", "startOffset": 58, "endOffset": 61}, {"referenceID": 1, "context": "The third was the baseline model regularised with dither [2].", "startOffset": 57, "endOffset": 60}, {"referenceID": 2, "context": "The fourth was the baseline model regularised with 100x parallel dither [3].", "startOffset": 72, "endOffset": 75}, {"referenceID": 2, "context": "The fifth was the baseline model regularised using 100x parallel dither w/ dropout [3].", "startOffset": 83, "endOffset": 86}, {"referenceID": 2, "context": "The final model was trained using oddball SGD and regularised with 100x parallel dither w/ dropout [3].", "startOffset": 99, "endOffset": 102}, {"referenceID": 6, "context": "For parallel dither w/ dropout [7], both dither and dropout were applied at the same time (i.", "startOffset": 31, "endOffset": 34}, {"referenceID": 7, "context": "We also note that (data not shown) the same oddball SGD similarly improved the equivalent ReLU model (as in [8]) but the performance with ReLU was not as good as with the optimally-biased sigmoid [6] function of the present experiments [see 8 for discussion of why].", "startOffset": 108, "endOffset": 111}, {"referenceID": 5, "context": "We also note that (data not shown) the same oddball SGD similarly improved the equivalent ReLU model (as in [8]) but the performance with ReLU was not as good as with the optimally-biased sigmoid [6] function of the present experiments [see 8 for discussion of why].", "startOffset": 196, "endOffset": 199}, {"referenceID": 8, "context": "Given that similar novelty-driven adaptation has been observed in human perception [9], it may be that a similar learning strategy is used in the brain.", "startOffset": 83, "endOffset": 86}], "year": 2015, "abstractText": "Stochastic Gradient Descent (SGD) is arguably the most popular of the machine learning methods applied to training deep neural networks (DNN) today. It has recently been demonstrated that SGD can be statistically biased so that certain elements of the training set are learned more rapidly than others. In this article, we place SGD into a feedback loop whereby the probability of selection is proportional to error magnitude. This provides a novelty-driven oddball SGD process that learns more rapidly than traditional SGD by prioritising those elements of the training set with the largest novelty (error). In our DNN example, oddball SGD trains some 50x faster than regular SGD.", "creator": "PDFCreator Version 1.7.1"}}}