{"id": "1509.06053", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "20-Sep-2015", "title": "Early text classification: a Naive solution", "abstract": "Text classification is a widely studied problem, and it can be considered solved for some domains and under certain circumstances. There are scenarios, however, that have received little or no attention at all, despite its relevance and applicability. One of such scenarios is early text classification, where one needs to know the category of a document by using partial information only. A document is processed as a sequence of terms, and the goal is to devise a method that can make predictions as fast as possible. The importance of this variant of the text classification problem is evident in domains like sexual predator detection, where one wants to identify an offender as early as possible. This paper analyzes the suitability of the standard naive Bayes classifier for approaching this problem. Specifically, we assess its performance when classifying documents after seeing an increasingly number of terms. A simple modification to the standard naive Bayes implementation allows us to make predictions with partial information. To the best of our knowledge naive Bayes has not been used for this purpose before. Throughout an extensive experimental evaluation we show the effectiveness of the classifier for early text classification. What is more, we show that this simple solution is very competitive when compared with state of the art methodologies that are more elaborated. We foresee our work will pave the way for the development of more effective early text classification techniques based in the naive Bayes formulation.", "histories": [["v1", "Sun, 20 Sep 2015 21:01:51 GMT  (75kb)", "http://arxiv.org/abs/1509.06053v1", "8 pages, preprint submitted to SDM'16"]], "COMMENTS": "8 pages, preprint submitted to SDM'16", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["hugo jair escalante", "manuel montes-y-g\\'omez", "luis villase\\~nor-pineda", "marcelo luis errecalde"], "accepted": false, "id": "1509.06053"}, "pdf": {"name": "1509.06053.pdf", "metadata": {"source": "CRF", "title": null, "authors": ["Hugo Jair Escalante", "Manuel Montes", "Luis Villase\u00f1or", "Marcelo L. Errecalde"], "emails": [], "sections": [{"heading": null, "text": "ar Xiv: 150 9.06 053v 1 [cs.C L] September 20, 2015The classification of texts is a widely studied problem and can be considered solved in some areas and under certain circumstances. However, there are scenarios that, despite their relevance and applicability, have received little or no attention. One of these scenarios is early text classification, where one only needs to know the category of a document on the basis of partial information. A document is treated as a sequence of terms, and the aim is to develop a method that can predict as quickly as possible. The importance of this variant of the text classification problem manifests itself in areas such as detection of sexual predators, where one wants to identify an offender as early as possible. This paper analyses the suitability of the standard text for approaching this problem. Specifically, we judge its performance in classifying documents after seeing an increasing number of terms."}, {"heading": "1 Introduction", "text": "However, this is one of the most studied topics within natural language processing (manuscript submitted to the SIAM International Conference on Data Mining 2016). This work has been supported by CONACyT grants whenever it comes to how far people are able to identify themselves. (Clasificacio \"n\" n \"n\" n \"n\" n \"n\" n \"n\" n \"n\" n \"n\" n \"n\" n \"n\" n \"n\" n \"n\" n \"n\" n \"n\" n \"n\" n \"n\" n \"n\" n \"n\" n \"n\" n \"n\" n \"n\" n \"n\" n \"n\" n \"n\" n \"n\" n \"n\" n \"n\" n \"n\" n \"n\" n \"n\" n \"n\" n \"n\" n \"n\" n \"n\" n \"n\" n \"n\" n \"n\" n \"n\" n \"n\" n \"n\" n \"n\" n \"n\" n \"n\" n \"n\" n \"n\" n \"n\" n \"n\" n \"n\" n \"n\" n \"n\" n \"n\" n \"n\" n \"n\" n \"n\" n \"n\" n \"n\" n \"n\" n \"n\" n \"n\" n \"n\" n \"n\" n \"n\" n \"n\" n \"n\" n \"n\" n \"n\" n \"n n n\" n \"n\" n n n \"n n n n n n\" n n n n \"n n\" n n n n \"n\" n \"n n n\" n \"n n n\" n n \"n n n n n\" n n n \"n n\" n n n \"n n\" n n n \"n n n n n n\" n n n n n \"n n n n n n n n n n\" n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n"}, {"heading": "2 Related work", "text": "In this context, it should be noted that the case concerns a case in which a person has been injured."}, {"heading": "3 Early text classification with Na\u0308\u0131ve Bayes", "text": "This section describes the way in which we assess the classification in the early text. (D = (xi, yi) We assume that the classification in the placement (xi, yi) in the placement (xi, yi) in the placement (xi) in the placement (xi) in the placement (xi) in the placement (xi) in the placement (xi) in the placement (xi) in the placement (xi) in the placement (xi) in the placement (xi) in the placement (xi) in the placement (xi) in the placement (xi) in the placement (xi) in the placement (xi) in the placement (xi) in the placement in the placement (xi) in the placement (xi) in the placement (xi) in the placement in the placement (xi) in the placement (xi) in the placement in the placement (xi) in the placement (xi) in the placement in the placement (xi) in the placement (xi) in the placement (xi) in the placement in the placement (xi) in the placement (xi) in the placement in the placement (xi) in the placement (xi) in the placement in the placement (xi) in the placement (xi) in the placement (xi) in the placement in the placement (xi) in the placement (xi) in the placement (xi) in the placement in the placement (xi) in the placement (xi) in the placement in the placement (xi) in the placement in the placement (xi) in the placement (xi) in the placement (xi) in the placement in the placement (xi) in the placement (xi) in the placement in the placement in the placement (xi) in the placement (xi) in the placement (xi) in the placement in the placement (xi) in the placement in the placement (xi) in the placement (xi) in the placement in the placement (xi) in the placement in the placement in the placement (xi) in the placement (xi) in the placement (xi) in the placement (xi) in the placement"}, {"heading": "4 Experiments and results", "text": "This year, it is closer than ever before in the history of the country."}, {"heading": "5 Conclusions", "text": "We have demonstrated the effectiveness of this simple approach in three types of problems and compared its performance with the only existing state-of-the-art method. Our method compares favourably in terms of effectiveness and seriousness performance with the reference method, a much more complex model. Also, our method consistently exceeds an SVM baseline. Furthermore, we are the first to approach the early classification of chat conversations for the detection of sexual predators. Although the results are encouraging, there is too much work to do. We see our work for the development of more sophisticated techniques based on more elaborated bayes for early classification. The following conclusions can be drawn from our work: \u2022 Na \u0131ve Bayes proved to be very effective for early text classification, comparable results to the state of the art."}], "references": [{"title": "Incremental learning of tree augmented naive bayes classifiers", "author": ["J.R. Alcob\u00e9"], "venue": "In IBERAMIA\u201902,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2002}, {"title": "Distributional term representations for short-text categorization", "author": ["J.M. Cabrera", "H.J. Escalante", "M. Montes y G\u00f3mez"], "venue": "In Proc. of CICLING,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2013}, {"title": "Hmm\u2013 based passage models for document classification and ranking", "author": ["L. Denoyer", "H. Zaragoza", "P. Gallinari"], "venue": "In Proc. of 23rd European Colloquium on Information Retrieval Research (ECIR\u201901),", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2001}, {"title": "Text classification: A sequential reading approach", "author": ["G. Dulac-Arnold", "L. Denoyer", "P. Gallinari"], "venue": "In Advances in Information Retrieval, Proc. of 33rd European Conference on IR Research, (ECIR\u201911),", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2011}, {"title": "Sequential approaches for learning datum-wise sparse representations", "author": ["G. Dulac-Arnold", "L. Denoyer", "P. Preux", "P. Gallinari"], "venue": "Machine Learning,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2012}, {"title": "Bayesian network classifiers", "author": ["N. Friedman", "D. Geiger", "M. Goldszmidt"], "venue": "Machine Learning,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 1997}, {"title": "Improving naive bayes text classifier using smoothing methods", "author": ["F. He", "X. Ding"], "venue": "In Proc. ECIR\u201907 Proceedings of the 29th European conference on IR research,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2007}, {"title": "Anytime classification for a pool of instances", "author": ["B. Hui", "Y. Yang", "G.I. Webb"], "venue": "Machine Learning,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2009}, {"title": "Overview of the international sexual predator identification competition at pan-2012", "author": ["G. Inches", "F. Crestani"], "venue": "In CEUR Workshop Proceedings, Working Notes for CLEF 2012 Conference,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2012}, {"title": "Text categorization with support vector machines: Learning with many relevant features", "author": ["T. Joachims"], "venue": "In Proceedings of ECML-98,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2008}, {"title": "Multinomial naive bayes for text categorization revisited", "author": ["A.M. Kibriya", "E. Frank", "B. Pfahringer", "G. Holmes"], "venue": "In AI 2004: Adv. Artificial Intelligence,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2005}, {"title": "Evolving extended n\u00e4\u0131ve bayes classifiers", "author": ["F. Klawonn", "P. Angelov"], "venue": "In Proc. of Sixth IEEE International Conference on Data Mining Workshops, ICDM Workshops,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2006}, {"title": "A comparison of event models for naive bayes text classification", "author": ["A. McCallum", "K. Nigam"], "venue": "In Proc. of AAAI/ICML-98 Workshop on Learning for Text Categorization,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 1998}, {"title": "Machine learning in automated text categorization", "author": ["F. Sebastiani"], "venue": "ACM Computer Surveys,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2008}, {"title": "Exploiting term relationship to boost text classification", "author": ["D. Shen", "J. Wu", "B. Cao", "J.T. Sun", "Q. Yang", "Z. Chen", "Y. Li"], "venue": "In Proc. of the 18th ACM Conference on  Information and Knowledge Management,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2009}, {"title": "A twostep approach for effective detection of misbehaving users in chats", "author": ["E. Villatoro-Tello", "A. Juarez-Gonzalez", "H.J. Escalante", "M. Montes y Gomez", "L. Villase nor Pineda"], "venue": "In CLEF 2012 Evaluation Labs and Workshop - Working Notes Papers,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2012}, {"title": "Not so naive bayes: Aggregating one-dependence estimators", "author": ["G. Webb", "J.R. Boughton", "Z. Wang"], "venue": "Machine Learning,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2005}, {"title": "Classifying under computational resource constraints: anytime classification using probabilistic estimators", "author": ["Y. Yang", "G.I. Webb", "K. Korb", "K.M. Ting"], "venue": "Machine Learning,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2007}, {"title": "Enhancing naive bayes with various smoothing methods for short text classification", "author": ["Q. Yuan", "G. Cong", "N.M. Thalmann"], "venue": "In Proc. of WWW Companion,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2012}, {"title": "Alleviating naive bayes attribute independence assumption by attribute weighting", "author": ["N.A. Zaidi", "J. Cerquides", "M.J. Carman", "G.I. Webb"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2013}, {"title": "Grouping Multidimensional Data: Recent Advances in Clustering, chapter TMG: A MATLAB toolbox for generating termdocument matrices from text collections", "author": ["D. Zeimpekis", "E. Gallopoulos"], "venue": null, "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2006}], "referenceMentions": [{"referenceID": 13, "context": "Text classification is the task of assigning documents to its correct categories [14].", "startOffset": 81, "endOffset": 85}, {"referenceID": 3, "context": "The early text classification topic has received little attention in the community, and there exist only a few works that have approached similar scenarios [4] (please note that in this work the problem is not stated as one of early recognition).", "startOffset": 156, "endOffset": 159}, {"referenceID": 12, "context": ", n\u00e4\u0131ve Bayes [13, 14], to approach the early-classification setting: early n\u00e4\u0131ve Bayes.", "startOffset": 14, "endOffset": 22}, {"referenceID": 13, "context": ", n\u00e4\u0131ve Bayes [13, 14], to approach the early-classification setting: early n\u00e4\u0131ve Bayes.", "startOffset": 14, "endOffset": 22}, {"referenceID": 3, "context": "1 Early text classification To the best of our knowledge, the early text categorization problem has been approached only in [4]; although the authors\u2019 main focus was not on making predictions earlier but on improving the classification performance with a sequential reading approach.", "startOffset": 124, "endOffset": 127}, {"referenceID": 3, "context": "Although the performance of such method is competitive (it was compared to a SVM classifier), it remains unknown whether a much more simpler approach would be as effective as the complex procedure in [4].", "startOffset": 200, "endOffset": 203}, {"referenceID": 2, "context": "In [3], the authors propose a hidden Markov model (HMM) to classify passages within documents.", "startOffset": 3, "endOffset": 6}, {"referenceID": 4, "context": "In [5] the authors extend the MDP proposed for sequential text classification to deal with any other type of data.", "startOffset": 3, "endOffset": 6}, {"referenceID": 3, "context": "The formulation is almost the same as in [4], although this time the MDP can decide what feature to sample from the instance under analysis (i.", "startOffset": 41, "endOffset": 44}, {"referenceID": 5, "context": "Most notably TAN [6], AODE [17], and WANBIA [20] extensions have reported outstanding results.", "startOffset": 17, "endOffset": 20}, {"referenceID": 16, "context": "Most notably TAN [6], AODE [17], and WANBIA [20] extensions have reported outstanding results.", "startOffset": 27, "endOffset": 31}, {"referenceID": 19, "context": "Most notably TAN [6], AODE [17], and WANBIA [20] extensions have reported outstanding results.", "startOffset": 44, "endOffset": 48}, {"referenceID": 17, "context": "The goal of this type of extensions is to provide n\u00e4\u0131ve Bayes with mechanisms that allow it to make predictions at anytime [18, 8].", "startOffset": 123, "endOffset": 130}, {"referenceID": 7, "context": "The goal of this type of extensions is to provide n\u00e4\u0131ve Bayes with mechanisms that allow it to make predictions at anytime [18, 8].", "startOffset": 123, "endOffset": 130}, {"referenceID": 0, "context": "Refers to developing learning and inference mechanisms to allow the classifier be trained in an online learning setting [1, 12].", "startOffset": 120, "endOffset": 127}, {"referenceID": 11, "context": "Refers to developing learning and inference mechanisms to allow the classifier be trained in an online learning setting [1, 12].", "startOffset": 120, "endOffset": 127}, {"referenceID": 14, "context": ", in short text categorization) [15, 2, 7, 19].", "startOffset": 32, "endOffset": 46}, {"referenceID": 1, "context": ", in short text categorization) [15, 2, 7, 19].", "startOffset": 32, "endOffset": 46}, {"referenceID": 6, "context": ", in short text categorization) [15, 2, 7, 19].", "startOffset": 32, "endOffset": 46}, {"referenceID": 18, "context": ", in short text categorization) [15, 2, 7, 19].", "startOffset": 32, "endOffset": 46}, {"referenceID": 12, "context": ", we know for each document, the number of times each term from the vocabulary occurs) [13, 11].", "startOffset": 87, "endOffset": 95}, {"referenceID": 10, "context": ", we know for each document, the number of times each term from the vocabulary occurs) [13, 11].", "startOffset": 87, "endOffset": 95}, {"referenceID": 12, "context": "For more details we refer the reader to [13, 11].", "startOffset": 40, "endOffset": 48}, {"referenceID": 10, "context": "For more details we refer the reader to [13, 11].", "startOffset": 40, "endOffset": 48}, {"referenceID": 5, "context": ", [6, 17, 20]) in order to increase the predictive power of the classifier; also one can adopt advanced/alternative smoothing techniques to account for partial and missing information properly [15, 2, 7]; as well as many other possibilities.", "startOffset": 2, "endOffset": 13}, {"referenceID": 16, "context": ", [6, 17, 20]) in order to increase the predictive power of the classifier; also one can adopt advanced/alternative smoothing techniques to account for partial and missing information properly [15, 2, 7]; as well as many other possibilities.", "startOffset": 2, "endOffset": 13}, {"referenceID": 19, "context": ", [6, 17, 20]) in order to increase the predictive power of the classifier; also one can adopt advanced/alternative smoothing techniques to account for partial and missing information properly [15, 2, 7]; as well as many other possibilities.", "startOffset": 2, "endOffset": 13}, {"referenceID": 14, "context": ", [6, 17, 20]) in order to increase the predictive power of the classifier; also one can adopt advanced/alternative smoothing techniques to account for partial and missing information properly [15, 2, 7]; as well as many other possibilities.", "startOffset": 193, "endOffset": 203}, {"referenceID": 1, "context": ", [6, 17, 20]) in order to increase the predictive power of the classifier; also one can adopt advanced/alternative smoothing techniques to account for partial and missing information properly [15, 2, 7]; as well as many other possibilities.", "startOffset": 193, "endOffset": 203}, {"referenceID": 6, "context": ", [6, 17, 20]) in order to increase the predictive power of the classifier; also one can adopt advanced/alternative smoothing techniques to account for partial and missing information properly [15, 2, 7]; as well as many other possibilities.", "startOffset": 193, "endOffset": 203}, {"referenceID": 3, "context": "We considered three standard thematic text categorization tasks (also used in [4]) and a data set for sexual predator detection [9].", "startOffset": 78, "endOffset": 81}, {"referenceID": 8, "context": "We considered three standard thematic text categorization tasks (also used in [4]) and a data set for sexual predator detection [9].", "startOffset": 128, "endOffset": 131}, {"referenceID": 20, "context": "Text data sets were processed as follows: stop words were removed, then stemming was applied, next the bag-of-words representation was obtained using the TMG toolbox, a term-frequency (tf ) weighting scheme was used [21].", "startOffset": 216, "endOffset": 220}, {"referenceID": 9, "context": "In addition to the comparison to the state of the art, we considered a linear SVM classifier as baseline, since this is a mandatory baseline in text classification [10, 14].", "startOffset": 164, "endOffset": 172}, {"referenceID": 13, "context": "In addition to the comparison to the state of the art, we considered a linear SVM classifier as baseline, since this is a mandatory baseline in text classification [10, 14].", "startOffset": 164, "endOffset": 172}, {"referenceID": 3, "context": "In all of our experiments we report the performance of the early text classifiers when varying the percentage of the words in test documents (same procedure as in [4]).", "startOffset": 163, "endOffset": 166}, {"referenceID": 3, "context": "2 Comparison with related work In this section we compare the performance of n\u00e4\u0131ve Bayes with the MDP introduced in [4] using the same data sets from the previous section.", "startOffset": 116, "endOffset": 119}, {"referenceID": 3, "context": "For this comparison we replicated the experiment reported by the authors of [4].", "startOffset": 76, "endOffset": 79}, {"referenceID": 3, "context": "Please note that in [4] the authors optimized the parameters of their method, called STC, whereas we have used default implementation/parameters for ENB.", "startOffset": 20, "endOffset": 23}, {"referenceID": 3, "context": "When comparing the ENB approach with the sequential text classification technique (STC) from [4], it can be seen that the MDP from the reference work and our ENB perform very similar (even when we only show best/optimized results for STC).", "startOffset": 93, "endOffset": 96}, {"referenceID": 8, "context": "We used the development / test partitions of the data set used in the sexual predator competition from PAN\u201912 [9], see Table 1.", "startOffset": 110, "endOffset": 113}, {"referenceID": 15, "context": "We proceeded in this way because the original task was one of forensic analysis: detect predators offline using all of the conversations in which they were involved (see [16] for our solution that obtained the best result in that challenge).", "startOffset": 170, "endOffset": 174}, {"referenceID": 15, "context": "Based on our previous results from [16], and on the literature on non-thematic text classification we decided to represent chat conversations with 3-grams of characters (i.", "startOffset": 35, "endOffset": 39}, {"referenceID": 15, "context": ", terms in this data set are sequences of 3-letters extracted from the training corpus); with this data set we used a reduced vocabulary and preprocessing processes described in [16].", "startOffset": 178, "endOffset": 182}, {"referenceID": 8, "context": "As suggested in [9], for this experiment we report f1 measure on the minority class (i.", "startOffset": 16, "endOffset": 19}, {"referenceID": 3, "context": "The inference complexity of n\u00e4\u0131ve Bayes is negligible (adding the value of q\u2212terms, for K\u2212times), thus makes this method preferable over the MDP introduced in [4].", "startOffset": 159, "endOffset": 162}], "year": 2015, "abstractText": "Text classification is a widely studied problem, and it can be considered solved for some domains and under certain circumstances. There are scenarios, however, that have received little or no attention at all, despite its relevance and applicability. One of such scenarios is early text classification, where one needs to know the category of a document by using partial information only. A document is processed as a sequence of terms, and the goal is to devise a method that can make predictions as fast as possible. The importance of this variant of the text classification problem is evident in domains like sexual predator detection, where one wants to identify an offender as early as possible. This paper analyzes the suitability of the standard n\u00e4\u0131ve Bayes classifier for approaching this problem. Specifically, we assess its performance when classifying documents after seeing an increasingly number of terms. A simple modification to the standard n\u00e4\u0131ve Bayes implementation allows us to make predictions with partial information. To the best of our knowledge N\u00e4\u0131ve Bayes has not been used for this purpose before. Throughout an extensive experimental evaluation we show the effectiveness of the classifier for early text classification. What is more, we show that this simple solution is very competitive when compared with state of the art methodologies that are more elaborated. We foresee our work will pave the way for the development of more effective early text classification techniques based in the n\u00e4\u0131ve Bayes formulation.", "creator": "LaTeX with hyperref package"}}}