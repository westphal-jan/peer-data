{"id": "1509.06163", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "21-Sep-2015", "title": "The Utility of Clustering in Prediction Tasks", "abstract": "We explore the utility of clustering in reducing error in various prediction tasks. Previous work has hinted at the improvement in prediction accuracy attributed to clustering algorithms if used to pre-process the data. In this work we more deeply investigate the direct utility of using clustering to improve prediction accuracy and provide explanations for why this may be so. We look at a number of datasets, run k-means at different scales and for each scale we train predictors. This produces k sets of predictions. These predictions are then combined by a na\\\"ive ensemble. We observed that this use of a predictor in conjunction with clustering improved the prediction accuracy in most datasets. We believe this indicates the predictive utility of exploiting structure in the data and the data compression handed over by clustering. We also found that using this method improves upon the prediction of even a Random Forests predictor which suggests this method is providing a novel, and useful source of variance in the prediction process.", "histories": [["v1", "Mon, 21 Sep 2015 09:42:50 GMT  (687kb)", "http://arxiv.org/abs/1509.06163v1", "An experimental research report, dated 11 September 2011"]], "COMMENTS": "An experimental research report, dated 11 September 2011", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["shubhendu trivedi", "zachary a pardos", "neil t heffernan"], "accepted": false, "id": "1509.06163"}, "pdf": {"name": "1509.06163.pdf", "metadata": {"source": "CRF", "title": null, "authors": [], "emails": [], "sections": [{"heading": null, "text": "In this paper, we examine more deeply the direct benefit of using clusters to improve predictive accuracy and provide explanations for why this can be so. We look at a number of datasets executed at different scales and for each scale we assign to predictors, which produces k sets of predictions, which are then combined by a na\u00efve ensemble. We observe that this use of a predictor in conjunction with clustering improves prediction accuracy in most datasets. We believe that this improves the likely usability of the structure in the data and the data compression obtained by this method."}, {"heading": "II. CLUSTERING", "text": "It is reasonable to say that at least part of our understanding of the world is due to a semi-supervised process that involves a kind of clustering on a large scale. An example would be our ability to say, given a mix of objects that are similar and belong to the same category. It has been suggested that a mathematically precise notion of clustering is important in the sense that it can help us solve problems at least roughly as they are solved by the brain. Clustering is probably the most commonly used method of exploratory data analysis and is often used to get an intuition about the structure of the data."}, {"heading": "A. Related Work", "text": "In fact, the fact is that most of them are able to move to another world, in which they are able, in which they are able, in which they are able to integrate, and in which they are able, in which they are able to integrate."}, {"heading": "III. USING CLUSTERING FOR BOOTSTRAPPING", "text": "In this context, it should be noted that it is a model that is able to determine itself. (...) In this context, it must be stated that it is a model that is able to determine itself. (...) In this case, it is a model that is able to determine itself. (...) In this case, it is as if it is a model. (...) In this case, it is as if it is a model. (...) In this case, it is a model that is able to determine itself. (...) In this case, it is as if it is a model. (...) In this case, it is as if it is a model. (...) In this case, it is a model that is such. (...) In this case, it is a model that is a model. (...) It is a model that is a model. (...) In this case, it will be a model. (...)"}, {"heading": "A. k as a tunable parameter", "text": "The previous section describes a way in which clustering could be used to construct what we call a \"prediction model.\" Building on the generic method, using the number of clusters \"k\" in k averages (or other clustering where the number of clusters must be entered), several prediction models can be obtained as free parameters (fig. 3), i.e. k can be varied from 1 to a value K and a prediction model for each instance. For example, if K = 3, there would be three prediction models: PM1 (the predictor trains on the entire data set), PM2 (the predictor trains on two clusters) and PM3 (the predictor trains on three clusters). These K prediction models are then applied to make a number of different predictions on the test theorem, with the two-step procedure for mapping and predicting test points outlined before we can look at them as K-must be indicated."}, {"heading": "B. Combining Predictions", "text": "In fact, it is such that most of us will be able to move into a different world, in which they are able to move, in which they are able to move, in which they move, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live."}, {"heading": "C. Similarity with Other Existing Methods", "text": "Before looking at the empirical evaluation of the method discussed, we compare this method with some papers that at least talked about using clustering to predict. We introduced a simple but effective bootstrap-aggregating meta-algorithm that uses clustering as a means of bootstrap. This method can be discussed as a mixture of local experts similar to that of Jacobs, Hinton et al. [13] However, it is noteworthy that unlike other bagging methods that arandom select a subset of data on the bootstrap, this method has a specific expert for each \"locality,\" i.e. clusters, which can potentially lead to more interpretability. By varying the granularity of clustering, we are able to train a number of experts on different scales, leading to a number of different predictions that make it possible to merge."}, {"heading": "IV. EMPIRICAL VALIDATION", "text": "In this section, we report on the mechanics of an empirical study that was conducted using a set of benchmark data sets to task regression for three different predictors."}, {"heading": "A. Algorithms", "text": "The algorithm used for clustering the various datasets was the k-mean algorithm. K-mean finds a partition by optimizing a distortion function, and while it can be assumed that it converges in a certain sense (it can be said that it is Lyapunov-stable [16], whereby the objective function decreases monotonously), the distortion function for k-mean is not convex. It is therefore sensitive to the choice of initial cluster centrioids and often provides suboptimal solutions. We randomly initialized Kmeans 200 times during each run and selected the best solution. Linear regression, Step-Wise Linear Regression, and Random Forests (for regression) were used for the prediction task (i.e. for cluster model formation). While this work can also be extended to classification tasks, we do not discuss it in this paper."}, {"heading": "B. Datasets", "text": "The datasets used for empirical validation of this technique are from the University of California, Irvine Machine Learning repository [17]. Of the 17 available regression datasets, these datasets were considered as having no large number of missing values or nominal attributes, so we limited ourselves to datasets with purely numerical attributes. Some of these datasets had more than one target variable. Such cases are considered as separate datasets. The following datasets were considered as (a) Breast Cancer Wisconsin dataset (BREAST CANCER), which has 569 data instances, each with 32 attributes. The prediction is for diagnosis (benign or malignant); (b) Cement Compressive Strength Dataset (COMPRESSIVE) has 1030 data points in total. Each data instance is described by 10 characteristics."}, {"heading": "C. Methodology", "text": "This year, the number of job-related redundancies has fallen by more than a third."}, {"heading": "V. RESULTS", "text": "This year, there will be no significant change in the first half of the year."}, {"heading": "VI. DISCUSSION AND FUTURE WORK", "text": "The results obtained when using clusters associated with linear regression are not very surprising. Linear regression is a model with a high bias and is therefore not expected to be too good for most real-world datasets. Improving predictive power is very important because it provides access to more variance in the data, thereby improving the predictive power of the entire system. Improving predictive power is very significant when combined with gradual selection. In some cases where clustering occurs, the results are comparable to those of random forests without clustering."}, {"heading": "ACKNOWLEDGMENT", "text": "We thank Dr. Carolina Ruiz, Dr. Sergio Alvarez and Dr. Alexandru Niculescu-Mizil for helpful suggestions and discussions about the work."}], "references": [{"title": "Clustering Students to Generate an Ensemble to improve Standard Test Score Predictions", "author": ["S. Trivedi", "Z.A. Pardos", "N.T. Heffernan"], "venue": "Proceedings of the International Conference on Artificial Intelligence in Education, 2011, pp. 377-384", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2011}, {"title": "Spectral Clustering in Educational Data Mining", "author": ["S. Trivedi", "Z.A. Pardos", "G.N. S\u00e1rk\u00f6zy", "N.T. Heffernan"], "venue": "Proceedings of the 4 International Conference on Educational Data Mining", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2011}, {"title": "How many Clusters? An Information Theoretic Perspective", "author": ["S. Still", "W. Bialek"], "venue": "Neural Computation", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2004}, {"title": "Occam\u2019s Razor", "author": ["A. Blumer", "A. Ehrenfeucth", "D. Haussler", "M.K. Warmuth"], "venue": "Information Processing Letters", "citeRegEx": "4", "shortCiteRegEx": null, "year": 1987}, {"title": "Sample Compression", "author": ["S. Floyd", "M.K. Warmuth"], "venue": "Learnability and the Vapnik-Chervonenkis Dimension\u201d, Machine Learning", "citeRegEx": "5", "shortCiteRegEx": null, "year": 1995}, {"title": "and J", "author": ["A. Blum"], "venue": "Langord, \u201cPAC-MDL bounds\u201d", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2003}, {"title": "and J", "author": ["A. Banerjee"], "venue": "Langford, \u201cAn Objective Evaluation Criterion for Clustering\u201d", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2004}, {"title": "Learning Mixtures of Gaussians", "author": ["S. Dasgupta"], "venue": "Annual IEEE Symposium on Foundations of Computer", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 1999}, {"title": "Ensemble Methods in Machine Learning", "author": ["T.G. Dietterich"], "venue": "First International workshop on Multiple Classifier Systems. Kittler J., and Roli., F. (Eds.), Lecture Notes in Computer Science, New York, Springer Varlag", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2000}, {"title": "An Experimental Comparison of three Methods for Constructing Ensembles of Decision Trees: Bagging", "author": ["T.G. Dietterirch"], "venue": "Boosting, and Randomization\u201d, Machine Learning, Kluwer Academic Publishers", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2000}, {"title": "Managing Diversity in Regression Ensembles", "author": ["G. Brown", "J.L. Wyatt", "P. Tino"], "venue": "Journal of Machine Learning Research", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2005}, {"title": "Adaptive Mixture of Experts,", "author": ["R.A. Jacbos", "M.I. Jordan", "S.J. Nowlan", "G.E. Hinton"], "venue": "Neural Computation,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 1991}, {"title": "Statistical Predicate Invention", "author": ["S. Kok", "P. Domingos"], "venue": "Proceedings of the Twenty-Fourth International Conference on Machine Learning, pp. 433-440, ACM Press", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2007}, {"title": "A Framework for Simultaneous Coclustering and Learning from Complex Data", "author": ["M. Deodhar", "J. Ghosh"], "venue": "KDD 2007, pp. 250-259", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2007}, {"title": "Information Theory", "author": ["D.J.C. MacKay"], "venue": "Inference and Learning Algorithms, Cambridge University Press", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2003}, {"title": "and A", "author": ["A. Frank"], "venue": "Asuncion, UCI Machine Learning Repository [http://archive.ics.uci.edu/ml]. Irvine, CA: University of California, School of Information and Computer Science", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2010}, {"title": "Modeling of strength of high performance concrete using artificial neural networks,", "author": ["I-Cheng Yeh"], "venue": "Cement and Concrete Research,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 1998}, {"title": "A Data Mining Approach to Predict Forest Fires using Meteorological Data", "author": ["P. Cortez", "A. Morais"], "venue": "J. Neves, M. F. Santos and J. Machado Eds., New Trends in Artificial Intelligence, Proceedings of the 13th EPIA 2007 - Portuguese Conference on Artificial Intelligence, December, Guimares, Portugal, pp. 512-523, 2007. APPIA, ISBN-13 978-989-95618-0-9", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2007}, {"title": "Accurate telemonitoring of Parkinson\u2019s disease progression by noninvasive speech tests", "author": ["A. Tsanas", "M.A. Little", "P.E. McSharry", "L.O. Ramig"], "venue": "IEEE Transactions on Biomedical Engineering (to appear)", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2009}, {"title": "Using HMMs and bagged decision trees to leverage rich features of user and skill from an intelligent tutoring system dataset", "author": ["Z.A. Pardos", "N.T. Heffernan"], "venue": "the Journal of Machine Learning Research, 2010 KDD Cup Special Issu, Accepted ", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2011}, {"title": "Ensemble selection from libraries of models", "author": ["R. Caruana", "A. Niculescu-Mizil"], "venue": "Proceedings of the 21st International Conference on Machine Learning (ICML\u201904)", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2004}], "referenceMentions": [{"referenceID": 0, "context": "This also led to papers [1] [2] that explored this idea in an educational dataset.", "startOffset": 24, "endOffset": 27}, {"referenceID": 1, "context": "This also led to papers [1] [2] that explored this idea in an educational dataset.", "startOffset": 28, "endOffset": 31}, {"referenceID": 2, "context": "approximately as solved by the brain [3].", "startOffset": 37, "endOffset": 40}, {"referenceID": 0, "context": "there is some evidence [1][2].", "startOffset": 23, "endOffset": 26}, {"referenceID": 1, "context": "there is some evidence [1][2].", "startOffset": 26, "endOffset": 29}, {"referenceID": 3, "context": "One of the most basic results in Learning Theory is the Occam\u2019s Razor [4] i.", "startOffset": 70, "endOffset": 73}, {"referenceID": 4, "context": "The notion of compression implies learning for different description languages has lead to a number of important sample complexity bounds [5][6] and has been generalized to any description language by Blum & Langford [7].", "startOffset": 138, "endOffset": 141}, {"referenceID": 5, "context": "The notion of compression implies learning for different description languages has lead to a number of important sample complexity bounds [5][6] and has been generalized to any description language by Blum & Langford [7].", "startOffset": 217, "endOffset": 220}, {"referenceID": 5, "context": "For a description language the bound on the test error (\ud835\u0302\udf0etest) as a function of the error on the training set is given by the PAC-MDL bound [7] [8]: For any given distribution D and for the set of all description languages L = {\u03c3} with probability 1 \u2212 \u03b4 over the train and test sets: Strain , Stest~ D : \u2200\u03c3 \ud835\u0302\udf0etest \u2264 bmax(m, n, \ud835\u0302\udf0etrain , 2 \u03b4) While the PAC-MDL bound is used for a transductive setting Banerjee & Langford [7] show that clustering can be converted to a transductive classification problem.", "startOffset": 142, "endOffset": 145}, {"referenceID": 6, "context": "For a description language the bound on the test error (\ud835\u0302\udf0etest) as a function of the error on the training set is given by the PAC-MDL bound [7] [8]: For any given distribution D and for the set of all description languages L = {\u03c3} with probability 1 \u2212 \u03b4 over the train and test sets: Strain , Stest~ D : \u2200\u03c3 \ud835\u0302\udf0etest \u2264 bmax(m, n, \ud835\u0302\udf0etrain , 2 \u03b4) While the PAC-MDL bound is used for a transductive setting Banerjee & Langford [7] show that clustering can be converted to a transductive classification problem.", "startOffset": 146, "endOffset": 149}, {"referenceID": 5, "context": "For a description language the bound on the test error (\ud835\u0302\udf0etest) as a function of the error on the training set is given by the PAC-MDL bound [7] [8]: For any given distribution D and for the set of all description languages L = {\u03c3} with probability 1 \u2212 \u03b4 over the train and test sets: Strain , Stest~ D : \u2200\u03c3 \ud835\u0302\udf0etest \u2264 bmax(m, n, \ud835\u0302\udf0etrain , 2 \u03b4) While the PAC-MDL bound is used for a transductive setting Banerjee & Langford [7] show that clustering can be converted to a transductive classification problem.", "startOffset": 425, "endOffset": 428}, {"referenceID": 7, "context": "One useful way of looking at this is thinking of clustering as [9]: Consider a dataset that is obtained by sampling a collection of distributions {D1, D2 , .", "startOffset": 63, "endOffset": 66}, {"referenceID": 2, "context": "e whether some arbitrary PMi would do better than PM1 would depend on two main factors: Clusterabilty of the dataset [3] and the choice of predictor.", "startOffset": 117, "endOffset": 120}, {"referenceID": 2, "context": "Previous work by Still & Bialek [3] has formalized and extended this notion of relevance.", "startOffset": 32, "endOffset": 35}, {"referenceID": 2, "context": "Thus, in a sense there is no single best clustering of the data but a family of solutions that evolves with the tradeoff parameter [3].", "startOffset": 131, "endOffset": 134}, {"referenceID": 8, "context": "Ensemble methods have seen a rapid growth in the past decade in the machine learning community [10][11][12].", "startOffset": 95, "endOffset": 99}, {"referenceID": 9, "context": "Ensemble methods have seen a rapid growth in the past decade in the machine learning community [10][11][12].", "startOffset": 99, "endOffset": 103}, {"referenceID": 10, "context": "Ensemble methods have seen a rapid growth in the past decade in the machine learning community [10][11][12].", "startOffset": 103, "endOffset": 107}, {"referenceID": 8, "context": "Dietterich [10] suggests three reasons why ensembles perform better than the individual predictors.", "startOffset": 11, "endOffset": 15}, {"referenceID": 11, "context": "[13].", "startOffset": 0, "endOffset": 4}, {"referenceID": 12, "context": "Domingos [14] use multiple clusterings to better capture the", "startOffset": 9, "endOffset": 13}, {"referenceID": 13, "context": "Ghosh [15] also mention the same, however they use it in coclustering framework and both of these works do not combine", "startOffset": 6, "endOffset": 10}, {"referenceID": 14, "context": "k-means finds a partition by optimizing a distortion function, and while it can be considered to converge in a certain sense (it can be stated to be Lyapunov Stable [16] and thus the objective function decreases monotonically), the distortion function for k-means is non-convex.", "startOffset": 165, "endOffset": 169}, {"referenceID": 15, "context": "The datasets used for the empirical validation of this technique were taken from the University of California, Irvine Machine Learning repository [17].", "startOffset": 146, "endOffset": 150}, {"referenceID": 16, "context": "Each data instance is described by 10 features [18].", "startOffset": 47, "endOffset": 51}, {"referenceID": 17, "context": "This dataset has 103 data instances and 10 attributes, out of which 3 are target attributes (slump, flow and compressive strength); (f) The Forest Fires Dataset (FIRES) is one of the hardest regression datasets available[19].", "startOffset": 220, "endOffset": 224}, {"referenceID": 18, "context": "The Parkinson\u2019s Telemonitoring Dataset (PARKINSON) [20] is a unique dataset in which about 5875 instances are provided, each with 26 attributes.", "startOffset": 51, "endOffset": 55}, {"referenceID": 0, "context": "Normalize the dataset such that the features are scaled to the interval [-1, 1] 2.", "startOffset": 72, "endOffset": 79}, {"referenceID": 0, "context": "Normalize the features between [-1, 1] like in the previous case.", "startOffset": 31, "endOffset": 38}, {"referenceID": 2, "context": "This is understandable, as for small datasets clustering at a high value of k might not be able to reveal the true structure for lack of enough data points and might just end up considering sampling noise as structure [3].", "startOffset": 218, "endOffset": 221}, {"referenceID": 19, "context": "This discussion poses an open model selection problem that could be solved by methods such as those used by the authors in the KDD cup [22] or using averaging as discussed by Caruana [23].", "startOffset": 135, "endOffset": 139}, {"referenceID": 20, "context": "This discussion poses an open model selection problem that could be solved by methods such as those used by the authors in the KDD cup [22] or using averaging as discussed by Caruana [23].", "startOffset": 183, "endOffset": 187}, {"referenceID": 6, "context": "Perhaps the best method for model selection in this case would be the PAC-MDL bound [8].", "startOffset": 84, "endOffset": 87}], "year": 2015, "abstractText": "We explore the utility of clustering in reducing error in various prediction tasks. Previous work has hinted at the improvement in prediction accuracy attributed to clustering algorithms if used to pre-process the data. In this work we more deeply investigate the direct utility of using clustering to improve prediction accuracy and provide explanations for why this may be so. We look at a number of datasets, run k-means at different scales and for each scale we train predictors. This produces k sets of predictions. These predictions are then combined by a na\u00efve ensemble. We observed that this use of a predictor in conjunction with clustering improved the prediction accuracy in most datasets. We believe this indicates the predictive utility of exploiting structure in the data and the data compression handed over by clustering. We also found that using this method improves upon the prediction of even a Random Forests predictor which suggests this method is providing a novel, and useful source of variance in the prediction process.", "creator": "Microsoft\u00ae Word 2013"}}}