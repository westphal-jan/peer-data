{"id": "1509.06279", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "18-Sep-2015", "title": "Sports highlights generation based on acoustic events detection: A rugby case study", "abstract": "We approach the challenging problem of generating highlights from sports broadcasts utilizing audio information only. A language-independent, multi-stage classification approach is employed for detection of key acoustic events which then act as a platform for summarization of highlight scenes. Objective results and human experience indicate that our system is highly efficient.", "histories": [["v1", "Fri, 18 Sep 2015 12:47:09 GMT  (536kb)", "http://arxiv.org/abs/1509.06279v1", "IEEE International Conference on Consumer Electronics (IEEE ICCE 2015)"]], "COMMENTS": "IEEE International Conference on Consumer Electronics (IEEE ICCE 2015)", "reviews": [], "SUBJECTS": "cs.SD cs.AI cs.LG", "authors": ["anant baijal", "jaeyoun cho", "woojung lee", "byeong-seob ko"], "accepted": false, "id": "1509.06279"}, "pdf": {"name": "1509.06279.pdf", "metadata": {"source": "CRF", "title": "Sports highlights generation based on acoustic events detection: A rugby case study", "authors": ["Anant Baijal", "Jaeyoun Cho"], "emails": [], "sections": [{"heading": null, "text": "In fact, most of the people who are able to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance"}, {"heading": "II. SYSTEM", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "A. Training stage", "text": "The training mechanism has three modules, as shown in Fig.1, and explained in the following sub-sub-sections: 1) Data labeling Based on audio content of rugby matches, we observe the following acoustic events as representative candidates to generate highlights: Referee whistle of a referee is one of the most important indicators of when a scene could be classified as the climax of this acoustic event. (See Fig.2) and clearly audible in the audio stream of the broadcast. The audio content of rugby games has a whistle sound that is distinguishable - it has a shrill sound and pitch that is high enough for the whistle to be heard in rugby broadcasts (see Fig.2) and clearly audible in the audio stream example of the broadcast. The audio content of rugby games has a whistle sound that is high enough for the whistle to be heard despite high audience noise or commentators."}, {"heading": "B. Highlights Generator Engine", "text": "In fact, most of them are able to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move."}, {"heading": "III. EXPERIMENTAL RESULTS", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "A. Dataset", "text": "We use actual broadcasts of a variety of rugby matches. The audio stream of rugby matches is converted to 16 kHz, 16-bit, mono-channel format. Training data is annotated manually for 11 different games to determine the veracity for different audio event categories. For evaluation purposes, our test data set consists of 5 games with a total duration of 10 hours and 30 minutes."}, {"heading": "B. Objective evaluation", "text": "We first calculate the recall and precision rates for the detection of important acoustic events in both levels of classification (see Fig.3), as the generation of highlights depends directly on it. For level 1 classification, we select 2,808 \"language segments,\" while for level 2 classification, we select 217 \"excited speech segments\" and 112 \"whistling\" segments (excluding training data) and test our approach; the segments vary in duration. The results in Table I show that our approach yields extremely high recall precision rates in both stages. Next, we evaluate the overall performance of our approach to detecting highlights. The objective results in Table II suggest that our approach to detecting highlights in rugby yields promising results, with a total recall rate of 97.06% and a precision of 93.41% for highlights. A consumer may not only evaluate the highlights on any unexciting or uninteresting scenes, but we would only rate the highlights on each of the unexciting or uninteresting scenes, if they would match or therefore be unsatisfactory for the highlights."}, {"heading": "C. User experience", "text": "To assess the user experience, we asked 11 subjects (9 men, 2 women) to rate our Highlights Generator Engine with a Mean Opinion Score (MOS), taking into account the following: i) the overall highlights are entertaining, enjoyable and enjoyable and not marred by unexciting scenes; ii) the scenes created do not begin or end abruptly; iii) the scenes are acoustically and / or visually exciting; and each user has seen at least ten randomly selected scenes per game."}, {"heading": "IV. DISCUSSION AND CONCLUSIONS", "text": "The precision error in the \"whistle\" category (see Table I) results from overlapping segments in the test data. Our experiments show that these segments were sometimes classified as speech under the \"whistle event\" category after the initial classification, but the multi-level GMM classification approach proved advantageous, as all of these segments were then recovered as \"excited speech.\" The overlapping segments were rightly classified as such: a commentator's voice is in an agitated state when a whistle sounds. In addition, we note that our engine failed to detect a single \"trial\" in Match 4 (see Table II), and our analysis showed that excited speech was overshadowed by crowd sound for large parts of the decision window. We also note that few studies in the literature have reported actual sports highlights generated once important acoustic events are detected; our Highlights Generator Engine effectively reduces the scenes."}], "references": [{"title": "Hierarchical language modeling for audio events detection in a sports game", "author": ["Qiang Huang", "Cox, S."], "venue": "IEEE ICASSP, pp.2286-2289, 2010", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2010}, {"title": "Automatically extracting highlights for TV baseball programs", "author": ["Y. Rui", "A. Gupta", "A. Acero"], "venue": "Proc. ACM Multimedia, pp.105-115, 2000", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2000}, {"title": "Audio based soccer game summarization", "author": ["H. Duxans", "X. Anguera", "D. Conejero"], "venue": "IEEE International Symposium on Broadband Multimedia Systems and Broadcasting, pp.1-6, 2009", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2009}, {"title": "A highlight scene detection and video summarization system using audio feature for a personal video recorder", "author": ["I. Otsuka", "K. Nakane", "A. Divakaran", "K. Hatanaka", "M. Ogawa"], "venue": "IEEE Trans. on Consumer Electronics , vol.51 (1), pp.112-116, 2005", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2005}, {"title": "Audio events detection based highlights extraction from baseball, golf and soccer games in a unified framework", "author": ["Ziyou Xiong", "R. Radhakrishnan", "A. Divakaran", "T.S. Huang"], "venue": "IEEE ICASSP, pp.V-632-5, 2003", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2003}, {"title": "An Autonomous Framework to Produce and Distribute Personalized Team-Sport Video Summaries: A Basketball Case Study", "author": ["F. Chen", "D. Delannay", "C. De Vleeschouwer"], "venue": "IEEE Trans. on Multimedia, , vol.13 (6), pp.1381-1394, 2011", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2011}, {"title": "Multi-modal highlight generation for sports videos using an information-theoretic excitability measure", "author": ["T. Hasan", "H. Bo\u0159il", "A. Sangwan", "J.H. Hansen"], "venue": "EURASIP Journal on Advances in Signal Processing,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2013}, {"title": "Event detection in field sports video using audio-visual features and a support vector Machine", "author": ["D.A. Sadlier", "N.E. O'Connor"], "venue": "IEEE Trans. on Circuits and Systems for Video Technology, vol.15 (10) pp.1225- 1233, 2005", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2005}, {"title": "Integrating highlights for more complete sports video summarization", "author": ["D. Tjondronegoro", "Y.P.P. Chen", "B. Pham"], "venue": "Proc. of the IEEE MultiMedia,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2004}, {"title": "Video abstraction: A systematic review and classification", "author": ["B.T. Truong", "S. Venkatesh"], "venue": "ACM Transactions on Multimedia Computing, Communications, and Applications (TOMCCAP),", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2007}, {"title": "User generated highlight system for baseball games with social media activities", "author": ["Dongmahn Seo", "Suhyun Kim", "Hogun Park", "Heedong Ko"], "venue": "IEEE International Conference on Consumer Electronics (ICCE), pp.349-350, 2014", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2014}, {"title": "Comparison of Parametric Representations for Monosyllabic Word Recognition in Continuously Spoken Sentences.", "author": ["S. Davis", "P. Mermelstein"], "venue": "IEEE Trans. on Acoustics, Speech, and Signal Processing,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 1980}], "referenceMentions": [{"referenceID": 0, "context": "Prior studies have used audio cues [1-5], visual cues [6], mixture of audio-visual cues [7-8], audio-textual cues [9], and a variety of features and classifiers to generate highlights [10].", "startOffset": 35, "endOffset": 40}, {"referenceID": 1, "context": "Prior studies have used audio cues [1-5], visual cues [6], mixture of audio-visual cues [7-8], audio-textual cues [9], and a variety of features and classifiers to generate highlights [10].", "startOffset": 35, "endOffset": 40}, {"referenceID": 2, "context": "Prior studies have used audio cues [1-5], visual cues [6], mixture of audio-visual cues [7-8], audio-textual cues [9], and a variety of features and classifiers to generate highlights [10].", "startOffset": 35, "endOffset": 40}, {"referenceID": 3, "context": "Prior studies have used audio cues [1-5], visual cues [6], mixture of audio-visual cues [7-8], audio-textual cues [9], and a variety of features and classifiers to generate highlights [10].", "startOffset": 35, "endOffset": 40}, {"referenceID": 4, "context": "Prior studies have used audio cues [1-5], visual cues [6], mixture of audio-visual cues [7-8], audio-textual cues [9], and a variety of features and classifiers to generate highlights [10].", "startOffset": 35, "endOffset": 40}, {"referenceID": 5, "context": "Prior studies have used audio cues [1-5], visual cues [6], mixture of audio-visual cues [7-8], audio-textual cues [9], and a variety of features and classifiers to generate highlights [10].", "startOffset": 54, "endOffset": 57}, {"referenceID": 6, "context": "Prior studies have used audio cues [1-5], visual cues [6], mixture of audio-visual cues [7-8], audio-textual cues [9], and a variety of features and classifiers to generate highlights [10].", "startOffset": 88, "endOffset": 93}, {"referenceID": 7, "context": "Prior studies have used audio cues [1-5], visual cues [6], mixture of audio-visual cues [7-8], audio-textual cues [9], and a variety of features and classifiers to generate highlights [10].", "startOffset": 88, "endOffset": 93}, {"referenceID": 8, "context": "Prior studies have used audio cues [1-5], visual cues [6], mixture of audio-visual cues [7-8], audio-textual cues [9], and a variety of features and classifiers to generate highlights [10].", "startOffset": 114, "endOffset": 117}, {"referenceID": 9, "context": "Prior studies have used audio cues [1-5], visual cues [6], mixture of audio-visual cues [7-8], audio-textual cues [9], and a variety of features and classifiers to generate highlights [10].", "startOffset": 184, "endOffset": 188}, {"referenceID": 10, "context": "Studies have also used textual cues from social media for generating sports highlights [11].", "startOffset": 87, "endOffset": 91}, {"referenceID": 4, "context": "Some studies in the past have applied their approach or features for detecting highlights in more than one sport [5], but it may not be efficient to do so, given that some of the sounds found in one sport may be absent in another, and furthermore, the same sounds in different kind of sports can have different characteristics.", "startOffset": 113, "endOffset": 116}, {"referenceID": 11, "context": "We then employ Mel Frequency Cepstral Coefficients (MFCC) [12] and their first order differential coefficients, commonly known as delta-MFCC, to extract requisite features.", "startOffset": 58, "endOffset": 62}], "year": 2015, "abstractText": "We approach the challenging problem of generating highlights from sports broadcasts utilizing audio information only. A language-independent, multi-stage classification approach is employed for detection of key acoustic events which then act as a platform for summarization of highlight scenes. Objective results and human experience indicate that our system is highly efficient.", "creator": "PDFMerge! (http://www.pdfmerge.com)"}}}