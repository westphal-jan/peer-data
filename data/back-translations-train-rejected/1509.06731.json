{"id": "1509.06731", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "22-Sep-2015", "title": "Poker-CNN: A Pattern Learning Strategy for Making Draws and Bets in Poker Games", "abstract": "Poker is a family of card games that includes many variations. We hypothesize that most poker games can be solved as a pattern matching problem, and propose creating a strong poker playing system based on a unified poker representation. Our poker player learns through iterative self-play, and improves its understanding of the game by training on the results of its previous actions without sophisticated domain knowledge. We evaluate our system on three poker games: single player video poker, two-player Limit Texas Hold'em, and finally two-player 2-7 triple draw poker. We show that our model can quickly learn patterns in these very different poker games while it improves from zero knowledge to a competitive player against human experts.", "histories": [["v1", "Tue, 22 Sep 2015 19:05:39 GMT  (610kb,D)", "http://arxiv.org/abs/1509.06731v1", "8 pages"]], "COMMENTS": "8 pages", "reviews": [], "SUBJECTS": "cs.AI", "authors": ["nikolai yakovenko", "liangliang cao", "colin raffel", "james fan"], "accepted": false, "id": "1509.06731"}, "pdf": {"name": "1509.06731.pdf", "metadata": {"source": "CRF", "title": "Poker-CNN: A Pattern Learning Strategy for Making Draws and Bets in Poker Games", "authors": ["Nikolai Yakovenko", "Liangliang Cao", "Colin Raffel", "James Fan"], "emails": ["nvy2101@columbia.edu", "liangliang.cao@gmail.com", "craffel@gmail.com", "jfan.us@gmail.com"], "sections": [{"heading": "Introduction", "text": "In fact, it is the case that most of them are able to abide by the rules that they have imposed on themselves. (...) Most of them are able to understand the rules. (...) Most of them are not able to understand the rules. (...) Most of them are not able to understand the rules. (...) Most of them are not able to understand the rules. (...) Most of them are not able to understand the rules. (...) Most of them are not able to understand the rules. (...) Most of them are not able to understand the rules. (...) Most of them are able to understand the rules. (...) Most of them are able to understand the rules, to understand the rules. (...) Most of them are not able to understand the rules. (...)"}, {"heading": "Related Work", "text": "For example, Bowling (Bowling et al. 2015) et al. have focused much of their research on Heads-up Limit Texas Hold'em, and recently claimed that this limited game is now essentially poorly solved. Their work employs a method called Counterfactual Regret Minimization (CFR) to find an equilibrium solution for Heads-up Limit Texas Holdem, and which examines all possible states in the game of poker. Most of the existing work on CFR is based on Texas Hold'em.While CFR-based approaches have achieved breakthroughs in Texas Hold'em, there are limitations in adapting to other poker games. Firstly, it is not easy or straightforward to generalize this work to other poker variations. Secondly, because quite a few poker games have a search space larger than Heads-up Limit Texas Hold'em, it is very difficult to traverse the game states and find the best answers (with Johads-up Limit 2007)."}, {"heading": "Three poker games", "text": "A player deposits $1 and is dealt five random cards. He can keep any or all of the five cards, and the rest is replaced by new cards from the dealer's stack. 2. Texas Hold'em: Texas Hold'em is a four-betting round multiplayer game. Two cards (hole cards) are dealt face down to each player, and then five community cards are dealt face up by the dealer in three rounds - the first three cards (\"the flop\"), then an additional single card (\"the turn\" or \"fourth street\"), and finally another additional card (\"the river\" or \"fifth street\"), with the best five community cards being dealt face up by the dealer - the first three cards (\"the flop\"), then an additional single card (\"the turn\" or \"fourth street\"), and finally another additional card (\"the river\" or \"fifth street\"), with the best five cards in the poker being dealt either from the community or their hole cards. Players have the third option to check or raise three times after the game, each drawing seven cards in each pile."}, {"heading": "A Unified Representation for Poker Games", "text": "In fact, most of them will be able to move to another world where they are not able, in which they want to live, and in which they are not able, in which they want to live."}, {"heading": "Learning", "text": "A poker agent should perform two types of actions in the game: draw cards and place bets. In the following sections we will describe how to learn these two types of actions."}, {"heading": "Learning to draw", "text": "We consider the task of learning as the following: Given a hand full of poker cards, we estimate the return of all possible ties. In a poker game where the player can have five cards, there are 25 to 32 possible choices per hand, so the machine must estimate the win / loss for each of the possible 32 choices and choose the choice by a large majority. In many cases, it is easy to judge whether a hand has a good draw or a bad result."}, {"heading": "Learning to bet", "text": "This year it has come to the point that it has never come as far as this year."}, {"heading": "Experiments on Video Poker", "text": "We will discuss in this section how our model plays video poker, so it will be easier to understand more complicated game in the next section. Table 2 compares the performance of the different models for video poker. Note that if a player plays each hand perfectly, the average payout is $1,000, but few human players actually get this output4. Human peak performance is usually 1% - 2% worse than perfect 5. However, since video poker is a simple game, it is possible to develop a heuristic (rules-based) player that returns $0.90 + to the Dollar6. We compare our models with a perfect player and a heuristic player. It is easy to see that all of our learning models outperform the random action player and the heuristic player. The performance of our best model ($0.977) is comparable to the strong human player ($0.98 - $0.99).Table 3 takes a closer look at the differences in learning models by comparing the error in each category."}, {"heading": "Experiments on Texas Hold\u2019em & Triple Draw", "text": "We have discussed the simple video poker game in previous sections. Now, we will discuss more complicated games: Heads-up-Limit Texas Hold'em and Heads-up-Limit 2-7 Triple Draw. First, we generate 500,000 examples from which our model learns how to play draws (no draw for Texas Hold'em), then we have two heuristic players play against each other to play 100,000 hands and learn how to make bets based on these outcomes. As betting is difficult, we let the model play against itself, train on these results and repeat this self-play cycle several times. As mentioned above, each round of self-play consists of the latest CNN model that plays against the previous three models to avoid over-optimizing for the weaknesses of the current best model.7Our heuristic player is based on the all-round value of each hand against a random opponent. For each training session, we generate 100,000 poker hands and from these we train on 700,000 weather events. Our last Texas Hold '8 model was trained for self-poker during the Self-poker session."}, {"heading": "Playing Texas hold\u2019em", "text": "To judge the performance of the poker player, we compare our performance to the following models: \u2022 Random Player from ACPC. 8 \u2022 The heuristic player from whom our model is based is a role model. \u2022 An open source implementation of a CFR9 player is generally better than the others. The first noteworthy finding is that our model outperforms the heuristic player from whom it was learned. This supports our hypothesis that the poker game model can outperform its training data. Our model also outperforms the random player and the open source player CFR player. The open source model CFR (Gibson 2014) calculates an equilibrium solution using the \"Pure CFR\" algorithm. This is the only public implementation we can find for a version of the CFR algorithm used by the ACPC limit holdem champions. However, the limitation of this implementation is that it uses all of the poker hands as a bucket."}], "references": [{"title": "Heads-up limit holdem poker is solved", "author": ["Bowling"], "venue": "Science", "citeRegEx": "Bowling,? \\Q2015\\E", "shortCiteRegEx": "Bowling", "year": 2015}, {"title": "and Storkey", "author": ["C. Clark"], "venue": "A.", "citeRegEx": "Clark and Storkey 2014", "shortCiteRegEx": null, "year": 2014}, {"title": "2001", "author": ["Dahl", "F. A"], "venue": "A reinforcement learning algorithm applied to simplified two-player texas holdem poker. In Machine Learning: ECML", "citeRegEx": "Dahl 2001", "shortCiteRegEx": null, "year": 2001}, {"title": "takacsg84; peterderivaz; Jon", "author": ["Dieleman"], "venue": null, "citeRegEx": "Dieleman,? \\Q2015\\E", "shortCiteRegEx": "Dieleman", "year": 2015}, {"title": "Regret Minimization in Games and the Development of Champion Multiplayer Computer Poker-Playing Agents", "author": ["R. Gibson"], "venue": "Ph.D. Dissertation, University of Alberta", "citeRegEx": "Gibson,? \\Q2014\\E", "shortCiteRegEx": "Gibson", "year": 2014}, {"title": "Fictitious self-play in extensiveform games", "author": ["Lanctot Heinrich", "J. Silver 2015] Heinrich", "M. Lanctot", "D. Silver"], "venue": "In Proceedings of the 32nd International Conference on Machine Learning,", "citeRegEx": "Heinrich et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Heinrich et al\\.", "year": 2015}, {"title": "Accelerating best response calculation in large extensive games", "author": ["Johanson"], "venue": "In IJCAI,", "citeRegEx": "Johanson,? \\Q2011\\E", "shortCiteRegEx": "Johanson", "year": 2011}, {"title": "Evaluating state-space abstractions in extensive-form games", "author": ["Johanson"], "venue": "In Proceedings of the 2013 international conference on Autonomous agents and multiagent systems,", "citeRegEx": "Johanson,? \\Q2013\\E", "shortCiteRegEx": "Johanson", "year": 2013}, {"title": "Evaluating state-space abstractions in extensive-form games", "author": ["Johanson"], "venue": "In Proceedings of the 2013 international conference on Autonomous agents and multiagent systems,", "citeRegEx": "Johanson,? \\Q2013\\E", "shortCiteRegEx": "Johanson", "year": 2013}, {"title": "G", "author": ["A. Krizhevsky", "I. Sutskever", "Hinton"], "venue": "E.", "citeRegEx": "Krizhevsky. Sutskever. and Hinton 2012", "shortCiteRegEx": null, "year": 2012}, {"title": "Gradient based learning applied to document recognition", "author": ["LeCun"], "venue": "PIEEE", "citeRegEx": "LeCun,? \\Q1998\\E", "shortCiteRegEx": "LeCun", "year": 1998}, {"title": "C", "author": ["Maddison"], "venue": "J.; Huang, A.; Sutskever, I.; and Silver, D.", "citeRegEx": "Maddison et al. 2014", "shortCiteRegEx": null, "year": 2014}, {"title": "A", "author": ["V. Mnih", "K. Kavukcuoglu", "D. Silver", "A.A. Rusu", "J. Veness", "M.G. Bellemare", "A. Graves", "M. Riedmiller", "Fidjeland"], "venue": "K.; Ostrovski, G.; et al.", "citeRegEx": "Mnih et al. 2015", "shortCiteRegEx": null, "year": 2015}, {"title": "and Watson", "author": ["J. Rubin"], "venue": "I.", "citeRegEx": "Rubin and Watson 2011", "shortCiteRegEx": null, "year": 2011}, {"title": "Abstraction for solving large incomplete-information games", "author": ["T. Sandholm 2015a] Sandholm"], "venue": "In AAAI Conference on Artificial Intelligence (AAAI). Senior Member Track", "citeRegEx": "Sandholm,? \\Q2015\\E", "shortCiteRegEx": "Sandholm", "year": 2015}, {"title": "and Zisserman", "author": ["K. Simonyan"], "venue": "A.", "citeRegEx": "Simonyan and Zisserman 2014", "shortCiteRegEx": null, "year": 2014}, {"title": "On the importance of initialization and momentum in deep learning", "author": ["Sutskever"], "venue": null, "citeRegEx": "Sutskever,? \\Q2013\\E", "shortCiteRegEx": "Sutskever", "year": 2013}, {"title": "Deepface: Closing the gap to humanlevel performance in face verification", "author": ["Taigman"], "venue": null, "citeRegEx": "Taigman,? \\Q2014\\E", "shortCiteRegEx": "Taigman", "year": 2014}, {"title": "Regret minimization in games with incomplete information", "author": ["Zinkevich"], "venue": "In Advances in neural information processing systems,", "citeRegEx": "Zinkevich,? \\Q2007\\E", "shortCiteRegEx": "Zinkevich", "year": 2007}], "referenceMentions": [], "year": 2015, "abstractText": "Poker is a family of card games that includes many variations. We hypothesize that most poker games can be solved as a pattern matching problem, and propose creating a strong poker playing system based on a unified poker representation. Our poker player learns through iterative self-play, and improves its understanding of the game by training on the results of its previous actions without sophisticated domain knowledge. We evaluate our system on three poker games: single player video poker, two-player Limit Texas Hold\u2019em, and finally two-player 2-7 triple draw poker. We show that our model can quickly learn patterns in these very different poker games while it improves from zero knowledge to a competitive player against human experts. The contributions of this paper include: (1) a novel representation for poker games, extendable to different poker variations, (2) a CNN based learning model that can effectively learn the patterns in three different games, and (3) a selftrained system that significantly beats the heuristic-based program on which it is trained, and our system is competitive against human expert players.", "creator": "LaTeX with hyperref package"}}}