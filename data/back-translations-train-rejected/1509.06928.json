{"id": "1509.06928", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "23-Sep-2015", "title": "Automatic Dialect Detection in Arabic Broadcast Speech", "abstract": "We investigate different approaches for dialect identification in Arabic broadcast speech, using phonetic, lexical features obtained from a speech recognition system, and acoustic features using the i-vector framework. We studied both generative and discriminate classifiers, and we combined these features using a multi-class Support Vector Machine (SVM). We validated our results on an Arabic/English language identification task, with an accuracy of 100%. We used these features in a binary classifier to discriminate between Modern Standard Arabic (MSA) and Dialectal Arabic, with an accuracy of 100%. We further report results using the proposed method to discriminate between the five most widely used dialects of Arabic: namely Egyptian, Gulf, Levantine, North African, and MSA, with an accuracy of 52%. We discuss dialect identification errors in the context of dialect code-switching between Dialectal Arabic and MSA, and compare the error pattern between manually labeled data, and the output from our classifier. We also release the train and test data as standard corpus for dialect identification.", "histories": [["v1", "Wed, 23 Sep 2015 11:41:10 GMT  (1299kb,D)", "https://arxiv.org/abs/1509.06928v1", null], ["v2", "Thu, 11 Aug 2016 00:28:59 GMT  (1450kb)", "http://arxiv.org/abs/1509.06928v2", null]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["ahmed ali", "najim dehak", "patrick cardinal", "sameer khurana", "sree harsha yella", "james glass", "peter bell", "steve renals"], "accepted": false, "id": "1509.06928"}, "pdf": {"name": "1509.06928.pdf", "metadata": {"source": "CRF", "title": "Automatic Dialect Detection in Arabic Broadcast Speech", "authors": ["Ahmed Ali", "Najim Dehak", "Patrick Cardinal", "Sameer Khurana", "Sree Harsha Yella", "James Glass", "Peter Bell", "Steve Renals"], "emails": ["amali@qf.org.qa,", "skhurana@qf.org.qa,", "najim@jhu.edu"], "sections": [{"heading": null, "text": "ar Xiv: 150 9.06 928v 2 [cs.C L] 11 Aug 201 6"}, {"heading": "1. Introduction", "text": "In fact, we will be able to go in search of a solution that meets the needs of the people, \"he told the German Press Agency.\" We are not afraid, \"he said,\" but we know that we will be able to find a solution. \""}, {"heading": "2. Vector Space Models", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "2.1. Senone based Utterance VSM", "text": "In our case n \u2264 4. VSM construction takes place in two steps: First, a phoneme detector is used to extract the senon [10] sequence for a given1https: / / github.com / Qatar-Computing-Research-Institute / dialectIDspeech. The phoneme sequence is achieved by automatic vowelization of the training text, followed by vowelization for phonization (V2P). The 36 selected phonemes cover all dialectal Arabic sounds. Further details about the speech recognition pipeline, training data and phoneme set are given in [11]. For the phoneme sequence, we process the phoneme grid and obtain the one-best transcription, silence and noisy silence."}, {"heading": "2.2. Word based Utterance VSM", "text": "The word-based utterance VSM (Uw) is constructed in two steps similar to the senone characteristics: An ASR system is used to extract the word sequence for each utterance in the language database. Details of the ASR system can be found in [11]. Each speech utterance (u) is then presented as a high-dimensional sparse vector (# \"u): #\" u = (A (f (u, w1), A (f (u, w2))),..., A (f (u, wd \u2032)))), (2) where f (u, wi) is the number of times a word wi occurs in the speech utterance u and A is the scaling function that has the same interpretation as for us (above). Vocabulary size was 55k. The size of the tri-gram dictionary was 580k, with which we constructed the word-based VSM (Uw)."}, {"heading": "2.3. i-vector-based Utterance VSM", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "2.3.1. Bottleneck Features (BN)", "text": "Recently, bottlenecks extracted from an ASR DNN-based model have been successfully applied to speech recognition [29, 30, 28]. In this work, we used a similar bottleneck as in our previous ASR DNN system for MSA speech recognition [27]. This system is based on two successive DNN models. Both DNNs use the same structure of 5 hidden sigmoid layers and 1 linear BN layer, and both are based on bound states as the target output. The senon labels of dimension 3040 are generated by a forced alignment from an HMM-GMM baseline trained on 60 hours of manually transcribed Al-Jazeera MSA message recordings [11]. The input to the first DNN consists of 23 critical band energies extracted from Mel Bendruck's filter bank. Pitch and voicing probability are then added."}, {"heading": "2.3.2. Modeling", "text": "An effective and well-studied method in speech and dialect recognition is the i-vector approach [7, 16, 17]. The i-vector involves modeling speech using a universal background model (UBM) - typically a large GMM - trained on a large amount of data to represent general characteristics that play a role in how all dialects look. The i-vector approach is a powerful technique that summarizes all updates that take place during the adaptation of the UBM middlecomponents to a given utterance, all of which information is modeled in a low-dimensional subspace called total variability space. In the i-vector framework, each speech expression can be represented by a GMM supervector that is generated as follows: M = u + TvWhere u is the channel and dialect of independent supervector (which can be considered the UVector supervector)."}, {"heading": "3. Dataset", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "3.1. Train Data", "text": "The training corpus was collected from the broadcast news section in four Arabic dialects (EGY, LAV, GLF and NOR) as well as in MSA. Data was recorded at 16Khz. Recordings were segmented to avoid overlapping between speakers, removing non-linguistic parts such as music and background noise. Further details on training data can be found in [7]. Although the test database came from the same broadcast area, the recording setup differs. Test data was downloaded directly from the high-quality video server for Aljazeera (bright-cove) between July 2104 and January 2015 as part of the QCRI Advanced Transcription Service (QATS) [19]."}, {"heading": "3.2. Test Data", "text": "The test set was labeled with the Crowdsource platform CrowdFlower, with criteria of at least three judges per file and up to nine judges or 75% Inter-Annotator Agreement (whichever comes first). Further details on the test set and the crowdsourcing experiment can be found in [20]. The test set used in this article differs from the one used in [7] for two reasons: Firstly, the crowdsourced data is available to reproduce the results and can therefore be used as the default test set for Arabic DID; secondly, the new test set was collected on different channels and the record setup compared to the training data, making our experiments less sensitive to channel / speaker characteristics. The move and test data can be found on the QCRI web portal2. Table 2 and Table 3 present some statistics on the move and test data."}, {"heading": "4. Experiments", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.1. Choosing the Best Classifier", "text": "First, we investigated the best classification approach for the DID task using two generative models: the n-gram language model [21] and Naive Bayes [22], as well as two discriminatory classifiers: linear SVM [23] and maximum entropy [24]. We measured the performance of each model against the DID task, in the word or lexical expression vector space constructed using the approach mentioned in Section 2, using the identity scaling function A and without dimension reduction. Therefore, the dimensionality of an expression vector, # \"u, corresponds to the size of the lexicon, which in our case was 55k. Results can be seen in Table 4. Since linear SVM performs best, it is our choice of the classifier for the rest of the experiments."}, {"heading": "4.2. Feature Selection Study", "text": "Here we examine the dialect information captured by the three VSMs discussed in Section 2. We also examine the concatenation of vector expressions and report on the results in Tables 5 and 6. Details of the terms in the result table are listed below: \u2022 Uiw: Refers to the VSM expression in which each expression is represented by a vector given by Equation 2, with A selected as the identity function. Basen2http: / / alt.qcri.org / resources / ArabicDialectIDCorpus / Model ACC PRC RCLn-gram Language Model 40.4% 40.2% 41.3% Naive Bayes 37.9% 37.5% 50.2% Max Ent 40% 40% 40.6% SVM 45.2% 44.8% 45.4%"}, {"heading": "4.3. One Vs All classification (Sanity Check)", "text": "We constructed a senon-based expression VSM (Section 2.1) based on 20 hours of language; 10 hours of English (which we received from [?]) and 10 hours of Arabic (randomly from our training data, Section 3); then a binary classification (English versus Arabic) was performed using an SVM classifier, which yielded a 100% accuracy of the 1.5-hour test set. The reason to choose the senon-based attribute space rather than the i-vector-based attribute space for the classification is to avoid channel errors as the English data came from a different source range. We conducted a similar experiment to classify MSA against all dialect Arabs and again achieved 100% classification accuracy."}, {"heading": "4.4. System Output Combination", "text": "We merged the values of the best senon system and the SVM-based i vector system. In the fusion steps, the original values of each system were normalized and combined with the same fusion weights for both systems. This approach yielded a final accuracy of 60.2%, which is the best performance we have achieved. An explanation for this gain is that the error patterns for the two attribute spaces are quite different, and we were able to confirm this by analyzing the confusion matrix for each system."}, {"heading": "5. Discussion", "text": "We conclude from the confusion matrix in Table 7 that GLF and LAV are the most confusing dialect pair. We believe this is due to the greater lexical similarity between these two dialects (see Table 1). The confusion matrix comes from the best DID system. We borrowed Table 8 from previous work [20] on the test kit, which shows how long the same speakers switch between dialect and other dialects (mainly MSA and their own native dialect). For example, in the second row of Table 8 there are 200 samples of potential speakers from the Gulf. After manual labeling, 106 (53%) segments were validated as MSA, 82 (41%) as GLF, 8 (4%) as LAV, and 4 segments were not identified with sufficient certainty to be considered."}, {"heading": "6. Conclusions", "text": "We demonstrated a dialect classifier with an accuracy of 60.2% using system combinations and achieved 100% accuracy in two binary classification tasks: MSA versus dialectal Arabic and English versus Arabic. We investigated the possible code switching pattern in our classifier and its correlation with manual annotation. Further work for this research is investigating the code switch between MSA and dialectal Arabic without taking into account speaker diarization or silence between language segments in what can be called dialect diarization. We will also explore deep neural network approaches to classification to learn a more complex nonlinear decision boundary."}, {"heading": "7. References", "text": "[1] G. Liu, Y. Lei, and J. H. Hansen, \"Dialect Identification: Impact ofdifferences between read versus spontaneous speech,\" EUSIPCO2010: European Signal Processing Conference, Aalborg, Denmark, 2010. [2] D. A. Reynolds, W. M. Campbell, W. Shen, and E. Singer, \"Automatic language recognition via spectral and token based approaches,\" in Springer Handbook of Speech Processing, J. Benesty, M. M. Sondhi, and Y. Huang, Eds. Springer, 2008. [3] E. Ambikairajah, H. Li, L. Yin, and V. Sethu \"Language identification: A tutorial.\""}], "references": [{"title": "Dialect identification: Impact of differences between read versus spontaneous speech", "author": ["G. Liu", "Y. Lei", "J.H. Hansen"], "venue": "EUSIPCO- 2010: European Signal Processing Conference, Aalborg, Denmark, 2010.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2010}, {"title": "Automatic language recognition via spectral and token based approaches", "author": ["D.A. Reynolds", "W.M. Campbell", "W. Shen", "E. Singer"], "venue": "Springer Handbook of Speech Processing, J. Benesty, M. M. Sondhi, and Y. Huang, Eds. Springer, 2008.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2008}, {"title": "Language identification: A tutorial", "author": ["E. Ambikairajah", "H. Li", "L. Wang", "B. Yin", "V. Sethu"], "venue": "Circuits and Systems Magazine, IEEE, vol. 11, no. 2, pp. 82\u2013108.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 0}, {"title": "Comparison of four approaches to automatic language identification of telephone speech", "author": ["M. Zissman"], "venue": "IEEE Transactions on Speech and Audio Processing, vol. 4, no. 1, pp. 31\u201344, 1996.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 1996}, {"title": "ivector-based prosodic system for language identification", "author": ["D. Mart\u0131\u0301nez", "L. Burget", "L. Ferrer", "N. Scheffer"], "venue": "ICASSP, 2012, pp. 4861\u20134864.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2012}, {"title": "Pllr features in language recognition system for rats", "author": ["O. Plchot", "M. Diez", "M. Soufifar", "L. Burget"], "venue": "Fifteenth Annual Conference of the International Speech Communication Association, 2014.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2014}, {"title": "Nonnegative factor analysis for gmm weight adaptation", "author": ["M.H. Bahari", "N. Dehak", "L. Burget", "A. Ali", "J. Glass"], "venue": "IEEE Transactions on Audio Speech and Language Processing, 2014.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2014}, {"title": "From modern standard arabic to levantine asr: Leveraging gale for dialects", "author": ["H. Soltau", "L. Mangu", "F. Biadsy"], "venue": "ASRU, 2011, pp. 266\u2013271.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2011}, {"title": "Discriminative classifiers for phonotactic language recognition with ivectors", "author": ["M. Soufifar", "S. Cumani", "L. Burget", "J. \u010cernocky"], "venue": "ICASSP, 2012, pp. 4853\u20134856.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2012}, {"title": "Subphonetic Modeling for Speech Recognition", "author": ["M.-y. Hwang"], "venue": "pp. 174\u2013179.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 0}, {"title": "A complete kaldi recipe for building arabic speech recognition systems", "author": ["A. Ali", "Y. Zhang", "P. Cardinal", "N. Dahak", "S. Vogel", "J. Glass"], "venue": "SLT, 2014.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2014}, {"title": "A Vector Space Model for Automatic Indexing", "author": ["G. Salton", "a. Wong", "C.S. Yang"], "venue": "Magazine Communications of the ACM, vol. 18, no. 11, pp. 613\u2013620, 1975.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 1975}, {"title": "Division of Informatics , University of Edinburgh Institute for Adaptive and Neural Computation The Direct Route : Mediated Priming in Semantic Space by The Direct Route : Mediated Priming in Semantic Space", "author": ["W. Lowe", "S. Mcdonald", "W. Lowe", "S. Mcdonald"], "venue": "no. April, 2000.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2000}, {"title": "Dependency-Based Construction of Semantic Space Models", "author": ["S. Pad\u00f3", "M. Lapata"], "venue": "Computational Linguistics, vol. 33, no. December 2004, pp. 161\u2013199, 2007.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2004}, {"title": "Front-end factor analysis for speaker verification", "author": ["N. Dehak", "P. Kenny", "R. Dehak", "P. Dumouchel", "P. Ouellet"], "venue": "Audio, Speech, and Language Processing, IEEE Transactions on, pp. 788\u2013798, 2011.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2011}, {"title": "Language recognition via i-vectors and dimensionality reduction", "author": ["N. Dehak", "P.A. Torres-Carrasquillo", "D.A. Reynolds", "R. Dehak"], "venue": "INTERSPEECH, 2011, pp. 857\u2013860.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2011}, {"title": "A study of interspeaker variability in speaker verification", "author": ["P. Kenny", "P. Ouellet", "N. Dehak", "V. Gupta", "P. Dumouchel"], "venue": "Audio, Speech, and Language Processing, IEEE Transactions on, pp. 980\u2013988, 2008.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2008}, {"title": "QCRI advanced transcription ssystem (QATS)", "author": ["A. Ali", "Y. Zhang", "S. Vogel"], "venue": "SLT, 2014.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2014}, {"title": "Crowdsource a little to label a lot: Labeling a speech corpus of dialectal arabic", "author": ["S. Wray", "A. Ali"], "venue": "INTERSPEECH, 2015.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2015}, {"title": "CS229 Lecture notes Generative Learning algorithms", "author": ["A. Ng"], "venue": "no. 0, pp. 1\u201314.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 0}, {"title": "Support vector machines for spam categorization", "author": ["H. Drucker", "D. Wu", "V.N. Vapnik"], "venue": "Neural Networks, IEEE Transactions on, vol. 10, no. 5, pp. 1048\u20131054, 1999.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 1999}, {"title": "Lium spkdiarization: an open source toolkit for diarization", "author": ["S. Meignier", "T. Merlin"], "venue": "CMU SPUD Workshop, 2010.", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2010}, {"title": "Code-Switching Event Detection by Using a Latent Language Space Model and the Delta- Bayesian", "author": ["C.-h. Wu", "H.-p. Shen", "C.-s. Hsu"], "venue": "vol. 23, no. 11, pp. 1892\u20131903, 2015.", "citeRegEx": "26", "shortCiteRegEx": null, "year": 1892}, {"title": "Speaker Adaptation Using the I-Vector Technique for Bottleneck Features", "author": ["P. Cardinal", "N. Dehak", "Y. Zhang", "J. Glass"], "venue": "INTERSPEECH, 2015.", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2015}, {"title": "A Unified Deep Neural Network for Speaker and Language Recognition", "author": ["F. Richardson", "D. Reynolds", "N. Dehak"], "venue": "INTER- SPEECH, 2015.", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2015}, {"title": "I-vector representation based on bottleneck features for language identification", "author": ["Y. Song", "B. Jiang", "Y. Bao", "S. Wei", "L.R. Dai"], "venue": "IEEE Electronics Letters, 2013.", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2013}, {"title": "Neural network bottleneck features for language identification", "author": ["P. Matejka", "L. Zhang", "T. Ng", "H.S. Mallidi", "O. Glembek", "J. Ma", "B. Zhang"], "venue": "Proc. of Odyssey, 2014. This figure \"DialectDetectionSpace.png\" is available in \"png\" format from: http://arxiv.org/ps/1509.06928v2 This figure \"bnf.png\" is available in \"png\" format from: http://arxiv.org/ps/1509.06928v2", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2014}], "referenceMentions": [{"referenceID": 0, "context": "The importance of addressing DID can be gauged from its growing interest in the Automatic Speech Recognition (ASR) community [1].", "startOffset": 125, "endOffset": 128}, {"referenceID": 1, "context": "In the lexical area, words, roots, morphology, and grammars [2, 3] have been studied.", "startOffset": 60, "endOffset": 66}, {"referenceID": 2, "context": "In the lexical area, words, roots, morphology, and grammars [2, 3] have been studied.", "startOffset": 60, "endOffset": 66}, {"referenceID": 15, "context": "Acoustic features such as shifted delta cepstral coefficients [17] and prosodic features [5] using Gaussian mixture models (GMMs), i-vector representations and support vector machine (SVM) classifiers [17] have been shown to be effective for LID.", "startOffset": 62, "endOffset": 66}, {"referenceID": 4, "context": "Acoustic features such as shifted delta cepstral coefficients [17] and prosodic features [5] using Gaussian mixture models (GMMs), i-vector representations and support vector machine (SVM) classifiers [17] have been shown to be effective for LID.", "startOffset": 89, "endOffset": 92}, {"referenceID": 15, "context": "Acoustic features such as shifted delta cepstral coefficients [17] and prosodic features [5] using Gaussian mixture models (GMMs), i-vector representations and support vector machine (SVM) classifiers [17] have been shown to be effective for LID.", "startOffset": 201, "endOffset": 205}, {"referenceID": 5, "context": "More recent work explored the use of frame-by-frame phone posteriors (PLLRs) [6] as new features for LID.", "startOffset": 77, "endOffset": 80}, {"referenceID": 6, "context": "New subspace approaches based on non-negative factor analysis (NFA) for GMM weight decomposition and adaptation [7] were also applied to both LID and DID tasks.", "startOffset": 112, "endOffset": 115}, {"referenceID": 7, "context": "Finally, phoneme sequence modeling and its n-gram subspace have been studied for both Arabic DID [8] and LID [9].", "startOffset": 97, "endOffset": 100}, {"referenceID": 8, "context": "Finally, phoneme sequence modeling and its n-gram subspace have been studied for both Arabic DID [8] and LID [9].", "startOffset": 109, "endOffset": 112}, {"referenceID": 9, "context": "VSM construction takes place in two steps: first, a phoneme recognizer is used to extract the senone [10] sequence for a given", "startOffset": 101, "endOffset": 105}, {"referenceID": 10, "context": "Further details about the speech recognition pipeline, training data, and phoneme set is given in [11].", "startOffset": 98, "endOffset": 102}, {"referenceID": 11, "context": "This approach and the notation used to define a VSM is directly inspired by the seminal works in the area of VSM of Natural Language in [13, 14, 15] and in LID [?].", "startOffset": 136, "endOffset": 148}, {"referenceID": 12, "context": "This approach and the notation used to define a VSM is directly inspired by the seminal works in the area of VSM of Natural Language in [13, 14, 15] and in LID [?].", "startOffset": 136, "endOffset": 148}, {"referenceID": 13, "context": "This approach and the notation used to define a VSM is directly inspired by the seminal works in the area of VSM of Natural Language in [13, 14, 15] and in LID [?].", "startOffset": 136, "endOffset": 148}, {"referenceID": 10, "context": "Details about the ASR system can be found in [11].", "startOffset": 45, "endOffset": 49}, {"referenceID": 25, "context": "Recently, bottleneck features extracted from an ASR DNNbased model were applied successfully to language identification [29, 30, 28].", "startOffset": 120, "endOffset": 132}, {"referenceID": 26, "context": "Recently, bottleneck features extracted from an ASR DNNbased model were applied successfully to language identification [29, 30, 28].", "startOffset": 120, "endOffset": 132}, {"referenceID": 24, "context": "Recently, bottleneck features extracted from an ASR DNNbased model were applied successfully to language identification [29, 30, 28].", "startOffset": 120, "endOffset": 132}, {"referenceID": 23, "context": "In this paper, we used a similar bottleneck features configurations as in our previous ASR-DNN system for MSA speech recognition [27].", "startOffset": 129, "endOffset": 133}, {"referenceID": 10, "context": "The senone labels of dimension 3040 are generated by a forced alignment from an HMM-GMM baseline trained on 60 hours of manually transcribed Al-Jazeera MSA news recordings [11].", "startOffset": 172, "endOffset": 176}, {"referenceID": 6, "context": "An effective and well-studied method in language and dialect recognition is the i-vector approach [7, 16, 17].", "startOffset": 98, "endOffset": 109}, {"referenceID": 14, "context": "An effective and well-studied method in language and dialect recognition is the i-vector approach [7, 16, 17].", "startOffset": 98, "endOffset": 109}, {"referenceID": 15, "context": "An effective and well-studied method in language and dialect recognition is the i-vector approach [7, 16, 17].", "startOffset": 98, "endOffset": 109}, {"referenceID": 16, "context": "An efficient procedure for training and for MAP adaptation of ivector can be found in [18].", "startOffset": 86, "endOffset": 90}, {"referenceID": 15, "context": "In order to maximize the discrimination between the different dialect classes in the i-vector space, we combine Linear Discriminant Analysis (LDA) and Within Class Co-variance Normalization [17].", "startOffset": 190, "endOffset": 194}, {"referenceID": 15, "context": "This intersession compensation method has been used with both SVM [17] and cosine scoring [7].", "startOffset": 66, "endOffset": 70}, {"referenceID": 6, "context": "This intersession compensation method has been used with both SVM [17] and cosine scoring [7].", "startOffset": 90, "endOffset": 93}, {"referenceID": 6, "context": "More details about the training data can be found in [7].", "startOffset": 53, "endOffset": 56}, {"referenceID": 17, "context": "cove) over the period of July 2104 until January 2015, as part of QCRI Advanced Transcription Service (QATS) [19].", "startOffset": 109, "endOffset": 113}, {"referenceID": 18, "context": "More details about the test set and crowdsourcing experiment can be found in [20].", "startOffset": 77, "endOffset": 81}, {"referenceID": 6, "context": "The test set used in this paper differs from that used in [7] for two reasons: First, the crowdsourced data is available to reproduce the results, and thus can be used as a standard test set for Arabic DID; second, the new test set has been collected using different channels, and recording setup compared to the training data, which makes our experiments less sensitive to channel/speaker characteristics.", "startOffset": 58, "endOffset": 61}, {"referenceID": 19, "context": "We first studied the best classification approach for the DID task from a set of two generative models: n-gram language model [21] and Naive Bayes [22], and two discriminative classifiers: linear SVM [23] and Maximum Entropy [24].", "startOffset": 147, "endOffset": 151}, {"referenceID": 20, "context": "We first studied the best classification approach for the DID task from a set of two generative models: n-gram language model [21] and Naive Bayes [22], and two discriminative classifiers: linear SVM [23] and Maximum Entropy [24].", "startOffset": 200, "endOffset": 204}, {"referenceID": 15, "context": "We do not experiment with different i-vector dimensions and take the best dimension reported in [17] for the LID task.", "startOffset": 96, "endOffset": 100}, {"referenceID": 15, "context": "\u2022 U iVec+LDA+WCNN: Reducing the dimensionality of the i-vector space using LDA and performing WCNN has been reported to do well in LID tasks [17] and we use the same technique and see a significant improvement in the DID results.", "startOffset": 141, "endOffset": 145}, {"referenceID": 18, "context": "We borrowed Table 8 from previous work [20] on the test set, which shows the amount of time the same speakers switch between dialect and another (mainly MSA, and their own native dialect).", "startOffset": 39, "endOffset": 43}], "year": 2016, "abstractText": "In this paper, we investigate different approaches for dialect identification in Arabic broadcast speech. These methods are based on phonetic and lexical features obtained from a speech recognition system, and bottleneck features using the i-vector framework. We studied both generative and discriminative classifiers, and we combined these features using a multi-class Support Vector Machine (SVM). We validated our results on an Arabic/English language identification task, with an accuracy of 100%. We also evaluated these features in a binary classifier to discriminate between Modern Standard Arabic (MSA) and Dialectal Arabic, with an accuracy of 100%. We further reported results using the proposed methods to discriminate between the five most widely used dialects of Arabic: namely Egyptian, Gulf, Levantine, North African, and MSA, with an accuracy of 59.2%. We discuss dialect identification errors in the context of dialect code-switching between Dialectal Arabic and MSA, and compare the error pattern between manually labeled data, and the output from our classifier. All the data used on our experiments have been released to the public as a language identification corpus.", "creator": "LaTeX with hyperref package"}}}