{"id": "1509.07211", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "24-Sep-2015", "title": "Noise-Robust ASR for the third 'CHiME' Challenge Exploiting Time-Frequency Masking based Multi-Channel Speech Enhancement and Recurrent Neural Network", "abstract": "In this paper, the Lingban entry to the third 'CHiME' speech separation and recognition challenge is presented. A time-frequency masking based speech enhancement front-end is proposed to suppress the environmental noise utilizing multi-channel coherence and spatial cues. The state-of-the-art speech recognition techniques, namely recurrent neural network based acoustic and language modeling, state space minimum Bayes risk based discriminative acoustic modeling, and i-vector based acoustic condition modeling, are carefully integrated into the speech recognition back-end. To further improve the system performance by fully exploiting the advantages of different technologies, the final recognition results are obtained by lattice combination and rescoring. Evaluations carried out on the official dataset prove the effectiveness of the proposed systems. Comparing with the best baseline result, the proposed system obtains consistent improvements with over 57% relative word error rate reduction on the real-data test set.", "histories": [["v1", "Thu, 24 Sep 2015 02:16:11 GMT  (95kb,D)", "http://arxiv.org/abs/1509.07211v1", "The 3rd 'CHiME' Speech Separation and Recognition Challenge, 5 pages, 1 figure"]], "COMMENTS": "The 3rd 'CHiME' Speech Separation and Recognition Challenge, 5 pages, 1 figure", "reviews": [], "SUBJECTS": "cs.SD cs.CL", "authors": ["zaihu pang", "fengyun zhu"], "accepted": false, "id": "1509.07211"}, "pdf": {"name": "1509.07211.pdf", "metadata": {"source": "CRF", "title": "NOISE-ROBUST ASR FOR THE THIRD \u2018CHIME\u2019 CHALLENGE EXPLOITING TIME-FREQUENCY MASKING BASED MULTI-CHANNEL SPEECH ENHANCEMENT AND RECURRENT NEURAL NETWORK", "authors": ["Zaihu Pang", "Fengyun Zhu"], "emails": ["fyzhu}@ling-ban.com"], "sections": [{"heading": null, "text": "In fact, most of them are able to go to another world, in which they go to another world, but in which they have not gone to another world."}, {"heading": "2.1. MSC based time-frequency masking", "text": "The MSC between two signals x (t) and y (t) is defined as: Cxy (f) = | Sxy (f) | 2Sxx (f) Syy (f), (1) where f is the frequency, Sxy (f) is the cross-spectral density between the two signals, Sxx (f) and Syy (f) is the auto-spectral density of x or y. The values of the coherence function, which are always 0 \u2264 Cxy (f) \u2264 1, indicate to what extent the power of y could be predicted by a linear system. In multi-channel signal processing, the MSC is an efficient means of noise reduction [16]. In the case of a microphone field with a sufficiently large microphone pitch, incoherent (diffuse) noise would be indicated by small coherence values, while the directional signals would have large coherence values."}, {"heading": "2.2. PDM based time-frequency masking", "text": "In the past, it has been shown that the phase difference based on the time frequency masked by noise suppression is effective for two-channel simulated data [19]. After the multi-channel signals are directed in the target direction by deceleration alignment, phase differences between the channels could be achieved. Time frequency vessels with a phase difference that is not close to zero are less likely to be those from the target direction. In the current \"CHiME\" challenge, the experimental condition of two-channel simulated data is extended to six-channel real recordings. In the case of multi-channel subband input signals, the delay alignment occurs by steering the signals in the direction indicated by the loudspeaker locator. Phase differences between all microphone pairs (except the second channel and the failed ones) could be calculated for all frequency bins (multi-channel input values) by means of the PDM."}, {"heading": "2.3. Microphone array self-calibration", "text": "In fact, you will be able to go to a place where you can go to a place where you can go to a place where you can go to another world."}, {"heading": "4.1. Experimental setups", "text": "The GMM-HMM system was trained with Kaldi [28]. All networks were trained on the basis of the alignment result of the GMM-HMM system with 2824 context-dependent HMM states. In the training sequence of the LSTM networks, the gradient reduction strategy introduced by [29] was applied. As the information from the future frames helps the LSTM networks to make better decisions, we also delayed the output of the HMM state labels by 4 frames. Upstream DNNs used the concatenated features generated by concatenating the current frame with 7 frames in the left and right context. Inputs to the LSTMP networks were only the current frames."}, {"heading": "4.2. Speech enhancement", "text": "Table 1 shows the evaluation results of the proposed speech enhancement methods with the acoustic base model (GMM-HMM) and the language model (3-gram). Comparing the results with the baseline using noise data, the baseline performance with the front-end speech enhancement in the simulated test is greatly improved, but worsened in the real test. This phenomenon shows that the mismatch between improved real and simulated data is introduced in the baseline through the based speech enhancement process. System performance on the real test set would be greatly affected by this imbalance as relatively large amounts of simulated data are used in the training phases. By introducing MSC-based time frequency masking into the front-end of the speech enhancement, the WER is reduced in the real test kit by 10.94%. It proves the effectiveness of the MSC-based time frequency masking."}, {"heading": "4.3. Lexicon and language modeling", "text": "Table 2 shows the evaluation results of different language models with the proposed front-end (MSC + PDM) and the acoustic base model (GMM-HMM). N-gram language models are used in the first pass, while the RNNME-based language model is used in the second pass. Results of the experiments show that system performance is consistently improved by introducing a pronunciation lexicon with silence probability. System performance is further improved by introducing the 4-gram language model and the RNNME-based language model."}, {"heading": "4.4. Acoustic modeling", "text": "Table 3 shows the evaluation results of different characteristics and different acoustic models with the proposed front-end (MSC + PDM) and language model (4-gram + SP). It should be noted that the RNNME-based second-pass rescoring is not used in this evaluation. The maxout-deep neural network has 4 hidden layers and each layer has 1000 neurons with a group size of 4. The LSTMP recursive neural network has a single hidden layer with 1000 neurons and 700 projection units. The 40-dimensional fMLLR features are achieved based on the 13-dimensional MFCC features. The fMLR transformation matrix is estimated using the baseline GMM-HMM. The 50-dimensional i-vector functions are extracted at the level. The language model used in sMBR discrimination training is the baseline-3 gram language modeling."}, {"heading": "4.5. Overall comparison", "text": "The results of the overall comparison of the proposed systems and baseline systems: \u2022 Baseline I: the GMM-HMM baseline on noisy data; \u2022 Baseline II: the GMM-HMM baseline on enhanceddata; \u2022 System I: proposed speech enhancement front-end witheline acoustic model (GMM-HMM) and language model (3-gram) on noisy data; \u2022 System II: proposed language model with baseline acoustic model (GMM-HMM) on noisy data; \u2022 System III: proposed acoustic model with baseline language model (3-gram) on noisy data; \u2022 System IV: the best-performed proposed system.Table 6 shows the proposed language with baseline acoustic environment (GMM-HMM) on noisy data; Experimental results from the upper part of Table 5 show the contribution of the proposed model, acoustic and language model separately."}], "references": [{"title": "The PASCAL CHiME speech separation and recognition challenge", "author": ["J. Barker", "E. Vincent", "N. Ma", "H. Christensen", "P. Green"], "venue": "Computer Speech & Language, vol. 27, no. 3, pp. 621\u2013633, May 2013.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2013}, {"title": "The second CHiME speech separation and recognition challenge an overview of challenge systems and outcomes", "author": ["E. Vincent", "J. Barker", "S. Watanabe", "J. Le Roux", "F. Nesta", "M. Matassoni"], "venue": "Proceedings of the 2013 IEEE Workshop on Automatic Speech Recognition and Understanding (ASRU), 2013.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2013}, {"title": "The third \u2018CHiME\u2019 speech separation and recognition challenge: Dataset, task and baselines", "author": ["J. Barker", "R. Marxer", "E. Vincent", "S. Watanabe"], "venue": "Proceedings of the 2015 IEEE Workshop on Automatic Speech Recognition and Understanding (ASRU), 2015.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2015}, {"title": "Investigation of maxout networks for speech recognition", "author": ["P. Swietojanski", "J. Li", "J. Huang"], "venue": "Proceedings of the 2014 ICASSP, 2014, pp. 7649\u20137653.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2014}, {"title": "Long short-term memory recurrent neural network architectures for large scale acoustic modeling", "author": ["H. Sak", "A. Senior", "F. Beaufays"], "venue": "Proceedings of the 2014 INTERSPEECH, 2014, pp. 338\u2013342.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2014}, {"title": "Constructing long short-term memory based deep recurrent neural network for large vocabulary speech recognition", "author": ["X. Li", "X. Wu"], "venue": "Proceedings of the 2015 ICASSP, 2015.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2015}, {"title": "Front-end factor analysis for speaker verification", "author": ["N. Dehak", "P. Kenny", "R. Dehak", "P. Dumouchel", "P. Ouellet"], "venue": "IEEE Transactions on Audio, Speech, and Language Processing, vol. 19, no. 4, pp. 788\u2013798, 2011.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2011}, {"title": "Simplification and optimization of i-vector extraction", "author": ["O. Glembek", "L. Burget", "P. Matejka", "M. Karafi\u00e1t", "P. Kenny"], "venue": "Proceedings of the 2011 ICASSP, 2011, pp. 4516\u20134519.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2011}, {"title": "Speaker adaptation of neural network acoustic models using i-vectors", "author": ["G. Saon", "H. Soltau", "D. Nahamoo", "M. Picheny"], "venue": "Proceedings of the 2013 IEEE Workshop on Automatic Speech Recognition and Understanding (ASRU), 2013, pp. 55\u201359.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2013}, {"title": "Improving dnn speaker independence with i-vector inputs", "author": ["A. Senior", "I. Lopez-Moreno"], "venue": "Proceedings of the 2014 ICASSP, 2014, pp. 225\u2013229.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2014}, {"title": "Sequence-discriminative training of deep neural networks", "author": ["K. Vesel\u00fd", "A. Ghoshal", "L. Burget", "D. Povey"], "venue": "Proceedings of the 2013 INTERSPEECH, 2013.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2013}, {"title": "Sequence discriminative distributed training of long short-term memory recurrent neural networks", "author": ["H. Sak", "O. Vinyals", "G. Heigold", "A. Senior", "E. McDermott", "R. Monga", "M. Mao"], "venue": "Proceedings of the 2014 INTERSPEECH, 2014, pp. 1209\u20131213.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2014}, {"title": "Pronunciation and silence probability modeling for ASR", "author": ["G. Chen", "H. Xu", "M. Wu", "D. Povey", "S. Khudanpur"], "venue": "Proceedings of the 2015 INTERSPEECH, 2015.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2015}, {"title": "Statistical language models based on neural networks, Ph.D", "author": ["T. Mikolov"], "venue": "thesis, Brno University of Technology,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2012}, {"title": "RNNLM - recurrent neural network language modeling toolkit", "author": ["T. Mikolov", "S. Kombrink", "A. Deoras", "L. Burget", "J. \u010cernock\u00fd"], "venue": "Proceedings of the 2011 IEEE Workshop on Automatic Speech Recognition and Understanding (ASRU), 2011, pp. 196\u2013201.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2011}, {"title": "Using the coherence function for noise reduction", "author": ["R. Le Bouquin", "G. Faucon"], "venue": "Communications, Speech and Vision, IEE Proceedings I, vol. 139, no. 3, pp. 276\u2013 280, June 1992.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 1992}, {"title": "Distributed multichannel speech enhancement with minimum meansquare error short-time spectral amplitude, log-spectral amplitude, and spectral phase estimation", "author": ["M.B. Trawicki", "M.T. Johnson"], "venue": "Signal Processing, vol. 92, no. 2, pp. 345\u2013356, Feb. 2012.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2012}, {"title": "Discriminative methods for noise robust speech recognition: A CHiME Challenge Benchmark", "author": ["Y. Tachioka", "S. Watanabe", "J. Le Roux", "J.R. Hershey"], "venue": "The 2nd CHiME Workshop on Machine Listening in Multisource Environments, Vancouver, Canada, June 2013, pp. 19\u2013 24.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2013}, {"title": "Self-calibrating microphone arrays for speech signal acquisition: A systematic approach", "author": ["M. Buck", "T. Haulick", "H. Pfleiderer"], "venue": "Signal Processing, vol. 86, no. 6, pp. 1230\u20131238, June 2006.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2006}, {"title": "Speech recognition with deep recurrent neural networks", "author": ["A. Graves", "A. Mohamed", "G. Hinton"], "venue": "Proceedings of the 2013 ICASSP, 2013, pp. 6645\u20136649.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2013}, {"title": "Connectionist temporal classification: Labelling unsegmented sequence data with recurrent neural networks", "author": ["A. Graves", "S. Fern\u00e1ndez", "F. Gomez", "J. Schmidhuber"], "venue": "Proceedings of the 23rd International Conference on Machine Learning (ICML), 2006, pp. 369\u2013 376.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2006}, {"title": "Robust speech recognition using long short-term memory recurrent neural networks for hybrid acoustic modelling", "author": ["J. Geiger", "Z. Zhang", "F. Weninger", "B. Schuller", "G. Rigoll"], "venue": "Proceedings of the 2014 INTER- SPEECH, 2014, pp. 631\u2013635.", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2014}, {"title": "Feature engineering in context-dependent deep neural networks for conversational speech transcription", "author": ["F. Seide", "G. Li", "X. Chen", "D. Yu"], "venue": "Proceedings of the 2011 IEEE Workshop on Automatic Speech Recognition and Understanding (ASRU), 2011, pp. 24\u201329.", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2011}, {"title": "Automatic speech recognition - A deep learning", "author": ["D. Yu", "L. Deng"], "venue": null, "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2015}, {"title": "Fast speaker adaptation of hybrid nn/hmm model for speech recognition based on discriminative learning of speaker code", "author": ["O. Abdel-Hamid", "H. Jiang"], "venue": "Proceedings of the 2013 ICASSP, 2013, pp. 7942\u20137946.", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2013}, {"title": "The Kaldi speech recognition toolkit", "author": ["D. Povey", "A. Ghoshal", "G. Boulianne", "L. Burget", "O. Glembek", "N. Goel", "M. Hannemann", "P. Motl\u0131\u0301\u010dek", "Y. Qian", "P. Schwarz", "J. Silovsk\u00fd", "G. Stemmer", "K. Vesel\u00fd"], "venue": "Proceedings of the 2011 IEEE Workshop on Automatic Speech Recognition and Understanding (ASRU), 2011.", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2011}, {"title": "On the difficulty of training recurrent neural networks", "author": ["R. Pascanu", "T. Mikolov", "Y. Bengio"], "venue": "CoRR, vol. 1211.5063, 2012.", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2012}], "referenceMentions": [{"referenceID": 0, "context": "The past \u2018CHiME\u2019 challenges have contributed to the development of several speech enhancement techniques, and novel framework that integrate speech enhancement and ASR [1, 2].", "startOffset": 168, "endOffset": 174}, {"referenceID": 1, "context": "The past \u2018CHiME\u2019 challenges have contributed to the development of several speech enhancement techniques, and novel framework that integrate speech enhancement and ASR [1, 2].", "startOffset": 168, "endOffset": 174}, {"referenceID": 2, "context": "The main difference between the current \u2018CHiME\u2019 challenge and the past ones is instead of working on simulated data only, the current challenge focuses on real-world data: multi-channel recording using mobile tablet device in a variety of noisy public environments [3].", "startOffset": 265, "endOffset": 268}, {"referenceID": 3, "context": "For acoustic modeling, neural network acoustic models including maxout neural network [4] and long short term memory with project layers (LSTMP) [5, 6] are adopted.", "startOffset": 86, "endOffset": 89}, {"referenceID": 4, "context": "For acoustic modeling, neural network acoustic models including maxout neural network [4] and long short term memory with project layers (LSTMP) [5, 6] are adopted.", "startOffset": 145, "endOffset": 151}, {"referenceID": 5, "context": "For acoustic modeling, neural network acoustic models including maxout neural network [4] and long short term memory with project layers (LSTMP) [5, 6] are adopted.", "startOffset": 145, "endOffset": 151}, {"referenceID": 6, "context": "Moreover, online extracted i-vector is also used as the network input for encoding these effects: speaker, channel and background noise [7, 8, 9, 10].", "startOffset": 136, "endOffset": 149}, {"referenceID": 7, "context": "Moreover, online extracted i-vector is also used as the network input for encoding these effects: speaker, channel and background noise [7, 8, 9, 10].", "startOffset": 136, "endOffset": 149}, {"referenceID": 8, "context": "Moreover, online extracted i-vector is also used as the network input for encoding these effects: speaker, channel and background noise [7, 8, 9, 10].", "startOffset": 136, "endOffset": 149}, {"referenceID": 9, "context": "Moreover, online extracted i-vector is also used as the network input for encoding these effects: speaker, channel and background noise [7, 8, 9, 10].", "startOffset": 136, "endOffset": 149}, {"referenceID": 10, "context": "State-space Minimum Bayes Risk (sMBR) discriminative training is conducted on the neural networks based acoustic models [11, 12].", "startOffset": 120, "endOffset": 128}, {"referenceID": 11, "context": "State-space Minimum Bayes Risk (sMBR) discriminative training is conducted on the neural networks based acoustic models [11, 12].", "startOffset": 120, "endOffset": 128}, {"referenceID": 12, "context": "For language and lexicon modeling, to model the inter-word silence more precisely, pronunciation lexicon with silence probability is adopted [13].", "startOffset": 141, "endOffset": 145}, {"referenceID": 13, "context": "A language model based on jointly trained recurrent neural network and maximum entropy models (RNNME) is adopted for the second-pass rescoring [14, 15].", "startOffset": 143, "endOffset": 151}, {"referenceID": 14, "context": "A language model based on jointly trained recurrent neural network and maximum entropy models (RNNME) is adopted for the second-pass rescoring [14, 15].", "startOffset": 143, "endOffset": 151}, {"referenceID": 15, "context": "In multi-channel signal processing, the MSC is an efficient mean of noise reduction [16].", "startOffset": 84, "endOffset": 88}, {"referenceID": 16, "context": "In the case of microphone array with sufficiently large microphone spacing, incoherent (diffused) noise would be indicated by small coherence values, while the directional signals would have large coherence values [17, 18].", "startOffset": 214, "endOffset": 222}, {"referenceID": 17, "context": "In the past \u2018CHiME\u2019 challenges, phase difference based timefrequency masking noise suppression front-ends was proved to be effective in the case of dual-channel simulated data [19].", "startOffset": 176, "endOffset": 180}, {"referenceID": 0, "context": "Since the PDM values don\u2019t lies within the range of [0, 1], the PDM based time-frequency masker W \u2032 P (f, t) is obtained by performing a non-linear transformation on the PDM value WP (f, t), and hard clipped to have a maximum value of 1:", "startOffset": 52, "endOffset": 58}, {"referenceID": 18, "context": "In this study, adaptive microphone array self-calibration with recursive configuration is adopted [21].", "startOffset": 98, "endOffset": 102}, {"referenceID": 19, "context": "[22] proposed to use stacked bidirectional LSTM trained with connectionist temporal classification [23] for phoneme recognition.", "startOffset": 0, "endOffset": 4}, {"referenceID": 20, "context": "[22] proposed to use stacked bidirectional LSTM trained with connectionist temporal classification [23] for phoneme recognition.", "startOffset": 99, "endOffset": 103}, {"referenceID": 21, "context": "Subsequently, LSTM RNNs have been successfully applied and shown to give state-of-theart performance on robust speech recognition task [24], and many large vocabulary speech recognition tasks [5, 6].", "startOffset": 135, "endOffset": 139}, {"referenceID": 4, "context": "Subsequently, LSTM RNNs have been successfully applied and shown to give state-of-theart performance on robust speech recognition task [24], and many large vocabulary speech recognition tasks [5, 6].", "startOffset": 192, "endOffset": 198}, {"referenceID": 5, "context": "Subsequently, LSTM RNNs have been successfully applied and shown to give state-of-theart performance on robust speech recognition task [24], and many large vocabulary speech recognition tasks [5, 6].", "startOffset": 192, "endOffset": 198}, {"referenceID": 22, "context": "Efforts have been made to train acoustic models using speakeradapted features, which can be obtained by speaker normalization techniques such as the vocal tract length normalization (VTLN) and fMLLR [25].", "startOffset": 199, "endOffset": 203}, {"referenceID": 23, "context": "Another group of approaches are using speaker-aware training [26], where the speaker information, such as speaker code [27] and speaker- or utterance-level i-vector [9, 10], is provided to the neural networks directly.", "startOffset": 61, "endOffset": 65}, {"referenceID": 24, "context": "Another group of approaches are using speaker-aware training [26], where the speaker information, such as speaker code [27] and speaker- or utterance-level i-vector [9, 10], is provided to the neural networks directly.", "startOffset": 119, "endOffset": 123}, {"referenceID": 8, "context": "Another group of approaches are using speaker-aware training [26], where the speaker information, such as speaker code [27] and speaker- or utterance-level i-vector [9, 10], is provided to the neural networks directly.", "startOffset": 165, "endOffset": 172}, {"referenceID": 9, "context": "Another group of approaches are using speaker-aware training [26], where the speaker information, such as speaker code [27] and speaker- or utterance-level i-vector [9, 10], is provided to the neural networks directly.", "startOffset": 165, "endOffset": 172}, {"referenceID": 3, "context": "Neural network acoustic models including maxout neural network [4] and LSTMP [5, 6] are adopted.", "startOffset": 63, "endOffset": 66}, {"referenceID": 4, "context": "Neural network acoustic models including maxout neural network [4] and LSTMP [5, 6] are adopted.", "startOffset": 77, "endOffset": 83}, {"referenceID": 5, "context": "Neural network acoustic models including maxout neural network [4] and LSTMP [5, 6] are adopted.", "startOffset": 77, "endOffset": 83}, {"referenceID": 10, "context": "sMBR discriminative training is conducted on the neural networks based acoustic models [11, 12].", "startOffset": 87, "endOffset": 95}, {"referenceID": 11, "context": "sMBR discriminative training is conducted on the neural networks based acoustic models [11, 12].", "startOffset": 87, "endOffset": 95}, {"referenceID": 12, "context": "To model the inter-word silence more precisely, pronunciation lexicon with silence probability is adopted [13].", "startOffset": 106, "endOffset": 110}, {"referenceID": 13, "context": "An RNNME based language model [14, 15] is adopted for the second-pass rescoring.", "startOffset": 30, "endOffset": 38}, {"referenceID": 14, "context": "An RNNME based language model [14, 15] is adopted for the second-pass rescoring.", "startOffset": 30, "endOffset": 38}, {"referenceID": 25, "context": "The GMM-HMM system was trained using Kaldi [28].", "startOffset": 43, "endOffset": 47}, {"referenceID": 26, "context": "In the training procedure of LSTM networks, the strategy introduced by [29] was applied to scale down the gradients.", "startOffset": 71, "endOffset": 75}], "year": 2015, "abstractText": "In this paper, the Lingban entry to the third \u2018CHiME\u2019 speech separation and recognition challenge is presented. A timefrequency masking based speech enhancement front-end is proposed to suppress the environmental noise utilizing multichannel coherence and spatial cues. The state-of-the-art speech recognition techniques, namely recurrent neural network based acoustic and language modeling, state space minimum Bayes risk based discriminative acoustic modeling, and i-vector based acoustic condition modeling, are carefully integrated into the speech recognition back-end. To further improve the system performance by fully exploiting the advantages of different technologies, the final recognition results are obtained by lattice combination and rescoring. Evaluations carried out on the official dataset prove the effectiveness of the proposed systems. Comparing with the best baseline result, the proposed system obtains consistent improvements with over 57% relative word error rate reduction on the real-data test set.", "creator": "LaTeX with hyperref package"}}}