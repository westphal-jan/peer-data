{"id": "1509.07308", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "24-Sep-2015", "title": "Bilingual Distributed Word Representations from Document-Aligned Comparable Data", "abstract": "We propose a new model for learning bilingual word representations from non-parallel document-aligned data. Following the recent advances in word representation learning, our model learns dense real-valued word vectors, that is, bilingual word embeddings (BWEs). Unlike prior work on inducing BWEs which heavily relied on parallel sentence-aligned corpora and/or readily available translation resources such as dictionaries, the article reveals that BWEs may be learned solely on the basis of document-aligned comparable data without any additional lexical resources nor syntactic information. We present a comparison of our approach with previous state-of-the-art models for learning bilingual word representations from comparable data that rely on the framework of multilingual probabilistic topic modeling (MuPTM), as well as with distributional local context-counting models. We demonstrate the utility of the induced BWEs in two semantic tasks: (1) bilingual lexicon extraction, (2) suggesting word translations in context for polysemous words. Our simple yet effective BWE-based models significantly outperform the MuPTM-based and context-counting representation models from comparable data as well as prior BWE-based models, and acquire the best reported results on both tasks for all three tested language pairs.", "histories": [["v1", "Thu, 24 Sep 2015 11:00:04 GMT  (391kb,D)", "http://arxiv.org/abs/1509.07308v1", null], ["v2", "Sun, 28 Feb 2016 12:47:15 GMT  (419kb,D)", "http://arxiv.org/abs/1509.07308v2", null]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["ivan vuli\\'c", "marie-francine moens"], "accepted": false, "id": "1509.07308"}, "pdf": {"name": "1509.07308.pdf", "metadata": {"source": "CRF", "title": "Bilingual Distributed Word Representations from Document-Aligned Comparable Data", "authors": ["Ivan Vuli\u0107"], "emails": ["ivan.vulic@cs.kuleuven.be", "marie-francine.moens@cs.kuleuven.be"], "sections": [{"heading": "1. Introduction", "text": "This year, it has reached the point where it will be able to retaliate."}, {"heading": "2. Related Work", "text": "In this section, we explain why we decided to build a model for generating bilingual word embeddings from comparable data. To get a clearer overview, we have divided related work into three large clusters: (1) monolingual word embeddings, (2) bilingual word embeddings, and (3) bilingual word representations from comparable data."}, {"heading": "2.1 Monolingual Word Embeddings", "text": "The idea of representing words as continuous, real-valued vectors dates back to the mid-1980s (Rumelhart, Hinton, & Williams, 1986; Elman, 1990); the idea met its resurgence a decade ago (Bengio et al., 2003), where a neural language model is being tested as part of a neural network architecture for statistical speech modeling; this work inspired other approaches that model word embeddings within the neural network (Collobert & Weston, Bottou, Karlen, Kavukcuoglu, & Kuksa, 2011); word embeddings are tailored to capture and encode semantic similarities, a continuous notion of semantic similarity (as opposed to semantically inferior discrete representations) representations necessary to share information between words and other texts."}, {"heading": "2.2 Bilingual Word Embeddings", "text": "Bilingual word representations could be useful sources of knowledge for interlingual information retrieval problems (Levow, Oard, & Resnik, 2005; Vulic, De Smet, & Moens, 2013), statistical machine translation (Wu, Wang, & Zong, 2008), document classification (Ni, Sun, Hu, & Chen, 2011; Klementiev et al., 2012; Hermann & Blunsom, 2014b; Chandar et al., 2014; Vulic, De Smet, Tang, & Moens, & Moens, 2015), bilingual lexicon extraction (Tamura, Watanabe, & Sumita, 2012; Vulic, & Moens, 2013a), or knowledge transfer and annotation of resource-rich to-resource-poor languages."}, {"heading": "2.3 Bilingual Word Representations from Document-Aligned Data", "text": "Previous work on creating bilingual word representations in the early days followed the tradition of window-based context-counting distribution models (Rapp, 1999; Gaussier, Renders, Matveeva, Goutte, & De \u0301 jean, 2004; Laroche & Langlais, 2010), and a bilingual lexicon was once again needed as a critical resource. To address this problem, the latest work relies on the knowledge-based framework of multilingual probabilistic topic modeling (MuPTM) 1. Theoretically, the work (Hermann & Blunsom, 2014b; Chandar et al., 2014) can also be extended to the same setting of document-oriented data, since these two models were originally based on sentence embeddings calculated as aggregations of their individual word embeddings plus sentence orientations."}, {"heading": "3. Model Architecture", "text": "After describing SGNS and our bilingual BWE model, we offer a comparison of our new model with other basic representations from documentary-oriented comparative data in terms of modeling assumptions, training complexity, and end results. Finally, we give a brief introduction to calculating semantic similarity and constructing composite representations beyond the level of words, two basic concepts needed to use induced BWEs in various semantic tasks, such as bilingual lexicon extraction or suggesting word translations in context."}, {"heading": "3.1 Skip-Gram with Negative Sampling (SGNS)", "text": "The starting point is the loglinear representation of the SGNS or the vol vocabulary. (...) It is not as if it is an objective procedure. (...) It is not as if it is an objective procedure. (...) It is not as if it is an objective procedure. (...) It is not as if it is an objective procedure. (...) It is not as if it is an objective procedure. (...) It is as if it is an objective procedure. (...) It is not as if it is an objective procedure. (...) It is as if it is an objective procedure. (...) It is as if it is an objective procedure. (...) It is as if it is an objective procedure. (...) It is as if it is an objective procedure."}, {"heading": "3.2 Final Model - BWESG: BWE Skip-Gram", "text": "In the next step, we propose a novel method to extend SGNS to work with bilingual document-oriented comparative data. Let's assume that we both have a document-oriented comparable corpus that acts as C = d1, d2,. (dS1, dT1), (dS1, dT1), (dS2, dT2), (dS2, dT2), (dS2, dT2), (d2), (dS2), (dS2, dT2), (dT2), (dS2), (dS2), (dS2), (dS2), (D2), (D2), (D2), (D2), (D2), (D2), (D2), (D2), (D2), (D2), (D2), (D3, D3, D3, D3, (), (D3), (D3, D3, D2, D2, (D3), (D2), (D2), (D2), (D2), (D2), (D2), (D2), (D1), (D2), (D1), (D1), (D1), (D1), (D1), (D1), (D1), (D1), (D1), (D1), (D1), (D1), (D1), (D1), (), (D1), (D1), (), (D1), (D1), (), (D1), (), (), (), (D1), (), (), (D1), (), (), (D1), (D1), (D1), (), (), (), (D1), (), (D1), (), (D1), (), (D1, (), (), (), (D1), (), (), (), ("}, {"heading": "3.3 A Comparison of BWESG with Baseline Representation Models", "text": "In the following three subsections, we quickly navigate through other approaches to text representation that we learn from document-oriented comparison data."}, {"heading": "3.3.1 Basic-MuPTM", "text": "Early approaches (e.g. Dumais, Landauer, & Littman, 1996; Carbonell, Yang, Frederking, Brown, Lee, Frederking, E, Geng, & Yang, 1997) have attempted to mince the current structure of documentary-oriented comparable texts with a monolingual theme model (e.g. LSA or LDA) based on pseudo-bilingual documents with the target document simply attached to its source language counterpart, and then use the discovered topical structure as a common semantic space in which both words and documents from two languages can be represented in a uniform manner. Recent work on multilingual theme modeling (MuPTM) (Mimno et al., 2009; De Smet & Moens, 2009; Vulic, et al., 2011) showed that higher quality word representations can be built when a multilingual theme model such as bilingual LDA (BiLDA) is formed."}, {"heading": "3.3.2 Association-MuPTM", "text": "Another basic representation is also based on the MuPTM model, but contains association values P (wa | w) for each W, wa \u00b2 V \u00b2 V \u00b2 V T (Vulic \u00b2 & Moens, 2013a) as dimensions of real word vectors, which are calculated as P (wa | w) = \u2211 K = 1 P (wa | zk) P (zk | w) (Griffiths, Steyvers, & Tenenbaum, 2007), and the word representation is a (| V S | + | V T |) -dimensional vector: ~ w = [P (wS1 | w),.., P (wS | V | w), P (w T 1 | w),.., P (wT | V | T | w) -dimensional vector: ~ w = [P (wS1 | w),."}, {"heading": "3.3.3 Traditional-PPMI", "text": "Finally, a traditional approach to building bilingual word representations in (cross-border) distributional semantics is the calculation of weighted co-occurrence values (e.g. with PMI, TFIDF) between scripts and their context words in a window of predefined size plus an external bilingual lexicon to align context words / dimensions across a group of semantic tasks (Gaussier et al., 2004; Laroche & Langlais, 2010).A weighting function (WeF) that is a standard selection in distributional semantics and provides optimal results across a group of semantic tasks (Bullinaria & Levy, 2007) is the smoothed positive punch line reciprocal information statistics (Pantel & Lin, 2002; Turney & Pantel, 2010).In addition, the contextual c models to induce context words are easily available without the need of a lexicon, which we deal with in 2011 (Peiotboksman)."}, {"heading": "3.4 From Word Representations to Semantic Word Similarity", "text": "Now let us suppose that we have caused bilingual word representations, regardless of the RM types chosen. (wi, wj) Given two words wi and wj, regardless of their actual language, we can calculate the degree of their semantic similarity by applying a similarity function (SF) to their vector representations \u2212 wi and \u2212 wj: sim (wi, wj) = SF (\u2212 wi, \u2212 wj). Different choices (or rather families of) SFs are cosmic, the Kullback Leibler or Jensen-Shannon divergence, the Hellinger distance, the Jaccard distance, etc. (Lee, 1999; Cha, 2007) and different RMs typically require to achieve optimal or near-optimal results across different semantic tasks. If we work with word embedding, a default selection for SF is cosmic similarity (cos) (Mikolov et al., modal., 2013), which is also a typical choice in traditional distribution."}, {"heading": "3.5 Context Sensitive Models of (Cross-Lingual) Semantic Similarity", "text": "In practice, this means that the coach of his team was not satisfied with yesterday's game. \"These context-sensitive models of cross-lingual semantic similarities are unable to recognize that the Spanish word\" entrenador \"is more similar in the context of this sentence than the Spanish word\" autocar, \"although this word is listed as the most semantically similar word to the subject of\" coach. \"In another example, while the Spanish words\" partido, \"\" cerilla \"or\" correspondencia \"resemble another ambiguous English word\" match, \"when observed in isolation, it is unable to find a match."}, {"heading": "4. Training Setup", "text": "This year it has come to the point where it will be able to retaliate, \"he said.\" We've never lost so much time as this year, \"he said.\" We've never lost so much time, \"he said.\" We've never lost so much time, \"he said."}, {"heading": "5. Evaluation Task I: Bilingual Lexicon Extraction", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "5.1 Task Description", "text": "You can use the context-independent CLSS models from paragraph 3.4 to automatically extract bilingual lexicographies from data. By harvesting cross-language neighboring languages, you are able to create a bilingual lexicography of one-to-one translation pairs (wSi, w T j). We test the validity of our BWEs and basic representations in the BLE task."}, {"heading": "5.2 Experimental Setup", "text": "Test data. For each language pair, we rate one-to-one translation pairs built for the three language pairs (ES / IT / NL-EN) of Vulic \u0301 and Moens (2013a, 2013b) on standard 1,000 soil truth. Translation direction is ES / IT / NL \u2192 EN. Test data is available online. 8Evaluation Metrics. Since we can build a one-to-one bilingual lexicon by collecting one-to-one translation pairs, the lexicon qualification is best reflected in the Acc1 rating, that is, the number of source languages (ES / IT / NL) words wSi of soil-truth translation pairs for which the best rated word is the correct translation into the other language (EN) according to the basic truth about the total number of basic truth translation pairs (= 1000) (Gaussier et al., 2004; Tamura et al.)."}, {"heading": "5.3 Results and Discussion", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "5.3.1 Experiment 0: Qualitative Analysis and Comparison", "text": "The BWESG + cos model is capable of finding semantically coherent word lists for all three directions of similarity (i.e., monolingual, multilingual, multilingual).In the combined (multilingual) ranking, words from both languages are presented as top-like words. This initial qualitative analysis already demonstrates the ability of BWESG to induce a common bilingual embedding space by using only alignments as bilingual signals. In another brief analysis, we compare the bilingual rankings created by the BWESG + model with the other three foundations of CLSS / BLE models."}, {"heading": "5.3.2 Experiment I: BWESG vs Baseline Representation Models", "text": "Although both BWESG strategies show results that exceed the fundamentals of the strategy, a solid strategy of three distinct BWESG strategies and a BWESG model that does not translate pseudo-bilingual documents is to be expected. Results clearly demonstrate the superior performance of the BLE model, which relies on our new framework to induce bilingual word beds from a documentary point of view, we add up the key data on other BLE models based on previously used bilingual word representations. The increase in Acc1 scores over the best scoring baseline models is 22.2% for ES-EN, 7% for IT-EN and 67.5% for NL-EN.BWESG shuffling strategies."}, {"heading": "5.3.3 Experiment II: BWESG vs. Other BWE Induction Models", "text": "In fact, it is that we are able to assert ourselves, that we are able, that we are able, to hide ourselves, and that we are able, to hide ourselves, \"he said.\" We have to be able to hide ourselves, \"he said."}, {"heading": "6. Evaluation Task II: Suggesting Word Translations in Context", "text": "In another task, we will test the ability of BWEs to create context-sensitive semantic similarity models (see paragraph 3.5), which can in turn be used to solve the task of proposing word translations in context (SWTC) recently proposed (Vulic \u0301 & Moens, 2014).We will show that our new BWESG-based SWTC models outperform the best SWTC models (Vulic \u0301 & Moens, 2014).http: / / www.sarathchandar.in / crl.html 11. Suggestions for parameter values obtained through personal correspondence with the authors are available online: https: / / github.com / gouwsmeister / bilbowa.at."}, {"heading": "6.1 Task Description", "text": "In view of the occurrence of a polysemic word wi \u0441V S and the context of this occurrence, the SWTC task is to select the correct translation in the target language LT of this particular occurrence of wi from the given sentence T C (wi) = {t1,..., ttq}, T C (wi) V T, its possible tq translations / meanings. We can designate T C (wi) as an inventory of translation candidates for wi. The task of proposing word translations in context (SWTC) can be interpreted in such a way that the tq translation candidates are ranked in relation to the observed local context Con (wi) of occurrence of the word wi. The best translation candidate for the evaluation according to the scores sim (wi, tj, Con (wi)) (see Section 3.5) in the ranking list is then the correct translation for the respective occurrence of wi taking into account its local context (wi)."}, {"heading": "6.2 Experimental Setup", "text": "The test set consists of 15 ambiguous nouns in three languages (ES, IT and NL) together with sets of their translation candidates (i.e., sets T C), resulting in 360 test sets for each language pair (and 1080 test sets in total), and an additional set of 100 IT sentences (5 more ambiguous IT models plus 20 sentences for each noun) is used as a development set to optimize the parameters (see Section 3.5) for all language pairs and all models in comparison. In summary, the final goal can be formulated as follows: For each ambiguous word wi in ES / IT / NL, the correct translation is proposed."}, {"heading": "6.3 Results and Discussion", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "6.3.1 Experiment I: BWESG vs Baseline Representation Models", "text": "This year it is as far as never before in the history of the Federal Republic of Germany."}, {"heading": "6.3.2 Experiment II: BWESG vs. Other BWE Induction Models", "text": "The results of the BWE induction model by Mikolov et al. (2013b) are summarized in Tab. 8, while the results with the other two models (Chandar et al., 2014; Gouws et al., 2015) in Tab. 9.BWESG surpass other BWE induction models in the SWTC task and further confirm their usefulness in cross-language semantic modeling. Mikolov et al. (2013b) \"s model represents a stronger baseline: Good results in the SWTC task with this model are inherently an interesting finding. While the model is not competitive with complex spatial models from BTC base models, which form less complex data pairs from BLE base models when the BLE algorithms form a stronger basis: Good results in the SWTC task with this model are inherently an interesting finding."}, {"heading": "6.3.3 Further Discussion", "text": "By analyzing the impact of pre-training shuffling on the results of two different evaluation tasks, we can safely determine its usefulness when introducing bilingual word embedding with the BWESG model. While we have already presented two blending strategies in this paper, one line of future work will explore various ways to mix words from two different vocabularies into pseudo-bilingual documents in a more structured and systematic manner (Hill & Korhonen, 2014), however, it is not easy to extend this approach to the generation of pseudo-bilingual education documents. Another idea in the same vein is to create higher-quality artificial training data based on noisy comparable data by computing similar words monolingually and cross-linguistically from the noisy data."}, {"heading": "7. Conclusions and Future Work", "text": "This year is the highest in the history of the country."}], "references": [{"title": "SemEval-2014 task 10: Multilingual semantic textual similarity", "author": ["E. Agirre", "C. Banea", "C. Cardie", "D. Cer", "M. Diab", "A. Gonzalez-Agirre", "W. Guo", "R. Mihalcea", "G. Rigau", "J. Wiebe"], "venue": "In Proceedings of the 8th International Workshop on Semantic Evaluation (SEMEVAL),", "citeRegEx": "Agirre et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Agirre et al\\.", "year": 2014}, {"title": "Polyglot: Distributed word representations for multilingual NLP", "author": ["R. Al-Rfou", "B. Perozzi", "S. Skiena"], "venue": "In Proceedings of the Seventeenth Conference on Computational Natural Language Learning (CoNLL),", "citeRegEx": "Al.Rfou et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Al.Rfou et al\\.", "year": 2013}, {"title": "Don\u2019t count, predict! A systematic comparison of context-counting vs. context-predicting semantic vectors", "author": ["M. Baroni", "G. Dinu", "G. Kruszewski"], "venue": "In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (ACL),", "citeRegEx": "Baroni et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Baroni et al\\.", "year": 2014}, {"title": "Nouns are vectors, adjectives are matrices: Representing adjective-noun constructions in semantic space", "author": ["M. Baroni", "R. Zamparelli"], "venue": "In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing (EMNLP),", "citeRegEx": "Baroni and Zamparelli,? \\Q2010\\E", "shortCiteRegEx": "Baroni and Zamparelli", "year": 2010}, {"title": "A neural probabilistic language model", "author": ["Y. Bengio", "R. Ducharme", "P. Vincent", "C. Janvin"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Bengio et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Bengio et al\\.", "year": 2003}, {"title": "A comparison of vector-based representations for semantic composition", "author": ["W. Blacoe", "M. Lapata"], "venue": "In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP-CoNLL),", "citeRegEx": "Blacoe and Lapata,? \\Q2012\\E", "shortCiteRegEx": "Blacoe and Lapata", "year": 2012}, {"title": "Latent Dirichlet Allocation", "author": ["D.M. Blei", "A.Y. Ng", "M.I. Jordan"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Blei et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Blei et al\\.", "year": 2003}, {"title": "Multilingual topic models for unaligned text", "author": ["J. Boyd-Graber", "D.M. Blei"], "venue": "In Proceedings of the 25th Conference on Uncertainty in Artificial Intelligence (UAI),", "citeRegEx": "Boyd.Graber and Blei,? \\Q2009\\E", "shortCiteRegEx": "Boyd.Graber and Blei", "year": 2009}, {"title": "Extracting semantic representations from word co-occurrence statistics: A computational study", "author": ["J.A. Bullinaria", "J.P. Levy"], "venue": "Behavior Research Methods,", "citeRegEx": "Bullinaria and Levy,? \\Q2007\\E", "shortCiteRegEx": "Bullinaria and Levy", "year": 2007}, {"title": "A framework for the construction of monolingual and cross-lingual word similarity datasets", "author": ["J. Camacho-Collados", "M.T. Pilehvar", "R. Navigli"], "venue": null, "citeRegEx": "Camacho.Collados et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Camacho.Collados et al\\.", "year": 2015}, {"title": "Translingual information retrieval: A comparative evaluation", "author": ["J.G. Carbonell", "J.G. Yang", "R.E. Frederking", "R.D. Brown", "Y. Geng", "D. Lee", "Y. Frederking", "R. E", "R.D. Geng", "Y. Yang"], "venue": "In Proceedings of the 15th International Joint Conference on Artificial Intelligence (IJCAI),", "citeRegEx": "Carbonell et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Carbonell et al\\.", "year": 1997}, {"title": "Comprehensive survey on distance/similarity measures between probability density functions", "author": ["Cha", "S.-H."], "venue": "International Journal of Mathematical Models and Methods in Applied Sciences, 1 (4), 300\u2013307.", "citeRegEx": "Cha and S..H.,? 2007", "shortCiteRegEx": "Cha and S..H.", "year": 2007}, {"title": "An autoencoder approach to learning bilingual word representations", "author": ["S. Chandar", "S. Lauly", "H. Larochelle", "M.M. Khapra", "B. Ravindran", "V.C. Raykar", "A. Saha"], "venue": "In Proceedings of the 27th Annual Conference on Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "Chandar et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Chandar et al\\.", "year": 2014}, {"title": "A context-theoretic framework for compositionality in distributional semantics", "author": ["D. Clarke"], "venue": "Computational Linguistics, 38 (1), 41\u201371.", "citeRegEx": "Clarke,? 2012", "shortCiteRegEx": "Clarke", "year": 2012}, {"title": "A unified architecture for natural language processing: Deep neural networks with multitask learning", "author": ["R. Collobert", "J. Weston"], "venue": "In Proceedings of the 25th International Conference on Machine Learning (ICML),", "citeRegEx": "Collobert and Weston,? \\Q2008\\E", "shortCiteRegEx": "Collobert and Weston", "year": 2008}, {"title": "Natural language processing (almost) from scratch", "author": ["R. Collobert", "J. Weston", "L. Bottou", "M. Karlen", "K. Kavukcuoglu", "P.P. Kuksa"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Collobert et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Collobert et al\\.", "year": 2011}, {"title": "Unsupervised part-of-speech tagging with bilingual graphbased projections", "author": ["D. Das", "S. Petrov"], "venue": "In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies (ACL-HLT),", "citeRegEx": "Das and Petrov,? \\Q2011\\E", "shortCiteRegEx": "Das and Petrov", "year": 2011}, {"title": "Domain adaptation for machine translation by mining unseen words", "author": ["H. Daum\u00e9 III", "J. Jagarlamudi"], "venue": "In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies (ACL-HLT),", "citeRegEx": "III and Jagarlamudi,? \\Q2011\\E", "shortCiteRegEx": "III and Jagarlamudi", "year": 2011}, {"title": "Cross-language linking of news stories on the Web using interlingual topic modeling", "author": ["W. De Smet", "Moens", "M.-F"], "venue": "In Proceedings of the CIKM 2009 Workshop on Social Web Search and Mining (SWSM@CIKM),", "citeRegEx": "Smet et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Smet et al\\.", "year": 2009}, {"title": "The latent words language model", "author": ["K. Deschacht", "J. De Belder", "Moens", "M.-F"], "venue": "Computer Speech & Language,", "citeRegEx": "Deschacht et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Deschacht et al\\.", "year": 2012}, {"title": "Semi-supervised semantic role labeling using the latent words language model", "author": ["K. Deschacht", "Moens", "M.-F"], "venue": "In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing (EMNLP),", "citeRegEx": "Deschacht et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Deschacht et al\\.", "year": 2009}, {"title": "Measuring distributional similarity in context", "author": ["G. Dinu", "M. Lapata"], "venue": "In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing (EMNLP),", "citeRegEx": "Dinu and Lapata,? \\Q2010\\E", "shortCiteRegEx": "Dinu and Lapata", "year": 2010}, {"title": "Improving zero-shot learning by mitigating the hubness problem", "author": ["G. Dinu", "A. Lazaridou", "M. Baroni"], "venue": "In ICLR Workshop Papers", "citeRegEx": "Dinu et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Dinu et al\\.", "year": 2015}, {"title": "Adaptive subgradient methods for online learning and stochastic optimization", "author": ["J.C. Duchi", "E. Hazan", "Y. Singer"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Duchi et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Duchi et al\\.", "year": 2011}, {"title": "Automatic cross-linguistic information retrieval using Latent Semantic Indexing", "author": ["S.T. Dumais", "T.K. Landauer", "M. Littman"], "venue": "In Proceedings of the SIGIR Workshop on Cross-Linguistic Information Retrieval,", "citeRegEx": "Dumais et al\\.,? \\Q1996\\E", "shortCiteRegEx": "Dumais et al\\.", "year": 1996}, {"title": "Finding structure in time", "author": ["J.L. Elman"], "venue": "Cognitive Science, 14, 179\u2013211.", "citeRegEx": "Elman,? 1990", "shortCiteRegEx": "Elman", "year": 1990}, {"title": "Improving vector space word representations using multilingual correlation", "author": ["M. Faruqui", "C. Dyer"], "venue": "In Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics (EACL),", "citeRegEx": "Faruqui and Dyer,? \\Q2014\\E", "shortCiteRegEx": "Faruqui and Dyer", "year": 2014}, {"title": "Symmetric correspondence topic models for multilingual text analysis", "author": ["K. Fukumasu", "K. Eguchi", "E.P. Xing"], "venue": "In Proceedings of the 25th Annual Conference on Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "Fukumasu et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Fukumasu et al\\.", "year": 2012}, {"title": "Cross-lingual discriminative learning of sequence models with posterior regularization", "author": ["K. Ganchev", "D. Das"], "venue": "In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing (EMNLP),", "citeRegEx": "Ganchev and Das,? \\Q2013\\E", "shortCiteRegEx": "Ganchev and Das", "year": 2013}, {"title": "A geometric view on bilingual lexicon extraction from comparable corpora", "author": ["\u00c9. Gaussier", "Renders", "J.-M", "I. Matveeva", "C. Goutte", "H. D\u00e9jean"], "venue": "In Proceedings of the 42nd Annual Meeting of the Association for Computational Linguistics (ACL),", "citeRegEx": "Gaussier et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Gaussier et al\\.", "year": 2004}, {"title": "Stochastic relaxation, Gibbs distributions, and the Bayesian restoration of images", "author": ["S. Geman", "D. Geman"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence,", "citeRegEx": "Geman and Geman,? \\Q1984\\E", "shortCiteRegEx": "Geman and Geman", "year": 1984}, {"title": "Word2vec explained: Deriving Mikolov et al.\u2019s negativesampling word-embedding method. CoRR, abs/1402.3722", "author": ["Y. Goldberg", "O. Levy"], "venue": null, "citeRegEx": "Goldberg and Levy,? \\Q2014\\E", "shortCiteRegEx": "Goldberg and Levy", "year": 2014}, {"title": "BilBOWA: Fast bilingual distributed representations without word alignments", "author": ["S. Gouws", "Y. Bengio", "G. Corrado"], "venue": "In ICML,", "citeRegEx": "Gouws et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Gouws et al\\.", "year": 2015}, {"title": "Simple task-specific bilingual word embeddings", "author": ["S. Gouws", "A. S\u00f8gaard"], "venue": "In Proceedings of the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL-HLT),", "citeRegEx": "Gouws and S\u00f8gaard,? \\Q2015\\E", "shortCiteRegEx": "Gouws and S\u00f8gaard", "year": 2015}, {"title": "Learning bilingual lexicons from monolingual corpora", "author": ["A. Haghighi", "P. Liang", "T. Berg-Kirkpatrick", "D. Klein"], "venue": "In Proceedings of the 46th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies (ACL-HLT),", "citeRegEx": "Haghighi et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Haghighi et al\\.", "year": 2008}, {"title": "Distributional structure", "author": ["Z.S. Harris"], "venue": "Word, 10 (23), 146\u2013162.", "citeRegEx": "Harris,? 1954", "shortCiteRegEx": "Harris", "year": 1954}, {"title": "The role of syntax in vector space models of compositional semantics", "author": ["K.M. Hermann", "P. Blunsom"], "venue": "In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (ACL),", "citeRegEx": "Hermann and Blunsom,? \\Q2013\\E", "shortCiteRegEx": "Hermann and Blunsom", "year": 2013}, {"title": "Multilingual Distributed Representations without Word Alignment", "author": ["K.M. Hermann", "P. Blunsom"], "venue": "In Proceedings of the 2014 International Conference on Learning Representations (ICLR)", "citeRegEx": "Hermann and Blunsom,? \\Q2014\\E", "shortCiteRegEx": "Hermann and Blunsom", "year": 2014}, {"title": "Multilingual models for compositional distributed semantics", "author": ["K.M. Hermann", "P. Blunsom"], "venue": "In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (ACL),", "citeRegEx": "Hermann and Blunsom,? \\Q2014\\E", "shortCiteRegEx": "Hermann and Blunsom", "year": 2014}, {"title": "Learning abstract concept embeddings from multi-modal data: Since you probably can\u2019t see what I mean", "author": ["F. Hill", "A. Korhonen"], "venue": "In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP),", "citeRegEx": "Hill and Korhonen,? \\Q2014\\E", "shortCiteRegEx": "Hill and Korhonen", "year": 2014}, {"title": "A convolutional neural network for modelling sentences", "author": ["N. Kalchbrenner", "E. Grefenstette", "P. Blunsom"], "venue": "In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (ACL),", "citeRegEx": "Kalchbrenner et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Kalchbrenner et al\\.", "year": 2014}, {"title": "A Bayesian method for robust estimation of distributional similarities", "author": ["J. Kazama", "S.D. Saeger", "K. Kuroda", "M. Murata", "K. Torisawa"], "venue": "In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics (ACL),", "citeRegEx": "Kazama et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Kazama et al\\.", "year": 2010}, {"title": "Learning image embeddings using convolutional neural networks for improved multi-modal semantics", "author": ["D. Kiela", "L. Bottou"], "venue": "In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP),", "citeRegEx": "Kiela and Bottou,? \\Q2014\\E", "shortCiteRegEx": "Kiela and Bottou", "year": 2014}, {"title": "A systematic study of semantic vector space model parameters", "author": ["D. Kiela", "S. Clark"], "venue": "In Proceedings of the 2nd Workshop on Continuous Vector Space Models and their Compositionality (CVSC),", "citeRegEx": "Kiela and Clark,? \\Q2014\\E", "shortCiteRegEx": "Kiela and Clark", "year": 2014}, {"title": "Inducing crosslingual distributed representations of words", "author": ["A. Klementiev", "I. Titov", "B. Bhattarai"], "venue": "In Proceedings of the 24th International Conference on Computational Linguistics (COLING),", "citeRegEx": "Klementiev et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Klementiev et al\\.", "year": 2012}, {"title": "Learning a translation lexicon from monolingual corpora", "author": ["P. Koehn", "K. Knight"], "venue": "In Proceedings of the ACL Workshop on Unsupervised Lexical Acquisition (ULA),", "citeRegEx": "Koehn and Knight,? \\Q2002\\E", "shortCiteRegEx": "Koehn and Knight", "year": 2002}, {"title": "Learning bilingual word representations by marginalizing alignments", "author": ["T. Ko\u010disk\u00fd", "K.M. Hermann", "P. Blunsom"], "venue": "In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (ACL),", "citeRegEx": "Ko\u010disk\u00fd et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Ko\u010disk\u00fd et al\\.", "year": 2014}, {"title": "Solutions to Plato\u2019s problem: The Latent Semantic Analysis theory of acquisition, induction, and representation of knowledge", "author": ["T.K. Landauer", "S.T. Dumais"], "venue": "Psychological Review,", "citeRegEx": "Landauer and Dumais,? \\Q1997\\E", "shortCiteRegEx": "Landauer and Dumais", "year": 1997}, {"title": "Revisiting context-based projection methods for termtranslation spotting in comparable corpora", "author": ["A. Laroche", "P. Langlais"], "venue": "In Proceedings of the 23rd International Conference on Computational Linguistics (COLING),", "citeRegEx": "Laroche and Langlais,? \\Q2010\\E", "shortCiteRegEx": "Laroche and Langlais", "year": 2010}, {"title": "Distributed representations of sentences and documents", "author": ["Q.V. Le", "T. Mikolov"], "venue": "In Proceedings of the 31th International Conference on Machine Learning (ICML),", "citeRegEx": "Le and Mikolov,? \\Q2014\\E", "shortCiteRegEx": "Le and Mikolov", "year": 2014}, {"title": "Word embeddings through Hellinger PCA", "author": ["R. Lebret", "R. Collobert"], "venue": "In Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics (EACL),", "citeRegEx": "Lebret and Collobert,? \\Q2014\\E", "shortCiteRegEx": "Lebret and Collobert", "year": 2014}, {"title": "Measures of distributional similarity", "author": ["L. Lee"], "venue": "Proceedings of the 37th Annual Meeting of the Association for Computational Linguistics (ACL), pp. 25\u201332.", "citeRegEx": "Lee,? 1999", "shortCiteRegEx": "Lee", "year": 1999}, {"title": "Dictionary-based techniques for crosslanguage information retrieval", "author": ["Levow", "G.-A", "D.W. Oard", "P. Resnik"], "venue": "Information Processing and Management,", "citeRegEx": "Levow et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Levow et al\\.", "year": 2005}, {"title": "Dependency-based word embeddings", "author": ["O. Levy", "Y. Goldberg"], "venue": "In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (ACL),", "citeRegEx": "Levy and Goldberg,? \\Q2014\\E", "shortCiteRegEx": "Levy and Goldberg", "year": 2014}, {"title": "Neural word embedding as implicit matrix factorization", "author": ["O. Levy", "Y. Goldberg"], "venue": "In Proceedings of the 27th Annual Conference on Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "Levy and Goldberg,? \\Q2014\\E", "shortCiteRegEx": "Levy and Goldberg", "year": 2014}, {"title": "Improving distributional similarity with lessons learned from word embeddings", "author": ["O. Levy", "Y. Goldberg", "I. Dagan"], "venue": "Transactions of the ACL,", "citeRegEx": "Levy et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Levy et al\\.", "year": 2015}, {"title": "Improving corpus comparability for bilingual lexicon extraction from comparable corpora", "author": ["B. Li", "\u00c9. Gaussier"], "venue": "In Proceedings of the 23rd International Conference on Computational Linguistics (COLING),", "citeRegEx": "Li and Gaussier,? \\Q2010\\E", "shortCiteRegEx": "Li and Gaussier", "year": 2010}, {"title": "Clustering comparable corpora for bilingual lexicon extraction", "author": ["B. Li", "\u00c9. Gaussier", "A. Aizawa"], "venue": "In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies (ACL-HLT),", "citeRegEx": "Li et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Li et al\\.", "year": 2011}, {"title": "Learning semantic word embeddings based on ordinal knowledge constraints", "author": ["Q. Liu", "H. Jiang", "S. Wei", "Ling", "Z.-H", "Y. Hu"], "venue": "In Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (ACL-IJCNLP),", "citeRegEx": "Liu et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Liu et al\\.", "year": 2015}, {"title": "Topic models + word alignment = a flexible framework for extracting bilingual dictionary from comparable corpus", "author": ["X. Liu", "K. Duh", "Y. Matsumoto"], "venue": "In Proceedings of the 17th Conference on Computational Natural Language Learning (CoNLL),", "citeRegEx": "Liu et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Liu et al\\.", "year": 2013}, {"title": "Deep multilingual correlation for improved word embeddings", "author": ["A. Lu", "W. Wang", "M. Bansal", "K. Gimpel", "K. Livescu"], "venue": "In Proceedings of the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL-HLT),", "citeRegEx": "Lu et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Lu et al\\.", "year": 2015}, {"title": "A simple word embedding model for lexical substitution", "author": ["O. Melamud", "O. Levy", "I. Dagan"], "venue": "In Proceedings of the 1st Workshop on Vector Space Modeling for Natural Language Processing,", "citeRegEx": "Melamud et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Melamud et al\\.", "year": 2015}, {"title": "Efficient estimation of word representations in vector space", "author": ["T. Mikolov", "K. Chen", "G.S. Corrado", "J. Dean"], "venue": "In ICLR Workshop Papers", "citeRegEx": "Mikolov et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Exploiting similarities among languages for machine translation. CoRR, abs/1309.4168", "author": ["T. Mikolov", "Q.V. Le", "I. Sutskever"], "venue": null, "citeRegEx": "Mikolov et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Distributed representations of words and phrases and their compositionality", "author": ["T. Mikolov", "I. Sutskever", "K. Chen", "G.S. Corrado", "J. Dean"], "venue": null, "citeRegEx": "Mikolov et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Linguistic regularities in continuous space word representations", "author": ["T. Mikolov", "W. Yih", "G. Zweig"], "venue": "In Proceedings of the 14th Meeting of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL-HLT),", "citeRegEx": "Mikolov et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Evaluating neural word representations in tensor-based compositional settings", "author": ["D. Milajevs", "D. Kartsaklis", "M. Sadrzadeh", "M. Purver"], "venue": "In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP),", "citeRegEx": "Milajevs et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Milajevs et al\\.", "year": 2014}, {"title": "Polylingual topic models", "author": ["D. Mimno", "H. Wallach", "J. Naradowsky", "D.A. Smith", "A. McCallum"], "venue": "In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing (EMNLP),", "citeRegEx": "Mimno et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Mimno et al\\.", "year": 2009}, {"title": "Vector-based models of semantic composition", "author": ["J. Mitchell", "M. Lapata"], "venue": "In Proceedings of the 46th Annual Meeting of the Association for Computational Linguistics (ACL),", "citeRegEx": "Mitchell and Lapata,? \\Q2008\\E", "shortCiteRegEx": "Mitchell and Lapata", "year": 2008}, {"title": "Learning word embeddings efficiently with noisecontrastive estimation", "author": ["A. Mnih", "K. Kavukcuoglu"], "venue": "In Proceedings of the 27th Annual Conference on Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "Mnih and Kavukcuoglu,? \\Q2013\\E", "shortCiteRegEx": "Mnih and Kavukcuoglu", "year": 2013}, {"title": "Mining multilingual topics from Wikipedia", "author": ["X. Ni", "Sun", "J.-T", "J. Hu", "Z. Chen"], "venue": "In Proceedings of the 18th International World Wide Web Conference (WWW),", "citeRegEx": "Ni et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Ni et al\\.", "year": 2009}, {"title": "Cross lingual text classification by mining multilingual topics from Wikipedia", "author": ["X. Ni", "Sun", "J.-T", "J. Hu", "Z. Chen"], "venue": "In Proceedings of the 4th International Conference on Web Search and Web Data Mining (WSDM),", "citeRegEx": "Ni et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Ni et al\\.", "year": 2011}, {"title": "Cross-lingual annotation projection for semantic roles", "author": ["S. Pad\u00f3", "M. Lapata"], "venue": "Journal of Artificial Intelligence Research,", "citeRegEx": "Pad\u00f3 and Lapata,? \\Q2009\\E", "shortCiteRegEx": "Pad\u00f3 and Lapata", "year": 2009}, {"title": "Discovering word senses from text", "author": ["P. Pantel", "D. Lin"], "venue": "In Proceedings of the 8th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (KDD),", "citeRegEx": "Pantel and Lin,? \\Q2002\\E", "shortCiteRegEx": "Pantel and Lin", "year": 2002}, {"title": "Cross-lingual induction of selectional preferences with bilingual vector spaces", "author": ["Y. Peirsman", "S. Pad\u00f3"], "venue": "In Proceedings of the 11th Meeting of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL-HLT),", "citeRegEx": "Peirsman and Pad\u00f3,? \\Q2010\\E", "shortCiteRegEx": "Peirsman and Pad\u00f3", "year": 2010}, {"title": "Semantic relations in bilingual lexicons", "author": ["Y. Peirsman", "S. Pad\u00f3"], "venue": "ACM Transactions on Speech and Language Processing,", "citeRegEx": "Peirsman and Pad\u00f3,? \\Q2011\\E", "shortCiteRegEx": "Peirsman and Pad\u00f3", "year": 2011}, {"title": "Glove: Global vectors for word representation", "author": ["J. Pennington", "R. Socher", "C. Manning"], "venue": "In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP),", "citeRegEx": "Pennington et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Pennington et al\\.", "year": 2014}, {"title": "A User\u2019s Guide to Measure Theoretic Probability", "author": ["D. Pollard"], "venue": "Cambridge University Press.", "citeRegEx": "Pollard,? 2001", "shortCiteRegEx": "Pollard", "year": 2001}, {"title": "Rare word translation extraction from aligned comparable documents", "author": ["E. Prochasson", "P. Fung"], "venue": "In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies (ACL-HLT),", "citeRegEx": "Prochasson and Fung,? \\Q2011\\E", "shortCiteRegEx": "Prochasson and Fung", "year": 2011}, {"title": "Automatic identification of word translations from unrelated English and German corpora", "author": ["R. Rapp"], "venue": "Proceedings of the 37th Annual Meeting of the Association for Computational Linguistics (ACL), pp. 519\u2013526.", "citeRegEx": "Rapp,? 1999", "shortCiteRegEx": "Rapp", "year": 1999}, {"title": "A mixture model with sharing for lexical semantics", "author": ["J. Reisinger", "R.J. Mooney"], "venue": "In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing (EMNLP),", "citeRegEx": "Reisinger and Mooney,? \\Q2010\\E", "shortCiteRegEx": "Reisinger and Mooney", "year": 2010}, {"title": "Compositional matrix-space models of language", "author": ["S. Rudolph", "E. Giesbrecht"], "venue": "In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics (ACL),", "citeRegEx": "Rudolph and Giesbrecht,? \\Q2010\\E", "shortCiteRegEx": "Rudolph and Giesbrecht", "year": 2010}, {"title": "Learning representations by back-propagating errors", "author": ["D.E. Rumelhart", "G.E. Hinton", "R.J. Williams"], "venue": null, "citeRegEx": "Rumelhart et al\\.,? \\Q1986\\E", "shortCiteRegEx": "Rumelhart et al\\.", "year": 1986}, {"title": "Probabilistic part-of-speech tagging using decision trees", "author": ["H. Schmid"], "venue": "Proceedings of the International Conference on New Methods in Language Processing.", "citeRegEx": "Schmid,? 1994", "shortCiteRegEx": "Schmid", "year": 1994}, {"title": "Learning cross-lingual word embeddings via matrix co-factorization", "author": ["T. Shi", "Z. Liu", "Y. Liu", "M. Sun"], "venue": "In Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (ACL-IJCNLP),", "citeRegEx": "Shi et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Shi et al\\.", "year": 2015}, {"title": "Semantic compositionality through recursive matrix-vector spaces", "author": ["R. Socher", "B. Huval", "C.D. Manning", "A.Y. Ng"], "venue": "In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP-CoNLL),", "citeRegEx": "Socher et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Socher et al\\.", "year": 2012}, {"title": "Leveraging monolingual data for crosslingual compositional word representations", "author": ["H. Soyer", "P. Stenetorp", "A. Aizawa"], "venue": "In Proceedings of the 2015 International Conference on Learning Representations (ICLR)", "citeRegEx": "Soyer et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Soyer et al\\.", "year": 2015}, {"title": "Probabilistic topic models", "author": ["M. Steyvers", "T. Griffiths"], "venue": "Handbook of Latent Semantic Analysis,", "citeRegEx": "Steyvers and Griffiths,? \\Q2007\\E", "shortCiteRegEx": "Steyvers and Griffiths", "year": 2007}, {"title": "Model-based word embeddings from decompositions of count matrices", "author": ["K. Stratos", "M. Collins", "D. Hsu"], "venue": "In Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (ACL-IJCNLP),", "citeRegEx": "Stratos et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Stratos et al\\.", "year": 2015}, {"title": "Token and type constraints for cross-lingual part-of-speech tagging", "author": ["O. T\u00e4ckstr\u00f6m", "D. Das", "S. Petrov", "R. McDonald", "J. Nivre"], "venue": "Transactions of ACL,", "citeRegEx": "T\u00e4ckstr\u00f6m et al\\.,? \\Q2013\\E", "shortCiteRegEx": "T\u00e4ckstr\u00f6m et al\\.", "year": 2013}, {"title": "Bilingual lexicon extraction from comparable corpora using label propagation", "author": ["A. Tamura", "T. Watanabe", "E. Sumita"], "venue": "In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP-CoNLL),", "citeRegEx": "Tamura et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Tamura et al\\.", "year": 2012}, {"title": "Treebank translation for cross-lingual parser induction", "author": ["J. Tiedemann", "Agi\u0107", "J. Nivre"], "venue": "In Proceedings of the 18th Conference on Computational Natural Language Learning (CoNLL),", "citeRegEx": "Tiedemann et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Tiedemann et al\\.", "year": 2014}, {"title": "Modeling order in neural word embeddings at scale", "author": ["A. Trask", "D. Gilmore", "M. Russell"], "venue": "In Proceedings of the 32nd International Conference on Machine Learning (ICML),", "citeRegEx": "Trask et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Trask et al\\.", "year": 2015}, {"title": "Word representations: A simple and general method for semi-supervised learning", "author": ["J.P. Turian", "L. Ratinov", "Y. Bengio"], "venue": "In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics (ACL),", "citeRegEx": "Turian et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Turian et al\\.", "year": 2010}, {"title": "From frequency to meaning: Vector space models of semantics", "author": ["P.D. Turney", "P. Pantel"], "venue": "Journal of Artifical Intelligence Research,", "citeRegEx": "Turney and Pantel,? \\Q2010\\E", "shortCiteRegEx": "Turney and Pantel", "year": 2010}, {"title": "Identifying word translations from comparable corpora using latent topic models", "author": ["I. Vuli\u0107", "W. De Smet", "Moens", "M.-F"], "venue": "In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies (ACLHLT),", "citeRegEx": "Vuli\u0107 et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Vuli\u0107 et al\\.", "year": 2011}, {"title": "Cross-language information retrieval models based on latent topic models trained with document-aligned comparable corpora", "author": ["I. Vuli\u0107", "W. De Smet", "Moens", "M.-F"], "venue": "Information Retrieval,", "citeRegEx": "Vuli\u0107 et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Vuli\u0107 et al\\.", "year": 2013}, {"title": "Probabilistic topic modeling in multilingual settings: An overview of its methodology and applications", "author": ["I. Vuli\u0107", "W. De Smet", "J. Tang", "M. Moens"], "venue": "Information Processing and Management,", "citeRegEx": "Vuli\u0107 et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Vuli\u0107 et al\\.", "year": 2015}, {"title": "Detecting highly confident word translations from comparable corpora without any prior knowledge", "author": ["I. Vuli\u0107", "Moens", "M.-F"], "venue": "In Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics (EACL),", "citeRegEx": "Vuli\u0107 et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Vuli\u0107 et al\\.", "year": 2012}, {"title": "Cross-lingual semantic similarity of words as the similarity of their semantic word responses", "author": ["I. Vuli\u0107", "Moens", "M.-F"], "venue": "In Proceedings of the 14th Meeting of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL-HLT),", "citeRegEx": "Vuli\u0107 et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Vuli\u0107 et al\\.", "year": 2013}, {"title": "A study on bootstrapping bilingual vector spaces from non-parallel data (and nothing else)", "author": ["I. Vuli\u0107", "Moens", "M.-F"], "venue": "In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing (EMNLP),", "citeRegEx": "Vuli\u0107 et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Vuli\u0107 et al\\.", "year": 2013}, {"title": "Probabilistic models of cross-lingual semantic similarity in context based on latent cross-lingual concepts induced from comparable data", "author": ["I. Vuli\u0107", "Moens", "M.-F"], "venue": "In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP),", "citeRegEx": "Vuli\u0107 et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Vuli\u0107 et al\\.", "year": 2014}, {"title": "Monolingual and cross-lingual information retrieval models based on (bilingual) word embeddings", "author": ["I. Vuli\u0107", "Moens", "M.-F"], "venue": "In Proceedings of the 38th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR),", "citeRegEx": "Vuli\u0107 et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Vuli\u0107 et al\\.", "year": 2015}, {"title": "Improve statistical machine translation with context-sensitive bilingual semantic embedding model", "author": ["H. Wu", "D. Dong", "X. Hu", "D. Yu", "W. He", "H. Wang", "T. Liu"], "venue": "In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP),", "citeRegEx": "Wu et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Wu et al\\.", "year": 2014}, {"title": "Domain adaptation for statistical machine translation with domain dictionary and monolingual corpora", "author": ["H. Wu", "H. Wang", "C. Zong"], "venue": "In Proceedings of the 22nd International Conference on Computational Linguistics (COLING),", "citeRegEx": "Wu et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Wu et al\\.", "year": 2008}, {"title": "Distributed word representation learning for cross-lingual dependency parsing", "author": ["M. Xiao", "Y. Guo"], "venue": "In Proceedings of the 18th Conference on Computational Natural Language Learning (CoNLL),", "citeRegEx": "Xiao and Guo,? \\Q2014\\E", "shortCiteRegEx": "Xiao and Guo", "year": 2014}, {"title": "Inducing multilingual POS taggers and NP bracketers via robust projection across aligned corpora", "author": ["D. Yarowsky", "G. Ngai"], "venue": "In Proceedings of the 2nd Meeting of the North American Chapter of the Association for Computational Linguistics (NAACL),", "citeRegEx": "Yarowsky and Ngai,? \\Q2001\\E", "shortCiteRegEx": "Yarowsky and Ngai", "year": 2001}, {"title": "Cross-lingual latent topic extraction", "author": ["D. Zhang", "Q. Mei", "C. Zhai"], "venue": "In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics (ACL),", "citeRegEx": "Zhang et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Zhang et al\\.", "year": 2010}, {"title": "Bilingually-constrained phrase embeddings for machine translation", "author": ["J. Zhang", "S. Liu", "M. Li", "M. Zhou", "C. Zong"], "venue": "In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (ACL),", "citeRegEx": "Zhang et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Zhang et al\\.", "year": 2014}, {"title": "Bilingual word embeddings for phrase-based machine translation", "author": ["W.Y. Zou", "R. Socher", "D. Cer", "C.D. Manning"], "venue": "In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing (EMNLP),", "citeRegEx": "Zou et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Zou et al\\.", "year": 2013}], "referenceMentions": [{"referenceID": 35, "context": "A huge body of work in distributional semantics and word representation learning almost exclusively revolves around the distributional hypothesis (Harris, 1954) - an idea which states that similar words occur in similar contexts.", "startOffset": 146, "endOffset": 160}, {"referenceID": 25, "context": "The idea of representing words as continuous real-valued vectors dates way back to mid80s (Rumelhart, Hinton, & Williams, 1986; Elman, 1990).", "startOffset": 90, "endOffset": 140}, {"referenceID": 4, "context": "The idea met its resurgence a decade ago (Bengio et al., 2003), where a neural language model learns word embeddings as part of a neural network architecture for statistical language modeling.", "startOffset": 41, "endOffset": 62}, {"referenceID": 76, "context": "Other models inspired by skip-gram and CBOW are GloVe (Global Vectors for Word Representation) (Pennington et al., 2014), which combines local and global contexts of a word into a unified model, and a model which relies on dependency-based contexts instead of simpler word-based contexts (Levy &", "startOffset": 95, "endOffset": 120}, {"referenceID": 4, "context": "The idea met its resurgence a decade ago (Bengio et al., 2003), where a neural language model learns word embeddings as part of a neural network architecture for statistical language modeling. This work inspired other approaches that learn word embeddings within the neural-network language modeling framework (Collobert & Weston, 2008; Collobert, Weston, Bottou, Karlen, Kavukcuoglu, & Kuksa, 2011). Word embeddings are tailored to capture semantics and encode a continuous notion of semantic similarity (as opposed to semantically poorer discrete representations), necessary to share information between words and other text units. Recently, the skip-gram and continuous bag-of-words (CBOW) model from Mikolov et al. (2013a, 2013c) revealed that the full neural-network structure is not needed at all to learn high-quality word embeddings (with extremely decreased training times compared to the full-fledged neural network models, see Mikolov et al.\u2019s (2013a) work for the full analysis of complexity of the models).", "startOffset": 42, "endOffset": 963}, {"referenceID": 32, "context": "Figure 1: A toy 3D shared bilingual embedding space from Gouws et al. (2015): While in monolingual spaces words with similar meanings should have similar representations, in bilingual spaces words in two different languages with similar meanings should have similar representations (both mono- and cross-lingually).", "startOffset": 57, "endOffset": 77}, {"referenceID": 2, "context": ", context counting plus context weighting and/or dimensionality reduction), with a slight improvement in performance with SGNS (Baroni et al., 2014; Levy et al., 2015).", "startOffset": 127, "endOffset": 167}, {"referenceID": 55, "context": ", context counting plus context weighting and/or dimensionality reduction), with a slight improvement in performance with SGNS (Baroni et al., 2014; Levy et al., 2015).", "startOffset": 127, "endOffset": 167}, {"referenceID": 2, "context": "Their utility has been validated and proven in various semantic tasks such as semantic word similarity, synonymy detection or word analogy solving (Mikolov et al., 2013d; Baroni et al., 2014; Pennington et al., 2014).", "startOffset": 147, "endOffset": 216}, {"referenceID": 76, "context": "Their utility has been validated and proven in various semantic tasks such as semantic word similarity, synonymy detection or word analogy solving (Mikolov et al., 2013d; Baroni et al., 2014; Pennington et al., 2014).", "startOffset": 147, "endOffset": 216}, {"referenceID": 15, "context": "Moreover, word embeddings have been proven to serve as useful unsupervised features for plenty of downstream NLP tasks such as named entity recognition, chunking, semantic role labeling, part-of-speech tagging, or selectional preferences (Turian, Ratinov, & Bengio, 2010; Collobert et al., 2011).", "startOffset": 238, "endOffset": 295}, {"referenceID": 55, "context": ", 2013c; Levy & Goldberg, 2014b), with a clear advantage on similarity tasks when compared to traditional models from distributional semantics (Levy et al., 2015) in this article we will focus on the adaptation of the skip-gram model with negative sampling (Mikolov et al.", "startOffset": 143, "endOffset": 162}, {"referenceID": 44, "context": "Bilingual word representations could serve as an useful source knowledge for problems in cross-lingual information retrieval (Levow, Oard, & Resnik, 2005; Vuli\u0107, De Smet, & Moens, 2013), statistical machine translation (Wu, Wang, & Zong, 2008), document classification (Ni, Sun, Hu, & Chen, 2011; Klementiev et al., 2012; Hermann & Blunsom, 2014b; Chandar et al., 2014; Vuli\u0107, De Smet, Tang, & Moens, 2015), bilingual lexicon extraction (Tamura, Watanabe, & Sumita, 2012; Vuli\u0107 & Moens, 2013a), or knowledge transfer and annotation projection from resource-rich to resource-poor languages for a myriad of NLP tasks (Yarowsky & Ngai, 2001; Pad\u00f3 & Lapata, 2009; Peirsman & Pad\u00f3, 2010; Das & Petrov, 2011; T\u00e4ckstr\u00f6m, Das, Petrov, McDonald, & Nivre, 2013; Ganchev & Das, 2013; Tiedemann, Agi\u0107, & Nivre, 2014).", "startOffset": 269, "endOffset": 406}, {"referenceID": 12, "context": "Bilingual word representations could serve as an useful source knowledge for problems in cross-lingual information retrieval (Levow, Oard, & Resnik, 2005; Vuli\u0107, De Smet, & Moens, 2013), statistical machine translation (Wu, Wang, & Zong, 2008), document classification (Ni, Sun, Hu, & Chen, 2011; Klementiev et al., 2012; Hermann & Blunsom, 2014b; Chandar et al., 2014; Vuli\u0107, De Smet, Tang, & Moens, 2015), bilingual lexicon extraction (Tamura, Watanabe, & Sumita, 2012; Vuli\u0107 & Moens, 2013a), or knowledge transfer and annotation projection from resource-rich to resource-poor languages for a myriad of NLP tasks (Yarowsky & Ngai, 2001; Pad\u00f3 & Lapata, 2009; Peirsman & Pad\u00f3, 2010; Das & Petrov, 2011; T\u00e4ckstr\u00f6m, Das, Petrov, McDonald, & Nivre, 2013; Ganchev & Das, 2013; Tiedemann, Agi\u0107, & Nivre, 2014).", "startOffset": 269, "endOffset": 406}, {"referenceID": 44, "context": "We may cluster the current work in three different groups: (1) the models that rely on hard word alignments obtained from parallel data to constrain the learning of BWEs (Klementiev et al., 2012; Zou, Socher, Cer, & Manning, 2013; Wu, Dong, Hu, Yu, He, Wu, Wang, & Liu, 2014); (2) the models that use the alignment of parallel data at the sentence level (Ko\u010disk\u00fd, Hermann, & Blunsom, 2014; Hermann & Blunsom, 2014a, 2014b; Chandar et al.", "startOffset": 170, "endOffset": 275}, {"referenceID": 12, "context": ", 2012; Zou, Socher, Cer, & Manning, 2013; Wu, Dong, Hu, Yu, He, Wu, Wang, & Liu, 2014); (2) the models that use the alignment of parallel data at the sentence level (Ko\u010disk\u00fd, Hermann, & Blunsom, 2014; Hermann & Blunsom, 2014a, 2014b; Chandar et al., 2014; Shi, Liu, Liu, & Sun, 2015; Gouws et al., 2015); (3) the models that critically require readily available bilingual lexicons (Mikolov et al.", "startOffset": 166, "endOffset": 304}, {"referenceID": 32, "context": ", 2012; Zou, Socher, Cer, & Manning, 2013; Wu, Dong, Hu, Yu, He, Wu, Wang, & Liu, 2014); (2) the models that use the alignment of parallel data at the sentence level (Ko\u010disk\u00fd, Hermann, & Blunsom, 2014; Hermann & Blunsom, 2014a, 2014b; Chandar et al., 2014; Shi, Liu, Liu, & Sun, 2015; Gouws et al., 2015); (3) the models that critically require readily available bilingual lexicons (Mikolov et al.", "startOffset": 166, "endOffset": 304}, {"referenceID": 79, "context": "Prior work on inducing bilingual word representations in the early days followed the tradition of window-based context-counting distributional models (Rapp, 1999; Gaussier, Renders, Matveeva, Goutte, & D\u00e9jean, 2004; Laroche & Langlais, 2010) and it again required a bilingual lexicon as a critical resource.", "startOffset": 150, "endOffset": 241}, {"referenceID": 12, "context": "In theory, the work from (Hermann & Blunsom, 2014b; Chandar et al., 2014) may also be extended to the same setting with document-aligned data, as these two models originally rely on sentence embeddings computed as aggregations over their single word embeddings plus sentence alignments.", "startOffset": 25, "endOffset": 73}, {"referenceID": 4, "context": "2 The SGNS model learns word embeddings (WEs) in a similar way to neural language models (Bengio et al., 2003; Collobert & Weston, 2008), but without a non-linear hidden layer.", "startOffset": 89, "endOffset": 136}, {"referenceID": 61, "context": "Our departure point is the log-linear SGNS from Mikolov et al. (2013c) as implemented in the word2vec package.", "startOffset": 48, "endOffset": 71}, {"referenceID": 31, "context": "While the interested reader may find further details about the negative sampling procedure, and the new exact objective function along with its derivation elsewhere (Levy & Goldberg, 2014b), for illustrative purposes and simplicity, here we present the approximative objective function with negative sampling by Goldberg and Levy (2014):", "startOffset": 312, "endOffset": 337}, {"referenceID": 67, "context": "More recent work on multilingual probabilistic topic modeling (MuPTM) (Mimno et al., 2009; De Smet & Moens, 2009; Vuli\u0107 et al., 2011) showed that word representations of higher quality may be built if a multilingual topic model such as bilingual LDA (BiLDA) is trained jointly on document-aligned comparable corpora by retaining the structure of the corpus intact (i.", "startOffset": 70, "endOffset": 133}, {"referenceID": 95, "context": "More recent work on multilingual probabilistic topic modeling (MuPTM) (Mimno et al., 2009; De Smet & Moens, 2009; Vuli\u0107 et al., 2011) showed that word representations of higher quality may be built if a multilingual topic model such as bilingual LDA (BiLDA) is trained jointly on document-aligned comparable corpora by retaining the structure of the corpus intact (i.", "startOffset": 70, "endOffset": 133}, {"referenceID": 97, "context": ", BLE or SWTC) (Vuli\u0107 & Moens, 2013a; Vuli\u0107 et al., 2015).", "startOffset": 15, "endOffset": 57}, {"referenceID": 97, "context": "A multilingual topic model is typically trained by Gibbs sampling (Geman & Geman, 1984; Steyvers & Griffiths, 2007; Vuli\u0107 et al., 2015).", "startOffset": 66, "endOffset": 135}, {"referenceID": 29, "context": ", using PMI, TFIDF) between pivot words and their context words in a window of predefined size, plus an external bilingual lexicon to align context words/dimensions across languages (Gaussier et al., 2004; Laroche & Langlais, 2010).", "startOffset": 182, "endOffset": 231}, {"referenceID": 51, "context": "(Lee, 1999; Cha, 2007), and different RMs typically require different SFs to produce optimal or near-optimal results over various semantic tasks.", "startOffset": 0, "endOffset": 22}, {"referenceID": 77, "context": "On the other hand, a good choice for SF when working with probabilistic RMs such as Basic-MuPTM and Association-MuPTM RS is the Hellinger distance (Pollard, 2001; Cha, 2007; Kazama, Saeger, Kuroda, Murata, & Torisawa, 2010), which displays excellent results in the BLE task (Vuli\u0107 & Moens, 2013a).", "startOffset": 147, "endOffset": 223}, {"referenceID": 13, "context": "A plethora of models for semantic composition have been proposed in the relevant literature, differing in their choice of vector operators, input structures and required knowledge (Mitchell & Lapata, 2008; Baroni & Zamparelli, 2010; Rudolph & Giesbrecht, 2010; Socher, Huval, Manning, & Ng, 2012; Blacoe & Lapata, 2012; Clarke, 2012; Hermann & Blunsom, 2014b; Milajevs, Kartsaklis, Sadrzadeh, & Purver, 2014), to name only a few.", "startOffset": 180, "endOffset": 408}, {"referenceID": 66, "context": ", 2013d), we opt for simple addition (denoted by +) from Mitchell and Lapata (2008) as the compositional operator, due to its simplicity, the ease of applicability on bag-of-words contexts, and its relatively solid performance in various compositional tasks (Mitchell & Lapata, 2008; Milajevs et al., 2014).", "startOffset": 258, "endOffset": 306}, {"referenceID": 13, "context": "A plethora of models for semantic composition have been proposed in the relevant literature, differing in their choice of vector operators, input structures and required knowledge (Mitchell & Lapata, 2008; Baroni & Zamparelli, 2010; Rudolph & Giesbrecht, 2010; Socher, Huval, Manning, & Ng, 2012; Blacoe & Lapata, 2012; Clarke, 2012; Hermann & Blunsom, 2014b; Milajevs, Kartsaklis, Sadrzadeh, & Purver, 2014), to name only a few. In this work, driven by the observed linear linguistic regularities in the embedding spaces (Mikolov et al., 2013d), we opt for simple addition (denoted by +) from Mitchell and Lapata (2008) as the compositional operator, due to its simplicity, the ease of applicability on bag-of-words contexts, and its relatively solid performance in various compositional tasks (Mitchell & Lapata, 2008; Milajevs et al.", "startOffset": 320, "endOffset": 621}, {"referenceID": 61, "context": "For more details regarding the models, we refer the interested reader to the original paper (Melamud et al., 2015).", "startOffset": 92, "endOffset": 114}, {"referenceID": 34, "context": "Following prior work (Koehn & Knight, 2002; Haghighi et al., 2008; Prochasson & Fung, 2011; Vuli\u0107 & Moens, 2013b, 2014), we retain only nouns that occur at least 5 times in the corpus.", "startOffset": 21, "endOffset": 119}, {"referenceID": 83, "context": "TreeTagger (Schmid, 1994) is used for POS tagging and lemmatization.", "startOffset": 11, "endOffset": 25}, {"referenceID": 12, "context": "We have also trained BWESG with d = 40 to be directly comparable to readily available sets of BWEs from prior work (Chandar et al., 2014; Gouws et al., 2015).", "startOffset": 115, "endOffset": 157}, {"referenceID": 32, "context": "We have also trained BWESG with d = 40 to be directly comparable to readily available sets of BWEs from prior work (Chandar et al., 2014; Gouws et al., 2015).", "startOffset": 115, "endOffset": 157}, {"referenceID": 12, "context": "3 as well as against several benchmarking BWE induction models (Mikolov et al., 2013b; Chandar et al., 2014; Gouws et al., 2015).", "startOffset": 63, "endOffset": 128}, {"referenceID": 32, "context": "3 as well as against several benchmarking BWE induction models (Mikolov et al., 2013b; Chandar et al., 2014; Gouws et al., 2015).", "startOffset": 63, "endOffset": 128}, {"referenceID": 29, "context": "Since we can build a one-to-one bilingual lexicon by harvesting one-to-one translation pairs, the lexicon qualiy is best reflected in the Acc1 score, that is, the number of source language (ES/IT/NL) words wS i from ground truth translation pairs for which the top ranked word cross-lingually is the correct translation in the other language (EN) according to the ground truth over the total number of ground truth translation pairs (=1000) (Gaussier et al., 2004; Tamura et al., 2012; Vuli\u0107 & Moens, 2013b).", "startOffset": 441, "endOffset": 507}, {"referenceID": 90, "context": "Since we can build a one-to-one bilingual lexicon by harvesting one-to-one translation pairs, the lexicon qualiy is best reflected in the Acc1 score, that is, the number of source language (ES/IT/NL) words wS i from ground truth translation pairs for which the top ranked word cross-lingually is the correct translation in the other language (EN) according to the ground truth over the total number of ground truth translation pairs (=1000) (Gaussier et al., 2004; Tamura et al., 2012; Vuli\u0107 & Moens, 2013b).", "startOffset": 441, "endOffset": 507}, {"referenceID": 51, "context": "We have tested various families of similarity functions (SFs) (Lee, 1999; Cha, 2007) for each baseline word representation, and have opted for the best scoring SF for each representation.", "startOffset": 62, "endOffset": 84}, {"referenceID": 77, "context": "SF: Hellinger distance (Pollard, 2001; Cha, 2007).", "startOffset": 23, "endOffset": 49}, {"referenceID": 77, "context": "SF: Hellinger distance (Pollard, 2001; Cha, 2007). (3) AMu+HD. RM: Association-MuPTM. SF: Hellinger distance. BMu+HD and AMu+HD were the best scoring models in the work by Vuli\u0107 and Moens (2013a). (4) TPPMI+cos.", "startOffset": 24, "endOffset": 196}, {"referenceID": 77, "context": "SF: Hellinger distance (Pollard, 2001; Cha, 2007). (3) AMu+HD. RM: Association-MuPTM. SF: Hellinger distance. BMu+HD and AMu+HD were the best scoring models in the work by Vuli\u0107 and Moens (2013a). (4) TPPMI+cos. RM: Traditional-PPMI. SF: cosine similarity. This combination was the best scoring BLE model in the work by Vuli\u0107 and Moens (2013b), and it currently obtains the best reported results in the BLE task for these training and test data.", "startOffset": 24, "endOffset": 344}, {"referenceID": 79, "context": "The lower results of TPPMI+cos compared to other two baseline models are also attributed to the overall lower quality and size of NL-EN training data, which is then reflected in a lower quality of seed lexicons necessary to start the bootstrapping procedure from Vuli\u0107 and Moens (2013b). Computational Complexity.", "startOffset": 240, "endOffset": 287}, {"referenceID": 12, "context": ", parallel data, translation pairs): We compare against two benchmarking BWE induction models that rely on parallel data (Chandar et al., 2014; Gouws et al., 2015).", "startOffset": 121, "endOffset": 163}, {"referenceID": 32, "context": ", parallel data, translation pairs): We compare against two benchmarking BWE induction models that rely on parallel data (Chandar et al., 2014; Gouws et al., 2015).", "startOffset": 121, "endOffset": 163}, {"referenceID": 60, "context": "Another BWE induction that can be used in this setting with document-aligned data is the well known model from Mikolov et al. (2013b): First, two monolingual embedding spaces are induced separately in each of the two languages using a standard monolingual WE model such as SGNS.", "startOffset": 111, "endOffset": 134}, {"referenceID": 62, "context": "Table 4: BLE results: BWE induction model from Mikolov et al. (2013b) relying on SGNS.", "startOffset": 47, "endOffset": 70}, {"referenceID": 12, "context": "Table 5: BLE results: BWESG vs BWE induction models from Chandar et al. (2014), Gouws et al.", "startOffset": 57, "endOffset": 79}, {"referenceID": 12, "context": "Table 5: BLE results: BWESG vs BWE induction models from Chandar et al. (2014), Gouws et al. (2015). d=40 as in their original work for all vectors.", "startOffset": 57, "endOffset": 100}, {"referenceID": 12, "context": "of pre-trained 40-dimensional word embeddings for Spanish-English from Chandar et al. (2014) which are available online.", "startOffset": 71, "endOffset": 93}, {"referenceID": 12, "context": "of pre-trained 40-dimensional word embeddings for Spanish-English from Chandar et al. (2014) which are available online.10 We also train 40-dimensional embeddings from Gouws et al. (2015) (the same dimensionality as in the original paper) using the same data and setup as Gouws et al.", "startOffset": 71, "endOffset": 188}, {"referenceID": 12, "context": "of pre-trained 40-dimensional word embeddings for Spanish-English from Chandar et al. (2014) which are available online.10 We also train 40-dimensional embeddings from Gouws et al. (2015) (the same dimensionality as in the original paper) using the same data and setup as Gouws et al. (2015) with suggested parameters for the three language pairs.", "startOffset": 71, "endOffset": 292}, {"referenceID": 12, "context": "of pre-trained 40-dimensional word embeddings for Spanish-English from Chandar et al. (2014) which are available online.10 We also train 40-dimensional embeddings from Gouws et al. (2015) (the same dimensionality as in the original paper) using the same data and setup as Gouws et al. (2015) with suggested parameters for the three language pairs.11 The comparison with these two BWE induction models is provided in tab. 5. To make the comparison fair, we search for translations over the same vocabulary as with all other models. The results clearly reveal that, although both other BWE models critically rely on parallel Europarl data for training, and Gouws et al. (2015) in addition train on entire monolingual Wikipedias in both languages, our simple BWE induction model trained on much smaller amounts of document-aligned non-parallel data produces significantly higher BLI scores for IT-EN and ES-EN with sufficiently large windows.", "startOffset": 71, "endOffset": 675}, {"referenceID": 51, "context": "We have again tested various families of SFs (Lee, 1999; Cha, 2007) for each baseline word representation, and have opted for the best scoring SF for each representation.", "startOffset": 45, "endOffset": 67}, {"referenceID": 61, "context": "We have also experimented with the context-sensitive CLSS models proposed by Melamud et al. (2015), but we do not report the actual scores as this model, although displaying a similar relative ranking of different representation models, was consistently outperformed by the models from (Vuli\u0107 & Moens, 2014) in our evaluation runs: \u22480.", "startOffset": 77, "endOffset": 99}, {"referenceID": 61, "context": "We have also experimented with the context-sensitive CLSS models proposed by Melamud et al. (2015), but we do not report the actual scores as this model, although displaying a similar relative ranking of different representation models, was consistently outperformed by the models from (Vuli\u0107 & Moens, 2014) in our evaluation runs: \u22480.75-0.80 vs \u2248 0.60-0.65 for the models from Melamud et al. (2015).", "startOffset": 77, "endOffset": 400}, {"referenceID": 62, "context": "Table 8: SWTC results: BWE induction model from Mikolov et al. (2013b) relying on SGNS.", "startOffset": 48, "endOffset": 71}, {"referenceID": 12, "context": "Table 9: SWTC results: BWESG vs BWE induction models from Chandar et al. (2014), Gouws et al.", "startOffset": 58, "endOffset": 80}, {"referenceID": 12, "context": "Table 9: SWTC results: BWESG vs BWE induction models from Chandar et al. (2014), Gouws et al. (2015). d=40 as in their original work for all vectors.", "startOffset": 58, "endOffset": 101}, {"referenceID": 33, "context": "This finding is in line with the recent work from Gouws and S\u00f8gaard (2015).", "startOffset": 50, "endOffset": 75}, {"referenceID": 12, "context": "8, while the results with the other two models (Chandar et al., 2014; Gouws et al., 2015) are provided in tab.", "startOffset": 47, "endOffset": 89}, {"referenceID": 32, "context": "8, while the results with the other two models (Chandar et al., 2014; Gouws et al., 2015) are provided in tab.", "startOffset": 47, "endOffset": 89}, {"referenceID": 60, "context": "The results for the BWE induction model from Mikolov et al. (2013b) are summarized in tab.", "startOffset": 45, "endOffset": 68}, {"referenceID": 62, "context": "The model from Mikolov et al. (2013b) constitutes a stronger baseline: Good results in the SWTC task with this model are an interesting finding per se.", "startOffset": 15, "endOffset": 38}, {"referenceID": 12, "context": "Our new BWESG-based BLE and SWTC models outperform previous state-of-the-art models for BLE and SWTC from document-aligned comparable data and related BWE induction models (Mikolov et al., 2013b; Chandar et al., 2014; Gouws et al., 2015).", "startOffset": 172, "endOffset": 237}, {"referenceID": 32, "context": "Our new BWESG-based BLE and SWTC models outperform previous state-of-the-art models for BLE and SWTC from document-aligned comparable data and related BWE induction models (Mikolov et al., 2013b; Chandar et al., 2014; Gouws et al., 2015).", "startOffset": 172, "endOffset": 237}, {"referenceID": 2, "context": "The findings in this article follow the recently published surveys from Baroni et al. (2014), Levy et al.", "startOffset": 72, "endOffset": 93}, {"referenceID": 2, "context": "The findings in this article follow the recently published surveys from Baroni et al. (2014), Levy et al. (2015) regarding a solid and robust performance of neural word representations/word embeddings in semantic tasks: our new BWESG-based models for BLE and SWTC significantly outscore previous state-of-theart distributional approaches on both tasks across different parameter settings.", "startOffset": 72, "endOffset": 113}, {"referenceID": 53, "context": "Moreover, BWEs induced by BWESG may be used in other semantic tasks besides the ones discussed in this work, and it would be interesting to experiment with other types of context aggregation and selection beyond the bag-of-words assumption, such as dependency-based contexts (Levy & Goldberg, 2014a), or other objective functions during training in the same vein as proposed by Levy and Goldberg (2014b). Similar to the evolution in multilingual probabilistic topic modeling, another path of future work may lead to investigating bilingual models for learning BWEs which will be able to jointly learn from separate documents in aligned document pairs, without the need to construct pseudo-bilingual documents.", "startOffset": 378, "endOffset": 404}, {"referenceID": 109, "context": "Such low-cost multilingual embeddings beyond the word level extracted from comparable data may find its application in a variety of tasks such as statistical machine translation (Mikolov et al., 2013b; Zou et al., 2013; Zhang, Liu, Li, Zhou, & Zong, 2014; Wu et al., 2014), semantic tasks such as multilingual semantic textual similarity (Agirre, Banea, Cardie, Cer, Diab, Gonzalez-Agirre, Guo, Mihalcea, Rigau, & Wiebe, 2014), cross-lingual", "startOffset": 178, "endOffset": 272}, {"referenceID": 103, "context": "Such low-cost multilingual embeddings beyond the word level extracted from comparable data may find its application in a variety of tasks such as statistical machine translation (Mikolov et al., 2013b; Zou et al., 2013; Zhang, Liu, Li, Zhou, & Zong, 2014; Wu et al., 2014), semantic tasks such as multilingual semantic textual similarity (Agirre, Banea, Cardie, Cer, Diab, Gonzalez-Agirre, Guo, Mihalcea, Rigau, & Wiebe, 2014), cross-lingual", "startOffset": 178, "endOffset": 272}, {"referenceID": 96, "context": "information retrieval (Vuli\u0107 et al., 2013; Vuli\u0107 & Moens, 2015) or cross-lingual document classification (Klementiev et al.", "startOffset": 22, "endOffset": 63}, {"referenceID": 44, "context": ", 2013; Vuli\u0107 & Moens, 2015) or cross-lingual document classification (Klementiev et al., 2012; Hermann & Blunsom, 2014b; Chandar et al., 2014).", "startOffset": 70, "endOffset": 143}, {"referenceID": 12, "context": ", 2013; Vuli\u0107 & Moens, 2015) or cross-lingual document classification (Klementiev et al., 2012; Hermann & Blunsom, 2014b; Chandar et al., 2014).", "startOffset": 70, "endOffset": 143}], "year": 2015, "abstractText": "We propose a new model for learning bilingual word representations from non-parallel document-aligned data. Following the recent advances in word representation learning, our model learns dense real-valued word vectors, that is, bilingual word embeddings (BWEs). Unlike prior work on inducing BWEs which heavily relied on parallel sentence-aligned corpora and/or readily available translation resources such as dictionaries, the article reveals that BWEs may be learned solely on the basis of document-aligned comparable data without any additional lexical resources nor syntactic information. We present a comparison of our approach with previous state-of-the-art models for learning bilingual word representations from comparable data that rely on the framework of multilingual probabilistic topic modeling (MuPTM), as well as with distributional local context-counting models. We demonstrate the utility of the induced BWEs in two semantic tasks: (1) bilingual lexicon extraction, (2) suggesting word translations in context for polysemous words. Our simple yet effective BWE-based models significantly outperform the MuPTM-based and contextcounting representation models from comparable data as well as prior BWE-based models, and acquire the best reported results on both tasks for all three tested language pairs.", "creator": "TeX"}}}