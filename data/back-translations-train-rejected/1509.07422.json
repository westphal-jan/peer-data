{"id": "1509.07422", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "24-Sep-2015", "title": "Adaptive Sequential Optimization with Applications to Machine Learning", "abstract": "A framework is introduced for solving a sequence of slowly changing optimization problems, including those arising in regression and classification applications, using optimization algorithms such as stochastic gradient descent (SGD). The optimization problems change slowly in the sense that the minimizers change at either a fixed or bounded rate. A method based on estimates of the change in the minimizers and properties of the optimization algorithm is introduced for adaptively selecting the number of samples needed from the distributions underlying each problem in order to ensure that the excess risk, i.e., the expected gap between the loss achieved by the approximate minimizer produced by the optimization algorithm and the exact minimizer, does not exceed a target level. Experiments with synthetic and real data are used to confirm that this approach performs well.", "histories": [["v1", "Thu, 24 Sep 2015 16:19:43 GMT  (276kb)", "http://arxiv.org/abs/1509.07422v1", "submitted to ICASSP 2016, extended version"]], "COMMENTS": "submitted to ICASSP 2016, extended version", "reviews": [], "SUBJECTS": "cs.LG cs.DS", "authors": ["craig wilson", "venugopal v veeravalli"], "accepted": false, "id": "1509.07422"}, "pdf": {"name": "1509.07422.pdf", "metadata": {"source": "CRF", "title": "Adaptive Sequential Optimization with Applications to Machine Learning", "authors": ["Craig Wilson", "Venugopal V. Veeravalli"], "emails": ["wilson60@illinois.edu", "vvv@illinois.edu"], "sections": [{"heading": null, "text": "This is a problem that we must apply n (x, zn) at all times in order to learn another task. (1) Although, motivated by regression and classification, our framework works for each loss function (x, z) that satisfies certain characteristics later. (In the learning context, a task consists of the loss function (x, z) that satisfies certain characteristics that are discussed later. (2) A task consists of the task we perform in the function of the loss function (x, z) that satisfies certain characteristics. (2) A task corresponds to the characteristics of the function and the label. (x, z) and thus our problem can be considered as a learning sequence. (2) The task consists of the loss function (x, z) that satisfies certain characteristics that are discussed later. (3) A task consists of the loss function (x, z) and thus our problem can be considered as a learning sequence. (2) The task consists of the loss function (x, z) that satisfies certain characteristics that are discussed later."}], "references": [{"title": "Foundations of Machine Learning", "author": ["M. Mohri", "A. Rostamizadeh", "A. Talwalkar"], "venue": "The MIT Press", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2012}, {"title": "and S", "author": ["A. Agarwal", "H. Daum\u00e9"], "venue": "Gerber, \u201cLearning multiple tasks using manifold regularization.,\u201d in NIPS", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2011}, {"title": "Regularized multi\u2013task learning,", "author": ["T. Evgeniou", "M. Pontil"], "venue": "Proceedings of the Tenth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2004}, {"title": "A convex formulation for learning task relationships in multi-task learning,", "author": ["Y. Zhang", "D. Yeung"], "venue": "CoRR, vol. abs/1203.3536,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2012}, {"title": "A survey on transfer learning,", "author": ["S. Pan", "Q. Yang"], "venue": "IEEE Transactions on Knowledge and Data Engineering,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2010}, {"title": "Matrix regularization techniques for online multitask learning,", "author": ["A. Agarwal", "A. Rakhlin", "P. Bartlett"], "venue": "Tech. Rep. UCB/EECS-2008-138, EECS Department,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2008}, {"title": "and A", "author": ["Z. Towfic", "J. Chu"], "venue": "Sayed, \u201cOnline distirubted online classifcation in the midst of concept drifts,\u201d Neurocomputing, vol. 112, pp. 138\u2013152", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2013}, {"title": "and M", "author": ["C. Tekin", "L. Canzian"], "venue": "van der Schaar, \u201cContext adaptive big data stream mining,\u201d in Allerton Conference", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2014}, {"title": "Machine learning for sequential data: A review,", "author": ["T. Dietterich"], "venue": "Structural, Syntactic, and Statistical Pattern Recognition,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2002}, {"title": "Adaptive fraud detection.,", "author": ["T. Fawcett", "F. Provost"], "venue": "Data Min. Knowl. Discov.,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 1997}, {"title": "Predicting the secondary structure of globular proteins using neural network models,", "author": ["N. Qian", "T. Sejnowski"], "venue": "Journal of Molecular Biology,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 1988}, {"title": "Input-output HMM\u2019s for sequence processing,", "author": ["Y. Bengio", "P. Frasconi"], "venue": "IEEE Transactions on Neural Networks,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 1996}, {"title": "Implicit Functions and Solution Mappings: A View from Variational Analysis", "author": ["A. Dontchev", "R. Rockafellar"], "venue": "Springer, New York, New York", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2009}, {"title": "On the empirical estimation of integral probability metrics,", "author": ["B. Sriperumbudur"], "venue": "Electronic Journal of Statistics,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2012}, {"title": "Introduction to non-asymptotic analysis of random matrices,", "author": ["R. Veryshin"], "venue": "Tech. Rep., University of Michigan,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2012}, {"title": "and A", "author": ["A. Nemirovski", "A. Juditsky", "G. Lan"], "venue": "Shapiro, \u201cStochastic approximation approach to stochastic programming,\u201d SIAM Journal on Optimization, vol. 19, pp. 1574\u20131609", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2009}, {"title": "Pechuk, \u201cInequalities for the distributions of functionals of sub-gaussian vectors,", "author": ["E.D.V.V Buldygin"], "venue": "Theor. Probability and Math. Statist., pp", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2010}, {"title": "Large deviations for sums of partly dependent random variables,", "author": ["S. Janson"], "venue": "Random Structures Algorithms,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2004}, {"title": "Concentration Inequalities: A Nonasymptotic Theory of Independence", "author": ["S. Boucheron", "G. Lugosi", "P. Massart"], "venue": "Oxford University Press", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2013}, {"title": "Numerical Linear Algebra", "author": ["L. Trefethen"], "venue": "SIAM", "citeRegEx": "20", "shortCiteRegEx": null, "year": 1997}, {"title": "Uniqueness of positive fixed points for increasing concave functions on rn: An elementary result,", "author": ["J. Kennan"], "venue": "Review of Economic Dynamics,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2001}, {"title": "Fixed Point Theory", "author": ["A. Granas", "J. Dugundji"], "venue": "Springer-Verlag", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2003}, {"title": "Trends in income inequality", "author": ["S. Jenkins", "P. Van Kerm"], "venue": "pro-poor income growth, and income mobility,\u201d Oxford Economic Papers, vol. 58, no. 3, pp. 531\u2013548", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2006}, {"title": "The elements of statistical learning: data mining", "author": ["T. Hastie", "R. Tibshirani", "J.H. Friedman"], "venue": "inference, and prediction: with 200 full-color illustrations, New York: Springer-Verlag", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2001}, {"title": "Non-Asymptotic Analysis of Stochastic Approximation Algorithms for Machine Learning,", "author": ["F. Bach", "E. Moulines"], "venue": "Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2011}, {"title": "Nonlinear Programming", "author": ["D. Bertsekas"], "venue": "Athena Scientific", "citeRegEx": "29", "shortCiteRegEx": null, "year": 1999}, {"title": "Online learning and stochastic approximations,", "author": ["L\u00e9on Bottou"], "venue": null, "citeRegEx": "30", "shortCiteRegEx": "30", "year": 1998}, {"title": "Analysis of mirror descent for strongly convex functions,", "author": ["A. Nedic", "S. Lee"], "venue": "ArXiV,", "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2013}, {"title": "Introductory Lectures on Convex Optimization: A Basic Course", "author": ["Yu. Nesterov"], "venue": null, "citeRegEx": "32", "shortCiteRegEx": "32", "year": 2004}], "referenceMentions": [{"referenceID": 0, "context": "which is a standard criterion for optimization and learning problems [1].", "startOffset": 69, "endOffset": 72}, {"referenceID": 1, "context": "In multi-task learning, one tries to learn several tasks simultaneously as in [2],[3], and [4] by exploiting the relationships between the tasks.", "startOffset": 78, "endOffset": 81}, {"referenceID": 2, "context": "In multi-task learning, one tries to learn several tasks simultaneously as in [2],[3], and [4] by exploiting the relationships between the tasks.", "startOffset": 82, "endOffset": 85}, {"referenceID": 3, "context": "In multi-task learning, one tries to learn several tasks simultaneously as in [2],[3], and [4] by exploiting the relationships between the tasks.", "startOffset": 91, "endOffset": 94}, {"referenceID": 4, "context": "In transfer learning, knowledge from one source task is transferred to another target task either with or without additional training data for the target task [5].", "startOffset": 159, "endOffset": 162}, {"referenceID": 5, "context": "For multi-task and transfer learning, there are theoretical guarantees on regret for some algorithms [6].", "startOffset": 101, "endOffset": 104}, {"referenceID": 6, "context": "For example, we could observe a feature wn and predict the label yn as in [7].", "startOffset": 74, "endOffset": 77}, {"referenceID": 7, "context": "There are also some bandit approaches in which one of a finite number of predictors must be applied to the data as in [8].", "startOffset": 118, "endOffset": 121}, {"referenceID": 8, "context": "Another relevant model is sequential supervised learning (see [9]) in which we observe a stream of data consisting of feature/label pairs (wn,yn) at time n, with wn being the feature vector and yn being the label.", "startOffset": 62, "endOffset": 65}, {"referenceID": 9, "context": "One approach to this problem, studied in [10] and [11], is to look at L consecutive pairs {(wn\u2212i,yn\u2212i)}i=1 and develop a predictor at time n by applying a supervised learning algorithm to this training data.", "startOffset": 41, "endOffset": 45}, {"referenceID": 10, "context": "One approach to this problem, studied in [10] and [11], is to look at L consecutive pairs {(wn\u2212i,yn\u2212i)}i=1 and develop a predictor at time n by applying a supervised learning algorithm to this training data.", "startOffset": 50, "endOffset": 54}, {"referenceID": 11, "context": "Another approach is to assume that there is an underlying hidden Markov model (HMM) [12].", "startOffset": 84, "endOffset": 88}, {"referenceID": 12, "context": "Using the triangle inequality and variational inequalities from [13] yields \u2016xi \u2212xi\u22121\u2016 \u2264 \u2016xi \u2212xi\u22121\u2016+ \u2016xi\u2212xi \u2016+ \u2016xi\u22121\u2212xi\u22121\u2016 \u2264 \u2016xi \u2212xi\u22121\u2016+ 1 m \u2016\u2207x fi(xi)\u2016+ 1 m \u2016\u2207x fi(xi\u22121)\u2016 We then approximate \u2016\u2207x fi(xi)\u2016= \u2016Ezi\u223cpi [\u2207xl(xi,zi)]\u2016 by", "startOffset": 64, "endOffset": 68}, {"referenceID": 13, "context": "2 Vector Integral Probability Metric Estimate Given a class of functions F where each f \u2208F maps Z \u2192R, an integral probability metric (IPM) [14] between two distributions p and q is defined to be \u03b3F (p,q), sup f\u2208F \u2223 Ez\u223cp[ f (z)]\u2212Ez\u0303\u223cq[ f (z\u0303)] \u2223", "startOffset": 139, "endOffset": 143}, {"referenceID": 12, "context": "We consider an extension of this idea, which we call a vector IPM, in which the class of functions F maps Z \u2192 X : \u03b3V F (p,q), sup f\u2208F \u2016Ez\u223cp[ f (z)]\u2212Ez\u0303\u223cq[ f (z\u0303)]\u2016 (7) Lemma 1 shows that a vector IPM can be used to bound the change in minimizer at time i and follows from variational inequalities in [13] and the assumption that {\u2207xl(x, \u00b7) : x \u2208 X } \u2282 F .", "startOffset": 300, "endOffset": 304}, {"referenceID": 12, "context": "By exploiting variational inequalities from [13], we can show that \u2016xi \u2212xi\u22121\u2016 \u2264 1 m \u2016\u2207x fi(xi\u22121)\u2212\u2207x fi\u22121(xi\u22121)\u2016 = 1 m \u2016Ezi\u223cpi [ \u2207xl(xi\u22121,zi) ] \u2212Ezi\u22121\u223cpi\u22121 [ \u2207xl(xi\u22121,zi\u22121) ] \u2016 By assumption {\u2207xl(xi\u22121, \u00b7) : x \u2208 X } \u2282 F , so \u2016\u2207x fi(xi\u22121)\u2212\u2207x fi\u22121(xi\u22121)\u2016 = \u2016Ezi\u223cpi [ l(xi\u22121,zi) ] \u2212Ezi\u22121\u223cpi\u22121 [ l(xi\u22121,zi\u22121) ] \u2016 \u2264 sup f\u2208F \u2016Ezi\u223cpi [ f (zi)]\u2212Ezi\u22121\u223cpi\u22121 [ f (zi\u22121)]\u2016 = \u03b3 F (pi, pi\u22121) We cannot compute this vector IPM, since we do not know the distributions pi and pi\u22121.", "startOffset": 44, "endOffset": 48}, {"referenceID": 14, "context": "This allows us to apply standard concentration inequalities for norms of random variables as in [15].", "startOffset": 96, "endOffset": 100}, {"referenceID": 15, "context": "This is a common assumption for in high probability analysis of optimization algorithms as in [16] for example.", "startOffset": 94, "endOffset": 98}, {"referenceID": 16, "context": "By applying Lemma 25 from [17] to the conditional distribution P{\u00b7|Fi\u22121}, we have P {\u2225 \u2225 \u2225 \u2225 \u2225 1 Ki Ki \u2211 k=1 (\u2207xl(x\u0303i,zi(k))\u2212\u2207x fi(x\u0303i)) \u2225 \u2225 \u2225 \u2225 \u2225 > t \u2223 \u2223 \u2223 \u2223 \u2223 Fi\u22121 }", "startOffset": 26, "endOffset": 30}, {"referenceID": 17, "context": "The proof of Lemma 5 is nearly identical to the proof of the extension of Hoeffding\u2019s inequality from [18] with Lemma 22 used instead.", "startOffset": 102, "endOffset": 106}, {"referenceID": 18, "context": "Applying the Chernoff bound [19] and optimizing yields P { n \u2211 i=1 Vi > t }", "startOffset": 28, "endOffset": 32}, {"referenceID": 19, "context": "To apply gradient descent, we use eigenvalue perturbation results [20].", "startOffset": 66, "endOffset": 70}, {"referenceID": 20, "context": "3 from [21] to conclude that there exists a unique, positive fixed point v\u0304 of \u03c6K\u2217(v).", "startOffset": 7, "endOffset": 11}, {"referenceID": 21, "context": "Following [23], for any fixed point v\u0304, it holds that |\u03c6K\u2217(v)\u2212 v\u0304| \u2264 \u03c6 \u2032 K\u2217(v\u0304)|v\u2212 v\u0304| Therefore, applying the fixed point property repeatedly yields |\u03c6 (n) K\u2217 (v)\u2212 v\u0304| \u2264 (\u03c6 \u2032 K\u2217(v\u0304))|v\u2212 v\u0304| By Lemma 14, it holds that \u03c6 \u2032 K\u2217(v\u0304)< 1 and so the result follows.", "startOffset": 10, "endOffset": 14}, {"referenceID": 22, "context": "chosen based on previous economic studies in [25].", "startOffset": 45, "endOffset": 49}, {"referenceID": 23, "context": "We average over twenty runs of our algorithm by resampling without replacement [26].", "startOffset": 79, "endOffset": 83}, {"referenceID": 23, "context": "This is a smoothed version of the hinge loss used in support vector machines (SVM) [26].", "startOffset": 83, "endOffset": 87}], "year": 2015, "abstractText": "A framework is introduced for solving a sequence of slowly changing optimization problems, including those arising in regression and classification applications, using optimization algorithms such as stochastic gradient descent (SGD). The optimization problems change slowly in the sense that the minimizers change at either a fixed or bounded rate. A method based on estimates of the change in the minimizers and properties of the optimization algorithm is introduced for adaptively selecting the number of samples needed from the distributions underlying each problem in order to ensure that the excess risk, i.e., the expected gap between the loss achieved by the approximate minimizer produced by the optimization algorithm and the exact minimizer, does not exceed a target level. Experiments with synthetic and real data are used to confirm that this approach performs well.", "creator": "LaTeX with hyperref package"}}}