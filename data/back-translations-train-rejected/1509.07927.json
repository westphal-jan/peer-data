{"id": "1509.07927", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "26-Sep-2015", "title": "Algorithms for Linear Bandits on Polyhedral Sets", "abstract": "We study stochastic linear optimization problem with bandit feedback. The set of arms take values in an $N$-dimensional space and belong to a bounded polyhedron described by finitely many linear inequalities. We provide a lower bound for the expected regret that scales as $\\Omega(N\\log T)$. We then provide a nearly optimal algorithm and show that its expected regret scales as $O(N\\log^{1+\\epsilon}(T))$ for an arbitrary small $\\epsilon &gt;0$. The algorithm alternates between exploration and exploitation intervals sequentially where deterministic set of arms are played in the exploration intervals and greedily selected arm is played in the exploitation intervals. We also develop an algorithm that achieves the optimal regret when sub-Gaussianity parameter of the noise term is known. Our key insight is that for a polyhedron the optimal arm is robust to small perturbations in the reward function. Consequently, a greedily selected arm is guaranteed to be optimal when the estimation error falls below some suitable threshold. Our solution resolves a question posed by Rusmevichientong and Tsitsiklis (2011) that left open the possibility of efficient algorithms with asymptotic logarithmic regret bounds. We also show that the regret upper bounds hold with probability $1$. Our numerical investigations show that while theoretical results are asymptotic the performance of our algorithms compares favorably to state-of-the-art algorithms in finite time as well.", "histories": [["v1", "Sat, 26 Sep 2015 00:17:38 GMT  (427kb,D)", "http://arxiv.org/abs/1509.07927v1", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["manjesh k hanawal", "amir leshem", "venkatesh saligrama"], "accepted": false, "id": "1509.07927"}, "pdf": {"name": "1509.07927.pdf", "metadata": {"source": "CRF", "title": "Algorithms for Linear Bandits on Polyhedral Sets", "authors": ["Manjesh K Hanawal", "Amir Leshem"], "emails": ["mhanawal@bu.edu", "leshema@eng.biu.ac.il", "srv@bu.edu"], "sections": [{"heading": null, "text": "We study the stochastic linear optimization problem with the help of bandit feedback. The group of arms records values in a N-dimensional space and belongs to a limited polyhedron, which is described by a finite number of linear inequalities. We represent a lower limit for the expected regret scaled in any Small > 0. We then provide an almost optimal algorithm that switches between exploration and exploitation intervals and shows that its expected regret scales in any Small > 0 with O (N log1 + (T). We also present an algorithm that achieves the optimum regret when subgaussian parameters of noise are known. Our key finding is that for a polyhedron, the optimal arm is robust against small disturbances in the reward function. Consequently, a greedy selected arm is guaranteed to be optimal when the estimation error falls below a suitable threshold. Our solution solves a question that logically leaves the possibility of [1] the algorithmic duration open."}, {"heading": "1 Introduction", "text": "This year, it is more than ever in the history of the city, where it is so far that it is a place, where it is a place, where it is a place, where it is a place."}, {"heading": "2 Problem formulation", "text": "We consider a stochastic linear optimization problem with bandit feedback about a set of arms defined by a polyhedron. Let us designate C-RN as a limited group of arms, ranging from C = {x-RN: Ax \u2264 b} (1), where A-RM \u00b7 N, b-RM. In each turn there is a parameter we do not know, so that E [rt], like the expected reward for each arm, is a linear function, regardless of the story. I.e., for each historical Ht, there is a parameter we expect. [\u2212 1] N, fixed but unknown, so that E [rt)."}, {"heading": "3 Main results", "text": "In this section, we set a lower limit on expected regrets and present our proposed policy and the main results in terms of its complexity."}, {"heading": "3.1 Lower Bound", "text": "Using a simple example, we find that the regret of any asymptotically optimal linear bandit algorithm is lower than the number of points (N log T). Let us remember the fundamental property of linear optimization, that an optimal point is always an extreme point. Then, we need the following notations to prove the result. Let's [\u03b2)} \u03b2 [0,1] denote a series of distribution points parameterized by \u03b2 [0, 1] and so that each number (\u03b2) is absolutely continuous in relation to a positive metric m on R. Let's let p (\u03b2) \u03b2 [0,1] \u03b2 [0,1] denote a series of distribution points, and let's KL (\u03b21, \u03b22) denote the difference between the distributions (\u03b2).Let's p (\u03b2) density of probability associated with the distribution (\u03b2), and let's call the KL (\u03b22) the difference between the distributions (\u03b2)."}, {"heading": "3.2 Algorithms", "text": "The basic idea behind our proposal is based on the following observations for linear optimization over a polyhedron. 1) The amount of extreme points of the polyhedron is finite and therefore \u2206 > 0. 2) If we come sufficiently close to the quantity C, then both arg max\u03b8 \u2032 x and arg max \u2082 \u00b2 x give the same value. We take advantage of these observations and propose a two-step technique in which we first estimate the origin of the polyhedron and then exploit it it for a much longer block. This is repeated with increasing block lengths, so that at each point the regret is logarithmic. To simplify the exposure, let us first look at the polyhedron containing the origin and shift the general case to section 6. Suppose that the polyhedron C = {x-RN: Ax \u2264 b} contains the origin as an inner point."}, {"heading": "Algorithm-SEE", "text": "In our policy - which we call Sequential Estimation Exploitation (SEE) - we assume that each time horizon is divided into cycles and each cycle consists of an exploration interval followed by an exploitation interval. - At the end of Ec, with the reward intervals observed for each arm in past c cycles, we calculate ordinary minimum squares (OLS) to estimate each component. - n = 1, 2, N + 1) times. - At the end of Ec, with the rewards observed for each arm in past c cycles, we calculate ordinary minimum squares (OLS) to estimate each component. - n = 1, 2, N + 1) times. - At the end of Ec, as a proxy for a greedy arm x (c). - We calculate a greedy arm x (c) by repeating a linear for 2c 2 / 1 times (all intervals)."}, {"heading": "3.3 Risk Averse Variant", "text": "Our second algorithm - which we call SEE2 - is essentially the same as SEE, with the exception of the length of the exploration intervals, which are exponential rather than super-exponential and do not depend on it. Specifically, we play the greedy arm 2c times per cycle C. Compared to SEE, SEE2 spends significantly more time in the exploration intervals, and therefore the probability of making errors in the exploitation intervals is also significantly lower, and therefore its regret focuses on the expected regret faster. Theorem 3 Let the noise be R-Sub-Gaussian and \u03b8 [\u2212 1, 1] N. Then the expected regret of SEE2 is limited as follows: RT (SEE2) \u2264 2RmN log2 T + 4NRm\u03b32 (8), whereby \u03b32 is a constant that depends on the noise parameter R and the sub-optimality gap."}, {"heading": "4 Optimal Algorithm.", "text": "Next, we get an optimal algorithm that reaches the lower limit in (5) within a constant factor if the sub-Gaussian parameter R is known. 2For the general \u03b8, we replace it with \u03b8-\u03b8- \u221e and the same method works. Only Rm is scaled by a constant factor."}, {"heading": "Algorithm-PolyLin:", "text": "In our next policy - which we call Polyhedrallinear Bandits - we divide the time horizon again into cycles consisting of an exploration interval followed by an exploitation interval as in SEE. As before, we index the cycles according to i and designate the exploration and exploitation intervals in cycle i as egg and Ri as respectively. In the exploration interval egg we play each arm in B once. After c cycles we use the rewards for each arm in B in the past {Ei, i = 12, \u00b7, c} exploration intervals that we calculate to evaluate each component individually."}, {"heading": "5 Regret Analysis", "text": "In this section we prove theorem 2, the proof for theorem 3 follows in a similar manner and is omitted. We first derive the probability of error in estimating each component of \u03b8 in each cycle. Note that in the exploration stage of each cycle cwe-sample of each component znen-B, i = 1, 2, \u00b7 \u00b7, N, 2 times more than that in the exploration stage of the previous cycle. Thus, we have c2-sample of each component zen-B at the end of the cycle c-cycle is stated as follows: Lemma 1 Let the noise R-sub-Gaussian and \u03b4 > 0. In each cycle c of the two SEE and SEE2-sample-B, for all n = 1, \u00b7 \u00b7 \u00b7 \u00b7, N we haveP (c) -n-sample-sample-n after c-cycles the following error is given: Lemma 1 Let the noise R-sub-Gaussian and SEE2-sample-B, for all n = 1, \u00b7 \u00b7 \u00b7 \u00b7, N we haveP (c) -n-sample-sample-n after c-cycles."}, {"heading": "5.1 Regret of SEE.", "text": "Regrets in exploration: At the end of cycle c, each arm in B is played at most 1 x c i = 1 (2i + 1) = c2. The total expected regret from the exploration intervals after c cycles is at most Nc2Rm. Regrets in exploitation: Total expected regret from the exploration intervals after c cycle is4NRm c = 1 2i 2 / (1 +) 2 \u2212 i 2a \u00b2 2 = 4NRm c \u00b2 i = 1 2i 2 / (1 +) \u00b2 2 = 4NRm \u00b2 2 / (1 +) \u00b2 2 = 4NRm \u00b2 2 / (1 +) is a converged series. After c cycles, the total number of games is T = 1 x c i = 1 e i 2 + + + Nc2 \u2265 2 + and we get c2 \u2264 log1 + T."}, {"heading": "5.2 Regret of PolyLin.", "text": "We analyze the regrets in the exploration and exploitation phases separately as follows. Exploration regrets: After exploration cycles, each arm is played in B c-times. The total expected regrets from the exploration intervals after c-cycles is at most NcRm. Exploration regrets: The total expected regrets from the exploration interval after c-cycles is 4NRm c-times. (16) Consider the series GP-3: = GP-2 = 1 2IIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIII"}, {"heading": "6 General Polyhedron", "text": "In this section, we expand the analysis of the previous section to include the case that origin is not an inner point of C. Similar to sentence B, we first define a series of arms located at the boundary of the polyhedron, and these points are calculated with respect to an inner point x of C, which we use as a substitute for origin. We use OPT-1 to find an inner point whose smallest distance to the boundaries along all directions is the largest {e1, e2, \u00b7 \u00b7 eN}. The motivation to maximize the minimum distances to the boundaries comes from lesson 2, where a greater value corresponds to an implicitly lower probability of an estimation error."}, {"heading": "OPT-1:", "text": "(x, y) = arg maxx min i subject to: Ax \u2264 b yi \u2265 0 \u0432i = 1, 2, \u00b7 \u00b7 \u00b7, N A (x + yiei) \u2264 b \u0445i = 1, 2, \u00b7 \u00b7 \u00b7, N A (x \u2212 yiei) \u2264 b \u00b7 i = 1, 2, \u00b7 \u00b7 \u00b7, NOPT-2: (x, y) = arg maxx, y, \u03b1 subject to: \u03b1 > 0; Ax \u2264 b yi \u2212 \u03b1 > 0 \u30fb i = 1, \u00b7, N A (x + yiei). (x)."}, {"heading": "7 Probability 1 Regret Bounds", "text": "In this section, we show that with probability 1, the regret of our algorithms lies within a constant factor of their expected regret. Theorem 5 With probability 1, RT (SEE) is O (N log1 + T) and RT (SEE2) is O (N log2 T). Proof: Let Cn designate an event that we select as a suboptimal arm in the n-th cycle. From term 2, this event is limited as Pr {Cn} \u2264 N exp {\u2212 O (n2)}. Consequently, we play n = 1 Pr {Cn} < \u221e. Now, by applying the Borel-Cantelli problem, we get Pr {lim supn \u2192 \u221e Cn} = 0, which implies that SEE and SEE2 almost certainly play an optimal arm in almost infinitely many cycles. Therefore, the intervals of exploitation only contribute to a limited regret."}, {"heading": "8 Experiments", "text": "In this section we examine the numerical performance of our algorithms against known algorithms. We trace the algorithms back to a hypercube with dimension N = 10. We compare SEE (= 0.3) and SEE2 against UCB normal [20], where we treat each extreme point as an arm of an armed bandit problem. As expected, we execute our algorithms much better. UCB normal we have to evaluate each of the 2N atoms before they learn the right arm."}, {"heading": "9 Conclusion", "text": "We investigated stochastic linear optimization via polyhedral arms with bandit feedback. We provided an asymptotic lower limit for each policy and developed algorithms that are almost asymptotically optimal. The regret of the algorithms grows logarithmically (near) logarithmically in T and its growth rate is linear in the dimension of the polyhedron. We showed that the upper limits of regret are almost certainly valid. The regret growth rate of our algorithms is log1 + T for some > 0. It is interesting to develop strategies that work for = 0 while maintaining the linear growth rate in N."}, {"heading": "Proof of Lemma 1", "text": "Leave ti, n, j the noise as a reward for playing znen in phase i for the jte time. We have summarized the estimation error as follows: P (1), p (2), p (2), p (2), p (2), p (2), p (2), p (2), p (2), p (2), p (2), p (2), p (2), p (2), p (2), p (2), p (2), p (2), p (2), p (2), p (2), p (2), p (2), p (2), p (2), p (2), p (2), p (2), p (2), p (2), p (2), p (2), p (2)."}, {"heading": "Proof of Lemma 2", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "Part a:", "text": "We linked the error of estimation as follows: Pr (approximate). (c). (c). (c). (c). (c). (27). (N). (1). (n). (c). (c). (n). (c). (c). (c). (28). (2). (29). (28)."}, {"heading": "Part b:", "text": "For all x-C applies: | x-\u03b8 (c) \u2212 x-\u03b8 | \u2264 x-\u03b8 (c) \u2212 \u03b8 (c) \u2212 \u03b8 [1]. (30) Define the events A = [x] such that | x-\u03b8 (c) \u2212 x-\u03b8 | > \u03b7} and B = [c) \u2212 \u03b8. The last inequality implies Pr {A} \u2264 Pr {B}. The assertion follows from a part of the problem."}, {"heading": "Part c:", "text": "x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x"}], "references": [], "referenceMentions": [], "year": 2015, "abstractText": "We study stochastic linear optimization problem with bandit feedback. The set of<lb>arms take values in an N -dimensional space and belong to a bounded polyhedron<lb>described by finitely many linear inequalities. We provide a lower bound for the<lb>expected regret that scales as \u03a9(N log T ). We then provide a nearly optimal al-<lb>gorithm that alternates between exploration and exploitation intervals and show<lb>that its expected regret scales as O(N log (T )) for an arbitrary small > 0.<lb>We also present an algorithm than achieves the optimal regret when sub-Gaussian<lb>parameter of the noise is known. Our key insight is that for a polyhedron the op-<lb>timal arm is robust to small perturbations in the reward function. Consequently, a<lb>greedily selected arm is guaranteed to be optimal when the estimation error falls<lb>below some suitable threshold. Our solution resolves a question posed by [1] that<lb>left open the possibility of efficient algorithms with asymptotic logarithmic re-<lb>gret bounds. We also show that the regret upper bounds hold with probability 1.<lb>Our numerical investigations show that while theoretical results are asymptotic the<lb>performance of our algorithms compares favorably to state-of-the-art algorithms<lb>in finite time as well.", "creator": "LaTeX with hyperref package"}}}