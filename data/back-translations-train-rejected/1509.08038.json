{"id": "1509.08038", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "27-Sep-2015", "title": "Deep Trans-layer Unsupervised Networks for Representation Learning", "abstract": "Learning features from massive unlabelled data is a vast prevalent topic for high-level tasks in many machine learning applications. The recent great improvements on benchmark data sets achieved by increasingly complex unsupervised learning methods and deep learning models with lots of parameters usually requires many tedious tricks and much expertise to tune. However, filters learned by these complex architectures are quite similar to standard hand-crafted features visually. In this paper, unsupervised learning methods, such as PCA or auto-encoder, are employed as the building block to learn filter banks at each layer. The lower layer responses are transferred to the last layer (trans-layer) to form a more complete representation retaining more information. In addition, some beneficial methods such as local contrast normalization and whitening are added to the proposed deep trans-layer networks to further boost performance. The trans-layer representations are followed by block histograms with binary encoder schema to learn translation and rotation invariant representations, which are utilized to do high-level tasks such as recognition and classification. Compared to traditional deep learning methods, the implemented feature learning method has much less parameters and is validated in several typical experiments, such as digit recognition on MNIST and MNIST variations, object recognition on Caltech 101 dataset and face verification on LFW dataset. The deep trans-layer unsupervised learning achieves 99.45% accuracy on MNIST dataset, 67.11% accuracy on 15 samples per class and 75.98% accuracy on 30 samples per class on Caltech 101 dataset, 87.10% on LFW dataset.", "histories": [["v1", "Sun, 27 Sep 2015 00:46:08 GMT  (3498kb)", "http://arxiv.org/abs/1509.08038v1", "21 pages, 3 figures"]], "COMMENTS": "21 pages, 3 figures", "reviews": [], "SUBJECTS": "cs.NE cs.CV cs.LG", "authors": ["wentao zhu", "jun miao", "laiyun qing", "xilin chen"], "accepted": false, "id": "1509.08038"}, "pdf": {"name": "1509.08038.pdf", "metadata": {"source": "CRF", "title": "Deep Trans-layer Unsupervised Networks for Representation Learning", "authors": ["Wentao Zhu", "Jun Miao", "Laiyun Qing", "Xilin Chen"], "emails": ["wentao.zhu@vipl.ict.ac.cn", "jmiao@ict.ac.cn", "lyqing@ucas.ac.cn", "xlchen@ict.ac.cn"], "sections": [{"heading": null, "text": "ar Xiv: 150 9.08 038v 1 [cs.N E] 2Learning features from massive unlabelled data is a huge prevalent topic for high level tasks in many machine learning applications. Recent major improvements in benchmark data sets, achieved through increasingly complex unattended learning methods and deep learning models with many parameters, usually require many tedious tricks and a lot of know-how to set them up. The filters learned through these complex architectures are visually very similar to standard manual functions. In this paper, unattended learning methods, such as PCA or auto-encoder, are used as a building block to learn filter banks at each level. Reactions of the lower layer are transferred to the last layer (trans layer) to form a more complete representation that contains more information. In addition, some useful methods such as local contrast normalization and brightening of the surfaces are added to the proposed deep trans networks to further enhance performance."}, {"heading": "1. Introduction", "text": "In fact, we will be able to move into a new world, in which we will be able to create a new world, in which we will be able to create a new world, in which we will be able to create a new world, in which we will be able to create a new world, in which we will be able to create a new world, in which we will be able to create a new world, in which we will be able to create a new world, in which we will be able to create a new world, in which we will be able to create a new world, in which we are in which we are, in which we will be a new world, \"in which we\" we \"we\" we \"we.\""}, {"heading": "2. Related works", "text": "This year, the time has come for an agreement to be reached, and it will only take a few days."}, {"heading": "3. Deep trans-layer unsupervised network", "text": "In this section we present a novel framework, the deep trans-layer unsupervised network, for feature learning and representation. The framework of the proposed deep trans-layer unsupervised network is illustrated in Figure 1. Procedures of the deep trans-layer unsupervised network are similar to other commonly used frameworks in computer monitoring and do not require fine tuning to adapt these local filter banks. In addition, the deep trans-layer unsupervised network uses unsupervised learning methods, such as the PCA or autoencoder, to learn the local receptive filter banks."}, {"heading": "3.1. Representation learning", "text": "The structure of a deep trans-layer unattended network is partially similar to the convolution neural network, where folding operations are performed in small patches. In a deep trans-layer unattended network, local filters are learned through unattended learning, such as PCA or auto-encoders, which does not require fine-tuning by error feedback. In each unattended layer (the first and second layers), the system starts extracting a large number of random patches from unlabeled input images. Imagine that the images used here are all gray images. Each patch has a receptive field size or dimension of k1-by-k2. If the images are color images with d channels, the patch dimension is k1-by-k2-d. Simply process the other d \u2212 1 channels as the following procedures are done step by step. Then a dataset of m patches is constructed, {X = 1 (x)."}, {"heading": "3.1.1. Local contrast normalization and whitening", "text": "In the pre-processing of unattended learning of each layer, we perform several simple operations effectively in the implemented network. The first is local contrast normalization (LCN) [5]. For each local patch x (i) in the extracted patch dataset X, we normalize the patch x (i) by subtracting its mean and dividing by its standard deviation as, y (i) j, k = (i) j, k \u2212 1k2k1, k \u2212 1k2k1, x (i) j = 1k2, x (i) j = 1k2, the operator's own deviation, where the operator's own deviation, the operator's own deviation, the operator's own deviation, the operator's own deviation, the operator's own deviation."}, {"heading": "3.1.2. Trans-layer unsupervised learning", "text": "The first unsupervised level in which the number of unsupervised learning objectives in the first shift is to be achieved is L1 (1), z1 (2), z1 (1), z1 (1), z1 (1), z1 (1), z1 (1), z1 (1), z1 (1), z1 (1), z1 (1), z1 (1), z1 (1), z1 (1), z1 (1), z1 (1), z1 (1), z1 (1), z1 (1), z1 (1), z1 (1), z1 (1), z1 (1), 2 (1), 2 (1), 2 (1), 2 (1), 2 (1), 2 (1), 2 (1), 2 (1), 2 (1), 2 (1), 2 (1), 2 (1), 2 (1), 1 (1), 3 (1), 3 (1), 3 (1), 3 (1), 3 (1), 3 (1, 3 (1), 3 (1), 1 (1 (1), 1 (1), 1 (1, 5, 5, 5, 5, 5, 1, 5 (1, 5), 5 (1, 5), 5 (1, 5, 5, 1, 1, 5, 1, 1, 5 (1), 5 (1), 5 (1, 5), 5 (1, 5), 5 (1, 5, 1, 1, 5 (1), 5 (1), 5 (1), 5 (1, 5 (1), 5 (1), 5 (1, 5), 5 (1, 5 (1, 5), 5 (1), 1, 5 (1, 5 (1), 1, 5 (1), 5 (1), 5 (1, 5 (1), 5 (1, 5, 5, 1, 5, 1, 1, 1, 5 (1), 5 (1), 5 (1), 5 (1), 5 (1), 5 (1, 1, 5, 1, 1, 1,"}, {"heading": "3.1.3. Block-wise histogram", "text": "The third layer of the network is shown in Figure 2."}, {"heading": "3.2. Feature extraction and classification", "text": "After the learning phase for the presentation described above, the uncontrolled folding characteristics of the banks were learned. In view of a test image, the extraction of the function consists in mapping the image to the trans-layered representation with the NB (2 L1) dimensions of these feature banks."}, {"heading": "3.2.1. Convolutional extraction and block-wise histogram", "text": "Considering a test image, we should set the image to zero to maintain the same size between character cards and input image. Then, pre-processing of LCN and whitening is applied to the padded image. Characteristics cards are computed by the first unattended layer with L1 filters of size k1 \u00d7 k2. For the second unattended layer, the procedure is the same as the first layer. First, each character card is provided with a zero pad, and then pre-edited these character cards. Next, these character cards are inserted into the unattended learning of L2 character cards of size k1 \u00d7 k2. Finally, we merge the character cards of the first layer with the character cards of the second layer to create a more complete trans layer representation. The last phase is block-wise histogram. Binary encoding is used to combine these real-rated character cards with the character cards of the second layer, then we compose these character cards into blocks \u2212 1 and then compose them into blocks \u2212 1."}, {"heading": "3.2.2. Classification", "text": "For real-world applications in the next experiments, classifiers or dimension reduction methods will follow deep translational unattended imaging. Due to the relatively high dimensions of this imaging, whitening PCA (WPCA) is used to reduce imaging in object detection tasks. WPCA is performed by conventional PCA weights, which are weighted by reversing their corresponding square root energies. In addition, the deep translational unattended imaging can be used directly to train a classifier for detection tasks. In our digital detection and object detection experiments, a simple linear SVM classifier is used after the deep translational unattended network without matching parameters. The parameter, the cost factor C, in the LIBLINEAR linear SVM software kit used is standard 1 [16]."}, {"heading": "4. Experimental results", "text": "In the experiment, we will validate the performance of a deep trans-layer PCA network (using PCA unattended learning of local receiver characteristics) and a deep trans-layer auto-encoder network (using de-noising auto-encoder unattended learning of local receiver characteristics).The proposed deep trans-layer unattended network has two key phases, LCN in preprocessing and trans-layer concatenation. We will validate the two phases of the MNIST variation data set [17] using the deep trans-layer PCA network. In addition, other parameters such as block size and step size will be selected by cross-validation or validation. Benchmark experiments will be performed using digital detection of MNIST [18] and MNIST variations [17] and object detection of Caltech 101 datasets [7]."}, {"heading": "4.1. Effect of local contrast normalization", "text": "We first validate the effect of LCN followed by conventional PCA in our network. Experiments on deep trans-layer PCA network with LCN and without LCN are performed on the MNIST Variations datasets. The MNIST dataset contains 60,000 training samples and 10,000 test samples of 28 x 28 pixel gray images. The dataset is a subset of NIST that contains handwritten numbers in the real world. Identification targets have been normalized and centered in the images [18]. MNIST variations of datasets are generated by applying simple controllable factor variations on MNIST numbers. The datasets are effective methods to study the inventory of learning methods of representation. Details of detection tasks, numbers of classes and samples are included in the table. Block size, NISS size, strip size and patch size parameters are determined experimentally."}, {"heading": "4.2. Effect of trans-layer connection", "text": "The impact of the trans-layer connection is set on MNIST basic, mnistrotation, mnist-back-edge, mnist-back-image-rotation data sets in the second experiment. Parameters are set as those of Section 4.1. Performance of the deep trans-layer PCA network with and without trans-layer connection is recorded in Table 3.From Table 3, we observe that deep trans-layer PCA networks with translayer connection consistently perform better than those without trans-layer connection. Trans-layer connection increases performance even on the most difficult task of the MNIST basic dataset, and 1% performance for mnist-back-image-rotation data set. The trans-layer connection in deep trans-layer PCA network provides a more complete representation that is helpful for recognition."}, {"heading": "4.4. Object recognition on Caltech 101 data set", "text": "Caltech 101 dataset contains color images belonging to 102 categories, including a background class. The number of images in each class varies from 31 to 800 [7]. Pre-processing of the dataset consists of converting the images to grayscale and adjusting the longer side of the image to 300 with obtained aspect ratio. Two typical tasks are performed, one consisting of a training set of 15 samples per class, the other consisting of a training set of 30 samples per class. Training sets are randomly sampled from Caltech 101, and the rest are test sets. Five test rounds are recorded, and performance is recorded as an average of the five result rounds. Parameters are set as follows: The filter size is set to 7 x 7 pixels, the number of filters is set to L1 = L2 = 8, the block size is set to a quarter of the image size and the size of increments to half the size of the blocks. The WPCA is used to reduce the results of each layer 64."}, {"heading": "4.5. Face verification on LFW-a data set", "text": "The LFW dataset contains more than 13,000 faces of 5,749 different individuals, and these images were collected from the network of unrestricted conditions. In the LFW dataset, the unattended setting for the deep trans-layer PCA network is used to sufficiently validate the effectiveness of the display. LFW-a alignment dataset is used, and we have cropped the face images to 150 x 80 pixels. The standard evaluation protocol for LFW is used for performance evaluation. The size of the histogram is 15 x 13 without overlapping, other parameters are set as before. WPCA is used to reduce the dimension of the trans-layer PCA representation to 3,200 after the additional square root operation in the dataset."}, {"heading": "5. Discussion", "text": "One of the typical cases to illustrate why the representation of the trans layer works is that the local information of the naevi in our faces can disappear in the unattended representation of the top layer for the size of the susceptible fields and local discrimination can be neglected. However, the naevi possesses highly discriminatory features to recognize any person. In conventional deep neural networks, such discriminatory details are also difficult to obtain when the number of filters increases. Presentation of the trans layer from the bottom layer preserves these details and is helpful for detection. The trans layer representation scheme is quite successful for the folding network of PCs and auto-encoder filters because the number of filters is relatively small to preserve enough information for tasks. And information is quickly lost with the increase in layer numbers, as shown in Fig. 3. If the model uses only the characteristic sketches of the second layer as representation, it would give a lot of noise and useful information."}, {"heading": "6. Conclusion", "text": "This paper demonstrates a novel feature learning and representation model, the Deep Trans-Layer Unsupervised Network. Several key elements are added to promote Deep Unsupervised Learning, such as LCN operation and trans-layer representation. Several experiments on digit recognition and object recognition have shown that the proposed method, which relies on layered unsupervised learning of local receptive properties, also achieves impressive results through trans-layer representation. The Deep Trans-Layer Unsupervised Network has a fairly simple structure with very few parameters, where only the cascaded local receptive filters need to be learned through uncontrolled methods."}], "references": [{"title": "Distinctive image features from scale-invariant keypoints", "author": ["D.G. Lowe"], "venue": "International journal of computer vision 60 (2) ", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2004}, {"title": "Histograms of oriented gradients for human detection", "author": ["N. Dalal", "B. Triggs"], "venue": "in: Computer Vision and Pattern Recognition, 2005. CVPR 2005. IEEE Computer Society Conference on, Vol. 1, IEEE", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2005}, {"title": "Reducing the dimensionality of data with neural networks", "author": ["G.E. Hinton", "R.R. Salakhutdinov"], "venue": "Science 313 (5786) ", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2006}, {"title": "What is the best multistage architecture for object recognition", "author": ["K. Jarrett", "K. Kavukcuoglu", "M. Ranzato", "Y. LeCun"], "venue": "in: Computer Vision, 2009 IEEE 12th International Conference on, IEEE", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2009}, {"title": "Learning feature representations with k-means", "author": ["A. Coates", "A.Y. Ng"], "venue": "in: Neural Networks: Tricks of the Trade, Springer", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2012}, {"title": "Learning generative visual models from few training examples: An incremental bayesian approach tested on 101 object categories", "author": ["L. Fei-Fei", "R. Fergus", "P. Perona"], "venue": "Computer Vision and Image Understanding 106 (1) ", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2007}, {"title": "Multiresolution gray-scale and rotation invariant texture classification with local binary patterns", "author": ["T. Ojala", "M. Pietikainen", "T. Maenpaa"], "venue": "Pattern Analysis and Machine Intelligence, IEEE Transactions on 24 (7) ", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2002}, {"title": "Imagenet classification with deep convolutional neural networks", "author": ["A. Krizhevsky", "I. Sutskever", "G.E. Hinton"], "venue": "in: Advances in neural information processing systems", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2012}, {"title": "Regularization of neural networks using dropconnect", "author": ["L. Wan", "M. Zeiler", "S. Zhang", "Y.L. Cun", "R. Fergus"], "venue": "in: Proceedings of the 30th International Conference on Machine Learning (ICML-13)", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2013}, {"title": "Ica with reconstruction cost for efficient overcomplete feature learning", "author": ["Q.V. Le", "A. Karpenko", "J. Ngiam", "A.Y. Ng"], "venue": "in: Advances in Neural Information Processing Systems", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2011}, {"title": "Invariant scattering convolution networks", "author": ["J. Bruna", "S. Mallat"], "venue": "Pattern Analysis and Machine Intelligence, IEEE Transactions on 35 (8) ", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2013}, {"title": "Learning image representations from the pixel level via hierarchical sparse coding", "author": ["K. Yu", "Y. Lin", "J. Lafferty"], "venue": "in: Computer Vision and Pattern Recognition (CVPR), 2011 IEEE Conference on, IEEE", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2011}, {"title": "Learning mid-level features for recognition", "author": ["Y.-L. Boureau", "F. Bach", "Y. LeCun", "J. Ponce"], "venue": "in: Computer Vision and Pattern Recognition (CVPR), 2010 IEEE Conference on, IEEE", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2010}, {"title": "Convolutional deep belief networks for scalable unsupervised learning of hierarchical representations", "author": ["H. Lee", "R. Grosse", "R. Ranganath", "A.Y. Ng"], "venue": "in: Proceedings of the 26th Annual International Conference on Machine Learning, ACM", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2009}, {"title": "Liblinear: A library for large linear classification", "author": ["R.-E. Fan", "K.-W. Chang", "C.-J. Hsieh", "X.-R. Wang", "C.-J. Lin"], "venue": "The Journal of Machine Learning Research 9 ", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2008}, {"title": "An empirical evaluation of deep architectures on problems with many factors of variation", "author": ["H. Larochelle", "D. Erhan", "A. Courville", "J. Bergstra", "Y. Bengio"], "venue": "in: Proceedings of the 24th international conference on Machine learning, ACM", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2007}, {"title": "Gradient-based learning applied to document recognition", "author": ["Y. LeCun", "L. Bottou", "Y. Bengio", "P. Haffner"], "venue": "Proceedings of the IEEE 86 (11) ", "citeRegEx": "18", "shortCiteRegEx": null, "year": 1998}, {"title": "Shape matching and object recognition using shape contexts", "author": ["S. Belongie", "J. Malik", "J. Puzicha"], "venue": "Pattern Analysis and Machine Intelligence, IEEE Transactions on 24 (4) ", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2002}, {"title": "Deformation models for image recognition", "author": ["D. Keysers", "T. Deselaers", "C. Gollan", "H. Ney"], "venue": "Pattern Analysis and Machine Intelligence, IEEE Transactions on 29 (8) ", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2007}, {"title": "Contractive autoencoders: Explicit invariance during feature extraction", "author": ["S. Rifai", "P. Vincent", "X. Muller", "X. Glorot", "Y. Bengio"], "venue": "in: Proceedings of the 28th International Conference on Machine Learning (ICML-11)", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2011}, {"title": "Learning and selecting features jointly with point-wise gated {B} oltzmann machines", "author": ["K. Sohn", "G. Zhou", "C. Lee", "H. Lee"], "venue": "in: Proceedings of The 30th International Conference on Machine Learning", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2013}, {"title": "Learning convolutional feature hierarchies for visual recognition", "author": ["K. Kavukcuoglu", "P. Sermanet", "Y.-L. Boureau", "K. Gregor", "M. Mathieu", "Y.L. Cun"], "venue": "in: Advances in neural information processing systems", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2010}, {"title": "Deconvolutional networks", "author": ["M.D. Zeiler", "D. Krishnan", "G.W. Taylor", "R. Fergus"], "venue": "in: Computer Vision and Pattern Recognition (CVPR), 2010 IEEE Conference on, IEEE", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2010}, {"title": "The hierarchical beta process for convolutional factor analysis and deep learning", "author": ["B. Chen", "G. Polatkan", "G. Sapiro", "L. Carin", "D.B. Dunson"], "venue": "in: Proceedings of the 28th International Conference on Machine Learning (ICML- 11)", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2011}, {"title": "Deep learning of invariant features via simulated fixations in video", "author": ["W. Zou", "S. Zhu", "K. Yu", "A.Y. Ng"], "venue": "in: Advances in Neural Information Processing Systems", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2012}, {"title": "Effective unconstrained face recognition by combining multiple descriptors and learned background statistics", "author": ["L. Wolf", "T. Hassner", "Y. Taigman"], "venue": "Pattern Analysis and Machine Intelligence, IEEE Transactions on 33 (10) ", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2011}, {"title": "Enhanced patterns of oriented edge magnitudes for face recognition and image matching", "author": ["N.-S. Vu", "A. Caplier"], "venue": "Image Processing, IEEE Transactions on 21 (3) ", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2012}, {"title": "Blessing of dimensionality: Highdimensional feature and its efficient compression for face verification", "author": ["D. Chen", "X. Cao", "F. Wen", "J. Sun"], "venue": "in: Computer Vision and Pattern Recognition (CVPR), 2013 IEEE Conference on, IEEE", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2013}, {"title": "Fusing robust face region descriptors via multiple metric learning for face recognition in the wild", "author": ["Z. Cui", "W. Li", "D. Xu", "S. Shan", "X. Chen"], "venue": "in: Computer Vision and Pattern Recognition (CVPR), 2013 IEEE Conference on, IEEE", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2013}, {"title": "F", "author": ["S.U. Hussain", "T. Napol\u00e9on"], "venue": "Jurie, et al., Face recognition using local quantized patterns, in: British Machive Vision Conference", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2012}, {"title": "Fast high dimensional vector multiplication face recognition", "author": ["O. Barkan", "J. Weill", "L. Wolf", "H. Aronowitz"], "venue": "in: Computer Vision ", "citeRegEx": "35", "shortCiteRegEx": null, "year": 1967}], "referenceMentions": [{"referenceID": 0, "context": "Many successful features are proposed such as SIFT [1] and HoG [2] features in computer vision domain.", "startOffset": 51, "endOffset": 54}, {"referenceID": 1, "context": "Many successful features are proposed such as SIFT [1] and HoG [2] features in computer vision domain.", "startOffset": 63, "endOffset": 66}, {"referenceID": 2, "context": "The other way is representation learning, which is a quite prevalent topic after deep learning coming out [3].", "startOffset": 106, "endOffset": 109}, {"referenceID": 3, "context": "In addition, the local contrast normalization [5] and whitening are added in our trans-layer unsupervised network to boost its learning ability, which are commonly used in deep neural networks [6].", "startOffset": 46, "endOffset": 49}, {"referenceID": 4, "context": "In addition, the local contrast normalization [5] and whitening are added in our trans-layer unsupervised network to boost its learning ability, which are commonly used in deep neural networks [6].", "startOffset": 193, "endOffset": 196}, {"referenceID": 5, "context": "98 % accuracy on 30 samples per class on Caltech 101 dataset [7].", "startOffset": 61, "endOffset": 64}, {"referenceID": 6, "context": "Much research has been conducted to pursuit a good representation by manually designing elaborative low-level features, such as LBPH feature [8], SIFT feature [1] and HoG feature [2] in computer vision field.", "startOffset": 141, "endOffset": 144}, {"referenceID": 0, "context": "Much research has been conducted to pursuit a good representation by manually designing elaborative low-level features, such as LBPH feature [8], SIFT feature [1] and HoG feature [2] in computer vision field.", "startOffset": 159, "endOffset": 162}, {"referenceID": 1, "context": "Much research has been conducted to pursuit a good representation by manually designing elaborative low-level features, such as LBPH feature [8], SIFT feature [1] and HoG feature [2] in computer vision field.", "startOffset": 179, "endOffset": 182}, {"referenceID": 7, "context": "Much recent work in machine learning has focused on how to learn good feature representations from massive unlabelled data, and great progresses have been made by the methods [9, 10].", "startOffset": 175, "endOffset": 182}, {"referenceID": 8, "context": "Much recent work in machine learning has focused on how to learn good feature representations from massive unlabelled data, and great progresses have been made by the methods [9, 10].", "startOffset": 175, "endOffset": 182}, {"referenceID": 2, "context": "These deep learning methods typically learn multi-level features by greedily \u201cpre-training\u201d each layer using the specific unsupervised learning, and then fine-tuning the pre-trained features by stochastic gradient descent (SGD) method with supervised information [3], [5].", "startOffset": 263, "endOffset": 266}, {"referenceID": 3, "context": "These deep learning methods typically learn multi-level features by greedily \u201cpre-training\u201d each layer using the specific unsupervised learning, and then fine-tuning the pre-trained features by stochastic gradient descent (SGD) method with supervised information [3], [5].", "startOffset": 268, "endOffset": 271}, {"referenceID": 7, "context": "Besides, the SGD also has various parameters such as momentum, weight decay rate, learning rate, and extra parameters including the Dropout rate or DropConnet rate in recently proposed convolution deep neural networks (ConvNets) [9, 10].", "startOffset": 229, "endOffset": 236}, {"referenceID": 8, "context": "Besides, the SGD also has various parameters such as momentum, weight decay rate, learning rate, and extra parameters including the Dropout rate or DropConnet rate in recently proposed convolution deep neural networks (ConvNets) [9, 10].", "startOffset": 229, "endOffset": 236}, {"referenceID": 4, "context": "There is also some work on conventional unsupervised learning methods with only single layer [6, 11].", "startOffset": 93, "endOffset": 100}, {"referenceID": 9, "context": "There is also some work on conventional unsupervised learning methods with only single layer [6, 11].", "startOffset": 93, "endOffset": 100}, {"referenceID": 4, "context": "Although these methods have made much progress on benchmark datasets with almost no hyper parameters, these single layer unsupervised representational learning models require over complete features of dimensions as high as possible, and the parameters need to be elaborately chosen in order to obtain satisfactory results [6].", "startOffset": 322, "endOffset": 325}, {"referenceID": 10, "context": "Wavelet scattering networks (ScatNet) are such networks with pre-fixed wavelet filter banks in the deep convolution architectures [12].", "startOffset": 130, "endOffset": 134}, {"referenceID": 10, "context": "The PCANet presents a superior or highly comparable performance over other methods such as ScatNet [12], ConvNet [9] and HSC [13], especially in face recognition tasks with large occlusion, illumination, expression and pose changes.", "startOffset": 99, "endOffset": 103}, {"referenceID": 7, "context": "The PCANet presents a superior or highly comparable performance over other methods such as ScatNet [12], ConvNet [9] and HSC [13], especially in face recognition tasks with large occlusion, illumination, expression and pose changes.", "startOffset": 113, "endOffset": 116}, {"referenceID": 11, "context": "The PCANet presents a superior or highly comparable performance over other methods such as ScatNet [12], ConvNet [9] and HSC [13], especially in face recognition tasks with large occlusion, illumination, expression and pose changes.", "startOffset": 125, "endOffset": 129}, {"referenceID": 12, "context": "The procedures of the deep trans-layer unsupervised network is similar to other commonly used frameworks in computer vision [14] and feature learning work [15]", "startOffset": 124, "endOffset": 128}, {"referenceID": 13, "context": "The procedures of the deep trans-layer unsupervised network is similar to other commonly used frameworks in computer vision [14] and feature learning work [15]", "startOffset": 155, "endOffset": 159}, {"referenceID": 3, "context": "The first is local contrast normalization (LCN) [5].", "startOffset": 48, "endOffset": 51}, {"referenceID": 6, "context": "The third layer is quite similar to that of LBPH [8].", "startOffset": 49, "endOffset": 52}, {"referenceID": 14, "context": "The parameter, the cost factor C, in the used linear SVM software kit LIBLINEAR is 1 as default [16].", "startOffset": 96, "endOffset": 100}, {"referenceID": 15, "context": "We will validate the two phases in the MNIST variations data set [17] using the deep tran-layer PCA network.", "startOffset": 65, "endOffset": 69}, {"referenceID": 16, "context": "Benchmark experiments are conducted on digit recognition of MNIST [18] and", "startOffset": 66, "endOffset": 70}, {"referenceID": 16, "context": "Table 1: Details of the 9 used data sets for digit recognition on MNIST [18] and MNIST variations [17] Data sets Recognition tasks #Classes #Train-Test MNIST Handwritten digits from 0 to 9 10 60000-10000 mnist-basic Smaller subset of MNIST 10 12000-50000 mnist-rot Same as basic with rotation 10 12000-50000 mnist-back-rand Same as basic with random background 10 12000-50000 mnist-back-image Same as basic with image background 10 12000-50000 mnist-rot-back-image Same as basic with rotation and image background 10 12000-50000 rectangles Tall or wide rectangles 2 1200-50000 rectangles-image Same as rect.", "startOffset": 72, "endOffset": 76}, {"referenceID": 15, "context": "Table 1: Details of the 9 used data sets for digit recognition on MNIST [18] and MNIST variations [17] Data sets Recognition tasks #Classes #Train-Test MNIST Handwritten digits from 0 to 9 10 60000-10000 mnist-basic Smaller subset of MNIST 10 12000-50000 mnist-rot Same as basic with rotation 10 12000-50000 mnist-back-rand Same as basic with random background 10 12000-50000 mnist-back-image Same as basic with image background 10 12000-50000 mnist-rot-back-image Same as basic with rotation and image background 10 12000-50000 rectangles Tall or wide rectangles 2 1200-50000 rectangles-image Same as rect.", "startOffset": 98, "endOffset": 102}, {"referenceID": 15, "context": "MNIST variations [17] data sets, and object recognition of Caltech 101 data set [7].", "startOffset": 17, "endOffset": 21}, {"referenceID": 5, "context": "MNIST variations [17] data sets, and object recognition of Caltech 101 data set [7].", "startOffset": 80, "endOffset": 83}, {"referenceID": 16, "context": "The recognition targets have been size-normalized and centered in the images [18].", "startOffset": 77, "endOffset": 81}, {"referenceID": 15, "context": "MNIST variations data sets are created by applying simple controllable factor variations on MNIST digits [17].", "startOffset": 105, "endOffset": 109}, {"referenceID": 15, "context": "The validation sets are typically partitioned from the training set in consistence with the related work [17].", "startOffset": 105, "endOffset": 109}, {"referenceID": 14, "context": "A simple linear SVM with default parameters is connected to the deep trans-layer PCA representation to do recognition task [16].", "startOffset": 123, "endOffset": 127}, {"referenceID": 3, "context": "Digit recognition on MNIST and MNIST variations data sets We report the performance of the implemented model onMNIST andMNIST variations data sets compared with other methods such as convolution network (ConvNet) [5] and ScatNet-2 [12].", "startOffset": 213, "endOffset": 216}, {"referenceID": 10, "context": "Digit recognition on MNIST and MNIST variations data sets We report the performance of the implemented model onMNIST andMNIST variations data sets compared with other methods such as convolution network (ConvNet) [5] and ScatNet-2 [12].", "startOffset": 231, "endOffset": 235}, {"referenceID": 17, "context": "Methods MNIST error rates (%) K-NN-SCM [19] 0.", "startOffset": 39, "endOffset": 43}, {"referenceID": 18, "context": "63 K-NN-IDM [20] 0.", "startOffset": 12, "endOffset": 16}, {"referenceID": 13, "context": "54 CDBN [15] 0.", "startOffset": 8, "endOffset": 12}, {"referenceID": 11, "context": "82 HSC [13] 0.", "startOffset": 7, "endOffset": 11}, {"referenceID": 3, "context": "77 ConvNet [5] 0.", "startOffset": 11, "endOffset": 14}, {"referenceID": 10, "context": "45 ScatNet-2 (SVMrbf) [12] 0.", "startOffset": 22, "endOffset": 26}, {"referenceID": 19, "context": "CAE-2 [23] 2.", "startOffset": 6, "endOffset": 10}, {"referenceID": 20, "context": "50 - PGBM +DN-1 [25] - - 6.", "startOffset": 16, "endOffset": 20}, {"referenceID": 10, "context": "ScatNet-2 (SVMrbf) [12] 1.", "startOffset": 19, "endOffset": 23}, {"referenceID": 14, "context": "It is worthy to mention that the performance of ScatNet-2 is achieved by connected with a non-linear SVM with RBF kernels with tuned parameters, but our model is connected with a linear SVM with all default parameters in LIBLINEAR software kit [16].", "startOffset": 244, "endOffset": 248}, {"referenceID": 3, "context": "53) on MNIST data set [5].", "startOffset": 22, "endOffset": 25}, {"referenceID": 13, "context": "Methods 15 samples per class (%) 30 samples per class (%) CDBN [15] 57.", "startOffset": 63, "endOffset": 67}, {"referenceID": 21, "context": "50 ConvNet [26] 57.", "startOffset": 11, "endOffset": 15}, {"referenceID": 22, "context": "50 DeconvNet [27] 58.", "startOffset": 13, "endOffset": 17}, {"referenceID": 23, "context": "[28] 58.", "startOffset": 0, "endOffset": 4}, {"referenceID": 24, "context": "[29] 66.", "startOffset": 0, "endOffset": 4}, {"referenceID": 11, "context": "50 HSC [13] 74.", "startOffset": 7, "endOffset": 11}, {"referenceID": 5, "context": "The number of each class\u2019s images varies from 31 to 800 [7].", "startOffset": 56, "endOffset": 59}, {"referenceID": 14, "context": "Linear SVM with default parameters in LIBLINEAR [16] is used to tackle the recognition task.", "startOffset": 48, "endOffset": 52}, {"referenceID": 26, "context": "Methods Verification accuracy (%) POEM [31] 82.", "startOffset": 39, "endOffset": 43}, {"referenceID": 27, "context": "LBP [32] 84.", "startOffset": 4, "endOffset": 8}, {"referenceID": 27, "context": "LE [32] 84.", "startOffset": 3, "endOffset": 7}, {"referenceID": 28, "context": "58 SFRD [33] 84.", "startOffset": 8, "endOffset": 12}, {"referenceID": 29, "context": "81 I-LQP [34] 86.", "startOffset": 9, "endOffset": 13}, {"referenceID": 30, "context": "46 OCLBP [35] 86.", "startOffset": 9, "endOffset": 13}, {"referenceID": 11, "context": "our model with no elaborately tuned parameters gets about 2% upper than that of HSC [13] on 30 samples per class task.", "startOffset": 84, "endOffset": 88}, {"referenceID": 25, "context": "Face verification on LFW-a data set Face verification task is conducted with our model on LFW-a data set [30].", "startOffset": 105, "endOffset": 109}, {"referenceID": 3, "context": "So the future work could include that other translation and rotation invariance preserving methods to reduce the dimensions of the trans-layer representations such as pooling operations [5, 18, 21].", "startOffset": 186, "endOffset": 197}, {"referenceID": 16, "context": "So the future work could include that other translation and rotation invariance preserving methods to reduce the dimensions of the trans-layer representations such as pooling operations [5, 18, 21].", "startOffset": 186, "endOffset": 197}], "year": 2015, "abstractText": "Learning features from massive unlabelled data is a vast prevalent topic for highlevel tasks in many machine learning applications. The recent great improvements on benchmark data sets achieved by increasingly complex unsupervised learning methods and deep learning models with lots of parameters usually requires many tedious tricks and much expertise to tune. However, filters learned by these complex architectures are quite similar to standard hand-crafted features visually. In this paper, unsupervised learning methods, such as PCA or auto-encoder, are employed as the building block to learn filter banks at each layer. The lower layer responses are transferred to the last layer (trans-layer) to form a more complete representation retaining more information. In addition, some beneficial methods such as local contrast normalization and whitening are added to the proposed deep trans-layer networks to further boost performance. The trans-layer representations are followed by block histograms with binary encoder schema to learn translation and rotation invariant representations, which are utilized to do high-level tasks such as recognition and classification. Compared to traditional deep learning methods, the implemented feature learning method has much less parameters and is validated in several typical experiments, such as digit recognition on MNIST and MNIST variations, object recognition on Caltech 101 dataset, face verification on LFW dataset. The deep trans-layer unsupervised learning achieves 99.45 % accuracy on MNIST dataset, 67.11 % accuracy on 15 samples per class and 75.98 % accuracy on 30 samples per class on Caltech 101 dataset, 87.10 % on LFW dataset.", "creator": "LaTeX with hyperref package"}}}