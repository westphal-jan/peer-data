{"id": "1509.08062", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "27-Sep-2015", "title": "End-to-End Text-Dependent Speaker Verification", "abstract": "In this paper we present a data-driven, integrated approach to speaker verification, which maps a test utterance and a few reference utterances directly to a single score for verification and jointly optimizes the system's components using the same evaluation protocol and metric as at test time. Such an approach will result in simple and efficient systems, requiring little domain-specific knowledge and making few model assumptions. We implement the idea by formulating the problem as a single neural network architecture, including the estimation of a speaker model on only a few utterances, and evaluate it on our internal \"Ok Google\" benchmark for text-dependent speaker verification. The proposed approach appears to be very effective for big data applications like ours that require highly accurate, easy-to-maintain systems with a small footprint.", "histories": [["v1", "Sun, 27 Sep 2015 07:43:36 GMT  (473kb)", "http://arxiv.org/abs/1509.08062v1", "submitted to ICASSP 2016"]], "COMMENTS": "submitted to ICASSP 2016", "reviews": [], "SUBJECTS": "cs.LG cs.SD", "authors": ["georg heigold", "ignacio moreno", "samy bengio", "noam shazeer"], "accepted": false, "id": "1509.08062"}, "pdf": {"name": "1509.08062.pdf", "metadata": {"source": "CRF", "title": "End-to-End Text-Dependent Speaker Verification", "authors": ["Georg Heigold", "Ignacio Moreno", "Samy Bengio", "Noam Shazeer"], "emails": ["georg.heigold@dfki.de", "elnota@google.com", "bengio@google.com", "noam@google.com"], "sections": [{"heading": null, "text": "ar Xiv: 150 9.08 062v 1 [cs.L G] 27 Sep 2015"}, {"heading": "1. Introduction", "text": "In fact, most of them will be able to play by the rules they have established in the past, and they will be able to play by the rules they have established in the past."}, {"heading": "2. Speaker Verification Protocol", "text": "The standard verification protocol can be divided into three steps: training, enrollment, and evaluation, which we describe in more detail below. Training In the training phase, we find a suitable internal speaker representation from the enunciation that allows a simple scoring function. Generally, this representation depends on the type of model (e.g. Gaussian subspace model or deep neural network), the representation level (e.g. frame or enunciation), and the model training loss (e.g. maximum probability or softmax). State-of-the-art representations are a summary of frame information, such as i-vectors [7, 8] and d-vectors (section 3).Enrollment In the introductory phase, a speaker delivers a few statements (see Table 1) that are used to estimate a speaker model."}, {"heading": "3. D-Vector Baseline Approach", "text": "In fact, it is a reactionary, reactionary, reactionary and reactionary group in which the reactionary forces and the reactionary forces unite."}, {"heading": "4. End-To-End Speaker Verification", "text": "In this case, it is as if it were a reactionary act capable of blackmailing itself."}, {"heading": "5. Experimental Evaluation", "text": "We evaluate the proposed end-to-end approach based on our internal \"OK Google\" benchmark."}, {"heading": "5.1. Data Sets & Basic Setup", "text": "(We tested the proposed end-to-end approach on a set? Ok Google? utterances collected from anonymized language search logs. To improve the robustness of the noise, we conducted multi-style training. The data was supplemented by the artificial addition of auto and cafeteria sounds to various SNRs and the simulation of different distances between speaker and microphone, see [2] for more details. Enrollment and evaluation data contain only real data. Table 1 shows some statistical data set data. Statements are forcibly aligned to obtain the \"Ok Google\" snippet. The average length of these snippet is about 80 frames, at a frame rate of 100 Hz. Based on this observation, we extracted the last 80 frames from each snippet, possibly padding or cutting frames at the beginning of the snippet. The frames consist of 40 log filter banks (with some basic DNA we associate with).4"}, {"heading": "5.2. Frame-Level vs. Utterance-Level Representation", "text": "First, we compare frame level and enunciation level loudspeaker representations, see Table 2. Here, we use a DNN, as described in Figure 1, with a softmax layer and train on pull 2M (Table 1) with 50% dropouts [24] in the linear layer. The enunciation level approach exceeds the enunciation level approach by 30%. Normalization of the score always results in a significant power boost (up to 20% relative). For comparison, two i-vector baselines with 150 eigenvoices are shown. The first baseline is based on [6] and uses 13 PLPs with first and second order derivatives, 1024 Gausers and 300-dimensional i-vectors. The second baseline is based on [25] with 150 eigenvoices. The i-vector + PLDA baseline should be taken with a grain of salt, as the PLDA model was trained on only a subset of the 2M tensile dataset (450 per loudspeaker and 450 expressions)."}, {"heading": "5.3. Softmax vs. End-to-End Loss", "text": "Next, we compare the Softmax loss (section 2) and the end-to-end loss (section 4) for the formation of speakers. Table 3 shows the same error rates for the DNN in Figure 1. We have no gain from the dropout rate for the end-to-end loss. Similarly, t-normalization helps for the dropout rate, but not for the end-to-end loss. This result is consistent with the degree of dropout rate and the rating for the end-to-end loss."}, {"heading": "5.4. Feedforward vs. Recurrent Neural Networks", "text": "So far, we have focused on the \"Small Footprint\" DNN in Figure 1. The results are in Table 4. Compared to the Small Footprint DNN, the \"Best\" DNN uses an additional hidden layer and yields a relative gain of 10%. The LSTM in Figure 3 yields a further gain of 30% over this best DNN. The number of parameters is comparable to the DNN, but the LSTM contains about ten times more multiplications and additions. More hyperparameter tuning will hopefully further reduce the computational complexity to make it feasible. Slightly worse error rates are achieved with the Softmax loss (through t-normalization, candidate sampling, dropout and possibly early stopping, which were not necessary for the end-to-end approach)."}, {"heading": "6. Summary & Conclusion", "text": "We proposed a novel end-to-end approach to speaker verification that maps the utterance directly to a score and collectively optimizes the internal speaker representation and speaker model, using the same loss for training and evaluation. Provided sufficient training data, the proposed approach improved our best DNN baseline with a small footprint from over 3% to 2% with the same error rate on our internal \"OK Google\" benchmark. The greatest gain came from the utterance level compared to frame level modeling. Compared to other losses, the end-to-end loss achieved the same or slightly better results, but with fewer additional concepts. In the case of Softmax, for example, we only achieved comparable error rates when we applied score normalization at runtime, candidate sampling to make training feasible, and termination in training. In addition, we showed that the same error rate with a neural network is vector, but can be lowered further to a reasonable 1.4%, rather than a simple recursive network."}, {"heading": "7. References", "text": "[1] H. Aronowitz, R. Hoory, J. W. Pelecchel, and D. Na-hamoo, \"New Developments in voice biometrics for user authentication,\" in Interspeech, Florence, Italy, August 2011, pp. 17 - 20. [2] R. Prabhavalkar, R. Alvarez, C. Parada, P. Nakkiran, and T. Sainath, \"Automatic gain control and multi-style training for robust small-footprint keyword spotting with deep neural networks,\" in IEEE International Conference on Acoustics, Speech, and Signal Processing (ICASSP), Brisbane, Australia, Apr. 2015, pp. 4704-4708. [3] J. Schalkwyk, D. Beechel, F. Beaufays, B. Byrne, C. Chelba, M. Cohen, and B. Kamvar, and B. Strope, \"Your word is my command.\""}], "references": [{"title": "New developments in voice biometrics for user authentication", "author": ["H. Aronowitz", "R. Hoory", "J.W. Pelecanos", "D. Nahamoo"], "venue": "Interspeech, Florence, Italy, Aug. 2011, pp. 17 \u2013 20.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2011}, {"title": "Automatic gain control and multi-style training for robust small-footprint keyword spotting with deep neural networks", "author": ["R. Prabhavalkar", "R. Alvarez", "C. Parada", "P. Nakkiran", "T. Sainath"], "venue": "IEEE International Conference on Acoustics, Speech, and Signal Processing (ICASSP), Brisbane, Australia, Apr. 2015, pp. 4704\u20134708.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2015}, {"title": "Your word is my command\u201d: Google search by voice: A case study", "author": ["J. Schalkwyk", "D. Beeferman", "F. Beaufays", "B. Byrne", "C. Chelba", "M. Cohen", "M. Kamvar", "B. Strope"], "venue": "Advances in Speech Recognition: Mobile Environments, Call Centers and Clinics. Springer, 2010, ch. 4, pp. 61\u201390.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2010}, {"title": "Locallyconnected and convolutional neural networks for small footprint speaker recognition", "author": ["Y. Chen", "I. Lopez-Moreno", "T. Sainath"], "venue": "Interspeech, Dresden, Germany, Sep. 2015.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2015}, {"title": "Bayesian speaker verification with heavy-tailed priors", "author": ["P. Kenny"], "venue": "Proc. Odyssey Speaker and Language Recognition Workshop, Brno, Czech Republic, Jul. 2010.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2010}, {"title": "Front-end factor analysis for speaker verification", "author": ["N. Dehak", "P. Kenny", "R. Dehak", "P. Dumouchel", "P. Ouellet"], "venue": "IEEE Transactions on Audio, Speech, and Language Processing, vol. 19, no. 4, pp. 788\u2013798, 2011.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2011}, {"title": "Speaker verification using adapted Gaussian mixture models", "author": ["D. Reynolds", "T. Quoter", "R. Dunn"], "venue": "Digital Signal Processing, vol. 10, no. 1, pp. 19\u201341, 2000.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2000}, {"title": "Joint factor analysis versus eigenchannels in speaker recognition", "author": ["P. Kenny", "G. Boulianne", "P. Ouellet", "P. Dumouchel"], "venue": "IEEE Transactions on Audio, Speech, and Language Processing, vol. 15, pp. 1435\u20131447, 2007.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2007}, {"title": "Text-dependent speaker verification using a small development set", "author": ["H. Aronowitz"], "venue": "Proc. Odyssey Speaker and Language Recognition Workshop, Singapore, Jun. 2012.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2012}, {"title": "Text-dependent speaker recognition using PLDA with uncertainty propagation", "author": ["T. Stafylakis", "P. Kenny", "P. Ouellet", "P. Perez", "J. Kockmann", "P. Dumouchel"], "venue": "Interspeech, Lyon, France, Aug. 2013.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2013}, {"title": "Phoneticallyconstrained PLDA modeling for text-dependent speaker verification with multiple short utterances", "author": ["A. Larcher", "K.-A. Lee", "B. Ma", "H. Li"], "venue": "IEEE International Conference on Acoustics, Speech, and Signal Processing (ICASSP), Vancouver, Canada, May 2013.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2013}, {"title": "Improving speaker recognition performance in the domain adaptation challenge using deep neural networks", "author": ["D. Garcia-Romero", "X. Zhang", "A. McCree", "D. Povey"], "venue": "IEEE Spoken Language Technology Workshop (SLT), South Lake Tahoie, NV, USA, Dec. 2014, pp. 378\u2013383.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2014}, {"title": "A novel scheme for speaker recognition using a phoneticallyaware deep neural network", "author": ["Y. Lei", "N. Scheffer", "L. Ferrer", "M. McLaren"], "venue": "IEEE International Conference on Acoustics, Speech, and Signal Processing (ICASSP), Florence, Italy, May 2014, pp. 1695\u20131699.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2014}, {"title": "Deep neural network approaches to speaker and language recognition", "author": ["F. Richardson", "D. Reynolds", "N. Dehak"], "venue": "IEEE Signal Processing Letters, 2005.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2005}, {"title": "Deep neural networks for small footprint text-dependent speaker verification", "author": ["E. Variani", "X. Lei", "E. McDermott", "I. Lopez-Moreno", "J. Gonzalez-Dominguez"], "venue": "IEEE International Conference on Acoustics, Speech, and Signal Processing (ICASSP), Florence, Italy, May 2014.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2014}, {"title": "Speaker recognition with recurrent neural networks", "author": ["S. Parveen", "A. Qadeer", "P. Green"], "venue": "Sixth International Conference on Spoken Language Processing, ICSLP 2000 / INTERSPEECH 2000, Beijing, China, Oct 2000, pp. 16\u2013 20.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2000}, {"title": "Automatic language identification using long short-term memory recurrent neural networks", "author": ["J. Gonzalez-Dominguez", "I. Lopez-Moreno", "H. Sak", "J. Gonzalez-Rodriguez", "P. Moreno"], "venue": "Interspeech, Singapore, Sep. 2014, pp. 2155\u20132159.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2014}, {"title": "Deep unfolding: Model-based inspiration of novel deep architectures", "author": ["J. Hershey", "J.L. Roux", "F. Weninger"], "venue": "CoRR, vol. abs/1409.2574, 2014. [Online]. Available: http://arxiv.org/abs/1409.2574", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2014}, {"title": "The NIST 2014 speaker recognition i-vector machine learning challenge", "author": ["C. Greenberg", "D. Bans\u00e9", "G. Doddington", "D. Garcia- Romero", "J. Godfrey", "T. Kinnunen", "A. Martin", "A. Mc- Cree", "M. Przybocki", "D. Reynolds"], "venue": "Odyssey 2014: The Speaker and Language Recognition Workshop, Joensuu, Finland, Jun. 2014.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2014}, {"title": "On using very large target vocabulary for neural machine translation", "author": ["S. Jean", "K. Cho", "R. Memisevic", "Y. Bengio"], "venue": "CoRR, vol. abs/1412.2007, 2014. [Online]. Available: http://arxiv.org/abs/1412.2007", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2007}, {"title": "Long short-term memory", "author": ["S. Hochreiter", "J. Schmidhuber"], "venue": "Neural Computation, vol. 9, no. 8, pp. 1735\u2013 1780, 1997.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 1997}, {"title": "Long short-term memory recurrent neural network architectures for large scale acoustic modeling", "author": ["H. Sak", "A. Senior", "F. Beaufays"], "venue": "Interspeech, Singapore, Sep. 2014.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2014}, {"title": "Score normalization for text-independent speaker verification systems", "author": ["R. Auckenthaler", "M. Carey", "H. Lloyd-Thomas"], "venue": "Digital Signal Processing, vol. 10, no. 1-3, pp. 42\u201354, 2000.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2000}, {"title": "Improving neural networks by preventing co-adaptation of feature detectors", "author": ["G. Hinton", "N. Srivastava", "A. Krizhevsky", "I. Sutskever", "R. Salakhutdinov"], "venue": "CoRR, vol. abs/1207.0580, 2012. [Online]. Available: http://arxiv.org/abs/1207.0580", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2012}, {"title": "Analysis of ivector length normalization in speaker recognition systems", "author": ["D. Garcia-Romero", "C. Espy-Wilson"], "venue": "Interspeech, Florence, Italy, Aug. 2011.", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2011}], "referenceMentions": [{"referenceID": 0, "context": "By constraining the lexicon, text-dependent speaker verification aims to compensate for phonetic variability, which poses a significant challenge in speaker verification [1].", "startOffset": 170, "endOffset": 173}, {"referenceID": 1, "context": "6 seconds long global password relates to the Google Keyword Spotting system [2] and Google VoiceSearch [3] and facilitates the combination of the systems.", "startOffset": 77, "endOffset": 80}, {"referenceID": 2, "context": "6 seconds long global password relates to the Google Keyword Spotting system [2] and Google VoiceSearch [3] and facilitates the combination of the systems.", "startOffset": 104, "endOffset": 107}, {"referenceID": 3, "context": "This paper focuses on text-dependent speaker verification for small footprint systems, as discussed in [4].", "startOffset": 103, "endOffset": 106}, {"referenceID": 4, "context": "For example, the combination of i-vector and probabilistic linear discriminant analysis (PLDA) [5, 6] has become the dominant approach, both for text-independent speaker verification [7, 8, 5, 6] and text-dependent speaker verification [9, 10, 11].", "startOffset": 95, "endOffset": 101}, {"referenceID": 5, "context": "For example, the combination of i-vector and probabilistic linear discriminant analysis (PLDA) [5, 6] has become the dominant approach, both for text-independent speaker verification [7, 8, 5, 6] and text-dependent speaker verification [9, 10, 11].", "startOffset": 95, "endOffset": 101}, {"referenceID": 6, "context": "For example, the combination of i-vector and probabilistic linear discriminant analysis (PLDA) [5, 6] has become the dominant approach, both for text-independent speaker verification [7, 8, 5, 6] and text-dependent speaker verification [9, 10, 11].", "startOffset": 183, "endOffset": 195}, {"referenceID": 7, "context": "For example, the combination of i-vector and probabilistic linear discriminant analysis (PLDA) [5, 6] has become the dominant approach, both for text-independent speaker verification [7, 8, 5, 6] and text-dependent speaker verification [9, 10, 11].", "startOffset": 183, "endOffset": 195}, {"referenceID": 4, "context": "For example, the combination of i-vector and probabilistic linear discriminant analysis (PLDA) [5, 6] has become the dominant approach, both for text-independent speaker verification [7, 8, 5, 6] and text-dependent speaker verification [9, 10, 11].", "startOffset": 183, "endOffset": 195}, {"referenceID": 5, "context": "For example, the combination of i-vector and probabilistic linear discriminant analysis (PLDA) [5, 6] has become the dominant approach, both for text-independent speaker verification [7, 8, 5, 6] and text-dependent speaker verification [9, 10, 11].", "startOffset": 183, "endOffset": 195}, {"referenceID": 8, "context": "For example, the combination of i-vector and probabilistic linear discriminant analysis (PLDA) [5, 6] has become the dominant approach, both for text-independent speaker verification [7, 8, 5, 6] and text-dependent speaker verification [9, 10, 11].", "startOffset": 236, "endOffset": 247}, {"referenceID": 9, "context": "For example, the combination of i-vector and probabilistic linear discriminant analysis (PLDA) [5, 6] has become the dominant approach, both for text-independent speaker verification [7, 8, 5, 6] and text-dependent speaker verification [9, 10, 11].", "startOffset": 236, "endOffset": 247}, {"referenceID": 10, "context": "For example, the combination of i-vector and probabilistic linear discriminant analysis (PLDA) [5, 6] has become the dominant approach, both for text-independent speaker verification [7, 8, 5, 6] and text-dependent speaker verification [9, 10, 11].", "startOffset": 236, "endOffset": 247}, {"referenceID": 11, "context": "Hybrid approaches that include deep learning based components have also proved to be beneficial for text-independent speaker recognition [12, 13, 14].", "startOffset": 137, "endOffset": 149}, {"referenceID": 12, "context": "Hybrid approaches that include deep learning based components have also proved to be beneficial for text-independent speaker recognition [12, 13, 14].", "startOffset": 137, "endOffset": 149}, {"referenceID": 13, "context": "Hybrid approaches that include deep learning based components have also proved to be beneficial for text-independent speaker recognition [12, 13, 14].", "startOffset": 137, "endOffset": 149}, {"referenceID": 14, "context": "For small footprint systems, however, a more direct deep learning modeling may be an attractive alternative [15, 4].", "startOffset": 108, "endOffset": 115}, {"referenceID": 3, "context": "For small footprint systems, however, a more direct deep learning modeling may be an attractive alternative [15, 4].", "startOffset": 108, "endOffset": 115}, {"referenceID": 15, "context": "To the best of our knowledge, recurrent neural networks have been applied to related problems such as speaker identification [16] and language identification [17], but not to the speaker verification task.", "startOffset": 125, "endOffset": 129}, {"referenceID": 16, "context": "To the best of our knowledge, recurrent neural networks have been applied to related problems such as speaker identification [16] and language identification [17], but not to the speaker verification task.", "startOffset": 158, "endOffset": 162}, {"referenceID": 17, "context": "The proposed neural network architecture can be thought of as joint optimization of a generative-discriminative hybrid and is in the same spirit as deep unfolding [18] for adaptation.", "startOffset": 163, "endOffset": 167}, {"referenceID": 6, "context": "State-of-the art representations are a summary of frame-level information, such as i-vectors [7, 8] and d-vectors (Section 3).", "startOffset": 93, "endOffset": 99}, {"referenceID": 7, "context": "State-of-the art representations are a summary of frame-level information, such as i-vectors [7, 8] and d-vectors (Section 3).", "startOffset": 93, "endOffset": 99}, {"referenceID": 18, "context": "A common choice is to average the i-vectors [19] or d-vectors [15, 4] of these utterances.", "startOffset": 44, "endOffset": 48}, {"referenceID": 14, "context": "A common choice is to average the i-vectors [19] or d-vectors [15, 4] of these utterances.", "startOffset": 62, "endOffset": 69}, {"referenceID": 3, "context": "A common choice is to average the i-vectors [19] or d-vectors [15, 4] of these utterances.", "startOffset": 62, "endOffset": 69}, {"referenceID": 3, "context": "It includes a locally-connected layer [4] and several fully connected layers.", "startOffset": 38, "endOffset": 41}, {"referenceID": 19, "context": "The complexity issue (but not the estimation issue) can be alleviated by candidate sampling [20].", "startOffset": 92, "endOffset": 96}, {"referenceID": 6, "context": "For example, GMM-UBM [7] or i-vector models does not directly optimize a verification problem; the PLDA [5] model is not followed a refinement of the i-vector extraction; or long contextual features may be ignored by frame-based GMM-UBM models [7].", "startOffset": 21, "endOffset": 24}, {"referenceID": 4, "context": "For example, GMM-UBM [7] or i-vector models does not directly optimize a verification problem; the PLDA [5] model is not followed a refinement of the i-vector extraction; or long contextual features may be ignored by frame-based GMM-UBM models [7].", "startOffset": 104, "endOffset": 107}, {"referenceID": 6, "context": "For example, GMM-UBM [7] or i-vector models does not directly optimize a verification problem; the PLDA [5] model is not followed a refinement of the i-vector extraction; or long contextual features may be ignored by frame-based GMM-UBM models [7].", "startOffset": 244, "endOffset": 247}, {"referenceID": 20, "context": "1 and 3: a deep neural network (DNN) with locally-connected and fully connected layers as our baseline DNN in Section 3 and a long short-term memory recurrent neural network (LSTM) [21, 22] with a single output.", "startOffset": 181, "endOffset": 189}, {"referenceID": 21, "context": "1 and 3: a deep neural network (DNN) with locally-connected and fully connected layers as our baseline DNN in Section 3 and a long short-term memory recurrent neural network (LSTM) [21, 22] with a single output.", "startOffset": 181, "endOffset": 189}, {"referenceID": 1, "context": "The data were augmented by artificially adding in car and cafeteria noise at various SNRs, and simulating different distances between the speaker and the microphone, see [2] for further details.", "startOffset": 170, "endOffset": 173}, {"referenceID": 22, "context": "Results are reported in terms of equal error rate (EER), without and with t-norm score normalization [23].", "startOffset": 101, "endOffset": 105}, {"referenceID": 23, "context": "1 with a softmax layer and trained on train 2M (Table 1) with 50% dropout [24] in the linear layer.", "startOffset": 74, "endOffset": 78}, {"referenceID": 5, "context": "frame i-vector [6] 5.", "startOffset": 15, "endOffset": 18}, {"referenceID": 24, "context": "11 i-vector+PLDA [25] 4.", "startOffset": 17, "endOffset": 21}, {"referenceID": 3, "context": "89 DNN, softmax [4] 3.", "startOffset": 16, "endOffset": 19}, {"referenceID": 5, "context": "The first baseline is based on [6], and uses 13 PLPs with first-order and second-order derivatives, 1024 Gaussians, and 300-dimensional i-vectors.", "startOffset": 31, "endOffset": 34}, {"referenceID": 24, "context": "The second baseline is based on [25] with 150 eigenvoices.", "startOffset": 32, "endOffset": 36}, {"referenceID": 9, "context": "Also, this baseline does not include other refining techniques such as \u201duncertainty training\u201d [10] that have", "startOffset": 94, "endOffset": 98}, {"referenceID": 14, "context": "Note that compared to [15], we have improved our d-vectors significantly [4].", "startOffset": 22, "endOffset": 26}, {"referenceID": 3, "context": "Note that compared to [15], we have improved our d-vectors significantly [4].", "startOffset": 73, "endOffset": 76}, {"referenceID": 19, "context": "To reasonably scale the softmax layer to 80k speaker labels, we employed candidate sampling, similar to [20].", "startOffset": 104, "endOffset": 108}], "year": 2015, "abstractText": "In this paper we present a data-driven, integrated approach to speaker verification, which maps a test utterance and a few reference utterances directly to a single score for verification and jointly optimizes the system\u2019s components using the same evaluation protocol and metric as at test time. Such an approach will result in simple and efficient systems, requiring little domainspecific knowledge and making few model assumptions. We implement the idea by formulating the problem as a single neural network architecture, including the estimation of a speaker model on only a few utterances, and evaluate it on our internal \u201dOk Google\u201d benchmark for text-dependent speaker verification. The proposed approach appears to be very effective for big data applications like ours that require highly accurate, easy-to-maintain systems with a small footprint.", "creator": "gnuplot 4.6 patchlevel 6"}}}