{"id": "1509.08067", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "27-Sep-2015", "title": "Online Object Tracking, Learning and Parsing with And-Or Graphs", "abstract": "This paper presents a method, called \\textit{AOGTracker}, for simultaneously tracking, learning and parsing (TLP) objects in video sequences with a hierarchical and compositional And-Or graph (AOG) representation. In our TLP framework, the AOG explores latent part configurations to represent a target object. The TLP is formulated in the Bayesian framework and a spatial-temporal dynamic programming (DP) algorithm is derived to infer object bounding boxes on the fly. During online learning, the AOG is discriminatively trained in the latent structural SVM framework to account for the appearance (e.g., lighting and partial occlusion) and structural (e.g., different poses and viewpoints) variations of the object, as well as the distractors (e.g., similar objects) in the background scene. Three key issues in online learning are addressed: (i) maintaining the purity of positive and negative datasets collected online with the help from the spatial-temporal DP algorithm, (ii) controling the model complexity in latent structure learning, and (iii) identifying the critical moments to re-learn the structure of AOG based on its intrackability. The intrackability measures the uncertainty of the AOG based on its score maps. In experiments, our AOGTracker is tested in two main tracking benchmarks with the same parameter setting: the TB-100/50/CVPR2013 benchmarks, and the VOT benchmarks --- VOT 2013, 2014, 2015 and TIR2015 (thermal imagery tracking). In the former, our AOGTracker outperforms state-of-the-art tracking algorithms including two trackers based on deep convolutional network. In the latter, our AOGTracker outperforms all other trackers in VOT2013 and is comparable to the state-of-the-art methods in VOT2014 (comparison results of VOT2015 and VOT-TIR2015 will be released by the benchmark authors at the VOT2015 workshop in conjunction with ICCV2015).", "histories": [["v1", "Sun, 27 Sep 2015 08:14:57 GMT  (4221kb,D)", "https://arxiv.org/abs/1509.08067v1", "14 pages"], ["v2", "Fri, 6 May 2016 01:10:42 GMT  (5270kb,D)", "http://arxiv.org/abs/1509.08067v2", "18 pages"], ["v3", "Wed, 11 May 2016 19:35:37 GMT  (6034kb,D)", "http://arxiv.org/abs/1509.08067v3", "18 pages"], ["v4", "Sun, 15 May 2016 06:02:48 GMT  (6397kb,D)", "http://arxiv.org/abs/1509.08067v4", "17 pages"], ["v5", "Wed, 18 May 2016 05:44:50 GMT  (5495kb,D)", "http://arxiv.org/abs/1509.08067v5", "17 pages"], ["v6", "Sat, 3 Sep 2016 00:26:58 GMT  (7977kb,D)", "http://arxiv.org/abs/1509.08067v6", "17 pages, Reproducibility: The source code is released with this paper for reproducing all results, which is available atthis https URL"]], "COMMENTS": "14 pages", "reviews": [], "SUBJECTS": "cs.CV cs.LG", "authors": ["tianfu wu", "yang lu", "song-chun zhu"], "accepted": false, "id": "1509.08067"}, "pdf": {"name": "1509.08067.pdf", "metadata": {"source": "CRF", "title": "Online Object Tracking, Learning and Parsing with And-Or Graphs", "authors": ["Tianfu Wu", "Yang Lu", "Song-Chun Zhu"], "emails": ["wu@ncsu.edu"], "sections": [{"heading": null, "text": "Index Terms - Visual Tracking, And-Or Graphs, Latent SVM, Dynamic Programming, IntrackabilityF"}, {"heading": "1 INTRODUCTION", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "1.1 Motivation and Objective", "text": "In fact, it is in such a way that most of them are able to survive themselves without there being a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process"}, {"heading": "1.2 Method Overview", "text": "In fact, we are able to go in search of a solution that puts us in the position we are in and in which we go in search of a solution."}, {"heading": "2 RELATED WORK", "text": "In fact, most people who are able to survive themselves are not able to survive themselves, \"he told the German Press Agency in an interview with the Frankfurter Allgemeine Sonntagszeitung (FAZ)."}, {"heading": "3 PROBLEM FORMULATION", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "3.1 Formulation of Online Object Tracking", "text": "In this section, we first derive a generic formulation from a generative perspective within the Bajesian framework, and then the discriminatory counterpart."}, {"heading": "3.1.1 Tracking with HMM", "text": "Identify the screen on which video images are defined. (1) Identify a sequence of video images within the time period (1, T) by, I1: T = {I1, \u00b7 \u00b7, IT}. (1) Identify by Bt the delimitation box of a target object in E flat. (2) The motion model: Bt | Bt \u2212 1 (Bt \u2212 p) becomes by a tracker (t), (3) the probability: Bt \u00b2 p (Es | Bt). (4) Then the prediction model is defined by, p \u2212 1: t (Bt \u2212 1) the ability (Bt \u2212 p \u2212 1)."}, {"heading": "3.1.2 Tracking as Energy Minimization over Trajectories", "text": "To derive the discriminatory formulation of Eqn. (10), we show that only the log probability ratio plays a role in the calculation (Ii | Bi) in Eqn. (10) with very mild assumptions. \u2212 Let us be the image object occupied by a tracked object, and the remaining costs (i.e., Bi, Bi, Bi, Bi, Bi, Bi, Bi, Bi, Bi, Bi, Bi, Bi, Bi, Bi, Bi, Bi, Bi, Bi, Bi, Bi, Bi, Bi, Bi, Bi, Bi, Bi, Bi, Bi, Bi, Bi, Bi, Bi, Bi, Bi, Bi, Bi, Bi, Bi, Bi."}, {"heading": "3.2 Quantizing the Space of Part Configurations", "text": "In this context, we first present the construction of a complete structure that quantifies the space of the sub-configuration. [2] And then we ask ourselves whether it is a complete system. [3] Are we looking at a complete system of the sub-configuration. [4] Are we looking at the structure of the sub-configuration? [5] Are we looking at the structure of the sub-configuration? [5] Are we looking at the structure of the sub-configuration? [6] Are we looking at the structure of the sub-configuration? [7] Are we looking at the structure of the sub-configuration of the sub-configuration? \"[7] Are we looking at the structure of the sub-configuration of the sub-configuration?\" [7] Are we looking at the structure of the sub-configuration of the sub-configuration? \"[8]"}, {"heading": "4 TRACKING-BY-PARSING WITH OBJECT AOGS", "text": "In this section we present details of inference with object AOGs. First, we define scoring functions of nodes in an AOG. Then we introduce a spatial DP algorithm for calculating score (Ii | Bi; G) and a temporal DP algorithm for calculating trajectory B * 2: t in Equation (15)."}, {"heading": "4.1 Scoring Functions of Nodes in an AOG", "text": "s use p = (l, x, y) for positioning the pyramid. (VT, VAnd, VOr, E) (e.g., the left side in Fig.6), we define four types of edges, i.e., E = ET, EDef, EDec, ESwitch as shown in Fig.6. We elaborate the definitions of parameters to be applied in Fig.6. (D) Each terminal node T has appearance parameters that are used to ground a terminal node to image data. i) The parent and node A of the terminal node edge with deformation parameters."}, {"heading": "4.2 Tracking-by-Parsing", "text": "(15).Spatial DP: The DP algorithm (see algorithm 2) consists of two stages: (i) The bottom-up pass calculates score card pyramids (as shown in Figure 6). (ii) In the top-down pass we first find all candidate positions for the root node Or-node O based on its score maps and current thresholds of the object AOG, denoted bycand = {p; Score (O, p | F). (20) Then, following the BFS order of the nodes, we retrieve the optimal parse tree at each P node."}, {"heading": "4.3 The Trackability of an Object AOG", "text": "In order to detect critical moments online, we need to measure the quality of an object AOG, G at a given time t. We calculate its traceability based on the score maps in which the optimal parse tree is placed. For each node v in the score tree, we have its position in the score map pyramid (i.e., the level of the pyramid and the position in that plane), (lv, xv, yv). We define the traceability of the node v by, trackability (v | It, G) = S (lv, xv, yv) \u2212 \u00b5S (24), where S (lv, xv, yv) is the score of the node v, \u00b5S is the mean score calculated from the entire score map. Intuitively, we expect that the score map of a discriminating node v has peak and steep landscape, as examined in [51] the traceability of an object to relate to the structural traceability of a local object."}, {"heading": "5 ONLINE LEARNING OF OBJECT AOGS", "text": "In this section, we present online learning of object AOGs, which consists of three components: (i) maintaining a training data set based on tracking results; (ii) estimating the parameters of a particular object AOG; and (iii) learning structure of object AOG by truncating the complete structure AOG, which is (ii) required in this process."}, {"heading": "5.1 Maintaining the Training Dataset Online", "text": "In the first frame, we have D + 1 = {(I1, B1)} and allow B1 = (x1, y1, w1, h1). We supplement it with eight locally shifted positives, i.e., {I1, B1, i; i = 1, \u00b7 \u00b7 \u00b7, 8}, where x1, i-x1 \u00b1 d} and y1, i-yy \u00b1 d} with unchanged width and height. The initial D \u2212 1 uses the entire remaining image I.B1 for the removal of hard negatives in training. At a time t, when Bt is valid according to tracking-by-parsing, we have D + t = D + t \u2212 1 (It, Bt) and D-qt (Nqt) in relation to all other candidates that are not suppressed by D.n."}, {"heading": "5.2 Estimating Parameters of a Given Object AOG", "text": "We use the latent SVM method (LSVM) [1]. Based on the scoring functions defined in Section 4.1, we can rewrite the scoring function of applying a given object AOG, G to a training example (referred to as IB for simplicity reasons), Score (IB; G) = max pt."}, {"heading": "5.3 Learning Object AOGs", "text": "This year, it has reached the point where it will be able to put itself at the top of the list."}, {"heading": "6 EXPERIMENTS", "text": "In this section we present comparative results on the TB50 / 100 / CVPR2013 benchmarks [2], [3] and the VOT benchmarks [4]. We also analyze various aspects of our method. Source code 5 is released with this paper for reproducing all results. We refer to the proposed method of AOG in tables and plots.Parameter Setting. We use the same parameters for all experiments as we emphasize online learning in this paper. In learning object AOGs, the side length of the network used for constructing the full structure AOG is either 3 or 4 depending on the length of the inputbounding box (to reduce the time complexity of online learning).The number of intervals in the computing pyramid is set to 6 with the cell size. The factor s in the computing search ROI is set to sROI."}, {"heading": "6.1 Results on TB-50/100/CVPR2013", "text": "In fact, it is in such a way that we are able to put ourselves into another world, in which we put ourselves into another world, in which we are able to put ourselves into another world, in which we are able to put ourselves into another world, in which we are able to put ourselves into another world, in which we are able to put ourselves into another world, in which we have forced ourselves into another world, in which we are able to put ourselves into another world, in which we are able to put ourselves into a world, in which we have forced ourselves into a world, in which we have forced ourselves into a world, in which we live in which we have forced ourselves into a world, in which we live in which we have forced ourselves into another world, in which we live in which we live, in which we live, in which we live, in which we live, in which we live, in which we live, in which we live, in which we live, in which we live, in which we live, in which we live, in which we live, in which we live, in which we live, in which we live, in which we live, in which we live, in which we live, in which we live, in which we live, in which we live, in which we live, in which we live, in which we live, in which we live, in which we live, in which we live, in which we live, in which we live, in which we live, in which we live, in which we live, in which we live, in which we live, in which we live, in which we live, in which we live, in which we, in which we live, in which we live, in which we live, in which we live, in which we live, in which we live, in which we live, in which we live, in which we live, in which we live, in which we live, in which we, in which we live, in which we live, in which we, in which we live, in which we, in which we, in which we, in which we live, in which we, in which we, in which we live, in which we, in which we, in which we live, in which we live, in which we, in which we, in which we, in which we, in which we, in which we, in which we, in which we,"}, {"heading": "6.2 Analyses of AOG models and the TLP Algorithm", "text": "To analyze the contributions of different components in our AOGTracker, we compare the performance of six different variants - three different object rendering schemes: AOG with and without relearning the structure (referred to as AOG or AOGFixed) and only the entire object template (i.e. without partial configurations, referred to as ObjectOnly) and two different inference rates for each rendering scheme: inference with and without temporal DP (referred to as -st or -s). As mentioned above, we use a very simple setting for temporal DP that takes into account \u2206 t = 5 frames, [t \u2212 5, t] in our experiments. Figure 12 shows the performance comparison of the six variants. AOG-st achieves the best overall performance uniformly. Trackers with AOG perform better than those with temporal templates. AOG structure rendering shows a consistent time performance improvement in our experiments. But we observed that AOGFixed-st works slightly better than AOG object rendering is slightly better than AOG-out, only slightly better than two object rendering is sufficient."}, {"heading": "6.3 Comparison with State-of-the-Art Methods", "text": "We explain why our AOGTracker outperforms other trackers on the TB-100 benchmark in terms of representation, online learning and inference. Our AOGTracker uses three types of complementary traits (HOG + LBP + Color) to jointly capture manifestations, while most other trackers use simpler ones (e.g. TLD [17] uses intensity based on hair-like traits). More importantly, we address the problem of learning optimal part-based configurations in the quantified space of latent object structures, while most other trackers either focus on entire objects [58] or implicit configurations (e.g. the random fern forest used in the TLD). These two components are integrated into a latent structured, discriminatory learning framework system that improves overall tracking performance (e.g. comparability in the mapping)."}, {"heading": "6.4 Results on VOT", "text": "In VOT, evaluation focuses on short-term tracking (i.e., a tracker is not expected to lose a target), so the evaluation toolkit will re-initiate a tracker after it hits the target multiple times. Performance is calculated in terms of accuracy and robustness, while the number of failures falls to zero."}, {"heading": "7 DISCUSSION AND FUTURE WORK", "text": "We have presented a Tracking, Learning and Parsing (TLP) framework and derived a Spatial Dynamic Programming (DP) and a DP temporal algorithm for online object tracking with AOGs. We have also presented a method of online learning object AOGs, including its structure and parameters. In experiments, we test our method on two central public benchmark data sets and experimental results that have better or comparable performance. In our ongoing work, we examine more flexible computing schemes when tracking with AOGs. Naturally, the compositional property embedded in an AOG leads to different bottom-up / topdown arithmetic schemes, such as the three computing processes examined by Wu and Zhu. We can track an object by directly adapting the object template (i.e., the compositional property embedded in an AOG, naturally leads to different bottom-up / top-down arithmetic schemes of Zu, as the three arithmetic schemas the Wu and the three are examined)."}, {"heading": "ACKNOWLEDGMENTS", "text": "This work is supported by the DARPA SIMPLEX Award N6600115-C-4035, the ONR MURI Grant N00014-16-1-2007 and NSF IIS-1423305. T. Wu was also supported by the ECE Startup Fund 201473-02119 at NCSU. We thank Steven Holtzen for proofreading this paper. We also thank the NVIDIA Corporation for their support by donating a GPU."}], "references": [{"title": "Object detection with discriminatively trained part-based models", "author": ["P. Felzenszwalb", "R. Girshick", "D. McAllester", "D. Ramanan"], "venue": "PAMI, vol. 32, no. 9, pp. 1627\u20131645, 2010.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2010}, {"title": "Object tracking benchmark", "author": ["Y. Wu", "J. Lim", "M.-H. Yang"], "venue": "PAMI, vol. 37, no. 9, pp. 1834\u20131848, 2015.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 1834}, {"title": "Online object tracking: A benchmark", "author": ["\u2014\u2014"], "venue": "CVPR, 2013.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2013}, {"title": "A novel performance evaluation methodology for single-target trackers", "author": ["M. Kristan", "J. Matas", "A. Leonardis", "T. Vojir", "R.P. Pflugfelder", "G. Fern\u00e1ndez", "G. Nebehay", "F. Porikli", "L. Cehovin"], "venue": "CoRR, vol. abs/1503.01313, 2015. [Online]. Available: http://arxiv.org/abs/ 1503.01313", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2015}, {"title": "Robust visual tracking via convolutional networks", "author": ["K. Zhang", "Q. Liu", "Y. Wu", "M.-H. Yang"], "venue": "arXiv preprint arXiv:1501.04505v2, 2015.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2015}, {"title": "Transferring rich feature hierarchies for robust visual tracking", "author": ["N. Wang", "S. Li", "A. Gupta", "D.-Y. Yeung"], "venue": "arXiv preprint arXiv:1501.04587v2, 2015.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2015}, {"title": "The Origin of Concepts", "author": ["S. Carey"], "venue": null, "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2011}, {"title": "Discriminatively trained and-or tree models for object detection", "author": ["X. Song", "T. Wu", "Y. Jia", "S.-C. Zhu"], "venue": "CVPR, 2013.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2013}, {"title": "Object detection with grammar models", "author": ["R. Girshick", "P. Felzenszwalb", "D. McAllester"], "venue": "NIPS, 2011.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2011}, {"title": "Object detection grammars", "author": ["P. Felzenszwalb", "D. McAllester"], "venue": "University of Chicago, Computer Science TR-2010-02, Tech. Rep., 2010.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2010}, {"title": "A stochastic grammar of images", "author": ["S.C. Zhu", "D. Mumford"], "venue": "Foundations and Trends in Computer Graphics and Vision, vol. 2, no. 4, pp. 259\u2013362, 2006.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2006}, {"title": "POP: patchwork of parts models for object recognition", "author": ["Y. Amit", "A. Trouv\u00e9"], "venue": "IJCV, vol. 75, no. 2, pp. 267\u2013282, 2007.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2007}, {"title": "Object tracking: A survey", "author": ["A. Yilmaz", "O. Javed", "M. Shah"], "venue": "ACM Comput. Surv., vol. 38, no. 4, 2006.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2006}, {"title": "Readings in speech recognition", "author": ["L.R. Rabiner"], "venue": "A. Waibel and K.-F. Lee, Eds., 1990, ch. A Tutorial on Hidden Markov Models and Selected Applications in Speech Recognition, pp. 267\u2013296.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 1990}, {"title": "Condensation - conditional density propagation for visual tracking", "author": ["M. Isard", "A. Blake"], "venue": "IJCV, vol. 29, no. 1, pp. 5\u201328, 1998.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 1998}, {"title": "People-tracking-by-detection and people-detection-by-tracking", "author": ["M. Andriluka", "S. Roth", "B. Schiele"], "venue": "CVPR, 2008.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2008}, {"title": "Tracking-learning-detection", "author": ["Z. Kalal", "K. Mikolajczyk", "J. Matas"], "venue": "PAMI, vol. 34, no. 7, pp. 1409\u20131422, 2012.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2012}, {"title": "Self-paced learning for long-term tracking", "author": ["J.S. Supancic III", "D. Ramanan"], "venue": "CVPR, 2013.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2013}, {"title": "N-best maximal decoder for part models", "author": ["D. Park", "D. Ramanan"], "venue": "ICCV, 2011.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2011}, {"title": "Diverse m-best solutions in markov random fields", "author": ["D. Batra", "P. Yadollahpour", "A. Guzm\u00e1n-Rivera", "G. Shakhnarovich"], "venue": "ECCV, 2012.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2012}, {"title": "Global data association for multi-object tracking using network flows", "author": ["L. Zhang", "Y. Li", "R. Nevatia"], "venue": "CVPR, 2008.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2008}, {"title": "Globally-optimal greedy algorithms for tracking a variable number of objects", "author": ["H. Pirsiavash", "D. Ramanan", "C.C. Fowlkes"], "venue": "CVPR, 2011.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2011}, {"title": "Multiple object tracking using k-shortest paths optimization", "author": ["J. Berclaz", "F. Fleuret", "E. T\u00fcretken", "P. Fua"], "venue": "PAMI, vol. 33, no. 9, pp. 1806\u2013 1819, 2011.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 1806}, {"title": "An efficient implementation of a scaling minimum-cost flow algorithm", "author": ["A.V. Goldberg"], "venue": "J. Algorithms, vol. 22, no. 1, pp. 1\u201329, 1997.", "citeRegEx": "24", "shortCiteRegEx": null, "year": 1997}, {"title": "Visual tracking by sampling tree-structured graphical models", "author": ["S. Hong", "B. Han"], "venue": "ECCV, 2014.", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2014}, {"title": "Orderless tracking through modelaveraged posterior estimation", "author": ["S. Hong", "S. Kwak", "B. Han"], "venue": "ICCV, 2013.", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2013}, {"title": "Part-based visual tracking with online latent structural learning", "author": ["R. Yao", "Q. Shi", "C. Shen", "Y. Zhang", "A. van den Hengel"], "venue": "CVPR, 2013.", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2013}, {"title": "Online graph-based tracking", "author": ["H. Nam", "S. Hong", "B. Han"], "venue": "ECCV, 2014.", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2014}, {"title": "Incremental learning for robust visual tracking", "author": ["D.A. Ross", "J. Lim", "R.-S. Lin", "M.-H. Yang"], "venue": "IJCV, vol. 77, no. 1-3, pp. 125\u2013141, 2008.  ARXIV VERSION  17", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2008}, {"title": "Kernel-based object tracking", "author": ["D. Comaniciu", "V. Ramesh", "P. Meer"], "venue": "PAMI, vol. 25, no. 5, pp. 564\u2013575, 2003.", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2003}, {"title": "Robust visual tracking and vehicle classification via sparse representation", "author": ["X. Mei", "H. Ling"], "venue": "PAMI, vol. 33, no. 11, pp. 2259\u20132272, 2011.", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2011}, {"title": "Incremental learning of 3d-dct compact representations for robust visual tracking", "author": ["X. Li", "A.R. Dick", "C. Shen", "A. van den Hengel", "H. Wang"], "venue": "PAMI, vol. 35, no. 4, pp. 863\u2013881, 2013.", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2013}, {"title": "Learning multi-domain convolutional neural networks for visual tracking", "author": ["H. Nam", "B. Han"], "venue": "CVPR, 2016.", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2016}, {"title": "ImageNet: A Large-Scale Hierarchical Image Database", "author": ["J. Deng", "W. Dong", "R. Socher", "L.-J. Li", "K. Li", "L. Fei-Fei"], "venue": "CVPR, 2009.", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2009}, {"title": "Highly nonrigid object tracking via patch-based dynamic appearance modeling", "author": ["J. Kwon", "K.M. Lee"], "venue": "PAMI, vol. 35, no. 10, pp. 2427\u20132441, 2013.", "citeRegEx": "35", "shortCiteRegEx": null, "year": 2013}, {"title": "Robust visual tracking using an adaptive coupled-layer visual model", "author": ["L. Cehovin", "M. Kristan", "A. Leonardis"], "venue": "PAMI, vol. 35, no. 4, pp. 941\u2013 953, 2013.", "citeRegEx": "36", "shortCiteRegEx": null, "year": 2013}, {"title": "Visual tracking via adaptive structural local sparse appearance model", "author": ["X. Jia", "H. Lu", "M.-H. Yang"], "venue": "CVPR, 2012.", "citeRegEx": "37", "shortCiteRegEx": null, "year": 2012}, {"title": "Support vector tracking", "author": ["S. Avidan"], "venue": "PAMI, vol. 26, no. 8, pp. 1064\u2013 1072, 2004.", "citeRegEx": "38", "shortCiteRegEx": null, "year": 2004}, {"title": "Robust object tracking with online multiple instance learning", "author": ["B. Babenko", "M.-H. Yang", "S. Belongie"], "venue": "PAMI, vol. 33, no. 8, pp. 1619\u20131632, 2011.", "citeRegEx": "39", "shortCiteRegEx": null, "year": 2011}, {"title": "Struck: Structured output tracking with kernels", "author": ["S. Hare", "A. Saffari", "P.H.S. Torr"], "venue": "ICCV, 2011.", "citeRegEx": "40", "shortCiteRegEx": null, "year": 2011}, {"title": "Exploiting the circulant structure of tracking-by-detection with kernels", "author": ["J. Henriques", "R. Caseiro", "P. Martins", "J. Batista"], "venue": "ECCV, 2012.", "citeRegEx": "41", "shortCiteRegEx": null, "year": 2012}, {"title": "Biologically inspired object tracking using center-surround saliency mechanisms", "author": ["V. Mahadevan", "N. Vasconcelos"], "venue": "PAMI, vol. 35, no. 3, pp. 541\u2013554, 2013.", "citeRegEx": "42", "shortCiteRegEx": null, "year": 2013}, {"title": "Part-based visual tracking with online latent structural learning", "author": ["R. Yao", "Q. Shi", "C. Shen", "Y. Zhang", "A. van den Hengel"], "venue": "CVPR, 2013.", "citeRegEx": "43", "shortCiteRegEx": null, "year": 2013}, {"title": "Structure preserving object tracking", "author": ["L. Zhang", "L. van der Maaten"], "venue": "CVPR, 2013.", "citeRegEx": "44", "shortCiteRegEx": null, "year": 2013}, {"title": "Good feature to track", "author": ["J. Shi", "C. Tomasi"], "venue": "CVPR, 1994.", "citeRegEx": "45", "shortCiteRegEx": null, "year": 1994}, {"title": "Online object tracking, learning and parsing with and-or graphs", "author": ["Y. Lu", "T. Wu", "S.-C. Zhu"], "venue": "CVPR, 2014.", "citeRegEx": "46", "shortCiteRegEx": null, "year": 2014}, {"title": "A survey of appearance models in visual object tracking", "author": ["X. Li", "W. Hu", "C. Shen", "Z. Zhang", "A.R. Dick", "A. van den Hengel"], "venue": "CoRR, vol. abs/1303.4803, 2013.", "citeRegEx": "47", "shortCiteRegEx": null, "year": 2013}, {"title": "Lucas-kanade 20 years on: A unifying framework", "author": ["S. Baker", "I. Matthews"], "venue": "IJCV, vol. 56, no. 3, pp. 221\u2013255, 2004.", "citeRegEx": "48", "shortCiteRegEx": null, "year": 2004}, {"title": "Histograms of oriented gradients for human detection", "author": ["N. Dalal", "B. Triggs"], "venue": "CVPR, 2005.", "citeRegEx": "49", "shortCiteRegEx": null, "year": 2005}, {"title": "Performance evaluation of texture measures with classification based on kullback discrimination of distributions", "author": ["T. Ojala", "M. Pietikainen", "D. Harwood"], "venue": "ICPR, 1994.", "citeRegEx": "50", "shortCiteRegEx": null, "year": 1994}, {"title": "Highly nonrigid object tracking via patch-based dynamic appearance modeling", "author": ["J. Kwon", "K.M. Lee"], "venue": "TPAMI, vol. 35, no. 10, pp. 2427\u20132441, 2013.", "citeRegEx": "51", "shortCiteRegEx": null, "year": 2013}, {"title": "Intrackability: Characterizing video statistics and pursuing video representations", "author": ["H. Gong", "S.C. Zhu"], "venue": "IJCV, vol. 97, no. 3, pp. 255\u2013275, 2012.", "citeRegEx": "52", "shortCiteRegEx": null, "year": 2012}, {"title": "A limited memory algorithm for bound constrained optimization", "author": ["R.H. Byrd", "P. Lu", "J. Nocedal", "C. Zhu"], "venue": "SIAM J. Sci. Comput., vol. 16, no. 5, pp. 1190\u20131208, 1995.", "citeRegEx": "53", "shortCiteRegEx": null, "year": 1995}, {"title": "Exact acceleration of linear object detectors", "author": ["C. Dubout", "F. Fleuret"], "venue": "ECCV, 2012.", "citeRegEx": "54", "shortCiteRegEx": null, "year": 2012}, {"title": "Visual tracking via adaptive structural local sparse appearance model", "author": ["X. Jia", "H. Lu", "M.-H. Yang"], "venue": "CVPR, 2012.", "citeRegEx": "55", "shortCiteRegEx": null, "year": 2012}, {"title": "Beyond semi-supervised tracking: Tracking should be as simple as detection, but not simpler than recognition", "author": ["S. Stalder", "H. Grabner", "L. van Gool"], "venue": "ICCV Workshop, 2009.", "citeRegEx": "56", "shortCiteRegEx": null, "year": 2009}, {"title": "Color-based probabilistic tracking", "author": ["P. P\u00e9rez", "C. Hue", "J. Vermaak", "M. Gangnet"], "venue": "ECCV, 2002.", "citeRegEx": "57", "shortCiteRegEx": null, "year": 2002}, {"title": "Exploiting the circulant structure of tracking-by-detection with kernels", "author": ["J.F. Henriques", "R. Caseiro", "P. Martins", "J. Batista"], "venue": "ECCV, 2012.", "citeRegEx": "58", "shortCiteRegEx": null, "year": 2012}, {"title": "Fast compressive tracking", "author": ["K. Zhang", "L. Zhang", "M. Yang"], "venue": "PAMI, vol. 36, no. 10, pp. 2002\u20132015, 2014.", "citeRegEx": "59", "shortCiteRegEx": null, "year": 2002}, {"title": "Context tracker: Exploring supporters and distracters in unconstrained environments", "author": ["T.B. Dinh", "N. Vo", "G.G. Medioni"], "venue": "CVPR, 2011.", "citeRegEx": "60", "shortCiteRegEx": null, "year": 2011}, {"title": "Distribution fields for tracking", "author": ["L. Sevilla-Lara", "E. Learned-Miller"], "venue": "CVPR, 2012.", "citeRegEx": "61", "shortCiteRegEx": null, "year": 2012}, {"title": "Robustifying the flock of trackers", "author": ["T. Vojir", "J. Matas"], "venue": "Computer Vision Winter Workshop, 2011.", "citeRegEx": "62", "shortCiteRegEx": null, "year": 2011}, {"title": "Robust fragments-based tracking using the integral histogram", "author": ["A. Adam", "E. Rivlin", "I. Shimshoni"], "venue": "CVPR, 2006.", "citeRegEx": "63", "shortCiteRegEx": null, "year": 2006}, {"title": "Real time robust L1 tracker using accelerated proximal gradient approach", "author": ["C. Bao", "Y. Wu", "H. Ling", "H. Ji"], "venue": "CVPR, 2012.", "citeRegEx": "64", "shortCiteRegEx": null, "year": 2012}, {"title": "Locally orderless tracking", "author": ["S. Oron", "A. Bar-Hillel", "D. Levi", "S. Avidan"], "venue": "CVPR, 2012.", "citeRegEx": "65", "shortCiteRegEx": null, "year": 2012}, {"title": "Visual tracking via locality sensitive histograms", "author": ["S. He", "Q. Yang", "R.W. Lau", "J. Wang", "M.-H. Yang"], "venue": "CVPR, 2013.", "citeRegEx": "66", "shortCiteRegEx": null, "year": 2013}, {"title": "Robust tracking using local sparse appearance model and k-selection", "author": ["B. Liu", "J. Huang", "L. Yang", "C.A. Kulikowski"], "venue": "CVPR, 2011.", "citeRegEx": "67", "shortCiteRegEx": null, "year": 2011}, {"title": "Least soft-thresold squares tracking", "author": ["D. Wang", "H. Lu", "M.-H. Yang"], "venue": "CVPR, 2013.", "citeRegEx": "68", "shortCiteRegEx": null, "year": 2013}, {"title": "Robust visual tracking via multi-task sparse learning", "author": ["T.Zhang", "B. Ghanem", "S. Liu", "N. Ahuja"], "venue": "CVPR, 2012.", "citeRegEx": "69", "shortCiteRegEx": null, "year": 2012}, {"title": "Real-time tracking via on-line boosting", "author": ["H. Grabner", "M. Grabner", "H. Bischof"], "venue": "BMVC, 2006.", "citeRegEx": "70", "shortCiteRegEx": null, "year": 2006}, {"title": "Online robust image alignment via iterative convex optimization", "author": ["Y. Wu", "B. Shen", "H. Ling"], "venue": "CVPR, 2012.", "citeRegEx": "71", "shortCiteRegEx": null, "year": 2012}, {"title": "Visual tracking via probability continuous outlier model", "author": ["D. Wang", "H. Lu"], "venue": "CVPR, 2014.", "citeRegEx": "72", "shortCiteRegEx": null, "year": 2014}, {"title": "Robust object tracking via sparsitybased collaborative model", "author": ["W. Zhong", "H. Lu", "M. Yang"], "venue": "CVPR, 2012.", "citeRegEx": "73", "shortCiteRegEx": null, "year": 2012}, {"title": "Mean-shift blob tracking through scale space", "author": ["R.T. Collins"], "venue": "CVPR, 2003.", "citeRegEx": "74", "shortCiteRegEx": null, "year": 2003}, {"title": "Semi-supervised on-line boosting for robust tracking", "author": ["H. Grabner", "C. Leistner", "H. Bischof"], "venue": "ECCV, 2008.", "citeRegEx": "75", "shortCiteRegEx": null, "year": 2008}, {"title": "Online selection of discriminative tracking features", "author": ["R.T. Collins", "Y. Liu", "M. Leordeanu"], "venue": "PAMI, vol. 27, no. 10, pp. 1631\u20131643, 2005.", "citeRegEx": "76", "shortCiteRegEx": null, "year": 2005}, {"title": "Visual tracking decomposition", "author": ["J. Kwon", "K.M. Lee"], "venue": "CVPR, 2010.", "citeRegEx": "77", "shortCiteRegEx": null, "year": 2010}, {"title": "Tracking by sampling trackers", "author": ["\u2014\u2014"], "venue": "ICCV, 2011.", "citeRegEx": "78", "shortCiteRegEx": null, "year": 2011}, {"title": "The visual object tracking vot2013 challenge results", "author": ["M. Kristan"], "venue": "2013. [Online]. Available: http://www.votchallenge.net/vot2013/ program.html", "citeRegEx": "80", "shortCiteRegEx": null, "year": 2013}, {"title": "The visual object tracking vot2014 challenge results", "author": ["\u2014\u2014"], "venue": "2014. [Online]. Available: http://www.votchallenge.net/vot2014/program.html", "citeRegEx": "81", "shortCiteRegEx": null, "year": 2014}, {"title": "Robust visual tracking using an adaptive coupled-layer visual model", "author": ["L. Cehovin", "M. Kristan", "A. Leonardis"], "venue": "PAMI, vol. 35, no. 4, pp. 941\u2013 953, 2013.", "citeRegEx": "82", "shortCiteRegEx": null, "year": 2013}, {"title": "An enhanced adaptive coupledlayer lgtracker++", "author": ["J. Xiao", "R. Stolkin", "A. Leonardis"], "venue": "Vis. Obj. Track. Challenge VOT2013, In conjunction with ICCV2013, 2013.", "citeRegEx": "83", "shortCiteRegEx": null, "year": 2013}, {"title": "The visual object tracking vot2015 and tir2015 challenge results", "author": ["M. Kristan"], "venue": "2015. [Online]. Available: http: //www.votchallenge.net/vot2015/program.html", "citeRegEx": "84", "shortCiteRegEx": null, "year": 2015}, {"title": "A numerical study of the bottom-up and top-down inference processes in and-or graphs", "author": ["T. Wu", "S.C. Zhu"], "venue": "IJCV, vol. 93, no. 2, pp. 226\u2013252, 2011.", "citeRegEx": "85", "shortCiteRegEx": null, "year": 2011}, {"title": "Learning near-optimal cost-sensitive decision policy for object detection", "author": ["T. Wu", "S. Zhu"], "venue": "TPAMI, vol. 37, no. 5, pp. 1013\u20131027, 2015.", "citeRegEx": "86", "shortCiteRegEx": null, "year": 2015}, {"title": "Image parsing with stochastic scene grammar", "author": ["Y. Zhao", "S.C. Zhu"], "venue": "NIPS, 2011.", "citeRegEx": "87", "shortCiteRegEx": null, "year": 2011}], "referenceMentions": [{"referenceID": 0, "context": "During online learning, the AOG is discriminatively learned using latent SVM [1] to account for appearance (e.", "startOffset": 77, "endOffset": 80}, {"referenceID": 1, "context": "In experiments, our AOGTracker is tested on two popular tracking benchmarks with the same parameter setting: the TB-100/50/CVPR2013 benchmarks [2], [3], and the VOT benchmarks [4] \u2014 VOT 2013, 2014, 2015 and TIR2015 (thermal imagery tracking).", "startOffset": 143, "endOffset": 146}, {"referenceID": 2, "context": "In experiments, our AOGTracker is tested on two popular tracking benchmarks with the same parameter setting: the TB-100/50/CVPR2013 benchmarks [2], [3], and the VOT benchmarks [4] \u2014 VOT 2013, 2014, 2015 and TIR2015 (thermal imagery tracking).", "startOffset": 148, "endOffset": 151}, {"referenceID": 3, "context": "In experiments, our AOGTracker is tested on two popular tracking benchmarks with the same parameter setting: the TB-100/50/CVPR2013 benchmarks [2], [3], and the VOT benchmarks [4] \u2014 VOT 2013, 2014, 2015 and TIR2015 (thermal imagery tracking).", "startOffset": 176, "endOffset": 179}, {"referenceID": 4, "context": "In the former, our AOGTracker outperforms state-of-the-art tracking algorithms including two trackers based on deep convolutional network [5], [6].", "startOffset": 138, "endOffset": 141}, {"referenceID": 5, "context": "In the former, our AOGTracker outperforms state-of-the-art tracking algorithms including two trackers based on deep convolutional network [5], [6].", "startOffset": 143, "endOffset": 146}, {"referenceID": 6, "context": "O NLINE object tracking is an innate capability in human and animal vision for learning visual concepts [7], and is an important task in computer vision.", "startOffset": 104, "endOffset": 107}, {"referenceID": 1, "context": "1: Illustration of some typical issues in online object tracking using the \u201cskating1\u201d video in the benchmark [2].", "startOffset": 109, "endOffset": 112}, {"referenceID": 0, "context": "Such models have shown promising performance in object detection [1], [8], [9], [10], [11] and object recognition [12].", "startOffset": 65, "endOffset": 68}, {"referenceID": 7, "context": "Such models have shown promising performance in object detection [1], [8], [9], [10], [11] and object recognition [12].", "startOffset": 70, "endOffset": 73}, {"referenceID": 8, "context": "Such models have shown promising performance in object detection [1], [8], [9], [10], [11] and object recognition [12].", "startOffset": 75, "endOffset": 78}, {"referenceID": 9, "context": "Such models have shown promising performance in object detection [1], [8], [9], [10], [11] and object recognition [12].", "startOffset": 80, "endOffset": 84}, {"referenceID": 10, "context": "Such models have shown promising performance in object detection [1], [8], [9], [10], [11] and object recognition [12].", "startOffset": 86, "endOffset": 90}, {"referenceID": 11, "context": "Such models have shown promising performance in object detection [1], [8], [9], [10], [11] and object recognition [12].", "startOffset": 114, "endOffset": 118}, {"referenceID": 0, "context": "A popular modeling scheme represents object categories by mixtures of deformable part-based models (DPMs) [1].", "startOffset": 106, "endOffset": 109}, {"referenceID": 7, "context": "We quantize the space of part configurations recursively in a principled way with a hierarchical and compositional And-Or graph (AOG) representation [8], [11].", "startOffset": 149, "endOffset": 152}, {"referenceID": 10, "context": "We quantize the space of part configurations recursively in a principled way with a hierarchical and compositional And-Or graph (AOG) representation [8], [11].", "startOffset": 154, "endOffset": 158}, {"referenceID": 1, "context": "Online object tracking is usually posed as a maximum a posterior (MAP) problem using first order hidden Markov models (HMMs) [2], [13], [14].", "startOffset": 125, "endOffset": 128}, {"referenceID": 12, "context": "Online object tracking is usually posed as a maximum a posterior (MAP) problem using first order hidden Markov models (HMMs) [2], [13], [14].", "startOffset": 130, "endOffset": 134}, {"referenceID": 13, "context": "Online object tracking is usually posed as a maximum a posterior (MAP) problem using first order hidden Markov models (HMMs) [2], [13], [14].", "startOffset": 136, "endOffset": 140}, {"referenceID": 14, "context": "The maximization is based on either particle filtering [15] or dense sampling such as the tracking-by-detection methods [16], [17], [18].", "startOffset": 55, "endOffset": 59}, {"referenceID": 15, "context": "The maximization is based on either particle filtering [15] or dense sampling such as the tracking-by-detection methods [16], [17], [18].", "startOffset": 120, "endOffset": 124}, {"referenceID": 16, "context": "The maximization is based on either particle filtering [15] or dense sampling such as the tracking-by-detection methods [16], [17], [18].", "startOffset": 126, "endOffset": 130}, {"referenceID": 17, "context": "The maximization is based on either particle filtering [15] or dense sampling such as the tracking-by-detection methods [16], [17], [18].", "startOffset": 132, "endOffset": 136}, {"referenceID": 1, "context": ", the 29 trackers evaluated in the TB-100 benchmark [2]), no feedback inspection is applied to the history of inferred trajectory.", "startOffset": 52, "endOffset": 55}, {"referenceID": 16, "context": "Most prior approaches do not address this issue since they focus on marginally optimal solutions with which object models are updated, except for the P-N learning in TLD [17] and the self-paced learning for tracking [18].", "startOffset": 170, "endOffset": 174}, {"referenceID": 17, "context": "Most prior approaches do not address this issue since they focus on marginally optimal solutions with which object models are updated, except for the P-N learning in TLD [17] and the self-paced learning for tracking [18].", "startOffset": 216, "endOffset": 220}, {"referenceID": 1, "context": "Our TLP method obtains state-of-the-art performance on one popular tracking benchmark [2].", "startOffset": 86, "endOffset": 89}, {"referenceID": 0, "context": "(ii) We retrain the initial object AOG using latent SVM (LSVM) as it was done in learning the DPMs [1].", "startOffset": 99, "endOffset": 102}, {"referenceID": 13, "context": ", the Viterbi path [14]).", "startOffset": 19, "endOffset": 23}, {"referenceID": 18, "context": "This is similar in spirit to methods of keeping N-best maximal decoder for part models [19] and maintaining diverse M-best solutions in MRF [20].", "startOffset": 87, "endOffset": 91}, {"referenceID": 19, "context": "This is similar in spirit to methods of keeping N-best maximal decoder for part models [19] and maintaining diverse M-best solutions in MRF [20].", "startOffset": 140, "endOffset": 144}, {"referenceID": 20, "context": "Offline visual tracking [21], [22], [23], [24].", "startOffset": 24, "endOffset": 28}, {"referenceID": 21, "context": "Offline visual tracking [21], [22], [23], [24].", "startOffset": 30, "endOffset": 34}, {"referenceID": 22, "context": "Offline visual tracking [21], [22], [23], [24].", "startOffset": 36, "endOffset": 40}, {"referenceID": 23, "context": "Offline visual tracking [21], [22], [23], [24].", "startOffset": 42, "endOffset": 46}, {"referenceID": 0, "context": ", the DPMs [1]) and then form \u201ctracklets\u201d in consecutive frames.", "startOffset": 11, "endOffset": 14}, {"referenceID": 24, "context": "Recently, Hong and Han [25]", "startOffset": 23, "endOffset": 27}, {"referenceID": 25, "context": "proposed an offline single object tracking method by sampling tree-structured graphical models which exploit the underlying intrinsic structure of input video in an orderless tracking [26].", "startOffset": 184, "endOffset": 188}, {"referenceID": 14, "context": "In the literature, particle filtering [15] has been widely adopted, which approximately represents the posterior probability in a nonparametric form by maintaining a set of particles (i.", "startOffset": 38, "endOffset": 42}, {"referenceID": 15, "context": "More recently, tracking-bydetection methods [16], [17] have become popular which learn and update object models online and encode the posterior probability using dense sampling through sliding-window based detection onthe-fly.", "startOffset": 44, "endOffset": 48}, {"referenceID": 16, "context": "More recently, tracking-bydetection methods [16], [17] have become popular which learn and update object models online and encode the posterior probability using dense sampling through sliding-window based detection onthe-fly.", "startOffset": 50, "endOffset": 54}, {"referenceID": 0, "context": "To leverage the recent advance in object detection, object tracking research has made progress by incorporating discriminatively trained part-based models [1], [8], [27] (or more generally grammar models [9], [10], [11]).", "startOffset": 155, "endOffset": 158}, {"referenceID": 7, "context": "To leverage the recent advance in object detection, object tracking research has made progress by incorporating discriminatively trained part-based models [1], [8], [27] (or more generally grammar models [9], [10], [11]).", "startOffset": 160, "endOffset": 163}, {"referenceID": 26, "context": "To leverage the recent advance in object detection, object tracking research has made progress by incorporating discriminatively trained part-based models [1], [8], [27] (or more generally grammar models [9], [10], [11]).", "startOffset": 165, "endOffset": 169}, {"referenceID": 8, "context": "To leverage the recent advance in object detection, object tracking research has made progress by incorporating discriminatively trained part-based models [1], [8], [27] (or more generally grammar models [9], [10], [11]).", "startOffset": 204, "endOffset": 207}, {"referenceID": 9, "context": "To leverage the recent advance in object detection, object tracking research has made progress by incorporating discriminatively trained part-based models [1], [8], [27] (or more generally grammar models [9], [10], [11]).", "startOffset": 209, "endOffset": 213}, {"referenceID": 10, "context": "To leverage the recent advance in object detection, object tracking research has made progress by incorporating discriminatively trained part-based models [1], [8], [27] (or more generally grammar models [9], [10], [11]).", "startOffset": 215, "endOffset": 219}, {"referenceID": 27, "context": "Most popular methods also assume first-order HMMs except for the recently proposed online graph-based tracker [28].", "startOffset": 110, "endOffset": 114}, {"referenceID": 28, "context": "i) Appearance modeling of the whole object, such as incremental learning [29], kernel-based [30], particle filtering [15], sparse coding [31] and 3D-DCT representation [32]; More recently, Convolutional neural networks are utilized in improving tracking performance [5], [6], [33], which are usually pre-trained on some large scale image datasets such as the ImageNet [34] or on video sequences in a benchmark with the testing one excluded.", "startOffset": 73, "endOffset": 77}, {"referenceID": 29, "context": "i) Appearance modeling of the whole object, such as incremental learning [29], kernel-based [30], particle filtering [15], sparse coding [31] and 3D-DCT representation [32]; More recently, Convolutional neural networks are utilized in improving tracking performance [5], [6], [33], which are usually pre-trained on some large scale image datasets such as the ImageNet [34] or on video sequences in a benchmark with the testing one excluded.", "startOffset": 92, "endOffset": 96}, {"referenceID": 14, "context": "i) Appearance modeling of the whole object, such as incremental learning [29], kernel-based [30], particle filtering [15], sparse coding [31] and 3D-DCT representation [32]; More recently, Convolutional neural networks are utilized in improving tracking performance [5], [6], [33], which are usually pre-trained on some large scale image datasets such as the ImageNet [34] or on video sequences in a benchmark with the testing one excluded.", "startOffset": 117, "endOffset": 121}, {"referenceID": 30, "context": "i) Appearance modeling of the whole object, such as incremental learning [29], kernel-based [30], particle filtering [15], sparse coding [31] and 3D-DCT representation [32]; More recently, Convolutional neural networks are utilized in improving tracking performance [5], [6], [33], which are usually pre-trained on some large scale image datasets such as the ImageNet [34] or on video sequences in a benchmark with the testing one excluded.", "startOffset": 137, "endOffset": 141}, {"referenceID": 31, "context": "i) Appearance modeling of the whole object, such as incremental learning [29], kernel-based [30], particle filtering [15], sparse coding [31] and 3D-DCT representation [32]; More recently, Convolutional neural networks are utilized in improving tracking performance [5], [6], [33], which are usually pre-trained on some large scale image datasets such as the ImageNet [34] or on video sequences in a benchmark with the testing one excluded.", "startOffset": 168, "endOffset": 172}, {"referenceID": 4, "context": "i) Appearance modeling of the whole object, such as incremental learning [29], kernel-based [30], particle filtering [15], sparse coding [31] and 3D-DCT representation [32]; More recently, Convolutional neural networks are utilized in improving tracking performance [5], [6], [33], which are usually pre-trained on some large scale image datasets such as the ImageNet [34] or on video sequences in a benchmark with the testing one excluded.", "startOffset": 266, "endOffset": 269}, {"referenceID": 5, "context": "i) Appearance modeling of the whole object, such as incremental learning [29], kernel-based [30], particle filtering [15], sparse coding [31] and 3D-DCT representation [32]; More recently, Convolutional neural networks are utilized in improving tracking performance [5], [6], [33], which are usually pre-trained on some large scale image datasets such as the ImageNet [34] or on video sequences in a benchmark with the testing one excluded.", "startOffset": 271, "endOffset": 274}, {"referenceID": 32, "context": "i) Appearance modeling of the whole object, such as incremental learning [29], kernel-based [30], particle filtering [15], sparse coding [31] and 3D-DCT representation [32]; More recently, Convolutional neural networks are utilized in improving tracking performance [5], [6], [33], which are usually pre-trained on some large scale image datasets such as the ImageNet [34] or on video sequences in a benchmark with the testing one excluded.", "startOffset": 276, "endOffset": 280}, {"referenceID": 33, "context": "i) Appearance modeling of the whole object, such as incremental learning [29], kernel-based [30], particle filtering [15], sparse coding [31] and 3D-DCT representation [32]; More recently, Convolutional neural networks are utilized in improving tracking performance [5], [6], [33], which are usually pre-trained on some large scale image datasets such as the ImageNet [34] or on video sequences in a benchmark with the testing one excluded.", "startOffset": 368, "endOffset": 372}, {"referenceID": 34, "context": "ii) Appearance modeling of objects with parts, such as patchbased [35], coupled 2-layer models [36] and adaptive sparse appearance [37].", "startOffset": 66, "endOffset": 70}, {"referenceID": 35, "context": "ii) Appearance modeling of objects with parts, such as patchbased [35], coupled 2-layer models [36] and adaptive sparse appearance [37].", "startOffset": 95, "endOffset": 99}, {"referenceID": 36, "context": "ii) Appearance modeling of objects with parts, such as patchbased [35], coupled 2-layer models [36] and adaptive sparse appearance [37].", "startOffset": 131, "endOffset": 135}, {"referenceID": 37, "context": "iii) Tracking by discrimination using a single classifier, such as support vector tracking [38], multiple instance learning [39], STRUCK [40], circulant structure-based kernel method [41], and discriminant saliency based tracking [42]; iv) Tracking by part-based discriminative models, such as online extensions of DPMs [43], and structure preserving tracking method [27], [44].", "startOffset": 91, "endOffset": 95}, {"referenceID": 38, "context": "iii) Tracking by discrimination using a single classifier, such as support vector tracking [38], multiple instance learning [39], STRUCK [40], circulant structure-based kernel method [41], and discriminant saliency based tracking [42]; iv) Tracking by part-based discriminative models, such as online extensions of DPMs [43], and structure preserving tracking method [27], [44].", "startOffset": 124, "endOffset": 128}, {"referenceID": 39, "context": "iii) Tracking by discrimination using a single classifier, such as support vector tracking [38], multiple instance learning [39], STRUCK [40], circulant structure-based kernel method [41], and discriminant saliency based tracking [42]; iv) Tracking by part-based discriminative models, such as online extensions of DPMs [43], and structure preserving tracking method [27], [44].", "startOffset": 137, "endOffset": 141}, {"referenceID": 40, "context": "iii) Tracking by discrimination using a single classifier, such as support vector tracking [38], multiple instance learning [39], STRUCK [40], circulant structure-based kernel method [41], and discriminant saliency based tracking [42]; iv) Tracking by part-based discriminative models, such as online extensions of DPMs [43], and structure preserving tracking method [27], [44].", "startOffset": 183, "endOffset": 187}, {"referenceID": 41, "context": "iii) Tracking by discrimination using a single classifier, such as support vector tracking [38], multiple instance learning [39], STRUCK [40], circulant structure-based kernel method [41], and discriminant saliency based tracking [42]; iv) Tracking by part-based discriminative models, such as online extensions of DPMs [43], and structure preserving tracking method [27], [44].", "startOffset": 230, "endOffset": 234}, {"referenceID": 42, "context": "iii) Tracking by discrimination using a single classifier, such as support vector tracking [38], multiple instance learning [39], STRUCK [40], circulant structure-based kernel method [41], and discriminant saliency based tracking [42]; iv) Tracking by part-based discriminative models, such as online extensions of DPMs [43], and structure preserving tracking method [27], [44].", "startOffset": 320, "endOffset": 324}, {"referenceID": 26, "context": "iii) Tracking by discrimination using a single classifier, such as support vector tracking [38], multiple instance learning [39], STRUCK [40], circulant structure-based kernel method [41], and discriminant saliency based tracking [42]; iv) Tracking by part-based discriminative models, such as online extensions of DPMs [43], and structure preserving tracking method [27], [44].", "startOffset": 367, "endOffset": 371}, {"referenceID": 43, "context": "iii) Tracking by discrimination using a single classifier, such as support vector tracking [38], multiple instance learning [39], STRUCK [40], circulant structure-based kernel method [41], and discriminant saliency based tracking [42]; iv) Tracking by part-based discriminative models, such as online extensions of DPMs [43], and structure preserving tracking method [27], [44].", "startOffset": 373, "endOffset": 377}, {"referenceID": 16, "context": "i) More representational power: Unlike TLD [17] and many other methods (e.", "startOffset": 43, "endOffset": 47}, {"referenceID": 17, "context": ", [18]) which model an object as a single template or a mixture of templates and thus do not perform well in tracking objects with large structural and appearance variations, an AOG represents an object in a hierarchical and compositional graph expressing a large number of latent part configurations.", "startOffset": 2, "endOffset": 6}, {"referenceID": 44, "context": "This idea is similar in spirit to finding good features to track objects [45], and we find good part configurations online for both tracking and learning.", "startOffset": 73, "endOffset": 77}, {"referenceID": 45, "context": "Our preliminary work has been published in [46] and the method for constructing full structure AOG was published in [8].", "startOffset": 43, "endOffset": 47}, {"referenceID": 7, "context": "Our preliminary work has been published in [46] and the method for constructing full structure AOG was published in [8].", "startOffset": 116, "endOffset": 119}, {"referenceID": 1, "context": "iii) It outperforms the state-of-the-art tracking methods in a recent public benchmark, TB-100 [2], and obtains comparable performance on a series of VOT benchmarks [4].", "startOffset": 95, "endOffset": 98}, {"referenceID": 3, "context": "iii) It outperforms the state-of-the-art tracking methods in a recent public benchmark, TB-100 [2], and obtains comparable performance on a series of VOT benchmarks [4].", "startOffset": 165, "endOffset": 168}, {"referenceID": 14, "context": "which is usually solved using particle filtering [15] in practice.", "startOffset": 49, "endOffset": 53}, {"referenceID": 46, "context": "tracking by generative appearance modeling of an object [47].", "startOffset": 56, "endOffset": 60}, {"referenceID": 16, "context": "We use a thresholded motion model in experiments: the cost is 0 if the transition is accepted based on the median flow [17] (which is a forward-backward extension of the Lucas-Kanade optimal flow [48]) and +\u221e otherwise.", "startOffset": 119, "endOffset": 123}, {"referenceID": 47, "context": "We use a thresholded motion model in experiments: the cost is 0 if the transition is accepted based on the median flow [17] (which is a forward-backward extension of the Lucas-Kanade optimal flow [48]) and +\u221e otherwise.", "startOffset": 196, "endOffset": 200}, {"referenceID": 17, "context": "A similar method was explored in [18].", "startOffset": 33, "endOffset": 37}, {"referenceID": 0, "context": "We note that the object template is not allowed to perturb locally in inference since we infer the optimal part configuration for each given object location in the pyramid with sliding window technique used, as done in the DPM [1], so the parent And-node of the object terminal-node does not have deformation parameters.", "startOffset": 227, "endOffset": 230}, {"referenceID": 48, "context": "We use three types of features: histogram of oriented gradient (HOG) [49], local binary pattern features (LBP) [50], and RGB color histograms (for color videos).", "startOffset": 69, "endOffset": 73}, {"referenceID": 49, "context": "We use three types of features: histogram of oriented gradient (HOG) [49], local binary pattern features (LBP) [50], and RGB color histograms (for color videos).", "startOffset": 111, "endOffset": 115}, {"referenceID": 0, "context": "The deformation feature is defined by \u03a6 (\u03b4) = [dx, dx, dy, dy]\u2032 as done in DPMs [1].", "startOffset": 80, "endOffset": 83}, {"referenceID": 0, "context": "where the first case is for sharing score maps between the object terminal-node and its parent And-node since we do not allow local deformation for the whole object, the second case for computing transformed score maps of parent Andnode of a part terminal-node which is allowed to find the best placement through distance transformation [1], \u2295 represents the displacement operator in the position space in \u039b, and And-node", "startOffset": 337, "endOffset": 340}, {"referenceID": 18, "context": "We keep top Nbest parse trees to infer the best B\u2217 t together with a temporal DP algorithm, similar to the strategies used in [19], [20].", "startOffset": 126, "endOffset": 130}, {"referenceID": 19, "context": "We keep top Nbest parse trees to infer the best B\u2217 t together with a temporal DP algorithm, similar to the strategies used in [19], [20].", "startOffset": 132, "endOffset": 136}, {"referenceID": 50, "context": "Intuitively, we expect the score map of a discriminative node v has peak and steep landscape, as investigated in [51].", "startOffset": 113, "endOffset": 117}, {"referenceID": 51, "context": "More sophisticated definitions of intrackability in tracking are referred to [52].", "startOffset": 77, "endOffset": 81}, {"referenceID": 0, "context": "We use latent SVM method (LSVM) [1].", "startOffset": 32, "endOffset": 35}, {"referenceID": 0, "context": "While we can use stochastic gradient descent as done in DPMs [1], we adopt LBFGS method in practice 3 [53] since it is more robust and efficient with parallel implementation as investigated in [9], [54].", "startOffset": 61, "endOffset": 64}, {"referenceID": 52, "context": "While we can use stochastic gradient descent as done in DPMs [1], we adopt LBFGS method in practice 3 [53] since it is more robust and efficient with parallel implementation as investigated in [9], [54].", "startOffset": 102, "endOffset": 106}, {"referenceID": 8, "context": "While we can use stochastic gradient descent as done in DPMs [1], we adopt LBFGS method in practice 3 [53] since it is more robust and efficient with parallel implementation as investigated in [9], [54].", "startOffset": 193, "endOffset": 196}, {"referenceID": 53, "context": "While we can use stochastic gradient descent as done in DPMs [1], we adopt LBFGS method in practice 3 [53] since it is more robust and efficient with parallel implementation as investigated in [9], [54].", "startOffset": 198, "endOffset": 202}, {"referenceID": 0, "context": "We train the parameters of initial object AOG using LSVM [1] with two rounds of positive re-labeling and hard negative mining respectively.", "startOffset": 57, "endOffset": 60}, {"referenceID": 54, "context": "ASLA [55] X X X X X X X BSBT [56] H X X X CPF [57] X X X X X CSK [58] X X X X CT [59] H X X X CXT [60] B X X X DFT [61] X X X X X FOT [62] X X X X X FRAG [63] X X X X IVT [29] X X X X X KMS [30] X X X X X L1APG [64] X X X X X X LOT [65] X X X X X LSHT [66] X X X H X X X LSK [67] X X X X X X LSS [68] X X X X X X X MIL [39] H X X X MTT [69] X X X X X X OAB [70] H X X X ORIA [71] X X H X X X PCOM [72] X X X X X X SCM [73] X X X X X X X X X SMS [74] X X X X SBT [75] H X X X STRUCK [40] H X X X TLD [17] X B X X X VR [76] X X X X VTD [77] X X X X X X VTS [78] X X X X X X X AOG X X X X HOG [+Color] X X X X", "startOffset": 5, "endOffset": 9}, {"referenceID": 55, "context": "ASLA [55] X X X X X X X BSBT [56] H X X X CPF [57] X X X X X CSK [58] X X X X CT [59] H X X X CXT [60] B X X X DFT [61] X X X X X FOT [62] X X X X X FRAG [63] X X X X IVT [29] X X X X X KMS [30] X X X X X L1APG [64] X X X X X X LOT [65] X X X X X LSHT [66] X X X H X X X LSK [67] X X X X X X LSS [68] X X X X X X X MIL [39] H X X X MTT [69] X X X X X X OAB [70] H X X X ORIA [71] X X H X X X PCOM [72] X X X X X X SCM [73] X X X X X X X X X SMS [74] X X X X SBT [75] H X X X STRUCK [40] H X X X TLD [17] X B X X X VR [76] X X X X VTD [77] X X X X X X VTS [78] X X X X X X X AOG X X X X HOG [+Color] X X X X", "startOffset": 29, "endOffset": 33}, {"referenceID": 56, "context": "ASLA [55] X X X X X X X BSBT [56] H X X X CPF [57] X X X X X CSK [58] X X X X CT [59] H X X X CXT [60] B X X X DFT [61] X X X X X FOT [62] X X X X X FRAG [63] X X X X IVT [29] X X X X X KMS [30] X X X X X L1APG [64] X X X X X X LOT [65] X X X X X LSHT [66] X X X H X X X LSK [67] X X X X X X LSS [68] X X X X X X X MIL [39] H X X X MTT [69] X X X X X X OAB [70] H X X X ORIA [71] X X H X X X PCOM [72] X X X X X X SCM [73] X X X X X X X X X SMS [74] X X X X SBT [75] H X X X STRUCK [40] H X X X TLD [17] X B X X X VR [76] X X X X VTD [77] X X X X X X VTS [78] X X X X X X X AOG X X X X HOG [+Color] X X X X", "startOffset": 46, "endOffset": 50}, {"referenceID": 57, "context": "ASLA [55] X X X X X X X BSBT [56] H X X X CPF [57] X X X X X CSK [58] X X X X CT [59] H X X X CXT [60] B X X X DFT [61] X X X X X FOT [62] X X X X X FRAG [63] X X X X IVT [29] X X X X X KMS [30] X X X X X L1APG [64] X X X X X X LOT [65] X X X X X LSHT [66] X X X H X X X LSK [67] X X X X X X LSS [68] X X X X X X X MIL [39] H X X X MTT [69] X X X X X X OAB [70] H X X X ORIA [71] X X H X X X PCOM [72] X X X X X X SCM [73] X X X X X X X X X SMS [74] X X X X SBT [75] H X X X STRUCK [40] H X X X TLD [17] X B X X X VR [76] X X X X VTD [77] X X X X X X VTS [78] X X X X X X X AOG X X X X HOG [+Color] X X X X", "startOffset": 65, "endOffset": 69}, {"referenceID": 58, "context": "ASLA [55] X X X X X X X BSBT [56] H X X X CPF [57] X X X X X CSK [58] X X X X CT [59] H X X X CXT [60] B X X X DFT [61] X X X X X FOT [62] X X X X X FRAG [63] X X X X IVT [29] X X X X X KMS [30] X X X X X L1APG [64] X X X X X X LOT [65] X X X X X LSHT [66] X X X H X X X LSK [67] X X X X X X LSS [68] X X X X X X X MIL [39] H X X X MTT [69] X X X X X X OAB [70] H X X X ORIA [71] X X H X X X PCOM [72] X X X X X X SCM [73] X X X X X X X X X SMS [74] X X X X SBT [75] H X X X STRUCK [40] H X X X TLD [17] X B X X X VR [76] X X X X VTD [77] X X X X X X VTS [78] X X X X X X X AOG X X X X HOG [+Color] X X X X", "startOffset": 81, "endOffset": 85}, {"referenceID": 59, "context": "ASLA [55] X X X X X X X BSBT [56] H X X X CPF [57] X X X X X CSK [58] X X X X CT [59] H X X X CXT [60] B X X X DFT [61] X X X X X FOT [62] X X X X X FRAG [63] X X X X IVT [29] X X X X X KMS [30] X X X X X L1APG [64] X X X X X X LOT [65] X X X X X LSHT [66] X X X H X X X LSK [67] X X X X X X LSS [68] X X X X X X X MIL [39] H X X X MTT [69] X X X X X X OAB [70] H X X X ORIA [71] X X H X X X PCOM [72] X X X X X X SCM [73] X X X X X X X X X SMS [74] X X X X SBT [75] H X X X STRUCK [40] H X X X TLD [17] X B X X X VR [76] X X X X VTD [77] X X X X X X VTS [78] X X X X X X X AOG X X X X HOG [+Color] X X X X", "startOffset": 98, "endOffset": 102}, {"referenceID": 60, "context": "ASLA [55] X X X X X X X BSBT [56] H X X X CPF [57] X X X X X CSK [58] X X X X CT [59] H X X X CXT [60] B X X X DFT [61] X X X X X FOT [62] X X X X X FRAG [63] X X X X IVT [29] X X X X X KMS [30] X X X X X L1APG [64] X X X X X X LOT [65] X X X X X LSHT [66] X X X H X X X LSK [67] X X X X X X LSS [68] X X X X X X X MIL [39] H X X X MTT [69] X X X X X X OAB [70] H X X X ORIA [71] X X H X X X PCOM [72] X X X X X X SCM [73] X X X X X X X X X SMS [74] X X X X SBT [75] H X X X STRUCK [40] H X X X TLD [17] X B X X X VR [76] X X X X VTD [77] X X X X X X VTS [78] X X X X X X X AOG X X X X HOG [+Color] X X X X", "startOffset": 115, "endOffset": 119}, {"referenceID": 61, "context": "ASLA [55] X X X X X X X BSBT [56] H X X X CPF [57] X X X X X CSK [58] X X X X CT [59] H X X X CXT [60] B X X X DFT [61] X X X X X FOT [62] X X X X X FRAG [63] X X X X IVT [29] X X X X X KMS [30] X X X X X L1APG [64] X X X X X X LOT [65] X X X X X LSHT [66] X X X H X X X LSK [67] X X X X X X LSS [68] X X X X X X X MIL [39] H X X X MTT [69] X X X X X X OAB [70] H X X X ORIA [71] X X H X X X PCOM [72] X X X X X X SCM [73] X X X X X X X X X SMS [74] X X X X SBT [75] H X X X STRUCK [40] H X X X TLD [17] X B X X X VR [76] X X X X VTD [77] X X X X X X VTS [78] X X X X X X X AOG X X X X HOG [+Color] X X X X", "startOffset": 134, "endOffset": 138}, {"referenceID": 62, "context": "ASLA [55] X X X X X X X BSBT [56] H X X X CPF [57] X X X X X CSK [58] X X X X CT [59] H X X X CXT [60] B X X X DFT [61] X X X X X FOT [62] X X X X X FRAG [63] X X X X IVT [29] X X X X X KMS [30] X X X X X L1APG [64] X X X X X X LOT [65] X X X X X LSHT [66] X X X H X X X LSK [67] X X X X X X LSS [68] X X X X X X X MIL [39] H X X X MTT [69] X X X X X X OAB [70] H X X X ORIA [71] X X H X X X PCOM [72] X X X X X X SCM [73] X X X X X X X X X SMS [74] X X X X SBT [75] H X X X STRUCK [40] H X X X TLD [17] X B X X X VR [76] X X X X VTD [77] X X X X X X VTS [78] X X X X X X X AOG X X X X HOG [+Color] X X X X", "startOffset": 154, "endOffset": 158}, {"referenceID": 28, "context": "ASLA [55] X X X X X X X BSBT [56] H X X X CPF [57] X X X X X CSK [58] X X X X CT [59] H X X X CXT [60] B X X X DFT [61] X X X X X FOT [62] X X X X X FRAG [63] X X X X IVT [29] X X X X X KMS [30] X X X X X L1APG [64] X X X X X X LOT [65] X X X X X LSHT [66] X X X H X X X LSK [67] X X X X X X LSS [68] X X X X X X X MIL [39] H X X X MTT [69] X X X X X X OAB [70] H X X X ORIA [71] X X H X X X PCOM [72] X X X X X X SCM [73] X X X X X X X X X SMS [74] X X X X SBT [75] H X X X STRUCK [40] H X X X TLD [17] X B X X X VR [76] X X X X VTD [77] X X X X X X VTS [78] X X X X X X X AOG X X X X HOG [+Color] X X X X", "startOffset": 171, "endOffset": 175}, {"referenceID": 29, "context": "ASLA [55] X X X X X X X BSBT [56] H X X X CPF [57] X X X X X CSK [58] X X X X CT [59] H X X X CXT [60] B X X X DFT [61] X X X X X FOT [62] X X X X X FRAG [63] X X X X IVT [29] X X X X X KMS [30] X X X X X L1APG [64] X X X X X X LOT [65] X X X X X LSHT [66] X X X H X X X LSK [67] X X X X X X LSS [68] X X X X X X X MIL [39] H X X X MTT [69] X X X X X X OAB [70] H X X X ORIA [71] X X H X X X PCOM [72] X X X X X X SCM [73] X X X X X X X X X SMS [74] X X X X SBT [75] H X X X STRUCK [40] H X X X TLD [17] X B X X X VR [76] X X X X VTD [77] X X X X X X VTS [78] X X X X X X X AOG X X X X HOG [+Color] X X X X", "startOffset": 190, "endOffset": 194}, {"referenceID": 63, "context": "ASLA [55] X X X X X X X BSBT [56] H X X X CPF [57] X X X X X CSK [58] X X X X CT [59] H X X X CXT [60] B X X X DFT [61] X X X X X FOT [62] X X X X X FRAG [63] X X X X IVT [29] X X X X X KMS [30] X X X X X L1APG [64] X X X X X X LOT [65] X X X X X LSHT [66] X X X H X X X LSK [67] X X X X X X LSS [68] X X X X X X X MIL [39] H X X X MTT [69] X X X X X X OAB [70] H X X X ORIA [71] X X H X X X PCOM [72] X X X X X X SCM [73] X X X X X X X X X SMS [74] X X X X SBT [75] H X X X STRUCK [40] H X X X TLD [17] X B X X X VR [76] X X X X VTD [77] X X X X X X VTS [78] X X X X X X X AOG X X X X HOG [+Color] X X X X", "startOffset": 211, "endOffset": 215}, {"referenceID": 64, "context": "ASLA [55] X X X X X X X BSBT [56] H X X X CPF [57] X X X X X CSK [58] X X X X CT [59] H X X X CXT [60] B X X X DFT [61] X X X X X FOT [62] X X X X X FRAG [63] X X X X IVT [29] X X X X X KMS [30] X X X X X L1APG [64] X X X X X X LOT [65] X X X X X LSHT [66] X X X H X X X LSK [67] X X X X X X LSS [68] X X X X X X X MIL [39] H X X X MTT [69] X X X X X X OAB [70] H X X X ORIA [71] X X H X X X PCOM [72] X X X X X X SCM [73] X X X X X X X X X SMS [74] X X X X SBT [75] H X X X STRUCK [40] H X X X TLD [17] X B X X X VR [76] X X X X VTD [77] X X X X X X VTS [78] X X X X X X X AOG X X X X HOG [+Color] X X X X", "startOffset": 232, "endOffset": 236}, {"referenceID": 65, "context": "ASLA [55] X X X X X X X BSBT [56] H X X X CPF [57] X X X X X CSK [58] X X X X CT [59] H X X X CXT [60] B X X X DFT [61] X X X X X FOT [62] X X X X X FRAG [63] X X X X IVT [29] X X X X X KMS [30] X X X X X L1APG [64] X X X X X X LOT [65] X X X X X LSHT [66] X X X H X X X LSK [67] X X X X X X LSS [68] X X X X X X X MIL [39] H X X X MTT [69] X X X X X X OAB [70] H X X X ORIA [71] X X H X X X PCOM [72] X X X X X X SCM [73] X X X X X X X X X SMS [74] X X X X SBT [75] H X X X STRUCK [40] H X X X TLD [17] X B X X X VR [76] X X X X VTD [77] X X X X X X VTS [78] X X X X X X X AOG X X X X HOG [+Color] X X X X", "startOffset": 252, "endOffset": 256}, {"referenceID": 66, "context": "ASLA [55] X X X X X X X BSBT [56] H X X X CPF [57] X X X X X CSK [58] X X X X CT [59] H X X X CXT [60] B X X X DFT [61] X X X X X FOT [62] X X X X X FRAG [63] X X X X IVT [29] X X X X X KMS [30] X X X X X L1APG [64] X X X X X X LOT [65] X X X X X LSHT [66] X X X H X X X LSK [67] X X X X X X LSS [68] X X X X X X X MIL [39] H X X X MTT [69] X X X X X X OAB [70] H X X X ORIA [71] X X H X X X PCOM [72] X X X X X X SCM [73] X X X X X X X X X SMS [74] X X X X SBT [75] H X X X STRUCK [40] H X X X TLD [17] X B X X X VR [76] X X X X VTD [77] X X X X X X VTS [78] X X X X X X X AOG X X X X HOG [+Color] X X X X", "startOffset": 275, "endOffset": 279}, {"referenceID": 67, "context": "ASLA [55] X X X X X X X BSBT [56] H X X X CPF [57] X X X X X CSK [58] X X X X CT [59] H X X X CXT [60] B X X X DFT [61] X X X X X FOT [62] X X X X X FRAG [63] X X X X IVT [29] X X X X X KMS [30] X X X X X L1APG [64] X X X X X X LOT [65] X X X X X LSHT [66] X X X H X X X LSK [67] X X X X X X LSS [68] X X X X X X X MIL [39] H X X X MTT [69] X X X X X X OAB [70] H X X X ORIA [71] X X H X X X PCOM [72] X X X X X X SCM [73] X X X X X X X X X SMS [74] X X X X SBT [75] H X X X STRUCK [40] H X X X TLD [17] X B X X X VR [76] X X X X VTD [77] X X X X X X VTS [78] X X X X X X X AOG X X X X HOG [+Color] X X X X", "startOffset": 296, "endOffset": 300}, {"referenceID": 38, "context": "ASLA [55] X X X X X X X BSBT [56] H X X X CPF [57] X X X X X CSK [58] X X X X CT [59] H X X X CXT [60] B X X X DFT [61] X X X X X FOT [62] X X X X X FRAG [63] X X X X IVT [29] X X X X X KMS [30] X X X X X L1APG [64] X X X X X X LOT [65] X X X X X LSHT [66] X X X H X X X LSK [67] X X X X X X LSS [68] X X X X X X X MIL [39] H X X X MTT [69] X X X X X X OAB [70] H X X X ORIA [71] X X H X X X PCOM [72] X X X X X X SCM [73] X X X X X X X X X SMS [74] X X X X SBT [75] H X X X STRUCK [40] H X X X TLD [17] X B X X X VR [76] X X X X VTD [77] X X X X X X VTS [78] X X X X X X X AOG X X X X HOG [+Color] X X X X", "startOffset": 319, "endOffset": 323}, {"referenceID": 68, "context": "ASLA [55] X X X X X X X BSBT [56] H X X X CPF [57] X X X X X CSK [58] X X X X CT [59] H X X X CXT [60] B X X X DFT [61] X X X X X FOT [62] X X X X X FRAG [63] X X X X IVT [29] X X X X X KMS [30] X X X X X L1APG [64] X X X X X X LOT [65] X X X X X LSHT [66] X X X H X X X LSK [67] X X X X X X LSS [68] X X X X X X X MIL [39] H X X X MTT [69] X X X X X X OAB [70] H X X X ORIA [71] X X H X X X PCOM [72] X X X X X X SCM [73] X X X X X X X X X SMS [74] X X X X SBT [75] H X X X STRUCK [40] H X X X TLD [17] X B X X X VR [76] X X X X VTD [77] X X X X X X VTS [78] X X X X X X X AOG X X X X HOG [+Color] X X X X", "startOffset": 336, "endOffset": 340}, {"referenceID": 69, "context": "ASLA [55] X X X X X X X BSBT [56] H X X X CPF [57] X X X X X CSK [58] X X X X CT [59] H X X X CXT [60] B X X X DFT [61] X X X X X FOT [62] X X X X X FRAG [63] X X X X IVT [29] X X X X X KMS [30] X X X X X L1APG [64] X X X X X X LOT [65] X X X X X LSHT [66] X X X H X X X LSK [67] X X X X X X LSS [68] X X X X X X X MIL [39] H X X X MTT [69] X X X X X X OAB [70] H X X X ORIA [71] X X H X X X PCOM [72] X X X X X X SCM [73] X X X X X X X X X SMS [74] X X X X SBT [75] H X X X STRUCK [40] H X X X TLD [17] X B X X X VR [76] X X X X VTD [77] X X X X X X VTS [78] X X X X X X X AOG X X X X HOG [+Color] X X X X", "startOffset": 357, "endOffset": 361}, {"referenceID": 70, "context": "ASLA [55] X X X X X X X BSBT [56] H X X X CPF [57] X X X X X CSK [58] X X X X CT [59] H X X X CXT [60] B X X X DFT [61] X X X X X FOT [62] X X X X X FRAG [63] X X X X IVT [29] X X X X X KMS [30] X X X X X L1APG [64] X X X X X X LOT [65] X X X X X LSHT [66] X X X H X X X LSK [67] X X X X X X LSS [68] X X X X X X X MIL [39] H X X X MTT [69] X X X X X X OAB [70] H X X X ORIA [71] X X H X X X PCOM [72] X X X X X X SCM [73] X X X X X X X X X SMS [74] X X X X SBT [75] H X X X STRUCK [40] H X X X TLD [17] X B X X X VR [76] X X X X VTD [77] X X X X X X VTS [78] X X X X X X X AOG X X X X HOG [+Color] X X X X", "startOffset": 375, "endOffset": 379}, {"referenceID": 71, "context": "ASLA [55] X X X X X X X BSBT [56] H X X X CPF [57] X X X X X CSK [58] X X X X CT [59] H X X X CXT [60] B X X X DFT [61] X X X X X FOT [62] X X X X X FRAG [63] X X X X IVT [29] X X X X X KMS [30] X X X X X L1APG [64] X X X X X X LOT [65] X X X X X LSHT [66] X X X H X X X LSK [67] X X X X X X LSS [68] X X X X X X X MIL [39] H X X X MTT [69] X X X X X X OAB [70] H X X X ORIA [71] X X H X X X PCOM [72] X X X X X X SCM [73] X X X X X X X X X SMS [74] X X X X SBT [75] H X X X STRUCK [40] H X X X TLD [17] X B X X X VR [76] X X X X VTD [77] X X X X X X VTS [78] X X X X X X X AOG X X X X HOG [+Color] X X X X", "startOffset": 397, "endOffset": 401}, {"referenceID": 72, "context": "ASLA [55] X X X X X X X BSBT [56] H X X X CPF [57] X X X X X CSK [58] X X X X CT [59] H X X X CXT [60] B X X X DFT [61] X X X X X FOT [62] X X X X X FRAG [63] X X X X IVT [29] X X X X X KMS [30] X X X X X L1APG [64] X X X X X X LOT [65] X X X X X LSHT [66] X X X H X X X LSK [67] X X X X X X LSS [68] X X X X X X X MIL [39] H X X X MTT [69] X X X X X X OAB [70] H X X X ORIA [71] X X H X X X PCOM [72] X X X X X X SCM [73] X X X X X X X X X SMS [74] X X X X SBT [75] H X X X STRUCK [40] H X X X TLD [17] X B X X X VR [76] X X X X VTD [77] X X X X X X VTS [78] X X X X X X X AOG X X X X HOG [+Color] X X X X", "startOffset": 418, "endOffset": 422}, {"referenceID": 73, "context": "ASLA [55] X X X X X X X BSBT [56] H X X X CPF [57] X X X X X CSK [58] X X X X CT [59] H X X X CXT [60] B X X X DFT [61] X X X X X FOT [62] X X X X X FRAG [63] X X X X IVT [29] X X X X X KMS [30] X X X X X L1APG [64] X X X X X X LOT [65] X X X X X LSHT [66] X X X H X X X LSK [67] X X X X X X LSS [68] X X X X X X X MIL [39] H X X X MTT [69] X X X X X X OAB [70] H X X X ORIA [71] X X H X X X PCOM [72] X X X X X X SCM [73] X X X X X X X X X SMS [74] X X X X SBT [75] H X X X STRUCK [40] H X X X TLD [17] X B X X X VR [76] X X X X VTD [77] X X X X X X VTS [78] X X X X X X X AOG X X X X HOG [+Color] X X X X", "startOffset": 445, "endOffset": 449}, {"referenceID": 74, "context": "ASLA [55] X X X X X X X BSBT [56] H X X X CPF [57] X X X X X CSK [58] X X X X CT [59] H X X X CXT [60] B X X X DFT [61] X X X X X FOT [62] X X X X X FRAG [63] X X X X IVT [29] X X X X X KMS [30] X X X X X L1APG [64] X X X X X X LOT [65] X X X X X LSHT [66] X X X H X X X LSK [67] X X X X X X LSS [68] X X X X X X X MIL [39] H X X X MTT [69] X X X X X X OAB [70] H X X X ORIA [71] X X H X X X PCOM [72] X X X X X X SCM [73] X X X X X X X X X SMS [74] X X X X SBT [75] H X X X STRUCK [40] H X X X TLD [17] X B X X X VR [76] X X X X VTD [77] X X X X X X VTS [78] X X X X X X X AOG X X X X HOG [+Color] X X X X", "startOffset": 462, "endOffset": 466}, {"referenceID": 39, "context": "ASLA [55] X X X X X X X BSBT [56] H X X X CPF [57] X X X X X CSK [58] X X X X CT [59] H X X X CXT [60] B X X X DFT [61] X X X X X FOT [62] X X X X X FRAG [63] X X X X IVT [29] X X X X X KMS [30] X X X X X L1APG [64] X X X X X X LOT [65] X X X X X LSHT [66] X X X H X X X LSK [67] X X X X X X LSS [68] X X X X X X X MIL [39] H X X X MTT [69] X X X X X X OAB [70] H X X X ORIA [71] X X H X X X PCOM [72] X X X X X X SCM [73] X X X X X X X X X SMS [74] X X X X SBT [75] H X X X STRUCK [40] H X X X TLD [17] X B X X X VR [76] X X X X VTD [77] X X X X X X VTS [78] X X X X X X X AOG X X X X HOG [+Color] X X X X", "startOffset": 482, "endOffset": 486}, {"referenceID": 16, "context": "ASLA [55] X X X X X X X BSBT [56] H X X X CPF [57] X X X X X CSK [58] X X X X CT [59] H X X X CXT [60] B X X X DFT [61] X X X X X FOT [62] X X X X X FRAG [63] X X X X IVT [29] X X X X X KMS [30] X X X X X L1APG [64] X X X X X X LOT [65] X X X X X LSHT [66] X X X H X X X LSK [67] X X X X X X LSS [68] X X X X X X X MIL [39] H X X X MTT [69] X X X X X X OAB [70] H X X X ORIA [71] X X H X X X PCOM [72] X X X X X X SCM [73] X X X X X X X X X SMS [74] X X X X SBT [75] H X X X STRUCK [40] H X X X TLD [17] X B X X X VR [76] X X X X VTD [77] X X X X X X VTS [78] X X X X X X X AOG X X X X HOG [+Color] X X X X", "startOffset": 499, "endOffset": 503}, {"referenceID": 75, "context": "ASLA [55] X X X X X X X BSBT [56] H X X X CPF [57] X X X X X CSK [58] X X X X CT [59] H X X X CXT [60] B X X X DFT [61] X X X X X FOT [62] X X X X X FRAG [63] X X X X IVT [29] X X X X X KMS [30] X X X X X L1APG [64] X X X X X X LOT [65] X X X X X LSHT [66] X X X H X X X LSK [67] X X X X X X LSS [68] X X X X X X X MIL [39] H X X X MTT [69] X X X X X X OAB [70] H X X X ORIA [71] X X H X X X PCOM [72] X X X X X X SCM [73] X X X X X X X X X SMS [74] X X X X SBT [75] H X X X STRUCK [40] H X X X TLD [17] X B X X X VR [76] X X X X VTD [77] X X X X X X VTS [78] X X X X X X X AOG X X X X HOG [+Color] X X X X", "startOffset": 517, "endOffset": 521}, {"referenceID": 76, "context": "ASLA [55] X X X X X X X BSBT [56] H X X X CPF [57] X X X X X CSK [58] X X X X CT [59] H X X X CXT [60] B X X X DFT [61] X X X X X FOT [62] X X X X X FRAG [63] X X X X IVT [29] X X X X X KMS [30] X X X X X L1APG [64] X X X X X X LOT [65] X X X X X LSHT [66] X X X H X X X LSK [67] X X X X X X LSS [68] X X X X X X X MIL [39] H X X X MTT [69] X X X X X X OAB [70] H X X X ORIA [71] X X H X X X PCOM [72] X X X X X X SCM [73] X X X X X X X X X SMS [74] X X X X SBT [75] H X X X STRUCK [40] H X X X TLD [17] X B X X X VR [76] X X X X VTD [77] X X X X X X VTS [78] X X X X X X X AOG X X X X HOG [+Color] X X X X", "startOffset": 534, "endOffset": 538}, {"referenceID": 77, "context": "ASLA [55] X X X X X X X BSBT [56] H X X X CPF [57] X X X X X CSK [58] X X X X CT [59] H X X X CXT [60] B X X X DFT [61] X X X X X FOT [62] X X X X X FRAG [63] X X X X IVT [29] X X X X X KMS [30] X X X X X L1APG [64] X X X X X X LOT [65] X X X X X LSHT [66] X X X H X X X LSK [67] X X X X X X LSS [68] X X X X X X X MIL [39] H X X X MTT [69] X X X X X X OAB [70] H X X X ORIA [71] X X H X X X PCOM [72] X X X X X X SCM [73] X X X X X X X X X SMS [74] X X X X SBT [75] H X X X STRUCK [40] H X X X TLD [17] X B X X X VR [76] X X X X VTD [77] X X X X X X VTS [78] X X X X X X X AOG X X X X HOG [+Color] X X X X", "startOffset": 555, "endOffset": 559}, {"referenceID": 1, "context": "TABLE 2: Tracking algorithms evaluated in the TB-100 benchmark (reproduced from [2]).", "startOffset": 80, "endOffset": 83}, {"referenceID": 1, "context": "In this section, we present comparison results on the TB50/100/CVPR2013 benchmarks [2], [3] and the VOT benchmarks [4].", "startOffset": 83, "endOffset": 86}, {"referenceID": 2, "context": "In this section, we present comparison results on the TB50/100/CVPR2013 benchmarks [2], [3] and the VOT benchmarks [4].", "startOffset": 88, "endOffset": 91}, {"referenceID": 3, "context": "In this section, we present comparison results on the TB50/100/CVPR2013 benchmarks [2], [3] and the VOT benchmarks [4].", "startOffset": 115, "endOffset": 118}, {"referenceID": 53, "context": "In our current c++ implementation, we adopt FFT in computing score pyramids as done in [54] which also utilizes multi-threads with OpenMP.", "startOffset": 87, "endOffset": 91}, {"referenceID": 39, "context": "38 Runner-up STRUCK [40] SO-DLT [6] / STRUCK [40] STRUCK [40]", "startOffset": 20, "endOffset": 24}, {"referenceID": 5, "context": "38 Runner-up STRUCK [40] SO-DLT [6] / STRUCK [40] STRUCK [40]", "startOffset": 32, "endOffset": 35}, {"referenceID": 39, "context": "38 Runner-up STRUCK [40] SO-DLT [6] / STRUCK [40] STRUCK [40]", "startOffset": 45, "endOffset": 49}, {"referenceID": 39, "context": "38 Runner-up STRUCK [40] SO-DLT [6] / STRUCK [40] STRUCK [40]", "startOffset": 57, "endOffset": 61}, {"referenceID": 39, "context": "99 Runner-up STRUCK [40] TLD [17] SCM [73] MIL [39]", "startOffset": 20, "endOffset": 24}, {"referenceID": 16, "context": "99 Runner-up STRUCK [40] TLD [17] SCM [73] MIL [39]", "startOffset": 29, "endOffset": 33}, {"referenceID": 72, "context": "99 Runner-up STRUCK [40] TLD [17] SCM [73] MIL [39]", "startOffset": 38, "endOffset": 42}, {"referenceID": 38, "context": "99 Runner-up STRUCK [40] TLD [17] SCM [73] MIL [39]", "startOffset": 47, "endOffset": 51}, {"referenceID": 1, "context": "TABLE 3: Performance gain (in %) of our AOGTracker in term of success rate and precision rate in the benchmark [2].", "startOffset": 111, "endOffset": 114}, {"referenceID": 4, "context": "Two deep learning based trackers, CNT [5] and SO-DLT [6], are evaluated in TB-CVPR2013 using OPE (with their performance plots manually added in the left-bottom figure).", "startOffset": 38, "endOffset": 41}, {"referenceID": 5, "context": "Two deep learning based trackers, CNT [5] and SO-DLT [6], are evaluated in TB-CVPR2013 using OPE (with their performance plots manually added in the left-bottom figure).", "startOffset": 53, "endOffset": 56}, {"referenceID": 1, "context": "Usually, success plots are preferred to rank trackers [2], [4] (thus we focus on success plots in comparison).", "startOffset": 54, "endOffset": 57}, {"referenceID": 3, "context": "Usually, success plots are preferred to rank trackers [2], [4] (thus we focus on success plots in comparison).", "startOffset": 59, "endOffset": 62}, {"referenceID": 1, "context": "More details on the attributes and their distributions in the benchmark are referred to [2], [3].", "startOffset": 88, "endOffset": 91}, {"referenceID": 2, "context": "More details on the attributes and their distributions in the benchmark are referred to [2], [3].", "startOffset": 93, "endOffset": 96}, {"referenceID": 1, "context": "See more details about categorizing these trackers in [2].", "startOffset": 54, "endOffset": 57}, {"referenceID": 4, "context": "In TB-CVPR2013, two recent trackers trained by deep convolutional network (CNT [5], SO-DLT [6]) were evaluated using OPE.", "startOffset": 79, "endOffset": 82}, {"referenceID": 5, "context": "In TB-CVPR2013, two recent trackers trained by deep convolutional network (CNT [5], SO-DLT [6]) were evaluated using OPE.", "startOffset": 91, "endOffset": 94}, {"referenceID": 5, "context": "We note that for OPE in TB-CVPR2013, although the improvement of our AOGTracker over the SO-DLT [6] is not very big, the SO-DLT utilized two deep convolutional networks with different model update strategies in tracking, both of which are pretrained on the ImageNet [34].", "startOffset": 96, "endOffset": 99}, {"referenceID": 33, "context": "We note that for OPE in TB-CVPR2013, although the improvement of our AOGTracker over the SO-DLT [6] is not very big, the SO-DLT utilized two deep convolutional networks with different model update strategies in tracking, both of which are pretrained on the ImageNet [34].", "startOffset": 266, "endOffset": 270}, {"referenceID": 16, "context": ", TLD [17] uses intensity based Haar like features).", "startOffset": 6, "endOffset": 10}, {"referenceID": 57, "context": "More importantly, we address the issue of learning the optimal deformable part-based configurations in the quantized space of latent object structures, while most of other trackers focus on either whole objects [58] or implicit configurations (e.", "startOffset": 211, "endOffset": 215}, {"referenceID": 57, "context": ", CSK [58] and STRUCK [40]), our AOGTracker runs tracking-by-parsing in feature pyramid to detect scale changes (e.", "startOffset": 6, "endOffset": 10}, {"referenceID": 39, "context": ", CSK [58] and STRUCK [40]), our AOGTracker runs tracking-by-parsing in feature pyramid to detect scale changes (e.", "startOffset": 22, "endOffset": 26}, {"referenceID": 78, "context": "The plots for VOT2013 and 2014 might be different compared to those in the original VOT reports [80], [81] due to the new version of vot-toolkit.", "startOffset": 96, "endOffset": 100}, {"referenceID": 79, "context": "The plots for VOT2013 and 2014 might be different compared to those in the original VOT reports [80], [81] due to the new version of vot-toolkit.", "startOffset": 102, "endOffset": 106}, {"referenceID": 78, "context": "The VOT2013 dataset [80] has 16 sequences which was selected from a large pool such that various visual phenomena like occlusion and illumination changes, were still represented well within the selection.", "startOffset": 20, "endOffset": 24}, {"referenceID": 78, "context": "The readers are referred to the VOT technical report [80] for details.", "startOffset": 53, "endOffset": 57}, {"referenceID": 78, "context": ", PLT [80], LGT [82] and LGTpp [83], and PLT was the winner in VOT2013 challenge).", "startOffset": 6, "endOffset": 10}, {"referenceID": 80, "context": ", PLT [80], LGT [82] and LGTpp [83], and PLT was the winner in VOT2013 challenge).", "startOffset": 16, "endOffset": 20}, {"referenceID": 81, "context": ", PLT [80], LGT [82] and LGTpp [83], and PLT was the winner in VOT2013 challenge).", "startOffset": 31, "endOffset": 35}, {"referenceID": 79, "context": "The VOT2014 dataset [81] has 25 sequences extended from VOT2013.", "startOffset": 20, "endOffset": 24}, {"referenceID": 79, "context": "Details on the trackers are referred to [81].", "startOffset": 40, "endOffset": 44}, {"referenceID": 82, "context": "The VOT2015 dataset [84] consists of 60 short sequences (with rotated bounding box annotations) and VOT-TIR2015 comprises 20 sequences (with bounding box annotations).", "startOffset": 20, "endOffset": 24}, {"referenceID": 82, "context": "The details are referred to the reports [84] due to space limit here.", "startOffset": 40, "endOffset": 44}, {"referenceID": 83, "context": "The compositional property embedded in an AOG naturally leads to different bottom-up/topdown computing schemes such as the three computing processes studied by Wu and Zhu [85].", "startOffset": 171, "endOffset": 175}, {"referenceID": 84, "context": "We are trying to learn near optimal decision policies for tracking using the framework proposed by Wu and Zhu [86].", "startOffset": 110, "endOffset": 114}, {"referenceID": 7, "context": "In our future work, we will extend the TLP framework by incorporating generic category-level AOGs [8] to scale up the TLP framework.", "startOffset": 98, "endOffset": 101}, {"referenceID": 33, "context": ", using the PASCAL VOC [79] or the imagenet [34]), and will help the online learning of specific AOGs for a target object (e.", "startOffset": 44, "endOffset": 48}, {"referenceID": 85, "context": "Furthermore, we are also interested in integrating scene grammar [87] and event grammar [88] to leverage more top-down information.", "startOffset": 65, "endOffset": 69}], "year": 2016, "abstractText": "This paper presents a method, called AOGTracker, for simultaneously tracking, learning and parsing (TLP) of unknown objects in video sequences with a hierarchical and compositional And-Or graph (AOG) representation. The TLP method is formulated in the Bayesian framework with a spatial and a temporal dynamic programming (DP) algorithms inferring object bounding boxes on-the-fly. During online learning, the AOG is discriminatively learned using latent SVM [1] to account for appearance (e.g., lighting and partial occlusion) and structural (e.g., different poses and viewpoints) variations of a tracked object, as well as distractors (e.g., similar objects) in background. Three key issues in online inference and learning are addressed: (i) maintaining purity of positive and negative examples collected online, (ii) controling model complexity in latent structure learning, and (iii) identifying critical moments to re-learn the structure of AOG based on its intrackability. The intrackability measures uncertainty of an AOG based on its score maps in a frame. In experiments, our AOGTracker is tested on two popular tracking benchmarks with the same parameter setting: the TB-100/50/CVPR2013 benchmarks [2], [3], and the VOT benchmarks [4] \u2014 VOT 2013, 2014, 2015 and TIR2015 (thermal imagery tracking). In the former, our AOGTracker outperforms state-of-the-art tracking algorithms including two trackers based on deep convolutional network [5], [6]. In the latter, our AOGTracker outperforms all other trackers in VOT2013 and is comparable to the state-of-the-art methods in VOT2014, 2015 and TIR2015.", "creator": "LaTeX with hyperref package"}}}