{"id": "1509.08101", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "27-Sep-2015", "title": "Representation Benefits of Deep Feedforward Networks", "abstract": "This note provides a family of classification problems, indexed by a positive integer $k$, where all shallow networks with fewer than exponentially (in $k$) many nodes exhibit error at least $1/3$, whereas a deep network with 2 nodes in each of $2k$ layers achieves zero error, as does a recurrent network with 3 distinct nodes iterated $k$ times. The proof is elementary, and the networks are standard feedforward networks with ReLU (Rectified Linear Unit) nonlinearities.", "histories": [["v1", "Sun, 27 Sep 2015 15:26:58 GMT  (41kb,D)", "http://arxiv.org/abs/1509.08101v1", null], ["v2", "Tue, 29 Sep 2015 13:44:37 GMT  (41kb,D)", "http://arxiv.org/abs/1509.08101v2", null]], "reviews": [], "SUBJECTS": "cs.LG cs.NE", "authors": ["matus telgarsky"], "accepted": false, "id": "1509.08101"}, "pdf": {"name": "1509.08101.pdf", "metadata": {"source": "CRF", "title": "Representation Benefits of Deep Feedforward Networks", "authors": ["Matus Telgarsky"], "emails": [], "sections": [{"heading": "1 Overview", "text": "A neuronal network is a function whose rating is defined by a graph as: (1): (1): (1): (1): (1): (1): (1): (1): (1): (1): (1): (1): (1): (1): (1): (2): (2): (2): (2): (2): (2): (2): (2): (2): (2): (2): (2): (2): (2): (2): (2): (2): (2): (2): (2): (2): (2): (2): (2): (4): (4): (4): (4): (4): (4: (4): (4): (4): (4): (4: (4): (4): (4): (4): (4): (4): (4): (4): (4): (2): (4): (4): (4): (2): (4): (4): (4): (4): (4): (4): (2): (4): (4): (4): (4): (4): (4): (4: (4): (4): (4): (4): (2): (4): (4: (4): (4): (4): (4): (4: (4): (4): (4: (4): (4): (4): (2: (4): (4): (2: (4): (2): (2): (4): (2: (2): (2: 2): (2: 2): (2: 2: 2: 2: 2: 2: 2: 2: 2): 2: 2: 2: 2: 2: 2: 2: 2): 2: 2: 2: 2: 2: 2: 2): 2: 2: 2: 2: 2: 2: 2: 2"}, {"heading": "2 Analysis", "text": "This section will first prove the lower limit by a counter argument by simply tracking how often a function within N (\u03c3; m, l) can cross 1 / 2. The upper limit will have a network in N (\u03c3r; 2, 2) that can compose itself k-times to fit exactly on the n-ap. These limits together prove theorem 1.2, which in turn implies theorem 1.1."}, {"heading": "2.1 Lower bound", "text": "It is as if it is a way in which it is a way in which it is a way in which it is a way in which it is a way in which it is a way in which it is a way in which it is a way in which it is a way in which it is about a way in which it is about a way in which it is about a way in which it is about a way in which it is about a way in which it is about a way in which it is about a way in which it is about a way in which it is about a way in which it is about a way in which it is about a way in which it is about a way in which it is about a way in which it is about a way in which it is about a way in which it is about a way in which it is about a way in which it is about a way in which it is about a way in which it is about a way in which it is about a way in which it is about a way in which it is about a way in which it is about a way in which it is about a way in which it is about a way in which it is about which it is about a way in which it is about which it is about which it is about which it is about which it is in which it is in which it is in which it is in which it is in which it is in which it is in which it is in which it is in which it is in which way in which it is in which it is in which it is not in which it is in which it is in which it is in which it is in which it is in which it is in which it is in which it is in which it is in which it is in which it is in which it is in which it is in which way which it is in which it is in which it is in which it is in which it is in which it is which it is which it is in which it is which it is in which it is in which it is in which it is in which it is in which it is in which it is in which it is which it is which it is in which it is in which it is in which it is which it is in which it is which it is in which it is which it is in which it is in which it is which that way which it is in which which it is which it is in which it is in which it is which it is in which"}, {"heading": "2.3 Proof of Theorems 1.1 and 1.2", "text": "It is sufficient to prove theorem 1,2, which gives theorem 1,1, since \u03c3r is 2-saw tooth, whereby the condition m \u2264 2 (k \u2212 1) / l \u2212 1 implicitly \u2212 4 (2 m) l3n = 1 \u2212 (2 m) l2 \u2212 k (43) \u2265 1 \u2212 2k \u2212 12 \u2212 k (43) = 1 \u2212 23 and the upper limit since R (p; 2, 2; k) N (p; 2, 2k) is transferred. Following theorem 1,2, each f-N (n; m, l) is a (tm) l-saw tooth of lemma 2.1, with lemma 2.2 being the lower limit. For the upper limit, it should be noted that fkm-R (p; 2, 2; k) N (p; 2, 2k) is by construction and also fkm (xi) = f-k m (xi) = yi at each (xi, yi) in the n-ap of lemma 2.4."}, {"heading": "3 Related work", "text": "The classical standard result on the representational power of neural networks can be traced back to Cybenko (1989), who proved that neural networks can arbitrarily approximate continuous functions over [0, 1] d. However, this result is for flat networks. An early result showing the advantages of depth is due to H astad (1986), who established with incredible proof that Boolean circuits consisting only of and gates and or gates require exponential size to approximate the parity function well. These gates correspond to multiplication and addition over the Boolean range, and beyond that, the parity function is the Fourier base over the Boolean range; as mentioned above, fkm is a piecewise affinity approach to a Fourier base, and it was previously proposed by Bengio and LeCun (2007) that Fourier transforms efficient representations with deep networks."}], "references": [], "referenceMentions": [], "year": 2017, "abstractText": "This note provides a family of classification problems, indexed by a positive integer k, where all shallow networks with fewer than exponentially (in k) many nodes exhibit error at least 1/3, whereas a deep network with 2 nodes in each of 2k layers achieves zero error, as does a recurrent network with 3 distinct nodes iterated k times. The proof is elementary, and the networks are standard feedforward networks with ReLU (Rectified Linear Unit) nonlinearities.", "creator": "LaTeX with hyperref package"}}}