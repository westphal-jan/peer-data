{"id": "1509.08102", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "27-Sep-2015", "title": "Discriminative Learning of the Prototype Set for Nearest Neighbor Classification", "abstract": "The nearest neighbor classifier is one of the most widely used models for classification. The selection of prototype instances is an important problem for its applications, as the efficiency of the model largely depends on the compactness of the prototype set. The existing studies on prototype selection have been primarily motivated by instance-based analyses of the class distribution and consistency among local neighbors. In this paper, we explore an approximation and a parametrization of the nearest neighbor rule, which allows us to formulate the violation of the rule over the training set with regards to the prototypes and take advantage of a discriminative learning principle. We show that our approach reduces to a large-margin learning based on a sparse representation of the relations among the neighbors. We demonstrate the advantage of the proposed algorithm by empirical comparison with the recent state-of-the-art methods over a collection of public benchmarks.", "histories": [["v1", "Sun, 27 Sep 2015 15:43:33 GMT  (94kb,D)", "https://arxiv.org/abs/1509.08102v1", null], ["v2", "Thu, 31 Dec 2015 00:44:17 GMT  (105kb,D)", "http://arxiv.org/abs/1509.08102v2", null], ["v3", "Tue, 5 Jan 2016 07:49:27 GMT  (105kb,D)", "http://arxiv.org/abs/1509.08102v3", null], ["v4", "Sun, 17 Jan 2016 11:54:57 GMT  (117kb,D)", "http://arxiv.org/abs/1509.08102v4", null], ["v5", "Sun, 21 Aug 2016 14:40:20 GMT  (763kb,D)", "http://arxiv.org/abs/1509.08102v5", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["shin", "o"], "accepted": false, "id": "1509.08102"}, "pdf": {"name": "1509.08102.pdf", "metadata": {"source": "CRF", "title": "Discriminative Learning of the Prototype Set for Nearest Neighbor Classification", "authors": ["Shin Ando"], "emails": ["ando@rs.tus.ac.jp"], "sections": [{"heading": null, "text": "Tags Next Neighbor Classifier, Prototype Selection, Soft Maximum, Great Learning"}, {"heading": "1 Introduction", "text": "In fact, most of them are able to survive on their own."}, {"heading": "4 Algorithm", "text": "The procedure of prototype selection by REPS is briefly summarized as follows: - Calculation of the rank matrix from the input distance matrix - Calculation of parameter values in the constraints - Solution of the quadratic programming problem with respect to degradation parameters - Elimination of candidates with high degradation parameters. A more detailed description of the algorithm is given in Algorithm 1. To address large data sets, we used an efficient approximation algorithm for SVM training, shown in Algorithm 2. Algorithm 2 is a modification of the structural SVM [21] learning algorithm, which obtains the approximate solution to the linear SVM problem using the cutting plan method. It solves a modified SVM problem in which Rashi is used as a relatively scaled margins of the respective prototypes. In terms of computational requirements, the complexity of the space instances is dominated by the respective inscription matrix (where the inscription time between the instances)."}, {"heading": "5 Empirical Results", "text": "The empirical results of this study are usually such that it is able to analyze its results as they are applied in practice. (...) The results of this study are as they are applied in practice. (...) The results of this study are as they are applied in practice. (...) The results of this study are as they are applied in practice. (...) The results of the study are as they are applied in practice. (...) The results of the study are as they are applied in practice. (...) The results of the study are as they are applied in practice. (...) The results of the study are as they are applied in practice. (...) The results of the study are as they are applied in practice. (...) We are as they are in practice. (...) The results of the study are as they are applied in practice. (...)"}, {"heading": "5.4.1 Vector Data", "text": "Fig. 7 in appendix B illustrates the error and selection rates of the weighted algorithms. The x and y axes show the values of the respective measures. Each marker represents the performance of an algorithm on a dataset. Each algorithm is distinguished by a unique color and shape of the marker. Numbers within the x axis show the advantage in terms of compactness and displacement towards the y axis."}, {"heading": "5.4.2 Time Series Data", "text": "Comparison between individual countries and individual countries is very difficult."}, {"heading": "6 Conclusion", "text": "This work was an extension of the rule of the closest neighbor, which is based on adjusting the precedence order to parameterize the prototype selection problem and also approximate the violation of the rule on the training set. Consequently, the problem is defined as discriminatory learning in a sparse feature space. Our empirical results showed that it competes with the modern prototype selection algorithm, which has the advantage over many other existing algorithms in multi-level comparisons. The key intuition for this approach is to avoid an intermediate assessment and to learn the priorities of the prototypes in direct relation to performance over the training set. In fact, the proposed approach takes advantage of the sparse relationships between adjacent instances rather than using dense representations such as feature vectors and the distance matrix. Discrediting such a large amount of information is justified because the successful empirical results of the k-next-neighbor algorithm are used in many applications under a small split."}, {"heading": "A Tables", "text": "It is not the first time that the United States has taken a hard line on this issue. (...) Nor is it the first time that the United States has engaged in a war. (...) Nor is it the first time that the United States has engaged in a war. (...) It is not the first time that the United States has engaged in a war. (...) It is the first time that the United States has engaged in a war. (...) It is the first time that the United States has engaged in a war. (...) It is the second time that the United States has engaged in a war. (...) It is the second time that the United States has engaged in a war. (...) It is the second time that the United States has engaged in a war. (...)"}, {"heading": "B Figures", "text": "FI g.6 Prot otype eN eare stN eigh borR elat ions Gra ph (E CG) 11 22 33 44 55 66778899101112131415151516161717 18181919191920212222232324242525 26262627272929 3030313232343411 22 33445566778899101112131313131313131313131314141414141414151616 1717 ec181920212222232424242424242526262828282828282929292929261 2929303034341122333411 # 3344556677 889910111213141420202122232232323232323232424242424242626271."}, {"heading": "C Definition of Pareto-rank", "text": "The dominance between two algorithms i and j is defined as follows: Algorithm i is dominated by j if j is better than i in at least one target and equal or better than i in all other objectives. The exact definition of dominated and undominated relations can be described as follows: (ERRi) fj if (ERRj) fj that fi is dominated by fj, and fi (ERRi < ERRj) that fi (SLRi < SLRj) is not dominated. (16) fi (fj) otherwise (17) In view of a series of tuples P = (ERRi, SLRi), the boundary of P (ERRi, SLRi) is defined as the boundary of P."}], "references": [{"title": "Keel: a software tool to assess evolutionary algorithms for data mining problems", "author": ["J. Alcal\u00e0-Fdez", "L. S\u00e1nchez", "S. Garc\u0131\u0301a", "M. del Jesus", "S. Ventura", "J. Garrell", "J. Otero", "C. Romero", "J. Bacardit", "V. Rivas", "J. Fern\u00e1ndez", "F. Herrera"], "venue": "Soft Computing 13(3), 307\u2013318", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2009}, {"title": "Near-optimal hashing algorithms for approximate nearest neighbor in high dimensions", "author": ["A. Andoni", "P. Indyk"], "venue": "Commun. ACM 51(1), 117\u2013122", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2008}, {"title": "Fast nearest neighbor condensation for large data sets classification", "author": ["F. Angiulli"], "venue": "IEEE Trans. on Knowl. and Data Eng. 19(11), 1450\u20131464", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2007}, {"title": "UCI Machine Learning Repository", "author": ["K. Bache", "M. Lichman"], "venue": "http://archive.ics.uci.edu/ml", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2013}, {"title": "Using Dynamic Time Warping to Find Patterns in Time Series", "author": ["D.J. Berndt", "J. Clifford"], "venue": "Proceedings of KDD-94: AAAI Workshop on Knowledge Discovery in Databases, pp. 359\u2013370. Seattle, Washington", "citeRegEx": "5", "shortCiteRegEx": null, "year": 1994}, {"title": "Convex Optimization", "author": ["S. Boyd", "L. Vandenberghe"], "venue": "Cambridge University Press, New York, NY, USA", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2004}, {"title": "Advances in instance selection for instance-based learning algorithms", "author": ["H. Brighton", "C. Mellish"], "venue": "Data Min. Knowl. Discov. 6(2), 153\u2013172", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2002}, {"title": "Using evolutionary algorithms as instance selection for data reduction in kdd: An experimental study", "author": ["J.R. Cano", "F. Herrera", "M. Lozano"], "venue": "Trans. Evol. Comp 7(6), 561\u2013575", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2003}, {"title": "The UCR time series classification archive", "author": ["Y. Chen", "E. Keogh", "B. Hu", "N. Begum", "A. Bagnall", "A. Mueen", "G. Batista"], "venue": null, "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2015}, {"title": "A fast and elitist multiobjective genetic algorithm: NSGA-II", "author": ["K. Deb", "A. Pratap", "S. Agarwal", "T. Meyarivan"], "venue": "IEEE Trans. on Evolutionary Computation 6, 182\u2013197", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2002}, {"title": "Statistical comparisons of classifiers over multiple data sets", "author": ["J. Dem\u0161ar"], "venue": "J. Mach. Learn. Res. 7, 1\u201330", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2006}, {"title": "Condensed and edited nearest neighbor rules", "author": ["L. Devroye", "L. Gy\u00f6rfi", "G. Lugosi"], "venue": "A Probabilistic Theory of Pattern Recognition, Stochastic Modelling and Applied Probability, vol. 31, pp. 303\u2013313. Springer New York", "citeRegEx": "12", "shortCiteRegEx": null, "year": 1996}, {"title": "Querying and mining of time series data: Experimental comparison of representations and distance measures", "author": ["H. Ding", "G. Trajcevski", "P. Scheuermann", "X. Wang", "E. Keogh"], "venue": "Proc. VLDB Endow. 1(2), 1542\u20131552", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2008}, {"title": "A novel template reduction approach for the k-nearest neighbor method", "author": ["H.A. Fayed", "A.F. Atiya"], "venue": "Trans. Neur. Netw. 20(5), 890\u2013896", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2009}, {"title": "Bayesian instance selection for the nearest neighbor rule", "author": ["S. Ferrandiz", "M. Boull\u00e9"], "venue": "Mach. Learn. 81(3), 229\u2013256", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2010}, {"title": "Prototype reduction using an artificial immune model", "author": ["U. Garain"], "venue": "Pattern Anal. Appl. 11(3-4), 353\u2013363", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2008}, {"title": "A memetic algorithm for evolutionary prototype selection: A scaling up approach", "author": ["S. Garc\u0131\u0301a", "J.R. Cano", "F. Herrera"], "venue": "Pattern Recogn. 41(8), 2693\u20132709", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2008}, {"title": "Prototype selection for nearest neighbor classification: Taxonomy and empirical study", "author": ["S. Garcia", "J. Derrac", "J. Cano", "F. Herrera"], "venue": "IEEE Trans. Pattern Anal. Mach. Intell. 34(3), 417\u2013435", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2012}, {"title": "Constructing ensembles of classifiers by means of weighted instance selection", "author": ["N. Garc\u0131\u0301a-Pedrajas"], "venue": "Trans. Neur. Netw. 20(2), 258\u2013277", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2009}, {"title": "Boosting instance selection algorithms", "author": ["N. Garc\u0131\u0301a-Pedrajas", "A. De Haro-Garc\u0131\u0301a"], "venue": "Know.-Based Syst. 67, 342\u2013360", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2014}, {"title": "Cutting-plane Training of Structural SVMs", "author": ["T. Joachims", "T. Finley", "C.N.J. Yu"], "venue": "Mach. Learn. 77, 27\u201359", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2009}, {"title": "Classification of Multivariate Time Series and Structured Data Using Constructive Induction", "author": ["M.W. Kadous", "C. Sammut"], "venue": "Mach. Learn. 58(2-3), 179\u2013216", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2005}, {"title": "Fast minimization of structural risk by nearest neighbor rule", "author": ["B. Karacali", "H. Krim"], "venue": "Trans. Neur. Netw. 14(1), 127\u2013137", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2003}, {"title": "Iterative Methods for Optimization", "author": ["C. Kelley"], "venue": "Frontiers in Applied Mathematics. Society for Industrial and Applied Mathematics", "citeRegEx": "24", "shortCiteRegEx": null, "year": 1999}, {"title": "Maximum margin multiclass nearest neighbors", "author": ["A. Kontorovich", "R. Weiss"], "venue": "Proceedings of The 31st International Conference on Machine Learning, vol. 32, pp. 892\u2013900", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2014}, {"title": "A clustering method for automatic biometric template selection", "author": ["A. Lumini", "L. Nanni"], "venue": "Pattern Recognition 39(3), 495 \u2013 497", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2006}, {"title": "Class conditional nearest neighbor for large margin instance selection", "author": ["E. Marchiori"], "venue": "IEEE Trans. Pattern Anal. Mach. Intell. 32(2), 364\u2013370", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2010}, {"title": "Distribution-free multiple comparisons", "author": ["P. Nemenyi"], "venue": "Ph.D. thesis, Princeton University", "citeRegEx": "28", "shortCiteRegEx": null, "year": 1963}, {"title": "Generalized feature extraction for structural pattern recognition in time-series data", "author": ["R.T. Olszewski"], "venue": "Ph.D. thesis, Carnegie Mellon University, Pittsburgh, PA, USA", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2001}, {"title": "A new fast prototype selection method based on clustering", "author": ["J.A. Olvera-L\u00f3pez", "J.A. Carrasco-Ochoa", "J.F. Mart\u0131\u0301nez-Trinidad"], "venue": "Pattern Anal. Appl. 13(2), 131\u2013141", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2010}, {"title": "A review of instance selection methods", "author": ["J.A. Olvera-L\u00f3pez", "J.A. Carrasco-Ochoa", "J.F. Mart\u0131\u0301nez-Trinidad", "J. Kittler"], "venue": "Artif. Intell. Rev. 34(2), 133\u2013143", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2010}, {"title": "Locality sensitive hashing: A comparison of hash function types and querying mechanisms", "author": ["L. Paulev\u00e9", "H. J\u00e9gou", "L. Amsaleg"], "venue": "Pattern Recogn. Lett. 31(11), 1348\u20131358", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2010}, {"title": "Prototype selection for dissimilarity-based classifiers", "author": ["E. Pkalska", "R.P.W. Duin", "P. Pacl\u0131\u0301k"], "venue": "Pattern Recogn. 39(2), 189\u2013208", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2006}, {"title": "Finding representative patterns with ordered projections", "author": ["J.C. Riquelme", "J.S. Aguilar-Ruiz", "M. Toro"], "venue": "Pattern Recognition 36(4), 1009 \u2013 1018", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2003}, {"title": "Prototype and feature selection by sampling and random mutation hill climbing algorithms", "author": ["D.B. Skalak"], "venue": "Proceedings of the Eleventh International Conference on Machine Learning, pp. 293\u2013301. Morgan Kaufmann", "citeRegEx": "35", "shortCiteRegEx": null, "year": 1994}, {"title": "Sv-knnc: An algorithm for improving the efficiency of k-nearest neighbor", "author": ["A. Srisawat", "T. Phienthrakul", "B. Kijsirikul"], "venue": "Q. Yang, G. Webb (eds.) PRICAI 2006: Trends in Artificial Intelligence, Lecture Notes in Computer Science, vol. 4099, pp. 975\u2013 979. Springer Berlin Heidelberg", "citeRegEx": "36", "shortCiteRegEx": null, "year": 2006}, {"title": "A taxonomy and experimental study on prototype generation for nearest neighbor classification", "author": ["I. Triguero", "J. Derrac", "S. Garcia", "F. Herrera"], "venue": "Trans. Sys. Man Cyber Part C 42(1), 86\u2013100", "citeRegEx": "37", "shortCiteRegEx": null, "year": 2012}, {"title": "Distance Metric Learning for Large Margin Nearest Neighbor Classification", "author": ["K.Q. Weinberger", "L.K. Saul"], "venue": "J. Mach. Learn. Res. 10, 207\u2013244", "citeRegEx": "38", "shortCiteRegEx": null, "year": 2009}, {"title": "Reduction techniques for instance-basedlearning algorithms", "author": ["D.R. Wilson", "T.R. Martinez"], "venue": "Mach. Learn. 38(3), 257\u2013286", "citeRegEx": "39", "shortCiteRegEx": null, "year": 2000}, {"title": "The Top Ten Algorithms in Data Mining, 1st edn", "author": ["X. Wu", "V. Kumar"], "venue": "Chapman & Hall/CRC", "citeRegEx": "40", "shortCiteRegEx": null, "year": 2009}, {"title": "Fast Time Series Classification using Numerosity Reduction", "author": ["X. Xi", "E. Keogh", "C. Shelton", "L. Wei", "C.A. Ratanamahatana"], "venue": "ICML \u201906: Proceedings of the 23rd International Conference on Machine Learning, pp. 1033\u20131040. ACM, New York, NY, USA", "citeRegEx": "41", "shortCiteRegEx": null, "year": 2006}, {"title": "Optimal reference subset selection for nearest neighbor classification by tabu search", "author": ["H. Zhang", "G. Sun"], "venue": "Pattern Recognition 35(7), 1481 \u2013 1490", "citeRegEx": "42", "shortCiteRegEx": null, "year": 2002}], "referenceMentions": [{"referenceID": 30, "context": "Keywords Nearest neighbor classifier, Prototype selection, Soft maximum, Large-margin learning 1 Introduction Applying the nearest neighbor rule based on a set of prototype instances is one of the most widely used models for classification and a typical example of non-parametric, instance-based learning algorithms [31,40].", "startOffset": 316, "endOffset": 323}, {"referenceID": 39, "context": "Keywords Nearest neighbor classifier, Prototype selection, Soft maximum, Large-margin learning 1 Introduction Applying the nearest neighbor rule based on a set of prototype instances is one of the most widely used models for classification and a typical example of non-parametric, instance-based learning algorithms [31,40].", "startOffset": 316, "endOffset": 323}, {"referenceID": 6, "context": "It is a critical problem for practical applications due to the large cost of storing and computing distances in the neighbor algorithms [7].", "startOffset": 136, "endOffset": 139}, {"referenceID": 36, "context": "Improvements by sophisticated hybridization of such techniques have also been reported [37].", "startOffset": 87, "endOffset": 91}, {"referenceID": 30, "context": "Alternatively, critical parameters are selected empirically through cross validation and wrapper methods [31].", "startOffset": 105, "endOffset": 109}, {"referenceID": 1, "context": "2 Shin Ando Recently, the emergence of Big Data has ignited more approaches to efficiently compute the nearest neighbor algorithm, such as Locality Sensitive Hashing and Approximate Nearest Neighbor Search [2,32].", "startOffset": 206, "endOffset": 212}, {"referenceID": 31, "context": "2 Shin Ando Recently, the emergence of Big Data has ignited more approaches to efficiently compute the nearest neighbor algorithm, such as Locality Sensitive Hashing and Approximate Nearest Neighbor Search [2,32].", "startOffset": 206, "endOffset": 212}, {"referenceID": 17, "context": "First, the state-of-the-art prototype selection methods have the advantage in terms of classification accuracy and are competitive in terms of efficiency compared to ANNS [18].", "startOffset": 171, "endOffset": 175}, {"referenceID": 21, "context": ", in time series classification, the Euclidean distance is often problematic due to the sequential structure of the time series [22], and a dissimilarity function based on non-linear warping is known to be highly effective [13,41].", "startOffset": 128, "endOffset": 132}, {"referenceID": 12, "context": ", in time series classification, the Euclidean distance is often problematic due to the sequential structure of the time series [22], and a dissimilarity function based on non-linear warping is known to be highly effective [13,41].", "startOffset": 223, "endOffset": 230}, {"referenceID": 40, "context": ", in time series classification, the Euclidean distance is often problematic due to the sequential structure of the time series [22], and a dissimilarity function based on non-linear warping is known to be highly effective [13,41].", "startOffset": 223, "endOffset": 230}, {"referenceID": 17, "context": "The problem has been referred to as prototype selection [18], data reduction [39], instance selection [19,20,27], and template reduction [14,33].", "startOffset": 56, "endOffset": 60}, {"referenceID": 38, "context": "The problem has been referred to as prototype selection [18], data reduction [39], instance selection [19,20,27], and template reduction [14,33].", "startOffset": 77, "endOffset": 81}, {"referenceID": 18, "context": "The problem has been referred to as prototype selection [18], data reduction [39], instance selection [19,20,27], and template reduction [14,33].", "startOffset": 102, "endOffset": 112}, {"referenceID": 19, "context": "The problem has been referred to as prototype selection [18], data reduction [39], instance selection [19,20,27], and template reduction [14,33].", "startOffset": 102, "endOffset": 112}, {"referenceID": 26, "context": "The problem has been referred to as prototype selection [18], data reduction [39], instance selection [19,20,27], and template reduction [14,33].", "startOffset": 102, "endOffset": 112}, {"referenceID": 13, "context": "The problem has been referred to as prototype selection [18], data reduction [39], instance selection [19,20,27], and template reduction [14,33].", "startOffset": 137, "endOffset": 144}, {"referenceID": 32, "context": "The problem has been referred to as prototype selection [18], data reduction [39], instance selection [19,20,27], and template reduction [14,33].", "startOffset": 137, "endOffset": 144}, {"referenceID": 6, "context": ", an operation referred to as editing, eliminates or relocates noisy prototypes that cause misclassifications near the borders of different classes [7,39], and have the effects of smoothing and generalizing the decision boundaries.", "startOffset": 148, "endOffset": 154}, {"referenceID": 38, "context": ", an operation referred to as editing, eliminates or relocates noisy prototypes that cause misclassifications near the borders of different classes [7,39], and have the effects of smoothing and generalizing the decision boundaries.", "startOffset": 148, "endOffset": 154}, {"referenceID": 2, "context": "Condensation is another typical operation which discards prototypes far from the class borders in order to reduce the redundancy of the prototype set without affecting the decision boundary [3].", "startOffset": 190, "endOffset": 193}, {"referenceID": 33, "context": "Discriminative Learning of the Prototype Set for Nearest Neighbor Classification 3 as feature analysis [34] and clustering [30,37].", "startOffset": 103, "endOffset": 107}, {"referenceID": 29, "context": "Discriminative Learning of the Prototype Set for Nearest Neighbor Classification 3 as feature analysis [34] and clustering [30,37].", "startOffset": 123, "endOffset": 130}, {"referenceID": 36, "context": "Discriminative Learning of the Prototype Set for Nearest Neighbor Classification 3 as feature analysis [34] and clustering [30,37].", "startOffset": 123, "endOffset": 130}, {"referenceID": 25, "context": "are used to analyze boundaries and interior instances in hybrid methods [26,36].", "startOffset": 72, "endOffset": 79}, {"referenceID": 35, "context": "are used to analyze boundaries and interior instances in hybrid methods [26,36].", "startOffset": 72, "endOffset": 79}, {"referenceID": 30, "context": "The implementations of these techniques are generally categorized into wrapper and filtering methods [31].", "startOffset": 101, "endOffset": 105}, {"referenceID": 7, "context": "Heuristic combinatorial search such as evolutionary algorithms, a memetic algorithm, an artificial immune model, and tabu search, have been explored to find compact subsets of the training instances [8,16,17,42].", "startOffset": 199, "endOffset": 211}, {"referenceID": 15, "context": "Heuristic combinatorial search such as evolutionary algorithms, a memetic algorithm, an artificial immune model, and tabu search, have been explored to find compact subsets of the training instances [8,16,17,42].", "startOffset": 199, "endOffset": 211}, {"referenceID": 16, "context": "Heuristic combinatorial search such as evolutionary algorithms, a memetic algorithm, an artificial immune model, and tabu search, have been explored to find compact subsets of the training instances [8,16,17,42].", "startOffset": 199, "endOffset": 211}, {"referenceID": 41, "context": "Heuristic combinatorial search such as evolutionary algorithms, a memetic algorithm, an artificial immune model, and tabu search, have been explored to find compact subsets of the training instances [8,16,17,42].", "startOffset": 199, "endOffset": 211}, {"referenceID": 11, "context": "Theoretical approaches for prototype selection shown the upper-bound of generalization error based on VC dimensions, structural risk minimization, and Bayesian analysis [12,15,23].", "startOffset": 169, "endOffset": 179}, {"referenceID": 14, "context": "Theoretical approaches for prototype selection shown the upper-bound of generalization error based on VC dimensions, structural risk minimization, and Bayesian analysis [12,15,23].", "startOffset": 169, "endOffset": 179}, {"referenceID": 22, "context": "Theoretical approaches for prototype selection shown the upper-bound of generalization error based on VC dimensions, structural risk minimization, and Bayesian analysis [12,15,23].", "startOffset": 169, "endOffset": 179}, {"referenceID": 1, "context": "In recent years, Approximate Nearest Neighbor Search (ANNS) [2] have drawn strong interest and various techniques have been developed for image classification.", "startOffset": 60, "endOffset": 63}, {"referenceID": 17, "context": "As shown in the recent survey [18], the prototype selection methods aimed at reducing the generalization error achieve better accuracy than ANNS.", "startOffset": 30, "endOffset": 34}, {"referenceID": 37, "context": "Large-margin Nearest Neighbor Classification [38] addresses the problem of distance metric learning as a constrained optimization problem to minimize the violation of the nearest neighbor rule with regards to the affine transformation.", "startOffset": 45, "endOffset": 49}, {"referenceID": 24, "context": "Max-margin Multiclass Nearest Neighbors [25] also learns the metric space based on the minimization of its entropy.", "startOffset": 40, "endOffset": 44}, {"referenceID": 5, "context": "2 Soft-Maximum Function The soft maximum [6] is an approximation of the function max(\u00b7, \u00b7) as log (exp(\u00b7) + exp(\u00b7)).", "startOffset": 41, "endOffset": 44}, {"referenceID": 23, "context": "Given the form of (14), the problem is solved by a constrained gradient descent [24].", "startOffset": 80, "endOffset": 84}, {"referenceID": 20, "context": "Algorithm 2 is a modification of the Structural SVM [21] learning algorithm, which obtains the approximate solution to the linear SVM problem using the cutting-plane method.", "startOffset": 52, "endOffset": 56}, {"referenceID": 3, "context": "1 Datasets For the first part of the experiment, we employed a collection of 34 datasets with vector features from the UCI Machine Learning Repository [4] and the KEEL datasets [1].", "startOffset": 151, "endOffset": 154}, {"referenceID": 0, "context": "1 Datasets For the first part of the experiment, we employed a collection of 34 datasets with vector features from the UCI Machine Learning Repository [4] and the KEEL datasets [1].", "startOffset": 177, "endOffset": 180}, {"referenceID": 8, "context": "In the second part of the experiment, we use a collection of 27 datasets from the UCR Time Series Dataset Repository [9].", "startOffset": 117, "endOffset": 120}, {"referenceID": 4, "context": "The input data for time series classification is given as a dissimilarity matrix of Dynamic Time Warping (DTW) [5], which is known to be highly effective in time series classification [13].", "startOffset": 111, "endOffset": 114}, {"referenceID": 12, "context": "The input data for time series classification is given as a dissimilarity matrix of Dynamic Time Warping (DTW) [5], which is known to be highly effective in time series classification [13].", "startOffset": 184, "endOffset": 188}, {"referenceID": 17, "context": "2 Baseline Methods We selected five recent prototype selection methods, which ranked highly among the 42 algorithm reported in a recent survey [18], as baselines for comparative analysis.", "startOffset": 143, "endOffset": 147}, {"referenceID": 26, "context": "Class conditional instance selection (CCIS) uses pairwise relations among prototypes to define the selection criteria [27].", "startOffset": 118, "endOffset": 122}, {"referenceID": 2, "context": "Fast condensed nearest neighbor (FCNN) exploits the condensation operation for prototype selection [3].", "startOffset": 99, "endOffset": 102}, {"referenceID": 38, "context": "DROP3 uses a decremental procedure to reduce prototypes and is fast, efficient, and accurate among algorithms of the same approach [39].", "startOffset": 131, "endOffset": 135}, {"referenceID": 34, "context": "Random Mutation Hill Climbing (RMHC) uses stochastic hill climbing search to explore the prototype subset combination space [35].", "startOffset": 124, "endOffset": 128}, {"referenceID": 0, "context": "All baseline algorithms were executed in the KEEL software [1].", "startOffset": 59, "endOffset": 62}, {"referenceID": 9, "context": "First, we introduce the concept of the Pareto frontier and non-dominated sorting, commonly used in multi-objective population-based search algorithms [10], to compute the ranks of the algorithms based on two evaluation measures.", "startOffset": 150, "endOffset": 154}, {"referenceID": 10, "context": "In [11], Wilcoxon\u2019s signed ranks test and Friedman-Nemenyi test are recommended for comparison of two classifiers and three or more classifiers over multiple datasets, respectively.", "startOffset": 3, "endOffset": 7}, {"referenceID": 27, "context": "We also note that the Friedman-Nemenyi test is highly conservative, due to the substantial loss of power for handling unreplicated blocked data [28].", "startOffset": 144, "endOffset": 148}, {"referenceID": 17, "context": "SSMA and RMHC were two best algorithms in [18], which indicates that REPS has a significant advantage against many prototype selection algorithms and is competitive with the state-of-the-art.", "startOffset": 42, "endOffset": 46}, {"referenceID": 28, "context": "The ECG time series consists of positive (abnormal) and negative (normal) class examples and pose a binary classification problem [29].", "startOffset": 130, "endOffset": 134}], "year": 2016, "abstractText": "Abstract The nearest neighbor rule is one of the most widely used models for classification, and selecting a compact set of prototype instances is a primary challenges for its applications. Many existing approaches for prototype selection exploit instance-based analyses and locally-defined criteria on the class distribution, which are intractable for numerical optimization techniques. In this paper, we explore a parametric framework with an adjusted nearest neighbor rule, in which the selection of the neighboring prototypes is modified by their respective parameters. The framework allows us to formulate a minimization problem of the violation of the adjusted nearest neighbor rule over the training set with regards to numerical parameters. We show that the problem reduces to a large-margin principled learning and demonstrate its advantage by empirical comparisons with recent state-ofthe-art methods using public benchmark data.", "creator": "LaTeX with hyperref package"}}}