{"id": "1509.08112", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "27-Sep-2015", "title": "Feature Selection for classification of hyperspectral data by minimizing a tight bound on the VC dimension", "abstract": "Hyperspectral data consists of large number of features which require sophisticated analysis to be extracted. A popular approach to reduce computational cost, facilitate information representation and accelerate knowledge discovery is to eliminate bands that do not improve the classification and analysis methods being applied. In particular, algorithms that perform band elimination should be designed to take advantage of the specifics of the classification method being used. This paper employs a recently proposed filter-feature-selection algorithm based on minimizing a tight bound on the VC dimension. We have successfully applied this algorithm to determine a reasonable subset of bands without any user-defined stopping criteria on widely used hyperspectral images and demonstrate that this method outperforms state-of-the-art methods in terms of both sparsity of feature set as well as accuracy of classification.\\end{abstract}", "histories": [["v1", "Sun, 27 Sep 2015 17:36:18 GMT  (537kb,D)", "http://arxiv.org/abs/1509.08112v1", "basic papers are onthis http URL"]], "COMMENTS": "basic papers are onthis http URL", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["phool preet", "sanjit singh batra", "jayadeva"], "accepted": false, "id": "1509.08112"}, "pdf": {"name": "1509.08112.pdf", "metadata": {"source": "CRF", "title": "Feature Selection for classification of hyperspectral data by minimizing a tight bound on the VC dimension", "authors": ["Phool Preet", "Sanjit Singh Batra"], "emails": ["jayadeva@ee.iitd.ac.in"], "sections": [{"heading": null, "text": "In recent years, hyperspectral image analysis has become widely used among remote sensing communities. Hyperspectral sensors capture image data across hundreds of adjacent spectral channels (known as bands) covering a wide range of wavelengths (0.4-2.5 microns), and each hyperspectral scene is represented as an image cube. Hyperspectral data is increasingly becoming a valuable tool in many areas such as agriculture, mineralogy, monitoring, chemical imaging and automatic target recognition. A common task in such applications is to classify hyperspectral images. The wealth of information provided by hyperspectral data can enhance the ability to classify and detect materials, and the high dimensionality of hyperspectral data poses several obstacles."}, {"heading": "II. BACKGROUND AND RELATED WORK", "text": "There is a high probability that the high-dimensional method requires storage and computing power. [1] analysis supports this line of reasoning and suggests that feature selection can be a valuable method of pre-processing hyper-perspective data for classification by the widely used SVM classification. Hyperdimensionality requires storage and computing power. [1] analysis builds on this reasoning and suggests that feature selection is a valuable method for pre-processing hyper-speculative data."}, {"heading": "III. FEATURE SELECTION METHODS", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "A. PCA based Feature Selection", "text": "It transforms the data in such a way that the projections of the transformed data (referred to as the main components) show maximum variance. Chein et al. [6] represents a band prioritization method based on Principal Component Analysis. For our experiments, we consider the characteristics obtained from PCA as eigenvectors sorted by their respective eigenvalues. B. Information Theoretical feature selection techniques can be roughly divided into two categories: classifier-dependent (wrapper and embedded methods) and classifier-independent (filter methods). Wrapper methods rank the characteristics on the accuracy of a particular classifier. They have the disadvantage of being computationally expensive and classifier-specific. Embedded methods use the structure of certain classes of learning models to guide the selection process."}, {"heading": "C. RELIEF", "text": "Relief is a method of feature weight selection proposed by Kira and Rendell. Relief recognizes those features that are statistically relevant to the target concept.The algorithm starts with a weight vector W, which is initialized by zeros. With each iteration, the algorithm takes the feature vector Xk, which belongs to a random instance, and the feature vectors of the instance closest to Xk, from each class.The next instance of the same class is called a near-hit, and the next instance of the same class is called a near-miss. The weight vector is then updated to Equation 4.Wi = Wi \u2212 (xi \u2212 nearHit) 2 + (xi \u2212 nearMiss) 2 (4).The weight of a feature thus decreases when it deviates more from the value of this feature in nearby instances of the same class than nearby instances of the same class, and then increases in the reverse case, its relevance is defined to be greater when its relevance is selected."}, {"heading": "D. Feature Selection by VC Dimension Minimization", "text": "To perform the selection of features using MCM, we solve the following linear programming problem: Min w, b, h + C \u00b7 M \u2211 i = 1 qi (5) h \u2265 yi \u00b7 [wTxi + b] + qi, i = 1, 2,..., M (6) yi \u00b7 [wTxi + b] + qi \u2265 1, i = 1, 2,..., M (7) qi \u2265 0, i = 1, 2,..., M. (8), where xi, i = 1, 2,..., M are the input data and yi, i = 1, 2,..., M are the corresponding target labels. The classifier generated by solving the above problem minimizes a close connection to the VC dimension and therefore results in a classifier using a small number of features [8] [19] [20] [21] [22]."}, {"heading": "IV. EXPERIMENTAL SETUP AND RESULTS", "text": "In order to assess the classification accuracy for the multi-class data sets in this work, we use the \"one-on-one\" strategy. Each class is classified as negative training samples based on the data belonging to the rest of the classes. A Support Vector Machine Classifier [23] with an RBF kernel is balanced for classification. The box constraint parameter of SVM, C is placed on a high value to place more emphasis on the correct classification; the breadth of the Gaussian kernel is empirically selected. In order to evaluate the ability of the various methods to select the best properties in the conciseness of the training data, we evaluate the classification results for a specified test / pull ratio, while the number of attributes issued is varied by the different methods. Number of selected bands are 1, 2, 3, 4, 5, 6, 7, 9, 10, 13, 15, 15, 35, 40, 45 and we will classify the results for classification of the 50.Further, we will classify the effects of the classification of the model, also classify the effectiveness of the grades."}, {"heading": "B. Salinas", "text": "In this dataset, 20 water absorption bands [108-112], [154-167] and 224 were removed during pre-processing. A random band along with soil truth is shown in Figure 3. It includes vegetables, bare soils and vineyards. Salinas Basic Truth consists of 16 classes. The dataset is available online [?]. Table IV lists the different classes, the number of samples and the number of training and test points in the Salinas dataset that correspond to the test-pull ratio of 0.90."}, {"heading": "C. Botswana Data-Set", "text": "The Botswana dataset was acquired by the Hyperion sensor with 30 m pixel resolution over a 7.7 km long strip in 242 bands covering the 400-2500 nm part of the spectrum in 10 nm windows. Uncalibrated and noisy bands covering water absorption characteristics were removed, and the remaining 145 bands were recorded as candidate characteristics [?]. This dataset consists of observations from 14 identified classes representing land cover types in seasonal swamps, occasional swamps and drier forests. A random band along with the truth of the soil for Botswana dataset is shown in Figure 5.Table VII, which lists the number of samples and the number of training and test points in Botswana corresponding to the test reference ratio 0.90."}, {"heading": "V. CONCLUSION", "text": "This paper applies a recently proposed method for selecting filter characteristics based on minimizing a narrow boundary between the VC dimension and the task of hyperspectral image classification. We show that this method for selecting filter characteristics significantly exceeds modern methods in terms of classification accuracy, which is adequately measured in the presence of a large number of classes. Superior results obtained through different data sets and a variety of metrics suggest that the proposed method should be the method of choice for this problem. It has not escaped our attention that this method can also be applied to a variety of other high-dimensional classification tasks; we are working on developing modifications of this method for the same problem."}], "references": [{"title": "Feature selection for classification of hyperspectral data by SVM.", "author": ["Pal", "Mahesh", "Giles M. Foody"], "venue": "Geoscience and Remote Sensing, IEEE Transactions on 48.5", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2010}, {"title": "Unsupervised hyperspectral image analysis with projection pursuit.", "author": ["Ifarraguerri", "Agustin", "Chein-I. Chang"], "venue": "Geoscience and Remote Sensing, IEEE Transactions on 38, no", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2000}, {"title": "Efficient hierarchical-PCA dimension reduction for hyperspectral imagery.", "author": ["Agarwal", "Abhishek", "Tarek El-Ghazawi", "Hesham El-Askary", "Jacquline Le-Moigne"], "venue": "In Signal Processing and Information Technology,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2007}, {"title": "Unsupervised band selection for hyperspectral imagery classification without manual band removal.", "author": ["Jia", "Sen", "Zhen Ji", "Yuntao Qian", "Linlin Shen"], "venue": "Selected Topics in Applied Earth Observations and Remote Sensing, IEEE Journal of 5,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2012}, {"title": "Constrained band selection for hyperspectral imagery.", "author": ["Chang", "Chein-I", "Su Wang"], "venue": "Geoscience and Remote Sensing, IEEE Transactions on 44, no", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2006}, {"title": "A joint band prioritization and band-decorrelation approach to band selection for hyperspectral image classification.", "author": ["Chang", "Chein-I", "Qian Du", "Tzu-Lung Sun", "Mark LG Althouse"], "venue": "Geoscience and Remote Sensing, IEEE Transactions on 37, no", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 1999}, {"title": "Band selection for hyperspectral image classification using mutual information.", "author": ["Guo", "Baofeng", "Steve R. Gunn", "R.I. Damper", "J.D.B. Nelson"], "venue": "Geoscience and Remote Sensing Letters,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2006}, {"title": "Feature Selection through Minimization of the VC dimension.", "author": ["Jayadeva", "Batra", "Sanjit S", "Siddharth Sabharwal"], "venue": "arXiv preprint arXiv:1410.7372", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2014}, {"title": "Classification of hyperspectral remote sensing images with support vector machines.", "author": ["Melgani", "Farid", "Lorenzo Bruzzone"], "venue": "Geoscience and Remote Sensing, IEEE Transactions on 42, no", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2004}, {"title": "Clustering-based hyperspectral band selection using information measures.", "author": ["Martnez-Us", "Adolfo", "Filiberto Pla", "Jos Martnez Sotoca", "Pedro Garca-Sevilla"], "venue": "Geoscience and Remote Sensing, IEEE Transactions on 45, no", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2007}, {"title": "Gene selection for cancer classification using support vector machines.", "author": ["Guyon", "Isabelle", "Jason Weston", "Stephen Barnhill", "Vladimir Vapnik"], "venue": "Machine learning", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2002}, {"title": "Feature subset selection: a correlation based filter approach.", "author": ["Hall", "Mark A", "Lloyd A. Smith"], "venue": null, "citeRegEx": "12", "shortCiteRegEx": "12", "year": 1997}, {"title": "Feature selection based on mutual information criteria of max-dependency, max-relevance, and minredundancy.", "author": ["Peng", "Hanchuan", "Fulmi Long", "Chris Ding"], "venue": "Pattern Analysis and Machine Intelligence, IEEE Transactions on 27,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2005}, {"title": "Random forests.", "author": ["Breiman", "Leo"], "venue": "Machine learning 45,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2001}, {"title": "Conditional likelihood maximisation: a unifying framework for information theoretic feature selection.", "author": ["Brown", "Gavin", "Adam Pocock", "Ming-Jie Zhao", "Mikel Lujn"], "venue": "The Journal of Machine Learning Research 13,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2012}, {"title": "Data Visualization and Feature Selection: New Algorithms for Nongaussian Data.", "author": ["Yang", "Howard Hua", "John E. Moody"], "venue": "In NIPS,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 1999}, {"title": "Fast binary feature selection with conditional mutual information.", "author": ["Fleuret", "Franois"], "venue": "The Journal of Machine Learning Research", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2004}, {"title": "The feature selection problem: Traditional methods and a new algorithm.", "author": ["Kira", "Kenji", "Larry A. Rendell"], "venue": "In AAAI,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 1992}, {"title": "Learning a hyperplane regressor through a tight bound on the VC dimension.", "author": ["Jayadeva", "Chandra", "Suresh", "Sanjit S. Batra", "Siddarth Sabharwal"], "venue": null, "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2015}, {"title": "Learning a hyperplane regressor by minimizing an exact bound on the VC dimension.", "author": ["Jayadeva", "Chandra", "Suresh", "Siddarth Sabharwal", "Sanjit S. Batra"], "venue": null, "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2014}, {"title": "LIBSVM: A library for support vector machines.", "author": ["Chang", "Chih-Chung", "Chih-Jen Lin"], "venue": "ACM Transactions on Intelligent Systems and Technology (TIST)", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2011}], "referenceMentions": [{"referenceID": 0, "context": "Further, it has been noted that highly correlated features have a negative impact on classification accuracy [1].", "startOffset": 109, "endOffset": 112}, {"referenceID": 1, "context": "Feature extraction methods, which transform the original data into a projected space, include for instance, projection pursuit(PP) [2], principal component analysis(PCA) [3] and independent component analysis(ICA) [4].", "startOffset": 131, "endOffset": 134}, {"referenceID": 2, "context": "Feature extraction methods, which transform the original data into a projected space, include for instance, projection pursuit(PP) [2], principal component analysis(PCA) [3] and independent component analysis(ICA) [4].", "startOffset": 170, "endOffset": 173}, {"referenceID": 3, "context": "Feature extraction methods, which transform the original data into a projected space, include for instance, projection pursuit(PP) [2], principal component analysis(PCA) [3] and independent component analysis(ICA) [4].", "startOffset": 214, "endOffset": 217}, {"referenceID": 4, "context": "Most of the unsupervised feature selection methods are based on feature ranking, which construct and evaluate an objective matrix based on various criteria such as information divergence [5], maximum-variance principal component analysis (MVPCA) [6], and mutual information (MI) [7].", "startOffset": 187, "endOffset": 190}, {"referenceID": 5, "context": "Most of the unsupervised feature selection methods are based on feature ranking, which construct and evaluate an objective matrix based on various criteria such as information divergence [5], maximum-variance principal component analysis (MVPCA) [6], and mutual information (MI) [7].", "startOffset": 246, "endOffset": 249}, {"referenceID": 6, "context": "Most of the unsupervised feature selection methods are based on feature ranking, which construct and evaluate an objective matrix based on various criteria such as information divergence [5], maximum-variance principal component analysis (MVPCA) [6], and mutual information (MI) [7].", "startOffset": 279, "endOffset": 282}, {"referenceID": 7, "context": "This paper explores the application of a novel feature selection method based on minimizing a tight bound on the VC dimension [8], on hyperspectral data analysis.", "startOffset": 126, "endOffset": 129}, {"referenceID": 8, "context": "We used the Support Vector Machine (SVM) classifier [9] to assess the classification accuracy, following feature selection.", "startOffset": 52, "endOffset": 55}, {"referenceID": 6, "context": "BACKGROUND AND RELATED WORK Dimensionality reduction prior to classification is advantageous in hyperspectral data analysis because the dimensionality of the input space greatly affects the performance of many supervised classification methods [7].", "startOffset": 244, "endOffset": 247}, {"referenceID": 0, "context": "The analysis in [1] supports this line of reasoning and suggests that feature selection may be a valuable procedure in preprocessing hyperspectral data for classification by the widely used SVM classifier.", "startOffset": 16, "endOffset": 19}, {"referenceID": 0, "context": "In hyperspectral image analysis, feature selection is preferred over feature extraction for dimensionality reduction [1], [10].", "startOffset": 117, "endOffset": 120}, {"referenceID": 9, "context": "In hyperspectral image analysis, feature selection is preferred over feature extraction for dimensionality reduction [1], [10].", "startOffset": 122, "endOffset": 126}, {"referenceID": 0, "context": "[1] lists various feature selection methods for hyperspectral data such as the SVM Recursive Feature Elimination (SVM-RFE) [11], Correlation based Feature Selection(CFS) [12], Minimum Redundancy Maximum Relevance(MRMR) [13] feature selection and Random Forests [14].", "startOffset": 0, "endOffset": 3}, {"referenceID": 10, "context": "[1] lists various feature selection methods for hyperspectral data such as the SVM Recursive Feature Elimination (SVM-RFE) [11], Correlation based Feature Selection(CFS) [12], Minimum Redundancy Maximum Relevance(MRMR) [13] feature selection and Random Forests [14].", "startOffset": 123, "endOffset": 127}, {"referenceID": 11, "context": "[1] lists various feature selection methods for hyperspectral data such as the SVM Recursive Feature Elimination (SVM-RFE) [11], Correlation based Feature Selection(CFS) [12], Minimum Redundancy Maximum Relevance(MRMR) [13] feature selection and Random Forests [14].", "startOffset": 170, "endOffset": 174}, {"referenceID": 12, "context": "[1] lists various feature selection methods for hyperspectral data such as the SVM Recursive Feature Elimination (SVM-RFE) [11], Correlation based Feature Selection(CFS) [12], Minimum Redundancy Maximum Relevance(MRMR) [13] feature selection and Random Forests [14].", "startOffset": 219, "endOffset": 223}, {"referenceID": 13, "context": "[1] lists various feature selection methods for hyperspectral data such as the SVM Recursive Feature Elimination (SVM-RFE) [11], Correlation based Feature Selection(CFS) [12], Minimum Redundancy Maximum Relevance(MRMR) [13] feature selection and Random Forests [14].", "startOffset": 261, "endOffset": 265}, {"referenceID": 5, "context": "In [6], a band prioritization scheme based on Principal Component Analysis (PCA) and classification criterion is presented.", "startOffset": 3, "endOffset": 6}, {"referenceID": 6, "context": "In [7], mutual information is computed using the estimated reference map obtained by using available a priori knowledge about the spectral signature of frequently-encountered materials.", "startOffset": 3, "endOffset": 6}, {"referenceID": 14, "context": "Recently, Brown et al [15] have presented a framework for unifying many information based feature selection selection methods.", "startOffset": 22, "endOffset": 26}, {"referenceID": 7, "context": "In [8] a feature selection method based on minimization of a tight bound on the VC dimension is presented.", "startOffset": 3, "endOffset": 6}, {"referenceID": 5, "context": "[6] presents a band prioritization method based on Principal Component Analysis.", "startOffset": 0, "endOffset": 3}, {"referenceID": 12, "context": "[13].", "startOffset": 0, "endOffset": 4}, {"referenceID": 15, "context": "in [16] proposed Joint Mutual Information.", "startOffset": 3, "endOffset": 7}, {"referenceID": 16, "context": "CMIM: Conditional Mutual Information Maximization is another information theoretic criterion that was proposed by Fleuret [17].", "startOffset": 122, "endOffset": 126}, {"referenceID": 17, "context": "RELIEF Relief is a feature weight based algorithm statistical feature selection method proposed by Kira and Rendell [18].", "startOffset": 116, "endOffset": 120}, {"referenceID": 7, "context": "The classifier generated by the solving the above problem minimizes a tight bound on the VC dimension and hence yields a classifier that uses a small number of features [8] [19] [20] [21] [22].", "startOffset": 169, "endOffset": 172}, {"referenceID": 18, "context": "The classifier generated by the solving the above problem minimizes a tight bound on the VC dimension and hence yields a classifier that uses a small number of features [8] [19] [20] [21] [22].", "startOffset": 183, "endOffset": 187}, {"referenceID": 19, "context": "The classifier generated by the solving the above problem minimizes a tight bound on the VC dimension and hence yields a classifier that uses a small number of features [8] [19] [20] [21] [22].", "startOffset": 188, "endOffset": 192}, {"referenceID": 20, "context": "A Support Vector Machine classifier [23] with an RBF kernel is used for classification.", "startOffset": 36, "endOffset": 40}], "year": 2015, "abstractText": "Hyperspectral data consists of large number of features which require sophisticated analysis to be extracted. A popular approach to reduce computational cost, facilitate information representation and accelerate knowledge discovery is to eliminate bands that do not improve the classification and analysis methods being applied. In particular, algorithms that perform band elimination should be designed to take advantage of the specifics of the classification method being used. This paper employs a recently proposed filterfeature-selection algorithm based on minimizing a tight bound on the VC dimension. We have successfully applied this algorithm to determine a reasonable subset of bands without any user-defined stopping criteria on widely used hyperspectral images and demonstrate that this method outperforms state-of-the-art methods in terms of both sparsity of feature set as well as accuracy of classification.", "creator": "TeX"}}}