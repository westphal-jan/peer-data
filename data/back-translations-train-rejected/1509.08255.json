{"id": "1509.08255", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "28-Sep-2015", "title": "Encoding Reality: Prediction-Assisted Cortical Learning Algorithm in Hierarchical Temporal Memory", "abstract": "In the decade since Jeff Hawkins proposed Hierarchical Temporal Memory (HTM) as a model of neocortical computation, the theory and the algorithms have evolved dramatically. This paper presents a detailed description of HTM's Cortical Learning Algorithm (CLA), including for the first time a rigorous mathematical formulation of all aspects of the computations. Prediction Assisted CLA (paCLA), a refinement of the CLA is presented, which is both closer to the neuroscience and adds significantly to the computational power. Finally, we summarise the key functions of neocortex which are expressed in paCLA implementations.", "histories": [["v1", "Mon, 28 Sep 2015 09:54:08 GMT  (1306kb,D)", "https://arxiv.org/abs/1509.08255v1", null], ["v2", "Thu, 8 Oct 2015 16:13:44 GMT  (1306kb,D)", "http://arxiv.org/abs/1509.08255v2", "Updated reference to unofficial revision of Hawkins and Ahmad, 2011"]], "reviews": [], "SUBJECTS": "cs.NE cs.AI", "authors": ["fergal byrne"], "accepted": false, "id": "1509.08255"}, "pdf": {"name": "1509.08255.pdf", "metadata": {"source": "CRF", "title": "Encoding Reality: Prediction-Assisted Cortical Learning Algorithm in Hierarchical Temporal Memory", "authors": ["Fergal Byrne"], "emails": ["fergal@brenter.ie"], "sections": [{"heading": null, "text": "Encoding Reality: Prediction-Assisted Cortical Learning Algorithm in Hierarchical Temporal MemoryFergal Byrne HTM Theory Group, Dublin, Irelandfergal @ brenter.ie http: / / inbits.com"}, {"heading": "In the decade since Jeff Hawkins proposed Hierarchical Temporal Memory", "text": "s cortical learning algorithm (CLA), including, for the first time, a rigorous mathematical formulation of all aspects of calculations. Prediction-assisted CLA (paCLA), a refinement of the CLA, is presented, which is both closer to neuroscience and essential to computing power. Finally, we summarize the key functions of the neocortex expressed in paCLA implementations. An open source project, Comportex, is the leading implementation of this evolving theory of the brain."}, {"heading": "1 Introduction", "text": "The description and mathematics are presented here to provide an axiomatic basis for understanding the computing power of each component in an HTM system, as well as a basis for comparing HTM computing models with empirical evidence of structure and function in neocortical texts. In particular, we show the following: 1. A layer of HTM neurons automatically learns to efficiently represent sensory and sensorimotor inputs through semantic encodings in the form of sparsely distributed representations in neocortext.150 9,08 255v 2 [cs.N E] 8O ct2 015 (SDRs). These representations are robust against noise and missing or masked inputs, and generalize in a semantically usable sequence of time."}, {"heading": "2 Hierarchical Temporal Memory and the Cortical Learning", "text": "In fact, we are able to assert ourselves, we are able to adapt, we are able to adapt to the needs of the people and we are able to adapt to the needs of the people."}, {"heading": "3 Pattern Memory (aka Spatial Pooling)", "text": "We will begin to describe the details and mathematics of HTM by describing the simplest operation in HTM's cortical learning algorithm: Pattern Memory, also known as spatial pooling, forms a sparsely distributed representation of a binary input vector for feedback. Pattern Memory is a type of learned spatial pattern recognition that is able to identify and represent individual inputs. We will begin with a layer (a 1- or 2-dimensional array) of individual neurons that form an activity pattern aimed at efficiently representing the input vectors."}, {"heading": "3.1 Feedforward Processing on Proximal Dendrites", "text": "The HTM model neuron has a single proximal dendrites, which is used to process and detect a single injection = FF = afferent inputs to the neuron. We model the entire feedback forward input to a cortical plane as a bit vector xFF = 0, 1} nFF, where nFF is the width of the input.The dendrite consists of ns synapses, each acting as a binary gate for a single bit in the input vector. Each synapse i has a permanence pi [0, 1] representing the size and efficiency of the dendritic vortex and synaptic node. The synapse is transmitted a 1-bit (or on-bit) when the permanence exceeds a threshold value (often a global constant i = 0.2), which represents the size of the andeefficiency of the dendritic vertebra and synaptic node."}, {"heading": "3.2 Sparse Distributed Representations (SDRs)", "text": "We now show how a layer of neurons transforms an input vector into a sparse representation. From the description above, each neuron results in an estimated x-j of the input xFF, where length oj \u2212 nFF reflects how well the neuron represents or recognizes the input. We form a sparse representation of the input by selecting a set of YSDR of the uppermost nSDR = sN neurons, where N is the number of neurons in the layer, and s is the chosen spareness that we want to impose (typically = 0.02 = 2%). The algorithm for selecting the uppermost nSDR neurons may vary. In the neocortex, this is achieved by a mechanism that includes cascading inhibition: a cell fire quickly (because it quickly depolarizes due to its input) activates nearby inhibitory DR cells, which spread adjacent SDR cells to the outside, and also close the inhibitory cells."}, {"heading": "3.3 Matrix Form", "text": "The above can simply be represented as a matrix. Alternatively, we can stay in the input space Bnff and represent the model \u03c0j as a vector ~ \u03c0j = \u03c0 \u2212 1n (1ns), i.e. we can project the connection vector for the dendrites by elementary multiplication: ~ cj = \u03c0 \u2212 1j (xj) = ~ \u03c0j xFF represents the view of the neuron on the input vector xFF. Likewise, we can project the connection vector for the dendrites by elementary multiplication: ~ cj = junction \u2212 1j (cj) (cj) (cj), and thus ~ oj (xFF) = ~ cj xFF is the overlap vector projected back into BnFF, and the dot product oj (xFF)."}, {"heading": "3.4 Learning in HTM as an Optimisation Problem", "text": "We can now measure the distance between the input vector xFF and the reconstructed estimate x-SDR by taking a standard of the conference, thus presenting learning in HTM as an optimization problem. We want to minimize the error of estimation across all inputs at the level. In view of a series of (usually random) projection vectors ~ \u03c0j for the N neurons, the parameters of the model are the durability vectors ~ pj, which we adjust using a simple Hebbic update model. The update model for the permanence of a synapse pi on the neuron j is: p (t + 1) i = (1 + \u03b4inc) p (t) i, if j-YSDR, (xj) i = 1 and p (t) i (t) i (t) i (DR) and (t) ip)."}, {"heading": "3.5 Computational Power of SDR-forming Circuits", "text": "An SDR is a form of k-winner-takes-all (k-WTA) representation. Maass [2000] proves that a single k-WTA gate has the same computing power as a polynomically larger multilayered network of artificial threshold neurons, and that the soft (continuous) version can approximate any continuous function (just as a multilayered ANN network can - see Maass [1997])."}, {"heading": "4 Transition Memory - Making Predictions", "text": "In this section, we describe the process of learning temporal sequences. We have already shown that the neuron of the HTM model learns to recognize sub-patterns of feedback input on its proximal dendrites, which is somewhat similar to the way in which a Restricted Boltzmann machine can learn to represent its input in an unattended learning process. A characteristic feature of HTM is that the evolution of the world over time is a crucial aspect of what and how the system learns, provided that objects and processes in the world persist over time and possibly only show part of their structure at a particular time. By learning to model this evolving revelation of structure, the neocortex can more efficiently recognize and remember objects and concepts in the world."}, {"heading": "4.1 Distal Dendrites and Prediction", "text": "In addition to its one proximal dendrites, an HTM model can activate neurons that have a collection of distal (further) dendrite segments or simple dendrites that collect information from sources other than the supplying inputs to the layer. In some layers of the neocortex, these dendrites will combine signals from neurons in the same layer, as well as from other layers in the same layer, and even receive indirect inputs from neurons in higher regions of the cortex. We will describe the structure and function of each of these neurons; the simplest case concerns distal dendrites that collect signals from neurons within the same layer. Previous studies have shown that a layer of N neurons has converted an input vector x Bnff into an SDR ySDR segment, with a length of ySDR-1 = sN, where the economy is usually 2% (N is typical 2048, so the neurons will have SDR-SDR-40)."}, {"heading": "4.2 Learning Predictions", "text": "We use a very similar learning rule for distal dendrites segments as we have done for the upstream inputs: pi, j (t + 1) = (1 + \u03c3inc) p (t) i if the cell is active, segment k active, synapse i active (1 \u2212 \u03c3dec) p (t) i if the cell is active, segment k active, synapse i not active p (t) i otherwise This again reinforces synapses that contribute to the activity of the cell, and reduces the contribution of synapses that do not. An stimulation rule, similar to that for proximal synapses, allows to improve poorly functioning distal connections until they are good enough to use the main rule. Interpretation We can now see the layer of neurons as a number of representations in each time period that form a number of interpretations. The field of predictive potentials is a kind of sensory corpus in which we consider the type of pre-produced corpus as a time corpus."}, {"heading": "4.3 Higher-order Prediction", "text": "The current Numenta Cortical Learning Algorithm (or CLA, the detailed computational model in HTM) separates the individual stages of processing. A modification of this model (which we call Predictive Assisted Recognition or paCLA) combines these into a single step that involves competition between highly predictive pyramid cells and their surrounding slit sheaths, based on the idea that input signals cause the neurons to flow into the cell body, increasing its voltage (depolarizing) until it reaches a threshold level and causes fire. paCLA also models this idea, with the added complication that there are two separate pathways (proximal and distal) for input signals to be converted into effects on the voltage of the cell."}, {"heading": "4.4 Spatial/Columnar Interpretation of Transition Memory SDRs", "text": "Since the cells in each TM column exhibit a very similar feedback response, we can easily consider which columns contain active cells when presented with each input; this column-shaped SDR will be very similar to the SDR, which is formed solely by pattern memory (i.e., without prediction) and will differ only where the prediction has altered the outcome of the inhibition phase, preferring columns with highly predictive cells. TM column SDRs will be more immutable to occlusion or noise in the inputs, but may also hallucinate some inputs as the layer sees what is expected rather than what is actually seen. It is likely that this balance between error correction and hallucination in the real cortex will be dynamically adjusted."}, {"heading": "5 Sequence Memory - High-Order Sequences", "text": "A CLA layer with multicellular columns is able to learn high-order sequences of input patterns for feedback, i.e. sequences in which the next input text1 can be predicted based on all observed patterns {xt \u2212 i | 0 \u2264 i \u2264 k} for some k steps in the past, and not just for the current input text sequence1. Thus, a layer that has seen the sequences ABCD and XBCY will correctly predict D after seeing ABC and Y after seeing XBC. To explain this important function, consider the columns that B represent in the above sequences. In each cell, one cell will have a distal dendrite segment that receives input for A, and another will have learned to recognize an earlier X. Thus, while (essentially) the same columns become active for both B, the active cells will always differ from each other. Thus, the TM activation sequence encodes a single chain of multiple patterns in each example."}, {"heading": "6 Multiple levels of representation", "text": "Note that a CLA layer predicts a number of representations of its inputs simultaneously, and these representations can be viewed as nested within others. Columnar SDR The simplest and least detailed representation is the Columnar SDR, which is just a simple representation of the pattern currently seen by the layer. This is what you would see if you looked down at the layer and simply observed which columns had active cells. The number of patterns that can be represented is (NnSDR). In the typical software layer (2048 columns, 40 active), we can have (2048 40) = 2,37178 \u0432 1084 SDRs. (See [Ahmad and Hawkins, 2015] for a detailed treatment of the combinatorics of the SDRs.) Cell-level SDR The cell-level SDR encodes both the column SDR and the SDR (if you ignore the decisions of the cells) and the sequence we can place them in a single cell."}, {"heading": "7 Temporal Pooling: from single- to multi-layer models", "text": "A well understood aspect of the structure of the neocortex is the hierarchical organization of the visual cortex = 4. The key feature of this hierarchy is that the spatial and temporal scale of receptive fields increases from low to high regions. Early versions of HTM resembled artificial neural networks, or deep learning networks, in that for each region they have a single layer in the hierarchy George and Hawkins [2009]. The current CLA, as described in [Byrne, 2015], continues this design and models only a single layer corresponding to layer 2 / 3 in the cortex in each region. Recent developments in HTM include a new mechanism called Temporal Pooling, which models both layer 4 and layer 2 / 3. This section describes Temporal Pooling and its role in extracting spatiotemporal information from sensory and sensory inputs.Hawkins suggests that each hierarchical layer performs a similar task of learning and hierarchical task."}, {"heading": "8 Summary and Resources", "text": "This theory aims to combine a relatively simple abstraction of neocortic function with several central computing functions that we believe are central to the understanding of both mammals and artificial intelligence. By providing a simple but powerful mathematical description of paCLA algorithms, we can think about their computing power and learning ability. This work also provides a solid foundation for expanding the theory in new directions. In fact, we are developing a new, multi-layered model of neocortex algorithms based on current works.Comportex [Andrew and Lewis, 2015] is an open source implementation of HTM / CLA that demonstrates most of the theory presented here, including paCLA and Temporal Pooling. For other resources on HTM, we recommend visiting the open source community website at Numenta.org. References and NotesSubutai Ahmad and Jeff Hawkins. Characteristics of Sparse Distributed Representations and their Application to Hierarchical."}], "references": [], "referenceMentions": [], "year": 2015, "abstractText": "In the decade since Jeff Hawkins proposed Hierarchical Temporal Memory (HTM) as a model of neocortical computation, the theory and the algorithms have evolved dramatically. This paper presents a detailed description of HTM\u2019s Cortical Learning Algorithm (CLA), including for the first time a rigorous mathematical formulation of all aspects of the computations. Prediction Assisted CLA (paCLA), a refinement of the CLA, is presented, which is both closer to the neuroscience and adds significantly to the computational power. Finally, we summarise the key functions of neocortex which are expressed in paCLA implementations. An Open Source project, Comportex, is the leading implementation of this evolving theory of the brain.", "creator": "LaTeX with hyperref package"}}}