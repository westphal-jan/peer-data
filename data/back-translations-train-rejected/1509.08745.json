{"id": "1509.08745", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "29-Sep-2015", "title": "Compression of Deep Neural Networks on the Fly", "abstract": "Because of their performance, deep neural networks are increasingly used for object recognition. They are particularly attractive because of their ability to 'absorb' great quantities of labeled data through millions of parameters. However, as the accuracy and the model sizes increase, so does the memory requirements of the classifiers. This prohibits their usage on resource limited hardware, including cell phones or other embedded devices.", "histories": [["v1", "Tue, 29 Sep 2015 13:32:30 GMT  (395kb,D)", "http://arxiv.org/abs/1509.08745v1", "Under review as a conference paper at SPL 2015. arXiv admin note: text overlap witharXiv:1412.6115by other authors"], ["v2", "Thu, 12 Nov 2015 10:22:13 GMT  (150kb,D)", "http://arxiv.org/abs/1509.08745v2", "arXiv admin note: text overlap witharXiv:1412.6115by other authors"], ["v3", "Mon, 16 Nov 2015 13:08:50 GMT  (161kb,D)", "http://arxiv.org/abs/1509.08745v3", null], ["v4", "Thu, 7 Jan 2016 12:58:32 GMT  (162kb,D)", "http://arxiv.org/abs/1509.08745v4", null], ["v5", "Fri, 18 Mar 2016 09:33:01 GMT  (66kb,D)", "http://arxiv.org/abs/1509.08745v5", null]], "COMMENTS": "Under review as a conference paper at SPL 2015. arXiv admin note: text overlap witharXiv:1412.6115by other authors", "reviews": [], "SUBJECTS": "cs.LG cs.CV cs.NE", "authors": ["guillaume souli\\'e", "vincent gripon", "ma\\\"elys robert"], "accepted": false, "id": "1509.08745"}, "pdf": {"name": "1509.08745.pdf", "metadata": {"source": "CRF", "title": "Compression of Deep Neural Networks on the Fly", "authors": ["Guillaume Souli\u00e9", "Vincent Gripon"], "emails": ["name.surname@telecom-bretagne.eu"], "sections": [{"heading": "I. MOTIVATION", "text": "In fact, most of them are able to survive on their own."}, {"heading": "II. RELATED WORK", "text": "As mentioned in the introduction, a state-of-the-art CNN typically involves hundreds of millions of parameters that require an important storage capacity that is difficult to achieve in practice. The bottleneck comes from the storage of models and the test speed. Several papers have been published on the acceleration of CNN's prediction speed. In [20], the authors use tricks of CPUs to speed up the execution of CNN. Specifically, they focus on the alignment of storage and SIMD operations to increase matrix operations. The authors show that the revolutionary operations can be performed efficiently in the Fourier domain, resulting in a speed of 200%. Two very recent papers [12, 13] use linear matrix factoring methods methods to accelerate convergence layers and achieve a speed gain of 200% without being lost in the classification. Almost all of the above works focus on the pre-production of models of CNN's work; specifically, CNN has been focusing on low seasonal speed."}, {"heading": "III. METHODOLOGY", "text": "In this section, we will consider two methods for compressing the parameters in CNN layers: We will first present the product quantification method proposed in [14] and then present our proposed learning-based method."}, {"heading": "A. Product Quantization (PQ)", "text": "For more details on this quantization method, refer to [14]. The key idea is to consider structured vector quantization methods for compressing parameters. Specifically, the idea is to apply product quantization (PQ) [15] to exploit the redundancy of structures in vector space. PQ splits the vector space into different subspaces and quantizes in each of them. It works increasingly better as redundancy grows in each subspace. Specifically, we divide the matrix W column by column into several sub-matrices: W = [W 1, W 2,..., W s], (1) where W i-Rm (n / s) is divisible by s (n / s), the segment size is called. We can then perform k-mean compression for each sub-matrix matrix matrix matrix matrix matrix matrix matrix matrix."}, {"heading": "B. Proposed method", "text": "Our proposed method is twofold: First, we use a specific additional regulatory term to attract network weights to binary values (Q), then we roughly quantify the output layers. Remember that the formation of a neural network is usually done by minimizing a cost function using a derivative process of a gradient descendant algorithm. In order to attract network weights to binary values, we add a binary value (regularizer) during the learning phase. These additional costs push the weights down to binary values. As a result, solutions to the minimization problem are expected to be binary or almost binary, depending on the scaling parameters of the additional cost relative to the original value. This idea is not new, although we have not found any work applying it to deep learning in the literature. Our choice for the regulatory term was strongly inspired by [19]. Let us specify the onneural network weights (W)."}, {"heading": "IV. EXPERIMENTS", "text": "We evaluate these different methods using two image classification data sets: MNIST and CIFAR10."}, {"heading": "A. Experimental settings", "text": "1) MNIST: The MNIST database of handwritten digits has a training set of 60,000 examples and a test set of 10,000 examples. It is a subset of a larger set available at NIST. The numbers have been normalised and centered in a fixed-size image. The neural network we use at MNIST is LeNet5. LeNet5 is a revolutionary neural network [22] introduced by Yann Lecun. LeNet5 is one of the networks in the LeNet family described in Figure 1. We use exactly the same parameters for the network (number of layers, size of layers, etc....).2) CIFAR10: The CIFAR10 database has a training set of 50,000 examples and a test set of 10,000 examples. It is a subset of a larger set available from 80 million tiny image sets. It consists of 32 images divided into ten classes."}, {"heading": "B. Layers to quantify", "text": "Our first experiments (using the MNIST database shown in Figure 2) show the influence of quantified layers on performance. We observe that performance strongly depends on which layers are quantified. Specifically, this experiment shows that one should quantify layers from output to input, not the opposite. This result does not surprise us, since input layers are often described as similar to wave transformations, which are in themselves analog operators, while output layers are often compared to high-grade patterns, whose recognition in an image is often sufficient to achieve good classification results."}, {"heading": "C. Performance comparison", "text": "Our second experiment shows a comparison with previous work. The results are in Figures 3 and 4. Note that in both cases, compared networks have exactly the same architecture. In terms of our proposed method, we chose to compress only the two output levels that are completely connected to each other. As their size is different, we could not use the same PQ coefficients k and m twice. Note that layer 2 contains almost all the weights and is therefore the one we chose to examine the role of each parameter.We found that our additional regulation costs can significantly improve performance. For example, for the MNIST database, if we want to respect a loss of 2%, we have a compression rate of 33 with a single PQ, while our learning-based method leads to a compression rate of 107."}, {"heading": "V. BIOLOGICAL DISCUSSION", "text": "Deep neural networks are often compared to sensor parts of the human cortex, both in terms of architecture and performance [25]. In fact, some authors [26] suggest models that are very similar and biologically inspired to CNNs. For decades, the question of whether information encoding in the brain is analog or digital has been debated. In a recent paper [27], several experiments were conducted in which the brain could exhibit different encodings depending on the region in which it is used. Specifically, they show that the farther the region is from the sensors, the more likely the encodings are digital. It is tempting to draw a parallel between these results and the brain's ability to associate analog data such as sounds and images with digital data such as speech and labels.The results shown in Figure 2 suggest similar behavior in deep artificial neural systems."}, {"heading": "VI. CONCLUSION AND PERSPECTIVES", "text": "In this paper, we have presented a new method for compressing convolutionary neural networks, which consists of adding an additional term to the cost function that forces weights to become almost binary. To further compress the network, we are applying product quantification, and the combination of both allows us to achieve performance that is above the state of the art. We are also demonstrating the impact of the depth of the binary layer on performance. These results are of particular interest to us and a motivation to further explore the relationships between actual biological data and deep neural systems. In future work, we are considering exploring more complex regulatory functions. We also plan to conduct PQ on the fly during the learning phase. Finally, other strategies may seek to benefit from the scarcity of trained weights. Such scarcity may be forced by mechanisms similar to those we present in this paper."}], "references": [{"title": "Imagenet classification with deep convolutional neural networks", "author": ["A. Krizhevsky", "I. Sutskever", "G.E. Hinton"], "venue": "Advances in neural information processing systems, 2012, pp. 1097\u20131105.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2012}, {"title": "Handwritten digit recognition with a back-propagation network", "author": ["B.B. Le Cun", "J.S. Denker", "D. Henderson", "R.E. Howard", "W. Hubbard", "L.D. Jackel"], "venue": "Advances in neural information processing systems. Citeseer, 1990.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 1990}, {"title": "Going deeper with convolutions", "author": ["C. Szegedy", "W. Liu", "Y. Jia", "P. Sermanet", "S. Reed", "D. Anguelov", "D. Erhan", "V. Vanhoucke", "A. Rabinovich"], "venue": "arXiv preprint arXiv:1409.4842, 2014.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2014}, {"title": "Very deep convolutional networks for large-scale image recognition", "author": ["K. Simonyan", "A. Zisserman"], "venue": "arXiv preprint arXiv:1409.1556, 2014.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2014}, {"title": "Caffe: An open source convolutional architecture for fast feature embedding", "author": ["Y. Jia"], "venue": "2013.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2013}, {"title": "Decaf: A deep convolutional activation feature for generic visual recognition", "author": ["J. Donahue", "Y. Jia", "O. Vinyals", "J. Hoffman", "N. Zhang", "E. Tzeng", "T. Darrell"], "venue": "arXiv preprint arXiv:1310.1531, 2013.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2013}, {"title": "Overfeat: Integrated recognition, localization and detection using convolutional networks", "author": ["P. Sermanet", "D. Eigen", "X. Zhang", "M. Mathieu", "R. Fergus", "Y. LeCun"], "venue": "arXiv preprint arXiv:1312.6229, 2013.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2013}, {"title": "Stochastic pooling for regularization of deep convolutional neural networks", "author": ["M.D. Zeiler", "R. Fergus"], "venue": "arXiv preprint arXiv:1301.3557, 2013.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2013}, {"title": "Multiscale orderless pooling of deep convolutional activation features", "author": ["Y. Gong", "L. Wang", "R. Guo", "S. Lazebnik"], "venue": "Computer Vision\u2013ECCV 2014. Springer, 2014, pp. 392\u2013407.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2014}, {"title": "A 240 g-ops/s mobile coprocessor for deep neural networks", "author": ["V. Gokhale", "J. Jin", "A. Dundar", "B. Martini", "E. Culurciello"], "venue": "Computer Vision and Pattern Recognition Workshops (CVPRW), 2014 IEEE Conference on. IEEE, 2014, pp. 696\u2013701.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2014}, {"title": "Predicting parameters in deep learning", "author": ["M. Denil", "B. Shakibi", "L. Dinh", "N. de Freitas"], "venue": "Advances in Neural Information Processing Systems, 2013, pp. 2148\u2013 2156.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2013}, {"title": "Exploiting linear structure within convolutional networks for efficient evaluation", "author": ["E.L. Denton", "W. Zaremba", "J. Bruna", "Y. LeCun", "R. Fergus"], "venue": "Advances in Neural Information Processing Systems, 2014, pp. 1269\u2013 1277.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2014}, {"title": "Speeding up convolutional neural networks with low rank expansions", "author": ["M. Jaderberg", "A. Vedaldi", "A. Zisserman"], "venue": "arXiv preprint arXiv:1405.3866, 2014.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2014}, {"title": "Compressing deep convolutional networks using vector quantization", "author": ["Y. Gong", "L. Liu", "M. Yang", "L. Bourdev"], "venue": "arXiv preprint arXiv:1412.6115, 2014.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2014}, {"title": "Product quantization for nearest neighbor search", "author": ["H. Jegou", "M. Douze", "C. Schmid"], "venue": "Pattern Analysis  and Machine Intelligence, IEEE Transactions on, vol. 33, no. 1, pp. 117\u2013128, 2011.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2011}, {"title": "Approximate nearest neighbor search by residual vector quantization", "author": ["Y. Chen", "T. Guan", "C. Wang"], "venue": "Sensors, vol. 10, no. 12, pp. 11 259\u201311 273, 2010.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2010}, {"title": "Compressing neural networks with the hashing trick", "author": ["W. Chen", "J.T. Wilson", "S. Tyree", "K.Q. Weinberger", "Y. Chen"], "venue": "arXiv preprint arXiv:1504.04788, 2015.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2015}, {"title": "Compressing convolutional neural networks", "author": ["\u2014\u2014"], "venue": "arXiv preprint arXiv:1506.04449, 2015.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2015}, {"title": "An algorithm for nonlinear optimization problems with binary variables", "author": ["W. Murray", "K.-M. Ng"], "venue": "Computational Optimization and Applications, vol. 47, no. 2, pp. 257\u2013288, 2010.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2010}, {"title": "Improving the speed of neural networks on cpus", "author": ["V. Vanhoucke", "A. Senior", "M.Z. Mao"], "venue": "Proc. Deep Learning and Unsupervised Feature Learning NIPS Workshop, vol. 1, 2011.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2011}, {"title": "Fast training of convolutional networks through ffts", "author": ["M. Mathieu", "M. Henaff", "Y. LeCun"], "venue": "arXiv preprint arXiv:1312.5851, 2013.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2013}, {"title": "Gradient-based learning applied to document recognition", "author": ["Y. LeCun", "L. Bottou", "Y. Bengio", "P. Haffner"], "venue": "Proceedings of the IEEE, vol. 86, no. 11, pp. 2278\u2013 2324, 1998.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 1998}, {"title": "Theano: new features and speed improvements", "author": ["F. Bastien", "P. Lamblin", "R. Pascanu", "J. Bergstra", "I.J. Goodfellow", "A. Bergeron", "N. Bouchard", "Y. Bengio"], "venue": "Deep Learning and Unsupervised Feature Learning NIPS 2012 Workshop, 2012.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2012}, {"title": "Deep neural networks rival the representation of primate it cortex for core visual object recognition", "author": ["C.F. Cadieu", "H. Hong", "D.L. Yamins", "N. Pinto", "D. Ardila", "E.A. Solomon", "N.J. Majaj", "J.J. DiCarlo"], "venue": "PLoS computational biology, vol. 10, no. 12, p. e1003963, 2014.", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2014}, {"title": "Robust object recognition with cortex-like mechanisms", "author": ["T. Serre", "L. Wolf", "S. Bileschi", "M. Riesenhuber", "T. Poggio"], "venue": "Pattern Analysis and Machine Intelligence, IEEE Transactions on, vol. 29, no. 3, pp. 411\u2013426, 2007.", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2007}, {"title": "Analog and digital codes in the brain", "author": ["Y. Mochizuki", "S. Shinomoto"], "venue": "Physical Review E, vol. 89, no. 2, p. 022705, 2014.", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2014}], "referenceMentions": [{"referenceID": 0, "context": "Deep Convolutional Neural Networks (CNNs) [1, 2, 3, 4] have become the gold standard for object recognition, image classification and retrieval.", "startOffset": 42, "endOffset": 54}, {"referenceID": 1, "context": "Deep Convolutional Neural Networks (CNNs) [1, 2, 3, 4] have become the gold standard for object recognition, image classification and retrieval.", "startOffset": 42, "endOffset": 54}, {"referenceID": 2, "context": "Deep Convolutional Neural Networks (CNNs) [1, 2, 3, 4] have become the gold standard for object recognition, image classification and retrieval.", "startOffset": 42, "endOffset": 54}, {"referenceID": 3, "context": "Deep Convolutional Neural Networks (CNNs) [1, 2, 3, 4] have become the gold standard for object recognition, image classification and retrieval.", "startOffset": 42, "endOffset": 54}, {"referenceID": 3, "context": "Almost all of the recent successful recognition systems [4, 5, 6, 7, 8, 9] are built on top of this architecture.", "startOffset": 56, "endOffset": 74}, {"referenceID": 4, "context": "Almost all of the recent successful recognition systems [4, 5, 6, 7, 8, 9] are built on top of this architecture.", "startOffset": 56, "endOffset": 74}, {"referenceID": 5, "context": "Almost all of the recent successful recognition systems [4, 5, 6, 7, 8, 9] are built on top of this architecture.", "startOffset": 56, "endOffset": 74}, {"referenceID": 6, "context": "Almost all of the recent successful recognition systems [4, 5, 6, 7, 8, 9] are built on top of this architecture.", "startOffset": 56, "endOffset": 74}, {"referenceID": 7, "context": "Almost all of the recent successful recognition systems [4, 5, 6, 7, 8, 9] are built on top of this architecture.", "startOffset": 56, "endOffset": 74}, {"referenceID": 8, "context": "Almost all of the recent successful recognition systems [4, 5, 6, 7, 8, 9] are built on top of this architecture.", "startOffset": 56, "endOffset": 74}, {"referenceID": 9, "context": "Importing CNNs onto embedded platforms [10], the recent trend toward mobile computing, has a wide range of application impacts.", "startOffset": 39, "endOffset": 43}, {"referenceID": 4, "context": "CNN [5, 7, 8] with good performance in visual object recognition contains eight layers (five convolutional layers and three dense connected layers) and a huge number of parameters in order to achieve state-of-the-art results.", "startOffset": 4, "endOffset": 13}, {"referenceID": 6, "context": "CNN [5, 7, 8] with good performance in visual object recognition contains eight layers (five convolutional layers and three dense connected layers) and a huge number of parameters in order to achieve state-of-the-art results.", "startOffset": 4, "endOffset": 13}, {"referenceID": 7, "context": "CNN [5, 7, 8] with good performance in visual object recognition contains eight layers (five convolutional layers and three dense connected layers) and a huge number of parameters in order to achieve state-of-the-art results.", "startOffset": 4, "endOffset": 13}, {"referenceID": 10, "context": "These parameters are overparameterized [11] and we aim at compressing them.", "startOffset": 39, "endOffset": 43}, {"referenceID": 11, "context": "speeding up the testing time [12].", "startOffset": 29, "endOffset": 33}, {"referenceID": 11, "context": "In [12] and [13] the authors use compressing methods", "startOffset": 3, "endOffset": 7}, {"referenceID": 12, "context": "In [12] and [13] the authors use compressing methods", "startOffset": 12, "endOffset": 16}, {"referenceID": 13, "context": "In [14] the authors focus on compressing dense connected layers.", "startOffset": 3, "endOffset": 7}, {"referenceID": 11, "context": "In their work, instead of the traditional matrix factorization methods considered in [12, 13], they consider series of information theoretical vector quantization methods [15, 16] for compressing dense connected layers.", "startOffset": 85, "endOffset": 93}, {"referenceID": 12, "context": "In their work, instead of the traditional matrix factorization methods considered in [12, 13], they consider series of information theoretical vector quantization methods [15, 16] for compressing dense connected layers.", "startOffset": 85, "endOffset": 93}, {"referenceID": 14, "context": "In their work, instead of the traditional matrix factorization methods considered in [12, 13], they consider series of information theoretical vector quantization methods [15, 16] for compressing dense connected layers.", "startOffset": 171, "endOffset": 179}, {"referenceID": 15, "context": "In their work, instead of the traditional matrix factorization methods considered in [12, 13], they consider series of information theoretical vector quantization methods [15, 16] for compressing dense connected layers.", "startOffset": 171, "endOffset": 179}, {"referenceID": 16, "context": "In [17] the authors focus on compressing the fully connected layers of a MultiLayer Perceptron (MLP) using Hashing Trick, a low cost hash function to randomly group connection weights into hash buckets, and set the same value to all the parameters in the same bucket.", "startOffset": 3, "endOffset": 7}, {"referenceID": 17, "context": "In [18] the authors propose compressing convolutionnal layers using a Discrete Cosinus Transform applied on the convolutionnal filters, followed by Hashing Trick, as for the fully connected layers.", "startOffset": 3, "endOffset": 7}, {"referenceID": 7, "context": "For a typical CNN as the one introduced in [8], about 90% of the storage is taken up by the dense connected layers and more than 90% of the running time is taken by the convolutional layers.", "startOffset": 43, "endOffset": 46}, {"referenceID": 18, "context": "originally proposed in [19].", "startOffset": 23, "endOffset": 27}, {"referenceID": 13, "context": "In order to compress furthermore our obtained networks, we also use PQ as described in [14] afterwards.", "startOffset": 87, "endOffset": 91}, {"referenceID": 19, "context": "In [20] the authors use tricks of CPUs to speed up the execution of CNN.", "startOffset": 3, "endOffset": 7}, {"referenceID": 20, "context": "In [21] the authors show the convolutional operations can be efficiently carried out in the Fourier domain, leading to a speed-up of 200%.", "startOffset": 3, "endOffset": 7}, {"referenceID": 11, "context": "Two very recent works [12, 13] use linear matrix factorization methods for speeding up convolutional layers and obtain a 200% speed-up gain with almost no lost in classification performance.", "startOffset": 22, "endOffset": 30}, {"referenceID": 12, "context": "Two very recent works [12, 13] use linear matrix factorization methods for speeding up convolutional layers and obtain a 200% speed-up gain with almost no lost in classification performance.", "startOffset": 22, "endOffset": 30}, {"referenceID": 10, "context": "In [11], the authors demonstrate the redundancies in neural network parameters.", "startOffset": 3, "endOffset": 7}, {"referenceID": 8, "context": "These results motivate [9] to apply vector quantization methods to compress the redundancy in parameter space.", "startOffset": 23, "endOffset": 26}, {"referenceID": 10, "context": "Their work can be viewed as a compression of the parameter prediction results reported in [11].", "startOffset": 90, "endOffset": 94}, {"referenceID": 10, "context": "They obtain similar results to those of [11], in that they are able to compress the parameters about 20 times with little decrease of performance.", "startOffset": 40, "endOffset": 44}, {"referenceID": 10, "context": "This result further confirms the interesting empirical findings in [11].", "startOffset": 67, "endOffset": 71}, {"referenceID": 16, "context": "In [17], for the first time a learn-based method is proposed to compress neural networks.", "startOffset": 3, "endOffset": 7}, {"referenceID": 17, "context": "propose in [18] to compress filters in convolutional layers, arguing that the size of the convolutional layers in state-ofthe-art\u2019s CNN is increasing year after year.", "startOffset": 11, "endOffset": 15}, {"referenceID": 13, "context": "We first introduce the product quantization method proposed in [14], and then introduce our proposed learn-based method.", "startOffset": 63, "endOffset": 67}, {"referenceID": 13, "context": "For more details on this quantization method, refer to [14].", "startOffset": 55, "endOffset": 59}, {"referenceID": 14, "context": "More precisely, the idea consists in applying Product Quantization (PQ) [15] to exploit the redundancy of structures in vector space.", "startOffset": 72, "endOffset": 76}, {"referenceID": 18, "context": "greatly inspired by [19].", "startOffset": 20, "endOffset": 24}, {"referenceID": 18, "context": "To facilitate this selection of \u03b1, we use a barrier method [19] that consists in starting with small values of \u03b1 and incrementing it regularly to help the quantization process.", "startOffset": 59, "endOffset": 63}, {"referenceID": 21, "context": "LeNet5 is a convolutional neural network introduced by Yann Lecun [22].", "startOffset": 66, "endOffset": 70}, {"referenceID": 22, "context": "A graphical depiction of a LeNet model (figure inspired by [23])", "startOffset": 59, "endOffset": 63}, {"referenceID": 23, "context": "Deep neural networks have often been compared to sensor parts of the human cortex, both in terms of architecture and performance [25].", "startOffset": 129, "endOffset": 133}, {"referenceID": 24, "context": "As a matter of fact, some authors [26] pro-", "startOffset": 34, "endOffset": 38}, {"referenceID": 25, "context": "A recent paper [27] performed some experiments emphasizing the brain might have different encodings depending on the region it is applied to.", "startOffset": 15, "endOffset": 19}], "year": 2017, "abstractText": "Because of their performance, deep neural networks are increasingly used for object recognition. They are particularly attractive because of their ability to \u201cabsorb\u201d great quantities of labeled data through millions of parameters. However, as the accuracy and the model sizes increase, so does the memory requirements of the classifiers. This prohibits their usage on resource limited hardware, including cell phones or other embedded devices. We introduce a novel compression method for deep neural networks that performs during the learning phase. It consists in adding an extra regularization term to the cost function of fully-connected layers. We combine this method with Product Quantization (PQ) of the trained weights for higher savings in memory and storage consumption. We evaluate our method on two data sets (MNIST and CIFAR10), on which we achieve significantly larger compression than state-of-the-art methods.", "creator": "LaTeX with hyperref package"}}}