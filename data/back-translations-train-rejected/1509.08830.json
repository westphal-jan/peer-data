{"id": "1509.08830", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "29-Sep-2015", "title": "How to Formulate and Solve Statistical Recognition and Learning Problems", "abstract": "We formulate problems of statistical recognition and learning in a common framework of complex hypothesis testing. Based on arguments from multi-criteria optimization, we identify strategies that are improper for solving these problems and derive a common form of the remaining strategies. We show that some widely used approaches to recognition and learning are improper in this sense. We then propose a generalized formulation of the recognition and learning problem which embraces the whole range of sizes of the learning sample, including the zero size. Learning becomes a special case of recognition without learning. We define the concept of closest to optimal strategy, being a solution to the formulated problem, and describe a technique for finding such a strategy. On several illustrative cases, the strategy is shown to be superior to the widely used learning methods based on maximal likelihood estimation.", "histories": [["v1", "Tue, 29 Sep 2015 16:23:28 GMT  (595kb)", "http://arxiv.org/abs/1509.08830v1", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["michail schlesinger", "evgeniy vodolazskiy"], "accepted": false, "id": "1509.08830"}, "pdf": {"name": "1509.08830.pdf", "metadata": {"source": "CRF", "title": "How to Formulate and Solve Statistical Recognition and Learning Problems", "authors": [], "emails": ["schles@irtc.org.ua", "waterlaz@gmail.com"], "sections": [{"heading": null, "text": "ar Xiv: 150 9.08 83We formulate problems of statistical detection and learning within a common framework of complex hypotheses tests. Based on arguments of multi-criterion optimization, we identify strategies that are unsuitable for solving these problems and derive from this a common form of the remaining strategies. We show that some widespread approaches to detection and learning are inappropriate in this sense. We then propose a general formulation of the detection and learning problem covering the full range of the size of the learning sample, including zero size. Learning becomes a special case of recognition without learning. We define the concept of optimal strategy as a solution to the formulated problem and describe a technique for finding such a strategy. In several vivid cases, it is shown that the strategy is superior to the widely used learning methods based on maximum probability estimation. Keywords: complex object detection, learning, multi-criterialization, bayesian strategy, small sample problem."}, {"heading": "1 Introduction", "text": "This year it has come to the point that it has never come as far as this year."}, {"heading": "2 Complex object recognition", "text": "The main concepts of the complex discrimination hypothesis were formulated in the general statistical decision theory. The basic concepts of our approach are a set of observable signals, a set of hidden states and a set of models. All three propositions are assumed to be finite. This simplification allows to articulate the main ideas of statistical decision theory using the simplest mathematical tools. Basic concepts of our approach are a set of observable signals, a set of observable signals, a set of hidden states and a set of models."}, {"heading": "3 Empirical Bayesian approach and unsupervised", "text": "The property leads to the assumption that the recognition of a sample of independent and identically distributed objects is better than the isolated recognition of each element in the sample. However, the idea was explored decades ago by H.Robbins [16] and has initiated an empirical Bayesian method [15]. Indeed, the same idea is the basis of an approach to pattern recognition known as unattended learning. However, these two approaches exist as if they exist arouse and were developed independently of each other. We consider them two different paths to the same goal and formulate a problem that can be considered as one of the possible concretizations of empirical Bayean approaches and as one of the possible modifications of unattended learning processes. H.Robbins explains his idea using the following simple example [16], which we will use several times throughout the article."}, {"heading": "4 Recognition with learning", "text": "As in the previous sections, we look at complex objects defined by a quadruple < X, Y, \u0432, p = X \u00b7 Y = X \u00b7 Z >. Furthermore, a specific data source is taken into account; the source generates the so-called learning information z, which belongs to a finite set Z. The learning information z is random and depends on the model in which information is available, so that the probability p (z, y; \u03b8) is defined for each z-Z and each x-Z learning condition. Formally, it is crucial that the learning information z depends neither on the current state y nor on the current signal x of the object, so that p (z, y; \u03b8) = p (z). Formally, the source of the learning strategy is defined with a triple < Z element, p: Z \u00d7. \"Usually, learning information is defined with q."}, {"heading": "5 Developing closest to optimal learning proce-", "text": "duresWe see a complex object, presented with a quadruple < X, Y, p: X \u00b7 Y \u00b7 Z >, and a source of learning information, presented with a triple < Z, p: Z \u00b7. The following two theorems show the way in which the closest optimal learning procedure develops. Clearly, this approach is predominantly procedural and therefore it is Bayesian. The next theorem specifies a weight function with respect to which it Bayesian.Theorem 7. The method closest to the optimal learning process is Z = argmin gZ, max."}, {"heading": "6 Examples", "text": "For several simple cases, the method g0 closest to the optimal learning procedure is compared with the maximum probability of the learning process gML = 1 and with other learning procedures gH (to be defined later).In all examples, we have an object with sentences X = R, Y = {1, 2} and probability distribution of the forms (x, y; \u03b8) = py \u00b7 (270) \u2212 1 \u00b7 e \u2212 12 (x \u2212 y) 2with some unknown parameters that are specified later. Such an object is a complex object in the sense of definition 2 and the comparative learning procedures for it can be implemented almost exactly. We use the loss function (5).The required precision for the construction of optimal procedures is \u03b5 = 0.01.Example 8. [18] Let us leave \u00b51 = 1, \u00b52 = (\u2212 1), and only the a primary probabilities py, y."}, {"heading": "7 Results and corollaries", "text": "A common feature of the known methods of recognition with learning is that a single model is selected from a particular model set. This selected model \"best\" corresponds to the learning sample, usually it is a consistent estimation of the model. Nevertheless, only a single model is selected from the set of all possible models. Then, the recognition strategy is constructed as if the selected model were the true model. Due to the consistency of the model calculation, such methods are acceptable if the study sample is large enough. However, the methods give no guarantees for later recognition if the study sample has a limited size, especially if the size is obviously insufficient to clearly determine the model. The disadvantage is known as the small example problem and forms an obvious gap in today's knowledge of recognition with uncertain statistical model. Accuracy is achieved only in two extreme cases. If no learning sample is available, then it is appropriate to use the Minimax method, then it is large enough, then it is possible to use known methods with learning."}], "references": [{"title": "Minimax regret classifier for imprecise class distributions", "author": ["Ro\u0107\u0131o Alaiz-Rod\u0155\u0131guez", "Alicia Guerrero-Curieses", "Jes\u00fas Cid-Sueiro"], "venue": "J. Mach. Learn. Res.,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2007}, {"title": "Convex Analysis and Nonlinear Optimization", "author": ["J.M. Borwein", "A.S. Lewis"], "venue": null, "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2000}, {"title": "Convex Optimization", "author": ["S. Boyd", "L. Vandenberghe"], "venue": null, "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2004}, {"title": "An introduction to empirical bayes data analysis", "author": ["George Casella"], "venue": "The American Statistician,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2009}, {"title": "Elementary Decision Theory. Wiley series in probability and mathematical statistics: Applied probability and statistics", "author": ["H. Chernoff", "H.C.L.E. Moses"], "venue": null, "citeRegEx": "5", "shortCiteRegEx": "5", "year": 1959}, {"title": "Maximum likelihood from incomplete data via the em algorithm", "author": ["A.P. Dempster", "N.M. Laird", "D.B. Rubin"], "venue": "Journal of the Royal Statistical Society. Series B (Methodological),", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 1977}, {"title": "Multicriteria Optimization. Lecture notes in economics and mathematical systems", "author": ["M. Ehrgott"], "venue": null, "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2005}, {"title": "Proper Efficiency and the Theory of Vector Maximization", "author": ["Arthur M. Geoffrion"], "venue": "Journal of Mathematics, Analysis and Applications,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 1968}, {"title": "Fundamentals of Convex Analysis", "author": ["J.-B. Hiriart-Urruty", "C. Lemarechal"], "venue": null, "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2002}, {"title": "The Advanced Theory of Statistics: In Three Volumes", "author": ["M.G. Kendall", "A. Stuart"], "venue": "Number 2 in The Advanced Theory of Statistics. Griffin,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 1979}, {"title": "On combining classifiers", "author": ["Josef Kittler", "Mohamad Hatef", "Robert P.W. Duin", "Jiri Matas"], "venue": "IEEE Transaction on Pattern Analysis and Machine Intelligence,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 1998}, {"title": "Testing Statistical Hypotheses", "author": ["E.L.A. Lehmann"], "venue": null, "citeRegEx": "13", "shortCiteRegEx": "13", "year": 1986}, {"title": "Optimal Statistical Decisions", "author": ["H.DeGroot Morris"], "venue": null, "citeRegEx": "14", "shortCiteRegEx": "14", "year": 1970}, {"title": "Two Breakthroughs in the Theory of Statistical Decision Making", "author": ["J. Neyman"], "venue": "Revue de l\u2019Institut International de Statistique / Review of the International Statistical Institute,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 1962}, {"title": "Asymptotically Subminimax Solutions of Compound Statistical Decision Problems", "author": ["Herbert Robbins"], "venue": "In Jerzy Neyman, editor, Proceedings of the Second Berkeley Symposium on Mathematical Statistics and Probability,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 1951}, {"title": "An empirical bayes approach to statistics", "author": ["Herbert Robbins"], "venue": "Proc. Third Berkeley Symp. on Math. Statist. and Prob.,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 1956}, {"title": "On pattern recognition learning problem formulation. (in russian)", "author": ["M.I. Schlesinger", "A.V. Bondarenko"], "venue": "Control Systems and Computers,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2009}, {"title": "Ten Lectures on Statistical and Structural Pattern Recognition", "author": ["M.I. Schlesinger", "V. Hlavac"], "venue": "Computational Imaging and Vision Series. Kluwer Academic Pub,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2002}, {"title": "The interaction of learning and self-organization in pattern", "author": ["M.I. Shlezinger"], "venue": "recognition. Kibernetika,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 1968}, {"title": "Nondifferentiable Optimization and Polynomial Problems", "author": ["N.Z. Shor"], "venue": "Nonconvex Optimization and Its Applications. Springer,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 1998}, {"title": "Multiple criteria decision making. McGraw-Hill series in quantitative methods for management", "author": ["M. Zeleny"], "venue": null, "citeRegEx": "23", "shortCiteRegEx": "23", "year": 1982}], "referenceMentions": [{"referenceID": 4, "context": "1 Introduction We consider three wide directions in theoretical and applied statistics: complex hypothesis testing [5, 11, 13, 14], the empirical Bayesian approach [4, 15, 16, 17] and learning in pattern recognition [7, 22].", "startOffset": 115, "endOffset": 130}, {"referenceID": 9, "context": "1 Introduction We consider three wide directions in theoretical and applied statistics: complex hypothesis testing [5, 11, 13, 14], the empirical Bayesian approach [4, 15, 16, 17] and learning in pattern recognition [7, 22].", "startOffset": 115, "endOffset": 130}, {"referenceID": 11, "context": "1 Introduction We consider three wide directions in theoretical and applied statistics: complex hypothesis testing [5, 11, 13, 14], the empirical Bayesian approach [4, 15, 16, 17] and learning in pattern recognition [7, 22].", "startOffset": 115, "endOffset": 130}, {"referenceID": 12, "context": "1 Introduction We consider three wide directions in theoretical and applied statistics: complex hypothesis testing [5, 11, 13, 14], the empirical Bayesian approach [4, 15, 16, 17] and learning in pattern recognition [7, 22].", "startOffset": 115, "endOffset": 130}, {"referenceID": 3, "context": "1 Introduction We consider three wide directions in theoretical and applied statistics: complex hypothesis testing [5, 11, 13, 14], the empirical Bayesian approach [4, 15, 16, 17] and learning in pattern recognition [7, 22].", "startOffset": 164, "endOffset": 179}, {"referenceID": 13, "context": "1 Introduction We consider three wide directions in theoretical and applied statistics: complex hypothesis testing [5, 11, 13, 14], the empirical Bayesian approach [4, 15, 16, 17] and learning in pattern recognition [7, 22].", "startOffset": 164, "endOffset": 179}, {"referenceID": 14, "context": "1 Introduction We consider three wide directions in theoretical and applied statistics: complex hypothesis testing [5, 11, 13, 14], the empirical Bayesian approach [4, 15, 16, 17] and learning in pattern recognition [7, 22].", "startOffset": 164, "endOffset": 179}, {"referenceID": 15, "context": "1 Introduction We consider three wide directions in theoretical and applied statistics: complex hypothesis testing [5, 11, 13, 14], the empirical Bayesian approach [4, 15, 16, 17] and learning in pattern recognition [7, 22].", "startOffset": 164, "endOffset": 179}, {"referenceID": 5, "context": "It seems as if the well-known ideas of self-learning or unsupervised learning [6, 19, 20] appeared independently of the empirical Bayesian approach.", "startOffset": 78, "endOffset": 89}, {"referenceID": 17, "context": "It seems as if the well-known ideas of self-learning or unsupervised learning [6, 19, 20] appeared independently of the empirical Bayesian approach.", "startOffset": 78, "endOffset": 89}, {"referenceID": 18, "context": "It seems as if the well-known ideas of self-learning or unsupervised learning [6, 19, 20] appeared independently of the empirical Bayesian approach.", "startOffset": 78, "endOffset": 89}, {"referenceID": 6, "context": "Using the concepts of multi-criteria optimization [8, 9], we analyze the empirical Bayesian approach, learning in pattern recognition and complex hypothesis testing in a unified framework.", "startOffset": 50, "endOffset": 56}, {"referenceID": 7, "context": "Using the concepts of multi-criteria optimization [8, 9], we analyze the empirical Bayesian approach, learning in pattern recognition and complex hypothesis testing in a unified framework.", "startOffset": 50, "endOffset": 56}, {"referenceID": 14, "context": "Robbins [16], which addressed recognition of compound objects and initiated the empirical Bayesian approach [15], are revisited.", "startOffset": 8, "endOffset": 12}, {"referenceID": 13, "context": "Robbins [16], which addressed recognition of compound objects and initiated the empirical Bayesian approach [15], are revisited.", "startOffset": 108, "endOffset": 112}, {"referenceID": 4, "context": "2 Complex object recognition The main concepts of complex hypothesis discrimination have been formulated in the general statistical decision theory [5, 11, 13, 14].", "startOffset": 148, "endOffset": 163}, {"referenceID": 9, "context": "2 Complex object recognition The main concepts of complex hypothesis discrimination have been formulated in the general statistical decision theory [5, 11, 13, 14].", "startOffset": 148, "endOffset": 163}, {"referenceID": 11, "context": "2 Complex object recognition The main concepts of complex hypothesis discrimination have been formulated in the general statistical decision theory [5, 11, 13, 14].", "startOffset": 148, "endOffset": 163}, {"referenceID": 12, "context": "2 Complex object recognition The main concepts of complex hypothesis discrimination have been formulated in the general statistical decision theory [5, 11, 13, 14].", "startOffset": 148, "endOffset": 163}, {"referenceID": 6, "context": "We follow its main ideas (see, for example, the book [8]) and translate them into the context of our problem.", "startOffset": 53, "endOffset": 56}, {"referenceID": 1, "context": "Due to the well-known duality theorem [2, 3, 10], for such functions F we have max \u03c4\u2208T min qX\u2208QX F (\u03c4, qX) = F (\u03c4 , q X) = min qX\u2208QX max \u03c4\u2208T F (\u03c4, qX).", "startOffset": 38, "endOffset": 48}, {"referenceID": 2, "context": "Due to the well-known duality theorem [2, 3, 10], for such functions F we have max \u03c4\u2208T min qX\u2208QX F (\u03c4, qX) = F (\u03c4 , q X) = min qX\u2208QX max \u03c4\u2208T F (\u03c4, qX).", "startOffset": 38, "endOffset": 48}, {"referenceID": 8, "context": "Due to the well-known duality theorem [2, 3, 10], for such functions F we have max \u03c4\u2208T min qX\u2208QX F (\u03c4, qX) = F (\u03c4 , q X) = min qX\u2208QX max \u03c4\u2208T F (\u03c4, qX).", "startOffset": 38, "endOffset": 48}, {"referenceID": 11, "context": "Perhaps, the most popular requirement in the theory and practice of complex hypothesis discrimination is the following so-called minimax requirement [13].", "startOffset": 149, "endOffset": 153}, {"referenceID": 20, "context": "In multi-criteria decision making [23] another reasonable requirement is used that differs from the minimax one.", "startOffset": 34, "endOffset": 38}, {"referenceID": 20, "context": "Let us follow the way shown in [23] and define a number min qX\u2208QX RX(qX , \u03b8) for each \u03b8 \u2208 \u0398 and call it the risk of optimal strategy.", "startOffset": 31, "endOffset": 35}, {"referenceID": 20, "context": "The concept of the closest to optimal strategy is a straightforward application of the recommendations in [23] to our special case.", "startOffset": 106, "endOffset": 110}, {"referenceID": 0, "context": "On the other hand, the closest to optimal strategy is a generalization of the minimax deviation (regret) strategy from [1].", "startOffset": 119, "endOffset": 122}, {"referenceID": 14, "context": "Robbins [16] decades ago and initiated an empirical Bayesian approach [15].", "startOffset": 8, "endOffset": 12}, {"referenceID": 13, "context": "Robbins [16] decades ago and initiated an empirical Bayesian approach [15].", "startOffset": 70, "endOffset": 74}, {"referenceID": 14, "context": "Robbins explains his idea on the following simple example [16], which we will use several times through the article.", "startOffset": 58, "endOffset": 62}, {"referenceID": 13, "context": "The strategy (10) illustrates the main idea of the empirical Bayesian approach [15, 16, 17], namely that an object sample can be recognized much better than by recognizing each object independently.", "startOffset": 79, "endOffset": 91}, {"referenceID": 14, "context": "The strategy (10) illustrates the main idea of the empirical Bayesian approach [15, 16, 17], namely that an object sample can be recognized much better than by recognizing each object independently.", "startOffset": 79, "endOffset": 91}, {"referenceID": 15, "context": "The strategy (10) illustrates the main idea of the empirical Bayesian approach [15, 16, 17], namely that an object sample can be recognized much better than by recognizing each object independently.", "startOffset": 79, "endOffset": 91}, {"referenceID": 18, "context": "To solve (13), the algorithms [20] are used, which later became widely known as the EM-algorithms [6].", "startOffset": 30, "endOffset": 34}, {"referenceID": 5, "context": "To solve (13), the algorithms [20] are used, which later became widely known as the EM-algorithms [6].", "startOffset": 98, "endOffset": 101}, {"referenceID": 10, "context": "Moreover, the learning information can come from several independent sources simultaneously as it is mentioned in [12].", "startOffset": 114, "endOffset": 118}, {"referenceID": 14, "context": "Robbins when he wanted to derive the empirical Bayesian approach from the ideas of minimax decisions [16].", "startOffset": 101, "endOffset": 105}, {"referenceID": 1, "context": "Due to the well-known duality theorem [2, 3, 10] this function satisfies the equality", "startOffset": 38, "endOffset": 48}, {"referenceID": 2, "context": "Due to the well-known duality theorem [2, 3, 10] this function satisfies the equality", "startOffset": 38, "endOffset": 48}, {"referenceID": 8, "context": "Due to the well-known duality theorem [2, 3, 10] this function satisfies the equality", "startOffset": 38, "endOffset": 48}, {"referenceID": 19, "context": "Due to concavity of the function \u03a6, it can be maximized using well-known methods of convex optimization, such as supergradient hill-climbing [21].", "startOffset": 141, "endOffset": 145}, {"referenceID": 16, "context": "[18] Let \u03bc1 = 1, \u03bc2 = (\u22121), and only the a priori probabilities py, y \u2208 {1, 2}, be unknown.", "startOffset": 0, "endOffset": 4}], "year": 2015, "abstractText": "We formulate problems of statistical recognition and learning in a common framework of complex hypothesis testing. Based on arguments from multi-criteria optimization, we identify strategies that are improper for solving these problems and derive a common form of the remaining strategies. We show that some widely used approaches to recognition and learning are improper in this sense. We then propose a generalized formulation of the recognition and learning problem which embraces the whole range of sizes of the learning sample, including the zero size. Learning becomes a special case of recognition without learning. We define the concept of closest to optimal strategy, being a solution to the formulated problem, and describe a technique for finding such a strategy. On several illustrative cases, the strategy is shown to be superior to the widely used learning methods based on maximal likelihood estimation.", "creator": "LaTeX with hyperref package"}}}