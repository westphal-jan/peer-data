{"id": "1509.08842", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "29-Sep-2015", "title": "Automatically Segmenting Oral History Transcripts", "abstract": "Dividing oral histories into topically coherent segments can make them more accessible online. People regularly make judgments about where coherent segments can be extracted from oral histories. But making these judgments can be taxing, so automated assistance is potentially attractive to speed the task of extracting segments from open-ended interviews. When different people are asked to extract coherent segments from the same oral histories, they often do not agree about precisely where such segments begin and end. This low agreement makes the evaluation of algorithmic segmenters challenging, but there is reason to believe that for segmenting oral history transcripts, some approaches are more promising than others. The BayesSeg algorithm performs slightly better than TextTiling, while TextTiling does not perform significantly better than a uniform segmentation. BayesSeg might be used to suggest boundaries to someone segmenting oral histories, but this segmentation task needs to be better defined.", "histories": [["v1", "Tue, 29 Sep 2015 16:46:52 GMT  (130kb,D)", "http://arxiv.org/abs/1509.08842v1", "13 pages, 3 figures"]], "COMMENTS": "13 pages, 3 figures", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["ryan shaw"], "accepted": false, "id": "1509.08842"}, "pdf": {"name": "1509.08842.pdf", "metadata": {"source": "CRF", "title": "Automatically Segmenting Oral History Transcripts", "authors": ["Ryan Shaw"], "emails": ["ryanshaw@unc.edu"], "sections": [{"heading": null, "text": "Keywords: oral history, discourse segmentation, natural language processing, digital libraries"}, {"heading": "1 Introduction", "text": "Oral histories are rich and unique documents of the past and our memory of it. Putting oral histories on the web makes them more accessible, but they remain discouraging to consume [9]. It takes a significant amount of time to listen to a one- or two-hour interview. Therefore, curators of oral history collections typically choose short excerpts from longer interviews when creating \"exhibits\" of their material for the public. Scientists also choose excerpts from their recordings when presenting their work to a live audience [19, 265]. Short, self-contained extracts of oral stories are more accessible than the longer recordings from which they are taken, but the selection of these extracts is more labor-intensive. The labor required to select excerpts from long interviews could be reduced by partial automation. Extract selection can be modelled as a two-step process: first the interviews are divided into segments, then the segments of interest are selected."}, {"heading": "2 Choosing the Telling Extract", "text": "In fact, it is the case that most of them are able to surpass themselves by going in search of a new identity. In fact, it is the case that they are able to acquire a new identity by acquiring a new identity. In fact, it is the case that they are able to acquire a new identity by acquiring a new identity. In fact, it is the case that they are able to acquire a new identity. Indeed, it is the case that they are able to acquire a new identity by acquiring a new identity."}, {"heading": "3 Discourse Segmentation", "text": "In fact, the fact is that most of them are able to move to another world in which they are able to find themselves."}, {"heading": "4 Data and Methods", "text": "This year it has come to the point that it will be able to drown the aforementioned lcihsrcsrVo in order to stir it and stir it."}, {"heading": "5 Comparing Manual Segmentations", "text": "This year it has come to the point that it will be able to drown the aforementioned lcihsrcnlrVo in order to shake it and shake it."}, {"heading": "6 Comparing Automatic Segmentations", "text": "All 19 transcripts were automatically segmented by comparing our implementations of the TextTiling and BayesSeg algorithms. Stopwords were removed (using Choi's Stopword List [3]) and tokens were contained before segmentation. Before running the BayesSeg algorithm, the tokens were further filtered by marking the POS sequences with the Stanford POS tagger [20] and removing all but the nouns. TextTiling algorithm has a number of parameters that need to be set. I used the same defaults as [11] (token sequence size of 20, block size of 10, a round of smoothing and smoothing of the width of 2) and the \"liberal\" threshold of a standard deviation from the mean depth of the score. BayesSeg requires that the number of desired segments be specified so that the number of segments is the desired transcription for each segment."}, {"heading": "7 Discussion and Future Work", "text": "Segmentation and segmentation could make oral histories more accessible, but although identifying topically coherent segments within interviews is an accepted part of working with oral histories, people often do not agree on the boundaries of these segments. In this study, commentators agree on less than half of the boundaries they set. Despite this limited agreement, there is evidence that automatic segmentation algorithms can produce segmentations that are more consistent with human judgment than uniform segmentations. In particular, the richer language models of the BayesSeg algorithm and the globally optimal boundary selection [4] seem to improve its precision when segmentations of oral history are characterized by different diction and gradual topic shifts. Further progress will depend on both the clearer definition of the segmentation task and the assessment of whether automatic segmentations are suitable for building work systems."}, {"heading": "8 Acknowledgments", "text": "This work was funded by the Institute of Museum and Library Services. Thanks to the Southern Oral History Program for providing the oral history interview protocols, Greg Maslov for his TextTiling implementation, Jacob Eisenstein for publishing the BayesSeg source code on which my implementation was based, and Kathy Brennan and Sara Mannheimer for their annotation work."}], "references": [{"title": "Anecdote as Narrative Resource in Working-Class Life Stories: Parody, Dramatization and Sequence", "author": ["T.G. Ashplant"], "venue": "Chamberlain, M., Thompson, P. (eds.) Narrative and Genre, pp. 99\u2013113. Routledge, London", "citeRegEx": "1", "shortCiteRegEx": null, "year": 1998}, {"title": "Statistical Models for Text Segmentation", "author": ["D. Beeferman", "A. Berger", "J. Lafferty"], "venue": "Machine Learning 34(1-3),", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 1999}, {"title": "Advances in domain independent linear text segmentation", "author": ["F.Y.Y. Choi"], "venue": "Proceedings of the 1st North American chapter of the Association for Computational Linguistics conference", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2000}, {"title": "Bayesian unsupervised topic segmentation", "author": ["J. Eisenstein", "R. Barzilay"], "venue": "Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2008}, {"title": "Large sample variance of kappa in the case of different sets of raters", "author": ["J.L. Fleiss", "J.C. Nee", "J.R. Landis"], "venue": "Psychological Bulletin 86(5),", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 1979}, {"title": "Evaluating Text Segmentation", "author": ["C. Fournier"], "venue": "Ph.D. thesis, University of Ottawa (2013),", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2013}, {"title": "Evaluating Text Segmentation using Boundary Edit Distance", "author": ["C. Fournier"], "venue": "Proceedings of 51st Annual Meeting of the Association for Computational Linguistics. p. to appear. Association for Computational Linguistics,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2013}, {"title": "Automated Transcription and Topic Segmentation of Large Spoken Archives", "author": ["M. Franz", "B. Ramabhadran", "T. Ward", "M. Picheny"], "venue": "Eurospeech 2003. pp. 953\u2013956. Geneva", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2003}, {"title": "Oral History and the Digital Revolution: Toward a Post-Documentary Sensibility", "author": ["M. Frisch"], "venue": "Perks, R., Thomson, A. (eds.) The Oral History Reader. Routledge, London, 2nd edn.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2006}, {"title": "Discourse segmentation of multi-party conversation", "author": ["M. Galley", "K. McKeown", "E. Fosler-Lussier", "H. Jing"], "venue": "Proceedings of the 41st Annual Meeting on Association for Computational Linguistics - ACL \u201903", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2003}, {"title": "TextTiling: segmenting text into multi-paragraph subtopic passages", "author": ["M.A. Hearst"], "venue": "Computational Linguistics", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 1997}, {"title": "The Dillen: Memories of a man of Stratford-Upon-Avon", "author": ["G.H. Hewins", "A. Hewins"], "venue": "Elm Tree Books, London", "citeRegEx": "12", "shortCiteRegEx": null, "year": 1981}, {"title": "Nonparametric Statistical Methods", "author": ["M. Hollander", "D.A. Wolfe"], "venue": "Wiley, New York, 2nd edn.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 1999}, {"title": "Meaningful Access to Audio and Video Passages: A Two-Tiered Approach for Annotation, Navigation, and Cross-Referencing within and across Oral History Interviews", "author": ["D. Lambert", "M. Frisch"], "venue": "Institute of Museum and Library Services,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2012}, {"title": "An Exploratory Study of the Information Behavior of Users of Oral Histories", "author": ["S. Lippincott"], "venue": "Master\u2019s paper, University of North Carolina at Chapel Hill (2012),", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2012}, {"title": "Minimum cut model for spoken lecture segmentation", "author": ["I. Malioutov", "R. Barzilay"], "venue": "Proceedings of the 21st International Conference on Computational Linguistics and the 44th annual meeting of the ACL - ACL \u201906", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2006}, {"title": "Rethinking Text Segmentation Models: An Information Extraction Case Study", "author": ["C.D. Manning"], "venue": "Tech. rep., University of Sydney", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 1998}, {"title": "The Voice of the Past: Oral History", "author": ["P. Thompson"], "venue": "Oxford University Press, Oxford, 3rd edn.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2000}, {"title": "Feature-rich part-of-speech tagging with a cyclic dependency network. In: Proceedings of the 2003 Conference of the North American Chapter of the Association for Computational Linguistics on Human Language Technology - Volume 1 (NAACL \u201903)", "author": ["K. Toutanova", "D. Klein", "C.D. Manning"], "venue": null, "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2003}, {"title": "Knowledge-Based Approaches to the Segmentation of Oral History Interviews", "author": ["P. Zhang", "D. Soergel"], "venue": "Tech. rep., University of Maryland", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2006}], "referenceMentions": [{"referenceID": 8, "context": "Putting oral histories on the web makes them more accessible, but they remain daunting to consume [9].", "startOffset": 98, "endOffset": 101}, {"referenceID": 17, "context": "Scholars also select extracts from their recordings when presenting their work to a live audience [19, 265].", "startOffset": 98, "endOffset": 107}, {"referenceID": 10, "context": "I then used their judgments to evaluate the performance of two automatic text segmentation algorithms, TextTiling [11] and BayesSeg [4].", "startOffset": 114, "endOffset": 118}, {"referenceID": 3, "context": "I then used their judgments to evaluate the performance of two automatic text segmentation algorithms, TextTiling [11] and BayesSeg [4].", "startOffset": 132, "endOffset": 135}, {"referenceID": 17, "context": "Oral historians see \u201cchoosing the telling extract\u201d [19, 265] as an essential part of communicating to their audiences.", "startOffset": 51, "endOffset": 60}, {"referenceID": 14, "context": "The actual words of interviewees are more evocative that the historian\u2019s paraphrase, and extracts are seen as essential to providing \u201ctexture\u201d and \u201cvoice\u201d to the historian\u2019s interpretation [15, 27\u201328].", "startOffset": 189, "endOffset": 200}, {"referenceID": 17, "context": "When giving talks, oral historians will often build their presentation around a handful of extracts, four or five minutes each [19, 266].", "startOffset": 127, "endOffset": 136}, {"referenceID": 8, "context": "Yet as Frisch [9] and others have argued, this kind of description at the interview level is by itself insufficient for making the content of oral histories truly accessible to the public.", "startOffset": 14, "endOffset": 17}, {"referenceID": 13, "context": "Lambert and Frisch [14] advocate for segments of five to fifteen minutes in length, with boundaries at \u201ca natural ending of a topic, a break before a new question, or other", "startOffset": 19, "endOffset": 23}, {"referenceID": 13, "context": "\u201d Making such segments, rather than whole interviews, the target of archival description \u201cunifies the scale of the navigable unit\u201d [14] and solves the problem of longer interviews being less comprehensively described than shorter ones.", "startOffset": 131, "endOffset": 135}, {"referenceID": 16, "context": "But the more general notion of a topical segment is harder to characterize, as topic is not a clearly defined concept [18, 17].", "startOffset": 118, "endOffset": 126}, {"referenceID": 16, "context": "Topical structure is hierarchical [17].", "startOffset": 34, "endOffset": 38}, {"referenceID": 0, "context": "Ashplant [1, 107] demonstrates that this is true of oral histories in his analysis of part of The Dillen [12].", "startOffset": 9, "endOffset": 17}, {"referenceID": 11, "context": "Ashplant [1, 107] demonstrates that this is true of oral histories in his analysis of part of The Dillen [12].", "startOffset": 105, "endOffset": 109}, {"referenceID": 16, "context": "Unfortunately, hierarchies can be computationally intractable, so it is preferable to focus on a specific level in the topical hierarchy that is appropriate for the task at hand, making the topical segmentation a flat, non-overlapping partitioning of the text [18, 17].", "startOffset": 260, "endOffset": 268}, {"referenceID": 1, "context": "[2]) or conversational dialogue in settings such as phone conversations and meetings (e.", "startOffset": 0, "endOffset": 3}, {"referenceID": 9, "context": "[10]).", "startOffset": 0, "endOffset": 4}, {"referenceID": 7, "context": "[8], working with a collection of oral history interviews with Holocaust survivors, trained a classifier for recognizing segment boundaries, using as training data the segment boundaries defined by catalogers.", "startOffset": 0, "endOffset": 3}, {"referenceID": 19, "context": "In a follow-up study using a subset of the same corpus, Zhang and Soergel [21] did a qualitative coding of questions and cataloging segments, and explored using these codes as features in a classifier for recognizing topical segment boundaries, with inconclusive results.", "startOffset": 74, "endOffset": 78}, {"referenceID": 10, "context": "The TextTiling algorithm [11] treats regular chunks of text as term vectors and measures the cosine similarity between them.", "startOffset": 25, "endOffset": 29}, {"referenceID": 3, "context": "The BayesSeg algorithm [4] uses such a generative language model to formalize the notion of language similarity: similar texts are ones produced by the same lexical distribution.", "startOffset": 23, "endOffset": 26}, {"referenceID": 15, "context": "But Malioutov and Barzilay [16] demonstrated that only examining adjacent chunks performs less well when topic shifts are more subtle.", "startOffset": 27, "endOffset": 31}, {"referenceID": 3, "context": "BayesSeg [4] also finds a globally optimal solution, taking advantage of its probabilistic model to find the most likely segmentation.", "startOffset": 9, "endOffset": 12}, {"referenceID": 10, "context": "TextTiling [11], as described above, uses the cosine metric to quantify text similarity, and a local valley-finding algorithm to postulate boundaries.", "startOffset": 11, "endOffset": 15}, {"referenceID": 3, "context": "BayesSeg [4] probabilistically models similar text similarity as the product of a sparse lexical distribution, and uses a global expectation-maximization approach to postulate boundaries.", "startOffset": 9, "endOffset": 12}, {"referenceID": 5, "context": "This approach has various problems, including variance in how different kinds of errors are penalized and sensitivity to an arbitrary window size parameter [6].", "startOffset": 156, "endOffset": 159}, {"referenceID": 6, "context": "To avoid these issues I used the boundary edit distance metric proposed by Fournier [7], which does not require any parameters and does not rely on a gold standard segmentation.", "startOffset": 84, "endOffset": 87}, {"referenceID": 5, "context": "Figure 1 shows the distribution of segment lengths, which, like the topic segmentation datasets examined by Fournier [6], appears to be exponential.", "startOffset": 117, "endOffset": 120}, {"referenceID": 6, "context": "To measure inter-annotator agreement I used the boundary edit distance metric proposed by Fournier [7].", "startOffset": 99, "endOffset": 102}, {"referenceID": 5, "context": "Fournier [6, 152] suggests that for comparing segmentations of texts at paragraph boundaries, setting nt to 2 is appropriate, so that only pairs of boundary judgments that differ by one paragraph (and no more) will be considered \u201cnear misses.", "startOffset": 9, "endOffset": 17}, {"referenceID": 4, "context": "This pairwise mean boundary edit distance measures actual agreement; to correct for chance agreement one can calculate Fleiss\u2019 \u03c0\u2217 coefficient [5], which was also 0.", "startOffset": 142, "endOffset": 145}, {"referenceID": 2, "context": "Stopwords were removed (using Choi\u2019s stopword list[3]) and tokens were stemmed before segmentation.", "startOffset": 50, "endOffset": 53}, {"referenceID": 18, "context": "Before running the BayesSeg algorithm, the tokens were further filtered by POS-tagging them using the Stanford POS tagger [20] and removing all but the nouns.", "startOffset": 122, "endOffset": 126}, {"referenceID": 10, "context": "I used the same defaults as [11] (token-sequence size of 20, block size of 10, one round of smoothing, and smoothing width of 2), and the \u201cliberal\u201d boundary threshold of one standard deviation from the mean depth score.", "startOffset": 28, "endOffset": 32}, {"referenceID": 5, "context": "I evaluated the two automatic segmentation algorithms using the approach advocated by Fournier [6].", "startOffset": 95, "endOffset": 98}, {"referenceID": 12, "context": "The scores are not normally distributed, so a Kruskal-Wallis distribution-free test [13, 190\u2013199] was performed to establish that the scores differed significantly among all the groups (H = 329, 3d.", "startOffset": 84, "endOffset": 97}, {"referenceID": 12, "context": "Pairwise multiple comparisons among groups using the Dwass, Steel, Critchlow-Flinger procedure [13, 240\u2013248] established that there were significant differences between each pair except TextTiling and the uniform segmenter (W \u2217 = 0.", "startOffset": 95, "endOffset": 108}, {"referenceID": 3, "context": "In particular, the BayesSeg algorithm\u2019s richer language model and globally optimal boundary selection [4] seem to improve its precision when segmenting oral history transcripts characterized by varied diction and gradual topic shifts.", "startOffset": 102, "endOffset": 105}], "year": 2015, "abstractText": "Dividing oral histories into topically coherent segments can make them more accessible online. People regularly make judgments about where coherent segments can be extracted from oral histories. But making these judgments can be taxing, so automated assistance is potentially attractive to speed the task of extracting segments from openended interviews. When different people are asked to extract coherent segments from the same oral histories, they often do not agree about precisely where such segments begin and end. This low agreement makes the evaluation of algorithmic segmenters challenging, but there is reason to believe that for segmenting oral history transcripts, some approaches are more promising than others. The BayesSeg algorithm performs slightly better than TextTiling, while TextTiling does not perform significantly better than a uniform segmentation. BayesSeg might be used to suggest boundaries to someone segmenting oral histories, but this segmentation task needs to be better defined.", "creator": "LaTeX with hyperref package"}}}