{"id": "1509.08967", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "29-Sep-2015", "title": "Very Deep Multilingual Convolutional Neural Networks for LVCSR", "abstract": "Convolutional neural networks (CNNs) are a standard component of many current state-of-the-art Large Vocabulary Continuous Speech Recognition (LVCSR) systems. However, CNNs in LVCSR have not kept pace with recent advances in other domains where deeper neural networks provide superior performance. In this paper we propose a number of architectural advances in CNNs for LVCSR. First, we introduce a very deep convolutional network architecture with up to 14 weight layers. There are multiple convolutional layers before each pooling layer, with small 3x3 kernels, inspired by the VGG Imagenet 2014 architecture. Then, we introduce multilingual CNNs with multiple untied layers. Finally, we introduce multi-scale input features aimed at exploiting more context at negligible computational cost. We evaluate the improvements first on a Babel task for low resource speech recognition, obtaining an absolute 5.78% WER improvement over the baseline PLP DNN by training our CNN on the combined data of six different languages. We then evaluate the very deep CNNs on the Hub5'00 benchmark (using the 262 hours of SWB-1 training data) achieving a word error rate of 12.2% after cross-entropy training, a 1.6% WER improvement over our own baseline and a 1.0% WER improvement over the best published CNN result so far.", "histories": [["v1", "Tue, 29 Sep 2015 22:28:11 GMT  (66kb,D)", "http://arxiv.org/abs/1509.08967v1", "Manuscript submitted to ICASSP 2016"], ["v2", "Sat, 23 Jan 2016 18:18:58 GMT  (66kb,D)", "http://arxiv.org/abs/1509.08967v2", "Accepted for publication at ICASSP 2016"]], "COMMENTS": "Manuscript submitted to ICASSP 2016", "reviews": [], "SUBJECTS": "cs.CL cs.NE", "authors": ["tom sercu", "christian puhrsch", "brian kingsbury", "yann lecun"], "accepted": false, "id": "1509.08967"}, "pdf": {"name": "1509.08967.pdf", "metadata": {"source": "CRF", "title": "VERY DEEP MULTILINGUAL CONVOLUTIONAL NEURAL NETWORKS FOR LVCSR", "authors": ["Tom Sercu", "Christian Puhrsch", "Brian Kingsbury", "Yann LeCun", "T. J. Watson"], "emails": ["tsercu@us.ibm.com,", "bedk@us.ibm.com,"], "sections": [{"heading": null, "text": "Index terms - pleated networks, multilingual, acoustic modeling, speech recognition, neural networks"}, {"heading": "1. INTRODUCTION", "text": "In the context of these hybrid models, the use of CNNs has been shown to be relatively new [1]. In many areas related to natural data, the system architecture is not as advanced as in many other areas, especially in the areas of image classification [2], object recognition [6] and segmentation [7]. Early applications of neural networks for speech recognition are applied in combination with dynamic time recognition. [9] While the globally trained combination of neural networks and HMMs for speech and handwriting dates back to the 1990s, we are becoming dominant in ASR only due to recent developments [12, 13] HMM / DNN hybrid modeling."}, {"heading": "2. ARCHITECTURAL AND TRAINING NOVELTIES", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "2.1. Very Deep Convolutional Networks", "text": "The very deep coil meshes we describe here are adaptations of the VGG network architecture [3] to the LVCSR domain, where until now networks with two coil layers dominated [16, 17, 18]. Table 1 shows the configurations of the deep CNNs. The deepest configuration, WDX, has 14 weight layers: 10 coil layers and 4 fully connected ones. As in [3], we omit the Rectified Linear Unit (ReLU) layers after each coil and fully connected layers. The coil layers are written as conv ({input feature maps} - {output feature maps}), each kernel being understood as size 3 \u00d7 3. Pooling layers are written as (time x frequency) with steps equivalent to the pool size.For VDX and WDX architectures, we apply zero size 1 padding before each coil, while for VC (X) and VB (initiation) layers are not uniform."}, {"heading": "2.2. Multilingual Convolutional Networks", "text": "Figure 1 shows a multilingual VBX network that we used for most of our Babel experiments. It is similar to previous multilingual deep neural networks [20], with the main difference that the common lower layers of the network are revolutionary. A second difference is that we decouple more than just the last layer, which means that the weights and distortions of several fully connected layers are different for each language.Since the output dimension of conventional layers is typically large when using large context windows, most weights are in the first fully connected layer that affects the flat output of conventional layers. This is an argument for dividing this large, first fully connected layer across languages. Experimentally, we have confirmed that the decoupling of all fully connected layers except the lowest layer provides optimal performance, with sharp deterioration if the first fully connected layer also matches the common view and is completely decoupled from the first multilingual view."}, {"heading": "2.3. Multi-scale feature maps", "text": "Figure 2 illustrates the concept of multi-dimensional function boards, where additional input function boards provide a larger view of the context of the frame by sampling larger context windows with different steps. Cores on the first revolutionary level are able to combine information from multiple scales, i.e. different distances from the central frame. Since the only difference for configuration of the convent is that the first revolutionary layer has more function boards, the additional calculation costs and number of parameters are small."}, {"heading": "2.4. Training", "text": "We use Adadelta [29] and Adam [30] to perform the first training of deep CNNs. Using Adadelta has two main advantages: firstly, in our experience, the optimization problem converges much faster than with SGD; in the Babel experiments, we typically see convergence after about 40 million frames using the 18 hours of training data from Babel (after removing about 5.8 million frames from silence); secondly, the optimal working point of Adadelta's hyperparameters and \u03c1 was stable across all architectures and always delivered optimal performance, which was critical in exploring architectural variations; after the first training with Adadelta, we fine-tune the use of SGD with a small learning rate; another aspect of the training that improved our results is data balancing (something similar was done in [6]); we construct batches on the fly by matching target y = CDi with probability of the CDi depending on the frequency of the CDi with the CDi."}, {"heading": "3. EXPERIMENTAL RESULTS", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "3.1. Babel", "text": "Our first experiments on Babel focus on the multilingual and multi-scale aspects of this work. IARPA's Babel program aims to develop robust keyword search technologies for languages with limited resources. Although the word error rates given here are too high to be useful for simple speech and text applications, useful Keyword Search (KWS) systems can still be built on these ASR models. KWS performance generally correlates well with WER.For training data, we use a combination of 6 languages with 3 hours of training data per language. Languages used for training are languages from the second option period of the Babel program, i.e. Kurmanji (KUR), Tok Pisin (TOK), Cebuano (CEB), Kazakh (KAZ), Telugu (TEL) and Lithuanian (DNEL), with languages from the second option period of the Babel program being different. The experiments are multilingual and multilingual."}, {"heading": "3.2. Switchboard 300", "text": "We evaluate our deep CNN architecture using the 262-hour SWB-1 training data and report on the Word Error Rates on Hub5 '00 SWB (Table 5). The switchboard experiments focus on the very deep aspect of our work. Apart from not including multilingual training, we did not use multi-scale features in the switchboard experiments, but only used loudspeaker-dependent VTLN and deltas and double deltas, as this is shown to improve performance for classic CNNs [16]. In the switching experiments, we observed only marginal benefits from using a large context, so we only worked with context windows of \u00b1 8. We defined an epoch as one million frames and trained all our architectures initially with adadelta with = 3e \u2212 11 and \u0445 = 0.98, for about 40 epochs, while we changed the differences from 0.4 to 0.8 in the 25th epoch."}, {"heading": "4. DISCUSSION", "text": "In this work, we proposed a series of architectural advances in CNNs for LVCSR. Our most powerful model has 14 layers of weight. We also introduced multilingual CNNs that proved valuable in the context of low-resource speech recognition; we introduced multi-scale input capabilities that aimed to leverage a more acoustic context with minimal arithmetic enhancement; we showed an improvement of 2.50% WHO over a standard DNN PLP baseline using 3 hours of data; and an improvement of 5.78% WHO by combining six languages to train for 18 hours of data; and then we showed results on Hub5 '00 after training on 262 hours of SWB-1 data, where we received 12.2% WHO, representing an improvement of 1.6% WHO (11.6% relative) over our own baseline; and a 1.0% WHO (7.6% additional) over the classical training where we expect 17% WHO results."}, {"heading": "5. ACKNOWLEDGEMENT", "text": "These efforts use the very limited language packs from the language collections of the IARPA-Babel program IARPA-babel205b-v1.0a, IARPA-babel207b-v1.0e, IARPA-babel301b-v2.0b, IARPA-babel302bv1.0a, IARPA-babel303b-v1.0a, and IARPA-babel304b-v1.0b. This work is supported by the Intelligence Advanced Research Projects Activity (IARPA) through the Department of Defense U.S. Army Research Laboratory (DoD / ARL) under contract number W911NF-12-C-0012. The U.S. government is authorized to reproduce and distribute reprints for government purposes, regardless of any copyright notice thereon. Disclaimer: The views and conclusions contained therein are those of the authors and should not be interpreted as necessarily representing the official guidelines of the IARPA or the U.S. Government or CuVID Corporation."}, {"heading": "6. REFERENCES", "text": "[1] Y. LeCun, L. Bottou, Y. Bengio, and P. Haffner, \"Gradient-based learning applied to document recognition,\" Proc. of the IEEE, vol. 86, no. 11, pp. 2278-2324, 1998. [2] A. Krizhevsky, I. Sutskever, and G. E. Hinton, \"Imagenet classification with deep convolutional image recognition,\" Proc. ICLR. 2015, 2014. [4] P. Sermanet, K. Kavukcuoglu, S. Chintala, and Y. LeCun, \"Pedestrian detection with unsupervised multi-stage feature learning,\" in Proc. CVPR, 2013, pp. 3626-3633. [5] R. Donshue J. hierue, J. Malik T. object, and P. Hanahue T. feature and J. Malik networks. \""}], "references": [{"title": "Gradientbased learning applied to document recognition", "author": ["Y. LeCun", "L. Bottou", "Y. Bengio", "P. Haffner"], "venue": "Proc. of the IEEE, vol. 86, no. 11, pp. 2278\u20132324, 1998.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 1998}, {"title": "Imagenet classification with deep convolutional neural networks", "author": ["A. Krizhevsky", "I. Sutskever", "G.E. Hinton"], "venue": "Proc. NIPS, 2012, pp. 1097\u20131105.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2012}, {"title": "Very deep convolutional networks for large-scale image recognition", "author": ["K. Simonyan", "A. Zisserman"], "venue": "Proc. ICLR 2015, 2014.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2015}, {"title": "Pedestrian detection with unsupervised multi-stage feature learning", "author": ["P. Sermanet", "K. Kavukcuoglu", "S. Chintala", "Y. LeCun"], "venue": "Proc. CVPR, 2013, pp. 3626\u20133633.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2013}, {"title": "Rich feature hierarchies for accurate object detection and semantic segmentation", "author": ["R. Girshick", "J. Donahue", "T. Darrell", "J. Malik"], "venue": "Proc. CVPR, 2014, pp. 580\u2013587.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2014}, {"title": "Overfeat: Integrated recognition, localization and detection using convolutional networks", "author": ["P. Sermanet", "D. Eigen", "X. Zhang", "M. Mathieu", "R. Fergus", "Y. LeCun"], "venue": "Proc. ICLR 2014, 2013.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2014}, {"title": "Learning hierarchical features for scene labeling", "author": ["C. Farabet", "C. Couprie", "L. Najman", "Y. LeCun"], "venue": "IEEE Trans. on Pattern Analysis and Machine Intelligence, vol. 35, no. 8, pp. 1915\u20131929, 2013.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 1915}, {"title": "Phoneme recognition using time-delay neural networks", "author": ["A. Waibel", "T. Hanazawa", "G. Hinton", "K. Shikano", "K.J. Lang"], "venue": "IEEE Trans. on Acoustics, Speech and Signal Processing, vol. 37, no. 3, pp. 328\u2013339, 1989.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 1989}, {"title": "Experiments with time delay networks and dynamic time warping for speaker independent isolated digits recognition", "author": ["L. Bottou", "F.F. Souli\u00e9", "P. Blanchet", "J.S. Li\u00e9nard"], "venue": "Proc. Eurospeech, 1989.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 1989}, {"title": "Speaker-independent isolated digit recognition: multilayer perceptrons vs. dynamic time warping", "author": ["L. Bottou", "F.F. Souli\u00e9", "P. Blanchet", "J.S. Li\u00e9nard"], "venue": "Neural Networks, vol. 3, no. 4, pp. 453\u2013465, 1990.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 1990}, {"title": "Global optimization of a neural network-hidden Markov model hybrid", "author": ["Y. Bengio", "R. De Mori", "G. Flammia", "R. Kompe"], "venue": "IEEE Trans. on Neural Networks, vol. 3, no. 2, pp. 252\u2013 259, 1992.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 1992}, {"title": "Conversational speech transcription using context-dependent deep neural networks", "author": ["F. Seide", "G. Li", "D. Yu"], "venue": "Interspeech, 2011, pp. 437\u2013440.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2011}, {"title": "Deep belief networks using discriminative features for phone recognition", "author": ["A.-r. Mohamed", "T. N Sainath", "G. Dahl", "B. Ramabhadran", "G. E Hinton", "Michael A P."], "venue": "Proc. ICASSP. IEEE, 2011, pp. 5060\u20135063.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2011}, {"title": "Lattice-based optimization of sequence classification criteria for neural-network acoustic modeling", "author": ["Brian Kingsbury"], "venue": "Proc. ICASSP. IEEE, 2009, pp. 3761\u20133764.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2009}, {"title": "Applying convolutional neural networks concepts to hybrid NN- HMM model for speech recognition", "author": ["O. Abdel-Hamid", "A.-r. Mohamed", "H. Jiang", "G. Penn"], "venue": "Proc. ICASSP, 2012, pp. 4277\u20134280.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2012}, {"title": "Deep convolutional neural networks for LVCSR", "author": ["T.N. Sainath", "A.-r. Mohamed", "B. Kingsbury", "B. Ramabhadran"], "venue": "Proc. ICASSP, 2013.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2013}, {"title": "Joint training of convolutional and non-convolutional neural networks", "author": ["H. Soltau", "G. Saon", "T.N. Sainath"], "venue": "Proc. ICASSP, 2014.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2014}, {"title": "The IBM 2015 english conversational telephone speech recognition system", "author": ["G. Saon", "H.-K. Kuo", "S. Rennie", "M. Picheny"], "venue": "arXiv preprint arXiv:1505.05899, 2015.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2015}, {"title": "Very deep convolutional neural networks for LVCSR", "author": ["M. Bi", "Y. Qian", "K. Yu"], "venue": "Proc. Interspeech, 2015.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2015}, {"title": "On the use of a multilingual neural network front-end", "author": ["S. Scanzio", "P. Laface", "L. Fissore", "R. Gemello", "F. Mana"], "venue": "Proc. Interspeech, 2008.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2008}, {"title": "Multilingual MLP features for low-resource LVCSR systems", "author": ["S. Thomas", "S. Ganapathy", "H. Hermansky"], "venue": "Proc. ICASSP, 2012, pp. 4269\u20134272.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2012}, {"title": "Investigation on cross-and multilingual MLP features under matched and mismatched acoustical conditions", "author": ["Z. T\u00fcske", "J. Pinto", "D. Willett", "R. Schl\u00fcter"], "venue": "Proc. ICASSP, 2013, pp. 7349\u20137353.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2013}, {"title": "Investigation of multilingual deep neural networks for spoken term detection", "author": ["K.M. Knill", "M.J.F. Gales", "S.P. Rath", "P.C. Woodland", "C. Zhang", "S.-X. Zhang"], "venue": "Proc. ASRU, 2013, pp. 138\u2013143.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2013}, {"title": "Traffic sign recognition with multi-scale convolutional networks", "author": ["P. Sermanet", "Y. LeCun"], "venue": "Neural Networks (IJCNN), The 2011 International Joint Conference on. IEEE, 2011, pp. 2809\u20132813.", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2011}, {"title": "Fully convolutional networks for semantic segmentation", "author": ["J. Long", "E. Shelhamer", "T. Darrell"], "venue": "CVPR 2015, 2014.", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2015}, {"title": "Depth map prediction from a single image using a multi-scale deep network", "author": ["D. Eigen", "C. Puhrsch", "R. Fergus"], "venue": "Proc. NIPS, 2014, pp. 2366\u20132374.", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2014}, {"title": "Investigation into bottleneck features for meeting speech recognition", "author": ["F. Grezl", "M. Karafi\u00e1t", "L. Burget"], "venue": "INTER- SPEECH, 2009, pp. 2947\u20132950.", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2009}, {"title": "Convolutional, long short-term memory, fully connected deep neural networks", "author": ["T. N Sainath", "O. Vinyals", "A. Senior", "H. Sak"], "venue": ".", "citeRegEx": "28", "shortCiteRegEx": null, "year": 0}, {"title": "ADADELTA: An adaptive learning rate method", "author": ["M.D. Zeiler"], "venue": "arXiv preprint arXiv:1212.5701, 2012.", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2012}, {"title": "Adam: A method for stochastic optimization", "author": ["D. Kingma", "J. Ba"], "venue": "Proc. International Conference on Learning Representations (ICLR), 2015.", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2015}, {"title": "Understanding the difficulty of training deep feedforward neural networks", "author": ["X. Glorot", "Y. Bengio"], "venue": "International conference on artificial intelligence and statistics, 2010, pp. 249\u2013256.", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2010}], "referenceMentions": [{"referenceID": 0, "context": "Convolutional Neural Networks (CNNs) [1] have recently pushed the state of the art on large-scale tasks in many domains dealing with natural data, most notably in computer vision tasks like image classification [2, 3], object detection [4, 5], object localization [6] and segmentation [7].", "startOffset": 37, "endOffset": 40}, {"referenceID": 1, "context": "Convolutional Neural Networks (CNNs) [1] have recently pushed the state of the art on large-scale tasks in many domains dealing with natural data, most notably in computer vision tasks like image classification [2, 3], object detection [4, 5], object localization [6] and segmentation [7].", "startOffset": 211, "endOffset": 217}, {"referenceID": 2, "context": "Convolutional Neural Networks (CNNs) [1] have recently pushed the state of the art on large-scale tasks in many domains dealing with natural data, most notably in computer vision tasks like image classification [2, 3], object detection [4, 5], object localization [6] and segmentation [7].", "startOffset": 211, "endOffset": 217}, {"referenceID": 3, "context": "Convolutional Neural Networks (CNNs) [1] have recently pushed the state of the art on large-scale tasks in many domains dealing with natural data, most notably in computer vision tasks like image classification [2, 3], object detection [4, 5], object localization [6] and segmentation [7].", "startOffset": 236, "endOffset": 242}, {"referenceID": 4, "context": "Convolutional Neural Networks (CNNs) [1] have recently pushed the state of the art on large-scale tasks in many domains dealing with natural data, most notably in computer vision tasks like image classification [2, 3], object detection [4, 5], object localization [6] and segmentation [7].", "startOffset": 236, "endOffset": 242}, {"referenceID": 5, "context": "Convolutional Neural Networks (CNNs) [1] have recently pushed the state of the art on large-scale tasks in many domains dealing with natural data, most notably in computer vision tasks like image classification [2, 3], object detection [4, 5], object localization [6] and segmentation [7].", "startOffset": 264, "endOffset": 267}, {"referenceID": 6, "context": "Convolutional Neural Networks (CNNs) [1] have recently pushed the state of the art on large-scale tasks in many domains dealing with natural data, most notably in computer vision tasks like image classification [2, 3], object detection [4, 5], object localization [6] and segmentation [7].", "startOffset": 285, "endOffset": 288}, {"referenceID": 7, "context": "Early applications of neural nets to speech recognition used Time-Delay Neural Nets [8] which can be seen as simple forms of CNNs without pooling or subsampling.", "startOffset": 84, "endOffset": 87}, {"referenceID": 8, "context": "Full-fledged CNNs with pooling and subsampling were soon applied to speech recognition and combined with dynamic time warping [9, 10].", "startOffset": 126, "endOffset": 133}, {"referenceID": 9, "context": "Full-fledged CNNs with pooling and subsampling were soon applied to speech recognition and combined with dynamic time warping [9, 10].", "startOffset": 126, "endOffset": 133}, {"referenceID": 10, "context": "While the globally-trained combination of neural nets and HMMs for speech and handwriting goes back to the 1990s [11, 1], only due to recent developments [12, 13, 14] HMM/DNN hybrid modeling became dominant in ASR.", "startOffset": 113, "endOffset": 120}, {"referenceID": 0, "context": "While the globally-trained combination of neural nets and HMMs for speech and handwriting goes back to the 1990s [11, 1], only due to recent developments [12, 13, 14] HMM/DNN hybrid modeling became dominant in ASR.", "startOffset": 113, "endOffset": 120}, {"referenceID": 11, "context": "While the globally-trained combination of neural nets and HMMs for speech and handwriting goes back to the 1990s [11, 1], only due to recent developments [12, 13, 14] HMM/DNN hybrid modeling became dominant in ASR.", "startOffset": 154, "endOffset": 166}, {"referenceID": 12, "context": "While the globally-trained combination of neural nets and HMMs for speech and handwriting goes back to the 1990s [11, 1], only due to recent developments [12, 13, 14] HMM/DNN hybrid modeling became dominant in ASR.", "startOffset": 154, "endOffset": 166}, {"referenceID": 13, "context": "While the globally-trained combination of neural nets and HMMs for speech and handwriting goes back to the 1990s [11, 1], only due to recent developments [12, 13, 14] HMM/DNN hybrid modeling became dominant in ASR.", "startOffset": 154, "endOffset": 166}, {"referenceID": 14, "context": "In the context of these hybrid models, the use of CNNs is relatively recent [15].", "startOffset": 76, "endOffset": 80}, {"referenceID": 15, "context": "CNNs were shown to achieve state of the art performance on the benchmark datasets Broadcast News and Switchboard 300 [16].", "startOffset": 117, "endOffset": 121}, {"referenceID": 15, "context": "However, in contrast to the trend in other domains where deeper architectures are often shown to gain performance, the classical CNN architecture in LVCSR [16, 17, 18] has only two convolutional layers.", "startOffset": 155, "endOffset": 167}, {"referenceID": 16, "context": "However, in contrast to the trend in other domains where deeper architectures are often shown to gain performance, the classical CNN architecture in LVCSR [16, 17, 18] has only two convolutional layers.", "startOffset": 155, "endOffset": 167}, {"referenceID": 17, "context": "However, in contrast to the trend in other domains where deeper architectures are often shown to gain performance, the classical CNN architecture in LVCSR [16, 17, 18] has only two convolutional layers.", "startOffset": 155, "endOffset": 167}, {"referenceID": 2, "context": "[3] (subsequently referred to as \u201cVGG Net\u201d) which obtained second place in the classification section of the Imagenet 2014 competition.", "startOffset": 0, "endOffset": 3}, {"referenceID": 18, "context": "Most closely related to this is [19], which also uses VGG Net-inspired CNNs for LVCSR .", "startOffset": 32, "endOffset": 36}, {"referenceID": 18, "context": "In contrast to our work, the architectures investigated in [19] are quite different and the paper only provides results from training on a non-standard Switchboard51h dataset, with WER not close to state of the art performance on Hub5\u201900.", "startOffset": 59, "endOffset": 63}, {"referenceID": 19, "context": "This is related to multilingual neural networks in hybrid NN-HMM systems [20] which have been extended to multilingual bottleneck architectures for tandem models [21, 22] and have proven valuable for spoken term detection [23].", "startOffset": 73, "endOffset": 77}, {"referenceID": 20, "context": "This is related to multilingual neural networks in hybrid NN-HMM systems [20] which have been extended to multilingual bottleneck architectures for tandem models [21, 22] and have proven valuable for spoken term detection [23].", "startOffset": 162, "endOffset": 170}, {"referenceID": 21, "context": "This is related to multilingual neural networks in hybrid NN-HMM systems [20] which have been extended to multilingual bottleneck architectures for tandem models [21, 22] and have proven valuable for spoken term detection [23].", "startOffset": 162, "endOffset": 170}, {"referenceID": 22, "context": "This is related to multilingual neural networks in hybrid NN-HMM systems [20] which have been extended to multilingual bottleneck architectures for tandem models [21, 22] and have proven valuable for spoken term detection [23].", "startOffset": 222, "endOffset": 226}, {"referenceID": 23, "context": "They are inspired by the recent success of combining information at multiple scales in tasks like traffic sign recognition [24], semantic segmentation [7, 25] and depth map prediction [26].", "startOffset": 123, "endOffset": 127}, {"referenceID": 6, "context": "They are inspired by the recent success of combining information at multiple scales in tasks like traffic sign recognition [24], semantic segmentation [7, 25] and depth map prediction [26].", "startOffset": 151, "endOffset": 158}, {"referenceID": 24, "context": "They are inspired by the recent success of combining information at multiple scales in tasks like traffic sign recognition [24], semantic segmentation [7, 25] and depth map prediction [26].", "startOffset": 151, "endOffset": 158}, {"referenceID": 25, "context": "They are inspired by the recent success of combining information at multiple scales in tasks like traffic sign recognition [24], semantic segmentation [7, 25] and depth map prediction [26].", "startOffset": 184, "endOffset": 188}, {"referenceID": 26, "context": "In LVCSR the multi-scale idea has been explored in tandem systems [27] and the CLDNN architecture [28].", "startOffset": 66, "endOffset": 70}, {"referenceID": 27, "context": "In LVCSR the multi-scale idea has been explored in tandem systems [27] and the CLDNN architecture [28].", "startOffset": 98, "endOffset": 102}, {"referenceID": 28, "context": "As training becomes more challenging with increasing depth, we used two recently proposed optimization algorithms, Adadelta [29] and Adam [30] (Section 2.", "startOffset": 124, "endOffset": 128}, {"referenceID": 29, "context": "As training becomes more challenging with increasing depth, we used two recently proposed optimization algorithms, Adadelta [29] and Adam [30] (Section 2.", "startOffset": 138, "endOffset": 142}, {"referenceID": 15, "context": "# Fmaps Classic [16, 17, 18] VB(X) VC(X) VD(X) WD(X) 64 conv(3,64) conv(3,64) conv(3,64) conv(3,64) conv(64,64) conv(64,64) conv(64,64) conv(64,64) pool 1x3 pool 1x2 pool 1x2 pool 1x2", "startOffset": 16, "endOffset": 28}, {"referenceID": 16, "context": "# Fmaps Classic [16, 17, 18] VB(X) VC(X) VD(X) WD(X) 64 conv(3,64) conv(3,64) conv(3,64) conv(3,64) conv(64,64) conv(64,64) conv(64,64) conv(64,64) pool 1x3 pool 1x2 pool 1x2 pool 1x2", "startOffset": 16, "endOffset": 28}, {"referenceID": 17, "context": "# Fmaps Classic [16, 17, 18] VB(X) VC(X) VD(X) WD(X) 64 conv(3,64) conv(3,64) conv(3,64) conv(3,64) conv(64,64) conv(64,64) conv(64,64) conv(64,64) pool 1x3 pool 1x2 pool 1x2 pool 1x2", "startOffset": 16, "endOffset": 28}, {"referenceID": 2, "context": "The very deep convolutional networks we describe here are adaptations of the VGG Net architecture [3] to the LVCSR domain, where until now networks with two convolutional layers dominated [16, 17, 18].", "startOffset": 98, "endOffset": 101}, {"referenceID": 15, "context": "The very deep convolutional networks we describe here are adaptations of the VGG Net architecture [3] to the LVCSR domain, where until now networks with two convolutional layers dominated [16, 17, 18].", "startOffset": 188, "endOffset": 200}, {"referenceID": 16, "context": "The very deep convolutional networks we describe here are adaptations of the VGG Net architecture [3] to the LVCSR domain, where until now networks with two convolutional layers dominated [16, 17, 18].", "startOffset": 188, "endOffset": 200}, {"referenceID": 17, "context": "The very deep convolutional networks we describe here are adaptations of the VGG Net architecture [3] to the LVCSR domain, where until now networks with two convolutional layers dominated [16, 17, 18].", "startOffset": 188, "endOffset": 200}, {"referenceID": 2, "context": "As in [3], we omit the Rectified Linear Unit (ReLU) layers following every convolutional and fully connected layer.", "startOffset": 6, "endOffset": 9}, {"referenceID": 2, "context": "In contrast to [3], we do not reinitialize the deeper models with the shallower models.", "startOffset": 15, "endOffset": 18}, {"referenceID": 30, "context": "This follows the argument of [31] to initialize the weights such that the variance of the activations on each layer does not explode or vanish during the forward pass.", "startOffset": 29, "endOffset": 33}, {"referenceID": 19, "context": "It is similar to previous multilingual deep neural networks [20], with the main difference that the shared lower layers of the network are convolutional.", "startOffset": 60, "endOffset": 64}, {"referenceID": 28, "context": "We use Adadelta [29] and Adam [30] to do initial training of the deep CNNs.", "startOffset": 16, "endOffset": 20}, {"referenceID": 29, "context": "We use Adadelta [29] and Adam [30] to do initial training of the deep CNNs.", "startOffset": 30, "endOffset": 34}, {"referenceID": 5, "context": "Another aspect of training that improved our results is data balancing (something similar was done in [6]).", "startOffset": 102, "endOffset": 105}, {"referenceID": 16, "context": "Classic 512 [17] 13.", "startOffset": 12, "endOffset": 16}, {"referenceID": 16, "context": "6% relative improvement over [17].", "startOffset": 29, "endOffset": 33}, {"referenceID": 15, "context": "Apart from not involving multilingual training, we did not use multi-scale features in the Switchboard experiments, but did use speaker-dependent VTLN and deltas and double deltas as this is shown to help performance for classical CNNs [16].", "startOffset": 236, "endOffset": 240}, {"referenceID": 16, "context": "[17] using classical CNNs with 512 feature maps on both convolutional layers.", "startOffset": 0, "endOffset": 4}, {"referenceID": 17, "context": "[18] which introduces maxout networks with annealed dropout and a large number of CD states to this architecture.", "startOffset": 0, "endOffset": 4}, {"referenceID": 17, "context": "Though [18] does not provide this number, from personal communication with the authors we learned that the annealed dropout Maxout CNN of table 1 in that paper had a WER of 12.", "startOffset": 7, "endOffset": 11}, {"referenceID": 16, "context": "6% relative) improvement over the best result published on classical CNNs after cross-entropy training [17].", "startOffset": 103, "endOffset": 107}, {"referenceID": 16, "context": "We expect additional gains from sequence training, joint training with DNNs [17], and integrating improvements like annealed dropout and maxout nonlinearities [18].", "startOffset": 76, "endOffset": 80}, {"referenceID": 17, "context": "We expect additional gains from sequence training, joint training with DNNs [17], and integrating improvements like annealed dropout and maxout nonlinearities [18].", "startOffset": 159, "endOffset": 163}], "year": 2015, "abstractText": "Convolutional neural networks (CNNs) are a standard component of many current state-of-the-art Large Vocabulary Continuous Speech Recognition (LVCSR) systems. However, CNNs in LVCSR have not kept pace with recent advances in other domains where deeper neural networks provide superior performance. In this paper we propose a number of architectural advances in CNNs for LVCSR. First, we introduce a very deep convolutional network architecture with up to 14 weight layers. There are multiple convolutional layers before each pooling layer, with small 3\u00d73 kernels, inspired by the VGG Imagenet 2014 architecture. Then, we introduce multilingual CNNs with multiple untied layers. Finally, we introduce multiscale input features aimed at exploiting more context at negligible computational cost. We evaluate the improvements first on a Babel task for low resource speech recognition, obtaining an absolute 5.78% WER improvement over the baseline PLP DNN by training our CNN on the combined data of six different languages. We then evaluate the very deep CNNs on the Hub5\u201900 benchmark (using the 262 hours of SWB-1 training data) achieving a word error rate of 12.2% after cross-entropy training, a 1.6% WER improvement over our own baseline and a 1.0% WER improvement over the best published CNN result so far.", "creator": "LaTeX with hyperref package"}}}