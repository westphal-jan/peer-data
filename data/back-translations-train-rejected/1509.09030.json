{"id": "1509.09030", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "30-Sep-2015", "title": "Distributed Weighted Parameter Averaging for SVM Training on Big Data", "abstract": "Two popular approaches for distributed training of SVMs on big data are parameter averaging and ADMM. Parameter averaging is efficient but suffers from loss of accuracy with increase in number of partitions, while ADMM in the feature space is accurate but suffers from slow convergence. In this paper, we report a hybrid approach called weighted parameter averaging (WPA), which optimizes the regularized hinge loss with respect to weights on parameters. The problem is shown to be same as solving SVM in a projected space. We also demonstrate an $O(\\frac{1}{N})$ stability bound on final hypothesis given by WPA, using novel proof techniques. Experimental results on a variety of toy and real world datasets show that our approach is significantly more accurate than parameter averaging for high number of partitions. It is also seen the proposed method enjoys much faster convergence compared to ADMM in features space.", "histories": [["v1", "Wed, 30 Sep 2015 06:59:31 GMT  (178kb)", "http://arxiv.org/abs/1509.09030v1", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["ayan das", "sourangshu bhattacharya"], "accepted": false, "id": "1509.09030"}, "pdf": {"name": "1509.09030.pdf", "metadata": {"source": "CRF", "title": "Distributed Weighted Parameter Averaging for SVM Training on Big Data", "authors": ["Ayan Das"], "emails": ["ayand@cse.iitkgp.ernet.in", "sourangshu@cse.iitkgp.ernet.in"], "sections": [{"heading": null, "text": "ar Xiv: 150 9.09 030v 1 [cs.L G] 30 SeN) stability, which is tied to the final hypothesis of corrugators, using novel evidence techniques. Experimental results on a variety of toy and real data sets show that our approach is much more accurate than averaging the parameters for a high number of partitions. It is also seen that the proposed method exhibits a much faster convergence compared to ADMM in the feature space."}, {"heading": "1 Introduction", "text": "With the increasing number of people who feel able to acquire their identity, it is possible that they are able to acquire their identity and that they are able to acquire their identity."}, {"heading": "2 Distributed Weighted Parameter Averaging (DWPA)", "text": "In this section, we describe the distributed SVM training problem, the proposed solution approach, and a distributed algorithm. In Section 2.4, we describe a limit to the stability of the final hypothesis. Note that for simplicity, we focus on the distributed SVM problem, and the techniques described here apply to other distributed, regulated risk mitigation problems."}, {"heading": "2.1 Background", "text": "Faced with a training data set S = {(xi, yi): i = 1, \u00b7 \u00b7, ML, yi-1, xi-1, xi-Rd}, the linear SVM problem [4] is given by: min w\u03bb-w = 22 + 1mML \u2211 i = 1loss (w; (xi, yi)), (1), where the regularization parameter and the hinge loss is defined as loss (w; (xi, yi))) = max (0, 1 \u2212 yiw Txi). The separating hyperplane is given by the equation wTx + b = 0. Here, we include the bias b within w by performing the following transformation, w = [wT andxi = [xTi, 1] T. The above SVM problem can be solved in a distributed manner that is interesting if the volume of training data is effectively stored and processed on a single computer."}, {"heading": "2.2 Weighted parameter averaging (WPA)", "text": "The parameter averaging method uses a uniform weight of 1 M for each of the M components. A more general constellation can be described in which the final hypothesis is a weighted sum of the parameters obtained on each partition. \u2212 \u2212 \u2212 Target value: w = x m = 1 x m, where w = m are defined as above and \u03b2m = 1,. \u2212 Target value: \u03b2 = [\u03b21, \u00b7 \u00b7, \u03b2M] T = [1M,., 1 M] reaches the PA constellation. Note that Mann et al. [9] has suggested that \u03b2 be in a simple formula. However, no scheme has been proposed to learn an appropriate formula. Our goal is to find the optimal constellation of weights that achieves the lowest regulated loss."}, {"heading": "2.3 Distributed algorithms for WPA using ADMM", "text": "In the distributed environment, we assume that there is a central (master) computer that stores and updates the final hypothesis. Subdivisions of the training set S1,.., SM are distributed among M (slave) computers where local optimizations are performed, the master must communicate with the slaves and vice versa, but no communication between the slaves is necessary. Underlying networks have a star topology that can also be easily implemented with big data platforms such as Hadoop [1]."}, {"heading": "2.4 Bound on stability of WPA", "text": "In this section, we derive a limitation of O (1 ML) to the stability of PA (1 ML). This results in a limitation of O (1 ML) to a deviation from the generalization optimizer (see [9], Theorem 2).Let S = {S1, \u00b7 \u00b7 S \"1, \u00b7 \u00b7 M\" become two datasets with M partitions and L datapoints per partition, which differ in only one datapoint. Hence, Sm = {zm1, \u00b7 zmL \"and S\" m = {S. \"M\" become two datasets with M partitions and L datapoints per partition."}, {"heading": "3 Experimental Results", "text": "In this section, we experimentally analyze and compare the methods proposed here, the distributed weighted averaging of parameters (DWPA) and the accelerated DSVM (DWPAacc) described in Section 2.3, with the averaging of parameters (PA) [9], the distributed SVM (DSVM) using ADMM, and the accelerated DSVM (DSVMacc) [2]. For our experiments, we implemented all of the above algorithms in Matlab [10]. We used the liblinear library [5] to obtain the SVM parameters corresponding to each partition. Optimization problems that occur as partial problems in ADMM were solved with CVX [7], [8]. We used both toy data sets (Section 3.1) and real data sets (described in Table 1) for our experiments."}, {"heading": "3.1 Results on toy dataset", "text": "The main purpose of the toy data set was to visually observe the effects of changing the number of partitions on the final hypothesis for different algorithms. Data points are generated from a two-dimensional mixture of Gaussians. In Figure 1, the red and blue dots show the data points from two different classes. The top red dots contains only 20% of the red dots, so as the number of partitions increases, many partitions will not have data points from the top dots. In these partitions, the dividing hyperplane actually passes through the top red dots, causing the average hyperplane to pass through the top red dots, which reduces accuracy. \u2212 Finally, the right-hand chart of Figure 1 shows the resulting decrease in the number of PA calculations with the resulting increase in accuracy."}, {"heading": "3.2 Comparison of Accuracies", "text": "In this section, we compare accuracies achieved by different algorithms on real data sets with increasing number of paritions. Figure 2 reports accuracies of test sets for PA, WPA, and SVM on three real data sets with different partition sizes. It is clear that PA performance decreases dramatically with increasing paritions. Therefore, the effect shown in Section 3.1 is also observed on real data sets. We also observe that the performance of WPA improves with increasing number of paritions, due to the fact that the space size to which XMLs with H (Section 2.2) are projected increases, thereby reducing the information loss caused by projection. Finally, WPA performs slightly worse than SVM as expected."}, {"heading": "3.3 Convergence Analysis and time comparison", "text": "In this section we compare the convergence properties of DSVM, DSVMacc, DWPA and DWPAacc. Here we report on the results of Realsim due to lack of space. Results to other real datasets are provided in Appendix C. The top line of Figure 3 shows a variation of the original residual parameters (discrepancy between parameters on different partitions) with iterations. It is clear that DWPA and DWPAacc have much less discrepancy compared to DSVM and DWPAacc and therefore have a faster convergence. The bottom line of Figure 3 shows a variation in the accuracy of the test sets with iterations. The same behavior is also shown here, with the test set accuracy of DWPA and DWPAacc converging much faster than DSVM and DSVMacc. One of the reasons is also that the DWPA is an obvious good starting point for \u03b2 = [1M,...,..., 1 M], the PDPA function corresponding to the DWW, where the DWW is a number of time needed for the SVPA and DWiPA."}, {"heading": "4 Conclusion", "text": "We propose a novel approach to train SVM in a distributed way, learning an optimal set of weights to learn the SVM parameters independently on partitions of the entire dataset. Experimental results show that our method is much more accurate than averaging parameters and much faster than the SVM parameters in the feature pace. Furthermore, our method achieves accuracy in much less time than the SVM learned in the feature pace. We propose a novel proof to show that the final SVM parameter learned using DWPA is O (1ML). In addition, our method requires a much smaller network bandwidth compared to DSVM when the number of features of a given dataset is very large compared to the number of partitions, which is the usual scenario for big data."}, {"heading": "Appendix A", "text": "Theorem 4.1. For any two training samples of size L, which differ around a sample point, the stability that applies to the parameters returned by the support vector machine is as follows: \"W\" is O (1ML). (16) Suppose we have two training data sets S = (z1, \u00b7, zL \u2212 w) and S = (z1, \u00b7, zL \u2212 1, z \u00b2 L), where z = (x, y), so that X \u00d7 Y, Rd and Y = (z1, + 1), the two sets differ at a single data point: zL = (xi, yi) and z \u00b2 L = (x, y \u00b2 L). Let BF be the Bregman divergence associated with a convex and non-differable function."}, {"heading": "Appendix B", "text": "Theorem 4.2.: \"We have a lower limit,\" \"we have a lower limit,\" \"we have a lower limit,\" \"we have a lower limit,\" \"we have a lower limit,\" \"we have a lower limit,\" \"we have a lower limit,\" \"we have a lower limit,\" \"we have a lower limit,\" \"we have a lower limit,\" \"we have a lower limit,\" \"we have a lower limit,\" \"we have a lower limit,\" \"we have a lower limit,\" \"we have a lower limit,\" \"we have a lower limit,\" \"we have a lower limit,\" \"we\" have a lower limit."}], "references": [{"title": "Distributed optimization and statistical learning via the alternating direction method of multipliers", "author": ["Stephen Boyd", "Neal Parikh", "Eric Chu", "Borja Peleato", "Jonathan Eckstein"], "venue": "Found. Trends Mach. Learn.,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2011}, {"title": "LIBSVM: A library for support vector machines", "author": ["Chih-Chung Chang", "Chih-Jen Lin"], "venue": "ACM Transactions on Intelligent Systems and Technology,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2011}, {"title": "LIB- LINEAR: A library for large linear classification", "author": ["Rong-En Fan", "Kai-Wei Chang", "Cho-Jui Hsieh", "Xiang-Rui Wang", "Chih-Jen Lin"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2008}, {"title": "Consensus-based distributed support vector machines", "author": ["Pedro A. Forero", "Alfonso Cano", "Georgios B. Giannakis"], "venue": "J. Mach. Learn. Res.,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2010}, {"title": "Graph implementations for nonsmooth convex programs", "author": ["Michael Grant", "Stephen Boyd"], "venue": "Recent Advances in Learning and Control, Lecture Notes in Control and Information Sciences,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2008}, {"title": "CVX: Matlab software for disciplined convex programming, version 2.0 beta", "author": ["Michael Grant", "Stephen Boyd"], "venue": null, "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2013}, {"title": "Efficient large-scale distributed training of conditional maximum entropy models", "author": ["Gideon Mann", "Ryan McDonald", "Mehryar Mohri", "Nathan Silberman", "Dan Walker"], "venue": "Advances in Neural Information Processing Systems", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2009}, {"title": "Foundations of Machine Learning", "author": ["Mehryar Mohri", "Afshin Rostamizadeh", "Ameet Talwalkar"], "venue": null, "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2012}], "referenceMentions": [{"referenceID": 6, "context": "1 Introduction With the growing popularity of Big Data platforms [1] for various machine learning and data analytics applications [9, 12], distributed training of Support Vector Machines (SVMs)[4] on Big Data platforms have become increasingly important.", "startOffset": 130, "endOffset": 137}, {"referenceID": 6, "context": "Big data platforms such as Hadoop [1] provide simple programming abstraction (Map Reduce), scalability and fault tolerance at the cost of distributed iterative computation being slow and expensive [9].", "startOffset": 197, "endOffset": 200}, {"referenceID": 3, "context": "The problem of distributed training of support vector machines (SVM) [6] in particular, and distributed regularized loss minimization (RLM) in general [2, 9], has received a lot of attention in the recent times.", "startOffset": 69, "endOffset": 72}, {"referenceID": 0, "context": "The problem of distributed training of support vector machines (SVM) [6] in particular, and distributed regularized loss minimization (RLM) in general [2, 9], has received a lot of attention in the recent times.", "startOffset": 151, "endOffset": 157}, {"referenceID": 6, "context": "The problem of distributed training of support vector machines (SVM) [6] in particular, and distributed regularized loss minimization (RLM) in general [2, 9], has received a lot of attention in the recent times.", "startOffset": 151, "endOffset": 157}, {"referenceID": 6, "context": "Parameter averaging (PA), also called \u201cmixture weights\u201d [9] or \u201cparallelized SGD\u201d [12], suggests solving an appropriate RLM problem on data in each node, and use average of the resultant parameters.", "startOffset": 56, "endOffset": 59}, {"referenceID": 6, "context": "Another interesting result described in [9] is a bound of O( 1 ML ) on the stability of the final hypothesis, which results in a bound on deviation from optimizer of generalization error.", "startOffset": 40, "endOffset": 43}, {"referenceID": 0, "context": "Another popular approach for distributed RLM is alternating direction method of multipliers (ADMM) [2, 6].", "startOffset": 99, "endOffset": 105}, {"referenceID": 3, "context": "Another popular approach for distributed RLM is alternating direction method of multipliers (ADMM) [2, 6].", "startOffset": 99, "endOffset": 105}, {"referenceID": 1, "context": "webspam [3].", "startOffset": 8, "endOffset": 11}, {"referenceID": 0, "context": "We propose an ADMM [2] based distributed algorithm (DWPA), and an accelerated version (DWPAacc), for learning the weights.", "startOffset": 19, "endOffset": 22}, {"referenceID": 0, "context": "This problem is solved in [2] using ADMM (see section 2.", "startOffset": 26, "endOffset": 29}, {"referenceID": 6, "context": "[9], in the context of conditional maximum entropy model.", "startOffset": 0, "endOffset": 3}, {"referenceID": 6, "context": "[9] suggest the approximate final parameter to be the arithmetic mean of the parameters learnt on individual partitions, (\u0175m).", "startOffset": 0, "endOffset": 3}, {"referenceID": 0, "context": "Note that assumptions regarding differentiability of loss function made in [2] can be relaxed in case of convex loss function with an appropriate definition of bregmann divergence using sub-gradients (see [11], section 2.", "startOffset": 75, "endOffset": 78}, {"referenceID": 7, "context": "Note that assumptions regarding differentiability of loss function made in [2] can be relaxed in case of convex loss function with an appropriate definition of bregmann divergence using sub-gradients (see [11], section 2.", "startOffset": 205, "endOffset": 209}, {"referenceID": 6, "context": "[9] proposed \u03b2 to be in a simplex.", "startOffset": 0, "endOffset": 3}, {"referenceID": 0, "context": "Using results from [2], the ADMM updates for solving the above problem can derived as: \u03b3 m := argmin \u03b3 (loss(Ai\u03b3) + (\u03c1/2)\u2016\u03b3ml \u2212 \u03b2 k + ukm\u2016 2 2) (8) \u03b2 := argmin \u03b2 (r(\u03b2) + (M\u03c1/2)\u2016\u03b2 \u2212 \u03b3 \u2212 uk\u201622) (9) u m = u k m + \u03b3 k+1 m \u2212 \u03b2 .", "startOffset": 19, "endOffset": 22}, {"referenceID": 0, "context": "A heuristic called overrelaxation [2] is ofter used for improving the convergence rate of ADMM.", "startOffset": 34, "endOffset": 37}, {"referenceID": 6, "context": "[9] on the stability of PA.", "startOffset": 0, "endOffset": 3}, {"referenceID": 6, "context": "This leads to a O( 1 \u221a ML ) bound on deviation from optimizer of generalization error (see [9], theorem 2).", "startOffset": 91, "endOffset": 94}, {"referenceID": 6, "context": "This result is analogous to theorem 1 of [9].", "startOffset": 41, "endOffset": 44}, {"referenceID": 6, "context": "3, with parameter averaging (PA) [9], Distributed SVM (DSVM) using ADMM, and accelerated DSVM (DSVMacc) [2].", "startOffset": 33, "endOffset": 36}, {"referenceID": 0, "context": "3, with parameter averaging (PA) [9], Distributed SVM (DSVM) using ADMM, and accelerated DSVM (DSVMacc) [2].", "startOffset": 104, "endOffset": 107}, {"referenceID": 2, "context": "We have used the liblinear library [5] to obtain the SVM parameters corresponding each partition.", "startOffset": 35, "endOffset": 38}, {"referenceID": 4, "context": "Optimization problems which arise as subproblems in ADMM has been solved using CVX [7], [8].", "startOffset": 83, "endOffset": 86}, {"referenceID": 5, "context": "Optimization problems which arise as subproblems in ADMM has been solved using CVX [7], [8].", "startOffset": 88, "endOffset": 91}, {"referenceID": 1, "context": "Real world datasets were obtained from LIBSVM website [3].", "startOffset": 54, "endOffset": 57}, {"referenceID": 6, "context": "An criticism against PA is the lack of bound on bias [9].", "startOffset": 53, "endOffset": 56}], "year": 2014, "abstractText": "Two popular approaches for distributed training of SVMs on big data are parameter averaging and ADMM. Parameter averaging is efficient but suffers from loss of accuracy with increase in number of partitions, while ADMM in the feature space is accurate but suffers from slow convergence. In this paper, we report a hybrid approach called weighted parameter averaging (WPA), which optimizes the regularized hinge loss with respect to weights on parameters. The problem is shown to be same as solving SVM in a projected space. We also demonstrate an O( 1 N ) stability bound on final hypothesis given by WPA, using novel proof techniques. Experimental results on a variety of toy and real world datasets show that our approach is significantly more accurate than parameter averaging for high number of partitions. It is also seen the proposed method enjoys much faster convergence compared to ADMM in features space.", "creator": "gnuplot 4.4 patchlevel 3"}}}