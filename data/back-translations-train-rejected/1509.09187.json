{"id": "1509.09187", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "30-Sep-2015", "title": "Deep Haar Scattering Networks", "abstract": "An orthogonal Haar scattering transform is a deep network, computed with a hierarchy of additions, subtractions and absolute values, over pairs of coefficients. It provides a simple mathematical model for unsupervised deep network learning. It implements non-linear contractions, which are optimized for classification, with an unsupervised pair matching algorithm, of polynomial complexity. A structured Haar scattering over graph data computes permutation invariant representations of groups of connected points in the graph. If the graph connectivity is unknown, unsupervised Haar pair learning can provide a consistent estimation of connected dyadic groups of points. Classification results are given on image data bases, defined on regular grids or graphs, with a connectivity which may be known or unknown.", "histories": [["v1", "Wed, 30 Sep 2015 14:20:29 GMT  (411kb,D)", "http://arxiv.org/abs/1509.09187v1", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["xiuyuan cheng", "xu chen", "stephane mallat"], "accepted": false, "id": "1509.09187"}, "pdf": {"name": "1509.09187.pdf", "metadata": {"source": "CRF", "title": "Deep Haar Scattering Networks", "authors": ["XIUYUAN CHENG", "XU CHEN"], "emails": ["xiuyuan.cheng@yale.edu", "xuchen@princeton.edu", "stephane.mallat@ens.fr"], "sections": [{"heading": null, "text": "An orthogonal hair scattering transformation is a deep network computed with a hierarchy of additions, subtractions, and absolute values by coefficient pairs. It provides a simple mathematical model for unattended, deep network learning. It implements nonlinear contractions optimized for classifying polynomial complexity using an unattended pair matching algorithm. Structured hair scattering over graph data computes permutation-invariant representations of groups of connected points in the graph. If graph connectivity is unknown, unattended hair pair learning can provide a consistent assessment of connected dyadic point groups. Classification results are given on image databases defined on regular grids or graphs, with connectivity that may be known or unknown."}, {"heading": "1 Introduction", "text": "This year, it has reached the stage where it will be able to take the lead."}, {"heading": "2 Free Orthogonal Haar Scattering", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "2.1 Orthogonal Haar filter contractions", "text": "We introduce orthogonal hair scattering by specializing in a general deep neural network. We explain the architectural constraints and the resulting contractive properties. The input network is a positive d-dimensional signal x (R +) d, which we write S0x = x. We designate the network layer at depth jx. [21] A deep neural network calculates the next network layer by applying a linear operator H jx to S jx, followed by a non-linear operator. Particularly deep network architectures require that H j maintain alpha distances up to a constant normalization factor [21]: H jy \u2212 H jy \u2212 H jy \u00b2 -H value. The network is contractive when it applies a pointless contraction to each value of the output vector H jx. This means that for each (a) plane an alpha factor is an alpha \u03bb (alpha)."}, {"heading": "2.2 Complete representation with bagging", "text": "A single hair scattering loses information as it uses orthogonal operators, followed by an absolute value that loses the information of the character. However, the following theory proves that x of 2J can transform different orthogonal hair scattering unless it is proven that a hair scattering is transformed. There are 2J different orthogonal hair scattering operators via pairs that can reverse these operators but cannot reconstruct their locations. However, the recombination of these values on sufficiently overlapping sets allows a restoration of their locations and hence the original signal x. This is applied to the following problem that is applied to interlaced pairings. We say that two different signal pairs are restored."}, {"heading": "2.3 Sparse unsupervised learning with adaptive contractions", "text": "We will now explain how to optimize these pairings, and the absolute value suppresses the sign of any difference, and thus the coefficients over a smaller domain. Thus, the optimization of the network amounts to finding the best instructions along which to perform the space compression. Contractions reduce the volume of space and thus the variance of the scattering vectors, but it can also collapse into different classes belonging to different classes. So, to maximize the \"average discriminability\" among the signal examples, we will maximize the variance of the scattering transformation via the training platform."}, {"heading": "2.4 Supervised feature selection and classification", "text": "The question we have to ask ourselves is whether we will be able to find a solution that paves the way to another class before we apply the SVM class. The question we have to ask ourselves is: What is the answer to the question, what is the answer to the question, what is the answer to the question, what is the answer to the question, what is the answer to the question, what is the answer to the question, what is the answer to the question, what is the answer to the question, what is the answer to the question, what is the answer to the question, what is the answer to the question, what is the answer to the question, what is the answer to the question, what is the answer to the question, what is the answer to the question, what is the answer to the question, what is the answer to the question?"}, {"heading": "3 Orthogonal Haar Scattering on Graphs", "text": "Many classification problems are translation invariant, which motivates the calculation of translation invariant representations. A translation invariant representation can be calculated by averaging signal samples, but too much information is removed. Wave scattering operators [18] are calculated by cascading wave transformations and absolute values. Each wave transformation calculates multi-scale signal variations in the network. It provides a large vector of coefficients, whose spatial averaging defines a rich set of translation invariant coefficients. Data vectors can be defined on non-uniform graphs [28], for example in social, financial, or transport networks. A graph shift shifts data samples to the network, but is not equivalent to a uniform grid translation. Orthogonal hair scattering transformations on graphs are calculated from local multi-scale signal variations on the graph. Calculation of displacement invariant on the final classification problem is based on the 3.the specific graphs."}, {"heading": "3.1 Structured orthogonal Haar scattering", "text": "The free orthogonal layer is S0x (n) = x (n). The free orthogonal layer is S0x (n) = x (n). The free orthogonal layer is S0x (n) = x (n). The free orthogonal layer is S0x (n). The free orthogonal layer is S0x (n). The free orthogonal layer is S0x (n). The free orthogonal layer is S0x (n). The free orthogonal layer is S0x (n). The free orthogonal layer is S0x (n). The free orthogonal layer is S0x (n). The free orthogonal layer is S0x (n). The free orthogonal layer is S0x (n)."}, {"heading": "3.2 Scattering order", "text": "A scatter coefficient of order m is a coefficient calculated by cascading absolute values of m. Its amplitude has a random decay as the order m increases, and its locations are specified by the following default. If q = 0 then S jx (n, q) is a coefficient of order 0. Otherwise, S jx (n, q) is a coefficient of order m \u2264 j if it is 0 \u2264 j1 < jm < j such Thatq = m scatq = 12 j \u2212 jk. (17) There are (jm) 2 \u2212 jd coefficients of order m in S jx.Proof. This prediction is proved by induction on j. For j = 0 all coefficients are order 0 since S0x (n, 0) = x (n). If S jx (n, q) is equal."}, {"heading": "3.3 Scattering with orthogonal Haar wavelet bases", "text": "= = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = ="}, {"heading": "3.4 Learning graph connectivity by variation minimization", "text": "We focus on the l1 norm minimization, which has a simpler expression. In many problems, the graphics are unknown. Learning a connected dyadic partition is easier than learning the complete connectivity of a graph, which is typically a complete problem. Section 2.3 introduces a polynomial complexity algorithm, which has a minimal overall variation. The consistency of these pairing algorithms is investigated via certain Gaussian stationary processes, and we show that there is no curse of dimensional algorithms. Section 2.3 introduces two criteria to optimize the pairing coefficients, which are free orthogonal hair scattering, from a training set {xi} i \u2264 N. We focus on the l1 norm minimization, which has a simpler expression."}, {"heading": "4 Numerical classification experiments", "text": "We look at the cases where the grid or graph geometry is a priori known or discovered through unattended learning. Although calculations are reduced to addition and subtraction, we show that hair scattering can produce results if the graph geometry is unknown even in complex image databases. In images using a known uniform sampling grid, we show that the simplifications of hair scattering cause an error that is about 20% greater than the state of the art. A hair scattering classification contains few parameters that are checked. The scattering scale 2J \u2264 d is the permutation invariance scale. Scatter coefficients are calculated to a maximum order of magnitude that is below a magnitude of 3.4%, which is indeed efficient."}, {"heading": "4.1 Classification of image digits in MNIST", "text": "There are 10 classes (one per digit) with 5 images for training and 104 for testing. Examples of MNIST images are shown in Figure 5. To test the classification performance of a hair scattering when the geometry is unknown, we encrypt all image pixels with the same random permutations as shown in Figure 5. To test the classification performance of a hair scattering, the best MNIST classification results without data augmentation are given in Table 6a. Deep Convolution Networks with supervised learning achieve an error of 0.53%, and unsupervised learning with sparse coding have a slightly larger error of 0.59%."}, {"heading": "4.2 CIFAR-10 images", "text": "CIFAR-10 is a database of tiny color images of 32 x 32 pixels = 29.5%. It includes 10 classes, such as \"dogs,\" \"cars,\" \"ships\" with a total of 5 x 104 training examples and 104 test examples. There are many more variabilities within the hair class than in MNIST digital images, such as Figure 8. The 3 color bands are represented with Y, U, V channels and scatter coefficients, which are calculated independently in each channel.If the image geometry is known, structured hair scattering is calculated by pairing adjacent image pixels. The best performance is achieved on the scale 2J = 26, which is below the maximum scale d = 210. Similar to MNIST, we calculate T = 64 contiguous dyadic partitions for randomly translated and rotated computer networks. After the size reduction, the classification error is 21.3%. This error exceeds the state of the art of unattended learning algorithms, but 16.5%."}, {"heading": "4.3 CIFAR-100 images", "text": "CIFAR-100 also contains tiny color images of the same size as CIFAR-10 images. It has 100 classes of 600 images each, 500 of which are training images and 100 of which are for testing. Our tests with CIFAR-100 follow the same procedures as in Section 4.2. The 3 color channels are processed independently of each other. If the geometry of the image grid is known, the results of structured hair scattering are summarized in Table 10. Best performance is achieved with the same combination of parameters as in CIFAR-10, the T = 64 and 2J = 26. After dimensional reduction, the classification error is 47.4%. As in CIFAR-10, this error is about 20% greater than the level of uncontrolled methods, e.g. a non-negative OMP (39.2%) [17]. A roto-translation-free wave scatter has an error of 43.7%. Deep conversion networks with supervised training result in a lower OMP (39.2%) with an unknown result of 7% 3J = 6%."}, {"heading": "4.4 Images on a graph over a sphere", "text": "It is constructed by projecting the MNIST image digits on d = 4096 points randomly scanned on the 3D sphere, and by randomly rotating these images on the sphere. Random rotation is either uniformly distributed on the sphere or limited by a smaller variance (small rotations). [4] The number \"9\" is removed from the data set because it cannot be distinguished from a \"6\" after rotation. Examples of spherical digits are shown in Figure 11. This geometry of points on the sphere can be described by a graph that connects points with a sufficiently small distance on the sphere. Classification algorithms introduced in [4] use the known distribution of points on the sphere, with a representation on the graph Laplacian. Table 3 gives the results reported in [4] with a fully connected neural network and with a speculation."}, {"heading": "Acknowledgment", "text": "This work was supported by the ERC grant InvariantClass 320959."}, {"heading": "A Proof of Theorem 3.1", "text": "The proof for theorem 3,1. We derive from the definition of a scattering transformation in equations (3,4) in the text that S j + 1x (n, 2q) = S jx \u2212 jx (2n), q) + S jx (2n + 1), q) = < S jx (\u00b7, q), 1V j + 1, n >, S j + 1x (n, 2q + 1) = | S jx (2n), q) \u2212 S jx (2n (2n), q) \u2212 S jx \u2212 n > |.wo Vj + 1, n = Vj, p (2n), A.m (2n), q) \u2212 S jx (2n + 1), ltS (2n + 1), ltS (2n), ltS (1n), ltS jm, jm, jm, jm, jm, jm, jm, jm, jm, jm, jm, jm, jm, jm, jm, jm, jm, jm, jm, jm, jm, jm, jm, jm, jm, jm, jm, jm, 2n, jm)."}, {"heading": "B Proof of Theorem 3.2", "text": "The theory is made by analyzing the concentration of objective functions by their expected value as the result of sample numbers N = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1: 1 = 1: 1 = 1: 1 = 1: 1 = 1 = 1 = 1 = 1: 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1: 1: 1: 1: 1: 1: 1: 1: 1: 1: 1: 1: 1: 1: 1: 1: 1: 1: 1: 1 = 1: 1: 1: 1: 1: 1 = 1: 1: 1: 1 = 1: 1: 1: 1: 1: 1: 1 = 1: 1: 1: 1: 1: 1: 1 = 1: 1: 1: 1: 1: 1: 1: 1: 1: 1: 1: 1: 1: 1: 1: 1: 1: 1: 1: 1: 1: 1: 1 = 1: 1: 1: 1: 1: 1: 1: 1: 1: 1: 1: 1: 1: 1: 1: 1: 1: 1: 1: 1: 1: 1: 1:"}], "references": [], "referenceMentions": [], "year": 2015, "abstractText": "<lb>An orthogonal Haar scattering transform is a deep network, computed with a hierarchy of additions, subtractions<lb>and absolute values, over pairs of coefficients. It provides a simple mathematical model for unsupervised deep network<lb>learning. It implements non-linear contractions, which are optimized for classification, with an unsupervised pair match-<lb>ing algorithm, of polynomial complexity. A structured Haar scattering over graph data computes permutation invariant<lb>representations of groups of connected points in the graph. If the graph connectivity is unknown, unsupervised Haar<lb>pair learning can provide a consistent estimation of connected dyadic groups of points. Classification results are given<lb>on image data bases, defined on regular grids or graphs, with a connectivity which may be known or unknown. deep<lb>learning, neural network, scattering transform, Haar wavelet, classification, images, graphs<lb>2000 Math Subject Classification: 68Q32, 68T45, 68Q25, 68T05", "creator": "LaTeX with hyperref package"}}}