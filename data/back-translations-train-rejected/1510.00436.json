{"id": "1510.00436", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "1-Oct-2015", "title": "Response to Liu, Xu, and Liang (2015) and Ferrer-i-Cancho and G\\'omez-Rodr\\'iguez (2015) on Dependency Length Minimization", "abstract": "We address recent criticisms (Liu et al., 2015; Ferrer-i-Cancho and G\\'omez-Rodr\\'iguez, 2015) of our work on empirical evidence of dependency length minimization across languages (Futrell et al., 2015). First, we acknowledge error in failing to acknowledge Liu (2008)'s previous work on corpora of 20 languages with similar aims. A correction will appear in PNAS. Nevertheless, we argue that our work provides novel, strong evidence for dependency length minimization as a universal quantitative property of languages, beyond this previous work, because it provides baselines which focus on word order preferences. Second, we argue that our choices of baselines were appropriate because they control for alternative theories.", "histories": [["v1", "Thu, 1 Oct 2015 22:09:34 GMT  (213kb,D)", "http://arxiv.org/abs/1510.00436v1", null]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["richard futrell", "kyle mahowald", "edward gibson"], "accepted": false, "id": "1510.00436"}, "pdf": {"name": "1510.00436.pdf", "metadata": {"source": "CRF", "title": "Response to Liu, Xu, and Liang (2015) and Ferrer-i-Cancho and Go\u0301mez-Rodr\u0301\u0131guez (2015) on Dependency Length Minimization", "authors": ["Richard Futrell", "Kyle Mahowald", "Edward Gibson"], "emails": [], "sections": [{"heading": null, "text": "More recently, we have looked at the question of whether the length of dependence - the distance between syntactically related words in sentences of natural language - is shorter than one would expect under random baselines (Futrell et al., 2015). This idea has linguistic relevance, because assuming a universal pressure to minimize dependence length can explain a variety of universal properties of languages, including many of the universals of word order cited by Greenberg (1963). Evidence that language users allow word orders with shorter dependence length than chance supports this hypothesis, known as dependency length minimization (DLM). The DLM hypothesis is theoretically attractive because it is motivated by general limitations on human information processing: minimizing dependency length minimizes the online storage load for reading and generalizing human sentences."}, {"heading": "1 Random Trees and Random Word Orders", "text": "This year, it has come to the point where it can only take a year to find a solution that is capable of finding a solution."}, {"heading": "2 Projective Baselines", "text": "The second big question raised by both Liu et al. (2015) and Ferrer-i-Cancho & Go \"Mez-Rodr\" Iguez (2015) is our choice of baselines for comparison. We use projective linearisations, which means that when a dependency tree is drawn over a linear sentence, none of the arcs of the tree cross are affected. We also have uselinearizations that include other factors that may have an impact on word order: a pressure on fixed word order and a pressure on consistency in head direction. These three factors - projection, head direction consistency and fixed word order - all have the effect of reducing the dependence length, and so it was argued that they should not be considered as separate factors, but the result of DLM. Ferrer-i-Cancho and Go \"MezRodr\" Iguez \"Iguez (2015) argue that we argue that our use of these baselines\" should be considered."}, {"heading": "3 Other Issues", "text": "Liu et al. (2015) also raise a number of more specific criticisms, claiming that the uniformity of the genres of text in our corpora could be a confusing factor. Criticism is justified: it is true that our corpora were primarily (but not completely) written texts from newspapers and novels. Nevertheless, we would be surprised if the DLM influenced novels and newspapers universally, but not language usage in general. We welcome any work controlling this possible issue. Finally, Liu et al. (2015) also point out that in our original paper we stated that head languages appear to have longer dependencies than more head-catching or copmedia languages, but we do not offer statistical tests of this claim. We intended this remark not as a main claim of the paper, but as a conjecture intended to draw attention to the large differences between languages in their length of dependence and possible typological implications of this variation."}], "references": [{"title": "Why do syntactic links not cross", "author": ["R. Ferrer-i-Cancho"], "venue": "Europhysics Letters,", "citeRegEx": "Ferrer.i.Cancho,? \\Q2006\\E", "shortCiteRegEx": "Ferrer.i.Cancho", "year": 2006}, {"title": "Liberating language research from dogmas of the 20th century", "author": ["R. Ferrer-i-Cancho", "C. G\u00f3mez-Rod\u0155\u0131guez"], "venue": null, "citeRegEx": "Ferrer.i.Cancho and G\u00f3mez.Rod\u0155\u0131guez,? \\Q2015\\E", "shortCiteRegEx": "Ferrer.i.Cancho and G\u00f3mez.Rod\u0155\u0131guez", "year": 2015}, {"title": "Large-scale evidence of dependency length minimization in 37 languages", "author": ["R. Futrell", "K. Mahowald", "E. Gibson"], "venue": "Proceedings of the National Academy of Sciences,", "citeRegEx": "Futrell et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Futrell et al\\.", "year": 2015}, {"title": "Do grammars minimize dependency length", "author": ["D. Gildea", "D. Temperley"], "venue": "Cognitive Science,", "citeRegEx": "Gildea and Temperley,? \\Q2010\\E", "shortCiteRegEx": "Gildea and Temperley", "year": 2010}, {"title": "Some universals of grammar with particular reference to the order of meaningful elements", "author": ["J. Greenberg"], "venue": "In J. Greenberg (ed.), Universals of Language,", "citeRegEx": "Greenberg,? \\Q1963\\E", "shortCiteRegEx": "Greenberg", "year": 1963}, {"title": "Syntactic difficulty in English and Japanese: A textual study", "author": ["S. Hiranuma"], "venue": "UCL Working Papers in Linguistics,", "citeRegEx": "Hiranuma,? \\Q1999\\E", "shortCiteRegEx": "Hiranuma", "year": 1999}, {"title": "Mildly non-projective dependency grammar", "author": ["M. Kuhlmann"], "venue": "Computational Linguistics,", "citeRegEx": "Kuhlmann,? \\Q2013\\E", "shortCiteRegEx": "Kuhlmann", "year": 2013}, {"title": "Dependency distance as a metric of language comprehension difficulty", "author": ["H. Liu"], "venue": "Journal of Cognitive Science,", "citeRegEx": "Liu,? \\Q2008\\E", "shortCiteRegEx": "Liu", "year": 2008}, {"title": "Dependency length minimization: Puzzles and Promises", "author": ["H. Liu", "C. Xu", "J. Liang"], "venue": "arXiv, 1509.04393", "citeRegEx": "Liu et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Liu et al\\.", "year": 2015}], "referenceMentions": [{"referenceID": 5, "context": "Response to Liu, Xu, and Liang (2015) and Ferrer-i-Cancho and G\u00f3mez-Rod\u0155\u0131guez (2015) on Dependency Length Minimization", "startOffset": 12, "endOffset": 38}, {"referenceID": 0, "context": "Response to Liu, Xu, and Liang (2015) and Ferrer-i-Cancho and G\u00f3mez-Rod\u0155\u0131guez (2015) on Dependency Length Minimization", "startOffset": 42, "endOffset": 85}, {"referenceID": 2, "context": "G\u00f3mez-Rod\u0155\u0131guez, 2015) of our work on empirical evidence of dependency length minimization across languages (Futrell et al., 2015).", "startOffset": 108, "endOffset": 130}, {"referenceID": 2, "context": "G\u00f3mez-Rod\u0155\u0131guez, 2015) of our work on empirical evidence of dependency length minimization across languages (Futrell et al., 2015). First, we acknowledge error in failing to acknowledge Liu (2008)\u2019s previous work on corpora of 20 languages with similar aims.", "startOffset": 109, "endOffset": 197}, {"referenceID": 2, "context": "In recent work, we addressed the question of whether dependency length\u2014 the distance between syntactically related words in natural language sentences\u2014 is shorter than one would expect under random baselines (Futrell et al., 2015).", "startOffset": 208, "endOffset": 230}, {"referenceID": 2, "context": "In recent work, we addressed the question of whether dependency length\u2014 the distance between syntactically related words in natural language sentences\u2014 is shorter than one would expect under random baselines (Futrell et al., 2015). This idea has linguistic relevance because if one hypothesizes a universal pressure to minimize dependency length, one can explain a variety of universal properties of languages, including many of the word-order universals noted by Greenberg (1963). Evidence that language users perfer word orders with shorter dependency length than chance supports this hypothesis, known as the dependency length minimization (DLM) hypothesis.", "startOffset": 209, "endOffset": 481}, {"referenceID": 8, "context": "Two recent articles have raised important criticisms of our work (Liu et al., 2015; Ferrer-i-Cancho & G\u00f3mez-Rod\u0155\u0131guez, 2015).", "startOffset": 65, "endOffset": 124}, {"referenceID": 6, "context": "First, Liu et al. (2015) note correctly that we failed to cite a previous largescale empirical study with similar aims.", "startOffset": 7, "endOffset": 25}, {"referenceID": 6, "context": "First, Liu et al. (2015) note correctly that we failed to cite a previous largescale empirical study with similar aims. In particular, Liu (2008) compares average dependency length in attested sentences of 20 languages to dependency length in random trees.", "startOffset": 7, "endOffset": 146}, {"referenceID": 6, "context": "First, Liu et al. (2015) note correctly that we failed to cite a previous largescale empirical study with similar aims. In particular, Liu (2008) compares average dependency length in attested sentences of 20 languages to dependency length in random trees. Not acknowledging this important prior work was an error on our part. The reason for this omission is that, in all honesty, we did not fully understand this paper and its relationship to ours until conversations with Liu and colleagues after publication. But these are not good reasons: we acknowledge that we should have made more of an effort to understand and acknowledge prior similar work. Consequently, we apologize and we urge anyone pursuing research relating to our paper to also study Liu (2008). This prior work will be acknowledged in a correction to the PNAS article.", "startOffset": 7, "endOffset": 763}, {"referenceID": 6, "context": "First, Liu et al. (2015) note correctly that we failed to cite a previous largescale empirical study with similar aims. In particular, Liu (2008) compares average dependency length in attested sentences of 20 languages to dependency length in random trees. Not acknowledging this important prior work was an error on our part. The reason for this omission is that, in all honesty, we did not fully understand this paper and its relationship to ours until conversations with Liu and colleagues after publication. But these are not good reasons: we acknowledge that we should have made more of an effort to understand and acknowledge prior similar work. Consequently, we apologize and we urge anyone pursuing research relating to our paper to also study Liu (2008). This prior work will be acknowledged in a correction to the PNAS article. Nevertheless, we believe the difference between the Liu (2008) baselines and ours is non-trivial, such that our work represents new large-scale evidence for the DLM hypothesis.", "startOffset": 7, "endOffset": 901}, {"referenceID": 6, "context": "First, Liu et al. (2015) note correctly that we failed to cite a previous largescale empirical study with similar aims. In particular, Liu (2008) compares average dependency length in attested sentences of 20 languages to dependency length in random trees. Not acknowledging this important prior work was an error on our part. The reason for this omission is that, in all honesty, we did not fully understand this paper and its relationship to ours until conversations with Liu and colleagues after publication. But these are not good reasons: we acknowledge that we should have made more of an effort to understand and acknowledge prior similar work. Consequently, we apologize and we urge anyone pursuing research relating to our paper to also study Liu (2008). This prior work will be acknowledged in a correction to the PNAS article. Nevertheless, we believe the difference between the Liu (2008) baselines and ours is non-trivial, such that our work represents new large-scale evidence for the DLM hypothesis. Liu (2008) uses a \u201crandom tree\u201d baseline, comparing dependency length in attested dependency trees to dependency length in random ordered trees with the same numbers of nodes.", "startOffset": 7, "endOffset": 1026}, {"referenceID": 6, "context": "First, Liu et al. (2015) note correctly that we failed to cite a previous largescale empirical study with similar aims. In particular, Liu (2008) compares average dependency length in attested sentences of 20 languages to dependency length in random trees. Not acknowledging this important prior work was an error on our part. The reason for this omission is that, in all honesty, we did not fully understand this paper and its relationship to ours until conversations with Liu and colleagues after publication. But these are not good reasons: we acknowledge that we should have made more of an effort to understand and acknowledge prior similar work. Consequently, we apologize and we urge anyone pursuing research relating to our paper to also study Liu (2008). This prior work will be acknowledged in a correction to the PNAS article. Nevertheless, we believe the difference between the Liu (2008) baselines and ours is non-trivial, such that our work represents new large-scale evidence for the DLM hypothesis. Liu (2008) uses a \u201crandom tree\u201d baseline, comparing dependency length in attested dependency trees to dependency length in random ordered trees with the same numbers of nodes. For example, the dependency length of a sentence with a tree such as in Figure 1 is compared to the dependency length induced by random ordered trees as in Figure 2. The baseline trees do not share any syntactic structure with the attested trees they are compared to, beyond their length. In contrast, Gildea & Temperley (2010) and Futrell et al.", "startOffset": 7, "endOffset": 1519}, {"referenceID": 2, "context": "In contrast, Gildea & Temperley (2010) and Futrell et al. (2015) use \u201crandom word order\u201d baselines, keeping the syntactic dependency structure of attested sentences constant and investigating random word orders given that syntactic structure, subject to a number of linguistic constraints.", "startOffset": 43, "endOffset": 65}, {"referenceID": 7, "context": "Figure 2: Some random trees based on the sentence in Figure 1 according to the Liu (2008) random tree baseline.", "startOffset": 79, "endOffset": 90}, {"referenceID": 2, "context": "Figure 3: A random permutation of the sentence in Figure 1 according to a random word order baseline, specifically the head-fixed projective baseline in Futrell et al. (2015). This baseline permutes sister nodes while maintaining head direction.", "startOffset": 153, "endOffset": 175}, {"referenceID": 6, "context": "So we see this work as a complement of Liu (2008) and related work, strengthening the body of evidence for the DLM hypothesis, rather than a repetition.", "startOffset": 39, "endOffset": 50}, {"referenceID": 5, "context": "For example, we find relatively long dependency lengths for head-final languages such as Japanese and Turkish, whereas Hiranuma (1999) finds that dependency length in Japanese is highly optimized.", "startOffset": 119, "endOffset": 135}, {"referenceID": 5, "context": "For example, we find relatively long dependency lengths for head-final languages such as Japanese and Turkish, whereas Hiranuma (1999) finds that dependency length in Japanese is highly optimized. Hiranuma (1999)\u2019s finding is specifically that Japanese speakers drop verbal arguments to achieve dependency length minimization, trusting that the language comprehender will be able to infer the missing arguments from discourse context.", "startOffset": 119, "endOffset": 213}, {"referenceID": 6, "context": "The second major issue raised in both Liu et al. (2015) and Ferrer-i-Cancho & G\u00f3mez-Rod\u0155\u0131guez (2015) is our choice of baselines for comparison.", "startOffset": 38, "endOffset": 56}, {"referenceID": 0, "context": "(2015) and Ferrer-i-Cancho & G\u00f3mez-Rod\u0155\u0131guez (2015) is our choice of baselines for comparison.", "startOffset": 11, "endOffset": 52}, {"referenceID": 0, "context": "Ferrer-i-Cancho and G\u00f3mezRod\u0155\u0131guez (2015) argue that our use of these baselines is redundant for this reason.", "startOffset": 0, "endOffset": 42}, {"referenceID": 0, "context": "Ferrer-i-Cancho and G\u00f3mezRod\u0155\u0131guez (2015) argue that our use of these baselines is redundant for this reason. We believe comparison to these baselines provides stronger evidence for DLM than comparison only to a fully nonprojective baseline, because it shows that the phenomenon of short dependencies must be explained even if independent factors affecting word order are assumed. Since DLM can explain the phenomena attributed to these other factors, the most parsimonious theory seems to be that DLM is the only factor influencing word order. But we can only make this argument after showing that the shortness of dependencies persists as a phenomenon even after controlling for these other hypothetical factors. For example, suppose we had found that attested dependency length was not shorter than the projective random baselines. One would be left with the question of why, if DLM is the main factor influencing language structure, German speakers pass up opportunities to minimize dependency length. Then one could argue that DLM is not a good explanation for projectivity, since word orders are not minimized for dependency length beyond what is needed to establish projectivity, which itself might have independent motivations (such as enabling polynomial-time parsing). Since we found that dependency length is shorter than this baseline in many languages, this line of argumentation is no longer available. For the sake of completeness, we provide a comparison of attested dependency lengths with dependency lengths in random nonprojective linearizations in Figure 4. For this baseline, the dependency tree is linearized by shuffling nodes at random. The baselines from Futrell et al. (2015) are also shown.", "startOffset": 0, "endOffset": 1702}, {"referenceID": 0, "context": "Ferrer-i-Cancho and G\u00f3mezRod\u0155\u0131guez (2015) argue that our use of these baselines is redundant for this reason. We believe comparison to these baselines provides stronger evidence for DLM than comparison only to a fully nonprojective baseline, because it shows that the phenomenon of short dependencies must be explained even if independent factors affecting word order are assumed. Since DLM can explain the phenomena attributed to these other factors, the most parsimonious theory seems to be that DLM is the only factor influencing word order. But we can only make this argument after showing that the shortness of dependencies persists as a phenomenon even after controlling for these other hypothetical factors. For example, suppose we had found that attested dependency length was not shorter than the projective random baselines. One would be left with the question of why, if DLM is the main factor influencing language structure, German speakers pass up opportunities to minimize dependency length. Then one could argue that DLM is not a good explanation for projectivity, since word orders are not minimized for dependency length beyond what is needed to establish projectivity, which itself might have independent motivations (such as enabling polynomial-time parsing). Since we found that dependency length is shorter than this baseline in many languages, this line of argumentation is no longer available. For the sake of completeness, we provide a comparison of attested dependency lengths with dependency lengths in random nonprojective linearizations in Figure 4. For this baseline, the dependency tree is linearized by shuffling nodes at random. The baselines from Futrell et al. (2015) are also shown. The figure shows that dependency length is much shorter than the nonprojective baseline, and that the projective baselines are much more conservative than the nonprojective baseline. We felt that including the nonprojective baselines in the original paper would be redundant, since Ferreri-Cancho (2006) showed that projective trees on average have shorter de-", "startOffset": 0, "endOffset": 2022}, {"referenceID": 5, "context": "pendency length than nonprojective trees, and Kuhlmann (2013) (among others) showed that natural language dependency trees are overwhelmingly projective.", "startOffset": 46, "endOffset": 62}, {"referenceID": 0, "context": "We also want to stress that, contra Ferrer-i-Cancho & G\u00f3mez-Rod\u0155\u0131guez (2015), controlling for these possible alternative factors affecting word order does not imply that we are accepting traditional nativist or Universal Grammar-based hypotheses.", "startOffset": 36, "endOffset": 77}], "year": 2015, "abstractText": "We address recent criticisms (Liu et al., 2015; Ferrer-i-Cancho and G\u00f3mez-Rod\u0155\u0131guez, 2015) of our work on empirical evidence of dependency length minimization across languages (Futrell et al., 2015). First, we acknowledge error in failing to acknowledge Liu (2008)\u2019s previous work on corpora of 20 languages with similar aims. A correction will appear in PNAS. Nevertheless, we argue that our work provides novel, strong evidence for dependency length minimization as a universal quantitative property of languages, beyond this previous work, because it provides baselines which focus on word order preferences. Second, we argue that our choices of baselines were appropriate because they control for alternative theories. In recent work, we addressed the question of whether dependency length\u2014 the distance between syntactically related words in natural language sentences\u2014 is shorter than one would expect under random baselines (Futrell et al., 2015). This idea has linguistic relevance because if one hypothesizes a universal pressure to minimize dependency length, one can explain a variety of universal properties of languages, including many of the word-order universals noted by Greenberg (1963). Evidence that language users perfer word orders with shorter dependency length than chance supports this hypothesis, known as the dependency length minimization (DLM) hypothesis. The DLM hypothesis is theoretically attractive because it is motivated by general human information processing constraints: minimizing dependency length minimizes the online memory load for human sentence parsing and generation. 1 ar X iv :1 51 0. 00 43 6v 1 [ cs .C L ] 1 O ct 2 01 5 Two recent articles have raised important criticisms of our work (Liu et al., 2015; Ferrer-i-Cancho & G\u00f3mez-Rod\u0155\u0131guez, 2015). 1 Random Trees and Random Word Orders First, Liu et al. (2015) note correctly that we failed to cite a previous largescale empirical study with similar aims. In particular, Liu (2008) compares average dependency length in attested sentences of 20 languages to dependency length in random trees. Not acknowledging this important prior work was an error on our part. The reason for this omission is that, in all honesty, we did not fully understand this paper and its relationship to ours until conversations with Liu and colleagues after publication. But these are not good reasons: we acknowledge that we should have made more of an effort to understand and acknowledge prior similar work. Consequently, we apologize and we urge anyone pursuing research relating to our paper to also study Liu (2008). This prior work will be acknowledged in a correction to the PNAS article. Nevertheless, we believe the difference between the Liu (2008) baselines and ours is non-trivial, such that our work represents new large-scale evidence for the DLM hypothesis. Liu (2008) uses a \u201crandom tree\u201d baseline, comparing dependency length in attested dependency trees to dependency length in random ordered trees with the same numbers of nodes. For example, the dependency length of a sentence with a tree such as in Figure 1 is compared to the dependency length induced by random ordered trees as in Figure 2. The baseline trees do not share any syntactic structure with the attested trees they are compared to, beyond their length. In contrast, Gildea & Temperley (2010) and Futrell et al. (2015) use \u201crandom word order\u201d baselines, keeping the syntactic dependency structure of attested sentences constant and investigating random word orders given that syntactic structure, subject to a number of linguistic constraints. For example, dependency length for a sentence such as in Figure 1 is compared to dependency length in a sentence with different word order but the same (unordered) dependency tree structure, as in Figure 3. Attested dependency length is shorter than both the random tree and random word order baselines. Our finding that attested dependency length is shorter than random word order baselines shows that, given a syntactic structure, language users and language grammars tend to prefer the word order that minimizes dependency", "creator": "LaTeX with hyperref package"}}}