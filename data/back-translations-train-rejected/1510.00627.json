{"id": "1510.00627", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "2-Oct-2015", "title": "Multi-armed Bandits with Application to 5G Small Cells", "abstract": "Due to the pervasive demand for mobile services, next generation wireless networks are expected to be able to deliver high date rates while wireless resources become more and more scarce. This requires the next generation wireless networks to move towards new networking paradigms that are able to efficiently support resource-demanding applications such as personalized mobile services. Examples of such paradigms foreseen for the emerging fifth generation (5G) cellular networks include very densely deployed small cells and device-to-device communications. For 5G networks, it will be imperative to search for spectrum and energy-efficient solutions to the resource allocation problems that i) are amenable to distributed implementation, ii) are capable of dealing with uncertainty and lack of information, and iii) can cope with users' selfishness. The core objective of this article is to investigate and to establish the potential of multi-armed bandit (MAB) framework to address this challenge. In particular, we provide a brief tutorial on bandit problems, including different variations and solution approaches. Furthermore, we discuss recent applications as well as future research directions. In addition, we provide a detailed example of using an MAB model for energy-efficient small cell planning in 5G networks.", "histories": [["v1", "Fri, 2 Oct 2015 15:49:59 GMT  (712kb,D)", "http://arxiv.org/abs/1510.00627v1", "10 pages"]], "COMMENTS": "10 pages", "reviews": [], "SUBJECTS": "cs.LG cs.DC cs.NI", "authors": ["setareh maghsudi", "ekram hossain"], "accepted": false, "id": "1510.00627"}, "pdf": {"name": "1510.00627.pdf", "metadata": {"source": "CRF", "title": "Multi-armed Bandits with Application to 5G Small Cells", "authors": ["Setareh Maghsudi"], "emails": ["sudi.setareh@gmail.com,", "ekram.hossain@umanitoba.ca)."], "sections": [{"heading": null, "text": "This year, it has come to the point that it has never come as far as it has this year."}, {"heading": "II. SINGLE-AGENT MULTI-ARMED BANDIT", "text": "A single agent, multi-armed bandit (SA-MAB) problem was first introduced in [1]. In the most basic context, the problem models an agent who faces the challenge of sequentially selecting an arm from a range of weapons to obtain an a priori unknown reward drawn from an unknown reward generation process. Due to a lack of prior information, in each process the player can choose an inferior arm in terms of reward, which causes a certain regret, quantified by the difference between the reward that would have been achieved and the reward actually achieved. In such an unknown context, the player decides which arm to draw in a sequence of studies so as to minimize his accumulated regret over the game horizon. This problem is an example of an exploration-exploitation dilemma, i.e. the compromise between measures that produce immediate large rewards that could be fixed on the hand, and measures that could be taken in the future, for example, larger actions that lead to larger actions, and actions that could lead to, for example, a single-armed bandit (SA-MAB) problem first introduced in [1]."}, {"heading": "A. Stateful (Markovian) Bandits", "text": "In a stately bandit model, each arm is associated with a certain limited state space. After the draw, each arm pays a certain positive reward, which is derived from some stationary distribution related to the current state of that arm. Arms states change over time according to some stochastic model, which is, however, considered the Markov process; that is, it fulfills the Markov property, roughly speaking, a process satisfies the Markov property if its future depends exclusively on its current state and not on its complete history. This type of MAB is also called the Markov bandit; that is, in this formulation, after each round the reward as well as the state only of the played arm is revealed, and the mechanism of state transition is unknown. Thus, a Markovian MAB model can be formally defined as B: = {M, Sm, \u03c0m, s, \u00b5m} where m, M, M, M, M, M, M, M, the set of weapons is M \u2022."}, {"heading": "B. Stateless Bandits", "text": "The reward we accept, however, is not necessarily stationary. Therefore, a stateless MAB model is formally defined as B: = {M, \u00b5m, t}, where we describe these categories, but we offer a term of regret that is widely used to analyze almost every stateless bandit problem, regardless of the type of reward process to which it belongs. Let's call gm, t and denote it, or the instantaneous reward of arm, both at the time and at the time. Then, the regret of any policy for stateless bandits is up to a time called an external regret defined by RExt."}, {"heading": "C. Other Important Bandit Models", "text": "1) Contextual (covariate) bandits: In the basic bandit model, the agent only considers the reward of the actions played in each turn and therefore has to choose his future actions based only on his past performance. In contextual bandits (also called bandits with secondary information or covariate bandits), on the other hand, some additional information is revealed to the learner in each decision round. Therefore, the agent learns the best mapping of contexts to weapons. Note that this type of bandit model is distinguished only based on the availability of information, and the reward process can still be Markovian, stochastical or counterproductive. In any case, the aforementioned algorithms are adapted to solve contextual bandit problems as well. 2) Mortal bandits: While the basic bandit model assumes that all weapons are infinitely available, there are also some variants that differentiate the problems where this assumption does not hold."}, {"heading": "III. MULTI-AGENT (GAME-THEORETICAL) MULTI-ARMED BANDITS", "text": "For multi-agent, multi-armed bandits (MA-MAB), each player who selects an arm is assigned an action set Mk M. Similar to the one-agent model, each actor selects an action in successive attempts to obtain an initially unknown reward; if several actors select an arm, the reward achieved is divided arbitrarily. From the point of view of each actor, the reward depends on the common action profile of all actors. Note that the first actor is himself an actor and the reward achieved by each actor can be considered either private or public information, based on the specific system model and problem formulation. From the point of view of each actor k, a MAMAB model can be seen as a game with two actors: the first actor is himself an actor and the second actor is the set of all other K-1 actors whose common action profile affects the reward achieved by the actor."}, {"heading": "A. Convergence to Correlated Equilibrium", "text": "Remember the definition of internal regret by (3) in section (II-B2). The following (simplified) theorem describes the relationship between internal regret and the concept of correlated balance in games.1Theorem 1 ([9]) Consider a multi-agent bandit game with a number of players K, and let's leave player K's internal regret in turn n. If all players play according to a policy that disappears per turn Internal regret (i.e., limn \u2192 nR (k) Int, n = 0, then the game converts to turn n with the set of correlated equilibrium."}, {"heading": "B. Convergence to Nash Equilibrium", "text": "While they are few approaches to solving multi-agent bandit games that guarantee convergence to correlated general strategies1 The definition of correlated and Nash balance is standard and is therefore excluded here for the brevity of the article. Balances converging to Nash balance seem to be an even more difficult task. There are some convergent approaches for specific game classes such as potential games. For example, [12] algorithms for Nash convergence are proposed in potential games and games with more general forms of activity, the basic idea being to combine Q learning with stochastic better response or stochastic adaptive game dynamics to develop a multi-agent version of Q-learning that appreciates the reward functions using novel forms of - greedy learning policy.Details can be found in [12], while Multi-Agent approximates to some general game classes, only some Q-specific algorithms exist."}, {"heading": "IV. STATE-OF-THE-ART AND FUTURE RESEARCH DIRECTIONS", "text": "In the following, we will briefly consider the current applications of MABs (both in the single and multi-agent environment), and then discuss some outstanding issues and possible research directions. Note that our focus in this article is on the application of MAB models to address resource allocation problems."}, {"heading": "A. State-of-the-Art", "text": "In fact, it is as if most of them are able to survive themselves by blaming themselves and others. (...) In fact, it is as if they are able to survive themselves. (...) In fact, it is as if they are able to survive themselves. (...) It is as if they are able to survive themselves. (...) It is not as if they are able to survive themselves. (...) It is as if they were able to survive themselves. (...) It is as if they are able to survive themselves. (...) It is as if they are able to survive themselves. (...) It is as if they are able to survive themselves. (...) It is as if they are able to survive themselves. (...) It is as if they are able to survive themselves. (...) It is as if they are able to survive themselves."}, {"heading": "B. Future Research Directions", "text": "This year it has come to the point that it has never come as far as this year."}, {"heading": "V. EFFICIENT 5G SMALL CELLS WITH COMBINATORIAL (MULTI-PLAY) BANDITS", "text": "In Section IV, we briefly examined state-of-the-art base stations that (preferably) use the radio frequency spectrum as a basis for cell traffic to solve a variety of problems that arise in wireless networks. In addition, we discussed some issues and open issues. In this section, we present a new application of MABs in next-generation wireless networks to design energy-efficient 5G small cells. Primarily, we consider a single-agent model with independent cells as the user. However, unlike the state of the art, several arms are selected in each round to generalize the model to a multi-agent scenario, and also in the case where arms are interdependent, i.e. they interact with each other in an ever-increasing amount of data traffic, 5G networks are predictable to alleviate this problem by using small cells to underpin the existing macorcellular networks."}, {"heading": "VI. SUMMARY AND CONCLUSION", "text": "MAB is a class of sequential decision-making issues with strictly limited prior information and feedback. By providing an overview of MABs, we have argued that a wide range of wireless connectivity issues, including resource management, security, routing, scheduling, and power generation, can be formulated and solved as a bandit problem. We have also reviewed the state of the art and applications of MABs, with a focus on wireless resource allocation. There are a number of open research directions that need to be explored and outlined briefly. Finally, we have provided a detailed example of an application of MAB in energy-efficient 5G small cells, where the problem of optimal small cell planning is considered a multiplay (combination) bandit problem. Preliminary performance evaluation results have confirmed the effectiveness of the proposed model and approach."}], "references": [{"title": "Some aspects of the sequential design of experiments", "author": ["H. Robbins"], "venue": "Bulletin of the American Mathematical Society, vol. 58, no. 5, pp. 527\u2013 535, 1952.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 1952}, {"title": "Learning in a changing world: Restless multiarmed bandit with unknown dynamics", "author": ["H. Liu", "K. Liu", "Q. Zhao"], "venue": "IEEE Transactions on Information Theory, vol. 59, no. 3, pp. 1902\u20131916, March 2013.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 1902}, {"title": "Multi-Armed Bandit Allocation Indices, Wiley, 2 edition", "author": ["J. Gittins", "K. Glazebrook", "R. Weber"], "venue": null, "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2011}, {"title": "Approximation algorithms for restless bandit problems", "author": ["S. Guha", "K. Munagala", "P. Shi"], "venue": "Journal of ACM, vol. 58, no. 1, pp. 3, 2010.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2010}, {"title": "Restless bandits, linear programming relaxations, and a primal-dual index heuristic", "author": ["D. Bertsimas", "J. Nino-Mora"], "venue": "Operations Research, vol. 48, no. 1, pp. 2000, 2000.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2000}, {"title": "Regret analysis of stochastic and nonstochastic multi-armed bandit problems", "author": ["S. Bubeck", "N. Cesa-Bianchi"], "venue": "Foundations and Trends in Machine Learning, vol. 5, no. 1, pp. 1\u2013122, 2012.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2012}, {"title": "Mortal multiarmed bandits", "author": ["D. Chakrabarti", "R. Kumar", "F. Radlinski", "E. Upfal"], "venue": "Neural Information Processing Systems Conference, 2008, pp. 273\u2013280.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2008}, {"title": "Regret bounds for sleeping experts and bandits", "author": ["R.D. Kleinberg", "A. Niculescu-mizil", "Y. Sharma"], "venue": "Conference of Learning Theory, 2008, pp. 425\u2013436.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2008}, {"title": "Prediction, Learning, and Games", "author": ["N. Cesa-Bianchi", "G. Lugosi"], "venue": null, "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2006}, {"title": "Joint channel selection and power control in infrastructureless wireless networks: A multi-player multiarmed bandit framework", "author": ["S. Maghsudi", "S. Stanczak"], "venue": "IEEE Transactions on Vehicular Technology, vol. PP, 2014.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2014}, {"title": "Channel selection for network-assisted D2D communication via no-regret bandit learning with calibrated forecasting", "author": ["S. Maghsudi", "S. Stanczak"], "venue": "IEEE Transactions on Wireless Communications, vol. 14, no. 3, pp. 1309\u20131322, March 2015.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2015}, {"title": "Convergent learning algorithms for unknown reward games", "author": ["A.C. Chapman", "D.S. Leslie", "A. Rogers", "N.R. Jennings"], "venue": "SIAM Journal on Control and Optimization, vol. 51, no. 4, pp. 3154\u20133180, 2013.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2013}, {"title": "Transmission scheduling for sensor network lifetime maximization: A shortest path bandit formulation", "author": ["Y. Chen", "Q. Zhao", "V. Krishnamurthy", "D. Djonin"], "venue": "IEEE International Conference on Acoustics, Speech and Signal Processing, May 2006, vol. 4, pp. IV\u2013IV.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2006}, {"title": "Transmission mode selection for network-assisted device to device communication: A levy-bandit approach", "author": ["S. Maghsudi", "S. Stanczak"], "venue": "IEEE International Conference on Acoustics, Speech and Signal Processing, May 2014, pp. 7009\u20137013.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2014}, {"title": "Multi-access communications with energy harvesting: A multi-armed bandit model and the optimality of the myopic policy", "author": ["P. Blasco", "D. Gunduz"], "venue": "IEEE Journal on Selected Areas in Communications, vol. 33, no. 3, pp. 585\u2013597, March 2015.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2015}, {"title": "Online linear optimization and adaptive routing", "author": ["R. Kleinberg B. Awerbuch"], "venue": "Elsevier Journal of Computer and System Sciences, vol. 74, pp. 97\u2013104, 2008.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2008}, {"title": "Towards optimal adaptive UFH-based anti-jamming wireless communication", "author": ["Q. Wang", "P. Xu", "K. Ren", "X.-Y. Li"], "venue": "IEEE Journal on Selected Areas in Communications, vol. 30, no. 1, pp. 16\u201330, January 2012.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2012}, {"title": "Evolution toward 5G multi-tier cellular wireless networks: An interference management perspective", "author": ["E. Hossain", "M. Rasti", "H. Tabassum", "A. Abdelnasser"], "venue": "IEEE Wireless Communications, vol. 21, no. 3, pp. 118\u2013127, June 2014.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2014}, {"title": "Algorithms for adversarial bandit problems with multiple plays", "author": ["T. Uchiya", "A. Nakamura", "M. Kudo"], "venue": "Algorithmic Learning Theory, Oct 2010, pp. 375\u2013389.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2010}], "referenceMentions": [{"referenceID": 0, "context": "Single-agent multi-armed bandit (SA-MAB) problem was first introduced in [1].", "startOffset": 73, "endOffset": 76}, {"referenceID": 1, "context": "An example of an indexing policy can be found in [2].", "startOffset": 49, "endOffset": 52}, {"referenceID": 2, "context": "Such problems belong to bandit models, and are solved by using indexing policies as well, with an example being Gittins index [3].", "startOffset": 126, "endOffset": 129}, {"referenceID": 2, "context": "In addition to the Gittins indices, Whittle\u2019s indexing policy [3] is well-known and often used", "startOffset": 62, "endOffset": 65}, {"referenceID": 3, "context": "Important approximation algorithms for some families of restless bandit problems can be found in [4] and [5].", "startOffset": 97, "endOffset": 100}, {"referenceID": 4, "context": "Important approximation algorithms for some families of restless bandit problems can be found in [4] and [5].", "startOffset": 105, "endOffset": 108}, {"referenceID": 5, "context": "In order to solve the stochastic bandit problem, many methods have been developed so far that are based on the upper confidence bound (UCB) policy [6].", "startOffset": 147, "endOffset": 150}, {"referenceID": 5, "context": "Adversarial bandits are often solved by using potentialbased or weighted average algorithms [6].", "startOffset": 92, "endOffset": 95}, {"referenceID": 6, "context": "An example of solution approaches can be found in [7].", "startOffset": 50, "endOffset": 53}, {"referenceID": 7, "context": "Some solution approaches for different types of this problem can be found in [8].", "startOffset": 77, "endOffset": 80}, {"referenceID": 8, "context": "Theorem 1 ( [9]).", "startOffset": 12, "endOffset": 15}, {"referenceID": 9, "context": "This concept is used for instance in [10] to develop joint power control and channel selection strategies in distributed D2D networks.", "startOffset": 37, "endOffset": 41}, {"referenceID": 8, "context": "Theorem 2 ( [9]).", "startOffset": 12, "endOffset": 15}, {"referenceID": 10, "context": "The theorem is generalized in [11] to bandit games, and is applied as a basis to develop a convergent solution approach for non-stationary stochastic bandit models.", "startOffset": 30, "endOffset": 34}, {"referenceID": 11, "context": "For instance, in [12], algorithms are proposed for Nash convergence in potential games and games with more general forms of acyclicity.", "startOffset": 17, "endOffset": 21}, {"referenceID": 11, "context": "Details can be found in [12].", "startOffset": 24, "endOffset": 28}, {"referenceID": 8, "context": "A concept based on which multiple convergent bandit algorithms are proposed for general games is regret testing [9].", "startOffset": 112, "endOffset": 115}, {"referenceID": 10, "context": "See [11] for an example.", "startOffset": 4, "endOffset": 8}, {"referenceID": 1, "context": "An example can be found in [2].", "startOffset": 27, "endOffset": 30}, {"referenceID": 12, "context": "Reference [13] is an example.", "startOffset": 10, "endOffset": 14}, {"referenceID": 13, "context": "This problem can be simply cast and solved by a two-armed bandit model, as proposed in [14].", "startOffset": 87, "endOffset": 91}, {"referenceID": 9, "context": "An example can be found in [10].", "startOffset": 27, "endOffset": 31}, {"referenceID": 14, "context": "See [15] as an example.", "startOffset": 4, "endOffset": 8}, {"referenceID": 15, "context": "See [16] for an example.", "startOffset": 4, "endOffset": 8}, {"referenceID": 16, "context": "An example can be found in [17].", "startOffset": 27, "endOffset": 31}, {"referenceID": 17, "context": "This takes advantage from low-power and short-range base stations that operate (preferably) using the same radio spectrum as the macro base stations and offload macro cell traffic [18].", "startOffset": 180, "endOffset": 184}, {"referenceID": 18, "context": "M, can be found in [19].", "startOffset": 19, "endOffset": 23}, {"referenceID": 18, "context": "M [19], the complexity is of O(M(logN + 1)) and O(M), in time and space, respectively.", "startOffset": 2, "endOffset": 6}, {"referenceID": 18, "context": "M [19] with number of existing (M ) and activated (N ) small cells.", "startOffset": 2, "endOffset": 6}], "year": 2015, "abstractText": "Due to the pervasive demand for mobile services, next generation wireless networks are expected to be able to deliver high date rates while wireless resources become more and more scarce. This requires the next generation wireless networks to move towards new networking paradigms that are able to efficiently support resource-demanding applications such as personalized mobile services. Examples of such paradigms foreseen for the emerging fifth generation (5G) cellular networks include very densely deployed small cells and device-to-device communications. For 5G networks, it will be imperative to search for spectrum and energy-efficient solutions to the resource allocation problems that i) are amenable to distributed implementation, ii) are capable of dealing with uncertainty and lack of information, and iii) can cope with users\u2019 selfishness. The core objective of this article is to investigate and to establish the potential of multi-armed bandit (MAB) framework to address this challenge. In particular, we provide a brief tutorial on bandit problems, including different variations and solution approaches. Furthermore, we discuss recent applications as well as future research directions. In addition, we provide a detailed example of using an MAB model for energy-efficient small cell planning in 5G networks.", "creator": "LaTeX with hyperref package"}}}