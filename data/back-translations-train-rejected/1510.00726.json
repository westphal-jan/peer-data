{"id": "1510.00726", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "2-Oct-2015", "title": "A Primer on Neural Network Models for Natural Language Processing", "abstract": "Over the past few years, neural networks have re-emerged as powerful machine-learning models, yielding state-of-the-art results in fields such as image recognition and speech processing. More recently, neural network models started to be applied also to textual natural language signals, again with very promising results. This tutorial surveys neural network models from the perspective of natural language processing research, in an attempt to bring natural-language researchers up to speed with the neural techniques. The tutorial covers input encoding for natural language tasks, feed-forward networks, convolutional networks, recurrent networks and recursive networks, as well as the computation graph abstraction for automatic gradient computation.", "histories": [["v1", "Fri, 2 Oct 2015 20:17:33 GMT  (586kb,D)", "http://arxiv.org/abs/1510.00726v1", null]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["yoav goldberg"], "accepted": false, "id": "1510.00726"}, "pdf": {"name": "1510.00726.pdf", "metadata": {"source": "CRF", "title": "A Primer on Neural Network Models for Natural Language Processing", "authors": ["Yoav Goldberg"], "emails": ["first.last@gmail"], "sections": [{"heading": "1. Introduction", "text": "In fact, it is the case that most of us are able to abide by the rules that they have imposed on themselves. (...) In fact, it is the case that they are able to surpass themselves. \"(...)\" It is as if they surpass themselves. \"(...)\" It is as if they surpass themselves. \"(...)\" It is as if they surpass themselves. \"(...)\" It is as if they surpass themselves. \"(...)\" (...) \"(...)\" (...) \"(...) (...)\" (... \"(...), (...) (...), (...), (...), (...), (...), (...), (...), (...), (...), (...), (...), (...), (...), (...) (...), (...), (...), (...), (...), (...), (...), (...)."}, {"heading": "2. Neural Network Architectures", "text": "We will discuss two types of neural network architectures that can be interwoven and tuned to capture and analyze the results of 2015. All networks act as systematists, but each one has different strengths. They are non-linear learners that can largely be used as substitutes wherever a linear learner is used, including binary and multicultural classification problems, as well as more complex structured prediction problems (Section 4). The non-linearity of the network and the ability to easily integrate pre-trained word beds often lead to superior classifications."}, {"heading": "3. Feature Representation", "text": "In fact, it is such that most of them will be able to move into another world, in which they are able to move, in which they are able to move, in which they move, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they, in which they live, in which they, in which they live, in which they, in which they live, in which they live, in which they, in which they are able to move, in which they are able to move, in which they are able to live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live in which they live, in which they live, in which they live, in which they live in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they, in which they live, in which they live, in which they, in which they, in which they live, in which they live, in which they live, in which they, in which they, in which they live, in which they live, in which they, in which they, in which they live, in which they, in which they live, in which they, in"}, {"heading": "4. Feed-forward Neural Networks", "text": "This year, it is so far that it is only a matter of time before it is ready, until it is ready."}, {"heading": "4.1 Representation Power", "text": "In terms of representational power, MLP1 has been shown by (Hornik, Stinchcombe, & White, 1989; Cybenko, 1989) to be a universal approximator - it can approximate a family of functions with any desired error set that is not zero, 8 encompassing all continuous functions8. Specifically, a feed network with a linear output layer and at least one hidden layer with a \"squashing\" activation function can approximate any measurable Borel function from one finite dimensional space to another. However, the theoretical result does not say how large the hidden layer should be, nor does it say anything about the learning ability of end-dimensional discrete space to another."}, {"heading": "4.2 Common Non-linearities", "text": "Nonlinearity g can take many forms. Currently, there is no good theory about which nonlinearity is to be applied under which conditions, and choosing the correct nonlinearity for a given task is in most cases an empirical question. I will now go over the usual nonlinearity from literature: the sigmoid, tanh, hard tanh, and the rectified linear unit (ReLU). Some NLP researchers also experimented with other forms of nonlinearity such as cube and tanh-cube.Sigmoid The sigmoid activation function \u03c3 (x) = 1 + e \u2212 x is an S-shaped function that transforms any value x into the range."}, {"heading": "4.3 Output Transformations", "text": "In many cases, the output layer vector is also transformed. A common transformation is softmax: x = x1,.., xksoftmax (xi) = exi \u2211 k = 1 e xjThe result is a vector of non-negative real numbers that add up to one, giving a discrete probability distribution over k possible outcomes. Softmax output transformation is used when we are interested in modelling a probability distribution over the possible output classes. To be effective, it should be used in conjunction with a probabilistic training target such as cross entropy (see Section 4.5 below). If the softmax transformation is applied to the output of a network without a hidden layer, the result is the well-known multinomial logistic regression model, also known as the maximum entropy classifier."}, {"heading": "4.4 Embedding Layers", "text": "Until now, the discussion ignored the source of x, which treats it as an arbitrary vector =. \"In an NLP application, x is usually composed of different embedding vectors. (1) In an NLP application (1), the function of core features becomes an input vector (1). (1) Thus, it is common for the embedding vectors associated with each feature to experiment more effectively than other non-linearities in a feature of the NLP community and report success with other forms of non-linearity. (2) The cube activation function, g (x) 3, was proposed by Chen & Manning, which found it to be more effective than other non-linearities in a feed-forward network used to predict actions in a greedy transition-based dependency."}, {"heading": "4.5 Loss Functions", "text": "In fact, it is such that it is a matter of a way in which people are in a position, in which they are in a position, in which they are in a position, in which they are in a position, in which they are able to live in the world, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they, in which they live, in which they, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they, in which they live, in which they, in which they, in which they live, in which they, in which they live, in which they live, in which they, in which they live, in which they, in which they, in which they live, in which they, in which they live, in which they, in which they, in which they live, in which they, in which they, in which they, in fact, are able to live, in fact, in which they are able to be able to live, in which they are able to live, in which they are able to live, in which they are able to live, in which they are able to live, in which they are able to live, in which they are able to live"}, {"heading": "5. Word Embeddings", "text": "A key component of the neural networking approach is the use of embedding - representing each attribute as a vector in a low-dimensional space. But, where do the vectors come from? In this section, the common approaches are examined."}, {"heading": "5.1 Random Initialization", "text": "If sufficient supervised training data is available, the feature embedding can be treated in the same way as the other model parameters: Initialize the embedding vectors to random values and have them embedded in \"good\" vectors by the network training. Be careful when initializing the word vectors to uniformly recorded random numbers in the range [\u2212 12d, 12d] (Mikolov et al., 2013; Mikolov, Sutskever, Chen, Corrado, & Dean, 2013), where d is the number of dimensions. Another option is the use of xavier initialization (see Section 6.3) and initialization with uniformly recorded values from [\u2212 \u221a 6 \u00b0 d, \u221a 6 \u00b0 d]."}, {"heading": "5.2 Supervised Task-specific Pre-training", "text": "If we are interested in task A, for which we have only a limited amount of marked data (for example, syntactical parsing), but there is an auxiliary task B (say, part of the language marker) for which we have much more marked data, we can pre-train our word vectors to function well as predictors for task B, and then use the trained vectors for task A. In this way, we can use the larger amounts of marked data we have for task B. When we train task A, we can either treat the pre-trained vectors as fixed or further fine-tune them to task A. Another option is to train together for both goals, see section 7 for more details."}, {"heading": "5.3 Unsupervised Pre-training", "text": "The usual case is that we do not have an auxiliary task with large amounts of annotated data. (Or maybe we want to help equip the auxiliary programs with better vectors.) In such cases, we resort to \"unattended\" methods that can be trained on vast amounts of unannotated text. Techniques for training the word vectors are essentially those that take care of the supervised learning, but instead of taking care of the task we take care of, we create virtually unlimited numbers of supervised training instances from raw text in the hope that the tasks we have created will match (or be close enough to). The key concepts behind the unguarded approaches are that you want to embed \"similar\" words to have similar vectors. While word similarity is difficult to define and usually very task-dependent, the current approaches come from the distribution mortgage hypothesis."}, {"heading": "5.4 Training Objectives", "text": "In all cases, each word is presented as a d-dimensional vector initialized to a random value. Training the model to perform the auxiliary tasks well will lead to Good12. Interpretation of the creation of auxiliary problems from raw text is inspired by Ando and Zhang (Ando & Zhang, 2005a, 2005b) 13. While often treated as a single algorithm, word2vec is actually a software package with various educational goals, optimization methods and other hyperparameters. See (Rong, 2014; Levy, Goldberg, & Dagan, 2015) for a discussion. 14. http: /.nlukedfordings / i."}, {"heading": "5.5 The Choice of Contexts", "text": "In most cases, the contexts of a word are understood as other words that appear in its environment, either in a short window around it or within the same sentence, paragraph or document. In some cases, the text is automatically analyzed by a syntactic parser, and the contexts come from the syntactic neighborhood created by the automatic parse trees. Sometimes, the definitions of words and context change so that parts of words are also included, such as prefixes or suffixes. Neural word embeddings come from the world of language modeling, in which a network is trained to predict the next word based on a sequence of preceding words (Bengio et al., 2003). There, the text is used to create auxiliary tasks where the goal is to predict a word based on a context, the k previous words. While the training for language modeling actually produces useful embeddings, this approach is unnecessarily limited by the constraints we place on the previous language modeling task, but only by the constraints of the language modeling task."}, {"heading": "5.5.1 Window Approach", "text": "The approach is a sliding-window approach in which the interchange processes are characterized by looking at a sequence of 2k + 1 words. The middle word is referred to as focusing on the focused word and the words on the individual pages are presented as contexts. The two following terms are placed in the foreground: \"It is about linking language and language.\" \"It is about the interweaving of the individual terms.\" \"\" It is about the interweaving of the terms. \"\" \"It is about the interweaving of the individual terms.\" \"\".. \"\" \"\" It is about the interweaving of the terms.......... \"\" \"\". \"\" \"\".. \"\" \"\" \"...\" \"\" \"\" \"..\" \"\" \"\"... \"\" \"\" \"\" \"....\" \"\" \"\" \"..\" \"\" \"\".. \"\" \".\" \"\" \"..\" \"\" \"\".. \"\" \".\" \"\". \".\" \".\" \".\" \".\". \"\". \"\" \"\". \".\". \"\". \".\". \"\" \".\". \".\". \".\". \"\" \"\". \".\". \".\". \".\". \".\". \".\". \".\". \".\". \".\" \".\". \"\" \"\". \".\" \".\". \"..\" \".\". \"\" \"..\"... \"\" \".\" \"\" \"\"... \"\" \"\".. \"\" \"\"... \"\" \"\"... \"\" \"\".. \"\" \"\" \"..\" \"\" \"...\" \"\". \"\" \"\" \"\".... \"\". \"\" \"\" \"....\" \"\" \"\" \".\" \".\" \"...\" \"\" \"\". \"\" \"\" \"\"... \"\" \".\" \"\" \"\"...... \"\" \"\" \"\" \"\" \"\" \"\" \"\"..... \"\" \".....\" \"\". \"\" \"\""}, {"heading": "5.5.2 Sentences, Paragraphs or Documents", "text": "When using a skip program (or CBOW approach), the contexts of a word can be considered as any other word that occurs in the same sentence, paragraph, or document. This corresponds to the use of very large window sizes and should lead to word vectors that capture topical similarity (words from the same topic, i.e. words that one would expect to occur in the same document, will likely get similar vectors)."}, {"heading": "5.5.3 Syntactic Window", "text": "Some works replace the linear context within a sentence with a syntactic one (Levy & Goldberg, 2014a; Bansal, Gimpel, & Livescu, 2014); the text is automatically analyzed by means of a dependency parser; and the context of a word is understood as the words that are in its vicinity in the parse tree, along with the syntactic relationship by which they are connected to each other. Such approaches generate highly functional similarities by grouping words that can play the same role in a sentence (e.g. colors, school names, verbs of movement)."}, {"heading": "5.5.4 Multilingual", "text": "Another possibility is to use multilingual, translation-based contexts (Hermann & Blunsom, 2014; Faruqui & Dyer, 2014): For example, you can start a bilingual alignment model like IBM Model 1 or Model 2 (i.e. using the software GIZA + +) and then use the alignments generated to derive word contexts: The context of a word instance is the alignment of foreign-language words; such alignments tend to give synonyms similar vectors; some authors instead work at the alignment level without relying on word alignments; an appealing method is to mix a monolingual window-based approach with a multilingual approach, creating both types of helper tasks, which is likely to produce vectors that are similar to the window-based level but reduce the somewhat undesirable effect of the window-based approach, where antonyms (e.g. hot, similar and low) arise."}, {"heading": "5.5.5 Character-based and Sub-word Representations", "text": "An interesting line of work attempts to deduce the vector representation of a word from the characters that compose it. Such approaches are probably particularly useful for tasks that are synchronous in nature, since the character patterns within words are strongly linked to their syntactical function. These approaches also have the advantage of producing very small model sizes (only one vector for each character in the alphabet needs to be stored along with a handful of small matrices), and they can provide an embedding vector for each word that can be encountered. Dos Santos and Zadrozny model the embedding of a word with a handful of matrices (see section 9) above the characters. Ling et al models the embedding of a word."}, {"heading": "6. Neural Network Training", "text": "Roughly speaking, all training methods work by repeatedly calculating an estimate of the error over the dataset, calculating the gradient in relation to the error, and then shifting the parameters in the direction of the gradient. Models differ in the way the error estimate is calculated and how \"motion in the direction of the gradient\" is defined. We describe the basic algorithm, the stochastic gradient descent (SGD), and then briefly mention the other approaches with pointers for further reading. Gradient calculation is at the center of the approach. Gradients can be efficiently and automatically calculated by differentiating in reverse mode on a calculation graph - a general algorithmic framework for automatically calculating the gradient of any network and the loss function."}, {"heading": "6.1 Stochastic Gradient Training", "text": "The common procedure for the formation of neural networks is to use or a variant of stochastic gradients (SGD) (SGD) algorithms (Bottou, 2012; LeCun, Orr, & Muller, 1998a). SGD is a general optimization algorithm. It gets a function f, which is parameterized by descent function, and desired input and output pairs. It then tries to set the parameters so that the loss of f is small in relation to the training examples. The algorithms work as follows: Algorithm 1 Online Stochastic Training 1: Input: Function f (x) parameters set. 2: Input: Inputs of inputs x1,., xn and outputs y1,., yn. 3: Input: Input: Loss function L. 4: Input criteria not met."}, {"heading": "6.2 The Computation Graph Abstraction", "text": "The graph defines the order of dependencies between the various components. The graph is a DAG, not a tree, and as a result of the various operations you can separate several operations of the individual nodes from each other. As this procedure is cumbersome and error-prone, it is preferable for most purposes to use automatic tools for calculating gradients (Bengio, 2012).The graph of the calculation allows us to simply construct arbitrary networks, to evaluate their predictions for given inputs, and to calculate gradients for their parameters with respect to arbitrary scalar losses (Backward Pass).The graph of the calculation is a representation of an arbitrary mathematical network, the calculation of which is used as a graph-graph-graph-graph-graph-graph-graph-graph-graph-graph-graph-graph-graph-graph-graph-graph-graph-graph-graph. It is a directed acyclic graph-graph in which the nodes correspond to the mathematical dependencies between the intermediate values of the flow and the structure of the component (the intermediate)."}, {"heading": "6.3 Optimization Issues", "text": "In fact, it is not the case that it is a matter of a way in which it has taken place in the USA in recent years. (...) It is not the case that it has taken place in the USA, in Europe, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in Europe, in the USA, in Europe, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA"}, {"heading": "6.4 Regularization", "text": "A common regulation method is L2 regularization, in which parameters with large values squared are penalized by adding an additive term \"2\" to the objective function to be minimized. A recently proposed alternative regulation method is the square L2 norm (sum of squares of values) and a hyperparameter that controls the extent of regulation. A recently proposed alternative regulation method is aborted (Hinton, Srivastava, Krizhevsky, Sutskever, & Salakhutdinov, 2012). The dropout method was developed to prevent the network from learning to rely on specific weights. It works by randomly dropping (setting to 0) half of the neurons in the network (or in a specific shift). Wager & Salakhutdinov's work (2013) prevents the network from relying on specific weights."}, {"heading": "7. Cascading and Multi-task Learning", "text": "In fact, the fact is that most of us are able to survive ourselves, and that they are able to survive themselves, \"he told the German Press Agency in an interview with\" Welt am Sonntag \":\" I don't think we will be able to change the world. \""}, {"heading": "8. Structured Output Prediction", "text": "Many problems in NLP concern structured output: cases where the desired output is not a class name or distribution via class names, but a structured object such as a sequence, tree, or graph. Canonical examples include sequence marking (e.g. part-of-speech marking), sequence segmentation (chunking, NER), and syntactic parsing. In this section we will discuss how forward-looking neural network models can be used for structured tasks. In later sections we will discuss specialized neural network models for handling sequences (Section 10) and trees (Section 12)."}, {"heading": "8.1 Greedy Structured Prediction", "text": "The greedy approach to structured prediction is to break down the problem of structural prediction into a sequence of local prediction problems and train a classifier to execute each local decision; at test time, the trained classifier is greedily deployed; examples of this approach include left-to-right tagging models (Gime \u0301 nez & Ma rquez, 2004) and greedy transition-based parsing (Nivre, 2008); such approaches can easily be adapted to the use of neural networks by simply replacing the local classifier with a linear classifier such as SVM or a logistic regression model to a neural network, as in (Chen & Manning, 2014; Lewis & Steedman, 2014); greedy approaches suffer from error propagation, where errors in early decisions are transmitted and affect later decisions; the overall higher accuracy achieved with nonlinear neural network classifiers can, to some extent, compensate for this problem."}, {"heading": "8.2 Search Based Structured Prediction", "text": "The common approach to predicting natural language structures is to search for a result based on the search for a result. (Part) Predicting results is based on searching for results (Part). (Part) Predicting results is formulated as a search problem about possible structures: Prediction (x) = arg max y (x) score (x) score (x) score (x), y) where x is an input structure, y is an output over x (in a typical example x is a sentence and y is a tag assignment or a parse tree above the sentence), Y (x) is the set of all validity structures above x, and we are looking for a result that is maximized."}, {"heading": "9. Convolutional Layers", "text": "In fact, most people who are able to move, to move, to move, to move, to move, to move and to move, to move, to move, to move, to move, to move and to move, to move, to move and to move, to move, to move, to move, to move, to move and to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move and to move."}, {"heading": "9.1 Basic Convolution + Pooling", "text": "The main idea behind the confrontation and pooling architecture for language tasks is the application of a non-linear (learned) function over each instantiation of a k-word window over the sentence. This function is also applied in other areas, such as the USA, Canada, Canada, Canada, Canada, Canada, Canada, Canada, Canada, Canada, Canada, Canada, Canada, Canada, Canada, Canada, Canada, Mexico, Mexico, Mexico, Mexico, Mexico, Mexico, Mexico, Mexico, Mexico, Mexico, Mexico, Mexico, Mexico, Mexico, Mexico, Mexico, Mexico, Mexico, Mexico, Mexico, Mexico, Mexico, Mexico, Mexico, Mexico, Mexico, Mexico, Mexico, Mexico, Mexico, Mexico, Mexico, Mexico, Mexico, Mexico, Mexico, Mexico, Mexico, Mexico, Mexico, Mexico, Mexico, Mexico, Mexico, Mexico, Mexico, Mexico, Mexico, Mexico, Mexico, Mexico, Mexico, Mexico, Mexico, Mexico, Mexico, Mexico, Mexico, Mexico, Mexico, Mexico, Mexico, Mexico, Mexico, Mexico, Mexico, Mexico, Mexico, Mexico, Mexico, Mexico, Mexico, Mexico, Mexico, Mexico, Mexico, Mexico, Mexico, Mexico, Mexico, Mexico, Mexico, Mexico, Mexico, Mexico, Mexico, Mexico, Mexico, Mexico, Mexico, Mexico, Mexico, Mexico, Mexico, Mexico, Mexico, Mexico, Mexico, Mexico, Mexico, Mexico, Mexico, Mexico, Mexico, Mexico, Mexico, Mexico, Mexico, Mexico, Mexico, Mexico, Mexico, Mexico, Mexico Mexico Mexico, Mexico, Mexico Mexico, Mexico, Mexico, Mexico, Mexico Mexico Mexico Mexico Mexico, Mexico, Mexico, Mexico, Mexico Mexico Mexico Mexico Mexico, Mexico, Mexico, Mexico Mexico, Mexico Mexico Mexico Mexico Mexico, Mexico Mexico Mexico, Mexico, Mexico Mexico Mexico, Mexico Mexico, Mexico Mexico Mexico Mexico, Mexico Mexico Mexico Mexico, Mexico Mexico Mexico Mexico Mexico, Mexico, Mexico, Mexico Mexico Mexico, Mexico Mexico Mexico Mexico, Mexico Mexico, Mexico Mexico Mexico Mexico Mexico, Mexico, Mexico Mexico Mexico, Mexico Mexico, Mexico Mexico Mexico Mexico Mexico, Mexico, Mexico, Mexico Mexico Mexico, Mexico, Mexico Mexico Mexico Mexico Mexico, Mexico, Mexico Mexico Mexico, Mexico, Mexico Mexico Mexico Mexico Mexico Mexico Mexico Mexico, Mexico, Mexico Mexico, Mexico, Mexico Mexico Mexico, Mexico, Mexico, Mexico Mexico, Mexico, Mexico, Mexico, Mexico Mexico Mexico Mexico, Mexico, Mexico, Mexico Mexico Mexico Mexico, Mexico, Canada, Canada, Canada, Canada, Canada"}, {"heading": "9.2 Dynamic, Hierarchical and k-max Pooling", "text": "Instead of performing a single pool operation over the entire sequence, we can maintain some position information based on our domain understanding of the prediction problem. To this end, we can split the vectors into \"different groups.\" For example, we may suspect that the words that occur in the sentence are too late. We can split the sequence into \"same regions\" by applying a separate max pooling group."}, {"heading": "9.3 Variations", "text": "Instead of a single coil layer, several coil layers can be created in parallel. So, for example, we can have four different coil layers, each of which has a different window size in the range of 2-5, allowing n-gram sequences of different lengths to be captured. The result of each coil layer is then bundled and the resulting vectors concatenated and fed to further processing (Kim, 2014).The coil architecture does not need to be limited to the linear arrangement of a set. For example, Ma et al (2015) generalizes the convolution operation to work via syntactic dependency trees, where each window is located around a node in the syntactic tree, and the bundling is done via the different nodes. Liu et al (2015) also apply a coil architecture to dependence paths extracted from dependence trees. Le and Zuidema (2015) suggest performing maximum pooling over vectors representing different derivatives leading to the same point in a diagram."}, {"heading": "10. Recurrent Neural Networks \u2013 Modeling Sequences and Stacks", "text": "When dealing with speech data, it is very common to work with sequences such as words (strings of letters), sentences (strings of words), and documents. We have seen how feedback networks can use vector concatenation and vector addition (CBOW) to accommodate arbitrary functions via sequences. In particular, the CBOW representation allows encoding of arbitrary length sequences as fixed-size vectors. However, the CBOW representation is quite limited and forces you to disregard the order of the attributes. Constitutional networks also allow encoding of a sequence into a fixed-size vector. While representations derived from Convolutionary networks are an improvement over CBOW representation because they offer a certain sensitivity to the word sequence, their sequence is limited to predominantly local patterns and does not take into account the order of patterns that are far apart in the sequence."}, {"heading": "10.1 The RNN Abstraction", "text": "The question that arises is to what extent it is actually a way in which people in the USA, in Europe, in the EU, in the USA, in the EU, in the USA, in the EU, in the USA, in the EU, in the USA, in the USA, in the EU, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the"}, {"heading": "10.2 RNN Training", "text": "This year it is as far as never before in the history of the Federal Republic of Germany."}, {"heading": "10.3 Multi-layer (stacked) RNNs", "text": "RNNs can be stacked in layers that form a grid (Hihi & Bengio, 1996). Consider k RNNNs, RNN1,..., RNNk, where the jth RNN is the states s j 1: n and the outputs y j 1: n. The input for the first RNN is x1: n, while the input of the jth RNN (j \u2265 2) is the outputs of the underlying RNN, yj \u2212 11: n. The output of the entire formation is the output of the last RNN, y k 1: n. Such layered architectures are often called deep RNNNNs. A visual representation of a 3-layer RNN is shown in Figure 10.While theoretically it is not clear what is the additional power gained by the deeper architecture, it has been empirically observed that deep RNNNs work better in some tasks than flat ones."}, {"heading": "10.4 BI-RNN", "text": "A useful elaboration of an RNN is a bi-directional RNN (BI-RNN) (Schuster & Paliwal, 1997; Graves, 2008).31 Consider the task of sequence marking via a sentence x1,.., xn. An RNN allows us to calculate a function of the ith word xi on the basis of the past - the words x1: i up to and including the same. However, the following words xi: n can also be useful for predicting, as is evident by the common sliding-window approach in which the focus word is categorized on a window of k-words surrounding it. Similar to the RNN loosens the Markov assumption and allows to look arbitrarily back into the past, the BI-RNNN loosens the fixed window size, allowing both the past and the future to be considered."}, {"heading": "10.5 RNNs for Representing Stacks", "text": "In fact, it is the case that one is able to create a new system in which one abides by the rules that one has imposed on oneself, and in which one abides by the rules. (...) In fact, it is the case that one abides by the rules. (...) In fact, it is the case that one abides by the rules. (...) In fact, it is the case that one abides by the rules. (...) It is the case that one abides by the rules. (...) It is the case that one abides by the rules. (...) It is the case that one abides by the rules. (...) It is the case that one abides by the rules. (...)"}, {"heading": "11. Concrete RNN Architectures", "text": "We will now turn to three different instances of the abstract RNN architecture discussed in the previous section, which provide concrete definitions of the R and O functions: the Simple RNN (SRNN), the Long Short-Term Memory (LSTM), and the Gated Recurrent Unit (GRU)."}, {"heading": "11.1 Simple RNN", "text": "The simplest RNN formulation, known as the Elman network or Simple RNN (S-RNN), was proposed by Elman (1990) and explored by Mikolov (2012) for use in speech modeling. S-RNN takes the following form: si = RSRNN (si \u2212 1, xi) = g (xiW x + si \u2212 1W s + b) yi = OSRNN (si) = sisi, yi Rds, xi Rdx, Wx Rdx \u00d7 ds, Ws Rds \u00d7 ds, b RdsThat is, the state at position i is a linear combination of input at position i and the previous state guided by a nonlinear activation (usually tanh or ReLU). Output at position i is the same as the hidden state at that position."}, {"heading": "11.2 LSTM", "text": "The S-RNN is difficult to train because the disappearing gradients are a problem that arises in other areas. Error signals (gradients) in later stages in the sequence decrease rapidly in the real propagation process, and it is difficult for the S-RNN to capture long-term dependencies. The basic idea behind the LSTM is to introduce \"memory cells\" (a vector) as part of state representation that can maintain the gradients over time. Access to the memory cells is controlled by gating components that simulate logical gates. At each input state, a gate is used to decide how much of the new input should be written into the memory cell and how much of the current content of the memory cell should be forgotten."}, {"heading": "11.3 GRU", "text": "However, the complexity of the system makes it difficult to analyze and is also computationally expensive to work with the LSTM. The gated recurrent unit (GRU) was recently introduced by Cho et al (2014b) as an alternative to the LSTM. Subsequently, Chung et al (2014) showed that it is comparable to the LSTM on several (non-textual) datasets. Like the LSTM, the GRU is also based on a gating mechanism, but with significantly fewer gates and no separate memory components. sj = RGRU (sj \u2212 1, xj) = (1 \u2212 z) sj \u2212 1 + z h = nich (xjW xz + hj \u2212 1W hz) r = nit (chsW xr + hj \u2212 1W hr) and no separate memory component."}, {"heading": "11.4 Other Variants", "text": "The gated architects of the LSTM and the GRU help in linar hi polci the component = hi hi input = 1 component = 1 (Simple RNN), and allow these RNNs to capture dependencies that span long time range. Some researchers research simpler architectures than the LSTM and the GRU for achieving similar benefits. Mikolov et al (2014) observes that the matrix multiplication si \u2212 1Ws coupled with the nonlinearity g in the update rule R of the Simple RNN causes the state vector si to undergo major changes at each step, not remembering information over long periods. They suggest splitting the state vector si into a slowly changing component ci (\"context units\") and a rapidly changing component hi. 35 The slowly changing component ci is35. We deviate from the notation in (Mikolov al al al al al al al al al and the al al al al al al symbols are used in the al al description)."}, {"heading": "12. Modeling Trees \u2013 Recursive Neural Networks", "text": "The RNN is very useful for modeling sequences. In language processing, it is often natural and desirable to work with whole tree structures. Trees can be syntactic trees, discourse trees, or even trees that represent the feeling expressed by different parts of a sentence (Socher et al., 2013). In other cases, we may not care directly about the tree structure, but rather about the span in the sentence. In such cases, the tree is only used as a backbone structure that guides the encoding process of the sequence into a whole tree or part of a tree. In other cases, recursive neural abstraction (RecNN) is not used as the reason for the span in the sentence. In such cases, the tree is only used as a backbone structure that guides the encoding process of the sequence into a fixed size vector."}, {"heading": "12.1 Formal Definition", "text": "In fact, it is a reactionary, reactionary, reactionary, reactionary, reactionary, reactionary, reactionary, reactionary, reactionary, reactionary and reactionary act."}, {"heading": "12.2 Extensions and Variations", "text": "Since all of the above definitions of R suffer from the problem of disappearing gradients of the Simple RNN, several authors have attempted to replace it with functions inspired by the gated architecture of the Long ShortTerm Memory (LSTM), resulting in tree-shaped LSTMs (Tai, Socher, & Manning, 2015; Zhu, Sobhani, & Guo, 2015b). The question of optimal tree representation is still a very open research question, and the enormous range of possible combination functions R has yet to be explored. Other variants of tree-structured RNs are presented as a combination of a vector and a matrix, with the vector representing the static secondary content of the word as previously defined, and recursive neural tensor networks (Socher et al., 2013). In the first variant, each word is represented as a combination of a vector and a matrix, with the vector representing the static secondary content of the word as defined, while the secondary function is linked as the secondary function of the secondary matrix and the secondary function."}, {"heading": "12.3 Training Recursive Neural Networks", "text": "The training procedure for a recursive neural network follows the same recipe as the training of other forms of networks: define a loss, specify the computational graph, calculate gradients using backpropagation40, and train the parameters using SGD. In terms of the loss function, a loss similar to the RNN sequence can be associated either with the root of the tree, with any node, or with a series of nodes, usually combining the losses of the individual node by summing up. The loss function is based on the designated training data that a label or other set associates with different tree nodes. Additionally, the RecNN can be treated as an encoder, while the inner vector connected to a node is considered to be the encoding of the tree rooting on that node. The encoding can be potentially sensitive to arbitrary properties of the structure, and the vector is then passed on as input to another network."}, {"heading": "13. Conclusions", "text": "Neural networks are powerful learners, offering possibilities from nonlinear classification to non-Markovian modeling of sequences and trees. We hope that this exposure will help NLP researchers to incorporate neural network models into their work and harness their power."}], "references": [{"title": "Combination of Recurrent Neural Networks and Factored Language Models for Code-Switching Language Modeling", "author": ["H. Adel", "N.T. Vu", "T. Schultz"], "venue": "In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers),", "citeRegEx": "Adel et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Adel et al\\.", "year": 2013}, {"title": "A High-Performance Semi-Supervised Learning Method for Text Chunking", "author": ["R. Ando", "T. Zhang"], "venue": "In Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics (ACL\u201905),", "citeRegEx": "Ando and Zhang,? \\Q2005\\E", "shortCiteRegEx": "Ando and Zhang", "year": 2005}, {"title": "A framework for learning predictive structures from multiple tasks and unlabeled data", "author": ["R.K. Ando", "T. Zhang"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "Ando and Zhang,? \\Q2005\\E", "shortCiteRegEx": "Ando and Zhang", "year": 2005}, {"title": "Joint Language and Translation Modeling with Recurrent Neural Networks", "author": ["M. Auli", "M. Galley", "C. Quirk", "G. Zweig"], "venue": "In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing,", "citeRegEx": "Auli et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Auli et al\\.", "year": 2013}, {"title": "Decoder Integration and Expected BLEU Training for Recurrent Neural Network Language Models. In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)", "author": ["M. Auli", "J. Gao"], "venue": null, "citeRegEx": "Auli and Gao,? \\Q2014\\E", "shortCiteRegEx": "Auli and Gao", "year": 2014}, {"title": "Improved Transition-based Parsing by Modeling Characters instead of Words with LSTMs", "author": ["M. Ballesteros", "C. Dyer", "N.A. Smith"], "venue": "In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing,", "citeRegEx": "Ballesteros et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Ballesteros et al\\.", "year": 2015}, {"title": "Tailoring Continuous Word Representations for Dependency Parsing", "author": ["M. Bansal", "K. Gimpel", "K. Livescu"], "venue": "In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics", "citeRegEx": "Bansal et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Bansal et al\\.", "year": 2014}, {"title": "Automatic differentiation in machine learning: a survey", "author": ["A.G. Baydin", "B.A. Pearlmutter", "A.A. Radul", "J.M. Siskind"], "venue": null, "citeRegEx": "Baydin et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Baydin et al\\.", "year": 2015}, {"title": "Practical recommendations for gradient-based training of deep architectures", "author": ["Y. Bengio"], "venue": "arXiv:1206.5533 [cs].", "citeRegEx": "Bengio,? 2012", "shortCiteRegEx": "Bengio", "year": 2012}, {"title": "Deep Learning. Book in preparation for", "author": ["Y. Bengio", "I.J. Goodfellow", "A. Courville"], "venue": null, "citeRegEx": "Bengio et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Bengio et al\\.", "year": 2015}, {"title": "Non-Linear Text Regression with a Deep Convolutional Neural Network", "author": ["Z. Bitvai", "T. Cohn"], "venue": "In Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Volume 2: Short Papers),", "citeRegEx": "Bitvai and Cohn,? \\Q2015\\E", "shortCiteRegEx": "Bitvai and Cohn", "year": 2015}, {"title": "Compositional Morphology for Word Representations and Language Modelling", "author": ["J.A. Botha", "P. Blunsom"], "venue": "In Proceedings of the 31st International Conference on Machine Learning (ICML),", "citeRegEx": "Botha and Blunsom,? \\Q2014\\E", "shortCiteRegEx": "Botha and Blunsom", "year": 2014}, {"title": "Stochastic gradient descent tricks", "author": ["L. Bottou"], "venue": "Neural Networks: Tricks of the Trade, pp. 421\u2013436. Springer.", "citeRegEx": "Bottou,? 2012", "shortCiteRegEx": "Bottou", "year": 2012}, {"title": "Coarse-to-Fine n-Best Parsing and MaxEnt Discriminative Reranking", "author": ["E. Charniak", "M. Johnson"], "venue": "In Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics", "citeRegEx": "Charniak and Johnson,? \\Q2005\\E", "shortCiteRegEx": "Charniak and Johnson", "year": 2005}, {"title": "A Fast and Accurate Dependency Parser using Neural Networks", "author": ["D. Chen", "C. Manning"], "venue": "In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP),", "citeRegEx": "Chen and Manning,? \\Q2014\\E", "shortCiteRegEx": "Chen and Manning", "year": 2014}, {"title": "Event Extraction via Dynamic Multi-Pooling Convolutional Neural Networks", "author": ["Y. Chen", "L. Xu", "K. Liu", "D. Zeng", "J. Zhao"], "venue": "In Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Volume 1: Long Papers),", "citeRegEx": "Chen et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Chen et al\\.", "year": 2015}, {"title": "On the Properties of Neural Machine Translation: Encoder\u2013Decoder Approaches", "author": ["K. Cho", "B. van Merrienboer", "D. Bahdanau", "Y. Bengio"], "venue": "In Proceedings of SSST8, Eighth Workshop on Syntax, Semantics and Structure in Statistical Translation,", "citeRegEx": "Cho et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Cho et al\\.", "year": 2014}, {"title": "Learning Phrase Representations using RNN Encoder\u2013Decoder for Statistical Machine Translation", "author": ["K. Cho", "B. van Merrienboer", "C. Gulcehre", "D. Bahdanau", "F. Bougares", "H. Schwenk", "Y. Bengio"], "venue": "In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP),", "citeRegEx": "Cho et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Cho et al\\.", "year": 2014}, {"title": "Normalizing tweets with edit scripts and recurrent neural embeddings", "author": ["G. Chrupala"], "venue": "Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), pp. 680\u2013686, Baltimore, Maryland. Association for Computational Linguistics.", "citeRegEx": "Chrupala,? 2014", "shortCiteRegEx": "Chrupala", "year": 2014}, {"title": "Empirical Evaluation of Gated Recurrent Neural Networks on Sequence Modeling", "author": ["J. Chung", "C. Gulcehre", "K. Cho", "Y. Bengio"], "venue": null, "citeRegEx": "Chung et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Chung et al\\.", "year": 2014}, {"title": "Discriminative Training Methods for Hidden Markov Models: Theory and Experiments with Perceptron Algorithms", "author": ["M. Collins"], "venue": "Proceedings of the 2002 Confer-", "citeRegEx": "Collins,? 2002", "shortCiteRegEx": "Collins", "year": 2002}, {"title": "Discriminative Reranking for Natural Language Parsing", "author": ["M. Collins", "T. Koo"], "venue": "Computational Linguistics,", "citeRegEx": "Collins and Koo,? \\Q2005\\E", "shortCiteRegEx": "Collins and Koo", "year": 2005}, {"title": "A unified architecture for natural language processing: Deep neural networks with multitask learning", "author": ["R. Collobert", "J. Weston"], "venue": "In Proceedings of the 25th international conference on Machine learning,", "citeRegEx": "Collobert and Weston,? \\Q2008\\E", "shortCiteRegEx": "Collobert and Weston", "year": 2008}, {"title": "Natural language processing (almost) from scratch", "author": ["R. Collobert", "J. Weston", "L. Bottou", "M. Karlen", "K. Kavukcuoglu", "P. Kuksa"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "Collobert et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Collobert et al\\.", "year": 2011}, {"title": "On the algorithmic implementation of multiclass kernelbased vector machines", "author": ["K. Crammer", "Y. Singer"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "Crammer and Singer,? \\Q2002\\E", "shortCiteRegEx": "Crammer and Singer", "year": 2002}, {"title": "Unsupervised Models for Morpheme Segmentation and Morphology Learning", "author": ["M. Creutz", "K. Lagus"], "venue": "ACM Trans. Speech Lang. Process.,", "citeRegEx": "Creutz and Lagus,? \\Q2007\\E", "shortCiteRegEx": "Creutz and Lagus", "year": 2007}, {"title": "Approximation by superpositions of a sigmoidal function", "author": ["G. Cybenko"], "venue": "Mathematics of Control, Signals and Systems, 2 (4), 303\u2013314.", "citeRegEx": "Cybenko,? 1989", "shortCiteRegEx": "Cybenko", "year": 1989}, {"title": "Improving deep neural networks for LVCSR using rectified linear units and dropout", "author": ["G. Dahl", "T. Sainath", "G. Hinton"], "venue": "IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP),", "citeRegEx": "Dahl et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Dahl et al\\.", "year": 2013}, {"title": "Fast and Accurate Preordering for SMT using Neural Networks", "author": ["A. de Gispert", "G. Iglesias", "B. Byrne"], "venue": "In Proceedings of the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies,", "citeRegEx": "Gispert et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Gispert et al\\.", "year": 2015}, {"title": "Adaptive Recursive Neural Network for Target-dependent Twitter Sentiment Classification", "author": ["L. Dong", "F. Wei", "C. Tan", "D. Tang", "M. Zhou", "K. Xu"], "venue": "In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers),", "citeRegEx": "Dong et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Dong et al\\.", "year": 2014}, {"title": "Question Answering over Freebase with Multi-Column Convolutional Neural Networks", "author": ["L. Dong", "F. Wei", "M. Zhou", "K. Xu"], "venue": "In Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Volume 1: Long Papers),", "citeRegEx": "Dong et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Dong et al\\.", "year": 2015}, {"title": "Deep Convolutional Neural Networks for Sentiment Analysis of Short Texts", "author": ["C. dos Santos", "M. Gatti"], "venue": "In Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers,", "citeRegEx": "Santos and Gatti,? \\Q2014\\E", "shortCiteRegEx": "Santos and Gatti", "year": 2014}, {"title": "Classifying Relations by Ranking with Convolutional Neural Networks", "author": ["C. dos Santos", "B. Xiang", "B. Zhou"], "venue": "In Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Volume 1: Long Papers),", "citeRegEx": "Santos et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Santos et al\\.", "year": 2015}, {"title": "Adaptive subgradient methods for online learning and stochastic optimization", "author": ["J. Duchi", "E. Hazan", "Y. Singer"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "Duchi et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Duchi et al\\.", "year": 2011}, {"title": "Adaptation Data Selection using Neural Language Models: Experiments in Machine Translation", "author": ["K. Duh", "G. Neubig", "K. Sudoh", "H. Tsukada"], "venue": "In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers),", "citeRegEx": "Duh et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Duh et al\\.", "year": 2013}, {"title": "Neural CRF Parsing. In Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)", "author": ["G. Durrett", "D. Klein"], "venue": null, "citeRegEx": "Durrett and Klein,? \\Q2015\\E", "shortCiteRegEx": "Durrett and Klein", "year": 2015}, {"title": "TransitionBased Dependency Parsing with Stack Long Short-Term Memory", "author": ["C. Dyer", "M. Ballesteros", "W. Ling", "A. Matthews", "N.A. Smith"], "venue": "In Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Volume 1: Long Papers),", "citeRegEx": "Dyer et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Dyer et al\\.", "year": 2015}, {"title": "Finding Structure in Time", "author": ["J.L. Elman"], "venue": "Cognitive Science, 14 (2), 179\u2013211.", "citeRegEx": "Elman,? 1990", "shortCiteRegEx": "Elman", "year": 1990}, {"title": "Improving Vector Space Word Representations Using Multilingual Correlation", "author": ["M. Faruqui", "C. Dyer"], "venue": "In Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics,", "citeRegEx": "Faruqui and Dyer,? \\Q2014\\E", "shortCiteRegEx": "Faruqui and Dyer", "year": 2014}, {"title": "Sentence Compression by Deletion with LSTMs", "author": ["K. Filippova", "E. Alfonseca", "C.A. Colmenares", "L. Kaiser", "O. Vinyals"], "venue": "In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing,", "citeRegEx": "Filippova et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Filippova et al\\.", "year": 2015}, {"title": "Dropout as a Bayesian Approximation: Representing Model Uncertainty in Deep Learning", "author": ["Y. Gal", "Z. Ghahramani"], "venue": null, "citeRegEx": "Gal and Ghahramani,? \\Q2015\\E", "shortCiteRegEx": "Gal and Ghahramani", "year": 2015}, {"title": "Modeling Interestingness with Deep Neural Networks", "author": ["J. Gao", "P. Pantel", "M. Gamon", "X. He", "L. Deng"], "venue": "In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP),", "citeRegEx": "Gao et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Gao et al\\.", "year": 2014}, {"title": "SVMTool: A general POS tagger generator based on Support Vector Machines", "author": ["J. Gim\u00e9nez", "L. M\u00e0rquez"], "venue": "In Proceedings of the 4th LREC,", "citeRegEx": "Gim\u00e9nez and M\u00e0rquez,? \\Q2004\\E", "shortCiteRegEx": "Gim\u00e9nez and M\u00e0rquez", "year": 2004}, {"title": "Understanding the difficulty of training deep feedforward neural networks", "author": ["X. Glorot", "Y. Bengio"], "venue": "In International conference on artificial intelligence and statistics,", "citeRegEx": "Glorot and Bengio,? \\Q2010\\E", "shortCiteRegEx": "Glorot and Bengio", "year": 2010}, {"title": "Deep sparse rectifier neural networks", "author": ["X. Glorot", "A. Bordes", "Y. Bengio"], "venue": "In International Conference on Artificial Intelligence and Statistics,", "citeRegEx": "Glorot et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Glorot et al\\.", "year": 2011}, {"title": "An Efficient Algorithm for Easy-First Non-Directional Dependency Parsing", "author": ["Y. Goldberg", "M. Elhadad"], "venue": "In Human Language Technologies: The 2010 Annual Conference", "citeRegEx": "Goldberg and Elhadad,? \\Q2010\\E", "shortCiteRegEx": "Goldberg and Elhadad", "year": 2010}, {"title": "word2vec Explained: deriving Mikolov et al.\u2019s negativesampling word-embedding method", "author": ["Y. Goldberg", "O. Levy"], "venue": null, "citeRegEx": "Goldberg and Levy,? \\Q2014\\E", "shortCiteRegEx": "Goldberg and Levy", "year": 2014}, {"title": "Training Deterministic Parsers with Non-Deterministic Oracles", "author": ["Y. Goldberg", "J. Nivre"], "venue": "Transactions of the Association for Computational Linguistics,", "citeRegEx": "Goldberg and Nivre,? \\Q2013\\E", "shortCiteRegEx": "Goldberg and Nivre", "year": 2013}, {"title": "Efficient Implementation of Beam-Search Incremental Parsers", "author": ["Y. Goldberg", "K. Zhao", "L. Huang"], "venue": "In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics", "citeRegEx": "Goldberg et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Goldberg et al\\.", "year": 2013}, {"title": "Learning Task-Dependent Distributed Representations by Backpropagation Through Structure", "author": ["C. Goller", "A. K\u00fcchler"], "venue": "In In Proc. of the ICNN-96,", "citeRegEx": "Goller and K\u00fcchler,? \\Q1996\\E", "shortCiteRegEx": "Goller and K\u00fcchler", "year": 1996}, {"title": "Supervised sequence labelling with recurrent neural networks", "author": ["A. Graves"], "venue": "Ph.D. thesis, Technische Universit\u00e4t M\u00fcnchen.", "citeRegEx": "Graves,? 2008", "shortCiteRegEx": "Graves", "year": 2008}, {"title": "LSTM: A Search Space Odyssey", "author": ["K. Greff", "R.K. Srivastava", "J. Kout\u0144\u0131k", "B.R. Steunebrink", "J. Schmidhuber"], "venue": null, "citeRegEx": "Greff et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Greff et al\\.", "year": 2015}, {"title": "Search-based Structured Prediction. Machine Learning Journal (MLJ)", "author": ["Hal Daum\u00e9 III", "J. Langford", "D. Marcu"], "venue": null, "citeRegEx": "III et al\\.,? \\Q2009\\E", "shortCiteRegEx": "III et al\\.", "year": 2009}, {"title": "Distributional Structure", "author": ["Z. Harris"], "venue": "Word, 10 (23), 146\u2013162.", "citeRegEx": "Harris,? 1954", "shortCiteRegEx": "Harris", "year": 1954}, {"title": "Simple Customization of Recursive Neural Networks for Semantic Relation Classification", "author": ["K. Hashimoto", "M. Miwa", "Y. Tsuruoka", "T. Chikayama"], "venue": "In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing,", "citeRegEx": "Hashimoto et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Hashimoto et al\\.", "year": 2013}, {"title": "Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification", "author": ["K. He", "X. Zhang", "S. Ren", "J. Sun"], "venue": null, "citeRegEx": "He et al\\.,? \\Q2015\\E", "shortCiteRegEx": "He et al\\.", "year": 2015}, {"title": "Deep Neural Network Approach for the Dialog State Tracking Challenge", "author": ["M. Henderson", "B. Thomson", "S. Young"], "venue": "In Proceedings of the SIGDIAL 2013 Conference,", "citeRegEx": "Henderson et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Henderson et al\\.", "year": 2013}, {"title": "The Role of Syntax in Vector Space Models of Compositional Semantics. In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)", "author": ["K.M. Hermann", "P. Blunsom"], "venue": null, "citeRegEx": "Hermann and Blunsom,? \\Q2013\\E", "shortCiteRegEx": "Hermann and Blunsom", "year": 2013}, {"title": "Multilingual Models for Compositional Distributed Semantics. In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)", "author": ["K.M. Hermann", "P. Blunsom"], "venue": null, "citeRegEx": "Hermann and Blunsom,? \\Q2014\\E", "shortCiteRegEx": "Hermann and Blunsom", "year": 2014}, {"title": "Hierarchical Recurrent Neural Networks for Long-Term Dependencies", "author": ["S.E. Hihi", "Y. Bengio"], "venue": "Advances in Neural Information Processing Systems", "citeRegEx": "Hihi and Bengio,? \\Q1996\\E", "shortCiteRegEx": "Hihi and Bengio", "year": 1996}, {"title": "Improving neural networks by preventing co-adaptation of feature detectors", "author": ["G.E. Hinton", "N. Srivastava", "A. Krizhevsky", "I. Sutskever", "R.R. Salakhutdinov"], "venue": null, "citeRegEx": "Hinton et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Hinton et al\\.", "year": 2012}, {"title": "Long short-term memory", "author": ["S. Hochreiter", "J. Schmidhuber"], "venue": "Neural computation,", "citeRegEx": "Hochreiter and Schmidhuber,? \\Q1997\\E", "shortCiteRegEx": "Hochreiter and Schmidhuber", "year": 1997}, {"title": "Multilayer feedforward networks are universal approximators", "author": ["K. Hornik", "M. Stinchcombe", "H. White"], "venue": "Neural Networks,", "citeRegEx": "Hornik et al\\.,? \\Q1989\\E", "shortCiteRegEx": "Hornik et al\\.", "year": 1989}, {"title": "Opinion Mining with Deep Recurrent Neural Networks", "author": ["O. Irsoy", "C. Cardie"], "venue": "In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP),", "citeRegEx": "Irsoy and Cardie,? \\Q2014\\E", "shortCiteRegEx": "Irsoy and Cardie", "year": 2014}, {"title": "A Neural Network for Factoid Question Answering over Paragraphs", "author": ["M. Iyyer", "J. Boyd-Graber", "L. Claudino", "R. Socher", "H. Daum\u00e9 III"], "venue": "In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP),", "citeRegEx": "Iyyer et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Iyyer et al\\.", "year": 2014}, {"title": "Political Ideology Detection Using Recursive Neural Networks. In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)", "author": ["M. Iyyer", "P. Enns", "J. Boyd-Graber", "P. Resnik"], "venue": null, "citeRegEx": "Iyyer et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Iyyer et al\\.", "year": 2014}, {"title": "Deep Unordered Composition Rivals Syntactic Methods for Text Classification", "author": ["M. Iyyer", "V. Manjunatha", "J. Boyd-Graber", "H. Daum\u00e9 III"], "venue": "In Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Volume 1: Long Papers),", "citeRegEx": "Iyyer et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Iyyer et al\\.", "year": 2015}, {"title": "Effective Use of Word Order for Text Categorization with Convolutional Neural Networks", "author": ["R. Johnson", "T. Zhang"], "venue": null, "citeRegEx": "Johnson and Zhang,? \\Q2014\\E", "shortCiteRegEx": "Johnson and Zhang", "year": 2014}, {"title": "Effective Use of Word Order for Text Categorization with Convolutional Neural Networks", "author": ["R. Johnson", "T. Zhang"], "venue": "In Proceedings of the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies,", "citeRegEx": "Johnson and Zhang,? \\Q2015\\E", "shortCiteRegEx": "Johnson and Zhang", "year": 2015}, {"title": "An Empirical Exploration of Recurrent Network Architectures", "author": ["R. Jozefowicz", "W. Zaremba", "I. Sutskever"], "venue": "In Proceedings of the 32nd International Conference on Machine Learning", "citeRegEx": "Jozefowicz et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Jozefowicz et al\\.", "year": 2015}, {"title": "A Convolutional Neural Network for Modelling Sentences", "author": ["N. Kalchbrenner", "E. Grefenstette", "P. Blunsom"], "venue": null, "citeRegEx": "Kalchbrenner et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Kalchbrenner et al\\.", "year": 2014}, {"title": "Visualizing and Understanding Recurrent Networks. arXiv:1506.02078 [cs", "author": ["A. Karpathy", "J. Johnson", "Li", "F.-F"], "venue": null, "citeRegEx": "Karpathy et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Karpathy et al\\.", "year": 2015}, {"title": "Convolutional Neural Networks for Sentence Classification", "author": ["Y. Kim"], "venue": "Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pp. 1746\u20131751, Doha, Qatar. Association for Computational Linguistics.", "citeRegEx": "Kim,? 2014", "shortCiteRegEx": "Kim", "year": 2014}, {"title": "Adam: A Method for Stochastic Optimization", "author": ["D. Kingma", "J. Ba"], "venue": null, "citeRegEx": "Kingma and Ba,? \\Q2014\\E", "shortCiteRegEx": "Kingma and Ba", "year": 2014}, {"title": "ImageNet Classification with Deep Convolutional Neural Networks", "author": ["A. Krizhevsky", "I. Sutskever", "G.E. Hinton"], "venue": "Advances in Neural Information Processing Systems", "citeRegEx": "Krizhevsky et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Krizhevsky et al\\.", "year": 2012}, {"title": "Fast Methods for Kernel-based Text Analysis", "author": ["T. Kudo", "Y. Matsumoto"], "venue": "In Proceedings of the 41st Annual Meeting on Association for Computational Linguistics Volume 1,", "citeRegEx": "Kudo and Matsumoto,? \\Q2003\\E", "shortCiteRegEx": "Kudo and Matsumoto", "year": 2003}, {"title": "The Inside-Outside Recursive Neural Network model for Dependency Parsing", "author": ["P. Le", "W. Zuidema"], "venue": "In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP),", "citeRegEx": "Le and Zuidema,? \\Q2014\\E", "shortCiteRegEx": "Le and Zuidema", "year": 2014}, {"title": "The Forest Convolutional Network: Compositional Distributional Semantics with a Neural Chart and without Binarization", "author": ["P. Le", "W. Zuidema"], "venue": "In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing,", "citeRegEx": "Le and Zuidema,? \\Q2015\\E", "shortCiteRegEx": "Le and Zuidema", "year": 2015}, {"title": "A Simple Way to Initialize Recurrent Networks of Rectified Linear Units", "author": ["Q.V. Le", "N. Jaitly", "G.E. Hinton"], "venue": null, "citeRegEx": "Le et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Le et al\\.", "year": 2015}, {"title": "Convolutional Networks for Images, Speech, and TimeSeries", "author": ["Y. LeCun", "Y. Bengio"], "venue": "The Handbook of Brain Theory and Neural Networks", "citeRegEx": "LeCun and Bengio,? \\Q1995\\E", "shortCiteRegEx": "LeCun and Bengio", "year": 1995}, {"title": "Efficient BackProp", "author": ["Y. LeCun", "L. Bottou", "G. Orr", "K. Muller"], "venue": "Neural Networks: Tricks of the trade. Springer", "citeRegEx": "LeCun et al\\.,? \\Q1998\\E", "shortCiteRegEx": "LeCun et al\\.", "year": 1998}, {"title": "Gradient Based Learning Applied to Pattern Recognition", "author": ["Y. Lecun", "L. Bottou", "Y. Bengio", "P. Haffner"], "venue": null, "citeRegEx": "Lecun et al\\.,? \\Q1998\\E", "shortCiteRegEx": "Lecun et al\\.", "year": 1998}, {"title": "A tutorial on energybased learning", "author": ["Y. LeCun", "S. Chopra", "R. Hadsell", "M. Ranzato", "F. Huang"], "venue": "Predicting structured data,", "citeRegEx": "LeCun et al\\.,? \\Q2006\\E", "shortCiteRegEx": "LeCun et al\\.", "year": 2006}, {"title": "Loss functions for discriminative training of energybased models", "author": ["Y. LeCun", "F. Huang"], "venue": null, "citeRegEx": "LeCun and Huang,? \\Q2005\\E", "shortCiteRegEx": "LeCun and Huang", "year": 2005}, {"title": "Dependency-Based Word Embeddings", "author": ["O. Levy", "Y. Goldberg"], "venue": "In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics", "citeRegEx": "Levy and Goldberg,? \\Q2014\\E", "shortCiteRegEx": "Levy and Goldberg", "year": 2014}, {"title": "Neural Word Embedding as Implicit Matrix Factorization", "author": ["O. Levy", "Y. Goldberg"], "venue": null, "citeRegEx": "Levy and Goldberg,? \\Q2014\\E", "shortCiteRegEx": "Levy and Goldberg", "year": 2014}, {"title": "Improving Distributional Similarity with Lessons Learned from Word Embeddings", "author": ["O. Levy", "Y. Goldberg", "I. Dagan"], "venue": "Transactions of the Association for Computational Linguistics,", "citeRegEx": "Levy et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Levy et al\\.", "year": 2015}, {"title": "Improved CCG Parsing with Semi-supervised Supertagging", "author": ["M. Lewis", "M. Steedman"], "venue": "Transactions of the Association for Computational Linguistics,", "citeRegEx": "Lewis and Steedman,? \\Q2014\\E", "shortCiteRegEx": "Lewis and Steedman", "year": 2014}, {"title": "Recursive Deep Models for Discourse Parsing", "author": ["J. Li", "R. Li", "E. Hovy"], "venue": "In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP),", "citeRegEx": "Li et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Li et al\\.", "year": 2014}, {"title": "Two/Too Simple Adaptations of Word2Vec for Syntax Problems", "author": ["W. Ling", "C. Dyer", "A.W. Black", "I. Trancoso"], "venue": "In Proceedings of the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies,", "citeRegEx": "Ling et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Ling et al\\.", "year": 2015}, {"title": "Finding Function in Form: Compositional Character Models for Open Vocabulary Word Representation", "author": ["W. Ling", "C. Dyer", "A.W. Black", "I. Trancoso", "R. Fermandez", "S. Amir", "L. Marujo", "T. Luis"], "venue": "In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing,", "citeRegEx": "Ling et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Ling et al\\.", "year": 2015}, {"title": "A Dependency-Based Neural Network for Relation Classification", "author": ["Y. Liu", "F. Wei", "S. Li", "H. Ji", "M. Zhou", "H. WANG"], "venue": "In Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Volume 2: Short Papers),", "citeRegEx": "Liu et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Liu et al\\.", "year": 2015}, {"title": "Tagging The Web: Building A Robust Web Tagger with Neural Network", "author": ["J. Ma", "Y. Zhang", "J. Zhu"], "venue": "In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),", "citeRegEx": "Ma et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Ma et al\\.", "year": 2014}, {"title": "Dependency-based Convolutional Neural Networks for Sentence Embedding", "author": ["M. Ma", "L. Huang", "B. Zhou", "B. Xiang"], "venue": "In Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Volume 2: Short Papers),", "citeRegEx": "Ma et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Ma et al\\.", "year": 2015}, {"title": "Maximum Entropy Markov Models for Information Extraction and Segmentation", "author": ["A. McCallum", "D. Freitag", "F.C. Pereira"], "venue": "In ICML,", "citeRegEx": "McCallum et al\\.,? \\Q2000\\E", "shortCiteRegEx": "McCallum et al\\.", "year": 2000}, {"title": "Efficient Estimation of Word Representations in Vector Space", "author": ["T. Mikolov", "K. Chen", "G. Corrado", "J. Dean"], "venue": null, "citeRegEx": "Mikolov et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Learning Longer Memory in Recurrent Neural Networks", "author": ["T. Mikolov", "A. Joulin", "S. Chopra", "M. Mathieu", "M. Ranzato"], "venue": null, "citeRegEx": "Mikolov et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2014}, {"title": "Recurrent neural network based language model", "author": ["T. Mikolov", "M. Karafi\u00e1t", "L. Burget", "J. Cernocky", "S. Khudanpur"], "venue": "In INTERSPEECH 2010, 11th Annual Conference of the International Speech Communication Association,", "citeRegEx": "Mikolov et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2010}, {"title": "Extensions of recurrent neural network language model", "author": ["T. Mikolov", "S. Kombrink", "Luk\u00e1\u0161 Burget", "J.H. \u010cernocky", "S. Khudanpur"], "venue": "In Acoustics, Speech and Signal Processing (ICASSP),", "citeRegEx": "Mikolov et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2011}, {"title": "Distributed Representations of Words and Phrases and their Compositionality", "author": ["T. Mikolov", "I. Sutskever", "K. Chen", "G.S. Corrado", "J. Dean"], "venue": "Advances in Neural Information Processing Systems", "citeRegEx": "Mikolov et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Statistical language models based on neural networks", "author": ["T. Mikolov"], "venue": "Ph.D. thesis, Ph. D. thesis, Brno University of Technology.", "citeRegEx": "Mikolov,? 2012", "shortCiteRegEx": "Mikolov", "year": 2012}, {"title": "Learning word embeddings efficiently with noisecontrastive estimation", "author": ["A. Mnih", "K. Kavukcuoglu"], "venue": "Advances in Neural Information Processing Systems", "citeRegEx": "Mnih and Kavukcuoglu,? \\Q2013\\E", "shortCiteRegEx": "Mnih and Kavukcuoglu", "year": 2013}, {"title": "Multi-domain Dialog State Tracking using Recurrent Neural Networks", "author": ["N. Mrk\u0161i\u0107", "D. \u00d3 S\u00e9aghdha", "B. Thomson", "M. Gasic", "Su", "P.-H", "D. Vandyke", "Wen", "T.-H", "S. Young"], "venue": "In Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Volume", "citeRegEx": "Mrk\u0161i\u0107 et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Mrk\u0161i\u0107 et al\\.", "year": 2015}, {"title": "Introduction to Automatic Differentiation and MATLAB ObjectOriented Programming", "author": ["R. Neidinger"], "venue": "SIAM Review, 52 (3), 545\u2013563.", "citeRegEx": "Neidinger,? 2010", "shortCiteRegEx": "Neidinger", "year": 2010}, {"title": "Event Detection and Domain Adaptation with Convolutional Neural Networks", "author": ["T.H. Nguyen", "R. Grishman"], "venue": "In Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Volume 2: Short Papers),", "citeRegEx": "Nguyen and Grishman,? \\Q2015\\E", "shortCiteRegEx": "Nguyen and Grishman", "year": 2015}, {"title": "Algorithms for Deterministic Incremental Dependency Parsing", "author": ["J. Nivre"], "venue": "Computational Linguistics, 34 (4), 513\u2013553.", "citeRegEx": "Nivre,? 2008", "shortCiteRegEx": "Nivre", "year": 2008}, {"title": "Purely Functional Data Structures", "author": ["C. Okasaki"], "venue": "Cambridge University Press, Cambridge, U.K.; New York.", "citeRegEx": "Okasaki,? 1999", "shortCiteRegEx": "Okasaki", "year": 1999}, {"title": "On the difficulty of training Recurrent Neural Networks", "author": ["R. Pascanu", "T. Mikolov", "Y. Bengio"], "venue": null, "citeRegEx": "Pascanu et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Pascanu et al\\.", "year": 2012}, {"title": "An Effective Neural Network Model for Graph-based Dependency Parsing", "author": ["W. Pei", "T. Ge", "B. Chang"], "venue": "In Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Volume 1: Long Papers),", "citeRegEx": "Pei et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Pei et al\\.", "year": 2015}, {"title": "Glove: Global Vectors for Word Representation", "author": ["J. Pennington", "R. Socher", "C. Manning"], "venue": "In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP),", "citeRegEx": "Pennington et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Pennington et al\\.", "year": 2014}, {"title": "Recursive Distributed Representations", "author": ["J.B. Pollack"], "venue": "Artificial Intelligence, 46, 77\u2013105.", "citeRegEx": "Pollack,? 1990", "shortCiteRegEx": "Pollack", "year": 1990}, {"title": "Some methods of speeding up the convergence of iteration methods", "author": ["B.T. Polyak"], "venue": "USSR Computational Mathematics and Mathematical Physics, 4 (5), 1 \u2013 17.", "citeRegEx": "Polyak,? 1964", "shortCiteRegEx": "Polyak", "year": 1964}, {"title": "Learning Tag Embeddings and Tag-specific Composition Functions in Recursive Neural Network", "author": ["Q. Qian", "B. Tian", "M. Huang", "Y. Liu", "X. Zhu"], "venue": "In Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Volume 1: Long Papers),", "citeRegEx": "Qian et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Qian et al\\.", "year": 2015}, {"title": "word2vec Parameter Learning Explained", "author": ["X. Rong"], "venue": "arXiv:1411.2738 [cs].", "citeRegEx": "Rong,? 2014", "shortCiteRegEx": "Rong", "year": 2014}, {"title": "Learning representations by back-propagating", "author": ["D.E. Rumelhart", "G.E. Hinton", "R.J. Williams"], "venue": "errors. Nature,", "citeRegEx": "Rumelhart et al\\.,? \\Q1986\\E", "shortCiteRegEx": "Rumelhart et al\\.", "year": 1986}, {"title": "Learning Character-level Representations for Partof-Speech Tagging", "author": ["C.D. Santos", "B. Zadrozny"], "venue": null, "citeRegEx": "Santos and Zadrozny,? \\Q2014\\E", "shortCiteRegEx": "Santos and Zadrozny", "year": 2014}, {"title": "Bidirectional recurrent neural networks", "author": ["M. Schuster", "K.K. Paliwal"], "venue": "IEEE Transactions on Signal Processing,", "citeRegEx": "Schuster and Paliwal,? \\Q1997\\E", "shortCiteRegEx": "Schuster and Paliwal", "year": 1997}, {"title": "Kernel Methods for Pattern Analysis", "author": ["J. Shawe-Taylor", "N. Cristianini"], "venue": null, "citeRegEx": "Shawe.Taylor and Cristianini,? \\Q2004\\E", "shortCiteRegEx": "Shawe.Taylor and Cristianini", "year": 2004}, {"title": "Linguistic Structure Prediction", "author": ["N.A. Smith"], "venue": "Synthesis Lectures on Human Language Technologies. Morgan and Claypool.", "citeRegEx": "Smith,? 2011", "shortCiteRegEx": "Smith", "year": 2011}, {"title": "Recursive Deep Learning For Natural Language Processing and Computer Vision", "author": ["R. Socher"], "venue": "Ph.D. thesis, Stanford University.", "citeRegEx": "Socher,? 2014", "shortCiteRegEx": "Socher", "year": 2014}, {"title": "Parsing with Compositional Vector Grammars", "author": ["R. Socher", "J. Bauer", "C.D. Manning", "N. Andrew Y"], "venue": "In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),", "citeRegEx": "Socher et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Socher et al\\.", "year": 2013}, {"title": "Semantic Compositionality through Recursive Matrix-Vector Spaces", "author": ["R. Socher", "B. Huval", "C.D. Manning", "A.Y. Ng"], "venue": "In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning,", "citeRegEx": "Socher et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Socher et al\\.", "year": 2012}, {"title": "Parsing Natural Scenes and Natural Language with Recursive Neural Networks", "author": ["R. Socher", "Lin", "C.C.-Y", "A.Y. Ng", "C.D. Manning"], "venue": "Proceedings of the 28th International Conference on Machine Learning,", "citeRegEx": "Socher et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Socher et al\\.", "year": 2011}, {"title": "Learning Continuous Phrase Representations and Syntactic Parsing with Recursive Neural Networks", "author": ["R. Socher", "C. Manning", "A. Ng"], "venue": "In Proceedings of the Deep Learning and Unsupervised Feature Learning Workshop of {NIPS}", "citeRegEx": "Socher et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Socher et al\\.", "year": 2010}, {"title": "Recursive Deep Models for Semantic Compositionality Over a Sentiment Treebank", "author": ["R. Socher", "A. Perelygin", "J. Wu", "J. Chuang", "C.D. Manning", "A. Ng", "C. Potts"], "venue": "In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing,", "citeRegEx": "Socher et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Socher et al\\.", "year": 2013}, {"title": "A Neural Network Approach to Context-Sensitive Generation of Conversational Responses", "author": ["A. Sordoni", "M. Galley", "M. Auli", "C. Brockett", "Y. Ji", "M. Mitchell", "Nie", "J.-Y", "J. Gao", "B. Dolan"], "venue": "In Proceedings of the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies,", "citeRegEx": "Sordoni et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Sordoni et al\\.", "year": 2015}, {"title": "Translation Modeling with Bidirectional Recurrent Neural Networks", "author": ["M. Sundermeyer", "T. Alkhouli", "J. Wuebker", "H. Ney"], "venue": "In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP),", "citeRegEx": "Sundermeyer et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Sundermeyer et al\\.", "year": 2014}, {"title": "LSTM Neural Networks for Language Modeling", "author": ["M. Sundermeyer", "R. Schl\u00fcter", "H. Ney"], "venue": "In INTERSPEECH", "citeRegEx": "Sundermeyer et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Sundermeyer et al\\.", "year": 2012}, {"title": "On the importance of initialization and momentum in deep learning", "author": ["I. Sutskever", "J. Martens", "G. Dahl", "G. Hinton"], "venue": "In Proceedings of the 30th international conference on machine learning", "citeRegEx": "Sutskever et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Sutskever et al\\.", "year": 2013}, {"title": "Generating text with recurrent neural networks", "author": ["I. Sutskever", "J. Martens", "G.E. Hinton"], "venue": "In Proceedings of the 28th International Conference on Machine Learning", "citeRegEx": "Sutskever et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Sutskever et al\\.", "year": 2011}, {"title": "Sequence to Sequence Learning with Neural Networks", "author": ["I. Sutskever", "O. Vinyals", "Q.V.V. Le"], "venue": "Advances in Neural Information Processing Systems", "citeRegEx": "Sutskever et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Sutskever et al\\.", "year": 2014}, {"title": "Improved Semantic Representations From Tree-Structured Long Short-Term Memory Networks", "author": ["K.S. Tai", "R. Socher", "C.D. Manning"], "venue": "In Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Volume 1: Long Papers),", "citeRegEx": "Tai et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Tai et al\\.", "year": 2015}, {"title": "Recurrent Neural Networks for Word Alignment Model. In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)", "author": ["A. Tamura", "T. Watanabe", "E. Sumita"], "venue": null, "citeRegEx": "Tamura et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Tamura et al\\.", "year": 2014}, {"title": "Lecture 6.5\u2014RmsProp: Divide the gradient by a running average of its recent magnitude. COURSERA: Neural Networks for Machine Learning", "author": ["T. Tieleman", "G. Hinton"], "venue": null, "citeRegEx": "Tieleman and Hinton,? \\Q2012\\E", "shortCiteRegEx": "Tieleman and Hinton", "year": 2012}, {"title": "A Neural Network Approach to Selectional Preference Acquisition", "author": ["T. Van de Cruys"], "venue": "Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pp. 26\u201335, Doha, Qatar. Association for Computational Linguistics.", "citeRegEx": "Cruys,? 2014", "shortCiteRegEx": "Cruys", "year": 2014}, {"title": "Decoding with Large-Scale Neural Language Models Improves Translation", "author": ["A. Vaswani", "Y. Zhao", "V. Fossum", "D. Chiang"], "venue": "In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing,", "citeRegEx": "Vaswani et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Vaswani et al\\.", "year": 2013}, {"title": "Dropout Training as Adaptive Regularization", "author": ["S. Wager", "S. Wang", "P.S. Liang"], "venue": "Advances in Neural Information Processing Systems", "citeRegEx": "Wager et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Wager et al\\.", "year": 2013}, {"title": "Semantic Clustering and Convolutional Neural Network for Short Text Categorization", "author": ["P. Wang", "J. Xu", "B. Xu", "C. Liu", "H. Zhang", "F. Wang", "H. Hao"], "venue": "In Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Volume 2: Short Papers),", "citeRegEx": "Wang et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2015}, {"title": "Predicting Polarities of Tweets by Composing Word Embeddings with Long Short-Term Memory", "author": ["X. Wang", "Y. Liu", "C. SUN", "B. Wang"], "venue": "In Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Volume 1: Long Papers),", "citeRegEx": "Wang et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2015}, {"title": "Transition-based Neural Constituent Parsing", "author": ["T. Watanabe", "E. Sumita"], "venue": "Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Volume 1: Long Papers),", "citeRegEx": "Watanabe and Sumita,? \\Q2015\\E", "shortCiteRegEx": "Watanabe and Sumita", "year": 2015}, {"title": "Structured Training for Neural Network Transition-Based Parsing", "author": ["D. Weiss", "C. Alberti", "M. Collins", "S. Petrov"], "venue": "In Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Volume 1: Long Papers),", "citeRegEx": "Weiss et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Weiss et al\\.", "year": 2015}, {"title": "Backpropagation through time: What it does and how to do it", "author": ["P.J. Werbos"], "venue": "Proceedings of the IEEE,", "citeRegEx": "Werbos,? \\Q1990\\E", "shortCiteRegEx": "Werbos", "year": 1990}, {"title": "Connecting Language and Knowledge Bases with Embedding Models for Relation Extraction", "author": ["J. Weston", "A. Bordes", "O. Yakhnenko", "N. Usunier"], "venue": "In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing,", "citeRegEx": "Weston et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Weston et al\\.", "year": 2013}, {"title": "CCG Supertagging with a Recurrent Neural Network", "author": ["W. Xu", "M. Auli", "S. Clark"], "venue": "In Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Volume 2: Short Papers),", "citeRegEx": "Xu et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Xu et al\\.", "year": 2015}, {"title": "Convolutional Neural Network for Paraphrase Identification", "author": ["W. Yin", "H. Sch\u00fctze"], "venue": "In Proceedings of the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies,", "citeRegEx": "Yin and Sch\u00fctze,? \\Q2015\\E", "shortCiteRegEx": "Yin and Sch\u00fctze", "year": 2015}, {"title": "Recurrent Neural Network Regularization", "author": ["W. Zaremba", "I. Sutskever", "O. Vinyals"], "venue": null, "citeRegEx": "Zaremba et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Zaremba et al\\.", "year": 2014}, {"title": "ADADELTA: An Adaptive Learning Rate Method", "author": ["M.D. Zeiler"], "venue": "arXiv:1212.5701", "citeRegEx": "Zeiler,? 2012", "shortCiteRegEx": "Zeiler", "year": 2012}, {"title": "Relation Classification via Convolutional Deep Neural Network", "author": ["D. Zeng", "K. Liu", "S. Lai", "G. Zhou", "J. Zhao"], "venue": "In Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers,", "citeRegEx": "Zeng et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Zeng et al\\.", "year": 2014}, {"title": "A Neural Probabilistic StructuredPrediction Model for Transition-Based Dependency Parsing", "author": ["H. Zhou", "Y. Zhang", "S. Huang", "J. Chen"], "venue": "In Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Volume 1: Long Papers),", "citeRegEx": "Zhou et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Zhou et al\\.", "year": 2015}, {"title": "A Re-ranking Model for Dependency Parser with Recursive Convolutional Neural Network", "author": ["C. Zhu", "X. Qiu", "X. Chen", "X. Huang"], "venue": "In Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Volume 1: Long Papers),", "citeRegEx": "Zhu et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Zhu et al\\.", "year": 2015}, {"title": "Long Short-Term Memory Over Tree Structures", "author": ["X. Zhu", "P. Sobhani", "H. Guo"], "venue": null, "citeRegEx": "Zhu et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Zhu et al\\.", "year": 2015}], "referenceMentions": [{"referenceID": 8, "context": "In particular, the book by Bengio et al (2015) is highly recommended.", "startOffset": 27, "endOffset": 47}, {"referenceID": 72, "context": "Convolutional and pooling architecture show promising results on many tasks, including document classification (Johnson & Zhang, 2015), short-text categorization (Wang, Xu, Xu, Liu, Zhang, Wang, & Hao, 2015a), sentiment classification (Kalchbrenner, Grefenstette, & Blunsom, 2014; Kim, 2014), relation type classification between entities (Zeng, Liu, Lai, Zhou, & Zhao, 2014; dos Santos, Xiang, & Zhou, 2015), event detection (Chen, Xu, Liu, Zeng, & Zhao, 2015; Nguyen & Grishman, 2015), paraphrase identification (Yin & Sch\u00fctze, 2015) semantic role labeling (Collobert, Weston, Bottou, Karlen, Kavukcuoglu, & Kuksa, 2011), question answering (Dong, Wei, Zhou, & Xu, 2015), predicting box-office revenues of movies based on critic reviews (Bitvai & Cohn, 2015) modeling text interestingness (Gao, Pantel, Gamon, He, & Deng, 2014), and modeling the relation between character-sequences and part-of-speech tags (Santos & Zadrozny, 2014).", "startOffset": 235, "endOffset": 291}, {"referenceID": 8, "context": "Straight-forward applications of a feed-forward network as a classifier replacement (usually coupled with the use of pre-trained word vectors) provide benefits also for CCG supertagging (Lewis & Steedman, 2014), dialog state tracking (Henderson, Thomson, & Young, 2013), pre-ordering for statistical machine translation (de Gispert, Iglesias, & Byrne, 2015) and language modeling (Bengio, Ducharme, Vincent, & Janvin, 2003; Vaswani, Zhao, Fossum, & Chiang, 2013). Iyyer et al (2015) demonstrate that multilayer feed-forward networks can provide competitive results on sentiment classification and factoid question answering.", "startOffset": 381, "endOffset": 483}, {"referenceID": 37, "context": "Recurrent networks (Elman, 1990) are designed to model sequences, while recursive networks (Goller & K\u00fcchler, 1996) are generalizations of recurrent networks that can handle trees.", "startOffset": 19, "endOffset": 32}, {"referenceID": 100, "context": "Recurrent models have been shown to produce very strong results for language modeling, including (Mikolov, Karafi\u00e1t, Burget, Cernocky, & Khudanpur, 2010; Mikolov, Kombrink, Luk\u00e1\u0161 Burget, \u010cernocky, & Khudanpur, 2011; Mikolov, 2012; Duh, Neubig, Sudoh, & Tsukada, 2013; Adel, Vu, & Schultz, 2013; Auli, Galley, Quirk, & Zweig, 2013; Auli & Gao, 2014); as well as for sequence tagging (Irsoy & Cardie, 2014; Xu, Auli, & Clark, 2015; Ling, Dyer, Black, Trancoso, Fermandez, Amir, Marujo, & Luis, 2015b), machine translation (Sundermeyer, Alkhouli, Wuebker, & Ney, 2014; Tamura, Watanabe, & Sumita, 2014; Sutskever, Vinyals, & Le, 2014; Cho, van Merrienboer, Gulcehre, Bahdanau, Bougares, Schwenk, & Bengio, 2014b), dependency parsing (Dyer et al.", "startOffset": 97, "endOffset": 348}, {"referenceID": 36, "context": "Recurrent models have been shown to produce very strong results for language modeling, including (Mikolov, Karafi\u00e1t, Burget, Cernocky, & Khudanpur, 2010; Mikolov, Kombrink, Luk\u00e1\u0161 Burget, \u010cernocky, & Khudanpur, 2011; Mikolov, 2012; Duh, Neubig, Sudoh, & Tsukada, 2013; Adel, Vu, & Schultz, 2013; Auli, Galley, Quirk, & Zweig, 2013; Auli & Gao, 2014); as well as for sequence tagging (Irsoy & Cardie, 2014; Xu, Auli, & Clark, 2015; Ling, Dyer, Black, Trancoso, Fermandez, Amir, Marujo, & Luis, 2015b), machine translation (Sundermeyer, Alkhouli, Wuebker, & Ney, 2014; Tamura, Watanabe, & Sumita, 2014; Sutskever, Vinyals, & Le, 2014; Cho, van Merrienboer, Gulcehre, Bahdanau, Bougares, Schwenk, & Bengio, 2014b), dependency parsing (Dyer et al., 2015; Watanabe & Sumita, 2015), sentiment analysis (Wang, Liu, SUN, Wang, & Wang, 2015b), noisy text normalization (Chrupala, 2014), dialog state tracking (Mrk\u0161i\u0107, \u00d3 S\u00e9aghdha, Thomson, Gasic, Su, Vandyke, Wen, & Young, 2015), response generation (Sordoni, Galley, Auli, Brockett, Ji, Mitchell, Nie, Gao, & Dolan, 2015), and modeling the relation between character sequences and part-of-speech tags (Ling et al.", "startOffset": 730, "endOffset": 774}, {"referenceID": 18, "context": ", 2015; Watanabe & Sumita, 2015), sentiment analysis (Wang, Liu, SUN, Wang, & Wang, 2015b), noisy text normalization (Chrupala, 2014), dialog state tracking (Mrk\u0161i\u0107, \u00d3 S\u00e9aghdha, Thomson, Gasic, Su, Vandyke, Wen, & Young, 2015), response generation (Sordoni, Galley, Auli, Brockett, Ji, Mitchell, Nie, Gao, & Dolan, 2015), and modeling the relation between character sequences and part-of-speech tags (Ling et al.", "startOffset": 117, "endOffset": 133}, {"referenceID": 23, "context": "The majority of work (pioneered by (Collobert & Weston, 2008; Collobert et al., 2011; Chen & Manning, 2014)) advocate the use of dense, trainable embedding vectors for all features.", "startOffset": 35, "endOffset": 107}, {"referenceID": 147, "context": "Instead, distance features are encoded similarly to the other feature types: each bin is associated with a d-dimensional vector, and these distance-embedding vectors are then trained as regular parameters in the network (Zeng et al., 2014; dos Santos et al., 2015; Zhu et al., 2015a; Nguyen & Grishman, 2015).", "startOffset": 220, "endOffset": 308}, {"referenceID": 14, "context": "Using embeddings for representing not only words but arbitrary features was popularized following Chen and Manning (2014).", "startOffset": 98, "endOffset": 122}, {"referenceID": 26, "context": "In terms of representation power, it was shown by (Hornik, Stinchcombe, & White, 1989; Cybenko, 1989) that MLP1 is a universal approximator \u2013 it can approximate with any desired non-zero amount of error a family of functions8 that include all continuous functions", "startOffset": 50, "endOffset": 101}, {"referenceID": 108, "context": "The tanh cube activation function g(x) = tanh((x) + x) was proposed by (Pei et al., 2015), who found it to be more effective than other non-linearities in a feed-forward network that was used as a component in a structured-prediction graph-based dependency parser.", "startOffset": 71, "endOffset": 89}, {"referenceID": 9, "context": "For a detailed discussion on loss functions for neural networks see (LeCun, Chopra, Hadsell, Ranzato, & Huang, 2006; LeCun & Huang, 2005; Bengio et al., 2015).", "startOffset": 68, "endOffset": 158}, {"referenceID": 24, "context": "Hinge (multiclass) The hinge loss was extended to the multiclass setting by Crammer and Singer (2002). Let \u0177 = \u01771, .", "startOffset": 76, "endOffset": 102}, {"referenceID": 82, "context": "Log loss The log loss is a common variation of the hinge loss, which can be seen as a \u201csoft\u201d version of the hinge loss with an infinite margin (LeCun et al., 2006).", "startOffset": 143, "endOffset": 163}, {"referenceID": 41, "context": "An example of using the ranking log loss can be found in (Gao et al., 2014).", "startOffset": 57, "endOffset": 75}, {"referenceID": 132, "context": "Similarly, Van de Cruys (2014) used the ranking loss in a selectional-preferences task, in which the network was trained to rank correct verb-object pairs above incorrect, automatically derived ones, and (Weston, Bordes, Yakhnenko, & Usunier, 2013) trained a model to score correct (head,relation,trail) triplets above corrupted ones in an information-extraction setting.", "startOffset": 18, "endOffset": 31}, {"referenceID": 95, "context": "The method used by the effective word2vec implementation (Mikolov et al., 2013; Mikolov, Sutskever, Chen, Corrado, & Dean, 2013) is to initialize the word vectors to uniformly sampled random numbers in the range [\u2212 1 2d , 1 2d ] where d is the number of dimensions.", "startOffset": 57, "endOffset": 128}, {"referenceID": 53, "context": "While word similarity is hard to define and is usually very task-dependent, the current approaches derive from the distributional hypothesis (Harris, 1954), stating that words are similar if they appear in similar contexts.", "startOffset": 141, "endOffset": 155}, {"referenceID": 86, "context": "However, they are deeply connected to another family of algorithms which evolved in the NLP and IR communities, and that are based on matrix factorization (see (Levy & Goldberg, 2014b; Levy et al., 2015) for a discussion).", "startOffset": 160, "endOffset": 203}, {"referenceID": 113, "context": "See (Rong, 2014; Levy, Goldberg, & Dagan, 2015) for a discussion.", "startOffset": 4, "endOffset": 47}, {"referenceID": 95, "context": "Language-modeling inspired approaches such as those taken by (Mikolov et al., 2013; Mnih & Kavukcuoglu, 2013) as well as GloVe (Pennington et al.", "startOffset": 61, "endOffset": 109}, {"referenceID": 109, "context": ", 2013; Mnih & Kavukcuoglu, 2013) as well as GloVe (Pennington et al., 2014) use auxiliary tasks in which the goal is to predict the word given its context.", "startOffset": 51, "endOffset": 76}, {"referenceID": 95, "context": "Then, either a single task is created in which the goal is to predict the focus word based on all of the context words (represented either using CBOW (Mikolov et al., 2013) or vector concatenation (Collobert & Weston, 2008)), or 2k distinct tasks are created, each pairing the focus word with a different context word.", "startOffset": 150, "endOffset": 172}, {"referenceID": 95, "context": "The 2k tasks approach, popularized by (Mikolov et al., 2013) is referred to as a skip-gram model.", "startOffset": 38, "endOffset": 60}, {"referenceID": 95, "context": "Skip-gram based approaches are shown to be robust and efficient to train (Mikolov et al., 2013; Pennington et al., 2014), and often produce state of the art results.", "startOffset": 73, "endOffset": 120}, {"referenceID": 109, "context": "Skip-gram based approaches are shown to be robust and efficient to train (Mikolov et al., 2013; Pennington et al., 2014), and often produce state of the art results.", "startOffset": 73, "endOffset": 120}, {"referenceID": 86, "context": "Some of these hyperparameters (and others) are discussed in (Levy et al., 2015).", "startOffset": 60, "endOffset": 79}, {"referenceID": 31, "context": "dos Santos and Gatti (2014) and dos Santos and Zadrozny (2014) model the embedding of a word using a convolutional network (see Section 9) over the characters.", "startOffset": 4, "endOffset": 28}, {"referenceID": 31, "context": "dos Santos and Gatti (2014) and dos Santos and Zadrozny (2014) model the embedding of a word using a convolutional network (see Section 9) over the characters.", "startOffset": 4, "endOffset": 63}, {"referenceID": 31, "context": "dos Santos and Gatti (2014) and dos Santos and Zadrozny (2014) model the embedding of a word using a convolutional network (see Section 9) over the characters. Ling et al (2015b) model the embedding of a word using the concatenation of the final states of two RNN (LSTM) encoders (Section 10), one reading the characters from left to right, and the other from right to left.", "startOffset": 4, "endOffset": 179}, {"referenceID": 31, "context": "dos Santos and Gatti (2014) and dos Santos and Zadrozny (2014) model the embedding of a word using a convolutional network (see Section 9) over the characters. Ling et al (2015b) model the embedding of a word using the concatenation of the final states of two RNN (LSTM) encoders (Section 10), one reading the characters from left to right, and the other from right to left. Both produce very strong results for part-of-speech tagging. The work of Ballesteros et al (2015) show that the two-LSTMs encoding of (Ling et al.", "startOffset": 4, "endOffset": 473}, {"referenceID": 41, "context": "Gao et al (Gao et al., 2014) suggest using as core features not only the word form itself but also a unique feature (hence a unique embedding vector) for each of the letter-trigrams in the word.", "startOffset": 10, "endOffset": 28}, {"referenceID": 11, "context": "Botha and Blunsom (2014) suggest to model the embedding vector of a word as a sum of the word-specific vector if such vector is available, with vectors for the different morphological components that comprise it (the components are derived using Morfessor (Creutz & Lagus, 2007), an unsupervised morphological segmentation method).", "startOffset": 0, "endOffset": 25}, {"referenceID": 12, "context": "The common approach for training neural networks is using the stochastic gradient descent (SGD) algorithm (Bottou, 2012; LeCun, Bottou, Orr, & Muller, 1998a) or a variant of it.", "startOffset": 106, "endOffset": 157}, {"referenceID": 8, "context": "More generally, the backpropagation algorithm is a special case of the reverse-mode automatic differentiation algorithm (Neidinger, 2010, Section 7), (Baydin, Pearlmutter, Radul, & Siskind, 2015; Bengio, 2012).", "startOffset": 150, "endOffset": 209}, {"referenceID": 111, "context": "The SGD+Momentum (Polyak, 1964) and Nesterov Momentum (Sutskever, Martens, Dahl, & Hinton, 2013) algorithms are variants of SGD in which previous gradients are accumulated and affect the current update.", "startOffset": 17, "endOffset": 31}, {"referenceID": 146, "context": "Adaptive learning rate algorithms including AdaGrad (Duchi, Hazan, & Singer, 2011), AdaDelta (Zeiler, 2012),", "startOffset": 93, "endOffset": 107}, {"referenceID": 8, "context": "For most purposes, it is preferable to use automatic tools for gradient computation (Bengio, 2012).", "startOffset": 84, "endOffset": 98}, {"referenceID": 7, "context": "For further information on automatic differentiation see (Neidinger, 2010, Section 7), (Baydin et al., 2015).", "startOffset": 87, "endOffset": 108}, {"referenceID": 8, "context": "4), (Lecun et al., 1998b; Bengio, 2012).", "startOffset": 4, "endOffset": 39}, {"referenceID": 12, "context": "For various practical tips and recommendations, see (LeCun et al., 1998a; Bottou, 2012).", "startOffset": 52, "endOffset": 87}, {"referenceID": 8, "context": "An effective scheme due to Glorot and Bengio (2010), called xavier initialization after Glorot\u2019s first name, suggests initializing a weight matrix W \u2208 Rdin\u00d7dout as:", "startOffset": 38, "endOffset": 52}, {"referenceID": 8, "context": "The problem becomes more severe in deeper networks, and especially so in recursive and recurrent networks (Pascanu, Mikolov, & Bengio, 2012). Dealing with the vanishing gradients problem is still an open research question. Solutions include making the networks shallower, step-wise training (first train the first layers based on some auxiliary output signal, then fix them and train the upper layers of the complete network based on the real task signal), or specialized architectures that are designed to assist in gradient flow (e.g., the LSTM and GRU architectures for recurrent networks, discussed in Section 11). Dealing with the exploding gradients has a simple but very effective solution: clipping the gradients if their norm exceeds a given threshold. Let \u011d be the gradients of all parameters in the network, and \u2016\u011d\u2016 be their L2 norm. Pascanu et al (2012) suggest to set: \u011d\u2190 threshold \u2016\u011d\u2016 \u011d if \u2016\u011d\u2016 > threshold.", "startOffset": 127, "endOffset": 866}, {"referenceID": 12, "context": "L\u00e9on Bottou (2012) recommends using a learning rate of the form \u03b7t = \u03b70(1 + \u03b70\u03bbt) \u22121 where \u03b70 is the initial learning rate, \u03b7t is the learning rate to use on the tth training example, and \u03bb is an additional hyperparameter.", "startOffset": 5, "endOffset": 19}, {"referenceID": 23, "context": "Multi-task learning in the context of language-processing is introduced and discussed in (Collobert et al., 2011).", "startOffset": 89, "endOffset": 113}, {"referenceID": 105, "context": "Examples of this approach are left-to-right tagging models (Gim\u00e9nez & M\u00e0rquez, 2004) and greedy transition-based parsing (Nivre, 2008).", "startOffset": 121, "endOffset": 134}, {"referenceID": 118, "context": "For indepth discussion of search-based structure prediction in NLP, see the book by Smith (Smith, 2011).", "startOffset": 90, "endOffset": 103}, {"referenceID": 118, "context": ", (Smith, 2011), such as cost-augmented decoding, can be easily applied or adapted to the neural-network framework.", "startOffset": 2, "endOffset": 15}, {"referenceID": 118, "context": "Probabilistic objective (CRF) In a probabilistic framework (\u201cCRF\u201d), we treat each of the parts scores as a clique potential (see (Smith, 2011)) and define the score of each structure y to be:", "startOffset": 129, "endOffset": 142}, {"referenceID": 35, "context": "A hinge based approached was used by Pei et al (2015) for arc-factored dependency parsing, and the probabilistic approach by Durrett and Klein (Durrett & Klein, 2015) for a CRF constituency parser. The approximate beam-based partition function was effectively used by Zhou et al (2015) in a transition based parser.", "startOffset": 125, "endOffset": 286}, {"referenceID": 120, "context": "Works using the reranking approach include (Socher et al., 2013; Auli et al., 2013; Le & Zuidema, 2014; Zhu et al., 2015a)", "startOffset": 43, "endOffset": 122}, {"referenceID": 3, "context": "Works using the reranking approach include (Socher et al., 2013; Auli et al., 2013; Le & Zuidema, 2014; Zhu et al., 2015a)", "startOffset": 43, "endOffset": 122}, {"referenceID": 140, "context": "In particular, Weiss et al (Weiss et al., 2015) report strong results for transition-based dependency parsing in a two-stage model.", "startOffset": 27, "endOffset": 47}, {"referenceID": 20, "context": "In the second stage, the neural network model is held fixed, and the different layers (output as well as hidden layer vectors) for each input are then concatenated and used as the input features of a linear structured perceptron model (Collins, 2002) that is trained to perform beam-search for the best resulting structure.", "startOffset": 235, "endOffset": 250}, {"referenceID": 74, "context": "Convolution-and-pooling architectures (LeCun & Bengio, 1995) evolved in the neural networks vision community, where they showed great success as object detectors \u2013 recognizing an object from a predefined category (\u201ccat\u201d, \u201cbicycles\u201d) regardless of its position in the image (Krizhevsky et al., 2012).", "startOffset": 273, "endOffset": 298}, {"referenceID": 72, "context": "Convolutional networks were introduced to the NLP community in the pioneering work of Collobert, Weston and Colleagues (2011) who used them for semantic-role labeling, and later by Kalchbrenner et al (2014) and Kim (Kim, 2014) who used them for sentiment and question-type classification.", "startOffset": 215, "endOffset": 226}, {"referenceID": 8, "context": "Convolution-and-pooling architectures (LeCun & Bengio, 1995) evolved in the neural networks vision community, where they showed great success as object detectors \u2013 recognizing an object from a predefined category (\u201ccat\u201d, \u201cbicycles\u201d) regardless of its position in the image (Krizhevsky et al., 2012). When applied to images, the architecture is using 2dimensional (grid) convolutions. When applied to text, NLP we are mainly concerned with 1-d (sequence) convolutions. Convolutional networks were introduced to the NLP community in the pioneering work of Collobert, Weston and Colleagues (2011) who used them for semantic-role labeling, and later by Kalchbrenner et al (2014) and Kim (Kim, 2014) who used them for sentiment and question-type classification.", "startOffset": 47, "endOffset": 594}, {"referenceID": 8, "context": "Convolution-and-pooling architectures (LeCun & Bengio, 1995) evolved in the neural networks vision community, where they showed great success as object detectors \u2013 recognizing an object from a predefined category (\u201ccat\u201d, \u201cbicycles\u201d) regardless of its position in the image (Krizhevsky et al., 2012). When applied to images, the architecture is using 2dimensional (grid) convolutions. When applied to text, NLP we are mainly concerned with 1-d (sequence) convolutions. Convolutional networks were introduced to the NLP community in the pioneering work of Collobert, Weston and Colleagues (2011) who used them for semantic-role labeling, and later by Kalchbrenner et al (2014) and Kim (Kim, 2014) who used them for sentiment and question-type classification.", "startOffset": 47, "endOffset": 675}, {"referenceID": 70, "context": "Depending on whether we pad the sentence with k \u2212 1 words to each side, we may get either m = n\u2212k+1 (narrow convolution) or m = n+k+1 windows (wide convolution) (Kalchbrenner et al., 2014).", "startOffset": 161, "endOffset": 188}, {"referenceID": 15, "context": "We could argue that the words before the first word, the words after the second word, and the words between them provide three different kinds of information (Chen et al., 2015).", "startOffset": 158, "endOffset": 177}, {"referenceID": 70, "context": "Finally, (Kalchbrenner et al., 2014) introduced a k-max pooling operation, in which the top k values in each dimension are retained instead of only the best one, while preserving the order in which they appeared in the text.", "startOffset": 9, "endOffset": 36}, {"referenceID": 70, "context": "It can also discern more finely the number of times the feature is highly activated (Kalchbrenner et al., 2014).", "startOffset": 84, "endOffset": 111}, {"referenceID": 72, "context": "The result of each convolutional layer will then be pooled, and the resulting vectors concatenated and fed to further processing (Kim, 2014).", "startOffset": 129, "endOffset": 140}, {"referenceID": 72, "context": "The result of each convolutional layer will then be pooled, and the resulting vectors concatenated and fed to further processing (Kim, 2014). The convolutional architecture need not be restricted into the linear ordering of a sentence. For example, Ma et al (2015) generalize the convolution operation to work over syntactic dependency trees.", "startOffset": 130, "endOffset": 265}, {"referenceID": 72, "context": "The result of each convolutional layer will then be pooled, and the resulting vectors concatenated and fed to further processing (Kim, 2014). The convolutional architecture need not be restricted into the linear ordering of a sentence. For example, Ma et al (2015) generalize the convolution operation to work over syntactic dependency trees. There, each window is around a node in the syntactic tree, and the pooling is performed over the different nodes. Similarly, Liu et al (2015) apply a convolutional architecture on top of dependency paths extracted from dependency trees.", "startOffset": 130, "endOffset": 485}, {"referenceID": 72, "context": "The result of each convolutional layer will then be pooled, and the resulting vectors concatenated and fed to further processing (Kim, 2014). The convolutional architecture need not be restricted into the linear ordering of a sentence. For example, Ma et al (2015) generalize the convolution operation to work over syntactic dependency trees. There, each window is around a node in the syntactic tree, and the pooling is performed over the different nodes. Similarly, Liu et al (2015) apply a convolutional architecture on top of dependency paths extracted from dependency trees. Le and Zuidema (2015) propose to perform max pooling over vectors representing the different derivations leading to the same chart item in a chart parser.", "startOffset": 130, "endOffset": 602}, {"referenceID": 37, "context": "Recurrent neural networks (RNNs) (Elman, 1990) allow representing arbitrarily sized structured inputs in a fixed-size vector, while paying attention to the structured properties of the input.", "startOffset": 33, "endOffset": 46}, {"referenceID": 141, "context": "This procedure is referred to in the RNN literature as backpropagation through time, or BPTT (Werbos, 1990).", "startOffset": 93, "endOffset": 107}, {"referenceID": 143, "context": "A CCG super-tagger based on such an architecture provides state-of-the art CCG super-tagging results (Xu et al., 2015).", "startOffset": 101, "endOffset": 118}, {"referenceID": 97, "context": "language models are shown to provide much better perplexities than traditional language models (Mikolov et al., 2010; Sundermeyer, Schl\u00fcter, & Ney, 2012; Mikolov, 2012).", "startOffset": 95, "endOffset": 168}, {"referenceID": 100, "context": "language models are shown to provide much better perplexities than traditional language models (Mikolov et al., 2010; Sundermeyer, Schl\u00fcter, & Ney, 2012; Mikolov, 2012).", "startOffset": 95, "endOffset": 168}, {"referenceID": 130, "context": "Encoder - Decoder Finally, an important special case of the encoder scenario is the Encoder-Decoder framework (Cho, van Merrienboer, Bahdanau, & Bengio, 2014a; Sutskever et al., 2014).", "startOffset": 110, "endOffset": 183}, {"referenceID": 130, "context": "Such an approach was shown to be surprisingly effective for Machine Translation (Sutskever et al., 2014) using LSTM RNNs.", "startOffset": 80, "endOffset": 104}, {"referenceID": 63, "context": "Irsoy and Cardie (2014) also report improved results from moving from a onelayer BI-RNN to an architecture with several layers.", "startOffset": 0, "endOffset": 24}, {"referenceID": 50, "context": "A useful elaboration of an RNN is a bidirectional-RNN (BI-RNN) (Schuster & Paliwal, 1997; Graves, 2008).", "startOffset": 63, "endOffset": 103}, {"referenceID": 50, "context": "A useful elaboration of an RNN is a bidirectional-RNN (BI-RNN) (Schuster & Paliwal, 1997; Graves, 2008).31 Consider the task of sequence tagging over a sentence x1, . . . , xn. An RNN allows us to compute a function of the ith word xi based on the past \u2013 the words x1:i up to and including it. However, the following words xi:n may also be useful for prediction, as is evident by the common sliding-window approach in which the focus word is categorized based on a window of k words surrounding it. Much like the RNN relaxes the Markov assumption and allows looking arbitrarily back into the past, the BI-RNN relaxes the fixed window size assumption, allowing to look arbitrarily far at both the past and the future. Consider an input sequence x1:n. The BI-RNN works by maintaining two separate states, si and s b i for each input position i. The forward state s f i is based on x1,x2, . . . ,xi, while the backward state si is based on xn,xn\u22121, . . . ,xi. The forward and backward states are generated by two different RNNs. The first RNN (Rf , Of ) is fed the input sequence x1:n as is, while the second RNN (R b, Ob) is fed the input sequence in reverse. The state representation si is then composed of both the forward and backward states. The output at position i is based on the concatenation of the two output vectors yi = [y f i ; y b i ] = [O f (si );O (si )], taking into account both the past and the future. The vector yi can then be used directly for prediction, or fed as part of the input to a more complex network. While the two RNNs are run independently of each other, the error gradients at position i will flow both forward and backward through the two RNNs. A visual representation of the BI-RNN architecture is given in Figure 11. The use of BI-RNNs for sequence tagging was introduced to the NLP community by Irsoy and Cardie (2014).", "startOffset": 90, "endOffset": 1856}, {"referenceID": 105, "context": "Some algorithms in language processing, including those for transition-based parsing (Nivre, 2008), require performing feature extraction over a stack.", "startOffset": 85, "endOffset": 98}, {"referenceID": 106, "context": "Dealing with pop operation is more challenging, but can be solved by using the persistent-stack data-structure (Okasaki, 1999; Goldberg, Zhao, & Huang, 2013).", "startOffset": 111, "endOffset": 157}, {"referenceID": 36, "context": "This modeling approach was proposed independently by Dyer et al and Watanabe et al (Dyer et al., 2015; Watanabe & Sumita, 2015) for transition-based dependency parsing.", "startOffset": 83, "endOffset": 127}, {"referenceID": 37, "context": "The simplest RNN formulation, known as an Elman Network or Simple-RNN (S-RNN), was proposed by Elman (1990) and explored for use in language modeling by Mikolov (2012).", "startOffset": 42, "endOffset": 108}, {"referenceID": 37, "context": "The simplest RNN formulation, known as an Elman Network or Simple-RNN (S-RNN), was proposed by Elman (1990) and explored for use in language modeling by Mikolov (2012). The S-RNN takes the following form:", "startOffset": 42, "endOffset": 168}, {"referenceID": 143, "context": "32 In spite of its simplicity, the Simple RNN provides strong results for sequence tagging (Xu et al., 2015) as well as language modeling.", "startOffset": 91, "endOffset": 108}, {"referenceID": 100, "context": "For comprehensive discussion on using Simple RNNs for language modeling, see the PhD thesis by Mikolov (2012).", "startOffset": 95, "endOffset": 110}, {"referenceID": 71, "context": "34 For an analysis of the behavior of an LSTM when used as a character-level language model, see (Karpathy et al., 2015).", "startOffset": 97, "endOffset": 120}, {"referenceID": 50, "context": "For further discussion on the LSTM architecture see the PhD thesis by Alex Graves (2008), as well as Chris Olah\u2019s description.", "startOffset": 75, "endOffset": 89}, {"referenceID": 113, "context": "Practical Considerations When training LSTM networks, Jozefowicz et al (2015) strongly recommend to always initialize the bias term of the forget gate to be close to one. When applying dropout to an RNN with an LSTM, Zaremba et al (2014) found out that it is", "startOffset": 80, "endOffset": 238}, {"referenceID": 69, "context": "For an empirical exploration of the GRU and the LSTM architectures, see (Jozefowicz et al., 2015).", "startOffset": 72, "endOffset": 97}, {"referenceID": 100, "context": "Mikolov et al (2014) observed that the matrix multiplication si\u22121W coupled with the nonlinearity g in the update rule R of the Simple RNN causes the state vector si to undergo large changes at each time step, prohibiting it from remembering information over long time periods.", "startOffset": 0, "endOffset": 21}, {"referenceID": 96, "context": "We depart from the notation in (Mikolov et al., 2014) and reuse the symbols used in the LSTM description.", "startOffset": 31, "endOffset": 53}, {"referenceID": 100, "context": "Mikolov et al demonstrate that this architecture provides competitive perplexities to the much more complex LSTM on language modeling tasks. The approach of Mikolov et al can be interpreted as constraining the block of the matrix Ws in the S-RNN corresponding to ci to be a multiply of the identity matrix (see Mikolov et al (2014) for the details).", "startOffset": 0, "endOffset": 332}, {"referenceID": 120, "context": "The trees can be syntactic trees, discourse trees, or even trees representing the sentiment expressed by various parts of a sentence (Socher et al., 2013).", "startOffset": 133, "endOffset": 154}, {"referenceID": 110, "context": "The recursive neural network (RecNN) abstraction (Pollack, 1990), popularized in NLP by Richard Socher and colleagues (Socher, Manning, & Ng, 2010; Socher, Lin, Ng, & Manning, 2011; Socher et al.", "startOffset": 49, "endOffset": 64}, {"referenceID": 120, "context": "The recursive neural network (RecNN) abstraction (Pollack, 1990), popularized in NLP by Richard Socher and colleagues (Socher, Manning, & Ng, 2010; Socher, Lin, Ng, & Manning, 2011; Socher et al., 2013; Socher, 2014) is a generalization of the RNN from sequences to (binary) trees.", "startOffset": 118, "endOffset": 216}, {"referenceID": 119, "context": "The recursive neural network (RecNN) abstraction (Pollack, 1990), popularized in NLP by Richard Socher and colleagues (Socher, Manning, & Ng, 2010; Socher, Lin, Ng, & Manning, 2011; Socher et al., 2013; Socher, 2014) is a generalization of the RNN from sequences to (binary) trees.", "startOffset": 118, "endOffset": 216}, {"referenceID": 120, "context": "An alternative approach, due to (Socher et al., 2013) is to untie the weights according to the non-terminals, using a different composition matrix for each B,C pair of symbols:39", "startOffset": 32, "endOffset": 53}, {"referenceID": 76, "context": "Le and Zuidema (2014) extend the RecNN definition such that each node has, in addition to its inside state vector, also an outside state vector representing the entire structure around the subtree rooted at that node.", "startOffset": 0, "endOffset": 22}, {"referenceID": 54, "context": "A similar model was also used by (Hashimoto et al., 2013) to encode subtrees in semantic-relation classification task.", "startOffset": 33, "endOffset": 57}, {"referenceID": 120, "context": "Other proposed variants on tree-structured RNNs includes a recursive matrix-vector model (Socher, Huval, Manning, & Ng, 2012) and recursive neural tensor network (Socher et al., 2013).", "startOffset": 162, "endOffset": 183}, {"referenceID": 119, "context": "For further discussion on recursive neural networks and their use in natural language tasks, refer to the PhD thesis of Richard Socher (2014).", "startOffset": 128, "endOffset": 142}], "year": 2015, "abstractText": "Over the past few years, neural networks have re-emerged as powerful machine-learning models, yielding state-of-the-art results in fields such as image recognition and speech processing. More recently, neural network models started to be applied also to textual natural language signals, again with very promising results. This tutorial surveys neural network models from the perspective of natural language processing research, in an attempt to bring natural-language researchers up to speed with the neural techniques. The tutorial covers input encoding for natural language tasks, feed-forward networks, convolutional networks, recurrent networks and recursive networks, as well as the computation graph abstraction for automatic gradient computation.", "creator": "TeX"}}}