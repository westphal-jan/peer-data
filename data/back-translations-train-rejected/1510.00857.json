{"id": "1510.00857", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "3-Oct-2015", "title": "Approximate Fisher Kernels of non-iid Image Models for Image Categorization", "abstract": "The bag-of-words (BoW) model treats images as sets of local descriptors and represents them by visual word histograms. The Fisher vector (FV) representation extends BoW, by considering the first and second order statistics of local descriptors. In both representations local descriptors are assumed to be identically and independently distributed (iid), which is a poor assumption from a modeling perspective. It has been experimentally observed that the performance of BoW and FV representations can be improved by employing discounting transformations such as power normalization. In this paper, we introduce non-iid models by treating the model parameters as latent variables which are integrated out, rendering all local regions dependent. Using the Fisher kernel principle we encode an image by the gradient of the data log-likelihood w.r.t. the model hyper-parameters. Our models naturally generate discounting effects in the representations; suggesting that such transformations have proven successful because they closely correspond to the representations obtained for non-iid models. To enable tractable computation, we rely on variational free-energy bounds to learn the hyper-parameters and to compute approximate Fisher kernels. Our experimental evaluation results validate that our models lead to performance improvements comparable to using power normalization, as employed in state-of-the-art feature aggregation methods.", "histories": [["v1", "Sat, 3 Oct 2015 19:35:38 GMT  (1089kb,D)", "http://arxiv.org/abs/1510.00857v1", "IEEE Transactions on Pattern Analysis and Machine Intelligence, in press, 2015"]], "COMMENTS": "IEEE Transactions on Pattern Analysis and Machine Intelligence, in press, 2015", "reviews": [], "SUBJECTS": "cs.CV cs.LG", "authors": ["ramazan gokberk cinbis", "jakob verbeek", "cordelia schmid"], "accepted": false, "id": "1510.00857"}, "pdf": {"name": "1510.00857.pdf", "metadata": {"source": "CRF", "title": "Approximate Fisher Kernels of non-iid Image Models for Image Categorization", "authors": ["Ramazan Gokberk Cinbis", "Jakob Verbeek"], "emails": ["firstname.lastname@inria.fr"], "sections": [{"heading": null, "text": "Index terms - Statistical image representation, object recognition, image classification, Fisher cores"}, {"heading": "1 INTRODUCTION", "text": "We are widely used in image categorization and retrieval systems. BoW descriptor represents an image as a histogram of visual word numbers. However, histograms are constructed by mapping local feature vectors in images to cluster indexes, where clustering is typically learned using k-means. Perronnin and Dance [38] have improved this basic representation using the term Fisher kernel [20]. In this case, local descriptors are associated with the components of a mixture of Gaussian (MoG) density, and the image is based on the gradient of the log likelihood of local descriptors w.r.The MoG parameters show that both BoW and MoG Fisher vector representations are based on models that assume that local descriptors are independently and identically distributed."}, {"heading": "2 RELATED WORK", "text": "The use of non-linear feature transformations in BoW image representations is widely recognized as beneficial for image categorization [22], [39], [52], [56], [56]. These transformations mitigate an obvious lack of linear classifiers on BoW image representations: the fact that a fixed change in a BoW histogram, from h to h + 0, results in a score increment that appears independent of the original histogram. [f (h) \u2212 f (h) > (h + \u2206) \u2212 w >. This means that the effect on the score for a change is not dependent on the context h in which it appears. Therefore, the score increment from images (a) will be comparable to what is not desirable in Figure 2: the classic score for the cow should rise significantly and then remain stable between (b), (c) and (d)."}, {"heading": "3 FISHER VECTORS AND VARIATIONAL APPROXIMATION", "text": "In this section, we present an overview of the Fisher kernel framework, variation conclusions, and the varied Fisher kernel."}, {"heading": "3.1 Fisher vectors", "text": "Images can be considered samples from a generative process, and therefore class-based generative models are used for image categorization. However, discriminatory classifiers are widely observed to typically outperform generative-based classification, see, for example, [17]. A simple explanation is that discriminatory classifiers aim to maximize the end goal, which is to categorize entities based on their content. By contrast, generative classifiers require the modeling of class-based data distributions, which is arguably a more difficult task than just learning surfaces, and thus leads to poorer categorization performance. The Fisher Kernel Framework proposed by Jaakkola and Haussler [20] allows for combining the power of generative models and discriminatory classifiers. Specifically, Fisher Kernel provides a framework for deriving a core from a probable model."}, {"heading": "3.2 Variational approximate inference", "text": "Variation methods are a family of mathematical tools that can be used to illustrate irrevocable calculations, especially those involving difficult integrals. Originally developed in statistical physics on the basis of the calculus of variation and the theory of intermediates, the variational approximation framework we use in this work is known as the variational inference q (q) q (q), and it is now one of the most successful approximate inference techniques [2], [24]. In the context of probability models, the central idea in the variational methods is a limit to the log likelihood function in relation to an approximate posterior distribution."}, {"heading": "3.3 Variational Fisher kernel", "text": "In this paper, we use the variational limits of free energy for two purposes. The first is to estimate the hyperparameters of the LDA (Section 4.2) and the latent MoG (Section 4.3) models using an approximate maximum probability method. To this end, we iteratively update the variation parameters with respect to the variation parameters and the model of the hyperparameters; an approach known as the variation maintenance maximization method [24]. Our second main use of the variable free energy is the calculation of approximate Fisher vectors where the original Fisher vector is intractable in order to calculate. In particular, we approach the Fisher vector by the gradient of the lower variable limit given by Eq. (6), i.e (x). We assume that we have the variation parameters F (p, q), which we call the variable Fisher vector. Since the entropy (H) is constant."}, {"heading": "4 NON-IID IMAGE REPRESENTATIONS", "text": "In this section we present our non-id models for local image descriptors. We start with a model for BoW quantization indexes and extend the model in Section 4.2 to gather statistics on the incidence of visual words using LDA. Finally, in Section 4.3 we consider a non-id extension of the mix of Gaussian models over groups of local descriptors."}, {"heading": "4.1 Bag-of-words and the multivariate Po\u0301lya model", "text": "The default BoW representation can be interpreted as applying the Fisher kernel framework to a simple iid multinomial model that goes beyond visual word indexes, as shown in [27]. Let us show the visual word indexes in an image, and let it appear as a learned multinomial model, in which the visual word indexes in log space, i.e, p (wi = k), p, p, p, p, p, p, p, p, exp, exp, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, let us show the visual word indexes in a picture."}, {"heading": "4.2 Capturing co-occurrence with topic models", "text": "The model shows that more visual words of the type it has seen before can only be seen in our second model. In our second model, we extend the Po-lya model to capture co-occurrence statistics of visual words using latent Dirichlet allocation (LDA) [3]. We model the visual words in an image as a mixture of T-themes, represented by a multinomial mixing of the themes themselves, with each topic represented by a multinomial distribution. We associate a variable zi drawn by each patch indicating which theme was used to draw its visual word."}, {"heading": "4.3 Modeling descriptors using latent MoG models", "text": "In this section, we turn to the representation of Perronnin and Dance that the Fisher Kernel Framework applies to the mixing of Gaussian (MoG) models via local descriptors; an improved version of this representation using power normalization has been presented in [40]. K-Gaussian components of the mix correspond to the K-Visual Words in a BoW model; in [38], local descriptors across images are taken as iid examples from a single MoG model; they represent an image through the gradient of the log likelihood of the extracted local descriptors x1: N w.r.t. Using the model parameters of the softassignments p (k) = N-id models underlying all images, they represent an image through the gradient of the log likelihood of the local descriptors x1: N w.r.t."}, {"heading": "5 EXPERIMENTAL EVALUATION", "text": "In this section, we present a detailed evaluation of the latent BOW, LDA, and latent MoG models versus local SIFT descriptors using the PASCAL dataset VOC '07 [14] in Section 5.2, Section 5.3, and Section 5.4, respectively. We then present an empirical study of the relationship between model probability and image categorization performance in Section 5.5. Finally, we evaluate the latent MoG model, which is the most advanced model we consider compared to CNN-based local descriptors, and compare it with the state of the art on the PASCAL VOC' 07 and MIT Indoor [41] datasets in Section 5.6.Now, we first describe our experimental setup for the SIFT-based experiments used in the following sections."}, {"heading": "5.1 Experimental setup", "text": "To extract SIFT descriptors, we use the experimental setup described in the evaluation paper by Chatfield et al. [6]: We project local SIFT descriptors from the same dense grid (3 pixel step, over 4 scales), resulting in approximately 60,000 pixels, project the local descriptors to 80 dimensions with PCA, and train the visual vocabulary of 1.5 x 106 descriptors. For the PASCAL-VOC '07 dataset, we use the interpolated mAP score specified by the VOC evaluation protocol [14]. We compare global image representations and representations that combine spatial layout by concatenating signatures calculated using different spatial cells, as in the spatial pyramid fit (SPM matching) method [30]. Again, we follow [6] and combine one \u00d7 2, one \u00d7 2."}, {"heading": "5.2 Evaluating BoW and Po\u0301lya models", "text": "In Table 1, we compare the results obtained with the help of standard BoW histograms, two types of power-normalized histograms, and the Po'lya model. In all three cases, we generate the visual number of words from soft assignments of patches to the MoG components. Overall, we see that the spatial information of the SPM is useful, and that larger vocabulary increases performance. We observe that both the power normalization and the Po'lya model consistently improve the BoW representation across all dictionary sizes and with or without SPM. Furthermore, the Po'lya model generally leads to greater improvements than visual normalization. These results are consistent with the observation in Section 4.1 that the non-iid Po'lya model generates similar transformations on BoW histograms, with or without SPM. Furthermore, the Po'lya model generally leads to greater improvements than visual normalization. These results are consistent with the observation in that the non-iid Po'lya model generates similar transformations on BoW histograms, such as the power normalization, and show that normalization by the Digamma function is at least as effective as the power-iid Po'lya function corresponds to the power-64 normalization illustrates the stronger display of the Po's function in the Po'lyk function."}, {"heading": "5.3 Evaluating topic model representations", "text": "We compare different topic model representations of Section 4.2: Fisher vectors calculated on the PLSA model, its performance normalized version, and use the corresponding latent variable LDA model. We compare the corresponding BoW representations and include SPM in all experiments. For the sake of brevity, we report only on cross-validation-based performance normalization, since squaring produces similar results. To train LDA models, we first train a PLSA model and then adjust Dirichlet priors to the topic and document topic distributions derived from PLSA. In Figure 9, we look at topic models that use T = 2 topics for different dictionary sizes, and in Figure 10, we use dictionaries of K = 1024 visual words and consider performance as a function of the number of topics. We note that (i) topic models consistently improve performance over BoW models, and (ii) the simple PLDA representations are exceeded by the previous version of the normalized performance and performance of the model."}, {"heading": "5.4 Evaluating latent MoG model", "text": "In fact, the majority of them are able to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to fight, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to fight, to move, to move, to move, to move, to move, to move, to move, to move, to move, to fight, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to fight, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move,"}, {"heading": "5.5 Relationship between model likelihood and categorization performance", "text": "We have seen that the Fisher vectors of our non-iid image models provide significantly better image classification performance than the Fisher vectors of the corresponding iid models, unless performance normalization is used to implement a discounting transformation on the image descriptors. More broadly, our experimental results suggest that Fisher cores, in combination with more powerful generative models, may lead to better image categorization. To investigate the relationship between the image models and categorization performance using the corresponding Fisher vectors, we propose empirically examining the MoG models and the corresponding image descriptors at a number of PCA projection measures (D) and vocabulary sizes (K), using the log likelihood of each model on a validation set as a measure of the generative performance of the models and the image categorization of the corresponding image descriptors."}, {"heading": "5.6 Experiments using CNN features", "text": "In this section, we evaluate the latent MoG representation based on local descriptors extracted from a Convolutionary Neural Network (CNN). [16] and Liu et al. [31], and extract local descriptors by feeding truncated regions to a CNN model. Second, inspired by the R-CNN object detector [15], we propose to extract local CNN properties for the image regions generated from a candidate window. In contrast to the R-CNN detector, we use the region descriptors to extract image descriptors rather than evaluate individual regions as detection candidates."}, {"heading": "6 CONCLUSIONS", "text": "In this paper, we have introduced latent variables for local image descriptors that avoid common but unrealistic assumptions. Fisher vectors of our non-iidal models are calculated from the same adequate statistics used to calculate fish vectors of the corresponding iiid models. In fact, these functions are similar to the transformations used in previous work, in the way that the performance description, or the significant square root. Our models provide an explanation for the success of such transformations, as we derive them by removing the unrealistic iid assumptions from the popular BoW and MoG models. Second, we have shown that gradients of variant free energy are bound, there is an exact Fisher score as long as the varied posterior distribution is. Third, we have shown that approximate Fisher vectors are successfully extracted for the proposed latent models."}], "references": [{"title": "Weakly supervised object detection with posterior regularization", "author": ["H. Bilen", "M. Pedersoli", "T. Tuytelaars"], "venue": "In British Machine Vision Conference,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2014}, {"title": "Pattern recognition and machine learning", "author": ["C. Bishop"], "venue": "Spinger-Verlag,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2006}, {"title": "Latent Dirichlet allocation", "author": ["D. Blei", "A. Ng", "M. Jordan"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2003}, {"title": "Using Fisher kernels from topic models for dimensionality reduction", "author": ["G. Chandalia", "M. Beal"], "venue": "In NIPS Workshop on Novel Applications of Dimensionality Reduction,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2006}, {"title": "PLSI: The true Fisher kernel and beyond", "author": ["J.C. Chappelier", "E. Eckard"], "venue": "In Machine Learning and Knowledge Discovery in Databases,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2009}, {"title": "The devil is in the details: an evaluation of recent feature encoding methods", "author": ["K. Chatfield", "V. Lempitsky", "A. Vedaldi", "A. Zisserman"], "venue": "In British Machine Vision Conference,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2011}, {"title": "Return of the devil in the details: Delving deep into convolutional nets", "author": ["K. Chatfield", "K. Simonyan", "A. Vedaldi", "A. Zisserman"], "venue": "In British Machine Vision Conference,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2014}, {"title": "Image categorization using Fisher kernels of non-iid image models", "author": ["R.G. Cinbis", "J. Verbeek", "C. Schmid"], "venue": "In IEEE Conference on Computer Vision and Pattern Recognition,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2012}, {"title": "Segmentation driven object detection with Fisher vectors", "author": ["R.G. Cinbis", "J. Verbeek", "C. Schmid"], "venue": "In International Conference on Computer Vision,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2013}, {"title": "Visual categorization with bags of keypoints", "author": ["G. Csurka", "C. Dance", "L. Fan", "J. Willamowski", "C. Bray"], "venue": "In ECCV Int. Workshop on Stat. Learning in Computer Vision,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2004}, {"title": "Imagenet: A large-scale hierarchical image database", "author": ["J. Deng", "W. Dong", "R. Socher", "L.-J. Li", "K. Li", "L. Fei-Fei"], "venue": "In IEEE Conference on Computer Vision and Pattern Recognition,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2009}, {"title": "Mid-level Visual Element Discovery as Discriminative Mode Seeking", "author": ["C. Doersch", "A. Gupta", "A.A. Efros"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2013}, {"title": "Deriving TF-IDF as a Fisher kernel", "author": ["C. Elkan"], "venue": "In String Processing and Information Retrieval,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2005}, {"title": "The Pascal Visual Object Classes Challenge", "author": ["M. Everingham", "S.M.A. Eslami", "L. Van Gool", "C.K.I. Williams", "J. Winn", "A. Zisserman"], "venue": "International Journal on Computer Vision,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2015}, {"title": "Rich feature hierarchies for accurate object detection and semantic segmentation", "author": ["R. Girshick", "J. Donahue", "T. Darrell", "J. Malik"], "venue": "In IEEE Conference on Computer Vision and Pattern Recognition,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2013}, {"title": "Multi-scale Orderless Pooling of Deep Convolutional Activation Features", "author": ["Y. Gong", "L. Wang", "R. Guo", "S. Lazebnik"], "venue": "In European Conference on Computer Vision,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2014}, {"title": "The unreasonable effectiveness of data", "author": ["A. Halevy", "P. Norvig", "F. Pereira"], "venue": "Intelligent Systems, IEEE,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2009}, {"title": "Learning the similarity of documents: An informationgeometric approach to document retrieval and categorization", "author": ["T. Hofmann"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 1999}, {"title": "Unsupervised learning by probabilistic latent semantic analysis", "author": ["T. Hofmann"], "venue": "Machine Learning,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2001}, {"title": "Exploiting generative models in discriminative classifiers", "author": ["T. Jaakkola", "D. Haussler"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 1999}, {"title": "On the burstiness of visual elements", "author": ["H. J\u00e9gou", "M. Douze", "C. Schmid"], "venue": "In IEEE Conference on Computer Vision and Pattern Recognition,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2009}, {"title": "Aggregating local image descriptors into compact codes", "author": ["H. J\u00e9gou", "F. Perronnin", "M. Douze", "J. S\u00e1nchez", "P. P\u00e9rez", "C. Schmid"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2012}, {"title": "Caffe: Convolutional architecture for fast feature embedding", "author": ["Y. Jia", "E. Shelhamer", "J. Donahue", "S. Karayev", "J. Long", "R. Girshick", "S. Guadarrama", "T. Darrell"], "venue": "arXiv preprint arXiv:1408.5093,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2014}, {"title": "An introduction to variational methods for graphical models", "author": ["M. Jordan", "Z. Ghahramani", "T. Jaakola", "L. Saul"], "venue": "Machine Learning,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 1999}, {"title": "Blocks that shout: Distinctive parts for scene classification", "author": ["M. Juneja", "A. Vedaldi", "C.V. Jawahar", "A. Zisserman"], "venue": "In IEEE Conference on Computer Vision and Pattern Recognition,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2013}, {"title": "Dirichlet-based histogram feature transform for image classification", "author": ["T. Kobayashi"], "venue": "In IEEE Conference on Computer Vision and Pattern Recognition,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2014}, {"title": "Modeling spatial layout with Fisher vectors for image categorization", "author": ["J. Krapac", "J. Verbeek", "F. Jurie"], "venue": "In International Conference on Computer Vision,", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2011}, {"title": "Imagenet classification with deep convolutional neural networks", "author": ["A. Krizhevsky", "I. Sutskever", "G. Hinton"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2012}, {"title": "Latent mixture vocabularies for object categorization and segmentation", "author": ["D. Larlus", "F. Jurie"], "venue": "Image and Vision Computing,", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2009}, {"title": "Beyond bags of features: spatial pyramid matching for recognizing natural scene categories", "author": ["S. Lazebnik", "C. Schmid", "J. Ponce"], "venue": "In IEEE Conference on Computer Vision and Pattern Recognition,", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2006}, {"title": "Encoding high dimensional local features by sparse coding based Fisher vectors", "author": ["L. Liu", "C. Shen", "L. Wang", "A. van den Hengel", "C. Wang"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2014}, {"title": "Information theory, inference, and learning algorithms", "author": ["D.J. MacKay"], "venue": null, "citeRegEx": "32", "shortCiteRegEx": "32", "year": 2003}, {"title": "Modeling word burstiness using the Dirichlet distribution", "author": ["R. Madsen", "D. Kauchak", "C. Elkan"], "venue": "In International Conference on Machine Learning,", "citeRegEx": "33", "shortCiteRegEx": "33", "year": 2005}, {"title": "Estimating a Dirichlet distribution", "author": ["T. Minka"], "venue": "http://research.microsoft. com/en-us/um/people/minka/papers/dirichlet/minka-dirichlet.pdf,", "citeRegEx": "34", "shortCiteRegEx": "34", "year": 2012}, {"title": "Free energy score space", "author": ["A. Perina", "M. Cristani", "U. Castellani", "V. Murino", "N. Jojic"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "35", "shortCiteRegEx": "35", "year": 2009}, {"title": "Capturing spatial interdependence in image features: the counting grid, an epitomic representation for bags of features", "author": ["A. Perina", "N. Jojic"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence,", "citeRegEx": "36", "shortCiteRegEx": "36", "year": 2015}, {"title": "Expression microarray data classification using counting grids and Fisher kernel", "author": ["A. Perina", "M. Kesa", "M. Bicego"], "venue": "In IAPR International Conference on Pattern Recognition,", "citeRegEx": "37", "shortCiteRegEx": "37", "year": 2014}, {"title": "Fisher kernels on visual vocabularies for image categorization", "author": ["F. Perronnin", "C. Dance"], "venue": "In IEEE Conference on Computer Vision and Pattern Recognition,", "citeRegEx": "38", "shortCiteRegEx": "38", "year": 2007}, {"title": "Large-scale image categorization with explicit data embedding", "author": ["F. Perronnin", "J. S\u00e1nchez", "Y. Liu"], "venue": "In IEEE Conference on Computer Vision and Pattern Recognition,", "citeRegEx": "39", "shortCiteRegEx": "39", "year": 2010}, {"title": "Improving the Fisher kernel for large-scale image classification", "author": ["F. Perronnin", "J. S\u00e1nchez", "T. Mensink"], "venue": "In European Conference on Computer Vision,", "citeRegEx": "40", "shortCiteRegEx": "40", "year": 2010}, {"title": "Recognizing indoor scenes", "author": ["A. Quattoni", "A. Torralba"], "venue": "In IEEE Conference on Computer Vision and Pattern Recognition,", "citeRegEx": "41", "shortCiteRegEx": "41", "year": 2009}, {"title": "Modeling scenes with local descriptors and latent aspects", "author": ["P. Quelhas", "F. Monay", "J.-M. Odobez", "D. Gatica-Perez", "T. Tuytelaars", "L. Van-Gool"], "venue": "In International Conference on Computer Vision,", "citeRegEx": "42", "shortCiteRegEx": "42", "year": 2005}, {"title": "Feature learning for the image retrieval task", "author": ["A. Rana", "J. Zepeda", "P. Perez"], "venue": "In Asian Conference on Computer Vision Workshop on Feature and Similarity Learning for Computer Vision,", "citeRegEx": "43", "shortCiteRegEx": "43", "year": 2014}, {"title": "CNN features off-the-shelf: an astounding baseline for recognition", "author": ["A.S. Razavian", "H. Azizpour", "J. Sullivan", "S. Carlsson"], "venue": null, "citeRegEx": "44", "shortCiteRegEx": "44", "year": 2014}, {"title": "Image classification with the Fisher vector: Theory and practice", "author": ["J. S\u00e1nchez", "F. Perronnin", "T. Mensink", "J. Verbeek"], "venue": "International Journal on Computer Vision,", "citeRegEx": "45", "shortCiteRegEx": "45", "year": 2013}, {"title": "Exponential family Fisher vector for image classification", "author": ["J. S\u00e1nchez", "J. Redolfi"], "venue": "Pattern Recognition Letters,", "citeRegEx": "46", "shortCiteRegEx": "46", "year": 2015}, {"title": "Overfeat: Integrated recognition, localization and detection using convolutional networks", "author": ["P. Sermanet", "D. Eigen", "X. Zhang", "M. Mathieu", "R. Fergus", "Y. Le- Cun"], "venue": "In International Conference on Learning Representations,", "citeRegEx": "47", "shortCiteRegEx": "47", "year": 2014}, {"title": "Very deep convolutional networks for large-scale image recognition", "author": ["K. Simonyan", "A. Zisserman"], "venue": "Computing Research Repository,", "citeRegEx": "48", "shortCiteRegEx": "48", "year": 2014}, {"title": "Video Google: a text retrieval approach to object matching in videos", "author": ["J. Sivic", "A. Zisserman"], "venue": "In International Conference on Computer Vision,", "citeRegEx": "49", "shortCiteRegEx": "49", "year": 2003}, {"title": "Selective search for object recognition", "author": ["J. Uijlings", "K. van de Sande", "T. Gevers", "A. Smeulders"], "venue": "International Journal on Computer Vision,", "citeRegEx": "50", "shortCiteRegEx": "50", "year": 2013}, {"title": "Visual word ambiguity", "author": ["J. van Gemert", "C. Veenman", "A. Smeulders", "J.-M. Geusebroek"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence,", "citeRegEx": "51", "shortCiteRegEx": "51", "year": 2010}, {"title": "Efficient additive kernels via explicit feature maps", "author": ["A. Vedaldi", "A. Zisserman"], "venue": "In IEEE Conference on Computer Vision and Pattern Recognition,", "citeRegEx": "52", "shortCiteRegEx": "52", "year": 2010}, {"title": "Spatial latent dirichlet allocation", "author": ["X. Wang", "E. Grimson"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "53", "shortCiteRegEx": "53", "year": 2008}, {"title": "CNN: single-label to multi-label", "author": ["Y. Wei", "W. Xia", "J. Huang", "B. Ni", "J. Dong", "Y. Zhao", "S. Yan"], "venue": "Computing Research Repository,", "citeRegEx": "54", "shortCiteRegEx": "54", "year": 2014}, {"title": "Object categorization by learned universal visual dictionary", "author": ["J. Winn", "A. Criminisi", "T. Minka"], "venue": "In International Conference on Computer Vision,", "citeRegEx": "55", "shortCiteRegEx": "55", "year": 2005}, {"title": "Local features and kernels for classification of texture and object categories: a comprehensive study", "author": ["J. Zhang", "M. Marsza\u0142ek", "S. Lazebnik", "C. Schmid"], "venue": "International Journal on Computer Vision,", "citeRegEx": "56", "shortCiteRegEx": "56", "year": 2007}], "referenceMentions": [{"referenceID": 9, "context": "P ATCH-based image representations, such bag of visual words (BoW) [10], [49], are widely utilized in image categorization and retrieval systems.", "startOffset": 67, "endOffset": 71}, {"referenceID": 48, "context": "P ATCH-based image representations, such bag of visual words (BoW) [10], [49], are widely utilized in image categorization and retrieval systems.", "startOffset": 73, "endOffset": 77}, {"referenceID": 37, "context": "Perronnin and Dance [38] have enhanced this basic representation using the notion of Fisher kernels [20].", "startOffset": 20, "endOffset": 24}, {"referenceID": 19, "context": "Perronnin and Dance [38] have enhanced this basic representation using the notion of Fisher kernels [20].", "startOffset": 100, "endOffset": 104}, {"referenceID": 3, "context": "Following [4], which is the first and one of the very few studies utilizing this approximation method, we refer to the resulting kernel as the variational Fisher kernel.", "startOffset": 10, "endOffset": 13}, {"referenceID": 21, "context": "Interestingly, we find that our non-iid models yield gradients that are qualitatively similar to popular transformations of BoW image representations, such as square-rooting histogram entries or more generally applying power normalization [22], [39], [40], [52].", "startOffset": 239, "endOffset": 243}, {"referenceID": 38, "context": "Interestingly, we find that our non-iid models yield gradients that are qualitatively similar to popular transformations of BoW image representations, such as square-rooting histogram entries or more generally applying power normalization [22], [39], [40], [52].", "startOffset": 245, "endOffset": 249}, {"referenceID": 39, "context": "Interestingly, we find that our non-iid models yield gradients that are qualitatively similar to popular transformations of BoW image representations, such as square-rooting histogram entries or more generally applying power normalization [22], [39], [40], [52].", "startOffset": 251, "endOffset": 255}, {"referenceID": 51, "context": "Interestingly, we find that our non-iid models yield gradients that are qualitatively similar to popular transformations of BoW image representations, such as square-rooting histogram entries or more generally applying power normalization [22], [39], [40], [52].", "startOffset": 257, "endOffset": 261}, {"referenceID": 2, "context": "Our second contribution is the analysis of Fisher vector representations over the latent Dirichlet allocation (LDA) model [3] for image classification purposes.", "startOffset": 122, "endOffset": 125}, {"referenceID": 3, "context": "In this case the computation of the gradients is intractable, therefore, we compute approximate variational Fisher vectors [4].", "startOffset": 123, "endOffset": 126}, {"referenceID": 18, "context": "We compare performance to Fisher vectors of PLSA [19], a topic model that does not treat the model parameters as latent variables.", "startOffset": 49, "endOffset": 53}, {"referenceID": 39, "context": "This leads to a representation that performs on par with the improved Fisher vector (FV) representation of [40] based on iid MoG models, which includes power normalization.", "startOffset": 107, "endOffset": 111}, {"referenceID": 15, "context": "First, following recent work [16], [31], we compute Fisher vectors over densely sampled image patches that are encoded using CNN features.", "startOffset": 29, "endOffset": 33}, {"referenceID": 30, "context": "First, following recent work [16], [31], we compute Fisher vectors over densely sampled image patches that are encoded using CNN features.", "startOffset": 35, "endOffset": 39}, {"referenceID": 49, "context": "Second, we propose to extract Fisher vectors over image regions sampled by a selective search method [50].", "startOffset": 101, "endOffset": 105}, {"referenceID": 13, "context": "The experimental results on the PASCAL VOC 2007 [14] and MIT Indoor Scenes [41] datasets confirm the effectiveness of the proposed latent MoG image model, and the corresponding non-iid image descriptors.", "startOffset": 48, "endOffset": 52}, {"referenceID": 40, "context": "The experimental results on the PASCAL VOC 2007 [14] and MIT Indoor Scenes [41] datasets confirm the effectiveness of the proposed latent MoG image model, and the corresponding non-iid image descriptors.", "startOffset": 75, "endOffset": 79}, {"referenceID": 7, "context": "This paper extends our earlier paper [8].", "startOffset": 37, "endOffset": 40}, {"referenceID": 40, "context": "We perform additional experimental evaluation on the MIT Indoor dataset [41].", "startOffset": 72, "endOffset": 76}, {"referenceID": 21, "context": "The use of non-linear feature transformations in BoW image representations is widely recognized to be beneficial for image categorization [22], [39], [40], [52], [56].", "startOffset": 138, "endOffset": 142}, {"referenceID": 38, "context": "The use of non-linear feature transformations in BoW image representations is widely recognized to be beneficial for image categorization [22], [39], [40], [52], [56].", "startOffset": 144, "endOffset": 148}, {"referenceID": 39, "context": "The use of non-linear feature transformations in BoW image representations is widely recognized to be beneficial for image categorization [22], [39], [40], [52], [56].", "startOffset": 150, "endOffset": 154}, {"referenceID": 51, "context": "The use of non-linear feature transformations in BoW image representations is widely recognized to be beneficial for image categorization [22], [39], [40], [52], [56].", "startOffset": 156, "endOffset": 160}, {"referenceID": 55, "context": "The use of non-linear feature transformations in BoW image representations is widely recognized to be beneficial for image categorization [22], [39], [40], [52], [56].", "startOffset": 162, "endOffset": 166}, {"referenceID": 55, "context": "Popular remedies to this problem include the use of chi-square kernels [56], or taking the square-root of histogram entries [39], also referred to as the Hellinger kernel [52].", "startOffset": 71, "endOffset": 75}, {"referenceID": 38, "context": "Popular remedies to this problem include the use of chi-square kernels [56], or taking the square-root of histogram entries [39], also referred to as the Hellinger kernel [52].", "startOffset": 124, "endOffset": 128}, {"referenceID": 51, "context": "Popular remedies to this problem include the use of chi-square kernels [56], or taking the square-root of histogram entries [39], also referred to as the Hellinger kernel [52].", "startOffset": 171, "endOffset": 175}, {"referenceID": 38, "context": "Power normalization [39], defined as f(x) = sign(x)|x|, is a similar transformation that can be applied to non-histogram feature vectors, and it is equivalent to signed square-rooting for the coefficient \u03c1 = 1/2.", "startOffset": 20, "endOffset": 24}, {"referenceID": 0, "context": "The qualitative similarity is illustrated in Figure 3, where we compare the `2, chi-square, and Hellinger distances on the range [0, 1].", "startOffset": 129, "endOffset": 135}, {"referenceID": 38, "context": "Sometimes it is based on empirical observations of improved performance [39], [52], by reducing sparsity in Fisher vectors [40], or in terms of variance stabilizing transformations [22], [55].", "startOffset": 72, "endOffset": 76}, {"referenceID": 51, "context": "Sometimes it is based on empirical observations of improved performance [39], [52], by reducing sparsity in Fisher vectors [40], or in terms of variance stabilizing transformations [22], [55].", "startOffset": 78, "endOffset": 82}, {"referenceID": 39, "context": "Sometimes it is based on empirical observations of improved performance [39], [52], by reducing sparsity in Fisher vectors [40], or in terms of variance stabilizing transformations [22], [55].", "startOffset": 123, "endOffset": 127}, {"referenceID": 21, "context": "Sometimes it is based on empirical observations of improved performance [39], [52], by reducing sparsity in Fisher vectors [40], or in terms of variance stabilizing transformations [22], [55].", "startOffset": 181, "endOffset": 185}, {"referenceID": 54, "context": "Sometimes it is based on empirical observations of improved performance [39], [52], by reducing sparsity in Fisher vectors [40], or in terms of variance stabilizing transformations [22], [55].", "startOffset": 187, "endOffset": 191}, {"referenceID": 25, "context": "Recently, Kobayashi [26] showed that a similar discounting transformation based on taking logarithm of histogram entries, can be derived via modeling `1-normalized descriptors by Dirichlet distribution.", "startOffset": 20, "endOffset": 24}, {"referenceID": 42, "context": "[43] propose to discriminatively learn power normalization coefficients for image retrieval using a triplet-", "startOffset": 0, "endOffset": 4}, {"referenceID": 20, "context": "Similar transformations are also used in image retrieval to counter burstiness effects [21], i.", "startOffset": 87, "endOffset": 91}, {"referenceID": 32, "context": "Burstiness also occurs in text, and the Dirichlet compound multinomial distribution, also known as multivariate P\u00f3lya distribution, has been used to model this effect [33].", "startOffset": 167, "endOffset": 171}, {"referenceID": 12, "context": "Elkan [13] shows the relationship between the Fisher kernel of the multivariate P\u00f3lya distribution and the tf-idf document representation.", "startOffset": 6, "endOffset": 10}, {"referenceID": 2, "context": "Our use of latent Dirichlet allocation (LDA) [3] differs from earlier work on using topic models such as LDA or PLSA [19] for object recognition [29], [42].", "startOffset": 45, "endOffset": 48}, {"referenceID": 18, "context": "Our use of latent Dirichlet allocation (LDA) [3] differs from earlier work on using topic models such as LDA or PLSA [19] for object recognition [29], [42].", "startOffset": 117, "endOffset": 121}, {"referenceID": 28, "context": "Our use of latent Dirichlet allocation (LDA) [3] differs from earlier work on using topic models such as LDA or PLSA [19] for object recognition [29], [42].", "startOffset": 145, "endOffset": 149}, {"referenceID": 41, "context": "Our use of latent Dirichlet allocation (LDA) [3] differs from earlier work on using topic models such as LDA or PLSA [19] for object recognition [29], [42].", "startOffset": 151, "endOffset": 155}, {"referenceID": 3, "context": "Similarly, Chandalia and Beal [4] propose to compress BoW document representation by computing LDA Fisher vector with respect to the parameters of the Dirichlet prior on the topic distributions.", "startOffset": 30, "endOffset": 33}, {"referenceID": 4, "context": "Finally, in contrast to the PLSA Fisher kernel, which was previously studied as a document similarity measure [5], [18], we show that the LDA Fisher kernel naturally involves discounting transformations.", "startOffset": 110, "endOffset": 113}, {"referenceID": 17, "context": "Finally, in contrast to the PLSA Fisher kernel, which was previously studied as a document similarity measure [5], [18], we show that the LDA Fisher kernel naturally involves discounting transformations.", "startOffset": 115, "endOffset": 119}, {"referenceID": 52, "context": "For example, the Spatial LDA model [53] extends the LDA model such that spatially neighboring visual words are more likely assigned to the same topic.", "startOffset": 35, "endOffset": 39}, {"referenceID": 35, "context": "The counting grid model [36], which is a grid of multinomial distributions, can be considered as an alternative to the spatial topic models.", "startOffset": 24, "endOffset": 28}, {"referenceID": 52, "context": "While these studies show that incorporation of spatial information can improve unsupervised semantic segmentation results [53], or lead to better generative classifiers compared to LDA [36], we limit our focus to Fisher kernels of orderless, i.", "startOffset": 122, "endOffset": 126}, {"referenceID": 35, "context": "While these studies show that incorporation of spatial information can improve unsupervised semantic segmentation results [53], or lead to better generative classifiers compared to LDA [36], we limit our focus to Fisher kernels of orderless, i.", "startOffset": 185, "endOffset": 189}, {"referenceID": 23, "context": "We overcome this difficulty by relying on the variational free-energy bound [24], which is obtained by subtracting the Kullback-Leibler divergence between an approximate posterior on the latent variables and the true posterior.", "startOffset": 76, "endOffset": 80}, {"referenceID": 3, "context": "The method of approximating Fisher kernels with the gradient vector of a variational bound was first proposed by Chandalia and Beal [4] in order to obtain the LDA Fisher kernel.", "startOffset": 132, "endOffset": 135}, {"referenceID": 36, "context": "[37], which proposes a variational Fisher kernel for micro-array data.", "startOffset": 0, "endOffset": 4}, {"referenceID": 34, "context": "[35], which uses the variational free-energy to define an alternative encoding, replacing the Fisher kernel.", "startOffset": 0, "endOffset": 4}, {"referenceID": 16, "context": "[17].", "startOffset": 0, "endOffset": 4}, {"referenceID": 19, "context": "The Fisher kernel framework proposed by Jaakkola and Haussler [20] allows combining the power of generative models and", "startOffset": 62, "endOffset": 66}, {"referenceID": 44, "context": "[45] refer to the normalized gradient given by \u03c6(x) as the Fisher vector.", "startOffset": 0, "endOffset": 4}, {"referenceID": 1, "context": "Another motivation is that I makes the Fisher kernel invariant to the re-parameterization \u03b8 \u2192 \u03c8(\u03b8) for any differentiable and invertible function \u03c8 [2], which can be easily shown using the chain rule and the Jakobian matrix of the inverse function \u03c8\u22121.", "startOffset": 148, "endOffset": 151}, {"referenceID": 19, "context": "Alternatively, I can be dropped altogether [20] or analytical approximations can be derived, see e.", "startOffset": 43, "endOffset": 47}, {"referenceID": 37, "context": "[38], [45], [46].", "startOffset": 0, "endOffset": 4}, {"referenceID": 44, "context": "[38], [45], [46].", "startOffset": 6, "endOffset": 10}, {"referenceID": 45, "context": "[38], [45], [46].", "startOffset": 12, "endOffset": 16}, {"referenceID": 1, "context": "Originally developed in statistical physics based on the calculus of variations and the mean field theory, the variational approximation framework that we utilize in this paper is known as the variational inference, and it is now among the most successful approximate probabilistic inference techniques [2], [24], [32].", "startOffset": 303, "endOffset": 306}, {"referenceID": 23, "context": "Originally developed in statistical physics based on the calculus of variations and the mean field theory, the variational approximation framework that we utilize in this paper is known as the variational inference, and it is now among the most successful approximate probabilistic inference techniques [2], [24], [32].", "startOffset": 308, "endOffset": 312}, {"referenceID": 31, "context": "Originally developed in statistical physics based on the calculus of variations and the mean field theory, the variational approximation framework that we utilize in this paper is known as the variational inference, and it is now among the most successful approximate probabilistic inference techniques [2], [24], [32].", "startOffset": 314, "endOffset": 318}, {"referenceID": 23, "context": "For this purpose, we iteratively update the variational lowerbound with respect to the variational distribution parameters, and the model hyper-parameters; an approach that is known as the variational expectation-maximization procedure [24].", "startOffset": 236, "endOffset": 240}, {"referenceID": 26, "context": "The standard BoW image representation can be interpreted as applying the Fisher kernel framework to a simple iid multinomial model over visual word indices, as shown in [27].", "startOffset": 169, "endOffset": 173}, {"referenceID": 32, "context": "This model is known as the multivariate P\u00f3lya, or Dirichlet compound multinomial [33], and the integral simplifies to", "startOffset": 81, "endOffset": 85}, {"referenceID": 0, "context": "Digamma functions \u03c8(\u03b1+n) for various \u03b1, and \u221a n as a function of n; functions have been rescaled to the range [0, 1].", "startOffset": 110, "endOffset": 116}, {"referenceID": 2, "context": "In our second model, we extend the P\u00f3lya model to capture co-occurrence statistics of visual words using latent Dirichlet allocation (LDA) [3].", "startOffset": 139, "endOffset": 142}, {"referenceID": 2, "context": "Here we use a completely factorized approximate posterior as in [3] of the form", "startOffset": 64, "endOffset": 67}, {"referenceID": 18, "context": "In our experiments we compare LDA with the PLSA model [19].", "startOffset": 54, "endOffset": 58}, {"referenceID": 37, "context": "In this section we turn to the image representation of Perronnin and Dance [38] that applies the Fisher kernel framework to mixture of Gaussian (MoG) models over local descriptors.", "startOffset": 75, "endOffset": 79}, {"referenceID": 39, "context": "An improved version of this representation using power normalization was presented in [40].", "startOffset": 86, "endOffset": 90}, {"referenceID": 37, "context": "In [38], [40], local descriptors across images are assumed to be iid samples from a single MoG model underlying all images.", "startOffset": 3, "endOffset": 7}, {"referenceID": 39, "context": "In [38], [40], local descriptors across images are assumed to be iid samples from a single MoG model underlying all images.", "startOffset": 9, "endOffset": 13}, {"referenceID": 26, "context": "where we re-parameterize the mixing weights as \u03c0k = exp(\u03b3k)/ \u2211K k\u2032=1 exp(\u03b3k\u2032), and the Gaussians with precisions \u03bbk = \u03c3 \u22121 k , as in [27].", "startOffset": 133, "endOffset": 137}, {"referenceID": 37, "context": "(47), reveals that the gradient is similar to the square-root of the gradient obtained in [38] for the MoG mean parameters.", "startOffset": 90, "endOffset": 94}, {"referenceID": 37, "context": "In [38] the FV components corresponding to the mixing weights of the MoG, which are also based on zero-order statistics, were shown to be redundant when also including the components corresponding to the means and variances.", "startOffset": 3, "endOffset": 7}, {"referenceID": 13, "context": "In this section, we present a detailed evaluation of the latent BoW, LDA and the latent MoG models over SIFT local descriptors using the PASCAL VOC\u201907 [14] data set in Section 5.", "startOffset": 151, "endOffset": 155}, {"referenceID": 40, "context": "Finally, we evaluate the Latent MoG model, which is the most advanced model that we consider, over the CNN-based local descriptors, and compare against the state-of-the-art on the PASCAL VOC\u201907 and MIT Indoor [41] data sets in Section 5.", "startOffset": 209, "endOffset": 213}, {"referenceID": 5, "context": "[6]: we sample local SIFT descriptors from the same dense grid (3 pixel stride, across 4 scales), which results in around 60, 000 patches per image, project the local descriptors to 80 dimensions with PCA, and train the MoG visual vocabularies from 1.", "startOffset": 0, "endOffset": 3}, {"referenceID": 13, "context": "For the PASCAL VOC\u201907 data set, we use the interpolated mAP score specified by the VOC evaluation protocol [14].", "startOffset": 107, "endOffset": 111}, {"referenceID": 29, "context": "We compare global image representations, and representations that capture spatial layout by concatenating the signatures computed over various spatial cells as in the spatial pyramid matching (SPM) method [30].", "startOffset": 205, "endOffset": 209}, {"referenceID": 5, "context": "Again, we follow [6] and combine a 1\u00d7 1, a 2\u00d72, and a 3\u00d71 grid.", "startOffset": 17, "endOffset": 20}, {"referenceID": 39, "context": "Second, following [40], we also `2 normalize the image representations.", "startOffset": 18, "endOffset": 22}, {"referenceID": 39, "context": "As in [40], power normalization is applied after whitening, and before `2 normalization.", "startOffset": 6, "endOffset": 10}, {"referenceID": 5, "context": "We evaluate two types of power normalization: (i) signed square-rooting (\u03c1 = 1/2) as in [6], [40], which we denote by a prefix \u201cSqrt\u201d, (ii) more general power normalization, which we denote by a prefix \u201cPn\u201d.", "startOffset": 88, "endOffset": 91}, {"referenceID": 39, "context": "We evaluate two types of power normalization: (i) signed square-rooting (\u03c1 = 1/2) as in [6], [40], which we denote by a prefix \u201cSqrt\u201d, (ii) more general power normalization, which we denote by a prefix \u201cPn\u201d.", "startOffset": 93, "endOffset": 97}, {"referenceID": 13, "context": "[14], at 95% confidence interval.", "startOffset": 0, "endOffset": 4}, {"referenceID": 0, "context": "Transformed counts are rescaled to the range [0, 1].", "startOffset": 45, "endOffset": 51}, {"referenceID": 33, "context": "We initialize the Dirichlet distribution on the mixing weights by matching the moments of the distribution of normalized visual word frequencies sk, which gives an approximate maximum likelihood estimation [34].", "startOffset": 206, "endOffset": 210}, {"referenceID": 0, "context": "All FV values are scaled to the range [-1,1].", "startOffset": 38, "endOffset": 44}, {"referenceID": 27, "context": "In this section, we evaluate the latent MoG representation based on local descriptors extracted using a convolutional neural network (CNN) model [28].", "startOffset": 145, "endOffset": 149}, {"referenceID": 15, "context": "[16] and Liu et al .", "startOffset": 0, "endOffset": 4}, {"referenceID": 30, "context": "[31], and extract local descriptors by feeding cropped regions to a CNN model.", "startOffset": 0, "endOffset": 4}, {"referenceID": 14, "context": "Second, inspired from the R-CNN object detector [15], we propose to extract local CNN features for the image regions sampled by a candidate window generation Regions CNN Layer MoG SqrtMoG PnMoG LatMoG", "startOffset": 48, "endOffset": 52}, {"referenceID": 43, "context": "CNN baseline [44] 73.", "startOffset": 13, "endOffset": 17}, {"referenceID": 43, "context": "[44] 77.", "startOffset": 0, "endOffset": 4}, {"referenceID": 0, "context": "[1] 80.", "startOffset": 0, "endOffset": 3}, {"referenceID": 30, "context": "[31] 76.", "startOffset": 0, "endOffset": 4}, {"referenceID": 30, "context": "[31]: a given image is first scaled to a size of 512\u00d7 512 pixels, then, regions of size 227 \u00d7 227 are sampled in a sliding window fashion with a stride of 8 pixels.", "startOffset": 0, "endOffset": 4}, {"referenceID": 27, "context": "[28], which is pre-trained on the ImageNet ILSVRC2012 dataset [11] using the Caffe library [23].", "startOffset": 0, "endOffset": 4}, {"referenceID": 10, "context": "[28], which is pre-trained on the ImageNet ILSVRC2012 dataset [11] using the Caffe library [23].", "startOffset": 62, "endOffset": 66}, {"referenceID": 22, "context": "[28], which is pre-trained on the ImageNet ILSVRC2012 dataset [11] using the Caffe library [23].", "startOffset": 91, "endOffset": 95}, {"referenceID": 49, "context": "[50].", "startOffset": 0, "endOffset": 4}, {"referenceID": 15, "context": "Following the iid MoG based experiments in [16] and [31], we use models with K = 100 Gaussian components, `2 normalize the resulting image representations, and do not use SPM grids.", "startOffset": 43, "endOffset": 47}, {"referenceID": 30, "context": "Following the iid MoG based experiments in [16] and [31], we use models with K = 100 Gaussian components, `2 normalize the resulting image representations, and do not use SPM grids.", "startOffset": 52, "endOffset": 56}, {"referenceID": 43, "context": "[44], which corresponds to training an SVM classifier over the full image CNN descriptors.", "startOffset": 0, "endOffset": 4}, {"referenceID": 0, "context": "[1] (80.", "startOffset": 0, "endOffset": 3}, {"referenceID": 30, "context": "[31] (76.", "startOffset": 0, "endOffset": 4}, {"referenceID": 6, "context": "[7] achieve 82.", "startOffset": 0, "endOffset": 3}, {"referenceID": 46, "context": "4% mAP by utilizing the OverFeat [47] architecture, combined with a carefully selected set of data augmentation, data normalization and CNN fine-tuning techniques.", "startOffset": 33, "endOffset": 37}, {"referenceID": 53, "context": "[54] achieve 85.", "startOffset": 0, "endOffset": 4}, {"referenceID": 47, "context": "Simonyan and Zisserman [48] report that the classification performance can be improved up to 89.", "startOffset": 23, "endOffset": 27}, {"referenceID": 24, "context": "[25] 63.", "startOffset": 0, "endOffset": 4}, {"referenceID": 11, "context": "[12] 64.", "startOffset": 0, "endOffset": 4}, {"referenceID": 43, "context": "CNN baseline [44] 58.", "startOffset": 13, "endOffset": 17}, {"referenceID": 43, "context": "[44] 69.", "startOffset": 0, "endOffset": 4}, {"referenceID": 30, "context": "[31] 68.", "startOffset": 0, "endOffset": 4}, {"referenceID": 15, "context": "[16] 68.", "startOffset": 0, "endOffset": 4}, {"referenceID": 24, "context": "[25] (63.", "startOffset": 0, "endOffset": 4}, {"referenceID": 11, "context": "[12] (64.", "startOffset": 0, "endOffset": 4}, {"referenceID": 43, "context": "[44].", "startOffset": 0, "endOffset": 4}, {"referenceID": 30, "context": "[31] result in a comparable performance at 68.", "startOffset": 0, "endOffset": 4}, {"referenceID": 15, "context": "[16] (68.", "startOffset": 0, "endOffset": 4}, {"referenceID": 44, "context": "As noted in [45], the MoG FV descriptor in this case can be also interpreted as a generalization of the VLAD descriptor [22].", "startOffset": 12, "endOffset": 16}, {"referenceID": 21, "context": "As noted in [45], the MoG FV descriptor in this case can be also interpreted as a generalization of the VLAD descriptor [22].", "startOffset": 120, "endOffset": 124}, {"referenceID": 50, "context": "Although the hard-assignment method can provide significantly speeds up in the descriptor aggregation process, it may also cause significant information loss [51].", "startOffset": 158, "endOffset": 162}, {"referenceID": 8, "context": "done in [9], can be justified in the variational framework.", "startOffset": 8, "endOffset": 11}, {"referenceID": 30, "context": "[31], which we have experimentally compared against in Section 5.", "startOffset": 0, "endOffset": 4}, {"referenceID": 30, "context": "[31] propose to approximate p(x) by the point estimate for u that maximizes the likelihood:", "startOffset": 0, "endOffset": 4}, {"referenceID": 30, "context": "However, as noted in [31], this is leads to a relatively complicated calculation since u is dependent on B.", "startOffset": 21, "endOffset": 25}, {"referenceID": 30, "context": "Using a series of techniques, it is shown in [31] that the gradient is given by:", "startOffset": 45, "endOffset": 49}], "year": 2015, "abstractText": "The bag-of-words (BoW) model treats images as sets of local descriptors and represents them by visual word histograms. The Fisher vector (FV) representation extends BoW, by considering the first and second order statistics of local descriptors. In both representations local descriptors are assumed to be identically and independently distributed (iid), which is a poor assumption from a modeling perspective. It has been experimentally observed that the performance of BoW and FV representations can be improved by employing discounting transformations such as power normalization. In this paper, we introduce non-iid models by treating the model parameters as latent variables which are integrated out, rendering all local regions dependent. Using the Fisher kernel principle we encode an image by the gradient of the data log-likelihood w.r.t. the model hyper-parameters. Our models naturally generate discounting effects in the representations; suggesting that such transformations have proven successful because they closely correspond to the representations obtained for non-iid models. To enable tractable computation, we rely on variational free-energy bounds to learn the hyper-parameters and to compute approximate Fisher kernels. Our experimental evaluation results validate that our models lead to performance improvements comparable to using power normalization, as employed in state-of-the-art feature aggregation methods.", "creator": "LaTeX with hyperref package"}}}