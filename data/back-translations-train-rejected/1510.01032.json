{"id": "1510.01032", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "5-Oct-2015", "title": "Deep convolutional acoustic word embeddings using word-pair side information", "abstract": "Recent studies have been revisiting whole words as the basic modelling unit in speech recognition and query applications, instead of phonetic units. Such whole-word segmental systems rely on a function that maps a variable-length speech segment to a vector in a fixed-dimensional space; the resulting acoustic word embeddings need to allow for accurate discrimination between different word types, directly in the embedding space. We compare several old and new approaches in a word discrimination task. Our best approach uses side information in the form of known word pairs to train a Siamese convolutional neural network (CNN): a pair of tied networks that take two speech segments as input and produce their embeddings, trained with a hinge loss that separates same-word pairs and different-word pairs by some margin. A word classifier CNN performs similarly, but requires much stronger supervision. Both types of CNNs yield large improvements over the best previously published results on the word discrimination task.", "histories": [["v1", "Mon, 5 Oct 2015 05:25:32 GMT  (353kb,D)", "https://arxiv.org/abs/1510.01032v1", "5 pages, 3 figures"], ["v2", "Fri, 8 Jan 2016 14:54:44 GMT  (353kb,D)", "http://arxiv.org/abs/1510.01032v2", "5 pages, 3 figures; added reference, acknowledgement and link to code"]], "COMMENTS": "5 pages, 3 figures", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["herman kamper", "weiran wang", "karen livescu"], "accepted": false, "id": "1510.01032"}, "pdf": {"name": "1510.01032.pdf", "metadata": {"source": "CRF", "title": "DEEP CONVOLUTIONAL ACOUSTIC WORD EMBEDDINGS USING WORD-PAIR SIDE INFORMATION", "authors": ["Herman Kamper", "Weiran Wang", "Karen Livescu"], "emails": ["h.kamper@sms.ed.ac.uk,", "weiranwang@ttic.edu,", "klivescu@ttic.edu"], "sections": [{"heading": null, "text": "Index terms - Acoustic word embedding, segmental acoustic models, fixed dimensional representations, search for examples."}, {"heading": "1. INTRODUCTION", "text": "Most current speech processing systems rely on a deep architecture for classifying language frames into subword units (often telephone networks), an approach that still relies on assumptions about frame-level independence and a pronunciation dictionary to break down words into their subword components. Alternatively, some researchers [1-7] have begun to rethink the use of whole words as a basic modeling unit. Some of the earliest speech recognition systems are based on template-based full-word modeling [8]. This idea has been revised in modern template-based automatic speech recognition (ASR) systems, as well as modern speech indexing applications such as query-by-example search [9, 10]. These systems typically use dynamic time warping (DTW) to quantify the similarity of telephone or word segments of varying lengths."}, {"heading": "2. ACOUSTIC WORD EMBEDDING APPROACHES", "text": "Formally, we use the notation Y = y1: T to denote a vector time series in which each yt-Rb is a b-dimensional frame-level attribute vector (e.g. MFCCs). An acoustic approach to word embedding is a function f (Y) that maps any length time series Y into a fixed-dimensional space Rd; if Y1 and Y2 are two word segments, the distance between the vectors f (Y1) and f (Y2) should indicate whether they are of the same word type or not. Typical embedding approaches use a set of known word segments Rd; if Y1 and Y2 are two word segments, the distance between the vectors f (Y1) and f (Y2) should indicate whether they are of the same word type or not. Typical embedding approaches use a training set of known word segments Ytrain = {Yi Ntraf} segments, where 0g1 segment = 1 preceding word view can be assumed (where 0gr = 2 preceding word pairs are known), whereby 0g1 is assumed for each word pair of unpasted segments."}, {"heading": "2.1. Reference vector methods", "text": "Several embedding approaches were proposed and compared in Levin et al. [3], based on the idea of a reference vector for constructing the mapping f. For a target language segment, a reference vector consists of the DTW alignment costs for each copy of a reference set Yref Ytrain. Applying the dimension reduction to the reference vector yields the desired embedding in Rd. The intuition is that the content of a language segment should be well characterized by its similarity to other segments. Such embedding was then used for keyword search [15] and unattended term detection [18]. One drawback is the need to calculate a large number of DTW approaches. In [3] several dimension reduction approaches were considered, and in Section 3.3 we compare their best overall approach, which uses a combination of laplac eigenmaps (a non-linear graphene embedding approach) and discriminator."}, {"heading": "2.2. Word classification CNN", "text": "For the overall word recognition system in Ytrain, Bengio and Heigold suggested that if word labels Wtrain = {wi} Ntraini = 1 are available for the training segments Ytrain, a supervised neural network can be easily trained to predict the word class (type) that the speech is entering. The Softmax predictive layer of such a neural network then gives a fixed-dimensional representation of the input in Rd, where d is the number of different word types (the word size). During testing, some inputs may correspond to invisible words, but even in these cases, the Softmax layer gives a fixed-dimensional distribution of the input in terms of seen word types. However, a standard feed classification of neural requires a fixed-dimensional input. A simple solution was used in [4]: All word segments are padded to the same length given by the maximum duration of a word segment in Ytrain."}, {"heading": "2.3. Word similarity Siamese CNNs", "text": "If the LabelsWtrain for training Ytrain are not known, a weaker form of monitoring that has also been used [11-13, 19] is the knowledge that pairs of word segments in Ytrain share the same unknown word type: Strain = {(m, n): (Ym, Yn) are of the same type}. This type of page information is attractive because it is often easier to obtain in low-resource settings, for example by using an unattended term discovery system [20, 21] to find unidentified matching word pairs. Such paired monitoring has been used for several problems and domains, including phonetic discovery [13, 22], semantic word embeddings [23-25] and vision applications. Many of these studies use Siamese networks, a term that has been used since the early 1990s to describe a pair of networks with bound parameters that are trained to optimize a distance function between representations of two data cases [17]."}, {"heading": "2.4. Controlling embedding dimensionality", "text": "For word classification networks (Section 2.2), the output dimensionality is determined by the size of the vocabulary. In our experiments (Section 2.3), we investigate the adjustment of dimensionality by inserting an additional linear bottleneck layer before the final Softmax, with the number of units corresponding to the desired final dimensionality. In Siamese networks (Section 2.3), the final dimensionality can be directly matched. If, in addition to the word pairs Strain, we have access to the word names Wtrain, we can also perform an additional dimension reduction with a supervised technique on the Siamese CNN editions; in our experiments, we use linear discrimination analysis (LDA)."}, {"heading": "3. EXPERIMENTS", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "3.1. Evaluation and experimental setup", "text": "In fact, we don't want to be tied to a specific recognition architecture, and we want to quickly compare many embedding approaches. So we use a word discrimination designed for this purpose, but usually we will get a pair of acoustic segments that correspond to one word each, and we have to decide whether the segments are examples of the same word or another."}, {"heading": "3.2. Network architectures", "text": "We used the Theano [28] toolkit to implement CNN-based models of sections 2.2 and 2.3.3 models are trained with ADADELTA [29], an adaptive learning rate stochastic optimization method that adapts over time based on an accumulation of past gradients; we set the pulse hyperparameter to \u03c1 = 0.9 and the precision parameter to = 10 \u2212 6. Input language segments are added to npad = 200 frames (2 s), which is the longest word segment in Ytrain. CNNs architectures were optimized separately for the development data for each network type, resulting in the following structures: \u2022 Word Classified CNN: 1-D folding with 96 filters over 9 frames; ReLU; max folding over 3 units; 1-D folding with 96 filters over 8 units; ReLU; max."}, {"heading": "3.3. Results", "text": "Table 1 shows the performance of AP on the test set of previous studies (models 1 to 4) as well as our newly implemented models (5 to 11).The first three models perform word discrimination with DTW at the frame level embedding of word segments; Model 1 works directly on acoustic features, while Models 2 and 3 work on features optimized for word discrimination.Model 3 provides the best previously reported result on this task. Model 4 is the best acoustic word embedding approach from [3] (Section 2.1), which is the best previous result for an approach that produces embedding of whole word segments. Models 5 to 11 are the neural network-based approaches. The effect of using the revolutionary layers is derived from the big improvement of Model 6 over Model 5. Both of these models are trained on the word type labelsWtrain, which is also the type of supervision used for Model 4, which makes the improvement of Model 4, making the improvement of Word model 2train comparable to the W.Nevertheless, the size of the W4 is not comparable to the one of the other models in the school.3"}, {"heading": "3.4. Further discussion and analysis", "text": "Although the structures of the models 8 and 9 are identical, the model with lcos hinge significantly outperforms its counterpart with lcos cos2. This is consistent with the fact that a loss such as lcos cos2, which optimizes embedding based on relative distances between positive and negative pairs, is much more closely aligned with the discrimination task than a loss such as lcos cos2, which looks at distances of word pairs in isolation (without taking into account their distances relative to other pairs).The loss of lcos hinge also allows more freedom in the model as it does not penalize concurrent word pairs (x1, x2) when they are already similar by the distance of word pairs in isolation (x1, x3).The closer match between the same task and the training loss lcos hinge could also explain the improvements compared to the DTW-based model 3 (both with exactly the same supervision Strain); the latter model aims to learn better characteristics at a local level."}, {"heading": "4. CONCLUSION", "text": "We examined several approaches to acoustic word embedding based on Convolutionary Neural Networks (CNNs), which use a whole-word language segment as input and generate a fixed-dimensional vector. Our best new approach is a Siamese CNN that uses a hinge-based loss function to minimize the distance between pairs of words of the same type relative to the distance between pairs of different types. In the same different word discrimination task, this approach delivers an average precision (AP) of 0.549, an improvement from the best published results on this task to date for full-word embedding (0.365 AP) and DTW with learned frame characteristics (0.469 AP). A word classifier CNN works similarly (0.532 AP) to the Siamese CNN, but requires much stronger labeled monitoring and performs worse on smaller dimensions. Future work will consider sequence models (e.g. RNNNs, LSTMs) and apply these field recognition tasks such as speech recognition."}, {"heading": "5. REFERENCES", "text": "In fact, most people who stand up for people's rights are not willing to stand up for people's rights, \"he said.\" I don't think people are able to respect people's rights. \"He added,\" I don't think people are able to respect people's rights. \"He added,\" I don't think people are able to respect people's rights. \"He added,\" I think I believe I believe I believe I don't believe people are able to respect people's rights. \"He added,\" I don't think people are able to respect people's rights. \"He added,\" I think I believe I believe I believe I don't believe people are able to respect people's rights. \""}], "references": [{"title": "Template-based continuous speech recognition", "author": ["M. De Wachter", "M. Matton", "K. Demuynck", "P. Wambacq", "R. Cools", "D. Van Compernolle"], "venue": "IEEE Trans. Audio, Speech, Language Process., vol. 15, no. 4, pp. 1377\u20131390, 2007.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2007}, {"title": "Investigations on exemplar-based features for speech recognition towards thousands of hours of unsupervised, noisy data", "author": ["G. Heigold", "P. Nguyen", "M. Weintraub", "V. Vanhoucke"], "venue": "Proc. ICASSP, 2012.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2012}, {"title": "Fixeddimensional acoustic embeddings of variable-length segments in low-resource settings", "author": ["K. Levin", "K. Henry", "A. Jansen", "K. Livescu"], "venue": "Proc. ASRU, 2013.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2013}, {"title": "Word embeddings for speech recognition", "author": ["S. Bengio", "G. Heigold"], "venue": "Proc. Interspeech, 2014.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2014}, {"title": "Query-by-example keyword spotting using long short-term memory networks", "author": ["G. Chen", "C. Parada", "T.N. Sainath"], "venue": "Proc. ICASSP, 2015.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2015}, {"title": "Word-level acoustic modeling with convolutional vector regression", "author": ["A.L. Maas", "S.D. Miller", "T.M. O\u2019neil", "A.Y. Ng", "P. Nguyen"], "venue": "Proc. ICML Workshop Representation Learn., 2012.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2012}, {"title": "Generating hyperdimensional distributed representations from continuous-valued multivariate sensory input", "author": ["O.J. R\u00e4s\u00e4nen"], "venue": "Proc. CogSci, 2015.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2015}, {"title": "Connected digit recognition using a level-building DTW algorithm", "author": ["C.S. Myers", "L.R. Rabiner"], "venue": "IEEE Trans. Acoust., Speech, Signal Process., vol. 29, no. 3, pp. 351\u2013363, 1981.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 1981}, {"title": "Unsupervised spoken keyword spotting via segmental DTW on Gaussian posteriorgrams", "author": ["Y. Zhang", "J.R. Glass"], "venue": "Proc. ASRU, 2009.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2009}, {"title": "Resource configurable spoken query detection using deep Boltzmann machines", "author": ["Y. Zhang", "R. Salakhutdinov", "H.-A. Chang", "J.R. Glass"], "venue": "Proc. ICASSP, 2012.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2012}, {"title": "Weak top-down constraints for unsupervised acoustic model training", "author": ["A. Jansen", "S. Thomas", "H. Hermansky"], "venue": "Proc. ICASSP, 2013.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2013}, {"title": "Unsupervised neural network based feature extraction using weak top-down constraints", "author": ["H. Kamper", "M. Elsner", "A. Jansen", "S.J. Goldwater"], "venue": "Proc. ICASSP, 2015.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2015}, {"title": "A hybrid dynamic time warping-deep neural network architecture for unsupervised acoustic modeling", "author": ["R. Thiolli\u00e8re", "E. Dunbar", "G. Synnaeve", "M. Versteegh", "E. Dupoux"], "venue": "Proc. Interspeech, 2015.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2015}, {"title": "Considerations in dynamic time warping algorithms for discrete word recognition", "author": ["L.R. Rabiner", "A.E. Rosenberg", "S.E. Levinson"], "venue": "IEEE Trans. Acoust., Speech, Signal Process., vol. 26, no. 6, pp. 575\u2013582, 1978.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 1978}, {"title": "Segmental acoustic indexing for zero resource keyword search", "author": ["K. Levin", "A. Jansen", "B. Van Durme"], "venue": "Proc. ICASSP, 2015.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2015}, {"title": "Rapid evaluation of speech representations for spoken term discovery", "author": ["M.A. Carlin", "S. Thomas", "A. Jansen", "H. Hermansky"], "venue": "Proc. Interspeech, 2011.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2011}, {"title": "Signature verification using a \u2018Siamese\u2019 time delay neural network", "author": ["J. Bromley", "J.W. Bentz", "L. Bottou", "I. Guyon", "Y. LeCun", "C. Moore", "E. S\u00e4ckinger", "R. Shah"], "venue": "Int. J. Pattern Rec., vol. 7, no. 4, pp. 669\u2013688, 1993.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 1993}, {"title": "Fully unsupervised small-vocabulary speech recognition using a segmental Bayesian model", "author": ["H. Kamper", "A. Jansen", "S. Goldwater"], "venue": "Proc. Interspeech, 2015.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2015}, {"title": "A comparison of neural network methods for unsupervised representation learning on the Zero Resource Speech Challenge", "author": ["D. Renshaw", "H. Kamper", "A. Jansen", "S.J. Goldwater"], "venue": "Proc. Interspeech, 2015.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2015}, {"title": "Unsupervised pattern discovery in speech", "author": ["A.S. Park", "J.R. Glass"], "venue": "IEEE Trans. Audio, Speech, Language Process., vol. 16, no. 1, pp. 186\u2013197, 2008.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2008}, {"title": "Efficient spoken term discovery using randomized algorithms", "author": ["A. Jansen", "B. Van Durme"], "venue": "Proc. ASRU, 2011.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2011}, {"title": "An autoencoder based approach to unsupervised learning of subword units", "author": ["L. Badino", "C. Canevari", "L. Fadiga", "G. Metta"], "venue": "Proc. ICASSP, 2014.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2014}, {"title": "Learning deep structured semantic models for web search using clickthrough data", "author": ["P.-S. Huang", "X. He", "J. Gao", "L. Deng", "A. Acero", "L. Heck"], "venue": "Proc. CIMK, 2013.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2013}, {"title": "Efficient estimation of word representations in vector space", "author": ["T. Mikolov", "K. Chen", "G. Corrado", "J. Dean"], "venue": "arXiv preprint arXiv:1301.3781, 2013.", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2013}, {"title": "From paraphrase database to compositional paraphrase model and back", "author": ["J. Wieting", "M. Bansal", "K. Gimpel", "K. Livescu"], "venue": "Trans. ACL, vol. 3, pp. 345\u2013358, 2015.", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2015}, {"title": "Dimensionality reduction by learning an invariant mapping", "author": ["R. Hadsell", "S. Chopra", "Y. LeCun"], "venue": "Proc. CVPR, 2006.", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2006}, {"title": "Phonetics embedding learning with side information", "author": ["G. Synnaeve", "T. Schatz", "E. Dupoux"], "venue": "Proc. SLT, 2014.", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2014}, {"title": "Theano: a CPU and GPU math expression compiler", "author": ["J. Bergstra", "O. Breuleux", "F. Bastien", "P. Lamblin", "R. Pascanu", "G. Desjardins", "J. Turian", "D. Warde-Farley", "Y. Bengio"], "venue": "Proc. SciPy, 2010.", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2010}, {"title": "ADADELTA: An adaptive learning rate method", "author": ["M.D. Zeiler"], "venue": "arXiv preprint arXiv:1212.5701, 2012.", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2012}], "referenceMentions": [{"referenceID": 0, "context": "As an alternative, some researchers [1\u20137] have started to reconsider using whole words as the basic modelling unit.", "startOffset": 36, "endOffset": 41}, {"referenceID": 1, "context": "As an alternative, some researchers [1\u20137] have started to reconsider using whole words as the basic modelling unit.", "startOffset": 36, "endOffset": 41}, {"referenceID": 2, "context": "As an alternative, some researchers [1\u20137] have started to reconsider using whole words as the basic modelling unit.", "startOffset": 36, "endOffset": 41}, {"referenceID": 3, "context": "As an alternative, some researchers [1\u20137] have started to reconsider using whole words as the basic modelling unit.", "startOffset": 36, "endOffset": 41}, {"referenceID": 4, "context": "As an alternative, some researchers [1\u20137] have started to reconsider using whole words as the basic modelling unit.", "startOffset": 36, "endOffset": 41}, {"referenceID": 5, "context": "As an alternative, some researchers [1\u20137] have started to reconsider using whole words as the basic modelling unit.", "startOffset": 36, "endOffset": 41}, {"referenceID": 6, "context": "As an alternative, some researchers [1\u20137] have started to reconsider using whole words as the basic modelling unit.", "startOffset": 36, "endOffset": 41}, {"referenceID": 7, "context": "Some of the earliest speech recognition systems were based on template-based whole-word modelling [8].", "startOffset": 98, "endOffset": 101}, {"referenceID": 0, "context": "This idea has been revisited in modern template-based automatic speech recognition (ASR) systems [1, 2], as well as modern speech indexing applications such as query-by-example search [9, 10].", "startOffset": 97, "endOffset": 103}, {"referenceID": 1, "context": "This idea has been revisited in modern template-based automatic speech recognition (ASR) systems [1, 2], as well as modern speech indexing applications such as query-by-example search [9, 10].", "startOffset": 97, "endOffset": 103}, {"referenceID": 8, "context": "This idea has been revisited in modern template-based automatic speech recognition (ASR) systems [1, 2], as well as modern speech indexing applications such as query-by-example search [9, 10].", "startOffset": 184, "endOffset": 191}, {"referenceID": 9, "context": "This idea has been revisited in modern template-based automatic speech recognition (ASR) systems [1, 2], as well as modern speech indexing applications such as query-by-example search [9, 10].", "startOffset": 184, "endOffset": 191}, {"referenceID": 10, "context": "Recent work has also considered frame-level embeddings which map acoustic features to a new frame-level representation that is tailored to word discrimination when combined with DTW [11\u201313].", "startOffset": 182, "endOffset": 189}, {"referenceID": 11, "context": "Recent work has also considered frame-level embeddings which map acoustic features to a new frame-level representation that is tailored to word discrimination when combined with DTW [11\u201313].", "startOffset": 182, "endOffset": 189}, {"referenceID": 12, "context": "Recent work has also considered frame-level embeddings which map acoustic features to a new frame-level representation that is tailored to word discrimination when combined with DTW [11\u201313].", "startOffset": 182, "endOffset": 189}, {"referenceID": 13, "context": "DTW, however, has known inadequacies [14] and is quadratic-time in the duration of the segments.", "startOffset": 37, "endOffset": 41}, {"referenceID": 2, "context": "[3] proposed a segmental approach where an arbitrarylength speech segment is embedded in a fixed-dimensional space such that segments of the same word type have similar embeddings.", "startOffset": 0, "endOffset": 3}, {"referenceID": 2, "context": "Several approaches were developed in [3], and in [15] these were successfully applied in a query-by-example search system.", "startOffset": 37, "endOffset": 40}, {"referenceID": 14, "context": "Several approaches were developed in [3], and in [15] these were successfully applied in a query-by-example search system.", "startOffset": 49, "endOffset": 53}, {"referenceID": 3, "context": "Bengio and Heigold [4] similarly used whole-word fixeddimensional representations in a segmental ASR lattice rescoring system.", "startOffset": 19, "endOffset": 22}, {"referenceID": 4, "context": "A similar approach was followed in [5], where long short-term memory (LSTM) networks were used to obtain wholeword embeddings for a query-by-example search task.", "startOffset": 35, "endOffset": 38}, {"referenceID": 5, "context": "[6] trained a regression CNN that reconstructs a semantic word embedding from acoustic speech input; these features were used in a segmental conditional random field ASR system.", "startOffset": 0, "endOffset": 3}, {"referenceID": 2, "context": "[3], on a word discrimination task.", "startOffset": 0, "endOffset": 3}, {"referenceID": 10, "context": "This task has been used in several other studies [11,12,16] to assess the accuracy of acoustic embedding approaches without the need to train a complete recognition or search system.", "startOffset": 49, "endOffset": 59}, {"referenceID": 11, "context": "This task has been used in several other studies [11,12,16] to assess the accuracy of acoustic embedding approaches without the need to train a complete recognition or search system.", "startOffset": 49, "endOffset": 59}, {"referenceID": 15, "context": "This task has been used in several other studies [11,12,16] to assess the accuracy of acoustic embedding approaches without the need to train a complete recognition or search system.", "startOffset": 49, "endOffset": 59}, {"referenceID": 16, "context": "The approach is based on Siamese networks: tied networks that take in pairs of input vectors and minimize or maximize a distance depending on whether a pair comes from the same or different classes [17].", "startOffset": 198, "endOffset": 202}, {"referenceID": 2, "context": "[3], and performs similarly to a word classifier CNN, despite the weaker form of supervision.", "startOffset": 0, "endOffset": 3}, {"referenceID": 2, "context": "[3] based on the idea of using a reference vector to construct the mapping f .", "startOffset": 0, "endOffset": 3}, {"referenceID": 14, "context": "Such embeddings have subsequently been used for keyword search [15] and unsupervised term discovery [18].", "startOffset": 63, "endOffset": 67}, {"referenceID": 17, "context": "Such embeddings have subsequently been used for keyword search [15] and unsupervised term discovery [18].", "startOffset": 100, "endOffset": 104}, {"referenceID": 2, "context": "Several dimensionality reduction approaches were considered in [3], and in Section 3.", "startOffset": 63, "endOffset": 66}, {"referenceID": 3, "context": "For the whole-word speech recognition system in [4], Bengio and", "startOffset": 48, "endOffset": 51}, {"referenceID": 3, "context": "A simple solution was used in [4]: All word segments are padded to the same length, given by the maximum duration of a word segment in Ytrain.", "startOffset": 30, "endOffset": 33}, {"referenceID": 3, "context": "Instead of using the representation from the word classification CNN directly, Bengio and Heigold [4] used a paired network with a ranking loss to map acoustic word embeddings into a common space with orthography-based word embeddings obtained by also mapping the word labelsWtrain into a lower-dimensional space.", "startOffset": 98, "endOffset": 101}, {"referenceID": 10, "context": "If the labelsWtrain for the training set Ytrain are not known, a weaker form of supervision that has also been used [11\u201313, 19] is the knowledge that pairs of word segments in Ytrain share the same unknown word type: Strain = {(m,n) : (Ym, Yn) are of the same type}.", "startOffset": 116, "endOffset": 127}, {"referenceID": 11, "context": "If the labelsWtrain for the training set Ytrain are not known, a weaker form of supervision that has also been used [11\u201313, 19] is the knowledge that pairs of word segments in Ytrain share the same unknown word type: Strain = {(m,n) : (Ym, Yn) are of the same type}.", "startOffset": 116, "endOffset": 127}, {"referenceID": 12, "context": "If the labelsWtrain for the training set Ytrain are not known, a weaker form of supervision that has also been used [11\u201313, 19] is the knowledge that pairs of word segments in Ytrain share the same unknown word type: Strain = {(m,n) : (Ym, Yn) are of the same type}.", "startOffset": 116, "endOffset": 127}, {"referenceID": 18, "context": "If the labelsWtrain for the training set Ytrain are not known, a weaker form of supervision that has also been used [11\u201313, 19] is the knowledge that pairs of word segments in Ytrain share the same unknown word type: Strain = {(m,n) : (Ym, Yn) are of the same type}.", "startOffset": 116, "endOffset": 127}, {"referenceID": 19, "context": "This type of side information is appealing since it is often easier to obtain in low-resource settings, for example by using an unsupervised term discovery system [20, 21] to find unidentified matching word pairs.", "startOffset": 163, "endOffset": 171}, {"referenceID": 20, "context": "This type of side information is appealing since it is often easier to obtain in low-resource settings, for example by using an unsupervised term discovery system [20, 21] to find unidentified matching word pairs.", "startOffset": 163, "endOffset": 171}, {"referenceID": 12, "context": "Such paired supervision has been used for several problems and domains, including phonetic discovery [13, 22], semantic word embeddings [23\u201325] and vision applications [26].", "startOffset": 101, "endOffset": 109}, {"referenceID": 21, "context": "Such paired supervision has been used for several problems and domains, including phonetic discovery [13, 22], semantic word embeddings [23\u201325] and vision applications [26].", "startOffset": 101, "endOffset": 109}, {"referenceID": 22, "context": "Such paired supervision has been used for several problems and domains, including phonetic discovery [13, 22], semantic word embeddings [23\u201325] and vision applications [26].", "startOffset": 136, "endOffset": 143}, {"referenceID": 23, "context": "Such paired supervision has been used for several problems and domains, including phonetic discovery [13, 22], semantic word embeddings [23\u201325] and vision applications [26].", "startOffset": 136, "endOffset": 143}, {"referenceID": 24, "context": "Such paired supervision has been used for several problems and domains, including phonetic discovery [13, 22], semantic word embeddings [23\u201325] and vision applications [26].", "startOffset": 136, "endOffset": 143}, {"referenceID": 25, "context": "Such paired supervision has been used for several problems and domains, including phonetic discovery [13, 22], semantic word embeddings [23\u201325] and vision applications [26].", "startOffset": 168, "endOffset": 172}, {"referenceID": 16, "context": "Many of these studies use Siamese networks, a term used since the early 1990s to describe a pair of networks with tied parameters which is trained to optimize a distance function between representations of two data instances [17].", "startOffset": 225, "endOffset": 229}, {"referenceID": 26, "context": "We found that losses based on cosine similarity outperformed Euclidean-based losses, and in particular the coscos loss from [27] gave the best performance of the losses in [17, 27]:", "startOffset": 124, "endOffset": 128}, {"referenceID": 16, "context": "We found that losses based on cosine similarity outperformed Euclidean-based losses, and in particular the coscos loss from [27] gave the best performance of the losses in [17, 27]:", "startOffset": 172, "endOffset": 180}, {"referenceID": 26, "context": "We found that losses based on cosine similarity outperformed Euclidean-based losses, and in particular the coscos loss from [27] gave the best performance of the losses in [17, 27]:", "startOffset": 172, "endOffset": 180}, {"referenceID": 23, "context": "This motivates a margin-based (hinge) loss, similar to that of [24, 25]:", "startOffset": 63, "endOffset": 71}, {"referenceID": 24, "context": "This motivates a margin-based (hinge) loss, similar to that of [24, 25]:", "startOffset": 63, "endOffset": 71}, {"referenceID": 15, "context": "We therefore use a word discrimination task developed for this purpose [16]; in the same-different task, we are given a pair of acoustic segments, each corresponding to a word, and we must decide whether the segments are examples of the same or different words.", "startOffset": 71, "endOffset": 75}, {"referenceID": 10, "context": "For the training set Ytrain we use the set of about 10k word tokens from [11,12]; it consists of word segments of at least 5 characters and 0.", "startOffset": 73, "endOffset": 80}, {"referenceID": 11, "context": "For the training set Ytrain we use the set of about 10k word tokens from [11,12]; it consists of word segments of at least 5 characters and 0.", "startOffset": 73, "endOffset": 80}, {"referenceID": 2, "context": "For testing, we use the 11k-token set Ytest from [3, 11, 12], making the results from", "startOffset": 49, "endOffset": 60}, {"referenceID": 10, "context": "For testing, we use the 11k-token set Ytest from [3, 11, 12], making the results from", "startOffset": 49, "endOffset": 60}, {"referenceID": 11, "context": "For testing, we use the 11k-token set Ytest from [3, 11, 12], making the results from", "startOffset": 49, "endOffset": 60}, {"referenceID": 2, "context": "1We also tried Euclidean distance, but as in [3,12,13], cosine worked better.", "startOffset": 45, "endOffset": 54}, {"referenceID": 11, "context": "1We also tried Euclidean distance, but as in [3,12,13], cosine worked better.", "startOffset": 45, "endOffset": 54}, {"referenceID": 12, "context": "1We also tried Euclidean distance, but as in [3,12,13], cosine worked better.", "startOffset": 45, "endOffset": 54}, {"referenceID": 10, "context": "As mentioned in Section 1, recent studies [11, 12] have also been using frame-level embedding approaches in combination with DTW to perform the same-different task.", "startOffset": 42, "endOffset": 50}, {"referenceID": 11, "context": "As mentioned in Section 1, recent studies [11, 12] have also been using frame-level embedding approaches in combination with DTW to perform the same-different task.", "startOffset": 42, "endOffset": 50}, {"referenceID": 10, "context": "We compare our results to that of [11], which uses posteriograms over a partitioned universal background model (UBM), as well as [12], which uses a correspondence autoencoder.", "startOffset": 34, "endOffset": 38}, {"referenceID": 11, "context": "We compare our results to that of [11], which uses posteriograms over a partitioned universal background model (UBM), as well as [12], which uses a correspondence autoencoder.", "startOffset": 129, "endOffset": 133}, {"referenceID": 27, "context": "We used the Theano [28] toolkit to implement the CNN-based models of Sections 2.", "startOffset": 19, "endOffset": 23}, {"referenceID": 28, "context": "Models are trained using ADADELTA [29], an adaptive learning rate stochastic optimization method that adapts over time based on an accumulation of past gradients; we set the momentum hyper-parameter to \u03c1 = 0.", "startOffset": 34, "endOffset": 38}, {"referenceID": 2, "context": "Model 4 is the best acoustic word embedding approach from [3] (Section 2.", "startOffset": 58, "endOffset": 61}, {"referenceID": 2, "context": "2In [3], a slightly different training set was used.", "startOffset": 4, "endOffset": 7}, {"referenceID": 10, "context": "2 Best partitioned UBM [11] 100 0.", "startOffset": 23, "endOffset": 27}, {"referenceID": 11, "context": "3 Correspondence autoencoder [12] 100 0.", "startOffset": 29, "endOffset": 33}, {"referenceID": 2, "context": "4 Reference vector approach [3] 50 0.", "startOffset": 28, "endOffset": 31}], "year": 2016, "abstractText": "Recent studies have been revisiting whole words as the basic modelling unit in speech recognition and query applications, instead of phonetic units. Such whole-word segmental systems rely on a function that maps a variable-length speech segment to a vector in a fixed-dimensional space; the resulting acoustic word embeddings need to allow for accurate discrimination between different word types, directly in the embedding space. We compare several old and new approaches in a word discrimination task. Our best approach uses side information in the form of known word pairs to train a Siamese convolutional neural network (CNN): a pair of tied networks that take two speech segments as input and produce their embeddings, trained with a hinge loss that separates same-word pairs and different-word pairs by some margin. A word classifier CNN performs similarly, but requires much stronger supervision. Both types of CNNs yield large improvements over the best previously published results on the word discrimination task.", "creator": "LaTeX with hyperref package"}}}