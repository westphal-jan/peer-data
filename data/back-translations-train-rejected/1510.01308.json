{"id": "1510.01308", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "5-Oct-2015", "title": "Tight Variational Bounds via Random Projections and I-Projections", "abstract": "Information projections are the key building block of variational inference algorithms and are used to approximate a target probabilistic model by projecting it onto a family of tractable distributions. In general, there is no guarantee on the quality of the approximation obtained. To overcome this issue, we introduce a new class of random projections to reduce the dimensionality and hence the complexity of the original model. In the spirit of random projections, the projection preserves (with high probability) key properties of the target distribution. We show that information projections can be combined with random projections to obtain provable guarantees on the quality of the approximation obtained, regardless of the complexity of the original model. We demonstrate empirically that augmenting mean field with a random projection step dramatically improves partition function and marginal probability estimates, both on synthetic and real world data.", "histories": [["v1", "Mon, 5 Oct 2015 19:53:22 GMT  (138kb,D)", "http://arxiv.org/abs/1510.01308v1", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["lun-kai hsu", "tudor achim", "stefano ermon"], "accepted": false, "id": "1510.01308"}, "pdf": {"name": "1510.01308.pdf", "metadata": {"source": "CRF", "title": "Tight Variational Bounds via Random Projections and I-Projections", "authors": ["Lun-Kai Hsu", "Tudor Achim", "Stefano Ermon"], "emails": ["luffykai@cs.stanford.edu", "tachim@cs.stanford.edu", "ermon@cs.stanford.edu"], "sections": [{"heading": "1 Introduction", "text": "Probabilistic conclusions are a core problem in machine learning, physics and statistics [1]. Probabilistic conclusions are required for the training, evaluation and prediction of probabilistic models. [2] The development of scalable and precise inference techniques is key to the use of large-scale statistical models, but exact inference is known to be mathematically insoluble. [3] The root cause is the curse of dimensionality - the number of scenarios to consider grows exponentially in the number of variables, and in the continuous domains the volume grows exponentially in the number of dimensions [3]. Approximate techniques are therefore almost always applied in practice. Sampling-based techniques and variable approaches are the two main paradigms of insoluble inference [4, 5]."}, {"heading": "2 Preliminaries", "text": "We also assume that p (x) is a member of an exponential family of distributions specified by the number of binary variables x (0, 1) n, and has sufficient statistics \u03c6 (x) [5], i.e. p (x) = exp (x)). The constant Z (x) is one of the most important computational challenges in the field of probability calculation and statistical machine learning, since it is necessary to evaluate probabilities and compare competing data models. It is well known that this calculation is insoluble (# -P-hard), since the sum of approximate terms is an approximate number of 13."}, {"heading": "2.1 Variational Inference and I-projections", "text": "The approach is to define a family of Q tractable distributions and then look for a distribution in that family that minimizes an idea of divergence of q q = q q q q (x). Typically, however, the Kullback Leibler Divergence DKL (q | | p) is used, which minimizes this divergence as followsDKL (q | | p) = x q (x) log q (x) x (x) x x (x) \u03c6 (x) + logZ (1) a distribution q \u00b2 Q, which minimizes this divergence, q \u00b2 QDKL (q | p) log q (x) q (x) x (x) log q (x) x x x x (x) x (x) x (x) x (x) x (x) x (x) x (x) x (x) x (x) x (x) x (x) x x x x x x) x (x x) x x x x x (x) x x x x x x x) x (x x x) x (x x x) x (x x x) x (x x x x x) x (x) x x x (x) x (x x) x x x (x) x x (x x) x (x) x x (x x x) x (x) x x x x (x x x x) x (x x x x) x x (x x x x x x x x) x (x x x x x x x x x x) x (x x x) x x x x x x (x x x x x) x x x x x x x (x x x x x x) x x x x x x x x x x x x x (x x x) x x x x x x x x (x x x x x x) x x x x x x x x x x x x x x x x x (x x x x x x x) x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x (x x x x x x x x x x x x x x x x x x x x x x x x x"}, {"heading": "2.2 Random Projections", "text": "Let P set the set of all probability distributions above {0, 1} n. We present a family of operators RmA, b: P \u00b2 P, where m \u00b2 [0, n], A \u00b2 {0, 1} m \u00b7 n, and b \u00b2 {0, 1} m, whose probability mass function is proportional to p. Formally, RmA (p) (x) = 1Z (A) \u03b1) can form a new probability distribution RmA, b (p) limited to {x: Ax = b mod 2}, whose probability mass function is proportional to p. Formally, RmA, b \u2212 n \u00b2 (x) = 1Z (A) \u03b1 ({x} \u03b1). In other words, we are, for all x = b mod 2}, RmA (b), b \u00b2 mod 2}, b \u00b2 x (p) the proportion to p."}, {"heading": "3 Combining Random Projections with I-Projections", "text": "Given an insoluble target distribution p and a candidate set of (traceable) distribution problems Q, there are two main problems with variable approximation techniques: (i) p can be far from the approximate familyQ in the sense that even the optimal q * = argminq * QDKL (q * p) can have a large divergence and therefore yield a poor lower limit in Equation (2), and (ii) the variation problem in Equation (2) is not convex and therefore difficult to solve accurately in high dimensions. Our key idea is to (i) \"simplify\" by using the random projections p introduced in the previous section and generate a projection RmA, b (p) that is demonstrably closer to Q. (2). Crucial is that due to the statistical properties of the random projection used (variational) inferences on the randomly projected model A, Rmjan (b), Rmjan (b), the original information (b)."}, {"heading": "3.1 Provably Tight Variational Bounds on the Partition Function", "text": "There are 2n such probability distributions, and the entropy of each of them is zero. Given that any probability distribution p, its projection on D, i.e., argminq [1] on D, i.e., argminq [2] on D, i.e., argminq [3] on D, i.e., argminq [4] on DKL (q], is given by a distribution that places all probability distributions on argmaxx X log p (x). Therefore, calculating the I projection on D is equivalent to solving one of the most predictable explanatory queries [1], which is equivalent at worst. LetQ D is a family of probability distributions that D. Our key result is that we can obtain demonstrably narrow boundaries on the partition function Z by taking a suitable random projection on Q."}, {"heading": "3.2 Solving Randomly Projected Variational Inference Problems", "text": "# # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # #"}, {"heading": "4 Algorithm: Mean Field with Random Projections", "text": "Theorem 1 guarantees that the approach to Z has a narrow lower limit only if we are able to find globally optimal solutions for (8). However, the non-trivial variation problems (2) are generally also without any random projections and even if the objective function is still coordinating, as in a traditional center field equation: prognosis 2. The objective function of Proposition 2 is linear with respect to certain free marginal micrometers."}, {"heading": "5 Experiments", "text": "We examine the empirical performance of MFRP on Ising models and Restricted Boltzmann machines. In particular, we are interested in log partition function estimates and the quality of marginal estimates. If applicable, exact ground truth estimates are determined using the libDAI implementation of Junction Tree [20]. Upper limits are calculated using Tree-Reweighted Belief Propagation (TRW-BP) [21], also implemented in libDAI. All methods are compared with the center field (MF), which has been optimized by coordinated ascent and random restarts."}, {"heading": "5.1 Ising Models", "text": "We consider n \u00b7 n binary grid Ising models with the variables xi-1, 1} and the potentials \u0435ij (xi, xj) = exp (wijxixj + fixi + fjxj). In particular, we consider mixed models in which the Wij's are drawn uniformly from [\u2212 10, 10] and the Fis uniformly from [\u2212 1, 1]. Figure 2 compares the log partition estimates of MF, Junction Tree, MFRP and TRWBP. For each grid size, we have created five different grids and calculated the center estimate for each as the lower limit. For each of the five grids, we also calculated the best MFRP lower limits above m [0, 20] with each T = 5 attempts. For comparison, we use the exact log partition calculation from Junction Tree to n = 20 and the TRW-BP upper limits as a cross."}, {"heading": "5.2 Restricted Boltzmann Machines", "text": "We train Restricted Boltzmann Machines (RBMs) [22] using contrasting divergence (CD) [23, 24] on the handwritten data set of the MNIST. In an RBM, there is a layer of nh hidden binary variables h = h1, \u00b7 \u00b7, hnh and a layer of nv binary visible units v = v1, \u00b7 \u00b7 \u00b7, vnv. The common probability distribution is given by P (h, v) = 1Z exp (b \u2032 v + c \u2032 h + h \u2032 Wv). We use nh = 100, 200} hidden units and nv = 28 \u00d7 28 = 784 visible units. We then learn the parameters b, c, W using CD-k for k: 1, 5, 15}, where k denotes the number of Gibbs sampling steps used in the inference phase, with 15 training periods and minibatches of size 20.We then use MF and MFRP to estimate the minor differences in most cases, taking into account the minor problems and the P in most cases."}, {"heading": "6 Conclusions", "text": "Our approach is the first in which universal hash functions and their properties are used in a variable sense. We have demonstrated the effectiveness of this idea by adding random projections to the center field and empirically showing a major improvement in partition function, namely lower boundaries and margins achieved on both synthetic and real data. Natural extensions of the approach include applications to other variational methods, such as Bethe approximation, and the use of more powerful global optimization techniques instead of the currently used coordinate ascent."}, {"heading": "A Appendix : Proofs", "text": "The proof of theorem 2.min q = max | DKL (q | RiAi, t = q q = q = q = q (q = q = = max (p) = min q (x) x | Ai, tx = bi, t (x) \u00b7 q = q (q = q = max) = max q (x) = max x (x) + logZ (Ai, t, bi, t) = min q (x) \u00b7 log q (x) \u00b7 q (x) \u00b7 q (x) \u00b7 q (x) \u00b7 q (x) = logZ (Ai, t, bi, t, t) = min q (x) \u2212 D \u2212 \u03b8 (x) + logZ (x)."}, {"heading": "6. Binary term, \u00b5kl, l \u2264 m", "text": "If Clk = 0, \u00b5kl = \u00b5k\u00b5l and its derivative is\u00b5l + \u00b5k\u00b5 \u2032 l = \u00b5l = 1 2 (1 \u2212 (1 \u2212 2bl) n, i = m + 1, i 6 = k (1 \u2212 2Cli\u00b5i)) If Clk = 1, \u00b5kl = \u00b5k 12 (1 + (1 \u2212 2bl), i = 6 = k, i = m + 1 (1 \u2212 2Cli\u00b5i))) The derivative is1 2 (1 + (1 \u2212 2bl) n, i = 6 = k, i = m + 1 (1 \u2212 2Cli\u00b5i)))"}, {"heading": "7. Binary term, \u00b5pl, where p \u2265 m+ 1, p 6= k, l \u2264 m", "text": "If Clp = 0, \u00b5pl = \u00b5p\u00b5l and its derivative is\u00b5p\u00b5 \u2032 l, if Clp = 1, \u00b5pl = \u00b5p 12 (1 + (1 \u2212 2bl) \u0445n i 6 = p, i = m + 1 (1 \u2212 2Cli\u00b5i)), the derivative is \u2212 \u00b5pCkl (1 \u2212 2bl) n, i6 = k, i6 = p, i = m + 1 (1 \u2212 2Cli\u00b5i)) 8 Binary term \u00b5pl, where both p, l \u2264 m \u2202 \u00b5pl = \u2202 \u00b5k1 4 (1 \u2212 2bp) n, i = m + 1 (1 \u2212 2bl) n, i = m + 1 (1 \u2212 2bl) n, i = m + 1 (1 \u2212 2Cli\u00b5i) + (1 \u2212 2Cli\u00b5i) + (1 \u2212 2Cli\u00b5i) + (1 \u2212 2bp) n, Cli (1 \u2212 2p) 2p (2 \u2212 2p), 2p (2 \u2212 2p), 2p (1 \u2212 2p), 2p (1 \u2212 2p), 2p (1 \u2212 2p), 2p (1 \u2212 2p), 2p (1 \u2212 2p) 2p (1 \u2212 2p), 2p (1 \u2212 2p), 2p (1 \u2212 2p (1 \u2212 2p), 2p (1 \u2212 2p)"}], "references": [], "referenceMentions": [], "year": 2015, "abstractText": "<lb>Information projections are the key building block of variational inference algo-<lb>rithms and are used to approximate a target probabilistic model by projecting it<lb>onto a family of tractable distributions. In general, there is no guarantee on the<lb>quality of the approximation obtained. To overcome this issue, we introduce a<lb>new class of random projections to reduce the dimensionality and hence the com-<lb>plexity of the original model. In the spirit of random projections, the projection<lb>preserves (with high probability) key properties of the target distribution. We show<lb>that information projections can be combined with random projections to obtain<lb>provable guarantees on the quality of the approximation obtained, regardless of<lb>the complexity of the original model. We demonstrate empirically that augmenting<lb>mean field with a random projection step dramatically improves partition function<lb>and marginal probability estimates, both on synthetic and real world data.", "creator": "LaTeX with hyperref package"}}}