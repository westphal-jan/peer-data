{"id": "1510.01443", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "6-Oct-2015", "title": "A Waveform Representation Framework for High-quality Statistical Parametric Speech Synthesis", "abstract": "State-of-the-art statistical parametric speech synthesis (SPSS) generally uses a vocoder to represent speech signals and parameterize them into features for subsequent modeling. Magnitude spectrum has been a dominant feature over the years. Although perceptual studies have shown that phase spectrum is essential to the quality of synthesized speech, it is often ignored by using a minimum phase filter during synthesis and the speech quality suffers. To bypass this bottleneck in vocoded speech, this paper proposes a phase-embedded waveform representation framework and establishes a magnitude-phase joint modeling platform for high-quality SPSS. Our experiments on waveform reconstruction show that the performance is better than that of the widely-used STRAIGHT. Furthermore, the proposed modeling and synthesis platform outperforms a leading-edge, vocoded, deep bidirectional long short-term memory recurrent neural network (DBLSTM-RNN)-based baseline system in various objective evaluation metrics conducted.", "histories": [["v1", "Tue, 6 Oct 2015 06:12:31 GMT  (553kb,D)", "http://arxiv.org/abs/1510.01443v1", "accepted and will appear in APSIPA2015; keywords: speech synthesis, LSTM-RNN, vocoder, phase, waveform, modeling"]], "COMMENTS": "accepted and will appear in APSIPA2015; keywords: speech synthesis, LSTM-RNN, vocoder, phase, waveform, modeling", "reviews": [], "SUBJECTS": "cs.SD cs.LG", "authors": ["bo fan", "siu wa lee", "xiaohai tian", "lei xie", "minghui dong"], "accepted": false, "id": "1510.01443"}, "pdf": {"name": "1510.01443.pdf", "metadata": {"source": "CRF", "title": "A Waveform Representation Framework for High-quality Statistical Parametric Speech Synthesis", "authors": ["Bo Fan", "Siu Wa Lee", "Xiaohai Tian", "Lei Xie", "Minghui Dong"], "emails": ["bofan@nwpu-aslp.org,", "lxie@nwpu-aslp.org,", "swylee@i2r.a-star.edu.sg,", "mhdong@i2r.a-star.edu.sg,", "xhtian@ntu.edu.sg"], "sections": [{"heading": null, "text": "This year, we have reached a point where it can only take one year to reach an agreement."}, {"heading": "II. TD-PSOLA", "text": "The time domain pitch-synchronous overlap add (TD-PSOLA) is used for pitch and timing modifications of speech signals [17], [24]. It is also popular for chain-based TTS. Since no decomposition or vocoding of source filters is performed, the quality of the resulting language is very similar after analysis and reconstruction of the original speech. In the face of any voice wave signal x (n), TDPSOLA is performed in the time domain. It first decomposes x (n) into a sequence of overlapping, pitch-synchronized segments. Each segment xs (n) takes two pitch periods running from one pitch period before and another pitch period after the segment center. Then, a window function hs (n) is applied to each segment, e.g. the Hanning window. Assuming that S indicates the total number of segments, with times = 1, S (xLA), S (xH) (n) for speech quality."}, {"heading": "III. WAVEFORM REPRESENTATION FRAMEWORK", "text": "Similar to TD-PSOLA, Glottal Closed Instants (GCIs) represent both the pitch contours and the boundaries of individual speech cycles. Existing GCI recognition approaches typically estimate the GCI positions locally, ignoring the resulting trajectories of various acoustic attributes, i.e. segment length (representing basic frequency (F0)), size, and phase spectrum that can be seen in the utterance. Since smooth trajectories of these attributes are necessary for SPSS, we are revising a state-of-the-art GCI recognition approach to enable satisfactory modeling of these attributes."}, {"heading": "A. System Overview", "text": "The proposed framework consists of two parts, as shown in Figure 1: Analysis and Synthesis. In the analysis phase, the GCI positions based on an arbitrary waveform are first recognized by the following revised GCI detection module, and then the waveform is broken down into overlapping short-term segments. Each segment is defined by two arbitrary successive GCI periods. Finally, segment lengths, size, and phase spectrum are used to represent these segments. In the synthesis phase, we convert them into overlapping short-term segments at appropriate segment lengths, orders of magnitude, and phase spectra, and then reconstruct the waveform using a technique similar to TD-PSOLA [17]."}, {"heading": "B. Glottal Closure Instant Detection", "text": "The GCI positions determine the characteristics including segment lengths, magnitude and phase spectrum. Thus, the GCI detection method is of great importance. Among current GCI detection techniques, the highest robustness and reliability has been demonstrated. During detection, SEDREAMS, there is only one GCI position for each GCI segment [25]. This is a local assessment process without taking into account the GCI detection results in the neighborhood."}, {"heading": "IV. WAVEFORM MODELING", "text": "The state of the art SPSS usually models the size spectrum of the speech signals and discards the phase spectrum. [1] For the detailed implementations of the moving mean filter and the interval determination, refer to [25] During synthesis, a vocoder based on the minimum phase or zero phase spectrum is often used along with the generated magnitude spectrum of the speech recordings to produce the synthesized output. Nevertheless, the phase spectrum has proven to be essential for speech perception. Speech quality of the vocoded outputs is often deteriorated along with the generated magnitude spectrum of the speech recordings [6]. This could shed light on SPSS, where voice waveform is modeled with phase information in addition to the existing magnitude spectrum. In our work, speech signals are modeled by the corresponding magnitude and phase spectra without the use of a vocoder."}, {"heading": "V. EXPERIMENTS", "text": "We conducted two experiments to assess the effectiveness of our waveform representation frame. In the waveform reconstruction experiment, objective and subjective assessments were conducted to compare the performance between our frame and other three vocoders: STRAIGHT, TandemSTRAIGHT [32] and AHOCoder [33]. As we know, STRAIGHT is a very popular vocoder used for speech analysis and reconstruction, and Tandem-STRAIGHT is the upgrade version of STRAIGHT. AHOCoder is reportedly of similar quality to STRAIGHT. In the waveform modeling experiment, we trained a text-to-speech (TTS) system based on our frame and also a TTS base system [4] for comparison. This baseline is a ground-breaking approach compared to STRAIGHT."}, {"heading": "A. Experiment on Waveform Reconstruction", "text": "The reconstructed voice waveform was then used for objective and subjective ratings. (RMSE) Objective rating: In the objective rating we calculated the root mean square error (RMSE) between the reconstructed and original voice wave signals in the spoken parts (RMSE), the unspoken parts (RMSE) and the whole waveform (RMSE). The results are shown in the table. These spoken / unspoken results from our frame and the three vocoders generally represent the performance on vowels / consonants. The objective rating result shows that the performance of our frame is much better than that of the three waveforms (RMSE)."}, {"heading": "B. Experiment on Waveform Modeling", "text": "In the baseline DBLSTM-RNN-based TTS [4], STRAIGHT is used to vocode the voice waveform through a moving window of 25 ms and move every 5 ms. The magnitude spectrum generated by STRAIGHT was converted to LSP. The dimensionality of the input contextual label is 427. The output function contains a linguistic / non-lingual flag (1 dimension), protocol F0 (1 dimension), LSP (40 dimensions) and gain (1 dimension), a total of 43 dimensions. As proposed in [4], a neural network with two BLSTM layers sitting on two feed layers with 256 nodes in each layer is used to input the DBLSTM-RNNN-based T.For our TTS system, properties were extracted from the short-term segments specified by GCI sites. The input line format is the same as the baseline."}, {"heading": "VI. CONCLUSIONS AND FUTURE WORK", "text": "This work proposed a glottal-synchronous-based framework for displaying waveforms for high-quality statistical parametric speech synthesis. Speech signals were represented by size and phase full spectral components, without the use of a vocoder. We revised the SEDREAMS GCI recognition approach to improve the functional stability for statistical modeling. Both objective and subjective assessments were conducted to assess the reconstruction performance of our framework. Results suggest that the proposed framework, compared to the reconstructed signal obtained from three popular vocoders, shows promising results in RMSE in the temporal speech waveform and preference core.We also proposed a platform for speech modeling. DBLSTM-RNN is used to jointly model the corresponding size and phase spectra with the corresponding vocoders, and group delay-based phaseaseaseaseaseaseaseaseaseaseaseaseaseaseaseaseaseaseaseaseaseaseaseaseaseaseaseaseaseaseaseaseaseaseaseaseaseaseaseaseaseaseaseaseaseaseaseaseaseaseaseaseaseaseaseaseaseaseaseaseaseaseaseaseaseaseaseaseaseaseaseaseaseaseaseaseaseaseaseaseaseaseaseaseaseaseaseaseaseaseaseaseaseaseaseaseaseaseaseaseaseaseaseaseaseaseaseaseaseaseaseaseaseaseaseaseaseaseaseaseaseaseaseaseaseaseaseaseaseaseaseaseaseaseaseaseaseaseaseaseaseaseaseaseaseaseaseaseaseaseaseaseaseaseaseaseas"}, {"heading": "ACKNOWLEDGMENT", "text": "This work was supported by the National Natural Science Foundation of China (61175018 and 61571363)."}], "references": [{"title": "Simultaneous modeling of spectrum, pitch and duration in HMM-based speech synthesis", "author": ["T. Yoshimura", "K. Tokuda", "T. Masuko", "T. Kobayashi", "T. Kitamura"], "venue": "Proc. Eurospeech, 1999, pp. 2347\u20132350.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 1999}, {"title": "Statistical parametric speech synthesis", "author": ["H. Zen", "K. Tokuda", "A. Black"], "venue": "Speech Communication, vol. 51, no. 11, pp. 1039\u20131064, 2009.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2009}, {"title": "Statistical parametric speech synthesis using deep neural networks", "author": ["H. Zen", "A. Senior", "M. Schuster"], "venue": "Proc. ICASSP. IEEE, 2013, pp. 7962\u20137966.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2013}, {"title": "TTS synthesis with bidirectional LSTM based recurrent neural networks", "author": ["Y. Fan", "Y. Qian", "F. Xie", "F. Soong"], "venue": "Proc. Interspeech, 2014, pp. 1964\u20131968.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2014}, {"title": "Acoustic Theory of Speech Production", "author": ["G. Fant"], "venue": null, "citeRegEx": "5", "shortCiteRegEx": "5", "year": 1960}, {"title": "Investigating source and filter contributions, and their interaction, to statistical parametric speech synthesis", "author": ["T. Merritt", "T. Raitio", "S. King"], "venue": "Proc. Interspeech, 2014, pp. 1509\u20131513.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2014}, {"title": "On the use of a sinusoidal model for speech synthesis in textto-speech", "author": ["M. Crespo", "P. Velasco", "L. Serrano", "J. Sardina"], "venue": "Progress in Speech Synthesis, pp. 57\u201370. Springer, 1997.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 1997}, {"title": "The effect of group delay spectrum on timbre", "author": ["H. Banno", "K. Takeda", "F. Itakura"], "venue": "Acoustical Science and Technology, vol. 23, no. 1, pp. 1\u20139, 2002.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2002}, {"title": "Phase minimization for glottal model estimation", "author": ["G. Degottex", "A. Roebel", "X. Rodet"], "venue": "Audio, Speech, and Language Processing, IEEE Transactions on, vol. 19, no. 5, pp. 1080\u20131090, 2011.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2011}, {"title": "Iterative reconstruction of speech from short-time Fourier transform phase and magnitude spectra", "author": ["L. Alsteris", "K. Paliwal"], "venue": "Computer Speech & Language, vol. 21, no. 1, pp. 174\u2013186, 2007.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2007}, {"title": "Using phase spectrum information for improved speech recognition performance", "author": ["R. Schluter", "H. Ney"], "venue": "Proc. ICASSP. IEEE, 2001, vol. 1, pp. 133\u2013136.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2001}, {"title": "On the importance of phase in human speech recognition", "author": ["G. Shi", "M. Shanechi", "P. Aarabi"], "venue": "Audio, Speech, and Language Processing, IEEE Transactions on, vol. 14, no. 5, pp. 1867\u20131874, 2006.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 1867}, {"title": "Squared error as a measure of perceived phase distortion", "author": ["H. Pobloth", "W. Kleijn"], "venue": "The Journal of the Acoustical Society of America, vol. 114, no. 2, pp. 1081\u2013 1094, 2003.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2003}, {"title": "Dominance spectrum based v/uv classification and F0 estimation", "author": ["T. Nakatani", "T. Irino", "P. Zolfaghari"], "venue": "Proc. Eurospeech, 2003, pp. 2313\u20132316.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2003}, {"title": "Usefulness of phase spectrum in human speech perception", "author": ["K. Paliwal", "L. Alsteris"], "venue": "Proc. Eurospeech, 2003, pp. 2117\u20132120.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2003}, {"title": "The importance of phase on voice quality assessment", "author": ["M. Koutsogiannaki", "O. Simantiraki", "G. Degottex", "Y. Stylianou"], "venue": "Proc. Interspeech, 2014, pp. 1653\u20131657.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2014}, {"title": "Pitch-synchronous waveform processing techniques for text-to-speech synthesis using diphones", "author": ["E. Moulines", "F. Charpentier"], "venue": "Speech communication, vol. 9, no. 5, pp. 453\u2013467, 1990.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 1990}, {"title": "MBR-PSOLA: Text-to-speech synthesis based on an MBE re-synthesis of the segments database", "author": ["T. Dutoit", "H. Leich"], "venue": "Speech Communication, vol. 13, no. 3, pp. 435\u2013440, 1993.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 1993}, {"title": "Removing linear phase mismatches in concatenative speech synthesis", "author": ["Y. Stylianou"], "venue": "Speech and Audio Processing, IEEE Transactions on, vol. 9, no. 3, pp. 232\u2013 239, 2001.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2001}, {"title": "Directly modeling speech waveforms by neural networks for statistical parametric speech synthesis", "author": ["K. Tokuda", "H. Zen"], "venue": "Proc. ICASSP, 2015, pp. 4215\u2013 4219.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2015}, {"title": "Complex cepstrum as phase information in statistical parametric speech synthesis", "author": ["R. Maia", "M. Akamine", "M. Gales"], "venue": "Proc. ICASSP. IEEE, 2012, pp. 4581\u2013 4584.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2012}, {"title": "Determination of instants of significant excitation in speech using group delay function", "author": ["R. Smits", "B. Yegnanarayana"], "venue": "Speech and Audio Processing, IEEE Transactions on, vol. 3, no. 5, pp. 325\u2013333, 1995.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 1995}, {"title": "Modeling spectral envelopes using restricted Boltzmann machines and deep belief networks for statistical parametric speech synthesis", "author": ["Z. Ling", "L. Deng", "D. Yu"], "venue": "Audio, Speech, and Language Processing, IEEE Transactions on, vol. 21, no. 10, pp. 2129\u20132139, 2013.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2013}, {"title": "Text-to-Speech Synthesis, United Kingdom", "author": ["P. Taylor"], "venue": "University of Cambridge,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2007}, {"title": "Glottal closure and opening instant detection from speech signals", "author": ["T. Drugman", "T. Dutoit"], "venue": "Proc. Interspeech, 2009, pp. 2891\u20132894.", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2009}, {"title": "Detection of glottal closure instants from speech signals: a quantitative review", "author": ["T. Drugman", "M. Thomas", "J. Gudnason", "P. Naylor", "T. Dutoit"], "venue": "Audio, Speech, and Language Processing, IEEE Transactions on, vol. 20, no. 3, pp. 994\u20131006, 2012.", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2012}, {"title": "Restructuring speech representations using a pitch adaptive time-frequency smoothing and an instantaneousfrequency-based F0 extraction: Possible role of a repetitive structure in sounds", "author": ["H. Kawahara", "I. Masuda-Katsuse", "A. De Cheveigne"], "venue": "Speech communication, vol. 27, no. 3, pp. 187\u2013207, 1999.", "citeRegEx": "27", "shortCiteRegEx": null, "year": 1999}, {"title": "Supervised Sequence Labelling with Recurrent", "author": ["A. Graves"], "venue": "Neural Networks,", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2012}, {"title": "Speech recognition with deep recurrent neural networks", "author": ["A. Graves", "A. Mohamed", "G. Hinton"], "venue": "Proc. ICASSP. IEEE, 2013, pp. 6645\u20136649.", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2013}, {"title": "Line spectrum pair (LSP) and speech data compression", "author": ["F.K. Soong", "B.-H. Juang"], "venue": "Proc. ICASSP. IEEE, 1984, pp. 37\u201340.", "citeRegEx": "30", "shortCiteRegEx": null, "year": 1984}, {"title": "Line spectrum representation of linear predictor coefficients of speech signals", "author": ["F. Itakura"], "venue": "The Journal of the Acoustical Society of America, vol. 57, pp. S35, 1975.", "citeRegEx": "31", "shortCiteRegEx": null, "year": 1975}, {"title": "TANDEM-STRAIGHT: A temporally stable power spectral representation for periodic signals and applications to interference-free spectrum, F0, and aperiodicity estimation", "author": ["H. Kawahara", "M. Morise", "T. Takahashi", "R. Nisimura", "T. Irino", "H. Banno"], "venue": "Proc. ICASSP, 2008, pp. 3933\u20133936.", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2008}, {"title": "Harmonics plus noise model based vocoder for statistical parametric speech synthesis", "author": ["D. Erro", "I. Sainz", "E. Navas", "I. Hernaez"], "venue": "IEEE Journal of Selected Topics in Signal Processing, vol. 8, no. 2, pp. 184\u2013194, 2014.", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2014}], "referenceMentions": [{"referenceID": 0, "context": "Statistical parametric speech synthesis (SPSS) has been increasingly popular due to its compact and flexible representation of voice characteristics [1].", "startOffset": 149, "endOffset": 152}, {"referenceID": 1, "context": "Conventionally, in an SPSS system, we firstly extract parametric representations of speech including spectral and excitation parameters from a speech database and then model them with a set of models [2].", "startOffset": 200, "endOffset": 203}, {"referenceID": 1, "context": ", hidden Markov model (HMM)-based SPSS [2], deep neural network (DNN)-based SPSS [3] and deep bidirectional long short-term memory recurrent neural network (DBLSTM-RNN)-based SPSS [4].", "startOffset": 39, "endOffset": 42}, {"referenceID": 2, "context": ", hidden Markov model (HMM)-based SPSS [2], deep neural network (DNN)-based SPSS [3] and deep bidirectional long short-term memory recurrent neural network (DBLSTM-RNN)-based SPSS [4].", "startOffset": 81, "endOffset": 84}, {"referenceID": 3, "context": ", hidden Markov model (HMM)-based SPSS [2], deep neural network (DNN)-based SPSS [3] and deep bidirectional long short-term memory recurrent neural network (DBLSTM-RNN)-based SPSS [4].", "startOffset": 180, "endOffset": 183}, {"referenceID": 4, "context": "It is based on the source-filter model [5], which assumes a stationary speech segment is generated by passing a sound source through a vocal tract filter.", "startOffset": 39, "endOffset": 42}, {"referenceID": 5, "context": "However, in [6], their subjective listening test shows clear degradation of quality in vocoded speech.", "startOffset": 12, "endOffset": 15}, {"referenceID": 6, "context": "Besides, to assure interframe coherence [7], a minimum phase hypothesis [7] has been used in most vocoders, which ignores the natural mixedphase characteristics of speech signals, resulting in apparent degradation of the speech waveform quality.", "startOffset": 40, "endOffset": 43}, {"referenceID": 6, "context": "Besides, to assure interframe coherence [7], a minimum phase hypothesis [7] has been used in most vocoders, which ignores the natural mixedphase characteristics of speech signals, resulting in apparent degradation of the speech waveform quality.", "startOffset": 72, "endOffset": 75}, {"referenceID": 7, "context": "More and more works have reported the importance of phase information in different speech processing applications, such as speech synthesis [8, 9], iterative signal reconstruction [10], automatic speech recognition [11, 12], speech coding [13] and pitch extraction [14].", "startOffset": 140, "endOffset": 146}, {"referenceID": 8, "context": "More and more works have reported the importance of phase information in different speech processing applications, such as speech synthesis [8, 9], iterative signal reconstruction [10], automatic speech recognition [11, 12], speech coding [13] and pitch extraction [14].", "startOffset": 140, "endOffset": 146}, {"referenceID": 9, "context": "More and more works have reported the importance of phase information in different speech processing applications, such as speech synthesis [8, 9], iterative signal reconstruction [10], automatic speech recognition [11, 12], speech coding [13] and pitch extraction [14].", "startOffset": 180, "endOffset": 184}, {"referenceID": 10, "context": "More and more works have reported the importance of phase information in different speech processing applications, such as speech synthesis [8, 9], iterative signal reconstruction [10], automatic speech recognition [11, 12], speech coding [13] and pitch extraction [14].", "startOffset": 215, "endOffset": 223}, {"referenceID": 11, "context": "More and more works have reported the importance of phase information in different speech processing applications, such as speech synthesis [8, 9], iterative signal reconstruction [10], automatic speech recognition [11, 12], speech coding [13] and pitch extraction [14].", "startOffset": 215, "endOffset": 223}, {"referenceID": 12, "context": "More and more works have reported the importance of phase information in different speech processing applications, such as speech synthesis [8, 9], iterative signal reconstruction [10], automatic speech recognition [11, 12], speech coding [13] and pitch extraction [14].", "startOffset": 239, "endOffset": 243}, {"referenceID": 13, "context": "More and more works have reported the importance of phase information in different speech processing applications, such as speech synthesis [8, 9], iterative signal reconstruction [10], automatic speech recognition [11, 12], speech coding [13] and pitch extraction [14].", "startOffset": 265, "endOffset": 269}, {"referenceID": 14, "context": "[15] have investigated the relative importance of short-time magnitude and phase spectra on speech perception through human perception listening test.", "startOffset": 0, "endOffset": 4}, {"referenceID": 15, "context": "[16] have proposed the phase distortion deviation feature, enabling to capture voice irregularities and highlights the importance of the phase spectrum in voice quality assessment.", "startOffset": 0, "endOffset": 4}, {"referenceID": 16, "context": "Time domain pitch-synchronous overlap-add (TD-PSOLA) [17] performs pitch-synchronous analysis, modification and synthesis.", "startOffset": 53, "endOffset": 57}, {"referenceID": 17, "context": "Multi-band re-synthesis pitch synchronous overlap add (MBR-PSOLA) [18] comments TD-PSOLA with three mismatches: phase mismatch, pitch mismatch, spectral envelope mismatch.", "startOffset": 66, "endOffset": 70}, {"referenceID": 18, "context": "The artificial processing in MBR-PSOLA decreases the quality of speech and leads to buzzy sound [19].", "startOffset": 96, "endOffset": 100}, {"referenceID": 19, "context": "[20] have proposed an approach to model cepstral coefficients to approximate the speech waveform.", "startOffset": 0, "endOffset": 4}, {"referenceID": 20, "context": "In [21], complex cepstrum has been used to embed phase information for hidden semi-Markov models (HSMM) speech modelling.", "startOffset": 3, "endOffset": 7}, {"referenceID": 21, "context": "GCIs refer to the moments of most significant excitation that occur at the level of the vocal folds during each glottal period [22].", "startOffset": 127, "endOffset": 131}, {"referenceID": 22, "context": "Full spectrum is used in this framework, which is in line with the satisfactory performance in recent deep learningbased TTS [23].", "startOffset": 125, "endOffset": 129}, {"referenceID": 16, "context": "Time domain pitch-synchronous overlap add (TD-PSOLA) is used for pitch and timing modification of speech signals [17], [24].", "startOffset": 113, "endOffset": 117}, {"referenceID": 23, "context": "Time domain pitch-synchronous overlap add (TD-PSOLA) is used for pitch and timing modification of speech signals [17], [24].", "startOffset": 119, "endOffset": 123}, {"referenceID": 23, "context": "Finally, modified segments are overlapped and added to produce the speech output [24].", "startOffset": 81, "endOffset": 85}, {"referenceID": 17, "context": "is because matched attributes on phase and pitch are needed [18].", "startOffset": 60, "endOffset": 64}, {"referenceID": 16, "context": "Then, the waveform is reconstructed using the similar technique as TD-PSOLA [17].", "startOffset": 76, "endOffset": 80}, {"referenceID": 24, "context": "Among the present GCIs detection techniques, the Speech Event Detection using the Residual Excitation And a Meanbased Signal (SEDREAMS) algorithm [25] is widely used.", "startOffset": 146, "endOffset": 150}, {"referenceID": 25, "context": "[26], SEDREAMS was shown to have the highest robustness and reliability.", "startOffset": 0, "endOffset": 4}, {"referenceID": 24, "context": "During the detection, SEDREAMS outputs only one GCI location for each GCI segment [25].", "startOffset": 82, "endOffset": 86}, {"referenceID": 26, "context": "In our implementation, M is five and the reference F0 is extracted by STRAIGHT [27].", "startOffset": 79, "endOffset": 83}, {"referenceID": 24, "context": "1For the detailed implementations of the moving average filter and interval determination, please refer to [25] 3200 3400 3600 3800 4000 4200 4400 4600 4800 -1 0 1 (a)", "startOffset": 107, "endOffset": 111}, {"referenceID": 5, "context": "The speech quality of vocoded outputs are found to be degraded from the original speech recordings [6].", "startOffset": 99, "endOffset": 102}, {"referenceID": 27, "context": "DBLSTMRNN is well-suited for learning sequential events apart from long time lags of unknown size [28].", "startOffset": 98, "endOffset": 102}, {"referenceID": 28, "context": "Promising performance in various speech applications is observed [29], [4].", "startOffset": 65, "endOffset": 69}, {"referenceID": 3, "context": "Promising performance in various speech applications is observed [29], [4].", "startOffset": 71, "endOffset": 74}, {"referenceID": 29, "context": "LSP, being an alternative LPC spectral representation, is robust and suitable for interpolation and modeling [30], [31].", "startOffset": 109, "endOffset": 113}, {"referenceID": 30, "context": "LSP, being an alternative LPC spectral representation, is robust and suitable for interpolation and modeling [30], [31].", "startOffset": 115, "endOffset": 119}, {"referenceID": 31, "context": "In the experiment on waveform reconstruction, objective and subjective evaluations were carried out to compare the performance between our framework and other three vocoders: STRAIGHT, TandemSTRAIGHT [32] and AHOCoder [33] respectively.", "startOffset": 200, "endOffset": 204}, {"referenceID": 32, "context": "In the experiment on waveform reconstruction, objective and subjective evaluations were carried out to compare the performance between our framework and other three vocoders: STRAIGHT, TandemSTRAIGHT [32] and AHOCoder [33] respectively.", "startOffset": 218, "endOffset": 222}, {"referenceID": 3, "context": "In the experiment on waveform modeling, we trained a text-to-speech (TTS) system based on our framework and also a baseline TTS system [4] as a comparison.", "startOffset": 135, "endOffset": 138}, {"referenceID": 26, "context": "031 STRAIGHT [27] 0.", "startOffset": 13, "endOffset": 17}, {"referenceID": 31, "context": "152 Tandem-STRAIGHT [32] 0.", "startOffset": 20, "endOffset": 24}, {"referenceID": 32, "context": "156 AHOCoder [33] 0.", "startOffset": 13, "endOffset": 17}, {"referenceID": 3, "context": "In the baseline DBLSTM-RNN-based TTS [4], STRAIGHT is used to vocode the speech waveform by a 25-ms moving window, and shifted every 5-ms.", "startOffset": 37, "endOffset": 40}, {"referenceID": 3, "context": "As suggested in [4], a neural network with two BLSTM layers sitting on two feed forward layers with 256 37.", "startOffset": 16, "endOffset": 19}, {"referenceID": 3, "context": "Measures Methods Our TTS system Baseline [4]", "startOffset": 41, "endOffset": 44}], "year": 2015, "abstractText": "State-of-the-art statistical parametric speech synthesis (SPSS) generally uses a vocoder to represent speech signals and parameterize them into features for subsequent modeling. Magnitude spectrum has been a dominant feature over the years. Although perceptual studies have shown that phase spectrum is essential to the quality of synthesized speech, it is often ignored by using a minimum phase filter during synthesis and the speech quality suffers. To bypass this bottleneck in vocoded speech, this paper proposes a phase-embedded waveform representation framework and establishes a magnitude-phase joint modeling platform for high-quality SPSS. Our experiments on waveform reconstruction show that the performance is better than that of the widely-used STRAIGHT. Furthermore, the proposed modeling and synthesis platform outperforms a leading-edge, vocoded, deep bidirectional long short-term memory recurrent neural network (DBLSTM-RNN)-based baseline system in various objective evaluation metrics conducted.", "creator": "LaTeX with hyperref package"}}}