{"id": "1510.01562", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "6-Oct-2015", "title": "Parameterized Neural Network Language Models for Information Retrieval", "abstract": "Information Retrieval (IR) models need to deal with two difficult issues, vocabulary mismatch and term dependencies. Vocabulary mismatch corresponds to the difficulty of retrieving relevant documents that do not contain exact query terms but semantically related terms. Term dependencies refers to the need of considering the relationship between the words of the query when estimating the relevance of a document. A multitude of solutions has been proposed to solve each of these two problems, but no principled model solve both. In parallel, in the last few years, language models based on neural networks have been used to cope with complex natural language processing tasks like emotion and paraphrase detection. Although they present good abilities to cope with both term dependencies and vocabulary mismatch problems, thanks to the distributed representation of words they are based upon, such models could not be used readily in IR, where the estimation of one language model per document (or query) is required. This is both computationally unfeasible and prone to over-fitting. Based on a recent work that proposed to learn a generic language model that can be modified through a set of document-specific parameters, we explore use of new neural network models that are adapted to ad-hoc IR tasks. Within the language model IR framework, we propose and study the use of a generic language model as well as a document-specific language model. Both can be used as a smoothing component, but the latter is more adapted to the document at hand and has the potential of being used as a full document language model. We experiment with such models and analyze their results on TREC-1 to 8 datasets.", "histories": [["v1", "Tue, 6 Oct 2015 13:07:31 GMT  (53kb,D)", "http://arxiv.org/abs/1510.01562v1", null]], "reviews": [], "SUBJECTS": "cs.IR cs.CL", "authors": ["benjamin piwowarski", "sylvain lamprier", "nicolas despres"], "accepted": false, "id": "1510.01562"}, "pdf": {"name": "1510.01562.pdf", "metadata": {"source": "CRF", "title": "Parameterized Neural Network Language Models for Information Retrieval", "authors": ["N. Despres", "S. Lamprier", "B. Piwowarski"], "emails": ["nicolas.despres@gmail.com", "sylvain.lamprier@lip6.fr", "benjamin@bpiwowar.net"], "sections": [{"heading": "1 Introduction", "text": "In fact, most of them are able to play by the rules that they have adopted in recent years."}, {"heading": "2 Related works", "text": "This section introduces related work by first introducing those who deal with the term dependencies and inmatch problems, and then introducing related work on Neural Network Language Models (NNLM)."}, {"heading": "2.1 Handling term dependencies", "text": "Such an approach was chosen by Fagan [7] for vector space models, while the equivalent of the language model was proposed in the late 1990s [22, 24, 8], where the authors suggested using a mixture of bigram and uniform language models, with the difference being how the Bigram language model or how bigrams are selected ([8] used a dependency grammar to generate candidate bigrams).This approach proved less successful, since more complex units imply sparser data [27], which in turn implies imprecise bigram probability estimates. An alternative, defining the mix within the quantum probability framework, was proposed by Sordoni et al."}, {"heading": "2.2 Vocabulary mismatch", "text": "One of the most commonly used techniques to address the problem of vocabulary mismatch is query enhancement, which relies on pseudo-relevant feedback, adding terms based on a number of (pseudo) relevant documents to the query. [13] It has been shown to improve search results in some cases, but is prone to the problem of query drift, which can be controlled using statistical theories such as portfolio theory [4]. The use of pseudo-relevance feedback is orthogonal to our work and could be used as an extension to estimate a query-relevant language model [11]. Global analyses can be used to enrich document representation by using co-occurence information at the dataset level."}, {"heading": "2.3 Neural Network Language Models", "text": "The idea of using neural networks to build language models emerged in the last ten years, and this field of research is part of the current, very active field of \"representation learning.\" Bengio et al. [2] is one of the first works to model text generation (at the word level) using neural networks, using a state (a vector in Rn) to represent history, a state that defines a probability distribution through words and can be updated with a new observation that makes it possible to define a language model through a text. Such an idea of the representation of texts in a latent space has long been explored."}, {"heading": "3 Neural Network IR Models", "text": "In this section, we first present some background information on classical language models for IR. Then, we present our contribution, which allows the resulting IR model to handle term dependencies and solve problems with term mismatches using representation techniques. Finally, we introduce a parametric extension that performs document-dependent transformations of the model. Since most models handle sequences, we define Xi... j as a sequence Xi, Xi + 1,..., Xj \u2212 1, Xj, and assume that the sequence is empty when i > j is used."}, {"heading": "3.1 Background: Language Models for IR", "text": "If a text is composed of a sequence of terms, it is unlikely that a sequence of terms t1... n, where each ti corresponds to a word in a predefined vocabulary, we can calculate the probability of observing this sequence, since the language model M as: P (t1... n | M) = N, where each ti corresponds to a word in a predefined vocabulary, we can consider the probability of observing this sequence as a simple but effective method of calculating the relevance of a document to a query [27]. There are different types of language models for IR, but one of the most standard models is to equate the relevance of document d with the probability of the language model by using the evaluated language model. P (d relevant to q) = P (q | Md), where Md is the so-called language model, which is the model within the family of model delM that maximizes probability."}, {"heading": "3.2 Neural Language Models for IR", "text": "Distributed representations of words and documents have long been known in IR as latent semantic indexing techniques, which are considered better induced things (1999 [5]). They are useful because they overcome the sparseness problem. \u2212 It may be the collection to which the document belongs, or any external collection of documents that we have just evoked, relying on the spatial relationship of the embedded objects. This has been exploited in IR to solve the vocabulary problem, but the idea of using this type of representation for language models is more recent [2]. Such language models are created using neural network language models (hence their name neural language models) and offer several advantages: \u2022 Compression capabilities offered by imaging techniques allow us to consider longer-term dependencies (i.e., longer n-grams) as classical approaches. \u2212 In this model neural models are created (hence their name, neural models are offered by neural models, which are neural models, and therefore neural documents)."}, {"heading": "3.2.1 Generic Neural Model", "text": "There are two types of neural network information that take into account an infinite context, and those that only take into account a limited number of previous terms. The former is based on recursive neural networks, while the latter are standard operations. In this thesis, we examine the use of upstream networks because they are easier to learn. Furthermore, we expect the document-specific model that we will describe in the next section to have a longer duration of dependencies. Input of the neural network corresponds to n \u2212 1 previous terms. For the first n \u2212 1 words of a document, we use a special \"padding\" term that corresponds to each vector zti in the vector space Rm0. The n \u2212 1 vectors are then transformed by a function (descr below) into a state vector s in Rmf, in which f is the index of the last layer."}, {"heading": "3.2.2 Document-Dependent Model", "text": "This language model can be a good alternative to the multinomial universal models used for smoothing. However, we believe that taking into account specific characteristics of the document at hand leads to a better language model, and thus to better retrievable results. In fact, as explained above in an example of Boston, conceptual relationships may differ from documents to others. Learning specific neural language models for all individual documents is not feasible for the same reasons as for n-gram language models for n > 1: learning term dependencies and semantic kinship on the sequences contained in a single document is likely to lead to translation problems due to the lack of vocabulary diversity and sequence patterns. To overcome this problem, we are following the approach of [12], where the probability distribution of a generic neural network model is modified by a relatively small number of documents that can be learned."}, {"heading": "4 Experiments", "text": "We used the TREC-1 to 8 surveys for experimenting with our models. We used BM25 = = 3.19] with default parameter settings (k1 = 1.2 and b = 0.5) as a starting point, as there is reasonable performance on these collections. We omitted the comparison with stronger baselines, as we were initially interested in comparing the different models with each other. The collection was pre-processed with the Porter model, as its learned representation would have been miscalculated. In order to learn our language models, we used the same pre-processing (no stopwords, porter stemmers), but removed words that occur less than 5 times in the datasets, as their learned representation would have been miscalculated. The vocabulary size we obtained was from 375,219 words. We used the word2vec [15] implementation 2 until the initial word representations and hierarchical parameters were calculated using a maximum learning rate of 0.5."}, {"heading": "5 Conclusion", "text": "In this thesis, we have proposed new parametric language models of neural networks developed specifically for ad hoc IR tasks. We have different variants for both a generic and a document-specific version of the3We have used a different technique here, as we aim for rapid convergence across a small number of parameters and a single document."}, {"heading": "M1* 0.0056 0.0024 0.0105 0.0061 0.0055 0.0119 0.0148 0.0093", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "M1+ 0.0044 0.0029 0.0127 0.0043 0.0046 0.0133 0.0120 0.0108", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "M2* 0.0054 0.0028 0.0147 0.0035 0.0115 0.0149 0.0132 0.0167", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "M2+ 0.0026 0.0020 0.0125 0.0050 0.0096 0.0154 0.0156 0.0125", "text": "Model. While the generic version of the model learns term dependencies and accurate representations of words at the collection level, our document-specific language model is modified for each document by using a small-dimensional vector that contains only a few hundred parameters and can therefore be efficiently and correctly learned from a single document. This vector is used to modify the state vector that represents the context of the word for which we want to calculate the likelihood of occurrence by using component-by-component multiplication or an additional operator. We experimented with TREC-1 to TREC-8 IR test collections, in which the 100 best results retrieved from BM25 were reordered using our generic and document-specific language models. Results show that using a document-specific language model improves results across a baseline (classical Jelinek-Mercer language model). We also demonstrated that document-specific model can achieve better document-specific results than the validated model of the specific documents."}], "references": [{"title": "Modeling higher-order term dependencies in information retrieval using query hypergraphs", "author": ["M. Bendersky", "W.B. Croft"], "venue": "In SIGIR \u201912,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2012}, {"title": "A Neural Probabilistic Language Model", "author": ["Y. Bengio", "R. Ducharme", "P. Vincent", "C. Jauvin"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2003}, {"title": "Extending BM25 with multiple query operators", "author": ["R. Blanco", "P. Boldi"], "venue": "In SIGIR \u201912,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2012}, {"title": "Reducing the risk of query expansion via robust constrained optimization", "author": ["K. Collins-Thompson"], "venue": "In CIKM", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2009}, {"title": "Indexing by latent semantic analysis", "author": ["Deerwester", "Scott", "Dumais", "Susan T", "Furnas", "George W", "Landauer", "Thomas K", "Harshman", "Richard"], "venue": null, "citeRegEx": "5", "shortCiteRegEx": "5", "year": 1990}, {"title": "Unsupervised Latent Concept Modeling to Identify Query Facets", "author": ["R. Deveaud", "E. SanJuan", "P. Bellot"], "venue": "In OAIR\u201913,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2013}, {"title": "Automatic Phrase Indexing for Document Retrieval: An Examination of Syntactic and Non-Syntactic Methods", "author": ["J.L. Fagan"], "venue": "In SIGIR\u201987,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 1987}, {"title": "Dependence language model for information retrieval", "author": ["J. Gao", "J.-Y. Nie", "G. Wu", "G. Cao"], "venue": "In SIGIR\u201904,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2004}, {"title": "A novel neighborhood based document smoothing model for information", "author": ["P. Goyal", "L. Behera", "T.M. McGinnity"], "venue": "retrieval. IR,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2013}, {"title": "A Convolutional Neural Network for Modelling Sentences", "author": ["N. Kalchbrenner", "E. Grefenstette", "P. Blunsom"], "venue": null, "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2014}, {"title": "A Generative Theory of Relevance", "author": ["V. Lavrenko"], "venue": null, "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2010}, {"title": "Distributed Representations of Sentences and Documents", "author": ["Q.V. Le", "T. Mikolov"], "venue": "In ICML\u201914,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2014}, {"title": "Introduction to Information Retrieval", "author": ["C.D. Manning", "P. Raghavan", "H. Sch\u00fctze"], "venue": null, "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2008}, {"title": "A Markov random field model for term dependencies", "author": ["D. Metzler", "W.B. Croft"], "venue": "In SIGIR,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2005}, {"title": "Distributed Representations of Words and Phrases and their Compositionality", "author": ["T. Mikolov", "I. Sutskever", "K. Chen", "G.S. Corrado", "J. Dean"], "venue": null, "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2013}, {"title": "Hierarchical Probabilistic Neural Network Language Model", "author": ["F. Morin", "Y. Bengio"], "venue": null, "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2005}, {"title": "Semantic Modelling with Long-Short-Term Memory for Information", "author": ["H. Palangi", "L. Deng", "Y. Shen", "J. Gao", "X. He", "J. Chen", "X. Song", "R. Ward"], "venue": "Retrieval. arXiv.org,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2014}, {"title": "A direct adaptive method for faster backpropagation learning: the RPROP algorithm", "author": ["M. Riedmiller", "H. Braun"], "venue": "In IEEE International Conference on Neural Networks,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 1993}, {"title": "The Probabilistic Relevance Framework: BM25 and Beyond", "author": ["S.E. Robertson", "H. Zaragoza"], "venue": "Foundations and Trends in Information Retrieval", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2009}, {"title": "Continuous Space Translation Models for Phrase-Based Statistical Machine Translation", "author": ["H. Schwenk"], "venue": "Proceedings of COLING 2012: Posters,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2013}, {"title": "Semantic compositionality through recursive matrix-vector spaces", "author": ["R. Socher", "C.D. Manning", "B. Huval", "A.Y. Ng"], "venue": "Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2012}, {"title": "A general language model for information retrieval", "author": ["F. Song", "W.B. Croft"], "venue": "In CIKM\u201999,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 1999}, {"title": "Modeling Term Dependencies with Quantum Language Models for IR", "author": ["A. Sordoni", "J.-Y. Nie", "Y. Bengio"], "venue": "Proceedings of the 36th international ACM SIGIR conference on Research and development in information retrieval,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2013}, {"title": "Biterm language models for document retrieval", "author": ["M. Srikanth", "R.K. Srihari"], "venue": "In SIGIR\u201902,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2002}, {"title": "Regularized Latent Semantic Indexing: A New Approach to Large-Scale Topic Modeling", "author": ["Q. Wang", "J. Xu", "H. Li", "N. Craswell"], "venue": "ACM TOIS,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2013}, {"title": "Parametric hidden Markov models for gesture recognition", "author": ["A.D. Wilson", "A.F. Bobick"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 1999}, {"title": "Statistical Language Models for Information Retrieval: A Critical Review", "author": ["C. Zhai"], "venue": "Foundations and Trends in Information Retrieval,", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2008}], "referenceMentions": [{"referenceID": 6, "context": "[7] proposed to consider pairs of successive terms (bi-grams) in vector space models.", "startOffset": 0, "endOffset": 3}, {"referenceID": 21, "context": "The same principle can be found in language models such as in [22] that performs mixtures of uni- and bi-gram models.", "startOffset": 62, "endOffset": 66}, {"referenceID": 13, "context": "Other works have sought to combine the scores of models by taking into account different co-occurrence patterns, such as [14] which proposes a Markov random field model to capture the term dependencies of queries and documents.", "startOffset": 121, "endOffset": 125}, {"referenceID": 1, "context": "Neural Language Models [2] have been successfully used in many natural language processing tasks, like part-of-speech tagging [2], semantic labeling and more recently, translation.", "startOffset": 23, "endOffset": 26}, {"referenceID": 1, "context": "Neural Language Models [2] have been successfully used in many natural language processing tasks, like part-of-speech tagging [2], semantic labeling and more recently, translation.", "startOffset": 126, "endOffset": 129}, {"referenceID": 11, "context": "An interesting alternative was proposed by Le and Mikolov [12] who recently published a neural network language model in which they propose to represent a context (a document, or a paragraph) as a vector that modifies the language model, which avoids building costly individual models for each document to consider.", "startOffset": 58, "endOffset": 62}, {"referenceID": 11, "context": "Our work is based on the findings of Le and Mikolov [12].", "startOffset": 52, "endOffset": 56}, {"referenceID": 11, "context": "We generalize the model proposed in [12], defining a more powerful architecture and new ways to consider individual specificities of the documents;", "startOffset": 36, "endOffset": 40}, {"referenceID": 6, "context": "Such an approach was taken by Fagan [7] for vector space models, while the language model counterpart were proposed in the late of 90s [22, 24, 8] where the authors proposed to use a mixture of the bigram and unigram language models, the difference being in how to estimate the bigram language model or on how bigram are selected ([8] used a dependency grammar parsing to generate candidate bigrams).", "startOffset": 36, "endOffset": 39}, {"referenceID": 21, "context": "Such an approach was taken by Fagan [7] for vector space models, while the language model counterpart were proposed in the late of 90s [22, 24, 8] where the authors proposed to use a mixture of the bigram and unigram language models, the difference being in how to estimate the bigram language model or on how bigram are selected ([8] used a dependency grammar parsing to generate candidate bigrams).", "startOffset": 135, "endOffset": 146}, {"referenceID": 23, "context": "Such an approach was taken by Fagan [7] for vector space models, while the language model counterpart were proposed in the late of 90s [22, 24, 8] where the authors proposed to use a mixture of the bigram and unigram language models, the difference being in how to estimate the bigram language model or on how bigram are selected ([8] used a dependency grammar parsing to generate candidate bigrams).", "startOffset": 135, "endOffset": 146}, {"referenceID": 7, "context": "Such an approach was taken by Fagan [7] for vector space models, while the language model counterpart were proposed in the late of 90s [22, 24, 8] where the authors proposed to use a mixture of the bigram and unigram language models, the difference being in how to estimate the bigram language model or on how bigram are selected ([8] used a dependency grammar parsing to generate candidate bigrams).", "startOffset": 135, "endOffset": 146}, {"referenceID": 7, "context": "Such an approach was taken by Fagan [7] for vector space models, while the language model counterpart were proposed in the late of 90s [22, 24, 8] where the authors proposed to use a mixture of the bigram and unigram language models, the difference being in how to estimate the bigram language model or on how bigram are selected ([8] used a dependency grammar parsing to generate candidate bigrams).", "startOffset": 331, "endOffset": 334}, {"referenceID": 26, "context": "This approach has proved to be not so successful, most probably because more complex units imply sparser data [27], which in turn implies inaccurate bigram probability estimations.", "startOffset": 110, "endOffset": 114}, {"referenceID": 22, "context": "[23].", "startOffset": 0, "endOffset": 4}, {"referenceID": 13, "context": "Metzler and Croft [14] proposed a Markov Random Field approach where each clique corresponds to a set or sequence of query terms.", "startOffset": 18, "endOffset": 22}, {"referenceID": 0, "context": "[1] who considered sets of concepts (phrases or set of words) instead of set of words.", "startOffset": 0, "endOffset": 3}, {"referenceID": 2, "context": "In a different probabilistic framework, Blanco [3] proposed to extend BM25F, a model able to take into account different source of evidence to compute the importance of a term for a document, to take into account term proximity by defining operators on so-called virtual regions.", "startOffset": 47, "endOffset": 50}, {"referenceID": 12, "context": "2 Vocabulary mismatch One of the most used techniques to deal with the problem of vocabulary mismatch is query expansion, based on pseudo-relevance feedback, whereby terms are added to the query based on a set of (pseudo) relevant documents [13].", "startOffset": 241, "endOffset": 245}, {"referenceID": 3, "context": "It has been shown to improve search results in some cases, but is prone to the problem of query drift, which can be controlled using statistical theories like the portfolio theory [4].", "startOffset": 180, "endOffset": 183}, {"referenceID": 10, "context": "Using pseudorelevance feedback is orthogonal to our work, and could be used as an extension to estimate a query relevance language model [11].", "startOffset": 137, "endOffset": 141}, {"referenceID": 8, "context": "[9] use a term association matrix to modify the term-document matrix and account for term relatedness.", "startOffset": 0, "endOffset": 3}, {"referenceID": 4, "context": "Dimensionality reduction techniques such as latent semantic models have been proposed for dealing with vocabulary mismatch issues [5].", "startOffset": 130, "endOffset": 133}, {"referenceID": 24, "context": "However, such models do not work well in practice because many document specific terms are discarded during the dimensionality reduction process [25].", "startOffset": 145, "endOffset": 149}, {"referenceID": 24, "context": "This approach has been followed by [25] in vector spaces and Deveaud et al.", "startOffset": 35, "endOffset": 39}, {"referenceID": 5, "context": "[6] for probabilistic (LDA) models.", "startOffset": 0, "endOffset": 3}, {"referenceID": 1, "context": "[2] is one of the first works that model text generation (at a word level) using neural networks.", "startOffset": 0, "endOffset": 3}, {"referenceID": 14, "context": "A recent successful work is the well-known Word2Vec model [15] that proposed a simple neural network architecture to predict a term within a predefined window.", "startOffset": 58, "endOffset": 62}, {"referenceID": 20, "context": "Other works based on similar ideas were applied to sentiment detection [21] or automated translation [20].", "startOffset": 71, "endOffset": 75}, {"referenceID": 19, "context": "Other works based on similar ideas were applied to sentiment detection [21] or automated translation [20].", "startOffset": 101, "endOffset": 105}, {"referenceID": 1, "context": "Closer to IR, the idea of representing the history as a state, as in [2], in a vectorial space has been exploited by Palangi et al.", "startOffset": 69, "endOffset": 72}, {"referenceID": 16, "context": "[17] who proposed to use the state obtained at the end of the document (resp.", "startOffset": 0, "endOffset": 4}, {"referenceID": 25, "context": "Wilson and Bobick [26] proposed probabilistic models where the means of Gaussian distribution vary linearly as a function of the context.", "startOffset": 18, "endOffset": 22}, {"referenceID": 11, "context": "This idea has been exploited by Le and Mikolov [12], who proposed a parameterized language model which they experimented for sentiment analysis and an information related task where relationships between query snippets are encoded by distances of their representations in the considered projection space.", "startOffset": 47, "endOffset": 51}, {"referenceID": 26, "context": "Language models are used in IR as a simple yet effective way to compute the relevance of a document to a query [27].", "startOffset": 111, "endOffset": 115}, {"referenceID": 21, "context": "Works like [22] have explored the use of language models with n > 1.", "startOffset": 11, "endOffset": 15}, {"referenceID": 0, "context": "Formally, given a smoothing coefficient \u03bb \u2208 [0, 1], the language model of the document becomes:", "startOffset": 44, "endOffset": 50}, {"referenceID": 4, "context": "2 Neural Language Models for IR Distributed representations of words and documents have been known for long in IR as Latent Semantic Indexing techniques were introduced in 1999 [5].", "startOffset": 177, "endOffset": 180}, {"referenceID": 1, "context": "This has been exploited in IR to deal with vocabulary mismatch problem, but the idea of leveraging this kind of representation for language models is more recent [2].", "startOffset": 162, "endOffset": 165}, {"referenceID": 9, "context": "Maximum pooling layers are useful in deep neural networks because they introduce an invariant [10], i.", "startOffset": 94, "endOffset": 98}, {"referenceID": 15, "context": "In our model, we use a Hierarchical SoftMax (HSM), which corresponds to an efficient multivariate differentiable function that maps an input vector (given by \u03c6 ) to a vector in RV whose values sum to 1 [16].", "startOffset": 202, "endOffset": 206}, {"referenceID": 14, "context": "This was used by Mikolov [15] to ensure a better learning, and we used the same settings.", "startOffset": 25, "endOffset": 29}, {"referenceID": 11, "context": "To overcome this problem, we follow the approach of [12] where the probability distribution of a generic neural network language model is modified by a relatively small set of parameters that can be reliably learned from the observation of a single document: The dimension of such a vector (100-200) is typically much smaller than the number of parameters of a multinomial distribution (size of the vocabulary).", "startOffset": 52, "endOffset": 56}, {"referenceID": 25, "context": "Using parameters to modify the behavior of a generative probabilistic model has been used in many works in signal processing, like gesture recognition [26], where the model has to be quickly adapted to a specific user: Such models benefit from a large source of information (the collection of all gestures or documents in our case) and at the same time can be made specific enough to describe an individual user or document.", "startOffset": 151, "endOffset": 155}, {"referenceID": 18, "context": "We used as a baseline BM25 [19] with standard parameter settings (k1 = 1.", "startOffset": 27, "endOffset": 31}, {"referenceID": 14, "context": "We used the word2vec [15] implementation2 to pre-compute the initial word representations and hierarchical SoftMax parameters.", "startOffset": 21, "endOffset": 25}, {"referenceID": 17, "context": "We also used a gradient descent3 with resilient backpropagation (Rprop) [18] using an initial learning rate of 0.", "startOffset": 72, "endOffset": 76}, {"referenceID": 10, "context": "Future work will thus study different architectures of the neural language model (including recurrent to consider longer dependencies), as well as use a relevance language model [11] based on pseudo-relevance feedback \u2013 this might be more reliable since language models will be learned from documents and applied on documents (rather than queries).", "startOffset": 178, "endOffset": 182}], "year": 2015, "abstractText": "Information Retrieval (IR) models need to deal with two difficult issues, vocabulary mismatch and term dependencies. Vocabulary mismatch corresponds to the difficulty of retrieving relevant documents that do not contain exact query terms but semantically related terms. Term dependencies refers to the need of considering the relationship between the words of the query when estimating the relevance of a document. A multitude of solutions has been proposed to solve each of these two problems, but no principled model solve both. In parallel, in the last few years, language models based on neural networks have been used to cope with complex natural language processing tasks like emotion and paraphrase detection. Although they present good abilities to cope with both term dependencies and vocabulary mismatch problems, thanks to the distributed representation of words they are based upon, such models could not be used readily in IR, where the estimation of one language model per document (or query) is required. This is both computationally unfeasible and prone to over-fitting. Based on a recent work that proposed to learn a generic language model that can be modified through a set of document-specific parameters, we explore use of new neural network models that are adapted to ad-hoc IR tasks. Within the language model IR framework, we propose and study the use of a generic language model as well as a document-specific language model. Both can be used as a smoothing component, but the latter is more adapted to the document at hand and has the potential of being used as a full document language model. We experiment with such models and analyze their results on TREC-1 to 8 datasets.", "creator": "LaTeX with hyperref package"}}}