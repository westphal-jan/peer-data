{"id": "1510.01717", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "6-Oct-2015", "title": "Language Segmentation", "abstract": "Language segmentation consists in finding the boundaries where one language ends and another language begins in a text written in more than one language. This is important for all natural language processing tasks. The problem can be solved by training language models on language data. However, in the case of low- or no-resource languages, this is problematic. I therefore investigate whether unsupervised methods perform better than supervised methods when it is difficult or impossible to train supervised approaches. A special focus is given to difficult texts, i.e. texts that are rather short (one sentence), containing abbreviations, low-resource languages and non-standard language. I compare three approaches: supervised n-gram language models, unsupervised clustering and weakly supervised n-gram language model induction. I devised the weakly supervised approach in order to deal with difficult text specifically. In order to test the approach, I compiled a small corpus of different text types, ranging from one-sentence texts to texts of about 300 words. The weakly supervised language model induction approach works well on short and difficult texts, outperforming the clustering algorithm and reaching scores in the vicinity of the supervised approach. The results look promising, but there is room for improvement and a more thorough investigation should be undertaken.", "histories": [["v1", "Tue, 6 Oct 2015 19:35:23 GMT  (584kb)", "http://arxiv.org/abs/1510.01717v1", "Master Thesis"]], "COMMENTS": "Master Thesis", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["david alfter"], "accepted": false, "id": "1510.01717"}, "pdf": {"name": "1510.01717.pdf", "metadata": {"source": "CRF", "title": null, "authors": [], "emails": [], "sections": [{"heading": null, "text": "U TF II C D HLanguage SegmentationAuthor: David Asupervisor: Prof. Dr. Caroline SDr. Sven NAugust 18, 2015"}, {"heading": "Erkl\u00e4rung zur Masterarbeit", "text": "In fact, it is too early to say that we will be able, that we will be able to do what we need to do to do it."}, {"heading": "List of Figures", "text": "Personnel Personnel Personnel Personnel Personnel Personnel Personnel Personnel Personnel Personnel Personnel Personnel Personnel Personnel Personnel Personnel Personnel Personnel Personnel Personnel Personnel Personnel Personnel Personnel Personnel Personnel Personnel Personnel Personnel Personnel Personnel Personnel Personnel Personnel Personnel Personnel Personnel Personnel Personnel Personnel Personnel Personnel Personnel Personnel PersonPersonnel Personnel Personnel Personnel Personnel Personnel Personnel Personnel Personnel Personnel Personnel Personnel Personnel Personnel Personnel Personnel Personnel Personnel Personnel Personnel Personnel Personnel Personnel Personnel Personnel Personnel Personnel Personnel Personnel Personnel Personnel Personnel Personnel Personnel Personnel Personnel Personnel Personnel Personnel Personnel Personnel Personnel Personnel Personnel Personnel Personnel Personnel Personnel Personnel Personnel Personnel Personnel Personnel Personnel Personnel Personnel Personnel Personnel Personnel Personnel Personnel Personnel Personnel Personnel Personnel Personnel Personnel Personnel Personnel Personnel Personnel Personnel Personnel Personnel Personnel Personnel Personnel Personnel Personnel Personnel Personnel Personnel Personnel Personnel Personnel Personnel Personnel Personnel Personnel Personnel Personnel Personnel Personnel Personnel Personnel Personnel Personnel Personnel Personnel Personnel Personnel Personnel Personnel Personnel Personnel Personnel Personnel Personnel Personnel Personnel Personnel Personnel Personnel Personnel Personnel Personnel Personnel Personnel Personnel Personnel Personnel Personnel Personnel Personnel Personnel Personnel Personnel Personnel Personnel Personnel Personnel Personnel Personnel Personnel Personnel Personnel Personnel Personnel Personnel Personnel Personnel Personnel Personnel Personnel Personnel Personnel Personnel Personnel Personnel Personnel Personnel Personnel Personnel Personnel Personnel Personnel Personnel Personnel Personnel Personnel Personnel Personnel Personnel Personnel Personnel Personnel Personnel Personnel Personnel Personnel Personnel Personnel Personnel Personnel Personnel Personnel Personnel Personnel Personnel Personnel Personnel Personnel Personnel Personnel Personnel Personnel Personnel Personnel Personnel Personnel Personnel Personnel Personnel Personnel Personnel Personnel Personnel Personnel Personnel Personnel Personnel Personnel Personnel Personnel Personnel Personnel Personnel Personnel Personnel Personnel Person"}, {"heading": "List of Tables", "text": "......................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................"}, {"heading": "List of Algorithms", "text": "....................................................................................................................................................................................................................................................."}, {"heading": "Contents", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "1 Introduction 1", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "2 Related work 2", "text": "......................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................"}, {"heading": "4 Experimental setup 18", "text": "......................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................"}, {"heading": "5 Results 38", "text": "5.1 N-gram language model............................................................................................................................."}, {"heading": "6 Discussion 54", "text": "6.1 N-gram language models......................................................................................................................"}, {"heading": "7 Conclusion 65", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "8 Appendix 72", "text": ".)......................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................."}, {"heading": "1 Introduction", "text": "In fact, the use of \"traditional\" monolingual natural language processing components on mixed language data leads to lousy results (Jain and Bhat, 2014). Even if the results are not terrible, language identification and segmentation can improve overall results. For example, by identifying foreign language inclusions in an otherwise monolingual text, the accuracy of parsers can be increased (Alex et al., 2007). An important point to take into account is the difference between language identification and language segmentation. Language identification concerns the recognition of the language itself. It is possible to use language identification for language segmentations that indeed arise from the identification of languages in a text."}, {"heading": "2 Related work", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "2.1 N-Grams and rank order statistics", "text": "Cavnar and Trenkle (1994) use a N-gram language model for language identification purposes. Their program \"Textcat\" is designed to classify documents by language. e system then computes n-grams for 1 6 n 6 5 from training data and assigns the n-grams according to the inverse frequency, i.e. from the most common n-grams to the rarest n-grams. e numerical frequency data are then discarded and presented only by nature. e category with the lowest difference score is taken as a category for the document, which consists of these ngram lists for each category (i.e., language to classify).e New data are classified by first computing the n-gram profile and then comparing the profile with existing profiles. e category with the lowest difference score score is taken as a category for the document. e Score-Score-Score-Score-Score-Score-Score-Score-Score-Score-Score-Score-Score-Score-Score-Score-Score-Score-Score-Score-Score-Score-Score-Score-Score-Score-Score-Score-Score-Score-Score-Score-Score-Score-Score-Score-Score-Score-Score-Score-Score-Score-Score-Score-Score-Score-Score-Score-Score-Score-Score-Score-Score-Score-Score-Score-Score-Score-Score-Score-Score-Score-Score-Score-Score-Score-Score-Score-Score-Score-Score-Score-Score-Score-Score-Score-Score-Score-Score-Score-Score-Score-Score-Score-Score-Score-Score-Score-Score-Score-Score-Score-Score-Score-Score-Score-Score-Score-Score-Score-Score-Score-Score-Score-Score-Score-Score-"}, {"heading": "2.2 N-Grams and maximum likelihood estimator", "text": "Dunning (1994) also uses an n-gram language model for language identification purposes. The e program calculates n-gram and its frequency from the training data and estimates the probability P of a given string using the Maximum Likelihood Estimator (MLE) with Laplace addition one smoothing. Formal: P (wi | w1,.., wi \u2212 1) = C (w1,.., wi \u2212 1) + | V \u2212 1) + | (2) with C (w1,.,., Ci) the number of times the n-gram w1,..., wi (w1,.,., Ci \u2212 1) the number of times the (n \u2212 1) -gram w1,."}, {"heading": "2.3 Trigrams and short words", "text": "Grefenstee (1995) compares trigrams with short words for language identification. Short words are functional words that are typical and very common in a particular language. e trigram language guesser was trained on one million characters of text in 10 languages: Danish, Dutch, English, French, German, Italian, Norwegian, Portuguese, Spanish and Swedish. From the same texts, all words with 5 or less characters were counted for the short word strategy. Results indicate that the trigram approach works for small text fragments of up to 15 words, while for any text longer than 15 words, both methods work equally well with reported accuracies of up to 100% in the 11-15 word range."}, {"heading": "2.4 N-Grams and clustering", "text": "Gao et al. (2001) present a system that supplements n-gram language models with cluster information. ey cluster words by similarity and use these clusters to overcome the problem of data sparseness.In traditional cluster-based n-gram models, the probability P (wi) of a word wi is defined as the product of the probability of a word given to a cluster ci and the probability of the cluster ci. For a trigram model, the probability P (wi) of a word wi is calculated as the result of the probability of a word wi, with P (wi \u2212 2wi \u2212 1) = P (wi | 2wi \u2212 1) = P (wi | ci) \u00b7 P (ci) \u00b7 P (ci \u2212 2ci \u2212 2ci \u2212 1) e probability of a word given to a cluster."}, {"heading": "2.5 Inclusion detection", "text": "Beatrice Alex (cf. Alex (2005, 2006, 2007); Alex et al. (2007); Alex and Onysko (2010) deal with the problem of English inclusions in predominantly non-English texts. For the German-English language pair, inclusions are identified by means of a German and an English lexicon as the first resource. If a word is found only in the English lexicon, it is marked as uniquely English. If the word is not found in any lexicon, a web search is carried out that restricts the search options to either German or English and counts the number of results. If the German search yields more results, the word is marked as German, otherwise as English inclusion. If a word is found in both lexicon, a post-processing module solves the ambiguity. Alex is mainly concerned with improving the parsing results by recognizing the inclusion. For example, in (Alex et al., 2007) they report an increase in the F score of 4.35 due to the use of the inclusion of a German text with the TER (2002)."}, {"heading": "2.6 Clustering and spee", "text": "In the field of clustering and identification of spoken language, Yin et al. (2007) present a hierarchical spoken language cluster. ey Cluster 10 languages1 with prosodic characteristics and Mel Frequency Cepstral Coefficient (MFCC). MFCC vectors are a way to represent acoustic signals (Logan et al., 2000). e-signals are initially divided into smaller \"frames,\" each frame is guided through the discrete Fourier transformation, and only the logarithm of the amplitude spectrum remains intact (Logan et al., 2000). e-spectra are then projected onto the \"Mel Frequency Scale,\" a scale that maps the actual pitch to the perceived pitch, \"since apparently the human auditory system does not perceive the pitch in a linear manner\" (Logan et al., 2000). Finally, a discrete cosine transformation is applied to the spectrum, not the MFCC signal representation in this original MFCC model."}, {"heading": "2.7 Monolingual training data", "text": "Yamaguchi and Tanaka-Ishii (2012), King and Abney (2013) and Lui et al. (2014) use monolingual training data to train a language recognition system in a multilingual text. Yamaguchi and Tanaka-Ishii (2012) use a dynamic programming approach to segmenting a text by language. the test data contain fragments of 40 to 160 characters and achieve F scores of 0.94 on the relatively \"closed\" data set of the Universal Declaration of Human Rights 2 and 0.84 on the \"open\" Wikipedia dataset. However, the approach is computer-intensive, not to say prohibitive; while Yamaguchi and Tanaka-Ishii (2012) themselves have a processing time of 1 second for a 1000 character input, Lui et al. (2014) found that with 44 languages, the approach of Yamaguchi and Tanaka-Ishii (2012) have a distinct sequence of characters representing a 1000 second input problem for a 1000 second Lui et."}, {"heading": "2.8 Predictive suffix trees", "text": "Seldin et al. (2001) propose a system for automatic unattended speech segmentation and protein sequence segmentation. Each system uses Variable Memory Markov (VMM) sources, an alternative to Hidden Markov Models (HMM) implemented as predictive suffix trees (PST). While HMMs require significant amounts of training data and a deep understanding of the problem to constrain the model architecture, VMMs are simpler and less expressive than HMMs, but have been shown to \"solve many applications with remarkable success\" (Begleiter et al., 2004). Unlike n-gram models, which limit the probability of w as P (w | N) with the context (typically the n previous words), VMMs can vary in function of the available context N (Begleiter et al., 2004)."}, {"heading": "3.1 Supervised language model", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "3.1.1 N-Gram models", "text": "Among monitored language models, n-gram models are very popular (Gao et al., 2001). An n-gram is a snippet of the original string (Cavnar and Trenkle, 1994). ese slices can be contiguous or not. Uncontiguous n-grams are also called skipped words (Guthrie et al., 2006). In this example, an additional parameter k shows the difference between (traditional) n-grams and skipped grams. In this context, contiguous n-grams can be considered 0-skipped n-grams (Guthrie et al., 2006).e following example shows the difference between (traditional) n-grams and skipped grams. Considering the following sentence: Th i blank s and en c e contiguous words can be considered 0-skipped n-grams. We can construct the following word, for example k-skip-n-grams, n-grams. \""}, {"heading": "3.1.2 Formal definition", "text": "Traditional n-gram language models predict the next word wi, because the previous words w1,.., wi \u2212 1. the prediction uses the conditional probability P (wi | w1,.., wi \u2212 1). Instead of using the entire historical w1,.., wi \u2212 1, the probability is estimated only by using the n previous words wi \u2212 n + 1,.., wi \u2212 1.P (wi | w1,.., wi \u2212 1) = P (wi | wi \u2212 n + 1,..., wi \u2212 1) (10) e probability using the maximum likelihood estimation (MLE): P (wi | wi \u2212 n + 1,.., wi \u2212 1) = C (wi \u2212 n + 1,.., wi) C (wi \u2212 n + 1, wi \u2212 1)."}, {"heading": "3.1.3 Smoothing", "text": "To avoid this problem, various smoothing techniques can be used (Chen and Goodman, 1996). \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 The simplest smoothing technology is additive (Laplace) smoothing (Chen and Goodman, 1996). Let V be the vocabulary size (i.e. the total number of unique words in the test corpus). \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 n Recounting the probability PLaplace will be: PLaplace (wi \u2212 n + 1,.., wi \u2212 1) = C (wi \u2212 1) = C (wi \u2212 1,. \u2212 n Recounting the probability (i.e. the total number of unique words in the test corpus). \u2212 p \u2212 n \u2212 p \u2212 n Recounting the probability PLaplace will be: PLaplace (wi \u2212 n + 1,.,., wi \u2212 1) = C (Plaplace = 1,.,."}, {"heading": "3.2 Unsupervised clustering", "text": "Clustering is the grouping of objects based on their reciprocal similarity (Biemann, 2006).Objects to be clustered are typically represented as feature vectors (Biemann, 2006); a feature representation is calculated from the original objects and used for further processing.11Clustering can be partial or hierarchical (Yin et al., 2007).Partitional clustering divides the source objects into separate groups in one step, whereas hierarchical clustering builds a hierarchy of objects by first grouping the most similar objects and then grouping the next hierarchy in relation to the existing clusters (Yin et al., 2007).e Clustering algorithm uses a distancemetric method to determine the distance between the feature vectors of objects (Biemann, 2006).e Distance metrics defines the similarity of objects to the feature space in which the objects are represented (Jain al, 1999)."}, {"heading": "3.3 Weakly supervised language model induction", "text": "rE \"s tis for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green."}, {"heading": "4 Experimental setup", "text": "In this chapter, I will present experiments that have been conducted with the approaches described in the previous section to determine whether there are approaches that apply beer to specific text types, and the central hypothesis is that unattended speech segmentation approaches are more successful on difficult data. Difficult data is data for which there is not enough data to train a language model or data that contains a lot of non-standard language such as abbreviations. First, I will present the data used to test the language segmentation systems and work out the various aspects that needed to be taken into account for data acquisition, and then I will introduce two monitored speech segmentation experiments with n-gram language models and text kate.For unattended speech segmentation, I will first present experiments with cluster algorithms before introducing experiments with language model induction."}, {"heading": "4.1 Data", "text": "To test the different approaches to language segmentation, I have compiled different sets of test data. As I want to focus on short texts, most texts from the test corpus are rather small and sometimes consist of only one sentence. However, to test the general applicability of the approach, the test corpus also contains larger text examples. The test corpus can be divided into different sub-corpus: \u2022 Latin: texts that consist of languages with Latin scripts, such as German, English, Finnish or Italian \u2022 Mixed script: texts that consist of languages with Latin scripts and languages with non-Latin scripts. \u2022 Twier data: short texts from Twier \u2022 Pali dictionary data: unstructured texts with many different language inclusions such as Vedic Sanskrit, Sanskrit, Indo-Germanic scriptions and languages with non-Latin scriptions."}, {"heading": "4.2 Supervised language model", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.2.1 Implementation", "text": "For the monitored language segmentation method, I have implemented an N gram language model, as described by Dunning (1994). The e gram language model is implemented as a cartogram model with nonlinear deviation from bigram and unigram models. e conditional probability P is calculated with the formula: P (wi | wi \u2212 2, wi \u2212 1) = \u03b11 C (wi \u2212 2, wi \u2212 1) C (wi \u2212 2, wi \u2212 1) if C (wi \u2212 2, wi \u2212 1) > 0 \u03b12 C (wi \u2212 1, wi) C (wi \u2212 1) > 0 \u03b13 C (wi) Vif C (wi) > 0 \u03b14 1 V + W + X otherwise (17) with \u03b11 = 0.7, \u03b12 = 0.3 = 0.09, \u03b14 = 0.01, V the number of unigrams \u2212 rare improbability, W \u2212 the number of three grams."}, {"heading": "4.2.2 Training phase", "text": "First, models are trained on training data in the relevant languages. I have not included the languages from the Pali dictionary data as there are too many different languages19 and there are typically only small inclusions of different languages in a dictionary entry; as such, it would not have made sense to train a language model just to recognize a single word. Another reason for not using the Pali dictionary data languages is that sometimes it is not possible to find data for a language, such as ancient Bulgarian or reconstructed Indo-Germanic. In some cases, it would have been conceivable to train models in similar languages, but again, the effort of training a model is disproportionately high compared to the (uncertain) result of recognizing a single inclusion. Instead, an additional catch-all language model is used to capture words that do not seem to belong to a trained model."}, {"heading": "4.2.3 Application of the approa", "text": "In the second step, an input text is segmented into words. en, each word is evaluated by each language model and the model with the highest score is assigned as the language model of the word. e is to match words either to a trained language model or to the additional, all-encompassing model, which simply means that the word could not be assigned to a trained model class."}, {"heading": "4.2.4 Textcat and language segmentation", "text": "I have also tested how well Textcat is suited to the task of language segmentation. The e approach is similar to the n-gram approach, except that I do not train models and rely on Textcat's classifier for language prediction. In the first step, an input text is segmented into words. en, each word is passed to Textcat and the guess made by Textcat is assumed to be the language of the word."}, {"heading": "4.3 Unsupervised clustering", "text": "In order to test the efficiency of cluster algorithms in the task of language segmentation, I looked at various algorithms available via WEKA, \"a collection of machine learning algorithms for data mining tasks\" of the University of Waikato in New Zealand (Hall et al., 2009) and the Environment for Developing KDD-Applications Supported by Index-Structures (ELKI), \"an open source data mining software [...] with an emphasis on unattended methods of cluster analysis and outlier detection\" of the Ludwig Maximilians University of Munich (Achtert et al., 2013). I also looked at JavaML, \"a collection of machine learning and data mining algorithms\" (Abeel et al., 2009) to integrate clusters into my own code framework. JavaML provides various cluster algorithms and also provides access to WEKA's cluster algorithms. In contrast to a larger WEKI and a larger JavaPI, a cluster algorithm can be used."}, {"heading": "4.3.1 Preprocessing", "text": "But for the cluster algorithms to work, the > segment document must be pre-processed in various ways, as shown in Figure 16.First, the document must be read by the program, and the step is straightforward.22e Document must then be separated by one or more spaces. Tokenization is not trivial and depends on the definition of a \"word.\" For this task, I have used a whitespace tokenizer that defines a word as a continuous sequence of letter literals separated by one or more spaces. While it may be objected that in scripts that do not use spaces to separate words, such as Chinese, tokenization fails, this is not too much of a concern. If a continuous block of Chinese letters is treated as one word, it is likely that these spaces will be clustered separately."}, {"heading": "4.3.2 Defining features", "text": "The last step consists in defining characteristics by which the number of Latin words in Latin is defined. The following characteristics have been conceived: 231. Word length: the length of the word in Latin letters: the length of the word in Latin letters: the length of the word in Latin letters: the length of the word in Latin letters: the length of the word in Latin letters: the length of the word in Latin letters: the length of the word in Latin letters: the length of the word in Latin letters: the length of the word in Latin letters: the length of the word in Latin letters: the length of the word in Latin letters: the length of the word in Latin letters: the length of the word in Latin letters: the length of the word in Latin letters: the length of the word in Latin letters: the length of the word in Latin letters: the length of the word in Latin letters: the length of the word in Latin letters: the word in Latin letters: the length of the word in Latin letters: the word in Latin letters: the length of the word in Latin letters: the word in Latin letters: the length of the word in Latin letters: the length of the word in Latin letters: the word in Latin letters: the length of the word in Latin letters: the length of the word in Latin letters: the word in Latin letters: the length of the word in Latin letters: the word in Latin letters: the length of the word in Latin letters: the word in Latin letters: the length of the word in Latin letters: the word in Latin letters: the word in the word in Latin letters: the length of the word in Latin letters: the word in the word in the word in Latin letters: the word in the length of the word in Latin letters: the word in the word in the word in the word in the Latin letters: the word in the length of the word in the word in the word is the word in the word in the length of the word in the word in Latin letters: the word in the word is the word in the word in the word in the length of the word is the word in the word in the word in the word in the Latin letters: the word in the word in the length of the word is the word in the word is the word in the word in the word in the word in the word is the word in the word in the word in the word is the word in the word in the word"}, {"heading": "4.3.3 Mapping features to a common scale", "text": "Since JavaML requires numerical characteristics, all characteristics are mapped on numerical scales: \u2022 Binary characteristics are mapped to 0 (false) and 1 (true) \u2022 Ternary characteristics are mapped to 0 (false), 1 (true) and 99 (not applicable) \u2022 Numerical characteristics are mapped as themselves, either as integers (e.g. word length) or as floating-point numbers (e.g. vowel ratio) \u2022 Java-specific characteristics (18.20) assume the underlying numerical value as attribute \u2022 N-grams are encoded numerically using algorithm 15e. For the full list, refer to the documentation of the Java character class hp: / / docs.oracle.com / javase / 7 / docs / api / java / lang / Character.html25Algorithm 1 N-gram encoding 1: function (word) 2: sum \u2190 0 3: for character 4: character value: 5 characters long / java / java / long / numerical encoding."}, {"heading": "4.3.4 e problem of unambiguous encoding", "text": "I have tried to use unique encodings. The main problem with unique encoding is that the term \"distance\" is distorted. the idea behind the unique encoding is that every \"word\" (i.e. string) is encoded numerically so that no two \"words\" are represented as the same number. in addition to encoding every single character, the position of the character within the string must also be encoded. However, a possible encoding e for a string w1w2w3 could mean that 1w2w3 = n (w1) + x * n (w2) + y * n (w3) with the character of the string at position i, n (wi) the numerical encoding of the character w1w2w3 = n (w2w3) = n (w1) + x x. \""}, {"heading": "4.3.5 e clusterer", "text": "Since we want to work as flexibly as possible, I have ignored all algorithms that require the number of clusters before they cluster. In contrast, the x-Mean algorithm (Pelleg and Moore, 2000) estimates the number of clusters to generate them oneself. The algorithm has been selected to perform the language cluster tasks. While WEKA and ELKI offer a graphical user interface and various graphical representations of the results, the output is not easy to interpret. In fact, we can get a visualization of a cluster operation as shown in Figures 17 (WEKA) and 18 (ELKI). However, all data points must be checked manually by either clicking on each point 27 to get additional information about that data point (WEKA) or by hovering over the data points that the cluster program has selected."}, {"heading": "4.3.6 Evaluating clusterings", "text": "esemethods are based on counting\u03b2 29pairs (Wagner and Wagner, 2007). Consider clustering C = {C1,.., Ck}. C is a set of pairs located in the same cluster in C and C. \u2022 S00: set of pairs located in different clusters in C and C. \u2022 S10: set of pairs located in the same cluster in C and C. \u2022 S00: set of pairs located in different clusters in C and C. \u2022 S10: set of pairs located in the same cluster in C and C. \u2022 S10: set of pairs located in different clusters in C. \u2022 S01: set of pairs located in different clusters in C and in the same cluster. \u2022 S10: set of pairs located in different clusters in C and in the same cluster."}, {"heading": "4.4 Weakly supervised language model induction", "text": "In the second stage, the text is mapped to the induced models. (E algorithm for the language model induction is as follows: Algorithm 2Model Induction 1: IM 2: for word in words 3: modelAndScore \u2190 MS (word) 4: Score \u2190 4: Score \u2190 2: Score 5: if Score < threshold then 6: Model M (word) 7: models.add (model) 8: maxModel 1: maxModel 1: maxScore 4: Score 5: end forfirst of all score < word 7: models.add (model) 8: maxModel 9: maxModel \u2190 Model 10: update (word): end forfirst of all language model is created. For each word, themaximummodel and maximum score is calatededededededededededededed. these values correspond to the language model that gives the highest probability for the word in question and the associated probability."}, {"heading": "4.4.1 Distributional similarity", "text": "Suppose we had three models with the distributions of spaces shown in Figures 19, 20 and 216, and the similarity could be calculated solely on the basis of the occurrence of unique / spaces, i.e. if Model1 contains the empty \"a\" and Model2 also contains the empty \"a,\" their similarity increases by 1. However, if we calculate the similarity in this way, all three models are equally similar, since each of the spaces occurs at least once in each model. Nevertheless, it should be clear that Models 1 and 2 are very similar to each other, while Model 3 differs. Thus, to include the distribution of spaces in the similarity measure, the similarity shown in Algorithm 6.6efigures is used for illustration purposes only and does not necessarily reflect real language models.3435a b b c e f h i 0246 with the return of the frequency of the character c. e number 2 (2 q 6) the similarity of 1."}, {"heading": "4.4.2 Evaluating results", "text": "The results of this approach can be interpreted as clusters, where each language model represents a cluster core and all the words associated with this model form this cluster. Therefore, the evaluation is analogous to the evaluation of the cluster approach."}, {"heading": "4.4.3 Estimating the parameters", "text": "Since the induction of the language model can be controlled by parameters, we need to find a combination of parameters that work well for our assignment. e parameters i, j and \"merge mode\" were estimated on the development set. e development set contains documents similar to those in the testset. e development set can be found in the appendix. e It was found that the parameter combination i = 4, j = 2, ADD provides good results within the development set. Therefore, these values were used to evaluate the testset.37"}, {"heading": "5 Results", "text": "\"Baseline\" means the measurement where all words are thrown into a cluster, measured against the gold standard. In \"Baseline 2,\" each word is placed in a separate cluster and this clustering is evaluated against the gold standard. e column \"F1\" represents the F1 score and the \"F5\" column represents the F5 score. If one of the \"runs\" results in a higher score than any of the baseline values, the maximum score is printed in bold. If a field contains \"n / a,\" this means that the value could not be calculated for some reason (most of a division by zero would have occurred)."}, {"heading": "5.1 N-Gram language model", "text": ""}, {"heading": "5.2 Textcat", "text": ""}, {"heading": "5.3 Clustering", "text": "The first run shows the value of a cluster step and the second run the value that the cluster algorithm applies to the results of the first run.46474849"}, {"heading": "5.4 Language model induction", "text": "In addition to highlighting results that exceed the baseline values, the following tables were color-coded: results that exceed the cluster algorithm are displayed in red, and results that exceed both the cluster algorithm and the language model of the ngram are displayed in blue. Results that exceed only the language model of the n-gram would have been displayed in green, but there is no score that exceeds only the language model of the n-gram. 50515253"}, {"heading": "6 Discussion", "text": "The work of Seldin et al. (2001) is similar to the work presented here. ey proposes an uncontrolled language (and protein sequence) segmentation approach that leads to precise segmentation. Although their work looks promising, it also has its disadvantages. Their method requires longer monolingual text fragments and a considerable amount of text. Furthermore, they prohibit the switching of language models for each word. If the assumption is that inclusions and structures from a word are not recognized, as shown in Figure 22, where the language changes with each word.w1 w2 w3 w4 w5 w6 w7... Figure 22: Alternative language structure While this structure looks very artificial, such a structure can be found for example in the fih Pali dictionary text, in the passage \"Pacati, [Ved. pacati, Igd. * peq\u014d, Av. pac-;.\" In this case, \"red\" Pali, \"blue\" (abbreviations in English) and \"reconstructed.\""}, {"heading": "6.1 N-Gram language models", "text": "The results are more diverse than the results of the Greek, Anglo-Spanish-Russian segmentedwell, with English, French and Italian due to the shared language. The results are more diverse than the results of the Greek, Anglo-Russian and Ukrainian-Russian segmentedwell, with Spanish, French and Italian due to the shared language."}, {"heading": "6.2 Textcat", "text": "Textcat works well with monolingual texts. However, it fails with multilingual texts and does not work well with short fragments of text, such as individual words. Many of the words are marked as unknown, and if a language is identified, the presumption of language is incorrect. Therefore, Textcat cannot be used for language segmentation purposes. In fact, Textcat does not exceed the basic values except in two cases: \"Twier 3\" and \"Twier 4\" yield beer values as the basic values. However, on closer inspection, it is clear that the numerical index values are not a reliable picture of the quality of clustering. In fact, clustering \"Twier 3\" is not absurd, but it is not very good as it does not extract the French insertion \"Breuvages.\""}, {"heading": "6.3 Clustering", "text": "In fact, it is such that most of them will be able to move into another world, in which they are able to move, in which they are able to move, in which they move, in which they move, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they are able"}, {"heading": "Pali: abhijjhitar: Second run", "text": "\u2022 abhijjhita, abhijjh\u0101tar, covets, function], med., one, who, \u00b0 itar, \u00b0 \u0101tar). \u2022 (T., A, M \u2022 =, l., v. \u2022 < smallcaps > i. < / smallcaps >, < smallcaps > v. < / smallcaps >, ag., fr., in \u2022 265, 287 \u2022 [n.In some cases, clustering fails in the task of language segmentation, as in the various English-French texts and the English-German example in German inclusion. Thus, we can say that the surface structure or morphology, or in other words the basis from which we can extract traits, is not sufficient to derive relevant information about the \"language.\" If there are more than two languages to be separated, clustering does not work well either. In fact, most dissimilar objects are separated first."}, {"heading": "6.4 Language model induction", "text": "The language model induction approach does not seem to work particularly well on the Latin script data, which is almost exclusively impure clusters containing more than one language. However, the approach consistently outperforms the cluster approach when we look at the F5 score. In the Anglo-French dataset, the cluster approach even outperforms the n-gram language model approach. In fact, the French words are relatively well separated from the English text, with the exception of \"sucr\u00e9,\" which is still thrown together with English words."}, {"heading": "Latin script: English\u2013Fren", "text": "In fact, we achieve a good separation of the languages by script. However, although there are also Latin scripts, we encounter the same problems as mentioned above with rather modest results. For example, for the English-Greek text, the approach separates the Greek characters, but it fails because of the mixed script data. Also for the English-Greek text, the approach separates the Greek characters, but it fails because of the separation of the Greek languages by script. Also for the English-Spanish text, Arabic is separated, but English and Spanish are not separated."}, {"heading": "6.5 Scores", "text": "From the values I have used for valuation purposes, a combination of a high edge index and a high F5 value seems to indicate good language segmentation. A high F5 value alone is not significant. For example, the cluster algorithm achieves an F5 value of 0.7215 in \"Twier 3.\" looks good, but the edge index value is 0.4571, and the segmentation is not good. Twitter 3: Cluster Analysis \u2022 Edmonton, Food \u2022 go, in, to \u2022 and, are, breuvages, fans, for, just, ready, the, the, waiting Similarly, a high edge index value alone is not significant."}, {"heading": "7 Conclusion", "text": "This year it has come to the point that it will be able to mention the aforementioned hreeeisrcnlrVo rf\u00fc eid eerwtlrlrrVo rf\u00fc eid eerwtlrVrrsrtee\u00fceerwtlrteeeegnln rf\u00fc ide eerwtlrVnree\u00fceegnln. ndwr"}, {"heading": "8 Appendix", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "8.1 Development data", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "8.1.1 Latin script data", "text": "Karl Marx anses som en af de fire klassiske sociologer. Marx er epokeg\u00f8rende for den historiske videnskab. Og Marx spillede en vigtig Rolle for den samtidige og eerf\u00f8lgende arbejderbev\u00e6gelse.1891, after a tuberculosis Hope, the couple opened a modern lung sanatorium in Nordrach in the Black Forest, which they stirred together until 1893. In 1895 the marriage was divorced. Sources: hps: / / da.wikipedia.org / wiki / Karl _ Marx hps: / / en.wikipedia.org / wiki / Hope _ Bridges _ Adams _ Lehmann"}, {"heading": "8.1.2 Mixed script data", "text": "Capitalism is an economic system and mode of production in which commerce, industry and means of production are largely or completely privately owned. Private companies and owners usually operate for profit, but can operate as private non-profit organisations."}, {"heading": "8.1.3 Twitter data", "text": "Twitter 1 \"Fallo ergo sum\": From the lunatic."}, {"heading": "Source:", "text": "Roland Hieber (daniel _ bohrer). \"\" Fallo ergo sum \": On being wrong..\" 26 July 2015, 16: 47. Tweet.72Twitter 2 Music for airports > le piano en libre-acc\u00e8s dans l'a\u00e9roport Charles-deGaulles"}, {"heading": "Source:", "text": "Yannick Rochat (yrochat): \"Music for airports > le piano en libre-acc\u00e8s dans l'a\u00e9roport Charles-de-Gaulles.\" 26 July 2015, 18: 12. Tweet."}, {"heading": "8.1.4 Pali dictionary data", "text": "All entries are taken from the Pali Text Society's Pali-English dictionary (T. W. Rhys Davids, William Stede, editors, e Pali Text Society's Pali-English dictionary. Chipstead: Pali Text Society, 1921-5). 8 parts [738 pp.].) Hambho Hambho, (indecl.) [hao + bho] a particle expressing surprise or arrogance J.I, 184,494. See also ambho. (page 729) Ussada Ussada, [most to ud + syad; see ussanna]: This word is fraught with difficulties, the expression sa-ussada is used in all possible meanings, obviously the result of an original application & meaning that is obliterated. sa \u00b0 is called * sapta (seven) as well as * sava (being), ussada as prominent, fullness, arrogance."}, {"heading": "8.2 Test data", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "8.2.1 Latin script data", "text": "The German word navel-gazing means \"navel-gazing\" or \"staring at the navel.\" But in this case it does not refer to someone else's belly, but only to one's own."}, {"heading": "Source:", "text": "Glass, Nicole (2015): \"German Missions in the United States - Word of the Week.\" Germany.info.English - Fren doux, mou: both translate to \"so\" in English, although their meanings vary widely. Doux is the opposite of \"rough\" or \"coarse\" (rugueux), while mou is the opposite of \"hard.\" Doux can also mean sweet, but almost only for wines (otherwise sucr\u00e9 is used)."}, {"heading": "Source:", "text": "Maciamo, (2015): \"French words and nuances that do not exist in English.\" Eupedia.English - Transliterated Greek e Greek language distinguishes at least four different ways in which the word love is used. In Ancient Greek there are four different words for love: ag\u00e1pe, \u00e9ros, phil\u00eda, and storg\u0113. As in other languages, it has historically been difficult to separate the meanings of these words when used outside their respective contexts. Nevertheless, the senses in which these words were commonly used are as follows: Source: hps: / en.wikipedia.org / wiki / Greek _ words _ for _ loveItalian - German Milano ne custodisce l'esempio pi\u00f9 struggente: quel Cenacolo che esa il vinciano affresc\u00f2 con amore, cura e rivoluzionaria psicologia (il Giuda non viene privato dell'aureola, ma si condanna da solo, con la centro. \""}, {"heading": "Source:", "text": "Stalinski, Sandra (2015): \"Ingenieure: Mythos Fachkr\u00e4emlack?\" tagesschau.de. Scorranese, Roberta (2015): \"Nelle grandi opere il racconto sofferto della natura mortale.\" Archiviostorico.corriere.it.English - Finnish - Turkish Summer is the warmest of the four seasons in the temperate and arctic climates. Depending on whether it is in the northern or southern hemispheres, one speaks of the northern or southern summer. The northern summer takes place simultaneously with the southern winter sta.Kes\u00e4 eli suvi on vuodenaika kev\u00e4\u00e4n ja K\u00fcrksyn v\u00e4lipediss\u00e4. Kes\u00e4 on vuodenajoista l\u00e4mpimin G\u00fcmpimin, koska maapallo auf silloin kallistunut niin."}, {"heading": "8.2.2 Mixed script data", "text": "In the second half of the 19th century, when the United States and the Soviet Union went to war, it was the United Kingdom, the United States, the United Kingdom, France, France, France, France, France, France, Great Britain, France, France, France, France, France, France, France, France, France, France, France, France, France, France, France, France, Great Britain, Great Britain, France, France, Great Britain, Great Britain, France, France, Great Britain, Great Britain, Great Britain, France, Spain, France, Spain, France, France, Great Britain, Great Britain, Great Britain, Great Britain, Spain, Spain, Spain, Spain, Spain, Spain, Spain, Spain, Spain, Spain, Spain, Spain, Spain, Spain, Spain, Spain, Spain, Spain, Spain, Spain, Spain, Spain, Spain, Spain, Spain, Spain, Spain, Spain, Spain, Spain, Spain, Spain, Spain, Spain, Spain, Spain, Spain, Spain, Spain, Spain, Spain, Spain, Spain, Spain, Spain, Spain, Spain, Spain, Spain, Spain, Spain, Spain, Spain, Spain, Spain, Spain, Spain, Spain, Spain, Spain, Spain, Spain, Spain, Spain, Spain, Spain, Spain, Spain, Spain, United Kingdom, France, France, France, France, France, France, France, France, France, France, Great Britain, France, France, France, Spain, Great Britain, Spain, Spain, Spain, Spain, Great Britain, Spain, Spain, Spain, Spain, Great Britain, Spain, Spain, Spain, Spain, Spain, Spain, Spain, Spain, Spain, Spain, Spain, Spain, Spain, Spain, Spain, Spain, Spain, Spain, Spain, Spain, Spain, Spain, Great Britain, Spain, Spain, Spain, Spain, Spain, Spain, Spain, Spain, Spain, Spain, Spain, Spain, Spain, Spain, Spain, Spain, Spain, Spain, Spain, Great Britain, Spain, Spain, Spain, Spain, Spain, Spain, Spain, Spain, Spain, Spain, Spain, Spain, Spain, Spain, Spain, Spain, Spain, Spain, Spain, Spain, Spain, Spain, Spain, Spain, Spain, Spain, Spain, Spain, Spain, Spain, Spain, Spain, Spain, Spain, Spain, Spain, Spain, Spain, Spain, Spain, Spain, Spain, Spain, Spain, Spain,"}, {"heading": "8.2.3 Twitter data", "text": "Tweet 1: Greek - English, Greek - English, Greek - English, Greek - English, Greek - English, Greek - Greek - English, Greek - English, Greek - Greek."}, {"heading": "Source:", "text": "June 19, 2015, 12: 06 pm: TweetTweet 2: English - Fren Demain # dhiha6 Keynote 18h @ dhiparis \"e publish or perish collective dynamics of science; is it all that matters?\" par David @ chavalarias"}, {"heading": "Source:", "text": "Claudine Moulin (ClaudineMoulin): \"Demain # dhiha6 Keynote 18h @ dhiparis\" e collective dynamics of scientific publishing or doom; is that all that matters? \"par David @ chavalarias.\" 10 June 2015, 17: 35. Tweet.Tweet 3: English - Fren Food and breuvages in Edmonton are ready to go and just waiting for the fans # FWWC2015 # Bilingualism"}, {"heading": "Source:", "text": "HBS (HBS _ Tweets). \"Food and drink in Edmonton are ready and waiting for the fans # FWWC2015 # Bilingualism.\" 6 June 2015, 23: 29. Tweet.Tweet 4: English - Polish My father comes back from Poland with two boxes of strawberries, \u017cubr\u00f3wka and adidas jackets."}, {"heading": "Source:", "text": "katarzyne (wifeyriddim). \"My father comes back from Poland with two boxes of strawberries, \u017cubr\u00f3wka and adidas jackets.\" June 8, 2015, 08: 49. Tweet.Tweet 5: Translated Amharic - English Buna dabo naw (coffee is our bread)."}, {"heading": "Source:", "text": "eCodeswitcher. \"Buna dabo naw (coffee is our bread)..\" 9 June 2015, 02: 12. Tweet."}, {"heading": "8.2.4 Pali dictionary data", "text": "< A < A < A < A < A < A < A < A < A < B > < A < A < A < A < B > A < A < A < B (n) < A (n) < B (n) > B (n) < A (n) > B (n) < B (n) < B (n) < B (n) < B (n) < B (n) < B (n) < A (n) < B (n) < A (n) < A (n) < A (n) & lt."}, {"heading": "8.3 Results", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "8.3.1 N-Gram Language Models", "text": "For the n-gram language approach, the determined language in Parentheses. e-language abbreviations are: abbreviations (F), (F), (F), (F), (F), (F), (F), (F), (F), (F), (F), (F), (F), (F), (F), (F), (F), (F), (F), (F), (F), (F), (F), (F), (F), (F), (F), (F), (F), (F), (F), (F), (F), (F), (F), (F), (F), (F), (F), (F), (F), (F), (F), (F), (F), (F, (F), (F), (F), (F, (F), (F), (F), (F), (F), (F), (F), (F), (F), (F, (F), (F), (F), (F), (F, (F), (F), (F), (F, (F), (F), (F, (F), (F), (F), (F, (F), (F, (F), (F), (F, (F), (F), (F), (F, (F), (F, (F), (F), (F, (F), (F, (F), (F), (F, (F), (F, (F, (F), (), (F), (, (, (F), (F, (, (F), (), (), (F), (, (), (), (F, (, (, (), (,), (, (,), (), (, (,), (, (, (,), (,), (,), (, (,), (,), (, ("}, {"heading": "8.3.2 Textcat", "text": "Since Textcat is unknown for many words, I only specify the non-unknown categories to save space and create peace of mind, indicating that all other words in the text have been classified as indefinable. \u2022 Abbreviations are: DE Danish (HU) German (HU) English (HU) Spanish (HU) French (HU) Hungarian (HU) Indonesian (HU) Italian (HU) Latin (HU) Russian (HU) German (HU), English (HU), German (HU), English (HU), German (HU)."}, {"heading": "8.3.3 Clustering", "text": "Clustering the different datasets led to the following clusters. In the second run, the clusters of the first run are used and each cluster may be subdivided into two or more clusters. Data: Latin script: German - English"}, {"heading": "First run", "text": "\u2022 \"navel gaze,\" no, otherwise \u2022 \"staring, but, German, navel gaze, anyone, belly, buon, fall, only, means, navel.,\" possess, refer, this, word, your \u2022 to, in, it, or, to \u2022 -, eSecond run \u2022 does, else's \u2022 \"navel gaze\" \u2022 \"staring, but, German, navel gaze, belly, fall, means, navel.,\" refer, this \u2022 anyone, buon, only, possess, word, your \u2022 it, or, to \u2022 on, in \u2022 -, e95Data: Latin script: German - Finnish - Turkish"}, {"heading": "First run", "text": "\u2022 D\u00fcnya, G\u00fcney, K\u00fcre'de, Southern hemisphere, Southern summer., Southern winter, S\u0131cak, aras\u0131nda, temperate, g\u00fcnler, i\u00e7in, kes\u00e4kuukausiksi, l\u00e4mpimin, s\u00e4teilee, s\u0131cak, warmest, \u00e7\u0131kar., Der \u2022 Aral\u0131k, Eyl\u00fcl, Kes\u00e4, Yar\u0131m, aras\u0131ndad\u0131r., etel\u00e4isell\u00e4, ei\u011fi, e\u00e4, ger\u00e7ekle\u015fir., hein\u00e4, jyrkemm\u00e4ss\u00e4, kes\u00e4-., kev\u00e4\u00e4n, v\u00e4liss\u00e4., yakla\u015f\u0131k, \u0131s\u0131y\u0131, ei\u011fi, e\u00e4, ger\u00e7ekle\u015fir., hein\u00e4, jyrkemm\u00e4ss\u00e4, kes\u00e4-., kev\u00e4\u00e4n, v\u00e4liss\u00e4."}, {"heading": "Second run", "text": "\u2022 Southern Hemisphere, Southern Summer., Southern Winter, aras\u0131nda, temperate, kes\u00e4kuukausiksi, l\u00e4mpimin, s\u00e4teilee, warmest \u2022 D\u00fcnya, G\u00fcney, K\u00fcre'de, S\u0131cak, g\u00fcnler, i\u00e7in, s\u0131cak, \u00e7\u0131kar., Der \u2022 aras\u0131ndad\u0131r., etel\u00e4isell\u00e4, ei\u011fi, e\u00e4, ger\u00e7ekle\u015fir. \u2022 Kes\u00e4 \u2022 22 \u2022 21 \u2022 Der, Haziran, jyrkemm\u00e4ss\u00e4, kes\u00e4-., kev\u00e4\u00e4n, v\u00e4liss\u00e4., yakla\u015f\u0131k, \u0131s\u0131y\u0131 \u2022 Aral\u0131k, Eylket\u00fcl, Yar\u0131m \u2022 Kes\u00e4 \u2022 Der, Haziran, Seasons, Climatic Zone, Kuzey, Mart, Northern, Northern Summer, Pohjostunisella, reigns, aurinko, dem, helaya, die, likeluile, jallvile."}, {"heading": "First run", "text": "\u2022 \"rough,\" \"hard.,\" \"rough,\" \"so,\" (otherwise, (rugueux), doux, English, almost, also, though, but, can, different., doux, for, mean, meaning, mou, only, opposite, sucr\u00e9, sweet, the, their, translate, used)., very, while, wines \u2022 is, or \u2022 how, in, bySecond run \u2022 Doux, English, \u2022 \"rough,\" (otherwise, (rugueux), almost, though, differently., meaning, opposite, translate \u2022 \"hard,\" \"rough,\" \"so,\" also, both, but, may, doux, for, mean, mou, only, sucr\u00e9, sweet, which, used)., very, while, wines \u2022 or \u2022 is \u2022 in \u2022 these languages, in \u2022 as97Data: Latin script: English - transliterated GreekErsterrun \u2022 e, phil\u00e1pe, phil\u00eda, Greek, distinguishable, in general, are different."}, {"heading": "First run", "text": "\u2022 (il, E, So, a, ad, da, di, e, es, ha, i, il, in, la, le, lo, ma, ne, se, si, un, va, zu98 \u2022 \"one, annunciazione, Baista, Cenacolo, Certo, Come, Doing, Germans, Germany, all-clear, result, Giuda, engineers, Is, years, Barely, Leonardo, STEM professions, myth?, scientists, bull associations, voices, study, study, scenario, ema, survey, Venezia, warning, science, week, time, acque, ali, alla, all, generally, also, amore, anche, admonished, anni, anti-Turchi., approfonditamente, arginato., aento, also, autodistruggersi, bacini, barbaglio, alla, conconconconconce, bei, bellissima, cancro, caurata, che, con, consisionente, condole, coneevanna, more."}, {"heading": "Second run", "text": "In this context, it should be noted that this is a case in which the United States, United Kingdom, France, Great Britain, France, France, France, France, France, France, France, France, France, France, France, France, France, France, France, France, France, Great Britain, Great Britain, Great Britain, France, France, France, Great Britain, Great Britain, Great Britain, France, France, France, France, France, France, France, France, France, France, France, France, France, France, France, Great Britain, France, France, France, Spain, Spain, Spain, Spain, Spain, Spain, Spain, Spain, Spain, Spain, Spain, Spain, Spain, Spain, Spain, Spain, Spain, Spain, Spain, Spain, Spain, Spain, Spain, Spain, Spain, Spain, Spain, Spain, Spain, Spain, Spain, Spain, Spain, Spain, Spain, Spain, Spain, Spain, Spain, Spain, Spain, Spain, Spain, Spain, Spain, Spain, Spain, Spain, Spain, Spain, Spain, Spain, Spain, Spain, Spain, Spain, Spain, Spain, Spain, Spain, Spain, Spain, Spain, Spain, Spain, Spain, Spain, Spain, Spain, Spain, Spain, Spain, Spain, Spain, Spain, Spain, Spain, Spain, Spain, Spain, Spain, Spain, Spain, Spain, Spain, Spain, Spain, Spain, Spain, Spain, Spain, Spain, Spain, Spain, Spain, Spain, Spain, Spain, Spain, Spain, Spain, Spain, United Kingdom, Spain, Spain, Spain, Spain, Spain, Spain, Spain, Spain, Spain, Spain, Spain, Spain, Spain, Spain, Spain, Spain, Spain, Spain, Spain, Spain, Spain, Spain, Spain, Spain, Spain, Spain, Spain, Spain, Spain, Spain, Spain, Spain, Spain, Spain, Spain, Spain, Spain, Spain, Spain, Spain, Spain, Spain, Spain, Spain, Spain, Spain, Spain, Spain, Spain, Spain, Spain, Spain, Spain, Spain, Spain, Spain, Spain, Spain, Spain, Spain, Spain, Spain, Spain, Spain, Spain,"}, {"heading": "First run", "text": "\u2022 \"intimate,\" without, though, Aquinas, Christians, Corinthians, Socrates, Symposium, Testament, Whether, affection, old, another. \u2022 \"Appreciation, striving, Araction, Araction.,\" becomes, benevolence., biblical, fraternal, chapter, \"Charity;, children, contemplation, content, continues, contributes, definition:, described, existence;, explains, express, feel, feelings, find, continue, hold, first, inspired, knowledge, marriage., necessary, not physical, passage, passion.,\" philosophers, physical, platonic, refined, relationships, returned, self-use)., sensual, spiritual, subject, suggest, through, everywhere, transcendence., unconditionally, understanding, without, youthful \u2022 (...), becomes, (...), Ag\u00e1pe, Ag\u00e1pe, ag\u00e1p\u0113), eros, as a man,. \""}, {"heading": "Second run", "text": "\u2022 Affection, old, another., \"aspires, becomes, biblically, chapter\", charity \";, children, children., Content, definition:, feeling, feelings, finding, keeping, marriage., necessary, passage, passion., platonic, refined, returned, subject, by, without \u2022 Although, Aquinas, Christians, Corinthians, Socrates, symposium, testament, whether \u2022\" intimate, esteem, Araction., \"goodness., fraternal, contemplation, further, carries, described, existence;, explains, expressed, further, initially inspired, knowledge, not physical, philosophers, physical, relationships, self-interest)., sensual, spiritual, suggesting, everywhere, transcendence., unconditionally, understanding, youthful \u2022 Ag\u00e1pe, ag\u00e1p\u0113), \u00c9ros, \u00e9r\u014ds,\" as a word based on \"Ag\u00e1os.\""}, {"heading": "First run", "text": "\u2022 El, POW / MIA, Wearing, a, as, been, black, de, displaying, duelo., en, es, estados, for, has, is, lazo, mourns, grieves., negro, o, of, organizaciones, personas, political, por, commemoration, commemoration, representando, ribbon, sentimiento, sociedades, statement., symbol, tragedies, un, used, utilizado, y \u2022 cresp\u00f3n, pol\u00edtico-social, se\u00f1al, s\u00edmbolo \u2022 A \u2022 1 \u2022 2 \u2022 3 \u2022 4 \u2022 5 \u2022 5 \u2022 5"}, {"heading": "Second run", "text": "\u2022 a, o, y \u2022 El, as, de, en, es, is, of, or, un \u2022 Wearing, been, black, display, duelo., estados, for, has, lazo, grieving, grieving., negro, organizaciones, personas, political, por, commemorate, commemorate, commemorate, representando, ribbon, sentimiento, sociedades, statement., symbol, tragedies, used, utilizado \u2022 POW / MIA \u2022 pol\u00edtico-social, s\u00edmbolo \u2022 cresp\u00f3n, se\u00f1al \u2022 A \u2022 a \u2022 a, o, y \u2022 a, o, y \u2022 El, as, as, de, en, es, is, of, or, por, commemorate, commemorate, commemorate, representando, band, sentimiento, sociedades, statement."}, {"heading": "First run", "text": "(simplified, words, words, words, words, words, words, words, words, words, words, words, words, words, words, words, words, words, words, words, words, words, words, words, words, words, words, words, words, words, words, words, words, words, words, words, words, words, words, words, words, words, words, words, words, words, words, words, words, words, words, words, words, words, words, words, words, words, words, words, words, words, words, words, words, words, words, words, words, words, words, words, words, words, words, words, words, words, words, words, words, words, words, words, words, words, words, words, words, words, words, words, words, words, words, words, words, words, words, words, words, words, words, words, words, words, words, words, words, words, words, words, words, words, words, words, words, words, words, words, words, words, words, words, words, words, words, words, words, words, words, words, words, words, words, words, words, words, words, words, words, words, words, words, words, words, words, words, words, words, words, words, words, words, words, words, words, words, words"}, {"heading": "First run", "text": "< < / > Script < < < / > Script < < / > Script < < / > Script < < / > Script < < / > Script < < / > Script < / > Script < < / > Script < < / > Script < < / > Script < < / > < / > Script < < / > < < < / > Script < < < < < < < < < < < < < < < < < < < / > Script < < < < < < < < < < < < < < < < < < / > Script < < < < < < < < < < < < < < < < < < < < < < < < / > Script < < < < < < < < < < / < < < < < < / < < / Script; < < < < < < / < < / < < Script; < < < < < / Script; < < < < < / Script; < < < < / Script"}, {"heading": "Second run", "text": "< < < / smallcaps > < / smallcaps > vi. < / smallcaps > vi. < / smallcaps > vi. < / mbros < / at >, < / at >, < < / at >, < smallcaps > ii. < / smallcaps > iv < / smallcaps > iv < / smallcaps > vi. < / smallcaps >, abb. < / at >, & ltltltltlt. (< / smallcaps > iv. < < / smallcaps > vi. < / smallcaps > vi. < / smallcaps >, abhra, abhra, adj.), ambha, ambu., cloud, clouds."}, {"heading": "First run", "text": "\u2022 abhijjhita, abhijjh\u0101tar, covets, function], med., one, who, \u00b0 itar), \u00b0 itar, \u00b0 \u0101tar. \u2022 (T., < smallcaps > i. < / smallcaps >, < smallcaps > v. < / smallcaps >, =, A, M, ag., fr., in, l., v. \u2022 265, 287 \u2022 [n."}, {"heading": "Second run", "text": "\u2022 abhijjhita, abhijjh\u0101tar, covets, function], med., one, who, \u00b0 itar, \u00b0 itar, \u00b0 \u0101tar). \u2022 (T., A, M \u2022 =, l., v. \u2022 < smallcaps > i. < / smallcaps >, < smallcaps > v. < / smallcaps >, ag., fr., in \u2022 265, 287 \u2022 [n.107Data: Pali: ajja"}, {"heading": "First run", "text": "\u2022 -divasa, Freq., Loc., [Vedic, adya, ajjatagge, ajjato, an, and, as, base, being, day, demonstr., dyaus, from, in, morning, not, of, old, or, phrase, present, pron., the, this, with \u2022 &, +, Mhvs, s., v. \u2022 -k\u0101la\u0442, 10), 15,64., 32,23., Ajjj\u0101, D.I, 85;, DA.I, 235., Dh.326;, III, 425, J.I, 279;, J.VI, 180;, Kern, Toev., Pv.I, 117, PvA.59);, PvA.6,23;, Sn.75,153, 158,970,998;, Vin.I, 18;, a3), ady\u0101, a, agge (?), ajidja-tagj\u0101, see ja\u0101, (see, di\u0101, di\u0101, (see)."}, {"heading": "Second run", "text": "\u2022 an, as, in, of, or \u2022 Freq., Loc., [Vedic \u2022 -divasa, adya, ajjatagge, ajjato, and, base, being, day, demonstr., dyaus, from, morning, not, old, phrase, present, pron., the, this, with \u2022 & \u2022 + \u2022 Mhvs \u2022 s., v. \u2022 \"on,\" - \u203a, Ajja, &, (=, (Page, (adv.), (lies, (see \u2022 -k\u0101la\u0442, 10), 15,64., 32,23., Ajjj\u0101, D.I, 85;, DA.I, 235., Dh.326;, III, 425, J.I, 279;, J.VI, 180;, Kern, Toev., Pv.I, 117, PvA.6,23;, Sn.75.153.158, 970,998;, Vin.I, 18, jidaj\u0101j\u0101j\u0101j\u0101j\u0101, see, jaga\u0101, 8j\u0101, see)."}, {"heading": "First run", "text": "\u2022 253), pug. 19. Cf. pari \u00b0. (page, [abstract fr. g\u016bhati] = g\u016bhan\u0101, G\u016bhan\u0101, (f.), (q. v.)"}, {"heading": "Second run", "text": "\u2022 253), pug. 19. Cf. pari \u00b0. (page, [abstract fr. g\u016bhati] = g\u016bhan\u0101, G\u016bhan\u0101, (f.), (q. v.) Data: Pali: pacati"}, {"heading": "First run", "text": "\u2022 382), Kausa. pac\u0101peti, DA. I, 159, wo, Obulg. peka, Pas. paccati, Vin. IV, 264;, bake, Gr. pac\u0101peti, cook, fry, cook, p\u044bpwn, da\u1e47ena, fig. torment, fry, fry, lith, kep\u016b, intrs.): Niraye, paccato, by, ppr. pacanto, pp. pakka, p\u012b\u0439 entassa. -, torment, Gen. pacato \u2022 D. I, 52, N. S. II, 225, PvA. 10,14. -, Pacati, [Ved. pacati, Idg. * peq\u014d, Av. pac-;, (+ caus. p\u0101cayato, p\u0101cayato, (expld, (q. v.)."}, {"heading": "Second run", "text": "\u2022 Caus. pac\u0101peti, DA. I, 159, wo, Obulg. peka, Pas. paccati, Vin. IV, 264;, bake, Gr. pinaussw, cook, cook, fry, cook, p\u03b1pwn, da\u1e47\u044bena, fig. torment, fry, fry, lith, kep\u016b, intrs.): Niraye, paccato, by, ppr. pacanto, p\u012b\u0439 entassa). -, torment, Gen. pacato \u2022 382), pp. pakka \u2022 D. I, 52, N. S. II, 225, PvA. 10,14. -, (q. v.). - \u2022 Pacati, [Ved. pacati, Idg. * peq\u043e, Av. pac-;, (+ Caus. p\u0101cayato, (q.)."}, {"heading": "First run", "text": "\u2022 Data: Twier 2 (French-English) \u2022 PERFORMANCE, PERFORMANCE, PERFORMANCE, PERFORMANCE, PERFORMANCE, PERFORMANCE, PERFORMANCE, PERFORMANCE, PERFORMANCE, PERFORMANCE, PERFORMANCE, PERFORMANCE, PERFORMANCE, PERFORMANCE"}, {"heading": "First run", "text": "\u2022 \"e, 18h, @ dhiparis, David, Demain, Keynote, all, collective, counts?,\" dynamics, par, perish;, science-publish, that \u2022 is, it, of, or110"}, {"heading": "Second run", "text": "\u2022 \"e, @ dhiparis, David, Demain, Keynote, all, collective, counts?,\" Dynamics, par, perish;, science-publish, that \u2022 18h \u2022 is, it, or \u2022 ofData: Twier 3 (French-English) First run \u2022 Edmonton, Food \u2022 go, in, to \u2022 and, are, breuvages, fans, for, just, ready, the, warteSecond run \u2022 Edmonton, Food \u2022 to \u2022 go, in \u2022 for, just \u2022 and, are, breuvages, fans, ready, the, warteData: Twier 4 (English-Polish) First run \u2022 \u017cubr\u00f3wka, my \u2022 adidas, and, back, comes, crates, dad, from, jackets, of, omg, polaband twies, two, with111Second run \u2022 \u017cubr\u00f3wka, my \u2022 adidas, comes, dad, of \u2022 and, back, crates, from, English, Twist, Twist, withtwist."}, {"heading": "8.3.4 Language Model Induction", "text": "For all language model tasks, the threshold t = 0.02 and the silver threshold s = 0.1. e other parameters were set to \"maximum iteration number\" i = 4, \"maximum random iteration number\" j = 2 and \"merge mode ADD.\" Data: Latin, English, German, word, word, navel, means, \"or,\" staring, at, your, this, it, doesn't, refer, to, anyone, other's, buon, just your, own, \"-\" navel gazing, \"navel, navel, navel, navel.,\" Fall, belly112Data: German-Finnish-Turkish, in, and, climatic zone, Depending on, on, on, southern hemisphere, vom, eli, on, vuodenaika, ja, on, vuodenajoista, koska, mamaallo, on, on, sillokala, stunut, stunut."}, {"heading": "Normalized data", "text": "\u2022 Twipacati, peka, pempelsw, pempelsw, pempelsw, pacitv\u0101, ppr., pacanto, Gen., pacato, (+ Caus., pempelsw), pacato, paccato, pare, pempelsentassa)., pp., pakka, Caus., pac\u0101peti, Pass., paccati \u2022 * peq\u043e, backen \u2022 pac-;, 264;, 52, &, 382) \u2022 10,14.-, 159, -, < - >, - \u2022 fry, Niraye, I, I, by \u2022 Av., Obulg., Gr., (trs., D., DA., (q.v.)., (q.v.). \u2022 [Ved., to, fry, kep\u043e, cook, ripe], to, transcslib, fry, pempelsb, litter.pempelsb \u2022 (Ved.)."}], "references": [{"title": "Java-ML: A Machine Learning Library", "author": ["T. Abeel", "Y.V. de Peer", "Y. Saeys"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Abeel et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Abeel et al\\.", "year": 2009}, {"title": "Interactive data mining with 3D-parallel-coordinate-trees", "author": ["E. Achtert", "H. Kriegel", "E. Schubert", "A. Zimek"], "venue": "In Proceedings of the ACM SIGMOD International Conference on Management of Data,", "citeRegEx": "Achtert et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Achtert et al\\.", "year": 2013}, {"title": "An unsupervised system for identifying English inclusions in German text", "author": ["B. Alex"], "venue": "In Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics (ACL", "citeRegEx": "Alex,? \\Q2005\\E", "shortCiteRegEx": "Alex", "year": 2005}, {"title": "Integrating language knowledge resources to extend the English inclusion classifier to a new language", "author": ["B. Alex"], "venue": "In Proceedings of the 5th International Conference on Language Resources and Evaluation (LREC). European Language Resources Association", "citeRegEx": "Alex,? \\Q2006\\E", "shortCiteRegEx": "Alex", "year": 2006}, {"title": "Automatic detection of English inclusions in mixed-lingual data with an application to parsing", "author": ["B. Alex"], "venue": "PhD thesis,", "citeRegEx": "Alex,? \\Q2007\\E", "shortCiteRegEx": "Alex", "year": 2007}, {"title": "Using Foreign Inclusion Detection to Improve Parsing Performance", "author": ["B. Alex", "A. Dubey", "F. Keller"], "venue": "In EMNLP-CoNLL,", "citeRegEx": "Alex et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Alex et al\\.", "year": 2007}, {"title": "Zum Erkennen von Anglizismen im Deutschen: der Vergleich von einer automatisierten mit einer manuellen Erhebung", "author": ["B. Alex", "A. Onysko"], "venue": "Strategien der Integration und Isolation nicht-nativer Einheiten und Strukturen,", "citeRegEx": "Alex and Onysko,? \\Q2010\\E", "shortCiteRegEx": "Alex and Onysko", "year": 2010}, {"title": "On prediction using variable order Markov models", "author": ["R. Begleiter", "R. El-Yaniv", "G. Yona"], "venue": "Journal of Artificial Intelligence Research,", "citeRegEx": "Begleiter et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Begleiter et al\\.", "year": 2004}, {"title": "Chinese whispers: an efficient graph clustering algorithm and its application to natural language processing problems. In Proceedings of the first workshop on graph based methods for natural language processing, pages 73\u201380", "author": ["C. Biemann"], "venue": null, "citeRegEx": "Biemann,? \\Q2006\\E", "shortCiteRegEx": "Biemann", "year": 2006}, {"title": "\ue049e TIGER treebank", "author": ["S. Brants", "S. Dipper", "S. Hansen", "W. Lezius", "G. Smith"], "venue": "In Proceedings of the workshop on treebanks and linguistic theories,", "citeRegEx": "Brants et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Brants et al\\.", "year": 2002}, {"title": "Algebraic complexity theory, volume", "author": ["P. B\u00fcrgisser", "M. Clausen", "M.A. Shokrollahi"], "venue": null, "citeRegEx": "B\u00fcrgisser et al\\.,? \\Q1997\\E", "shortCiteRegEx": "B\u00fcrgisser et al\\.", "year": 1997}, {"title": "Improving language models by clustering training sentences", "author": ["D. Carter"], "venue": "In Proceedings of the fourth conference on Applied natural language processing,", "citeRegEx": "Carter,? \\Q1994\\E", "shortCiteRegEx": "Carter", "year": 1994}, {"title": "N-gram-based text categorization", "author": ["W.B. Cavnar", "J.M. Trenkle"], "venue": "In Proceedings of SDAIR-94, 3rd Annual Symposium on Document Analysis and Information Retrieval,", "citeRegEx": "Cavnar and Trenkle,? \\Q1994\\E", "shortCiteRegEx": "Cavnar and Trenkle", "year": 1994}, {"title": "An empirical study of smoothing techniques for language modeling", "author": ["S.F. Chen", "J. Goodman"], "venue": "In Proceedings of the 34th annual meeting on Association for Computational Linguistics,", "citeRegEx": "Chen and Goodman,? \\Q1996\\E", "shortCiteRegEx": "Chen and Goodman", "year": 1996}, {"title": "Clustering Methods for Improving Language Models", "author": ["E. Dreyfuss", "I. Goodfellow", "P. Baumstarck"], "venue": null, "citeRegEx": "Dreyfuss et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Dreyfuss et al\\.", "year": 2007}, {"title": "How many clusters are best?-an experiment", "author": ["R.C. Dubes"], "venue": "Pa\ue03cern Recognition,", "citeRegEx": "Dubes,? \\Q1987\\E", "shortCiteRegEx": "Dubes", "year": 1987}, {"title": "Statistical Identification of Language", "author": ["T. Dunning"], "venue": "Computing Research Laboratory,", "citeRegEx": "Dunning,? \\Q1994\\E", "shortCiteRegEx": "Dunning", "year": 1994}, {"title": "Good-turing smoothing without tears", "author": ["W. Gale", "G. Sampson"], "venue": "Journal of \ue048antitative Linguistics,", "citeRegEx": "Gale and Sampson,? \\Q1995\\E", "shortCiteRegEx": "Gale and Sampson", "year": 1995}, {"title": "\ue049e use of clustering techniques for language modeling\u2013application to Asian languages", "author": ["J. Gao", "J. Goodman", "J Miao"], "venue": "International Journal of Computational Linguistics and Chinese Language Processing,", "citeRegEx": "Gao et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Gao et al\\.", "year": 2001}, {"title": "What every computer scientist should know about floating-point arithmetic", "author": ["D. Goldberg"], "venue": "ACM Computing Surveys (CSUR),", "citeRegEx": "Goldberg,? \\Q1991\\E", "shortCiteRegEx": "Goldberg", "year": 1991}, {"title": "Language model size reduction by pruning and clustering", "author": ["J. Goodman", "J. Gao"], "venue": "In INTERSPEECH,", "citeRegEx": "Goodman and Gao,? \\Q2000\\E", "shortCiteRegEx": "Goodman and Gao", "year": 2000}, {"title": "A bit of progress in language modeling", "author": ["J.T. Goodman"], "venue": "Computer Speech and Language,", "citeRegEx": "Goodman,? \\Q2001\\E", "shortCiteRegEx": "Goodman", "year": 2001}, {"title": "Comparing two language identification schemes", "author": ["G. Grefenste\ue03ce"], "venue": "In Proceedings of the 3rd International conference on Statistical Analysis of Textual Data. JADT", "citeRegEx": "Grefenste\ue03ce,? \\Q1995\\E", "shortCiteRegEx": "Grefenste\ue03ce", "year": 1995}, {"title": "\ue049e minimum description length principle", "author": ["P.D. Gr\u00fcnwald"], "venue": null, "citeRegEx": "Gr\u00fcnwald,? \\Q2007\\E", "shortCiteRegEx": "Gr\u00fcnwald", "year": 2007}, {"title": "A closer look at skip-gram modelling", "author": ["D. Guthrie", "B. Allison", "W. Liu", "L. Guthrie", "Y. Wilks"], "venue": "In Proceedings of the 5th international Conference on Language Resources and Evaluation", "citeRegEx": "Guthrie et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Guthrie et al\\.", "year": 2006}, {"title": "\ue049e WEKA Data Mining So\ue039ware: An Update", "author": ["M. Hall", "E. Frank", "G. Holmes", "B. Pfahringer", "P. Reutemann", "I.H. Wi\ue03cen"], "venue": "SIGKDD Explorations,", "citeRegEx": "Hall et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Hall et al\\.", "year": 2009}, {"title": "Data clustering: a review", "author": ["A.K. Jain", "M.N. Murty", "P.J. Flynn"], "venue": "ACM computing surveys (CSUR),", "citeRegEx": "Jain et al\\.,? \\Q1999\\E", "shortCiteRegEx": "Jain et al\\.", "year": 1999}, {"title": "Language Identification in Code-Switching Scenario", "author": ["N. Jain", "R.A. Bhat"], "venue": "In Proceedings of the Conference on Empirical Methods on Natural Language Processing,", "citeRegEx": "Jain and Bhat,? \\Q2014\\E", "shortCiteRegEx": "Jain and Bhat", "year": 2014}, {"title": "Speech and language processing. An Introduction to Natural Language Processing, Computational Linguistics, and Speech Recognition", "author": ["D. Jurafsky", "J.H. Martin"], "venue": "Pearson Education India,", "citeRegEx": "Jurafsky and Martin,? \\Q2000\\E", "shortCiteRegEx": "Jurafsky and Martin", "year": 2000}, {"title": "Estimation of probabilities from sparse data for the language model component of a speech recognizer", "author": ["S. Katz"], "venue": "Acoustics, Speech and Signal Processing, IEEE Transactions", "citeRegEx": "Katz,? \\Q1987\\E", "shortCiteRegEx": "Katz", "year": 1987}, {"title": "Labeling the Languages of Words in Mixed-Language Documents using Weakly Supervised Methods", "author": ["B. King", "S.P. Abney"], "venue": "In Proceedings of the Conference of the North American Chapter of the Association for Computational Linguistics \u2013 Human Language Technologies,", "citeRegEx": "King and Abney,? \\Q2013\\E", "shortCiteRegEx": "King and Abney", "year": 2013}, {"title": "Language clustering with word co-occurrence networks based on parallel texts", "author": ["H. Liu", "J. Cong"], "venue": "Chinese Science Bulletin,", "citeRegEx": "Liu and Cong,? \\Q2013\\E", "shortCiteRegEx": "Liu and Cong", "year": 2013}, {"title": "Mel frequency cepstral coefficients for music modeling", "author": ["B Logan"], "venue": "In Proceedings of the 1st International Symposium onMusic Information Retrieval (ISMIR)", "citeRegEx": "Logan,? \\Q2000\\E", "shortCiteRegEx": "Logan", "year": 2000}, {"title": "Automatic detection and language identification of multilingual documents. Transactions of the Association for Computational Linguistics, 2:27\u201340", "author": ["M. Lui", "J.H. Lau", "T. Baldwin"], "venue": null, "citeRegEx": "Lui et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Lui et al\\.", "year": 2014}, {"title": "Introduction to information retrieval, volume 1", "author": ["C.D. Manning", "P. Raghavan", "H. Sch\u00fctze"], "venue": null, "citeRegEx": "Manning et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Manning et al\\.", "year": 2008}, {"title": "Foundations of statistical natural language processing", "author": ["C.D. Manning", "H. Sch\u00fctze"], "venue": null, "citeRegEx": "Manning and Sch\u00fctze,? \\Q1999\\E", "shortCiteRegEx": "Manning and Sch\u00fctze", "year": 1999}, {"title": "Novelty detection in learning systems", "author": ["S. Marsland"], "venue": "Neural computing surveys,", "citeRegEx": "Marsland,? \\Q2003\\E", "shortCiteRegEx": "Marsland", "year": 2003}, {"title": "TweetSafa: Tweet language identification", "author": ["I. Mendizabal", "J. Carandell", "D. Horowitz"], "venue": "TweetLID @ SEPLN", "citeRegEx": "Mendizabal et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Mendizabal et al\\.", "year": 2014}, {"title": "Efficient estimation of word representations in vector space", "author": ["T. Mikolov", "K. Chen", "G. Corrado", "J. Dean"], "venue": "In Proceedings of the International Conference on Learning Representations (ICLR)", "citeRegEx": "Mikolov et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "On structuring probabilistic dependences in stochastic language modelling", "author": ["H. Ney", "U. Essen", "R. Kneser"], "venue": "Computer Speech & Language,", "citeRegEx": "Ney et al\\.,? \\Q1994\\E", "shortCiteRegEx": "Ney et al\\.", "year": 1994}, {"title": "X-means: Extending K-means with Efficient Estimation of the Number of Clusters", "author": ["D. Pelleg", "A.W. Moore"], "venue": "In Proceedings of the Seventeenth International Conference on Machine Learning (ICML", "citeRegEx": "Pelleg and Moore,? \\Q2000\\E", "shortCiteRegEx": "Pelleg and Moore", "year": 2000}, {"title": "Distributional clustering of english words", "author": ["F. Pereira", "N. Tishby", "L. Lee"], "venue": "In Proceedings of the 31st annual meeting on Association for Computational Linguistics,", "citeRegEx": "Pereira et al\\.,? \\Q1993\\E", "shortCiteRegEx": "Pereira et al\\.", "year": 1993}, {"title": "Twi\ue03cer Language Identification using Rational Kernels and its potential application to Sociolinguistics", "author": ["J. Porta"], "venue": "TweetLID @ SEPLN", "citeRegEx": "Porta,? \\Q2014\\E", "shortCiteRegEx": "Porta", "year": 2014}, {"title": "Parallel Algorithms for Unsupervised Tagging", "author": ["S. Ravi", "S. Vassilivitskii", "V. Rastogi"], "venue": "Transactions of the Association for Computational Linguistics,", "citeRegEx": "Ravi et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Ravi et al\\.", "year": 2014}, {"title": "\ue049e power of amnesia: Learning probabilistic automata with variable memory length", "author": ["D. Ron", "Y. Singer", "N. Tishby"], "venue": "Machine learning,", "citeRegEx": "Ron et al\\.,? \\Q1996\\E", "shortCiteRegEx": "Ron et al\\.", "year": 1996}, {"title": "Support vector method for novelty detection", "author": ["B. Sch\u00f6lkopf", "R.C. Williamson", "A.J. Smola", "J. Shawe-Taylor", "J.C. Pla"], "venue": "In Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "Sch\u00f6lkopf et al\\.,? \\Q1999\\E", "shortCiteRegEx": "Sch\u00f6lkopf et al\\.", "year": 1999}, {"title": "Unsupervised sequence segmentation by a mixture of switching variable memory Markov sources", "author": ["Y. Seldin", "G. Bejerano", "N. Tishby"], "venue": "In Proceedings of the Seventeenth International Conference on Machine Learning (ICML),", "citeRegEx": "Seldin et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Seldin et al\\.", "year": 2001}, {"title": "Overview for the First Shared Task on Language Identification in Code-Switched Data", "author": ["T. Solorio", "E. Blair", "S. Maharjan", "S. Bethard", "M. Diab", "M. Gohneim", "A. Hawwari", "F. AlGhamdi", "J. Hirschberg", "A Chang"], "venue": "In Proceedings of the Conference on Empirical Methods on Natural Language Processing,", "citeRegEx": "Solorio et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Solorio et al\\.", "year": 2014}, {"title": "Graphing the distribution of English le\ue03cers towards the beginning, middle or end of words. http://www.prooffreader.com/2014/05/ graphing-distribution-of-english.html", "author": ["D. Taylor"], "venue": null, "citeRegEx": "Taylor,? \\Q2015\\E", "shortCiteRegEx": "Taylor", "year": 2015}, {"title": "Distributed word clustering for large scale classbased language modeling in machine translation", "author": ["J. Uszkoreit", "T. Brants"], "venue": "In Proceedings of the 46th Annual Meeting of the Association for Computational Linguistics,", "citeRegEx": "Uszkoreit and Brants,? \\Q2008\\E", "shortCiteRegEx": "Uszkoreit and Brants", "year": 2008}, {"title": "Comparing clusterings: an overview. Universit\u00e4t Karlsruhe, Fakult\u00e4t f\u00fcr Informatik Karlsruhe", "author": ["S. Wagner", "D. Wagner"], "venue": null, "citeRegEx": "Wagner and Wagner,? \\Q2007\\E", "shortCiteRegEx": "Wagner and Wagner", "year": 2007}, {"title": "Text segmentation by language using minimum description length. In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 969\u2013978", "author": ["H. Yamaguchi", "K. Tanaka-Ishii"], "venue": null, "citeRegEx": "Yamaguchi and Tanaka.Ishii,? \\Q2012\\E", "shortCiteRegEx": "Yamaguchi and Tanaka.Ishii", "year": 2012}, {"title": "Hierarchical language identification based on automatic language clustering", "author": ["B. Yin", "E. Ambikairajah", "F. Chen"], "venue": "In INTERSPEECH,", "citeRegEx": "Yin et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Yin et al\\.", "year": 2007}, {"title": "Language model based on word clustering", "author": ["L. Yuan"], "venue": "In Proceedings of the 20th Pacific Asia Conference on Language, Information and Computation,", "citeRegEx": "Yuan,? \\Q2006\\E", "shortCiteRegEx": "Yuan", "year": 2006}, {"title": "Overview of TweetLID: Tweet language identification", "author": ["A. Zubiaga", "I. San Vicente", "P. Gamallo", "J.R. Pichel", "I. Alegria", "N. Aranberri", "A. Ezeiza", "V. Fresno"], "venue": "SEPLN", "citeRegEx": "Zubiaga et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Zubiaga et al\\.", "year": 2014}], "referenceMentions": [{"referenceID": 27, "context": "Language segmentation and identification are important for all natural language processing operations that are language-specific, such as taggers, parsers or machine translation (Jain and Bhat, 2014; Zubiaga et al., 2014).", "startOffset": 178, "endOffset": 221}, {"referenceID": 54, "context": "Language segmentation and identification are important for all natural language processing operations that are language-specific, such as taggers, parsers or machine translation (Jain and Bhat, 2014; Zubiaga et al., 2014).", "startOffset": 178, "endOffset": 221}, {"referenceID": 27, "context": "Indeed, using \u201ctraditional\u201dmonolingual natural language processing components on mixed language data leads to miserable results (Jain and Bhat, 2014).", "startOffset": 128, "endOffset": 149}, {"referenceID": 5, "context": "For example, by identifying foreign language inclusions in an otherwise monolingual text, parser accuracy can be increased (Alex et al., 2007).", "startOffset": 123, "endOffset": 142}, {"referenceID": 12, "context": "1 N-Grams and rank order statistics Cavnar and Trenkle (1994) use an n-gram language model for language identification purposes.", "startOffset": 36, "endOffset": 62}, {"referenceID": 12, "context": "Cavnar and Trenkle (1994) collected 3713 Usenet texts with a cultural theme in different languages.", "startOffset": 0, "endOffset": 26}, {"referenceID": 16, "context": "2 N-Grams and maximum likelihood estimator Dunning (1994) also uses an n-gram language model for language identification purposes.", "startOffset": 43, "endOffset": 58}, {"referenceID": 16, "context": "In order to test the system, Dunning (1994) uses a specially constructed test corpus from a bilingual parallel translated English-Spanish corpus containing English and Spanish texts with 10 texts varying from 1000 to 50000 bytes for the training set and 100 texts varying from 10 to 500 bytes for the test set.", "startOffset": 29, "endOffset": 44}, {"referenceID": 15, "context": "Dunning (1994) criticizes Cavnar and Trenkle (1994) for saying that their system would be insensitive to the length of the string to be classified, as the shortest text they classified was about 50 words.", "startOffset": 0, "endOffset": 15}, {"referenceID": 12, "context": "Dunning (1994) criticizes Cavnar and Trenkle (1994) for saying that their system would be insensitive to the length of the string to be classified, as the shortest text they classified was about 50 words.", "startOffset": 26, "endOffset": 52}, {"referenceID": 12, "context": "Dunning (1994) criticizes Cavnar and Trenkle (1994) for saying that their system would be insensitive to the length of the string to be classified, as the shortest text they classified was about 50 words. \ue049e system implemented by Dunning (1994) can classify strings of 10 characters in length \u201cmoderately well\u201d, while strings of 50 characters or more are classified \u201cvery well\u201d.", "startOffset": 26, "endOffset": 245}, {"referenceID": 22, "context": "3 Trigrams and short words Grefenste\ue03ce (1995) compares trigrams versus short words for language identification.", "startOffset": 27, "endOffset": 46}, {"referenceID": 18, "context": "4 N-Grams and clustering Gao et al. (2001) present a system that augments n-gram language models with clustering techniques.", "startOffset": 25, "endOffset": 43}, {"referenceID": 13, "context": "P (wi|wi\u22122wi\u22121) = P (ci|ci\u22122ci\u22121)\u00d7 P (wi|ci\u22122ci\u22121ci) (9) Similarly, Dreyfuss et al. (2007) use clustering to cluster words by their context in order to improve trigram language models.", "startOffset": 68, "endOffset": 91}, {"referenceID": 13, "context": "P (wi|wi\u22122wi\u22121) = P (ci|ci\u22122ci\u22121)\u00d7 P (wi|ci\u22122ci\u22121ci) (9) Similarly, Dreyfuss et al. (2007) use clustering to cluster words by their context in order to improve trigram language models. In addition to Gao et al. (2001), they also use information about the subject-verb and verb-object relations of the sentence.", "startOffset": 68, "endOffset": 218}, {"referenceID": 11, "context": "Carter (1994) clusters training sentences (i.", "startOffset": 0, "endOffset": 14}, {"referenceID": 11, "context": "Carter (1994) clusters training sentences (i.e. the corpus) into subcorpora of similar sentences and calculates separate language model parameters for each subcorpus in order to capture contextual information. In contrast to other works, Carter (1994) clusters sentences instead of single words (compare Pereira et al.", "startOffset": 0, "endOffset": 252}, {"referenceID": 11, "context": "Carter (1994) clusters training sentences (i.e. the corpus) into subcorpora of similar sentences and calculates separate language model parameters for each subcorpus in order to capture contextual information. In contrast to other works, Carter (1994) clusters sentences instead of single words (compare Pereira et al. (1993) and Ney et al.", "startOffset": 0, "endOffset": 326}, {"referenceID": 11, "context": "Carter (1994) clusters training sentences (i.e. the corpus) into subcorpora of similar sentences and calculates separate language model parameters for each subcorpus in order to capture contextual information. In contrast to other works, Carter (1994) clusters sentences instead of single words (compare Pereira et al. (1993) and Ney et al. (1994)).", "startOffset": 0, "endOffset": 348}, {"referenceID": 11, "context": "Carter (1994) clusters training sentences (i.e. the corpus) into subcorpora of similar sentences and calculates separate language model parameters for each subcorpus in order to capture contextual information. In contrast to other works, Carter (1994) clusters sentences instead of single words (compare Pereira et al. (1993) and Ney et al. (1994)). Carter (1994) shows that the subdivision into smaller clusters increases the accuracy of bigram language models, but not trigram models.", "startOffset": 0, "endOffset": 364}, {"referenceID": 5, "context": "For example in (Alex et al., 2007) they report an increase in F-Score of 4.", "startOffset": 15, "endOffset": 34}, {"referenceID": 2, "context": "5 Inclusion detection Beatrice Alex (cf. Alex (2005, 2006, 2007); Alex et al. (2007); Alex and Onysko (2010)) addresses the problem of English inclusions in mainly non-English texts.", "startOffset": 31, "endOffset": 85}, {"referenceID": 2, "context": "5 Inclusion detection Beatrice Alex (cf. Alex (2005, 2006, 2007); Alex et al. (2007); Alex and Onysko (2010)) addresses the problem of English inclusions in mainly non-English texts.", "startOffset": 31, "endOffset": 109}, {"referenceID": 9, "context": "by using inclusion detection when parsing a German text with a parser trained on the TIGER corpus (Brants et al., 2002).", "startOffset": 98, "endOffset": 119}, {"referenceID": 51, "context": "6 Clustering and spee\ue03b In the area of clustering and spoken language identification, Yin et al. (2007) present a hierarchical clusterer for spoken language.", "startOffset": 85, "endOffset": 103}, {"referenceID": 32, "context": "MFCC vectors are a way of representing acoustic signals (Logan et al., 2000). \ue049e signal is first divided into smaller \u2018frames\u2019, each frame is passed through the discrete Fourier transform and only the logarithm of the amplitude spectrum is retained (Logan et al., 2000). \ue049e spectrum is then projected onto the \u2018Mel frequency scale\u2019, a scale that maps actual pitch to perceived pitch, \u201cas apparently the human auditory system does not perceive pitch in a linear manner\u201d (Logan et al., 2000). Finally, a discrete cosine transform is applied to the spectrum to get the MFCC representations of the original signal (Logan et al., 2000). Yin et al. (2007) show that their hierarchical clusterer outperforms traditionalAcoustic Gaussian Mixture Model systems.", "startOffset": 57, "endOffset": 650}, {"referenceID": 49, "context": "7 Monolingual training data Yamaguchi and Tanaka-Ishii (2012), King and Abney (2013) and Lui et al.", "startOffset": 28, "endOffset": 62}, {"referenceID": 30, "context": "7 Monolingual training data Yamaguchi and Tanaka-Ishii (2012), King and Abney (2013) and Lui et al.", "startOffset": 63, "endOffset": 85}, {"referenceID": 30, "context": "7 Monolingual training data Yamaguchi and Tanaka-Ishii (2012), King and Abney (2013) and Lui et al. (2014) use monolingual training data in order to train a system capable of recognizing the languages in a multilingual text.", "startOffset": 63, "endOffset": 107}, {"referenceID": 30, "context": "7 Monolingual training data Yamaguchi and Tanaka-Ishii (2012), King and Abney (2013) and Lui et al. (2014) use monolingual training data in order to train a system capable of recognizing the languages in a multilingual text. Yamaguchi and Tanaka-Ishii (2012) use a dynamic programming approach to segment a text by language.", "startOffset": 63, "endOffset": 259}, {"referenceID": 30, "context": "7 Monolingual training data Yamaguchi and Tanaka-Ishii (2012), King and Abney (2013) and Lui et al. (2014) use monolingual training data in order to train a system capable of recognizing the languages in a multilingual text. Yamaguchi and Tanaka-Ishii (2012) use a dynamic programming approach to segment a text by language. \ue049eir test data contains fragments of 40 to 160 characters and achieves F-scores of 0.94 on the relatively \u2018closed\u2019 data set of the Universal Declaration of Human Rights2 and 0.84 on the more \u2018open\u2019 Wikipedia data set. However, the approach is computationally intensive, not to say prohibitive; while Yamaguchi and Tanaka-Ishii (2012) self-report a processing time of 1 second for an input of 1000 characters, Lui et al.", "startOffset": 63, "endOffset": 659}, {"referenceID": 30, "context": "7 Monolingual training data Yamaguchi and Tanaka-Ishii (2012), King and Abney (2013) and Lui et al. (2014) use monolingual training data in order to train a system capable of recognizing the languages in a multilingual text. Yamaguchi and Tanaka-Ishii (2012) use a dynamic programming approach to segment a text by language. \ue049eir test data contains fragments of 40 to 160 characters and achieves F-scores of 0.94 on the relatively \u2018closed\u2019 data set of the Universal Declaration of Human Rights2 and 0.84 on the more \u2018open\u2019 Wikipedia data set. However, the approach is computationally intensive, not to say prohibitive; while Yamaguchi and Tanaka-Ishii (2012) self-report a processing time of 1 second for an input of 1000 characters, Lui et al. (2014) found that with 44 languages, the approach by Yamaguchi and Tanaka-Ishii (2012) takes almost 24 hours to complete the computation on a 16 core workstation.", "startOffset": 63, "endOffset": 752}, {"referenceID": 30, "context": "7 Monolingual training data Yamaguchi and Tanaka-Ishii (2012), King and Abney (2013) and Lui et al. (2014) use monolingual training data in order to train a system capable of recognizing the languages in a multilingual text. Yamaguchi and Tanaka-Ishii (2012) use a dynamic programming approach to segment a text by language. \ue049eir test data contains fragments of 40 to 160 characters and achieves F-scores of 0.94 on the relatively \u2018closed\u2019 data set of the Universal Declaration of Human Rights2 and 0.84 on the more \u2018open\u2019 Wikipedia data set. However, the approach is computationally intensive, not to say prohibitive; while Yamaguchi and Tanaka-Ishii (2012) self-report a processing time of 1 second for an input of 1000 characters, Lui et al. (2014) found that with 44 languages, the approach by Yamaguchi and Tanaka-Ishii (2012) takes almost 24 hours to complete the computation on a 16 core workstation.", "startOffset": 63, "endOffset": 832}, {"referenceID": 30, "context": "7 Monolingual training data Yamaguchi and Tanaka-Ishii (2012), King and Abney (2013) and Lui et al. (2014) use monolingual training data in order to train a system capable of recognizing the languages in a multilingual text. Yamaguchi and Tanaka-Ishii (2012) use a dynamic programming approach to segment a text by language. \ue049eir test data contains fragments of 40 to 160 characters and achieves F-scores of 0.94 on the relatively \u2018closed\u2019 data set of the Universal Declaration of Human Rights2 and 0.84 on the more \u2018open\u2019 Wikipedia data set. However, the approach is computationally intensive, not to say prohibitive; while Yamaguchi and Tanaka-Ishii (2012) self-report a processing time of 1 second for an input of 1000 characters, Lui et al. (2014) found that with 44 languages, the approach by Yamaguchi and Tanaka-Ishii (2012) takes almost 24 hours to complete the computation on a 16 core workstation. King and Abney (2013) use weakly supervised methods to label the languages of words.", "startOffset": 63, "endOffset": 930}, {"referenceID": 30, "context": "One problem that these approaches all have is that they need to know the languages that will occur in the test data (King and Abney, 2013; Lui et al., 2014).", "startOffset": 116, "endOffset": 156}, {"referenceID": 33, "context": "One problem that these approaches all have is that they need to know the languages that will occur in the test data (King and Abney, 2013; Lui et al., 2014).", "startOffset": 116, "endOffset": 156}, {"referenceID": 32, "context": "Lui et al. (2014) consider the task as multi-label classification task.", "startOffset": 0, "endOffset": 18}, {"referenceID": 7, "context": "Whereas HMMs require substantial amounts of training data and a deep understanding of the problem in order to restrict the model architecture, VMMs are simpler and less expressive than HMMs, but have been shown to \u201csolve many applications with notable success\u201d (Begleiter et al., 2004).", "startOffset": 261, "endOffset": 285}, {"referenceID": 7, "context": "In contrast to n-gram models that estimate the probability of w as P (w|N) with N the context (typically the n previous words), VMMs can vary N in function of the available context (Begleiter et al., 2004).", "startOffset": 181, "endOffset": 205}, {"referenceID": 7, "context": "\ue049us, they can capture both small and large order dependencies, depending on the training data (Begleiter et al., 2004).", "startOffset": 94, "endOffset": 118}, {"referenceID": 44, "context": "One of these algorithms is called Predictive Suffix Tree (PST) (Ron et al., 1996).", "startOffset": 63, "endOffset": 81}, {"referenceID": 44, "context": "A PST is a tree over an alphabet \u03a3, with each node either having 0 (leaf nodes) or |\u03a3| children (non-terminal nodes) (Ron et al., 1996).", "startOffset": 117, "endOffset": 135}, {"referenceID": 44, "context": "Each node is labeled with the result of the walk from that node up to the root (Ron et al., 1996).", "startOffset": 79, "endOffset": 97}, {"referenceID": 44, "context": "Each edge is labeled by a symbol s \u2208 \u03a3 and the probability for the next symbol being s (Ron et al., 1996).", "startOffset": 87, "endOffset": 105}, {"referenceID": 23, "context": "\ue049e MDL principle avoids overfi\ue03cing of the model by favoring low complexity over goodness-of-fit (Gr\u00fcnwald, 2007).", "startOffset": 96, "endOffset": 112}, {"referenceID": 43, "context": "8 Predictive suffix trees Seldin et al. (2001) propose a system for automatic unsupervised language segmentation and protein sequence segmentation.", "startOffset": 26, "endOffset": 47}, {"referenceID": 7, "context": "Whereas HMMs require substantial amounts of training data and a deep understanding of the problem in order to restrict the model architecture, VMMs are simpler and less expressive than HMMs, but have been shown to \u201csolve many applications with notable success\u201d (Begleiter et al., 2004). In contrast to n-gram models that estimate the probability of w as P (w|N) with N the context (typically the n previous words), VMMs can vary N in function of the available context (Begleiter et al., 2004). \ue049us, they can capture both small and large order dependencies, depending on the training data (Begleiter et al., 2004). \ue049ere is no single VMM algorithm, but rather a family of related algorithms. One of these algorithms is called Predictive Suffix Tree (PST) (Ron et al., 1996). A PST is a tree over an alphabet \u03a3, with each node either having 0 (leaf nodes) or |\u03a3| children (non-terminal nodes) (Ron et al., 1996). Each node is labeled with the result of the walk from that node up to the root (Ron et al., 1996). Each edge is labeled by a symbol s \u2208 \u03a3 and the probability for the next symbol being s (Ron et al., 1996). By modifying the Predictive Suffix Tree (PST) algorithm using the Minimum Description Length (MDL) principle, Seldin et al. (2001) end up with a non-parametric self-regulating algorithm.", "startOffset": 262, "endOffset": 1247}, {"referenceID": 18, "context": "1 N-Gram models Among supervised languagemodels, n-grammodels are very popular (Gao et al., 2001).", "startOffset": 79, "endOffset": 97}, {"referenceID": 12, "context": "An n-gram is a slice from the original string (Cavnar and Trenkle, 1994).", "startOffset": 46, "endOffset": 72}, {"referenceID": 24, "context": "Non-contiguous n-grams are also called skip-grams (Guthrie et al., 2006).", "startOffset": 50, "endOffset": 72}, {"referenceID": 24, "context": "In this parlance, contiguous n-grams can be regarded as 0-skip-n-grams (Guthrie et al., 2006).", "startOffset": 71, "endOffset": 93}, {"referenceID": 24, "context": "As can be seen from this example, the number of skip-grams ismore than two times higher than the number of contiguous n-grams, and this trend continues the more skips are allowed (Guthrie et al., 2006).", "startOffset": 179, "endOffset": 201}, {"referenceID": 12, "context": "O\ue039en, the word to decompose is padded with start and end tags in order to improve the model (Cavnar and Trenkle, 1994).", "startOffset": 92, "endOffset": 118}, {"referenceID": 12, "context": "\ue049e use of paddings allows themodel to capture details about character distribution with regard to the start and end of words (Cavnar and Trenkle, 1994).", "startOffset": 125, "endOffset": 151}, {"referenceID": 48, "context": "at the beginning of words, while the le\ue03cer \u2018w\u2019 occurs mainly at the beginning of words (Taylor, 2015).", "startOffset": 87, "endOffset": 101}, {"referenceID": 12, "context": "One advantage of n-gram models is that the decomposition of a string into smaller units reduces the impact of typing errors (Cavnar and Trenkle, 1994).", "startOffset": 124, "endOffset": 150}, {"referenceID": 12, "context": "Indeed, a typing error only affects a limited number of units (Cavnar and Trenkle, 1994).", "startOffset": 62, "endOffset": 88}, {"referenceID": 12, "context": "Due to this property, n-gram models have been shown to be able to deal well with noisy text (Cavnar and Trenkle, 1994).", "startOffset": 92, "endOffset": 118}, {"referenceID": 13, "context": "In order to avoid this problem, different smoothing techniques can be used (Chen and Goodman, 1996).", "startOffset": 75, "endOffset": 99}, {"referenceID": 13, "context": "\ue049e simplest smoothing technique is additive (Laplace) smoothing (Chen and Goodman, 1996).", "startOffset": 64, "endOffset": 88}, {"referenceID": 28, "context": "If we choose \u03bb = 1, we speak of \u201cadd one\u201d smoothing (Jurafsky and Martin, 2000).", "startOffset": 52, "endOffset": 79}, {"referenceID": 35, "context": "In practice, \u03bb < 1 is o\ue039en chosen (Manning and Sch\u00fctze, 1999).", "startOffset": 34, "endOffset": 61}, {"referenceID": 13, "context": "\ue049us, instead of using the actual count c, the count is taken to be c\u2217 (Chen and Goodman, 1996).", "startOffset": 70, "endOffset": 94}, {"referenceID": 29, "context": "Katz\u2019s back-off model (Katz, 1987) for instance calculates probability Pbo using the formula:", "startOffset": 22, "endOffset": 34}, {"referenceID": 8, "context": "2 Unsupervised clustering Clustering consists in the grouping of objects based on their mutual similarity (Biemann, 2006).", "startOffset": 106, "endOffset": 121}, {"referenceID": 8, "context": "Objects to be clustered are typically represented as feature vectors (Biemann, 2006); from the original objects, a feature representation is calculated and used for further processing.", "startOffset": 69, "endOffset": 84}, {"referenceID": 52, "context": "Clustering can be partitional or hierarchical (Yin et al., 2007).", "startOffset": 46, "endOffset": 64}, {"referenceID": 52, "context": "Partitional clustering divides the initial objects into separate groups in one step, whereas hierarchical clustering builds a hierarchy of objects by first grouping the most similar objects together and then clustering the next level hierarchy with regard to the existing clusters (Yin et al., 2007).", "startOffset": 281, "endOffset": 299}, {"referenceID": 8, "context": "\ue049e clustering algorithm uses a distancemetric tomeasure the distance between the feature vectors of objects (Biemann, 2006).", "startOffset": 108, "endOffset": 123}, {"referenceID": 26, "context": "\ue049e distance metric defines the similarity of objects based on the feature space in which the objects are represented (Jain et al., 1999).", "startOffset": 117, "endOffset": 136}, {"referenceID": 8, "context": "the angle between them (Biemann, 2006).", "startOffset": 23, "endOffset": 38}, {"referenceID": 26, "context": "In order for a clustering algorithm to work, features that represent the object to be clustered have to be defined (Jain et al., 1999).", "startOffset": 115, "endOffset": 134}, {"referenceID": 26, "context": "word starts with a capital le\ue03cer) (Jain et al., 1999).", "startOffset": 34, "endOffset": 53}, {"referenceID": 26, "context": "k-means, need the number of clusters to generate (Jain et al., 1999).", "startOffset": 49, "endOffset": 68}, {"referenceID": 26, "context": "When hard-clustering, an object can belong to one class only, while in so\ue039-clustering, an object can belong to one or more classes, sometimes with different probabilities (Jain et al., 1999).", "startOffset": 171, "endOffset": 190}, {"referenceID": 8, "context": "\ue049e clustering algorithm uses a distancemetric tomeasure the distance between the feature vectors of objects (Biemann, 2006). \ue049e distance metric defines the similarity of objects based on the feature space in which the objects are represented (Jain et al., 1999). \ue049ere are different metrics available. A frequently chosen metric is the cosine similarity that calculates the distance between two vectors, i.e. the angle between them (Biemann, 2006). In order for a clustering algorithm to work, features that represent the object to be clustered have to be defined (Jain et al., 1999). Features can be quantitative (e.g. word length) or qualitative (e.g. word starts with a capital le\ue03cer) (Jain et al., 1999). Most clustering algorithms, e.g. k-means, need the number of clusters to generate (Jain et al., 1999). \ue049e question how to best choose this key number has been addressed in-depth by Dubes (1987). Clustering can be so\ue039 or hard.", "startOffset": 109, "endOffset": 902}, {"referenceID": 46, "context": "\ue049is approach is similar in character to the work by Seldin et al. (2001) in that the text itself is used as data set.", "startOffset": 52, "endOffset": 73}, {"referenceID": 46, "context": "\ue049is approach is similar in character to the work by Seldin et al. (2001) in that the text itself is used as data set. However, the realization differs greatly. Whereas Seldin et al. (2001) use predictive suffix trees, I use n-gram language models.", "startOffset": 52, "endOffset": 189}, {"referenceID": 16, "context": "1 Implementation For the supervised language segmentation method, I implemented an n-gram language model as described by Dunning (1994). \ue049e n-gram language model is implemented as a character trigram model with non-linear back-off to bigram and unigram models.", "startOffset": 121, "endOffset": 136}, {"referenceID": 19, "context": "Indeed, multiplying very small numbers can lead to the result being approximated as zero by the computer when the numbers become too small to be represented as normalized number (Goldberg, 1991).", "startOffset": 178, "endOffset": 194}, {"referenceID": 10, "context": "Using the sum of logarithms avoids this problem and is less computationally expensive (B\u00fcrgisser et al., 1997).", "startOffset": 86, "endOffset": 110}, {"referenceID": 25, "context": "3 Unsupervised clustering In order to test the efficiency of clustering algorithms on the task of language segmentation, I looked at various algorithms readily available throughWEKA, \u201ca collection of machine learning algorithms for data mining tasks\u201d by the University of Waikato in New Zealand (Hall et al., 2009) and the Environment for Developing KDD-Applications Supported by Index-Structures (ELKI), \u201can open source data mining so\ue039ware [.", "startOffset": 295, "endOffset": 314}, {"referenceID": 1, "context": "] with an emphasis on unsupervised methods in cluster analysis and outlier detection\u201d by the Ludwig-Maximilians-Universit\u00e4t M\u00fcnchen (Achtert et al., 2013).", "startOffset": 132, "endOffset": 154}, {"referenceID": 0, "context": "I also looked at JavaML, \u201ca collection of machine learning and data mining algorithms\u201d (Abeel et al., 2009), in order to integrate clusterers into my own code framework.", "startOffset": 87, "endOffset": 107}, {"referenceID": 40, "context": "In contrast, the x-means algorithm (Pelleg and Moore, 2000) estimates the number of clusters to generate itself.", "startOffset": 35, "endOffset": 59}, {"referenceID": 50, "context": "pairs (Wagner and Wagner, 2007).", "startOffset": 6, "endOffset": 31}, {"referenceID": 50, "context": "\ue049e Rand Index measures the accuracy of the clustering given a reference partition (Wagner and Wagner, 2007).", "startOffset": 82, "endOffset": 107}, {"referenceID": 50, "context": "However, it is criticized for being highly dependent on the number of clusters (Wagner and Wagner, 2007).", "startOffset": 79, "endOffset": 104}, {"referenceID": 50, "context": "It is similar to the Rand Index, but it disregards S00, the set of pairs that are clustered into different clusters in C and C \u2032 (Wagner and Wagner, 2007).", "startOffset": 129, "endOffset": 154}, {"referenceID": 50, "context": "\ue049e Fowlkes-Mallows Index has the undesired property of yielding high values when the number of clusters is small (Wagner and Wagner, 2007).", "startOffset": 113, "endOffset": 138}, {"referenceID": 34, "context": "According toManning et al. (2008), in the context of clustering evaluation the F(\u03b2) score is defined as", "startOffset": 12, "endOffset": 34}, {"referenceID": 34, "context": "By varying \u03b2, it is possible to give more weight to either precision (\u03b2 < 0) or recall (\u03b2 > 1) (Manning et al., 2008).", "startOffset": 95, "endOffset": 117}, {"referenceID": 50, "context": "As there is no ultimate measure and all measures of similarity have their drawbacks (Wagner and Wagner, 2007), all measures will be indicated in the results section.", "startOffset": 84, "endOffset": 109}, {"referenceID": 46, "context": "\ue049e work by Seldin et al. (2001) is similar to the work presented here.", "startOffset": 11, "endOffset": 32}, {"referenceID": 51, "context": "\ue049e work by Yin et al. (2007) and the work by Seldin et al.", "startOffset": 11, "endOffset": 29}, {"referenceID": 46, "context": "(2007) and the work by Seldin et al. (2001) are closest in topic to this thesis.", "startOffset": 23, "endOffset": 44}, {"referenceID": 46, "context": "(2007) and the work by Seldin et al. (2001) are closest in topic to this thesis. However, Yin et al. (2007) concern themselves with spoken language, with requires a different approach than dealing with wri\ue03cen language.", "startOffset": 23, "endOffset": 108}, {"referenceID": 46, "context": "(2007) and the work by Seldin et al. (2001) are closest in topic to this thesis. However, Yin et al. (2007) concern themselves with spoken language, with requires a different approach than dealing with wri\ue03cen language. As I concentrated on wri\ue03cen language, their work was not conducive to this thesis. In contrast, Seldin et al. (2001) present a work that looks promising.", "startOffset": 23, "endOffset": 336}], "year": 2015, "abstractText": "Language segmentation consists in finding the boundaries where one language ends and another language begins in a text wri\ue03cen in more than one language. \ue049is is important for all natural language processing tasks. \ue049e problem can be solved by training language models on language data. However, in the case of lowor no-resource languages, this is problematic. I therefore investigate whether unsupervised methods perform be\ue03cer than supervised methods when it is difficult or impossible to train supervised approaches. A special focus is given to difficult texts, i.e. texts that are rather short (one sentence), containing abbreviations, low-resource languages and non-standard language. I compare three approaches: supervised n-gram language models, unsupervised clustering and weakly supervised n-gram language model induction. I devised the weakly supervised approach in order to deal with difficult text specifically. In order to test the approach, I compiled a small corpus of different text types, ranging from one-sentence texts to texts of about 300 words. \ue049e weakly supervised language model induction approach works well on short and difficult texts, outperforming the clustering algorithm and reaching scores in the vicinity of the supervised approach. \ue049e results look promising, but there is room for improvement and a more thorough investigation should be undertaken.", "creator": " XeTeX output 2015.08.28:1436"}}}