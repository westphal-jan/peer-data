{"id": "1510.01949", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "7-Oct-2015", "title": "Hierarchical Representation of Prosody for Statistical Speech Synthesis", "abstract": "Prominences and boundaries are the essential constituents of prosodic structure in speech. They provide for means to chunk the speech stream into linguistically relevant units by providing them with relative saliences and demarcating them within coherent utterance structures. Prominences and boundaries have both been widely used in both basic research on prosody as well as in text-to-speech synthesis. However, there are no representation schemes that would provide for both estimating and modelling them in a unified fashion. Here we present an unsupervised unified account for estimating and representing prosodic prominences and boundaries using a scale-space analysis based on continuous wavelet transform. The methods are evaluated and compared to earlier work using the Boston University Radio News corpus. The results show that the proposed method is comparable with the best published supervised annotation methods.", "histories": [["v1", "Wed, 7 Oct 2015 14:08:13 GMT  (1916kb,D)", "http://arxiv.org/abs/1510.01949v1", "22 pages, 5 figures"]], "COMMENTS": "22 pages, 5 figures", "reviews": [], "SUBJECTS": "cs.CL cs.SD", "authors": ["antti suni", "daniel aalto", "martti vainio"], "accepted": false, "id": "1510.01949"}, "pdf": {"name": "1510.01949.pdf", "metadata": {"source": "CRF", "title": "Hierarchical Representation of Prosody for Statistical Speech Synthesis", "authors": ["Antti Suni", "Daniel Aalto", "Martti Vainioa"], "emails": [], "sections": [{"heading": null, "text": "Prominence and boundaries are the essential components of prosodic structures in language, providing a means of dividing the language flow into linguistically relevant units by giving them relative emphasis and delimiting them within coherent structures of expression. Prominence and boundaries have been widely used in both basic prosody research and textto speech synthesis, but there are no representation schemes that would allow a uniform estimation and modelling of these structures. At this point, we present an unsupervised uniform representation of prosodic prominences and boundaries by means of a scale space analysis based on continuous wave transformation.The methods are evaluated and compared with previous work using the corpus of Boston University Radio News. Results show that the proposed method is comparable to the best published supervised annotation methods.Keywords: phonetics, prosody, speech synthesis, wave formation."}, {"heading": "1. Introduction", "text": "This year, it has reached the stage where it will be able to take the lead."}, {"heading": "2. Methods", "text": "In fact, it is such that most of them will be able to move into another world, in which they are able to move into another world, in which they are able to move into another world, in which they are able to move, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they, in which they live, in which they, in which they live, in which they live, in which they, in which they live, in which they, in which they live, in which they live, in which they live, in which they, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they, in which they live, in which they live, in which they live, in which they, in which they live, in which they live, in which they live, in which they live, in which they, in which they live, in which they live, in which they, in which they, in which they live, in which they, in which they, in which they live, in which they, in which they, in which they, in which they live, in which they, in which they, in which they, in which they live, in which they, in which they, in which they, in which they live, in which they, in which they, in which they, in which they live, in which they, in which they, in which they, in which they, in which they live, in which they, in which they, in which they, in which they are able to live, in which they, in which they, in which they, in which they, in fact, in fact, are able"}, {"heading": "2.1. Wavelet decomposition", "text": "Continuous theory is explained in detail by Daubechies and the theory is applied to time series such as Torrence and Compo [50, 51]. CWT is a decomposition of a signal into scales that can be summed up approximately to the original signal. To define the transformation, let's be a one-dimensional signal with real values and finite energy. On a scale \u03c3 > 0 and a temporal translation \u03c4, the continuous waveform can be defined as Ws (\u03c3, \u03c4) = 1 / 2s, when it denotes the folding and finite energy, is the Mexican mother waveform that is translated from the earth and scattered from the earth. Although the Mexican mother waveform has infinite support, the values decay exponentially far away from the origin and the mother waveguide can be used effectively on a fixed > wavelength of seven scales."}, {"heading": "2.2. Lines of maximum amplitude", "text": "The Mexican hat nut wavelength belongs to a family of Gaussian wavelengths. These wavelengths seem to provide a suitable compromise between temporal and frequency selectivity in the time-frequency representation of the prosodic signals.It is important that the Gaussian wavelengths give trees that allow a complete reconstruction of the original signal.Visually, the trees look stable and consistent with the Mexican nut wavelengths. Instead of a complete tree representation of the prosody (as in Fig. 3.), a reduced tree representation is used here. For this purpose, the lines of maximum amplitude (LoMA) are defined recursively by linking local maxima over scales. Firstly, let t1.0, t2.0,. tMW, 0 be the points of time where the local maxima occurred, in the finest scale."}, {"heading": "2.3. Preprocessing of the signals", "text": "The acoustic signal reflects the physiological control actions behind speech communication. Emphasized words are often louder, higher pitch, and longer as a result of more production effort, higher fundamental frequency, and longer duration. To analyze the acoustic patterns, the abrupt changes in f0 or gain, e.g. due to occlusions in the vocal tract during breaks, generate strong hierarchical structures in the wave image that may not be part of the auditory shape [52]. Due to the more continuous underlying articulation gestures, and due to the seemingly more continuous perceptions, the acoustic signals are \"filled\" in areas where the signal cannot be found (for f0) or where it is very weak (amplification). Furthermore, a continuous (temporal) representation of duration is sought. Although inspired by the physiology of the vocal and auditory apparatus, the goal of these transformations is not to model the systems, but to make the algorithm more comparable to the other phenomenological approaches to describe the most important patterns."}, {"heading": "2.3.1. Intensity", "text": "The intensity fluctuations in the speech signal are caused primarily by (intentional and accidental) fluctuations in subglottal pressure and degree of hyperarticulation (especially in friction).As a representative of the articulation claim, the gain of the acoustic signal is transformed by iterative interpolation of the silent gaps. Allow the Gaussian core and g the original amplification signal (i.e. a logarithm of amplitude).A family of scaling functions, {\u03c6i} i, is obtained by widening and scaling the window. G is the size of the family. G is smoothed recursively. For i = 0, 1, 2,..., where wmax is the maximum smoothing window size, wmin is the minimum window and n the size of the family."}, {"heading": "2.3.2. Fundamental frequency", "text": "The pitch of the spoken tones is closely related to the lowest self-resonances of the vocal folds = = fundamental frequency = first signal. However, in unvoiced speech segments, the association between the acoustic signal and the self-resonances of the vocal folds is closely related. It is important that even during quiet periods, controls on the vocal folds that affect the f0 take place as soon as the vibration is triggered either by adduction of the vocal folds or by restoration of the air flow through the vocal tract. In addition to the internal state of the larynx, the frequency of the glottal vibration is influenced by the subglottal pressure. Unsurprisingly, then the f0 and the intensity are strongly correlated. In order to estimate the state of the f0control during unspoken parts, an algorithm is proposed in which the surface energy f0 values for the spoken passages remain unchanged and the underlying state of the vocal folds is estimated by unspoken passages."}, {"heading": "2.3.3. Duration", "text": "The duration of a phonological unit varies depending on its position within a given speech. For example, the speech rate often changes across borders and accentuated words are longer. As there are no signal-based speech rate estimators, the continuous signal must be based on analytical linguistic units and not on the raw signal. To quantify the duration, a relationship between the acoustic (continuous) duration and a suitable discrete linguistic unit is required. A natural candidate could be a syllable, but here an orthographic word is chosen instead, since the syllable boundaries are not easily derived from the text without supervision. To apply the wave analysis to the duration, it is extended to a continuous time-dependent variable that would ideally reflect the local duration of the linguistic units. In the current experiment, word alignments were used, the word boundaries, x0, x1,.. xNw, where Nw is the number of orthographic words within a given language and the length of the signal duration."}, {"heading": "2.4. Annotation", "text": "The note of accents and pauses (highlighting and limits) is based on the wave decomposition of the basic frequency, amplification and duration signals. These three acoustic signals were normalized to have unity variance, and then added together to obtain an aprosodic signal. The finest scale to be analyzed was defined as an octave below the word entry rate. To normalize the speech rate, the finest scale was selected separately for each utterance, i.e. 8a0. If no LoMA took place during the word, the accent strength was set to zero. Prosodic pauses manifested largely on larger scales, since the word scale was taken as the finest scale, i.e. 8a0. If no LoMA took place during the word, the accent strength was set to zero."}, {"heading": "3. Experimental Results", "text": "This year it has come to the point where it will be able to retaliate, \"he said.\" We have never lost so much time as this year, \"he said.\" We have never lost so much time, \"he said,\" but we have never lost so much time. \""}, {"heading": "3.1. Corpus", "text": "The corpus consists of approximately two and a half hours of messages, which are read by 6 speakers with manual tone and break index annotations. The ToBi marking scheme was originally developed for the transcription of speech melodies [64], so high (H), low (L) and complex accent types (H *, L *, L * + H, L + H *, H +! H *) are used, which deal with syllable plane and peak alignment. Prosodic boundaries are commented with boundary tones (L-, H-, L - L%, L - H%, H - H - H *), which are limits."}, {"heading": "3.2. Features and Processing", "text": "The proposed method was evaluated using standard prosodic features; f0, energy and word duration, as well as all combinations thereof. Raw f0 and energy parameters were analyzed by 16 kHz voice signals using GlottHMM analysis synthesis framework [53] with five millisecond image shift. The method used iterative-adaptive inverse filtering to separate the contributions of vowel tract and speech source, and performs f0 analysis on the source signal using autocorrelation method. Log energy is calculated from the entire signal. Pitch range was set separately for male and female speakers, 70-300 Hz and 120-400 Hz, respectively. Received f0 and energy parameters were interpolated with peak-obtaining methods and word durations were specified to continuous signals as described in Section 2.3."}, {"heading": "3.3. Results", "text": "We report on the results of the CWT-LoMA analysis of f0 (f0) energy (s) and duration (dur) and their combinations of prominence and boundary detection. The performance of the gap-filling of energy is evaluated separately, and if the gap-filling margin improves the performance of prominence or boundary detection, it is also used for energy in combined characteristics. Limits are defined as manual brake indices of either 3 or 4; prominence if any syllable of a word carries an accent. Results are presented in terms of correct detection characteristics, i.e. accuracy, recall and F-score, we report on the majority class, predictions that come from the best combination signal without wavelength analysis, as well as the current state that is not monitored."}, {"heading": "4. Discussion and Conclusions", "text": "Unlike most published work on language prosody, the results here show that prosodic structure can - and probably should - be presented within a unified framework that includes all relevant signal variables at the same time. For statistical language synthesis, we are now in a position where we can perform complete annotations and modelling of prosody within a unified framework. Although we have presented the methods at the service of speech synthesis, the results are interesting in themselves, that is, they show that annotations and limitations can be regarded as manifestations of the same underlying language production process. Of course, this has many theoretical implications, such is the fact that the suprasegmental variables are used (f0, envelope, duration)."}, {"heading": "Acknowledgements", "text": "The research that led to these results was funded by the Seventh Framework Programme of the European Community (FP7 / 2007-2013) under Funding Agreement No. 287678 (Simple4All) and the Finnish Academy (Project No. 1265610 (the MIND Programme). We thank Juraj \u0160imko for his insights into this manuscript. Special thanks go to Paavo Alku and Tuomo Raitio for their cooperation with GlottHMM."}], "references": [{"title": "Machine learning paradigms for speech recognition: An overview", "author": ["L. Deng", "X. Li"], "venue": "IEEE Transactions on Audio, Speech & Language Processing 21 (5) ", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2013}, {"title": "From text to speech with srs", "author": ["S.R. Hertz"], "venue": "The Journal of the Acoustical Society of America 72 (4) ", "citeRegEx": "3", "shortCiteRegEx": null, "year": 1982}, {"title": "The delta rule development system for speech synthesis from text", "author": ["S.R. Hertz", "J. Kadin", "K.J. Karplus"], "venue": "Proceedings of the IEEE 73 (11) ", "citeRegEx": "4", "shortCiteRegEx": null, "year": 1985}, {"title": "Developing a Finnish concept-to-speech system", "author": ["M. Vainio", "A. Suni", "P. Sirjola"], "venue": "in: M. Langemets, P. Penjam (Eds.), Proceedings of the Second Baltic Conference on HUMAN LANGUAGE TECHNOLOGIES, Tallinn", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2005}, {"title": "Autosegmental and metrical phonology", "author": ["J.A. Goldsmith"], "venue": "Vol. 11, Blackwell Oxford", "citeRegEx": "6", "shortCiteRegEx": null, "year": 1990}, {"title": "Three dimensional phonology", "author": ["M. Halle", "J.-R. Vergnaud"], "venue": "Journal of linguistic research 1 (1) ", "citeRegEx": "8", "shortCiteRegEx": null, "year": 1980}, {"title": "Review of text-to-speech conversion for english", "author": ["D.H. Klatt"], "venue": "The Journal of the Acoustical Society of America 82 (3) ", "citeRegEx": "9", "shortCiteRegEx": null, "year": 1987}, {"title": "Word and sentence intonation: A quantitative model", "author": ["S. \u00d6hman"], "venue": "Speech Transmission Laboratory, Department of Speech Communication, Royal Institute of Technology", "citeRegEx": "10", "shortCiteRegEx": null, "year": 1967}, {"title": "A generative model for the prosody of connected speech in japanese", "author": ["H. Fujisaki", "H. Sudo"], "venue": "Annual Report of Engineering Research Institute 30 ", "citeRegEx": "11", "shortCiteRegEx": null, "year": 1971}, {"title": "Analysis of voice fundamental frequency contours for declarative sentences of Japanese", "author": ["H. Fujisaki", "K. Hirose"], "venue": "Journal of the Acoustical Society of Japan (E) 5 (4) ", "citeRegEx": "12", "shortCiteRegEx": null, "year": 1984}, {"title": "Sfc: a trainable prosodic model", "author": ["G. Bailly", "B. Holm"], "venue": "Speech Communication 46 (3) ", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2005}, {"title": "A", "author": ["G.K. Anumanchipalli", "L.C. Oliveira"], "venue": "W. Black, A statistical phrase/accent model for intonation modeling., in: INTERSPEECH", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2011}, {"title": "C", "author": ["G. Kochanski"], "venue": "Shih, Stem-ml: language-independent prosody description., in: INTERSPEECH", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2000}, {"title": "Prosody modeling with soft templates", "author": ["G. Kochanski", "C. Shih"], "venue": "Speech Communication 39 (3) ", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2003}, {"title": "Syllable prominence: A matter of vocal effort", "author": ["A. Eriksson", "G.C. Thunberg", "H. Traunm\u00fcller"], "venue": "phonetic distinctness and top-down processing, in: Proc. European Conf. on Speech Communication and Technology Aalborg, September 2001, Vol. 1", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2001}, {"title": "Signal-based and expectation-based factors in the perception of prosodic prominence", "author": ["J. Cole", "Y. Mo", "M. Hasegawa-Johnson"], "venue": "Laboratory Phonology 1 (2) ", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2010}, {"title": "New method for delexicalization and its application to prosodic tagging for textto-speech synthesis", "author": ["M. Vainio", "A. Suni", "T. Raitio", "J. Nurminen", "J. J\u00e4rvikivi", "P. Alku"], "venue": "in: Interspeech, Brighton, UK", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2009}, {"title": "Obtaining prominence judgments from n\u00e4\u0131ve listeners\u2013influence of rating scales, linguistic levels and normalisation", "author": ["D. Arnold", "P. Wagner", "B. M\u00f6bius"], "venue": "Proceedings of Interspeech", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2012}, {"title": "Rule-based prosody prediction for German text-to-speech synthesis", "author": ["S. Becker", "M. Schr\u00f6der", "W.J. Barry"], "venue": "in: Proceedings of Speech Prosody, Vol. 2006", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2006}, {"title": "The GlottHMM speech synthesis entry for Blizzard Challenge 2010", "author": ["A. Suni", "T. Raitio", "M. Vainio", "P. Alku"], "venue": "in: Blizzard Challenge 2010 Workshop, Kyoto, Japan", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2010}, {"title": "The GlottHMM entry for Blizzard Challenge 2011: Utilizing source unit selection in HMM-based speech synthesis for improved excitation generation", "author": ["A. Suni", "T. Raitio", "M. Vainio", "P. Alku"], "venue": "in: Blizzard Challenge 2011 Workshop, Florence, Italy", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2011}, {"title": "The GlottHMM entry for Blizzard Challenge 2012 \u2013 Hybrid approach", "author": ["A. Suni", "T. Raitio", "M. Vainio", "P. Alku"], "venue": "in: Blizzard Challenge 2012 Workshop, Portland, Oregon", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2012}, {"title": "M", "author": ["L. Badino", "R.A. Clark"], "venue": "Wester, Towards hierarchical prosodic prominence generation in tts synthesis., in: INTERSPEECH", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2012}, {"title": "Tonal features", "author": ["M. Vainio", "J. J\u00e4rvikivi"], "venue": "intensity, and word order in the perception of prominence, Journal of Phonetics 34 ", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2006}, {"title": "Wavelets for intonation modeling in HMM speech synthesis", "author": ["A. Suni", "D. Aalto", "T. Raitio", "P. Alku", "M. Vainio"], "venue": "in: 8th ISCA Speech Synthesis Workshop (SSW8), Barcelona, Spain", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2013}, {"title": "Continuous wavelet transform for analysis of speech prosody", "author": ["M. Vainio", "A. Suni", "D. Aalto"], "venue": "TRASP 2013-Tools and Resources for the Analysys of Speech Prosody, An Interspeech 2013 satellite event, August 30", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2013}, {"title": "Emphasis", "author": ["M. Vainio", "A. Suni", "D. Aalto"], "venue": "word prominence, and continuous wavelet transform in the control of hmm based synthesis, in: Hirose, Tao (Eds.), Speech Prosody in Speech Synthesis - Modeling, realizing, converting prosody for high quality and flexible speech synthesis, Springer", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2015}, {"title": "The image processing handbook", "author": ["J.C. Russ", "R.P. Woods"], "venue": "Journal of Computer Assisted Tomography 19 (6) ", "citeRegEx": "30", "shortCiteRegEx": null, "year": 1995}, {"title": "A geometric model for the functional circuits of the visual front-end, in: Brain-Inspired", "author": ["B.M. ter Haar Romeny"], "venue": null, "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2014}, {"title": "Basilar membrane motion", "author": ["G. Zweig"], "venue": "in: Cold Spring Harbor Symposia on Quantitative Biology, Vol. 40, Cold Spring Harbor Laboratory Press", "citeRegEx": "32", "shortCiteRegEx": null, "year": 1976}, {"title": "Auditory representations of acoustic signals", "author": ["X. Yang", "K. Wang", "S.A. Shamma"], "venue": "Information Theory, IEEE Transactions on 38 (2) ", "citeRegEx": "34", "shortCiteRegEx": null, "year": 1992}, {"title": "Modern methods of speech processing", "author": ["R. Ramachandran", "R. Mammone"], "venue": "Springer", "citeRegEx": "35", "shortCiteRegEx": null, "year": 1995}, {"title": "Speech Time-Frequency Representation", "author": ["M.D. Riley"], "venue": "Vol. 63, Springer", "citeRegEx": "36", "shortCiteRegEx": null, "year": 1989}, {"title": "Signal processing in the cochlea: The structure equations", "author": ["H. Reimann"], "venue": "J. Math. Neuroscience 1 (5) ", "citeRegEx": "37", "shortCiteRegEx": null, "year": 2011}, {"title": "Cortical oscillations and speech processing: emerging computational principles and operations", "author": ["A.-L. Giraud", "D. Poeppel"], "venue": "Nature neuroscience 15 (4) ", "citeRegEx": "38", "shortCiteRegEx": null, "year": 2012}, {"title": "Application of Wavelets in Speech Processing", "author": ["M.H. Farouk"], "venue": "Springer", "citeRegEx": "39", "shortCiteRegEx": null, "year": 2014}, {"title": "A wavelet tour of signal processing", "author": ["S. Mallat"], "venue": "Access Online via Elsevier", "citeRegEx": "40", "shortCiteRegEx": null, "year": 1999}, {"title": "Decomposition of functions into wavelets of constant shape", "author": ["A. Grossman", "J. Morlet"], "venue": "and related transforms, Mathematics and Physics: Lectures on Recent Results 11 ", "citeRegEx": "41", "shortCiteRegEx": null, "year": 1985}, {"title": "M", "author": ["H. Kruschke"], "venue": "Lenz, Estimation of the parameters of the quantitative intonation model with continuous wavelet analysis., in: INTERSPEECH", "citeRegEx": "42", "shortCiteRegEx": null, "year": 2003}, {"title": "Estimating phrase curves in the general superpositional intonation", "author": ["J.P. v. Santen", "T. Mishra", "E. Klabbers"], "venue": "Fifth ISCA Workshop on Speech Synthesis,", "citeRegEx": "44", "shortCiteRegEx": "44", "year": 2004}, {"title": "F", "author": ["M. Lei", "Y.-J. Wu"], "venue": "K. Soong, Z.-H. Ling, L.-R. Dai, A hierarchical f0 modeling method for hmm-based speech synthesis., in: INTERSPEECH", "citeRegEx": "45", "shortCiteRegEx": null, "year": 2010}, {"title": "N", "author": ["H. Zen"], "venue": "Braunschweiler, Context-dependent additive log f 0 model for hmm-based speech synthesis., in: INTERSPEECH", "citeRegEx": "46", "shortCiteRegEx": null, "year": 2009}, {"title": "Pitch accent in context: Predicting intonational prominence from text", "author": ["J. Hirschberg"], "venue": "Artificial Intelligence 63 (1-2) ", "citeRegEx": "47", "shortCiteRegEx": null, "year": 1993}, {"title": "An introduction to text-to-speech synthesis", "author": ["T. Dutoit"], "venue": "Vol. 3, Springer", "citeRegEx": "48", "shortCiteRegEx": null, "year": 1997}, {"title": "Text-to-speech synthesis", "author": ["P. Taylor"], "venue": "Cambridge University Press", "citeRegEx": "49", "shortCiteRegEx": null, "year": 2009}, {"title": "A practical guide to wavelet analysis", "author": ["C. Torrence", "G.P. Compo"], "venue": "Bulletin of the American Meteorological society 79 (1) ", "citeRegEx": "51", "shortCiteRegEx": null, "year": 1998}, {"title": "Voiceless intervals and perceptual completion in f0 contours: Evidence from scaling perception in american english", "author": ["J. Barnes", "A. Brugos", "N. Veilleux", "S. Shattuck-Hufnagel"], "venue": "Proc. 16th ICPhS, Hong Kong, China ", "citeRegEx": "52", "shortCiteRegEx": null, "year": 2011}, {"title": "Hmm-based speech synthesis utilizing glottal inverse filtering", "author": ["T. Raitio", "A. Suni", "J. Yamagishi", "H. Pulakka", "J. Nurminen", "M. Vainio", "P. Alku"], "venue": "Audio, Speech, and Language Processing, IEEE Transactions on 19 (1) ", "citeRegEx": "53", "shortCiteRegEx": null, "year": 2011}, {"title": "A maximal theorem with function-theoretic applications", "author": ["G.H. Hardy", "J.E. Littlewood"], "venue": "Acta Mathematica 54 (1) ", "citeRegEx": "54", "shortCiteRegEx": null, "year": 1930}, {"title": "Geometry of sets and measures in Euclidean spaces: fractals and rectifiability", "author": ["P. Mattila"], "venue": "no. 44, Cambridge University Press", "citeRegEx": "55", "shortCiteRegEx": null, "year": 1999}, {"title": "K", "author": ["H. Zen", "T. Nose", "J. Yamagishi", "S. Sako", "T. Masuko", "A. Black"], "venue": "Tokuda, The hmm-based speech synthesis system (hts) version 2.0, in: Proc. of Sixth ISCA Workshop on Speech Synthesis", "citeRegEx": "56", "shortCiteRegEx": null, "year": 2007}, {"title": "Combining acoustic", "author": ["S. Ananthakrishnan", "S.S. Narayanan"], "venue": "lexical, and syntactic evidence for automatic unsupervised prosody labeling, in: Proceedings of InterSpeech, Pittsburgh, PA", "citeRegEx": "57", "shortCiteRegEx": null, "year": 2006}, {"title": "Unsupervised prominence prediction for speech synthesis", "author": ["M. Mehrabani", "T. Mishra", "A. Conkie"], "venue": "in: INTERSPEECH 2013, 14th Annual Conference of the International Speech Communication Association, Lyon, France, August 25-29, 2013", "citeRegEx": "58", "shortCiteRegEx": null, "year": 2013}, {"title": "An automatic system for detecting prosodic prominence in american english continuous speech", "author": ["F. Tamburini", "C. Caini"], "venue": "International Journal of Speech Technology 8 (1) ", "citeRegEx": "59", "shortCiteRegEx": null, "year": 2005}, {"title": "Accent and prominence in Finnish speech synthesis", "author": ["M. Vainio", "A. Suni", "P. Sirjola"], "venue": "in: G. Kokkinakis, N. Fakotakis, E. Dermatos, R. Potapova (Eds.), Proceedings of the 10th International Conference on Speech and Computer ", "citeRegEx": "60", "shortCiteRegEx": null, "year": 2005}, {"title": "A saliency-based auditory attention model with applications to unsupervised prominent syllable detection in speech", "author": ["O. Kalinli", "S.S. Narayanan"], "venue": "in: INTERSPEECH 2007, 8th Annual Conference of the International Speech Communication Association, Antwerp, Belgium, August 27-31, 2007", "citeRegEx": "61", "shortCiteRegEx": null, "year": 2007}, {"title": "Automatic detection and classification of prosodic events", "author": ["A. Rosenberg"], "venue": "Ph.D. thesis, Columbia University ", "citeRegEx": "62", "shortCiteRegEx": null, "year": 2009}, {"title": "The boston university radio news corpus", "author": ["P.S.-H.S. Ostendorf", "M. Price"], "venue": "Tech. rep. ", "citeRegEx": "63", "shortCiteRegEx": null, "year": 2005}, {"title": "J", "author": ["K.E.A. Silverman", "M.E. Beckman", "J.F. Pitrelli", "M. Ostendorf", "C.W. Wightman", "P. Price", "J.B. Pierrehumbert"], "venue": "Hirschberg, Tobi: a standard for labeling english prosody., in: ICSLP, ISCA", "citeRegEx": "64", "shortCiteRegEx": null, "year": 1992}, {"title": "Automatic prosodic event detection using acoustic", "author": ["S. Ananthakrishnan", "S. Narayanan"], "venue": "lexical, and syntactic evidence, Audio, Speech, and Language Processing, IEEE Transactions on 16 (1) ", "citeRegEx": "65", "shortCiteRegEx": null, "year": 2008}, {"title": "IEEE Transactions on Audio", "author": ["O. Kalinli", "S.S. Narayanan"], "venue": "Speech and Language Processing 17 (5) ", "citeRegEx": "66", "shortCiteRegEx": null, "year": 2009}, {"title": "Detecting pitch accents at the word", "author": ["A. Rosenberg", "J. Hirschberg"], "venue": "syllable and vowel level, in: Proceedings of Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the Association for Computational Linguistics, Companion Volume: Short Papers, NAACL-Short \u201909, Association for Computational Linguistics", "citeRegEx": "67", "shortCiteRegEx": null, "year": 2009}, {"title": "Loudness predicts prominence: Fundamental frequency lends little", "author": ["G. Kochanski", "E. Grabe", "J. Coleman", "B. Rosner"], "venue": "The Journal of the Acoustical Society of America 118 (2) ", "citeRegEx": "68", "shortCiteRegEx": null, "year": 2005}], "referenceMentions": [{"referenceID": 0, "context": "The assumption of hierarchical structure combined with new deep learning algorithms has lead to recent breakthroughs in automatic speech recognition [2].", "startOffset": 149, "endOffset": 152}, {"referenceID": 1, "context": "The prosodic hierarchy has been central in TTS since 1970\u2019s [3, 4] and most current systems are based on some kind of a hierarchical utterance structure.", "startOffset": 60, "endOffset": 66}, {"referenceID": 2, "context": "The prosodic hierarchy has been central in TTS since 1970\u2019s [3, 4] and most current systems are based on some kind of a hierarchical utterance structure.", "startOffset": 60, "endOffset": 66}, {"referenceID": 3, "context": "Few systems go above single utterances (which typically represent sentence in written form), but some take the paragraph sized units as a basis of production [5].", "startOffset": 158, "endOffset": 161}, {"referenceID": 4, "context": "The phonologically based ones stem from the so called Autosegmental Metrical theory [6] which is based on the three-dimensional phonology developed in [7, 8] as noted in [9].", "startOffset": 84, "endOffset": 87}, {"referenceID": 5, "context": "The phonologically based ones stem from the so called Autosegmental Metrical theory [6] which is based on the three-dimensional phonology developed in [7, 8] as noted in [9].", "startOffset": 151, "endOffset": 157}, {"referenceID": 6, "context": "The phonologically based ones stem from the so called Autosegmental Metrical theory [6] which is based on the three-dimensional phonology developed in [7, 8] as noted in [9].", "startOffset": 170, "endOffset": 173}, {"referenceID": 7, "context": "Actual models capturing the superpositional nature of intonation were first proposed in [10] by \u00d6hman, whose model was further developed by Fujisaki [11, 12] as a so called command-response model which assumes two separate types of articulatory commands; accents associated with stressed syllables superposed on phrases with their own commands.", "startOffset": 88, "endOffset": 92}, {"referenceID": 8, "context": "Actual models capturing the superpositional nature of intonation were first proposed in [10] by \u00d6hman, whose model was further developed by Fujisaki [11, 12] as a so called command-response model which assumes two separate types of articulatory commands; accents associated with stressed syllables superposed on phrases with their own commands.", "startOffset": 149, "endOffset": 157}, {"referenceID": 9, "context": "Actual models capturing the superpositional nature of intonation were first proposed in [10] by \u00d6hman, whose model was further developed by Fujisaki [11, 12] as a so called command-response model which assumes two separate types of articulatory commands; accents associated with stressed syllables superposed on phrases with their own commands.", "startOffset": 149, "endOffset": 157}, {"referenceID": 10, "context": "Several superpositional models with a varying degree of levels have been proposed since Fujisaki [13, 14, 15, 16].", "startOffset": 97, "endOffset": 113}, {"referenceID": 11, "context": "Several superpositional models with a varying degree of levels have been proposed since Fujisaki [13, 14, 15, 16].", "startOffset": 97, "endOffset": 113}, {"referenceID": 12, "context": "Several superpositional models with a varying degree of levels have been proposed since Fujisaki [13, 14, 15, 16].", "startOffset": 97, "endOffset": 113}, {"referenceID": 13, "context": "Several superpositional models with a varying degree of levels have been proposed since Fujisaki [13, 14, 15, 16].", "startOffset": 97, "endOffset": 113}, {"referenceID": 14, "context": "within a word stand out as stressed [17].", "startOffset": 36, "endOffset": 40}, {"referenceID": 15, "context": "These four categories are fairly easily and consistently labeled even by non-expert listeners [18, 19, 20].", "startOffset": 94, "endOffset": 106}, {"referenceID": 16, "context": "These four categories are fairly easily and consistently labeled even by non-expert listeners [18, 19, 20].", "startOffset": 94, "endOffset": 106}, {"referenceID": 17, "context": "These four categories are fairly easily and consistently labeled even by non-expert listeners [18, 19, 20].", "startOffset": 94, "endOffset": 106}, {"referenceID": 18, "context": "Word prominence has been shown to work well in TTS for a number of languages, even for English which has been characterised as a so called intonation language [21, 22, 23, 24, 25].", "startOffset": 159, "endOffset": 179}, {"referenceID": 19, "context": "Word prominence has been shown to work well in TTS for a number of languages, even for English which has been characterised as a so called intonation language [21, 22, 23, 24, 25].", "startOffset": 159, "endOffset": 179}, {"referenceID": 20, "context": "Word prominence has been shown to work well in TTS for a number of languages, even for English which has been characterised as a so called intonation language [21, 22, 23, 24, 25].", "startOffset": 159, "endOffset": 179}, {"referenceID": 21, "context": "Word prominence has been shown to work well in TTS for a number of languages, even for English which has been characterised as a so called intonation language [21, 22, 23, 24, 25].", "startOffset": 159, "endOffset": 179}, {"referenceID": 22, "context": "Word prominence has been shown to work well in TTS for a number of languages, even for English which has been characterised as a so called intonation language [21, 22, 23, 24, 25].", "startOffset": 159, "endOffset": 179}, {"referenceID": 23, "context": "The perceived prominence of a given word in an utterance is a product of many separate sources of information; mostly signal based although other linguistic factors can modulate the perception [26, 19].", "startOffset": 193, "endOffset": 201}, {"referenceID": 16, "context": "The perceived prominence of a given word in an utterance is a product of many separate sources of information; mostly signal based although other linguistic factors can modulate the perception [26, 19].", "startOffset": 193, "endOffset": 201}, {"referenceID": 24, "context": "In what follows we present recently developed methods for automatic prominence estimation based on CWT (Section 2) which allow for fully automatic and unsupervised means to estimate both (word) prominences and boundary values from a hierarchical representation of speech (see [27, 28, 29] for earlier work).", "startOffset": 276, "endOffset": 288}, {"referenceID": 25, "context": "In what follows we present recently developed methods for automatic prominence estimation based on CWT (Section 2) which allow for fully automatic and unsupervised means to estimate both (word) prominences and boundary values from a hierarchical representation of speech (see [27, 28, 29] for earlier work).", "startOffset": 276, "endOffset": 288}, {"referenceID": 26, "context": "In what follows we present recently developed methods for automatic prominence estimation based on CWT (Section 2) which allow for fully automatic and unsupervised means to estimate both (word) prominences and boundary values from a hierarchical representation of speech (see [27, 28, 29] for earlier work).", "startOffset": 276, "endOffset": 288}, {"referenceID": 27, "context": "Wavelets are used in a great variety of applications for effectively compressing and denoising signals, to represent the hierarchical properties of multidimensional signals like polychromatic visual patterns in image retrieval, and to model optical signal processing of visual neural fields [30, 31].", "startOffset": 291, "endOffset": 299}, {"referenceID": 28, "context": "Wavelets are used in a great variety of applications for effectively compressing and denoising signals, to represent the hierarchical properties of multidimensional signals like polychromatic visual patterns in image retrieval, and to model optical signal processing of visual neural fields [30, 31].", "startOffset": 291, "endOffset": 299}, {"referenceID": 29, "context": "In speech and auditory research there is also a long history going back to the 1970\u2019s [32, 33, 34, 35, 36, 37, 38].", "startOffset": 86, "endOffset": 114}, {"referenceID": 30, "context": "In speech and auditory research there is also a long history going back to the 1970\u2019s [32, 33, 34, 35, 36, 37, 38].", "startOffset": 86, "endOffset": 114}, {"referenceID": 31, "context": "In speech and auditory research there is also a long history going back to the 1970\u2019s [32, 33, 34, 35, 36, 37, 38].", "startOffset": 86, "endOffset": 114}, {"referenceID": 32, "context": "In speech and auditory research there is also a long history going back to the 1970\u2019s [32, 33, 34, 35, 36, 37, 38].", "startOffset": 86, "endOffset": 114}, {"referenceID": 33, "context": "In speech and auditory research there is also a long history going back to the 1970\u2019s [32, 33, 34, 35, 36, 37, 38].", "startOffset": 86, "endOffset": 114}, {"referenceID": 34, "context": "In speech and auditory research there is also a long history going back to the 1970\u2019s [32, 33, 34, 35, 36, 37, 38].", "startOffset": 86, "endOffset": 114}, {"referenceID": 35, "context": "A recent summary of wavelets in speech technology can be found in [39].", "startOffset": 66, "endOffset": 70}, {"referenceID": 24, "context": "[27] and Vainio et al.", "startOffset": 0, "endOffset": 4}, {"referenceID": 25, "context": "[28] used amplitude of the word prosody scale which was chosen from a discrete set of scales with ratio 2 between ascending scales as the one with the number of local maxima as close to the number of words in the corpus as possible.", "startOffset": 0, "endOffset": 4}, {"referenceID": 26, "context": "presented in [29] where the lines of maximum amplitude (LoMA) in the wavelet image were used [40, 36, 41].", "startOffset": 13, "endOffset": 17}, {"referenceID": 36, "context": "presented in [29] where the lines of maximum amplitude (LoMA) in the wavelet image were used [40, 36, 41].", "startOffset": 93, "endOffset": 105}, {"referenceID": 32, "context": "presented in [29] where the lines of maximum amplitude (LoMA) in the wavelet image were used [40, 36, 41].", "startOffset": 93, "endOffset": 105}, {"referenceID": 37, "context": "presented in [29] where the lines of maximum amplitude (LoMA) in the wavelet image were used [40, 36, 41].", "startOffset": 93, "endOffset": 105}, {"referenceID": 38, "context": "In earlier work, wavelets have been used in speech synthesis context mainly for parameter estimation [42, 43, 44] but never as a full modelling paradigm.", "startOffset": 101, "endOffset": 113}, {"referenceID": 39, "context": "In earlier work, wavelets have been used in speech synthesis context mainly for parameter estimation [42, 43, 44] but never as a full modelling paradigm.", "startOffset": 101, "endOffset": 113}, {"referenceID": 40, "context": "In the HMM based synthesis framework, decomposition of f0 to its explicit hierarchical components during acoustic modelling has been investigated in [45, 46].", "startOffset": 149, "endOffset": 157}, {"referenceID": 41, "context": "In the HMM based synthesis framework, decomposition of f0 to its explicit hierarchical components during acoustic modelling has been investigated in [45, 46].", "startOffset": 149, "endOffset": 157}, {"referenceID": 40, "context": "The layers can then be modelled separately as individual streams [45], or jointly with adaptive training methods [46].", "startOffset": 65, "endOffset": 69}, {"referenceID": 41, "context": "The layers can then be modelled separately as individual streams [45], or jointly with adaptive training methods [46].", "startOffset": 113, "endOffset": 117}, {"referenceID": 42, "context": "This brings the labelling system closer to the traditional tone-sequence models which have been widely used \u2013 with varying rates of success \u2013 in English TTS [47, 48, 49].", "startOffset": 157, "endOffset": 169}, {"referenceID": 43, "context": "This brings the labelling system closer to the traditional tone-sequence models which have been widely used \u2013 with varying rates of success \u2013 in English TTS [47, 48, 49].", "startOffset": 157, "endOffset": 169}, {"referenceID": 44, "context": "This brings the labelling system closer to the traditional tone-sequence models which have been widely used \u2013 with varying rates of success \u2013 in English TTS [47, 48, 49].", "startOffset": 157, "endOffset": 169}, {"referenceID": 45, "context": "The continuous theory is explained in detail by Daubechies and the theory is applied to time series as by Torrence and Compo [50, 51].", "startOffset": 125, "endOffset": 133}, {"referenceID": 46, "context": "closures in the vocal tract during stops, create strong hierarchical structures in the wavelet image that might not be part of the auditory gestalt [52].", "startOffset": 148, "endOffset": 152}, {"referenceID": 47, "context": "In practice, the voicedness of a time point is defined using the GlottHMM [53] analysis which applies low-frequency energy and zero-crossings thresholds for voicing decision.", "startOffset": 74, "endOffset": 78}, {"referenceID": 48, "context": "Instead, every point converges and the resulting (maximal) function has comparable energy to the original which can be seen by iterating a result of Hardy and Littlewood [54], (for modern approach, see Theorem 2.", "startOffset": 170, "endOffset": 174}, {"referenceID": 49, "context": "19 in [55]).", "startOffset": 6, "endOffset": 10}, {"referenceID": 50, "context": "As stated in the introduction, a solid method for prosody annotation would be very welcome in speech synthesis field, where recent development has concentrated on acoustic modelling side [56].", "startOffset": 187, "endOffset": 191}, {"referenceID": 25, "context": "Although this hierarchical method does lend itself naturally to multi-level prosody annotation [28], here, we restrict ourselves to binary detection task, in order to produce comparable results with previous studies.", "startOffset": 95, "endOffset": 99}, {"referenceID": 51, "context": "For example, Ananthakrishnan & Narayanan [57] performed twoclass unsupervised clustering on syllable level acoustic features combined with lexical and syntactic features, achieving accent detection accuracy of 78% using Boston University Radio News Corpus (BURNC).", "startOffset": 41, "endOffset": 45}, {"referenceID": 52, "context": "[58] annotated a corpus with four level prominence scale by K-means clustering on foot-level acoustic features, achieving improved synthesis quality compared to a rule-based prominence model.", "startOffset": 0, "endOffset": 4}, {"referenceID": 53, "context": "Tambourini [59] derived a continuous prominence function, using expert knowledge to weight various acoustic correlates of prominence, achieving 80% accuracy on syllable prominence detection on TIMIT corpus.", "startOffset": 11, "endOffset": 15}, {"referenceID": 54, "context": "Word prominence was annotated by Vainio & Suni [60] with similar method, using prosodic features generated by parametric synthesis build without prominence labels as a powerful normalizing method.", "startOffset": 47, "endOffset": 51}, {"referenceID": 55, "context": "An ambitious approach was presented by Kalinli & Narayanan [61], extracting multi-scale auditory features insipired on the processing stages in the human auditory system, combined to an auditory salience map.", "startOffset": 59, "endOffset": 63}, {"referenceID": 56, "context": "For example in BURNC, intonational phrase boundaries can be predicted by silence alone with 88% accuracy, though with only 45% recall, and traditional acoustic features offer little improvent over this trivial baseline [62].", "startOffset": 219, "endOffset": 223}, {"referenceID": 51, "context": "In terms of combining text and acoustic evidence, Ananthakrishnan & Narayanan [57] obtained 81% accuracy in combined intermediate and intonational boundary detection with two class k-means model.", "startOffset": 78, "endOffset": 82}, {"referenceID": 57, "context": "We perform the evaluation of our prominence and boundary detection method on Boston Radio News corpus [63], chosen for high quality prosodic labeling and comparability with several previous methods also evaluated on BURNC.", "startOffset": 102, "endOffset": 106}, {"referenceID": 58, "context": "The ToBi labelling scheme was originally developed for transcribing speech melody [64], thus high (H), low (L) and complex accent types are employed (H*, L*, L*+H, L+H*, H+ !H*), concerned with syllable level shape and peak alignment.", "startOffset": 82, "endOffset": 86}, {"referenceID": 59, "context": "These binary boundary and prominence categories are consistent with previous prosodic event detection studies [65, 66].", "startOffset": 110, "endOffset": 118}, {"referenceID": 60, "context": "These binary boundary and prominence categories are consistent with previous prosodic event detection studies [65, 66].", "startOffset": 110, "endOffset": 118}, {"referenceID": 47, "context": "Raw f0 and energy parameters were analyzed from 16 kHz speech signals with GlottHMM analysis-synthesis framework [53] with five millisecond frame shift.", "startOffset": 113, "endOffset": 117}, {"referenceID": 55, "context": "1Kalinli & Narayanan [61],2Rosenberg & Hirchberg [67],3Ananthakrishnan & al.", "startOffset": 21, "endOffset": 25}, {"referenceID": 61, "context": "1Kalinli & Narayanan [61],2Rosenberg & Hirchberg [67],3Ananthakrishnan & al.", "startOffset": 49, "endOffset": 53}, {"referenceID": 51, "context": "[57],4Ananthakrishnan & al.", "startOffset": 0, "endOffset": 4}, {"referenceID": 59, "context": "[65].", "startOffset": 0, "endOffset": 4}, {"referenceID": 59, "context": "The results are not far from performance of supervised methods using acoustic, lexical, and syntactic evidence, where reported accuracies for both word level prominence and boundary detection range from 84% to 87% [65, 66].", "startOffset": 214, "endOffset": 222}, {"referenceID": 60, "context": "The results are not far from performance of supervised methods using acoustic, lexical, and syntactic evidence, where reported accuracies for both word level prominence and boundary detection range from 84% to 87% [65, 66].", "startOffset": 214, "endOffset": 222}, {"referenceID": 62, "context": "The role of signal energy as a reliable determinant of prosodic structure is interesting, but not altogether surprising [68].", "startOffset": 120, "endOffset": 124}, {"referenceID": 28, "context": "would be a truly multidimensional representation of speech signal similar to the multi-scale visual analyses [31].", "startOffset": 109, "endOffset": 113}], "year": 2015, "abstractText": "Prominences and boundaries are the essential constituents of prosodic structure in speech. They provide for means to chunk the speech stream into linguistically relevant units by providing them with relative saliences and demarcating them within coherent utterance structures. Prominences and boundaries have both been widely used in both basic research on prosody as well as in textto-speech synthesis. However, there are no representation schemes that would provide for both estimating and modelling them in a unified fashion. Here we present an unsupervised unified account for estimating and representing prosodic prominences and boundaries using a scale-space analysis based on continuous wavelet transform. The methods are evaluated and compared to earlier work using the Boston University Radio News corpus. The results show that the proposed method is comparable with the best published supervised annotation methods.", "creator": "LaTeX with hyperref package"}}}