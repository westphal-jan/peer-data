{"id": "1510.02049", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "7-Oct-2015", "title": "Assisting Composition of Email Responses: a Topic Prediction Approach", "abstract": "We propose an approach for helping agents compose email replies to customer requests. To enable that, we use LDA to extract latent topics from a collection of email exchanges. We then use these latent topics to label our data, obtaining a so-called \"silver standard\" topic labelling. We exploit this labelled set to train a classifier to: (i) predict the topic distribution of the entire agent's email response, based on features of the customer's email; and (ii) predict the topic distribution of the next sentence in the agent's reply, based on the customer's email features and on features of the agent's current sentence. The experimental results on a large email collection from a contact center in the tele- com domain show that the proposed ap- proach is effective in predicting the best topic of the agent's next sentence. In 80% of the cases, the correct topic is present among the top five recommended topics (out of fifty possible ones). This shows the potential of this method to be applied in an interactive setting, where the agent is presented a small list of likely topics to choose from for the next sentence.", "histories": [["v1", "Wed, 7 Oct 2015 18:08:45 GMT  (99kb,D)", "http://arxiv.org/abs/1510.02049v1", "8 pages, 5 figures"]], "COMMENTS": "8 pages, 5 figures", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["spandana gella", "marc dymetman", "jean michel renders", "sriram venkatapathy"], "accepted": false, "id": "1510.02049"}, "pdf": {"name": "1510.02049.pdf", "metadata": {"source": "CRF", "title": "Assisting Composition of Email Responses: a Topic Prediction Approach", "authors": ["Spandana Gella", "Marc Dymetman", "Jean Michel Renders", "Sriram Venkatapathy"], "emails": ["S.Gella@ed.ac.uk", "Marc.Dymetman@xrce.xerox.com", "Jean-Michel.Renders@xrce.xerox.com", "vesriram@amazon.com"], "sections": [{"heading": null, "text": "The experimental results of a large email collection from a contact centre in the telecommunications domain show that the proposed approach is effective in predicting the best topic of the agent's next sentence. In 80% of cases, the right topic is present among the five best recommended topics (out of fifty possible), demonstrating the potential of this method to be applied in an interactive environment where the agent is presented with a small list of likely topics from which to choose for the next sentence."}, {"heading": "1 Introduction", "text": "The focus of the work is on developing models that help a person answer an email request. This is very relevant to the situation of customers, who often have to respond to similar requests from other customers. Of course, these questions are similar to the answers from customers who have similar structures and vocabulary."}, {"heading": "2 Related Work", "text": "Most of the work devoted to \"e-mail text analysis\" in recent years has focused on the classification and summary of e-mail messages. E-mail classification has proven useful in many standard applications such as spam detection and filtering of high-priority messages. Research topics such as summarizing and answering questions have come to the fore because of the need to better interpret the overwhelming amount of e-mail / messages generated with the advent of e-mail groups and discussion forums in 1976. One of the earlier contributions to the e-mail summary was the work of (Muresan et al., 2001), whereas (Rambow et al., 2004) it was expanded to e-mail threads; Scheffer et al. (2004) on the other hand, semi-supervised classification techniques for answering questions in the context of such theses have been proposed. Some of the most recent classification and summary techniques \"are based on reactions, such as\" on a 2011 meeting, \"or\" the \"based on a set of reactions.\""}, {"heading": "3 Dataset", "text": "The data set we used for our study is a collection of e-mails from the technical support team of a major telecommunications company in the UK. Typically, an e-mail thread is started by a customer reporting a problem or seeking information, followed by the response of an agent suggesting solutions or asking for further details to fix the reported problem. These threads continue until the problem is resolved or the customer is satisfied with the agent's response. An example of an e-mail conversation between a customer and an agent can be found in the left column of Table 3. Usually, customer e-mails are in free form, while the agent's responses have a moderately systematic structure. On average, there are 8 e-mails in one thread. In our study, we looked at only the first two e-mails in one thread, namely the query of the original customer and the response of the corresponding agent. We have limited our experiments to e-mails that use at least 10 words for customer e-mails and 20 words for agent responses, resulting in an average of 8-38.1% of e-mails."}, {"heading": "4 Extracting Topics and Building a \u201cSilver Standard\u201d based on LDA", "text": "As we explained, we focus on two main tasks: \u2022 Task T1: predicting the likely overall topics of the overall agent's response based on knowledge of the customer's email; \u2022 Task T2: when an agent writes an email, he will predict the likely topics of the next sentence based on the original query and the additional knowledge of the previous sentences. However, our training data is not commented on at the level of the topics. To synthesize these topics, we use a popular, uncontrolled technique - Latent Dirichlet Allocation (Blei et al., 2003) - for modeling the topic space of different views of the collection. There are potentially three ways to extract topics in our series of conversations. In the first setting, we keep customers and agent emails separate, identifying different top models for each collection: one document is a customer email in the first collection, and an agent email in the second collection."}, {"heading": "5 Influence of the Customer Query on the word-level perplexity", "text": "We study the influence of knowledge of the context of the client's query on the content of the agent's e-mail. To do this, we look at the amount of e-mails from the test agent and compare the perplexity of the language model based on MMA with that identified with MMCA. Remember that the MMA model influences the probability distribution (\u03c4MA (Ai)) over the topics by evaluating only the agent's e-mails from the training set, while the MMCA model mediates a probability distribution (\u03c4MCA (Ai)) over the topics by evaluating both the client queries and the agent's responses in the training set. Perplexity values are calculated using the following formulas: Perplexity (A | MMA) = exp (\u0445di = 1 logL (Ai) \u0445d = 1NAi) = Perplexity (A | C, MMCA) is relevant."}, {"heading": "6 Predicting Relevant Topics of the Agent Response", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "6.1 Topic prediction for the overall agent\u2019s email", "text": "In this section, we will focus on T1, namely predicting the distribution of the agent response using only context-dependent information: the customer query Ci and its distribution \u03c4MCA (Ci).The choice of the MMCA model instead of the MNC is motivated by the considerations described in the previous section (Section 5).Note that the MMCA model is used both to calculate synthetic semantic characteristics (\u03c4MCA (Ci)) and to provide a silver standard for predicting the topic (\u03c4MCA (Ai).The predictor can be written as follows: \u03c4 M (Ai) = f (Ci), \u03c4 M CA (Ci))) (2), where \u03c9 (Ci) represents the vocabulary of the customer query."}, {"heading": "6.2 Topic prediction for each sentence of the agent\u2019s email", "text": "In order to solve our second task (task T2), which is to predict the distribution of the topics of the next sentence of an agent's answer, we use the words of the customer request Ci, its distribution of topics \u03c4MCA (Ci), the words of the current sentence and the distribution of topics of the current sentence \u03c4MS (Ai, j). Note that we are making a kind of Markovian assumption for the agent-side content: We believe that the current sentence and its distribution of topics are sufficient to predict the topic of the next sentence, since the context of the customer request is given. Note Ai, j the last sentence in the agent email Ai, then build up the predictor as follows: \u0432 \u0432 MS (Ai, j + 1) = f (Ci), \u0432 M (Ci),."}, {"heading": "7 Evaluation", "text": "In this section, we present experimental results showing how our proposed methods perform in predicting subjects of agent response. To learn the LDA theme models (as described in Section 4), we have used the MALLET toolkit (McCallum, 2002) with the default setting (default setting). We evaluate our methods using three metrics: 1. Bhattacharya coefficient (Bhattacharya, 1943) Here, we evaluate how close the predicted topic distribution is to the silver standard topic distribution. For T1, we compare for each sentence (j + 1) of agent e-mail (Ai) with \u03c4MCA (Ai) for each agent e-mail Ai of the test set. For T2, we compare Hate-MS (Ai, j + 1) with Hate-M topics (Ai, j + 1) for each sentence (j + 1) of agent e-mail (Ai) with test MCA (AI)."}, {"heading": "7.1 Topic prediction of agent email", "text": "In Figure 3, we present the average Bhattacharya coefficient across all test e-mails for different themes (M) using the MMCA.1 This figure illustrates the trade-off between the difficulty of the task and the usefulness of the model: A higher number of themes equals a multiples1. For our purposes, the Bhattacharya coefficient is preferable to other measures of distribution distance, such as climatic divergence. At the same time, it is more difficult to achieve a certain level of performance (measured by the Bhattacharya coefficient) than with a small number of themes. For comparison, we also show a baseline where predicting the agent's e-mail topic distribution is simply a copy of the e-mail topic distribution, with a much lower performance. Figure 4 indicates the evolution of the average number of themes."}, {"heading": "7.2 Topic Prediction of Next Sentence", "text": "We compare the predictive accuracy of the proposed approach with other baseline approaches. The baseline approaches we have examined are: \u2022 Uniform: we assign a uniform distribution of topics for each set in the test set and compare it with the Silver Standard. In other words, we perform a completely random ranking of topics. \u2022 Average: we assign the same topic distribution for each set in the test set; this topic distribution is the global average topic distribution and can be derived directly from the hyper-parameter topic \u03b1 in LDA models. \"The values of the \u03b1 vector (note that this hyper parameter is learned from the training data).Table 4 gives the prevailing topic prediction accuracy for the\" next topic prediction \"task with K = 1 (i.e. the relative number of times in which the dominant topic corresponds to the predicted dominant topic)."}, {"heading": "8 Conclusion and Future Work", "text": "We have presented new, unsupervised models for discovering the discourse structures of e-mail responses in customer-oriented e-mail systems and evaluated their predictive power based on a real contact center e-mail dataset on a global and local level. However, our experiments suggest the potential of these techniques for an interactive scenario in which the agent is guided in selecting whole e-mails or individual sentences based on predicted topics. Nevertheless, many interesting ways could be explored further. A natural extension of our work would be to consider multi-dimensional LDA models in the sense of (Paul and Dredze, 2013), which are able to identify topics along various semantic aspects that would be very useful in unraveling several dimensions that we do not currently distinguish, such as: which topic is discussed, what the device in question is, what stage of a conversation we are in, and so on. Another extension would be the investigation of pre-models that we anticipate subject-dependent on the assumption of a longer distance (the marginal assumption of a problem)."}], "references": [{"title": "On a measure of divergence between two statistical populations defined by their probability distributions", "author": ["Anil Bhattacharya"], "venue": "Bulletin of the Calcutta Mathematical Society,", "citeRegEx": "Bhattacharya.,? \\Q1943\\E", "shortCiteRegEx": "Bhattacharya.", "year": 1943}, {"title": "Latent dirichlet allocation", "author": ["Blei et al.2003] David M. Blei", "Andrew Y. Ng", "Michael I. Jordan"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Blei et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Blei et al\\.", "year": 2003}, {"title": "The semantics of dialogue acts", "author": ["Harry Bunt"], "venue": "In Proceedings of the Ninth International Conference on Computational Semantics,", "citeRegEx": "Bunt.,? \\Q2011\\E", "shortCiteRegEx": "Bunt.", "year": 2011}, {"title": "Two-stage stochastic natural language generation for email synthesis by modeling sender style and topic structure", "author": ["Chen", "Rudnicky2014] Yun-Nung Chen", "Alexander Rudnicky"], "venue": "In Proceedings of the 8th International Natural", "citeRegEx": "Chen et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Chen et al\\.", "year": 2014}, {"title": "Learning to classify email into \u201cSpeech Acts", "author": ["Vitor R. Carvalho", "Tom M. Mitchell"], "venue": "In EMNLP,", "citeRegEx": "Cohen et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Cohen et al\\.", "year": 2004}, {"title": "Emotion detection in email customer care", "author": ["Gupta et al.2013] Narendra Gupta", "Mazin Gilbert", "Giuseppe Di Fabbrizio"], "venue": "Computational Intelligence,", "citeRegEx": "Gupta et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Gupta et al\\.", "year": 2013}, {"title": "Textual reuse for email response", "author": ["Lamontagne", "Lapalme2004] Luc Lamontagne", "Guy Lapalme"], "venue": "In Advances in Case-Based Reasoning,", "citeRegEx": "Lamontagne et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Lamontagne et al\\.", "year": 2004}, {"title": "Mallet: A machine learning for language toolkit. http://mallet.cs.umass.edu", "author": ["Andrew Kachites McCallum"], "venue": null, "citeRegEx": "McCallum.,? \\Q2002\\E", "shortCiteRegEx": "McCallum.", "year": 2002}, {"title": "Combining linguistic and machine learning techniques for email summarization", "author": ["Evelyne Tzoukermann", "Judith L Klavans"], "venue": "In Proceedings of the 2001 workshop on Computational Natural Lan-", "citeRegEx": "Muresan et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Muresan et al\\.", "year": 2001}, {"title": "Stochastic language generation for spoken dialogue systems", "author": ["Oh", "Rudnicky2002] Alice H. Oh", "Alexander I. Rudnicky"], "venue": "Computer Speech and Language,", "citeRegEx": "Oh et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Oh et al\\.", "year": 2002}, {"title": "Extractive summarization and dialogue act modeling on email threads: An integrated probabilistic approach", "author": ["Oya", "Carenini2014] Tatsuro Oya", "Giuseppe Carenini"], "venue": "In Proceedings of the 15th Annual Meeting of the Special Interest Group", "citeRegEx": "Oya et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Oya et al\\.", "year": 2014}, {"title": "Drug extraction from the web: Summarizing drug experiences with multi-dimensional topic models", "author": ["Paul", "Dredze2013] J. Michael Paul", "Mark Dredze"], "venue": "In Proceedings of the 2013 Conference of the North American Chapter of the Associ-", "citeRegEx": "Paul et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Paul et al\\.", "year": 2013}, {"title": "Summarizing email threads", "author": ["Rambow et al.2004] Owen Rambow", "Lokesh Shrestha", "John Chen", "Chirsty Lauridsen"], "venue": "In Proceedings of HLT-NAACL 2004: Short Papers,", "citeRegEx": "Rambow et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Rambow et al\\.", "year": 2004}, {"title": "Email answering assistance by semi-supervised text classification", "author": ["Tobias Scheffer"], "venue": "Intelligent Data Analysis,", "citeRegEx": "Scheffer.,? \\Q2004\\E", "shortCiteRegEx": "Scheffer.", "year": 2004}, {"title": "A classification of illocutionary acts", "author": ["John R Searle"], "venue": "Language in society,", "citeRegEx": "Searle.,? \\Q1976\\E", "shortCiteRegEx": "Searle.", "year": 1976}, {"title": "Discovering latent structure in task-oriented dialogues", "author": ["Zhai", "Williams2014] Ke Zhai", "Jason D. Williams"], "venue": "ACL", "citeRegEx": "Zhai et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Zhai et al\\.", "year": 2014}], "referenceMentions": [{"referenceID": 8, "context": "One of the earlier contributions to email summarization was the work by (Muresan et al., 2001) whereas (Rambow et al.", "startOffset": 72, "endOffset": 94}, {"referenceID": 12, "context": ", 2001) whereas (Rambow et al., 2004) extended it to email threads; Scheffer et al.", "startOffset": 16, "endOffset": 37}, {"referenceID": 14, "context": "Some of the recent classification and summarization techniques have been based on\u201cspeech acts\u201d or \u201cdialog acts\u201d such as proposing a meeting, requesting information (Searle, 1976; Bunt, 2011).", "startOffset": 164, "endOffset": 190}, {"referenceID": 2, "context": "Some of the recent classification and summarization techniques have been based on\u201cspeech acts\u201d or \u201cdialog acts\u201d such as proposing a meeting, requesting information (Searle, 1976; Bunt, 2011).", "startOffset": 164, "endOffset": 190}, {"referenceID": 4, "context": "Several email studies including summarizing of email threads (Oya and Carenini, 2014) or classification of emails (Cohen et al., 2004) involve dialog-act based analysis.", "startOffset": 114, "endOffset": 134}, {"referenceID": 5, "context": "Some of these works include identification of emotional emails related to customer dissatisfaction/frustration (Gupta et al., 2013), as well as learning possible patterns/phrases for textual re-use in email responses (Lamontagne and Lapalme, 2004).", "startOffset": 111, "endOffset": 131}, {"referenceID": 5, "context": "One of the earlier contributions to email summarization was the work by (Muresan et al., 2001) whereas (Rambow et al., 2004) extended it to email threads; Scheffer et al. (2004) on the other hand proposed semi-supervised classification techniques for question answering in the context of such threads.", "startOffset": 73, "endOffset": 178}, {"referenceID": 1, "context": "In order to synthesize such an annotation, we use a popular unsupervised technique \u2013 Latent Dirichlet Allocation (Blei et al., 2003) \u2013 for modeling the topic space of various views of the collection.", "startOffset": 113, "endOffset": 132}, {"referenceID": 7, "context": "For learning the LDA topic models (as described in section 4), we have used MALLET (McCallum, 2002) toolkit, with the standard (default) setting.", "startOffset": 83, "endOffset": 99}, {"referenceID": 0, "context": "Bhattacharya coefficient (Bhattacharya, 1943)", "startOffset": 25, "endOffset": 45}], "year": 2015, "abstractText": "We propose an approach for helping agents compose email replies to customer requests. To enable that, we use LDA to extract latent topics from a collection of email exchanges. We then use these latent topics to label our data, obtaining a so-called \u201csilver standard\u201d topic labelling. We exploit this labelled set to train a classifier to: (i) predict the topic distribution of the entire agent\u2019s email response, based on features of the customer\u2019s email; and (ii) predict the topic distribution of the next sentence in the agent\u2019s reply, based on the customer\u2019s email features and on features of the agent\u2019s current sentence. The experimental results on a large email collection from a contact center in the telecom domain show that the proposed approach is effective in predicting the best topic of the agent\u2019s next sentence. In 80% of the cases, the correct topic is present among the top five recommended topics (out of fifty possible ones). This shows the potential of this method to be applied in an interactive setting, where the agent is presented a small list of likely topics to choose from for the next sentence.", "creator": "LaTeX with hyperref package"}}}