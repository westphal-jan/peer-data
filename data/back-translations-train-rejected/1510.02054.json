{"id": "1510.02054", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "7-Oct-2015", "title": "Stochastic Optimization for Deep CCA via Nonlinear Orthogonal Iterations", "abstract": "Deep CCA is a recently proposed deep neural network extension to the traditional canonical correlation analysis (CCA), and has been successful for multi-view representation learning in several domains. However, stochastic optimization of the deep CCA objective is not straightforward, because it does not decouple over training examples. Previous optimizers for deep CCA are either batch-based algorithms or stochastic optimization using large minibatches, which can have high memory consumption. In this paper, we tackle the problem of stochastic optimization for deep CCA with small minibatches, based on an iterative solution to the CCA objective, and show that we can achieve as good performance as previous optimizers and thus alleviate the memory requirement.", "histories": [["v1", "Wed, 7 Oct 2015 18:32:41 GMT  (112kb)", "http://arxiv.org/abs/1510.02054v1", "in 2015 Annual Allerton Conference on Communication, Control and Computing"]], "COMMENTS": "in 2015 Annual Allerton Conference on Communication, Control and Computing", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["weiran wang", "raman arora", "karen livescu", "nathan srebro"], "accepted": false, "id": "1510.02054"}, "pdf": {"name": "1510.02054.pdf", "metadata": {"source": "CRF", "title": "Stochastic Optimization for Deep CCA via Nonlinear Orthogonal Iterations", "authors": ["Weiran Wang"], "emails": ["weiranwang@ttic.edu,", "klivescu@ttic.edu,", "nati@ttic.edu,", "arora@cs.jhu.edu"], "sections": [{"heading": null, "text": "This year, it is only a matter of time before there is an agreement, until there is an agreement."}, {"heading": "II. DEEP CCA", "text": "It is not that we include the data matrices of DNNs with a corresponding set of learnable parameters, e.g. Wf. The dimensionality of the learned characteristics is denoted L.Deep CCA (DCCA) [21] extends (linear) CCA [7] by evaluating dx- and dy-dimensional characteristics with two DNNs and two-dimensional characteristics."}, {"heading": "III. OUR ALGORITHM", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "A. An iterative solution to linear CCA", "text": "Our solution to (1) is from the iterative solution for finding the linear CCA projections (U F, V G) for the input factors (F, G) as shown in Algorithm 1. \u2212 This algorithm calculates the top L vectors (U F, V) of the idea going back to J. Von Neumann. A similar algorithm is also used by [32, Algorithm 1] for large linear CCA inputs, although their algorithms do not implement the whitening operations (AtA t) \u2212 1. At and Bt (BtB t) \u2212 1."}, {"heading": "B. Extension to DCCA", "text": "In fact, it is not so that it is about a way and a way in which it is about the question, to what extent it is about a way and a way in which it is about a way and a way in which it is about a way and a way in which it is about a way and a way in which it is about a way and a way in which it is about a way and a way in which it is about a way and a way in which it is about a way and a way in which it is about a way and a way in which it is about a way and a way in which it is about a way and a way in which it is about a way and a way in which it is about a way and a way in which it is about a way and a way in which it is about a way and a way in which it is about a way and a way in which it is about a way and a way in which it is about a way in which it is about a way and a way in which it is about a way in which it is about a way in which it is about a way and a way in which it is about a way in which it is about a way in which it is about a way and a way in which it is about a way in which it is about a way and a way in which it is about a way in which it is about a way and a way in which it is about a way in which it is about a way in which it is about a way and a way in which it is about a way and a way in which it is about a way in which it is about a way in which it is about a way and a way in which it is about a way in which it is about a way in which it is about a way and a way in which it is about a way in which it is about a way in which it is about a way in which it is about a way and a way in which it is about a way in which it is about a way in which it is about a way in which it is about a way in which it is about a way and a way in which it is about a way in which it is about a way in which it is about a way in which it is about a way and a way in which it is not a way in which it is not a way in which it is about a way in which it is about a way in which it is about a"}, {"heading": "IV. RELATED WORK", "text": "In fact, it is not the case that this would be a way in which people would act in a country in which they are able to live and live in a world in which they are able to move, in which they are able to live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live in which they live, in which they live in which they live, in which they live, in which they live in which they live, in which they live in which they live, in which they live, in which they live in which they live, in which they live, in which they live in which they live, in which they live, in which they live in which they live, in which they live, in which they live, in which they live, in which they live in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they"}, {"heading": "V. EXPERIMENTS", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "A. Experimental setup", "text": "We will now demonstrate the NOI algorithm on the two real-world datasets used by [21] in the introduction of DCCA. The first dataset is a subset of the University of Wisconsin XRay Microbeam Corpus [50], which consists of simultaneously recorded acoustic and articulatory measurements while speaking. Following [21], [22], the acoustic visual inputs are directed to 39D Mel frequency receiver coefficients and the articulatory visual inputs to horizontal / vertical displacement of 8 pellets to different parts of the vocal tract, each linked via a 7-frame context window, for loudspeakers \"JW11.\" The second dataset consists of left / right image halves in the MNIST dataset [51], and so the input of each view consists of 28 x 14 grayscale images."}, {"heading": "B. Effect of minibatch size n", "text": "In the first group of experiments, we vary the minibatch size n of NOI by {10, 20, 50, 100}, while the learning curves (objective value versus number of epochs) on the tuning set for each n with the corresponding optimal hyperparameters in Fig. 2. For comparison, we also show the learning curves of STOL with n = 100 and n = 500, while \u03b7 and \u00b5 are also matched by the grid search. We observe that STOL performs very well at n = 500 (although the performance at the MNIST is slightly better due to higher data redundancy), but for the reasons described above cannot make much progress towards the target compared to random initialization with n = 100. In contrast, NOI performs very competitively with various small mini-batch sizes, with the objective results rapidly improving during the first few iterations, although larger n tend to achieve slightly higher correlations in tuning / test sets."}, {"heading": "C. Effect of time constant \u03c1", "text": "In the second group of experiments, we show the importance of \u03c1 in NOI for various minibatch sizes. The total canonical correlations achieved by NOI on the tuning set for \u03c1 = {0, 0.2, 0.4, 0.6, 0.8, 0.9, 0.99, 0.999, 0.9999} are shown in Fig. 3, while other hyperparameters are adjusted to their optimum values. We confirm that NOI works relatively well for relatively large n with \u03c1 = 0 (hence we use the same covariance estimate / brightening as [47]), but even if n is small, it is advantageous to use the TABLE II TEST SET KANONICAL CORRELATION OBTAINED BY FROM DIFFERENT ALGORITHMS. Data set L-BFGS STOL NOIn = 100 n = 500 n = 50 n = 100JW11 78.7 3.86.86.86.86.86.86.46.4 = 89.450 NO450 = 849.9 In 86.4L = 46.4N n n."}, {"heading": "D. Pure stochastic optimization for CCA", "text": "Finally, we perform a purely stochastic optimization (n = 1) for linear CCA on the MNIST dataset. Note that linear CCA is a special case of DCCA, both of which are single-layer linear networks (although we have used small weight-loss terms for weights that lead to a slightly different goal from that of CCA).Also, the total canonical correlations achieved by STOL with n = 500 and by NOI (50 training periods) on the training set with different IQ values are shown in Fig. 4. The goal of random initialization and the closed solution (by SVD) are also shown for comparison. NOI was not able to improve random initialization without memory (0 = 0, according to the algorithm of [47]), but comes very close to the optimal solution and coincides with the goal achieved by the previous large minibatch approach. This result also shows the adaptive importance of our CCA (8) estimation."}, {"heading": "VI. CONCLUSIONS", "text": "In this thesis, we have proposed a stochastic optimization algorithm NOI for the formation of DCCA, which updates DNN weights based on small minibatches and becomes competitive with previous optimizers. One direction for future work is to better understand the convergence properties of NOI, which presents several difficulties. First, we note that the convergence of the alternately least square formulation of CCA (algorithm 1, or rather orthogonal iterations) is usually stated as the angle between the estimated subspace and the subspace of truth converging to zero. In the stochastic optimization setting, we must refer this measure of progress (or another measure) to the nonlinear problems we are trying to solve in the NOI iterations."}], "references": [{"title": "Stochastic gradient learning in neural networks", "author": ["L. Bottou"], "venue": "Proc. Neuron\u0131\u0302mes, 1991.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 1991}, {"title": "Efficient backprop", "author": ["Y. LeCun", "L. Bottou", "G.B. Orr", "K.-R. M\u00fcller"], "venue": "Neural Networks: Tricks of the Trade, ser. Lecture Notes in Computer Science, vol. 1524, 1998, pp. 9\u201350.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 1998}, {"title": "Stochastic learning", "author": ["L. Bottou"], "venue": "Advanced Lectures on Machine Learning, ser. Lecture Notes in Artificial Intelligence 3176, 2004, pp. 146\u2013168.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2004}, {"title": "Solving large scale linear prediction problems using stochastic gradient descent algorithms", "author": ["T. Zhang"], "venue": "Proc. of the 21st Int. Conf. Machine Learning (ICML\u201904), 2004, pp. 919\u2013926.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2004}, {"title": "Incremental gradient, subgradient, and proximal methods for convex optimization: A survey", "author": ["D.P. Bertsekas"], "venue": "Optimization for Machine Learning, S. Sra, S. Nowozin, and S. J. Wright, Eds. MIT Press, 2011.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2011}, {"title": "The tradeoffs of large scale learning", "author": ["L. Bottou", "O. Bousquet"], "venue": "Advances in Neural Information Processing Systems (NIPS), vol. 20, 2008, pp. 161\u2013168.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2008}, {"title": "Relations between two sets of variates", "author": ["H. Hotelling"], "venue": "Biometrika, vol. 28, no. 3/4, pp. 321\u2013377, 1936.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 1936}, {"title": "Pixels that sound", "author": ["E. Kidron", "Y.Y. Schechner", "M. Elad"], "venue": "Proc. of the 2005 IEEE Computer Society Conf. Computer Vision and Pattern Recognition (CVPR\u201905), 2005, pp. 88\u201395.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2005}, {"title": "Multiview clustering via canonical correlation analysis", "author": ["K. Chaudhuri", "S.M. Kakade", "K. Livescu", "K. Sridharan"], "venue": "Proc. of the 26th Int. Conf. Machine Learning (ICML\u201909), 2009, pp. 129\u2013136.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2009}, {"title": "Multi-view CCA-based acoustic features for phonetic recognition across speakers and domains", "author": ["R. Arora", "K. Livescu"], "venue": "Proc. of the IEEE Int. Conf. Acoustics, Speech and Sig. Proc. (ICASSP\u201913), 2013.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2013}, {"title": "Canonical correlation analysis: An overview with application to learning methods", "author": ["D.R. Hardoon", "S. Szedmak", "J. Shawe-Taylor"], "venue": "Neural Computation, vol. 16, no. 12, pp. 2639\u20132664, 2004.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2004}, {"title": "Connecting modalities: Semi-supervised segmentation and annotation of images using unaligned text corpora", "author": ["R. Socher", "F.-F. Li"], "venue": "Proc. of the 2010 IEEE Computer Society Conf. Computer Vision and Pattern Recognition (CVPR\u201910), 2010, pp. 966\u2013973.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2010}, {"title": "Framing image description as a ranking task: Data, models and evaluation metrics", "author": ["M. Hodosh", "P. Young", "J. Hockenmaier"], "venue": "Journal of Artificial Intelligence Research, vol. 47, pp. 853\u2013899, 2013.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2013}, {"title": "Inferring a semantic representation of text via cross-language correlation analysis", "author": ["A. Vinokourov", "N. Cristianini", "J. Shawe-Taylor"], "venue": "Advances in Neural Information Processing Systems (NIPS), vol. 15, 2003, pp. 1497\u20131504.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2003}, {"title": "Learning bilingual lexicons from monolingual corpora", "author": ["A. Haghighi", "P. Liang", "T. Berg-Kirkpatrick", "D. Klein"], "venue": "Proceedings of the Annual Meeting of the Association for Computational Linguistics (ACL 2008), 2008, pp. 771\u2013779.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2008}, {"title": "An autoencoder approach to learning bilingual word representations", "author": ["S. Chandar", "S. Lauly", "H. Larochelle", "M.M. Khapra", "B. Ravindran", "V. Raykar", "A. Saha"], "venue": "Advances in Neural Information Processing Systems (NIPS), vol. 27, 2014, pp. 1853\u20131861.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2014}, {"title": "Improving vector space word representations using multilingual correlation", "author": ["M. Faruqui", "C. Dyer"], "venue": "Proceedings of EACL, 2014.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2014}, {"title": "Deep multilingual correlation for improved word embeddings", "author": ["A. Lu", "W. Wang", "M. Bansal", "K. Gimpel", "K. Livescu"], "venue": "The 2015 Conference of the North American Chapter of the Association for Computational Linguistics - Human Language Technologies (NAACL HLT 2015), 2015.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2015}, {"title": "GloVe: Global vectors for word representation", "author": ["J. Pennington", "R. Socher", "C.D. Manning"], "venue": "Proc. 2014 Conference on Empirical Methods in Natural Language Processing, 2014.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2014}, {"title": "Multi-view clustering", "author": ["S. Bickel", "T. Scheffer"], "venue": "Proc. of the 4th IEEE Int. Conf. Data Mining (ICDM\u201904), 2004, pp. 19\u201326.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2004}, {"title": "Deep canonical correlation analysis", "author": ["G. Andrew", "R. Arora", "J. Bilmes", "K. Livescu"], "venue": "Proc. of the 30th Int. Conf. Machine Learning (ICML 2013), 2013, pp. 1247\u20131255.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2013}, {"title": "Unsupervised learning of acoustic features via deep canonical correlation analysis", "author": ["W. Wang", "R. Arora", "K. Livescu", "J. Bilmes"], "venue": "Proc. of the IEEE Int. Conf. Acoustics, Speech and Sig. Proc. (ICASSP\u201915), 2015.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2015}, {"title": "On deep multi-view representation learning", "author": ["\u2014\u2014"], "venue": "Proc. of the 32st Int. Conf. Machine Learning (ICML 2015), 2015, pp. 1083\u20131092.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2015}, {"title": "Deep correlation for matching images and text", "author": ["F. Yan", "K. Mikolajczyk"], "venue": "Proc. of the 2015 IEEE Computer Society Conf. Computer Vision and Pattern Recognition (CVPR\u201915), 2015, pp. 3441\u20133450.", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2015}, {"title": "Kernel and nonlinear canonical correlation analysis", "author": ["P.L. Lai", "C. Fyfe"], "venue": "Int. J. Neural Syst., vol. 10, no. 5, pp. 365\u2013377, 2000.", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2000}, {"title": "Kernel independent component analysis", "author": ["F.R. Bach", "M.I. Jordan"], "venue": "Journal of Machine Learning Research, vol. 3, pp. 1\u201348, 2002.", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2002}, {"title": "Updating quasi-Newton matrices with limited storage", "author": ["J. Nocedal"], "venue": "Math. Comp., vol. 35, no. 151, pp. 773\u2013782, 1980.", "citeRegEx": "27", "shortCiteRegEx": null, "year": 1980}, {"title": "Large scale distributed deep networks", "author": ["J. Dean", "G. Corrado", "R. Monga", "K. Chen", "M. Devin", "Q. Le", "M. Mao", "M. Ranzato", "A. Senior", "P. Tucker", "K. Yang", "A. Ng"], "venue": "Advances in Neural Information Processing Systems (NIPS), vol. 25, 2012, pp. 1232\u20131240.", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2012}, {"title": "Learning representations by back-propagating errors", "author": ["D.E. Rumelhart", "G.E. Hinton", "R.J. Williams"], "venue": "Nature, vol. 323, pp. 533\u2013 536, 1986.", "citeRegEx": "29", "shortCiteRegEx": null, "year": 1986}, {"title": "Matrix Computations, 3rd ed", "author": ["G.H. Golub", "C.F. van Loan"], "venue": null, "citeRegEx": "30", "shortCiteRegEx": "30", "year": 1996}, {"title": "Linear Algebra for Signal Processing, ser. The IMA Volumes in Mathematics and its Applications", "author": ["G.H. Golub", "H. Zha"], "venue": "ch. The Canonical Correlations of Matrix Pairs and their Numerical Computation,", "citeRegEx": "31", "shortCiteRegEx": "31", "year": 1995}, {"title": "Large scale canonical correlation analysis with iterative least squares", "author": ["Y. Lu", "D.P. Foster"], "venue": "Advances in Neural Information Processing Systems (NIPS), vol. 27, 2014, pp. 91\u201399.", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2014}, {"title": "Some methods of speeding up the convergence of iteration methods", "author": ["B.T. Polyak"], "venue": "USSR Computational Mathematics and Mathematical Physics, vol. 4, no. 5, pp. 1\u201317, 1964.", "citeRegEx": "33", "shortCiteRegEx": null, "year": 1964}, {"title": "On the importance of initialization and momentum in deep learning", "author": ["I. Sutskever", "J. Martens", "G. Dahl", "G. Hinton"], "venue": "Proc. of the 30th Int. Conf. Machine Learning (ICML 2013), 2013, pp. 1139\u20131147.", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2013}, {"title": "No more pesky learning rate", "author": ["T. Schaul", "S. Zhang", "Y. LeCun"], "venue": "Proc. of the 30th Int. Conf. Machine Learning (ICML 2013), 2013, pp. 343\u2013351.", "citeRegEx": "35", "shortCiteRegEx": null, "year": 2013}, {"title": "Fast low-rank modifications of the thin singular value decomposition", "author": ["M. Brand"], "venue": "Linear Algebra Appl., vol. 415, no. 1, pp. 20\u201330, 2006.", "citeRegEx": "36", "shortCiteRegEx": null, "year": 2006}, {"title": "Data stream anomaly detection through principal subspace tracking", "author": ["P.H. dos Santos Teixeira", "R.L. Milidi\u00fa"], "venue": "Proceedings of the 2010 ACM Symposium on Applied Computing (SAC\u201910), 2010, pp. 1609\u20131616.", "citeRegEx": "37", "shortCiteRegEx": null, "year": 2010}, {"title": "Adaptive canonical correlation analysis based on matrix manifolds", "author": ["F. Yger", "M. Berar", "G. Gasso", "A. Rakotomamonjy"], "venue": "Proc. of the 29th Int. Conf. Machine Learning (ICML 2012), 2012, pp. 1071\u2013 1078.", "citeRegEx": "38", "shortCiteRegEx": null, "year": 2012}, {"title": "A method of stochastic approximation for the determination of the least eigenvalue of a symmetric matrix", "author": ["T.P. Krasulina"], "venue": "USSR Computational Mathematics and Mathematical Physics, vol. 9, no. 6, pp. 189\u2013195, 1969.", "citeRegEx": "39", "shortCiteRegEx": null, "year": 1969}, {"title": "On stochastic approximation of the eigenvectors and eigenvalues of the expectation of a random matrix", "author": ["E. Oja", "J. Karhunen"], "venue": "J. Math. Anal. Appl., vol. 106, no. 1, pp. 69\u201384, 1985.", "citeRegEx": "40", "shortCiteRegEx": null, "year": 1985}, {"title": "Randomized online PCA algorithms with regret bounds that are logarithmic in the dimension", "author": ["M.K. Warmuth", "D. Kuzmin"], "venue": "Journal of Machine Learning Research, vol. 9, pp. 2287\u20132320, 2008.", "citeRegEx": "41", "shortCiteRegEx": null, "year": 2008}, {"title": "Stochastic optimization for PCA and PLS", "author": ["R. Arora", "A. Cotter", "K. Livescu", "N. Srebro"], "venue": "50th Annual Allerton Conference on Communication, Control, and Computing, Montcello, IL, Oct. 1\u20135 2012, pp. 861\u2013868.", "citeRegEx": "42", "shortCiteRegEx": null, "year": 2012}, {"title": "Stochastic optimization of PCA with capped MSG", "author": ["R. Arora", "A. Cotter", "N. Srebro"], "venue": "Advances in Neural Information Processing Systems (NIPS), vol. 26, 2013, pp. 1815\u20131823.", "citeRegEx": "43", "shortCiteRegEx": null, "year": 2013}, {"title": "Memory limited, streaming PCA", "author": ["I. Mitliagkas", "C. Caramanis", "P. Jain"], "venue": "Advances in Neural Information Processing Systems (NIPS), vol. 26, 2013, pp. 2886\u20132894.", "citeRegEx": "44", "shortCiteRegEx": null, "year": 2013}, {"title": "The fast convergence of incremental PCA", "author": ["A. Balsubramani", "S. Dasgupta", "Y. Freund"], "venue": "Advances in Neural Information Processing Systems (NIPS), vol. 26, 2013, pp. 3174\u20133182.", "citeRegEx": "45", "shortCiteRegEx": null, "year": 2013}, {"title": "A stochastic PCA and SVD algorithm with an exponential convergence rate", "author": ["O. Shamir"], "venue": "Proc. of the 32st Int. Conf. Machine Learning (ICML 2015), 2015, pp. 144\u2013152.", "citeRegEx": "46", "shortCiteRegEx": null, "year": 2015}, {"title": "Finding linear structure in large datasets with scalable canonical correlation analysis", "author": ["Z. Ma", "Y. Lu", "D. Foster"], "venue": "Proc. of the 32st Int. Conf. Machine Learning (ICML 2015), 2015, pp. 169\u2013178.", "citeRegEx": "47", "shortCiteRegEx": null, "year": 2015}, {"title": "Distributed optimization of deeply nested systems", "author": ["M.\u00c1. Carreira-Perpi\u00f1\u00e1n", "W. Wang"], "venue": "Proc. of the 17th Int. Workshop on Artificial Intelligence and Statistics (AISTATS 2014), Reykjavik, Iceland, Apr. 22 \u2013 Apr. 25 2014, pp. 10\u201319.", "citeRegEx": "49", "shortCiteRegEx": null, "year": 2014}, {"title": "X-Ray Microbeam Speech Production", "author": ["J.R. Westbury"], "venue": "Database User\u2019s Handbook Version", "citeRegEx": "50", "shortCiteRegEx": "50", "year": 1994}, {"title": "Gradient-based learning applied to document recognition", "author": ["Y. LeCun", "L. Bottou", "Y. Bengio", "P. Haffner"], "venue": "Proc. IEEE, vol. 86, no. 11, pp. 2278\u20132324, 1998.", "citeRegEx": "51", "shortCiteRegEx": null, "year": 1998}, {"title": "Rectified linear units improve restricted Boltzmann machines", "author": ["V. Nair", "G.E. Hinton"], "venue": "Proc. of the 27th Int. Conf. Machine Learning (ICML 2010), 2010, pp. 807\u2013814.", "citeRegEx": "52", "shortCiteRegEx": null, "year": 2010}, {"title": "minFunc", "author": ["M. Schmidt"], "venue": "2012, code available at http://www.cs.ubc.ca/ \u0303schmidtm/Software/minFunc.html.", "citeRegEx": "53", "shortCiteRegEx": null, "year": 2012}], "referenceMentions": [{"referenceID": 0, "context": "Stochastic gradient descent (SGD) is a fundamental and popular optimization method for machine learning problems [1], [2], [3], [4], [5].", "startOffset": 113, "endOffset": 116}, {"referenceID": 1, "context": "Stochastic gradient descent (SGD) is a fundamental and popular optimization method for machine learning problems [1], [2], [3], [4], [5].", "startOffset": 118, "endOffset": 121}, {"referenceID": 2, "context": "Stochastic gradient descent (SGD) is a fundamental and popular optimization method for machine learning problems [1], [2], [3], [4], [5].", "startOffset": 123, "endOffset": 126}, {"referenceID": 3, "context": "Stochastic gradient descent (SGD) is a fundamental and popular optimization method for machine learning problems [1], [2], [3], [4], [5].", "startOffset": 128, "endOffset": 131}, {"referenceID": 4, "context": "Stochastic gradient descent (SGD) is a fundamental and popular optimization method for machine learning problems [1], [2], [3], [4], [5].", "startOffset": 133, "endOffset": 136}, {"referenceID": 5, "context": "SGD is particularly well-suited for large-scale machine learning problems because it is extremely simple and easy to implement, it often achieves better generalization (test) performance (which is the focus of machine learning research) than sophisticated batch algorithms, and it usually achieves large error reduction very quickly in a small number of passes over the training set [6].", "startOffset": 383, "endOffset": 386}, {"referenceID": 1, "context": "One intuitive explanation for the empirical success of stochastic gradient descent for large data is that it makes better use of data redundancy, with an extreme example given by [2]: If the training set consists of 10 copies of the same set of examples, then computing an estimate of the gradient over one single copy is 10 times more efficient than computing the full gradient over the entire training set, while achieving the same optimization progress in the following gradient descent step.", "startOffset": 179, "endOffset": 182}, {"referenceID": 6, "context": "At the same time, \u201cmulti-view\u201d data are becoming increasingly available, and methods based on canonical correlation analysis (CCA) [7] that use such data to learn representations (features) form an active research area.", "startOffset": 131, "endOffset": 134}, {"referenceID": 7, "context": "The views can be multiple measurement modalities, such as simultaneously recorded audio + video [8], [9], audio + articulation [10], images + text [11], [12], [13], or parallel text in two languages [14], [15], [16], [17], [18], but may also be different information extracted from the same source, such", "startOffset": 96, "endOffset": 99}, {"referenceID": 8, "context": "The views can be multiple measurement modalities, such as simultaneously recorded audio + video [8], [9], audio + articulation [10], images + text [11], [12], [13], or parallel text in two languages [14], [15], [16], [17], [18], but may also be different information extracted from the same source, such", "startOffset": 101, "endOffset": 104}, {"referenceID": 9, "context": "The views can be multiple measurement modalities, such as simultaneously recorded audio + video [8], [9], audio + articulation [10], images + text [11], [12], [13], or parallel text in two languages [14], [15], [16], [17], [18], but may also be different information extracted from the same source, such", "startOffset": 127, "endOffset": 131}, {"referenceID": 10, "context": "The views can be multiple measurement modalities, such as simultaneously recorded audio + video [8], [9], audio + articulation [10], images + text [11], [12], [13], or parallel text in two languages [14], [15], [16], [17], [18], but may also be different information extracted from the same source, such", "startOffset": 147, "endOffset": 151}, {"referenceID": 11, "context": "The views can be multiple measurement modalities, such as simultaneously recorded audio + video [8], [9], audio + articulation [10], images + text [11], [12], [13], or parallel text in two languages [14], [15], [16], [17], [18], but may also be different information extracted from the same source, such", "startOffset": 153, "endOffset": 157}, {"referenceID": 12, "context": "The views can be multiple measurement modalities, such as simultaneously recorded audio + video [8], [9], audio + articulation [10], images + text [11], [12], [13], or parallel text in two languages [14], [15], [16], [17], [18], but may also be different information extracted from the same source, such", "startOffset": 159, "endOffset": 163}, {"referenceID": 13, "context": "The views can be multiple measurement modalities, such as simultaneously recorded audio + video [8], [9], audio + articulation [10], images + text [11], [12], [13], or parallel text in two languages [14], [15], [16], [17], [18], but may also be different information extracted from the same source, such", "startOffset": 199, "endOffset": 203}, {"referenceID": 14, "context": "The views can be multiple measurement modalities, such as simultaneously recorded audio + video [8], [9], audio + articulation [10], images + text [11], [12], [13], or parallel text in two languages [14], [15], [16], [17], [18], but may also be different information extracted from the same source, such", "startOffset": 205, "endOffset": 209}, {"referenceID": 15, "context": "The views can be multiple measurement modalities, such as simultaneously recorded audio + video [8], [9], audio + articulation [10], images + text [11], [12], [13], or parallel text in two languages [14], [15], [16], [17], [18], but may also be different information extracted from the same source, such", "startOffset": 211, "endOffset": 215}, {"referenceID": 16, "context": "The views can be multiple measurement modalities, such as simultaneously recorded audio + video [8], [9], audio + articulation [10], images + text [11], [12], [13], or parallel text in two languages [14], [15], [16], [17], [18], but may also be different information extracted from the same source, such", "startOffset": 217, "endOffset": 221}, {"referenceID": 17, "context": "The views can be multiple measurement modalities, such as simultaneously recorded audio + video [8], [9], audio + articulation [10], images + text [11], [12], [13], or parallel text in two languages [14], [15], [16], [17], [18], but may also be different information extracted from the same source, such", "startOffset": 223, "endOffset": 227}, {"referenceID": 18, "context": "as words + context [19] or document text + text of inbound hyperlinks [20].", "startOffset": 19, "endOffset": 23}, {"referenceID": 19, "context": "as words + context [19] or document text + text of inbound hyperlinks [20].", "startOffset": 70, "endOffset": 74}, {"referenceID": 20, "context": "Among various multi-view learning approaches, the recently proposed deep canonical correlation analysis [21], which extends traditional CCA with deep neural networks (DNNs), has been shown to be advantageous over previous methods in several domains [22], [23], [24], and scales to large data better than its nonparametric counterpart kernel CCA [25], [26], [11].", "startOffset": 104, "endOffset": 108}, {"referenceID": 21, "context": "Among various multi-view learning approaches, the recently proposed deep canonical correlation analysis [21], which extends traditional CCA with deep neural networks (DNNs), has been shown to be advantageous over previous methods in several domains [22], [23], [24], and scales to large data better than its nonparametric counterpart kernel CCA [25], [26], [11].", "startOffset": 249, "endOffset": 253}, {"referenceID": 22, "context": "Among various multi-view learning approaches, the recently proposed deep canonical correlation analysis [21], which extends traditional CCA with deep neural networks (DNNs), has been shown to be advantageous over previous methods in several domains [22], [23], [24], and scales to large data better than its nonparametric counterpart kernel CCA [25], [26], [11].", "startOffset": 255, "endOffset": 259}, {"referenceID": 23, "context": "Among various multi-view learning approaches, the recently proposed deep canonical correlation analysis [21], which extends traditional CCA with deep neural networks (DNNs), has been shown to be advantageous over previous methods in several domains [22], [23], [24], and scales to large data better than its nonparametric counterpart kernel CCA [25], [26], [11].", "startOffset": 261, "endOffset": 265}, {"referenceID": 24, "context": "Among various multi-view learning approaches, the recently proposed deep canonical correlation analysis [21], which extends traditional CCA with deep neural networks (DNNs), has been shown to be advantageous over previous methods in several domains [22], [23], [24], and scales to large data better than its nonparametric counterpart kernel CCA [25], [26], [11].", "startOffset": 345, "endOffset": 349}, {"referenceID": 25, "context": "Among various multi-view learning approaches, the recently proposed deep canonical correlation analysis [21], which extends traditional CCA with deep neural networks (DNNs), has been shown to be advantageous over previous methods in several domains [22], [23], [24], and scales to large data better than its nonparametric counterpart kernel CCA [25], [26], [11].", "startOffset": 351, "endOffset": 355}, {"referenceID": 10, "context": "Among various multi-view learning approaches, the recently proposed deep canonical correlation analysis [21], which extends traditional CCA with deep neural networks (DNNs), has been shown to be advantageous over previous methods in several domains [22], [23], [24], and scales to large data better than its nonparametric counterpart kernel CCA [25], [26], [11].", "startOffset": 357, "endOffset": 361}, {"referenceID": 26, "context": ", limited-memory BFGS (L-BFGS) [27] as in [21], or stochastic optimization with large minibatches [22], because it is difficult to obtain an accurate estimate of the gradient with a small subset of the training examples (again due to the whitening constraint).", "startOffset": 31, "endOffset": 35}, {"referenceID": 20, "context": ", limited-memory BFGS (L-BFGS) [27] as in [21], or stochastic optimization with large minibatches [22], because it is difficult to obtain an accurate estimate of the gradient with a small subset of the training examples (again due to the whitening constraint).", "startOffset": 42, "endOffset": 46}, {"referenceID": 21, "context": ", limited-memory BFGS (L-BFGS) [27] as in [21], or stochastic optimization with large minibatches [22], because it is difficult to obtain an accurate estimate of the gradient with a small subset of the training examples (again due to the whitening constraint).", "startOffset": 98, "endOffset": 102}, {"referenceID": 27, "context": "As a result, these approaches have high memory complexity and may not be practical for large DNN models with hundreds of millions of weight parameters (common with web-scale data [28]), or if one would like to run the training procedure on GPUs which are equipped with faster but smaller (more expensive) memory than CPUs.", "startOffset": 179, "endOffset": 183}, {"referenceID": 20, "context": "Deep CCA (DCCA) [21] extends (linear) CCA [7] by extracting dx- and dy-dimensional nonlinear features with two DNNs f and g for views 1 and 2 respectively, such that the canonical correlation (measured by CCA) between the DNN outputs is maximized, as illustrated in Fig.", "startOffset": 16, "endOffset": 20}, {"referenceID": 6, "context": "Deep CCA (DCCA) [21] extends (linear) CCA [7] by extracting dx- and dy-dimensional nonlinear features with two DNNs f and g for views 1 and 2 respectively, such that the canonical correlation (measured by CCA) between the DNN outputs is maximized, as illustrated in Fig.", "startOffset": 42, "endOffset": 45}, {"referenceID": 6, "context": "canonical variables, [7]) are maximally correlated with their counterparts in the other view, constrained such that the dimensions in the representation are uncorrelated with each other.", "startOffset": 21, "endOffset": 24}, {"referenceID": 20, "context": "When introducing deep CCA, [21] used the L-BFGS algorithm for optimization.", "startOffset": 27, "endOffset": 31}, {"referenceID": 28, "context": "One can then compute the gradients with respect to Wf and Wg via the standard backpropagation procedure [29].", "startOffset": 104, "endOffset": 108}, {"referenceID": 21, "context": "Later, it was observed by [22] that stochastic optimization still works well even for the DCCA objective, as long as larger minibatches are used to estimate the covariances and \u03a3\u0303fg when computing the gradient with (4).", "startOffset": 26, "endOffset": 30}, {"referenceID": 21, "context": "on a minibatch using (4) does not equal the true gradient of the objective in expectation, indicating that the stochastic approach of [22] does not qualify as a stochastic gradient descent method for the DCCA objective.", "startOffset": 134, "endOffset": 138}, {"referenceID": 29, "context": "This algorithm computes the top-L singular vectors (\u0168, \u1e7c) of \u03a3\u0303fg via orthogonal iterations [30].", "startOffset": 92, "endOffset": 96}, {"referenceID": 31, "context": "For [32], the advantage of the alternating least squares formulation over the exact solution to CCA is that it does not need to form the high-dimensional (nonsparse) matrix \u03a3\u0303fg; instead it directly operates on the projections, which are much smaller in size, and one can solve the least squares problems using iterative algorithms that require only sparse matrix-vector multiplications.", "startOffset": 4, "endOffset": 8}, {"referenceID": 0, "context": "where \u03c1 \u2208 [0, 1], Xb denotes a minibatch of data with index set b, and |b| denotes the size (number of samples) of this minibatch.", "startOffset": 10, "endOffset": 16}, {"referenceID": 32, "context": "The update in (8) has a form similar to that of the widely used momentum technique in the optimization [33] and neural network literature [34], [35], and is also used by [36], [37], [38] for online subspace tracking and anomaly detection.", "startOffset": 103, "endOffset": 107}, {"referenceID": 33, "context": "The update in (8) has a form similar to that of the widely used momentum technique in the optimization [33] and neural network literature [34], [35], and is also used by [36], [37], [38] for online subspace tracking and anomaly detection.", "startOffset": 138, "endOffset": 142}, {"referenceID": 34, "context": "The update in (8) has a form similar to that of the widely used momentum technique in the optimization [33] and neural network literature [34], [35], and is also used by [36], [37], [38] for online subspace tracking and anomaly detection.", "startOffset": 144, "endOffset": 148}, {"referenceID": 35, "context": "The update in (8) has a form similar to that of the widely used momentum technique in the optimization [33] and neural network literature [34], [35], and is also used by [36], [37], [38] for online subspace tracking and anomaly detection.", "startOffset": 170, "endOffset": 174}, {"referenceID": 36, "context": "The update in (8) has a form similar to that of the widely used momentum technique in the optimization [33] and neural network literature [34], [35], and is also used by [36], [37], [38] for online subspace tracking and anomaly detection.", "startOffset": 176, "endOffset": 180}, {"referenceID": 37, "context": "The update in (8) has a form similar to that of the widely used momentum technique in the optimization [33] and neural network literature [34], [35], and is also used by [36], [37], [38] for online subspace tracking and anomaly detection.", "startOffset": 182, "endOffset": 186}, {"referenceID": 31, "context": "These advantages validate our choice of whitening operations over the more commonly used QR decomposition used by [32].", "startOffset": 114, "endOffset": 118}, {"referenceID": 33, "context": "the stochastic gradient steps for the nonlinear least squares problems as is commonly used in the deep learning community [34].", "startOffset": 122, "endOffset": 126}, {"referenceID": 38, "context": "Stochastic (and online) optimization techniques for fundamental problems, such as principal component analysis and partial least squares, are of continuous research interest [39], [40], [41], [42], [43], [44], [45], [46].", "startOffset": 174, "endOffset": 178}, {"referenceID": 39, "context": "Stochastic (and online) optimization techniques for fundamental problems, such as principal component analysis and partial least squares, are of continuous research interest [39], [40], [41], [42], [43], [44], [45], [46].", "startOffset": 180, "endOffset": 184}, {"referenceID": 40, "context": "Stochastic (and online) optimization techniques for fundamental problems, such as principal component analysis and partial least squares, are of continuous research interest [39], [40], [41], [42], [43], [44], [45], [46].", "startOffset": 186, "endOffset": 190}, {"referenceID": 41, "context": "Stochastic (and online) optimization techniques for fundamental problems, such as principal component analysis and partial least squares, are of continuous research interest [39], [40], [41], [42], [43], [44], [45], [46].", "startOffset": 192, "endOffset": 196}, {"referenceID": 42, "context": "Stochastic (and online) optimization techniques for fundamental problems, such as principal component analysis and partial least squares, are of continuous research interest [39], [40], [41], [42], [43], [44], [45], [46].", "startOffset": 198, "endOffset": 202}, {"referenceID": 43, "context": "Stochastic (and online) optimization techniques for fundamental problems, such as principal component analysis and partial least squares, are of continuous research interest [39], [40], [41], [42], [43], [44], [45], [46].", "startOffset": 204, "endOffset": 208}, {"referenceID": 44, "context": "Stochastic (and online) optimization techniques for fundamental problems, such as principal component analysis and partial least squares, are of continuous research interest [39], [40], [41], [42], [43], [44], [45], [46].", "startOffset": 210, "endOffset": 214}, {"referenceID": 45, "context": "Stochastic (and online) optimization techniques for fundamental problems, such as principal component analysis and partial least squares, are of continuous research interest [39], [40], [41], [42], [43], [44], [45], [46].", "startOffset": 216, "endOffset": 220}, {"referenceID": 41, "context": "However, as pointed out by [42], the CCA objective is more challenging due to the whitening constraints.", "startOffset": 27, "endOffset": 31}, {"referenceID": 37, "context": "Recently, [38] proposed an adaptive CCA algorithm with efficient online updates based on matrix manifolds defined by the whitening constraints.", "startOffset": 10, "endOffset": 14}, {"referenceID": 31, "context": "Based on the alternating least squares formulation of CCA (Algorithm 1), [32] propose an iterative solution of CCA for very high-dimensional and sparse input features, and the key idea is to solve the high dimensional least squares problems with randomized PCA and (batch) gradient descent.", "startOffset": 73, "endOffset": 77}, {"referenceID": 46, "context": "Upon the submission of this paper, we have become aware of the very recent publication of [47], which extends [32] by solving the linear least squares problems with (stochastic) gradient descent.", "startOffset": 90, "endOffset": 94}, {"referenceID": 31, "context": "Upon the submission of this paper, we have become aware of the very recent publication of [47], which extends [32] by solving the linear least squares problems with (stochastic) gradient descent.", "startOffset": 110, "endOffset": 114}, {"referenceID": 46, "context": "(for a one-dimensional projection, to be consistent with the notation of [47]) in Algorithm 3, where we take a batch gradient descent step over the least squares objectives in each iteration.", "startOffset": 73, "endOffset": 77}, {"referenceID": 46, "context": "This algorithm is equivalent to Algorithm 3 of [47].", "startOffset": 47, "endOffset": 51}, {"referenceID": 46, "context": "In [47] it is shown that the solution to the CCA objective is a fixed point of this algorithm, but no global convergence property is given.", "startOffset": 3, "endOffset": 7}, {"referenceID": 46, "context": "In the whitening steps of [47], however, the covariances are estimated using only the current minibatch at each iterate, without consideration of the remaining training samples or previous estimates, which corresponds to \u03c1 \u2192 0 in our estimate.", "startOffset": 26, "endOffset": 30}, {"referenceID": 46, "context": "[47] also suggests using a minibatch size of the order O(L), the dimensionality of the covariance matrices to be estimated, in order to obtain a high-accuracy estimate for whitening.", "startOffset": 0, "endOffset": 4}, {"referenceID": 46, "context": "5Although Algorithm 3 of [47] maintains two copies\u2014the normalized and the unnormalized versions\u2014of the weight parameters, we observe that the sole purpose of the normalized version in the intermediate iterations is to provide whitened target output for the least squares problems; our version of the algorithm eliminates this copy and the normalized version can be retrieved by a whitening step at the end.", "startOffset": 25, "endOffset": 29}, {"referenceID": 24, "context": "Since the difficulty lies in the whitening constraints, one can relax the constraints and solve the Lagrangian formulation repeatedly with updated Lagrangian multipliers, as done by [25]; or one can introduce auxiliary variables and apply the quadratic penalty method [48], as done by [49].", "startOffset": 182, "endOffset": 186}, {"referenceID": 47, "context": "Since the difficulty lies in the whitening constraints, one can relax the constraints and solve the Lagrangian formulation repeatedly with updated Lagrangian multipliers, as done by [25]; or one can introduce auxiliary variables and apply the quadratic penalty method [48], as done by [49].", "startOffset": 285, "endOffset": 289}, {"referenceID": 20, "context": "We now demonstrate the NOI algorithm on the two realworld datasets used by [21] when introducing DCCA.", "startOffset": 75, "endOffset": 79}, {"referenceID": 48, "context": "The first dataset is a subset of the University of Wisconsin XRay Microbeam corpus [50], which consists of simultaneously recorded acoustic and articulatory measurements during speech.", "startOffset": 83, "endOffset": 87}, {"referenceID": 20, "context": "Following [21], [22], the acoustic view inputs are 39D Mel-frequency cepstral coefficients and the articulatory view inputs are horizontal/vertical displacement of 8 pellets attached to different parts of the vocal tract, each then concatenated over a 7-frame context window, for speaker \u2018JW11\u2019.", "startOffset": 10, "endOffset": 14}, {"referenceID": 21, "context": "Following [21], [22], the acoustic view inputs are 39D Mel-frequency cepstral coefficients and the articulatory view inputs are horizontal/vertical displacement of 8 pellets attached to different parts of the vocal tract, each then concatenated over a 7-frame context window, for speaker \u2018JW11\u2019.", "startOffset": 16, "endOffset": 20}, {"referenceID": 49, "context": "The second dataset consists of left/right halves of the images in the MNIST dataset [51], and so the input of each view consists of 28 \u00d7 14 grayscale images.", "startOffset": 84, "endOffset": 88}, {"referenceID": 20, "context": "Instead, we use DNN architectures similar to those used by [21] with ReLU activations [52], and we achieve better generalization performance with these architectures mainly due to better optimization.", "startOffset": 59, "endOffset": 63}, {"referenceID": 50, "context": "Instead, we use DNN architectures similar to those used by [21] with ReLU activations [52], and we achieve better generalization performance with these architectures mainly due to better optimization.", "startOffset": 86, "endOffset": 90}, {"referenceID": 20, "context": "The projection dimensionality L is set to 112/50 for JW11/MNIST respectively as in [21]; these are also the maximum possible total canonical correlations for the two datasets.", "startOffset": 83, "endOffset": 87}, {"referenceID": 20, "context": "We compare three optimization approaches: full batch optimization by L-BFGS [21], using the implementation of [53] which includes a good line-search procedure; stochastic JW11 MNIST", "startOffset": 76, "endOffset": 80}, {"referenceID": 51, "context": "We compare three optimization approaches: full batch optimization by L-BFGS [21], using the implementation of [53] which includes a good line-search procedure; stochastic JW11 MNIST", "startOffset": 110, "endOffset": 114}, {"referenceID": 21, "context": "optimization with large minibatches [22], denoted STOL; and our algorithm, denoted NOI.", "startOffset": 36, "endOffset": 40}, {"referenceID": 20, "context": "Total canonical correlations on the test sets are given in Table II, showing that we achieve better results than [21] with similar DNN architectures.", "startOffset": 113, "endOffset": 117}, {"referenceID": 46, "context": "We confirm that for relatively large n, NOI works reasonably well with \u03c1 = 0 (so we are using the same covariance estimate/whitening as [47]).", "startOffset": 136, "endOffset": 140}, {"referenceID": 46, "context": "NOI could not improve over the random initialization without memory (\u03c1 = 0, corresponding to the algorithm of [47]), but gets very close to the optimal solution and matches the objective obtained by the previous large minibatch approach when \u03c1 \u2192 1.", "startOffset": 110, "endOffset": 114}, {"referenceID": 46, "context": "As discussed in Section IV, even the convergence of the linear CCA version of NOI with batch gradient descent is not well understood [47].", "startOffset": 133, "endOffset": 137}], "year": 2015, "abstractText": "Deep CCA is a recently proposed deep neural network extension to the traditional canonical correlation analysis (CCA), and has been successful for multi-view representation learning in several domains. However, stochastic optimization of the deep CCA objective is not straightforward, because it does not decouple over training examples. Previous optimizers for deep CCA are either batch-based algorithms or stochastic optimization using large minibatches, which can have high memory consumption. In this paper, we tackle the problem of stochastic optimization for deep CCA with small minibatches, based on an iterative solution to the CCA objective, and show that we can achieve as good performance as previous optimizers and thus alleviate the memory requirement.", "creator": "LaTeX with hyperref package"}}}