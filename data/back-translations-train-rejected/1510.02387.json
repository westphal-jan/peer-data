{"id": "1510.02387", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "8-Oct-2015", "title": "Mapping Unseen Words to Task-Trained Embedding Spaces", "abstract": "We consider the setting in which we train a supervised model that learns task-specific word representations. We assume that we have access to some initial word representations (e.g., unsupervised embeddings), and that the supervised learning procedure updates them to task-specific representations for words contained in the training data. But what about words not contained in the supervised training data? When such unseen words are encountered at test time, they are typically represented by either their initial vectors or a single unknown vector, which often leads to errors. In this paper, we address this issue by learning to map from initial representations to task-specific ones. We present a general technique that uses a neural network mapper with a weighted multiple-loss criterion. This allows us to use the same learned model parameters at test time but now with appropriate task-specific representations for unseen words. We consider the task of dependency parsing and report improvements in performance (and reductions in out-of-vocabulary rates) across multiple domains such as news, Web, and speech. We also achieve downstream improvements on the task of parsing-based sentiment analysis.", "histories": [["v1", "Thu, 8 Oct 2015 16:17:47 GMT  (214kb)", "http://arxiv.org/abs/1510.02387v1", "10 + 3 pages, 3 figures"], ["v2", "Thu, 23 Jun 2016 06:24:18 GMT  (223kb)", "http://arxiv.org/abs/1510.02387v2", "8 + 3 pages, 3 figures"]], "COMMENTS": "10 + 3 pages, 3 figures", "reviews": [], "SUBJECTS": "cs.CL cs.LG", "authors": ["pranava swaroop madhyastha", "mohit bansal", "kevin gimpel", "karen livescu"], "accepted": false, "id": "1510.02387"}, "pdf": {"name": "1510.02387.pdf", "metadata": {"source": "CRF", "title": "Mapping Unseen Words to Task-Trained Embedding Spaces", "authors": ["Pranava Swaroop Madhyastha", "Mohit Bansal Kevin Gimpel", "Karen Livescu"], "emails": ["pranava@cs.upc.edu", "mbansal@ttic.edu", "kgimpel@ttic.edu", "klivescu@ttic.edu"], "sections": [{"heading": null, "text": "ar Xiv: 151 0.02 387v 1 [cs.C L] 8O ctWe look at the setting in which we train a supervised model that learns task-specific word representations. We assume that we have access to some initial word representations (e.g. unsupervised embedding) and that the supervised learning process updates them to task-specific representations for words that are included in the training data. But what about words that are not included in the supervised training data? If such invisible words occur at the test date, they are typically represented either by their initial vectors or by a single unknown vector, which often leads to errors. In this essay, we address this problem by learning to match task-specific words from initial representations. We present a general technique that uses a neural network mapper with a weighted multiple loss criterion. This allows us to use the same subject-learning parameters we now use for the model time point and the task-specific Vocabulary parameters."}, {"heading": "1 Introduction", "text": "In fact, we are able to go in search of a solution that is capable of finding a solution, that is capable of finding a solution that is capable of finding a solution, that is capable of finding a solution, and that is able to find a solution that is capable of finding a solution, that is able to find a solution, that is able to find a solution, that is able to find a solution, that is able to find a solution, that is able to find a solution, that is able to find a solution, that is able to find a solution, that is able to find a solution, that is able to find a solution, that is able to find a solution."}, {"heading": "2 Mapping Unseen Representations", "text": "In this section, we present our approach, including an overview of the pipeline, the mapper architecture, and the training / tuning components (e.g. loss function, regulation, and thresholds)."}, {"heading": "2.1 Pipeline Overview and Definitions", "text": "Let's call eoi the initial (original) embedding of words derived from this corpus. Initial embedding is typically learned in an unattended manner, but for our purposes it can be any initial embedding. Let T'V be the subset of words that appear in the annotated training data for some supervised task-specific training. We define invisible words as those in the sentence V / T. While our approach is general, let's consider the task of embedding words for concreteness, let's consider the task of embedding Eoi, the annotated data consists of sentences paired with dependency trees. We're assuming a dependency saver who learns task-specific word embedding eti for word wi\\ T, starting from the original embedding of Eoi."}, {"heading": "2.2 Mapper Architecture", "text": "Our proposed mapper is a multi-layered feedback neural network that uses an initial word as an embedding and outputs a mapped representation of the same dimensionality. Specifically, we use a single hidden layer with a hardtanh nonlinearity, so that the function G is defined as follows: G (eoi) = W2 (hard tanh (W1e o i + b1))) + b2 (1), where W1 and W2 are parameter matrices and b1 and b2 are bias vectors. The \"hardtanh\" nonlinearity is the standard version of the hyperbolic tangent: hard tanh (z) = \u2212 1 if z < \u2212 1z if \u2212 1 \u2264 z \u2264 11 if z > 1In preliminary experiments, we compared it with other nonlinear functions (sigmoid, tanh and ReLU) as well as zero and more than one nonlinear layer. We found that hard tanearthly layers are not better or less linear."}, {"heading": "2.3 Loss Function", "text": "We use a weighted multi-loss regression approach, which optimizes a weighted sum of the mean squared errors and means an absolute error: loss (y, y) = \u03b1n \u2211 j = 1 | yj \u2212 y-j | + (1 \u2212 \u03b1) n \u2211 j = 1 | yj \u2212 y-j | 2 (2), where y = eti (the basic truth) and y = e m i (the prediction) are n-dimensional vectors. This multi-loss approach attempts to make both the conditional mean of the predicted representation close to the task-trained representation (the squared loss) and the conditional mean of the predicted representation close to the task-trained one (the average absolute loss). A weighted multi-criterion target allows us to avoid strong assumptions about the optimal transformation to be learned. We adjust the hyperparameter to domain-specific hold-out data (the BGS), the BGS (BGS) and the other recent BGS (BGS), and BGS (BGS)."}, {"heading": "2.4 Regularization", "text": "We use the elastic network regulation (Liu and Nocedal, 1989), which combines linear penalties in the parameters \"1\" and \"2\" to control the capacity of the mapper function, which corresponds to a minimization: F (\u03b8) = L (\u03b8) + \u03bb1 (\u03b8) + \u03bb22 \"\u03b8\" - 2, where \u03b8 is the complete set of mapper parameters and L (\u03b8) is the loss function (Eq. 2 summed up by mapper training examples). We adjust the hyperparameters of the regulator and the loss function for each task separately, using a task-specific development set. This gives us additional flexibility to map the embedding for the area of interest, especially if the parser training data comes from a particular domain (e.g. message wire) and we want to use the parser on a new domain (e.g. email). We have also tried to perform failure-based regulation (Srivastaal, but no significant improvement in 2014)."}, {"heading": "2.5 Mapper-Parser Thresholds", "text": "Certain words in the parser training data T are very rare, which can lead to inferior task-specific embeddings learned by the parser. We want our mapper function to be learned on high-quality embeddings trained by the task. Once we have learned a strong mapping function, we can use it to recalculate the inferior embeddings trained by the task. Therefore, we will consider several frequency thresholds that determine which word embeddings should be used to train the mapper and which to map at test times. In the following, the specific thresholds that we consider: Mapper Training Threshold (\u03c4m) The mapper is trained only on embedding pairs for words that can be seen at least once in the training data T."}, {"heading": "3 Related Work", "text": "The most common approach to solving invisible neural networks is to replace them with a specific, unknown word (S\u00f8gaard and Johannsen, 2012; Chen and Manning, 2014; Collobert et al., 2011). The representation of the unknown symbol is either learned specifically or calculated from a selection of rare words, for example by analyzing their embedding vocabulary. This approach could be problematic as all invisible words are assigned to the same vector, regardless of their syntactic or semantic category.There are several threads of previous work using invisible word forms. Some combine unmonitored morphological capabilities with compositional neural network architectures (Luong et al., 2013; Botha and Blunsom, 2014). (2015) and Ballesteros et al. (2015) we use long-term memory recurrent neural networks to embed character sequences."}, {"heading": "4 Experimental Setup", "text": "In this section, we describe our primary parsing setup, embedding and data sets. We also describe the setup for a downstream task: mood analysis based on a neural network model based on dependency trees. Finally, we discuss the settings for the mapper."}, {"heading": "4.1 Dependency Parser", "text": "In all our experiments, we use the default configuration for the neural network parser and hyperparameter (unless otherwise stated). For evaluation, we calculate the percentage of words that get the correct header, specifying both the unmarked attachment score (UAS) and the marked attachment score (LAS). LAS additionally requires the correct marking of the predicted dependence. To measure statistical significance, we use a bootstrap test (Efron and Tibshirani, 1986) with 100,000 samples."}, {"heading": "4.2 Pre-Trained Word Embeddings", "text": "We use the 100-dimensional GloVe word embeddings from Pennington et al. (2014), which were trained on Wikipedia 2014 and the Gigaword v5 corpus and have a vocabulary of about 400.000.1 We have also experimented with the downloadable 50-dimensional SENNA embeddings from Collobert et al. (2011) and with self-trained word2vec embeddings (Mikolov et al., 2013); in preliminary tests, the GloVe embeddings cut off best, so that we use them for all experiments.1 http: / / www-nlp.stanford.edu / data / glove.6B.100d.txt.gz"}, {"heading": "4.3 Datasets", "text": "We look at a number of datasets with different rates of OOTV words."}, {"heading": "4.4 Mapper Settings and Hyperparameters", "text": "The first embedding given to the mapper is the same as the first embedding given to the parser. These are the aforementioned 100-dimensional GloVe embedding. The output dimension of the mapper is also set to 100. All model parameters of the mapper are initialized to zero. We set the dimensionality of the nonlinear layer in all experiments to 400. The model parameters are optimized by maximizing the weighted multiple-loss lens using L-BFGS with elastic mesh regulation (section 2). Hyper parameters include the relative weight of the two objective terms (\u03b1) and the regulation constants (\u03bb1, \u03bb2). For \u03b1, we look for values in {0, 0.1, 0.2,...,... 1}. For each of these hyperparameters, we consider values in {10 \u2212 1, 10 \u2212 2,.. 10, U9 \u2212 0, the evolution of the hyperparameters will be adjusted by the grid."}, {"heading": "5 Results and Analysis", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "5.1 Results on WSJ, OntoNotes, and Switchboard", "text": "The top half of Table 1 shows our most important test results on WSJ, OntoNotes and Switchboard, the low OOTV rates. Due to the low initial OOTV rates (< 3%), we see only modest increases of 0.3-0.4% in the UAS with statistical significance at p < 0.05 for WSJ and OntoNotes and p < 0.07 for Switchboard. The initial OOTV rates are halved by our mapper, with the remaining unknown words largely being numerical strings and spelling errors. 3 Looking only at test sets containing OOTV words (the line called \"OTV subset\"), the gains are significantly greater (0.5-0.8% UAS at p < 0.05).3 We could potentially exercise the initial insertions on a larger corpus or use hay ridges to migrate into our original numbers."}, {"heading": "5.2 Results on Web Treebank", "text": "The bottom half of Table 1 shows our most important test results on the five domains of the Web Treebank, the high OOTV rate datasets. As expected, the mapper has a much greater impact in parsing these out-of-domain datasets with high OOTV word rates. 4The decrease in OOTV rate is much greater than in the WSJ datasets, and the parsing improvements (UAS and LAS) are statistically significant at p < 0.05. For subsets containing at least one OTV word (which also has an initial embedding), we see an average gain of 1.14% UAS (see the line called \"OOTV subset\"). In this case, all improvements are statistically significant at p < 0.02. We observe that the relative decrease in OOTV% in the Web Treebanks is greater than in the WSJ, Nottoboard or Switch datasets."}, {"heading": "5.3 Downstream Results", "text": "We are now reporting on the results using the Dependency TreeLSTM of Tai et al. (2015) for mood analysis on the Stanford Sentiment Treebank. We are looking at both the binary (positive / negative) and fine-grained classification tasks ({very negative, negative, neutral, positive and very positive}). We are using the implementation of Tai et al. (2015), changing only the dependence spares that are fed into their model. The mood dataset contains approximately 25% OOTV words in the training vocabulary, 5% in the development vocabulary and 9% in the test vocabulary. We are mapping invisible words using the mapper matched to the WSJ development set. We are using the same experimental settings of the Dependency Tree-LSTM as Tai et al. The results are presented in Table 2. We are improving the original accuracies in both the binary and the fine-grained classification. We are reducing the OTV rate training from 25% to 4% in the ocabulary and 4% in the onlinear range."}, {"heading": "5.4 Effect of Thresholds", "text": "We also experimented with different values for the thresholds described in Section 2. For the mapping threshold \u03c4m, the mapper training threshold \u03c4t and the parser threshold \u03c4p we consider the following four settings: t1: \u03c4m = \u03c4t = \u03c4p = 1t3: \u03c4m = \u03c4t = \u03c4p = 3t5: \u03c4m = \u03c4t = 5t \u221e: \u03c4m = \u221e, \u03c4p = \u03c4t = 5Using \u03c4m = \u221e corresponds to the mapping of all words at the test date, even words that we have seen many times in the training data and learned task-specific embedding. We report on the average development level UAS across all web treadmill domains in Table 3. We see that t3 performs best, although the settings t1 and t5 also improve above the baseline. At threshold t3 we have ap-approximately 20,000 examples for training the mapper, while at threshold t5 we have only about 10,000 examples."}, {"heading": "5.5 Effect of Weighted Multi-Loss Objective", "text": "We analyzed the results by varying \u03b1, which established an equilibrium between the two components of the multi-loss objective function of the mapper. We found that the best results were achieved in all areas except for answers with some \u03b1 between 0 and 1. The optimal values exceeded the cases with \u03b1 = 0 and \u03b1 = 1 by 0.1-0.3% UAS absolute. In the response domain, however, the best performance was achieved with \u03b1 = 0; i.e. the mapper preferred mean square errors. In other domains, the optimal \u03b1 tended to be in the range [0.3, 0.7]."}, {"heading": "5.6 Comparison with Related Work", "text": "We compare the approach presented by Tafforeau et al. (2015). They propose to refine embedding for invisible words based on the relative shifts of their closest neighbors into the original embedding space. Specifically, they define \"artificial refinement\" as: \u03c6r (t) = \u03c6o (t) + K \u2211 k = 1\u03b1k (\u03c6r (nk) \u2212 \u03c6o (nk)) (3), where \u03c6r (.) is the vector in the refined embedding space and \u03c6o (.) is the vector in the original embedding space. They define \u03b1k as proportional to the cosmic similarity between the invisible target word (t) and the neighbor (nk): \u03b1k = s (t, nk) = \u03c6o (t).\u03c6o (nk) | \u0445 (t) | | \u03c6o (nk) | Table 4 shows the average performance of the models over the development sets of the Web Treebank. On average, our approach exceeds Nk (nk)."}, {"heading": "5.7 Dependency Parsing Examples", "text": "In Figure 2, we show two sentences: one case in which the mapper helps, and another in which the mapper impairs parsing performance.5 In the first sentence (Figure 2a), the parsing model did not see the word \"tried\" during the training. Note that the sentence contains three verbs: \"tried,\" \"adopted,\" and \"was.\" Even with the POS tags, the parser could not get the correct dependency linkage. After mapping, the parser correctly \"tries\" the root and gets the correct arcs and tree. The three closest neighbors of \"tried\" in the mapped embedding space \"try,\" 5sentences in Figure 2 come from the development part of the Answers domain from the Web Treebank.and \"try.\" Again, we see that a single invisible word leads to multiple errors in the parse.In the second example (Figure 2b), the standard model shows the correct arcs in the Web Treebank.and \"try.\""}, {"heading": "5.8 Analyzing Mapped Representations", "text": "In order to understand the mapped embedding space, we use t-SNE (Van der Maaten and Hinton, 2008) to visualize a small subset of embeddings. In Figure 3, we draw the initial embeddings, the parser-trained embeddings, and finally the mapped embeddings. We include four invisible words (represented in caps): \"terrible,\" \"poor,\" \"wonderful,\" and \"great.\" In Figure 3a and Figure 3b, the embeddings for the invisible words are identical (even if t-SNE placed them in different places when creating its projection). In Figure 3c, we find that the mapper has placed the invisible words in appropriate areas of the space in relation to the similarity with the words seen."}, {"heading": "6 Conclusion", "text": "We have described a simple way to resolve invisible words by training supervised models that learn task-specific word embeddings: a feedback neural network that maps initial embeddings to task-specific embeddings. We showed significant improvements in the accuracy of dependency analysis across multiple areas, as well as improvements in a downstream task. Our approach is simple, effective, and applicable to many other settings, both inside and outside of NLP."}], "references": [{"title": "Handling unknown words in statistical latent-variable parsing models for arabic, english and french", "author": ["Mohammed Attia", "Jennifer Foster", "Deirdre Hogan", "Joseph Le Roux", "Lamia Tounsi", "Josef Van Genabith."], "venue": "Proceedings of the NAACL HLT 2010 First", "citeRegEx": "Attia et al\\.,? 2010", "shortCiteRegEx": "Attia et al\\.", "year": 2010}, {"title": "Improved transition-based parsing by modeling characters instead of words with lstms", "author": ["Miguel Ballesteros", "Chris Dyer", "Noah A. Smith."], "venue": "Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 349\u2013359,", "citeRegEx": "Ballesteros et al\\.,? 2015", "shortCiteRegEx": "Ballesteros et al\\.", "year": 2015}, {"title": "Web-scale features for full-scale parsing", "author": ["Mohit Bansal", "Dan Klein."], "venue": "Proceedings of ACL.", "citeRegEx": "Bansal and Klein.,? 2011", "shortCiteRegEx": "Bansal and Klein.", "year": 2011}, {"title": "Tailoring continuous word representations for dependency parsing", "author": ["Mohit Bansal", "Kevin Gimpel", "Karen Livescu."], "venue": "Proceedings of ACL.", "citeRegEx": "Bansal et al\\.,? 2014", "shortCiteRegEx": "Bansal et al\\.", "year": 2014}, {"title": "Compositional morphology for word representations and language modelling", "author": ["Jan A. Botha", "Phil Blunsom."], "venue": "International Conference on Machine Learning (ICML).", "citeRegEx": "Botha and Blunsom.,? 2014", "shortCiteRegEx": "Botha and Blunsom.", "year": 2014}, {"title": "The NXT-format Switchboard Corpus: A rich resource for investigating the syntax, semantics, pragmatics and prosody of dialogue", "author": ["Sasha Calhoun", "Jean Carletta", "Jason M. Brenier", "Neil Mayo", "Dan Jurafsky", "Mark Steedman", "David Beaver."], "venue": "Lang.", "citeRegEx": "Calhoun et al\\.,? 2010", "shortCiteRegEx": "Calhoun et al\\.", "year": 2010}, {"title": "Improving generative statistical parsing with semi-supervised word clustering", "author": ["Marie Candito", "Beno\u0131\u0302t Crabb\u00e9"], "venue": "In Proceedings of the 11th International Conference on Parsing Technologies,", "citeRegEx": "Candito and Crabb\u00e9.,? \\Q2009\\E", "shortCiteRegEx": "Candito and Crabb\u00e9.", "year": 2009}, {"title": "Edit detection and parsing for transcribed speech", "author": ["Eugene Charniak", "Mark Johnson."], "venue": "Proceedings of the Second Conference of the North American chapter of the Association for Computational Linguistics (NAACL \u201901).", "citeRegEx": "Charniak and Johnson.,? 2001", "shortCiteRegEx": "Charniak and Johnson.", "year": 2001}, {"title": "A fast and accurate dependency parser using neural networks", "author": ["Danqi Chen", "Christopher D Manning."], "venue": "Empirical Methods in Natural Language Processing (EMNLP).", "citeRegEx": "Chen and Manning.,? 2014", "shortCiteRegEx": "Chen and Manning.", "year": 2014}, {"title": "Variable-length word encodings for neural translation models", "author": ["Rohan Chitnis", "John DeNero."], "venue": "Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 2088\u2013 2093, Lisbon, Portugal, September. Association for", "citeRegEx": "Chitnis and DeNero.,? 2015", "shortCiteRegEx": "Chitnis and DeNero.", "year": 2015}, {"title": "Bootstrap methods for", "author": ["B. Efron", "R. Tibshirani"], "venue": null, "citeRegEx": "Efron and Tibshirani.,? \\Q1986\\E", "shortCiteRegEx": "Efron and Tibshirani.", "year": 1986}, {"title": "Remoov: A tool for online", "author": ["20\u201325. Nizar Habash"], "venue": null, "citeRegEx": "Habash.,? \\Q2009\\E", "shortCiteRegEx": "Habash.", "year": 2009}, {"title": "On using very large target", "author": ["Yoshua Bengio"], "venue": null, "citeRegEx": "Bengio.,? \\Q2015\\E", "shortCiteRegEx": "Bengio.", "year": 2015}, {"title": "A convolutional neural network for modelling sentences", "author": ["Nal Kalchbrenner", "Edward Grefenstette", "Phil Blunsom."], "venue": "Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 655\u2013665, Balti-", "citeRegEx": "Kalchbrenner et al\\.,? 2014", "shortCiteRegEx": "Kalchbrenner et al\\.", "year": 2014}, {"title": "Using the web to obtain frequencies for unseen bigrams", "author": ["Frank Keller", "Mirella Lapata."], "venue": "Computational linguistics, 29(3):459\u2013484.", "citeRegEx": "Keller and Lapata.,? 2003", "shortCiteRegEx": "Keller and Lapata.", "year": 2003}, {"title": "Character-aware neural language models", "author": ["Yoon Kim", "Yacine Jernite", "David Sontag", "Alexander M. Rush."], "venue": "CoRR, abs/1508.06615.", "citeRegEx": "Kim et al\\.,? 2015", "shortCiteRegEx": "Kim et al\\.", "year": 2015}, {"title": "Convolutional neural networks for sentence classification", "author": ["Yoon Kim."], "venue": "Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1746\u20131751, Doha, Qatar, October. Association for Computational Linguistics.", "citeRegEx": "Kim.,? 2014", "shortCiteRegEx": "Kim.", "year": 2014}, {"title": "Skip-thought vectors", "author": ["Ryan Kiros", "Yukun Zhu", "Ruslan Salakhutdinov", "Richard S Zemel", "Antonio Torralba", "Raquel Urtasun", "Sanja Fidler."], "venue": "arXiv preprint arXiv:1506.06726.", "citeRegEx": "Kiros et al\\.,? 2015", "shortCiteRegEx": "Kiros et al\\.", "year": 2015}, {"title": "A dependency parser for tweets", "author": ["Lingpeng Kong", "Nathan Schneider", "Swabha Swayamdipta", "Archna Bhatia", "Chris Dyer", "Noah A. Smith."], "venue": "Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing", "citeRegEx": "Kong et al\\.,? 2014", "shortCiteRegEx": "Kong et al\\.", "year": 2014}, {"title": "Simple semisupervised dependency parsing", "author": ["Terry Koo", "Xavier Carreras", "Michael Collins."], "venue": "Proceedings of ACL08: HLT, page 595603, Columbus, Ohio, June. Association for Computational Linguistics.", "citeRegEx": "Koo et al\\.,? 2008", "shortCiteRegEx": "Koo et al\\.", "year": 2008}, {"title": "Non-lexical neural architecture for fine-grained pos tagging", "author": ["Matthieu Labeau", "Kevin L\u00f6ser", "Alexandre Allauzen."], "venue": "Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 232\u2013237, Lisbon, Portugal, September. As-", "citeRegEx": "Labeau et al\\.,? 2015", "shortCiteRegEx": "Labeau et al\\.", "year": 2015}, {"title": "Finding function in form: Compositional character models for open vocabulary word representation", "author": ["Wang Ling", "Tiago Lu\u0131\u0301s", "Lu\u0131\u0301s Marujo", "R\u00e1mon Fernandez Astudillo", "Silvio Amir", "Chris Dyer", "Alan W Black", "Isabel Trancoso"], "venue": "In Proc. of EMNLP", "citeRegEx": "Ling et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Ling et al\\.", "year": 2015}, {"title": "On the limited memory BFGS method for large scale optimization", "author": ["D.C. Liu", "J. Nocedal."], "venue": "Math. Programming, 45(3, (Ser. B)):503\u2013528.", "citeRegEx": "Liu and Nocedal.,? 1989", "shortCiteRegEx": "Liu and Nocedal.", "year": 1989}, {"title": "Better word representations with recursive neural networks for morphology", "author": ["Thang Luong", "Richard Socher", "Christopher Manning."], "venue": "Proceedings of the Seventeenth Conference on Computational Natural Language Learning, pages 104\u2013113, Sofia, Bul-", "citeRegEx": "Luong et al\\.,? 2013", "shortCiteRegEx": "Luong et al\\.", "year": 2013}, {"title": "Addressing the rare word problem in neural machine translation", "author": ["Thang Luong", "Ilya Sutskever", "Quoc Le", "Oriol Vinyals", "Wojciech Zaremba."], "venue": "Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th Inter-", "citeRegEx": "Luong et al\\.,? 2015", "shortCiteRegEx": "Luong et al\\.", "year": 2015}, {"title": "Building a large annotated corpus of english: The penn treebank", "author": ["Mitchell P Marcus", "Mary Ann Marcinkiewicz", "Beatrice Santorini."], "venue": "Computational linguistics, 19(2):313\u2013330.", "citeRegEx": "Marcus et al\\.,? 1993", "shortCiteRegEx": "Marcus et al\\.", "year": 1993}, {"title": "Improving arabic dependency parsing with lexical and inflectional morphological features", "author": ["Yuval Marton", "Nizar Habash", "Owen Rambow."], "venue": "Proceedings of the NAACL HLT 2010 First Workshop on Statistical Parsing of Morphologically-Rich Languages, pages", "citeRegEx": "Marton et al\\.,? 2010", "shortCiteRegEx": "Marton et al\\.", "year": 2010}, {"title": "Reranking and self-training for parser adaptation", "author": ["David McClosky", "Eugene Charniak", "Mark Johnson."], "venue": "Proceedings of the 21st International Conference on Computational Linguistics and the 44th annual meeting of the Association for Computational", "citeRegEx": "McClosky et al\\.,? 2006", "shortCiteRegEx": "McClosky et al\\.", "year": 2006}, {"title": "Characterizing the errors of data-driven dependency parsing models", "author": ["Ryan T McDonald", "Joakim Nivre."], "venue": "EMNLP-CoNLL, pages 122\u2013131.", "citeRegEx": "McDonald and Nivre.,? 2007", "shortCiteRegEx": "McDonald and Nivre.", "year": 2007}, {"title": "Efficient estimation of word representations in vector space", "author": ["Tomas Mikolov", "Kai Chen", "Greg Corrado", "Jeffrey Dean."], "venue": "arXiv preprint arXiv:1301.3781.", "citeRegEx": "Mikolov et al\\.,? 2013", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Reading between the lines: Overcoming data sparsity for accurate classification of lexical relationships", "author": ["Silvia Necsulescu", "Sara Mendes", "David Jurgens", "N\u00faria Bel", "Roberto Navigli."], "venue": "Proceedings of the Fourth Joint Conference on Lexical and Computa-", "citeRegEx": "Necsulescu et al\\.,? 2015", "shortCiteRegEx": "Necsulescu et al\\.", "year": 2015}, {"title": "On optimization methods for deep learning", "author": ["Jiquan Ngiam", "Adam Coates", "Ahbik Lahiri", "Bobby Prochnow", "Quoc V. Le", "Andrew Y. Ng."], "venue": "Proceed-", "citeRegEx": "Ngiam et al\\.,? 2011", "shortCiteRegEx": "Ngiam et al\\.", "year": 2011}, {"title": "The conll 2007 shared task on dependency parsing", "author": ["Jens Nilsson", "Sebastian Riedel", "Deniz Yuret."], "venue": "Proceedings of the CoNLL shared task session of EMNLP-CoNLL, pages 915\u2013932. sn.", "citeRegEx": "Nilsson et al\\.,? 2007", "shortCiteRegEx": "Nilsson et al\\.", "year": 2007}, {"title": "Global belief recursive neural networks", "author": ["Romain Paulus", "Richard Socher", "Christopher D Manning."], "venue": "Advances in Neural Information Processing Systems, pages 2888\u20132896.", "citeRegEx": "Paulus et al\\.,? 2014", "shortCiteRegEx": "Paulus et al\\.", "year": 2014}, {"title": "Glove: Global vectors for word representation", "author": ["Jeffrey Pennington", "Richard Socher", "Christopher Manning."], "venue": "Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1532\u20131543, Doha, Qatar, Octo-", "citeRegEx": "Pennington et al\\.,? 2014", "shortCiteRegEx": "Pennington et al\\.", "year": 2014}, {"title": "Overview of the 2012 shared task on parsing the web", "author": ["Slav Petrov", "Ryan McDonald."], "venue": "Notes of the First Workshop on Syntactic Analysis of Non-Canonical Language (SANCL).", "citeRegEx": "Petrov and McDonald.,? 2012", "shortCiteRegEx": "Petrov and McDonald.", "year": 2012}, {"title": "Uptraining for accurate deterministic question parsing", "author": ["Slav Petrov", "Pi-Chuan Chang", "Michael Ringgaard", "Hiyan Alshawi."], "venue": "Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 705\u2013713. Association for", "citeRegEx": "Petrov et al\\.,? 2010", "shortCiteRegEx": "Petrov et al\\.", "year": 2010}, {"title": "Big data small data, in domain out-of domain, known word unknown word: The impact of word representation on sequence labelling tasks", "author": ["Lizhen Qu", "Gabriela Ferraro", "Liyuan Zhou", "Weiwei Hou", "Nathan Schneider", "Timothy Baldwin."], "venue": "arXiv preprint", "citeRegEx": "Qu et al\\.,? 2015", "shortCiteRegEx": "Qu et al\\.", "year": 2015}, {"title": "Lemmatization and lexicalized statistical parsing of morphologically-rich languages: the case of french", "author": ["Djam\u00e9 Seddah", "Grzegorz Chrupa\u0142a", "Ozlem Cetinoglu", "Josef van Genabith", "Marie Candito."], "venue": "Proceedings of the NAACL HLT 2010 First Workshop", "citeRegEx": "Seddah et al\\.,? 2010", "shortCiteRegEx": "Seddah et al\\.", "year": 2010}, {"title": "Recursive deep models for semantic compositionality over a sentiment treebank", "author": ["Richard Socher", "Alex Perelygin", "Jean Wu", "Jason Chuang", "Christopher D. Manning", "Andrew Ng", "Christopher Potts."], "venue": "Proceedings of the 2013 Conference on Empirical Meth-", "citeRegEx": "Socher et al\\.,? 2013", "shortCiteRegEx": "Socher et al\\.", "year": 2013}, {"title": "Robust learning in random subspaces: Equipping NLP for OOV effects", "author": ["Anders S\u00f8gaard", "Anders Johannsen."], "venue": "Proceedings of COLING 2012: Posters, Mumbai, India, December.", "citeRegEx": "S\u00f8gaard and Johannsen.,? 2012", "shortCiteRegEx": "S\u00f8gaard and Johannsen.", "year": 2012}, {"title": "Dependency parsing for", "author": ["William W Cohen"], "venue": null, "citeRegEx": "Cohen.,? \\Q2014\\E", "shortCiteRegEx": "Cohen.", "year": 2014}, {"title": "Structured training for neu", "author": ["Slav Petrov"], "venue": null, "citeRegEx": "Petrov.,? \\Q2015\\E", "shortCiteRegEx": "Petrov.", "year": 2015}], "referenceMentions": [{"referenceID": 36, "context": "In many NLP tasks, state-of-the-art systems achieve very good performance, but only when restricted to standard and heavily edited datasets (Petrov et al., 2010).", "startOffset": 140, "endOffset": 161}, {"referenceID": 35, "context": "For example, while state-of-the-art accuracies exceed 97% for part-ofspeech tagging and 90% for dependency parsing, performance on non-standard, real-world datasets is substantially worse, dropping by nearly 10% absolute (Petrov and McDonald, 2012).", "startOffset": 221, "endOffset": 248}, {"referenceID": 3, "context": "Recently, continuous vector word representations, or embeddings, have shown promise in a variety of NLP tasks (Turian et al., 2010; Collobert et al., 2011; Anderson et al., 2013; Bansal et al., 2014).", "startOffset": 110, "endOffset": 199}, {"referenceID": 30, "context": "Using embeddings as features in NLP systems can help counter the effects of data sparsity (Necsulescu et al., 2015).", "startOffset": 90, "endOffset": 115}, {"referenceID": 3, "context": "However, the quality of such embeddings has been found to be heavily task-dependent (Bansal et al., 2014).", "startOffset": 84, "endOffset": 105}, {"referenceID": 13, "context": "There is a great deal of work on updating embeddings during supervised training to make them more task-specific, whether through back-propagation or other techniques (Kalchbrenner et al., 2014; Qu et al., 2015; Chen and Manning, 2014).", "startOffset": 166, "endOffset": 234}, {"referenceID": 37, "context": "There is a great deal of work on updating embeddings during supervised training to make them more task-specific, whether through back-propagation or other techniques (Kalchbrenner et al., 2014; Qu et al., 2015; Chen and Manning, 2014).", "startOffset": 166, "endOffset": 234}, {"referenceID": 8, "context": "There is a great deal of work on updating embeddings during supervised training to make them more task-specific, whether through back-propagation or other techniques (Kalchbrenner et al., 2014; Qu et al., 2015; Chen and Manning, 2014).", "startOffset": 166, "endOffset": 234}, {"referenceID": 40, "context": "In the latter case, at test time, most trained systems either fall back to some generic, single representation for all unknown words or use the initial representation (typically derived from unlabeled data) (S\u00f8gaard and Johannsen, 2012; Collobert et al., 2011).", "startOffset": 207, "endOffset": 260}, {"referenceID": 8, "context": "In this work, we use the Stanford neural dependency parser (Chen and Manning, 2014).", "startOffset": 59, "endOffset": 83}, {"referenceID": 22, "context": "For optimization, we use batch limited memory BFGS (L-BFGS) (Liu and Nocedal, 1989).", "startOffset": 60, "endOffset": 83}, {"referenceID": 31, "context": "In preliminary experiments comparing with stochastic optimization, we found L-BFGS to be more stable to train and easier to check for convergence (as has recently been found in other settings as well (Ngiam et al., 2011)).", "startOffset": 200, "endOffset": 220}, {"referenceID": 22, "context": "We use elastic net regularization (Liu and Nocedal, 1989), which linearly combines l1 and l2 penalties on the parameters to control the capacity of the mapper function.", "startOffset": 34, "endOffset": 57}, {"referenceID": 40, "context": "The most common approach to resolving unseen words is to replace them with a special unknown word token (S\u00f8gaard and Johannsen, 2012; Chen and Manning, 2014; Collobert et al., 2011).", "startOffset": 104, "endOffset": 181}, {"referenceID": 8, "context": "The most common approach to resolving unseen words is to replace them with a special unknown word token (S\u00f8gaard and Johannsen, 2012; Chen and Manning, 2014; Collobert et al., 2011).", "startOffset": 104, "endOffset": 181}, {"referenceID": 23, "context": "Some combine unsupervised morphological analysis with compositional neural network architectures (Luong et al., 2013; Botha and Blunsom, 2014).", "startOffset": 97, "endOffset": 142}, {"referenceID": 4, "context": "Some combine unsupervised morphological analysis with compositional neural network architectures (Luong et al., 2013; Botha and Blunsom, 2014).", "startOffset": 97, "endOffset": 142}, {"referenceID": 20, "context": "Others use convolutional neural networks on character streams (Labeau et al., 2015; Kim et al., 2015; Zhang and LeCun, 2015).", "startOffset": 62, "endOffset": 124}, {"referenceID": 15, "context": "Others use convolutional neural networks on character streams (Labeau et al., 2015; Kim et al., 2015; Zhang and LeCun, 2015).", "startOffset": 62, "endOffset": 124}, {"referenceID": 3, "context": ", 2013; Botha and Blunsom, 2014). Ling et al. (2015) and Ballesteros et al.", "startOffset": 8, "endOffset": 53}, {"referenceID": 1, "context": "(2015) and Ballesteros et al. (2015) use long short-term memory recurrent neural networks to embed character sequences.", "startOffset": 11, "endOffset": 37}, {"referenceID": 6, "context": "There is also a great deal of work using morphological information for rare or unseen words (Candito and Crabb\u00e9, 2009; Habash, 2009; Marton et al., 2010; Seddah et al., 2010; Attia et al., 2010).", "startOffset": 92, "endOffset": 194}, {"referenceID": 11, "context": "There is also a great deal of work using morphological information for rare or unseen words (Candito and Crabb\u00e9, 2009; Habash, 2009; Marton et al., 2010; Seddah et al., 2010; Attia et al., 2010).", "startOffset": 92, "endOffset": 194}, {"referenceID": 26, "context": "There is also a great deal of work using morphological information for rare or unseen words (Candito and Crabb\u00e9, 2009; Habash, 2009; Marton et al., 2010; Seddah et al., 2010; Attia et al., 2010).", "startOffset": 92, "endOffset": 194}, {"referenceID": 38, "context": "There is also a great deal of work using morphological information for rare or unseen words (Candito and Crabb\u00e9, 2009; Habash, 2009; Marton et al., 2010; Seddah et al., 2010; Attia et al., 2010).", "startOffset": 92, "endOffset": 194}, {"referenceID": 0, "context": "There is also a great deal of work using morphological information for rare or unseen words (Candito and Crabb\u00e9, 2009; Habash, 2009; Marton et al., 2010; Seddah et al., 2010; Attia et al., 2010).", "startOffset": 92, "endOffset": 194}, {"referenceID": 2, "context": "Other work has focused on using contextual information, such as using n-gram based sequence models or web data (Bansal and Klein, 2011).", "startOffset": 111, "endOffset": 135}, {"referenceID": 0, "context": ", 2010; Attia et al., 2010). Other work has focused on using contextual information, such as using n-gram based sequence models or web data (Bansal and Klein, 2011). Keller and Lapata (2003) use the web to obtain", "startOffset": 8, "endOffset": 191}, {"referenceID": 16, "context": "Other work has also found improvements by combining pretrained, fixed embeddings with task-trained embeddings (Kim, 2014; Paulus et al., 2014).", "startOffset": 110, "endOffset": 142}, {"referenceID": 33, "context": "Other work has also found improvements by combining pretrained, fixed embeddings with task-trained embeddings (Kim, 2014; Paulus et al., 2014).", "startOffset": 110, "endOffset": 142}, {"referenceID": 24, "context": "Also relevant are approaches developed specifically to handle large target vocabularies (including many rare words) in neural machine translation systems (Jean et al., 2015; Luong et al., 2015; Chitnis and DeNero, 2015).", "startOffset": 154, "endOffset": 219}, {"referenceID": 9, "context": "Also relevant are approaches developed specifically to handle large target vocabularies (including many rare words) in neural machine translation systems (Jean et al., 2015; Luong et al., 2015; Chitnis and DeNero, 2015).", "startOffset": 154, "endOffset": 219}, {"referenceID": 9, "context": ", 2015; Chitnis and DeNero, 2015). Closely related to our approach is that of Tafforeau et al. (2015), which also tries to map an initial, unsupervised word embedding space to the word embedding space learned during supervised training.", "startOffset": 8, "endOffset": 102}, {"referenceID": 9, "context": ", 2015; Chitnis and DeNero, 2015). Closely related to our approach is that of Tafforeau et al. (2015), which also tries to map an initial, unsupervised word embedding space to the word embedding space learned during supervised training. Their method generates updated embeddings for unseen words by combining the embeddings of their k nearest neighbors. In Section 5, we show that our approach outperforms this kNN approach. Another related technique proposed by Kiros et al. (2015) learns a linear mapping from an initial embedding space to their encoder\u2019s vocabulary space by solving an unregularized linear regression problem.", "startOffset": 8, "endOffset": 483}, {"referenceID": 28, "context": "Our work is also somewhat related to domain adaptation for dependency parsing, which has been extensively studied in recent years (McDonald and Nivre, 2007; Nilsson et al., 2007).", "startOffset": 130, "endOffset": 178}, {"referenceID": 32, "context": "Our work is also somewhat related to domain adaptation for dependency parsing, which has been extensively studied in recent years (McDonald and Nivre, 2007; Nilsson et al., 2007).", "startOffset": 130, "endOffset": 178}, {"referenceID": 19, "context": ", 2012), word distribution features (Koo et al., 2008; Bansal et al., 2014; Weiss et al., 2015), and selftraining (McClosky et al.", "startOffset": 36, "endOffset": 95}, {"referenceID": 3, "context": ", 2012), word distribution features (Koo et al., 2008; Bansal et al., 2014; Weiss et al., 2015), and selftraining (McClosky et al.", "startOffset": 36, "endOffset": 95}, {"referenceID": 27, "context": ", 2015), and selftraining (McClosky et al., 2006).", "startOffset": 26, "endOffset": 49}, {"referenceID": 10, "context": "To measure statistical significance, we use a bootstrap test (Efron and Tibshirani, 1986) with 100K samples.", "startOffset": 61, "endOffset": 89}, {"referenceID": 8, "context": "We use the feed-forward neural network dependency parser of Chen and Manning (2014). In all our experiments (unless stated otherwise), we use the default arc-standard parsing configuration and hyperparameter settings.", "startOffset": 60, "endOffset": 84}, {"referenceID": 29, "context": "(2011) and with word2vec (Mikolov et al., 2013) embeddings that we trained ourselves; in preliminary experiments the GloVe embeddings performed best, so we use them for all experiments below.", "startOffset": 25, "endOffset": 47}, {"referenceID": 33, "context": "We use the 100-dimensional GloVe word embeddings from Pennington et al. (2014). These were trained on Wikipedia 2014 and the Gigaword v5 corpus and have a vocabulary size of approximately 400,000.", "startOffset": 54, "endOffset": 79}, {"referenceID": 33, "context": "We use the 100-dimensional GloVe word embeddings from Pennington et al. (2014). These were trained on Wikipedia 2014 and the Gigaword v5 corpus and have a vocabulary size of approximately 400,000.1 We have also experimented with the downloadable 50-dimensional SENNA embeddings from Collobert et al. (2011) and with word2vec (Mikolov et al.", "startOffset": 54, "endOffset": 307}, {"referenceID": 25, "context": "Wall Street Journal (WSJ) and OntoNotesWSJ We conduct experiments on the Wall Street Journal portion of the English Penn Treebank dataset (Marcus et al., 1993).", "startOffset": 138, "endOffset": 159}, {"referenceID": 35, "context": "For example, when training a parser on newswire and testing on web data, many errors occur due to differing patterns of syntactic usage and unseen words (Foster et al., 2011; Petrov and McDonald, 2012; Kong et al., 2014; Wang et al., 2014).", "startOffset": 153, "endOffset": 239}, {"referenceID": 18, "context": "For example, when training a parser on newswire and testing on web data, many errors occur due to differing patterns of syntactic usage and unseen words (Foster et al., 2011; Petrov and McDonald, 2012; Kong et al., 2014; Wang et al., 2014).", "startOffset": 153, "endOffset": 239}, {"referenceID": 35, "context": "We explore this setting by training our parser on OntoNotes-WSJ and testing on the Web Treebank (Petrov and McDonald, 2012), which includes five domains: answers, email, newsgroups, reviews, and weblogs.", "startOffset": 96, "endOffset": 123}, {"referenceID": 5, "context": "Switchboard Speech Corpus The NXT Switchboard speech corpus (Calhoun et al., 2010) contains annotated parses of spoken telephone conversations.", "startOffset": 60, "endOffset": 82}, {"referenceID": 5, "context": "Switchboard Speech Corpus The NXT Switchboard speech corpus (Calhoun et al., 2010) contains annotated parses of spoken telephone conversations. We obtain ground truth dependencies from phrase structure trees using the Stanford converter as above, as also done by Honnibal and Johnson (2014). We perform their other preprocessing steps of lowercasing the text, removing punctuation, and removing partial utterances and one-token sentences.", "startOffset": 61, "endOffset": 291}, {"referenceID": 5, "context": "Switchboard Speech Corpus The NXT Switchboard speech corpus (Calhoun et al., 2010) contains annotated parses of spoken telephone conversations. We obtain ground truth dependencies from phrase structure trees using the Stanford converter as above, as also done by Honnibal and Johnson (2014). We perform their other preprocessing steps of lowercasing the text, removing punctuation, and removing partial utterances and one-token sentences. Since the current version of the Stanford parser cannot perform non-monotonic parsing,2 we also remove disfluent utterances in such a way that we get a purely non-disfluent speech dataset. We use the standard train/development/test splits of Charniak and Johnson (2001).", "startOffset": 61, "endOffset": 709}, {"referenceID": 39, "context": "Downstream Task: Sentiment Analysis with Dependency Tree LSTMs We also perform experiments to analyze the effects of embedding mapping on a downstream task, in this case sentiment analysis using the Stanford Sentiment Treebank (Socher et al., 2013).", "startOffset": 227, "endOffset": 248}, {"referenceID": 39, "context": "Downstream Task: Sentiment Analysis with Dependency Tree LSTMs We also perform experiments to analyze the effects of embedding mapping on a downstream task, in this case sentiment analysis using the Stanford Sentiment Treebank (Socher et al., 2013). We use the dependency tree long short-term memory network (Tree-LSTM) proposed by Tai et al. (2015), simply replacing their default dependency parser with our version that maps unseen words.", "startOffset": 228, "endOffset": 350}], "year": 2017, "abstractText": "We consider the setting in which we train a supervised model that learns task-specific word representations. We assume that we have access to some initial word representations (e.g., unsupervised embeddings), and that the supervised learning procedure updates them to task-specific representations for words contained in the training data. But what about words not contained in the supervised training data? When such unseen words are encountered at test time, they are typically represented by either their initial vectors or a single unknown vector, which often leads to errors. In this paper, we address this issue by learning to map from initial representations to task-specific ones. We present a general technique that uses a neural network mapper with a weighted multiple-loss criterion. This allows us to use the same learned model parameters at test time but now with appropriate task-specific representations for unseen words. We consider the task of dependency parsing and report improvements in performance (and reductions in out-of-vocabulary rates) across multiple domains such as news, Web, and speech. We also achieve downstream improvements on the task of parsingbased sentiment analysis.", "creator": "LaTeX with hyperref package"}}}