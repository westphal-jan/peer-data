{"id": "1510.02674", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "9-Oct-2015", "title": "Technical Report of Participation in Higgs Boson Machine Learning Challenge", "abstract": "This report entails the detailed description of the approach and methodologies taken as part of competing in the Higgs Boson Machine Learning Competition hosted by Kaggle Inc. and organized by CERN et al. It briefly describes the theoretical background of the problem and the motivation for taking part in the competition. Furthermore, the various machine learning models and algorithms analyzed and implemented during the 4 month period of participation are discussed and compared. Special attention is paid to the Deep Learning techniques and architectures implemented from scratch using Python and NumPy for this competition.", "histories": [["v1", "Fri, 9 Oct 2015 14:00:48 GMT  (197kb)", "http://arxiv.org/abs/1510.02674v1", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["s raza ahmad"], "accepted": false, "id": "1510.02674"}, "pdf": {"name": "1510.02674.pdf", "metadata": {"source": "CRF", "title": null, "authors": ["Francois Englert"], "emails": [], "sections": [{"heading": null, "text": "AbstractThis report provides a detailed description of the approach and methods chosen in the Higgs Boson Machine Learning Competition, hosted by Kaggle Inc. and organized by CERN et al. It briefly describes the theoretical background of the problem and the motivation for participating in the competition. It also discusses and compares the various models and algorithms of machine learning that were analyzed and implemented during the 4-month participation period, with particular emphasis on the deep learning techniques and architectures implemented from the ground up with Python and NumPy for this competition."}, {"heading": "Physics Background:", "text": "The discovery of the Higgs particle was announced on July 4th, 2012.In 2013, the Nobel Prize was awarded to two scientists, Francois Englert and Peter Higgs, for their contribution to its discovery.One characteristic of the Higgs boson is its decay into other particles.At the ATLAS detector at CERN, very high-energy protons are accelerated in a circular orbit in both directions, thus colliding with themselves and leading to hundreds of particles per second. These events are categorized as either background or signal events.The background events consist of the decay of particles already detected in previous experiments.The signal events are the decay of exotic particles: a region in the characteristic space that is not explained by the background processes.The significance of these signal events is analyzed using various statistical tests.If the probability that the event was not generated by a background process, it is well below a threshold, a small particle falling into the ATLAS is considered to be a particle signal.Although the ATLAS is a significant particle."}, {"heading": "Machine Learning Background:", "text": "The Higgs Boson Machine Learning Challenge started on May 14, 2014 and culminated on September 15, 2014. The objective of the challenge was to improve the techniques that generate the signal selection region. It introduced a formal objective function called the Approximation of the Median Significance (AMS), a function of the weights of selected events that took into account the unnormalized true and false positive rates. The problem is formally defined and elaborated in the technical documentation provided by the organizers and can be accessed here. http: / / higgsml.lal.in2p3.fr / documentation /"}, {"heading": "Data:", "text": "This year, it has reached the stage where it will be able to take the lead."}, {"heading": "Introduction:", "text": "Deep Learning is a new area of machine learning that attempts to model high-grade abstractions contained in the raw data to understand the highly diverse functions underlying the data and to make well-generalized predictions for invisible data. This is achieved through certain nonlinear transformations of data through different deep architectures such as neural networks. Deep Learning aims to meet the goal of true artificial intelligence and has lately been of great interest to researchers in the field of machine learning. Tech giants such as Google, Microsoft, Facebook and Baidu are investing hundreds of millions of dollars in groundbreaking deep learning research and the development of their applications. The focus of this research work has continued to be on the study and implementation of various deep learning techniques to search for high-energy exotic particles in this competition. The deep networks used were different types of deep neural networks. The motivation came in part from the work of P. Baldi al of UC Irford University of Montreal, who had recently worked on Boovine University's Higgins and the University of Montreal."}, {"heading": "Architecture:", "text": "The deep feed forward architecture designed for the competition was a deep neural network with a total of 5 layers, 4 hidden and 1 output; the output layer consisted of one, while each hidden layer consisted of 300 logistics units; all units used the sigmoid activation function; linear and tanh activation functions were also tried out, but produced sufficiently good results; in addition to the prime architecture, many other deep and flat neural networks were designed and compared, consisting of networks with 2, 3, 4 and 6 layers with different numbers of units in each layer."}, {"heading": "Implementation:", "text": "Implementation took place in Python using NumPy and SciPy open source libraries and ran on a distributed cluster of 12 nodes under Red Hat Enterprise Linux with Xeon processors and 64 GB of memory (Rustam3). Parameter optimization was performed using Stochastic Gradient Descent with mini-batches of size 50. Initial learning rate and impulse were set to 0.05 and 0.9, respectively. Training ended when the number of epochs reached a maximum of 500 and the minimum error in a 20% held validation rate did not decrease by a factor of 0.001 over the last 30 epochs. Learning rate decreased by a factor of 0.0005 per epoch. Momentum increased linearly in the first 100 epochs from 0.9 to 0.99 and then remained constant. RMSProp technology was used with a beta value of 0.9. Weights were varied by a gauge of zero with a mean value of 0.0000s in the remaining 0.0000s and 0.0000s in the remaining layers."}, {"heading": "Results:", "text": "The limited implementation time meant that the results were not quite as good as with the Gradient Boosting Classifier. The scripts ran for about 7-10 days and the best accuracy achieved was 83% and the AMS 2.1. This was accomplished by averaging different types of depth models. Conclusion In the wake of entering the competition, a hands-on experience with machine learning in a real-world competitive environment was achieved, which would certainly prove helpful for subsequent work. There is a need to learn more about different types of learning models and their underlying theoretical background."}, {"heading": "Important Considerations:", "text": "The following are just a few things to keep in mind for future research endeavors: 1 A month is usually not enough time to implement a project. Implementation should begin at the very beginning and literature review should be done side by side. 2 A methodological approach should be taken to try to keep track of the scope and limitations of the project during implementation. 3 An appropriate plan should be worked out on how the code should be implemented and whether all libraries should be applied after each update. 4 A full data set should be maintained for each implementation in an automated manner."}], "references": [{"title": "Searching for Exotic Particles in High-energy Physics with Deep Learning.", "author": ["P. Baldi", "P. Sadowski", "D. Whiteson"], "venue": "Nature Communications", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2014}, {"title": "Reducing the dimensionality of data with neural  networks.", "author": ["G.E. Hinton", "R. Salakhutdinov"], "venue": "Science, Vol. 313", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2006}, {"title": "Learning Deep Architectures for AI", "author": ["Yoshua Bengio"], "venue": "Foundations and Trends in Machine Learning,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2009}], "referenceMentions": [], "year": 2014, "abstractText": "3 Introduction 4 Initial Implementation 6 Deep Learning Implementation 8 Conclusion 11 Appendix 12 References 14", "creator": "Writer"}}}