{"id": "1510.02675", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "9-Oct-2015", "title": "Controlled Experiments for Word Embeddings", "abstract": "An experimental approach to studying the properties of word embeddings is proposed. Controlled experiments, achieved through modifications of the training corpus, permit the demonstration of direct relations between word properties and word vector direction and length. The approach is demonstrated using the word2vec CBOW model with experiments that independently vary word frequency and word co-occurrence noise. The experiments reveal that word vector length depends more or less linearly on both word frequency and the level of noise in the co-occurrence distribution of the word. The coefficients of linearity depend upon the word. The special point in feature space, defined by the (artificial) word with pure noise in its co-occurrence distribution, is found to be small but non-zero.", "histories": [["v1", "Fri, 9 Oct 2015 14:03:33 GMT  (166kb)", "https://arxiv.org/abs/1510.02675v1", "15 pages"], ["v2", "Mon, 14 Dec 2015 05:10:29 GMT  (159kb)", "http://arxiv.org/abs/1510.02675v2", "Chagelog: Rerun experiment with subsampling turned off; re-interpreted results in light of Schnabel et al. (2015). 15 pages"]], "COMMENTS": "15 pages", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["benjamin j wilson", "adriaan m j schakel"], "accepted": false, "id": "1510.02675"}, "pdf": {"name": "1510.02675.pdf", "metadata": {"source": "CRF", "title": null, "authors": [], "emails": ["benjamin@lateral.io", "adriaan.schakel@gmail.com"], "sections": [{"heading": null, "text": "ar Xiv: 151 0.02 675v 2 [cs.C L] 14 An experimental approach to studying the properties of word embedding is proposed. Controlled experiments, achieved by modifications of the training corpus, allow the detection of direct relationships between word properties and word vector direction and word length. The approach is demonstrated using the word2vec CBOW model with experiments that vary independently word frequency and word side-by-side noise. Experiments show that word vector length is more or less linear depending on both word frequency and noise level in the parallelism distribution of the word. Coefficients of linearity depend on the word. The particular point in the feature space defined by the (artificial) word with pure noise in its parallelism distribution proves to be small but not zero."}, {"heading": "1 Introduction", "text": "This year is the highest in the history of the country."}, {"heading": "2 Related work", "text": "Our experimental finding that word vector length decreases with the simultaneous occurrence of noise is related to previous work by Vecchi, Baroni, and Zamparelli [11], which empirically examined a relationship between vector length and the \"semantic deviation\" of an adjective-noun composite. In this paper, which is also based on the coexistence statistics of the word, the authors examined adjective and noun compositions. They have constructed a vocabulary of the most common 8k nouns and 4k adjectives in a large general language corpus and have added 22k adjective nouns compositions. For each element in the vocabulary, they have described the co-occurrences with the most common words (nouns, adjectives, or verbs), with the word embeddings composed by the composition of the compositors described as positive."}, {"heading": "3 Corpus and model", "text": "Our training data is based on the Wikipedia database from October 2013. To remove the majority of the robot-generated pages from the training data, only pages with at least 20 monthly page views are stored. 2 stubs and ambiguity pages are also removed, leaving 463 thousand pages with a total of 482 million words. Punctuation marks and numbers have been removed from the pages, and all words have been lowered. Word frequencies are summarized in Table 1. This basic corpus is then modified as described in Sections 4 and 5."}, {"heading": "3.1 Word2vec", "text": "Word2vec, a forward-facing neural network with a single hidden layer, learns word vectors from word coincidences in an unattended manner. Word2vec comes in two versions. In the Continuous Bagof Words (CBOW) model, the words that appear around a target word serve as input, which is projected linearly onto the hidden layer and the network then tries to predict the target word on output. Training is typically done by backpropagation. Word vectors are encoded in the weights of the first synaptic layer \"syn0.\" The weights of the second synaptic layer (\"syn1neg,\" in the case of negative sampling) are typically discarded. In the other model, the so-called Skip-gram, target and context words are interchanged so that the target word now serves as input, while the network tries to predict the context words on output."}, {"heading": "3.2 Replacement procedure", "text": "In the experiments listed below, we modify the corpus in a controlled manner by introducing pseudo-words into the corpus using a replacement method. In the frequency experiment, the procedure is as follows: Consider a word, say cat. For each occurrence of this word, a sample i, 1 6 i 6 n is taken from an abbreviated geometric distribution, and the occurrence of the word cat is replaced by the pseudo-word CAT i. In this way, the word cat is replaced throughout the corpus by a family of pseudo-words with different frequencies, but roughly the same co-occurrence distribution as cat. That is, all of these pseudo-words are used in roughly the same contexts as the original word.2For further justification and to obtain the dataset, see https: / / blog.lateral.io / 2015 / 06 / die-unbekannt-perils-of-wikipedia / SV3revision-lev2googoo.com / http: / / 4good.com."}, {"heading": "4 Varying word frequency", "text": "In this first experiment, we examine the effects of word frequency on word embedding. By means of the replacement procedure, we introduce a small number of pseudo-word families into the corpus. Pseudo-words in each family vary in frequency, but when we replace a single word, all have a common distribution of occurrence. This allows us to examine the role of word frequency in isolation, while everything else remains the same. We look at two types of pseudo-words."}, {"heading": "4.1 Pseudowords derived from existing words", "text": "To ensure that the inserted pseudo-words do not have too low a frequency, only words that occur at least ten thousand times are selected. We also include the high-frequency stop word for comparison. Table 2 lists the words selected for this experiment along with their frequencies. Section 3.2's replacement procedure is then performed for each of these words, using a geometric decay rate of p = 12 and a maximum value of n = 20, so that the first pseudo-word is inserted with an probability of about 0.5, the second with a probability of about 0.25, and so on. This value of p is one of values that ensure that for each word several pseudo-words are inserted with a frequency sufficient to survive the low-frequency abbreviation of 128. A maximum value of n = 20 is sufficient for this p-choice, since 220 + log2 128 is the word that exceeds the frequency of all the pseudo-word images in the sample."}, {"heading": "4.2 Pseudowords derived from an artificial, meaningless word", "text": "While the mainly introduced pseudo-words replace an existing word that carries a meaning, we now add a high-frequency, meaningless word for comparison. We select an artificial, completely meaningless word VOID into the corpus instead of choosing an existing (stop) word whose meaninglessness is assumed only. To achieve this, we randomly inject the word into the whole corpus so that its relative frequency is 0.005. Thus, the simultaneous occurrence distribution of VOID coincides with the unconditional word distribution. The replacement procedure is then performed for this word, using the same values for p and n as above. Figure 2 shows the effect of these modifications on an example text, where a higher relative frequency of 0.05 is used instead for illustrative purposes."}, {"heading": "4.3 Experimental results", "text": "Next, we present the results of the word frequency experiment. We consider the effect of the word frequency on the direction and length of word vectors separately."}, {"heading": "4.3.1 Word frequency and vector direction", "text": "In fact, the number of pseudo-words associated with an experiment is so high that their frequency can be halved and that they are above the low-frequency average of 128. First, consider the vectors for the pseudo-words associated with the word."}, {"heading": "4.3.2 Word frequency and vector length", "text": "Next, we look at the effect of frequency on the length of the word vector. Furthermore, we measure the length of the vector according to the Euclidean norm. Figure 4 shows this relationship for individual words, both for the word vectors represented by the weights of the first synaptic layer syn0, in the word2vec neural network, and for the vectors represented by the weights of the second synaptic layer syn1neg. We include the latter, which are normally ignored, for the sake of completeness. Each line corresponds to a single word, and the dots on each line indicate the frequency and vector length of the pseudo-words derived from that word... The six dots on the line corresponding to the word Protestant are denoted from right to left by the pseudo-words PROTESTANT 1, PROTESTANT 2, and the dots on each line indicate the frequency and vector length of the pseudo-words."}, {"heading": "5 Varying co-occurrence noise", "text": "This second experiment is complementary to the first. Whereas in the first experiment we investigated the effect of word frequency on word vectors for fixed coexistence, here we are investigating the effect of the simultaneous occurrence of noise at fixed frequency. As before, we are doing this in a controlled manner."}, {"heading": "5.1 Generating noise", "text": "Sounds can then be added to the simultaneous distribution of a word by simply scattering occurrences of that word. \u2212 As in Section 4, we select a small number of words from the unchanged corpus for this experiment. Table 3 lists the words to be selected along with their frequencies in the corpus. For each of these words, the substitution procedure of Section 3.2 is performed using the distribution (2) with n = 7. For each substitution word pseudoword (e.g. pseudoword i), additional occurrences of that pseudoword along with their frequencies are listed in the corpus."}, {"heading": "5.2 Experimental results", "text": "Figure 6 shows the cosinal similarity of vector pairs representing some of the pseudo-words used in this experiment. Remember that the first pseudo-word (i = 1) in a family is noise-free in its coexistence distribution, while the last (i = n, with n = 7) stands for pure noise and thus no longer relates to the word from which it originates. The figure shows that the vectors within a family differ only moderately from the original direction defined by the first pseudo-word (i = 1) when noise is added to the coexistence distribution. For 1 < i < 7, the deviation typically increases with the proportion of noise. The vector of the last pseudo-word (i = n) associated with pure noise is seen within each of the families to point in a completely different direction to the coefficient distribution, with the vector facing in a different direction, more or less of the original vector."}, {"heading": "6 Discussion", "text": "Our principal contribution was to show that controlled experiments can be used to gain insights into the embedding of a word. These experiments can be performed for any word embedding (or language model), as they are only achieved by modifying the training corpus. They do not require knowledge of the implementation of the model. Of course, it would be of interest to perform these experiments for word embedding other than word2vec CBOW, such as skipgrams and GloVe, as well as for various hyperparameter settings. More elaborate experiments could be carried out, for example by introducing pseudo-words into the corpus that mix with different proportions, the co-occurrence distributions of two words, the path between the word vectors in the attribute space could be investigated."}, {"heading": "7 Acknowledgments", "text": "The authors thank Tobias Schnabel for the helpful conversations."}], "references": [{"title": "Random walks on context spaces: Towards an explanation of the mysteries of semantic word embeddings", "author": ["Sanjeev Arora", "Yuanzhi Li", "Yingyu Liang", "Tengyu Ma", "Andrej Risteski"], "venue": "CoRR, abs/1502.03520,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2015}, {"title": "Don\u2019t count, predict! a systematic comparison of context-counting vs. context-predicting semantic vectors. In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)", "author": ["Marco Baroni", "Georgiana Dinu", "Germ\u00e1n Kruszewski"], "venue": null, "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2014}, {"title": "Natural language processing (almost) from scratch", "author": ["Ronan Collobert", "Jason Weston", "L\u00e9on Bottou", "Michael Karlen", "Koray Kavukcuoglu", "Pavel Kuksa"], "venue": "J. Mach. Learn. Res.,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2011}, {"title": "Work on statistical methods for word sense disambiguation", "author": ["William A Gale", "Kenneth W Church", "David Yarowsky"], "venue": "In Working Notes of the AAAI Fall Symposium on Probabilistic Approaches to Natural Language,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 1992}, {"title": "A solution to platos problem: The latent semantic analysis theory of acquisition, induction, and representation of knowledge", "author": ["Thomas K Landauer", "Susan T. Dutnais"], "venue": "PSYCHOLOGICAL REVIEW,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 1997}, {"title": "Efficient estimation of word representations in vector space", "author": ["Tomas Mikolov", "Kai Chen", "Greg Corrado", "Jeffrey Dean"], "venue": "CoRR, abs/1301.3781,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2013}, {"title": "Distributed representations of words and phrases and their compositionality", "author": ["Tomas Mikolov", "Ilya Sutskever", "Kai Chen", "Greg Corrado", "Jeffrey Dean"], "venue": "CoRR, abs/1310.4546,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2013}, {"title": "Glove: Global vectors for word representation", "author": ["Jeffrey Pennington", "Richard Socher", "Christopher D Manning"], "venue": "Proceedings of the Empiricial Methods in Natural Language Processing (EMNLP 2014),", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2014}, {"title": "Measuring word significance using distributed representations of words", "author": ["Adriaan M.J. Schakel", "Benjamin J. Wilson"], "venue": null, "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2015}, {"title": "Evaluation methods for unsupervised word embeddings", "author": ["Tobias Schnabel", "Igor Labutov", "David Mimno", "Thorsten Joachims"], "venue": "In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2015}, {"title": "Linear) Maps of the Impossible: Capturing Semantic Anomalies in Distributional Space", "author": ["Eva M. Vecchi", "Marco Baroni", "Roberto Zamparelli"], "venue": "In Proceedings of the Workshop on Distributional Semantics and Compositionality,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2011}], "referenceMentions": [{"referenceID": 1, "context": "Most applications of word embeddings explore not the word vectors themselves, but relations between them to solve, for example, similarity and word relation tasks [2].", "startOffset": 163, "endOffset": 166}, {"referenceID": 8, "context": "In a previous paper [9], we proposed the use of word vector length as measure of word significance.", "startOffset": 20, "endOffset": 23}, {"referenceID": 6, "context": "Although we present our approach using the word2vec CBOW model, these and related experiments could equally well be carried out for other word embedding methods such as the word2vec skip-gram model [7, 6], GloVe [8], and SENNA [3].", "startOffset": 198, "endOffset": 204}, {"referenceID": 5, "context": "Although we present our approach using the word2vec CBOW model, these and related experiments could equally well be carried out for other word embedding methods such as the word2vec skip-gram model [7, 6], GloVe [8], and SENNA [3].", "startOffset": 198, "endOffset": 204}, {"referenceID": 7, "context": "Although we present our approach using the word2vec CBOW model, these and related experiments could equally well be carried out for other word embedding methods such as the word2vec skip-gram model [7, 6], GloVe [8], and SENNA [3].", "startOffset": 212, "endOffset": 215}, {"referenceID": 2, "context": "Although we present our approach using the word2vec CBOW model, these and related experiments could equally well be carried out for other word embedding methods such as the word2vec skip-gram model [7, 6], GloVe [8], and SENNA [3].", "startOffset": 227, "endOffset": 230}, {"referenceID": 10, "context": "Our experimental finding that word vector length decreases with co-occurrence noise is related to earlier work by Vecchi, Baroni, and Zamparelli [11], where a relation between vector length and the \u201csemantic deviance\u201d of an adjective-noun composite was studied empirically.", "startOffset": 145, "endOffset": 149}, {"referenceID": 4, "context": "For each item in the vocabulary, they recorded the co-occurrences with the top 10k most frequent content words (nouns, adjectives or verbs), and constructed word embeddings via singular value decomposition of the co-occurrence matrix [5].", "startOffset": 234, "endOffset": 237}, {"referenceID": 3, "context": "We refer to these tokens as pseudowords, since their properties are modeled upon words in the lexicon and because our corpus modification approach is reminiscent of the pseudoword approach for generating labeled data for word sense disambiguation tasks in [4].", "startOffset": 256, "endOffset": 259}, {"referenceID": 0, "context": "Recent theoretical work [1] has approached the problem of explaining the so-called \u201ccompositionality\u201d property exhibited by some word embeddings.", "startOffset": 24, "endOffset": 27}, {"referenceID": 0, "context": "We have chosen this distribution for the noise experiment, because it leads to evenly spaced proportions of co-occurrence noise that cover the entire interval [0, 1].", "startOffset": 159, "endOffset": 165}, {"referenceID": 9, "context": "[10] trained logistic regression models to predict whether a word was rare or frequent given only the direction of its word vector.", "startOffset": 0, "endOffset": 4}, {"referenceID": 0, "context": "The particular choice of n assures a reasonable coverage of the interval [0, 1].", "startOffset": 73, "endOffset": 79}], "year": 2015, "abstractText": "An experimental approach to studying the properties of word embeddings is proposed. Controlled experiments, achieved through modifications of the training corpus, permit the demonstration of direct relations between word properties and word vector direction and length. The approach is demonstrated using the word2vec CBOW model with experiments that independently vary word frequency and word co-occurrence noise. The experiments reveal that word vector length depends more or less linearly on both word frequency and the level of noise in the co-occurrence distribution of the word. The coefficients of linearity depend upon the word. The special point in feature space, defined by the (artificial) word with pure noise in its co-occurrence distribution, is found to be small but non-zero.", "creator": "LaTeX with hyperref package"}}}