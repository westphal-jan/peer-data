{"id": "1510.02693", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "9-Oct-2015", "title": "Feedforward Sequential Memory Neural Networks without Recurrent Feedback", "abstract": "We introduce a new structure for memory neural networks, called feedforward sequential memory networks (FSMN), which can learn long-term dependency without using recurrent feedback. The proposed FSMN is a standard feedforward neural networks equipped with learnable sequential memory blocks in the hidden layers. In this work, we have applied FSMN to several language modeling (LM) tasks. Experimental results have shown that the memory blocks in FSMN can learn effective representations of long history. Experiments have shown that FSMN based language models can significantly outperform not only feedforward neural network (FNN) based LMs but also the popular recurrent neural network (RNN) LMs.", "histories": [["v1", "Fri, 9 Oct 2015 15:04:11 GMT  (184kb,D)", "http://arxiv.org/abs/1510.02693v1", "4 pages, 1figure"]], "COMMENTS": "4 pages, 1figure", "reviews": [], "SUBJECTS": "cs.NE cs.CL cs.LG", "authors": ["shiliang zhang", "hui jiang", "si wei", "lirong dai"], "accepted": false, "id": "1510.02693"}, "pdf": {"name": "1510.02693.pdf", "metadata": {"source": "CRF", "title": "Feedforward Sequential Memory Neural Networks without Recurrent Feedback", "authors": ["Shiliang Zhang", "Hui Jiang", "Si Wei", "Lirong Dai"], "emails": ["zsl2008@mail.ustc.edu.cn,", "hj@cse.yorku.ca,", "siwei@iflytek.com,", "lrdai@ustc.edu.cn"], "sections": [{"heading": null, "text": "The proposed FSMN is a standard feedback neural network equipped with learnable sequential memory blocks in the hidden layers. In this work, we applied FSMN to several speech modelling tasks (LM). Experimental results have shown that the memory blocks in the FSMN can learn effective representations of a long history. Experiments have shown that FSMN-based speech models can significantly outperform not only feedforward neural networks (FNN) based LMs, but also the popular recursive neural networks (RNN) LMs."}, {"heading": "1 Introduction", "text": "When machine learning methods are applied to sequential data such as text, speech and video, it is very important to take advantage of long-term dependence. Traditional approaches have been explored to capture the long-term structure within the sequential data using recurring feedback, such as in recurring neural networks (RNNMs) or LSTM-based models. [1, 2, 3, 4] RNNMs can learn and perform complicated sequential transformations of data over longer periods of time and store the memory in the weights of the network. As a result, they are becoming increasingly popular in sequential data modeling tasks. More recently, there has also been an increase in the construction of neural computing models with different forms of explicit memory units [5, 6, 7, 8]. [6] the proposed storage networks are used by NNexternals (externals) that use a memory component that can be written and read."}, {"heading": "2 Our Approach", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "2.1 Feedforward Sequential Memory Neural Networks", "text": "In fact, it is so that most people who are able to move are able to move, are able to move, are able to move, are able to move and move, are able to move and move. (nvo) In fact, it is as if they were able to be able to be able to be able to be able to be. (nvo) To be able to be able to be able to be able to be able to be able to be able to be able to be able. (nvo) To be able to be able to be able to be able to be able to be able to be able. (nvo) To be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be. (nvo)"}, {"heading": "2.2 Implement FSMN for language models", "text": "We will now explain how FSMNs can be implemented for this task. FSMN is a standard feedback neural network (FNN) except for the additional memory blocks. We will show that the memory block can be efficiently implemented as set-by-set matrix multiplications, which is suitable for the mini-batch-based stochastic gradient descent (SGD) method that runs on GPU. Let us present the FIR filter coefficients of the N order in the memory block as a = {a0, a1, a2, \u00b7, aN}."}, {"heading": "3 Experiments", "text": "We evaluated FSMNs using two benchmark LM tasks: i) the Penn Treebank (PTB) corpus of about 1 million words, in accordance with the same setup as [11]. ii) The Large Text Compression Benchmark (LTCB) [12]. In the LTCB, we use the enwik9 dataset, which consists of the first 109 bytes of enwiki20060303-pages-articles.xml. We divide it into three parts: Training (153M), Validation (8.9M) and Test (8.9M) sets. We limit the vocabulary size to 80k for LTCB and replace all words outside the vocabulary with < UNK >. For FSMNs, the hidden units use the reflected linear activation function, i.e. f (x) = max (0, x).The networks are set on the basis of normalized dynamics initialized [-13-2.000TB], respectively, with a preset of mini-0.001."}, {"heading": "3.1 Results", "text": "We have designed FSMN with a two-layer input context window in which the two previous words are used to predict the next word. FSMN contains a linear projection layer (of 200 nodes), two hidden layers (of 400 nodes prelayer), and a memory block in the first hidden layer. We use a 20-order FIR filter for the memory block. In Table 1, we have summarized the perplexities on the PTB test kit for different language models. For the LTCB task, we have trained several baseline systems: i) two n-gram LMs (3-gram and 5-gram) using the modified Kneser-Ney smoothing without count cutoffs; ii) several traditional feedforward NLMs with different model sizes and input context windows (Bigram, Trigram, 4-gram and 5-gram)."}, {"heading": "4 Conclusions and Future Work", "text": "In this work, we have proposed a novel architecture of neural networks, namely sequential memory networks (FSMN), which use FIR filter-like memory blocks in the hidden layer of standard neural networks. Experimental results on speech modeling tasks have shown that FSMN can effectively learn the long-term story. In future work, we will try to apply this model to other sequential data modeling tasks such as acoustic modeling in speech recognition."}], "references": [{"title": "Long short-term memory", "author": ["S. Hochreiter", "J. Schmidhuber"], "venue": "Neural computation", "citeRegEx": "1", "shortCiteRegEx": null, "year": 1997}, {"title": "Generating sequences with recurrent neural networks", "author": ["A. Graves"], "venue": "arXiv:1308.0850", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2013}, {"title": "et al", "author": ["J. Chung", "C. Gulcehre", "K.H. Cho"], "venue": "Empirical evaluation of gated recurrent neural networks on sequence modeling. arXiv:1412.3555", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2014}, {"title": "et al", "author": ["T. Mikolov", "A. Joulin", "S. Chopra"], "venue": "Learning longer memory in recurrent neural networks. arXiv:1412.7753", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2014}, {"title": "Neural Turing Machines", "author": ["A. Graves", "G. Wayne", "I. Danihelka"], "venue": "arXiv:1410.5401", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2014}, {"title": "Memory networks", "author": ["J. Weston", "S. Chopra", "A. Bordes"], "venue": "arXiv:1410.3916", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2014}, {"title": "End-To-End Memory Networks", "author": ["S. Sukhbaatar", "A. Szlam", "J. Weston"], "venue": "arXiv:1503.08895", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2015}, {"title": "Inferring Algorithmic Patterns with Stack-Augmented Recurrent Nets", "author": ["A. Joulin", "T. Mikolov"], "venue": "arXiv:1503.01007", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2015}, {"title": "Large-scale Simple Question Answering with Memory Networks", "author": ["A. Bordes", "N. Usunier", "S. Chopra", "J. Weston"], "venue": "arXiv:1506.02075", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2015}, {"title": "Learning long-term dependencies with gradient descent is difficult", "author": ["Y. Bengio", "P. Simard", "P. Frasconi"], "venue": "IEEE Transactions on Neural Networks. volume 5, no 2, pages 157-166", "citeRegEx": "10", "shortCiteRegEx": null, "year": 1994}, {"title": "Extensions of recurrent neural network language model", "author": ["T. Mikolov", "S. Kombrink", "L. Burget", "J. Cernocky", "S. Khudanpur"], "venue": "Proc. of International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 5528-5531", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2011}, {"title": "Large Text Compression Benchmark", "author": ["M. Mahoney"], "venue": "http://mattmahoney.net/dc/textdata.html", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2011}, {"title": "Understanding the difficulty of training deep feedforward neural networks", "author": ["G. Xavier", "Y. Bengio"], "venue": "Proc. of AISTATS", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2010}, {"title": "The Fixed-Size Ordinally-Forgetting Encoding Method for Neural Network Language Models", "author": ["S. Zhang", "H. Jiang", "M. Xu", "J. Hou", "L.R. Dai"], "venue": "Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics (ACL), pp.495-500", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2015}, {"title": "Recurrent neural network based language model", "author": ["T. Mikolov", "M. Karafi\u00e1t", "L. Burget", "J. Cernock\u1ef3", "S. Khudanpur"], "venue": "Proc. of Interspeech, pages 1045-1048", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2010}], "referenceMentions": [{"referenceID": 0, "context": "[1, 2, 3, 4].", "startOffset": 0, "endOffset": 12}, {"referenceID": 1, "context": "[1, 2, 3, 4].", "startOffset": 0, "endOffset": 12}, {"referenceID": 2, "context": "[1, 2, 3, 4].", "startOffset": 0, "endOffset": 12}, {"referenceID": 3, "context": "[1, 2, 3, 4].", "startOffset": 0, "endOffset": 12}, {"referenceID": 4, "context": "More recently, different from RNNs, there has also been a surge in constructing neural computing models with varying forms of explicit memory units [5, 6, 7, 8] .", "startOffset": 148, "endOffset": 160}, {"referenceID": 5, "context": "More recently, different from RNNs, there has also been a surge in constructing neural computing models with varying forms of explicit memory units [5, 6, 7, 8] .", "startOffset": 148, "endOffset": 160}, {"referenceID": 6, "context": "More recently, different from RNNs, there has also been a surge in constructing neural computing models with varying forms of explicit memory units [5, 6, 7, 8] .", "startOffset": 148, "endOffset": 160}, {"referenceID": 7, "context": "More recently, different from RNNs, there has also been a surge in constructing neural computing models with varying forms of explicit memory units [5, 6, 7, 8] .", "startOffset": 148, "endOffset": 160}, {"referenceID": 5, "context": "For example, in [6], the proposed memory networks employ a memory component that can be read from and written to.", "startOffset": 16, "endOffset": 19}, {"referenceID": 4, "context": "In [5], the proposed neural turing machines (NTM) improve the memory of neural networks by coupling with external memory resources, which can learn to sort a small set of numbers as well as other symbolic manipulation tasks.", "startOffset": 3, "endOffset": 6}, {"referenceID": 9, "context": "Moreover, the learning of IIR-filter-like RNNs requires to use the so-called back-propagation through time (BPTT) which significantly increases the computational complexity of the learning and also causes the problems of gradient vanishing and exploding [10].", "startOffset": 254, "endOffset": 258}, {"referenceID": 10, "context": "We have evaluated FSMNs on two benchmark LM tasks: i) the Penn Treebank (PTB) corpus of about 1M words, following the same setup as [11].", "startOffset": 132, "endOffset": 136}, {"referenceID": 11, "context": "ii) The Large Text Compression Benchmark (LTCB) [12].", "startOffset": 48, "endOffset": 52}, {"referenceID": 12, "context": "The nets are initialized based on the normalized initialization in [13], without using any pre-training.", "startOffset": 67, "endOffset": 71}, {"referenceID": 14, "context": "For the LTCB task, we have trained several baseline systems: i) two n-gram LMs (3-gram and 5-gram) using the modified Kneser-Ney smoothing without count cutoffs; ii) several traditional feedforward NNLMs with different model sizes and input context windows (bigram, trigram, 4gram and 5-gram); iii) an RNN-LM with one hidden layer of 600 nodes using the toolkit in [15]; iv)", "startOffset": 365, "endOffset": 369}, {"referenceID": 10, "context": "Model Test PPL KN 5-gram [11] 141 RNNLM [11] 123 LSTM [2] 117 MemN2N[7] 111 trigram FNNLM[14] 131 6-gram FNNLM[14] 113 FOFE-FNNLM[14] 108 FSMN-LM 102 Table 2: Perplexities on LTCB for various language models.", "startOffset": 25, "endOffset": 29}, {"referenceID": 10, "context": "Model Test PPL KN 5-gram [11] 141 RNNLM [11] 123 LSTM [2] 117 MemN2N[7] 111 trigram FNNLM[14] 131 6-gram FNNLM[14] 113 FOFE-FNNLM[14] 108 FSMN-LM 102 Table 2: Perplexities on LTCB for various language models.", "startOffset": 40, "endOffset": 44}, {"referenceID": 1, "context": "Model Test PPL KN 5-gram [11] 141 RNNLM [11] 123 LSTM [2] 117 MemN2N[7] 111 trigram FNNLM[14] 131 6-gram FNNLM[14] 113 FOFE-FNNLM[14] 108 FSMN-LM 102 Table 2: Perplexities on LTCB for various language models.", "startOffset": 54, "endOffset": 57}, {"referenceID": 6, "context": "Model Test PPL KN 5-gram [11] 141 RNNLM [11] 123 LSTM [2] 117 MemN2N[7] 111 trigram FNNLM[14] 131 6-gram FNNLM[14] 113 FOFE-FNNLM[14] 108 FSMN-LM 102 Table 2: Perplexities on LTCB for various language models.", "startOffset": 68, "endOffset": 71}, {"referenceID": 13, "context": "Model Test PPL KN 5-gram [11] 141 RNNLM [11] 123 LSTM [2] 117 MemN2N[7] 111 trigram FNNLM[14] 131 6-gram FNNLM[14] 113 FOFE-FNNLM[14] 108 FSMN-LM 102 Table 2: Perplexities on LTCB for various language models.", "startOffset": 89, "endOffset": 93}, {"referenceID": 13, "context": "Model Test PPL KN 5-gram [11] 141 RNNLM [11] 123 LSTM [2] 117 MemN2N[7] 111 trigram FNNLM[14] 131 6-gram FNNLM[14] 113 FOFE-FNNLM[14] 108 FSMN-LM 102 Table 2: Perplexities on LTCB for various language models.", "startOffset": 110, "endOffset": 114}, {"referenceID": 13, "context": "Model Test PPL KN 5-gram [11] 141 RNNLM [11] 123 LSTM [2] 117 MemN2N[7] 111 trigram FNNLM[14] 131 6-gram FNNLM[14] 113 FOFE-FNNLM[14] 108 FSMN-LM 102 Table 2: Perplexities on LTCB for various language models.", "startOffset": 129, "endOffset": 133}, {"referenceID": 13, "context": "2nd-order FOFE based FNNLM [14] with different hidden layer sizes.", "startOffset": 27, "endOffset": 31}], "year": 2015, "abstractText": "We introduce a new structure for memory neural networks, called feedforward sequential memory networks (FSMN), which can learn long-term dependency without using recurrent feedback. The proposed FSMN is a standard feedforward neural networks equipped with learnable sequential memory blocks in the hidden layers. In this work, we have applied FSMN to several language modeling (LM) tasks. Experimental results have shown that the memory blocks in FSMN can learn effective representations of long history. Experiments have shown that FSMN based language models can significantly outperform not only feedforward neural network (FNN) based LMs but also the popular recurrent neural network (RNN) LMs.", "creator": "LaTeX with hyperref package"}}}