{"id": "1510.02709", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "9-Oct-2015", "title": "Large-scale Artificial Neural Network: MapReduce-based Deep Learning", "abstract": "Faced with continuously increasing scale of data, original back-propagation neural network based machine learning algorithm presents two non-trivial challenges: huge amount of data makes it difficult to maintain both efficiency and accuracy; redundant data aggravates the system workload. This project is mainly focused on the solution to the issues above, combining deep learning algorithm with cloud computing platform to deal with large-scale data. A MapReduce-based handwriting character recognizer will be designed in this project to verify the efficiency improvement this mechanism will achieve on training and practical large-scale data. Careful discussion and experiment will be developed to illustrate how deep learning algorithm works to train handwritten digits data, how MapReduce is implemented on deep learning neural network, and why this combination accelerates computation. Besides performance, the scalability and robustness will be mentioned in this report as well. Our system comes with two demonstration software that visually illustrates our handwritten digit recognition/encoding application.", "histories": [["v1", "Fri, 9 Oct 2015 15:45:44 GMT  (893kb,D)", "http://arxiv.org/abs/1510.02709v1", null]], "reviews": [], "SUBJECTS": "cs.DC cs.LG cs.NE", "authors": ["kairan sun", "xu wei", "gengtao jia", "risheng wang", "ruizhi li"], "accepted": false, "id": "1510.02709"}, "pdf": {"name": "1510.02709.pdf", "metadata": {"source": "CRF", "title": "Large-scale Artificial Neural Network: MapReduce-based Deep Learning", "authors": ["Kairan Sun", "Xu Wei", "Gengtao Jia", "Risheng Wang", "Ruizhi Li"], "emails": ["ksun@ufl.edu)."], "sections": [{"heading": null, "text": "In fact, most of them will be able to move to another world, to move to another world, to move to another world, to move to another world."}, {"heading": "II. RELATED WORK", "text": "Of course, we are not the first to try to combine parallel neural network methods with cluster computation, but most existing work achieves parallelization by mapping the nodes in the network to the nodes in the computing cluster, such as [5] - [7]. However, these algorithms are not suitable for MapReduce. In the structure of MapReduce, users cannot specifically control a particular node in a cluster. Instead, MapReduce can only assign the tasks of the mapper and the reducer as a whole, so the above algorithms cannot be implemented in this cloud computing environment. Furthermore, the scope of this work is relatively small and lacks elasticity, and mapping the network nodes to different computing nodes will inevitably increase I / O costs. In most cases, neural network problems are big data, requiring large amounts of I / O operations."}, {"heading": "III. SYSTEM ARCHITECTURE", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "A. Main Strategy", "text": "In detail, we implement a deep learning algorithm [4] to train the input data, using a MapReduce programming model to parallelise the calculation; the MapReduce job consists of the mapper and reduction functions, of which the mapper function extracts key / value pairs from the input data and transfers them to a list of intermediate key / value pairs, and the reduction function merges these intermediate values generated from the mapper function to generate output values; in the case of machine learning, the input values will be the data of a specific object that a machine will \"learn,\" and in our project, the objects are data sets extracted from handwritten letters; the intermediate key / value pairs are weights so that a machine can determine whether it has correctly recognized the object."}, {"heading": "B. Modules and Subsystems", "text": "Our system consists of three distinct parts, deep learning implemented by Java, Hadoop's cloud computing, and Matlab's implanted demonstration software. Such a design is well illustrated in Figure 2. In short, we divide our progress in machine learning into three steps: the pre-training step, where deep learning technologies are used to initialize weights, the step to fine-tune weights, and the final step to improve precision."}, {"heading": "IV. DEEP LEARNING", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "A. Pre-training", "text": "The motivation to face fine-tuning is the unacceptable inefficiency of reverse motion in converting high-dimensional data into low-dimensional codes. If we have multiple layers, large initial weights are always the witnesses of bad local minimums, while small initial weights lead to tiny gradients in early layers, making it impossible to train the entire reverse propagation system with many hidden layers. [12] Therefore, we must make the initial weights near the solution. Consider two images of the number zero and the number one. Instead, we jump into the last step of number recognition, let the task be carried out step by step, where it relates to the internal pattern of two images, namely the recognition of a circle and a stick, then we encode the circle and the bar to zero."}, {"heading": "B. Fine-tuning", "text": "This is the step where we get zero and one from a circle and a stick. Pre-training initializes the weights so that they come close to the solution we want, but they are not the answer. We train the weights using the backpropagation algorithm [13], which is based on the MapReduce method in the multi-layered neural network [14]. Backpropagation and its improved methods are used in mobile data processing. Such applications can be found in [15] - [17]. Note that the trained data is not randomized weights, but well-initialized weights, making the solution easier to get. Each mapper receives a training element and then calculates all the update values of the weights. Subsequently, each reducer collects update values for one weight and calculates the average. The input value of the mapper is the input element, while the input key is empty. On the other hand, the value of the reducer is the weight between the starting weight and the corresponding weight of the batch should be updated, similar to the one before the input weight."}, {"heading": "C. Precision Refinement", "text": "Basically, a neural network model is considered a classifier after training. Unfortunately, the precision of a classifier is vulnerable to the loud and large-scale mobile data. Such learning is called weak learning, and we need to improve the performance of the classifiers. The general concepts of the adaboosting method can be summarized as follows: Get a weak classifier from a part of the training set; use more different parts of the training set that are scanned based on the characteristics of the previous one; put them together. That's just the basis, and we're not going to talk much about the adaboosting method here because we don't have a good idea of it now. Our current focus is on implementing the pre-training, using deep learning and implementing the back-propagation algorithm based on MapReduce."}, {"heading": "V. CLOUD COMPUTING BASED ON MAPREDUCE", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "A. Choice of Cloud Computing Platform", "text": "One of the most serious disadvantages of deep learning is the long training time. Such a time-consuming problem prevents trained machines from quickly equipping with accurate nuclear network data and completing the required task. To solve this problem, applying cloud computing algorithms to machine learning is a good ideal.Among all MapReduce platforms, Hadoop [19] is a good choice for our project. It is an open source software for data storage and large-scale processing of data sets on clusters of raw material hardware [20]. The main modules that make up the Hadoop Framework are Hadoop Common, Hadoop Distributed File System (HDFS), Hadoop Yarn and Hadoop MapReduce [21]. Hadoop Common contains all the utilities and libraries needed by all other Hadoop modules."}, {"heading": "B. MapRedeuce Structure Design", "text": "To implement MapReduce on RBM, we apply an algorithm that can be well illustrated in Figure 4. To facilitate the calculation, all weights that come from the paths between the nodes are mapped in a matrix. For each mapper task, a training element is sent to a mapper, and their results are matrices of variables of weight calculated by the RBM algorithm. A unique ID can identify each element in the output matrix of mapping tasks, which is considered the key of the reduction task. For each reduction task, a reducer accepts the ID and value of a specific element of the matrix as the key value pair. As each reducer only receives the variable value of a specific ID, it can summarize all the results that each training element provides and find the final update of that element."}, {"heading": "C. Pseudo-code for Algorithm", "text": "Our MapReduce-based deep learning algorithm contains only trains the weights for one case. A MapReduce driver class (DeepLearningDriver.java), two mappers and two reducers for RBM training and forward propagation tasks (RBMMapper.java, RBMReducer.java), and a class that implements all the matrix operations (Matrix.java).We present our algorithms using pseudo-codes listed below. First, Algorithm 1 described the driver of deep learning MapReduce program. It contains two MapReduce structures: RBM and forward propagation. It is the planner for the MapReduce tasks.Algorithm 2 describes the Mapper of restricted Boltzmann machine (RBM) training part in deep learning MapReduce program."}, {"heading": "VI. PERFORMANCE AND RESULTS", "text": "The experimental results demonstrate the efficiency and scalability of our proposed MapReduce method on neural networks. This method can be applied to large-scale realistic data on the AWS cloud computing platform. Algorithm 4 Forward propagation mapper is part of the deep learning program MapReduce. Initialization: Input of the mapper is a training case from network input. There are also arguments such as numV is, numHid and Weight (current) past in via configurations or distributed cache. Iteration: 1: initialize (); 2: prop2nextLayer (); 3: Initialize string update \"4: for i \u2190 0 to numHid \u2212 1 do 5: update \u2190 update + Caseupdate +\" \"; 6: End for 7: Output < Key, Value > Pair: < CaseID, update > 8: Each mapper returns the forward propagated case."}, {"heading": "A. Performance Experiment Set-up", "text": "The AWS cloud computing platform consists of multiple instances of EC2. Up to 32 EC2 nodes were used in these experiments to compare the performance of experiments in different running instances. Extensive input data and intermediate results are stored in the distributed cache provided by the platform. Raw input data is 300 megabytes in size and distributed across the platform. Each instance of EC2 has a 64-bit Intel CPU, 16GB of memory and high network performance. In addition, each node can be expanded to up to 4 virtual CPUs, tripling performance. AWS \"open source Hadoop framework is used for the distributed architecture of these nodes.Amazon EC2 instances provide a number of additional features to implement, manage and scale our applications."}, {"heading": "B. Result Evaluations", "text": "In the first part, we perform objective performance evaluation of the deep learning algorithm. However, the error rate of our system is the primary concern, so we perform several experiments to measure the error rate after multiple iterations. Figure 6 shows the number of training errors and test errors as the iteration number increases, for unattended learning of the handwritten digits. As we can see from Figure 6, the error rate for the reconstruction of the handwritten digits decreases significantly, both for the training cases and for the test cases. This is achieved by applying the RBM algorithm and tracing the algorithm. (a) Training error v.s. iteration time curve (b) The iteration time curve v.s. Iterr.Fig-time curveFig."}, {"heading": "C. GUI Demo by MATLAB", "text": "To further refine our project, we implement two GUI demo software inherited from MATLAB GUI package. GUIs also called graphical user interfaces are a kind of point-and-click control of the software application that we have developed. By implementing a GUI interface, users are easier to understand the performance of our system. The reason we use MATLAB GUIs is that MATLAB apps are self-contained programs with GUI fronts that automate a test. MATLAB GUIs usually includes controls such as menus, icons, buttons and sliders. There are two demos: Figure 9 shows the screenshots of the demo software for supervised learning (handwritten digit recognition) results, and Figure 10 shows the screenshots of the software's screen photos for being kept unattended."}, {"heading": "VII. CONCLUSIONS", "text": "In general, we have successfully developed and implemented a neural network algorithm from MapReduce that runs on the platform of Amazon Web Service. Due to the fact that our system runs on a distributed file platform, great improvements have been made in terms of efficiency and accuracy. We also observe that the runtime of data processing increases with increasing numbers of nodes, almost in reverse linear mode. The slight discrepancy between experiments and theory is due to the system effort involved in introducing a larger number of nodes into the system. We believe that neural networks are able to successfully detect handwritten digits with great efficiency and accuracy with the use of MapReduce. In the future, we plan to apply our algorithm to more complex input models such as face recognition, speech recognition and speech processing.APPENDIX The code repository can be found on Github at: https: / / githubsunkn.com / Learning-MapReduce-Based-Depository"}, {"heading": "ACKNOWLEDGEMENT", "text": "The authors thank Dr. Andy Li and his lecturers for their suggestions, which have significantly improved the quality of the paper. They also thank Dr. Dapeng Oliver Wu and his team for the in-depth discussions on our research details. The computing cluster resources used for this project are supported by the course EEL-6935: Cloud Computing and Storage."}], "references": [{"title": "Process control via artificial neural networks and reinforcement learning", "author": ["J. Hoskins", "D. Himmelblau"], "venue": "Computers & chemical engineering, vol. 16, no. 4, pp. 241\u2013251, 1992.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 1992}, {"title": "Parallel distributed processing: explorations in the microstructure of cognition. volume 1. foundations", "author": ["D.E. Rumelhart", "J.L. McClelland"], "venue": "1986.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 1986}, {"title": "A fast learning algorithm for deep belief nets", "author": ["G.E. Hinton", "S. Osindero", "Y.-W. Teh"], "venue": "Neural computation, vol. 18, no. 7, pp. 1527\u20131554, 2006.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2006}, {"title": "Parallelization of a backpropagation neural network on a cluster computer", "author": ["M. Pethick", "M. Liddle", "P. Werstein", "Z. Huang"], "venue": "International conference on parallel and distributed computing and systems (PDCS 2003), 2003.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2003}, {"title": "On the performance of parallel neural network implementations on distributed memory architectures", "author": ["K. Ganeshamoorthy", "D. Ranasinghe"], "venue": "Cluster Computing and the Grid, 2008. CCGRID\u201908. 8th IEEE International Symposium on. IEEE, 2008, pp. 90\u201397.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2008}, {"title": "Parallel implementation of backpropagation algorithm in networks of workstations", "author": ["S. Suresh", "S. Omkar", "V. Mani"], "venue": "Parallel and Distributed Systems, IEEE Transactions on, vol. 16, no. 1, pp. 24\u201334, 2005. 8 Project Report for Cloud Computing and Storage. Dept. of Electrical and Computer Engineering, University of Florida, October 12, 2015", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2005}, {"title": "Map-reduce for machine learning on multicore", "author": ["C. Chu", "S.K. Kim", "Y.-A. Lin", "Y. Yu", "G. Bradski", "A.Y. Ng", "K. Olukotun"], "venue": "Advances in neural information processing systems, vol. 19, p. 281, 2007.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2007}, {"title": "Mapreduce-based backpropagation neural network over large scale mobile data", "author": ["Z. Liu", "H. Li", "G. Miao"], "venue": "Natural Computation (ICNC), 2010 Sixth International Conference on, vol. 4. IEEE, 2010, pp. 1726\u2013 1730.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2010}, {"title": "Mapreduce: Distributed computing for machine learning", "author": ["D. Gillick", "A. Faria", "J. DeNero"], "venue": "Berkley (December 18, 2006), 2006.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2006}, {"title": "Systemml: Declarative machine learning on mapreduce", "author": ["A. Ghoting", "R. Krishnamurthy", "E. Pednault", "B. Reinwald", "V. Sindhwani", "S. Tatikonda", "Y. Tian", "S. Vaithyanathan"], "venue": "Data Engineering (ICDE), 2011 IEEE 27th International Conference on. IEEE, 2011, pp. 231\u2013242.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2011}, {"title": "A theory of the learnable", "author": ["L.G. Valiant"], "venue": "Communications of the ACM, vol. 27, no. 11, pp. 1134\u20131142, 1984.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 1984}, {"title": "Neural networks: a comprehensive foundation", "author": ["S. Haykin"], "venue": "Prentice Hall PTR,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 1994}, {"title": "Theory of the backpropagation neural network", "author": ["R. Hecht-Nielsen"], "venue": "Neural Networks, 1989. IJCNN., International Joint Conference on. IEEE, 1989, pp. 593\u2013605.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 1989}, {"title": "Predicting subscriber dissatisfaction and improving retention in the wireless telecommunications industry", "author": ["M.C. Mozer", "R. Wolniewicz", "D.B. Grimes", "E. Johnson", "H. Kaushansky"], "venue": "Neural Networks, IEEE Transactions on, vol. 11, no. 3, pp. 690\u2013696, 2000.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2000}, {"title": "Data mining techniques on the evaluation of wireless churn.", "author": ["J. Ferreira", "M.B. Vellasco", "M.A.C. Pacheco", "R. Carlos", "H. Barbosa"], "venue": "ESANN,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2004}, {"title": "An ltv model and customer segmentation based on customer value: a case study on the wireless telecommunication industry", "author": ["H. Hwang", "T. Jung", "E. Suh"], "venue": "Expert systems with applications, vol. 26, no. 2, pp. 181\u2013188, 2004.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2004}, {"title": "Boosting the margin: A new explanation for the effectiveness of voting methods", "author": ["R.E. Schapire", "Y. Freund", "P. Bartlett", "W.S. Lee"], "venue": "The annals of statistics, vol. 26, no. 5, pp. 1651\u20131686, 1998.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 1998}, {"title": "Cloud computing: state-of-the-art and research challenges", "author": ["Q. Zhang", "L. Cheng", "R. Boutaba"], "venue": "Journal of Internet Services and Applications, vol. 1, no. 1, pp. 7\u201318, 2010.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2010}, {"title": "Mapreduce: simplified data processing on large clusters", "author": ["J. Dean", "S. Ghemawat"], "venue": "Communications of the ACM, vol. 51, no. 1, pp. 107\u2013113, 2008.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2008}, {"title": "The MNIST database of handwritten digits", "author": ["C.C. Yann LeCun", "C.J. Burges"], "venue": null, "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2013}], "referenceMentions": [{"referenceID": 0, "context": "Inspired by animal central nervous system, artificial neural network (ANN) comes into the domain of machine learning and pattern recognition [1].", "startOffset": 141, "endOffset": 144}, {"referenceID": 1, "context": "Architecture of ANN [3].", "startOffset": 20, "endOffset": 23}, {"referenceID": 2, "context": "In our project, we implement the latest achievement of neural network algorithm, deep learning [4], to accelerate the performance of back-propagation neural network.", "startOffset": 95, "endOffset": 98}, {"referenceID": 3, "context": "Most of existing work achieves parallelization by mapping the nodes in network to the nodes in computing cluster, like [5]\u2013[7].", "startOffset": 119, "endOffset": 122}, {"referenceID": 5, "context": "Most of existing work achieves parallelization by mapping the nodes in network to the nodes in computing cluster, like [5]\u2013[7].", "startOffset": 123, "endOffset": 126}, {"referenceID": 6, "context": "[8] and Liu et al.", "startOffset": 0, "endOffset": 3}, {"referenceID": 7, "context": "[9] both proposed their own algorithm that used back-propagation algorithm to train a three-layer neural network based on MapReduce.", "startOffset": 0, "endOffset": 3}, {"referenceID": 2, "context": "In detail, we implement deep learning algorithm [4] to train the input data, where a MapReduce programming model is made use of to parallelize the computation.", "startOffset": 48, "endOffset": 51}, {"referenceID": 8, "context": "Reducer function uses these weights to compute the so-called \u201cacknowledgment\u201d of the machine of an intended object [10].", "startOffset": 115, "endOffset": 119}, {"referenceID": 9, "context": "Although deep learning learns from representation, it is not capable of eliminating similarity and noisiness contained in data sets, and this will terribly affect machine learning [11].", "startOffset": 180, "endOffset": 184}, {"referenceID": 10, "context": "When we have multiple layers, large initial weights always witnesses poor local minima, while small initial weights will lead to tiny gradients in early layers, making it infeasible to train the whole back propagation system with many hidden layers [12].", "startOffset": 249, "endOffset": 253}, {"referenceID": 11, "context": "We train the weights using back propagation [13] algorithm", "startOffset": 44, "endOffset": 48}, {"referenceID": 12, "context": "based on MapReduce method in multiple layer neural network [14].", "startOffset": 59, "endOffset": 63}, {"referenceID": 13, "context": "Such applications can be found in [15]\u2013[17].", "startOffset": 34, "endOffset": 38}, {"referenceID": 15, "context": "Such applications can be found in [15]\u2013[17].", "startOffset": 39, "endOffset": 43}, {"referenceID": 16, "context": "We are going to use Adaboosting [18] method to refine the result of data training.", "startOffset": 32, "endOffset": 36}, {"referenceID": 17, "context": "It is an open source software for data storage as well as large scale processing of data-sets on clusters of commodity hardware [20].", "startOffset": 128, "endOffset": 132}, {"referenceID": 18, "context": "The main modules that compose Hadoop framework are Hadoop Common, Hadoop Distributed File System (HDFS), Hadoop Yarn,and Hadoop MapReduce [21].", "startOffset": 138, "endOffset": 142}, {"referenceID": 2, "context": "Hinton\u2019s paper in [4].", "startOffset": 18, "endOffset": 21}, {"referenceID": 19, "context": "We use the hand-written digit training and testing cases from the on-line database available in [22].", "startOffset": 96, "endOffset": 100}], "year": 2015, "abstractText": "Faced with continuously increasing scale of data, original back-propagation neural network based machine learning algorithm presents two non-trivial challenges: huge amount of data makes it difficult to maintain both efficiency and accuracy; redundant data aggravates the system workload. This project is mainly focused on the solution to the issues above, combining deep learning algorithm with cloud computing platform to deal with large-scale data. A MapReduce-based handwriting character recognizer will be designed in this project to verify the efficiency improvement this mechanism will achieve on training and practical large-scale data. Careful discussion and experiment will be developed to illustrate how deep learning algorithm works to train handwritten digits data, how MapReduce is implemented on deep learning neural network, and why this combination accelerates computation. Besides performance, the scalability and robustness will be mentioned in this report as well. Our system comes with two demonstration software that visually illustrates our handwritten digit recognition/encoding application. 1", "creator": "LaTeX with hyperref package"}}}