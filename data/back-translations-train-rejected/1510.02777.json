{"id": "1510.02777", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "9-Oct-2015", "title": "Early Inference in Energy-Based Models Approximates Back-Propagation", "abstract": "We show that Langevin MCMC inference in an energy-based model with latent variables has the property that the early steps of inference, starting from a stationary point, correspond to propagating error gradients into internal layers, similarly to back-propagation. The error that is back-propagated is with respect to visible units that have received an outside driving force pushing them away from the stationary point. Back-propagated error gradients correspond to temporal derivatives of the activation of hidden units. This observation could be an element of a theory for explaining how brains perform credit assignment in deep hierarchies as efficiently as back-propagation does. In this theory, the continuous-valued latent variables correspond to averaged voltage potential (across time, spikes, and possibly neurons in the same minicolumn), and neural computation corresponds to approximate inference and error back-propagation at the same time.", "histories": [["v1", "Fri, 9 Oct 2015 19:21:32 GMT  (18kb)", "http://arxiv.org/abs/1510.02777v1", "arXiv admin note: text overlap witharXiv:1509.05936"], ["v2", "Sun, 7 Feb 2016 20:24:38 GMT  (19kb)", "http://arxiv.org/abs/1510.02777v2", "arXiv admin note: text overlap witharXiv:1509.05936"]], "COMMENTS": "arXiv admin note: text overlap witharXiv:1509.05936", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["yoshua bengio", "asja fischer"], "accepted": false, "id": "1510.02777"}, "pdf": {"name": "1510.02777.pdf", "metadata": {"source": "CRF", "title": "Early Inference in Energy-Based Models Approximates Back-Propagation", "authors": ["Yoshua Bengio"], "emails": [], "sections": [{"heading": null, "text": "ar Xiv: 151 0.02 777v 1 [cs.L G] 9O ct2 01"}, {"heading": "1. Introduction", "text": "Several hypotheses have been made (Hinton and Sejnowski, 1986; Friston and Stephan, 2007; Berkes et al., 2011) that, given the state of sensory information (current and past inputs), neurons collectively \"explain\" the observed sensory data better. We can imagine the configuration of internal neurons (hidden units or latent variables) as an \"explanation\" for the sensory data observed when an unexpected signal arrives at the visible neurons."}, {"heading": "2. Neural computation does inference: going down the energy", "text": "We consider the hypothesis that a central interpretation of neural computation (what neurons do, on a short time scale, on which weights can be considered fixed) is to perform iterative inference. Iterative inference means that the hidden units h of the network are gradually changed to configurations that are more likely, with the given sensory input x and the current \"world model\" being associated with the parameters of the model. In other words, they move roughly to configurations that are more likely under P (h | x), and are finally sampled from P (h | x). Before we make the connection between Boltzmann machines or energy-based models and backflow, let us see in mathematical detail how neural computation could be interpreted as inference."}, {"heading": "2.1. Leaky integrator neuron as Langevin MCMC", "text": "For this purpose, consider the classical leaky integrator neural equation. Let's call st the state of the system at the time t, a vector with one element per unit, where st i is a real value associated with the i-th unit and corresponds to a temporally integrated voltage potential.Let's call xt for the visible units, a subset of the elements of st (i.e. the externally driven input), and ht for the hidden units, i.e. st = (xt, ht).Let's call f the function that calculates the new value of the complete state st, where f = (fx, fh) denotes the parts of f that each output the predictions of the trapped (visible) units and the untrapped (hidden) units."}, {"heading": "2.2. Machine Learning Interpretation", "text": "A silent direction would be R (st) \u2212 st, but the injection of noise is important not only to find a single local mode of P (h | x), but to explore the complete distribution. If R (s) is the linear combination of the input rates, the above work establishes a link between R (s) \u2212 s and the energy of a probable model P (s) - E (s) with the energy function E (s), i.e. they find that R (s) + s (s) - logP (s) \u2212 s and the energy of a probable model P (s) - E (s) with the energy function E (s), i.e. they find that R (s) - s (s) - and s (s) - is not the last line."}, {"heading": "2.3. A Possible Energy Function", "text": "s machine, but with continuous nonlinearity inserted. E (s) = \u2211 is2i 2 \u2212 1 \u00b2 i6 = jWi, j\u03c1 (si) \u03c1 (sj) \u2212 \u2211 ibi\u03c1 (si) (6), where Wi, j is the weight between unit j to unit i, \u03c1 is neuronal nonlinearity, some kind of monotonic boundary function, which results in a value between 0 and 1 corresponding to a firing rate. With this energy function, the driving function R would precede R (s) = s \u2212 own sRi (s) = iron case (si) bi + iron state, which is the firing rate. (7) To achieve this, we assumed that Wi, j = Wj, i. Otherwise, we would get this derivative."}, {"heading": "3. Link to Back-propagation", "text": "We are now ready to present the main result of this work, namely a link between neural computation as a conclusion in an energy-based model and the backpropagation of error gradients in prediction."}, {"heading": "3.1. Propagation of Pertubations", "text": "Consider what happens when a network like the one described above happens near the equilibrium, i.e. near a fixed point according to the principle of the equation. (At this point, we would consider two types of visible units: input units x and output units x and output units, and s = (x, y, h). Suppose we place the network at a fixed point with x bracketed to the observed input values. (Then, we get an output at the fixed point s s s s s s s, where Ry (s) = y, Rh (s) = h). Equally important is that we place this network at a fixed point with x bracketed input values. (s) We get an output at the fixed point s s s s s s s s s s, where Ry (s) = y (s)."}, {"heading": "3.2. Stochastic Gradient Descent Weight Update", "text": "The above result is consistent with and inspired by the idea already proposed by Hinton (2007) that temporal changes can encode retroactive gradients. (21) It turns out that the state change represents the gradient of the predictive error in relation to s, SGD to Wi, j would require the weight change in relation to the rate of change of the state of the post-synaptic neuron. (21) It turns out that such a learning rule represents the firing rate of the pre-synaptic neuron in relation to s, SGD to Wi, j the weight change in relation to the rate of change of the post-synaptic neuron, s and proportional to the firing rate of the pre-synaptic neuron. (sj)"}, {"heading": "4. Related work, contributions and future work", "text": "In fact, it is as if most of them are able to survive themselves by going in search of their own identity. (...) It is not as if they are able to identify themselves. (...) It is not as if they are able to identify themselves. (...) It is not as if they are able to identify themselves. (...) It is not as if they are able to identify themselves. (...) It is as if they are doing it. (...) It is as if they are doing it, as if they are doing it. (...) It is as if they are doing it. (...) It is as if they are doing it. \"(...) It is as if they are doing it, as if they are doing it. (...) It is as if they are doing it. (...) It is not as if they want to do it. (...) It is not as if they are doing it."}, {"heading": "Acknowledgments", "text": "The authors thank Benjamin Scellier, Asja Fischer, Thomas Mesnard, Saizheng Zhang, Yuhuai Wu, Dong-Hyun Lee, Jyri Kivinen, J\u00f6rg Bornschein, Roland Memisevic and Tim Lillicrap for feedback and discussions, and NSERC, CIFAR, Samsung and Canada Research Chairs for funding."}], "references": [{"title": "What regularized autoencoders learn from the data generating distribution", "author": ["G. Alain", "Y. Bengio"], "venue": "ICLR\u20192013. also arXiv report 1211.4246.", "citeRegEx": "Alain and Bengio,? 2013", "shortCiteRegEx": "Alain and Bengio", "year": 2013}, {"title": "An introduction to MCMC for machine learning", "author": ["C. Andrieu", "N. de Freitas", "A. Doucet", "M. Jordan"], "venue": "Machine Learning,", "citeRegEx": "Andrieu et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Andrieu et al\\.", "year": 2003}, {"title": "An objective function for stdp", "author": ["Y. Bengio", "T. Mesnard", "A. Fischer", "S. Zhang", "Y. Wu"], "venue": "arXiv:1509.05936.", "citeRegEx": "Bengio et al\\.,? 2015a", "shortCiteRegEx": "Bengio et al\\.", "year": 2015}, {"title": "Towards biologically plausible deep learning", "author": ["Y. Bengio", "Lee", "D.-H.", "J. Bornschein", "Z. Lin"], "venue": "arXiv:1502.04156.", "citeRegEx": "Bengio et al\\.,? 2015b", "shortCiteRegEx": "Bengio et al\\.", "year": 2015}, {"title": "Spontaneous cortical activity reveals hallmarks of an optimal internal model of the environment", "author": ["P. Berkes", "G. Orban", "M. Lengyel", "J. Fiser"], "venue": "Science, 331, 83\u2013\u201387.", "citeRegEx": "Berkes et al\\.,? 2011", "shortCiteRegEx": "Berkes et al\\.", "year": 2011}, {"title": "Gradient learning in spiking neural networks by dynamic perturbations of conductances", "author": ["I.R. Fiete", "H.S. Seung"], "venue": "Physical Review Letters, 97(4).", "citeRegEx": "Fiete and Seung,? 2006", "shortCiteRegEx": "Fiete and Seung", "year": 2006}, {"title": "Free-energy and the brain", "author": ["K.J. Friston", "K.E. Stephan"], "venue": "Synthese, 159, 417\u2013\u2013458.", "citeRegEx": "Friston and Stephan,? 2007", "shortCiteRegEx": "Friston and Stephan", "year": 2007}, {"title": "How to do backpropagation in a brain", "author": ["G.E. Hinton"], "venue": "Invited talk at the NIPS\u20192007 Deep Learning Workshop.", "citeRegEx": "Hinton,? 2007", "shortCiteRegEx": "Hinton", "year": 2007}, {"title": "Learning and relearning in Boltzmann machines", "author": ["G.E. Hinton", "T.J. Sejnowski"], "venue": "D. E. Rumelhart and J. L. McClelland, editors, Parallel Distributed Processing: Explorations in the Microstructure of Cognition. Volume 1: Foundations, pages 282\u2013317. MIT Press,", "citeRegEx": "Hinton and Sejnowski,? 1986", "shortCiteRegEx": "Hinton and Sejnowski", "year": 1986}, {"title": "Auto-encoding variational bayes", "author": ["D.P. Kingma", "M. Welling"], "venue": "Proceedings of the International Conference on Learning Representations (ICLR).", "citeRegEx": "Kingma and Welling,? 2014", "shortCiteRegEx": "Kingma and Welling", "year": 2014}, {"title": "Spiketiming-dependent plasticity: A comprehensive overview", "author": ["H. Markram", "W. Gerstner", "P. Sj\u00f6str\u00f6m"], "venue": "Frontiers in synaptic plasticity, 4(2).", "citeRegEx": "Markram et al\\.,? 2012", "shortCiteRegEx": "Markram et al\\.", "year": 2012}, {"title": "A view of the EM algorithm that justifies incremental, sparse, and other variants", "author": ["R. Neal", "G. Hinton"], "venue": "M. I. Jordan, editor, Learning in Graphical Models. MIT Press, Cambridge, MA.", "citeRegEx": "Neal and Hinton,? 1999", "shortCiteRegEx": "Neal and Hinton", "year": 1999}, {"title": "Stochastic variational learning in recurrent spiking networks", "author": ["D.J. Rezende", "W. Gerstner"], "venue": "Frontiers in Computational Neuroscience, 8(38).", "citeRegEx": "Rezende and Gerstner,? 2014", "shortCiteRegEx": "Rezende and Gerstner", "year": 2014}, {"title": "A connection between score matching and denoising autoencoders", "author": ["P. Vincent"], "venue": "Neural Computation, 23(7).", "citeRegEx": "Vincent,? 2011", "shortCiteRegEx": "Vincent", "year": 2011}, {"title": "Stacked denoising autoencoders: Learning useful representations in a deep network with a local denoising criterion", "author": ["P. Vincent", "H. Larochelle", "I. Lajoie", "Y. Bengio", "Manzagol", "P.-A."], "venue": "J. Machine Learning Res., 11.", "citeRegEx": "Vincent et al\\.,? 2010", "shortCiteRegEx": "Vincent et al\\.", "year": 2010}, {"title": "Simple statistical gradientfollowing algorithms connectionist reinforcement learning", "author": ["R.J. Williams"], "venue": "Machine Learning, 8, 229\u2013256.", "citeRegEx": "Williams,? 1992", "shortCiteRegEx": "Williams", "year": 1992}], "referenceMentions": [{"referenceID": 8, "context": "Introduction It has been hypothesized numerous times (Hinton and Sejnowski, 1986; Friston and Stephan, 2007; Berkes et al., 2011), that, given a state of sensory information (current and past inputs), neurons are collectively performing inference, i.", "startOffset": 53, "endOffset": 129}, {"referenceID": 6, "context": "Introduction It has been hypothesized numerous times (Hinton and Sejnowski, 1986; Friston and Stephan, 2007; Berkes et al., 2011), that, given a state of sensory information (current and past inputs), neurons are collectively performing inference, i.", "startOffset": 53, "endOffset": 129}, {"referenceID": 4, "context": "Introduction It has been hypothesized numerous times (Hinton and Sejnowski, 1986; Friston and Stephan, 2007; Berkes et al., 2011), that, given a state of sensory information (current and past inputs), neurons are collectively performing inference, i.", "startOffset": 53, "endOffset": 129}, {"referenceID": 2, "context": "As shown in this paper, synaptic changes proportional to the stochastic gradient with respect to the prediction error could be achieved if the synaptic updates are similar to those resulting from minimizing the local objective function introduced by Bengio et al. (2015b) in order to mimic spike-timing dependent plasticity (STDP).", "startOffset": 250, "endOffset": 272}, {"referenceID": 13, "context": "We now draw an interesting link with recent work on unsupervised learning using denoising auto-encoders and denoising score matching (Vincent, 2011; Alain and Bengio, 2013).", "startOffset": 133, "endOffset": 172}, {"referenceID": 0, "context": "We now draw an interesting link with recent work on unsupervised learning using denoising auto-encoders and denoising score matching (Vincent, 2011; Alain and Bengio, 2013).", "startOffset": 133, "endOffset": 172}, {"referenceID": 1, "context": "1 seems to follow a Langevin Monte-Carlo Markov chain (Andrieu et al., 2003):", "startOffset": 54, "endOffset": 76}, {"referenceID": 7, "context": "The above result is consistent with and inspired by the idea previously proposed by Hinton (2007) that temporal change can encode back-propagated gradients.", "startOffset": 84, "endOffset": 98}, {"referenceID": 2, "context": "It turns out that such a learning rule allows to simulate the relationship between spike-timing and synaptic change according to the STDP (spike-timing dependent plasticity), as shown by Bengio et al. (2015a). It arises as a stochastic gradient step in a predictive objective function", "startOffset": 187, "endOffset": 209}, {"referenceID": 7, "context": "Related work, contributions and future work An important inspiration for this work is the idea proposed by Hinton (2007) that brains could implement backpropagation by using temporal derivatives to represent activation gradients, and the suggestion that combining this assumption with STDP would yield SGD on the synaptic weights.", "startOffset": 107, "endOffset": 121}, {"referenceID": 8, "context": "The idea of neural computation corresponding to a form of stochastic relaxation towards lower energy configurations is of course very old, for example with the Boltzmann machine (Hinton and Sejnowski, 1986) and its Gibbs sampling procedure.", "startOffset": 178, "endOffset": 206}, {"referenceID": 4, "context": "For more recent work in this direction, see also (Berkes et al., 2011).", "startOffset": 49, "endOffset": 70}, {"referenceID": 5, "context": "Many approaches (Fiete and Seung, 2006; Rezende and Gerstner, 2014) rely on variants of the REINFORCE algorithm (Williams, 1992) to estimate the gradient of a global objective function (basically by correlating stochastic variations at each neuron with the changes in the global objective).", "startOffset": 16, "endOffset": 67}, {"referenceID": 12, "context": "Many approaches (Fiete and Seung, 2006; Rezende and Gerstner, 2014) rely on variants of the REINFORCE algorithm (Williams, 1992) to estimate the gradient of a global objective function (basically by correlating stochastic variations at each neuron with the changes in the global objective).", "startOffset": 16, "endOffset": 67}, {"referenceID": 15, "context": "Many approaches (Fiete and Seung, 2006; Rezende and Gerstner, 2014) rely on variants of the REINFORCE algorithm (Williams, 1992) to estimate the gradient of a global objective function (basically by correlating stochastic variations at each neuron with the changes in the global objective).", "startOffset": 112, "endOffset": 128}, {"referenceID": 9, "context": "There are of course many other papers on theoretical interpretations of STDP, and the reader can find many references in Markram et al. (2012), but more work is needed to explore the connection of STDP to an unsupervised learning objective that could be used to train not just a single layer network (like PCA and traditional Hebbian updates) but also a deep unsupervised model.", "startOffset": 121, "endOffset": 143}, {"referenceID": 5, "context": "Now, in an EM or variational EM context such as discussed in Neal and Hinton (1999); Kingma and Welling (2014); Bengio et al.", "startOffset": 70, "endOffset": 84}, {"referenceID": 5, "context": "Now, in an EM or variational EM context such as discussed in Neal and Hinton (1999); Kingma and Welling (2014); Bengio et al.", "startOffset": 70, "endOffset": 111}, {"referenceID": 2, "context": "Now, in an EM or variational EM context such as discussed in Neal and Hinton (1999); Kingma and Welling (2014); Bengio et al. (2015b), with x fixed, the distribution we want to model and consider as a target towards which parameters should be updated is precisely the joint of h \u223c P (h|x) and x \u223c the training data, which we now call Q(h, x) (the inference distribution), following the above papers.", "startOffset": 112, "endOffset": 134}, {"referenceID": 14, "context": "servation is that earlier work on auto-encoders empirically showed that even when the forward and backward weights are not tied, they tend to converge to symmetric values, and in the linear case the minimization of reconstruction error automatically yields symmetric weights (Vincent et al., 2010).", "startOffset": 275, "endOffset": 297}], "year": 2017, "abstractText": "We show that Langevin MCMC inference in an energy-based model with latent variables has the property that the early steps of inference, starting from a stationary point, correspond to propagating error gradients into internal layers, similarly to back-propagation. The error that is backpropagated is with respect to visible units that have received an outside driving force pushing them away from the stationary point. Backpropagated error gradients correspond to temporal derivatives of the activation of hidden units. This observation could be an element of a theory for explaining how brains perform credit assignment in deep hierarchies as efficiently as backpropagation does. In this theory, the continuousvalued latent variables correspond to averaged voltage potential (across time, spikes, and possibly neurons in the same minicolumn), and neural computation corresponds to approximate inference and error back-propagation at the same time.", "creator": "dvips(k) 5.991 Copyright 2011 Radical Eye Software"}}}