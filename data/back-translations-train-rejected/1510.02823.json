{"id": "1510.02823", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "9-Oct-2015", "title": "Human languages order information efficiently", "abstract": "Most languages use the relative order between words to encode meaning relations. Languages differ, however, in what orders they use and how these orders are mapped onto different meanings. We test the hypothesis that, despite these differences, human languages might constitute different `solutions' to common pressures of language use. Using Monte Carlo simulations over data from five languages, we find that their word orders are efficient for processing in terms of both dependency length and local lexical probability. This suggests that biases originating in how the brain understands language strongly constrain how human languages change over generations.", "histories": [["v1", "Fri, 9 Oct 2015 21:05:02 GMT  (1382kb,D)", "http://arxiv.org/abs/1510.02823v1", null]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["daniel gildea", "t florian jaeger"], "accepted": false, "id": "1510.02823"}, "pdf": {"name": "1510.02823.pdf", "metadata": {"source": "CRF", "title": "Human languages order information efficiently", "authors": ["Daniel Gildea", "Florian Jaeger"], "emails": [], "sections": [{"heading": "1 Introduction", "text": "In fact, most of them will be able to play by the rules that they have set themselves, and they will be able to play by the rules that they have set themselves in order to play by the rules."}, {"heading": "2 Data", "text": "For English, we had access to a corpus of conversations with the necessary annotations. An overview of the corpora is in Table 1. Specifically, the Arabic database consists of 6776 sentences from Penn Arabic Treebank, depending on the Prague Arabic Dependency Treebank version 1 [38]. The Czech database consists of 72,703 sentences from the Prague Dependency Treebank version 1 [7] as used in the CoNLL 2006. The English database comes from two sources. For written data, we use the 39,832 sentences from the Wall Street Journal part of Penn Treebank version 3 [65]. For spoken data, we use 17,968 sentences from the Switchboard Corpus of spoken English."}, {"heading": "3 Estimating the Processing Efficiency of Languages", "text": "The last few years have shown that people are able to determine for themselves what they want, and that they are able to determine for themselves what they want, and that they are able to do the things that they have to do, and that they are not doing the things that they want, but what they have to do, and that they are doing what they have to do, and that they have to do it, and that they have to do it, what they have to do, and that they have to do it in order to understand the things that they have to do."}, {"heading": "3.1 Estimating Processing Efficiency", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "3.1.1 Surprisal and Information Density", "text": "In fact, it is such that most of them are in a position to move into another world, in which they are in a position, in which they are in a position, in which they are in a position, in which they are in a position, in which they are in a position, in which they are able to move, in which they are in a position, in which they are able to move, in which they are in a position, in which they are in a position, in which they are able, in which they are able to stay in the world, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live in which they live, in which they live, in which they live, in which they live, they live in which they live, in which they live, in which they live, in which they live in which they live, they live in which they live, in which they live, in which they live, in which they live, in which they live in which they live, they live in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live,"}, {"heading": "3.1.2 Dependency Length", "text": "Our other measure of processing difficulty is the length of the dependence. This measure can be read from the dependence trees by counting the number of words from each modifier to its head in the linear order of the sentence. For example, for the dependence structure in Figure 2, the word on the left is the fifth word from the word when. Thus, the length of the SBAR > S dependence between when and left is of length 5. The SBJ > S dependence between the words I and left, on the other hand, is of length 1. In our experiments, we calculate the average length of all dependencies in each sentence. In Figure 2, we have dependencies of length 1, 1, 1, 3, and 5 for an average length of 2.2. To measure the dependence length, a number of different metrics has been proposed. Thus, the length of dependence is sometimes measured by the number of non-discursive referents [29] or by the syntactical complexity of the engaging material. All these measures tend to [44] high measures."}, {"heading": "3.2 Estimating Chance", "text": "To obtain a random baseline against which the processing efficiency of each language can be compared, we create 1000 variants for each language. Specifically, we obtain 1000 pseudograms by randomly rearranging the dependency structures described above. \u2212 All context-independent lexical properties, including all semantic and phonological factors at the word level \u2022 the number and identity of sentences in the corpus \u2022 the number of words in each sentence (known to affect estimates of word information) and in the corpus the identity of words in each sentence (including their part of speech) and in the corpus we measure the number of heads, dependencies and dependencies in each sentence and in the corpus \u2022 the frequency of the different types of dependencies in each sentence and in the fixed corpus We then measure the average dependence."}, {"heading": "4 Results", "text": "First, we compare the actual information density and dependence length of five languages in our sample with the pseudograms derived from it. Then, we present four control studies that serve to demonstrate the robustness of our results."}, {"heading": "4.1 Study 1: Comparing the information density and dependency length of human languages to chance", "text": "Figure 6 shows both the actual human languages and the 1000 random samples for each of these languages at a level defined by the two measures of processing efficiency considered here. Table 3 provides a numerical summary. As can be seen, the processing efficiency of the actual Arabic, Czech, English, German and Mandarin Chinese is significantly better than expected by chance. Specifically, using a standard significance criterion of \u03b1 =.05, we show that all five languages have a lower information density than expected by chance, and all languages except Chinese have shorter dependency lengths than expected by the Chancellor. Next, we present three control studies demonstrating the robustness of our results. As the studies presented here are computer-demanding, we limit our control studies to one of the two estimates of information density. We decided to focus on estimating the persign because we believe that it reflects characteristics specific to the writing systems of the language less (such as a word)."}, {"heading": "4.2 Control study 1: Fixed vs. flexible constituent order", "text": "The results in Table 3 are based on pseudo-grammars calculated on the assumption that languages have fixed constituent orders within a type. Interestingly, this assumption is approximate but not entirely consistent with what is observed for human languages. Table 4 provides a measure of the consistency of languages in our sample. In calculating the information density, the fixed assumption made in Study 1 should be conservative, biased against the hypothesis we are testing: on average, the constituent orders increased, lowering the average information density, which should give the pseudo-grammars derived for the study a distinct advantage over the actual human languages, which often do not have fixed constituent orders (or at least not fully fixed)."}, {"heading": "4.3 Control study 2: Sensitivity to Genre and Mode", "text": "While our primary data sets are taken from the newspaper text, we wanted to test whether our results were sensitive to the genre of the corpus and, in particular, whether edited, written text might have different characteristics from spontaneous spoken text. Unfortunately, large syntactically annotated corpus is only available for very few languages. Here, we test our hypothesis against conversation data from English. Repeat Study 1 on the corpus of the conversational language in English again shows that the processing efficiency of actual English is better than expected randomly. Actual conversational English had a better information density per character than all 1,000 random word orders and a better dependency length than all 1,000 random orders (both ps <.0001)."}, {"heading": "4.4 Control study 3: Sensitivity to Corpus Size", "text": "Finally, we tested the sensitivity of our results to the amount of text available to estimate the parameters of the Kneser-Ney trigram model. Therefore, we repeated the above analysis using a much smaller set of 1000 random sentences taken from the Wall Street Journal (i.e. about 2.5% of the original corpus). Unsurprisingly, the information density estimates were higher compared to the main study (shown in Table 3) - this is a direct consequence of the reduced data size: for smaller corpus, there will be more n-grams in the test data that were never observed in the training data; these n-grams are associated with low probability (and therefore high information); the estimates based on a smaller corpus may also be less reliable, because a large portion of the n-grams in the test data will not be visible in the training data, regardless of the word sequence used. To quantify this effect, we find using the actual English sequence that only 10% of the test data is missing from the English test data, whereas the training data is actually better in the 1000 grams."}, {"heading": "4.5 Summary", "text": "In terms of information density, all five languages studied fall into the upper 95th percentile or better; in terms of dependence length, four of the five languages fall into the 95th percentile; and a language (Mandarin) falls into the 75th percentile of distribution, defined by random pseudograms. Our results apply regardless of the size of the corpus and, more importantly, to both the written and spoken language. Taken together, this suggests that language usage - especially pressure resulting from the gradual processing of language - shapes the grammar of languages over time. We note that information density and dependence length were not independent in our samples, so it is possible that what we have - after reading - is treated as two independent measures of processing efficiency is actually due to an underlying cause."}, {"heading": "5 Study 2: Are natural languages optimal with regard to", "text": "Next, we tested whether an even stronger assertion could be made. Specifically, we wondered whether the pressure to process efficiently was strong enough to limit the language change to the subspace of possible grammars that are optimal (or very close to optimal) in terms of processing efficiency. As described in the introduction, many constraints of language use were hypothesized to distort and restrict the language change, contributing to cross-linguistic observed characteristics of languages. Therefore, we did not expect languages to be optimal in terms of processing efficiency. We begin by describing the process of estimating the minimum possible information density for each language. Then, we describe the process of estimating the minimum possible dependence length for each language. Finally, we present a method that collectively optimizes both information density and dependence length for each language and allows us to compare human languages with pseudogrammatics that optimally balance the two main factors for processing efficiency for each language."}, {"heading": "5.1 Computing pseudo-grammars with optimal information density", "text": "We begin by describing the method for calculating the minimum possible information density h \u043c for each language: h \u043c = min \u03bb h (\u03bb) To find h \u0445, we optimize one weight after another, resulting in two dependencies in reverse order, with the target jumping discontinuously. However, this non-differentiation implies that methods based on gradient ascent are not applied, since the objective function changes only at points where one weight crosses the value of another, thus comparing the number of dependencies with different values of the objective function. In fact, the only significant points for dependence types that occur in the corpus are."}, {"heading": "5.2 Computing pseudo-grammars with optimal dependency length", "text": "We also optimize our pseudo-grammar to find the weights that yield the lowest dependency length: d \u0445 = min \u03bb d (\u03bb), where d (\u03bb) is the average dependence length for the pseudo-grammar with weights \u03bb. Searching for weights uses the same algorithm described above."}, {"heading": "5.3 Joint Optimization", "text": "There may be a trade-off between information density and dependence length. Although we found that information density and dependence length are positively correlated in the random pseudograms created for Study 1, these correlations were mild to moderate. Therefore, it is possible that the optimization of information density is swapped for the optimization of dependence length (and vice versa). We are therefore investigating the effects on the dependence length of the optimization of information density and vice versa. We are also experimenting with a common objective function j that combines information density and dependence length: j \u0445 = min \u03bb (1 \u2212 \u03b1) d (\u03bb) + \u03b1h (\u03bb) We then applied the optimization algorithm described above to this common measure of processing efficiency. The separate optimizations described in the previous two sections correspond to an order of magnitude of 1 or 0."}, {"heading": "5.4 Results", "text": "The left half of the table shows the information density and dependence length for the optimal pseudo-grammars, which are combined with \u03b1 = 0.5 (ID & DL) by optimizing the information density (ID), dependence length (DL) or both. The right half of the table shows that the information density and dependence length of the actual human language, as well as the mean of the random pseudo-grammars generated for the study, are actually at the expense of the other property (despite the generally positive correlations between information density and dependence length in the randomly pseudo-grammars we generated for the study. Figure 5) The average information density and dependence length of all five natural languages is generally closer to the common optimum."}, {"heading": "6 Discussion", "text": "Current results suggest that language processing alters language: all the natural languages for which we were able to test the hypotheses are easier to process than expected."}], "references": [{"title": "Avoiding attachment ambiguities : The role of constituent ordering", "author": ["Jennifer E Arnold", "Thomas Wasow", "Ash Asudeh", "Peter Alrenga"], "venue": "Journal of Memory and Language,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2004}, {"title": "Heaviness vs. newness: the effects of structural complexity and discourse status on constituent ordering", "author": ["Jennifer E Arnold", "Thomas Wasow", "Anthony Losongco", "Ryan Ginstrom"], "venue": null, "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2000}, {"title": "Morphological influences on the recognition of monosyllabic monomorphemic words", "author": ["R H Baayen"], "venue": "Journal of Memory and Language,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2006}, {"title": "Visual Word Recognition of Single-Syllable Words", "author": ["David A Balota", "Michael J Cortese", "Susan D Sergent-Marshall", "Daniel H Spieler", "Melvin J Yap"], "venue": "Journal of experimental psychology: General,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2004}, {"title": "Functionalist approaches to grammar. In Language acquisition: the state of the art, pages 173\u2013218", "author": ["Elizabeth Bates", "Brian MacWhinney"], "venue": null, "citeRegEx": "5", "shortCiteRegEx": "5", "year": 1982}, {"title": "Competition, Variation, and Language Learning", "author": ["Elizabeth Bates", "Brian MacWhinney"], "venue": "Mechanisms of Language Acquisition,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 1987}, {"title": "The PDT: a 3-level annotation scenario", "author": ["A. B\u00f6hmov\u00e1", "J. Haji\u010d", "E. Haji\u010dov\u00e1", "B. Hladk\u00e1"], "venue": null, "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2003}, {"title": "Parsing costs as predictors of reading difficulty: An evaluation using the Potsdam Sentence Corpus", "author": ["Marisa Ferrara Boston", "John Hale", "Reinhold Kliegl", "Umesh Patil", "Shravan Vasishth"], "venue": "Journal of Eye Movement Research,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2008}, {"title": "The TIGER treebank", "author": ["S. Brants", "S. Dipper", "S. Hansen", "W. Lezius", "G. Smith"], "venue": "In Proc. of the 1st Workshop on Treebanks and Linguistic Theories (TLT),", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2002}, {"title": "CoNLL-X shared task on multilingual dependency parsing", "author": ["Sabine Buchholz", "Erwin Marsi"], "venue": "In Proceedings of the Tenth Conference on Computational Natural Language Learning,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2006}, {"title": "Frequency and the Emergence of Linguistic Structure", "author": ["Joan Bybee", "Paul J Hopper"], "venue": null, "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2001}, {"title": "The effect of usage on degrees of constituency: the reduction of dont\u0301", "author": ["Joan Bybee", "Joanne Scheibman"], "venue": "in English. Linguistics,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 1999}, {"title": "An empirical study of smoothing techniques for language modeling", "author": ["Stanley F Chen", "Joshua Goodman"], "venue": "Computer Speech & Language,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 1999}, {"title": "Length and Order: A Corpus Study of Korean Dative-Accusative Construction", "author": ["Hye-won Choi"], "venue": null, "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2007}, {"title": "Using Information Content to Predict Phone Deletion", "author": ["Uriel Cohen Priva"], "venue": "Proceedings of the 27th West Coast Conference on Formal Linguistics,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2008}, {"title": "Head-driven Statistical Models for Natural Language Parsing", "author": ["Michael John Collins"], "venue": "PhD thesis,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 1999}, {"title": "Putting the bits together: an information theoretical perspective on morphological processing", "author": ["Aleksandar Kosti\u0107", "R Harald Baayen"], "venue": "Cognition,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2004}, {"title": "Data from eye-tracking corpora as evidence for theories of syntactic processing", "author": ["Vera Demberg", "Frank Keller"], "venue": "complexity. Cognition,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2008}, {"title": "Communicative Efficiency, Language Learning, and Language Universals", "author": ["Maryia Fedzechkina"], "venue": "PhD thesis, University of Rochester,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2014}, {"title": "Functional Biases in Language Learning: Evidence from Word Order and Case-Marking Interaction", "author": ["Maryia Fedzechkina", "T Florian Jaeger", "Elissa L Newport"], "venue": "In 33rd Annual Meeting of the Cognitive Science Society,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2004}, {"title": "Language learners restructure their input to facilitate efficient communication", "author": ["Maryia Fedzechkina", "T. Florian Jaeger", "Elissa L. Newport"], "venue": "Proceedings of the National Academy of Sciences of the United States of America,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2012}, {"title": "Communicative biases shape structures of newly acquired languages", "author": ["Maryia Fedzechkina", "T Florian Jaeger", "Elissa L Newport"], "venue": "Proceedings of the 35th Annual Meeting of the Cognitive Science Society", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2013}, {"title": "Euclidean distance between syntactically linked words", "author": ["Ramon Ferrer i Cancho"], "venue": "Physical Review E,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2004}, {"title": "Sequential vs. hierarchical syntactic models of human incremental sentence processing", "author": ["Victoria Fossum", "Roger Levy"], "venue": "In Proceedings of the 3rd Workshop on Cognitive Modeling and Computational Linguistics,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2012}, {"title": "Insensitivity of the human sentence-processing system to hierarchical structure", "author": ["Stefan L Frank", "Rens Bod"], "venue": "Psychological Science,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2011}, {"title": "Large-scale evidence of dependency length minimization in 37 languages", "author": ["Richard Futrell", "Kyle Mahowald", "Edward Gibson"], "venue": "Proceedings of the National Academy of Sciences,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2015}, {"title": "Entropy Rate Constancy in Text", "author": ["Dmitriy Genzel", "Eugene Charniak"], "venue": "In Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics (ACL),", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2002}, {"title": "Linguistic complexity: locality of syntactic dependencies", "author": ["E Gibson"], "venue": "Cognition,", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 1998}, {"title": "The Dependency Locality Theory: A Distance-Based Theory of Linguistic Complexity", "author": ["Edward Gibson"], "venue": "Image, language, brain: Papers from the first mind articulation symposium,", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2000}, {"title": "A noisy-channel account of crosslinguistic word-order variation", "author": ["Edward Gibson", "Steven T Piantadosi", "Kimberly Brink", "Leon Bergen", "Eunice Lim", "Rebecca Saxe"], "venue": "Psychological Science,", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2013}, {"title": "Do grammars minimize dependency length", "author": ["Daniel Gildea", "David Temperley"], "venue": "Cognitive Science,", "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2010}, {"title": "SWITCHBOARD: Telephone speech corpus for research and development", "author": ["J. Godfrey", "E. Holliman", "J. McDaniel"], "venue": "In IEEE ICASSP-92,", "citeRegEx": "32", "shortCiteRegEx": "32", "year": 1992}, {"title": "Speech and audio signal processing: processing and perception of speech and music", "author": ["Ben Gold", "Nelson Morgan", "Dan Ellis"], "venue": null, "citeRegEx": "33", "shortCiteRegEx": "33", "year": 2011}, {"title": "Substantive learning bias or an effect of familiarity? comment", "author": ["Adele E Goldberg"], "venue": "on. Cognition,", "citeRegEx": "34", "shortCiteRegEx": "34", "year": 2013}, {"title": "Locality and Feature Specificity in OCP Effects: Evidence from Aymara, Dutch, and Javanese", "author": ["Peter Graff", "T Florian Jaeger"], "venue": "In Proceedings of the Main Session of the 45th Meeting of the Chicago Linguistic Society,", "citeRegEx": "35", "shortCiteRegEx": "35", "year": 2009}, {"title": "Consequences of the serial nature of linguistic input for sentenial complexity", "author": ["Daniel Grodner", "Edward Gibson"], "venue": "Cognitive science,", "citeRegEx": "36", "shortCiteRegEx": "36", "year": 2005}, {"title": "Form and function in linguistic variation", "author": ["Gegory R Guy"], "venue": null, "citeRegEx": "37", "shortCiteRegEx": "37", "year": 1996}, {"title": "Be\u0161ka. Prague arabic dependency treebank: Development in data and tools", "author": ["Jan Haji\u010d", "Otakar Smr\u017e", "Petr Zem\u00e1nek", "Jan \u0160naidauf", "Emanuel"], "venue": "In Proc. of the NEMLAR Intern. Conf. on Arabic Language Resources and Tools,", "citeRegEx": "38", "shortCiteRegEx": "38", "year": 2004}, {"title": "A Probabilistic Earley Parser as a Psycholinguistic Model", "author": ["John Hale"], "venue": "Proceedings of the second meeting of the North American Chapter of the Association for Computational Linguistics on Language technologies,", "citeRegEx": "39", "shortCiteRegEx": "39", "year": 2001}, {"title": "The faculty of language: what is it, who has it, and how did it", "author": ["Marc D Hauser", "Noam Chomsky", "W Tecumseh Fitch"], "venue": "evolve? science,", "citeRegEx": "40", "shortCiteRegEx": "40", "year": 2002}, {"title": "Cross-linguistic variation and efficiency", "author": ["J.A. Hawkins"], "venue": null, "citeRegEx": "41", "shortCiteRegEx": "41", "year": 2014}, {"title": "A Performance Theory of Order and Constituency", "author": ["John Hawkins"], "venue": null, "citeRegEx": "42", "shortCiteRegEx": "42", "year": 1994}, {"title": "Efficiency and complexity in grammars", "author": ["John A Hawkins"], "venue": null, "citeRegEx": "43", "shortCiteRegEx": "43", "year": 2004}, {"title": "Processing typology and why psychologists need to know about it", "author": ["John A. Hawkins"], "venue": "New Ideas in Psychology,", "citeRegEx": "44", "shortCiteRegEx": "44", "year": 2007}, {"title": "Linguistic Variability and Intellectual Development", "author": ["W. von Humboldt"], "venue": null, "citeRegEx": "45", "shortCiteRegEx": "45", "year": 1972}, {"title": "The Role of Entropy and Surprisal in Phonologization and Language Change", "author": ["Elizabeth Hume", "Fr\u00e9d\u00e9ric Mailhot"], "venue": null, "citeRegEx": "46", "shortCiteRegEx": "46", "year": 2013}, {"title": "The cross-linguistic study of sentence production", "author": ["T F Jaeger", "E J Norcliffe"], "venue": "Language and Linguistics Compass,", "citeRegEx": "47", "shortCiteRegEx": "47", "year": 2009}, {"title": "Redundancy and reduction: speakers manage syntactic information density", "author": ["T Florian Jaeger"], "venue": "Cognitive Psychology,", "citeRegEx": "48", "shortCiteRegEx": "48", "year": 2010}, {"title": "Statistical Methods for Speech Recognition", "author": ["Frederick Jelinek"], "venue": null, "citeRegEx": "49", "shortCiteRegEx": "49", "year": 1997}, {"title": "Cumulative cultural evolution in the laboratory: an experimental approach to the origins of structure in human language", "author": ["Simon Kirby", "Hannah Cornish", "Kenny Smith"], "venue": "Proceedings of the National Academy of Sciences of the United States of America,", "citeRegEx": "50", "shortCiteRegEx": "50", "year": 2008}, {"title": "Innateness and culture in the evolution of language", "author": ["Simon Kirby", "Mike Dowman", "Thomas L Griffiths"], "venue": "Proceedings of the National Academy of Sciences,", "citeRegEx": "51", "shortCiteRegEx": "51", "year": 2007}, {"title": "Improved backing-off for m-gram language modeling", "author": ["Reinhard Kneser", "Hermann Ney"], "venue": "In International Conference on Acoustics, Speech, and Signal Processing (ICASSP),", "citeRegEx": "52", "shortCiteRegEx": "52", "year": 1995}, {"title": "Statistical machine translation", "author": ["Philipp Koehn"], "venue": null, "citeRegEx": "53", "shortCiteRegEx": "53", "year": 2009}, {"title": "The Phonetics/Phonology Issue in the Study of Articulatory Reduction", "author": ["K J Kohler"], "venue": "Phonetica,", "citeRegEx": "54", "shortCiteRegEx": "54", "year": 1991}, {"title": "Probabilistic Models of Word Order and Syntactic Discontinuity", "author": ["Roger Levy"], "venue": "PhD thesis, Stanford University,", "citeRegEx": "55", "shortCiteRegEx": "55", "year": 2005}, {"title": "Expectation-based syntactic comprehension", "author": ["Roger Levy"], "venue": null, "citeRegEx": "56", "shortCiteRegEx": "56", "year": 2008}, {"title": "Computational principles of working memory in sentence comprehension", "author": ["Richard L Lewis", "Shravan Vasishth", "Julie a Van Dyke"], "venue": "Trends in Cognitive Sciences,", "citeRegEx": "57", "shortCiteRegEx": "57", "year": 2006}, {"title": "Explaining phonetic variation: A sketch of the H&H theory", "author": ["Bj\u00f6rn Lindblom"], "venue": null, "citeRegEx": "58", "shortCiteRegEx": "58", "year": 1990}, {"title": "Domain Minimization in English Verb-Particle", "author": ["Barbara Lohse", "John A Hawkins", "Thomas Wasow"], "venue": "Constructions. Language,", "citeRegEx": "59", "shortCiteRegEx": "59", "year": 2004}, {"title": "Recognizing Spoken Words: The Neighborhood Activation Model", "author": ["Paul A Luce", "David B Pisoni"], "venue": "Ear and Hearing,", "citeRegEx": "60", "shortCiteRegEx": "60", "year": 1998}, {"title": "How language production shapes language form and comprehension", "author": ["Maryellen C MacDonald"], "venue": "Frontiers in Psychology,", "citeRegEx": "61", "shortCiteRegEx": "61", "year": 2013}, {"title": "Natural Language Parsing as Statistical Pattern Recognition", "author": ["David Magerman"], "venue": "PhD thesis, Stanford University,", "citeRegEx": "62", "shortCiteRegEx": "62", "year": 1994}, {"title": "The dynamics of lexical competition during spoken word recognition", "author": ["James S Magnuson", "James A Dixon", "Michael K Tanenhaus", "Richard N Aslin"], "venue": "Cognitive science,", "citeRegEx": "63", "shortCiteRegEx": "63", "year": 2007}, {"title": "Experiments on predictability of word in context and information rate in natural language", "author": ["D Yu Manin"], "venue": null, "citeRegEx": "64", "shortCiteRegEx": "64", "year": 2006}, {"title": "Building a large annotated corpus of English: The Penn treebank", "author": ["Mitchell P. Marcus", "Beatrice Santorini", "Mary Ann Marcinkiewicz"], "venue": "Computational Linguistics,", "citeRegEx": "65", "shortCiteRegEx": "65", "year": 1993}, {"title": "Low-level predictive inference in reading: the influence of transitional probabilities on eye movements", "author": ["Scott A. McDonald", "Richard C. Shillcock"], "venue": "Vision Research,", "citeRegEx": "66", "shortCiteRegEx": "66", "year": 2003}, {"title": "The evolution of syntactic communication", "author": ["Martin A Nowak", "Joshua B Plotkin", "Vincent AA Jansen"], "venue": "Nature, 404(6777):495\u2013498,", "citeRegEx": "67", "shortCiteRegEx": "67", "year": 2000}, {"title": "Discussion of Bjoern Lindblom\u2019s \u2019Phonetic Invariance and the adaptive nature of speech", "author": ["John J Ohala"], "venue": "In Working Models of Human Perception", "citeRegEx": "68", "shortCiteRegEx": "68", "year": 1988}, {"title": "Word lengths are optimized for efficient communication", "author": ["Steven T Piantadosi", "Harry Tily", "Edward Gibson"], "venue": null, "citeRegEx": "69", "shortCiteRegEx": "69", "year": 2011}, {"title": "The communicative function of ambiguity", "author": ["Steven T Piantadosi", "Harry Tily", "Edward Gibson"], "venue": "in language. Cognition,", "citeRegEx": "70", "shortCiteRegEx": "70", "year": 2012}, {"title": "Word-specific phonetics", "author": ["Janet B Pierrehumbert"], "venue": "Laboratory phonology", "citeRegEx": "71", "shortCiteRegEx": "71", "year": 2002}, {"title": "The faculty of language: what\u2019s special about it? Cognition, 95(2):201\u201336", "author": ["Steven Pinker", "Ray Jackendoff"], "venue": null, "citeRegEx": "72", "shortCiteRegEx": "72", "year": 2005}, {"title": "Aiming at shorter dependencies: the role of agreement morphology", "author": ["Idoia Ros", "Mike Santesteban", "Kumiko Fukumura", "Itziar Laka"], "venue": "Language, Cognition, and Neuroscience,", "citeRegEx": "73", "shortCiteRegEx": "73", "year": 2015}, {"title": "Language Change in Childhood and in History", "author": ["Dan I Slobin"], "venue": "Working Papers of the Language Behavior Research Laboratory,", "citeRegEx": "74", "shortCiteRegEx": "74", "year": 1975}, {"title": "The effect of word predictability on reading time is logarithmic", "author": ["Nathaniel J Smith", "Roger Levy"], "venue": "Cognition,", "citeRegEx": "75", "shortCiteRegEx": "75", "year": 2013}, {"title": "Szmrecs\u00e1nyi. On Operationalizing Syntactic Complexity", "author": ["M Benedikt"], "venue": "JADT", "citeRegEx": "76", "shortCiteRegEx": "76", "year": 2004}, {"title": "Hierarchic syntax improves reading time prediction", "author": ["Marten van Schijndel", "William Schuler"], "venue": "In Proceedings of the 2013 Meeting of the North American chapter of the Association for Computational Linguistics", "citeRegEx": "77", "shortCiteRegEx": "77", "year": 2015}, {"title": "How efficient is speech", "author": ["R J J H van Son", "Louis C W Pols"], "venue": "Proceedings Institute of Phonetic Sciences, University of Amsterdam,", "citeRegEx": "78", "shortCiteRegEx": "78", "year": 2003}, {"title": "An activation-based model of sentence processing as skilled memory retrieval", "author": ["Shravan Vasishth", "R L Lewis"], "venue": "Cognitive science,", "citeRegEx": "79", "shortCiteRegEx": "79", "year": 2005}, {"title": "Exemplar models, evolution and language change", "author": ["Andrew Wedel"], "venue": "The Linguistic Review,", "citeRegEx": "81", "shortCiteRegEx": "81", "year": 2006}, {"title": "Functional Load and the Lexicon: Evidence that Syntactic Category and Frequency Relationships in Minimal Lemma Pairs Predict the Loss of Phoneme contrasts in Language Change", "author": ["Andrew Wedel", "Scott Jackson", "Abby Kaplan"], "venue": "Language and Speech,", "citeRegEx": "82", "shortCiteRegEx": "82", "year": 2013}, {"title": "The penn chinese treebank: Phrase structure annotation of a large corpus", "author": ["Nianwen Xue", "Fei Xia", "Fu-Dong Chiou", "Martha Palmer"], "venue": "Natural Language Engineering,", "citeRegEx": "83", "shortCiteRegEx": "83", "year": 2005}, {"title": "Long before short\u201d preference in the production of a head-final", "author": ["Hiroko Yamashita", "Franklin Chang"], "venue": "language. Cognition,", "citeRegEx": "84", "shortCiteRegEx": "84", "year": 2001}, {"title": "Human Behavior and the Principle of Least Effort", "author": ["George K. Zipf"], "venue": "Addison-Wesley, New York,", "citeRegEx": "85", "shortCiteRegEx": "85", "year": 1949}], "referenceMentions": [{"referenceID": 73, "context": "Biases for grammars with higher processing efficiency could be the direct result of abstract learning biases [74, 21] or they could result from the pressures of language use [11, 42, 43], such as preferences that have been hypothesized to operate during language production [30, 48, 58] or biases originating in comprehension [37, 68, 71, 81].", "startOffset": 109, "endOffset": 117}, {"referenceID": 20, "context": "Biases for grammars with higher processing efficiency could be the direct result of abstract learning biases [74, 21] or they could result from the pressures of language use [11, 42, 43], such as preferences that have been hypothesized to operate during language production [30, 48, 58] or biases originating in comprehension [37, 68, 71, 81].", "startOffset": 109, "endOffset": 117}, {"referenceID": 10, "context": "Biases for grammars with higher processing efficiency could be the direct result of abstract learning biases [74, 21] or they could result from the pressures of language use [11, 42, 43], such as preferences that have been hypothesized to operate during language production [30, 48, 58] or biases originating in comprehension [37, 68, 71, 81].", "startOffset": 174, "endOffset": 186}, {"referenceID": 41, "context": "Biases for grammars with higher processing efficiency could be the direct result of abstract learning biases [74, 21] or they could result from the pressures of language use [11, 42, 43], such as preferences that have been hypothesized to operate during language production [30, 48, 58] or biases originating in comprehension [37, 68, 71, 81].", "startOffset": 174, "endOffset": 186}, {"referenceID": 42, "context": "Biases for grammars with higher processing efficiency could be the direct result of abstract learning biases [74, 21] or they could result from the pressures of language use [11, 42, 43], such as preferences that have been hypothesized to operate during language production [30, 48, 58] or biases originating in comprehension [37, 68, 71, 81].", "startOffset": 174, "endOffset": 186}, {"referenceID": 29, "context": "Biases for grammars with higher processing efficiency could be the direct result of abstract learning biases [74, 21] or they could result from the pressures of language use [11, 42, 43], such as preferences that have been hypothesized to operate during language production [30, 48, 58] or biases originating in comprehension [37, 68, 71, 81].", "startOffset": 274, "endOffset": 286}, {"referenceID": 47, "context": "Biases for grammars with higher processing efficiency could be the direct result of abstract learning biases [74, 21] or they could result from the pressures of language use [11, 42, 43], such as preferences that have been hypothesized to operate during language production [30, 48, 58] or biases originating in comprehension [37, 68, 71, 81].", "startOffset": 274, "endOffset": 286}, {"referenceID": 57, "context": "Biases for grammars with higher processing efficiency could be the direct result of abstract learning biases [74, 21] or they could result from the pressures of language use [11, 42, 43], such as preferences that have been hypothesized to operate during language production [30, 48, 58] or biases originating in comprehension [37, 68, 71, 81].", "startOffset": 274, "endOffset": 286}, {"referenceID": 36, "context": "Biases for grammars with higher processing efficiency could be the direct result of abstract learning biases [74, 21] or they could result from the pressures of language use [11, 42, 43], such as preferences that have been hypothesized to operate during language production [30, 48, 58] or biases originating in comprehension [37, 68, 71, 81].", "startOffset": 326, "endOffset": 342}, {"referenceID": 67, "context": "Biases for grammars with higher processing efficiency could be the direct result of abstract learning biases [74, 21] or they could result from the pressures of language use [11, 42, 43], such as preferences that have been hypothesized to operate during language production [30, 48, 58] or biases originating in comprehension [37, 68, 71, 81].", "startOffset": 326, "endOffset": 342}, {"referenceID": 70, "context": "Biases for grammars with higher processing efficiency could be the direct result of abstract learning biases [74, 21] or they could result from the pressures of language use [11, 42, 43], such as preferences that have been hypothesized to operate during language production [30, 48, 58] or biases originating in comprehension [37, 68, 71, 81].", "startOffset": 326, "endOffset": 342}, {"referenceID": 79, "context": "Biases for grammars with higher processing efficiency could be the direct result of abstract learning biases [74, 21] or they could result from the pressures of language use [11, 42, 43], such as preferences that have been hypothesized to operate during language production [30, 48, 58] or biases originating in comprehension [37, 68, 71, 81].", "startOffset": 326, "endOffset": 342}, {"referenceID": 34, "context": "Some evidence suggests that the sound structure and lexicon of natural languages exhibit properties that are expected under this hypothesis [35, 64, 69, 70, 85].", "startOffset": 140, "endOffset": 160}, {"referenceID": 63, "context": "Some evidence suggests that the sound structure and lexicon of natural languages exhibit properties that are expected under this hypothesis [35, 64, 69, 70, 85].", "startOffset": 140, "endOffset": 160}, {"referenceID": 68, "context": "Some evidence suggests that the sound structure and lexicon of natural languages exhibit properties that are expected under this hypothesis [35, 64, 69, 70, 85].", "startOffset": 140, "endOffset": 160}, {"referenceID": 69, "context": "Some evidence suggests that the sound structure and lexicon of natural languages exhibit properties that are expected under this hypothesis [35, 64, 69, 70, 85].", "startOffset": 140, "endOffset": 160}, {"referenceID": 83, "context": "Some evidence suggests that the sound structure and lexicon of natural languages exhibit properties that are expected under this hypothesis [35, 64, 69, 70, 85].", "startOffset": 140, "endOffset": 160}, {"referenceID": 80, "context": "those levels of linguistic organization, studies over the last couple of years have also provided more direct correlational evidence that language change is affected by processing [82].", "startOffset": 180, "endOffset": 184}, {"referenceID": 49, "context": "Miniature language learning experiments have documented similar biases during language acquisition and that these biases can accumulate over generations of learners [50].", "startOffset": 165, "endOffset": 169}, {"referenceID": 44, "context": "The level of linguistic organization that has remained elusive with regard to this question, however, is also arguably the one that is the one that makes human languages most unique compared to all other animal communication systems: syntax \u2013or some aspects of syntax (recursion)\u2013 give human languages infinite expressivity with finite means [45, 67] and it is syntax that has been taken to be the defining property of human languages (e.", "startOffset": 342, "endOffset": 350}, {"referenceID": 66, "context": "The level of linguistic organization that has remained elusive with regard to this question, however, is also arguably the one that is the one that makes human languages most unique compared to all other animal communication systems: syntax \u2013or some aspects of syntax (recursion)\u2013 give human languages infinite expressivity with finite means [45, 67] and it is syntax that has been taken to be the defining property of human languages (e.", "startOffset": 342, "endOffset": 350}, {"referenceID": 39, "context": ", [40]; but see [72]).", "startOffset": 2, "endOffset": 6}, {"referenceID": 71, "context": ", [40]; but see [72]).", "startOffset": 16, "endOffset": 20}, {"referenceID": 20, "context": "Whether at least some properties of the syntactic systems of languages can be derived from the fact that languages need to be processed continues to be a heatedly debated question (for recent high impact reviews, see [21, 51, 50, 72]).", "startOffset": 217, "endOffset": 233}, {"referenceID": 50, "context": "Whether at least some properties of the syntactic systems of languages can be derived from the fact that languages need to be processed continues to be a heatedly debated question (for recent high impact reviews, see [21, 51, 50, 72]).", "startOffset": 217, "endOffset": 233}, {"referenceID": 49, "context": "Whether at least some properties of the syntactic systems of languages can be derived from the fact that languages need to be processed continues to be a heatedly debated question (for recent high impact reviews, see [21, 51, 50, 72]).", "startOffset": 217, "endOffset": 233}, {"referenceID": 71, "context": "Whether at least some properties of the syntactic systems of languages can be derived from the fact that languages need to be processed continues to be a heatedly debated question (for recent high impact reviews, see [21, 51, 50, 72]).", "startOffset": 217, "endOffset": 233}, {"referenceID": 53, "context": "The pressures inherent to language processing have long been assumed to shape languages over time, including not only phonology and the lexicon [54, 58, 68, 46, 85], but also syntactic structure [11, 5, 6, 42, 74].", "startOffset": 144, "endOffset": 164}, {"referenceID": 57, "context": "The pressures inherent to language processing have long been assumed to shape languages over time, including not only phonology and the lexicon [54, 58, 68, 46, 85], but also syntactic structure [11, 5, 6, 42, 74].", "startOffset": 144, "endOffset": 164}, {"referenceID": 67, "context": "The pressures inherent to language processing have long been assumed to shape languages over time, including not only phonology and the lexicon [54, 58, 68, 46, 85], but also syntactic structure [11, 5, 6, 42, 74].", "startOffset": 144, "endOffset": 164}, {"referenceID": 45, "context": "The pressures inherent to language processing have long been assumed to shape languages over time, including not only phonology and the lexicon [54, 58, 68, 46, 85], but also syntactic structure [11, 5, 6, 42, 74].", "startOffset": 144, "endOffset": 164}, {"referenceID": 83, "context": "The pressures inherent to language processing have long been assumed to shape languages over time, including not only phonology and the lexicon [54, 58, 68, 46, 85], but also syntactic structure [11, 5, 6, 42, 74].", "startOffset": 144, "endOffset": 164}, {"referenceID": 10, "context": "The pressures inherent to language processing have long been assumed to shape languages over time, including not only phonology and the lexicon [54, 58, 68, 46, 85], but also syntactic structure [11, 5, 6, 42, 74].", "startOffset": 195, "endOffset": 213}, {"referenceID": 4, "context": "The pressures inherent to language processing have long been assumed to shape languages over time, including not only phonology and the lexicon [54, 58, 68, 46, 85], but also syntactic structure [11, 5, 6, 42, 74].", "startOffset": 195, "endOffset": 213}, {"referenceID": 5, "context": "The pressures inherent to language processing have long been assumed to shape languages over time, including not only phonology and the lexicon [54, 58, 68, 46, 85], but also syntactic structure [11, 5, 6, 42, 74].", "startOffset": 195, "endOffset": 213}, {"referenceID": 41, "context": "The pressures inherent to language processing have long been assumed to shape languages over time, including not only phonology and the lexicon [54, 58, 68, 46, 85], but also syntactic structure [11, 5, 6, 42, 74].", "startOffset": 195, "endOffset": 213}, {"referenceID": 73, "context": "The pressures inherent to language processing have long been assumed to shape languages over time, including not only phonology and the lexicon [54, 58, 68, 46, 85], but also syntactic structure [11, 5, 6, 42, 74].", "startOffset": 195, "endOffset": 213}, {"referenceID": 38, "context": ", its surprisal [39, 56]) and the length of the dependencies that are integrated at the word (dependency length, [28, 29]).", "startOffset": 16, "endOffset": 24}, {"referenceID": 55, "context": ", its surprisal [39, 56]) and the length of the dependencies that are integrated at the word (dependency length, [28, 29]).", "startOffset": 16, "endOffset": 24}, {"referenceID": 27, "context": ", its surprisal [39, 56]) and the length of the dependencies that are integrated at the word (dependency length, [28, 29]).", "startOffset": 113, "endOffset": 121}, {"referenceID": 28, "context": ", its surprisal [39, 56]) and the length of the dependencies that are integrated at the word (dependency length, [28, 29]).", "startOffset": 113, "endOffset": 121}, {"referenceID": 37, "context": "in the dependency representation of the Prague Arabic Dependency Treebank version 1 [38].", "startOffset": 84, "endOffset": 88}, {"referenceID": 6, "context": "The Czech data consists of 72,703 sentences from the Prague Dependency Treebank version 1 [7], as used in the CoNLL 2006 dependency parsing evaluation [10].", "startOffset": 90, "endOffset": 93}, {"referenceID": 9, "context": "The Czech data consists of 72,703 sentences from the Prague Dependency Treebank version 1 [7], as used in the CoNLL 2006 dependency parsing evaluation [10].", "startOffset": 151, "endOffset": 155}, {"referenceID": 64, "context": "For written data, we use the 39,832 sentences from the Wall Street Journal portion of the Penn Treebank version 3 [65].", "startOffset": 114, "endOffset": 118}, {"referenceID": 31, "context": "For spoken data, we use 17,968 sentences from the Switchboard corpus of spoken English [32].", "startOffset": 87, "endOffset": 91}, {"referenceID": 8, "context": "The German data consists of 45,422 sentences from the TIGER corpus [9], which primarily consists of articles from the German newspaper \u201cFrankfurter Rundschau\u201d.", "startOffset": 67, "endOffset": 70}, {"referenceID": 81, "context": "0 [83].", "startOffset": 2, "endOffset": 6}, {"referenceID": 22, "context": "The dependency representation is a directed graph specifying, for each word in the sentence, the head word (or \u2018sender\u2019 [23]) that it modifies.", "startOffset": 120, "endOffset": 124}, {"referenceID": 61, "context": ", the main component of the phrase [62, 16].", "startOffset": 35, "endOffset": 43}, {"referenceID": 15, "context": ", the main component of the phrase [62, 16].", "startOffset": 35, "endOffset": 43}, {"referenceID": 15, "context": "Specifically, we extract dependencies using the head-finding rules of Collins [16].", "startOffset": 78, "endOffset": 82}, {"referenceID": 38, "context": "As outlined in the introduction, we focus on two measures of processing efficiency that have received broad empirical support: surprisal [39, 56] and dependency length, [28, 29].", "startOffset": 137, "endOffset": 145}, {"referenceID": 55, "context": "As outlined in the introduction, we focus on two measures of processing efficiency that have received broad empirical support: surprisal [39, 56] and dependency length, [28, 29].", "startOffset": 137, "endOffset": 145}, {"referenceID": 27, "context": "As outlined in the introduction, we focus on two measures of processing efficiency that have received broad empirical support: surprisal [39, 56] and dependency length, [28, 29].", "startOffset": 169, "endOffset": 177}, {"referenceID": 28, "context": "As outlined in the introduction, we focus on two measures of processing efficiency that have received broad empirical support: surprisal [39, 56] and dependency length, [28, 29].", "startOffset": 169, "endOffset": 177}, {"referenceID": 54, "context": "A word\u2019s surprisal (conditioned on all relevant preceding context) has been shown to be identical to the relative entropy (or Kullback-Leibler divergence) between the distribution over all possible parses prior to the word and the distribution over all possible parses after processing the word [55].", "startOffset": 295, "endOffset": 299}, {"referenceID": 7, "context": "Crucially, a word\u2019s surprisal has been found to be a good predictor of its reading times in context [8, 18, 25, 66, 75].", "startOffset": 100, "endOffset": 119}, {"referenceID": 17, "context": "Crucially, a word\u2019s surprisal has been found to be a good predictor of its reading times in context [8, 18, 25, 66, 75].", "startOffset": 100, "endOffset": 119}, {"referenceID": 24, "context": "Crucially, a word\u2019s surprisal has been found to be a good predictor of its reading times in context [8, 18, 25, 66, 75].", "startOffset": 100, "endOffset": 119}, {"referenceID": 65, "context": "Crucially, a word\u2019s surprisal has been found to be a good predictor of its reading times in context [8, 18, 25, 66, 75].", "startOffset": 100, "endOffset": 119}, {"referenceID": 74, "context": "Crucially, a word\u2019s surprisal has been found to be a good predictor of its reading times in context [8, 18, 25, 66, 75].", "startOffset": 100, "endOffset": 119}, {"referenceID": 74, "context": "For example, in a large-scale reading experiment, Smith and Levy [75] found that per-word reading times were linear in the word\u2019s surprisal.", "startOffset": 65, "endOffset": 69}, {"referenceID": 27, "context": "Psycholinguistic research has found that non-local dependencies tend to cause processing difficulty ([28, 29, 36]; though see [57, 79] for discussion).", "startOffset": 101, "endOffset": 113}, {"referenceID": 28, "context": "Psycholinguistic research has found that non-local dependencies tend to cause processing difficulty ([28, 29, 36]; though see [57, 79] for discussion).", "startOffset": 101, "endOffset": 113}, {"referenceID": 35, "context": "Psycholinguistic research has found that non-local dependencies tend to cause processing difficulty ([28, 29, 36]; though see [57, 79] for discussion).", "startOffset": 101, "endOffset": 113}, {"referenceID": 56, "context": "Psycholinguistic research has found that non-local dependencies tend to cause processing difficulty ([28, 29, 36]; though see [57, 79] for discussion).", "startOffset": 126, "endOffset": 134}, {"referenceID": 78, "context": "Psycholinguistic research has found that non-local dependencies tend to cause processing difficulty ([28, 29, 36]; though see [57, 79] for discussion).", "startOffset": 126, "endOffset": 134}, {"referenceID": 72, "context": ", for Basque [73]; English: [2, 1, 59]; Japanese [84]; Korean [14]; for reviews and discussion, see [41, 47]).", "startOffset": 13, "endOffset": 17}, {"referenceID": 1, "context": ", for Basque [73]; English: [2, 1, 59]; Japanese [84]; Korean [14]; for reviews and discussion, see [41, 47]).", "startOffset": 28, "endOffset": 38}, {"referenceID": 0, "context": ", for Basque [73]; English: [2, 1, 59]; Japanese [84]; Korean [14]; for reviews and discussion, see [41, 47]).", "startOffset": 28, "endOffset": 38}, {"referenceID": 58, "context": ", for Basque [73]; English: [2, 1, 59]; Japanese [84]; Korean [14]; for reviews and discussion, see [41, 47]).", "startOffset": 28, "endOffset": 38}, {"referenceID": 82, "context": ", for Basque [73]; English: [2, 1, 59]; Japanese [84]; Korean [14]; for reviews and discussion, see [41, 47]).", "startOffset": 49, "endOffset": 53}, {"referenceID": 13, "context": ", for Basque [73]; English: [2, 1, 59]; Japanese [84]; Korean [14]; for reviews and discussion, see [41, 47]).", "startOffset": 62, "endOffset": 66}, {"referenceID": 40, "context": ", for Basque [73]; English: [2, 1, 59]; Japanese [84]; Korean [14]; for reviews and discussion, see [41, 47]).", "startOffset": 100, "endOffset": 108}, {"referenceID": 46, "context": ", for Basque [73]; English: [2, 1, 59]; Japanese [84]; Korean [14]; for reviews and discussion, see [41, 47]).", "startOffset": 100, "endOffset": 108}, {"referenceID": 2, "context": "To name just a few of these properties, a word\u2019s length, frequency, neighborhood density, part-of-speech, and morphological structure are all correlated with the average time it takes to comprehend or produce that word [3, 4, 8, 17, 18, 60, 63].", "startOffset": 219, "endOffset": 244}, {"referenceID": 3, "context": "To name just a few of these properties, a word\u2019s length, frequency, neighborhood density, part-of-speech, and morphological structure are all correlated with the average time it takes to comprehend or produce that word [3, 4, 8, 17, 18, 60, 63].", "startOffset": 219, "endOffset": 244}, {"referenceID": 7, "context": "To name just a few of these properties, a word\u2019s length, frequency, neighborhood density, part-of-speech, and morphological structure are all correlated with the average time it takes to comprehend or produce that word [3, 4, 8, 17, 18, 60, 63].", "startOffset": 219, "endOffset": 244}, {"referenceID": 16, "context": "To name just a few of these properties, a word\u2019s length, frequency, neighborhood density, part-of-speech, and morphological structure are all correlated with the average time it takes to comprehend or produce that word [3, 4, 8, 17, 18, 60, 63].", "startOffset": 219, "endOffset": 244}, {"referenceID": 17, "context": "To name just a few of these properties, a word\u2019s length, frequency, neighborhood density, part-of-speech, and morphological structure are all correlated with the average time it takes to comprehend or produce that word [3, 4, 8, 17, 18, 60, 63].", "startOffset": 219, "endOffset": 244}, {"referenceID": 59, "context": "To name just a few of these properties, a word\u2019s length, frequency, neighborhood density, part-of-speech, and morphological structure are all correlated with the average time it takes to comprehend or produce that word [3, 4, 8, 17, 18, 60, 63].", "startOffset": 219, "endOffset": 244}, {"referenceID": 62, "context": "To name just a few of these properties, a word\u2019s length, frequency, neighborhood density, part-of-speech, and morphological structure are all correlated with the average time it takes to comprehend or produce that word [3, 4, 8, 17, 18, 60, 63].", "startOffset": 219, "endOffset": 244}, {"referenceID": 11, "context": "As expected under the general hypothesis tested here, several studies have found the lexicon of languages to exhibit properties that are consistent with the hypothesis that processing efficiency over time shapes the phonology of words [12, 15, 35, 64, 69, 70, 85].", "startOffset": 235, "endOffset": 263}, {"referenceID": 14, "context": "As expected under the general hypothesis tested here, several studies have found the lexicon of languages to exhibit properties that are consistent with the hypothesis that processing efficiency over time shapes the phonology of words [12, 15, 35, 64, 69, 70, 85].", "startOffset": 235, "endOffset": 263}, {"referenceID": 34, "context": "As expected under the general hypothesis tested here, several studies have found the lexicon of languages to exhibit properties that are consistent with the hypothesis that processing efficiency over time shapes the phonology of words [12, 15, 35, 64, 69, 70, 85].", "startOffset": 235, "endOffset": 263}, {"referenceID": 63, "context": "As expected under the general hypothesis tested here, several studies have found the lexicon of languages to exhibit properties that are consistent with the hypothesis that processing efficiency over time shapes the phonology of words [12, 15, 35, 64, 69, 70, 85].", "startOffset": 235, "endOffset": 263}, {"referenceID": 68, "context": "As expected under the general hypothesis tested here, several studies have found the lexicon of languages to exhibit properties that are consistent with the hypothesis that processing efficiency over time shapes the phonology of words [12, 15, 35, 64, 69, 70, 85].", "startOffset": 235, "endOffset": 263}, {"referenceID": 69, "context": "As expected under the general hypothesis tested here, several studies have found the lexicon of languages to exhibit properties that are consistent with the hypothesis that processing efficiency over time shapes the phonology of words [12, 15, 35, 64, 69, 70, 85].", "startOffset": 235, "endOffset": 263}, {"referenceID": 83, "context": "As expected under the general hypothesis tested here, several studies have found the lexicon of languages to exhibit properties that are consistent with the hypothesis that processing efficiency over time shapes the phonology of words [12, 15, 35, 64, 69, 70, 85].", "startOffset": 235, "endOffset": 263}, {"referenceID": 48, "context": "N-gram models of this type are widely used in speech recognition [49, 33] and machine translation [53].", "startOffset": 65, "endOffset": 73}, {"referenceID": 32, "context": "N-gram models of this type are widely used in speech recognition [49, 33] and machine translation [53].", "startOffset": 65, "endOffset": 73}, {"referenceID": 52, "context": "N-gram models of this type are widely used in speech recognition [49, 33] and machine translation [53].", "startOffset": 98, "endOffset": 102}, {"referenceID": 26, "context": ", [27]).", "startOffset": 2, "endOffset": 6}, {"referenceID": 7, "context": "Second, n-gram models have also been successfully used as models of human language processing [8, 25, 48].", "startOffset": 94, "endOffset": 105}, {"referenceID": 24, "context": "Second, n-gram models have also been successfully used as models of human language processing [8, 25, 48].", "startOffset": 94, "endOffset": 105}, {"referenceID": 47, "context": "Second, n-gram models have also been successfully used as models of human language processing [8, 25, 48].", "startOffset": 94, "endOffset": 105}, {"referenceID": 24, "context": "In fact, recent studies have argued that models that primarily rely on the information captured by local context (such as the two preceding words) fair better in explaining word-by-word variation in human processing times than structurally more complex models ([25]; but see also [24, 77]).", "startOffset": 261, "endOffset": 265}, {"referenceID": 23, "context": "In fact, recent studies have argued that models that primarily rely on the information captured by local context (such as the two preceding words) fair better in explaining word-by-word variation in human processing times than structurally more complex models ([25]; but see also [24, 77]).", "startOffset": 280, "endOffset": 288}, {"referenceID": 76, "context": "In fact, recent studies have argued that models that primarily rely on the information captured by local context (such as the two preceding words) fair better in explaining word-by-word variation in human processing times than structurally more complex models ([25]; but see also [24, 77]).", "startOffset": 280, "endOffset": 288}, {"referenceID": 74, "context": "Indeed, the finding we mentioned above, that a word\u2019s probability in context is log-linearly related to the processing difficulty it causes, was based on a trigram estimate of the type employed here [75].", "startOffset": 199, "endOffset": 203}, {"referenceID": 51, "context": "In order to obtain reliable estimates of a word\u2019s trigram probability even when the preceding two words were rarely (or never) observed in the training corpus, we smooth our trigram probabilities using the interpolated Kneser-Ney method [52, 13].", "startOffset": 237, "endOffset": 245}, {"referenceID": 12, "context": "In order to obtain reliable estimates of a word\u2019s trigram probability even when the preceding two words were rarely (or never) observed in the training corpus, we smooth our trigram probabilities using the interpolated Kneser-Ney method [52, 13].", "startOffset": 237, "endOffset": 245}, {"referenceID": 74, "context": "specifically Kneser-Ney smoothed trigram estimates of surprisal that recent work found to be linearly correlated with reaction times [75].", "startOffset": 133, "endOffset": 137}, {"referenceID": 24, "context": "For example, in the psycholinguistic literature on sentence processing, surprisal is usually calculated per word [25, 56, 75].", "startOffset": 113, "endOffset": 125}, {"referenceID": 55, "context": "For example, in the psycholinguistic literature on sentence processing, surprisal is usually calculated per word [25, 56, 75].", "startOffset": 113, "endOffset": 125}, {"referenceID": 74, "context": "For example, in the psycholinguistic literature on sentence processing, surprisal is usually calculated per word [25, 56, 75].", "startOffset": 113, "endOffset": 125}, {"referenceID": 14, "context": ", the information per sound in a word, [15, 78]).", "startOffset": 39, "endOffset": 47}, {"referenceID": 77, "context": ", the information per sound in a word, [15, 78]).", "startOffset": 39, "endOffset": 47}, {"referenceID": 74, "context": "This is essentially the same measure that has found to correlate linearly with word-by-word reading times in English [75].", "startOffset": 117, "endOffset": 121}, {"referenceID": 28, "context": "For example, dependency length is sometimes measured in terms of the number of intervening non-discourse given referents [29], or in terms of the syntactic complexity of intervening material.", "startOffset": 121, "endOffset": 125}, {"referenceID": 75, "context": "All of these measures tend to be highly correlated [76, 80].", "startOffset": 51, "endOffset": 59}, {"referenceID": 22, "context": "For the current purpose, we measure dependency length in words (following [23, 31, 43, 44, 59, 73]).", "startOffset": 74, "endOffset": 98}, {"referenceID": 30, "context": "For the current purpose, we measure dependency length in words (following [23, 31, 43, 44, 59, 73]).", "startOffset": 74, "endOffset": 98}, {"referenceID": 42, "context": "For the current purpose, we measure dependency length in words (following [23, 31, 43, 44, 59, 73]).", "startOffset": 74, "endOffset": 98}, {"referenceID": 43, "context": "For the current purpose, we measure dependency length in words (following [23, 31, 43, 44, 59, 73]).", "startOffset": 74, "endOffset": 98}, {"referenceID": 58, "context": "For the current purpose, we measure dependency length in words (following [23, 31, 43, 44, 59, 73]).", "startOffset": 74, "endOffset": 98}, {"referenceID": 72, "context": "For the current purpose, we measure dependency length in words (following [23, 31, 43, 44, 59, 73]).", "startOffset": 74, "endOffset": 98}, {"referenceID": 17, "context": "This measure has the advantage that it is easy to calculate and achieves broad-coverage (see also [18]).", "startOffset": 98, "endOffset": 102}, {"referenceID": 30, "context": "For our representation of a possible fixed order, we use weighted grammars [31].", "startOffset": 75, "endOffset": 79}, {"referenceID": 30, "context": "This search procedure is similar to one previously used for finding the grammar that minimizes dependency length [31].", "startOffset": 113, "endOffset": 117}, {"referenceID": 22, "context": "There are three directly related previous findings that we are aware of [23, 31, 26].", "startOffset": 72, "endOffset": 84}, {"referenceID": 30, "context": "There are three directly related previous findings that we are aware of [23, 31, 26].", "startOffset": 72, "endOffset": 84}, {"referenceID": 25, "context": "There are three directly related previous findings that we are aware of [23, 31, 26].", "startOffset": 72, "endOffset": 84}, {"referenceID": 30, "context": "Gildea and Temperley [31] investigated the average dependency length of two closely related languages, English and German.", "startOffset": 21, "endOffset": 25}, {"referenceID": 22, "context": "Ferrer i Cancho [23] investigates Czech and Romanian, while Futrell et al.", "startOffset": 16, "endOffset": 20}, {"referenceID": 25, "context": "[26] study 37 languages spoken worldwide.", "startOffset": 0, "endOffset": 4}, {"referenceID": 19, "context": "These studies have tested whether learners of miniature languages designed by experimenters prefer languages that increase processing efficiency [20, 21, 22].", "startOffset": 145, "endOffset": 157}, {"referenceID": 20, "context": "These studies have tested whether learners of miniature languages designed by experimenters prefer languages that increase processing efficiency [20, 21, 22].", "startOffset": 145, "endOffset": 157}, {"referenceID": 21, "context": "These studies have tested whether learners of miniature languages designed by experimenters prefer languages that increase processing efficiency [20, 21, 22].", "startOffset": 145, "endOffset": 157}, {"referenceID": 33, "context": "[34]).", "startOffset": 0, "endOffset": 4}, {"referenceID": 18, "context": "For example, [19] finds that language learners prefer languages that reduce unnecessary uncertainty about the syntactic structure of sentences.", "startOffset": 13, "endOffset": 17}, {"referenceID": 56, "context": ", interference in memory due to similar words or referents, [57, 61]).", "startOffset": 60, "endOffset": 68}, {"referenceID": 60, "context": ", interference in memory due to similar words or referents, [57, 61]).", "startOffset": 60, "endOffset": 68}], "year": 2015, "abstractText": "Most languages use the relative order between words to encode meaning relations. Languages differ, however, in what orders they use and how these orders are mapped onto different meanings. We test the hypothesis that \u2013despite these differences\u2013 human languages might constitute different \u2018solutions\u2019 to common pressures of language use. Using Monte Carlo simulations over data from five languages, we find that their word orders are efficient for processing in terms of both dependency length and local lexical probability. This suggests that biases originating in how the brain understands language strongly constrain how human languages change over generations.", "creator": "LaTeX with hyperref package"}}}