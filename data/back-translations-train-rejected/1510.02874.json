{"id": "1510.02874", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "10-Oct-2015", "title": "TSEB: More Efficient Thompson Sampling for Policy Learning", "abstract": "In model-based solution approaches to the problem of learning in an unknown environment, exploring to learn the model parameters takes a toll on the regret. The optimal performance with respect to regret or PAC bounds is achievable, if the algorithm exploits with respect to reward or explores with respect to the model parameters, respectively. In this paper, we propose TSEB, a Thompson Sampling based algorithm with adaptive exploration bonus that aims to solve the problem with tighter PAC guarantees, while being cautious on the regret as well. The proposed approach maintains distributions over the model parameters which are successively refined with more experience. At any given time, the agent solves a model sampled from this distribution, and the sampled reward distribution is skewed by an exploration bonus in order to generate more informative exploration. The policy by solving is then used for generating more experience that helps in updating the posterior over the model parameters. We provide a detailed analysis of the PAC guarantees, and convergence of the proposed approach. We show that our adaptive exploration bonus encourages the additional exploration required for better PAC bounds on the algorithm. We provide empirical analysis on two different simulated domains.", "histories": [["v1", "Sat, 10 Oct 2015 04:16:08 GMT  (732kb,D)", "http://arxiv.org/abs/1510.02874v1", "9 pages"]], "COMMENTS": "9 pages", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["p prasanna", "sarath chandar", "balaraman ravindran"], "accepted": false, "id": "1510.02874"}, "pdf": {"name": "1510.02874.pdf", "metadata": {"source": "CRF", "title": "TSEB: More Efficient Thompson Sampling for Policy Learning", "authors": ["Prasanna P Sarath Chandar", "Balaraman Ravindran"], "emails": ["pp1403@gmail.com", "apsarathchandar@gmail.com", "ravi@cse.iitm.ac.in"], "sections": [{"heading": null, "text": "In model-based solutions to the problem of learning in an unknown environment, exploring the model parameters takes its toll on regret. Optimal performance in terms of regret or limitations of the PAC is achievable if the algorithm solves the problem with stricter PAC guarantees while being careful with the model parameters. In this paper, we propose TSEB, a Thompson sampling-based algorithm with an adaptive exploration bonus that aims to solve the problem with stricter PAC guarantees while also being cautious with regret. The proposed approach maintains distributions of the model parameters, which are gradually refined with more experience. At any given time, the agent solves a model tested from this distribution, and the proven reward distribution is distorted by an exploration bonus to generate a more informative exploration."}, {"heading": "1 INTRODUCTION", "text": "In the standard version of Reinforcement Learning (RL), the environment with which the agent interacts has been modeled as the Markov Decision Process (MDP), and the agent's goal is to learn a policy that is designed to maximize the cumulative reward he receives. If the parameters of the MDP are known, then the learning process is simple, and the optimal policy can be taught using traditional DP methods (Sutton and Barto, 1998). However, in any real application, the parameters of the MDP are not known. In such a scenario, the agent can directly learn the policy that maximizes return (model-free learning), or the agent can try to appreciate the parameters of the MDP and learn a policy."}, {"heading": "2 PRELIMINARIES", "text": "In reinforcement theory, a learning agent interacts with a world modeled as MDP. < S, A, R, T, \u03b3 >. The description of the MDP consists of, S \u2212 \u2212 r, the set of all states, A, the set of all actions, R, the reward function, R: SxA \u2192 R, T, the transition function, T: SxAxS \u2192 [0, 1] and the discount factor \u03b3 (0, 1). The agent must learn an optimal action mapping, B: S \u2192 A, which maximizes cumulative reward over a finite or infinite horizon, H: If the model parameters are known, the optimal policy, p, can be achieved by solving the MDP techniques such as evaluation techniques, political iteration or optimization methods. Further in the discussion, we will use a metric to limit the distance between the value function of the sampled and the true MDP."}, {"heading": "3 TSEB ALGORITHM", "text": "TSEB is an episodic approach where the incremental sampled model grows closer to the true model. Solving the convergent model gives us a near optimal policy. From the problem as it is, it is intuitive to understand that the agent must learn the true model in order to arrive at an optimal policy. TSEB has a modified Bellman update that takes into account the Exploration Bonus. Thus, the reward is a convex combination of the reward gained from the sampled world and the Exploration Bonus calculated for that state (s) (s), (a) in this episode. The Bellman update will be, V (s) is a convex combination of the reward generated from the sampled world (s) + an Exploration Bonus calculated for that state (s)."}, {"heading": "4 THEORETICAL ANALYSIS OF TSEB", "text": "The PAC analysis provides an upper limit on the number of suboptimal steps of an asymptotic agent that is required for selecting the algorithms to arrive at an optimal solution with probability. Suppose the algorithm requires a set of samples, M. Each sample is (st, at, st + 1, rt + 1). Although TS theory gives us guarantees on O (logT) (Gopalan and Mannor, 2015) where T is the number of time steps, we have no idea of PAC-bound for TS. This is primarily because the algorithm must be explorative to learn the model parameters for it. The constellation is here when the algorithm will regret its worse. Hence, the greedy action selection does not allow the agent to be explorative. In TSEB with the addition of the exploration bonus and thus skewing the exploration in a way that would make the exploration part of the agent."}, {"heading": "5 ARGUMENTS ON REGRET", "text": "The discussion so far explains the PAC guarantees offered by the TSEB algorithm; the claim that the algorithm will not deteriorate in regret has not been addressed so far; after a greedy policy of sampled MDP is not very different from the TS approach; the parameters sampled in each episode grow closer to the true model, as has been discussed empirically and theoretically in previous relevant sections; because the TSEB agent acts greedily with the sampled model parameters and the model parameters converge, after a certain number of episodes the agent will act optimally with the true parameters, since the greedy policy in the MDB will be an optimal policy approach; the Exploration Bonus, a linearly decreasing component in the modified Bellman update, will become meaningless, even if it does not become zero; this ensures that TSEB behaves like pure Thompson sampling after sufficient research; and let's take any step we choose."}, {"heading": "6 EMPIRICAL ANALYSIS", "text": "In this section we analyze experimentally the performance of TSEB. We conduct experiments in two simulated domains: Chain world (Kolter and Ng, 2009) and Queuing Domain (Gopalan and Mannor, 2015). The aim of the experiments is to experimentally confirm the claim of convergence of faith and to analyze the algorithm under different values of the target parameter, \u03bb [0,1]."}, {"heading": "6.1 CHAIN WORLD", "text": "The first state has a stochastic reward, of a Gaussian N (0.2, 0.5). The analysis shows a better performance (cumulative sum of rewards) for each non-zero value. This is intuitive, because if it is 0, the algorithm behaves only to decrease the variance and ignores the rewards obtained in the world (Table 2). The analysis shows a better performance (cumulative sum of rewards) for each non-zero value. The performance has a high deviation and is contradictory if it is 0. The algorithm behaves only to decrease the variance and ignores the rewards obtained in the world. This behavior is expected, but the performance increases with the increase of the effects and decreases after 0.5. The performance has a high deviation and is contradictory if it is the TS case; this is the TS case. The maximum cumulative reward in this case."}, {"heading": "6.2 QUEUING WORLD", "text": "We analyze the TSEB algorithm with different \u03bb values (Table 3) in the queuing world, which are defined in (Gopalan and Mannor, 2015) each in reward units. MDP states are simply the number of packets in the queue at a given time, i.e., S = {0,1,2,..., 50}. Applying the SLOW (resp. FAST) service causes a packet from the queue to have a probability of 0. 3 (resp. 0,8) if it is not empty, i.e. the service model is Bernoulli (\u00b5i), where the packet processing probability is under service type i = 1,2. Actions 1 and 2 each cause immediate costs of 0 and -0.25 units, respectively. In addition to these costs, there is a cost of -0.1 per packet in the queue."}, {"heading": "7 RELATED WORK", "text": "In fact, it is the case that one is able to go in search of a solution that is able to go in search of a solution that is able to find a solution, that is able to find a solution, that is able to find a solution, and that is able to find a solution that is able to find a solution, that is able to find a solution, that is able to find a solution, that is able to find a solution, and that is able to find a solution that is able to find a solution, that is able to find a solution, that is able to find a solution, that is able to find a solution that is able to find a solution, that is able to find a solution. \""}, {"heading": "8 CONCLUSION", "text": "In this paper, we propose TSEB - a Thompson sampling approach for model-based RL that uses an adaptive exploration bonus, which is the first TS variant to have a PAC limit. We have introduced a target parameter that controls how much the Exploration Bonus influences the policies learned on a sampled MDP. Adjusting this parameter allows us to achieve better empirical performance even in terms of regret. While this work provides an initial intuition into TS PAC analysis, more work needs to be done to establish a theory of useful exploration bonuses and performance guarantees. Extending the model estimation to a non-parameterized environment without narrow parameter space constraints will also be a useful extension that will apply to a wide range of problems."}], "references": [{"title": "Analysis of thompson sampling for the multi-armed bandit problem", "author": ["S. Agrawal", "N. Goyal"], "venue": "arXiv preprint arXiv:1111.1797.", "citeRegEx": "Agrawal and Goyal,? 2011", "shortCiteRegEx": "Agrawal and Goyal", "year": 2011}, {"title": "Thompson sampling for contextual bandits with linear payoffs", "author": ["S. Agrawal", "N. Goyal"], "venue": "30th International Conference on Machine Learning (ICML).", "citeRegEx": "Agrawal and Goyal,? 2013", "shortCiteRegEx": "Agrawal and Goyal", "year": 2013}, {"title": "A bayesian sampling approach to exploration in reinforcement learning", "author": ["J. Asmuth", "L. Li", "M.L. Littman", "A. Nouri", "D. Wingate"], "venue": "Uncertainty in Artificial Intelligence.", "citeRegEx": "Asmuth et al\\.,? 2009", "shortCiteRegEx": "Asmuth et al\\.", "year": 2009}, {"title": "Finitetime analysis of the multiarmed bandit problem", "author": ["P. Auer", "N. Cesa-Bianchi", "P. Fischer"], "venue": "Machine learning, 47(2-3):235\u2013256.", "citeRegEx": "Auer et al\\.,? 2002", "shortCiteRegEx": "Auer et al\\.", "year": 2002}, {"title": "Logarithmic online regret bounds for undiscounted reinforcement learning", "author": ["P. Auer", "R. Ortner"], "venue": "Advances in Neural Information Processing Systems, pages 49\u201356.", "citeRegEx": "Auer and Ortner,? 2006", "shortCiteRegEx": "Auer and Ortner", "year": 2006}, {"title": "An empirical evaluation of thompson sampling", "author": ["O. Chapelle", "L. Li"], "venue": "Advances in neural information processing systems, pages 2249\u20132257.", "citeRegEx": "Chapelle and Li,? 2011", "shortCiteRegEx": "Chapelle and Li", "year": 2011}, {"title": "A measure of asymptotic efficiency for tests of a hypothesis based on the sum of observations", "author": ["H. Chernoff"], "venue": "Ann. Math. Statist., 23(4):493\u2013507.", "citeRegEx": "Chernoff,? 1952", "shortCiteRegEx": "Chernoff", "year": 1952}, {"title": "Thompson sampling for learning parameterized markov decision processes", "author": ["A. Gopalan", "S. Mannor"], "venue": "Proceedings of The 28th Conference on Learning Theory, COLT 2015, Paris, France, July 3-6, 2015, pages 861\u2013898.", "citeRegEx": "Gopalan and Mannor,? 2015", "shortCiteRegEx": "Gopalan and Mannor", "year": 2015}, {"title": "Thompson sampling for complex bandit problems", "author": ["A. Gopalan", "S. Mannor", "Y. Mansour"], "venue": "arXiv preprint arXiv:1311.0466.", "citeRegEx": "Gopalan et al\\.,? 2013", "shortCiteRegEx": "Gopalan et al\\.", "year": 2013}, {"title": "Learning in embedded systems", "author": ["L.P. Kaelbling"], "venue": "Technical report, DTIC Document.", "citeRegEx": "Kaelbling,? 1990", "shortCiteRegEx": "Kaelbling", "year": 1990}, {"title": "Near-bayesian exploration in polynomial time", "author": ["J.Z. Kolter", "A.Y. Ng"], "venue": "Proceedings of the 26th Annual International Conference on Machine Learning, pages 513\u2013520. ACM.", "citeRegEx": "Kolter and Ng,? 2009", "shortCiteRegEx": "Kolter and Ng", "year": 2009}, {"title": "Approximate homomorphisms: A framework for nonexact minimization in markov decision processes", "author": ["B. Ravindran", "A. Barto"], "venue": "Proceedings of the 5th International Conference on Knowledge Based Computer Systems.", "citeRegEx": "Ravindran and Barto,? 2004", "shortCiteRegEx": "Ravindran and Barto", "year": 2004}, {"title": "An information-theoretic analysis of thompson sampling", "author": ["D. Russo", "B.V. Roy"], "venue": "CoRR, abs/1403.5341.", "citeRegEx": "Russo and Roy,? 2014", "shortCiteRegEx": "Russo and Roy", "year": 2014}, {"title": "Variancebased rewards for approximate bayesian reinforcement learning", "author": ["J. Sorg", "S. Singh", "R.L. Lewis"], "venue": null, "citeRegEx": "Sorg et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Sorg et al\\.", "year": 2010}, {"title": "An analysis of model-based interval estimation for markov decision", "author": ["A.L. Strehl", "M.L. Littman"], "venue": null, "citeRegEx": "Strehl and Littman,? \\Q2008\\E", "shortCiteRegEx": "Strehl and Littman", "year": 2008}, {"title": "An analysis of model-based interval estimation for markov decision processes", "author": ["A.L. Strehl", "M.L. Littman"], "venue": "Journal of Computer and System Sciences, 74(8):1309\u20131331.", "citeRegEx": "Strehl and Littman,? 2008b", "shortCiteRegEx": "Strehl and Littman", "year": 2008}, {"title": "A bayesian framework for reinforcement learning", "author": ["M. Strens"], "venue": "ICML, pages 943\u2013950.", "citeRegEx": "Strens,? 2000", "shortCiteRegEx": "Strens", "year": 2000}, {"title": "Introduction to reinforcement learning", "author": ["R.S. Sutton", "A.G. Barto"], "venue": "MIT Press.", "citeRegEx": "Sutton and Barto,? 1998", "shortCiteRegEx": "Sutton and Barto", "year": 1998}, {"title": "On the likelihood that one unknown probability exceeds another in view of the evidence of two samples", "author": ["W.R. Thompson"], "venue": "Biometrika, pages 285\u2013294.", "citeRegEx": "Thompson,? 1933", "shortCiteRegEx": "Thompson", "year": 1933}], "referenceMentions": [{"referenceID": 17, "context": "If the parameters of the MDP are known, then the learning process is straight forward, and the optimal policy can be learnt by traditional DP-methods (Sutton and Barto, 1998).", "startOffset": 150, "endOffset": 174}, {"referenceID": 16, "context": "Recently Model-based learning approaches have been receiving increasing attention (Strens, 2000; Kolter and Ng, 2009; Sorg et al., 2010; Russo and Roy, 2014).", "startOffset": 82, "endOffset": 157}, {"referenceID": 10, "context": "Recently Model-based learning approaches have been receiving increasing attention (Strens, 2000; Kolter and Ng, 2009; Sorg et al., 2010; Russo and Roy, 2014).", "startOffset": 82, "endOffset": 157}, {"referenceID": 13, "context": "Recently Model-based learning approaches have been receiving increasing attention (Strens, 2000; Kolter and Ng, 2009; Sorg et al., 2010; Russo and Roy, 2014).", "startOffset": 82, "endOffset": 157}, {"referenceID": 12, "context": "Recently Model-based learning approaches have been receiving increasing attention (Strens, 2000; Kolter and Ng, 2009; Sorg et al., 2010; Russo and Roy, 2014).", "startOffset": 82, "endOffset": 157}, {"referenceID": 18, "context": "Much of the recent work has been focused on Thompson sampling (TS) (Thompson, 1933) based approaches both in simpler bandit settings (Chapelle and Li, 2011; Agrawal and Goyal, 2011, 2013; Gopalan et al.", "startOffset": 67, "endOffset": 83}, {"referenceID": 5, "context": "Much of the recent work has been focused on Thompson sampling (TS) (Thompson, 1933) based approaches both in simpler bandit settings (Chapelle and Li, 2011; Agrawal and Goyal, 2011, 2013; Gopalan et al., 2013), as well as the full MDP problem (Strens, 2000; Gopalan and Mannor, 2015).", "startOffset": 133, "endOffset": 209}, {"referenceID": 8, "context": "Much of the recent work has been focused on Thompson sampling (TS) (Thompson, 1933) based approaches both in simpler bandit settings (Chapelle and Li, 2011; Agrawal and Goyal, 2011, 2013; Gopalan et al., 2013), as well as the full MDP problem (Strens, 2000; Gopalan and Mannor, 2015).", "startOffset": 133, "endOffset": 209}, {"referenceID": 16, "context": ", 2013), as well as the full MDP problem (Strens, 2000; Gopalan and Mannor, 2015).", "startOffset": 41, "endOffset": 81}, {"referenceID": 7, "context": ", 2013), as well as the full MDP problem (Strens, 2000; Gopalan and Mannor, 2015).", "startOffset": 41, "endOffset": 81}, {"referenceID": 16, "context": "The Bayesian RL approach proposed in (Strens, 2000) is an episodic way of incrementally converging to the true parameters of the model.", "startOffset": 37, "endOffset": 51}, {"referenceID": 18, "context": "This approach of posterior sampling is known as Thompson sampling (Thompson, 1933).", "startOffset": 66, "endOffset": 82}, {"referenceID": 5, "context": "Ever since Chapelle and Li (Chapelle and Li, 2011) discussed the efficacy of TS approaches for reinforcement learning, there have been concerted attempts made to achieve a better theoretical understanding of such approaches.", "startOffset": 27, "endOffset": 50}, {"referenceID": 7, "context": "Apart from the results in the bandit setting, Thompson sampling approach for full RL has been shown to work well in practice and has been shown to be regret optimal (Gopalan and Mannor, 2015).", "startOffset": 165, "endOffset": 191}, {"referenceID": 10, "context": ", (Kolter and Ng, 2009) proposed Bayesian Exploration Bonus (BEB) algorithm which added a constant exploration bonus to the problem of solving for an optimal policy in an unknown environment.", "startOffset": 2, "endOffset": 23}, {"referenceID": 10, "context": "(Kolter and Ng, 2009) computes a point estimate of the MDP and solves for the optimal policy in every episode.", "startOffset": 0, "endOffset": 21}, {"referenceID": 11, "context": "The metric is inspired from the homomorphism literature (Ravindran and Barto, 2004).", "startOffset": 56, "endOffset": 83}, {"referenceID": 11, "context": "Thus, with the given descriptions the difference in the values of the state in M1 and M2, v1 and v2, which we define as the f-function, can be bounded by the following expression (Ravindran and Barto, 2004),", "startOffset": 179, "endOffset": 206}, {"referenceID": 13, "context": "If we have a Dirichlet distribution governing the transition, Kp can be bounded by 1 n(s,a) (Sorg et al., 2010), where n(s, a) is the number of times the state-action pair was observed.", "startOffset": 92, "endOffset": 111}, {"referenceID": 13, "context": "TSEB, unlike the most other previous algorithms (Except (Sorg et al., 2010)) uses the uncertainty in the estimates to structure the exploration bonus.", "startOffset": 56, "endOffset": 75}, {"referenceID": 13, "context": "Further theoretical analysis shows us that the bound is indeed tighter than in (Sorg et al., 2010).", "startOffset": 79, "endOffset": 98}, {"referenceID": 13, "context": "puted here even more cleverly, thus avoiding integrating over the parameters, compared to (Sorg et al., 2010).", "startOffset": 90, "endOffset": 109}, {"referenceID": 10, "context": "Also there is a principal difference with (Kolter and Ng, 2009), wherein the exploration of the agent is concentrated around the uncertain region and the uncertainty is not assumed to be uniform over the world.", "startOffset": 42, "endOffset": 63}, {"referenceID": 7, "context": "Though, TS theory gives us regret guarantees at O(logT) (Gopalan and Mannor, 2015), where T is the number of time-steps, we don\u2019t have a notion of PAC-bound for TS.", "startOffset": 56, "endOffset": 82}, {"referenceID": 6, "context": "This is an extension of the Chernoff bounds (Chernoff, 1952) in a known variance setting.", "startOffset": 44, "endOffset": 60}, {"referenceID": 10, "context": "BEB (Kolter and Ng, 2009) O ( SAH 2 log SA \u03b4 )", "startOffset": 4, "endOffset": 25}, {"referenceID": 13, "context": "Variance Based (Sorg et al., 2010) O ( \u03b3SA \u03b4 2(1\u2212\u03b3)2 )", "startOffset": 15, "endOffset": 34}, {"referenceID": 7, "context": "The recent work on regret in parameterized MDP (Gopalan and Mannor, 2015) is a major contribution to the regret analysis of the full RL Thompson sampling approach.", "startOffset": 47, "endOffset": 73}, {"referenceID": 10, "context": "We run experiments in two simulated domains, Chain world (Kolter and Ng, 2009) and Queuing Domain (Gopalan and Mannor, 2015).", "startOffset": 57, "endOffset": 78}, {"referenceID": 7, "context": "We run experiments in two simulated domains, Chain world (Kolter and Ng, 2009) and Queuing Domain (Gopalan and Mannor, 2015).", "startOffset": 98, "endOffset": 124}, {"referenceID": 7, "context": "We analyse the TSEB algorithm with different \u03bb values (Table 3) in the Queuing world defined in (Gopalan and Mannor, 2015).", "startOffset": 96, "endOffset": 122}, {"referenceID": 9, "context": "In (Kaelbling, 1990), an algorithm proposed as Interval estimation Q-learning (IEQ), the action with the highest upper bound on the underlying Q-value gets chosen.", "startOffset": 3, "endOffset": 20}, {"referenceID": 3, "context": "This has been followed in approaches as early as UCB(Auer et al., 2002), where the empirical mean, \u03bc\u0302i, of an arm i is over-estimated by the confidence interval of the estimated mean.", "startOffset": 52, "endOffset": 71}, {"referenceID": 4, "context": "And, for solving an MDP, the UCRL (Auer and Ortner, 2006) takes an approach inspired by the UCB technique for over estimation to aid exploration.", "startOffset": 34, "endOffset": 57}, {"referenceID": 13, "context": "In an unknown environment setting, the variance based approach to over estimate the value of a state to aid in exploration was proposed in (Sorg et al., 2010), but it is not a TS approach.", "startOffset": 139, "endOffset": 158}, {"referenceID": 0, "context": "Also, the theoretical guarantees of TS have not been analyzed until recently (Agrawal and Goyal, 2011).", "startOffset": 77, "endOffset": 102}, {"referenceID": 7, "context": "Recently, (Gopalan and Mannor, 2015) gave a regret analysis of TS in full MDP setting that is logarithmic in T , the time-steps.", "startOffset": 10, "endOffset": 36}, {"referenceID": 12, "context": "(Russo and Roy, 2014) highlighted an information-theoretic analysis of TS, giving a better regret bound, considering the entropy of the actiondistribution.", "startOffset": 0, "endOffset": 21}, {"referenceID": 16, "context": "In the last decade, parameter estimation was extended for the MDP setting; an episodic way of solving for the model estimation in unknown environment (Strens, 2000).", "startOffset": 150, "endOffset": 164}, {"referenceID": 10, "context": "More recently, (Kolter and Ng, 2009) proposed Bayesian Exploration Policy (BEB) algorithm which added a constant exploration bonus to the standard (non-Thompson sampling based) Bayesian RL.", "startOffset": 15, "endOffset": 36}, {"referenceID": 15, "context": "This is improved upon the MBIE-EB (Strehl and Littman, 2008b), an interval based exploration bonus algorithm, by increasing the decay rate.", "startOffset": 34, "endOffset": 61}, {"referenceID": 10, "context": "(Kolter and Ng, 2009) states that the Bayesian approach cannot have a PAC solution if it doesn\u2019t encode an exploration bonus.", "startOffset": 0, "endOffset": 21}, {"referenceID": 16, "context": "In line of (Strens, 2000) BOSS, Best of Sampled Sets (Asmuth et al.", "startOffset": 11, "endOffset": 25}, {"referenceID": 2, "context": "In line of (Strens, 2000) BOSS, Best of Sampled Sets (Asmuth et al., 2009) that samples multiple models and merges them.", "startOffset": 53, "endOffset": 74}, {"referenceID": 10, "context": "The most recent of them include computing the mean MDP (Kolter and Ng, 2009) and ML MDP (Sorg et al.", "startOffset": 55, "endOffset": 76}, {"referenceID": 13, "context": "The most recent of them include computing the mean MDP (Kolter and Ng, 2009) and ML MDP (Sorg et al., 2010).", "startOffset": 88, "endOffset": 107}], "year": 2015, "abstractText": "In model-based solution approaches to the problem of learning in an unknown environment, exploring to learn the model parameters takes a toll on the regret. The optimal performance with respect to regret or PAC bounds is achievable, if the algorithm exploits with respect to reward or explores with respect to the model parameters, respectively. In this paper, we propose TSEB, a Thompson Sampling based algorithm with adaptive exploration bonus that aims to solve the problem with tighter PAC guarantees, while being cautious on the regret as well. The proposed approach maintains distributions over the model parameters which are successively refined with more experience. At any given time, the agent solves a model sampled from this distribution, and the sampled reward distribution is skewed by an exploration bonus in order to generate more informative exploration. The policy by solving is then used for generating more experience that helps in updating the posterior over the model parameters. We provide a detailed analysis of the PAC guarantees, and convergence of the proposed approach. We show that our adaptive exploration bonus encourages the additional exploration required for better PAC bounds on the algorithm. We provide empirical analysis on two different simulated domains.", "creator": "LaTeX with hyperref package"}}}