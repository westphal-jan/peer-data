{"id": "1510.02969", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "10-Oct-2015", "title": "Do Deep Neural Networks Learn Facial Action Units When Doing Expression Recognition?", "abstract": "Despite being the appearance-based classifier of choice in recent years, relatively few works have examined how much convolutional neural networks (CNNs) can improve performance on accepted expression recognition benchmarks and, more importantly, examine what it is they actually learn. In this work, not only do we show that CNNs can achieve strong performance, but we also introduce an approach to decipher which portions of the face influence the CNN's predictions. First, we train a zero-bias CNN on facial expression data and achieve, to our knowledge, state-of-the-art performance on two expression recognition benchmarks: the extended Cohn-Kanade (CK+) dataset and the Toronto Face Dataset (TFD). We then qualitatively analyze the network by visualizing the spatial patterns that maximally excite different neurons in the convolutional layers and show how they resemble Facial Action Units (FAUs). Finally, we use the FAU labels provided in the CK+ dataset to verify that the FAUs observed in our filter visualizations indeed align with the subject's facial movements.", "histories": [["v1", "Sat, 10 Oct 2015 18:53:21 GMT  (1702kb,D)", "https://arxiv.org/abs/1510.02969v1", "Accepted at ICCV 2015 CV4AC Workshop"], ["v2", "Fri, 28 Oct 2016 06:12:07 GMT  (1394kb,D)", "http://arxiv.org/abs/1510.02969v2", "Accepted at ICCV 2015 CV4AC Workshop. Corrected numbers in Table 1 and some minor typos"], ["v3", "Thu, 16 Mar 2017 03:07:21 GMT  (1393kb,D)", "http://arxiv.org/abs/1510.02969v3", "Accepted at ICCV 2015 CV4AC Workshop. Corrected numbers in Tables 2 and 3"]], "COMMENTS": "Accepted at ICCV 2015 CV4AC Workshop", "reviews": [], "SUBJECTS": "cs.CV cs.LG cs.NE", "authors": ["pooya khorrami", "tom le paine", "thomas s huang"], "accepted": false, "id": "1510.02969"}, "pdf": {"name": "1510.02969.pdf", "metadata": {"source": "CRF", "title": "Do Deep Neural Networks Learn Facial Action Units When Doing Expression Recognition?", "authors": ["Pooya Khorrami", "Tom Le Paine", "Thomas S. Huang"], "emails": ["pkhorra2@illinois.edu", "paine1@illinois.edu", "t-huang1@illinois.edu"], "sections": [{"heading": "1. Introduction", "text": "In this sense, the development of interactive computer systems in artificial intelligence is crucial. Extensive work in this area has shown that only a small number of regions change their expression and are located around the eyes, nose and mouth of the subject. In this context, Paul Ekman has proposed the Facial Action Coding System (FACS), which lists these regions and describes how each facial expression can be described as a combination of several action units (AUs), each corresponding to a specific muscle group in the face. However, in this case, computer-accurate learning of the parts of the face that convey emotions is a non-trivial task. Previous work in facial recognition can be divided into two broad categories: AU-based methods and rules-based methods."}, {"heading": "2. Related Work", "text": "In most cases, this is one of several (typically 7) expression phases. Until recently, most of the mentioned expression phases were able to unfold at the hand-picked level."}, {"heading": "3. Our Approach", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "3.1. Network Architecture", "text": "For all the experiments we present in this paper, we use a classic Convolutionary Neural Network with Feed. The networks we use consist of three Convolutionary Layers with 64, 128 and 256 filters, respectively, and filter sizes of 5x5, followed by activation functions of the ReLU (Rectified Linear Unit). Max Pooling Layers are placed after the first two Convolutionary Layers, while Quadrant Pooling [3] is applied after the third. The square Pooling Layer is then followed by a fully connected layer with 300 hidden units, and finally a Softmax Layer for classification. The Softmax Layer contains somewhere between 6-8 expressions, equal to the number of expressions in the training set.One modification we introduce into the classical configuration is that we ignore the distortions of the Convolutionary Layers."}, {"heading": "3.2. Network Training", "text": "In training our network, we train from the ground up with stochastic gradient descent with a stack size of 64, a mentum set to 0.9 and a weight decay parameter of 1e-5. We use a constant learning rate of 0.01 and do not use any form of annealing. The parameters of each layer are randomly initialized by drawing from a Gaussian distribution with zero mean and standard deviation \u03c3 = kNFAN IN, where NFAN IN is the number of input connections to each layer and k is uniformly pulled out of the range: [0.2, 1,2]. We also use suspenders and various forms of data augmentation to regulate our network and combat overfitting. We apply suspensions to the fully connected layer with a probability of 0.5 (i.e. the output of each neuron is set to zero with a probability of 0.5)."}, {"heading": "4. Experiments and Analysis", "text": "In our experiments, we use two sets of facial expressions: the extended Cohn-Kanade database (CK +) [18] and the Toronto Face Dataset (TFD) [26]. Meanwhile, the CK + database contains 327 image sequences, each of which is assigned one of 7 expressive labels: anger, contempt, disgust, fear, happiness, sadness, and surprise. To be fair, we follow the protocol used in previous work [15, 17] and use the first frame of each sequence as a neutral frame in addition to the last three expressive frames to form our dataset, resulting in a total of 1308 images and a total of 8 classes. We then divide the frames into 10 subjectively independent subsets in the manner represented by [15] and perform 10-fold cross-validation. TFD is a merging of several facial expression datasets, each containing only resistance data."}, {"heading": "4.1. Performance on Toronto Face Database (TFD)", "text": "First, we analyze CNN's discriminatory ability by evaluating its performance on the TFD dataset. Table 1 shows the recognition accuracy achieved in training a CNN without bias from random initialization with no other regularization, as well as CNNs with dropout (D), data augmentation (A), or both (AD), and we incorporate recognition accuracies from previous methods. Two key observations emerge from the results in Table 1: (i) Unsurprisingly, regulation significantly increases performance (ii) Data augmentation improves performance over regular CNN more than dropout (9.4% vs. 2.8%). Furthermore, when using both dropout and data augmentation, our model can outperform the previous state-of-the-art performance of the TFD by 3.6%."}, {"heading": "4.2. Performance on the Extended Cohn-Kanade", "text": "The CK + dataset usually contains eight terms (anger, contempt, disgust, fear, joy, neutrality, sadness and surprise), but many papers [34, 24, 17] ignore the samples designated as neutral or contemptible and evaluate only the six basic emotions. To ensure a fair comparison, we have therefore trained two different models: we present the results of the eight-grade model in Table 2 and the results of the six-grade model in Table 3. For the eight-grade model, we perform the same study as for the TFD and find quite similar results. Here, too, regulation seems to play a significant role in achieving good performance. Data augmentation leads to a significant performance boost (16.4%) and, in combination with an abortion, to an increase of 16.9%. For the eight-grade and six-class models, we achieve state-of-the-art and almost state-of-the-art accuracy in the CK + dataset."}, {"heading": "4.3. Visualization of higher-level neurons", "text": "Now, with a strong discriminatory model in hand, we will analyze which facial regions the neural network identifies as the most discriminatory in performing the classification. To do this, we will use the visualization technology presented by Zeiler and Fergus in [32]. For each data set, we will look at the third revolutionary layer and for each filter, we will find the N images in the selected educational set that produces the strongest magnitude response. We will then leave the strongest neuron high and set all other activations to zero, and use the devolutionary network to reconstruct the region in pixel space. For our experiments, we chose N = 10 training images that further refine our reconstructions by proposing a technique called \"Guided Backpropagation,\" which Springenberg et al. in [25] \"Guided Backpropogation\" aims to improve the reconstructed spatial patterns by not relying solely on the signal given by the upper activations."}, {"heading": "4.4. Finding Correspondences Between Filter Activations and the Ground Truth Facial Action", "text": "The fact is that we are going to be able to be in a position, and that we are going to be able, we are going to be able to be in a position, we are going to be able to be in a position, we are going to be able to be in a position, we are going to be able to be in a position, we are going to be able to be in a position, we are going to be able to put ourselves in a position, we are going to be able to put ourselves in a position, we are going to put ourselves in a position. \""}, {"heading": "5. Conclusions", "text": "In this work, we demonstrated both qualitatively and quantitatively that CNNs trained for emotion recognition are actually able to model high-grade features that correspond strongly to FAUs. Qualitatively, we showed which parts of the face provided the most diverse information by visualizing the spatial patterns that were most excited in the winding layers of our learned networks. Meanwhile, we quantitatively correlated the numerical activations of the visualized filters with the actual facial movements of the subjects using the FAU labels in the CK + dataset. Finally, we showed how a biased CNN can achieve state-of-the-art recognition accuracy on the extended Cohn Kanad (CK +) dataset and the Toronto Face Dataset (TFD)."}, {"heading": "Acknowledgments", "text": "The Tesla K40 GPU used for this research was donated by NVIDIA Corporation. The authors would also like to thank Dr. Kevin Brady, Dr. Charlie Dagli, Professor Yun Fu and Professor Usman Tariq for their insightful comments and suggestions on this work."}], "references": [{"title": "Recognizing facial expression: machine learning and application to spontaneous behavior", "author": ["M.S. Bartlett", "G. Littlewort", "M. Frank", "C. Lainscsek", "I. Fasel", "J. Movellan"], "venue": "In CVPR,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2005}, {"title": "An analysis of singlelayer networks in unsupervised feature learning", "author": ["A. Coates", "A.Y. Ng", "H. Lee"], "venue": "In International conference on artificial intelligence and statistics,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2011}, {"title": "Empath: A neural network that categorizes facial expressions", "author": ["M.N. Dailey", "G.W. Cottrell", "C. Padgett", "R. Adolphs"], "venue": "Journal of cognitive neuroscience,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2002}, {"title": "Emotion recognition in the wild challenge 2014: Baseline, data and protocol", "author": ["A. Dhall", "R. Goecke", "J. Joshi", "K. Sikka", "T. Gedeon"], "venue": "In 16th ACM International Conference on Multimodal Interaction. ACM,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2014}, {"title": "Emotion recognition in the wild challenge", "author": ["A. Dhall", "R. Goecke", "J. Joshi", "M. Wagner", "T. Gedeon"], "venue": "In Proceedings of the 15th ACM on International conference on multimodal interaction,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2013}, {"title": "Facial action coding system", "author": ["P. Ekman", "W.V. Friesen"], "venue": null, "citeRegEx": "7", "shortCiteRegEx": "7", "year": 1977}, {"title": "Emfacs-7: Emotional facial action coding system", "author": ["W.V. Friesen", "P. Ekman"], "venue": "Unpublished manuscript, University of California at San Francisco,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 1983}, {"title": "Rich feature hierarchies for accurate object detection and semantic segmentation", "author": ["R. Girshick", "J. Donahue", "T. Darrell", "J. Malik"], "venue": "In CVPR,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2014}, {"title": "Deep temporal appearance-geometry network for facial expression recognition", "author": ["H. Jung", "S. Lee", "S. Park", "I. Lee", "C. Ahn", "J. Kim"], "venue": "arXiv preprint arXiv:1503.01532,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2015}, {"title": "Emonets: Multimodal deep learning approaches for emotion recognition in video", "author": ["S.E. Kahou", "X. Bouthillier", "P. Lamblin", "C. Gulcehre", "V. Michalski", "K. Konda", "S. Jean", "P. Froumenty", "A. Courville", "P. Vincent"], "venue": "arXiv preprint arXiv:1503.01800,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2015}, {"title": "Combining modality specific deep neural networks for emotion recognition in video", "author": ["S.E. Kahou", "C. Pal", "X. Bouthillier", "P. Froumenty", "\u00c7. G\u00fcl\u00e7ehre", "R. Memisevic", "P. Vincent", "A. Courville", "Y. Bengio", "R.C. Ferrari"], "venue": "In ICMI,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2013}, {"title": "Imagenet classification with deep convolutional neural networks", "author": ["A. Krizhevsky", "I. Sutskever", "G.E. Hinton"], "venue": "In NIPS, pages 1097\u20131105,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2012}, {"title": "Au-aware deep networks for facial expression recognition", "author": ["M. Liu", "S. Li", "S. Shan", "X. Chen"], "venue": "In FG, pages", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2013}, {"title": "Au-inspired deep networks for facial expression feature learning", "author": ["M. Liu", "S. Li", "S. Shan", "X. Chen"], "venue": null, "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2015}, {"title": "Facial expression recognition via a boosted deep belief network", "author": ["P. Liu", "S. Han", "Z. Meng", "Y. Tong"], "venue": "In CVPR, pages 1805\u20131812,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2014}, {"title": "The extended cohn-kanade dataset (ck+): A complete dataset for action unit and emotion-specified expression", "author": ["P. Lucey", "J.F. Cohn", "T. Kanade", "J. Saragih", "Z. Ambadar", "I. Matthews"], "venue": "In CVPRW,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2010}, {"title": "Automatic classification of single facial", "author": ["M.J. Lyons", "J. Budynek", "S. Akamatsu"], "venue": "images. PAMI,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 1999}, {"title": "Zero-bias autoencoders and the benefits of co-adapting features", "author": ["R. Memisevic", "K. Konda", "D. Krueger"], "venue": "stat, 1050:10,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2014}, {"title": "An analysis of unsupervised pre-training in light of recent advances", "author": ["T.L. Paine", "P. Khorrami", "W. Han", "T.S. Huang"], "venue": "arXiv preprint arXiv:1412.6597,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2014}, {"title": "On deep generative models with applications to recognition", "author": ["M. Ranzato", "J. Susskind", "V. Mnih", "G. Hinton"], "venue": "In Computer Vision and Pattern Recognition (CVPR),", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2011}, {"title": "Disentangling factors of variation for facial expression recognition", "author": ["S. Rifai", "Y. Bengio", "A. Courville", "P. Vincent", "M. Mirza"], "venue": null, "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2012}, {"title": "Facial expression recognition based on local binary patterns: A comprehensive study", "author": ["C. Shan", "S. Gong", "P.W. McOwan"], "venue": "Image and Vision Computing,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2009}, {"title": "Striving for simplicity: The all convolutional net", "author": ["J.T. Springenberg", "A. Dosovitskiy", "T. Brox", "M. Riedmiller"], "venue": "arXiv preprint arXiv:1412.6806,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2014}, {"title": "The toronto face database", "author": ["J.M. Susskind", "A.K. Anderson", "G.E. Hinton"], "venue": "Department of Computer Science,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2010}, {"title": "Generating facial expressions with deep belief nets", "author": ["J.M. Susskind", "A.K. Anderson", "G.E. Hinton", "J.R. Movellan"], "venue": null, "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2008}, {"title": "Deepface: Closing the gap to human-level performance in face verification", "author": ["Y. Taigman", "M. Yang", "M. Ranzato", "L. Wolf"], "venue": "In CVPR,", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2014}, {"title": "Recognizing upper face action units for facial expression analysis", "author": ["Y.-l. Tian", "T. Kanada", "J.F. Cohn"], "venue": "In Computer Vision and Pattern Recognition,", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2000}, {"title": "Facial action unit recognition by exploiting their dynamic and semantic relationships", "author": ["Y. Tong", "W. Liao", "Q. Ji"], "venue": "Pattern Analysis and Machine Intelligence, IEEE Transactions on,", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2007}, {"title": "Haar features for facs au recognition", "author": ["J. Whitehill", "C.W. Omlin"], "venue": "In FGR,", "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2006}, {"title": "Visualizing and understanding convolutional networks", "author": ["M.D. Zeiler", "R. Fergus"], "venue": "In Computer Vision\u2013ECCV", "citeRegEx": "32", "shortCiteRegEx": "32", "year": 2014}, {"title": "Dynamic texture recognition using local binary patterns with an application to facial expressions", "author": ["G. Zhao", "M. Pietikainen"], "venue": "PAMI, 29(6):915\u2013928,", "citeRegEx": "33", "shortCiteRegEx": "33", "year": 2007}, {"title": "Learning active facial patches for expression analysis", "author": ["L. Zhong", "Q. Liu", "P. Yang", "B. Liu", "J. Huang", "D.N. Metaxas"], "venue": "In Computer Vision and Pattern Recognition (CVPR),", "citeRegEx": "34", "shortCiteRegEx": "34", "year": 2012}], "referenceMentions": [{"referenceID": 5, "context": "In [7], Paul Ekman proposed the Facial Action Coding System (FACS) which enumerated these regions and described how every facial expression can be described as the combination of multiple action units (AUs), each corresponding to a particular muscle group in the face.", "startOffset": 3, "endOffset": 6}, {"referenceID": 26, "context": "AU-based methods [29, 30] would detect the presence of individual AUs explicitly and then classify a person\u2019s emotion based on the combinations originally proposed by Friesen and Ekman in [8].", "startOffset": 17, "endOffset": 25}, {"referenceID": 27, "context": "AU-based methods [29, 30] would detect the presence of individual AUs explicitly and then classify a person\u2019s emotion based on the combinations originally proposed by Friesen and Ekman in [8].", "startOffset": 17, "endOffset": 25}, {"referenceID": 6, "context": "AU-based methods [29, 30] would detect the presence of individual AUs explicitly and then classify a person\u2019s emotion based on the combinations originally proposed by Friesen and Ekman in [8].", "startOffset": 188, "endOffset": 191}, {"referenceID": 0, "context": "On the other hand, appearance-based methods [1, 2, 31, 33] modeled a person\u2019s expression from their general facial shape and texture.", "startOffset": 44, "endOffset": 58}, {"referenceID": 28, "context": "On the other hand, appearance-based methods [1, 2, 31, 33] modeled a person\u2019s expression from their general facial shape and texture.", "startOffset": 44, "endOffset": 58}, {"referenceID": 30, "context": "On the other hand, appearance-based methods [1, 2, 31, 33] modeled a person\u2019s expression from their general facial shape and texture.", "startOffset": 44, "endOffset": 58}, {"referenceID": 11, "context": "Tasks such as object recognition [14], object detection [9], and face recognition [28] have seen huge boosts in performance on several accepted benchmarks.", "startOffset": 33, "endOffset": 37}, {"referenceID": 7, "context": "Tasks such as object recognition [14], object detection [9], and face recognition [28] have seen huge boosts in performance on several accepted benchmarks.", "startOffset": 56, "endOffset": 59}, {"referenceID": 25, "context": "Tasks such as object recognition [14], object detection [9], and face recognition [28] have seen huge boosts in performance on several accepted benchmarks.", "startOffset": 82, "endOffset": 86}, {"referenceID": 29, "context": "In this work, we apply the visualization techniques proposed by Zeiler and Fergus [32] and Springenberg et al.", "startOffset": 82, "endOffset": 86}, {"referenceID": 22, "context": "[25] where individual neurons in the network are excited and their corresponding spatial patterns are displayed in pixel space using a deconvolutional network.", "startOffset": 0, "endOffset": 4}, {"referenceID": 5, "context": "We show that CNNs trained for the emotion recognition task learn features that correspond strongly with the FAUs proposed by Ekman [7].", "startOffset": 131, "endOffset": 134}, {"referenceID": 17, "context": "We also show that our CNN model, based on works originally proposed by [20, 21], can achieve, to our knowledge, state-of-the-art performance on the extended Cohn-Kanade (CK+) dataset and the Toronto Face Dataset (TFD).", "startOffset": 71, "endOffset": 79}, {"referenceID": 18, "context": "We also show that our CNN model, based on works originally proposed by [20, 21], can achieve, to our knowledge, state-of-the-art performance on the extended Cohn-Kanade (CK+) dataset and the Toronto Face Dataset (TFD).", "startOffset": 71, "endOffset": 79}, {"referenceID": 0, "context": "Up until rather recently, most appearance-based expression recognition techniques relied on hand-crafted features, specifically Gabor wavelets [1, 2], Haar features [31] and LBP features [33], in order to make representations of different expression classes more discriminative.", "startOffset": 143, "endOffset": 149}, {"referenceID": 28, "context": "Up until rather recently, most appearance-based expression recognition techniques relied on hand-crafted features, specifically Gabor wavelets [1, 2], Haar features [31] and LBP features [33], in order to make representations of different expression classes more discriminative.", "startOffset": 165, "endOffset": 169}, {"referenceID": 30, "context": "Up until rather recently, most appearance-based expression recognition techniques relied on hand-crafted features, specifically Gabor wavelets [1, 2], Haar features [31] and LBP features [33], in order to make representations of different expression classes more discriminative.", "startOffset": 187, "endOffset": 191}, {"referenceID": 16, "context": "For some time, systems based on hand-crafted features were able to achieve impressive results on accepted expression recognition benchmarks such as the Japanese Female Facial Expression (JAFFE) database [19], the extended Cohn-Kanade (CK+) dataset [18], and the Multi-PIE dataset [10].", "startOffset": 203, "endOffset": 207}, {"referenceID": 15, "context": "For some time, systems based on hand-crafted features were able to achieve impressive results on accepted expression recognition benchmarks such as the Japanese Female Facial Expression (JAFFE) database [19], the extended Cohn-Kanade (CK+) dataset [18], and the Multi-PIE dataset [10].", "startOffset": 248, "endOffset": 252}, {"referenceID": 14, "context": "In [17], the authors trained a multi-layer boosted deep belief network (BDBN) and achieved state-of-the-art accuracy on the CK+ and JAFFE datasets.", "startOffset": 3, "endOffset": 7}, {"referenceID": 20, "context": "Meanwhile in [23], the authors used a convolutional contractive auto-encoder (CAE) as their underlying unsupervised model.", "startOffset": 13, "endOffset": 17}, {"referenceID": 12, "context": "In [15, 16], the authors learned a patch-based filter bank using K-means as their low-level feature.", "startOffset": 3, "endOffset": 11}, {"referenceID": 13, "context": "In [15, 16], the authors learned a patch-based filter bank using K-means as their low-level feature.", "startOffset": 3, "endOffset": 11}, {"referenceID": 24, "context": "[27], showed that the first layer features a deep belief network trained to generate facial expression images appeared to learn filters that were sensitive to face parts.", "startOffset": 0, "endOffset": 4}, {"referenceID": 10, "context": "[13, 12] and Jung et al.", "startOffset": 0, "endOffset": 8}, {"referenceID": 9, "context": "[13, 12] and Jung et al.", "startOffset": 0, "endOffset": 8}, {"referenceID": 8, "context": "[11].", "startOffset": 0, "endOffset": 4}, {"referenceID": 10, "context": "In [13, 12], the authors developed a system for doing audio/visual emotion recognition for the Emotion Recognition in the Wild Challenge (EmotiW) [6, 5] while in [11], the authors trained a network that incorporated both appearance and geometric features when doing recognition.", "startOffset": 3, "endOffset": 11}, {"referenceID": 9, "context": "In [13, 12], the authors developed a system for doing audio/visual emotion recognition for the Emotion Recognition in the Wild Challenge (EmotiW) [6, 5] while in [11], the authors trained a network that incorporated both appearance and geometric features when doing recognition.", "startOffset": 3, "endOffset": 11}, {"referenceID": 4, "context": "In [13, 12], the authors developed a system for doing audio/visual emotion recognition for the Emotion Recognition in the Wild Challenge (EmotiW) [6, 5] while in [11], the authors trained a network that incorporated both appearance and geometric features when doing recognition.", "startOffset": 146, "endOffset": 152}, {"referenceID": 3, "context": "In [13, 12], the authors developed a system for doing audio/visual emotion recognition for the Emotion Recognition in the Wild Challenge (EmotiW) [6, 5] while in [11], the authors trained a network that incorporated both appearance and geometric features when doing recognition.", "startOffset": 146, "endOffset": 152}, {"referenceID": 8, "context": "In [13, 12], the authors developed a system for doing audio/visual emotion recognition for the Emotion Recognition in the Wild Challenge (EmotiW) [6, 5] while in [11], the authors trained a network that incorporated both appearance and geometric features when doing recognition.", "startOffset": 162, "endOffset": 166}, {"referenceID": 1, "context": "Max pooling layers are placed after the first two convolutional layers while quadrant pooling [3] is applied after the third.", "startOffset": 94, "endOffset": 97}, {"referenceID": 17, "context": "in [20] for fully-connected networks and later extended by Paine et al.", "startOffset": 3, "endOffset": 7}, {"referenceID": 18, "context": "in [21] to convolutional layers.", "startOffset": 3, "endOffset": 7}, {"referenceID": 15, "context": "We use two facial expression datasets in our experiments: the extended Cohn-Kanade database (CK+) [18] and the Toronto Face Dataset (TFD) [26].", "startOffset": 98, "endOffset": 102}, {"referenceID": 23, "context": "We use two facial expression datasets in our experiments: the extended Cohn-Kanade database (CK+) [18] and the Toronto Face Dataset (TFD) [26].", "startOffset": 138, "endOffset": 142}, {"referenceID": 12, "context": "For fair comparison, we follow the protocol used by previous works [15, 17], and use the first frame of each sequence as a neutral frame in addition to the last three expressive frames to form our dataset.", "startOffset": 67, "endOffset": 75}, {"referenceID": 14, "context": "For fair comparison, we follow the protocol used by previous works [15, 17], and use the first frame of each sequence as a neutral frame in addition to the last three expressive frames to form our dataset.", "startOffset": 67, "endOffset": 75}, {"referenceID": 12, "context": "We then split the frames into 10 subject independent subsets in the manner presented by [15] and perform 10-fold cross-validation.", "startOffset": 88, "endOffset": 92}, {"referenceID": 2, "context": "Gabor+PCA [4] 80.", "startOffset": 10, "endOffset": 13}, {"referenceID": 19, "context": "Deep mPoT [22] 82.", "startOffset": 10, "endOffset": 14}, {"referenceID": 20, "context": "CDA [23] 85.", "startOffset": 4, "endOffset": 8}, {"referenceID": 31, "context": "However, many works [34, 24, 17] ignore the samples labeled as neutral or contempt, and only evaluate on the six basic emotions.", "startOffset": 20, "endOffset": 32}, {"referenceID": 21, "context": "However, many works [34, 24, 17] ignore the samples labeled as neutral or contempt, and only evaluate on the six basic emotions.", "startOffset": 20, "endOffset": 32}, {"referenceID": 14, "context": "However, many works [34, 24, 17] ignore the samples labeled as neutral or contempt, and only evaluate on the six basic emotions.", "startOffset": 20, "endOffset": 32}, {"referenceID": 12, "context": "AURF [15] 92.", "startOffset": 5, "endOffset": 9}, {"referenceID": 13, "context": "AUDN [16] 93.", "startOffset": 5, "endOffset": 9}, {"referenceID": 31, "context": "CSPL [34] 89.", "startOffset": 5, "endOffset": 9}, {"referenceID": 21, "context": "LBPSVM [24] 95.", "startOffset": 7, "endOffset": 11}, {"referenceID": 14, "context": "10% BDBN [17] 96.", "startOffset": 9, "endOffset": 13}, {"referenceID": 29, "context": "To do this, we employ the visualization technique presented by Zeiler and Fergus in [32].", "startOffset": 84, "endOffset": 88}, {"referenceID": 22, "context": "in [25].", "startOffset": 3, "endOffset": 7}], "year": 2017, "abstractText": "Despite being the appearance-based classifier of choice in recent years, relatively few works have examined how much convolutional neural networks (CNNs) can improve performance on accepted expression recognition benchmarks and, more importantly, examine what it is they actually learn. In this work, not only do we show that CNNs can achieve strong performance, but we also introduce an approach to decipher which portions of the face influence the CNN\u2019s predictions. First, we train a zero-bias CNN on facial expression data and achieve, to our knowledge, state-of-the-art performance on two expression recognition benchmarks: the extended Cohn-Kanade (CK+) dataset and the Toronto Face Dataset (TFD). We then qualitatively analyze the network by visualizing the spatial patterns that maximally excite different neurons in the convolutional layers and show how they resemble Facial Action Units (FAUs). Finally, we use the FAU labels provided in the CK+ dataset to verify that the FAUs observed in our filter visualizations indeed align with the subject\u2019s facial movements.", "creator": "LaTeX with hyperref package"}}}