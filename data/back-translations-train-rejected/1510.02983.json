{"id": "1510.02983", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "10-Oct-2015", "title": "OmniGraph: Rich Representation and Graph Kernel Learning", "abstract": "OmniGraph, a novel representation to support a range of NLP classification tasks, integrates lexical items, syntactic dependencies and frame semantic parses into graphs. Feature engineering is folded into the learning through convolution graph kernel learning to explore different extents of the graph. A high-dimensional space of features includes individual nodes as well as complex subgraphs. In experiments on a text-forecasting problem that predicts stock price change from news for company mentions, OmniGraph beats several benchmarks based on bag-of-words, syntactic dependencies, and semantic trees. The highly expressive features OmniGraph discovers provide insights into the semantics across distinct market sectors. To demonstrate the method's generality, we also report its high performance results on a fine-grained sentiment corpus.", "histories": [["v1", "Sat, 10 Oct 2015 21:22:00 GMT  (1070kb,D)", "http://arxiv.org/abs/1510.02983v1", null]], "reviews": [], "SUBJECTS": "cs.CL cs.LG", "authors": ["boyi xie", "rebecca j passonneau"], "accepted": false, "id": "1510.02983"}, "pdf": {"name": "1510.02983.pdf", "metadata": {"source": "CRF", "title": "OmniGraph: Rich Representation and Graph Kernel Learning", "authors": ["Boyi Xie", "Rebecca J. Passonneau"], "emails": ["xie@cs.columbia.edu", "becky@ccls.columbia.edu"], "sections": [{"heading": "1 Introduction", "text": "This year, it has come to the point where it only takes one year to get to the next round."}, {"heading": "2 Related Work", "text": "In fact, the majority of them are in a position to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to fight, to move, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to"}, {"heading": "3 Methods", "text": "This year, it is as far as ever in the history of the city, where it is as far as never before."}, {"heading": "3.2 Weisfeiler-Lehman Graph Kernel", "text": "We chose the Weisfeiler-Lehman (WL) graph kernel (Shervashidze et al., 2011) for SVM learning because it has less computational complexity than other graph cores and can measure similarities between graphs of different neighborhood sizes. At each grade i of the neighborhood, all nodes with their neighborhoods are re-labeled, then the similarity of the graph is measured. For example, to explore its first-degree neighbors, the immediate neighborhood of the Designated Entity node in Figure 3 is used to relativize the node as {Designated Entity \u2192 C.Message, S.Message, S.Speaker}. Weisfeiler-Lehman's graph calculation is based on the isomorphism test (Weisfeiler and Lehman, 1968), which iteratively enlarges the node by the assorted amount of its neighboring node designations."}, {"heading": "3.3 Node Edge Weighting Graph Kernel", "text": "The WL kernel is efficient at neighborhood augmentation, but there is no distinction between different node types and nodes, and node augmentation covers all nodes for a certain degree. The 1-degree WL function for the Designated Entity (DE) in Figure 3 is < EN \u2192 Spkr, Msg, Msg >, i.e. DE fills in one SPEAKER element and two MESSAGE elements (one for the statement frame and the other for the convey meaning frame) when this diagram is compared with another instance in which DE only fills in the MESSAGE element of convey importance. To allow partial matching and take advantage of the type of nodes and edges, we introduce a novel graph: node edge weighting (NEW) graph. As NEW measures the similarities by neighborhood."}, {"heading": "4 Financial News Analytics", "text": "We test the performance of OmniGraph with WL and NEW cores against a polarity task: predicting the direction of price change for 321 companies in eight market sectors of the Standard & Poor's 500 Index. On average, there are between 27 and 67 companies per sector. One of the biggest challenges in the financial sector is the unpredictability of the market. As mentioned above, using NLP methods to predict the price is successful when it is better than a random performance as described in Related Work. To test the statistical significance of classification accuracy, we rely on Studentis T. We use the majority-class label as a baseline, which varies between 54% and 56% depending on the market sector. Compared with three NLP benchmarks, only OmniGraph is above the baseline, and the results are statistically significant."}, {"heading": "4.1 Experimental Setup", "text": "The experiments use Reuters news data from 2007 to 2013 for eight GICS1 sectors. Sentences in which companies are mentioned are extracted from company name variants using high-precision, high-resolution recall patterns. A data instance for a company consists of an OmniGraph forest representing all sentences that the company mentions on a given day. On average, each instance of data encodes from 4.11 to 7.18 sentences, and each company has an average total of 605 to 858 sentences, depending on the sector. In work reported elsewhere, we found that we could increase the number of sentences per company by 15-30% using core frequency, depending on the sector. However, the additional sentences did not improve performance (Anon). Sentries mentioning companies by name tend to appear early in news articles and are apparently more predictable. A binary class designation {-1, + 1} indicates the direction of the price change the next day after the data link."}, {"heading": "4.2 Benchmark Methods", "text": "For comparison with OmniGraph, three benchmark methods are used: (1) BOW - a vector space model that contains unicrams, bigrams, and trigrams; (2) DepTree - a tree space representation based on the dependency parses used to create OmniGraph; the root is sentence entry; and dependency types such as SUB, OBJ, VMOD, and the lexical elements are represented as tree nodes; (3) SemTreeFWD - a state-of-the-art representation for the price prediction task that is an enriched mix of vector and tree space (Xie et al., 2013); it includes semantic frames, lexical elements, and partially language-specific psycholinguistic features. Learning is based on Tree Kernel SVM (Moschitti, 2006).2"}, {"heading": "4.3 Features", "text": "In a detailed analysis of the 26 companies in GICS 30 (Consumer Staples), a sector with average message volumes, all but three companies have non-zero coefficients on two or more of the base cores. Two of the three outliers rely only on step size 1 and the third on step size 2. Thirteen companies combine two base cores and the remaining ten combine three. On average, only 9% of the features are non-relational (p = 0). The other sectors show a similar trend. Grid search determines the step size and also determines which types of nodes should be included during augmentation in the neighborhood; the nodes are weighted 0 or 1. Figure 6 shows the proportion of GICS 30 companies each using seven types of nodes."}, {"heading": "4.4 Results", "text": "Table 1 summarizes the average accuracy for all eight sectors of the majority-class baseline, the three benchmarks, and the two OmniGraph models. Both versions of OmniGraph significantly exceed the three benchmarks, and the cells with asterisks represent a difference from the baseline, which is statistically significant. OmniGraphWL beats the baseline with statistical significance in six sectors, and OmniGraphNEW in seven. Note that none of the benchmarks exceeds the baseline. Despite BOW's excellent performance for topical classification tasks, it performs poorly on this price prediction task. Both DepTree and SemTreeFWD perform worse than BOW, suggesting that features derived from dependency syntax and semantic framework analysis improve performance. DepTree represents the dependency sparse directly with dependencies and words as nodes, without the root of its semantic information being referred to as the boundary of the SemniGraph."}, {"heading": "5 GoodFor/BadFor Corpus", "text": "To further test OmniGraph's performance in terms of corporate text analysis, we used a recently introduced publicly available dataset - GoodFor / BadFor (gfbf) Corpus3 (Deng et al., 2013), which is part of MPQA (Wiebe et al., 2005). gfbf was commented on for two fine-grained sentiment judgments: 1) benefactive / malefactive event note and 2) writer setting. We treated the object as a designated entity. The write task asked annotators to identify the entity (the object) and the entity causing the event (the agent) and to indicate whether the representative and event is benevolent or malicious on the object. The write task asked annotators to identify the writer's attitude toward the agent and object."}, {"heading": "6 Discussion", "text": "In this context, it should be noted that the two models are a complex system that increases the predictive power of OmniGraph models. Figure 7 represents six high-level features from our experiments. Features 1-3 are derived from the financial analysis task. Feature 1 is a complex feature with box names, box elements and the dependencies between frames. It generalizes across several sectors and predicts a positive change in price. It is the feature that corresponds to the example rates in Introduction. Feature 2 combines box names, box elements and the dependencies between frames."}, {"heading": "7 Conclusion", "text": "In this study, we presented a novel graph-based representation - OmniGraph - with Weisfeiler-Lehman and node weighting for semantic analysis of documents. This method demonstrates superior performance in a text prediction task, where financial news is used to predict the stock market performance of company mentions, and a fine-grained mood task. OmniGraphs advantages are based on the use of semantic frames to generalize word meanings in a flexible and expandable graph structure, where rich relational linguistic information, such as dependencies between frames and lexical items, can be modeled and learned with graphics cores that make feature engineering a part of learning. The resulting graphics functions are able to reflect deeper semantic patterns beyond words and help provide insight into the problem domain. In this context, we have applied graphs to two potential graphics problems that could support fairly different classification problems."}], "references": [{"title": "Frame semantic tree kernels for social network extraction from text", "author": ["Sriramkumar Balabsubramanian", "Anup Kotalwar", "Jiehan Zheng", "Owen Rambow"], "venue": "In Proceedings of the 14th Conference of the European", "citeRegEx": "Agarwal et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Agarwal et al\\.", "year": 2014}, {"title": "Multiple kernel learning, conic duality, and the smo algorithm", "author": ["Bach et al.2004] Francis R. Bach", "Gert R.G. Lanckriet", "Michael I. Jordan"], "venue": "In Proceedings of the Twenty-first International Conference on Machine Learning,", "citeRegEx": "Bach et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Bach et al\\.", "year": 2004}, {"title": "The Berkeley Framenet project", "author": ["Charles J. Fillmore", "John B. Lowe"], "venue": "In Proceedings of the 36th Annual Meeting of the ACL and 17th International Conference on Computational Linguistics - Volume", "citeRegEx": "Baker et al\\.,? \\Q1998\\E", "shortCiteRegEx": "Baker et al\\.", "year": 1998}, {"title": "Identifying and following expert investors in stock microblogs", "author": ["Elad Dinur", "Ronen Feldman", "Moshe Fresko", "Guy Goldstein"], "venue": "In Proceedings of the 2011 Conference on Empirical Methods in Natural Language", "citeRegEx": "Bar.Haim et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Bar.Haim et al\\.", "year": 2011}, {"title": "Stock price reaction to news and no-news: Drift and reversal after headlines", "author": ["Wesley S. Chan"], "venue": "Journal of Financial Economics,", "citeRegEx": "Chan.,? \\Q2003\\E", "shortCiteRegEx": "Chan.", "year": 2003}, {"title": "Convolution kernels for natural language", "author": ["Collins", "Duffy2001] Michael Collins", "Nigel Duffy"], "venue": "In Proceedings of the 14th Conference on Neural Information Processing Systems", "citeRegEx": "Collins et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Collins et al\\.", "year": 2001}, {"title": "News and sentiment analysis of the european market with a hybrid expert weighting algorithm", "author": ["Yong Ren", "Yasuaki Sacamoto", "Jeffrey V. Nickerson"], "venue": "In SocialCom\u201913,", "citeRegEx": "Creamer et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Creamer et al\\.", "year": 2013}, {"title": "Semi-supervised frame-semantic parsing for unknown predicates", "author": ["Das", "Smith2011] Dipanjan Das", "Noah A. Smith"], "venue": "In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Tech-", "citeRegEx": "Das et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Das et al\\.", "year": 2011}, {"title": "Graph-based lexicon expansion with sparsity-inducing penalties. In HLT-NAACL, pages 677\u2013687", "author": ["Das", "Smith2012] Dipanjan Das", "Noah A. Smith"], "venue": null, "citeRegEx": "Das et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Das et al\\.", "year": 2012}, {"title": "Sentiment polarity identification in financial news: A cohesion-based approach", "author": ["Devitt", "Ahmad2007] Ann Devitt", "Khurshid Ahmad"], "venue": "In Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics,", "citeRegEx": "Devitt et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Devitt et al\\.", "year": 2007}, {"title": "Three new probabilistic models for dependency parsing: An exploration", "author": ["Jason M. Eisner"], "venue": "In Proceedings of the 16th Conference on Computational Linguistics - Volume 1,", "citeRegEx": "Eisner.,? \\Q1996\\E", "shortCiteRegEx": "Eisner.", "year": 1996}, {"title": "The causal impact of media in financial markets", "author": ["Engelberg", "Parsons2011] Joseph Engelberg", "Christopher A. Parsons"], "venue": "Journal of Finance,", "citeRegEx": "Engelberg et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Engelberg et al\\.", "year": 2011}, {"title": "Market efficiency, long-term returns, and behavioral finance", "author": ["Eugene F. Fama"], "venue": "Journal of Financial Economics,", "citeRegEx": "Fama.,? \\Q1998\\E", "shortCiteRegEx": "Fama.", "year": 1998}, {"title": "The stock sonar - sentiment analysis of stocks based on a hybrid approach", "author": ["Benjamin Rosenfeld", "Roy Bar-Haim", "Moshe Fresko"], "venue": "In Proceedings of the Twenty-Third Conference on Innovative", "citeRegEx": "Feldman et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Feldman et al\\.", "year": 2011}, {"title": "Learning parse structure of paragraphs and its applications in search", "author": ["Boris Galitsky"], "venue": "Engineering Applications of Artificial Intelligence,", "citeRegEx": "Galitsky.,? \\Q2014\\E", "shortCiteRegEx": "Galitsky.", "year": 2014}, {"title": "What drives media slant? evidence from u.s. daily newspapers", "author": ["Gentzkow", "Shapiro2010] M. Gentzkow", "J.M. Shapiro"], "venue": null, "citeRegEx": "Gentzkow et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Gentzkow et al\\.", "year": 2010}, {"title": "Does the media matter? a field experiment measuring the effect of newspapers on voting behavior and political opinions", "author": ["Gerber et al.2009] A.S. Gerber", "D. Karlan", "D. Bergan"], "venue": "American Economic Journal: Applied Economics,", "citeRegEx": "Gerber et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Gerber et al\\.", "year": 2009}, {"title": "Movie reviews and revenues: An experiment in text regression. In Human Language Technologies: The 2010", "author": ["Joshi et al.2010] Mahesh Joshi", "Dipanjan Das", "Kevin Gimpel", "Noah A. Smith"], "venue": null, "citeRegEx": "Joshi et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Joshi et al\\.", "year": 2010}, {"title": "Extracting opinions, opinion holders, and topics expressed in online news media text", "author": ["Kim", "Hovy2006] Soo-Min Kim", "Eduard Hovy"], "venue": "In Proceedings of the Workshop on Sentiment and Subjectivity in Text,", "citeRegEx": "Kim et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Kim et al\\.", "year": 2006}, {"title": "Predicting risk from financial reports with regression", "author": ["Kogan et al.2009] Shimon Kogan", "Dimitry Levin", "Bryan R. Routledge", "Jacob S. Sagi", "Noah A. Smith"], "venue": "In Proceedings of Human Language Technologies:", "citeRegEx": "Kogan et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Kogan et al\\.", "year": 2009}, {"title": "On the importance of text analysis for stock price prediction", "author": ["Lee et al.2014] Heeyoung Lee", "Mihai Surdeanu", "Bill Maccartney", "Dan Jurafsky"], "venue": "In LREC\u201914,", "citeRegEx": "Lee et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Lee et al\\.", "year": 2014}, {"title": "Text classification using string kernels", "author": ["Lodhi et al.2002] Huma Lodhi", "Craig Saunders", "John Shawe-Taylor", "Nello Cristianini", "Chris Watkins"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Lodhi et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Lodhi et al\\.", "year": 2002}, {"title": "Non-projective dependency parsing using spanning tree algorithms", "author": ["Fernando Pereira", "Kiril Ribarov", "Jan Haji\u010d"], "venue": "In Proceedings of the Conference on Human Language Technology and Empirical", "citeRegEx": "McDonald et al\\.,? \\Q2005\\E", "shortCiteRegEx": "McDonald et al\\.", "year": 2005}, {"title": "Making tree kernels practical for natural language learning", "author": ["Alessandro Moschitti"], "venue": "Proceedings of the 11th Conference of the European Chapter of the Association for Computational Linguistics", "citeRegEx": "Moschitti.,? \\Q2006\\E", "shortCiteRegEx": "Moschitti.", "year": 2006}, {"title": "Dynamics of Trade-byTrade Price Movements: Decomposition and Models", "author": ["Rydberg", "Shephard2003] Tina H. Rydberg", "Neil Shephard"], "venue": "Journal of Financial Econometrics,", "citeRegEx": "Rydberg et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Rydberg et al\\.", "year": 2003}, {"title": "Grammatical structures for word-level sentiment detection", "author": ["Jordan BoydGraber", "Bryan Rusk", "Amy Weinberg"], "venue": "In Proceedings of the 2012 Conference of the North American Chapter of the Association", "citeRegEx": "Sayeed et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Sayeed et al\\.", "year": 2012}, {"title": "Evaluating sentiment in financial news articles", "author": ["Yulei Zhang", "Chun-Neng Huang", "Hsinchun Chen"], "venue": "Decision Support Systems,", "citeRegEx": "Schumaker et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Schumaker et al\\.", "year": 2012}, {"title": "Weisfeilerlehman graph kernels", "author": ["Karsten M. Borgwardt"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Borgwardt.,? \\Q2011\\E", "shortCiteRegEx": "Borgwardt.", "year": 2011}, {"title": "Improved svm regression using mixtures of kernels", "author": ["Smits", "Jordaan2002] G.F. Smits", "E.M. Jordaan"], "venue": "In Proceedings of 2002 International Joint Conference on Neural Networks,", "citeRegEx": "Smits et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Smits et al\\.", "year": 2002}, {"title": "Hierarchical directed acyclic graph kernel: Methods for structured natural language data", "author": ["Suzuki et al.2003] Jun Suzuki", "Tsutomu Hirao", "Yutaka Sasaki", "Eisaku Maeda"], "venue": "In Proceedings of the 41st Annual Meeting on Association for Computa-", "citeRegEx": "Suzuki et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Suzuki et al\\.", "year": 2003}, {"title": "More than Words: Quantifying Language to Measure Firms", "author": ["Maytal SaarTsechansky", "Sofus Macskassy"], "venue": null, "citeRegEx": "Tetlock et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Tetlock et al\\.", "year": 2008}, {"title": "Giving Content to Investor Sentiment: The Role of Media in the Stock Market", "author": ["Paul C. Tetlock"], "venue": null, "citeRegEx": "Tetlock.,? \\Q2007\\E", "shortCiteRegEx": "Tetlock.", "year": 2007}, {"title": "A reduction of graph to a canonical form and an algebra arising during this reduction", "author": ["Weisfeiler", "Lehman1968] B. Weisfeiler", "A.A. Lehman"], "venue": "Nauchno-Technicheskaya Informatsiya,", "citeRegEx": "Weisfeiler et al\\.,? \\Q1968\\E", "shortCiteRegEx": "Weisfeiler et al\\.", "year": 1968}, {"title": "Annotating expressions of opinions and emotions in language. Language Resources and Evaluation, 39(2-3):165\u2013210", "author": ["Wiebe et al.2005] Janyce Wiebe", "Theresa Wilson", "Claire Cardie"], "venue": null, "citeRegEx": "Wiebe et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Wiebe et al\\.", "year": 2005}, {"title": "Stock market prediction from WSJ: text mining via sparse matrix factorization", "author": ["Zhenming Liu", "Mung Chiang"], "venue": "IEEE International Conference on Data Mining,", "citeRegEx": "Wong et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Wong et al\\.", "year": 2014}, {"title": "Semantic frames to predict stock price movement", "author": ["Xie et al.2013] Boyi Xie", "Rebecca J. Passonneau", "Leon Wu", "Germ\u00e1n Creamer"], "venue": "In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics", "citeRegEx": "Xie et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Xie et al\\.", "year": 2013}, {"title": "Linguistic structured sparsity in text categorization", "author": ["Yogatama", "Smith2014] Dani Yogatama", "Noah A. Smith"], "venue": "In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics", "citeRegEx": "Yogatama et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Yogatama et al\\.", "year": 2014}, {"title": "Question classification using support vector machines", "author": ["Zhang", "Lee2003] Dell Zhang", "Wee Sun Lee"], "venue": "In Proceedings of the 26th Annual International ACM SIGIR Conference on Research and Development in Informaion Retrieval,", "citeRegEx": "Zhang et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Zhang et al\\.", "year": 2003}, {"title": "Trading strategies to exploit blog and news sentiment", "author": ["Zhang", "Skiena2010] Wenbin Zhang", "Steven Skiena"], "venue": null, "citeRegEx": "Zhang et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Zhang et al\\.", "year": 2010}], "referenceMentions": [{"referenceID": 19, "context": "For diverse NLP classification tasks, such as sentiment and opinion mining, or text-forecasting, in which text documents are used to make predictions about measurable phenomena in the real world (Kogan et al., 2009), there is a need to generalize over words while simultaneously capturing relational and structural information.", "startOffset": 195, "endOffset": 215}, {"referenceID": 2, "context": "Four semantic frames from FrameNet (Baker et al., 1998), a linguistic resource that exemplifies Fillmore\u2019s frame seman-", "startOffset": 35, "endOffset": 55}, {"referenceID": 17, "context": "Another approach to forecasting from text (Joshi et al., 2010) combines BOW and the names of dependency relations to engineer features for predicting movie revenue from reviews.", "startOffset": 42, "endOffset": 62}, {"referenceID": 24, "context": "To similarly mine opinion triples, Sayeed et al. (2012) depend more on syntax, using a suffix-tree data structure to represent syntactic relationships.", "startOffset": 35, "endOffset": 56}, {"referenceID": 24, "context": "To similarly mine opinion triples, Sayeed et al. (2012) depend more on syntax, using a suffix-tree data structure to represent syntactic relationships. Instead of feature engineering, Yogatama and Smith (2014), develop structured regularization for BOW based on parse trees, topics and hierarchical word clusters to improve BOW for 3 classification tasks: topic, sentiment, and text-driven forecasting.", "startOffset": 35, "endOffset": 210}, {"referenceID": 21, "context": "Convolution kernels have been used in NLP to exploit structured information using trees for parsing and tagging (Collins and Duffy, 2001), text categorization (Lodhi et al., 2002), and question answering", "startOffset": 159, "endOffset": 179}, {"referenceID": 29, "context": "(Zhang and Lee, 2003; Suzuki et al., 2003; Moschitti, 2006).", "startOffset": 0, "endOffset": 59}, {"referenceID": 23, "context": "(Zhang and Lee, 2003; Suzuki et al., 2003; Moschitti, 2006).", "startOffset": 0, "endOffset": 59}, {"referenceID": 0, "context": "To learn social networks, Agarwal et al. (2014) use partial tree kernels on a representation with frame semantic information (Fillmore, 1976).", "startOffset": 26, "endOffset": 48}, {"referenceID": 0, "context": "To learn social networks, Agarwal et al. (2014) use partial tree kernels on a representation with frame semantic information (Fillmore, 1976). The tree representation in Xie et al. (2013) also incorporates frame semantics, and uses sub-", "startOffset": 26, "endOffset": 188}, {"referenceID": 16, "context": "the market (Gerber et al., 2009; Gentzkow and Shapiro, 2010; Engelberg and Parsons, 2011) have been increasingly important since Tetlock (2007) investigated the role of media in the stock market.", "startOffset": 11, "endOffset": 89}, {"referenceID": 16, "context": "the market (Gerber et al., 2009; Gentzkow and Shapiro, 2010; Engelberg and Parsons, 2011) have been increasingly important since Tetlock (2007) investigated the role of media in the stock market.", "startOffset": 12, "endOffset": 144}, {"referenceID": 16, "context": "the market (Gerber et al., 2009; Gentzkow and Shapiro, 2010; Engelberg and Parsons, 2011) have been increasingly important since Tetlock (2007) investigated the role of media in the stock market. As mentioned in Wong et al. (2014), a better solution to the problem can help gain more in-", "startOffset": 12, "endOffset": 231}, {"referenceID": 12, "context": "sights to the long-lasting question in finance about how financial markets react to news (Fama, 1998; Chan, 2003).", "startOffset": 89, "endOffset": 113}, {"referenceID": 4, "context": "sights to the long-lasting question in finance about how financial markets react to news (Fama, 1998; Chan, 2003).", "startOffset": 89, "endOffset": 113}, {"referenceID": 6, "context": ", 2011; Creamer et al., 2013; Xie et al., 2013). Wong et al. (2014) report that even \u201ctextbook models\u201d that uses time series data have less than 51.", "startOffset": 8, "endOffset": 68}, {"referenceID": 26, "context": "Work in NLP and related areas (Devitt and Ahmad, 2007; Schumaker et al., 2012; Feldman et al., 2011; Zhang and Skiena, 2010) often treats stock price prediction from news as a sentiment classification problem.", "startOffset": 30, "endOffset": 124}, {"referenceID": 13, "context": "Work in NLP and related areas (Devitt and Ahmad, 2007; Schumaker et al., 2012; Feldman et al., 2011; Zhang and Skiena, 2010) often treats stock price prediction from news as a sentiment classification problem.", "startOffset": 30, "endOffset": 124}, {"referenceID": 13, "context": ", 2012; Feldman et al., 2011; Zhang and Skiena, 2010) often treats stock price prediction from news as a sentiment classification problem. Xie et al. (2013) point out that this is consistent with the direction component of the three-part ADS model (Rydberg and Shephard, 2003).", "startOffset": 8, "endOffset": 157}, {"referenceID": 35, "context": "As in Xie et al. (2013), we refer to the company we make predictions about as the designated entity.", "startOffset": 6, "endOffset": 24}, {"referenceID": 1, "context": "Combining basis kernels is a common problem in machine learning and several multiple kernel learning techniques have been developed to allow benefits from multiple kernels (Smits and Jordaan, 2002; Bach et al., 2004).", "startOffset": 172, "endOffset": 216}, {"referenceID": 30, "context": "The one-day delay of price response to news is due to (Tetlock et al., 2008).", "startOffset": 54, "endOffset": 76}, {"referenceID": 22, "context": "Sentences are parsed using the MST dependency parser (McDonald et al., 2005), which implements the Eisner algorithm (Eisner, 1996) for dependency parsing, and provides an efficient and robust performance.", "startOffset": 53, "endOffset": 76}, {"referenceID": 10, "context": ", 2005), which implements the Eisner algorithm (Eisner, 1996) for dependency parsing, and provides an efficient and robust performance.", "startOffset": 47, "endOffset": 61}, {"referenceID": 35, "context": "representation for the price prediction task that is an enriched hybrid of vector and tree space (Xie et al., 2013).", "startOffset": 97, "endOffset": 115}, {"referenceID": 23, "context": "Learning relies on Tree Kernel SVM (Moschitti, 2006).", "startOffset": 35, "endOffset": 52}, {"referenceID": 33, "context": "which is part of MPQA (Wiebe et al., 2005).", "startOffset": 22, "endOffset": 42}, {"referenceID": 14, "context": "Inspired by the work of Galitsky (2014), who constructed dependency parse forests for paragraphs of text, one of our future directions is to extend OmniGraph to incorporate discourse information.", "startOffset": 24, "endOffset": 40}], "year": 2015, "abstractText": "OmniGraph, a novel representation to support a range of NLP classification tasks, integrates lexical items, syntactic dependencies and frame semantic parses into graphs. Feature engineering is folded into the learning through convolution graph kernel learning to explore different extents of the graph. A high-dimensional space of features includes individual nodes to complex networks. In experiments on a text-forecasting problem that predicts stock price change from news for company mentions, OmniGraph beats several benchmarks based on bag-of-words, syntactic dependencies, and semantic trees. The highly expressive features OmniGraph discovers provide insights into the semantics across distinct market sectors. To demonstrate the method\u2019s generality, we also report its high performance results on a fine-grained sentiment corpus.", "creator": "LaTeX with hyperref package"}}}