{"id": "1510.03130", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "12-Oct-2015", "title": "On Correcting Inputs: Inverse Optimization for Online Structured Prediction", "abstract": "Algorithm designers typically assume that the input data is correct, and then proceed to find \"optimal\" or \"sub-optimal\" solutions using this input data. However this assumption of correct data does not always hold in practice, especially in the context of online learning systems where the objective is to learn appropriate feature weights given some training samples. Such scenarios necessitate the study of inverse optimization problems where one is given an input instance as well as a desired output and the task is to adjust the input data so that the given output is indeed optimal. Motivated by learning structured prediction models, in this paper we consider inverse optimization with a margin, i.e., we require the given output to be better than all other feasible outputs by a desired margin. We consider such inverse optimization problems for maximum weight matroid basis, matroid intersection, perfect matchings, minimum cost maximum flows, and shortest paths and derive the first known results for such problems with a non-zero margin. The effectiveness of these algorithmic approaches to online learning for structured prediction is also discussed.", "histories": [["v1", "Mon, 12 Oct 2015 03:33:47 GMT  (78kb,D)", "http://arxiv.org/abs/1510.03130v1", "Conference version to appear in FSTTCS, 2015"]], "COMMENTS": "Conference version to appear in FSTTCS, 2015", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["hal daum\\'e iii", "samir khuller", "manish purohit", "gregory sanders"], "accepted": false, "id": "1510.03130"}, "pdf": {"name": "1510.03130.pdf", "metadata": {"source": "CRF", "title": "On Correcting Inputs: Inverse Optimization for Online Structured Prediction", "authors": ["Samir Khuller", "Manish Purohit", "Gregory Sanders"], "emails": ["gsanders}@cs.umd.edu"], "sections": [{"heading": "1 Introduction", "text": "We generally assume that the input data is sacrosanct and correct, so this problem is usually executed on the basis of this input data in order to quickly calculate \"optimal\" or \"sub-optimal\" solutions whether it is the calculation of a maximum problem that has a maximum margin, maximum margin, maximum margin or shortest paths. However, with increasing dependence on automatic methods for collecting data, as well as in systems that learn, this assumption is not always tenable. Input data can be erroneous (even if it may be roughly correct), and it becomes important to \"adjust\" the input data to achieve certain desired conditions. A simple example can be used to illustrate the main point - suppose that we obtain a weighted graph G = (V, E) and a tree T, and tells T that T is a maximum weight tree in G. The goal is to minimize the marginal weights of the graph, so that the problem of the graph 2 is to pervert."}, {"heading": "1.1 Related Work", "text": "This year, it is so far that it will be able to retaliate, \"he says.\" But it's too early to say what we want to do, \"he says.\" It's too early to do it, \"he says,\" but it's too early to do it. \""}, {"heading": "1.2 Contribution and Techniques", "text": "Many previous work in the literature on inverse optimization formulated the problem as a linear program and then used strong duality conditions to find the new disturbed weights. However, such techniques cannot be extended to achieve the margin required by the application for structured predictions. We formulate inverse optimization to minimize the L2 standard of disturbances as a square program and use problem-specific optimization conditions to determine a precise set of linear constraints that are both necessary and sufficient to guarantee the required margin. Specifically, one of the key components is a set of polynomically many linear constraints that ensure that a suitably defined guide diagram does not contain small directed cycles. We find that our formulations can be easily adapted to minimize the L1 standard of disturbances by simply modifying the target and using linear programming."}, {"heading": "2 Problem Description", "text": "As explained in the introduction, we need a predetermined solution in order to be better than all other feasible solutions with a range of \u03b4. We now formalize this concept of \u03b4-optimality.Definition 1 (\u03b4-optimality.For a maximization problem P, F should denote the set of feasible solutions, w should be the weight vector, c (w, A) should denote the cost of the feasible solution A under weights w, and g \u00b2 0 should be a scalar.A feasible solution S \u00b2 F is called under weights w, if and only ifc (w, S) \u2265 c (w, S \u00b2) + \u03b4, \u0445S \u00b2 (6 = S) and F \u00b2 optimalityfor minimization problems.All problems we consider in this work can be classified as \"optimal\" under weight margin. Definition 2 (\u03b4-Margin Inverse Optimization)."}, {"heading": "3 Maximum weight matroid basis", "text": "To give an intuition about the kind of problems we propose in this work, let us first give the simple example of inverse matroid optimization. We remember the definition of a matroid 3. Definition 3 (matroid). A matroid is a pair of M = (X, I), where X is a base set of elements and I is a family of subsets of X (referred to as a matroid), so that - (i) I 6 = (ii) (hereditary) If B, and A B, then A I. (iii) (exchange property) If A, B I, and | A | < | B |, then there is an element e B\\ A, so that A {e} (matroid base and circuit). Let M = (X, I) be an optimal property of matroid. Then, every maximum independent unit in I is referred to as the basis of matroid. Conversely, every minimum dependence is referred to as a problem."}, {"heading": "4 Matroid Intersection", "text": "Similar to the case with a single matroid, we must derive a necessary and sufficient condition for a common base B of two matroids (1). We can determine such an optimality condition by means of an exchange diagram associated with base B and matroids M1 and M2.Definition 5 (exchange diagram). In the face of two matroids M1 = (X, I1) and M2 = (X, I2), a weight function w: X \u2192 R +, and a common base B, an exchange diagram is a direct, two-part graph G = (V, A) with a length function l on edges defined as the following. V = B = X\\ B (3) A = A1 (4) A1 = (x, y), an exchange diagram is a direct, two-part graph G = (V, A) with a length l on edges defined as the following."}, {"heading": "4.1 Lower bounding cycles", "text": "To use term 2 to solve the inverse matroid intersection problem in polynomial time, it is often too slow for us to formulate this condition as a polynomial number of linear constraints. We are now researching a technique to express the condition that a given graph has no small (lengths smaller than \u03b4) cycles. Let's say that we are given a directed graph G = (V, A) and our task is to assign edge lengths so that all cycles in G have at least one weight. If we leave the edge lengths as variables, the feasible region in this case is unlimited and is defined by a constraint for each cycle in G, i.e. we have the region R1 in m dimensions defined by -R1."}, {"heading": "4.2 Putting it together", "text": "Lemmas 2 and 5 propose a way to solve the problem of reverse matroid margin. In accordance with the requirements of Lemma 2, taking into account the two matroids and common base B, we construct the exchange diagram G = (V, A = A1 and A2). Let w: X \u2192 R + be the original weight function and let w: be the new weight function we are aiming for. If l is the arc lengths of G, we can now add region R2 according to the construction of Lemma 2, lxy = w (x) and lyx = \u2212 w (y) to achieve the minimum change in the weights of the elements, so that the exchange diagram does not have small cycles and therefore B is the L2 standard of w \u2212 w \u00b2. We can now add the additional constraints and the destination of region R2 according to Lemma 5 to achieve the minimum change in the weights of the elements, so that the exchange diagram does not have small cycles and thus the exchange diagram is not optimal \u2212 B."}, {"heading": "4.3 Maximum Weight Arborescence", "text": "Considering a directed graph, an r-arborescence (also known as branching) is the directed analogue of an overarching tree, and is defined as a series of edges T that encompass all vertices, so that each vertex (except r) has exactly one incidental edge in T. It is well known that an arborescence in a directed graph is a basis at the intersection of a graphic matroid and a partition matroid. We will analyze the complexity of the above technique for the special case of maximum weight arborescence. Let G designate the graph in question with n vertices and m edges. The replacement graph Gex has a vertex for each edge of G, i.e., nex = m. The division of Gex is such that we have components of size n and m \u2212 n. Therefore, we have Mex = O (mn) adratic. As seen in Section 4.1, we use (O) (2) and O (O)."}, {"heading": "5 Perfect Matchings in Bipartite Graphs", "text": "In this section, we show that we can actually get more concise formulations. Remember that for a given edge, weighted bifurcated curve G = (X-Y, E, w) and a perfect match M \u2212 an alternating cycle is a cycle in G in which edges alternate between those that belong to M and those that do not. An alternating cycle C is called \"augmenting\" when it is called (X-Y, w) and \"augmenting\" when it is (X-M w, e) < an alternating cycle in C is a cycle in G in which the edges belong to M and those that do not. A perfect match M is \"augmenting\" when and only when the diagram does not contain augmenting cycles. The central idea is to construct a coordinated diagram H on exactly the nodes from X to X."}, {"heading": "6 Application : Online learning for structured prediction", "text": "The structured prediction task is to predict a discrete combinatorial structure (such as an arborescence) that requires structured input (such as a diagram); the learning task is to learn model parameters so that solving a combinatorial optimization problem on the input instance would return the desired output structure, consisting of an input xt (for example, a sentence) and an output yt (for example, a syntactical analysis of this sentence described as arborescence on a graph above the words in the sentence); each edge in this graph is parameterized by a series of F attributes that indicate, for example, how likely one word is to be the subject of another."}, {"heading": "A Minimum Cost Maximum Flow", "text": "Suppose we would like to have R reviewers per paper and no more than P papers per reviewer. Such a scenario can easily be modeled as a generalization of bipartite balancing of supply without unit and requires the use of a maximum cost flow problem. To learn the weights of such an instance (a reviewer's suitability for a paper), we can apply structured prediction methods that require, for example, solving problems with inverse minimal cost flow. Formally, Definition 6 (Reverse Minimum Cost Flow Problem). In the face of a directed chart G = (V, A), a capacity function at edges c: E \u2192 R, a cost function w: E \u2192 R, and a feasible maximum flow problem f: E \u2192 R +, the inverse minimum cost maximum flow problem consists in finding a new cost function w: (V, A), if a capacity function at edges of flow cycle is a minimum flow function \u2192 E + maximum flow function, and feasible E: E: E and feasible E: E)."}, {"heading": "B Shortest Path Trees", "text": "Suppose we get a directed graph G = (V, E) with a weighting function w > at edges, a sub-tree Tspt rooted at r, and a margin \u03b4. The inverse shortest path-tree problem is then to modify the edge weights minimally so that Tspt becomes the least optimal path-tree, i.e. for each vertex v (6 = r) in G, the path prescribed by Tspt is the most optimal path from r to v.To solve this problem, we define variables dv, which represent distance labels for each vertex, and generate the following quadratic program."}, {"heading": "C Proofs of Learning Theory Results", "text": "Theorem 1 (convergence) is the abbreviation for the difference in the difference, while the difference in the difference is the difference in the difference, while the difference in the difference is the difference in the difference, while the difference in the difference is the difference in the difference. (2) Theorem 1 (convergence) is the difference in the difference. (3) Theorem 1 (convergence) is the difference in the difference. (4) Theorem 1 (convergence) is the difference in the difference. (4) Theorem 2 (convergence) is the difference in the difference. (4) Theorem 1 (convergence) is the difference in the difference. (4) Theorem 1 (convergence) is the difference in the difference. (4) Theorem 2 (convergence) is the difference in the difference. (4) Theorem 1 (convergence) is the difference in the difference."}, {"heading": "D Experimental Analysis", "text": "We are conducting introductory experiments to demonstrate the effectiveness of the online learning framework (suitable). We are looking at two structured prediction tasks: dependency sparsing and word alignment for language translation. We are looking at two structured prediction tasks: dependency parameter and word alignment for language translation. Dependency is a sentence, and the goal is to find its dependence, i.e. it is evaluated how words in one sentence relate to another and form a tree that begins with an empty root node. The task is now to learn eigenweights so that the maximum weight distribution in the diagram corresponds to the sentence tree. In word alignment for language translation, we are given two equivalent sentences in two languages and the task is identified as correct words. In this case, the input instance is considered as a complete two-part graph, and the output would be an assignment (suitable)."}], "references": [{"title": "Conditional random fields: Probabilistic models for segmenting and labeling sequence data", "author": ["J. Lafferty", "A. McCallum", "F. Pereira"], "venue": "Proceedings of the International Conference on Machine Learning (ICML)", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2001}, {"title": "The use of classifiers in sequential inference", "author": ["V. Punyakanok", "D. Roth"], "venue": "Advances in Neural Information Processing Systems (NIPS)", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2001}, {"title": "Discriminative training methods for hidden Markov models: Theory and experiments with perceptron algorithms", "author": ["M. Collins"], "venue": "Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP)", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2002}, {"title": "Learning structured prediction models: A large margin approach", "author": ["B. Taskar", "V. Chatalbashev", "D. Koller", "C. Guestrin"], "venue": "Proceedings of the International Conference on Machine Learning (ICML)", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2005}, {"title": "Large margin methods for structured and interdependent output variables", "author": ["I. Tsochantaridis", "T. Hofmann", "T. Joachims", "Y. Altun"], "venue": "Journal of Machine Learning Research (JMLR)", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2005}, {"title": "Optimum branchings", "author": ["J. Edmonds"], "venue": "Journal of Research of the National Bureau of Standards", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 1967}, {"title": "On the shortest arborescence of a directed graph", "author": ["Y. Chu", "T. Liu"], "venue": "Science Sinica", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 1965}, {"title": "Ultraconservative online algorithms for multiclass problems", "author": ["K. Crammer", "Y. Singer"], "venue": "Journal of Machine Learning Research (JMLR)", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2003}, {"title": "Exponentiated gradient versus gradient descent for linear predictors", "author": ["J. Kivenen", "M. Warmuth"], "venue": "In: Symposium on the Theory of Computing (STOC)", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 1995}, {"title": "Inverse combinatorial optimization: A survey on problems, methods, and results", "author": ["C. Heuberger"], "venue": "Journal of Combinatorial Optimization", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2004}, {"title": "The base-matroid and inverse combinatorial optimization problems. Discrete applied mathematics", "author": ["M. Dell\u2019Amico", "F. Maffioli", "F. Malucelli"], "venue": null, "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2003}, {"title": "Inverse matroid intersection problem", "author": ["C. Mao-Cheng", "Y. Li"], "venue": "Mathematical Methods of Operations Research", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 1997}, {"title": "Case-factor diagrams for structured probabilistic modeling", "author": ["D. McAllester", "M. Collins", "F. Pereira"], "venue": "Proceedings of the Converence on Uncertainty in Artificial Intelligence (UAI)", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2004}, {"title": "Online passiveaggressive algorithms", "author": ["K. Crammer", "O. Dekel", "J. Keshet", "S. Shalev-Shwartz", "Y. Singer"], "venue": "Journal of Machine Learning Research (JMLR)", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2006}, {"title": "Online large-margin training of dependency parsers", "author": ["R. McDonald", "K. Crammer", "F. Pereira"], "venue": "Proceedings of the Conference of the Association for Computational Linguistics (ACL)", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2005}, {"title": "Hope and fear for discriminative training of statistical translation models", "author": ["D. Chiang"], "venue": "Journal of Machine Learning Research", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2012}, {"title": "Combinatorial optimization: polyhedra and efficiency", "author": ["A. Schrijver"], "venue": "Volume 24. Springer Verlag", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2003}, {"title": "A primal approach to the independent assignment problem", "author": ["S. Fujishige"], "venue": "Journal of the Operations Research Society of Japan", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 1977}, {"title": "A weighted matroid intersection algorithm", "author": ["A. Frank"], "venue": "Journal of Algorithms", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 1981}, {"title": "Non-projective dependency parsing using spanning tree algorithms", "author": ["R. McDonald", "F. Pereira", "K. Ribarov", "J. Hajic"], "venue": "Proceedings of the Joint Conference  on Human Language Technology Conference and Empirical Methods in Natural Language Processing (HLT/EMNLP)", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2005}, {"title": "A strongly polynomial algorithm for the inverse shortest arborescence problem. Discrete applied mathematics", "author": ["H. Zhiquan", "L. Zhenhong"], "venue": null, "citeRegEx": "22", "shortCiteRegEx": "22", "year": 1998}, {"title": "A statistical approach to parsing of czech", "author": ["D. Zeman"], "venue": "Prague Bulletin of Mathematical Linguistics", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 1998}, {"title": "Online Learning of Complex Categorical Problems", "author": ["K. Crammer"], "venue": "PhD thesis, Hebrew University of Jerusalem", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2004}, {"title": "A discriminative matching approach to word alignment", "author": ["B. Taskar", "S. Lacoste-Julien", "D. Klein"], "venue": "Proceedings of EMNLP 2005", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2005}, {"title": "The conll 2007 shared task on dependency parsing", "author": ["J. Nivre", "J. Hall", "J.Nilsson", "A. Chanev", "G. Eryigit", "S. Kubler", "S. Marinov", "E. Marsi"], "venue": "Proceedings of CoNLL-2007", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2007}, {"title": "Paraphrasing with bilingual parallel corpora", "author": ["C. Callison-Burch", "C. Bannard"], "venue": "Proceedings of 43rd Annual Meeting of the Association for Computational Linguistics", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2005}, {"title": "Cutting-plane training of structural svms", "author": ["T. Joachims", "T. Finley", "C.N. Yu"], "venue": "Machine Learning Journal", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2009}, {"title": "Perturb-and-MAP random fields: Using discrete optimization to learn and sample from energy models", "author": ["G. Papandreou", "A. Yuille"], "venue": "In: International Conference on Computer Vison (ICCV)", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2011}], "referenceMentions": [{"referenceID": 0, "context": "Our initial motivation for studying this problem comes from the structured prediction task in machine learning [1, 2, 3, 4, 5].", "startOffset": 111, "endOffset": 126}, {"referenceID": 1, "context": "Our initial motivation for studying this problem comes from the structured prediction task in machine learning [1, 2, 3, 4, 5].", "startOffset": 111, "endOffset": 126}, {"referenceID": 2, "context": "Our initial motivation for studying this problem comes from the structured prediction task in machine learning [1, 2, 3, 4, 5].", "startOffset": 111, "endOffset": 126}, {"referenceID": 3, "context": "Our initial motivation for studying this problem comes from the structured prediction task in machine learning [1, 2, 3, 4, 5].", "startOffset": 111, "endOffset": 126}, {"referenceID": 4, "context": "Our initial motivation for studying this problem comes from the structured prediction task in machine learning [1, 2, 3, 4, 5].", "startOffset": 111, "endOffset": 126}, {"referenceID": 5, "context": "As linguistic constraints dictate that the required dependency parse must form a rooted, spanning arborescence of the graph, one can use off-the-shelf combinatorial algorithms [6, 7] to find the highest weight arborescence.", "startOffset": 176, "endOffset": 182}, {"referenceID": 6, "context": "As linguistic constraints dictate that the required dependency parse must form a rooted, spanning arborescence of the graph, one can use off-the-shelf combinatorial algorithms [6, 7] to find the highest weight arborescence.", "startOffset": 176, "endOffset": 182}, {"referenceID": 7, "context": "It is well established in the learning theory literature that achieving a large margin solution enables better generalization [8].", "startOffset": 126, "endOffset": 129}, {"referenceID": 8, "context": "We consider minimizing the L2 norm because of connections to prior work [9].", "startOffset": 72, "endOffset": 75}, {"referenceID": 9, "context": "Heuberger [10] provides an excellent survey of the diverse inverse optimization problems that have been tackled.", "startOffset": 10, "endOffset": 14}, {"referenceID": 10, "context": "Both the inverse matroid optimization [11] and matroid intersection [12] have previously been studied in the setting of minimizing the L1 norm and with zero margin.", "startOffset": 38, "endOffset": 42}, {"referenceID": 11, "context": "Both the inverse matroid optimization [11] and matroid intersection [12] have previously been studied in the setting of minimizing the L1 norm and with zero margin.", "startOffset": 68, "endOffset": 72}, {"referenceID": 0, "context": "see [1, 13, 5, 3, 14, 15]), the discrete optimization problem is considered a \u201cblack box\u201d.", "startOffset": 4, "endOffset": 25}, {"referenceID": 12, "context": "see [1, 13, 5, 3, 14, 15]), the discrete optimization problem is considered a \u201cblack box\u201d.", "startOffset": 4, "endOffset": 25}, {"referenceID": 4, "context": "see [1, 13, 5, 3, 14, 15]), the discrete optimization problem is considered a \u201cblack box\u201d.", "startOffset": 4, "endOffset": 25}, {"referenceID": 2, "context": "see [1, 13, 5, 3, 14, 15]), the discrete optimization problem is considered a \u201cblack box\u201d.", "startOffset": 4, "endOffset": 25}, {"referenceID": 13, "context": "see [1, 13, 5, 3, 14, 15]), the discrete optimization problem is considered a \u201cblack box\u201d.", "startOffset": 4, "endOffset": 25}, {"referenceID": 14, "context": "see [1, 13, 5, 3, 14, 15]), the discrete optimization problem is considered a \u201cblack box\u201d.", "startOffset": 4, "endOffset": 25}, {"referenceID": 2, "context": "The simplest approach to solving the online structured prediction problem is the structured perceptron [3].", "startOffset": 103, "endOffset": 106}, {"referenceID": 7, "context": "An alternative approach is the large margin discriminative approach [8] that seeks to change the parameters as little as possible subject to the constraint that the true output has a higher score than all incorrect outputs.", "startOffset": 68, "endOffset": 71}, {"referenceID": 14, "context": "[15] circumvent this infeasibility by using a k-best list of possible outputs and restrict the set of constraints to require that", "startOffset": 0, "endOffset": 4}, {"referenceID": 14, "context": "This has been shown to be effective for small values of k on simple parsing tasks [15].", "startOffset": 82, "endOffset": 86}, {"referenceID": 15, "context": "However, for more complex tasks, like machine translation, one needs more complicated update frameworks [16].", "startOffset": 104, "endOffset": 108}, {"referenceID": 3, "context": "[4], who also consider structured prediction using inverse bipartite matchings.", "startOffset": 0, "endOffset": 3}, {"referenceID": 16, "context": "An easy generalization of [17] for \u03b4 \u2265 0 gives the following lemma.", "startOffset": 26, "endOffset": 30}, {"referenceID": 17, "context": "Fujishige [19] shows the following lemma for the case of \u03b4 = 0.", "startOffset": 10, "endOffset": 14}, {"referenceID": 18, "context": "It is important to note that while there are other optimality conditions for matroid intersection such as the weight decomposition theorem by Frank [20], these conditions do not easily generalize for non-zero \u03b4.", "startOffset": 148, "endOffset": 152}, {"referenceID": 16, "context": "We\u2019ll refer to two well-known lemmas [17] regarding the relationship between bases of a matroid and matchings in the exchange graph.", "startOffset": 37, "endOffset": 41}, {"referenceID": 19, "context": "The inverse maximum weight arborescence problem is important as it can used as a subroutine in the online learning for dependency parsing [21].", "startOffset": 138, "endOffset": 142}, {"referenceID": 20, "context": "As shown by [22], the inverse shortest s-t path problem can be reduced to the inverse arborescence problem.", "startOffset": 12, "endOffset": 16}, {"referenceID": 21, "context": "These consist of an input xt (for instance, a sentence) and an output yt (for instance, a syntactic analysis of this sentence described as an arborescence on a graph over the words in the sentence [23, 21]).", "startOffset": 197, "endOffset": 205}, {"referenceID": 19, "context": "These consist of an input xt (for instance, a sentence) and an output yt (for instance, a syntactic analysis of this sentence described as an arborescence on a graph over the words in the sentence [23, 21]).", "startOffset": 197, "endOffset": 205}, {"referenceID": 22, "context": "Algorithm 1 is an adaptation of the Passive-Aggressive MIRA algorithm [24] for structured prediction.", "startOffset": 70, "endOffset": 74}, {"referenceID": 21, "context": "For example, maximum weight arborescences are used to predict the parse tree of a sentence [23, 21], while maximum weight matchings are used for language translation and word alignments [25].", "startOffset": 91, "endOffset": 99}, {"referenceID": 19, "context": "For example, maximum weight arborescences are used to predict the parse tree of a sentence [23, 21], while maximum weight matchings are used for language translation and word alignments [25].", "startOffset": 91, "endOffset": 99}, {"referenceID": 23, "context": "For example, maximum weight arborescences are used to predict the parse tree of a sentence [23, 21], while maximum weight matchings are used for language translation and word alignments [25].", "startOffset": 186, "endOffset": 190}, {"referenceID": 22, "context": "Since we have shown that we can efficiently solve the inverse optimization problems for a variety of combinatorial structures, we can extend the error bounds of the MIRA algorithm [24] to work for learning the corresponding", "startOffset": 180, "endOffset": 184}, {"referenceID": 22, "context": "dissertation [24] and are relegated to Appendix C for clarity and brevity.", "startOffset": 13, "endOffset": 17}], "year": 2015, "abstractText": "Algorithm designers typically assume that the input data is correct, and then proceed to find \u201coptimal\u201d or \u201csub-optimal\u201d solutions using this input data. However this assumption of correct data does not always hold in practice, especially in the context of online learning systems where the objective is to learn appropriate feature weights given some training samples. Such scenarios necessitate the study of inverse optimization problems where one is given an input instance as well as a desired output and the task is to adjust the input data so that the given output is indeed optimal. Motivated by learning structured prediction models, in this paper we consider inverse optimization with a margin, i.e., we require the given output to be better than all other feasible outputs by a desired margin. We consider such inverse optimization problems for maximum weight matroid basis, matroid intersection, perfect matchings, minimum cost maximum flows, and shortest paths and derive the first known results for such problems with a non-zero margin. The effectiveness of these algorithmic approaches to online learning for structured prediction is also discussed.", "creator": "LaTeX with hyperref package"}}}