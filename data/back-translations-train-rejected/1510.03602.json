{"id": "1510.03602", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "13-Oct-2015", "title": "A language model based approach towards large scale and lightweight language identification systems", "abstract": "Multilingual spoken dialogue systems have gained prominence in the recent past necessitating the requirement for a front-end Language Identification (LID) system. Most of the existing LID systems rely on modeling the language discriminative information from low-level acoustic features. Due to the variabilities of speech (speaker and emotional variabilities, etc.), large-scale LID systems developed using low-level acoustic features suffer from a degradation in the performance. In this approach, we have attempted to model the higher level language discriminative phonotactic information for developing an LID system. In this paper, the input speech signal is tokenized to phone sequences by using a language independent phone recognizer. The language discriminative phonotactic information in the obtained phone sequences are modeled using statistical and recurrent neural network based language modeling approaches. As this approach, relies on higher level phonotactical information it is more robust to variabilities of speech. Proposed approach is computationally light weight, highly scalable and it can be used in complement with the existing LID systems.", "histories": [["v1", "Tue, 13 Oct 2015 09:51:23 GMT  (52kb,D)", "http://arxiv.org/abs/1510.03602v1", "Under review at ICASSP 2016"]], "COMMENTS": "Under review at ICASSP 2016", "reviews": [], "SUBJECTS": "cs.SD cs.CL", "authors": ["brij mohan lal srivastava", "hari krishna vydana", "anil kumar vuppala", "manish shrivastava"], "accepted": false, "id": "1510.03602"}, "pdf": {"name": "1510.03602.pdf", "metadata": {"source": "CRF", "title": "A LANGUAGE MODEL BASED APPROACH TOWARDS LARGE SCALE AND LIGHTWEIGHT LANGUAGE IDENTIFICATION SYSTEMS", "authors": ["Brij Mohan Lal Srivastava", "Hari Krishna Vydana", "Anil Kumar Vuppala", "Manish Shrivastava"], "emails": ["hari.vydana}@research.iiit.ac.in", "m.shrivastava}@iiit.ac.in"], "sections": [{"heading": null, "text": "Index Terms - Language Identification, Recurrent Neuronal Network Language Model (RNNLM), SRI Language Model (SRILM), Phono Recognition followed by Language Model (PRLM), Phonotactics."}, {"heading": "1. INTRODUCTION", "text": "In fact, it is such that most of them will be able to move into a different world, in which they are able to move, in which they move, in which they move, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they, in which they, in which they live, in which they, in which they, in which they live, in which they, in which they live, in which they, in which they, in which they live, in which they, in which they, in which they live."}, {"heading": "2. DATA DESCRIPTION", "text": "This paper uses the voice data that Topcoder 1 openly provides as part of the Speech Recognition Challenge. The data sets include recorded speech in 176 languages; the data set contains 375 utterances per language and the language names of these utterances are also available; each utterance has a duration of 10 seconds; each voice recording is played in a separate file and only one language is spoken in each file; the available data is reorganized into training, test and validation kits; 330 utterances have been used to train SRILM n-programs to develop speech models and the remaining 45 utterances are used to test the models; in the case of RNLMs, 300 utterances have been used for training, 30 utterances for validation and 45 utterances for testing the developed models; the data provided contains mp3 voice recordings that are later converted to WAV format at a sampling rate of 16 kHz; 1ps: / / communitytopcodule / t165rd = 165rd"}, {"heading": "3. PROPOSED APPROACH", "text": "In this approach, when developing an LID system, we rely largely on higher-level phonotactic information extracted from the language. A language-independent telephone recognition mechanism is used to tokenise the input language, and the telephone sequence is obtained. Although the telephone recognition mechanism is independent of the language that is decoded, we assume that the similar-sounding acoustic patterns are decoded independently of the language. By using the language-independent telephone recognition mechanism, we rely on the consistency of the telephone recognition mechanism and not on accuracy, i.e. similar-sounding acoustic noises are marked with the same telephone label regardless of the language. In this work, we assume that the statistical patterns contained in the telephone sequence obtained contain the language-discriminating information. To this end, SRILM and RNNLM are researched to model the statistical patterns that reflect the discriminatory information of the language from the proposed phone block."}, {"heading": "3.1. Language-independent phone recognizer", "text": "The goal of voice-independent telephone detection is to maximize coverage of the telephone units in all the languages for which the system is designed. We use PocketSphinx [8] as the front-end telephone detection device, which uses HMM-based telephone decoders from voice signals. For efficient decoding, a phonetically bound mixing model (PTM) is used, which contains 256 mixing components per state and assigns different mixing weights to the split states of triphones. This model provides a good balance between speed and accuracy. As it can be trained over huge amounts of data, it delivers a decent real-time decoding result. We use a US English telephone set with 40 phones and an unbiased phonetic speech model for decoding. The phone recognition mechanism can be improved by training across multiple languages, which will certainly increase coverage of common phonetic and acoustic patterns."}, {"heading": "3.2. Language modeling", "text": "SRILM n-gram (Unigram up to 6 grams) and RNNLM were used for experiments to model the statistical patterns in the phone sequences.RNNLM uses, as shown in Figure 2, the current telephone token w (t) and the previous state of the hidden layer s (t \u2212 1) to predict the probability of the next token y (t).The neuron in hidden layer s (t) uses sigmoid activation function.Once the network is trained, we can use the output layer y (t) as the probability distribution of the next word and the state of the hidden lay. c (t) represents the class layer here, which can be used optionally to reduce the computational complexity of the model. As observed in the experiments we conducted, models with lower classes generally fare better at higher computational costs.The matrix W represents recurring weights of the network, which are trained by means of reagation over time (TT)."}, {"heading": "4. RESULTS & DISCUSSION", "text": "Column 1 of the table. 1 are the different language models developed during the study. Column 2 specifies the performance of the language model with minimal accuracy among all 176 language models. Similarly, Column 3 specifies the performance of the language model with maximum accuracy among all 176 language models. Column 4 is the average percentage of correctly detected test cases in all 176 languages. Columns 2, 3 are intended to show that the performance of the LID system is consistent in all languages. In Table 1, row 2-7 are the performances with SRILM and lines 8-10 are the performaces obtained with RNLM. Language models are estimated for each of the 176 languages, experimenting with different classes (1, 6, 100) and the sizes of the hidden layers (30, 40) in RNLM. Model with 6 classes and 40 units in the hidden layer RNLM gives the best accuracy in the average of all languages."}, {"heading": "5. CONCLUSION", "text": "In this paper, we have investigated the scalability of PRLM-based approaches to speech identification, using a language-independent telephone recognition to tokenise the input language on telephone sequences. We have investigated approaches to speech modelling such as SRILM and RNLM to model the statistical patterns in the telephone sequence. As the proposed approach is based on phonotactic information, this can be used as complementary information to approaches based on speech information of low acoustic characteristics. The proposed approach is highly computationally efficient and can be useful to improve many other approaches to LID systems. From the results, it can be observed that both SRILM and RNLM-based language models have performed equally well in the development of a large-scale LID system. We have developed LID systems that use probabilities at the sentence level and n-best values of different language models. Currently, language models are being developed independently of each other."}, {"heading": "6. REFERENCES", "text": "[1] Alan McCree and Daniel Garcia-Romero, \"Dnn senone map multinomial i-vectors for phonotactic language recognition,\" in: Proc. of interspeech, Dresden, Germany, 2015, pp. 394-397. [2] Pavel Matejka, Le Zhang, Tim Ng, HS Mallidi, Ondrej Glembek, Jeff Ma, and Bing Zhang, \"Neural network engleneck features for language identification,\" Proc. IEEE Odyssey, pp. 299-304, 2014. [3] Radek Fe, Pavel Mate Rodriguez, and Bing Zhang, \"Neural network engleneck features for language identification,\" Proc."}], "references": [{"title": "Dnn senone map multinomial i-vectors for phonotactic language recognition", "author": ["Alan McCree", "Daniel Garcia-Romero"], "venue": "Proc. of interspeech, Dresden, Germany, 2015, pp. 394\u2013397.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2015}, {"title": "Neural network bottleneck features for language identification", "author": ["Pavel Matejka", "Le Zhang", "Tim Ng", "HS Mallidi", "Ondrej Glembek", "Jeff Ma", "Bing Zhang"], "venue": "Proc. IEEE Odyssey, pp. 299\u2013304, 2014.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2014}, {"title": "Multilingual bottleneck features for language recognition", "author": ["Radek F\u00e9r", "Pavel Mat\u011bjka", "Franti\u0161ek Gr\u00e9zl", "Old\u0159ich Plchot", "Jan \u010cernock\u1ef3"], "venue": "Proc. of interspeech, 2015, pp. 389\u2013393.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2015}, {"title": "Multilingual tandem bottleneck feature for language identification", "author": ["Wang Geng", "Jie Li", "Shanshan Zhang", "Xinyuan Cai", "Bo Xu"], "venue": "Proc. of interspeech, 2015, pp. 413\u2013417.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2015}, {"title": "An end-to-end approach to language identification in short utterances using convolutional neural networks", "author": ["Alicia Lozano-Diez", "Ruben Zazo-Candil", "Javier Gonzalez-Dominguez", "Doroteo T Toledano", "Joaquin Gonzalez-Rodriguez"], "venue": "Proc. of interspeech, 2015, pp. 403\u2013407.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2015}, {"title": "Automatic language identification", "author": ["Marc A Zissman", "Kay M Berkling"], "venue": "Speech Communication, vol. 35, no. 1, pp. 115\u2013124, 2001.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2001}, {"title": "Automatic language identification of telephone speech messages using phoneme recognition and n-gram modeling", "author": ["Marc Zissman", "Elliot Singer"], "venue": "Proc. ICASSP. IEEE, 1994, vol. 1, pp. 305\u2013308.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 1994}, {"title": "Pocketsphinx: A free, real-time continuous speech recognition system for hand-held devices", "author": ["David Huggins-Daines", "Mohit Kumar", "Arthur Chan", "Alan W Black", "Mosur Ravishankar", "Alex Rudnicky"], "venue": "Proc. ICASSP. IEEE, 2006, vol. 1, pp. 185\u2013188.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2006}, {"title": "Rnnlm-recurrent neural network language modeling toolkit", "author": ["Tomas Mikolov", "Stefan Kombrink", "Anoop Deoras", "Lukar Burget", "Jan Cernocky"], "venue": "Proc. of the 2011 ASRU Workshop, 2011, pp. 196\u2013201.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2011}], "referenceMentions": [{"referenceID": 0, "context": "Lately i-vector based features are explored and they have exhibited better performance compared to the conventional spectral features like Mel-frequency cepstral coefficients (MFCC), Linear predictive cepstral coefficients (LPCC) and Shifted delta cepstral coefficients (SDC) in NIST evaluations for speaker and language recognition tasks [1].", "startOffset": 339, "endOffset": 342}, {"referenceID": 1, "context": "Neural networks have also been employed as feature extractors to compute the stacked bottleneck features for LID [2].", "startOffset": 113, "endOffset": 116}, {"referenceID": 2, "context": "Multilingual bottleneck, multilingual tandem bottleneck obtained by stacking the SDC with the corresponding bottleneck features are explored in [3, 4].", "startOffset": 144, "endOffset": 150}, {"referenceID": 3, "context": "Multilingual bottleneck, multilingual tandem bottleneck obtained by stacking the SDC with the corresponding bottleneck features are explored in [3, 4].", "startOffset": 144, "endOffset": 150}, {"referenceID": 4, "context": "Additionally, convolutional neural networks have been studied to develop an end-to-end LID system for 8 languages in[5].", "startOffset": 116, "endOffset": 119}, {"referenceID": 5, "context": "The systems which try to model the language discriminative information at higher-level (phones, phone frequency and phonotactics) have exhibited better performance [6].", "startOffset": 164, "endOffset": 167}, {"referenceID": 6, "context": "is used in [7].", "startOffset": 11, "endOffset": 14}, {"referenceID": 6, "context": "In [7], language dependent phone recognizer for every language operated in parallel (PPRLM) is used to decode the test utterance and the language model with large number of uni-gram and bi-gram counts is used as an indication to the spoken language identity.", "startOffset": 3, "endOffset": 6}, {"referenceID": 7, "context": "We employ PocketSphinx[8] as the front-end phone recognizer which uses HMM-based phone decoder from speech signal.", "startOffset": 22, "endOffset": 25}, {"referenceID": 8, "context": "pabilities [9] so the language-discriminative patterns from the phone sequences is captured to a good extent which can observed from the results of section 4.", "startOffset": 11, "endOffset": 14}], "year": 2015, "abstractText": "Multilingual spoken dialogue systems have gained prominence in the recent past necessitating the requirement for a front-end Language Identification (LID) system. Most of the existing LID systems rely on modeling the language discriminative information from low-level acoustic features. Due to the variabilities of speech (speaker and emotional variabilities, etc.), large-scale LID systems developed using low-level acoustic features suffer from a degradation in the performance. In this approach, we have attempted to model the higher level language discriminative phonotactic information for developing an LID system. In this paper, the input speech signal is tokenized to phone sequences by using a language independent phone recognizer. The language discriminative phonotactic information in the obtained phone sequences are modeled using statistical and recurrent neural network based language modeling approaches. As this approach, relies on higher level phonotactical information it is more robust to variabilities of speech. Proposed approach is computationally light weight, highly scalable and it can be used in complement with the existing LID systems.", "creator": "LaTeX with hyperref package"}}}