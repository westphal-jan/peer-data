{"id": "1510.03623", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "13-Oct-2015", "title": "Elastic regularization in restricted Boltzmann machines: Dealing with $p\\gg N$", "abstract": "Restricted Boltzmann machines (RBMs) are endowed with the universal power of modeling (binary) joint distributions. Meanwhile, as a result of their confining network structure, training RBMs confronts less difficulties (compared with more complicated models, e.g., Boltzmann machines) when dealing with approximation and inference issues. However, in certain computational biology scenarios, such as the cancer data analysis, employing RBMs to model data features may lose its efficacy due to the \"$p\\gg N$\" problem, in which the number of features/predictors is much larger than the sample size. The \"$p\\gg N$\" problem puts the bias-variance trade-off in a more crucial place when designing statistical learning methods. In this manuscript, we try to address this problem by proposing a novel RBM model, called elastic restricted Boltzmann machine (eRBM), which incorporates the elastic regularization term into the likelihood/cost function. We provide several theoretical analysis on the superiority of our model. Furthermore, attributed to the classic contrastive divergence (CD) algorithm, eRBMs can be trained efficiently. Our novel model is a promising method for future cancer data analysis.", "histories": [["v1", "Tue, 13 Oct 2015 11:14:03 GMT  (15kb,D)", "http://arxiv.org/abs/1510.03623v1", "12 pages, 1 figure"], ["v2", "Wed, 21 Oct 2015 12:19:13 GMT  (0kb,I)", "http://arxiv.org/abs/1510.03623v2", "This paper has been withdrawn by the author due to a critical error"]], "COMMENTS": "12 pages, 1 figure", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["sai zhang"], "accepted": false, "id": "1510.03623"}, "pdf": {"name": "1510.03623.pdf", "metadata": {"source": "CRF", "title": "Elastic regularization in restricted Boltzmann machines: Dealing with p N", "authors": ["Sai Zhang"], "emails": ["zhangsai13@mails.tsinghua.edu.cn."], "sections": [{"heading": null, "text": "* Department of Interdisciplinary Information Sciences, Tsinghua University, Beijing 100084, China. E-mail: zhangsai13 @ mails.tsinghua.edu.cn.ar Xiv: 151 0.03 623v 1 [cs.L G] 13 Oct 201 5"}, {"heading": "1 Introduction", "text": "This year, it is so far that it is only a matter of time before it is ready, until it is ready."}, {"heading": "2 Preliminaries", "text": "Restricted systems (RBMs, see Figure 1) [10] are undirected graphical models (also referred to as Markov random fields) that describe the probability distributions of binary variables. [20] Different generalizations [20] of the RBMs make them effective in modeling different types of data, e.g. counting vectors [17] and real evaluated data [20]. As a preliminary example, we can present the Bernoulli-Bernoulli RBMs (BBRBMs) here. In the following, we always refer to BBRBM from RBM for short-lived life. At RBM, one can consider a split graph: The visible layer consists of m visible units {V1, \u00b7 Vm} to represent observed data / variables, and the dependence on them is represented by n hidden units {H1, \u00b7 \u00b7 Hn} representing the hidden layer. We note that BRM are both visible units of a hidden BM."}, {"heading": "3 Elastic restricted Boltzmann machines", "text": "We have mentioned that RBMs are universal approximators of discrete distributions [21]. However, in the case of \"p n,\" i.e. the dimension of the data characteristics is much larger than the sample size, the traditional RBM models and their corresponding training algorithms run into a dilemma of model complexity versus generalization, where the bias-variance trade-off plays a more important role in model design. Mathematically, limited by the insufficient training samples, in the \"p N\" case the poorly posed problem [22] always arises, the wrong solutions of which reveal the wrong structures in the data leading to the overmatched phenomena. It has been shown that the well-studied regularization theory can satisfactorily solve the poorly posed problem and is widespread in statistical and optimization fields [7, 22]."}, {"heading": "3.1 From complexity to generalization", "text": "Based on the discussion in [20], the number of bits needed to identify an input case determines the extent of the constraints that each training example imposes on the model parameters. Therefore, it is reasonable to adjust m \u00b7 n parameters to m \u00b7 N training bits if N n. This means that we should determine the number of hidden units that are small enough to be limited by the sample size, resulting in large distortions but small deviations. On the other hand, since the input feature dimension is large, and the addition of hidden units improves modeling performance at least for BBRBM [21], we need a sufficient number of hidden units to characterize the complicated probabilistic distribution of high-dimensional input data that causes small distortions but large deviations. To address this particular bias-variance-trade-off problem, we consider the following problem, S | L, the optimizer problem \u2212 1."}, {"heading": "3.2 From generalization to complexity", "text": "It has been shown that in the \"p N\" problem there are strong correlations between visible variables that group the variables into groups [6, 9]. Unlike linear models (e.g. linear regression and logistic regression), in which variable correlations are embodied by the corresponding coefficients, RBMs can model this grouping effect with hidden variables, which is also the main superiority of latent variable models. However, the regulation occurring in problem (7) also limits the flexibility of the model, which weakens the ability of RBM to fully detect the variable correlations. In particular, it has been shown in theory that LASSO tends to randomly select only one variable from the correlated group [23]. In order to make another compromise between the model and its expression force, the well-known l2 regulation turns into our natural choice."}, {"heading": "4 Theoretical analysis", "text": "In order to characterize the effects of the elastic regularization of problem (9) (or equivalent problem (10)), we provide here several theoretical results on both extreme situations, in which several visible variables are positively correlated, namely that they are identical, as well as the general case. The techniques used here are analogous to those in [9]. Note that in this manuscript we can only look at the local minima of the problem (9), because they are not conventional. For the brewery, we refer to the objective function of the problem (9) as O (W) + R (W), whereas L (W) = \u2212 1 | L (S), R (W) = 1), ik ik (K) + 2).In addition, we have a local minimizer of the problem (9) in the U neighborhood as W (W), meaning that we have O (W)."}, {"heading": "5 Model training", "text": "With only a few modifications, our eRBM model can be efficiently trained on the basis of the CD algorithm. In the previous sections, we have noted that the CD algorithm is used to approximate the gradient of the log likelihood function, i.e. Equation 5. In the meantime, we note that the optimization function of the eRBMs (i.e. problem (9) contains the elastic regularization term in addition to the probability function. Therefore, in order to learn parameters of the eRBMs, we should integrate the derivatives of the l1 and l2 standards into the CD algorithm. It is trivial to calculate the derivative of the l2 standard for model parameters, i.e., we should integrate the derivatives of the l1 and l2 norms. As for the l1 standard, we adopt the subgrade method [24] here, i.e. we should amplify the results of the CD."}, {"heading": "6 Related work", "text": "The idea of regulation to solve the \"p N\" problem and generalize models is widely used in statistics [7, 25]. There are many other regulatory / punitive terms with different properties that equip statistical models, see [7] for a nice check. In particular, traditional comb regression [5], LASSO [8], and the elastic net [9] use similar regulation techniques used in our study. However, unlike our eRBMs, these basic models are linear in nature and have no latent variables that lack the ability to model complicated nonlinear statistics. Furthermore, both supervised and uncontrolled learning are practicable in eRBMs, which extend the scope of our method, as in the semi-supervised scenarios."}, {"heading": "7 Conclusion", "text": "The \"p N\" problem calls into question the existing statistical and computational models for the analysis of cancer data. In this manuscript, we propose a novel graphical model called Boltzmann Elastic Restricted Machines (eRBMs) to address this problem. In the principle of bias-variance compromise, we are reforming the optimization goal of traditional RBMs by combining l1 and l2 regularization. Comprehensive theoretical analysis shows that our eRBM models acquire elegant properties that satisfy our primary motivations. In addition, we are developing an efficient training algorithm called elastic contrast divergence (eCD) for eRBMs based on the classical CD algorithm."}, {"heading": "Acknowledgements", "text": "The author thanks Dr. T. Chen, Dr. N. Chen and Dr. J. Zeng for their valuable comments and suggestions on this manuscript."}], "references": [{"title": "Expanding the Computational Toolbox for Mining Cancer Genomes", "author": ["L. Ding", "M.C. Wendl", "J.F. McMichael", "B.J. Raphael"], "venue": "Nat Rev Genet, vol. 15, pp. 556\u2013570, Aug. 2014.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2014}, {"title": "Big Data Mining Yields Novel Insights on Cancer", "author": ["P. Jiang", "X.S. Liu"], "venue": "Nat Genet, vol. 47, pp. 103\u2013104, Feb. 2015.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2015}, {"title": "Principles and Methods of Integrative Genomic Analyses in Cancer", "author": ["V.N. Kristensen", "O.C. Lingjaerde", "H.G. Russnes", "H.K.M. Vollan", "A. Frigessi", "A.-L. Borresen-Dale"], "venue": "Nat Rev Cancer, vol. 14, pp. 299\u2013313, May 2014.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2014}, {"title": "The Cancer Genome Atlas Pan-Cancer Analysis Project", "author": ["T.C.G.A.R. Network", "J.N. Weinstein", "E.A. Collisson", "G.B. Mills", "K.R.M. Shaw", "B.A. Ozenberger", "K. Ellrott", "I. Shmulevich", "C. Sander", "J.M. Stuart"], "venue": "Nat Genet, vol. 45, pp. 1113\u20131120, Oct. 2013.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2013}, {"title": "The Elements of Statistical Learning: Data Mining, Inference, and Prediction", "author": ["T. Hastie", "R. Tibshirani", "J. Friedman"], "venue": null, "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2009}, {"title": "Predicting the Clinical Status of Human Breast Cancer by Using Gene Expression Profiles", "author": ["M. West", "C. Blanchette", "H. Dressman", "E. Huang", "S. Ishida", "R. Spang", "H. Zuzan", "J.A. Olson", "J.R. Marks", "J.R. Nevins"], "venue": "Proceedings of the National Academy of Sciences, vol. 98, no. 20, pp. 11462\u201311467, 2001.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2001}, {"title": "A Selective Overview of Variable Selection in High Dimensional Feature Space", "author": ["J. Fan", "J. Lv"], "venue": "Statistica Sinica, vol. 20, pp. 101\u2013148, Jan. 2010.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2010}, {"title": "Regression Shrinkage and Selection Via the Lasso", "author": ["R. Tibshirani"], "venue": "Journal of the Royal Statistical Society, Series B, vol. 58, pp. 267\u2013288, 1994.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 1994}, {"title": "Regularization and Variable Selection via the Elastic Net", "author": ["H. Zou", "T. Hastie"], "venue": "Journal of the Royal Statistical Society: Series B (Statistical Methodology), vol. 67, no. 2, pp. 301\u2013320, 2005.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2005}, {"title": "An Introduction to Restricted Boltzmann Machines", "author": ["A. Fischer", "C. Igel"], "venue": "Progress in Pattern Recognition, Image Analysis, Computer Vision, and Applications (L. Alvarez, M. Mejail, L. Gomez, and J. Jacobo, eds.), vol. 7441 of Lecture Notes in Computer Science, pp. 14\u201336, Springer Berlin Heidelberg, 2012.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2012}, {"title": "Reducing the Dimensionality of Data with Neural Networks", "author": ["G. Hinton", "R. Salakhutdinov"], "venue": "Science, vol. 313, no. 5786, pp. 504 \u2013 507, 2006.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2006}, {"title": "A Fast Learning Algorithm for Deep Belief Nets", "author": ["G.E. Hinton", "S. Osindero", "Y.-W. Teh"], "venue": "Neural computation, vol. 18, no. 7, pp. 1527\u20131554, 2006.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2006}, {"title": "Learning Deep Architectures for AI", "author": ["Y. Bengio"], "venue": "Foundations and Trends in Machine Learning, vol. 2, pp. 1\u2013127, Jan. 2009.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2009}, {"title": "Deep Boltzmann Machines", "author": ["R. Salakhutdinov", "G.E. Hinton"], "venue": "International Conference on Artificial Intelligence and Statistics, pp. 448\u2013455, 2009.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2009}, {"title": "Representational Power of Restricted Boltzmann Machines and Deep Belief Networks", "author": ["N. Le Roux", "Y. Bengio"], "venue": "Neural Comput., vol. 20, pp. 1631\u20131649, June 2008.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2008}, {"title": "Training Products of Experts by Minimizing Contrastive Divergence", "author": ["G.E. Hinton"], "venue": "Neural Comput., vol. 14, pp. 1771\u20131800, Aug. 2002.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2002}, {"title": "Replicated Softmax: An Undirected Topic Model", "author": ["G.E. Hinton", "R.R. Salakhutdinov"], "venue": "Advances in Neural Information Processing Systems 22 (Y. Bengio, D. Schuurmans, J. Lafferty, C. Williams, and A. Culotta, eds.), pp. 1607\u20131614, Curran Associates, Inc., 2009.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2009}, {"title": "Restricted Boltzmann Machines for Collaborative Filtering", "author": ["R. Salakhutdinov", "A. Mnih", "G. Hinton"], "venue": "Proceedings of the 24th International Conference on Machine Learning, ICML \u201907, (New York, NY, USA), pp. 791\u2013798, ACM, 2007. 11", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2007}, {"title": "Predicting Drug-Target Interactions Using Restricted Boltzmann Machines", "author": ["Y. Wang", "J. Zeng"], "venue": "Bioinformatics, vol. 29, no. 13, pp. i126\u2013i134, 2013.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2013}, {"title": "A Practical Guide to Training Restricted Boltzmann Machines", "author": ["G. Hinton"], "venue": "Tech. Rep. UTML-TR-2010- 003, Dept. of Computer Science, Univ.of Toronto, 2010.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2010}, {"title": "Representational Power of Restricted Boltzmann Machines and Deep Belief Networks", "author": ["N. Le Roux", "Y. Bengio"], "venue": "Neural Comput., vol. 20, pp. 1631\u20131649, June 2008.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2008}, {"title": "Least Angle Regression", "author": ["B. Efron", "T. Hastie", "L. Johnstone", "R. Tibshirani"], "venue": "Annals of Statistics, vol. 32, pp. 407\u2013499, 2004.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2004}, {"title": "Variable Selection via Nonconcave Penalized Likelihood and Its Oracle Properties", "author": ["J. Fan", "R. Li"], "venue": "Journal of the American Statistical Association, vol. 96, no. 456, pp. 1348\u20131360, 2001.", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2001}, {"title": "Sparse Coding with An Overcomplete Basis Set: A Strategy Employed by V1", "author": ["B.A. Olshausen", "D.J. Field"], "venue": "Vision Research, vol. 37, pp. 3311\u20133325, Dec. 1997.", "citeRegEx": "26", "shortCiteRegEx": null, "year": 1997}, {"title": "Emergence of Simple-Cell Receptive Field Properties by Learning A Sparse Code for Natural Images", "author": ["B.A. Olshausen", "D.J. Field"], "venue": "Nature, vol. 381, pp. 607\u2013609, June 1996.", "citeRegEx": "27", "shortCiteRegEx": null, "year": 1996}, {"title": "Representation Learning: A Review and New Perspectives", "author": ["Y. Bengio", "A. Courville", "P. Vincent"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 35, no. 8, pp. 1798\u20131828, 2013.", "citeRegEx": "28", "shortCiteRegEx": null, "year": 1828}, {"title": "Sparse Feature Learning for Deep Belief Networks", "author": ["M. aurelio Ranzato", "Y. lan Boureau", "Y.L. Cun"], "venue": "Advances in Neural Information Processing Systems 20 (J. Platt, D. Koller, Y. Singer, and S. Roweis, eds.), pp. 1185\u20131192, Curran Associates, Inc., 2008.", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2008}, {"title": "Efficient Learning of Sparse Representations with An Energy-Based Model", "author": ["M. aurelio Ranzato", "C. Poultney", "S. Chopra", "Y.L. Cun"], "venue": "Advances in Neural Information Processing Systems 19 (B. Sch\u00f6lkopf, J. Platt, and T. Hoffman, eds.), pp. 1137\u20131144, MIT Press, 2007.", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2007}, {"title": "Unsupervised Learning of Invariant Feature Hierarchies with Applications to Object Recognition", "author": ["M. Ranzato", "F. Huang", "Y. Boureau", "Y. LeCun"], "venue": "Computer Vision and Pattern Recognition, IEEE Computer Society Conference on, vol. 0, (Los Alamitos, CA, USA), pp. 1\u20138, IEEE, June 2007.", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2007}, {"title": "3D Object Recognition with Deep Belief Nets", "author": ["V. Nair", "G.E. Hinton"], "venue": "Advances in Neural Information Processing Systems 22 (Y. Bengio, D. Schuurmans, J. Lafferty, C. Williams, and A. Culotta, eds.), pp. 1339\u20131347, Curran Associates, Inc., 2009. 12", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2009}], "referenceMentions": [{"referenceID": 0, "context": "The availability of these large amounts of cancer molecular profiles facilitates extensive cancer disease studies and further novel biological discoveries, in which appropriate computational and statistical tools are needed to perform the data analysis task [1\u20133], e.", "startOffset": 258, "endOffset": 263}, {"referenceID": 1, "context": "The availability of these large amounts of cancer molecular profiles facilitates extensive cancer disease studies and further novel biological discoveries, in which appropriate computational and statistical tools are needed to perform the data analysis task [1\u20133], e.", "startOffset": 258, "endOffset": 263}, {"referenceID": 2, "context": "The availability of these large amounts of cancer molecular profiles facilitates extensive cancer disease studies and further novel biological discoveries, in which appropriate computational and statistical tools are needed to perform the data analysis task [1\u20133], e.", "startOffset": 258, "endOffset": 263}, {"referenceID": 3, "context": "For example, a typical cancer genomic dataset downloaded from The Cancer Genome Atlas (TCGA) [4] contains only hundreds (denoted by N) of samples, but each sample contains more than 10k (denoted by p) measurements of genomic profiling, such as gene expression and copy number variation.", "startOffset": 93, "endOffset": 96}, {"referenceID": 4, "context": "This well-known \u201cp N\u201d problem [5, 6] requires the designed model to be able to address the bias-variance trade-off issue [5] effectively: On one hand, as the feature dimension p is large, the model should be equipped with enough potential/complexity for discovering complicated statistical characteristics across the input features; one the other hand, we need the model to perform well in sensitivity/generalization since the observed sample size N is extremely limited compared to p, which will lead to overfitting easily.", "startOffset": 30, "endOffset": 36}, {"referenceID": 5, "context": "This well-known \u201cp N\u201d problem [5, 6] requires the designed model to be able to address the bias-variance trade-off issue [5] effectively: On one hand, as the feature dimension p is large, the model should be equipped with enough potential/complexity for discovering complicated statistical characteristics across the input features; one the other hand, we need the model to perform well in sensitivity/generalization since the observed sample size N is extremely limited compared to p, which will lead to overfitting easily.", "startOffset": 30, "endOffset": 36}, {"referenceID": 4, "context": "This well-known \u201cp N\u201d problem [5, 6] requires the designed model to be able to address the bias-variance trade-off issue [5] effectively: On one hand, as the feature dimension p is large, the model should be equipped with enough potential/complexity for discovering complicated statistical characteristics across the input features; one the other hand, we need the model to perform well in sensitivity/generalization since the observed sample size N is extremely limited compared to p, which will lead to overfitting easily.", "startOffset": 121, "endOffset": 124}, {"referenceID": 6, "context": "In statistics and machine learning fields, one popular principle to tackle the \u201cp N\u201d problem is variable selection [7], also referred to as feature selection, which integrates additional penalty/regularization terms into the original cost function, and performs estimation along with selecting pivotal features.", "startOffset": 115, "endOffset": 118}, {"referenceID": 7, "context": "LASSO [8] and its descendant elastic net [9] are two of the most classic methods realizing feature selection.", "startOffset": 6, "endOffset": 9}, {"referenceID": 8, "context": "LASSO [8] and its descendant elastic net [9] are two of the most classic methods realizing feature selection.", "startOffset": 41, "endOffset": 44}, {"referenceID": 9, "context": "Restricted Boltzmann machines (RBMs) [10] have been widely studied and used in the machine learning fields.", "startOffset": 37, "endOffset": 41}, {"referenceID": 10, "context": "For example, they are fundamental building blocks of the deep learning frameworks [11\u201313], like deep belief networks (DBNs) [12] and deep Boltzmann machines (DBMs) [14].", "startOffset": 82, "endOffset": 89}, {"referenceID": 11, "context": "For example, they are fundamental building blocks of the deep learning frameworks [11\u201313], like deep belief networks (DBNs) [12] and deep Boltzmann machines (DBMs) [14].", "startOffset": 82, "endOffset": 89}, {"referenceID": 12, "context": "For example, they are fundamental building blocks of the deep learning frameworks [11\u201313], like deep belief networks (DBNs) [12] and deep Boltzmann machines (DBMs) [14].", "startOffset": 82, "endOffset": 89}, {"referenceID": 11, "context": "For example, they are fundamental building blocks of the deep learning frameworks [11\u201313], like deep belief networks (DBNs) [12] and deep Boltzmann machines (DBMs) [14].", "startOffset": 124, "endOffset": 128}, {"referenceID": 13, "context": "For example, they are fundamental building blocks of the deep learning frameworks [11\u201313], like deep belief networks (DBNs) [12] and deep Boltzmann machines (DBMs) [14].", "startOffset": 164, "endOffset": 168}, {"referenceID": 14, "context": "In particular, RBMs have been proven to own the universal potential of approximating discrete distributions [15].", "startOffset": 108, "endOffset": 112}, {"referenceID": 15, "context": ", the contrastive divergence (CD) algorithm [16], can be used to train RBMs efficiently.", "startOffset": 44, "endOffset": 48}, {"referenceID": 9, "context": "In fact, based on the original RBMs, various modifications have been made in different applications, for instance, Gaussian RBMs are proposed to model images [10], replicated softmax model are used to model word distributions and extract latent topics in the documents [17], RBMs can also be made to perform collaborative filtering tasks [18].", "startOffset": 158, "endOffset": 162}, {"referenceID": 16, "context": "In fact, based on the original RBMs, various modifications have been made in different applications, for instance, Gaussian RBMs are proposed to model images [10], replicated softmax model are used to model word distributions and extract latent topics in the documents [17], RBMs can also be made to perform collaborative filtering tasks [18].", "startOffset": 269, "endOffset": 273}, {"referenceID": 17, "context": "In fact, based on the original RBMs, various modifications have been made in different applications, for instance, Gaussian RBMs are proposed to model images [10], replicated softmax model are used to model word distributions and extract latent topics in the documents [17], RBMs can also be made to perform collaborative filtering tasks [18].", "startOffset": 338, "endOffset": 342}, {"referenceID": 18, "context": "Note that in the bioinformatics field, Wang and Zeng have proposed an RBM-liked model to predict drug\u2013target interactions effectively [19].", "startOffset": 134, "endOffset": 138}, {"referenceID": 9, "context": "1) [10] are undirected graphical models (also referred to as Markov random fields) which describe the probability distributions of binary variables.", "startOffset": 3, "endOffset": 7}, {"referenceID": 19, "context": "Various generalizations [20] of RBMs make them effective to model different types of data, e.", "startOffset": 24, "endOffset": 28}, {"referenceID": 16, "context": ", count vectors [17] and real-valued data [20].", "startOffset": 16, "endOffset": 20}, {"referenceID": 19, "context": ", count vectors [17] and real-valued data [20].", "startOffset": 42, "endOffset": 46}, {"referenceID": 15, "context": "Intuitively, during sampling we can run the Markov chain for only a few steps before its convergence, which results in the contrastive divergence (CD) algorithm [16].", "startOffset": 161, "endOffset": 165}, {"referenceID": 20, "context": "We have mentioned that RBMs are universal approximators of discrete distributions [21].", "startOffset": 82, "endOffset": 86}, {"referenceID": 6, "context": "It has been shown that the well-studied regularization theory can solve the ill-posed problem satisfactorily and has been widely used in statistics and optimization fields [7, 22].", "startOffset": 172, "endOffset": 179}, {"referenceID": 19, "context": "1 From complexity to generalization Based on the discussion in [20], the number of bits it takes to identify an input data determines the amount of constraint each training example imposes on the model parameters.", "startOffset": 63, "endOffset": 67}, {"referenceID": 20, "context": "On the other hand, since the input feature dimension is severely large, and adding hidden units improves the modeling power at least for BBRBM [21], we need large enough number of hidden units to characterize the complicated probabilistic distribution of the high-dimensional input data, which yields small bias but large variance.", "startOffset": 143, "endOffset": 147}, {"referenceID": 7, "context": "For example, in linear regression, LASSO [8] plays the role of variable selection and omits those predictors less responsible for the regression objective.", "startOffset": 41, "endOffset": 44}, {"referenceID": 5, "context": "2 From generalization to complexity It has been shown that in the \u201cp N\u201d problem, there are strong correlations across visible variables, which clusters the variables into groups [6, 9].", "startOffset": 178, "endOffset": 184}, {"referenceID": 8, "context": "2 From generalization to complexity It has been shown that in the \u201cp N\u201d problem, there are strong correlations across visible variables, which clusters the variables into groups [6, 9].", "startOffset": 178, "endOffset": 184}, {"referenceID": 21, "context": "In particular, it has been shown theoretically that LASSO tends to randomly select only one variable from the correlated group [23].", "startOffset": 127, "endOffset": 131}, {"referenceID": 19, "context": "Note that when \u03b1 = 0, Problem (10) degenerates to Problem (7), while when \u03b1 = 1, Problem (10) loses its l1 regularization term and is called the weight decay method in neural networks [20].", "startOffset": 184, "endOffset": 188}, {"referenceID": 8, "context": "Techniques used here are analogous to those in [9].", "startOffset": 47, "endOffset": 50}, {"referenceID": 8, "context": "1 This terminology comes from [9] in part, and its intuition can also be found there.", "startOffset": 30, "endOffset": 33}, {"referenceID": 8, "context": "Following [9], we define D\u03bb1,\u03bb2(i, j, k) := |\u0175ik(\u03bb1, \u03bb2)\u2212 \u0175jk(\u03bb1, \u03bb2)| .", "startOffset": 10, "endOffset": 13}, {"referenceID": 19, "context": "Also, it has been verified empirically that the 1-step CD algorithm can yield satisfiable training results [20].", "startOffset": 107, "endOffset": 111}, {"referenceID": 19, "context": ", learning rate, training batch size and momentum [20], are omitted in Algorithm 1 for brevity.", "startOffset": 50, "endOffset": 54}, {"referenceID": 6, "context": "The idea of regularization to solve the \u201cp N\u201d problem and augment model generalization is widely used in statistics [7, 25].", "startOffset": 116, "endOffset": 123}, {"referenceID": 22, "context": "The idea of regularization to solve the \u201cp N\u201d problem and augment model generalization is widely used in statistics [7, 25].", "startOffset": 116, "endOffset": 123}, {"referenceID": 6, "context": "There are many other regularization/penalty terms with various properties equipping statistical models, see [7] for a nice review.", "startOffset": 108, "endOffset": 111}, {"referenceID": 4, "context": "In particular, the traditional ridge regression [5], LASSO [8] and elastic net [9] use similar regularization techniques adopted in our study.", "startOffset": 48, "endOffset": 51}, {"referenceID": 7, "context": "In particular, the traditional ridge regression [5], LASSO [8] and elastic net [9] use similar regularization techniques adopted in our study.", "startOffset": 59, "endOffset": 62}, {"referenceID": 8, "context": "In particular, the traditional ridge regression [5], LASSO [8] and elastic net [9] use similar regularization techniques adopted in our study.", "startOffset": 79, "endOffset": 82}, {"referenceID": 19, "context": "Several code packages have implemented the weight decay technique [20] to train an RBM.", "startOffset": 66, "endOffset": 70}, {"referenceID": 4, "context": "Indeed, sparsity is always a dominant topic in machine learning, signal processing and statistics [5, 13].", "startOffset": 98, "endOffset": 105}, {"referenceID": 12, "context": "Indeed, sparsity is always a dominant topic in machine learning, signal processing and statistics [5, 13].", "startOffset": 98, "endOffset": 105}, {"referenceID": 23, "context": "The consideration of sparsity first appears in computational neuroscience (the visual system [26]) and is further embodied as sparse coding [27].", "startOffset": 93, "endOffset": 97}, {"referenceID": 24, "context": "The consideration of sparsity first appears in computational neuroscience (the visual system [26]) and is further embodied as sparse coding [27].", "startOffset": 140, "endOffset": 144}, {"referenceID": 25, "context": "Based on this biological observation, researchers in machine learning, especially in neural networks and deep learning, have been exploring various effective distributed sparse representations [28], such as the sparse auto-encoders [29\u201331] and the sparse RBMs [20, 32].", "startOffset": 193, "endOffset": 197}, {"referenceID": 26, "context": "Based on this biological observation, researchers in machine learning, especially in neural networks and deep learning, have been exploring various effective distributed sparse representations [28], such as the sparse auto-encoders [29\u201331] and the sparse RBMs [20, 32].", "startOffset": 232, "endOffset": 239}, {"referenceID": 27, "context": "Based on this biological observation, researchers in machine learning, especially in neural networks and deep learning, have been exploring various effective distributed sparse representations [28], such as the sparse auto-encoders [29\u201331] and the sparse RBMs [20, 32].", "startOffset": 232, "endOffset": 239}, {"referenceID": 28, "context": "Based on this biological observation, researchers in machine learning, especially in neural networks and deep learning, have been exploring various effective distributed sparse representations [28], such as the sparse auto-encoders [29\u201331] and the sparse RBMs [20, 32].", "startOffset": 232, "endOffset": 239}, {"referenceID": 19, "context": "Based on this biological observation, researchers in machine learning, especially in neural networks and deep learning, have been exploring various effective distributed sparse representations [28], such as the sparse auto-encoders [29\u201331] and the sparse RBMs [20, 32].", "startOffset": 260, "endOffset": 268}, {"referenceID": 29, "context": "Based on this biological observation, researchers in machine learning, especially in neural networks and deep learning, have been exploring various effective distributed sparse representations [28], such as the sparse auto-encoders [29\u201331] and the sparse RBMs [20, 32].", "startOffset": 260, "endOffset": 268}, {"referenceID": 12, "context": "The detailed motivation and advantage of introducing sparsity can be found in an excellent survey [13].", "startOffset": 98, "endOffset": 102}], "year": 2017, "abstractText": "Restricted Boltzmann machines (RBMs) are endowed with the universal power of modeling (binary) joint distributions. Meanwhile, as a result of their confining network structure, training RBMs confronts less difficulties (compared with more complicated models, e.g., Boltzmann machines) when dealing with approximation and inference issues. However, in certain computational biology scenarios, such as the cancer data analysis, employing RBMs to model data features may lose its efficacy due to the \u201cp N\u201d problem, in which the number of features/predictors is much larger than the sample size. The \u201cp N\u201d problem puts the bias-variance trade-off in a more crucial place when designing statistical learning methods. In this manuscript, we try to address this problem by proposing a novel RBM model, called elastic restricted Boltzmann machine (eRBM), which incorporates the elastic regularization term into the likelihood/cost function. We provide several theoretical analysis on the superiority of our model. Furthermore, attributed to the classic contrastive divergence (CD) algorithm, eRBMs can be trained efficiently. Our novel model is a promising method for future cancer data analysis. \u2217 Institute for Interdisciplinary Information Sciences, Tsinghua University, Beijing 100084, China. E-mail: zhangsai13@mails.tsinghua.edu.cn. 1 ar X iv :1 51 0. 03 62 3v 1 [ cs .L G ] 1 3 O ct 2 01 5", "creator": "TeX"}}}