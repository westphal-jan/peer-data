{"id": "1510.03753", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "13-Oct-2015", "title": "Improved Deep Learning Baselines for Ubuntu Corpus Dialogs", "abstract": "This paper presents results of our experiments using the Ubuntu Dialog Corpus - the largest publicly available multi-turn dialog corpus. First, we use an in-house implementation of previously reported models to do an independent evaluation using the same data. Second, we evaluate the performances of various LSTMs, Bi-LSTMs and CNNs on the dataset. Third, we create an ensemble by averaging predictions of multiple models. The ensemble further improves the performance and it achieves a state-of-the-art result for this dataset. Finally, we discuss our future plans using this corpus.", "histories": [["v1", "Tue, 13 Oct 2015 15:56:26 GMT  (87kb,D)", "http://arxiv.org/abs/1510.03753v1", null], ["v2", "Tue, 3 Nov 2015 08:23:50 GMT  (88kb,D)", "http://arxiv.org/abs/1510.03753v2", "Accepted to Machine Learning for SLU &amp; Interaction NIPS 2015 Workshop"]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["rudolf kadlec", "martin schmid", "jan kleindienst"], "accepted": false, "id": "1510.03753"}, "pdf": {"name": "1510.03753.pdf", "metadata": {"source": "CRF", "title": "Improved Deep Learning Baselines for Ubuntu Corpus Dialogs", "authors": ["Rudolf Kadlec", "Martin Schmid", "Jan Kleindienst"], "emails": ["jankle}@cz.ibm.com"], "sections": [{"heading": "1 Introduction", "text": "The Ubuntu Dialogue Corpus is the largest freely available multi-turn dialog body [1] built from the Ubuntu Chatlogs 2 - a collection of protocols from Ubuntu-related chat rooms on the Freenode IRC network. Although several users can speak in the chat room at the same time, the protocols have been pre-processed using heuristics to create two-person conversations, resulting in nearly one million two-person conversations in which a user seeks help with his Ubuntu-related problems (the average length of a dialogue is 8 revolutions, with a minimum of 3 revolutions).Due to its size, the corpus is well suited for exploring deep learning techniques in the context of dialog systems. In this paper, we present our preliminary research and experiments with this corpus and report on the state of the art."}, {"heading": "2 Data", "text": "In this section, we briefly describe the data and evaluation metrics used in [1]. Firstly, all the data collected has been pre-processed by replacing designated units with appropriate tags (name, location, organization, url, path), which corresponds to the presence of [2] (note that the IT helpdesk record used there is not publicly available); secondly, this data is further processed to generate tuples of (context, response, flag); the flag is a Boolean variable indicating whether the response is correct or incorrect.1http: / / cs.mcgill.ca / \u02dc jpineau / datasets / ubuntu-corpus-1.0 / 2http: / / irclogs.ubuntu.com / ar Xiv: 151 0.03 753v 1 [cs.C L] October 13, 2In order to form the training set, every utterance (starting with the third) is considered a potential response during the previous utterances."}, {"heading": "2.1 Evaluation Metric", "text": "Randomly selected 2% of the conversations are used to create a test set. The proposed task is to select the best answer candidates. The system is presented with n answer candidates and asked for a ranking. To vary the difficulty of the task (and to fix that some of the candidates marked as incorrect may well be correct), the system's ranking is considered correct if the correct answer is among the first k candidates (Recall @ k). The starting lines are given with (n, k) of (2, 1), (10, 1), (10, 2) and (10, 5)."}, {"heading": "3 Approaches", "text": "This task can of course be formulated as a ranking problem, which is often tackled by three techniques [3]: (i) pointwise; (ii) in pairs and (iii) listwise ranking. While pairwise and listwise approach are empirically superior to the pointwise approach, our preliminary experiments use pointwise ranking. Note that pointwise method was also used in the original baselines [1]."}, {"heading": "3.1 Pointwise Ranking", "text": "In the senseless ranking, only the context and the answer are used directly to calculate the probability of the pair. All pairs are then sorted according to their probabilities. We refer to the function that returns the probability of the pair as g (context, answer). In our settings, the function g is represented by a neural network (learned from the training data)."}, {"heading": "4 Previous Work", "text": "Among the useful architectures described in [1] were (i) TF-IDF, (ii) RNN, and (iii) LSTM. These models are briefly described in this section."}, {"heading": "4.1 TF-IDF", "text": "The motivation for this is that the correct answer tends to share more words with the context than the wrong answers. First, the TF-IDF vectors are calculated for the context and each of the candidate answers. Second, the cosinal similarity between the context vector and each answer vector is used to classify the answers. Note that there is no learning, so the training set is simply ignored. tfidfcontext (w) = tf (w, context) \u00b7 idf (w, D) (1) tfidfdocument (w) = tf (w, document) \u00b7 idf (w, D) (2) g (context, response) = tfidfcontext \u00b7 tfidfcontext (3) tfidfcontext and tfidfcontext (tfidfcontext) are the resulting TF-IDF vectors for context and response respectively. D stands for the corpus and w is a word. Thus, the dimension of the resulting vectors corresponds to the word size."}, {"heading": "4.2 Neural Network Embeddings", "text": "These embeddings, called c and r, are then multiplied by a matrix M and the result is fed into the sigmoid function to determine the answer.c = f (context) (4) r = f (response) (5) g (context, response) = \u03c3 (c > Herr + b) (6) c and r are the resulting embeddings of the context and response that are calculated using a neural network.We present several different architectures to calculate these embeddings. Figure 1 illustrates the approach. Note that matrixM, bias b and parameters of function f (which is a neural network) are all learned from the training data.One can consider this approach as a predictive one - given the context, let's say the embedding of the response as r = c > M and measure the similarity of the predicted response r \u2032 with the actual response r based on the product (STA) or vice versa."}, {"heading": "5 Our Architectures", "text": "We implemented three different architectures (i) CNN [5] (ii) LSTM and (iii) Bi-Directional [6] LSTM. We also report on the interplay of our models. All of our architectures have the same design in which the words from the input sequence (context or response) are projected into the embedding vectors of the words. So, if the input sequence consists of 42 words, we project these words into a matrix E that has a dimension of e \u00d7 42, where e is the dimension of the embedding of the word."}, {"heading": "5.1 CNN", "text": "At the core of the CNN model, the nested filters are applied sequentially via the input sequence; the width of the filters may vary, and in the NLP it is typically between 1 and 5 (the filters can be considered a form of n-gram here), followed by a max-pooling layer to obtain a fixed-length input. In our architecture, the output of the max-pooling operation is the context / response embedding, so the resulting embedding has a dimension corresponding to the number of filters. Figure 2a shows this architecture with two filters. 5.2 LSTMLong short-term memory (LSTM) is a recursive neural network (RNN) designed to address the vanishing gradient problem of vanilla RNN [4]."}, {"heading": "5.3 Bi-Directional LSTM", "text": "Although the LSTM is tailored to keep context across large sequences, it can be empirically problematic for the network to grasp the meaning of the entire sequence as it grows longer. If the important parts of the sequence are found at the beginning of a long sequence, it may be difficult for the LSTM to obtain a powerful embedding. We have decided to experiment with bi-LSTMs to see if this is the case in our settings. Bi-directional [6] LSTMSs feed the sequence into two recurring networks - one reads the sequence as it is, the second reads the sequence from end to beginning. To avoid cycles, only the outputs of the recurring networks (not the state-to-state connections) lead to the same units in the next layers. Figure 2c illustrates this architecture."}, {"heading": "6 Experiments", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "6.1 Method", "text": "All our models have been implemented with Theano [9] and Blocks [10]. For the training, we use ADAM learning rules [11] and binary negative log probabilities as training targets. We finish the training as soon as Recall @ 1 begins, the dataset on 3The dataset in binary format is available at http: / / cs.mcgill.ca / \u02dc jpineau / datasets / ubuntu-corpus-1.0 / ubuntu _ blobs.tgz [accessed September 25, 2015] a validation set. The experiments were performed on Nvidia K40 GPUs. The best meta parameters were found by simple grid search. In all architectures, we tried to learn both parameters: (i) separate parameters for network coding and (ii) common learning parameters for both networks, which we determined with the same accuracy for each model N11."}, {"heading": "6.2 Results", "text": "Table 1 shows the performance of the models with the best meta-parameters in each category. Performance was evaluated after each training epoch. Most models achieved the best validation data after a single training epoch. However, the best recall metrics were usually recorded in the second training epoch."}, {"heading": "6.3 Discussion", "text": "Interestingly, LSTMs and Bi-LSTMs achieve almost the same accuracy. We assume that: (i) statements that appear at the beginning of the context are less important than the later statements, or (ii) LSTMs successfully capture all important parts of the sequence. If we examine the accuracy of individual models, we see that recurring models outperform CNNs. However, CNNs have been found to significantly improve performance for the ensemble. An ensemble without the 10 CNNs had Recall @ 1 accuracy of only 66.8 compared to 68.3 for the larger ensemble. This shows that CNNs have learned representations that complement the recurring models. We believe that our results are important because they can be used as baseline for more complex models (see Future Work)."}, {"heading": "6.4 Varying Train Data Size", "text": "We have also experimented with different training data sizes to see how this affects the resulting models. We trained all networks on a training data size of 100,000 to 1,000,000 examples; the graph in Figure 3 shows the recall @ 1 for all three models (on which test data was reported), with two main observations: (i) CNNs outperform recurring models when the training data set is small; we believe this is largely due to the \"max\" operation performed on the characteristic maps; due to the simplicity of this process, the model does not match the data and generalizes better when learned on small training data sets; on the other hand, the simplicity of the process does not allow the model to properly handle more complicated dependencies (such as the order in which the n-grams appear in the text), which means recurring models get better results when there is enough data; (ii) the recurring models have not reached their peak, which indicates that the recurring models would still improve the accuracy of the training data."}, {"heading": "7 Future Work", "text": "It is an attractive idea to equip the system with external sources of information (e.g. user manual or manual pages) to help the system select the right answer. To successfully apply this paradigm in reinforcement learning, see [17]. An alternative direction for future research could be to expand the model with attention [18] via sentences in the context of dialogue, which would allow the model to explain which facts were most important in the context of its prediction. Therefore, the prediction could be better interpreted by a human. Additional accuracy improvements could also be achieved through various text pre-processing pipelines. For example, in the current dataset, all named units were replaced by generic tags, which could potentially impair performance."}, {"heading": "8 Conclusion", "text": "In this work, we reached a new state of the art regarding the next problem, recently introduced in [1]. The best functioning system is an interaction of several different neural networks. In the future, we plan to use our system as a basis for more complicated models that go beyond the standard paradigm of neural networks."}], "references": [], "referenceMentions": [], "year": 2017, "abstractText": "This paper presents results of our experiments using the Ubuntu Dialog Corpus \u2013<lb>the largest publicly available multi-turn dialog corpus. First, we use an in-house<lb>implementation of previously reported models to do an independent evaluation<lb>using the same data. Second, we evaluate the performances of various LSTMs,<lb>Bi-LSTMs and CNNs on the dataset. Third, we create an ensemble by averaging<lb>predictions of multiple models. The ensemble further improves the performance<lb>and it achieves a state-of-the-art result for this dataset. Finally, we discuss our<lb>future plans using this corpus.", "creator": "LaTeX with hyperref package"}}}