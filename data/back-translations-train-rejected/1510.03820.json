{"id": "1510.03820", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "13-Oct-2015", "title": "A Sensitivity Analysis of (and Practitioners' Guide to) Convolutional Neural Networks for Sentence Classification", "abstract": "Convolutional Neural Networks (CNNs) have recently achieved remarkably strong performance on sentence classification tasks (Kim, 2014; Kalchbrenner et al.,2014; Wang et al., 2015). However, these models require practitioners to specify the exact model architecture and accompanying hyper-parameters, e.g., the choice of filter region size, regularization parameters, and so on. It is currently unknown how sensitive model performance is to changes in these configurations for the task of sentence classification. We thus conduct an empirical sensitivity analysis of one-layer CNNs to explore the effect of each part of the architecture on the performance; our aim is to assess the robustness of the model and to distinguish between important and comparatively inconsequential design decisions for sentence classification. We focus on one-layer CNNs (to the exclusion of more complex models) due to their comparative simplicity and strong empirical performance (Kim, 2014). We derive practical advice from our extensive empirical results for those interested in getting the most out of CNNs for sentence classification.", "histories": [["v1", "Tue, 13 Oct 2015 19:00:57 GMT  (141kb,D)", "http://arxiv.org/abs/1510.03820v1", null], ["v2", "Mon, 19 Oct 2015 19:26:44 GMT  (142kb,D)", "http://arxiv.org/abs/1510.03820v2", "Corrected typos and add some more references"], ["v3", "Sat, 20 Feb 2016 07:01:52 GMT  (152kb,D)", "http://arxiv.org/abs/1510.03820v3", null], ["v4", "Wed, 6 Apr 2016 23:20:27 GMT  (148kb,D)", "http://arxiv.org/abs/1510.03820v4", null]], "reviews": [], "SUBJECTS": "cs.CL cs.LG cs.NE", "authors": ["ye zhang", "byron wallace"], "accepted": false, "id": "1510.03820"}, "pdf": {"name": "1510.03820.pdf", "metadata": {"source": "CRF", "title": "A Sensitivity Analysis of (and Practitioners\u2019 Guide to) Convolutional Neural Networks for Sentence Classification", "authors": ["Ye Zhang", "Byron C. Wallace"], "emails": ["yezhang@utexas.edu", "byron.wallace@utexas.edu"], "sections": [{"heading": "1 Introduction", "text": "In fact, most of them are able to survive on their own."}, {"heading": "2 Background and Preliminaries", "text": "They have been particularly successful (and popular) in image and language processing tasks. Recently, however, such methods have begun to overtake traditional sparse, linear models for natural language processing tasks (NLP) and collectively embed such \"internal\" representations in models for symbolic classification (Collobert and Weston, 2008; Collobert et al., 2011) or word sequences (Kalchbrenner et al., 2014; Socher et al., 2014). In (Kalchbrenner et al., 2014), the authors are con-Theano package, based on an NVIDIA K20 GPU. Structures a CNN architecture with multiple evolution layers. Their model used dynamic k-max architecture. Their model has defined their results in terms of positioning. (and popular) They have been particularly successful (and popular) for image and language processing tasks. Lately, such methods have begun to induce traditional sparse, linear models for natural language processing tasks (NLP), al. Collobert et al has concentrated on these representations (in 2008; in 2008, Nolobert has largely overtaken internal space)."}, {"heading": "2.1 CNN Architecture", "text": "In fact, most people are able to decide for themselves what they want and what they want."}, {"heading": "3 Datasets", "text": "We use the same seven datasets as in (Kim, 2014), which are briefly summarized as follows: \u2022 MR: Sentence Polarity Dataset from (Pang and Lee, 2005). \u2022 SST-1: Stanford Sentiment Treebank (Socher et al.,). Note that in order to make input representations consistent across tasks, we train and test only sentences, in contrast to (Kim, 2014), in which the authors trained models on both phrases and sentences. \u2022 SST-2: Derived from SST-1, but separated into only two classes. We train and test again only on sentences, without phrases. \u2022 Subj: Subjectivity dataset from (Pang and Lee, 2005). \u2022 TREC: Question Classification dataset from (Li and Roth, 2002). \u2022 CR: Customer Rating dataset (Hu and Liu, 2004). \u2022 MPQA: Opinion Polarity Dataset (How long we show the 2004 dataset)."}, {"heading": "4 Performance of Baseline Models", "text": "To provide a reference point for the CNN results, we will first report on the performance achieved with sparsely regulated linear models for text classification: specifically with the Support Vector Machine (SVM) and logistic regression. We used unigram and bigram functions, retaining only the most common 30k n-grams for all data sets. We also wanted to examine the relative gains achieved by directly embedding information by sea into these models. To this end, we expanded this representation to include averaged word vectors (from word2vec or GloVe) calculated from the words that make up the set. We will then use an RBF kernel SVM as a classifier working in this dense functional space. We will also experiment with the combination of unigram, bigram and word2vec as a feature of the set and use linear SVM as a classification parameter."}, {"heading": "5 Sensitivity Analysis of CNNs", "text": "We will now report on the results of our main analysis, which aims to question the sensitivity of CNNs to sentence classification depending on specific architecture and hyperparameter settings. To this end, we will start from a baseline configuration (described below) that has been shown to be good in previous work (Kim, 2014), and then investigate the effects of changing components of this baseline configuration while keeping other settings constant. We have conducted experiments with both \"static\" and \"non-static\" word vectors; in the first case, word vectors are not updated during the inference, while in the second case the vectors are \"tuned\" to the task at hand. Non-static configuration has consistently outperformed the static variant. Therefore, in this paper we report only non-static results, although we provide results for the static configuration in the appendix."}, {"heading": "5.1 Baseline Configuration", "text": "We look at the performance of a basic model configuration of CNN. Specifically, we start with the architectural decisions and hyperparameters used in previous work (Kim, 2014). To contextualize the variance in performance due to different architectural decisions and hyperparameter settings, it is important to assess the variance based on the parameter method. Unfortunately, most previous work has reported no such variance, despite a very stochastic inference method. This variance is due to the stochastic gradient estimation (SGD), random dropouts, and random parameter initiation. We show that the performance over 10x cross-validation shows a relatively high variance over repeated runs. We first use the original parameter settings that we have presented in Table 3 and we replicate the individual data sets in which each replication takes place."}, {"heading": "5.2 Effect of input word vectors", "text": "One nice feature of sentence classification models that start with distributed representations of words as inputs is the flexibility that architecture offers to exchange in various pre-formed word vectors. Therefore, we first examine the sensitivity of CNNs to the sentence classification in terms of the input representations used. In particular, we replace Google word2vec with GloVe representations (pre-trained on 840 billion tokens of web data from Common Crawl (Pennington et al., 2014). We consider all other settings to be the same as in the original configuration. We report on the results in Table 5. (Note that we also augmented results for SVM with average GloVectors in Table 2). As a potentially simple means of realizing the best performance in all datasets, we also consider an approach that collectively relies on these two pre-formed representations."}, {"heading": "5.3 Effect of filter region size", "text": "In fact, the fact is that most of them will be able to move, to move, without being able to achieve their goals."}, {"heading": "5.4 Effect of number of feature maps for each filter region size", "text": "We study the effects of the number of characteristic maps for each filter region size. Again, we maintain the number of characteristic maps for each filter region size, with three filter region sizes: 3, 4, and 5, and only the number of characteristic maps for each of these region sizes is changed. Results for non-static CNN are shown in Table 11. The change in accuracy over baseline 100 is shown in Figure 4. You can see from the \"best\" number of characteristic maps for each filter region size that it depends on the dataset. However, as a practical observation, increasing the number of maps over 600 yields very marginal yields at best and often causes damage (probably due to overhauling). Another point to note is that it takes longer to train the model when the number of characteristic maps is increased."}, {"heading": "5.5 Effect of activation function", "text": "We study the effect of seven different activation functions in the folding layer, including: ReLU (according to the original setting), hyperbolic tangent (Tanh), sigmoid function (Maas et al., 2013), SoftPlus function (Dugas et al., 2001), cube function (Chen and Manning, 2014), and tanh cube function (Pei et al., 2015). We use \"Ides\" to denote the identity function, which means not to use an activation function. The effect of different activation functions in the non-static CNN is reported in Table 12, and the change in accuracy compared to the baseline \"ReLU\" is shown in Figure 5. From the figure, we can see that for 6 out of 7 datasets the best activation function is one of Ides, ReLU, and tanh. Softplus and sigmoid functions surpass these only in one dataset (MPQA). Practically, therefore, we propose to experiment with each of the Ides, ReLU, and tanh."}, {"heading": "5.6 Effect of pooling strategy", "text": "In fact, most people are able to decide for themselves what they want and what they want."}, {"heading": "5.7 Effect of regularization", "text": "We are experimenting with a variation in the failure rate from 0.0 to 0.9, while setting the l2 standard constraint to 3, according to the baseline configuration. Results for non-static CNN are shown in Table 17. We also report on the accuracy achieved by removing both the failure rate and the l2 standard constraint (i.e., if no regulation is performed); this is declared as \"None.\" The change in accuracy from baseline 0.5 is presented separately in Figure 7.We are considering the effect of the l2 standard on the weight vectors parameterizing the Softmax function. Let us remember that the l2 standard of a weight vector is scaled linearly to a constraint c when it exceeds this threshold, so c implies stronger regulation."}, {"heading": "6 Conclusions", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "6.1 Summary of Main Empirical Findings", "text": "From our experimental analysis, we draw several conclusions that we hope will guide future work and be useful to researchers who use CNNs for sentence classification. \u2022 Previous work has tended to report only the intermediate performance of data sets achieved by models, overlooking the variance purely due to the stochastic inference method used. This can be considerable: keeping everything constant (including the folds) so that the variance is due solely to the stochastic inference method, we find that the mean performance (calculated by 10-fold cross-validation) has a range of up to 1.5 points (Table 4). More replication should be done in future work, and ranges / variances should be reported to avoid potentials \u2022 Surprisingly (from our point of view) the regulation - i.e. exposure and L2 limitations on parameter weights - seems to have little impact on model performance on some data sets as opposed to having a more representative task for each set (that we can assign a large number of words)."}, {"heading": "6.2 Advice to practitioners", "text": "Based on our empirical results, we give the following guidance on CNN architecture and hyperparameters for practitioners trying to apply CNNs to a new record classification task. \u2022 Let's look at the basic configuration described in Table 3, and let's use the non-static word2vec or GloVe instead of a single hot vector CNN. \u2022 Search line by line for the size of each filter region to find the \"best\" single region size. A reasonable range could be 2 x 10. However, for records with very long records such as CR, it may be worth exploring larger filter regions. Once this \"best\" region size is identified, it may be worth combining several filters that are close to this single best possible size, as empirically several \"good\" region sizes are always executed using the only best region size. \u2022 Let's change the number of function cards for each filter region size from 50 to 600. Note that if there is an increase in the number of functionality it is possible to trade for another one."}, {"heading": "7 Appendix", "text": "Performance of Logistic Regression In Table 19, we also report on results obtained using logistic regression (regulated by an l2 standard penalty on the coefficients, weighted proportional to a hyperparameter that we reset) using the same feature sets. Effect of the single filter region size The result of the single filter region size with static CNN is shown in Table 20.Effect of the number of feature maps The effect of the number of feature maps for each filter region size with static CNN is shown in Table 21.In Table 22, we show the average runtime of a 10-fold CV sequentially on TREC data sets as we increase the feature maps. Effect of the activation function. Effect of the activation with static CNN is shown in Table 23.Effect of the pooling strategy with static CNN is shown in Table 24 and Table 25.Effect of the drop-out rate The effect of the static CNN function is shown in Table 26.Effect of the l2 standard restriction of the static CNN weight to 27,2."}], "references": [{"title": "A neural probabilistic language model", "author": ["Bengio et al.2003] Yoshua Bengio", "R\u00e9jean Ducharme", "Pascal Vincent", "Christian Janvin"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "Bengio et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Bengio et al\\.", "year": 2003}, {"title": "Learning deep architectures for ai. Foundations and trends in Machine Learning, 2(1):1\u2013127", "author": ["Yoshua Bengio"], "venue": null, "citeRegEx": "Bengio.,? \\Q2009\\E", "shortCiteRegEx": "Bengio.", "year": 2009}, {"title": "Learning mid-level features for recognition", "author": ["Francis Bach", "Yann LeCun", "Jean Ponce"], "venue": "In Computer Vision and Pattern Recognition (CVPR),", "citeRegEx": "Boureau et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Boureau et al\\.", "year": 2010}, {"title": "A theoretical analysis of feature pooling in visual recognition", "author": ["Jean Ponce", "Yann LeCun"], "venue": "In Proceedings of the 27th International Conference on Machine Learning", "citeRegEx": "Boureau et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Boureau et al\\.", "year": 2010}, {"title": "Ask the locals: multi-way local pooling for image recognition", "author": ["Nicolas Le Roux", "Francis Bach", "Jean Ponce", "Yann LeCun"], "venue": "In Computer Vision (ICCV),", "citeRegEx": "Boureau et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Boureau et al\\.", "year": 2011}, {"title": "The effects of hyperparameters on sgd training of neural networks. arXiv preprint arXiv:1508.02788", "author": ["Thomas M Breuel"], "venue": null, "citeRegEx": "Breuel.,? \\Q2015\\E", "shortCiteRegEx": "Breuel.", "year": 2015}, {"title": "A fast and accurate dependency parser using neural networks", "author": ["Chen", "Manning2014] Danqi Chen", "Christopher D Manning"], "venue": "In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP),", "citeRegEx": "Chen et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Chen et al\\.", "year": 2014}, {"title": "An analysis of single-layer networks in unsupervised feature learning", "author": ["Coates et al.2011] Adam Coates", "Andrew Y Ng", "Honglak Lee"], "venue": "In International conference on artificial intelligence and statistics,", "citeRegEx": "Coates et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Coates et al\\.", "year": 2011}, {"title": "A unified architecture for natural language processing: Deep neural networks with multitask learning", "author": ["Collobert", "Weston2008] Ronan Collobert", "Jason Weston"], "venue": "In Proceedings of the 25th international conference on Machine learning,", "citeRegEx": "Collobert et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Collobert et al\\.", "year": 2008}, {"title": "Natural language processing (almost) from scratch", "author": ["Jason Weston", "L\u00e9on Bottou", "Michael Karlen", "Koray Kavukcuoglu", "Pavel Kuksa"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "Collobert et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Collobert et al\\.", "year": 2011}, {"title": "Incorporating second-order functional knowledge for better option pricing", "author": ["Dugas et al.2001] Charles Dugas", "Yoshua Bengio", "Fran\u00e7ois B\u00e9lisle", "Claude Nadeau", "Ren\u00e9 Garcia"], "venue": "Advances in Neural Information Processing Systems,", "citeRegEx": "Dugas et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Dugas et al\\.", "year": 2001}, {"title": "A primer on neural network models for natural language processing", "author": ["Yoav Goldberg"], "venue": null, "citeRegEx": "Goldberg.,? \\Q2015\\E", "shortCiteRegEx": "Goldberg.", "year": 2015}, {"title": "Improving neural networks by preventing co-adaptation of feature detectors. arXiv preprint arXiv:1207.0580", "author": ["Nitish Srivastava", "Alex Krizhevsky", "Ilya Sutskever", "Ruslan R Salakhutdinov"], "venue": null, "citeRegEx": "Hinton et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Hinton et al\\.", "year": 2012}, {"title": "Mining and summarizing customer reviews", "author": ["Hu", "Liu2004] Minqing Hu", "Bing Liu"], "venue": "In Proceedings of the tenth ACM SIGKDD international conference on Knowledge discovery and data mining,", "citeRegEx": "Hu et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Hu et al\\.", "year": 2004}, {"title": "Text categorization with support vector machines: Learning with many relevant", "author": ["Thorsten Joachims"], "venue": null, "citeRegEx": "Joachims.,? \\Q1998\\E", "shortCiteRegEx": "Joachims.", "year": 1998}, {"title": "Effective use of word order for text categorization with convolutional neural networks. arXiv preprint arXiv:1412.1058", "author": ["Johnson", "Zhang2014] Rie Johnson", "Tong Zhang"], "venue": null, "citeRegEx": "Johnson et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Johnson et al\\.", "year": 2014}, {"title": "A convolutional neural network for modelling sentences", "author": ["Edward Grefenstette", "Phil Blunsom"], "venue": "In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics", "citeRegEx": "Kalchbrenner et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Kalchbrenner et al\\.", "year": 2014}, {"title": "Convolutional neural networks for sentence classification. arXiv preprint arXiv:1408.5882", "author": ["Yoon Kim"], "venue": null, "citeRegEx": "Kim.,? \\Q2014\\E", "shortCiteRegEx": "Kim.", "year": 2014}, {"title": "Learning question classifiers", "author": ["Li", "Roth2002] Xin Li", "Dan Roth"], "venue": "In Proceedings of the 19th international conference on Computational linguistics-Volume", "citeRegEx": "Li et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Li et al\\.", "year": 2002}, {"title": "Rectifier nonlinearities improve neural network acoustic models", "author": ["Maas et al.2013] Andrew L Maas", "Awni Y Hannun", "Andrew Y Ng"], "venue": "In Proc. ICML,", "citeRegEx": "Maas et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Maas et al\\.", "year": 2013}, {"title": "Distributed representations of words and phrases and their compositionality", "author": ["Ilya Sutskever", "Kai Chen", "Greg S Corrado", "Jeff Dean"], "venue": "In Advances in neural information processing systems,", "citeRegEx": "Mikolov et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Seeing stars: Exploiting class relationships for sentiment categorization with respect to rating scales", "author": ["Pang", "Lee2005] Bo Pang", "Lillian Lee"], "venue": "In Proceedings of the ACL", "citeRegEx": "Pang et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Pang et al\\.", "year": 2005}, {"title": "An effective neural network model for graph-based dependency parsing", "author": ["Pei et al.2015] Wenzhe Pei", "Tao Ge", "Baobao Chang"], "venue": "In Proc. of ACL", "citeRegEx": "Pei et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Pei et al\\.", "year": 2015}, {"title": "Glove: Global vectors for word representation", "author": ["Richard Socher", "Christopher D Manning"], "venue": "Proceedings of the Empiricial Methods in Natural Language Processing", "citeRegEx": "Pennington et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Pennington et al\\.", "year": 2014}, {"title": "Dropout: A simple way to prevent neural networks from overfitting", "author": ["Geoffrey Hinton", "Alex Krizhevsky", "Ilya Sutskever", "Ruslan Salakhutdinov"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "Srivastava et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Srivastava et al\\.", "year": 2014}, {"title": "Semantic clustering and convolutional neural network for short text categorization", "author": ["Wang et al.2015] Peng Wang", "Jiaming Xu", "Bo Xu", "Chenglin Liu", "Heng Zhang", "Fangyuan Wang", "Hongwei Hao"], "venue": "In Proceedings of the 53rd Annual Meet-", "citeRegEx": "Wang et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2015}, {"title": "Annotating expressions of opinions and emotions in language. Language resources and evaluation, 39(2-3):165\u2013210", "author": ["Wiebe et al.2005] Janyce Wiebe", "Theresa Wilson", "Claire Cardie"], "venue": null, "citeRegEx": "Wiebe et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Wiebe et al\\.", "year": 2005}, {"title": "Adadelta: An adaptive learning rate method", "author": ["Matthew D Zeiler"], "venue": "arXiv preprint arXiv:1212.5701", "citeRegEx": "Zeiler.,? \\Q2012\\E", "shortCiteRegEx": "Zeiler.", "year": 2012}], "referenceMentions": [{"referenceID": 17, "context": "performance on sentence classification tasks (Kim, 2014; Kalchbrenner et al., 2014; Wang et al., 2015).", "startOffset": 45, "endOffset": 102}, {"referenceID": 16, "context": "performance on sentence classification tasks (Kim, 2014; Kalchbrenner et al., 2014; Wang et al., 2015).", "startOffset": 45, "endOffset": 102}, {"referenceID": 25, "context": "performance on sentence classification tasks (Kim, 2014; Kalchbrenner et al., 2014; Wang et al., 2015).", "startOffset": 45, "endOffset": 102}, {"referenceID": 17, "context": "We focus on one-layer CNNs (to the exclusion of more complex models) due to their comparative simplicity and strong empirical performance (Kim, 2014).", "startOffset": 138, "endOffset": 149}, {"referenceID": 17, "context": "Convolutional Neural Networks (CNNs) have recently been shown to perform quite well for this task (Kim, 2014; Kalchbrenner et al., 2014; Wang et al., 2015; Goldberg, 2015).", "startOffset": 98, "endOffset": 171}, {"referenceID": 16, "context": "Convolutional Neural Networks (CNNs) have recently been shown to perform quite well for this task (Kim, 2014; Kalchbrenner et al., 2014; Wang et al., 2015; Goldberg, 2015).", "startOffset": 98, "endOffset": 171}, {"referenceID": 25, "context": "Convolutional Neural Networks (CNNs) have recently been shown to perform quite well for this task (Kim, 2014; Kalchbrenner et al., 2014; Wang et al., 2015; Goldberg, 2015).", "startOffset": 98, "endOffset": 171}, {"referenceID": 11, "context": "Convolutional Neural Networks (CNNs) have recently been shown to perform quite well for this task (Kim, 2014; Kalchbrenner et al., 2014; Wang et al., 2015; Goldberg, 2015).", "startOffset": 98, "endOffset": 171}, {"referenceID": 11, "context": ", 2015; Goldberg, 2015). Such models capitalize on distributed representations of words by first converting the tokens comprising each instance into a vector, forming a matrix to be used as input to the CNN (Figure 1). Empirical results have been impressive. And the models need not be complex to realize strong results: e.g., Kim (2014) proposed a straight forward one-layer CNN architecture that achieved consistent state of the art (or comparable) results across several tasks.", "startOffset": 8, "endOffset": 338}, {"referenceID": 14, "context": "contrast to the linear models widely used for text classification, such as regularized logistic regression and linear-kernel Support Vector Machines (SVMs) (Joachims, 1998).", "startOffset": 156, "endOffset": 172}, {"referenceID": 17, "context": ", ) using a similar configuration to that described in (Kim, 2014).", "startOffset": 55, "endOffset": 66}, {"referenceID": 5, "context": "(2011) and Breuel (Breuel, 2015), which investigated factors that effect performance of unsupervised feature learning and the effects of Stochastic Gradient Descent", "startOffset": 18, "endOffset": 32}, {"referenceID": 6, "context": "tion from previous empirical analyses of neural models due to Coates et al. (2011) and Breuel (Breuel, 2015), which investigated factors that effect performance of unsupervised feature learning and the effects of Stochastic Gradient Descent", "startOffset": 62, "endOffset": 83}, {"referenceID": 1, "context": "Deep learning methods are now well established in machine learning (LeCun et al., 2015; Bengio, 2009).", "startOffset": 67, "endOffset": 101}, {"referenceID": 11, "context": "However, recently such methods have begun to overtake traditional sparse, linear models for natural language processing (NLP) tasks (Goldberg, 2015).", "startOffset": 132, "endOffset": 148}, {"referenceID": 0, "context": "Much of the interest in this space has been focused on inducing distributed representations of words (Bengio et al., 2003; Mikolov et al., 2013) and jointly embedding such \u2018internal\u2019 representations into models for token classification (Collobert and Weston, 2008; Collobert et al.", "startOffset": 101, "endOffset": 144}, {"referenceID": 20, "context": "Much of the interest in this space has been focused on inducing distributed representations of words (Bengio et al., 2003; Mikolov et al., 2013) and jointly embedding such \u2018internal\u2019 representations into models for token classification (Collobert and Weston, 2008; Collobert et al.", "startOffset": 101, "endOffset": 144}, {"referenceID": 9, "context": ", 2013) and jointly embedding such \u2018internal\u2019 representations into models for token classification (Collobert and Weston, 2008; Collobert et al., 2011) or sentence modeling (Kalchbrenner et al.", "startOffset": 99, "endOffset": 151}, {"referenceID": 16, "context": "In (Kalchbrenner et al., 2014), the authors con-", "startOffset": 3, "endOffset": 30}, {"referenceID": 16, "context": "Kim (2014) defined a much simpler architecture that achieves comparable results to (Kalchbrenner et al., 2014) on the same datasets.", "startOffset": 83, "endOffset": 110}, {"referenceID": 20, "context": "This model also represents each word as a dense, low dimensional vector (Mikolov et al., 2013).", "startOffset": 72, "endOffset": 94}, {"referenceID": 20, "context": "For example, these might be outputs from the word2vec (Mikolov et al., 2013) or GloVe (Pennington et al.", "startOffset": 54, "endOffset": 76}, {"referenceID": 23, "context": ", 2013) or GloVe (Pennington et al., 2014) models.", "startOffset": 17, "endOffset": 42}, {"referenceID": 17, "context": "We use the same zero-padding strategy as in (Kim, 2014).", "startOffset": 44, "endOffset": 55}, {"referenceID": 12, "context": "At this level, one may opt to apply a \u2018dropout strategy\u2019 (Hinton et al., 2012) as means of regularization.", "startOffset": 57, "endOffset": 78}, {"referenceID": 17, "context": "We use the same seven datasets as in (Kim, 2014), summarized briefly as follows:", "startOffset": 37, "endOffset": 48}, {"referenceID": 17, "context": "This is in contrast to (Kim, 2014), wherein the authors trained models on both phrases and sentences.", "startOffset": 23, "endOffset": 34}, {"referenceID": 26, "context": "\u2022 MPQA: Opinion polarity dataset (Wiebe et al., 2005)", "startOffset": 33, "endOffset": 53}, {"referenceID": 17, "context": "For more details on these datasets, please refer to (Kim, 2014).", "startOffset": 52, "endOffset": 63}, {"referenceID": 17, "context": "3 For consistency, we use the same pre-processing steps for the data as described in previous work (Kim, 2014).", "startOffset": 99, "endOffset": 110}, {"referenceID": 17, "context": "To this end, we take as our starting point a baseline configuration (described below) which has been shown to work well in previous work (Kim, 2014).", "startOffset": 137, "endOffset": 148}, {"referenceID": 17, "context": "Specifically, we start with the architectural decisions and hyperparameters used in previous work (Kim, 2014).", "startOffset": 98, "endOffset": 109}, {"referenceID": 19, "context": "\u2018ReLU\u2019 in Table 3 refers to rectified linear unit(Maas et al., 2013), which", "startOffset": 49, "endOffset": 68}, {"referenceID": 17, "context": "We run 10-fold CV for all datasets, which is different from (Kim, 2014) effect of each component of CNN on the performance, we don\u2019t care much about the absolute accuracy and won\u2019t compare the results we got with the ones in previous works.", "startOffset": 60, "endOffset": 71}, {"referenceID": 17, "context": "For all experiments, we use the same preprocessing steps for the data as in (Kim, 2014).", "startOffset": 76, "endOffset": 87}, {"referenceID": 27, "context": "Similarly, we use the ADADELTA update rule for SGD (Zeiler, 2012), and set the minibatch size as 50 .", "startOffset": 51, "endOffset": 65}, {"referenceID": 23, "context": "we replace Google word2vec with GloVe representations (pre-trained on 840 billion tokens of web data from Common Crawl (Pennington et al., 2014)).", "startOffset": 119, "endOffset": 144}, {"referenceID": 19, "context": "gent (tanh), Sigmoid function, (Maas et al., 2013), SoftPlus function (Dugas et al.", "startOffset": 31, "endOffset": 50}, {"referenceID": 10, "context": ", 2013), SoftPlus function (Dugas et al., 2001), Cube function(Chen and Manning, 2014), and tanh cube function (Pei et al.", "startOffset": 27, "endOffset": 47}, {"referenceID": 22, "context": ", 2001), Cube function(Chen and Manning, 2014), and tanh cube function (Pei et al., 2015).", "startOffset": 71, "endOffset": 89}, {"referenceID": 16, "context": "We also consider a k-max pooling strategy similar to (Kalchbrenner et al., 2014), in which the maximum k values are extracted from the entire", "startOffset": 53, "endOffset": 80}, {"referenceID": 24, "context": "This observation is in contrast to results reported for image and speech tasks (Srivastava et al., 2014).", "startOffset": 79, "endOffset": 104}], "year": 2015, "abstractText": "Convolutional Neural Networks (CNNs) have recently achieved remarkably strong performance on sentence classification tasks (Kim, 2014; Kalchbrenner et al., 2014; Wang et al., 2015). However, these models require practitioners to specify the exact model architecture and accompanying hyper-parameters, e.g., the choice of filter region size, regularization parameters, and so on. It is currently unknown how sensitive model performance is to changes in these configurations for the task of sentence classification. We thus conduct an empirical sensitivity analysis of one-layer CNNs to explore the effect of each part of the architecture on the performance; our aim is to assess the robustness of the model and to distinguish between important and comparatively inconsequential design decisions for sentence classification. We focus on one-layer CNNs (to the exclusion of more complex models) due to their comparative simplicity and strong empirical performance (Kim, 2014). We derive practical advice from our extensive empirical results for those interested in getting the most out of CNNs for sentence classification.", "creator": "LaTeX with hyperref package"}}}