{"id": "1510.03931", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "14-Oct-2015", "title": "Structured Memory for Neural Turing Machines", "abstract": "Neural Turing Machines (NTM) contain memory component that simulates \"working memory\" in the brain to store and retrieve information to ease simple algorithms learning. So far, only linearly organized memory is proposed, and during experiments, we observed that the model does not always converge, and overfits easily when handling certain tasks. We think memory component is key to some faulty behaviors of NTM, and better organization of memory component could help fight those problems. In this paper, we propose several different structures of memory for NTM, and we proved in experiments that two of our proposed structured-memory NTMs could lead to better convergence, in term of speed and prediction accuracy on copy task and associative recall task as in (Graves et al. 2014).", "histories": [["v1", "Wed, 14 Oct 2015 00:08:17 GMT  (912kb,D)", "https://arxiv.org/abs/1510.03931v1", "4 pages, NIPS workshop requirement"], ["v2", "Tue, 20 Oct 2015 21:52:04 GMT  (912kb,D)", "http://arxiv.org/abs/1510.03931v2", "4 pages, NIPS workshop requirement"], ["v3", "Sun, 25 Oct 2015 03:12:49 GMT  (912kb,D)", "http://arxiv.org/abs/1510.03931v3", "4 pages, accepted to Reasoning, Attention, Memory (RAM) NIPS 2015 Workshop"]], "COMMENTS": "4 pages, NIPS workshop requirement", "reviews": [], "SUBJECTS": "cs.AI cs.NE", "authors": ["wei zhang", "yang yu", "bowen zhou"], "accepted": false, "id": "1510.03931"}, "pdf": {"name": "1510.03931.pdf", "metadata": {"source": "CRF", "title": "Structured Memory for Neural Turing Machines", "authors": ["Wei Zhang", "Yang Yu", "Bowen Zhou"], "emails": ["zhangwei@us.ibm.com", "yu@us.ibm.com", "zhou@us.ibm.com", "wynnzh@gmail.com"], "sections": [{"heading": "1 Introduction", "text": "Neural network storage devices have recently been introduced in Neural Turing Machines (NTMs) [2], Memory Networks [1], and Dynamic Memory Networks [3]. The purposes of these storage devices are similar, namely to simulate the \"working memory\" in the brain to store temporary information over time that is used by the Attention Module for reading and writing. In this paper, our focus is on NTMs. They are attractive because the controller can access them through blurred \"erase\" and \"write\" operations, which is well in line with Turing Machine operations. With small memory size, a nice convergence of the NTM model could often be observed, though not always. But when the memory size is large, the model struggles for convergence, and sometimes the test loss jumps dramatically over a wide range, which is a sign of overmatch, as observed in our experiments."}, {"heading": "2 Architectures", "text": "nI \"r\" i, eaMnn, \"nlrteew os,\" nlrteew os \"s.\" nlA \"D\" i \"nI nvo,\" nlrteew os, \"n\" iwr os, \"n os,\" nlrteew, \"n\" nlteew, \"nlrteew,\" nlrteew os, \"a\" nlsa, \"eeiw,\" a \"eeiw,\" nn \"nlsa,\" eeiw. \"nI\" D \"nI\" nI \"n,\" nlsa, \"lsa\" lsa \"lsa\" lsa, \"eeid,\" ew, \"nw,\" nw, \"ew,\" ew, \"lw,\" lw rsa rsa, rsa rsa, \"lw,\" lw eiw, \"lw nsa,\" lw eiw, \"lw nsa,\" lw eiw, \"lw."}, {"heading": "3 Experiments", "text": "In fact, it is that we see ourselves as being able to assert ourselves, that we are able, that we are able to put ourselves at the top, and that we are able, that we are able to assert ourselves, that we are able, that we are able to assert ourselves in the world."}, {"heading": "4 Conclusion", "text": "In this paper, three new structured memory architectures for Neural Turing Machines were discussed and demonstrated that proper hierarchical organization of memory blocks could reduce overmatch and sometimes increase predictive accuracy compared to NTM."}, {"heading": "5 Future work", "text": "In the future, we would also try NTM1 NTM2 on datasets that require more complex reasoning. We tested NTM on a synthetic QA dataset proposed in [1] and observed the instability of convergence. We would try NTM1 and NTM2 on the same dataset."}], "references": [{"title": "Ask Me Anything: Dynamic Memory Networks for Natural Language Processing", "author": ["Ankit Kumar", "Ozan Irsoy", "Jonathan Su", "James Bradbury", "Robert English", "Brian Pierce", "Peter Ondruska", "Ishaan Gulrajani", "Richard Socher"], "venue": "arXiv preprint arXiv:1506.07285", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2015}, {"title": "Lecture 6.5-rmsprop: Divide the gradient by a running average of its recent magnitude.", "author": ["Tijmen Tieleman", "Geoffrey Hinton"], "venue": "COURSERA: Neural Networks for Machine Learning", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2012}, {"title": "Long short-term memory", "author": ["Sepp Hochreiter", "Jrgen Schmidhuber"], "venue": "Neural computation,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 1997}], "referenceMentions": [{"referenceID": 0, "context": "Memory components for Neural Networks has been recently introduced in Neural Turing Machines (NTMs) [2], Memory Networks [1], and Dynamic Memory Networks [3].", "startOffset": 154, "endOffset": 157}, {"referenceID": 2, "context": "Figure 1: NTM and NTM variants that use Long short-term memory [5] as controllers.", "startOffset": 63, "endOffset": 66}, {"referenceID": 1, "context": "For all the models, the training criteria is binary cross entropy, and we use RMSProp [4] for optimization, with learning rate 1E \u2212 4, momentum 0.", "startOffset": 86, "endOffset": 89}], "year": 2015, "abstractText": "Neural Turing Machines (NTM) [2] contain memory component that simulates \u201cworking memroy\u201d in the brain to store and retrieve information to ease simple algorithms learning. So far, only linearly organized memory is proposed, and during experiments, we observed that the model does not always converge, and overfits easily when handling certain tasks. We think memory component is key to some faulty behaviors of NTM, and better organization of memory component could help fight those problems. In this paper, we propose several different structures of memory for NTM, and we proved in experiments that two of our proposed structured-memory NTMs could lead to better convergence, in term of speed and prediction accuracy on copy task and associative recall task as in [2].", "creator": "LaTeX with hyperref package"}}}