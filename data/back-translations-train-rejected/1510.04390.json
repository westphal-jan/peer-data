{"id": "1510.04390", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "15-Oct-2015", "title": "Dual Principal Component Pursuit", "abstract": "We consider the problem of outlier rejection in single subspace learning. Classical approaches work directly with a low-dimensional representation of the subspace. Our approach works with a dual representation of the subspace and hence aims to find its orthogonal complement. We pose this problem as an $\\ell_1$-minimization problem on the sphere and show that, under certain conditions on the distribution of the data, any global minimizer of this non-convex problem gives a vector orthogonal to the subspace. Moreover, we show that such a vector can still be found by relaxing the non-convex problem with a sequence of linear programs. Experiments on synthetic and real data show that the proposed approach, which we call Dual Principal Component Pursuit (DPCP), outperforms state-of-the art methods, especially in the case of high-dimensional subspaces.", "histories": [["v1", "Thu, 15 Oct 2015 03:50:01 GMT  (215kb,D)", "http://arxiv.org/abs/1510.04390v1", null], ["v2", "Sun, 4 Jun 2017 21:42:05 GMT  (3261kb)", "http://arxiv.org/abs/1510.04390v2", null], ["v3", "Sat, 22 Jul 2017 16:24:27 GMT  (862kb,D)", "http://arxiv.org/abs/1510.04390v3", null]], "reviews": [], "SUBJECTS": "cs.CV cs.LG", "authors": ["manolis c tsakiris", "rene vidal"], "accepted": false, "id": "1510.04390"}, "pdf": {"name": "1510.04390.pdf", "metadata": {"source": "CRF", "title": "Dual Principal Component Pursuit", "authors": ["Manolis C. Tsakiris", "Ren\u00e9 Vidal"], "emails": ["m.tsakiris@jhu.edu", "rvidal@jhu.edu"], "sections": [{"heading": "1. Introduction", "text": "This year is the highest in the history of the country."}, {"heading": "2. Problem Formulation", "text": "First we establish our data model in Section 2.1, then we formulate our DPCP problem conceptually and computationally in Section 2.2 and 2.3 respectively."}, {"heading": "2.1. Data Model", "text": "We use a deterministic, noise-free data model in which the outliers consist of N-points X = [x1,.., xN] and RD \u00b7 N, which lie at the intersection of the unit sphere SD \u2212 1 with an unknown property V of the RD of unknown dimension d. Accordingly, the outliers consist of M arbitrary points O = [o1,..., oM] and RD \u00b7 M, which lie on SD \u2212 1. The dataset which we assume contains X = [X O] is RD \u00b7 L, where L = N + M and \u0439 is a permutation, indicating that the division of the columns of X into X and O is unknown. We also assume that the columns of X are in a general sense in the following: First, that each d-tuple of the outliers and each D-tuple of the outliers are linearly independent of each other. Second, for each {1,,."}, {"heading": "2.2. Conceptual Formulation", "text": "Note that in our data model we have not made assumptions about the dimension of V - in fact, V can be anything from a line to a (D \u2212 1) dimensional hyperplane. Ideally, we would like to divide the columns of X \u2212 into those that lie in V and those that do not. But, under such a generality, this is not a well-established problem, since X resides in every subspace that contains V, which in turn contains some elements of O. In other words, given X and without any other a-priori knowledge, it may be impossible to divide X correctly in X and O. Instead, we formulate the following well-posed problem: Problem 1 Divide the columns of X-RD \u2212 L into two groups, so that one of the groups is a subgroup of X, with maximum cardinality, in terms of the property of lying within a (D \u2212 1) dimensional hyperplane of RD, so that the column can deviate from a (D \u2212 1) dimensional hyperplane of RD."}, {"heading": "2.3. Computational Formulation", "text": "A natural approach to solving problem 1 is solvemin b | | X > b | | 0 s.t. b 6 = 0. (1) The idea behind (1) is that a hyperplane H = span (b) contains a maximum number of X columns if and only if X- > b is as sparse as possible. Since (1) is insoluble, consideration for b | | X span (b) leads to a nonsmooth and nonconvex optimization problem. (2) Note that the target in (2) is convex while the constraint in b-SD \u2212 1 is non-convex and thus leads to a nonsmooth and non-convex optimization problem. problem 2 When is every global solution b \u00b2 from (2) orthogonal to chip (X)? How can we solve efficiently (2)? In this essay we propose (2) adapting a linear sequence of programs > Spark >: 1 (nb) to a uniform unit (1)."}, {"heading": "3. Related Work", "text": "In this section we want to familiarize the reader with the state of the art of outlier detection in modern single subspace learning (Section 3.1) and give a brief overview (Section 3.2) of existing work that technically relates to the interesting problems of this work, i.e. problems (2) and (3)."}, {"heading": "3.1. Outlier Rejection in PCA", "text": "In fact, it is the case that most of us are able to hide when we are able to play by the rules. (...) Most of them are able to play by the rules. (...) Most of them are able to play by the rules. (...) Most of them are able to play by the rules. (...) Most of them are able to play by the rules. (...) Most of them are able to play by the rules. (...) Most of them are able to play by the rules. (...) Most of them are able to play by the rules. (...) Most of them are able to play by the rules. (...) Most of them are not able to play by the rules. (...)"}, {"heading": "3.2. Connections with Compressed Sensing and Dictionary Learning", "text": "The problems of formmin b | | 0 s.t. b = 0, (6) and variants of its relaxation have arisen several times in literature and in different contexts, but are much less understood than the today classic sparse [1] and cosparous [15] problems of formmin x | x | 0 s.t. Ax = b (7) min x | | 0 s.t. Ax = b, (8). The main source of difficulty is that, in contrast to (8), it is a hard problem to obtain narrow convex relaxations of (6). (9) This is still a non-convex problem, where (6) was considered in the context of blind source separation [24], where it was suggested to deal with the problem b | 1 s.t."}, {"heading": "4. Theoretical Analysis", "text": "In this section we will explain and discuss our most important theoretical results2 on problems (2) and (3), but before doing so we need to introduce additional notation and make some interesting connections with the field of numerical integration on the sphere (Section 4.1)."}, {"heading": "4.1. An Integration Perspective", "text": "To start with a vector b \u2212 SD \u2212 1, the one of fb = > SD \u2212 K (SD \u2212 K) (SD \u2212 K) (SD \u2212 K) (SD \u2212 K) (SD \u2212 K) (SD \u2212 K) (SD \u2212 K) (SD \u2212 K) (SD \u2212 K) (SD \u2212 K) (SD \u2212 K) (SD \u2212 K) (SD \u2212 K) (SD \u2212 K) (SD \u2212 K) (SD \u2212 K) (SD \u2212 K) (SD \u2212 K) (SD \u2212 K) (SD \u2212 K) (SD \u2212 K) (SD \u2212 K) (SD \u2212 K) (SD \u2212 K) (SK \u2212 K) (SD \u2212 K) (SK \u2212 K) (SD \u2212 K) (SD \u2212 K) (SK \u2212 K) (SD \u2212 K) (SD \u2212 K \u2212 K) (SD \u2212 K) (SD \u2212 K \u2212) (SK \u2212) (SD \u2212 K \u2212) (SD \u2212 K \u2212) (SD \u2212 K \u2212) (SD \u2212 K \u2212) (SD \u2212 K \u2212) (SD \u2212) (SD \u2212 K \u2212) (SD \u2212 K \u2212) (SD \u2212) (SD \u2212 K \u2212) (SD \u2212 K \u2212) (SD \u2212 K \u2212 K \u2212) (SD \u2212) (SD \u2212 K \u2212 K \u2212 K \u2212) (SD \u2212 K \u2212) (SD \u2212 K \u2212 K \u2212) (SD \u2212 K \u2212) (SD \u2212 K \u2212 K \u2212) (SD \u2212 K \u2212 K \u2212) (SD \u2212 K \u2212 K \u2212 K \u2212 K \u2212 K \u2212 K \u2212 K \u2212 K \u2212 K \u2212) (\u2212 K \u2212) (\u2212 K \u2212 K \u2212 K \u2212 K \u2212 K \u2212 K \u2212 K \u2212 K \u2212 K \u2212 K \u2212) (\u2212 K \u2212 K \u2212 K \u2212 K \u2212 K \u2212 K \u2212 K \u2212 K \u2212 K \u2212 K \u2212 K \u2212 K \u2212 K \u2212 K \u2212 K \u2212 K \u2212 K \u2212 K \u2212 K \u2212 K \u2212 K \u2212 K \u2212 K \u2212 K \u2212 K \u2212 K \u2212 K \u2212 K \u2212 K \u2212 SD \u2212 SD \u2212 SD \u2212 SD \u2212 SD \u2212 K \u2212 K \u2212 SD \u2212 K \u2212"}, {"heading": "4.2. The Non-Convex Problem", "text": "Before we consider the discreet, non-konvexe issue (2), it is instructive to investigate its continuous equivalent b > b = 1 M-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o"}, {"heading": "4.3. The Sequence of Convex Relaxations", "text": "In this section, we will look at the sequence of convex relaxations (3); in particular, there are two important problems that need to be addressed. Firstly, it should be noted that the relaxation of the constraint b > b = 1 in (2) with a linear constraint b > n = 1 as in (10) has already proven to be a limited theoretical guarantee [20]. Therefore, it is natural to ask whether the idea of considering a sequence of such relaxations b > n = 1, k = 0, 1,. has an intrinsic property or not, regardless of the data distribution. For example, if the data are perfectly distributed but the sequence does not yield vectors orthogonally to the resting space, then we will know that the a-priori method is limited. Fortunately, this is not the case: if the data are perfectly well distributed, i.e. if we limit our attention to the continuous analogy of (3)."}, {"heading": "5. Dual Principal Component Pursuit", "text": "So far, we have established a mechanism to obtain an element b1 of the V series, where V = > chip (X) is estimated instead: Perform the sequence of linear programs (3) to the sequence of linear programs (3). \u2212 There are two possibilities: Either V is a hyperplane of dimension D \u2212 1 or dimV < D \u2212 1. In the first case, b1 is the unique element of the scale distributions, proving that in this case the sequence of (3) actually converges. \u2212 In such a case, we can identify our subspace model with the hyperplane defined by the normal b1. Next, if dimV < D \u2212 1, we can find a second element b2 of the V series that is orthogonal to b1, and so on."}, {"heading": "6. Experiments", "text": "In this section, we will experimentally examine the proposed DPCP alg. 1. Using synthetic (subsection 6.1) and real data (subsection 6.2), we compare DPCP with the three methods SE, L21 and RANSAC discussed in section 3.1, and with the method of equivalent (11) discussed in section 3.2, which we call SVS (Sparsest Vector in a Subspace).The parameters of the methods are set to fixed values, which are chosen in such a way that the methods function well in all tested dimensions and outlier configurations. In particular, we use \u03b1SE = 100, \u03c4L21 = 100 and \u03bbL21 = 3 / (7 \u221a M); see [19] and [23] for details. With regard to DPCP, we set Tmax = 10, = 10 \u2212 6, and if not noted otherwise, we equate c to the true codimension of the subspace."}, {"heading": "6.1. Synthetic Data", "text": "To begin with, we evaluate the performance of the DPCP in the absence of noise, so that the dimensions of the room are smaller than the dimensions of the room are minimal. (1: 1: 29 and outlier percentage R: = M + N) = 0.1: 0.9. We fix the ambient dimension D = 30, sample N = 200 intruders uniformly randomly according to V-SD \u2212 1andM outliers randomly. The results are shown in Fig. 1 for 10 independent experiments. (a) shows whether the theoretical conditions of the DPCP are met or not. In verifying these conditions, we estimate the abstract quantities O, X, RO, K1, RX by Monte Carlo simulation. Whenever these conditions are met, we select b0 in a controlled manner so that we are the dimensions of noise larger than we are. We estimate the abstract quantities O, RO, K2, and K2 by Monte Carlo simulation."}, {"heading": "6.2. Real Data", "text": "In this subsection, we consider an outlier detection scenario in the PCA based on real images. The outliers are considered as all N = 64 facial images of a single person from the Extended Yale B dataset [7], while the M outliers are randomly selected from Caltech101 [6]. All images are cut to the size of 48 x 42 as in [5]. To make a fair comparison, we use SE on the raw 2016-dimensional data, while all other methods project data to the dimension D = 50. Since it is known that facial images of an individual are close to a 9-dimensional subspace under different lighting conditions [5], we select the codimensional parameter of the DPCA to c = 41. We conduct 10 independent studies for each individual on all 38 individuals for a different number of outliers M = 32, 64, 128, and report on the entire ensemble of ROC curves in Fig. 4. As is obvious, the most robust A is of all PCs."}, {"heading": "7. Conclusions", "text": "We introduced Dual Principal Component Pursuit (DPCP), a novel \"1 outlier detection method\" based on solving a \"1 problem on the sphere through linear programs using a sequence of tangential spaces on the sphere. DPCP is capable of treating sub-spaces with a codimension as low as 1 in the presence of up to 50% outliers. Future research will focus on speeding up the method and extending it to multiple sub-spaces and other types of data corruption, such as missing entries and entry errors."}, {"heading": "Acknowledgement", "text": "This work was supported by NSF 1447822."}], "references": [{"title": "From sparse solutions of systems of equations to sparse modeling of signals and images", "author": ["A.M. Bruckstein", "D.L. Donoho", "M. Elad"], "venue": "SIAM Review,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2009}, {"title": "Robust principal component analysis", "author": ["E. Cand\u00e8s", "X. Li", "Y. Ma", "J. Wright"], "venue": "Journal of the ACM,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2011}, {"title": "An introduction to compressive sampling", "author": ["E. Cand\u00e8s", "M. Wakin"], "venue": "IEEE Signal Processing Magazine,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2008}, {"title": "Robust classification using structured sparse representation", "author": ["E. Elhamifar", "R. Vidal"], "venue": "In IEEE Conference on Computer Vision and Pattern Recognition,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2011}, {"title": "Sparse subspace clustering: Algorithm, theory, and applications", "author": ["E. Elhamifar", "R. Vidal"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2013}, {"title": "Learning generative visual models from few training examples: An incremental bayesian approach tested on 101 object categories", "author": ["L. Fei-Fei", "R. Fergus", "P. Perona"], "venue": "Comput. Vis. Image Underst.,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2007}, {"title": "From few to many: Illumination cone models for face recognition under variable lighting and pose", "author": ["A.S. Georghiades", "P.N. Belhumeur", "D.J. Kriegman"], "venue": "IEEE Trans. Pattern Anal. Mach. Intelligence,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2001}, {"title": "Discrepancies of point sequences on the sphere and numerical integration", "author": ["P.J. Grabner", "B. Klinger", "R.F. Tichy"], "venue": "Mathematical Research,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 1997}, {"title": "Spherical designs, discrepancy and numerical integration", "author": ["P.J. Grabner", "R.F. Tichy"], "venue": "Math. Comp.,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 1993}, {"title": "Analysis of a complex of statistical variables into principal components", "author": ["H. Hotelling"], "venue": "Journal of Educational Psychology,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 1933}, {"title": "Principal Component Analysis", "author": ["I. Jolliffe"], "venue": "Springer-Verlag, 2nd edition,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2002}, {"title": "Fast convex optimization algorithms for exact recovery of a corrupted low-rank matrix", "author": ["Z. Lin", "A. Ganesh", "J. Wright", "L. Wu", "M. Chen", "Y. Ma"], "venue": "Computational Advances in Multi-Sensor Adaptive Processing (CAMSAP),", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2009}, {"title": "Robust subspace segmentation by low-rank representation", "author": ["G. Liu", "Z. Lin", "Y. Yu"], "venue": "In International Conference on Machine Learning,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2010}, {"title": "The cosparse analysis model and algorithms", "author": ["S. Nam", "M.E. Davies", "M. Elad", "R. Gribonval"], "venue": "Applied and Computational Harmonic Analysis,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2013}, {"title": "On lines and planes of closest fit to systems of points in space", "author": ["K. Pearson"], "venue": "The London, Edinburgh and Dublin Philosphical Magazine and Journal of Science,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 1901}, {"title": "Finding a sparse vector in a subspace: Linear sparsity using alternating directions", "author": ["Q. Qu", "J. Sun", "J. Wright"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2014}, {"title": "Finding a sparse vector in a subspace: Linear sparsity using alternating directions", "author": ["Q. Qu", "J. Sun", "J. Wright"], "venue": "CoRR, abs/1412.4659,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2014}, {"title": "A geometric analysis of subspace clustering with outliers", "author": ["M. Soltanolkotabi", "E.J. Cand\u00e8s"], "venue": "Annals of Statistics,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2012}, {"title": "Exact recovery of sparsely-used dictionaries", "author": ["D.A. Spielman", "H. Wang", "J. Wright"], "venue": "In Proceedings of the Twenty-Third international joint conference on Artificial Intelligence,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2013}, {"title": "Complete dictionary recovery over the sphere", "author": ["J. Sun", "Q. Qu", "J. Wright"], "venue": "CoRR, abs/1504.06785,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2015}, {"title": "Subspace clustering", "author": ["R. Vidal"], "venue": "IEEE Signal Processing Magazine,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2011}, {"title": "Robust PCA via Outlier Pursuit", "author": ["H. Xu", "C. Caramanis", "S. Sanghavi"], "venue": "In Neural Information Processing Systems,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2010}, {"title": "Blind source separation by sparse decomposition in a signal dictionary", "author": ["M. Zibulevsky", "B. Pearlmutter"], "venue": "Neural computation,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2001}], "referenceMentions": [{"referenceID": 14, "context": "Principal Component Analysis (PCA) is one of the oldest [16, 11] and most fundamental techniques in data analysis, enjoying ubiquitous applications in modern science and engineering [12].", "startOffset": 56, "endOffset": 64}, {"referenceID": 9, "context": "Principal Component Analysis (PCA) is one of the oldest [16, 11] and most fundamental techniques in data analysis, enjoying ubiquitous applications in modern science and engineering [12].", "startOffset": 56, "endOffset": 64}, {"referenceID": 10, "context": "Principal Component Analysis (PCA) is one of the oldest [16, 11] and most fundamental techniques in data analysis, enjoying ubiquitous applications in modern science and engineering [12].", "startOffset": 182, "endOffset": 186}, {"referenceID": 10, "context": ", the angle between V\u0302 and V is relatively small and more importantly V\u0302 is optimal when the noise is Gaussian [12].", "startOffset": 111, "endOffset": 115}, {"referenceID": 10, "context": "Traditional outlier detection approaches come from robust statistics and include Influence-based Detection, Multivariate Trimming,M -Estimators, Iteratively Weighted Recursive Least Squares and Random Sampling Consensus (RANSAC) [12].", "startOffset": 229, "endOffset": 233}, {"referenceID": 21, "context": "Recently, two attractive methods have appeared [23, 19] with tight connections to compressed sensing [3] and low-rank representation [14].", "startOffset": 47, "endOffset": 55}, {"referenceID": 17, "context": "Recently, two attractive methods have appeared [23, 19] with tight connections to compressed sensing [3] and low-rank representation [14].", "startOffset": 47, "endOffset": 55}, {"referenceID": 2, "context": "Recently, two attractive methods have appeared [23, 19] with tight connections to compressed sensing [3] and low-rank representation [14].", "startOffset": 101, "endOffset": 104}, {"referenceID": 12, "context": "Recently, two attractive methods have appeared [23, 19] with tight connections to compressed sensing [3] and low-rank representation [14].", "startOffset": 133, "endOffset": 137}, {"referenceID": 17, "context": "Remarkably, the self-expressiveness method of [19] does not require an upper bound on the number of outliers as the method of [23] does.", "startOffset": 46, "endOffset": 50}, {"referenceID": 21, "context": "Remarkably, the self-expressiveness method of [19] does not require an upper bound on the number of outliers as the method of [23] does.", "startOffset": 126, "endOffset": 130}, {"referenceID": 21, "context": "In this paper we adopt a dual approach to the problem of robust PCA in the presence of outliers, which allows us to transcend the low-rank regime of modern methods such as [23, 19].", "startOffset": 172, "endOffset": 180}, {"referenceID": 17, "context": "In this paper we adopt a dual approach to the problem of robust PCA in the presence of outliers, which allows us to transcend the low-rank regime of modern methods such as [23, 19].", "startOffset": 172, "endOffset": 180}, {"referenceID": 21, "context": "in [23], we can search for a hyperplane H1 that contains as many points of the dataset as possible.", "startOffset": 3, "endOffset": 7}, {"referenceID": 21, "context": "Experiments on synthetic data demonstrate that the proposed method is able to handle more outliers and higher dimensional subspaces than the state-of-the-art methods [23, 19].", "startOffset": 166, "endOffset": 174}, {"referenceID": 17, "context": "Experiments on synthetic data demonstrate that the proposed method is able to handle more outliers and higher dimensional subspaces than the state-of-the-art methods [23, 19].", "startOffset": 166, "endOffset": 174}, {"referenceID": 10, "context": "One of the oldest and most popular outlier detection methods in PCA is Random Sampling Consensus (RANSAC) [12].", "startOffset": 106, "endOffset": 110}, {"referenceID": 21, "context": "Among many other outlier detection methods (see Section 1), in the remaining of this section we will focus on the modern low-rank/sparse-representation theoretic methods of [23] and [19], which we will later use experimentally to compare against our proposed method.", "startOffset": 173, "endOffset": 177}, {"referenceID": 17, "context": "Among many other outlier detection methods (see Section 1), in the remaining of this section we will focus on the modern low-rank/sparse-representation theoretic methods of [23] and [19], which we will later use experimentally to compare against our proposed method.", "startOffset": 182, "endOffset": 186}, {"referenceID": 21, "context": "The first method [23], referred to as L21, is a variation of the Robust PCA algorithm of [13, 2], which computes a (`\u2217 + `21)-norm decomposition1 of the data matrix, instead of the (`\u2217 + `1)-decomopsition in [2].", "startOffset": 17, "endOffset": 21}, {"referenceID": 11, "context": "The first method [23], referred to as L21, is a variation of the Robust PCA algorithm of [13, 2], which computes a (`\u2217 + `21)-norm decomposition1 of the data matrix, instead of the (`\u2217 + `1)-decomopsition in [2].", "startOffset": 89, "endOffset": 96}, {"referenceID": 1, "context": "The first method [23], referred to as L21, is a variation of the Robust PCA algorithm of [13, 2], which computes a (`\u2217 + `21)-norm decomposition1 of the data matrix, instead of the (`\u2217 + `1)-decomopsition in [2].", "startOffset": 89, "endOffset": 96}, {"referenceID": 1, "context": "The first method [23], referred to as L21, is a variation of the Robust PCA algorithm of [13, 2], which computes a (`\u2217 + `21)-norm decomposition1 of the data matrix, instead of the (`\u2217 + `1)-decomopsition in [2].", "startOffset": 208, "endOffset": 211}, {"referenceID": 21, "context": "It is shown in [23] that, under certain conditions, the optimal solution to this problem is of the form L = [X 0D\u00d7M ]\u0393 and E = [0D\u00d7N O]\u0393.", "startOffset": 15, "endOffset": 19}, {"referenceID": 3, "context": "The second method that we consider, referred to as SE, is based on the self-expressiveness property of the data matrix, a notion popularized by the work of [4, 5] in the area of subspace clustering [22].", "startOffset": 156, "endOffset": 162}, {"referenceID": 4, "context": "The second method that we consider, referred to as SE, is based on the self-expressiveness property of the data matrix, a notion popularized by the work of [4, 5] in the area of subspace clustering [22].", "startOffset": 156, "endOffset": 162}, {"referenceID": 20, "context": "The second method that we consider, referred to as SE, is based on the self-expressiveness property of the data matrix, a notion popularized by the work of [4, 5] in the area of subspace clustering [22].", "startOffset": 198, "endOffset": 202}, {"referenceID": 17, "context": "(5) If d is small enough with respect to D, an element is declared as an outlier if the `1 norm of its coefficient vector in C is large; see [19] for an explicit formula.", "startOffset": 141, "endOffset": 145}, {"referenceID": 17, "context": "SE admits theoretical guarantees [19] and efficient ADMM implementations [5].", "startOffset": 33, "endOffset": 37}, {"referenceID": 4, "context": "SE admits theoretical guarantees [19] and efficient ADMM implementations [5].", "startOffset": 73, "endOffset": 76}, {"referenceID": 0, "context": "and variants of its relaxations have appeared on several occasions and in diverse contexts in the literature, but are much less understood than the now classic sparse [1] and cosparse [15] problems of the form", "startOffset": 167, "endOffset": 170}, {"referenceID": 13, "context": "and variants of its relaxations have appeared on several occasions and in diverse contexts in the literature, but are much less understood than the now classic sparse [1] and cosparse [15] problems of the form", "startOffset": 184, "endOffset": 188}, {"referenceID": 22, "context": "One of the first instances where (6) was considered was in the context of blind source separation [24], where it was proposed to relax it with the problem", "startOffset": 98, "endOffset": 102}, {"referenceID": 18, "context": "was proposed, with w taken to be a row or a sum of two rows of \u03a9, and theorems of correctness were given in the context of dictionary learning [20].", "startOffset": 143, "endOffset": 147}, {"referenceID": 15, "context": "In the context of finding the sparsest vector in a subspace, which is intrinsically related to dictionary learning, an alternating direction minimization scheme was proposed in [17, 18] to solve a relaxation of the form", "startOffset": 177, "endOffset": 185}, {"referenceID": 16, "context": "In the context of finding the sparsest vector in a subspace, which is intrinsically related to dictionary learning, an alternating direction minimization scheme was proposed in [17, 18] to solve a relaxation of the form", "startOffset": 177, "endOffset": 185}, {"referenceID": 19, "context": "The geometry of (12) was further studied in a probabilistic framework in the recent [21], after replacing the `1-norm with a smooth surrogate.", "startOffset": 84, "endOffset": 88}, {"referenceID": 8, "context": "In a deterministic setting, this is an active subject of study in the fields of combinatorial geometry and numerical integration on the sphere [9, 8].", "startOffset": 143, "endOffset": 149}, {"referenceID": 7, "context": "In a deterministic setting, this is an active subject of study in the fields of combinatorial geometry and numerical integration on the sphere [9, 8].", "startOffset": 143, "endOffset": 149}, {"referenceID": 18, "context": "[20].", "startOffset": 0, "endOffset": 4}, {"referenceID": 17, "context": "In particular, we use \u03b1SE = 100, \u03c4L21 = 100 and \u03bbL21 = 3/(7 \u221a M); see [19] and [23] for details.", "startOffset": 70, "endOffset": 74}, {"referenceID": 21, "context": "In particular, we use \u03b1SE = 100, \u03c4L21 = 100 and \u03bbL21 = 3/(7 \u221a M); see [19] and [23] for details.", "startOffset": 79, "endOffset": 83}, {"referenceID": 6, "context": "The inliers are taken to be all N = 64 face images of a single individual from the Extended Yale B dataset [7], while the M outliers are randomly chosen from Caltech101 [6].", "startOffset": 107, "endOffset": 110}, {"referenceID": 5, "context": "The inliers are taken to be all N = 64 face images of a single individual from the Extended Yale B dataset [7], while the M outliers are randomly chosen from Caltech101 [6].", "startOffset": 169, "endOffset": 172}, {"referenceID": 4, "context": "All images are cropped to size 48 \u00d7 42 as was done in [5].", "startOffset": 54, "endOffset": 57}, {"referenceID": 4, "context": "Since it is known that face images of a single individual under different lighting conditions lie close to an approximately 9-dimensional subspace [5], we choose the codimension parameter of DPCA to be c = 41.", "startOffset": 147, "endOffset": 150}], "year": 2017, "abstractText": "We consider the problem of outlier rejection in single subspace learning. Classical approaches work directly with a low-dimensional representation of the subspace. Our approach works with a dual representation of the subspace and hence aims to find its orthogonal complement. We pose this problem as an `1-minimization problem on the sphere and show that, under certain conditions on the distribution of the data, any global minimizer of this non-convex problem gives a vector orthogonal to the subspace. Moreover, we show that such a vector can still be found by relaxing the non-convex problem with a sequence of linear programs. Experiments on synthetic and real data show that the proposed approach, which we call Dual Principal Component Pursuit (DPCP), outperforms state-of-the art methods, especially in the case of high-dimensional subspaces.", "creator": "LaTeX with hyperref package"}}}