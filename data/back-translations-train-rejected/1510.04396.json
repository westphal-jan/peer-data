{"id": "1510.04396", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "15-Oct-2015", "title": "Filtrated Spectral Algebraic Subspace Clustering", "abstract": "Algebraic Subspace Clustering (ASC) is a simple and elegant method based on polynomial fitting and differentiation for clustering noiseless data drawn from an arbitrary union of subspaces. In practice, however, ASC is limited to equi-dimensional subspaces because the estimation of the subspace dimension via algebraic methods is sensitive to noise. This paper proposes a new ASC algorithm that can handle noisy data drawn from subspaces of arbitrary dimensions. The key ideas are (1) to construct, at each point, a decreasing sequence of subspaces containing the subspace passing through that point; (2) to use the distances from any other point to each subspace in the sequence to construct a subspace clustering affinity, which is superior to alternative affinities both in theory and in practice. Experiments on the Hopkins 155 dataset demonstrate the superiority of the proposed method with respect to sparse and low rank subspace clustering methods.", "histories": [["v1", "Thu, 15 Oct 2015 04:12:37 GMT  (245kb,D)", "http://arxiv.org/abs/1510.04396v1", null]], "reviews": [], "SUBJECTS": "cs.CV cs.LG", "authors": ["manolis c tsakiris", "rene vidal"], "accepted": false, "id": "1510.04396"}, "pdf": {"name": "1510.04396.pdf", "metadata": {"source": "CRF", "title": "Filtrated Spectral Algebraic Subspace Clustering", "authors": ["Manolis C. Tsakiris", "Ren\u00e9 Vidal"], "emails": ["m.tsakiris@jhu.edu", "rvidal@jhu.edu"], "sections": [{"heading": "1. Introduction", "text": "In fact, most of them will be able to play by the rules they have imposed on themselves."}, {"heading": "2. Algebraic Subspace Clustering: A Review", "text": "We begin with a brief overview of the ASC theory and algorithms. We refer the reader to [25, 26, 3, 14] for details. Subspace clustering problem. LetX = {x1,.. xN} be a set of points that lie in an unknown union of n > 1 subspacesA = > S1 \u00b7 \u00b7 S1 \u00b7 \u00b7 \u00b7 Sn, where Si is a linear subspace of RD of dimension di < D. The goal of subspace clustering is to find the number of subspaces, a basis for each subspace, and cluster the data points based on their subspace affiliation, i.e., find the correct decomposition or clustering of X = X1. \u00b7 Xn, where Xi = X's subspace clustering problem is well-defined, we need to make certain assumptions on the geometry of both the subspaces Si and the data X. In this work we assume that the underlying union of spaces A is transversal."}, {"heading": "3. Filtrated Spectral ASC", "text": "In this section, we propose a new sub-space clustering process that deals with the robustness of ASC in relation to noise and unknown sub-space dimensions, especially in the case of sub-spaces of different sizes."}, {"heading": "3.1. A Distance-Based Affinity", "text": "Our first contribution is to replace the angle-based affinity in (4) with a distance-based affinity in (1) and to show that the new affinity has superior theoretical guarantees. Given unit norm data points X = {xj} Nj = 1, which are close to an unknown combination of n subspaces, let p be an approximate vanishing polynomial whose coefficients are given by the correct singular vector of Vn (X) with its smallest singular value. We define the distance-based affinity in Djj \u2032 = 1 \u2212 12% < < p | xj \u2032 < < p | xj \u2032, xj \u2032, xj > (5), at which the gradient vectors are normalized as a unit of the Euclidean norm. We refer to this spectral ASC method with distance-based affinity in (5) SSC-point to the denomination distance of the D. The fact is that the point in the denomination distance is 1."}, {"heading": "3.2. Filtrated ASC", "text": "Theorem 1 shows the superiority of distance-based affinity in (5) over angle-based affinity in (4) because it ensures that points from the same subspace can have an affinity of maximum value. (5) What still limits the theoretical guarantees of (5) is the fact that points from different subspaces can also have a maximum affinity. (1) In this section we show that it is possible to further refine (5) by placing each point in one of the n subspaces and normalizing it to have a series of points in RD in the general position of n transverse subspaces."}, {"heading": "3.3. Filtrated Spectral ASC", "text": "In this case, algorithm 1 (FASC) is not applicable, because the noisy embedded data matrix Vn (X) is generally fully valid. Nevertheless, we will next show that we can still use the insights uncovered by the theoretical guarantees of FASC to construct a robust ASC algorithm. First, it should be noted that algorithm 1 requires a single disappearing polynomial in each step of each filtration. We can use all approximate disappearing polynomial in step k, for example to determine the points passed through filtration in step k \u2212 1, we can be pk \u2212 1, whose coefficients are given by the correct singular vector of Vn (X \u2212 1) corresponding to its smallest singular value."}, {"heading": "4. Experiments", "text": "Synthetic data we randomly generate n = 3 sub-spaces of dimensions di = 1, 2, 3, 4, 4 in R5. < For each choice of {di} we randomly generate Ni = 100 standard points per sub-space and add 500 independent sub-space cluster experiments using the algebraic methods FSASC, SASC-D and SASC-A, and compare with state methods such as SSC [6], LRR [11, 12], LRSC [23] and LSR with Equation (16) in [13]. We also use heuristic post-processing of affinity for LRR (LRR-H) and LSR (LSR-H)."}, {"heading": "5. Conclusions", "text": "We presented a novel algebraic clustering method based on the geometric idea of filtration, and we experimentally demonstrated its robustness to noise based on synthetic and real data, and its superiority over state-of-the-art algorithms on several occasions. Overall, the method works very well for sub-spaces of any dimension in a low-dimensional environment, and it can handle higher dimensions via projection. A major weakness of the method is its high computational complexity, resulting from the large number of required filtrations, as well as the exponential costs of adapting polynomials to sub-spaces. Future research will focus on reducing complexity and dealing with outliers and missing entries."}], "references": [{"title": "Introduction to Commutative Algebra", "author": ["M. Atiyah", "I. MacDonald"], "venue": null, "citeRegEx": "1", "shortCiteRegEx": "1", "year": 1994}, {"title": "k-plane clustering", "author": ["P.S. Bradley", "O.L. Mangasarian"], "venue": "Journal of Global Optimization,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2000}, {"title": "Hilbert series of subspace arrangements", "author": ["H. Derksen"], "venue": "Journal of Pure and Applied Algebra,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2007}, {"title": "Sparse subspace clustering", "author": ["E. Elhamifar", "R. Vidal"], "venue": "In IEEE Conference on Computer Vision and Pattern Recognition,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2009}, {"title": "Clustering disjoint subspaces via sparse representation", "author": ["E. Elhamifar", "R. Vidal"], "venue": "In IEEE International Conference on Acoustics, Speech, and Signal Processing,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2010}, {"title": "Sparse subspace clustering: Algorithm, theory, and applications", "author": ["E. Elhamifar", "R. Vidal"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2013}, {"title": "A closed form solution to robust subspace estimation and clustering", "author": ["P. Favaro", "R. Vidal", "A. Ravichandran"], "venue": "In IEEE Conference on Computer Vision and Pattern Recognition,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2011}, {"title": "Minimum effective dimension for mixtures of subspaces: A robust GPCA algorithm and its applications", "author": ["K. Huang", "Y. Ma", "R. Vidal"], "venue": "In IEEE Conference on Computer Vision and Pattern Recognition,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2004}, {"title": "Benford\u2019s Law : Theory, the General Law of Relative Quantities, and Forensic Fraud Detection Applications", "author": ["A. Kossovsky"], "venue": "World Scientific Publishing Company,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2014}, {"title": "Gradientbased learning applied to document recognition", "author": ["Y. LeCun", "L. Bottou", "Y. Bengio", "P. Haffner"], "venue": "In Proceedings of the IEEE,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 1998}, {"title": "Robust recovery of subspace structures by low-rank representation", "author": ["G. Liu", "Z. Lin", "S. Yan", "J. Sun", "Y. Ma"], "venue": "IEEE Trans. Pattern Analysis and Machine Intelligence,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2013}, {"title": "Robust subspace segmentation by low-rank representation", "author": ["G. Liu", "Z. Lin", "Y. Yu"], "venue": "In International Conference on Machine Learning,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2010}, {"title": "Robust and efficient subspace segmentation via least squares regression", "author": ["C.-Y. Lu", "H. Min", "Z.-Q. Zhao", "L. Zhu", "D.-S. Huang", "S. Yan"], "venue": "In European Conference on Computer Vision,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2012}, {"title": "Estimation of subspace arrangements with applications in modeling and segmenting mixed data", "author": ["Y. Ma", "A. Yang", "H. Derksen", "R. Fossum"], "venue": "SIAM Review,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2008}, {"title": "Subspace clustering of high-dimensional data: a predictive approach", "author": ["B. McWilliams", "G. Montana"], "venue": "Data Mining and Knowledge Discovery,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2014}, {"title": "A geometric analysis of subspace clustering with outliers", "author": ["M. Soltanolkotabi", "E.J. Cand\u00e8s"], "venue": "Annals of Statistics,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2012}, {"title": "Mixtures of probabilistic principal component analyzers", "author": ["M. Tipping", "C. Bishop"], "venue": "Neural Computation,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 1999}, {"title": "A benchmark for the comparison of 3-D motion segmentation algorithms", "author": ["R. Tron", "R. Vidal"], "venue": "In IEEE Conference on Computer Vision and Pattern Recognition,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2007}, {"title": "Abstract algebraic-geometric subspace clustering", "author": ["M.C. Tsakiris", "R. Vidal"], "venue": "In Proceedings of Asilomar Conference on Signals, Systems and Computers,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2014}, {"title": "Abstract algebraic subspace clustering", "author": ["M.C. Tsakiris", "R. Vidal"], "venue": "CoRR, abs/1506.06289,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2015}, {"title": "Nearest q-flat tom points", "author": ["P. Tseng"], "venue": "Journal of Optimization Theory and Applications,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2000}, {"title": "Subspace clustering", "author": ["R. Vidal"], "venue": "IEEE Signal Processing Magazine,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2011}, {"title": "Low rank subspace clustering (LRSC)", "author": ["R. Vidal", "P. Favaro"], "venue": "Pattern Recognition Letters,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2014}, {"title": "A new GPCA algorithm for clustering subspaces by fitting, differentiating and dividing polynomials", "author": ["R. Vidal", "Y. Ma", "J. Piazzi"], "venue": "In IEEE Conference on Computer Vision and Pattern Recognition,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2004}, {"title": "Generalized Principal Component Analysis (GPCA)", "author": ["R. Vidal", "Y. Ma", "S. Sastry"], "venue": "In IEEE Conference on Computer Vision and Pattern Recognition,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2003}, {"title": "Generalized Principal Component Analysis (GPCA)", "author": ["R. Vidal", "Y. Ma", "S. Sastry"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2005}], "referenceMentions": [{"referenceID": 21, "context": "This is an important problem in pattern recognition with diverse applications from computer vision [22] to genomics [15].", "startOffset": 99, "endOffset": 103}, {"referenceID": 14, "context": "This is an important problem in pattern recognition with diverse applications from computer vision [22] to genomics [15].", "startOffset": 116, "endOffset": 120}, {"referenceID": 1, "context": "Early subspace clustering methods were based on alternating between finding the subspaces given the clustering and vice versa [2, 21, 17], and were very sensitive to initialization.", "startOffset": 126, "endOffset": 137}, {"referenceID": 20, "context": "Early subspace clustering methods were based on alternating between finding the subspaces given the clustering and vice versa [2, 21, 17], and were very sensitive to initialization.", "startOffset": 126, "endOffset": 137}, {"referenceID": 16, "context": "Early subspace clustering methods were based on alternating between finding the subspaces given the clustering and vice versa [2, 21, 17], and were very sensitive to initialization.", "startOffset": 126, "endOffset": 137}, {"referenceID": 25, "context": "The need for good initialization motivated the development of an algebraic technique called Generalized Principal Component Analysis (GPCA) [26], which solves the problem in closed form.", "startOffset": 140, "endOffset": 144}, {"referenceID": 23, "context": "is exploited in [24] and [8] for clustering a known number of subspaces.", "startOffset": 16, "endOffset": 20}, {"referenceID": 7, "context": "is exploited in [24] and [8] for clustering a known number of subspaces.", "startOffset": 25, "endOffset": 28}, {"referenceID": 18, "context": "The recent Abstract Algebraic Subspace Clustering (AASC) method of [19, 20], unifies the ideas of [8, 26], into a provably correct method for the decomposition of a union of subspaces to its constituent subspaces.", "startOffset": 67, "endOffset": 75}, {"referenceID": 19, "context": "The recent Abstract Algebraic Subspace Clustering (AASC) method of [19, 20], unifies the ideas of [8, 26], into a provably correct method for the decomposition of a union of subspaces to its constituent subspaces.", "startOffset": 67, "endOffset": 75}, {"referenceID": 7, "context": "The recent Abstract Algebraic Subspace Clustering (AASC) method of [19, 20], unifies the ideas of [8, 26], into a provably correct method for the decomposition of a union of subspaces to its constituent subspaces.", "startOffset": 98, "endOffset": 105}, {"referenceID": 25, "context": "The recent Abstract Algebraic Subspace Clustering (AASC) method of [19, 20], unifies the ideas of [8, 26], into a provably correct method for the decomposition of a union of subspaces to its constituent subspaces.", "startOffset": 98, "endOffset": 105}, {"referenceID": 3, "context": "State-of-the-art methods, such as Sparse Subspace Clustering [4, 5, 6] and Low Rank Subspace Clustering [12, 7, 23, 11], exploit the fact that a point in a union of subspaces can always be expressed as a linear combination of other points in the subspaces.", "startOffset": 61, "endOffset": 70}, {"referenceID": 4, "context": "State-of-the-art methods, such as Sparse Subspace Clustering [4, 5, 6] and Low Rank Subspace Clustering [12, 7, 23, 11], exploit the fact that a point in a union of subspaces can always be expressed as a linear combination of other points in the subspaces.", "startOffset": 61, "endOffset": 70}, {"referenceID": 5, "context": "State-of-the-art methods, such as Sparse Subspace Clustering [4, 5, 6] and Low Rank Subspace Clustering [12, 7, 23, 11], exploit the fact that a point in a union of subspaces can always be expressed as a linear combination of other points in the subspaces.", "startOffset": 61, "endOffset": 70}, {"referenceID": 11, "context": "State-of-the-art methods, such as Sparse Subspace Clustering [4, 5, 6] and Low Rank Subspace Clustering [12, 7, 23, 11], exploit the fact that a point in a union of subspaces can always be expressed as a linear combination of other points in the subspaces.", "startOffset": 104, "endOffset": 119}, {"referenceID": 6, "context": "State-of-the-art methods, such as Sparse Subspace Clustering [4, 5, 6] and Low Rank Subspace Clustering [12, 7, 23, 11], exploit the fact that a point in a union of subspaces can always be expressed as a linear combination of other points in the subspaces.", "startOffset": 104, "endOffset": 119}, {"referenceID": 22, "context": "State-of-the-art methods, such as Sparse Subspace Clustering [4, 5, 6] and Low Rank Subspace Clustering [12, 7, 23, 11], exploit the fact that a point in a union of subspaces can always be expressed as a linear combination of other points in the subspaces.", "startOffset": 104, "endOffset": 119}, {"referenceID": 10, "context": "State-of-the-art methods, such as Sparse Subspace Clustering [4, 5, 6] and Low Rank Subspace Clustering [12, 7, 23, 11], exploit the fact that a point in a union of subspaces can always be expressed as a linear combination of other points in the subspaces.", "startOffset": 104, "endOffset": 119}, {"referenceID": 5, "context": "These methods perform very well when the subspace dimensions are much smaller than the dimension of the ambient space, the subspaces are sufficiently separated and the data are well distributed inside the subspaces [6, 16].", "startOffset": 215, "endOffset": 222}, {"referenceID": 15, "context": "These methods perform very well when the subspace dimensions are much smaller than the dimension of the ambient space, the subspaces are sufficiently separated and the data are well distributed inside the subspaces [6, 16].", "startOffset": 215, "endOffset": 222}, {"referenceID": 25, "context": "As a secondary contribution, we propose to replace the angle-based affinity proposed in [26] by a superior distance-based affinity.", "startOffset": 88, "endOffset": 92}, {"referenceID": 24, "context": "We refer the reader to [25, 26, 3, 14] for details.", "startOffset": 23, "endOffset": 38}, {"referenceID": 25, "context": "We refer the reader to [25, 26, 3, 14] for details.", "startOffset": 23, "endOffset": 38}, {"referenceID": 2, "context": "We refer the reader to [25, 26, 3, 14] for details.", "startOffset": 23, "endOffset": 38}, {"referenceID": 13, "context": "We refer the reader to [25, 26, 3, 14] for details.", "startOffset": 23, "endOffset": 38}, {"referenceID": 13, "context": "In this work we assume that the underlying union of subspaces A is transversal [14], which in particular implies that there are no inclusions between subspaces.", "startOffset": 79, "endOffset": 83}, {"referenceID": 0, "context": "Such a set is called an algebraic variety [1].", "startOffset": 42, "endOffset": 45}, {"referenceID": 24, "context": "Even though an elegant solution based on polynomial factorization exists for the case of hyperplanes [25], it has not been generalized for subspaces of different dimensions.", "startOffset": 101, "endOffset": 105}, {"referenceID": 25, "context": "However, an alternative solution has been achieved by observing that given any degree n vanishing polynomial p on A, and a point x in A, the gradient of p evaluated at x will be orthogonal to the subspace associated with point x (see [26] and [14] for a geometric and algebraic argument respectively).", "startOffset": 234, "endOffset": 238}, {"referenceID": 13, "context": "However, an alternative solution has been achieved by observing that given any degree n vanishing polynomial p on A, and a point x in A, the gradient of p evaluated at x will be orthogonal to the subspace associated with point x (see [26] and [14] for a geometric and algebraic argument respectively).", "startOffset": 243, "endOffset": 247}, {"referenceID": 25, "context": ",\u2207ps|x [26, 14].", "startOffset": 7, "endOffset": 15}, {"referenceID": 13, "context": ",\u2207ps|x [26, 14].", "startOffset": 7, "endOffset": 15}, {"referenceID": 7, "context": "Motivated by the limitation of the polynomial differentiation algorithm to a known number of subspaces, and the association of undesired ghost-subspaces with the recursive method of [8], an alternative algebraic subspace clustering procedure based on filtrations of subspace arrangements was proposed in [19, 20].", "startOffset": 182, "endOffset": 185}, {"referenceID": 18, "context": "Motivated by the limitation of the polynomial differentiation algorithm to a known number of subspaces, and the association of undesired ghost-subspaces with the recursive method of [8], an alternative algebraic subspace clustering procedure based on filtrations of subspace arrangements was proposed in [19, 20].", "startOffset": 304, "endOffset": 312}, {"referenceID": 19, "context": "Motivated by the limitation of the polynomial differentiation algorithm to a known number of subspaces, and the association of undesired ghost-subspaces with the recursive method of [8], an alternative algebraic subspace clustering procedure based on filtrations of subspace arrangements was proposed in [19, 20].", "startOffset": 304, "endOffset": 312}, {"referenceID": 18, "context": "It is the very purpose of the remaining of this paper to adapt the work of [19, 20] to a numerical algorithm and to experimentally demonstrate its merit.", "startOffset": 75, "endOffset": 83}, {"referenceID": 19, "context": "It is the very purpose of the remaining of this paper to adapt the work of [19, 20] to a numerical algorithm and to experimentally demonstrate its merit.", "startOffset": 75, "endOffset": 83}, {"referenceID": 5, "context": "For each choice of {di} and \u03c3, we perform 500 independent subspace clustering experiments using the algebraic methods FSASC, SASC-D and SASC-A, and compare to state of the art methods such as SSC [6], LRR [11, 12], LRSC [23] and LSR using equation (16) in [13].", "startOffset": 196, "endOffset": 199}, {"referenceID": 10, "context": "For each choice of {di} and \u03c3, we perform 500 independent subspace clustering experiments using the algebraic methods FSASC, SASC-D and SASC-A, and compare to state of the art methods such as SSC [6], LRR [11, 12], LRSC [23] and LSR using equation (16) in [13].", "startOffset": 205, "endOffset": 213}, {"referenceID": 11, "context": "For each choice of {di} and \u03c3, we perform 500 independent subspace clustering experiments using the algebraic methods FSASC, SASC-D and SASC-A, and compare to state of the art methods such as SSC [6], LRR [11, 12], LRSC [23] and LSR using equation (16) in [13].", "startOffset": 205, "endOffset": 213}, {"referenceID": 22, "context": "For each choice of {di} and \u03c3, we perform 500 independent subspace clustering experiments using the algebraic methods FSASC, SASC-D and SASC-A, and compare to state of the art methods such as SSC [6], LRR [11, 12], LRSC [23] and LSR using equation (16) in [13].", "startOffset": 220, "endOffset": 224}, {"referenceID": 12, "context": "For each choice of {di} and \u03c3, we perform 500 independent subspace clustering experiments using the algebraic methods FSASC, SASC-D and SASC-A, and compare to state of the art methods such as SSC [6], LRR [11, 12], LRSC [23] and LSR using equation (16) in [13].", "startOffset": 256, "endOffset": 260}, {"referenceID": 17, "context": "We evaluate different methods on the Hopkins155 motion segmentation data set [18], which contains 155 videos of n = 2,3 moving objects, each one with N = 100-500 feature point trajectories of dimension D = 56-80.", "startOffset": 77, "endOffset": 81}, {"referenceID": 8, "context": "[9]).", "startOffset": 0, "endOffset": 3}, {"referenceID": 9, "context": ", 9, we randomly select 200 images from the MNIST database [10] corresponding to each digit and compute the clustering errors averaged over 100 independent experiments.", "startOffset": 59, "endOffset": 63}], "year": 2015, "abstractText": "Algebraic Subspace Clustering (ASC) is a simple and elegant method based on polynomial fitting and differentiation for clustering noiseless data drawn from an arbitrary union of subspaces. In practice, however, ASC is limited to equi-dimensional subspaces because the estimation of the subspace dimension via algebraic methods is sensitive to noise. This paper proposes a new ASC algorithm that can handle noisy data drawn from subspaces of arbitrary dimensions. The key ideas are (1) to construct, at each point, a decreasing sequence of subspaces containing the subspace passing through that point; (2) to use the distances from any other point to each subspace in the sequence to construct a subspace clustering affinity, which is superior to alternative affinities both in theory and in practice. Experiments on the Hopkins 155 dataset demonstrate the superiority of the proposed method with respect to sparse and low rank subspace clustering methods.", "creator": "LaTeX with hyperref package"}}}