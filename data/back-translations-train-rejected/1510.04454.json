{"id": "1510.04454", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "15-Oct-2015", "title": "Online Markov decision processes with policy iteration", "abstract": "The online Markov decision process (MDP) is a generalization of the classical Markov decision process that incorporates changing reward functions. In this paper, we propose practical online MDP algorithms with policy iteration and theoretically establish a sublinear regret bound. A notable advantage of the proposed algorithm is that it can be easily combined with function approximation, and thus large and possibly continuous state spaces can be efficiently handled. Through experiments, we demonstrate the usefulness of the proposed algorithm.", "histories": [["v1", "Thu, 15 Oct 2015 09:19:49 GMT  (166kb)", "http://arxiv.org/abs/1510.04454v1", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["yao ma", "hao zhang", "masashi sugiyama"], "accepted": false, "id": "1510.04454"}, "pdf": {"name": "1510.04454.pdf", "metadata": {"source": "CRF", "title": "Online Markov decision processes with policy iteration", "authors": ["Yao Ma", "Hao Zhang", "Masashi Sugiyama"], "emails": [], "sections": [{"heading": null, "text": "ar Xiv: 1The Online Markov Decision Process (MDP) is a generalization of the classic Markov decision process, which includes changing reward functions. In this paper, we propose practical online MDP algorithms with political iteration and theoretically establish a sublinear boundary of regret. A notable advantage of the proposed algorithm is that it can easily be combined with functional approximations, and thus large and possibly continuous states can be efficiently handled. In experiments, we demonstrate the usefulness of the proposed algorithm."}, {"heading": "1 Introduction", "text": "A generalization of the classical Short Path problem in graph theory, called the stochastic Short Path problem, is the best regret of politics (Bertsekas and Tsitsiklis, 1996), considering a probability distribution over all possible next nodes. A standard way to solve the problems of the stochastic Short Path problem is to formulate it as the Markov Decision Process (MDP) and to find a policy that maximizes cumulative reward over the path. In the MDP problem, the agent selects the best action according to the current state and moves to the next state according to Markovian dynamics. A fixed reward function assigns a reward value to each state. A generalization of the MDP, called the online MDP, considers the situation in which the reward function is changed over time, the learning agent decides the strategy of action selection by using the knowledge of past reward functions. Then the reward function is selected by the current behavior function, according to the environment."}, {"heading": "2 Problem definition and preliminaries", "text": "In this section we present the formal definition and related preparatory work of the online MDP problem."}, {"heading": "2.1 Online Markov decision process", "text": "\"First, we formulate the problem of online MDP learning (Even-Dar et al., 2003, 2009). (Even-Dar et al.). (Even-Dar et al., 2003, 2009). (Even-Dar et al.). (Even-Dar et al., 2003, 2009). (Even-Dar et al., 2009). (Even-Dar et al., 2009). (Even-Dar et al.). (Even-Dar et al.). (A, P, P, P, P, P, [rt] t = 1,..., T]. (More). (A, P: S, A). (A). (A). (A). (A). (A). (A). (A). (A). (A). (A). (A). (A). (A. (A). (A). (A. (A). (A). (A. (A). (A). (A). (A. (A). (A). (A). (A). (A. (A). (A). (A). (A). (A). (A). (A. (A). (A). (A). (A). (A. (A). (A). (A). (A). (A). (A). (A). (A). (A). (A). (A). (A). (A. (A). (A). (A). (A.). (A). (A). (A. (A). (A. (A). (A. (A). (A). (A). (A.). (A.). (A.). (A. (A). (A). (A. (A). (A).). (A). (A). (A. (A). (A.). (A. (A.). (A.). (A.). (A). (A. (A.).). (A."}, {"heading": "2.2 Preliminaries", "text": "Next, we present some necessary terms for discussing online MDP problems. First, we present some criteria for evaluating the performance of any stochastic policy. In each policy, the expected average reward \u03c1 (zipes) as\u03c1r (zipes) = Es \u0445 d\u03c0 (s), a \u0445 \u03c0 [r (s, a) = \u2211 s \u00b2 S \u00b2 a \u00b2 A \u00b2 A \u00b2 p (s) p (s) s (s, a) is defined, where d\u03c0 (s) is the stationary state distribution that is satisfactory. In this paper, we assume that the target MDP is in any case an optimal MDP. Another way to evaluate policy is to define the value of policy as Sutti (s) = Sutti (s). In this paper, we assume that the target MDP is in any case an optimal MDP. Another way to evaluate policy is to define the value function as Sutti (s) = Sutti (1) (r)."}, {"heading": "3 Online MDPs with policy iteration", "text": "In this section we present the proposed method for online MDPs. The basic idea of the proposed algorithm is motivated by the lazy FPL algorithm of Yu et al. (2009), which performs linear programming to maintain the \"leader\" policy. As Yu et al. (2009) pointed out, the solution of linear programming may not be suitable for problems with large (continuous) state space. Therefore, in our proposed method we apply a method of political iteration together with a stochastic policy."}, {"heading": "3.1 Algorithm", "text": "First, we define the policy improvement operator that we use depending on the specific policy model. First, we define the policy improvement operator that we use depending on the specific policy model. First, we define the policy improvement operator that we use depending on the specific policy model. First, we define the policy improvement operator. First, we define the policy. First, we define the policy. First, we use the policy. First, we use the reward function. (r, V) instead of the reward function. (r, a), the policy. (s), its policy. (s), proposed policy. (s) We make two assumptions about the defined operator. (s, V2) Assumptions. (s, \u00b7 Reward function. (r, V) Application function. (s) Application function. (s) Application scope. (s) Application scope. (s)"}, {"heading": "3.2 Regret analysis", "text": "In this section, we provide a repentance analysis for the proposed OMDP PI algorithm. First, we present several key assumptions that are included in the proof. Similar to previous work (Even-Dar et al., 2003, 2009; Yu et al., 2009; Neu et al., 2010b, 2014; Ma et al., 2014), we assume the following conditions. Assumption 3: For all these measures, there is a positive constant, so that two arbitrary state distributions d (s) and d (s) are satisfactory."}, {"heading": "3.3 OMDP-PI algorithm with approximation", "text": "When considering the large (continuous) state space in the online MDP problem, it is essential to apply a functional approximation (Tsitsiklis and Roy (1999) introduced the linear functional approximation of the value function for stochastic policies. A major advantage of the linear approximation is that the convergence guarantee is provided (Tsitsiklis and Roy, 1999). In the following, we present their theoretical results for discrete (possibly continuous) state parameters. By following the same idea as Tsitsiklis and Roy (1999), we use the linear approximation function of the value function: V (s) = answer (s). (s) In the following, we present their theoretical results for discrete (possibly continuous) state parameters, and in the following, the parameter space RK is the parameter approximation ability."}, {"heading": "4 Online MDPs with stochastic iteration", "text": "In this section, we present a more general framework of our proposed methodology for online MDPs. Specifically, we are expanding our algorithm to include stochastic iteration (Bertsekas and Tsitsiklis, 1996) for policy evaluation along with improving policies for solving online MDPs. A general form of stochastic iteration algorithms (Szita et al., 2002; Csa \"i and Monostori, 2008) can be expressed as asVt (s) (s))) Vt \u2212 1 (s) (HtVt \u2212 1) (s) (s) + wt (s), (s) where Vt\" R \"S\" (s), Ht \"S\" S. \""}, {"heading": "5 Experiments", "text": "In this section, we demonstrate experimentally the behavior of the proposed online algorithm. The goal of the Grid World problem is to run an agent from the starting block to the target block in the Grid environment. We conduct experiments in the Grid world based on the Inverse Reinforcement Learning (IRL) toolkit1 (Levine et al., 2011).In our experiments, the Grid world has 16 x 16 states with 2 actions in each state corresponding to a move to the East and North. Each action has a 30% chance of moving in the other direction.The 256 states are further grouped into 16 supergrids, each of which consists of 4 x 4 states with the same rewards. In each episode, the agent tries to find a path from the southwest corner to the northeast corner, with the highest cumulative rewards. In the North or East border states, the agent can only move to the East or North."}, {"heading": "6 Comparison with previous works", "text": "\u2022 Expert algorithm based methods (Even-Dar et al., 2003, 2009; Neu et al., 2010a, 2014): The basic idea of Expert algorithm based methods is to place an expert algorithm in each state. By taking a close look at these algorithms, the idea does not exploit the state structure of the MDP problem. \u2022 The OMDPPI algorithm can easily be combined with a functional approximation. Since it is popular to simplify the great state space problem by using the linear span of state characteristics, the OMDP PI algorithm is natural to handle the large state space online MDPs. \u2022 Online linear optimization methods (Zimin und Neu, 2013; Dick et al., 2014): By introducing stationary occupancy measures via state action pairs, the online MDP problems can be solved, just like the online linear optimization problems."}, {"heading": "7 Conclusion and future work", "text": "As a generalization of the MDP, the online MDP is a promising model for solving many online problems with guaranteed performance. In this paper, we proposed a policy iteration algorithm with a rule for updating online MDP problems. We demonstrated that the proposed algorithm achieves sublinear regret for a political sentence. It is noteworthy that the proposed algorithm is still practicable for online MDP problems with large (continuous) government space. We demonstrated that the proposed algorithm can easily be combined with functional approximation and theoretical warranty. We demonstrated the performance of the proposed algorithm through experiments in the network world. Our future work will extend the proposed algorithm to the bandit feedback scenario, in which the full information about the reward function is not disclosed to the agent."}, {"heading": "A Proof of Gibbs policy", "text": "The evidence that the divergence of two Gibbs strategies p (s) p (s) p (s) p (s) p (s) s (s) s (s) s (s) s (s) s (s) s (s) s (s) s (s) s (s) s (s) s (s) s (s) s (s) s (s) s (s) s (s) s (s) s (s) s (s) s (s) s (s) s (s) s (s) s (s) s (s) s (s) s (s) s (s) s (s) s (s) s (s) s (s) s (s) s) s (s) s (s) s (s) s (s) s (s) s (s) s (s) s (s) s (s) s (s) s (s) s (s) s (s) s (s) s (s) s (s) s (s (s) s (s) s (s) s (s) s (s) s (s) s (s) s (s) s (s) s (s) s (s) s (s) s (s) s (s) s (s) s (s) s (s) s (s) s (s) s) s (s (s) s) s (s) s (s) s) s (s (s) s) s (s) s (s) s (s) s (s) s (s) s (s) s (s) s) s (s (s) s (s) s) s) s (s (s) s (s) s (s) s (s (s) s (s (s) s) s (s) s) s (s) s (s (s) s (s) s (s (s) s (s (s) s (s) s (s (s) s (s) s (s (s) s) s (s (s) s (s) s (s (s) s (s) s (s (s) s (s) s) s (s (s (s) s (s) s) s (s"}, {"heading": "B Proof of Lemma 2", "text": "Sentence 5: The value generated by the Equ (1) fulfils the sequence generated by the Equ (1) V1 (s),.., VT (s), where the value generated by the Equ (2 \u2212 Cv + 1Cv + 1 \u2212 Cv) is the value V1 (\u00b7) \u2212 Vt (\u00b7), the value CCv (t + 1), the value CCv (t + 1), the value Cv \u2212 1 (2 \u2212 Cv + 1 \u2212 Cv), the value CCv (t + 1). By the sentence 3 in Ma et al. (2014) and the sentence 4.1 in Yu et al. (2009) we obtain the following result T = 1 (p) \u2212 1 (p) \u2212 2 \u2212 e \u2212 1 (p) \u2212 zt. \u2212 zt. \u2212 zt. t = 1 (p)."}, {"heading": "C Proof of Lemma 3", "text": "The proof follows the same line as the previous work (Even-Dar et al., 2003, 2009; Ma et al., 2014), we rewrite the proof with our notations. \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 t (a) \u2212 t (a) \u2212 t (T) p (s) p (s) p (t) p (s) p (s) p (t) p (t) p (n) p (n) p (n) t (n) t (n) t (t) p (t) p (t) p (n) p (n) p (p) p) p) p) p) p) p) p) p) p) p) p) t (n) p (n) p (n) p) p (p) p) p (p) p) p) p (p) p) p) p (n) p (p) p) p) p (p) p) p) p) p) p) p) p) p) p) p) p) p) p) p) p) p) p) p) p) p) p) p) p) p) p) p) p) p) p) p) p) p) p) p) p) p) p) p) p) p) p) p) p) p) p) p) p) p) p) p) p) p) p) p) p) p) p) p) p) p) p) p) p) p) p) p) p) p) p) p) p) p) p) p) p) p) p) p) p) p) p) p) p) p) p) p) p) p) p) p) p) p) p) p) p) p) p) p) p) p) p) p) p) p) p) p) p) p) p) p) p) p) p) p) p) p) p) p) p) p) p) p) p) p) p) p) p) p) p) p) p) p) p) p) p) p) p) p) p) p) p) p) p) p) p) p) p) p) p)"}, {"heading": "D Proof of Proposition 5", "text": "Proposition 6: For any reward function k = 1 k = 1 k = 1 k = 1 k = 1 k = 1 k = 1 k = 1 k = 1 k = 1 k = 1 k = 1 k = 1 k = 1 k = 1 k = 1 k = 1 k = 1 k = 1 k = 1 k = 1 k = 1 k = 1 k = 1 k = 1 k = 1 k = 1 k = 1 k = 1 k = 1 k = 1 k = 1 k = 1 k = 1 k + 1 k = 1 k = 1 s (s) + 1 s (s) + 1 k \u00b2 + 1 s + 1 k + 1 k + 1 k) + 1 s (k = 1 k) k (k = 1 k) k = 1 k k (k) k = 1 k = 1 k) k (k = 1 k) k = 1 k (k) k = 1 k) k = 1 k (k) k = 1 k = 1 k) k (k) k = 1 k = 1 k) k (k) k = 1 k = 1 k) k = 1 k (k) k = 1 k = 1 k) k = 1 k = 1 k = 1 k = 1 k = 1 k."}, {"heading": "E Proof of Proposition 6", "text": "We define the operator T \u03c0V\u03c0 \u2212 - (s) = E\u03c0 [r (s, a) - (s, a) - (s) - (s) - (s) - (s) - (s) - (s) - (s) - (s) - (s) - (s) - (s) - (s) - (T) - (s) - (s) - (s) - (s) - (s) - (s) - (s) - (s) - (s) - (s) \u2212 - (s) - (s) - (s) - (s) - (s) - (s) - (s) - (s) - (s) - (s) - (s) - (s) - (s)."}], "references": [{"title": "Online learning in Markov decision processes with adversarially chosen transition probability distributions", "author": ["Y. Abbasi-Yadkori", "P. Bartlett", "V. Kanade", "Y. Seldin", "C. Szepesvari"], "venue": "Advances in Neural Information Processing Systems 26, pages 2508\u20132516.", "citeRegEx": "Abbasi.Yadkori et al\\.,? 2013", "shortCiteRegEx": "Abbasi.Yadkori et al\\.", "year": 2013}, {"title": "Neuro-Dynamic Programming", "author": ["D.P. Bertsekas", "J.N. Tsitsiklis"], "venue": "Athena Scientific, 1st edition.", "citeRegEx": "Bertsekas and Tsitsiklis,? 1996", "shortCiteRegEx": "Bertsekas and Tsitsiklis", "year": 1996}, {"title": "Prediction, Learning, and Games", "author": ["N. Cesa-Bianchi", "G. Lugosi"], "venue": "Cambridge University Press, New York, NY, USA.", "citeRegEx": "Cesa.Bianchi and Lugosi,? 2006", "shortCiteRegEx": "Cesa.Bianchi and Lugosi", "year": 2006}, {"title": "Value function based reinforcement learning in changing Markovian environments", "author": ["B.C. Cs\u00e1ji", "L. Monostori"], "venue": "Journal of Machine Learning Research, 9:1679\u2013 1709.", "citeRegEx": "Cs\u00e1ji and Monostori,? 2008", "shortCiteRegEx": "Cs\u00e1ji and Monostori", "year": 2008}, {"title": "Online learning in Markov decision processes with changing cost sequences", "author": ["T. Dick", "A. Gy\u00f6rgy", "C. Szepesv\u00e1ri"], "venue": "Proceedings of the 31th International Conference on Machine Learning, ICML 2014, Beijing, China, 21-26 June 2014, pages 512\u2013520.", "citeRegEx": "Dick et al\\.,? 2014", "shortCiteRegEx": "Dick et al\\.", "year": 2014}, {"title": "Experts in a Markov decision process", "author": ["E. Even-Dar", "S.M. Kakade", "Y. Mansour"], "venue": "Advances in Neural Information Processing System, pages 401\u2013408.", "citeRegEx": "Even.Dar et al\\.,? 2003", "shortCiteRegEx": "Even.Dar et al\\.", "year": 2003}, {"title": "Online Markov decision processes", "author": ["E. Even-Dar", "S.M. Kakade", "Y. Mansour"], "venue": "Mathematics of Operations Research, 34(3):726\u2013736. 21", "citeRegEx": "Even.Dar et al\\.,? 2009", "shortCiteRegEx": "Even.Dar et al\\.", "year": 2009}, {"title": "Powers of tensors and fast matrix multiplication", "author": ["F. Le Gall"], "venue": "Proceedings of the 39th International Symposium on Symbolic and Algebraic Computation, ISSAC \u201914, pages 296\u2013303, New York, NY, USA. ACM.", "citeRegEx": "Gall,? 2014", "shortCiteRegEx": "Gall", "year": 2014}, {"title": "Nonlinear inverse reinforcement learning with Gaussian processes", "author": ["S. Levine", "Z. Popovic", "V. Koltun"], "venue": "Advances in Neural Information Processing Systems 24, pages 19\u201327.", "citeRegEx": "Levine et al\\.,? 2011", "shortCiteRegEx": "Levine et al\\.", "year": 2011}, {"title": "An online policy gradient algorithm for Markov decision processes with continuous states and actions", "author": ["Y. Ma", "T. Zhao", "K. Hatano", "M. Sugiyama"], "venue": "Machine Learning and Knowledge Discovery in Databases - European Conference, ECML PKDD 2014, Nancy, France, September 15-19, 2014. Proceedings, Part II, pages 354\u2013369.", "citeRegEx": "Ma et al\\.,? 2014", "shortCiteRegEx": "Ma et al\\.", "year": 2014}, {"title": "Online Markov decision processes under bandit feedback", "author": ["G. Neu", "G. Andr\u00e1s", "C. Szepesv\u00e1ri", "A. Antos"], "venue": "IEEE Transactions on Automatic Control, 59(3):676\u2013 691.", "citeRegEx": "Neu et al\\.,? 2014", "shortCiteRegEx": "Neu et al\\.", "year": 2014}, {"title": "The online loop-free stochastic shortest-path problem", "author": ["G. Neu", "A. Gy\u00f6rgy", "C. Szepesv\u00e1ri"], "venue": "The 23rd Conference on Learning Theory, Haifa, Israel, June 27-29, 2010, pages 231\u2013243.", "citeRegEx": "Neu et al\\.,? 2010a", "shortCiteRegEx": "Neu et al\\.", "year": 2010}, {"title": "The adversarial stochastic shortest path problem with unknown transition probabilities", "author": ["G. Neu", "A. Gy\u00f6rgy", "C. Szepesv\u00e1ri"], "venue": "Proceedings of the Fifteenth International Conference on Artificial Intelligence and Statistics (AISTATS-12), volume 22, pages 805\u2013813. 22", "citeRegEx": "Neu et al\\.,? 2012", "shortCiteRegEx": "Neu et al\\.", "year": 2012}, {"title": "Online Markov decision processes under bandit feedback", "author": ["G. Neu", "A. Gyrgy", "C. Szepesvri", "A. Antos"], "venue": "Advances in Neural Information Processing System,NIPS, pages 1804\u20131812.", "citeRegEx": "Neu et al\\.,? 2010b", "shortCiteRegEx": "Neu et al\\.", "year": 2010}, {"title": "Learning to predict by the methods of temporal differences", "author": ["R.S. Sutton"], "venue": "Machine learning, pages 9\u201344.", "citeRegEx": "Sutton,? 1988", "shortCiteRegEx": "Sutton", "year": 1988}, {"title": "Reinforcement Learning: An Introduction", "author": ["R.S. Sutton", "A.G. Barto"], "venue": "MIT Press.", "citeRegEx": "Sutton and Barto,? 1998", "shortCiteRegEx": "Sutton and Barto", "year": 1998}, {"title": "MDPs: Learning in varying environments", "author": ["I. Szita", "B. Tak\u00e1cs", "A. L\u00f6rincz"], "venue": "Journal of Machine Learning Research, 3:145\u2013174.", "citeRegEx": "Szita et al\\.,? 2002", "shortCiteRegEx": "Szita et al\\.", "year": 2002}, {"title": "Average cost temporal-difference learning", "author": ["J.N. Tsitsiklis", "B.V. Roy"], "venue": "Automatica, 35:1799\u20131808.", "citeRegEx": "Tsitsiklis and Roy,? 1999", "shortCiteRegEx": "Tsitsiklis and Roy", "year": 1999}, {"title": "Markov decision processes with arbitrary reward processes", "author": ["J.Y. Yu", "S. Mannor", "N. Shimkin"], "venue": "Mathematics of Operations Research, 34(3):737\u2013757.", "citeRegEx": "Yu et al\\.,? 2009", "shortCiteRegEx": "Yu et al\\.", "year": 2009}, {"title": "Online learning in episodic Markovian decision processes by relative entropy policy search", "author": ["A. Zimin", "G. Neu"], "venue": "Advances in Neural Information Processing Systems 26, pages 1583\u20131591. 23", "citeRegEx": "Zimin and Neu,? 2013", "shortCiteRegEx": "Zimin and Neu", "year": 2013}, {"title": "Proof of Lemma 3 The proof is following the same line as previous works(Even-Dar", "author": [], "venue": "Ma et al.,", "citeRegEx": "C,? \\Q2003\\E", "shortCiteRegEx": "C", "year": 2003}], "referenceMentions": [{"referenceID": 1, "context": "1 Introduction A generalization of the classical shortest path problem in graph theory, called the stochastic shortest path problem (Bertsekas and Tsitsiklis, 1996), considers a probability distribution over all possible next nodes.", "startOffset": 132, "endOffset": 164}, {"referenceID": 1, "context": "1 Introduction A generalization of the classical shortest path problem in graph theory, called the stochastic shortest path problem (Bertsekas and Tsitsiklis, 1996), considers a probability distribution over all possible next nodes. A standard way to solve the stochastic shortest path problem is to formulate it as a Markov decision process (MDP) and find a policy that maximizes the cumulative reward over the path. In the MDP problem, the agent chooses the best action according to the current state and moves to the next state following the Markovian dynamics. A fixed reward function assigns a reward value to each state-action pair. A generalization of MDP, called the online MDP, considers the situation where the reward function changes over time. At each time step, the learning agent decides the strategy of choosing actions by using the knowledge of past reward functions. Then, the current reward function which is chosen by the environment is revealed to the agent after observing its behavior. The goal of online MDP is to minimize the regret against the best offline policy, which is the optimal fixed policy in hindsight. We expect that the regret vanishes as the time step T tends to infinity, implying that the agent can behave as well as the best offline policy asymptotically. Many online problems can be solved as online MDP problems. By setting the optimization variables as the state, the online MDP algorithm chooses the change of variables (action) which performs reasonably well in a non-stationary environment. Even-Dar et al. (2009) presented several typical online problems which can be formulated as online MDP perfectly, e.", "startOffset": 133, "endOffset": 1561}, {"referenceID": 11, "context": "Furthermore, the MDP-E algorithm was proved to achieve regret O(L \u221a T log |A|) for online MDP problems with L-layered state space (Neu et al., 2010a).", "startOffset": 130, "endOffset": 149}, {"referenceID": 18, "context": "Another online MDP algorithm called the lazy follow-the-perturbed-leader (lazyFPL) (Yu et al., 2009) follows the main idea of the FPL algorithm which solves the Bellman equation using the average reward function.", "startOffset": 83, "endOffset": 100}, {"referenceID": 19, "context": "Similarly to lazy-FPL, the online relative entropy policy search (O-REPS) algorithm (Zimin and Neu, 2013) also requires to solve an optimization problem at the end of each time step.", "startOffset": 84, "endOffset": 105}, {"referenceID": 4, "context": "By introducing the stationary occupation measure, Dick et al. (2014) proposed the mirror descent with approximation projections algorithm, which formulate the online MDP problem as online linear opti3", "startOffset": 50, "endOffset": 69}, {"referenceID": 12, "context": "Yu et al. (2009), Abbasi-Yadkori et al.", "startOffset": 0, "endOffset": 17}, {"referenceID": 0, "context": "(2009), Abbasi-Yadkori et al. (2013), and Neu et al.", "startOffset": 8, "endOffset": 37}, {"referenceID": 0, "context": "(2009), Abbasi-Yadkori et al. (2013), and Neu et al. (2012) considered even more challenging online MDP problems under unknown or changing transition dynamics.", "startOffset": 8, "endOffset": 60}, {"referenceID": 0, "context": "(2009), Abbasi-Yadkori et al. (2013), and Neu et al. (2012) considered even more challenging online MDP problems under unknown or changing transition dynamics. Recently, Ma et al. (2014) proposed the online policy gradient (OPG) algorithm for online MDP problems with continuous state and action spaces, and it was proved to achieve regret O( \u221a T ) under the concavity assumption about the expected average reward function.", "startOffset": 8, "endOffset": 187}, {"referenceID": 2, "context": "Here, in the same way as standard online learning literature (Cesa-Bianchi and Lugosi, 2006), we consider the regret against the best offline time independent policy \u03c0 in the policy set \u03a0: LA(T ) = R\u03c0\u2217(T )\u2212RA(T ).", "startOffset": 61, "endOffset": 92}, {"referenceID": 19, "context": "Note that the regret we consider here is different from previous literature (Even-Dar et al., 2003, 2009; Zimin and Neu, 2013; Dick et al., 2014): we compare the performance of algorithm A against the best offline policy within a specific policy set \u03a0.", "startOffset": 76, "endOffset": 145}, {"referenceID": 4, "context": "Note that the regret we consider here is different from previous literature (Even-Dar et al., 2003, 2009; Zimin and Neu, 2013; Dick et al., 2014): we compare the performance of algorithm A against the best offline policy within a specific policy set \u03a0.", "startOffset": 76, "endOffset": 145}, {"referenceID": 15, "context": "Since the optimal value function leads to the optimal policy, MDP is often solved by deriving the optimal value function (Sutton and Barto, 1998).", "startOffset": 121, "endOffset": 145}, {"referenceID": 17, "context": "For this reason, in this paper we only consider the stochastic policy, since the convergence guarantee is provided (Tsitsiklis and Roy, 1999).", "startOffset": 115, "endOffset": 141}, {"referenceID": 18, "context": "The key idea of the proposed algorithm is motivated by the Lazy FPL algorithm by Yu et al. (2009), which performs linear programming to obtain the \u2018leader\u2019 policy.", "startOffset": 81, "endOffset": 98}, {"referenceID": 18, "context": "The key idea of the proposed algorithm is motivated by the Lazy FPL algorithm by Yu et al. (2009), which performs linear programming to obtain the \u2018leader\u2019 policy. As Yu et al. (2009) pointed out, solving linear programming may not be appropriate for problems with large (continuous) state space.", "startOffset": 81, "endOffset": 184}, {"referenceID": 15, "context": "It is well known (Sutton and Barto, 1998) that the value function satisfies V r (s) = E\u03c0 [", "startOffset": 17, "endOffset": 41}, {"referenceID": 15, "context": "It is well known (Sutton and Barto, 1998) that the above equation has no unique solution.", "startOffset": 17, "endOffset": 41}, {"referenceID": 18, "context": "Similarly to the previous works (Even-Dar et al., 2003, 2009; Yu et al., 2009; Neu et al., 2010b, 2014; Ma et al., 2014), we assume the following conditions.", "startOffset": 32, "endOffset": 120}, {"referenceID": 9, "context": "Similarly to the previous works (Even-Dar et al., 2003, 2009; Yu et al., 2009; Neu et al., 2010b, 2014; Ma et al., 2014), we assume the following conditions.", "startOffset": 32, "endOffset": 120}, {"referenceID": 0, "context": "To prove the claimed result in Theorem 1, we decompose the regret into three parts in the same way as previous works (Even-Dar et al., 2003, 2009; Abbasi-Yadkori et al., 2013; Ma et al., 2014): LA(T ) = (", "startOffset": 117, "endOffset": 192}, {"referenceID": 9, "context": "To prove the claimed result in Theorem 1, we decompose the regret into three parts in the same way as previous works (Even-Dar et al., 2003, 2009; Abbasi-Yadkori et al., 2013; Ma et al., 2014): LA(T ) = (", "startOffset": 117, "endOffset": 192}, {"referenceID": 9, "context": "The first term has been analyzed in previous works (Even-Dar et al., 2003, 2009; Ma et al., 2014), which is bounded as E\u03c0\u2217 [ T \u2211", "startOffset": 51, "endOffset": 97}, {"referenceID": 17, "context": "A significant benefit of the linear approximation is that the convergence guarantee is provided (Tsitsiklis and Roy, 1999).", "startOffset": 96, "endOffset": 122}, {"referenceID": 17, "context": "Tsitsiklis and Roy (1999) introduced the linear function approximation of the value function for stochastic policies.", "startOffset": 0, "endOffset": 26}, {"referenceID": 17, "context": "Tsitsiklis and Roy (1999) introduced the linear function approximation of the value function for stochastic policies. A significant benefit of the linear approximation is that the convergence guarantee is provided (Tsitsiklis and Roy, 1999). Below we present their theoretical results for discrete (possibly continuous) state space. By following the same idea as Tsitsiklis and Roy (1999), we use the linear approximation of the value function: V\u0302(s) = \u03b8\u03c6(s), 13", "startOffset": 0, "endOffset": 389}, {"referenceID": 17, "context": "The approximation parameter was proved to converge to the unique solution of the following equation (Tsitsiklis and Roy, 1999): P(Rt(\u03c0t)\u2212 e|S|\u03c1rt(\u03c0t) + P t\u03b8\u03c6) = \u03b8\u03c6, (4) where Rt(\u03c0t) is the |S|-dimensional column vector whose sth element is rt(s, \u03c0t) =", "startOffset": 100, "endOffset": 126}, {"referenceID": 17, "context": "Furthermore, by using Theorem 3 in Tsitsiklis and Roy (1999), the approximation error is bounded as \u2016(I|S| \u2212 e|S|d\u22a4\u03c0t)\u03b8\u2217\u22a4 t \u03c6\u2212Vt rt \u2016D\u03c0t \u2264 1 \u221a 1\u2212 e\u22122/\u03c4 inf \u03b8\u2208RK \u2016(I|S| \u2212 e|S|d\u22a4\u03c0t)\u03b8\u22a4\u03c6\u2212V\u03c0 rt \u2016D\u03c0t , where \u03b8 t is the unique solution to Eq.", "startOffset": 35, "endOffset": 61}, {"referenceID": 1, "context": "More specifically, we extend our algorithm to use stochastic iteration (Bertsekas and Tsitsiklis, 1996) for policy evaluation together with policy improvement to solve online MDPs.", "startOffset": 71, "endOffset": 103}, {"referenceID": 16, "context": "A general form of the stochastic iteration algorithm (Szita et al., 2002; Cs\u00e1ji and Monostori, 2008) can be expressed as Vt(s) = (1\u2212 \u03b3t(s))Vt\u22121(s) + \u03b3t(s) ((HtVt\u22121)(s) + wt(s)) , (5) where Vt \u2208 R, Ht : R \u2192 R, \u2200t = 1, .", "startOffset": 53, "endOffset": 100}, {"referenceID": 3, "context": "A general form of the stochastic iteration algorithm (Szita et al., 2002; Cs\u00e1ji and Monostori, 2008) can be expressed as Vt(s) = (1\u2212 \u03b3t(s))Vt\u22121(s) + \u03b3t(s) ((HtVt\u22121)(s) + wt(s)) , (5) where Vt \u2208 R, Ht : R \u2192 R, \u2200t = 1, .", "startOffset": 53, "endOffset": 100}, {"referenceID": 1, "context": "Many popular reinforcement learning algorithms based on value functions such as the temporal difference (TD) learning algorithm (Bertsekas and Tsitsiklis, 1996; Sutton and Barto, 1998; Sutton, 1988) and the SARSA algorithm (Bertsekas and Tsitsiklis, 1996; Sutton and Barto, 1998) can be regarded as stochastic iteration.", "startOffset": 128, "endOffset": 198}, {"referenceID": 15, "context": "Many popular reinforcement learning algorithms based on value functions such as the temporal difference (TD) learning algorithm (Bertsekas and Tsitsiklis, 1996; Sutton and Barto, 1998; Sutton, 1988) and the SARSA algorithm (Bertsekas and Tsitsiklis, 1996; Sutton and Barto, 1998) can be regarded as stochastic iteration.", "startOffset": 128, "endOffset": 198}, {"referenceID": 14, "context": "Many popular reinforcement learning algorithms based on value functions such as the temporal difference (TD) learning algorithm (Bertsekas and Tsitsiklis, 1996; Sutton and Barto, 1998; Sutton, 1988) and the SARSA algorithm (Bertsekas and Tsitsiklis, 1996; Sutton and Barto, 1998) can be regarded as stochastic iteration.", "startOffset": 128, "endOffset": 198}, {"referenceID": 1, "context": "Many popular reinforcement learning algorithms based on value functions such as the temporal difference (TD) learning algorithm (Bertsekas and Tsitsiklis, 1996; Sutton and Barto, 1998; Sutton, 1988) and the SARSA algorithm (Bertsekas and Tsitsiklis, 1996; Sutton and Barto, 1998) can be regarded as stochastic iteration.", "startOffset": 223, "endOffset": 279}, {"referenceID": 15, "context": "Many popular reinforcement learning algorithms based on value functions such as the temporal difference (TD) learning algorithm (Bertsekas and Tsitsiklis, 1996; Sutton and Barto, 1998; Sutton, 1988) and the SARSA algorithm (Bertsekas and Tsitsiklis, 1996; Sutton and Barto, 1998) can be regarded as stochastic iteration.", "startOffset": 223, "endOffset": 279}, {"referenceID": 2, "context": "By using Theorem 20 in Cs\u00e1ji and Monostori (2008), we have lim T\u2192\u221e \u2016VT \u2212 V \u2217 r\u0302T \u2016\u221e = 0, where \u03c0 = argmax\u03c0\u2208\u03a0 \u03c1r\u0302T (\u03c0) is the best offline policy.", "startOffset": 23, "endOffset": 50}, {"referenceID": 8, "context": "We conduct experiments on the grid world based on the Inverse Reinforcement Learning (IRL) toolkit1(Levine et al., 2011).", "startOffset": 99, "endOffset": 120}, {"referenceID": 19, "context": "\u2022 Online linear optimization based methods (Zimin and Neu, 2013; Dick et al., 2014): By introducing the stationary occupancy measures over state-action pairs, the online MDP problems can be solved as the online linear optimization problems.", "startOffset": 43, "endOffset": 83}, {"referenceID": 4, "context": "\u2022 Online linear optimization based methods (Zimin and Neu, 2013; Dick et al., 2014): By introducing the stationary occupancy measures over state-action pairs, the online MDP problems can be solved as the online linear optimization problems.", "startOffset": 43, "endOffset": 83}, {"referenceID": 18, "context": "\u2022 Linear programming based method (Yu et al., 2009): Our OMDP-PI is motivated 19", "startOffset": 34, "endOffset": 51}, {"referenceID": 1, "context": "Exploring other stochastic iteration methods such as the SARSA algorithm (Bertsekas and Tsitsiklis, 1996; Sutton and Barto, 1998) and the value iteration algorithm (Cs\u00e1ji and Monostori, 2008; Szita et al.", "startOffset": 73, "endOffset": 129}, {"referenceID": 15, "context": "Exploring other stochastic iteration methods such as the SARSA algorithm (Bertsekas and Tsitsiklis, 1996; Sutton and Barto, 1998) and the value iteration algorithm (Cs\u00e1ji and Monostori, 2008; Szita et al.", "startOffset": 73, "endOffset": 129}, {"referenceID": 3, "context": "Exploring other stochastic iteration methods such as the SARSA algorithm (Bertsekas and Tsitsiklis, 1996; Sutton and Barto, 1998) and the value iteration algorithm (Cs\u00e1ji and Monostori, 2008; Szita et al., 2002) is also an important future direction.", "startOffset": 164, "endOffset": 211}, {"referenceID": 16, "context": "Exploring other stochastic iteration methods such as the SARSA algorithm (Bertsekas and Tsitsiklis, 1996; Sutton and Barto, 1998) and the value iteration algorithm (Cs\u00e1ji and Monostori, 2008; Szita et al., 2002) is also an important future direction.", "startOffset": 164, "endOffset": 211}], "year": 2015, "abstractText": "The online Markov decision process (MDP) is a generalization of the classical Markov decision process that incorporates changing reward functions. In this paper, we propose practical online MDP algorithms with policy iteration and theoretically establish a sublinear regret bound. A notable advantage of the proposed algorithm is that it can be easily combined with function approximation, and thus large and possibly continuous state spaces can be efficiently handled. Through experiments, we demonstrate the usefulness of the proposed algorithm.", "creator": "LaTeX with hyperref package"}}}