{"id": "1510.04609", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "15-Oct-2015", "title": "Layer-Specific Adaptive Learning Rates for Deep Networks", "abstract": "The increasing complexity of deep learning architectures is resulting in training time requiring weeks or even months. This slow training is due in part to vanishing gradients, in which the gradients used by back-propagation are extremely large for weights connecting deep layers (layers near the output layer), and extremely small for shallow layers (near the input layer); this results in slow learning in the shallow layers. Additionally, it has also been shown that in highly non-convex problems, such as deep neural networks, there is a proliferation of high-error low curvature saddle points, which slows down learning dramatically. In this paper, we attempt to overcome the two above problems by proposing an optimization method for training deep neural networks which uses learning rates which are both specific to each layer in the network and adaptive to the curvature of the function, increasing the learning rate at low curvature points. This enables us to speed up learning in the shallow layers of the network and quickly escape high-error low curvature saddle points. We test our method on standard image classification datasets such as MNIST, CIFAR10 and ImageNet, and demonstrate that our method increases accuracy as well as reduces the required training time over standard algorithms.", "histories": [["v1", "Thu, 15 Oct 2015 16:31:46 GMT  (627kb,D)", "http://arxiv.org/abs/1510.04609v1", "ICMLA 2015, deep learning, adaptive learning rates for training, layer specific learning rate"]], "COMMENTS": "ICMLA 2015, deep learning, adaptive learning rates for training, layer specific learning rate", "reviews": [], "SUBJECTS": "cs.CV cs.AI cs.LG cs.NE", "authors": ["bharat singh", "soham de", "yangmuzi zhang", "thomas goldstein", "gavin taylor"], "accepted": false, "id": "1510.04609"}, "pdf": {"name": "1510.04609.pdf", "metadata": {"source": "CRF", "title": "Layer-Specific Adaptive Learning Rates for Deep Networks", "authors": ["Bharat Singh", "Soham De", "Yangmuzi Zhang", "Thomas Goldstein", "Gavin Taylor"], "emails": ["tomg}@cs.umd.edu,", "ymzhang@umiacs.umd.edu,", "taylor@usna.edu"], "sections": [{"heading": null, "text": "This is a very successful problem in the last few years in which we can see a general trend in these papers: Results tend to get better as the amount of training data increases, along with an increase in the complexity of the deep network architecture. However, increasingly complex deep networks can take weeks or months to train even with high-performance hardware. So there is a need for more efficient methods for training deep networks.Deep neural networks learn at a high level by performing a sequence of non-linear transformations. Let our training data from n data points a1, a2, a2., an., an. Rm and the corresponding labels B = {bi} ni = 1. Let us consider a 3-layer network with activation f."}, {"heading": "II. RELATED WORK", "text": "In SGD, the updates for the parameters are defined by equations (2) and (3), and the learning rate is reduced over time as it reaches a local optimum. Many changes to the basic learning rate have been proposed. A popular method in the convex optimization literature is Newton's method, which uses the Hessian objective function f (x) to determine the step size: x (k) nt = \u2212 2f (k) \u2212 1g (k)."}, {"heading": "III. OUR APPROACH", "text": "In fact, it is as if most of us are able to learn in the US, both in the US and in the US. (...) It is not as if we are able to save the world, as if we were able to get a grip on it. (...) It is not as if we are able to get a grip on it. (...) It is not as if we are able to save the world. (...) It is as if we are able to save the world. (...) It is not as if we are able to get a grip on it. (...) It is as if we are able to save the world. (...) It is not as if we are able to save the world. (...) It is not as if we are able to save the world. (...) It is not as if we are able to save the world."}, {"heading": "IV. EXPERIMENTAL RESULTS", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "A. Dataset", "text": "We present the results of image classification based on three standard data sets: MNIST, CIFAR10 and ImageNet (ILSVRC 2012 dataset, part of the ImageNet Challenge). MNIST contains 60,000 handwritten images for training and 10,000 handwritten images for testing. CIFAR10 contains 10 classes with 6,000 images per class. ImageNet contains 1.2 million color images from 1,000 different classes."}, {"heading": "B. Experimental Details", "text": "This year it has come to the point that it will only be a matter of time before it will happen, until it does."}, {"heading": "V. CONCLUSIONS", "text": "In this paper, we propose a general method for forming deep neural networks that can be used beyond any optimization method with a global learning rate. It uses gradients from each level to calculate an adaptive learning rate for each level. It aims to accelerate convergence when the parameters are located in a region with low curvature in the saddle point. Level-specific learning rates also enable the method to prevent slow learning in initial layers of the deep network, which is usually caused by very small gradients."}, {"heading": "ACKNOWLEDGMENT", "text": "The authors acknowledge ONR grant numbers N0001415WX01341 and N000141512676, as well as the University of Maryland's supercomputing resources (http: / / www.it.umd.edu / hpcc)."}], "references": [{"title": "On the saddle point problem for non-convex optimization", "author": ["R. Pascanu", "Y.N. Dauphin", "S. Ganguli", "Y. Bengio"], "venue": "arXiv preprint arXiv:1405.4604, 2014.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2014}, {"title": "Imagenet classification with deep convolutional neural networks", "author": ["A. Krizhevsky", "I. Sutskever", "G.E. Hinton"], "venue": "Advances in neural information processing systems, 2012, pp. 1097\u20131105.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2012}, {"title": "Deepface: Closing the gap to human-level performance in face verification", "author": ["Y. Taigman", "M. Yang", "M. Ranzato", "L. Wolf"], "venue": "Computer Vision and Pattern Recognition (CVPR), 2014 IEEE Conference on. IEEE, 2014, pp. 1701\u20131708.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2014}, {"title": "Recursive deep models for semantic compositionality over a sentiment treebank", "author": ["R. Socher", "A. Perelygin", "J.Y. Wu", "J. Chuang", "C.D. Manning", "A.Y. Ng", "C. Potts"], "venue": "Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP). Citeseer, 2013, pp. 1631\u20131642.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2013}, {"title": "Deep neural networks for acoustic modeling in speech recognition: The shared views of four research groups", "author": ["G. Hinton", "L. Deng", "D. Yu", "G.E. Dahl", "A.-r. Mohamed", "N. Jaitly", "A. Senior", "V. Vanhoucke", "P. Nguyen", "T.N. Sainath"], "venue": "Signal Processing Magazine, IEEE, vol. 29, no. 6, pp. 82\u201397, 2012.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2012}, {"title": "Deep sparse rectifier networks", "author": ["X. Glorot", "A. Bordes", "Y. Bengio"], "venue": "Proceedings of the 14th International Conference on Artificial Intelligence and Statistics. JMLR W&CP Volume, vol. 15, 2011, pp. 315\u2013323.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2011}, {"title": "A stochastic approximation method", "author": ["H. Robbins", "S. Monro"], "venue": "The Annals of Mathematical Statistics, vol. 22, no. 3, pp. 400\u2013407, 1951.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 1951}, {"title": "Adadelta: An adaptive learning rate method", "author": ["M.D. Zeiler"], "venue": "arXiv preprint arXiv:1212.5701, 2012.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2012}, {"title": "Adaptive subgradient methods for online learning and stochastic optimization", "author": ["J. Duchi", "E. Hazan", "Y. Singer"], "venue": "The Journal of Machine Learning Research, vol. 12, pp. 2121\u20132159, 2011.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2011}, {"title": "Long short-term memory", "author": ["S. Hochreiter", "J. Schmidhuber"], "venue": "Neural computation, vol. 9, no. 8, pp. 1735\u20131780, 1997.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 1997}, {"title": "Statistics of critical points of gaussian fields on large-dimensional spaces", "author": ["A.J. Bray", "D.S. Dean"], "venue": "Physical review letters, vol. 98, no. 15, p. 150201, 2007.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2007}, {"title": "Replica symmetry breaking condition exposed by random matrix calculation of landscape complexity", "author": ["Y.V. Fyodorov", "I. Williams"], "venue": "Journal of Statistical Physics, vol. 129, no. 5-6, pp. 1081\u20131116, 2007.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2007}, {"title": "Some methods of speeding up the convergence of iteration methods", "author": ["B.T. Polyak"], "venue": "USSR Computational Mathematics and Mathematical Physics, vol. 4, no. 5, pp. 1\u201317, 1964.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 1964}, {"title": "A method of solving a convex programming problem with convergence rate o (1/k2)", "author": ["Y. Nesterov"], "venue": "Soviet Mathematics Doklady, vol. 27, no. 2, 1983, pp. 372\u2013376.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 1983}, {"title": "On the importance of initialization and momentum in deep learning", "author": ["I. Sutskever", "J. Martens", "G. Dahl", "G. Hinton"], "venue": "Proceedings of the 30th International Conference on Machine Learning (ICML-13), 2013, pp. 1139\u20131147.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2013}, {"title": "Caffe: Convolutional architecture for fast feature embedding", "author": ["Y. Jia", "E. Shelhamer", "J. Donahue", "S. Karayev", "J. Long", "R. Girshick", "S. Guadarrama", "T. Darrell"], "venue": "arXiv preprint arXiv:1408.5093, 2014.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2014}], "referenceMentions": [{"referenceID": 0, "context": "Additionally, it has also been shown that in highly non-convex problems, such as deep neural networks, there is a proliferation of high-error low curvature saddle points, which slows down learning dramatically [1].", "startOffset": 210, "endOffset": 213}, {"referenceID": 1, "context": "Deep neural networks have been extremely successful over the past few years, achieving state of the art performance on a large number of tasks such as image classification [2], face recognition [3], sentiment analysis [4], speech recognition [5], etc.", "startOffset": 172, "endOffset": 175}, {"referenceID": 2, "context": "Deep neural networks have been extremely successful over the past few years, achieving state of the art performance on a large number of tasks such as image classification [2], face recognition [3], sentiment analysis [4], speech recognition [5], etc.", "startOffset": 194, "endOffset": 197}, {"referenceID": 3, "context": "Deep neural networks have been extremely successful over the past few years, achieving state of the art performance on a large number of tasks such as image classification [2], face recognition [3], sentiment analysis [4], speech recognition [5], etc.", "startOffset": 218, "endOffset": 221}, {"referenceID": 4, "context": "Deep neural networks have been extremely successful over the past few years, achieving state of the art performance on a large number of tasks such as image classification [2], face recognition [3], sentiment analysis [4], speech recognition [5], etc.", "startOffset": 242, "endOffset": 245}, {"referenceID": 5, "context": "Recently, rectified linear (ReLu) units (f(z) = max{0, z}) have become popular because they tend to be easy to train and yield superior results for some problems [6].", "startOffset": 162, "endOffset": 165}, {"referenceID": 6, "context": "This is the basis for stochastic gradient descent (SGD) [7], which is the most widely used method for training deep nets.", "startOffset": 56, "endOffset": 59}, {"referenceID": 7, "context": "The performance of SGD, however, is very sensitive to this choice of update, leading to adaptive methods that automatically adjust the learning rate as the system learns [8], [9].", "startOffset": 170, "endOffset": 173}, {"referenceID": 8, "context": "The performance of SGD, however, is very sensitive to this choice of update, leading to adaptive methods that automatically adjust the learning rate as the system learns [8], [9].", "startOffset": 175, "endOffset": 178}, {"referenceID": 9, "context": "This dramatically slows down the rate of learning in the initial layers, and slows down convergence of the whole network [10].", "startOffset": 121, "endOffset": 125}, {"referenceID": 0, "context": "Instead, in these problems, there is an exponentially large number of high error saddle points with low curvature [1], [11], [12].", "startOffset": 114, "endOffset": 117}, {"referenceID": 10, "context": "Instead, in these problems, there is an exponentially large number of high error saddle points with low curvature [1], [11], [12].", "startOffset": 119, "endOffset": 123}, {"referenceID": 11, "context": "Instead, in these problems, there is an exponentially large number of high error saddle points with low curvature [1], [11], [12].", "startOffset": 125, "endOffset": 129}, {"referenceID": 8, "context": "\u2022 It requires very little extra computation over standard stochastic gradient methods, and requires no extra storage of previous gradients required as in AdaGrad [9].", "startOffset": 162, "endOffset": 165}, {"referenceID": 12, "context": "The classical momentum method [13] is a technique that increases the learning rate for parameters for which the gradient consistently points in the same direction, while decreasing the learning rate for parameters for which the gradient is changing fast.", "startOffset": 30, "endOffset": 34}, {"referenceID": 0, "context": "where \u03bc \u2208 [0, 1] is called the momentum coefficient, and t > 0 is the global learning rate.", "startOffset": 10, "endOffset": 16}, {"referenceID": 13, "context": "Nesterov\u2019s Accelerated Gradient (NAG) [14], a first order method, has a better convergence rate than gradient descent in certain situations.", "startOffset": 38, "endOffset": 42}, {"referenceID": 14, "context": "Recently, [15] showed that this method can be thought of as a momentum method with the update equation as follows:", "startOffset": 10, "endOffset": 14}, {"referenceID": 14, "context": "Through a carefully designed random initialization and using a particular type of slowly increasing schedule for \u03bc, this method can reach high levels of performance when used on deep networks [15].", "startOffset": 192, "endOffset": 196}, {"referenceID": 8, "context": "A method that has gained popularity is AdaGrad [9], which uses the following update rule:", "startOffset": 47, "endOffset": 50}, {"referenceID": 7, "context": "A method which builds on AdaGrad and attempts to address some of the above-mentioned disadvantages is AdaDelta [8].", "startOffset": 111, "endOffset": 114}, {"referenceID": 7, "context": "However, it does not perform as well as other methods like SGD and AdaGrad in terms of accuracy [8].", "startOffset": 96, "endOffset": 99}, {"referenceID": 9, "context": "Because of the \u201cvanishing gradients\u201d phenomenon, shallow network layers tend to have much smaller gradients than deep layers \u2013 sometimes differing by orders of magnitude from one layer to the next [10].", "startOffset": 197, "endOffset": 201}, {"referenceID": 15, "context": "We use Caffe [16] to implement our method.", "startOffset": 13, "endOffset": 17}, {"referenceID": 1, "context": "SGD was initialized with the learning rate used in [2] for experiments done on ImageNet.", "startOffset": 51, "endOffset": 54}, {"referenceID": 1, "context": "3) ImageNet: We use an implementation of AlexNet [2] in Caffe, a deep convolutional neural network architecture, for comparing our method with other optimization algorithms.", "startOffset": 49, "endOffset": 52}, {"referenceID": 1, "context": "More details regarding the architecture can be found in the paper [2].", "startOffset": 66, "endOffset": 69}, {"referenceID": 1, "context": "Since we use the Caffe implementation of the AlexNet architecture and do not use any data augmentation techniques, our results are slightly lower than those reported in [2].", "startOffset": 169, "endOffset": 172}], "year": 2015, "abstractText": "The increasing complexity of deep learning architectures is resulting in training time requiring weeks or even months. This slow training is due in part to \u201cvanishing gradients,\u201d in which the gradients used by back-propagation are extremely large for weights connecting deep layers (layers near the output layer), and extremely small for shallow layers (near the input layer); this results in slow learning in the shallow layers. Additionally, it has also been shown that in highly non-convex problems, such as deep neural networks, there is a proliferation of high-error low curvature saddle points, which slows down learning dramatically [1]. In this paper, we attempt to overcome the two above problems by proposing an optimization method for training deep neural networks which uses learning rates which are both specific to each layer in the network and adaptive to the curvature of the function, increasing the learning rate at low curvature points. This enables us to speed up learning in the shallow layers of the network and quickly escape high-error low curvature saddle points. We test our method on standard image classification datasets such as MNIST, CIFAR10 and ImageNet, and demonstrate that our method increases accuracy as well as reduces the required training time over standard algorithms.", "creator": "LaTeX with hyperref package"}}}