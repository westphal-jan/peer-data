{"id": "1510.04709", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "15-Oct-2015", "title": "Multilingual Image Description with Neural Sequence Models", "abstract": "In this paper we present an approach to multi-language image description bringing together insights from neural machine translation and neural image description. To create a description of an image for a given target language, our sequence generation models condition on feature vectors from the image, the description from the source language, and/or a multimodal vector computed over the image and a description in the source language. In image description experiments on the IAPR-TC12 dataset of images aligned with English and German sentences, we find significant and substantial improvements in BLEU4 and Meteor scores for models trained over multiple languages, compared to a monolingual baseline.", "histories": [["v1", "Thu, 15 Oct 2015 20:29:21 GMT  (3153kb)", "http://arxiv.org/abs/1510.04709v1", null], ["v2", "Wed, 18 Nov 2015 17:04:35 GMT  (1036kb)", "http://arxiv.org/abs/1510.04709v2", "Under review as a conference paper at ICLR 2016"]], "reviews": [], "SUBJECTS": "cs.CL cs.CV cs.LG cs.NE", "authors": ["desmond elliott", "stella frank", "eva hasler"], "accepted": false, "id": "1510.04709"}, "pdf": {"name": "1510.04709.pdf", "metadata": {"source": "CRF", "title": null, "authors": [], "emails": ["d.elliott@uva.nl", "s.c.frank@uva.nl", "ech57@cam.ac.uk"], "sections": [{"heading": null, "text": "ar Xiv: 151 0.04 709v 1 [cs.C L] 1"}, {"heading": "1 INTRODUCTION", "text": "However, the reasons for automatic image description, such as text-based image search or the provision of old texts to visually impaired people, are also found in other languages. Current image description models are not per se English-language and imply a simple approach to generating a new language: they collect new annotations and build a model for that language. However, the wealth of image description resources for English points to a cross-border resource transfer that we are exploring here. In other words, how can we best use resources for language A if we generate descriptions for language B? In this paper, we present a multilingual image description resource model for the language."}, {"heading": "2 MODELS", "text": "Our models are models for generating neural sequences with additional inputs of either visual or linguistic modalities, or both. We are inspired by the CNN-to-RNN decoding models (Kiros et al., 2014; Donahue et al., 2014; Vinyals et al., 2015; Karpathy & Fei-Fei, 2015; Mao et al., 2015) used for image description, as well as the sequence-to-sequence models (Sutskever et al., 2014) developed for MT. Our main modeling contribution is the connection between these architectures. We present the models in increasing order of complexity to clarify their compositional character, starting with a simple sequence model of words (a neural language model) and concluding with the complete models that use both image and source features."}, {"heading": "2.1 RECURRENT LANGUAGE MODEL (LM)", "text": "The core of our model is a recursive neural network model using word sequences, i.e. a neural language model (LM). The model is trained to predict the next word in the sequence, considering the sequence seen so far. At each time step i for the input sequence w0... n, the input word wi, which is represented as a unified vector over the vocabulary, is embedded in a high-dimensional continuous vector using the learned embedding matrix E (Eqn 1). At embedding, a non-linear function f is used, combined with the previous hidden state hi (Eqn 2). At the output level, the next word oi is predicted via the softmax function over the vocabulary (Eqn 3)."}, {"heading": "2.2 MULTIMODAL LANGUAGE MODEL (MLM)", "text": "The above recurring LM generates word strings conditioned only on the previously seen words and therefore cannot use visual input to describe the image. However, in the multimodal language model (MLM), the sequence generation is additionally dependent on image attributes, which leads to a model that generates word sequences corresponding to the image. The image attributes v (visual) are entered into the model at h0 at the first time step: h0 = f (Whhh \u2212 1 + Whee0 + Whvv) (4)"}, {"heading": "2.3 TRANSLATION MODEL (SOURCE-LM \u2192 TARGET-LM)", "text": "Instead of conditioning an image vector to generate a sentence in a particular target language, we can condition a vector that represents a sentence in a different language. This vector s is the last hidden state extracted from a sequence model above the source language, the source LM. Essentially, this is the sequence-to-sequence model presented in Sutskever et al. (2014), except that the encoder state is given as input to the model and is not used as a special initial state h0. Thus, the initial state for the TARGET LM is now defined as: h0 = f (Whhh \u2212 1 + Whee0 + Whss) (5)."}, {"heading": "2.4 MULTIMODAL TRANSLATION MODEL (SOURCE-MLM \u2192 TARGET-MLM)", "text": "Finally, we can use both image and source language attributes in a combined multimodal translation model. Image attributes can be added on both the source and destination page (SOURCEMLM \u2192 TARGET-MLM) or only on the source or destination page (SOURCE-MLM \u2192 TARGET-LM or SOURCE-LMa yellow building with white columns in the background in yellow building with white columns in background Figure 3: Image 00 / 25 from the IAPR-TC12 dataset with its English or German description. \u2192 TARGET-MLM) The initial state of the TARGET-MLM is, regardless of the source model type: h0 = f (Whhh \u2212 1 + Whee0 + Whss + Whvv) (6)."}, {"heading": "2.5 GENERATING SENTENCES", "text": "Initially, the model is initialized with the special record start token and all visual or source attributes. The word oi with maximum probability is sampled from the model output and used as input token at time step i + 1. This process continues until the model generates the end of record token or a predefined number of timestamps (30)."}, {"heading": "3 METHODOLOGY", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "3.1 DATA", "text": "We are using the IAPR-TC12 dataset, originally introduced in the ImageCLEF object segmentation task and later extended to include complete image descriptions (Grubinger et al., 2006).This dataset contains 20,000 images with multiple sentence descriptions in both English and German. We are conducting experiments with the standard split of 17,665 images for training, of which we reserve 10% for hyperparameter estimates and 1,962 for evaluation. We are using only the first description of each image, because in this dataset the different descriptions cover different aspects of the image. Note that the English descriptions are the originals; the German data has been professionally translated from English. Figure 3 shows an example image bittext tupple from the dataset. The descriptions are downloaded and scanned using the ptbtokenizer.py script from the MSCOCO application types."}, {"heading": "3.2 BASELINES", "text": "MLM: The first baseline is a monolingual image description model, i.e. a multimodal language model without source language attributes. Visual input consists of the CNN image attributes. Source-LM \u2192 TARGET-LM: The second baseline is a sequence-to-sequence Neural Machine Translation model that is trained only on source and destination descriptions without visual attributes. Once the source set has been generated, the final hidden state of the source LM is entered into the TARGET-LM as described in Section 2.1: https: / / github.com / tylin / coco-caption"}, {"heading": "3.3 MULTILINGUAL MULTIMODAL MODEL VARIANTS", "text": "SOURCE-MLM \u2192 TARGET-MLM: This model replaces the LMs in the MT baseline with multimodal language models on the source and destination side. This results in the image attributes being entered twice, with two separately parameterized weight matrices. SOURCE-LM \u2192 TARGET-MLM: In this model, the source language attributes are generated by a pure LM; visual attributes are entered only during target language generation.SOURCE-MLM \u2192 TARGET-LM: Here, visual input is given only to the QUELLE-MLM and the TARGET-LM uses a single input vector from the QUELLE-MLM. This source vector combines both linguistic and visual attributes, displaying the visual attributes within the hidden layer of the QUELLE-MLM."}, {"heading": "3.4 HYPERPARAMETERS", "text": "We use an LSTM (Hochreiter & Schmidhuber, 1997) as f in the recurring language model, the size of the hidden layer is set to 256 dimensions, the word embedding is 256 dimensional and was learned along with other model parameters, we also experimented with larger hidden layers (as well as deeper architectures), which led to improvements, but also took longer to train them. v is the 4096-dimensional penultimate layer of the object recognition network VGG-16 (Simonyan & Zisserman, 2015), which is consistent with recent work in this area."}, {"heading": "3.5 TRAINING AND OPTIMISATION", "text": "Using the ADAM Optimizer (Kingma & Ba, 2014), we trained the models on objective function (cross entropy of predicted words) and on the fact that BLEU4 is only weakly correlated with BLEU4 for MT (Auli et al., 2013) and that BLEU4 is only moderately correlated with human judgments in image description (Elliott & Keller, 2014).In this case, we stop model selection based on BLEU4 early on: If validation BLEU4 has not increased for 10 epochs and the perplexity of the validation language model no longer decreases, training is hamperd.We apply Dropout to image attributes, source attributes, and word representations with p = 0.5 to discourage overadjustments (Srivastava et al., 2014).The objective function includes an L2 regulation term with \u03bb = 1e \u2212 8.All reported results are weighted over benchmarks with a differing score (2010)."}, {"heading": "4 RESULTS", "text": "The results for the description of the image in both German and English are shown in Tables 1 and 2. Example editions can be seen in Figure 42. To our knowledge, these are the first published results for the description of the image in German. Overall, we note that the description of the image in German results in lower BLEU4 and Meteor scores than English. This is most likely due to the more complex German morphology leading to a larger vocabulary. We get a BLEU4 score of 15.8 on the monolingual multimodal data set for English (En-MLM) is consistent with other state-of-theart models that transfer the results to the Flickr8K dataset. We get a BLEU4 score of 15.8 on the Flickr8K dataset, comparable to the 16.0 of Karpathy & Fei-Fei-File (2015), which uses a combination of models and a beam search."}, {"heading": "5 DISCUSSION", "text": "In fact, it is a reactionary, reactionary, reactionary and reactionary party that is able to hide in order to hide, \"he said.\" It is as it is, \"he said."}, {"heading": "6 RELATED WORK", "text": "The flexibility of using these architectures can be seen as a strong point suggesting that the representations learned in these general models are sufficiently powerful to perform well. Another advantage we have taken advantage of in the work presented here is that it becomes relatively easy to make connections between models for different tasks, in this case image description and machine translation. Another advantage we have taken advantage of in the work presented here is that it becomes relatively easy to make connections between models for different tasks (RNN) (Kiros et al., 2014; Donahue et al., 2014; Vinyals et al., 2015; Karpathy & Fei-Fei-Fei, 2015; Mao et al., 2015) The main difference between these models and discrete tuple-based representations for image description is."}, {"heading": "7 CONCLUSIONS", "text": "The main difference between this task and monolingual image description is the availability of references in more than one language compared to multiple reference descriptions. Image description data sets typically contain multiple reference sets, which are essential for capturing linguistic diversity (Rashtchian et al., 2010; Elliott & Keller, 2013; Hodosh et al., 2013; Chen et al., 2015). In our experiments, we have shown that useful image description diversity can be found in other languages rather than in multilingual references. It is now an open question whether the benefits of multilingual references extend to multilingual references. Our approach to using multilingual educational data essentially acts as an encoder decoder model. The encoder captures a multimodal representation of the image and source words."}, {"heading": "ACKNOWLEDGMENTS", "text": "D. Elliott was supported by an Alain Bensoussain Career Development Fellowship. S. Frank is supported by funding from the European Union Research and Innovation Programme under grant agreement no. 645452. We thank Philip Schulz and Khalil Sima'an for discussions and feedback on the work. We built the models using the Keras library, which is based on Theano. We are grateful to the Database Architectures Group at CWI for accessing their K20x GPUs."}], "references": [{"title": "Joint language and translation modeling with recurrent neural networks", "author": ["Auli", "Michael", "Galley", "Michel", "Quirk", "Chris", "Zweig", "Geoffrey"], "venue": "In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing,", "citeRegEx": "Auli et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Auli et al\\.", "year": 2013}, {"title": "Neural machine translation by jointly learning to align and translate", "author": ["Bahdanau", "Dzmitry", "Cho", "Kyunghyun", "Bengio", "Yoshua"], "venue": "In Proceedings of ICLR,", "citeRegEx": "Bahdanau et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2015}, {"title": "Findings of the 2015 workshop on statistical machine translation", "author": ["Bojar", "Ond\u0159ej", "Chatterjee", "Rajen", "Federmann", "Christian", "Haddow", "Barry", "Huck", "Matthias", "Hokamp", "Chris", "Koehn", "Philipp", "Logacheva", "Varvara", "Monz", "Christof", "Negri", "Matteo", "Post", "Matt", "Scarton", "Carolina", "Specia", "Lucia", "Turchi", "Marco"], "venue": "In Proceedings of the Tenth Workshop on Statistical Machine Translation,", "citeRegEx": "Bojar et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Bojar et al\\.", "year": 2015}, {"title": "Building a persistent workforce on Mechanical Turk for multilingual data collection", "author": ["Chen", "David L", "Dolan", "William B"], "venue": "In Proceedings of The 3rd Human Computation Workshop (HCOMP", "citeRegEx": "Chen et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Chen et al\\.", "year": 2011}, {"title": "Microsoft COCO captions: Data collection and evaluation", "author": ["Chen", "Xinlei", "Fang", "Hao", "Lin", "Tsung-Yi", "Vedantam", "Ramakrishna", "Gupta", "Saurabh", "Doll\u00e1r", "Piotr", "Zitnick", "C. Lawrence"], "venue": "server. CoRR,", "citeRegEx": "Chen et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Chen et al\\.", "year": 2015}, {"title": "Better hypothesis testing for statistical machine translation: Controlling for optimizer instability", "author": ["Clark", "Jonathan H", "Dyer", "Chris", "Lavie", "Alon", "Smith", "Noah A"], "venue": "In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies: Short Papers - Volume", "citeRegEx": "Clark et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Clark et al\\.", "year": 2011}, {"title": "Meteor Universal: Language Specific Translation Evaluation for Any Target Language", "author": ["M. Denkowski", "A. Lavie"], "venue": "In EACL Workshop on Statistical Machine Translation,", "citeRegEx": "Denkowski and Lavie,? \\Q2014\\E", "shortCiteRegEx": "Denkowski and Lavie", "year": 2014}, {"title": "Fast and robust neural network joint models for statistical machine translation", "author": ["Devlin", "Jacob", "Zbib", "Rabih", "Huang", "Zhongqiang", "Lamar", "Thomas", "Schwartz", "Richard", "Makhoul", "John"], "venue": "In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),", "citeRegEx": "Devlin et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Devlin et al\\.", "year": 2014}, {"title": "Long-term recurrent convolutional networks for visual recognition and description", "author": ["J. Donahue", "L.A. Hendricks", "S. Guadarrama", "M. Rohrbach", "S. Venugopalan", "K. Saenko", "T. Darrell"], "venue": "CoRR, abs/1411.4389,", "citeRegEx": "Donahue et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Donahue et al\\.", "year": 2014}, {"title": "Describing images using inferred visual dependency representations", "author": ["Elliott", "Desmond", "de Vries", "Arjen P"], "venue": "In ACL,", "citeRegEx": "Elliott et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Elliott et al\\.", "year": 2015}, {"title": "Image Description using Visual Dependency Representations", "author": ["Elliott", "Desmond", "Keller", "Frank"], "venue": "In EMNLP,", "citeRegEx": "Elliott et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Elliott et al\\.", "year": 2013}, {"title": "Comparing Automatic Evaluation Measures for Image Description", "author": ["Elliott", "Desmond", "Keller", "Frank"], "venue": "In ACL,", "citeRegEx": "Elliott et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Elliott et al\\.", "year": 2014}, {"title": "From captions to visual concepts and back", "author": ["Fang", "Hao", "Gupta", "Saurabh", "Iandola", "Forrest", "Srivastava", "Rupesh K", "Deng", "Li", "Dollar", "Piotr", "Gao", "Jianfeng", "He", "Xiaodong", "Mitchell", "Margaret", "Platt", "John C", "C. Lawrence Zitnick", "Zweig", "Geoffrey"], "venue": "In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR),", "citeRegEx": "Fang et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Fang et al\\.", "year": 2015}, {"title": "Every picture tells a story: Generating sentences from images", "author": ["Farhadi", "Ali", "M Hejrati", "Sadeghi", "Mohammad Amin", "P Young", "C Rashtchian", "J Hockenmaier", "Forsyth", "David"], "venue": "In ECCV,", "citeRegEx": "Farhadi et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Farhadi et al\\.", "year": 2010}, {"title": "Image-mediated learning for zero-shot cross-lingual document retrieval", "author": ["Funaki", "Ruka", "Nakayama", "Hideki"], "venue": "In Empirical Methods in Natural Language Processing", "citeRegEx": "Funaki et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Funaki et al\\.", "year": 2015}, {"title": "Understanding the difficulty of training deep feedforward neural networks", "author": ["Glorot", "Xavier", "Bengio", "Yoshua"], "venue": "In International Conference on Artificial Intelligence and Statistics,", "citeRegEx": "Glorot et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Glorot et al\\.", "year": 2010}, {"title": "The IAPR TC-12 benchmark: A new evaluation resource for visual information systems", "author": ["M. Grubinger", "P.D. Clough", "H. Muller", "D. Thomas"], "venue": "In LREC,", "citeRegEx": "Grubinger et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Grubinger et al\\.", "year": 2006}, {"title": "Long short-term memory", "author": ["Hochreiter", "Sepp", "Schmidhuber", "J\u00fcrgen"], "venue": "Neural Comput.,", "citeRegEx": "Hochreiter et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Hochreiter et al\\.", "year": 1997}, {"title": "Framing Image Description as a Ranking Task: Data, Models and Evaluation Metrics", "author": ["Hodosh", "Micah", "P Young", "J. Hockenmaier"], "venue": "Journal of Artificial Intelligence Research,", "citeRegEx": "Hodosh et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Hodosh et al\\.", "year": 2013}, {"title": "Montreal neural machine translation systems for wmt\u201915", "author": ["Jean", "S\u00e9bastien", "Firat", "Orhan", "Cho", "Kyunghyun", "Memisevic", "Roland", "Bengio", "Yoshua"], "venue": "In Proceedings of the Tenth Workshop on Statistical Machine Translation,", "citeRegEx": "Jean et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Jean et al\\.", "year": 2015}, {"title": "Deep visual-semantic alignments for generating image descriptions", "author": ["Karpathy", "Andrej", "Fei-Fei", "Li"], "venue": null, "citeRegEx": "Karpathy et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Karpathy et al\\.", "year": 2015}, {"title": "Learning Image Embeddings using Convolutional Neural Networks for Improved Multi-Modal Semantics", "author": ["Kiela", "Douwe", "Bottou", "L\u00e9on"], "venue": "In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP-14),", "citeRegEx": "Kiela et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Kiela et al\\.", "year": 2014}, {"title": "Adam: A method for stochastic optimization", "author": ["Kingma", "Diederik P", "Ba", "Jimmy"], "venue": "CoRR, abs/1412.6980,", "citeRegEx": "Kingma et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Kingma et al\\.", "year": 2014}, {"title": "Multimodal neural language models", "author": ["Kiros", "Ryan", "Salakhutdinov", "Ruslan", "Zemel", "Rich"], "venue": "In ICML, pp", "citeRegEx": "Kiros et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Kiros et al\\.", "year": 2014}, {"title": "Imagenet classification with deep convolutional neural networks", "author": ["Krizhevsky", "Alex", "Sutskever", "Ilya", "Hinton", "Geoffrey E"], "venue": "Advances in Neural Information Processing Systems", "citeRegEx": "Krizhevsky et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Krizhevsky et al\\.", "year": 2012}, {"title": "Continuous space translation models with neural networks", "author": ["Le", "Hai-Son", "Allauzen", "Alexandre", "Yvon", "Francois"], "venue": "In Proceedings of NAACL HLT,", "citeRegEx": "Le et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Le et al\\.", "year": 2012}, {"title": "Composing simple image descriptions using web-scale n-grams", "author": ["S Li", "G Kulkarni", "T L Berg", "A C Berg", "Choi", "Y Young"], "venue": "In CoNLL,", "citeRegEx": "Li et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Li et al\\.", "year": 2011}, {"title": "Deep captioning with multimodal recurrent neural networks (m-RNN)", "author": ["Mao", "Junhua", "Xu", "Wei", "Yang", "Yi", "Wang", "Jiang", "Huang", "Zhiheng", "Yuille", "Alan"], "venue": null, "citeRegEx": "Mao et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Mao et al\\.", "year": 2015}, {"title": "Midge: generating image descriptions from computer vision detections", "author": ["Mitchell", "Margaret", "Han", "Xufeng", "Dodge", "Jesse", "Mensch", "Alyssa", "Goyal", "Amit", "A C Berg", "Yamaguchi", "Kota", "T L Berg", "Stratos", "Karl", "III Daume", "Hal", "III"], "venue": "In EACL,", "citeRegEx": "Mitchell et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Mitchell et al\\.", "year": 2012}, {"title": "BLEU: A method for automatic evaluation of machine translation", "author": ["K. Papineni", "S. Roukos", "T. Ward", "Zhu", "W.-J"], "venue": "In ACL,", "citeRegEx": "Papineni et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Papineni et al\\.", "year": 2002}, {"title": "Collecting image annotations using Amazon\u2019s Mechanical Turk", "author": ["C. Rashtchian", "P. Young", "M. Hodosh", "J. Hockenmaier"], "venue": "In NAACLHLT Workshop on Creating Speech and Language Data with Amazon\u2019s Mechanical Turk,", "citeRegEx": "Rashtchian et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Rashtchian et al\\.", "year": 2010}, {"title": "Continuous space translation models for phrase-based statistical machine translation", "author": ["Schwenk", "Holger"], "venue": "Proceedings of COLING (Posters),", "citeRegEx": "Schwenk and Holger.,? \\Q2012\\E", "shortCiteRegEx": "Schwenk and Holger.", "year": 2012}, {"title": "Learning grounded meaning representations with autoencoders", "author": ["Silberer", "Carina", "Lapata", "Mirella"], "venue": "In Proceedings of ACL,", "citeRegEx": "Silberer et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Silberer et al\\.", "year": 2014}, {"title": "Very deep convolutional networks for large-scale image recognition", "author": ["Simonyan", "Karen", "Zisserman", "Andrew"], "venue": "In ICLR \u201915,", "citeRegEx": "Simonyan et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Simonyan et al\\.", "year": 2015}, {"title": "Dropout: A simple way to prevent neural networks from overfitting", "author": ["Srivastava", "Nitish", "Hinton", "Geoffrey", "Krizhevsky", "Alex", "Sutskever", "Ilya", "Salakhutdinov", "Ruslan"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Srivastava et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Srivastava et al\\.", "year": 2014}, {"title": "Sequence to sequence learning with neural networks", "author": ["Sutskever", "Ilya", "Vinyals", "Oriol", "Le", "Quoc V. V"], "venue": "Advances in Neural Information Processing Systems", "citeRegEx": "Sutskever et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Sutskever et al\\.", "year": 2014}, {"title": "Accelerating t-sne using tree-based algorithms", "author": ["L.J.P. van der Maaten"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Maaten,? \\Q2014\\E", "shortCiteRegEx": "Maaten", "year": 2014}, {"title": "Show and tell: A neural image caption generator", "author": ["Vinyals", "Oriol", "Toshev", "Alexander", "Bengio", "Samy", "Erhan", "Dumitru"], "venue": "In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR),", "citeRegEx": "Vinyals et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Vinyals et al\\.", "year": 2015}, {"title": "Show, attend and tell: Neural image caption generation with visual attention", "author": ["Xu", "Kelvin", "Ba", "Jimmy", "Kiros", "Ryan", "Cho", "Kyunghyun", "Courville", "Aaron C", "Salakhutdinov", "Ruslan", "Zemel", "Richard S", "Bengio", "Yoshua"], "venue": "CoRR, abs/1502.03044,", "citeRegEx": "Xu et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Xu et al\\.", "year": 2015}, {"title": "Corpus-guided sentence generation of natural images", "author": ["Y Yang", "C L Teo", "III Daume", "Hal", "Y. Aloimonos"], "venue": "In EMNLP,", "citeRegEx": "Yang et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Yang et al\\.", "year": 2011}, {"title": "See no evil, say no evil: Description generation from densely labeled", "author": ["M Yatskar", "L Vanderwende", "L. Zettlemoyer"], "venue": "images. SEM,", "citeRegEx": "Yatskar et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Yatskar et al\\.", "year": 2014}], "referenceMentions": [{"referenceID": 37, "context": "Our multilingual multimodal language model extends a monolingual neural image description model (Vinyals et al., 2015) with features extracted from the source language, in an architecture similar to sequence-to-sequence machine translation (Sutskever et al.", "startOffset": 96, "endOffset": 118}, {"referenceID": 35, "context": ", 2015) with features extracted from the source language, in an architecture similar to sequence-to-sequence machine translation (Sutskever et al., 2014).", "startOffset": 129, "endOffset": 153}, {"referenceID": 23, "context": "We draw inspiration from the CNN-to-RNN decoding models (Kiros et al., 2014; Donahue et al., 2014; Vinyals et al., 2015; Karpathy & Fei-Fei, 2015; Mao et al., 2015), used for image description, as well as the sequence-to-sequence models (Sutskever et al.", "startOffset": 56, "endOffset": 164}, {"referenceID": 8, "context": "We draw inspiration from the CNN-to-RNN decoding models (Kiros et al., 2014; Donahue et al., 2014; Vinyals et al., 2015; Karpathy & Fei-Fei, 2015; Mao et al., 2015), used for image description, as well as the sequence-to-sequence models (Sutskever et al.", "startOffset": 56, "endOffset": 164}, {"referenceID": 37, "context": "We draw inspiration from the CNN-to-RNN decoding models (Kiros et al., 2014; Donahue et al., 2014; Vinyals et al., 2015; Karpathy & Fei-Fei, 2015; Mao et al., 2015), used for image description, as well as the sequence-to-sequence models (Sutskever et al.", "startOffset": 56, "endOffset": 164}, {"referenceID": 27, "context": "We draw inspiration from the CNN-to-RNN decoding models (Kiros et al., 2014; Donahue et al., 2014; Vinyals et al., 2015; Karpathy & Fei-Fei, 2015; Mao et al., 2015), used for image description, as well as the sequence-to-sequence models (Sutskever et al.", "startOffset": 56, "endOffset": 164}, {"referenceID": 35, "context": ", 2015), used for image description, as well as the sequence-to-sequence models (Sutskever et al., 2014) developed for MT.", "startOffset": 80, "endOffset": 104}, {"referenceID": 35, "context": "This is essentially the sequence-to-sequence model presented in Sutskever et al. (2014), except that the encoder state is given as input to the model, rather than being used as a special initial h0 state.", "startOffset": 64, "endOffset": 88}, {"referenceID": 16, "context": "We use the IAPR-TC12 dataset, originally introduced in the ImageCLEF shared task for object segmentation and later expanded with complete image descriptions (Grubinger et al., 2006).", "startOffset": 157, "endOffset": 181}, {"referenceID": 27, "context": "1 Mao et al. (2015) 20.", "startOffset": 2, "endOffset": 20}, {"referenceID": 0, "context": "It is known that language model perplexity is only weakly correlated with BLEU4 for MT (Auli et al., 2013), and that BLEU4 is only moderately correlated with human judgements for image description (Elliott & Keller, 2014).", "startOffset": 87, "endOffset": 106}, {"referenceID": 34, "context": "5 to discourage overfitting (Srivastava et al., 2014).", "startOffset": 28, "endOffset": 53}, {"referenceID": 29, "context": "We report image description quality using BLEU4 (Papineni et al., 2002), Meteor (Denkowski & Lavie, 2014), and language-model perplexity.", "startOffset": 48, "endOffset": 71}, {"referenceID": 5, "context": "The BLEU4 and Meteor scores are calculated using MultEval (Clark et al., 2011).", "startOffset": 58, "endOffset": 78}, {"referenceID": 27, "context": "On the IAPR-TC12 dataset, the En MLM performs worse than Mao et al. (2015), but their model is trained and evaluated on all reference descriptions.", "startOffset": 57, "endOffset": 75}, {"referenceID": 27, "context": "On the IAPR-TC12 dataset, the En MLM performs worse than Mao et al. (2015), but their model is trained and evaluated on all reference descriptions. All multilingual models beat the monolingual image description baseline, by a margin of up to 8.9 BLEU4 and 8.8 Meteor points for the best models. The best model variant outperforms Mao et al. (2015) by 2.", "startOffset": 57, "endOffset": 348}, {"referenceID": 23, "context": "Deep neural networks for automatic image description typically estimate a joint image-sentence representation in a multimodal recurrent neural network (RNN) (Kiros et al., 2014; Donahue et al., 2014; Vinyals et al., 2015; Karpathy & Fei-Fei, 2015; Mao et al., 2015).", "startOffset": 157, "endOffset": 265}, {"referenceID": 8, "context": "Deep neural networks for automatic image description typically estimate a joint image-sentence representation in a multimodal recurrent neural network (RNN) (Kiros et al., 2014; Donahue et al., 2014; Vinyals et al., 2015; Karpathy & Fei-Fei, 2015; Mao et al., 2015).", "startOffset": 157, "endOffset": 265}, {"referenceID": 37, "context": "Deep neural networks for automatic image description typically estimate a joint image-sentence representation in a multimodal recurrent neural network (RNN) (Kiros et al., 2014; Donahue et al., 2014; Vinyals et al., 2015; Karpathy & Fei-Fei, 2015; Mao et al., 2015).", "startOffset": 157, "endOffset": 265}, {"referenceID": 27, "context": "Deep neural networks for automatic image description typically estimate a joint image-sentence representation in a multimodal recurrent neural network (RNN) (Kiros et al., 2014; Donahue et al., 2014; Vinyals et al., 2015; Karpathy & Fei-Fei, 2015; Mao et al., 2015).", "startOffset": 157, "endOffset": 265}, {"referenceID": 13, "context": "The main difference between these models and discrete tuple-based representations for image description (Farhadi et al., 2010; Yang et al., 2011; Li et al., 2011; Mitchell et al., 2012; Elliott & Keller, 2013; Yatskar et al., 2014; Elliott & de Vries, 2015) is that it is not necessary to explicitly define the joint representation; the structure of the neural network can be used to estimate the optimal joint representation for the description task.", "startOffset": 104, "endOffset": 257}, {"referenceID": 39, "context": "The main difference between these models and discrete tuple-based representations for image description (Farhadi et al., 2010; Yang et al., 2011; Li et al., 2011; Mitchell et al., 2012; Elliott & Keller, 2013; Yatskar et al., 2014; Elliott & de Vries, 2015) is that it is not necessary to explicitly define the joint representation; the structure of the neural network can be used to estimate the optimal joint representation for the description task.", "startOffset": 104, "endOffset": 257}, {"referenceID": 26, "context": "The main difference between these models and discrete tuple-based representations for image description (Farhadi et al., 2010; Yang et al., 2011; Li et al., 2011; Mitchell et al., 2012; Elliott & Keller, 2013; Yatskar et al., 2014; Elliott & de Vries, 2015) is that it is not necessary to explicitly define the joint representation; the structure of the neural network can be used to estimate the optimal joint representation for the description task.", "startOffset": 104, "endOffset": 257}, {"referenceID": 28, "context": "The main difference between these models and discrete tuple-based representations for image description (Farhadi et al., 2010; Yang et al., 2011; Li et al., 2011; Mitchell et al., 2012; Elliott & Keller, 2013; Yatskar et al., 2014; Elliott & de Vries, 2015) is that it is not necessary to explicitly define the joint representation; the structure of the neural network can be used to estimate the optimal joint representation for the description task.", "startOffset": 104, "endOffset": 257}, {"referenceID": 40, "context": "The main difference between these models and discrete tuple-based representations for image description (Farhadi et al., 2010; Yang et al., 2011; Li et al., 2011; Mitchell et al., 2012; Elliott & Keller, 2013; Yatskar et al., 2014; Elliott & de Vries, 2015) is that it is not necessary to explicitly define the joint representation; the structure of the neural network can be used to estimate the optimal joint representation for the description task.", "startOffset": 104, "endOffset": 257}, {"referenceID": 24, "context": "As in our MLM, the image\u2013sentence representation in the multimodal RNN is initialised with image features from the penultimate fully-connected layer of a convolutional neural network (CNN) trained for multi-class object recognition (Krizhevsky et al., 2012).", "startOffset": 232, "endOffset": 257}, {"referenceID": 27, "context": "Alternative formulations input the image features into the model at each timestep (Mao et al., 2015), or first detect the objects in an image and generate sentences using a maximum-entropy language model (Fang et al.", "startOffset": 82, "endOffset": 100}, {"referenceID": 12, "context": ", 2015), or first detect the objects in an image and generate sentences using a maximum-entropy language model (Fang et al., 2015).", "startOffset": 111, "endOffset": 130}, {"referenceID": 25, "context": "In the domain of machine translation, a greater variety of neural models have been used for subtasks within the MT pipeline, such as neural network language models (Schwenk, 2012) or joint translation and language models for re-ranking in phrase-based translation models (Le et al., 2012; Auli et al., 2013) or directly during decoding (Devlin et al.", "startOffset": 271, "endOffset": 307}, {"referenceID": 0, "context": "In the domain of machine translation, a greater variety of neural models have been used for subtasks within the MT pipeline, such as neural network language models (Schwenk, 2012) or joint translation and language models for re-ranking in phrase-based translation models (Le et al., 2012; Auli et al., 2013) or directly during decoding (Devlin et al.", "startOffset": 271, "endOffset": 307}, {"referenceID": 7, "context": ", 2013) or directly during decoding (Devlin et al., 2014).", "startOffset": 36, "endOffset": 57}, {"referenceID": 35, "context": "More recently, end-to-end neural MT systems using Long Short-Term Memory Networks and Gated Recurrent Units have been proposed as Encoder-Decoder models for translation (Sutskever et al., 2014; Bahdanau et al., 2015), and have proven to be highly effective (Bojar et al.", "startOffset": 169, "endOffset": 216}, {"referenceID": 1, "context": "More recently, end-to-end neural MT systems using Long Short-Term Memory Networks and Gated Recurrent Units have been proposed as Encoder-Decoder models for translation (Sutskever et al., 2014; Bahdanau et al., 2015), and have proven to be highly effective (Bojar et al.", "startOffset": 169, "endOffset": 216}, {"referenceID": 2, "context": ", 2015), and have proven to be highly effective (Bojar et al., 2015; Jean et al., 2015).", "startOffset": 48, "endOffset": 87}, {"referenceID": 19, "context": ", 2015), and have proven to be highly effective (Bojar et al., 2015; Jean et al., 2015).", "startOffset": 48, "endOffset": 87}, {"referenceID": 30, "context": "Image description datasets typically include multiple reference sentences, which are essential for capturing linguistic diversity (Rashtchian et al., 2010; Elliott & Keller, 2013; Hodosh et al., 2013; Chen et al., 2015).", "startOffset": 130, "endOffset": 219}, {"referenceID": 18, "context": "Image description datasets typically include multiple reference sentences, which are essential for capturing linguistic diversity (Rashtchian et al., 2010; Elliott & Keller, 2013; Hodosh et al., 2013; Chen et al., 2015).", "startOffset": 130, "endOffset": 219}, {"referenceID": 4, "context": "Image description datasets typically include multiple reference sentences, which are essential for capturing linguistic diversity (Rashtchian et al., 2010; Elliott & Keller, 2013; Hodosh et al., 2013; Chen et al., 2015).", "startOffset": 130, "endOffset": 219}, {"referenceID": 1, "context": "It would also be interesting to explore attention-based recurrent neural networks, which have recently been used for machine translation (Bahdanau et al., 2015; Jean et al., 2015) and image description (Xu et al.", "startOffset": 137, "endOffset": 179}, {"referenceID": 19, "context": "It would also be interesting to explore attention-based recurrent neural networks, which have recently been used for machine translation (Bahdanau et al., 2015; Jean et al., 2015) and image description (Xu et al.", "startOffset": 137, "endOffset": 179}, {"referenceID": 38, "context": ", 2015) and image description (Xu et al., 2015).", "startOffset": 30, "endOffset": 47}], "year": 2017, "abstractText": "In this paper we present an approach to multi-language image description bringing together insights from neural machine translation and neural image description. To create a description of an image for a given target language, our sequence generation models condition on feature vectors from the image, the description from the source language, and/or a multimodal vector computed over the image and a description in the source language. In image description experiments on the IAPR-TC12 dataset of images aligned with English and German sentences, we find significant and substantial improvements in BLEU4 and Meteor scores for models trained over multiple languages, compared to a monolingual baseline.", "creator": "LaTeX with hyperref package"}}}