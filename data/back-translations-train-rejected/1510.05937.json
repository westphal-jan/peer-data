{"id": "1510.05937", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "20-Oct-2015", "title": "Binary Speaker Embedding", "abstract": "The popular i-vector model represents speakers as lowdimensional continuous vectors (i-vectors), and hence is a way of continuous speaker embedding. In this paper, we investigate binary speaker embedding, which transforms ivectors to binary vectors (codes) by a hash function. We start from locality sensitive hashing (LSH), a simple binarization approach where binary codes are derived from a set of random hash functions. A potential problem of LSH is that the randomly sampled hash functions might be suboptimal, we therefore propose an improved Hamming distance learning approach, where the hash function is learned by a variablesized block training that projects each dimension of the original i-vectors to variable-sized binary codes independently. Our experiments show that binary speaker embedding can deliver competitive or even better results on both speaker verification and identification tasks, while the memory usage and the computation cost are significant reduced.", "histories": [["v1", "Tue, 20 Oct 2015 15:49:59 GMT  (94kb)", "https://arxiv.org/abs/1510.05937v1", null], ["v2", "Thu, 31 Mar 2016 05:33:49 GMT  (28kb)", "http://arxiv.org/abs/1510.05937v2", null]], "reviews": [], "SUBJECTS": "cs.SD cs.LG", "authors": ["lantian li", "dong wang", "chao xing", "kaimin yu", "thomas fang zheng"], "accepted": false, "id": "1510.05937"}, "pdf": {"name": "1510.05937.pdf", "metadata": {"source": "CRF", "title": "Binary Speaker Embedding", "authors": ["Lantian Li", "Chao Xing", "Dong Wang", "Kaimin Yu", "Thomas Fang Zheng"], "emails": ["lilt@cslt.riit.tsinghua.edu.cn;", "xingchao@cslt.riit.tsinghua.edu.cn;", "yukm@cslt.riit.tsinghua.edu.cn;", "wangdong99@mails.tsinghua.edu.cn", "fzheng@tsinghua.edu.cn"], "sections": [{"heading": null, "text": "ar Xiv: 151 0.05 937v 2 [cs.S D] 31 Mar 201 6-dimensional continuous vectors (i-vectors), and therefore it is a possibility of continuous embedding of loudspeakers. In this thesis, we will examine binary speaker embedding, which transforms i-vectors into binary vectors (codes) by a hash function. We will start with locality-sensitive hashing (LSH), a simple binary approach in which binary codes are derived from a series of random hash functions. A potential problem with LSH is that the randomly sampled hash functions may be suboptimal. We therefore propose an improved approach to hamming remote learning, in which the hash function is learned through a variable-sized block training, in which each dimension of the original i-vectors is converted into variable-sized binary codes. Our experiments show that binary speaker embedding tasks can significantly reduce the number of compatible loudspeakers, or even improve the results of competitive use during fication."}, {"heading": "1. Introduction", "text": "This year it is more than ever before."}, {"heading": "2. Related work", "text": "Limited research focuses on exploiting the advantages of binary codes in terms of robustness and fast computing. [10] For example, [11] proposed a time-spectral binary masking approach to improve the robustness of loudspeaker detection under high interference conditions. In addition, [11] presented a solution for the large-scale search and indexing of loudspeakers under the vector model, where the search and indexing algorithm is based on LSH. [12] The work proposed in this paper is more relevant to our proposal. Through its approach, a universal background model (UBM) is used to divide acoustic space into sub-regions, and each sub-region is populated with a series of Gaussian components. Each acoustic frame is then converted into a binary vector by evaluating the Gaussian components to which the frame belongs, and the frame-level vectors are eventually accumulated to produce the segment loudspeaker sector."}, {"heading": "3. Binary speaker embedding with LSH", "text": "Basically, the continuous i-vectors are projected onto binary codes in such a way that the distance between i-vectors is largely maintained by the binary codes. We consider the cosinal distance for i-vectors (which is the simplest and most effective for speaker recognition) and the hamming distance for binary codes (the most popular distance measure for binary codes).The LSH approach [6, 7, 8] seeks a length-normalized i-vector function, and the similarity between i-vectors is measured by cosinal distance. Our goal is to project a continuous vector x onto a binary code h (x) of b-bits. The LSH approach [6, 7, 8] looks for a hash function that works on x, so that more similar i-vectors have more chance of hashing.We apply a simple LSH-xi approach in which the hash functions are activated by combining (each cosi-1) with a random (x) result."}, {"heading": "4. Binary embedding with variable-sized block training", "text": "A potential problem with LSH embedding is that x is not necessarily evenly distributed across the hypersphere, and therefore the uniformly captured hash functions may be suboptimal. A better approach is to derive the hash function by learning from data. An interesting method in this category is the hamming distance learning suggested by [9]. This section first presents this approach and then suggests a variably sized block training method that can improve training speed and quality."}, {"heading": "4.1. Hamming distance learning", "text": "The hamming approach of distance learning [9] learns a projection function f (x; w), where x is the input (an i vector in our case) and w is the model parameter. Once the projection function is learned, the binary code for x is obtained simply by b (x; w) = character (f (x; w). Choosing another f leads to different learning methods. The simple linear model f (x; w) = wTx is chosen in this study. Note that if w is randomly sampled from N (0; I) and no training is performed, this approach gives the LSH. Hamming distance learning defines a loss function on triplets (x, x +, x \u2212) where x is an i vector of a particular speaker, x + is another i vector of the same speaker derived from a different language segment, and x \u2212 is the i vector of a stacker."}, {"heading": "4.2. Variable-sized block training", "text": "A particular problem of hamming distance learning is the high computing requirement when the dimensions of the continuous and / or binary vector are large. Furthermore, the learning algorithm treats each dimension of the input continuous vector equally, which is not optimal for the LDA-projected i-vectors, for which the low dimensions contain more discriminatory information. We propose a variably large blocking training approach to solve this problem. Considering that the expected number of bits of the binary codes is b, we hope that these bits are unevenly distributed among the dimensions of the original i-vectors and are subject to the constraints resulting from Di = 1 Ti = b, where D is the dimensionality of the original ivectors, and Ti is the number of bits associated with the dimension i. Ti is designed so that the linear descent of the I-vectors is performed as follows: Ti + 1 T1 = T1 iD \u2212 D \u2212 D (Ti = iD \u2212 D \u2212 D \u2212 D)."}, {"heading": "5. Experiments", "text": "We first present the data and configurations used in the experiments, and then report on the results in terms of verification and identification tasks. 5.1. Data \u2022 Development data: - Fisher: 7, 196 female speakers with 13, 287 expressions were used to train the i-vector, LDA models. The same data was also used to perform block training with variable size. \u2022 Evaluation data: - NIST SRE08: The data of the core test NIST SRE08 in short2 and short3 [14] were used for speaker verification evaluation. It consists of 1, 997 female enrolment expressions and 3, 858 test expressions. We constructed 59, 343 studies based on the database, including 12, 159 target studies and 47, 184 fraudulent trialities. - WSJ: The database was used for the evaluation of speaker identification. It consists of 2843 studies selected for the pretractors, 317 for the 317 and 317 for the 317."}, {"heading": "5.2. Experimental setup", "text": "The acoustic characteristic comprised 19-dimensional Mel frequency receiver coefficients (MFCCs) together with the log energy; the first and second order derivatives were added to the static characteristics, resulting in 60-dimensional feature vectors; the UBM comprised 2 048 Gaussian components and was trained with approximately 8000 female expressions randomly selected from the Fisher database; the dimensionality of the i-vectors was 400; the LDA model was trained with expressions by 7 196 female speakers, also randomly selected from the Fischer database; and the dimensionality of the LDA projection space was set at 150."}, {"heading": "5.3. Speaker verification task", "text": "The first experiment examines the performance of binary loudspeakers embedded in the speaker verification task. All Ivectors were transformed by LDA, and the dimensionality is 150. Performance is evaluated in terms of equal error rate (EER) under the NIST SRE08 evaluation set, and the results are shown in Table 1 for the LSH approach and Table 2 for variable size block training. In each table, performance with binary codes (referred to as \"b vector\") of different sizes is reported. Note that we did not specify the time cost in this experiment, as the calculation does not pose a serious problem in the verification of loudspeakers, although binary codes are certainly faster.From the results in Table 1 and Table 2, it can be observed that binary vectors can achieve performance comparable to conventional i-vectors, as the binary codes form the binary codes (for example, the largest binary codes)."}, {"heading": "5.4. Speaker identification task", "text": "The advantage of binary embedding is more evident on the speaker identification task, where significant calculation is required when calculating the k-next candidates of a given loudspeaker vector. We use the WSJ database for evaluation, which contains 282 speakers and 35,907 target attempts. For each study (x, y), V, where V is the loudspeaker correspondence, x is the enrollment loudspeaker vector and y is the test speaker vector. In loudspeaker identification, in the face of a test expression y, whose loudspeaker vector is x, the task is to search for the k-next loudspeaker vectors by x. If a vector is y in the k-next candidate and (x, y) is set V in the loudspeaker correspondence, then a top k hit is achieved. We evaluate the performance of loudspeaker identification by the top k precision, which is defined as the share of the top k hits in all studies."}, {"heading": "6. Conclusions", "text": "We have studied two binarization approaches, one based on LSH and the other on hamming distance learning. Our experiments on both verification and loudspeaker identification show that binary loudspeaker vectors with smaller vectors and less computing power can provide competitive results compared to conventional i-vectors. This is particularly true of the proposed variable block training algorithm, an extension of the conventional remote hamming method. Although it has not completely surpassed the continuous vectors, the embedding of binary loudspeakers proposed in this paper is still very promising. Future work will examine more powerful methods for learning hash function and methods for learning binary vectors directly from speech signals."}, {"heading": "7. References", "text": "[1] P. Kenny, G. Boulianne, P. Ouellet, and P. Dumouchel, \"Jointfactor analysis versus eigenchannels in speaker recognition,\" IEEE Transactions on Audio, Speech, and Language Processing, vol. 15, pp. 1435-1447, 2007. [2] - \"Speaker and session variability in gmm-based speaker verification,\" IEEE Transactions on Audio, Speech, and Language Processing, vol. 1448-1460, 2007. [3] N. Dehak, P. Kenny, R. Dehak, P. Ouellet, and P. Dumouchel \"Front-end factor analysis for speaker verification,\" IEEE Transactions on Audio, Speech and Language Processing, vol. 19, pp. 788-798, 2011. [4] A. O. Hatch, S. Kajarekar, and A. Stolcke. \""}], "references": [{"title": "Joint factor analysis versus eigenchannels in speaker recognition", "author": ["P. Kenny", "G. Boulianne", "P. Ouellet", "P. Dumouchel"], "venue": "IEEE Transactions on Audio, Speech, and Language Processing, vol. 15, pp. 1435\u20131447, 2007.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2007}, {"title": "Speaker and session variability in gmm-based speaker verification", "author": ["\u2014\u2014"], "venue": "IEEE Transactions on Audio, Speech, and Language Processing, vol. 15, pp. 1448\u20131460, 2007.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2007}, {"title": "Front-end factor analysis for speaker verification", "author": ["N. Dehak", "P. Kenny", "R. Dehak", "P. Ouellet", "P. Dumouchel"], "venue": "IEEE Transactions on Audio, Speech and Language Processing, vol. 19, no. 4, pp. 788\u2013798, 2011.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2011}, {"title": "Within-class covariance normalization for svm-based speaker recognition", "author": ["A.O. Hatch", "S. Kajarekar", "A. Stolcke"], "venue": "Interspeech, 2006.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2006}, {"title": "Probabilistic linear discriminant analysis", "author": ["S. Ioffe"], "venue": "Computer Vision ECCV 2006, Springer Berlin Heidelberg, pp. 531\u2013542, 2006.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2006}, {"title": "Similarity search in high dimensions via hashing", "author": ["A. Gionis", "P. Indyk", "R. Motwani"], "venue": "VLDB, vol. 99, 1999, pp. 518\u2013529.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 1999}, {"title": "Similarity estimation techniques from rounding algorithms", "author": ["M.S. Charikar"], "venue": "Proceedings of the thiry-fourth annual ACM symposium on Theory of computing. ACM, 2002, pp. 380\u2013388.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2002}, {"title": "Near-optimal hashing algorithms for approximate nearest neighbor in high dimensions", "author": ["A. Andoni", "P. Indyk"], "venue": "Foundations of Computer Science, 2006. FOCS\u201906. 47th Annual IEEE Symposium on. IEEE, 2006, pp. 459\u2013468.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2006}, {"title": "Hamming distance metric learning", "author": ["M. Norouziy", "D.J. Fleety", "R. Salakhutdinovy"], "venue": "NIPS, pp. 1061\u20131069, 2012.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2012}, {"title": "Robust speaker recognition using binary time-frequency masks", "author": ["Y. Shao", "D. Wang"], "venue": "Acoustics, Speech and Signal Processing, 2006. ICASSP 2006 Proceedings. 2006 IEEE International Conference on, vol. 1. IEEE, 2006, pp. I\u2013I.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2006}, {"title": "Random projections for large-scale speaker search", "author": ["R. Leary", "W. Andrews"], "venue": "Interspeech, 2014.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2014}, {"title": "Discriminant binary data representation for speaker recognition", "author": ["J.-F. Bonastre", "P.-M. Bousquet", "D. Matrouf", "X. Anguera"], "venue": "Acoustics, Speech and Signal Processing (ICASSP), 2011 IEEE International Conference on. IEEE, 2011, pp. 5284\u20135287.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2011}, {"title": "Improved approximation algorithms for maximum cut and satisfiability problems using semidefinite programming", "author": ["M.X. Goemans", "D.P. Williamson"], "venue": "Journal of the ACM (JACM), vol. 42, no. 6, pp. 1115\u20131145, 1995.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 1995}, {"title": "the nist year 2008 speaker recognition evaluation plan", "author": ["NIST"], "venue": "Online: http://www.itl.nist.gov/iad/mig/tests/sre/2008/ sre08 evalplan release4.pdf, 2008.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2008}, {"title": "Approximate nearest neighbors: towards removing the curse of dimensionality", "author": ["P. Indyk", "R. Motwani"], "venue": "Proceedings of the thirtieth annual ACM symposium on Theory of computing. ACM, 1998, pp. 604\u2013613.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 1998}], "referenceMentions": [{"referenceID": 0, "context": "The popular i-vector model for speaker recognition assumes that a speech segment can be represented as a low-dimensional continuous vector (i-vector) in a subspace that involves both speaker and channel variances [1, 2].", "startOffset": 213, "endOffset": 219}, {"referenceID": 1, "context": "The popular i-vector model for speaker recognition assumes that a speech segment can be represented as a low-dimensional continuous vector (i-vector) in a subspace that involves both speaker and channel variances [1, 2].", "startOffset": 213, "endOffset": 219}, {"referenceID": 2, "context": ", linear discriminant analysis (LDA) [3], within-class covariance normalization (WCCN) [4], probabilistic linear discriminant analysis (PLDA) [5].", "startOffset": 37, "endOffset": 40}, {"referenceID": 3, "context": ", linear discriminant analysis (LDA) [3], within-class covariance normalization (WCCN) [4], probabilistic linear discriminant analysis (PLDA) [5].", "startOffset": 87, "endOffset": 90}, {"referenceID": 4, "context": ", linear discriminant analysis (LDA) [3], within-class covariance normalization (WCCN) [4], probabilistic linear discriminant analysis (PLDA) [5].", "startOffset": 142, "endOffset": 145}, {"referenceID": 5, "context": "We start from the simple binary embedding method based on locality sensitive hashing (LSH) [6, 7, 8], and then extend to a Hamming distance learning method [9].", "startOffset": 91, "endOffset": 100}, {"referenceID": 6, "context": "We start from the simple binary embedding method based on locality sensitive hashing (LSH) [6, 7, 8], and then extend to a Hamming distance learning method [9].", "startOffset": 91, "endOffset": 100}, {"referenceID": 7, "context": "We start from the simple binary embedding method based on locality sensitive hashing (LSH) [6, 7, 8], and then extend to a Hamming distance learning method [9].", "startOffset": 91, "endOffset": 100}, {"referenceID": 8, "context": "We start from the simple binary embedding method based on locality sensitive hashing (LSH) [6, 7, 8], and then extend to a Hamming distance learning method [9].", "startOffset": 156, "endOffset": 159}, {"referenceID": 9, "context": "For example, [10] proposed a time-spectral binary masking approach to improve robustness of speaker recognition in conditions with high interference.", "startOffset": 13, "endOffset": 17}, {"referenceID": 10, "context": "Besides, [11] presented a solution for large-scale speaker search and indexing under the ivector model, where the search and indexing algorithm is based on LSH.", "startOffset": 9, "endOffset": 13}, {"referenceID": 11, "context": "The work proposed in [12] is more relevant to our proposal.", "startOffset": 21, "endOffset": 25}, {"referenceID": 5, "context": "The LSH approach [6, 7, 8] seeks for a hash function operating on x, such that more similar i-vectors have more chance to coincide after hashing.", "startOffset": 17, "endOffset": 26}, {"referenceID": 6, "context": "The LSH approach [6, 7, 8] seeks for a hash function operating on x, such that more similar i-vectors have more chance to coincide after hashing.", "startOffset": 17, "endOffset": 26}, {"referenceID": 7, "context": "The LSH approach [6, 7, 8] seeks for a hash function operating on x, such that more similar i-vectors have more chance to coincide after hashing.", "startOffset": 17, "endOffset": 26}, {"referenceID": 6, "context": "We employ a simple LSH approach proposed in [7].", "startOffset": 44, "endOffset": 47}, {"referenceID": 12, "context": "It was shown by [13] that the following LSH requirement is satisfied:", "startOffset": 16, "endOffset": 20}, {"referenceID": 8, "context": "An interesting method of this category is the Hamming distance learning proposed by [9].", "startOffset": 84, "endOffset": 87}, {"referenceID": 8, "context": "The Hamming distance learning approach [9] learns a projection function f(x;w) where x is the input (an i-vector in our case) and w is the model parameter.", "startOffset": 39, "endOffset": 42}, {"referenceID": 8, "context": "Note that this approach has been employed to image retrieval in [9], though in this paper we use it for speaker recognition.", "startOffset": 64, "endOffset": 67}, {"referenceID": 13, "context": "\u2013 NIST SRE08: The data of the NIST SRE08 core test in short2 and short3 conditions [14] were used for the speaker verification evaluation.", "startOffset": 83, "endOffset": 87}, {"referenceID": 14, "context": ", the PLEB algorithm [15, 7].", "startOffset": 21, "endOffset": 28}, {"referenceID": 6, "context": ", the PLEB algorithm [15, 7].", "startOffset": 21, "endOffset": 28}], "year": 2016, "abstractText": "The popular i-vector model represents speakers as lowdimensional continuous vectors (i-vectors), and hence it is a way of continuous speaker embedding. In this paper, we investigate binary speaker embedding, which transforms i-vectors to binary vectors (codes) by a hash function. We start from locality sensitive hashing (LSH), a simple binarization approach where binary codes are derived from a set of random hash functions. A potential problem of LSH is that the randomly sampled hash functions might be suboptimal. We therefore propose an improved Hamming distance learning approach, where the hash function is learned by a variable-sized block training that projects each dimension of the original i-vectors to variablesized binary codes independently. Our experiments show that binary speaker embedding can deliver competitive or even better results on both speaker verification and identification tasks, while the memory usage and the computation cost are significantly reduced.", "creator": "LaTeX with hyperref package"}}}