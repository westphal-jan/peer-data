{"id": "1510.05940", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "20-Oct-2015", "title": "Max-margin Metric Learning for Speaker Recognition", "abstract": "Probabilistic linear discriminant analysis (PLDA) is among the most popular methods that accompany the i-vector model to deliver state-of-the-art performance for speaker recognition. A potential problem of the PLDA model, however, is that it essentially assumes strong Gaussian distributions over i-vectors as well as speaker mean vectors, and the objective function is not directly related to the goal of the task, e.g., discriminating true speakers and imposters. We propose a max-margin metric learning approach to solve the problem. It learns a linear transform with the criterion that target trials and imposter trials are discriminated from each other by a large margin. Experiments conducted on the SRE08 core test show that this new approach achieves a performance comparable to or even better than PLDA, though the scoring is as simple as a cosine computation.", "histories": [["v1", "Tue, 20 Oct 2015 16:01:05 GMT  (354kb,D)", "https://arxiv.org/abs/1510.05940v1", null], ["v2", "Thu, 31 Mar 2016 05:27:17 GMT  (112kb,D)", "http://arxiv.org/abs/1510.05940v2", null]], "reviews": [], "SUBJECTS": "cs.SD cs.LG", "authors": ["lantian li", "dong wang", "chao xing", "thomas fang zheng"], "accepted": false, "id": "1510.05940"}, "pdf": {"name": "1510.05940.pdf", "metadata": {"source": "CRF", "title": "Max-Margin Metric Learning for Speaker Recognition", "authors": ["Lantian Li", "Dong Wang", "Chao Xing", "Thomas Fang Zheng"], "emails": ["lilt@cslt.riit.tsinghua.edu.cn;", "xingchao@cslt.riit.tsinghua.edu.cn;", "wangdong99@mails.tsinghua.edu.cn", "fzheng@tsinghua.edu.cn"], "sections": [{"heading": null, "text": "However, one potential problem with the PLDA model is that it essentially assumes Gaussian distributions over loudspeaker vectors, which is not always true in practice. Furthermore, the objective function is not directly related to the purpose of the task, such as distinguishing between real loudspeakers and cheaters. In this paper, we propose a metric learning approach to solving the problems, where a linear transformation is learned by maximizing the margin between target and cheat attempts. Experiments conducted with the core test SRE08 show that the new approach can achieve comparable or even better performance compared to PLDA, even though the evaluation is merely a cosinal calculation. Index terms: maximum margin, metric learning, LDA, speech recognition."}, {"heading": "1. Introduction", "text": "The i-vector model represents the state-of-the-art architecture of modern loudspeaker recognition [1, 2]. With this model, a speaker segment is represented as a low-dimensional continuous vector (i-vector), although the discriminatory approach (and other tasks) can be carried out on the basis of the vector representations. However, a special feature of the i-vector model is that both the speaker and the session variants are embedded in a single low-dimensional subspace. This is an obvious advantage since more speaker-related information is retained compared to other factorization models, e.g. JFA [1]; however, since the specter-related information is buried among others, raw i-vectors are not sufficiently discriminatory with respect to speakers. In order to improve the discriminatory ability of i-vectors for loudspeaker recognition, various standardization or discrimination models have been proposed, including normalization within coance normalization (WN)."}, {"heading": "2. Related work", "text": "A representative paper proposed in [15] uses Neighbourhood Component Analysis (NCA) to learn a projection matrix that minimizes the average error in the classification of loudspeakers. Our model differs from the NCA approach proposed in [16] in that we use the maximum value as a training target and the cosine distance as a benchmark more suitable for the detection of loudspeakers. The model of cosine similarity proposed in [16] is more relevant for our proposal. The authors formulated the training task as a semi-defined program (SDP) [17] that brings i-vectors of the same loudspeaker closer by maximizing cosine distance, while i-vectors of different loudspeakers are separated by a large distance from each other."}, {"heading": "3. Max-margin metric learning", "text": "The simplest form is to learn a linear projection M, so that the distance between the projected data is better suited to the task in hand [13]. For speech recognition, the most popular distance measurement variable used is cosinal distance and the goal is to discriminate between real speakers and cheaters. Therefore, optimizeM is given as follows to make the projected i-vectors more discriminatory for real and fake speakers measured by cosinal distances. Formally, the cosinal distance between two i-vectors w1 and w2 is given as follows: d (w1, w2) = < w1 > projected the other than the projected i-vectors are. (1), where < > denotes the inner product, and | | is the contrasting triple (w +, w), where the two i-vectors and the speakers are."}, {"heading": "4. Experiments", "text": "We evaluate the proposed method using the core test SRE08. This section first presents the data used and the setup, and then reports on the results in the form of equal error rates (EER) and DET curves."}, {"heading": "4.1. Database", "text": "We selected 7,196 speakers to train the i-vector model, the LDA model, and the PLDA model, and the same data was used to perform metric learning, using the NIST SRE 2008 evaluation database as a test set. [19] We selected 1,997 female expressions from the short2-short3 core evaluation data set, based on which 59, 343 studies were conducted, including 12, 159 target studies, and 47, 184 impostor studies."}, {"heading": "4.2. Experimental setup", "text": "The basic acoustic characteristics included 19-dimensional Mel frequency ceptral coefficients (MFCCs) and log energy, complemented by their first and second order derivatives, resulting in 60-dimensional feature vectors. UBM comprised 2 048 Gaussian components and was trained with approximately 8000 randomly selected female expressions from the Fisher database; i-vector dimensions were 400; and the LDA model was trained with expressions from 7 196 female speakers, also randomly selected from the Fisher database; and the dimensionality of the LDA projection room was set at 150."}, {"heading": "4.3. Basic results", "text": "The test is based on the core task NIST SRE 2008, which is divided into 8 test conditions depending on channel, language and accent [19]. The EER results are shown in Table 1. It can be observed that the proposed MMML significantly improves the discriminatory ability of the raw i-vectors and surpasses both LDA and PLDA in state 1-4 (which takes up the majority of the test data). In state 5-8, the PLDA wins the competition. We attribute this discrepancy to the data imbalance in the development set: state 5-8 includes complex patterns (e.g. multilingual speakers, different accents) that were not included in the Fisher database used to form the MMML model."}, {"heading": "4.4. Tandem composition", "text": "We note that both LDA and MMML learn a linear projection, although they are based on different learning criteria: LDA uses Fisher discrimination, while MMML uses a maximum margin; the results in Table 1 show that the maximum margin criterion is clearly superior; an interesting question is whether the two criteria can be put together in tandem; the results are shown in Table 2, where the system \"LDA + MMML\" includes a 400 x 150 dimensional LDA projection, followed by a 150 x 150 dimensional MMMML projection, while the system \"MML + LDA\" includes a 400 x 150 dimensional MML and a 150 dimensional LDA. From these results, we conclude that the last projection is the most important: if it is an MMML, the performance is always good."}, {"heading": "4.5. Score fusion", "text": "The LDA / PLDA model and the MMML model complement each other: LDA / PLDA are generative models and can be better generalized to rare conditions where little training data is available, while MMML is purely discriminatory and more suitable for comparable conditions. Combining these two types of models can offer additional benefits. We experimented with a simple score fusion approach that interpolates the results from LDA / PLDA and MMML linearly. It can be represented as \u03b1SMMML + (1 \u2212 \u03b1) SLDA / PLDA, where \u03b1 is the interpolation factor. Results of the score fusion are presented in Table 3, where the interpolation factor \u03b1 is chosen as 0.2. Compared to Table 1, we observe that fusion consistently leads to better performance than the original LDA and PLDA systems. Interestingly, performance is also improved under conditions 5-8, although the MMML approach does not work well individually."}, {"heading": "5. Conclusions", "text": "This approach is a simple linear transformation trained on the criterion of maximum margin between real speakers and cheaters based on cosinal distance. Assessment is as simple as LDA, but performance is comparable or even better than PLDA, especially with large training data under congruent conditions. Future work will examine metric learning with deep nonlinear transformations and explore better approaches to combine PLDA and MML."}, {"heading": "6. References", "text": "[1] P. J. Kenny, G. Boulianne Recognition Workshop 2004, P. Ouellet and P. Dumouchel P. J. Ouk P. 5, \"Jointfactor Analysis versus eigenchannels in speaker recognition,\" IEEE Transactions on Audio, Speech, and Language Processing, vol. 15, pp. Wang Pacific margin, 2007. [2] - \"Speaker and session verification in GMM-based speaker verification,\" IEEE Transactions on Audio, Speech, and Language Processing, vol. 15, pp. 1448-1460, 2007. [3] A. O. Hatch, S. Kajarekar, and A. Stolcke, \"Within-class covariance normalization for SVM-based speaker recognition,\" 2006 A. Solomonoff, C. Quillen, and W. M. Campbell. \""}], "references": [{"title": "Joint factor analysis versus eigenchannels in speaker recognition", "author": ["P.J. Kenny", "G. Boulianne", "P. Ouellet", "P. Dumouchel"], "venue": "IEEE Transactions on Audio, Speech, and Language Processing, vol. 15, pp. 1435\u20131447, 2007.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2007}, {"title": "Speaker and session variability in GMM-based speaker verification", "author": ["\u2014\u2014"], "venue": "IEEE Transactions on Audio, Speech, and Language Processing, vol. 15, pp. 1448\u20131460, 2007.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2007}, {"title": "Within-class covariance normalization for SVM-based speaker recognition", "author": ["A.O. Hatch", "S.S. Kajarekar", "A. Stolcke"], "venue": "INTERSPEECH, 2006.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2006}, {"title": "Channel compensation for SVM speaker recognition", "author": ["A. Solomonoff", "C. Quillen", "W.M. Campbell"], "venue": "Proc Odyssey, Speaker Language Recognition Workshop 2004, 2004, pp. 57\u201362.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2004}, {"title": "Front-end factor analysis for speaker verification", "author": ["N. Dehak", "P.J. Kenny", "R. Dehak", "P. Ouellet", "P. Dumouchel"], "venue": "IEEE Transactions on Audio, Speech and Language Processing, vol. 19, no. 4, pp. 788\u2013798, 2011.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2011}, {"title": "Probabilistic linear discriminant analysis", "author": ["S. Ioffe"], "venue": "Computer Vision ECCV 2006, Springer Berlin Heidelberg, pp. 531\u2013542, 2006.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2006}, {"title": "Bayesian speaker verification with heavy-tailed priors", "author": ["P. Kenny"], "venue": "Odyssey\u20192010: The Speaker and Language Recognition Workshop, 2010.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2010}, {"title": "Analysis of i-vector length normalization in speaker recognition systems", "author": ["D. Garcia-Romero", "C.Y. Espy-Wilson"], "venue": "INTER- SPEECH, 2011, pp. 249\u2013252.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2011}, {"title": "Discriminatively trained probabilistic linear discriminant analysis for speaker verification", "author": ["L. Burget", "O. Plchot", "S. Cumani", "O. Glembek", "P. Mat\u011bjka", "N. Br\u00fcmmer"], "venue": "Acoustics, Speech and Signal Processing (ICASSP), 2011 IEEE International Conference on. IEEE, 2011, pp. 4832\u20134835.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2011}, {"title": "Pairwise discriminative speaker verification in the-vector space", "author": ["S. Cumani", "N. Brummer", "L. Burget", "P. Laface", "O. Plchot", "V. Vasilakakis"], "venue": "IEEE Transactions on Audio, Speech and Language Processing, vol. 21, no. 6, pp. 1217\u20131227, 2013.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2013}, {"title": "Single-sided approach to discriminative PLDA training for text-independent speaker verification without using expanded ivector", "author": ["I. Hirano", "K.A. Lee", "Z. Zhang", "L. Wang", "A. Kai"], "venue": "Chinese Spoken Language Processing (ISCSLP), 2014 9th International Symposium on, Sept 2014, pp. 59\u201363.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2014}, {"title": "Discriminative scoring for speaker recognition based on i-vectors", "author": ["J. Wang", "D. Wang", "Z. Zhu", "T.F. Zheng", "F. Soong"], "venue": "Asia- Pacific Signal and Information Processing Association, 2014 Annual Summit and Conference (APSIPA). IEEE, 2014, pp. 1\u20135.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2014}, {"title": "An overview of distance metric learning", "author": ["L. Yang"], "venue": "Proceedings of the Computer Vision and Pattern Recognition Conference, 2007.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2007}, {"title": "Learning a distance metric from relative comparisons", "author": ["M. Schultz", "T. Joachims"], "venue": "NIPS, p. 41, 2004.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2004}, {"title": "Bayesian distance metric learning on i-vector for speaker verification", "author": ["J.G. Xiao Fang", "Najim Dehak"], "venue": "INTERSPEECH, 2013.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2013}, {"title": "Cosine distance metric learning for speaker verification using large margin nearest neighbor method", "author": ["W. Ahmad", "H. Karnick", "R.M. Hegde"], "venue": "Advances in Multimedia Information Processing, pp. 294\u2013303, 2014.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2014}, {"title": "Distance metric learning for large margin nearest neighbor classification", "author": ["K.Q. Weinberger", "J. Blitzer", "L.K. Saul"], "venue": "Advances in neural information processing systems, 2005, pp. 1473\u2013 1480.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2005}, {"title": "Theano: A CPU and GPU math compiler in python", "author": ["J. Bergstra", "O. Breuleux", "F. Bastien", "P. Lamblin", "R. Pascanu", "G. Desjardins", "J. Turian", "D. Warde-Farley", "Y. Bengio"], "venue": "Proceedings of the 9th Python in Science Conference, S. van der Walt and J. Millman, Eds., 2010, pp. 3 \u2013 10.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2010}, {"title": "The NIST year 2008 speaker recognition evaluation plan", "author": ["NIST"], "venue": "Online: http://www.itl.nist.gov/iad/mig/tests/sre/2008/ sre08 evalplan release4.pdf, 2008.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2008}], "referenceMentions": [{"referenceID": 0, "context": "The i-vector model represents the state-of-the-art architecture of modern speaker recognition [1, 2].", "startOffset": 94, "endOffset": 100}, {"referenceID": 1, "context": "The i-vector model represents the state-of-the-art architecture of modern speaker recognition [1, 2].", "startOffset": 94, "endOffset": 100}, {"referenceID": 0, "context": ", JFA [1]; however, since the speakerrelated information is buried under others, raw i-vectors are not sufficiently discriminative with respect to speakers.", "startOffset": 6, "endOffset": 9}, {"referenceID": 2, "context": "In order to improve the discriminative capability of i-vectors for speaker recognition, various normalization or discrimination models have been proposed, including within-class covariance normalization (WCCN) [3], nuisance attribute projection (NAP) [4], linear discriminant analysis (LDA) [5], and its Bayesian counterpart, probabilistic linear discriminant analysis (PLDA) [6].", "startOffset": 210, "endOffset": 213}, {"referenceID": 3, "context": "In order to improve the discriminative capability of i-vectors for speaker recognition, various normalization or discrimination models have been proposed, including within-class covariance normalization (WCCN) [3], nuisance attribute projection (NAP) [4], linear discriminant analysis (LDA) [5], and its Bayesian counterpart, probabilistic linear discriminant analysis (PLDA) [6].", "startOffset": 251, "endOffset": 254}, {"referenceID": 4, "context": "In order to improve the discriminative capability of i-vectors for speaker recognition, various normalization or discrimination models have been proposed, including within-class covariance normalization (WCCN) [3], nuisance attribute projection (NAP) [4], linear discriminant analysis (LDA) [5], and its Bayesian counterpart, probabilistic linear discriminant analysis (PLDA) [6].", "startOffset": 291, "endOffset": 294}, {"referenceID": 5, "context": "In order to improve the discriminative capability of i-vectors for speaker recognition, various normalization or discrimination models have been proposed, including within-class covariance normalization (WCCN) [3], nuisance attribute projection (NAP) [4], linear discriminant analysis (LDA) [5], and its Bayesian counterpart, probabilistic linear discriminant analysis (PLDA) [6].", "startOffset": 376, "endOffset": 379}, {"referenceID": 6, "context": "For example, to go beyond the Gaussian assumption, Kenny [7] proposed a heavy-tailed PLDA which assumes a non-Gaussian prior over the speaker mean vector.", "startOffset": 57, "endOffset": 60}, {"referenceID": 7, "context": "[8] found that length normalization can compensate for the non-Gaussian effect and boost performance of Gaussian PLDA to the level of the heavy-tailed PLDA.", "startOffset": 0, "endOffset": 3}, {"referenceID": 8, "context": "Burget, Cumani and colleagues [9, 10] proposed a pair-wised discriminative model that discriminates true speakers and imposters.", "startOffset": 30, "endOffset": 37}, {"referenceID": 9, "context": "Burget, Cumani and colleagues [9, 10] proposed a pair-wised discriminative model that discriminates true speakers and imposters.", "startOffset": 30, "endOffset": 37}, {"referenceID": 9, "context": "The input features of the model are derived from the i-vector pairs according to a form derived from the PLDA score function (further generalized to any symmetric score functions in [10]), and the model is trained on i-vector pairs that have been labelled as identical or different speakers.", "startOffset": 182, "endOffset": 186}, {"referenceID": 10, "context": "To solve this problem, a partial discriminative training approach was proposed in [11], which optimizes the discriminative model on a subspace and does not require any feature expansion.", "startOffset": 82, "endOffset": 86}, {"referenceID": 11, "context": "In [12], we proposed a discriminative approach based on deep neural networks (DNN), which holds the same idea as the pair-wised training, whereas the features are defined manually.", "startOffset": 3, "endOffset": 7}, {"referenceID": 12, "context": "simplest metric learning methods which have been studied for decades in machine learning [13, 14], though it has not been extensively studied in speaker recognition.", "startOffset": 89, "endOffset": 97}, {"referenceID": 13, "context": "simplest metric learning methods which have been studied for decades in machine learning [13, 14], though it has not been extensively studied in speaker recognition.", "startOffset": 89, "endOffset": 97}, {"referenceID": 14, "context": "A representative work proposed in [15] employs neighborhood component analysis (NCA) to learn a projection matrix that minimizes the average leave-one-out k-nearest neighbor classification error.", "startOffset": 34, "endOffset": 38}, {"referenceID": 15, "context": "The cosine similarity large margin nearest neighborhood (CSLMNN) model proposed in [16] is more relevant to our proposal.", "startOffset": 83, "endOffset": 87}, {"referenceID": 16, "context": "The authors formulated the training task as a semidefinite program (SDP) [17] which moves i-vectors of the same speaker closer by maximizing the cosine distance among them, while separating i-vectors of different speakers by a large margin.", "startOffset": 73, "endOffset": 77}, {"referenceID": 12, "context": "The simplest form is to learn a linear projection M so that the distance among the projected data is more suitable for the task in hand [13].", "startOffset": 136, "endOffset": 140}, {"referenceID": 17, "context": "In this study, the Theano package [18] was used to implement the SGD training.", "startOffset": 34, "endOffset": 38}, {"referenceID": 18, "context": "The NIST SRE 2008 evaluation database [19] was used as the test set.", "startOffset": 38, "endOffset": 42}, {"referenceID": 18, "context": "The test is based on the NIST SRE 2008 core task, which is divided into 8 test conditions according to the channel, language and accent [19].", "startOffset": 136, "endOffset": 140}], "year": 2016, "abstractText": "Probabilistic linear discriminant analysis (PLDA) is a popular normalization approach for the i-vector model, and has delivered state-of-the-art performance in speaker recognition. A potential problem of the PLDA model, however, is that it essentially assumes Gaussian distributions over speaker vectors, which is not always true in practice. Additionally, the objective function is not directly related to the goal of the task, e.g., discriminating true speakers and imposters. In this paper, we propose a max-margin metric learning approach to solve the problems. It learns a linear transform with a criterion that the margin between target and imposter trials are maximized. Experiments conducted on the SRE08 core test show that compared to PLDA, the new approach can obtain comparable or even better performance, though the scoring is simply a cosine computation.", "creator": "LaTeX with hyperref package"}}}