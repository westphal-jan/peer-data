{"id": "1510.06002", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "20-Oct-2015", "title": "Fast and Scalable Structural SVM with Slack Rescaling", "abstract": "We present an efficient method for training slack-rescaled structural SVM. Although finding the most violating label in a margin-rescaled formulation is often easy since the target function decomposes with respect to the structure, this is not the case for a slack-rescaled formulation, and finding the most violated label might be very difficult. Our core contribution is an efficient method for finding the most-violating-label in a slack-rescaled formulation, given an oracle that returns the most-violating-label in a (slightly modified) margin-rescaled formulation. We show that our method enables accurate and scalable training for slack-rescaled SVMs, reducing runtime by an order of magnitude compared to previous approaches to slack-rescaled SVMs.", "histories": [["v1", "Tue, 20 Oct 2015 18:25:45 GMT  (248kb,D)", "https://arxiv.org/abs/1510.06002v1", null], ["v2", "Tue, 27 Oct 2015 22:25:33 GMT  (252kb,D)", "http://arxiv.org/abs/1510.06002v2", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["heejin choi", "ofer meshi", "nathan srebro"], "accepted": false, "id": "1510.06002"}, "pdf": {"name": "1510.06002.pdf", "metadata": {"source": "CRF", "title": "Fast and Scalable Structural SVM with Slack Rescaling", "authors": ["Heejin Choi", "Ofer Meshi", "Nathan Srebro"], "emails": [], "sections": [{"heading": null, "text": "We present an efficient method for training slackened structural SVMs. Although it is often easy to find the most offending label in a margin-reduced formulation, as the target function decomposes in terms of structure, this is not the case with a slackened rescale formulation, and finding the most offending label in a (slightly modified) margin-reduced formulation could be very difficult. Our key contribution is an efficient way to find the most offending label in a slackened formulation, given that an oracle returns the most offending label in a (slightly modified) margin-reduced formulation. We show that our method enables precise and scalable training for slackened SVMs and reduces runtime by an order of magnitude compared to previous approaches for slackened rescaled SVMs."}, {"heading": "1 Introduction", "text": "This year it has come to the point that it has never come as far as this year."}, {"heading": "2 Problem Formulation", "text": "In this section, we will look at the basics of structural prediction and the description of relevant learning objectives instead of a structured approach. (...) In this section, we will look at the basics of structural prediction and the description of relevant learning objectives. (...) In this section, we will look at the data (...) in the areas (...) and (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (... (...) (...) (...) (...) (... (...) (...) (...) (... (...) (...) (...) (...) (... (...) (...) (...) (...) (...) (...) (... (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (... (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (..."}, {"heading": "3 Optimization for Slack Rescaling", "text": "In this section, we will briefly have to compile an overview of the intersection planes and stochastic gradients in order to find a good solution [6]. This is helpful to understand the difference between our approach and that of previous work on the goal developed by Sarawagi and Gupta (11).The difficulty in optimizing (2) results from the number of constraints equal to the size of the production room Y (for each training instance).The intersection method maintains a small number of constraints and solves optimization only through this theorem. At each iteration, the active set of constraints is augmented, and it can be shown that too many such constraints do not need to be added to find a good solution."}, {"heading": "4 Algorithms", "text": "In this section, we present our main contribution, a framework for solving the maximization problem (6), which we describe as follows: max y \u03a6 (y): = max y h (y) g (y) (12) We describe two new algorithms for solving this problem by accessing the \u03bb oracle, which have several advantages over previous approaches. However, we also show that any algorithm that uses only the \u03bb oracle cannot always restore an optimal solution. Therefore, in Section 4.5, we have proposed an improved algorithm that requires access to an extended \u03bb oracle that can also handle linear constraints."}, {"heading": "4.1 Binary search", "text": "First, we present a binary search algorithm similar to that proposed by Sarawagi and Gupta, but with one major difference: Our algorithm can easily be used with training methods that optimize the unrestricted target (4) and can therefore be used for SGD, SDCA, and FW. The algorithm minimizes a convex upper limit to \u03a6 without a slip variable, the algorithm is based on the following problem (details and proofs are in Appendix A. Lemma 1. Leaving F-value F-value (\u03bb) = 14 maxy value Y + (1 \u03bbh (y) + \u03bbg (y) 2, thenmax y-value Y-value (y) \u2264 min \u03bb > 0 F-value (\u03bb) and F-value (\u03bb) is a convex function in the description. Instead of minimizing this upper limit, we next present an algorithm that aims to optimize pe-value (y) in a more direct way by using a Rapp2 interpretation of the geometric interpretation."}, {"heading": "4.2 Geometrical Interpretation of \u03bb-oracle search", "text": "In order to better understand the problem and to motivate our methods, it is useful to consider the following geometric interpretation of (12): We map each inscription y to a vector. (12) The maximization (max.) is reduced to the problem: If a label with dots ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ R2 is given, we maximize the product of its coordinates ~ y = argmax ~ y-y-Y = y [~ y] 1 \u00b7 [~ y] 2. The contours of our objective function ~ [~ y] 1 \u00b7 [~ y] 2 are then hyperbolas. We want to maximize this function by repeatedly finding dots that maximize linear targets of the form ~ yy, which is the inscription of the form ~ yy."}, {"heading": "4.3 Bisecting search", "text": "In this section, we propose a search algorithm based on the previous geometric interpretation. Similar to the binary search, our method is also based on the basic \u03bboracle. Next, we give an overview of the algorithm. We maintain a series of possible values returned by the oracle. However, we take an intersection of G and H with a segment of possible values of h (y) and g (y), each using Lemmas 2 and 3. Second, we reduce the space L of potential \u03bb's based on the following Lemma (shown in the appendix). Lemma 4. h (y\u03bb) is a non-increasing function of \u03bb, and g (y\u03bb) is a non-decreasing function of the search."}, {"heading": "4.4 Limitation of the \u03bb-oracle", "text": "x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x"}, {"heading": "4.5 Angular search with the constrained-\u03bb-oracle", "text": "The limitations of inequality defined in (8), even if we allow additional linear constraints, we can use this modified algorithm to imagine an algorithm that identifies the most infringing constraints, as covered by the following guarantee, which is described in Appendix F: Theorem 2. Angular search in Algorithm 2 finds the optimum in Algorithm 2. However, it is still disappointing, as the number of iterations and thus the number of oracles could actually be greater than the number of labels. This is already an improvement over the previous methods, as we return at least the most actually infringing labels. Unfortunately, it is possible even with a limited label, which is almost the best we can hope for."}, {"heading": "5 Experiments", "text": "In this section, we confirm our contributions by comparing the different behaviors of the search algorithms on standard benchmark datasets and their impact on optimization. In particular, we show that the angle search with SGD is not only much faster than the other alternatives, but also allows for much more precise optimization with a loose recalculation that exceeds the recalculation of margins. In contrast to the simple structure used in [2], we show applicability to complicated structures. We experiment with multi-mark datasets modeled by a Markov Random Field, with paired potentials as in [5]. Since the conclusion of margin scaling in this case is NP-hard, we rely on linear programming relaxation. Note that this complicates the problem and the number of labels becomes even greater (adds fractional solutions). Also, note that all our results in previous sections with light modification apply to this harder setting."}, {"heading": "5.1 Comparison of the search algorithms", "text": "Table 1 compares the performance of the search in terms of time spent, the number of searches and the percentage of success in finding the most infringing label. The cut plane algorithm calls the search algorithms to find the most infringing label y, and adds it to the active sentence if the infringement is greater than a certain margin, i.e., \"(y, yi)\" (1 + f (y) \u2212 f (yi) > i +. For cut plain optimization, we compare all three algorithms: angular search, double-edged search and Sarawagi's search, as well as Sarawagi's and Guptas [11] (but currently used edge search for the update). The success percentage is the percentage in which the search algorithm finds such an infringing label. As expected by Theorem 1, splitting and Sarawagi's search are missing the infringing label in cases where the edge search finds one successfully."}, {"heading": "5.2 Hierarchical Multi-label Classification", "text": "Furthermore, we experimented with the problem of hierarchical multi-label classification [3]. In hierarchical multi-label classification, each label y is a leaf node in a given diagram, and each label y divides ancestor nodes. It can be described as a graphical model where the potential of a multi-label Y = {y1,..., yk} is the sum of all the potential of its ancestors, i.e., \u03a6 (Y) = \u2211 n Y Anc (n) \u03a6 (n). We extracted 1500 instances with dimensions 17944 with a diagram structure of 156 nodes with 123 labels from SWIKI-2011. SWIKI-2011 is a multi-label data set from Wikipedia pages from the LSHTC competition1. We used 750 instances as a training set, 250 instances as a holdout set, and 750 instances as a test set."}, {"heading": "6 Summary", "text": "As we have seen in our experiments, and have also noted before, recalculating gaps in terms of predictive power is often advantageous compared to recalculating margins. However, recalculating margins is often much easier in arithmetical terms due to its additive form. Therefore, recalculating margins is much more commonly used in practice. Here, we show how an oracle is sufficient to solve an Argmax of the form (5), or perhaps a slightly modified form (the defined \u03bb oracle), to obtain exact solutions for the slackrescale Argmax (6), which allows us to train sloppy SVMs with the help of SGD to achieve better predictive results than by recalculating margins. Preparatory work in this direction [11] was only approximate and, more importantly, only possible if they apply methods at the cutting level, not SGD, and therefore not suitable for large predictive problems more recently suggested by a more efficient [2] programming approach."}, {"heading": "Appendix A Details of binary search", "text": "Lemma 1. Leave F * (\u03bb) = 14 x Y + (1 x) (1 x) (1 x) (2 x) (4 x) (2 x) (2 x) (2 x) (4 x) (4 x) (4 x) (4 x) (4 x) (4 x) (4 x) (4 x) (4 x) (4 x) (4 x) (4 x) (5 x) (5 x) (5 x) (5 x) (5 x) (5 x) (5 x) (5 x x) (5 x x) (5 x x) (5 x) (5 x) (5 x x) (5 x) (5 x) (5 x) (5 x) (5 x) (5 x) (5 x) (5 x) (5 x) (5 x) (5 x) (5 x) (5 x) (5 x) (5 x) (5 x (5 x) (5 x) (5 x) (5 x) (5 x) (5 x) (5 x) (5 x) (5 x) (5 x (5 x) (5 x) (5 x) (5 x (5 x) (5 x) (5 x) (5 x) (5 x (5 x) (5 x) (5 x) (5 x (5 x) (5 x) (5 x) (5 x (5 x) (5 x) (5 x (5 x) (5 x) (5 x) (5 x (5 x) (5 x) (5 x (5 x) (5 x) (5 x (5 x) (5 x) (5 x (5 x) (5 x) (5 x) (5 x (5 x) (5 x (5 x) (5 x) (5 x) (5 x) (5 x (5 x) (5 x) (5 x) (5 x (5 x) (5 x (5 x) (5 x) (5 x) (5 x) (5 x) (5 x (5 x) (5 x) (5 x) (5 x) (5 x) (5 x (5 x) (5 x) (5 x (5 x) (5 x) (5 x) (5"}, {"heading": "Appendix B An example of label mapping", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "Appendix C Monotonicity of h and g in \u03bb", "text": "Proof: Leave g1 = g (y\u03bb1), h1 = h (y\u03bb1), g2 = g (y\u03bb2) and h2 = h (y\u03bb2).h1 + \u03bb1g1 \u2265 h2 + \u03bb1g2, h2 + \u03bb2g2 \u2265 h1 + \u03bb2g1 = h1 \u2212 h2 + \u03bb1 (g1 \u2212 g2) \u2265 0, \u2212 h1 + h2 + \u03bb2 (g2 \u2212 g1) \u2265 0 (g2 \u2212 g1) (\u03bb2 \u2212 \u03bb1) \u2265 0For h change the role of g and h."}, {"heading": "Appendix D Improvements for the binary", "text": ""}, {"heading": "Appendix D.1 Early stopping", "text": "If L = [\u03bbm, \u03bbM] and both endpoints have the same designation, i.e. y\u03bbm = y\u03bbM, then we can safely terminate the binary search, because it follows from lesson 4 that the solution y\u03bb will not change in this segment."}, {"heading": "Appendix D.2 Suboptimality bound", "text": "Let K (\u03bb) be the value of the \u03bb oracle, i.e., K (\u03bb) = max y-Y-h (y) + \u03bbg (y). (14) Lemma 5. \u0439 is at the upper limit of \u03a6 (y-j) \u2264 K (\u03bb) 24\u03bb (15) Proof.h (y) + \u03bbg (y) \u2264 K (\u03bb) and \u21d2 g (y) (h (y) + \u03bbg (y) \u2264 g (y) K (\u03bb) \u0438 (y-j) \u0432\u0438\u0441\u0441\u0441\u0438 (y) \u0445g (y) \u2212 \u03bbg (y) 2 = \u2212 \u0432 (g (y) \u2212 K (\u03bb) 2\u043c) 2 + K (\u03bb) 24\u043c \u2264 K (\u03bb) 24\u03bb"}, {"heading": "Appendix E Proof of the limitation of the", "text": "For each > 0 there is a problem with three labels, so for each case more than 0 and less than 0 / 0 / 0 there is a problem with three labels that also apply to any > 0 / 0 / 0 / 0 / 0. We will first prove that there is a problem with three labels that apply to any label, namely \"Y\" (y) < and \"Y\" (y) < and \"Y\" (y) < and \"Y\" (y) < \"Y\" (y) < and \"Y\" (y) < and \"Y\" (y) < \"H\" (yp) = 1 / 2. \"We will prove the following problem first, which is in the evidence.Lemma 6. Let's leave A = [A1 A2],\" Y \"and\" Y \"(v),\" V \"(B1 B2), and\" C = [C1 C2], and \"Y.\""}, {"heading": "Appendix F Angular search", "text": "First, we must introduce the following notations:............................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................."}, {"heading": "25: t\u2190 t+ 1", "text": "26: if t = T then. Maximum iteration reaches 27: return y"}, {"heading": "Appendix G Illustration of the angular", "text": "searchThe following figure 6 illustrates the angle search. Block points are the labels from Figure 5. Blue X denotes the new label returned by the oracle. Red X is the maximum point. Two straight lines are the upper and the lower label used by the restricted oracle. Limited oracle returns a blue dot between the upper and lower label. We can draw a line that passes by blue X that no label can be above the line. Then we halve the angle. This process continues until the y is found."}, {"heading": "Appendix H Limitation of the constraint", "text": "In any case, the seeker can either label the oracle with a consistent label, or point out that there is no label in such an oracle according to M \u2212 1, and the oracle reveals a label according to the query, and the allegation is that with each choice of M \u2212 1 properties there can be either a consistent label, or indicates that there is no label in such a label according to M \u2212 1, and the oracle reveals a label according to the query, and the allegation is that with each choice of M \u2212 1 properties the oracle can either give a consistent label, or indicate that there is no label in such a label according to M \u2212 1 that is greater than any of the previous labels revealed. Denote each label with iteration > 0, and a label with closed and closed query."}], "references": [{"title": "Predicting Structured Data", "author": ["G.H. Bakir", "T. Hofmann", "B. Sch\u00f6lkopf", "A.J. Smola", "B. Taskar", "S.V.N. Vishwanathan"], "venue": null, "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2007}, {"title": "Efficient algorithms for exact inference in sequence labeling svms", "author": ["A. Bauer", "N. Gornitz", "F. Biegler", "Muller", "K.-R", "M. Kloft"], "venue": "Neural Networks and Learning Systems, IEEE Transactions on,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2014}, {"title": "Hierarchical document categorization with support vector machines", "author": ["L. Cai", "T. Hofmann"], "venue": "In Proceedings of the thirteenth ACM international conference on Information and knowledge management,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2004}, {"title": "A kernel method for multi-labelled classification", "author": ["A. Elisseeff", "J. Weston"], "venue": "In Advances in neural information processing systems,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2001}, {"title": "Training structural svms when exact inference is intractable", "author": ["T. Finley", "T. Joachims"], "venue": "In Proceedings of the 25th international conference on Machine learning,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2008}, {"title": "Cutting-plane training of structural svms", "author": ["T. Joachims", "T. Finley", "Yu", "C.-N. J"], "venue": "Machine Learning,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2009}, {"title": "Block-coordinate frank-wolfe optimization for structural svms", "author": ["S. Lacoste-Julien", "M. Jaggi", "M. Schmidt", "P. Pletscher"], "venue": "In ICML 2013 International Conference on Machine Learning,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2013}, {"title": "Rcv1: A new benchmark collection for text categorization research", "author": ["D.D. Lewis", "Y. Yang", "T.G. Rose", "F. Li"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2004}, {"title": "Minimizing the product of two discrete convex functions", "author": ["N.D. Nghia", "D.D. Chinh", "P.C. Duong"], "venue": "ACTA MATHEMATICA VIETNAMICA,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 1995}, {"title": "Online) subgradient methods for structured prediction", "author": ["N. Ratliff", "J.A.D. Bagnell", "M. Zinkevich"], "venue": "In AISTATS", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2007}, {"title": "Accurate maxmargin training for structured output spaces", "author": ["S. Sarawagi", "R. Gupta"], "venue": "In Proceedings of the 25th international conference on Machine learning,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2008}, {"title": "Pegasos: Primal estimated sub-gradient solver for svm", "author": ["S. Shalev-Shwartz", "Y. Singer", "N. Srebro", "A. Cotter"], "venue": "Mathematical programming,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2011}, {"title": "Accelerated proximal stochastic dual coordinate ascent for regularized loss minimization", "author": ["S. Shalev-Shwartz", "T. Zhang"], "venue": "Mathematical Programming,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2013}], "referenceMentions": [{"referenceID": 0, "context": "Many problems in machine learning can be seen as structured output prediction tasks, where one would like to predict a set of labels with rich internal structure [1].", "startOffset": 162, "endOffset": 165}, {"referenceID": 10, "context": "To address this challenge, Sarawagi and Gupta [11] propose a method to reduce the problem of slack rescaling to a series of modified margin rescaling problems.", "startOffset": 46, "endOffset": 50}, {"referenceID": 1, "context": "[2] proposed an elegant dynamic programming approach to the slack rescaling optimization problem.", "startOffset": 0, "endOffset": 3}, {"referenceID": 10, "context": "Similar to Sarawagi and Gupta [11] our method reduces finding the most violated label in slack rescaling to a series of margin rescaling problems.", "startOffset": 30, "endOffset": 34}, {"referenceID": 9, "context": "However, in contrast to their approach, our approach can be easily used with training algorithms like stochastic gradient descent (SGD) [10] and block Frank-Wolfe (FW) [7], which often scale much better than cutting plane.", "startOffset": 136, "endOffset": 140}, {"referenceID": 6, "context": "However, in contrast to their approach, our approach can be easily used with training algorithms like stochastic gradient descent (SGD) [10] and block Frank-Wolfe (FW) [7], which often scale much better than cutting plane.", "startOffset": 168, "endOffset": 171}, {"referenceID": 4, "context": "When the score and error functions h and g decompose into a sum of simpler functions, we can exploit that structure in order to solve the maximization efficiently [15, 14, 5].", "startOffset": 163, "endOffset": 174}, {"referenceID": 10, "context": "This is also the oracle used by Sarawagi and Gupta [11].", "startOffset": 51, "endOffset": 55}, {"referenceID": 10, "context": "This will be helpful in understanding the difference between our approach and that of prior work on the slack rescaled objective by Sarawagi and Gupta [11].", "startOffset": 151, "endOffset": 155}, {"referenceID": 5, "context": "The cutting plane algorithm was proposed for solving the structural SVM formulation in [15, 6].", "startOffset": 87, "endOffset": 94}, {"referenceID": 10, "context": "This algorithm has also been used in previous work on slack rescaling optimization [11, 2].", "startOffset": 83, "endOffset": 90}, {"referenceID": 1, "context": "This algorithm has also been used in previous work on slack rescaling optimization [11, 2].", "startOffset": 83, "endOffset": 90}, {"referenceID": 5, "context": "At each iteration the active set of constraints is augmented with new violated constraints, and it can be shown that not too many such constraints need to be added for a good solution to be found [6].", "startOffset": 196, "endOffset": 199}, {"referenceID": 10, "context": "Relying on this framework, Sarawagi and Gupta [11] use the formulation in (2) and rewrite the constraints as:", "startOffset": 46, "endOffset": 50}, {"referenceID": 10, "context": "Since F (\u03bb) is a convex function, (11) can be solved by a simple search method such as golden search over \u03bb [11].", "startOffset": 108, "endOffset": 112}, {"referenceID": 9, "context": "On the other hand, algorithms such as stochastic gradient descent (SGD) [10, 12], stochastic dual coordinate ascent (SDCA) [13], or block-coordinate Frank-Wolfe (FW) [7], all optimize the unconstrained objective form (4).", "startOffset": 72, "endOffset": 80}, {"referenceID": 11, "context": "On the other hand, algorithms such as stochastic gradient descent (SGD) [10, 12], stochastic dual coordinate ascent (SDCA) [13], or block-coordinate Frank-Wolfe (FW) [7], all optimize the unconstrained objective form (4).", "startOffset": 72, "endOffset": 80}, {"referenceID": 12, "context": "On the other hand, algorithms such as stochastic gradient descent (SGD) [10, 12], stochastic dual coordinate ascent (SDCA) [13], or block-coordinate Frank-Wolfe (FW) [7], all optimize the unconstrained objective form (4).", "startOffset": 123, "endOffset": 127}, {"referenceID": 6, "context": "On the other hand, algorithms such as stochastic gradient descent (SGD) [10, 12], stochastic dual coordinate ascent (SDCA) [13], or block-coordinate Frank-Wolfe (FW) [7], all optimize the unconstrained objective form (4).", "startOffset": 166, "endOffset": 169}, {"referenceID": 10, "context": "We first present a binary search algorithm similar to the one proposed by Sarawagi and Gupta [11], but with one main difference.", "startOffset": 93, "endOffset": 97}, {"referenceID": 10, "context": "First, the binary search needs explicit upper and lower bounds on \u03bb, thus it has to search the entire \u03bb space [11].", "startOffset": 110, "endOffset": 114}, {"referenceID": 10, "context": "the function only through \u03bb-oracle, including the method of Sarawagi and Gupta [11] and both methods presented above, cannot be guaranteed to find a label optimizing \u03a6(y), even approximately, and even with unlimited accesses to the oracle.", "startOffset": 79, "endOffset": 83}, {"referenceID": 1, "context": "Unlike the simple structure used in [2], we show applicability to complicated structure.", "startOffset": 36, "endOffset": 39}, {"referenceID": 4, "context": "We experiment with multilabel dataset modeled by a Markov Random Field with pair-wise potentials as in [5].", "startOffset": 103, "endOffset": 106}, {"referenceID": 3, "context": "Two standard benchmark multi-label datasets, yeast[4] (14 labels)and RCV1[8], are tested.", "startOffset": 50, "endOffset": 53}, {"referenceID": 7, "context": "Two standard benchmark multi-label datasets, yeast[4] (14 labels)and RCV1[8], are tested.", "startOffset": 73, "endOffset": 76}, {"referenceID": 10, "context": "For cutting-plain optimization, we compare all three algorithms: Angular search, Bisecting search, and Sarawagi and Gupta\u2019s [11] (but just used Angular search for the update).", "startOffset": 124, "endOffset": 128}, {"referenceID": 2, "context": "We further experimented on problem of hierarchical multilabel classification [3].", "startOffset": 77, "endOffset": 80}, {"referenceID": 10, "context": "Prior work in this direction [11] was only approximate, and more significantly, only enabled using cutting-plane methods, not SGD, and was thus not appropriate for large scale problems.", "startOffset": 29, "endOffset": 33}, {"referenceID": 1, "context": "More recently, [2] proposed an efficient dynamic programming approach for solving the slack-rescaled argmax (6), but their approach is only valid for sequence problems2 and only when using hamming errors, not for more general structured prediction problems.", "startOffset": 15, "endOffset": 18}], "year": 2015, "abstractText": "We present an efficient method for training slackrescaled structural SVM. Although finding the most violating label in a margin-rescaled formulation is often easy since the target function decomposes with respect to the structure, this is not the case for a slack-rescaled formulation, and finding the most violated label might be very difficult. Our core contribution is an efficient method for finding the most-violatinglabel in a slack-rescaled formulation, given an oracle that returns the most-violating-label in a (slightly modified) margin-rescaled formulation. We show that our method enables accurate and scalable training for slack-rescaled SVMs, reducing runtime by an order of magnitude compared to previous approaches to slack-rescaled SVMs.", "creator": "LaTeX with hyperref package"}}}