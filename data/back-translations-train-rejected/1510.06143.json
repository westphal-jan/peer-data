{"id": "1510.06143", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "21-Oct-2015", "title": "High Performance Latent Variable Models", "abstract": "Latent variable models have accumulated a considerable amount of interest from the industry and academia for their versatility in a wide range of applications. A large amount of effort has been made to develop systems that is able to extend the systems to a large scale, in the hope to make use of them on industry scale data. In this paper, we describe a system that operates at a scale orders of magnitude higher than previous works, and an order of magnitude faster than state-of-the-art system at the same scale, at the same time showing more robustness and more accurate results.", "histories": [["v1", "Wed, 21 Oct 2015 06:23:55 GMT  (4320kb,D)", "http://arxiv.org/abs/1510.06143v1", null], ["v2", "Thu, 5 Nov 2015 22:39:06 GMT  (0kb,I)", "http://arxiv.org/abs/1510.06143v2", "The paper was uploaded by the lead author without approval by any of the coauthors and without Google's approval"], ["v3", "Mon, 9 Nov 2015 03:37:21 GMT  (4316kb,D)", "http://arxiv.org/abs/1510.06143v3", "Corrected the list of authors"], ["v4", "Wed, 11 Nov 2015 05:16:06 GMT  (0kb,I)", "http://arxiv.org/abs/1510.06143v4", "arXiv admin note: This paper has been withdrawn due to an irreconcilable author dispute"]], "reviews": [], "SUBJECTS": "cs.LG cs.AI", "authors": ["aaron q li", "amr ahmed", "mu li", "vanja josifovski"], "accepted": false, "id": "1510.06143"}, "pdf": {"name": "1510.06143.pdf", "metadata": {"source": "CRF", "title": "High Performance Latent Variable Models", "authors": ["Aaron Q. Li", "Amr Ahmed", "Vanja Josifovski", "Alexander J. Smola"], "emails": ["aaron@potatos.io", "amra@google.com", "muli@cs.cmu.edu", "vanjaj@google.com", "alex@smola.org"], "sections": [{"heading": null, "text": "Our system leverages a number of advances in distributed reasoning: high performance in synchronizing sufficient statistics with a loose consistency model; rapid sampling using the Metropolis-Hastings-Walker method to overcome dense generative models; statistical modeling that goes beyond latent dirichlet allocation (LDA) to Pitman-Yor distributions (PDP) and Hierarchical Dirichlet Process (HDP) models; ingenious parameter projection schemes to resolve the conflicts within the constraints between parameters arising from the relaxed consistency model; this work significantly expands the scope of what is commonly known as parameter server. We obtain results with up to hundreds of billions of tokens, thousands of topics, and a vocabulary of a few million token types, using up to 60,000 processor cores operating on a production scale of a large Internet enterprise."}, {"heading": "1. INTRODUCTION", "text": "Permission to make digital or hard copies of all or part of this work for personal or class-specific use is granted at no charge, provided that copies are not made or distributed for profit or commercial purposes, and that copies bear this note and full quotation on the first page. Otherwise, copying, publishing on servers, or forwarding to lists requires prior specific permission and / or a fee. Copyright 20XX ACM X-XXXXX-XX / XX / XX. \"Metropolitan variable models are a highly versatile tool for deriving structures and hierarchies from unstructured data. Typical use cases of latent variable models include learning topics from documents, building user profiles, predicting user behavior, and generating class labels. Extending the boundaries of scalability for latent variable models is a fundamental challenge in large-scale modeling."}, {"heading": "1.1 Prior Work", "text": "The ground-breaking paper of [2] relied on variation methods to analyze only thousands of documents, and the subsequent progress of [9] introduced collapsed variation samples that proved to be a much more scalable and faster Gibbs sampling algorithm for latent variable inferences. More efficient samplers were introduced by [22], which exploited the fragmentation structure of the data. Unfortunately, much of the computational advantages of the latter approach disappear for very large document collections and a large number of topics [12], as the generative model becomes largely dense again. Distributed inference strategies for sampling were first proposed by [14], using essentially synchronous communication paradigms found in MapReduce [7], that is, they interchange phases of sampling latent variables with phases of aggregating sufficient statistics between different machines."}, {"heading": "2. LATENT VARIABLE MODELS", "text": "In this section we give a brief overview of three latently variable models: Latent Dirichlet Allocation (LDA), the Poisson Dirichlet Process (PDP) and the Hierarchical Dirichlet Process (HDP)."}, {"heading": "2.1 Latent Dirichlet Allocation", "text": "In LDA [2] it is assumed that the distributions of language models associated with the individual topics are reconciled. [3] This means that the documents are generated according to the following pattern: [4] \"The distribution issue is not yet resolved.\" (4) \"The distribution issue is not yet resolved.\" (4) \"The distribution issue is not yet resolved.\" (4) \"The distribution issue is not yet resolved.\" (4) \"The distribution issue is not yet resolved.\" (4) \"The distribution issue is not yet resolved.\" (4) \"The distribution issue is not yet resolved.\" (4) \"The distribution issue is not yet resolved.\" (4) \"The distribution issue is not yet resolved.\" (4) \"The distribution issue is not yet resolved.\" (4) \"The distribution issue is not yet resolved.\" (4) \"The distribution issue is not yet resolved.\" (4) The distribution issue is not yet resolved."}, {"heading": "2.2 Poisson Dirichlet Process", "text": "An example of such a model is Poisson Dirichlet Process [3, 15]. The model is given by the following variant of a theme model that describes the characteristics of observed actions / tokens.A means of addressing this problem is to integrate a level of hierarchy into the model of distribution."}, {"heading": "2.3 Hierarchical Dirichlet Process", "text": "To illustrate the effectiveness and universality of our approach, we will discuss a third case in which the document model itself is more sophisticated than a simple broken down Dirichlet multinomical scheme. We will show that conclusions can be made efficiently there, too. Consider the two-step theme model based on the Hierarchical Dirichlet Process [20] (HDP-LDA), in which the theme distribution for each document is derived from a Dirichlet process DP (b1, \u03b80). In other words, we add an additional hierarchy level on the document side (compared to the additional hierarchy on the language model used in the PDP).For all ifor all kH properties DP (b0, H (\u00b7))) that govern the distribution across themes. Formally, the common distribution on the document side (compared with the additional hierarchy on the language model level) is common. For all ifor all H-0 properties that zdi follows, the distribution is like wdi formally."}, {"heading": "3. METROPOLIS-HASTINGS-WALKER", "text": "A common strategy is to use a relaxed Gibbs sampler that acts independently on each machine without blocking the state between machines, as described in [17]. As the sampling progresses, the common state converges in the light of the data into a draw from the posterior distribution via the latent variables. The challenge is that recalculating a slowly changing distribution can be very costly - especially if we have a distribution via k results and we only draw a single sample before the distribution changes, each sample requires an O (k) calculation. In the following, we describe an algorithm to reduce these costs amortized to O (1) when the changes are sufficiently small [10]. The main difference from our previous work is that changes in distribution can now occur both due to local samples and to state updates through global synchronization. We start with the description of Walker's Alias Method [21, 13, and a simplified version of the Metropolis [8]."}, {"heading": "3.1 Walker\u2019s Alias Method", "text": "To achieve this, we process the distribution p into a list of l-triples of the form (i, j, \u03c0i) with \u03c0i \u2264 l \u2212 1 as follows: \u2022 Division of the indices {1.. l} into sentences U and L, in which pi > l \u2212 1 for i > U and pi \u2264 l \u2212 1 for i-L. \u2022 Select any i-L and j-U and add (i, j, pi) to L. \u2022 Remove i from L and j from U \u2022 Update pi > l \u2212 1 for i > l \u2212 1 and if pj > l \u2212 1 then add jto U, otherwise to L. By constructing the algorithm ends after l steps and, moreover, all probability masses in the form pi > l \u2212 1 that we have associated with Hi \u2212 l."}, {"heading": "3.2 Sampling with Proposal Distributions", "text": "To address this problem, we resort to Metropolis Hastings sample [8] using a stationary supply distribution, i.e. we treat the \"old-fashioned\" version of p as a suggestion distribution q and correct the effect of the sample from the \"wrong\" distribution by a subsequent acceptance step. This is very efficient, as it only requires the probability ratios to be close to each other. The disadvantage is that instead of drawing iid samples from p, we obtain a chain of dependent samples from p, as in q.For the purpose of the current method, we need only deal with stationary distributions p and q, i.e. p (i | j) = p (i) and q (i) = q (i | j), so we will discuss only this particular case below. It is well known that in order to satisfy the detailed equilibrium conditions a sample movement such as i \u2192 j occurs, where j \u00b2 q (j) is only accepted with probability, i.e. a state (q) is present."}, {"heading": "3.3 Constant Time Sampling", "text": "By combining both methods, we arrive at the conclusion that in our opinion this represents a significant improvement over each individual component. It works as follows: \u2022 By q: = p, we create the alias table L in O (l) time. \u2022 If necessary, we update p \u2022 Example j with n steps of Metropolis-Hastings sam-pling with q as suggestion distribution. After l / n steps, we discard the alias table L and restart. Since the sampling from the alias table O (1) takes time, the number of Metropolis-Hastings steps is constant, and each Metropolis-Hasting step contains a constant number of operations, which results in us taking each sample from p in constant time, provided that p does not deviate too far from q. Further details are available in [10]. One of the most important changes regarding the settings of individual machines is that whenever we receive a global parameter update from the parameter server, p is likely to be drastically changed."}, {"heading": "4. PARAMETER SERVER", "text": "Distributed Optimization and Conclusion is popular for solving machine learning problems. These problems are often associated with 1TB to 1PB training data, enabling the creation of powerful and complex models with 109 to 1012 parameters."}, {"heading": "5. TOWARDS LARGE SCALE LATENT VARIABLE MODELS", "text": "In this section, we describe our method of scaling the single-thread algorithm [10] proposed in our previous paper into an efficient distributed implementation. In Section 5.1, we first extend the alias sampler to a multi-thread version. Distributed implementation of the collapsed Gibbs sampling in the parameter server is then discussed in Section 5.2. Next, in Section 5.3, we describe how data can be communicated efficiently and how error tolerance can be achieved in Section 5.4. Finally, we show the algorithms for resolving the constraint conflicts based on the relaxed data consistency model."}, {"heading": "5.1 Multi-thread Alias Sampler", "text": "In our multi-thread version, there are two thread pools, one containing alias threads that construct the alias tables and calculate a stock of samples; the other pool consists of sampling threads that scan the documents. In practice, we create only one or a few threads for the alias pool, but use many more threads, at least the number of available CPU cores, for the sampling pool.The alias threads and the sampling threads formulate a relationship between vendor and consumer. The sampling threads continue to consume samples from the pre-calculated stock of samples produced by the alias threads, notify the alias threads whenever demand exceeds supply, and recycle from old samples when demand is severely constrained. The alias threads weigh the importance of each token type, adjust the amount of supply that should be generated for each token type in order to meet the demand, and choose to store one whole of the threads based on demand, or just an alias sample."}, {"heading": "5.2 Distributed Implementation Using the Parameter Server", "text": "In fact, most of them will be able to play by the rules they have set themselves, and they will be able to play by the rules they have set themselves."}, {"heading": "5.4 Failure and Load Balancing", "text": "In previous work [1], we used a synchronous snapshot scheme where we freeze the servers and clients and upload a snapshot of their memory to disk every N-minute. In practice, this approach does not scale well and requires unnecessary communication overhead due to global barriers. In addition, if a server or client fails, we need to reboot the entire system to take the most recent snapshot, and we lose all calculations since that last snapshot. In this work, we apply an asynchronous approach to both failure and recovery as follows. Clients and servers independently take a snapshot of their memory to capture every N-minute without global barrier. These snapshots are then used to restore the node."}, {"heading": "5.5 Parameter Projection for Constraint Violation Resolution", "text": "For aggregation parameters such as nt (Aggregation ntw) in LDA or mk (Aggregation mwk) in PDP, consistency can easily be maintained by deriving the aggregation parameters from their customer counterparts. However, this is not the case for parameters that have complex interactions and constraints. Furthermore, mtw is greater than zero. Furthermore, both mtw and stw are always greater or equal to zero. In the case of HDP, similar constraints exist between the root table and the number of items, the table and the number of items for each document and more.Figure 3 shows an example of this problem for the PDP model. In this example, each client has slightly different statistics for mwk and k = i = 2. In this case, data delays can often occur."}, {"heading": "At the end of each iteration on selected clients:", "text": "(Same as algorithm 1) based on local statistics, the update sent by Client 2 violates the constraint in PDP in such a way that mwk cannot be 0, while the update sent by Client 3 violates the constraint from Client 3's point of view, which must be more or less than twk. In the perspective of Client 2, the update sent by Client 3 violates the constraint that states cannot be twk 0, while mwk > 0. Either the client or server must correct the statistics on demand and from time to time, otherwise the samplers would be vulnerable to numerical errors and could soon deviate from unexpected results, such as inferencing from statistics that easily violate model constraints can produce NaN, infinite, or other unstable probabilities for the samples.Algorithm 3: On-demand projection on serverData: C1, where C rules restrict two parameters and A data are."}, {"heading": "6.1 YahooLDA vs AliasLDA", "text": "In YahooLDA and AliasLDA, the ntw and nt parameters are shared for all clients. In AliasLDA, a stock of samples for each type of token is kept on the local machine, and these stocks are not shared between the clients. Convergence of perplexity, average topics per word, and runtime over iterations are given in Figure 4 for the experiments with 200, 500, and 1000 clients, respectively, along with the number of data points collected for each iteration in each experiment. AliasLDA consistently performs better than YahooLDA in average / minimum perplexity, average / minimum runtime, and average / minimum number of topics per word. Furthermore, the runtime of AliasLDA does not increase with increased average number of topics per word or increased data size, while the runtime of YahooLDA scales up with these values. This observation is consistent with the theory that the alias sampling data are much faster than the two time complexity [reasons why the two times sampling are actually smaller than the 10]."}, {"heading": "6.2 Large scale LDA", "text": "We conducted a single large experiment using our best LDA model with 6,000 clients, and the result is shown in Figure 6, where performance is evaluated by the likelihood of documents logging. In this case, we have 5 billion documents, probably the largest results ever reported for LDA. As Figure 6 shows, a small deviation over the mean probability implies correct synchronization between clients. 6.3 PDP and HDPThe convergence of confusion, average topics per word, and runtime over iterations are shown in Figure 5 along with the number of data points for each iteration. Converged confusion shows that our system works for more complicated models and the correction mechanisms are effective (without corrections, we observed deviating values and much worse perplexity). The same statistics are shown in Figure 7. The experiment merged into a very good perplexity with 200 clients and a stable decreasing curve with 500 clients, with very small deviations."}, {"heading": "6.4 Effects of Using Projection", "text": "In Figure 8, we show the result of a single simple experiment to illustrate the importance of parameter correction and the effects of using projections in our implementation. Without using projections, perplexity converges more slowly and varies rapidly. We observed this behavior in many other experiments, such as the PDP model, and in particular when running a large number of clients."}, {"heading": "7. CONCLUSION", "text": "In this paper, we described a high-performance system for latent variable models and empirically demonstrated its efficiency by using it to analyze large amounts of data with a variety of latent variable topic models. We also demonstrated that our system is capable of efficiently executing complex latent variable topic models such as YahooLDA and AliasLDA to analyze large amounts of data with hundreds of billions of tokens and thousands of topics at an unprecedented speed. Furthermore, we demonstrated that it is possible to expand even more complex latent variable topic models such as PDP and HDP, which have many limitations between parameters, in a network of hundreds or more machines for analyzing real data on the scale of billions of tokens. Compared to other state-of-the-art systems such as YahooLDA, although the scale of data and the number of machines sharing parameters is orders of magnitude larger, our system with efficient alias sampling algorithms such as AliasLDA and Aliasa per system is still capable of generating efficiencies of millions of HDP per second, the efficiency is still greater than the efficiency in the HDP."}, {"heading": "8. REFERENCES", "text": "[1] A. Ahmed, M. Aly, J. Gonzalez, S. Narayanamurthy, and A. J. Smola. Scalable inference in latent variable models. In Proceedings of the 5th ACM International Conference on Web Search and Data Mining (WSDM), 2012. [2] D. Lead, A. Ng, and M. Jordan. Latent Dirichlet Allokation. Journal of Machine Learning Research, 3: 993-1022, Jan. 2003. [3] W. Buntine and M. Hutter. A Bayesian review of the poisson dirichlet process, 2010. [4] C. Chen, W. Buntine, N. Ding, L. Xie, and L. Du. Differential topic models. In IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI), 2014. [5] C. Chen, L. Du, and W. Buntine. Sampling table configurations for the hierarchical poisson-dilet."}], "references": [{"title": "Scalable inference in latent variable models", "author": ["A. Ahmed", "M. Aly", "J. Gonzalez", "S. Narayanamurthy", "A.J. Smola"], "venue": "In Proceedings of The 5th ACM International Conference on Web Search and Data Mining (WSDM),", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2012}, {"title": "Latent Dirichlet allocation", "author": ["D. Blei", "A. Ng", "M. Jordan"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2003}, {"title": "A bayesian review of the poisson-dirichlet", "author": ["W. Buntine", "M. Hutter"], "venue": null, "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2010}, {"title": "Differential topic models", "author": ["C. Chen", "W. Buntine", "N. Ding", "L. Xie", "L. Du"], "venue": "In IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI),", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2014}, {"title": "Sampling table configurations for the hierarchical poisson-dirichlet process", "author": ["C. Chen", "L. Du", "W. Buntine"], "venue": "European Conference on Machine Learning,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2011}, {"title": "Petuum: A framework for iterative-convergent distributed ml", "author": ["W. Dai", "J. Wei", "X. Zheng", "J.K. Kim", "S. Lee", "J. Yin", "Q. Ho", "E.P. Xing"], "venue": "arXiv preprint arXiv:1312.7651,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2013}, {"title": "MapReduce: simplified data processing on large", "author": ["J. Dean", "S. Ghemawat"], "venue": "clusters. CACM,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2008}, {"title": "Bayesian estimation of state-space model using the metropolis-hastings algorithm within gibbs sampling", "author": ["J. Geweke", "H. Tanizaki"], "venue": "Computational Statistics and Data Analysis,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2001}, {"title": "Finding scientific topics", "author": ["T. Griffiths", "M. Steyvers"], "venue": "Proceedings of the National Academy of Sciences,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2004}, {"title": "Reducing the sampling complexity of topic models", "author": ["A.Q. Li", "A. Ahmed", "S. Ravi", "A.J. Smola"], "venue": "In Proceedings of the 20th ACM SIGKDD international conference on Knowledge discovery and data mining. ACM,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2014}, {"title": "Communication efficient distributed machine learning with the parameter server", "author": ["M. Li", "D.G. Andersen", "A.J. Smola"], "venue": "In Neural Information Processing Systems,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2014}, {"title": "Scaling distributed machine learning with the parameter server", "author": ["M. Li", "A.J. Smola", "J. Park", "A. Ahmed", "V. Josifovski", "J. Long", "E. Shekita", "B.-Y. Su"], "venue": "In USENIX Symposium on Operating Systems Design and Implementation,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2014}, {"title": "Fast generation of discrete random variables", "author": ["G. Marsaglia", "W.W. Tsang", "J. Wang"], "venue": "Journal of Statistical Software,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2004}, {"title": "Distributed inference for latent dirichlet allocation", "author": ["D. Newman", "A. Asuncion", "P. Smyth", "M. Welling"], "venue": "In NIPS,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2007}, {"title": "The two-parameter poisson-dirichlet distribution derived from a stable subordinator", "author": ["J. Pitman", "M. Yor"], "venue": "Annals of Probability,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 1997}, {"title": "Topic models with power-law using pitman-yor process", "author": ["I. Sato", "H. Nakagawa"], "venue": "Knowledge Discovery and Data Mining,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2010}, {"title": "An architecture for parallel topic models", "author": ["A.J. Smola", "S. Narayanamurthy"], "venue": "In Very Large Databases (VLDB),", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2010}, {"title": "Chord: A scalable peer-to-peer lookup service for internet applications", "author": ["I. Stoica", "R. Morris", "D. Karger", "M.F. Kaashoek", "H. Balakrishnan"], "venue": "ACM SIGCOMM Computer Communication Review,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2001}, {"title": "Counting triangles and the curse of the last reducer", "author": ["S. Suri", "S. Vassilvitskii"], "venue": "Conference on World Wide Web,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2011}, {"title": "Hierarchical dirichlet processes", "author": ["Y. Teh", "M. Jordan", "M. Beal", "D. Blei"], "venue": "Journal of the American Statistical Association,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2006}, {"title": "An efficient method for generating discrete random variables with general distributions", "author": ["A.J. Walker"], "venue": "ACM Transactions on Mathematical Software (TOMS),", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 1977}, {"title": "Efficient methods for topic model inference on streaming document collections", "author": ["L. Yao", "D. Mimno", "A. McCallum"], "venue": "In KDD\u201909,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2009}], "referenceMentions": [{"referenceID": 1, "context": "While simple models such as Latent Dirichlet Allocation [2] per se are prevalent in use, scalability becomes a critical issue when the model is adopted on industrial data.", "startOffset": 56, "endOffset": 59}, {"referenceID": 9, "context": "We extend our previous work on the MetropolisHastings-Walker sampler (Alias sampler) [10] to the parameter server [12], which is a scalable and general purpose distributed machine learning framework.", "startOffset": 85, "endOffset": 89}, {"referenceID": 11, "context": "We extend our previous work on the MetropolisHastings-Walker sampler (Alias sampler) [10] to the parameter server [12], which is a scalable and general purpose distributed machine learning framework.", "startOffset": 114, "endOffset": 118}, {"referenceID": 16, "context": "The resulted system is an order of magnitude faster than the state-of-the-art YahooLDA [17] in both efficiency and scalability.", "startOffset": 87, "endOffset": 91}, {"referenceID": 11, "context": "Comparing to LDA on parameter server [12], our system is more accurate and is able to adapt to a variety of latent variable models.", "startOffset": 37, "endOffset": 41}, {"referenceID": 2, "context": "\u2022 We show how nontrivial hierarchical latent variable models such as the Pitman Yor Topic model [3] and the Hierarchical Dirichlet Process [5] can be distributed efficiently over thousands of cores.", "startOffset": 96, "endOffset": 99}, {"referenceID": 4, "context": "\u2022 We show how nontrivial hierarchical latent variable models such as the Pitman Yor Topic model [3] and the Hierarchical Dirichlet Process [5] can be distributed efficiently over thousands of cores.", "startOffset": 139, "endOffset": 142}, {"referenceID": 9, "context": "\u2022 We demonstrate how distributed synchronization and the Metropolis-Hastings-Walker sampler of [10] can be integrated into a very high throughput sampling inference algorithm.", "startOffset": 95, "endOffset": 99}, {"referenceID": 11, "context": "\u2022 We describe an efficient distributed implementation which takes the advantages of the parameter server [12] on efficient data communication and machine fault tolerance.", "startOffset": 105, "endOffset": 109}, {"referenceID": 1, "context": "The seminal paper of [2] relied on variational methods to analyze mere thousands of documents.", "startOffset": 21, "endOffset": 24}, {"referenceID": 8, "context": "Subsequent progress by [9] introduced collapsed variational sampling, which proved to be a much more scalable and rapidly mixing Gibbs sampling algorithm for latent variable inference.", "startOffset": 23, "endOffset": 26}, {"referenceID": 21, "context": "More efficient samplers were introduced by [22] which took advantage of the sparsity structure of the data.", "startOffset": 43, "endOffset": 47}, {"referenceID": 11, "context": "Unfortunately, a large portion of the computational advantages in the latter approach vanishes for very large collections of documents and large numbers of topics [12], since the generative model becomes mostly dense again.", "startOffset": 163, "endOffset": 167}, {"referenceID": 13, "context": "Distributed inference strategies for sampling were first proposed by [14].", "startOffset": 69, "endOffset": 73}, {"referenceID": 6, "context": "They essentially exploited bulk synchronous communications paradigms as can be found in MapReduce [7].", "startOffset": 98, "endOffset": 101}, {"referenceID": 16, "context": "Subsequent work introduced the notion of a parameter server to allow for asynchronous processing [17].", "startOffset": 97, "endOffset": 101}, {"referenceID": 0, "context": "A substantially improved synchronization protocol was proposed by [1], which demonstrated scalability to 10 brief documents and more sophisticated temporal model dependencies.", "startOffset": 66, "endOffset": 69}, {"referenceID": 5, "context": "[6] implement a rather similar system, albeit not quite as scalable and with somewhat different consistency properties.", "startOffset": 0, "endOffset": 3}, {"referenceID": 9, "context": "More details are available in [10].", "startOffset": 30, "endOffset": 34}, {"referenceID": 1, "context": "In LDA [2] one assumes that documents are mixture distributions of language models associated with individual topics.", "startOffset": 7, "endOffset": 10}, {"referenceID": 8, "context": "This means that the multinomial distributions \u03b8d and \u03c8k can be integrated out, thus allowing one to express p(w, z|\u03b1, \u03b2, nd) in closed-form [9].", "startOffset": 140, "endOffset": 143}, {"referenceID": 21, "context": "[22] devised an ingenious strategy for exploiting sparsity in terms of ntd and ntw.", "startOffset": 0, "endOffset": 4}, {"referenceID": 2, "context": "An example of such model is Poisson Dirichlet Process [3, 15].", "startOffset": 54, "endOffset": 61}, {"referenceID": 14, "context": "An example of such model is Poisson Dirichlet Process [3, 15].", "startOffset": 54, "endOffset": 61}, {"referenceID": 15, "context": "The ingredients for a refined language model are a PitmanYor Topic Model (PYTM) [16] that is more appropriate to deal with natural languages.", "startOffset": 80, "endOffset": 84}, {"referenceID": 14, "context": "This is then combined with the Poisson Dirichlet Process (PDP) [15, 3] to capture the fact that the number of occurrences of a word in a natural language corpus follows power-law.", "startOffset": 63, "endOffset": 70}, {"referenceID": 2, "context": "This is then combined with the Poisson Dirichlet Process (PDP) [15, 3] to capture the fact that the number of occurrences of a word in a natural language corpus follows power-law.", "startOffset": 63, "endOffset": 70}, {"referenceID": 3, "context": "The combined model described explicitly in [4]:", "startOffset": 43, "endOffset": 46}, {"referenceID": 4, "context": "Skipping details that can be found in [5, 4] it follows that an efficient sampler can be implemented by using the following auxiliary variables:", "startOffset": 38, "endOffset": 44}, {"referenceID": 3, "context": "Skipping details that can be found in [5, 4] it follows that an efficient sampler can be implemented by using the following auxiliary variables:", "startOffset": 38, "endOffset": 44}, {"referenceID": 19, "context": "Consider the two-level topic model based on the Hierarchical Dirichlet Process [20] (HDP-LDA).", "startOffset": 79, "endOffset": 83}, {"referenceID": 9, "context": "They can be found in [10].", "startOffset": 21, "endOffset": 25}, {"referenceID": 16, "context": "A common strategy is to use a relaxed Gibbs sampler which acts on each machine independently without the need for locking of state between machines, as described in [17].", "startOffset": 165, "endOffset": 169}, {"referenceID": 9, "context": "In the following we describe an algorithm to reduce this to O(1) amortized cost, whenever the changes are sufficiently small [10].", "startOffset": 125, "endOffset": 129}, {"referenceID": 20, "context": "We begin by describing Walker\u2019s alias method [21, 13] and a simplified version of the Metropolis-Hastings sampler [8].", "startOffset": 45, "endOffset": 53}, {"referenceID": 12, "context": "We begin by describing Walker\u2019s alias method [21, 13] and a simplified version of the Metropolis-Hastings sampler [8].", "startOffset": 45, "endOffset": 53}, {"referenceID": 7, "context": "We begin by describing Walker\u2019s alias method [21, 13] and a simplified version of the Metropolis-Hastings sampler [8].", "startOffset": 114, "endOffset": 117}, {"referenceID": 7, "context": "To address this problem we resort to Metropolis Hastings sampling [8] using a stationary proposal distribution.", "startOffset": 66, "endOffset": 69}, {"referenceID": 9, "context": "More detail is available in [10].", "startOffset": 28, "endOffset": 32}, {"referenceID": 11, "context": "These problems often use 1TB to 1PB training data, which allows the creation of powerful and complex models with 10 to 10 parameters [12].", "startOffset": 133, "endOffset": 137}, {"referenceID": 11, "context": "Here we briefly review the previous work related to the latent variable models, more detailed review is available in [12, 11].", "startOffset": 117, "endOffset": 125}, {"referenceID": 10, "context": "Here we briefly review the previous work related to the latent variable models, more detailed review is available in [12, 11].", "startOffset": 117, "endOffset": 125}, {"referenceID": 16, "context": "The first generation uses memcached as the synchronization mechanism [17], which lacks flexibility and performance.", "startOffset": 69, "endOffset": 73}, {"referenceID": 0, "context": "It improved the previews design by a dedicated and user-definable server and a more principled load distribution algorithm [1].", "startOffset": 123, "endOffset": 126}, {"referenceID": 11, "context": "Our work is based on the third generation of parameter server [12], which is a general purpose distributed framework for machine learning.", "startOffset": 62, "endOffset": 66}, {"referenceID": 17, "context": "These (key,value) pairs are then partitioned into server nodes by using consistent hashing in the form of a Chord-style layout [18].", "startOffset": 127, "endOffset": 131}, {"referenceID": 11, "context": "Besides, it provides continuous fault tolerance by using optimized chain replication [12].", "startOffset": 85, "endOffset": 89}, {"referenceID": 9, "context": "In this section we describe our method to scale the singlethread algorithm proposed in our previous work [10] into an efficient distributed implementation.", "startOffset": 105, "endOffset": 109}, {"referenceID": 0, "context": "Similar to previous work [1], we found the eventual consistency model best fits our requirements.", "startOffset": 25, "endOffset": 28}, {"referenceID": 0, "context": "In prior work [1], we utilized a synchronous snapshot scheme where we freeze the servers and clients and take a snapshot of their memory to disk every N minutes.", "startOffset": 14, "endOffset": 17}, {"referenceID": 11, "context": "We did not use the hot failover mechanism described in [12] due to the resource constraints.", "startOffset": 55, "endOffset": 59}, {"referenceID": 0, "context": "Instead, if there is a server failure, we freeze the whole system until mwk[1][2]=1 twk[1][2]=1 server group", "startOffset": 75, "endOffset": 78}, {"referenceID": 1, "context": "Instead, if there is a server failure, we freeze the whole system until mwk[1][2]=1 twk[1][2]=1 server group", "startOffset": 78, "endOffset": 81}, {"referenceID": 0, "context": "Instead, if there is a server failure, we freeze the whole system until mwk[1][2]=1 twk[1][2]=1 server group", "startOffset": 87, "endOffset": 90}, {"referenceID": 1, "context": "Instead, if there is a server failure, we freeze the whole system until mwk[1][2]=1 twk[1][2]=1 server group", "startOffset": 90, "endOffset": 93}, {"referenceID": 0, "context": "client 1 mwk[1][2]=2 twk[1][2]=1", "startOffset": 12, "endOffset": 15}, {"referenceID": 1, "context": "client 1 mwk[1][2]=2 twk[1][2]=1", "startOffset": 15, "endOffset": 18}, {"referenceID": 0, "context": "client 1 mwk[1][2]=2 twk[1][2]=1", "startOffset": 24, "endOffset": 27}, {"referenceID": 1, "context": "client 1 mwk[1][2]=2 twk[1][2]=1", "startOffset": 27, "endOffset": 30}, {"referenceID": 0, "context": "client 2 mwk[1][2]=2 twk[1][2]=2", "startOffset": 12, "endOffset": 15}, {"referenceID": 1, "context": "client 2 mwk[1][2]=2 twk[1][2]=2", "startOffset": 15, "endOffset": 18}, {"referenceID": 0, "context": "client 2 mwk[1][2]=2 twk[1][2]=2", "startOffset": 24, "endOffset": 27}, {"referenceID": 1, "context": "client 2 mwk[1][2]=2 twk[1][2]=2", "startOffset": 27, "endOffset": 30}, {"referenceID": 0, "context": "client 3 mwk[1][2]-=1 twk[1][2]-=1 mwk[1][2]-=1 twk[1][2]-=1 mwk[1][2]-=1 twk[1][2]-=1", "startOffset": 12, "endOffset": 15}, {"referenceID": 1, "context": "client 3 mwk[1][2]-=1 twk[1][2]-=1 mwk[1][2]-=1 twk[1][2]-=1 mwk[1][2]-=1 twk[1][2]-=1", "startOffset": 15, "endOffset": 18}, {"referenceID": 0, "context": "client 3 mwk[1][2]-=1 twk[1][2]-=1 mwk[1][2]-=1 twk[1][2]-=1 mwk[1][2]-=1 twk[1][2]-=1", "startOffset": 25, "endOffset": 28}, {"referenceID": 1, "context": "client 3 mwk[1][2]-=1 twk[1][2]-=1 mwk[1][2]-=1 twk[1][2]-=1 mwk[1][2]-=1 twk[1][2]-=1", "startOffset": 28, "endOffset": 31}, {"referenceID": 0, "context": "client 3 mwk[1][2]-=1 twk[1][2]-=1 mwk[1][2]-=1 twk[1][2]-=1 mwk[1][2]-=1 twk[1][2]-=1", "startOffset": 38, "endOffset": 41}, {"referenceID": 1, "context": "client 3 mwk[1][2]-=1 twk[1][2]-=1 mwk[1][2]-=1 twk[1][2]-=1 mwk[1][2]-=1 twk[1][2]-=1", "startOffset": 41, "endOffset": 44}, {"referenceID": 0, "context": "client 3 mwk[1][2]-=1 twk[1][2]-=1 mwk[1][2]-=1 twk[1][2]-=1 mwk[1][2]-=1 twk[1][2]-=1", "startOffset": 51, "endOffset": 54}, {"referenceID": 1, "context": "client 3 mwk[1][2]-=1 twk[1][2]-=1 mwk[1][2]-=1 twk[1][2]-=1 mwk[1][2]-=1 twk[1][2]-=1", "startOffset": 54, "endOffset": 57}, {"referenceID": 0, "context": "client 3 mwk[1][2]-=1 twk[1][2]-=1 mwk[1][2]-=1 twk[1][2]-=1 mwk[1][2]-=1 twk[1][2]-=1", "startOffset": 64, "endOffset": 67}, {"referenceID": 1, "context": "client 3 mwk[1][2]-=1 twk[1][2]-=1 mwk[1][2]-=1 twk[1][2]-=1 mwk[1][2]-=1 twk[1][2]-=1", "startOffset": 67, "endOffset": 70}, {"referenceID": 0, "context": "client 3 mwk[1][2]-=1 twk[1][2]-=1 mwk[1][2]-=1 twk[1][2]-=1 mwk[1][2]-=1 twk[1][2]-=1", "startOffset": 77, "endOffset": 80}, {"referenceID": 1, "context": "client 3 mwk[1][2]-=1 twk[1][2]-=1 mwk[1][2]-=1 twk[1][2]-=1 mwk[1][2]-=1 twk[1][2]-=1", "startOffset": 80, "endOffset": 83}, {"referenceID": 0, "context": "mwk[1][2]=1 twk[1][2]=1 server group", "startOffset": 3, "endOffset": 6}, {"referenceID": 1, "context": "mwk[1][2]=1 twk[1][2]=1 server group", "startOffset": 6, "endOffset": 9}, {"referenceID": 0, "context": "mwk[1][2]=1 twk[1][2]=1 server group", "startOffset": 15, "endOffset": 18}, {"referenceID": 1, "context": "mwk[1][2]=1 twk[1][2]=1 server group", "startOffset": 18, "endOffset": 21}, {"referenceID": 0, "context": "client 1 mwk[1][2]=2 twk[1][2]=1", "startOffset": 12, "endOffset": 15}, {"referenceID": 1, "context": "client 1 mwk[1][2]=2 twk[1][2]=1", "startOffset": 15, "endOffset": 18}, {"referenceID": 0, "context": "client 1 mwk[1][2]=2 twk[1][2]=1", "startOffset": 24, "endOffset": 27}, {"referenceID": 1, "context": "client 1 mwk[1][2]=2 twk[1][2]=1", "startOffset": 27, "endOffset": 30}, {"referenceID": 0, "context": "client 2 mwk[1][2]=2 twk[1][2]=2", "startOffset": 12, "endOffset": 15}, {"referenceID": 1, "context": "client 2 mwk[1][2]=2 twk[1][2]=2", "startOffset": 15, "endOffset": 18}, {"referenceID": 0, "context": "client 2 mwk[1][2]=2 twk[1][2]=2", "startOffset": 24, "endOffset": 27}, {"referenceID": 1, "context": "client 2 mwk[1][2]=2 twk[1][2]=2", "startOffset": 27, "endOffset": 30}, {"referenceID": 0, "context": "client 3 mwk[1][2]-=1 mwk[1][2]-=1 mwk[1][2]-=1", "startOffset": 12, "endOffset": 15}, {"referenceID": 1, "context": "client 3 mwk[1][2]-=1 mwk[1][2]-=1 mwk[1][2]-=1", "startOffset": 15, "endOffset": 18}, {"referenceID": 0, "context": "client 3 mwk[1][2]-=1 mwk[1][2]-=1 mwk[1][2]-=1", "startOffset": 25, "endOffset": 28}, {"referenceID": 1, "context": "client 3 mwk[1][2]-=1 mwk[1][2]-=1 mwk[1][2]-=1", "startOffset": 28, "endOffset": 31}, {"referenceID": 0, "context": "client 3 mwk[1][2]-=1 mwk[1][2]-=1 mwk[1][2]-=1", "startOffset": 38, "endOffset": 41}, {"referenceID": 1, "context": "client 3 mwk[1][2]-=1 mwk[1][2]-=1 mwk[1][2]-=1", "startOffset": 41, "endOffset": 44}, {"referenceID": 21, "context": "We implemented two LDA models on the parameter server: (1) YahooLDA, using the traditional sparse sampling method described in [22], (2) AliasLDA, using the alias sampling method as described in Section 3.", "startOffset": 127, "endOffset": 131}, {"referenceID": 0, "context": "Moreover, note that YahooLDA is a re-implementation of [1] in the new parameter server architecture described in this paper for a fair comparison.", "startOffset": 55, "endOffset": 58}, {"referenceID": 18, "context": "This was done to make sure that we don\u2019t burn up resources waiting for the slowest worker \u2013 in a shared environment this problem is known as the curse of the last reducer problem [19].", "startOffset": 179, "endOffset": 183}, {"referenceID": 9, "context": "This observation is consistent with the theory that the alias sampling method does indeed decrease the time complexity, as suggested by [10] There are two reasons that the alias sampling method arrives at better perplexity and more concentrated topics", "startOffset": 136, "endOffset": 140}], "year": 2017, "abstractText": "Latent variable models have accumulated a considerable amount of interest from the industry and academia for their versatility in a wide range of applications. A large amount of effort has been made to develop systems that is able to extend the systems to a large scale, in the hope to make use of them on industry scale data. In this paper, we describe a system that operates at a scale orders of magnitude higher than previous works, and an order of magnitude faster than state-of-the-art system at the same scale, at the same time showing more robustness and more accurate results. Our system uses a number of advances in distributed inference: high performance in synchronization of sufficient statistics with relaxed consistency model; fast sampling, using the Metropolis-Hastings-Walker method to overcome dense generative models; statistical modeling, moving beyond Latent Dirichlet Allocation (LDA) to Pitman-Yor distributions (PDP) and Hierarchical Dirichlet Process (HDP) models; sophisticated parameter projection schemes, to resolve the conflicts within the constraint between parameters arising from the relaxed consistency model. This work significantly extends the domain of applicability of what is commonly known as the parameter server. We obtain results with up to hundreds billion of tokens, thousands of topics, and a vocabulary of a few million token-types, using up to 60,000 processor cores operating on a production cluster of a large Internet company. This demonstrates the feasibility to scale to problems orders of magnitude larger than any previously published work.", "creator": "LaTeX with hyperref package"}}}