{"id": "1510.06168", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "21-Oct-2015", "title": "Part-of-Speech Tagging with Bidirectional Long Short-Term Memory Recurrent Neural Network", "abstract": "Bidirectional Long Short-Term Memory Recurrent Neural Network (BLSTM-RNN) has been shown to be very effective for tagging sequential data, e.g. speech utterances or handwritten documents. While word embedding has been demoed as a powerful representation for characterizing the statistical properties of natural language. In this study, we propose to use BLSTM-RNN with word embedding for part-of-speech (POS) tagging task. When tested on Penn Treebank WSJ test set, a state-of-the-art performance of 97.40 tagging accuracy is achieved. Without using morphological features, this approach can also achieve a good performance comparable with the Stanford POS tagger.", "histories": [["v1", "Wed, 21 Oct 2015 08:57:30 GMT  (55kb)", "http://arxiv.org/abs/1510.06168v1", "rejected by ACL 2015 short, score: 4,3,2 (full is 5)"]], "COMMENTS": "rejected by ACL 2015 short, score: 4,3,2 (full is 5)", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["peilu wang", "yao qian", "frank k soong", "lei he", "hai zhao"], "accepted": false, "id": "1510.06168"}, "pdf": {"name": "1510.06168.pdf", "metadata": {"source": "CRF", "title": "Part-of-Speech Tagging with Bidirectional Long Short-Term Memory Recurrent Neural Network", "authors": ["Peilu Wang", "Yao Qian", "Frank K. Soong", "Lei He", "Hai Zhao"], "emails": ["helei}@microsoft.com,", "zhaohai@cs.sjtu.edu.cn,", "yqian@ets.org"], "sections": [{"heading": null, "text": "ar Xiv: 151 0.06 168v 1 [cs.C L] 21 Oct 201 5"}, {"heading": "1 Introduction", "text": "It is a kind of recurrent neural network (RNN) that can include contextual information from a long period of before and after inputs. However, it has proven to be a powerful model for sequential labeling tasks (Yao et al., 2013), and machine translation (Sundermeyer et al., 2014), it has helped achieve superior performance in language modeling (Sundermeyer et al., 2015), language understanding (Yao et al., 2013), and machine translation (Sundermeyer et al.). Since part of the language (POS) tagged, it is a typical sequential labeling task, it seems natural that BLSTM RNN can also be effective for this task."}, {"heading": "2 Methods", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "2.1 BLSTM RNN for POS Tagging", "text": "Given a sentence w1, w2,..., wn with tags y1, y2,..., yn, BLSTM RNN is used to predict the tag probability distribution of each word. It is illustrated in Figure 1. Here, wi is the only hot representation of the current word. It is a binary vector of the dimension | V | where V is the vocabulary. To reduce the letters in the input word, each letter is transferred to uppercase. To retain the uppercase information, a function f (wi) is introduced to specify the original case information of the word wi. Specifically, f (wi) returns a three-dimensional binary vector to determine whether wi is full-case, uppercase or leading with a capital letter. The neural network input vector Ii is called: Ii = W1wi + W2f task (wi), where W2f matrixes are that connect two levels."}, {"heading": "2.2 Word Embedding", "text": "In this section, we propose a novel method of training BLSTM RNN to embed words in unlabeled data. BLSTM RNN is also used for a tagging task, but has only two types of tags that can be predicted: incorrect / correct. Input is a sequence of words that is a normal sentence in which some words are replaced by randomly selected words. In these replaced words, their tags are 0 (incorrect), and in those that are not replaced, their tags are 1 (correct). Although it is possible that some of the replaced words are reasonable in the sentence, they are still considered \"incorrect.\" Then, BLSTM RNN is trained to minimize the binary classification error on the training corpus. Neural network structure is the same as in Figure 1. When the neural network is trained, W1 contains all the trained word embeddings."}, {"heading": "3 Experiments", "text": "BLSTM RNN systems in our experiments are implemented with CURRENT (Weninger et al., 2014), a machine learning library for RNN that performs GPU acceleration; the input layer activation function is identity function, the hidden layer is logistics function, while the output layer uses the Softmax function for multi-class classification."}, {"heading": "3.1 Corpora", "text": "The portion of the tagged data used in our experiments is the data from the Wall Street Journal of Penn Treebank III (Marcus et al., 1993). Training, development and testing records are broken down by setup into (Collins, 2002). Table 1 lists the detailed information of the three records. To train Word embedding, we use North American news (Graff, 2008) as unlabeled data. This corpus contains about 536 million words. It is tokenized with the Penn Treebank Tokenizer Script 1. Any consecutive digits occurring within a word are replaced by the symbol \"#.\" For example, both words \"Tel192\" and \"Tel6\" are transferred to the same word \"Tel #.\""}, {"heading": "3.2 Hidden Layer Size", "text": "The size of the input layer is set to 100 and the size of the output layer is set to 45 in all experiments. The accuracy of the WSJ test set is shown in Figure 2. It shows that the size of the hidden layer has an alimitated effect on performance when it becomes large enough. To maintain a good compromise between accuracy, model size and runtime, we select 100, which is the smallest layer size, to achieve \"reasonable\" performance as a hidden layer size in all subsequent experiments."}, {"heading": "3.3 POS Tagging Accuracies", "text": "In fact, most people who go in search of a different path show a higher accuracy (97.44%), but this work relies on several educated people who combine their knowledge and skills with each other. Here, we focus on individual models that do not serve as a starting point. Furthermore, there is a higher accuracy (97.44%), but this work is based on the individual people who combine their knowledge and skills."}, {"heading": "3.4 Different Word Embeddings", "text": "In this experiment, six types of well-trained word embeddings are evaluated; the basic information of the word embeddings and results involved is listed in Table 3, with RCV1 representing the Reuters Corpus Volume 1 message sentence; the OOV column (from the vocabulary) indicates the vocabulary rate of BLSTM RNN for the POS embeddings, which is not covered by external word embeddings vocabulary; the use of word embeddings is the same as in the BLSTM RNN + WE experiment, except that the input coating size here corresponds to the dimension of external word embeddings; all word embeddings result in higher accuracy; however, none of them can improve BLSTM RNN embeddings to achieve competitive accuracy, despite larger companies on which they are trained, and lower OOV rate. (Pennington et al., 2014b.) (912% accuracy is higher than that of the Tanova, but it is lower on them)."}, {"heading": "4 Conclusions", "text": "In this paper, BLSTM RNN is proposed for POS tagging and training in word embedding. In combination with word embedding, which is trained on large, unlabeled data, this approach obtains state-of-the-art accuracy on the WSJ test set without using rich morphological features. BLSTM RNN with word embedding is expected to be an effective solution for tagging tasks and is worth further research."}], "references": [{"title": "Neural probabilistic language models", "author": ["Bengio et al.2006] Yoshua Bengio", "Holger Schwenk", "Jean-S\u00e9bastien Sen\u00e9cal", "Fr\u00e9deric Morin", "JeanLuc Gauvain"], "venue": "In Innovations in Machine Learning,", "citeRegEx": "Bengio et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Bengio et al\\.", "year": 2006}, {"title": "Discriminative Training Methods for Hidden Markov Models: Theory and Experiments with Perceptron Algorithms", "author": ["Michael Collins"], "venue": "In EMNLP,", "citeRegEx": "Collins.,? \\Q2002\\E", "shortCiteRegEx": "Collins.", "year": 2002}, {"title": "A unified architecture for natural language processing: Deep neural networks with multitask learning", "author": ["Collobert", "Weston2008] Ronan Collobert", "Jason Weston"], "venue": "In Proceedings of the 25th international conference on Machine learning,", "citeRegEx": "Collobert et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Collobert et al\\.", "year": 2008}, {"title": "Natural Language Processing (almost) from scratch", "author": ["Jason Weston", "L\u00e9on Bottou", "Michael Karlen", "Koray Kavukcuoglu", "Pavel Kuksa"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "Collobert et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Collobert et al\\.", "year": 2011}, {"title": "TTS synthesis with bidirectional LSTM based recurrent neural networks", "author": ["Fan et al.2014] Yuchen Fan", "Yao Qian", "Fenglong Xie", "Frank K. Soong"], "venue": "In INTERSPEECH,", "citeRegEx": "Fan et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Fan et al\\.", "year": 2014}, {"title": "Prosody contour prediction with long short-term memory, bi-directional, deep recurrent neural networks", "author": ["Asaf Rendel", "Bhuvana Ramabhadran", "Ron Hoory"], "venue": null, "citeRegEx": "Fernandez et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Fernandez et al\\.", "year": 2014}, {"title": "Supervised sequence labelling with recurrent neural networks, volume", "author": ["Alex Graves"], "venue": null, "citeRegEx": "Graves.,? \\Q2012\\E", "shortCiteRegEx": "Graves.", "year": 2012}, {"title": "Long short-term memory", "author": ["Hochreiter", "Schmidhuber1997] Sepp Hochreiter", "J\u00fcrgen Schmidhuber"], "venue": "Neural computation,", "citeRegEx": "Hochreiter et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Hochreiter et al\\.", "year": 1997}, {"title": "Structured Perceptron with Inexact Search", "author": ["Huang et al.2012] Liang Huang", "Suphan Fayong", "Yang Guo"], "venue": "In HLT-NAACL,", "citeRegEx": "Huang et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Huang et al\\.", "year": 2012}, {"title": "Building a large annotated corpus of English: The Penn Treebank", "author": ["Mary Ann Marcinkiewicz", "Beatrice Santorini"], "venue": null, "citeRegEx": "Marcus et al\\.,? \\Q1993\\E", "shortCiteRegEx": "Marcus et al\\.", "year": 1993}, {"title": "Recurrent neural network based language model", "author": ["Martin Karafi\u00e1t", "Lukas Burget", "Jan Cernock\u00fd", "Sanjeev Khudanpur"], "venue": "In INTERSPEECH,", "citeRegEx": "Mikolov et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2010}, {"title": "Efficient estimation of word representations in vector space. In ICLR", "author": ["Kai Chen", "Greg Corrado", "Jeffrey Dean"], "venue": null, "citeRegEx": "Mikolov et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Fast high-accuracy part-of-speech tagging by independent classifiers", "author": ["Robert Moore"], "venue": "In Coling,", "citeRegEx": "Moore.,? \\Q2014\\E", "shortCiteRegEx": "Moore.", "year": 2014}, {"title": "Glove: Global Vectors for Word Representation", "author": ["Richard Socher", "Christopher Manning"], "venue": "In EMNLP,", "citeRegEx": "Pennington et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Pennington et al\\.", "year": 2014}, {"title": "GloVe", "author": ["Pennington", "Richard Socher", "Christopher D. Manning."], "venue": "http://nlp.stanford.edu/projects/glove/.", "citeRegEx": "Pennington et al\\.,? 2014b", "shortCiteRegEx": "Pennington et al\\.", "year": 2014}, {"title": "A Maximum Entropy Model for Part-Of-Speech Tagging", "author": ["Adwait Ratnaparkhi"], "venue": "In EMNLP,", "citeRegEx": "Ratnaparkhi.,? \\Q1996\\E", "shortCiteRegEx": "Ratnaparkhi.", "year": 1996}, {"title": "Bidirectional recurrent neural networks", "author": ["Schuster", "Paliwal1997] Mike Schuster", "Kuldip K Paliwal"], "venue": "Signal Processing, IEEE Transactions", "citeRegEx": "Schuster et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Schuster et al\\.", "year": 1997}, {"title": "Guided Learning for Bidirectional Sequence Classification", "author": ["Shen et al.2007] Libin Shen", "Giorgio Satta", "Aravind Joshi"], "venue": "In ACL,", "citeRegEx": "Shen et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Shen et al\\.", "year": 2007}, {"title": "Semi-Supervised Training for the Averaged Perceptron POS Tagger", "author": ["Jan Haji\u010d", "Jan Raab", "Miroslav Spousta"], "venue": "In EACL,", "citeRegEx": "Spoustov\u00e1 et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Spoustov\u00e1 et al\\.", "year": 2009}, {"title": "LSTM Neural Networks for Language Modeling", "author": ["Ralf Schl\u00fcter", "Hermann Ney"], "venue": null, "citeRegEx": "Sundermeyer et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Sundermeyer et al\\.", "year": 2012}, {"title": "Translation Modeling with Bidirectional Recurrent Neural Networks", "author": ["Tamer Alkhouli", "Joern Wuebker", "Hermann Ney"], "venue": "In EMNLP,", "citeRegEx": "Sundermeyer et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Sundermeyer et al\\.", "year": 2014}, {"title": "From feedforward to recurrent lstm neural networks for language modeling", "author": ["Hermann Ney", "Ralf Schluter"], "venue": null, "citeRegEx": "Sundermeyer et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Sundermeyer et al\\.", "year": 2015}, {"title": "Feature-Rich Part-of-Speech Tagging with a Cyclic Dependency Network", "author": ["Dan Klein", "Christopher D. Manning", "Yoram Singer"], "venue": "In HLT-NAACL", "citeRegEx": "Toutanova et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Toutanova et al\\.", "year": 2003}, {"title": "Word Representations: A Simple and General Method for Semi-supervised Learning", "author": ["Lev Ratinov", "Yoshua Bengio"], "venue": "In ACL,", "citeRegEx": "Turian et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Turian et al\\.", "year": 2010}, {"title": "Word representations for NLP", "author": ["Lev Ratinov", "Yoshua Bengio."], "venue": "http://metaoptimize.com/projects/wordreprs/.", "citeRegEx": "Ratinov and Bengio.,? 2010b", "shortCiteRegEx": "Ratinov and Bengio.", "year": 2010}, {"title": "Introducing CURRENNT\u2013the Munich open-source CUDA RecurREnt Neural Network Toolkit", "author": ["Johannes Bergmann", "Bj\u00f6rn Schuller"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Weninger et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Weninger et al\\.", "year": 2014}, {"title": "Recurrent neural networks for language understanding", "author": ["Yao et al.2013] Kaisheng Yao", "Geoffrey Zweig", "MeiYuh Hwang", "Yangyang Shi", "Dong Yu"], "venue": "In INTERSPEECH,", "citeRegEx": "Yao et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Yao et al\\.", "year": 2013}], "referenceMentions": [{"referenceID": 19, "context": "For applications in natural language processing (NLP), it has helped achieve superior performance in language modeling (Sundermeyer et al., 2012; Sundermeyer et al., 2015), language understanding (Yao et al.", "startOffset": 119, "endOffset": 171}, {"referenceID": 21, "context": "For applications in natural language processing (NLP), it has helped achieve superior performance in language modeling (Sundermeyer et al., 2012; Sundermeyer et al., 2015), language understanding (Yao et al.", "startOffset": 119, "endOffset": 171}, {"referenceID": 26, "context": ", 2015), language understanding (Yao et al., 2013), and machine translation (Sundermeyer et al.", "startOffset": 32, "endOffset": 50}, {"referenceID": 20, "context": ", 2013), and machine translation (Sundermeyer et al., 2014).", "startOffset": 33, "endOffset": 59}, {"referenceID": 3, "context": "It is considered containing part of syntactic and semantic information and has shown a very attractive feature for various of language processing tasks (Collobert and Weston, 2008; Turian et al., 2010a; Collobert et al., 2011).", "startOffset": 152, "endOffset": 226}, {"referenceID": 0, "context": "Word embedding can be obtained by training a neural network model, especially, a neural network language model (Bengio et al., 2006; Mikolov et al., 2010) or a neural network designed for a specific task (Collobert et al.", "startOffset": 111, "endOffset": 154}, {"referenceID": 10, "context": "Word embedding can be obtained by training a neural network model, especially, a neural network language model (Bengio et al., 2006; Mikolov et al., 2010) or a neural network designed for a specific task (Collobert et al.", "startOffset": 111, "endOffset": 154}, {"referenceID": 3, "context": ", 2010) or a neural network designed for a specific task (Collobert et al., 2011; Mikolov et al., 2013a; Pennington et al., 2014a).", "startOffset": 57, "endOffset": 130}, {"referenceID": 6, "context": "(Graves, 2012) and therefore is skipped in this paper.", "startOffset": 0, "endOffset": 14}, {"referenceID": 25, "context": "BLSTM RNN systems in our experiments are implemented with CURRENT (Weninger et al., 2014), a machine learning library for RNN which adopts GPU acceleration.", "startOffset": 66, "endOffset": 89}, {"referenceID": 9, "context": "The part-of-speech tagged data used in our experiments is the Wall Street Journal data from Penn Treebank III (Marcus et al., 1993).", "startOffset": 110, "endOffset": 131}, {"referenceID": 1, "context": "(Collins, 2002).", "startOffset": 0, "endOffset": 15}, {"referenceID": 22, "context": "(Toutanova et al., 2003) is one of the most commonly used approaches which is also known as Stanford tagger.", "startOffset": 0, "endOffset": 24}, {"referenceID": 8, "context": "(Huang et al., 2012) is the system reports best accuracy on WSJ test set", "startOffset": 0, "endOffset": 20}, {"referenceID": 22, "context": "html Sys Acc (%) (Toutanova et al., 2003) 97.", "startOffset": 17, "endOffset": 41}, {"referenceID": 8, "context": "24 (Huang et al., 2012) 97.", "startOffset": 3, "endOffset": 23}, {"referenceID": 3, "context": "35 (Collobert et al., 2011) NN 96.", "startOffset": 3, "endOffset": 27}, {"referenceID": 3, "context": "36 (Collobert et al., 2011) NN+WE 97.", "startOffset": 3, "endOffset": 27}, {"referenceID": 18, "context": "In fact, (Spoustov\u00e1 et al., 2009) reports a higher accuracy (97.", "startOffset": 9, "endOffset": 33}, {"referenceID": 12, "context": "Besides, (Moore, 2014) (97.", "startOffset": 9, "endOffset": 22}, {"referenceID": 17, "context": "34%) and (Shen et al., 2007) (97.", "startOffset": 9, "endOffset": 28}, {"referenceID": 8, "context": "These two systems plus (Huang et al., 2012) are considered as current state-of-the-art systems.", "startOffset": 23, "endOffset": 43}, {"referenceID": 3, "context": "In contrast, (Collobert et al., 2011) NN only uses word form and capital features.", "startOffset": 13, "endOffset": 37}, {"referenceID": 3, "context": "(Collobert et al., 2011) NN+WE also incorporates word embeddings trained on unlabeled data like our approach.", "startOffset": 0, "endOffset": 24}, {"referenceID": 3, "context": "The main difference is that (Collobert et al., 2011) uses feedforward neural network instead of BLSTM RNN.", "startOffset": 28, "endOffset": 52}, {"referenceID": 3, "context": "However, BLSTM-RNN surpasses (Collobert et al., 2011) NN which is also neural network based method and uses the same input features.", "startOffset": 29, "endOffset": 53}, {"referenceID": 5, "context": "with (Fernandez et al., 2014; Fan et al., 2014), in which BLSTM RNN outperforms feedforward neural network.", "startOffset": 5, "endOffset": 47}, {"referenceID": 4, "context": "with (Fernandez et al., 2014; Fan et al., 2014), in which BLSTM RNN outperforms feedforward neural network.", "startOffset": 5, "endOffset": 47}, {"referenceID": 14, "context": "86 (Pennington et al., 2014b)1 100 400K Wiki (6B) 0.", "startOffset": 3, "endOffset": 29}, {"referenceID": 14, "context": "12 (Pennington et al., 2014b)2 100 1193K Twitter (27B) 0.", "startOffset": 3, "endOffset": 29}, {"referenceID": 22, "context": "WE(all) reduces over 20% error rate of BLSTM-RNN and lets the result comparable with (Toutanova et al., 2003).", "startOffset": 85, "endOffset": 109}, {"referenceID": 12, "context": "art systems (Moore, 2014; Shen et al., 2007; Huang et al., 2012) all utilize morphological features proposed in (Ratnaparkhi, 1996) which involves n-gram prefix and suffix (n = 1 to 4).", "startOffset": 12, "endOffset": 64}, {"referenceID": 17, "context": "art systems (Moore, 2014; Shen et al., 2007; Huang et al., 2012) all utilize morphological features proposed in (Ratnaparkhi, 1996) which involves n-gram prefix and suffix (n = 1 to 4).", "startOffset": 12, "endOffset": 64}, {"referenceID": 8, "context": "art systems (Moore, 2014; Shen et al., 2007; Huang et al., 2012) all utilize morphological features proposed in (Ratnaparkhi, 1996) which involves n-gram prefix and suffix (n = 1 to 4).", "startOffset": 12, "endOffset": 64}, {"referenceID": 15, "context": ", 2012) all utilize morphological features proposed in (Ratnaparkhi, 1996) which involves n-gram prefix and suffix (n = 1 to 4).", "startOffset": 55, "endOffset": 74}, {"referenceID": 17, "context": "Moreover, (Shen et al., 2007) also involves prefix and suffix of length from 5 to 9.", "startOffset": 10, "endOffset": 29}, {"referenceID": 12, "context": "(Moore, 2014) adds extra elaborately designed features, including flags indicating if word ends with \u2212ed or \u2212ing, etc.", "startOffset": 0, "endOffset": 13}, {"referenceID": 14, "context": "(Pennington et al., 2014b)1", "startOffset": 0, "endOffset": 26}, {"referenceID": 22, "context": "12%) has the highest accuracy among them but it is still lower than (Toutanova et al., 2003) (97.", "startOffset": 68, "endOffset": 92}], "year": 2015, "abstractText": "Bidirectional Long Short-Term Memory Recurrent Neural Network (BLSTMRNN) has been shown to be very effective for tagging sequential data, e.g. speech utterances or handwritten documents. While word embedding has been demoed as a powerful representation for characterizing the statistical properties of natural language. In this study, we propose to use BLSTM-RNN with word embedding for part-of-speech (POS) tagging task. When tested on Penn Treebank WSJ test set, a state-of-the-art performance of 97.40 tagging accuracy is achieved. Without using morphological features, this approach can also achieve a good performance comparable with the Stanford POS tagger.", "creator": "LaTeX with hyperref package"}}}