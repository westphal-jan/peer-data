{"id": "1510.06335", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "21-Oct-2015", "title": "Time-Sensitive Bayesian Information Aggregation for Crowdsourcing Systems", "abstract": "Crowdsourcing systems commonly face the problem of aggregating multiple judgments provided by possibly unreliable workers. In addition, several aspects of the design of efficient crowdsourcing processes, such as defining worker's bonuses, fair prices and time limits of the tasks, involve the knowledge of the actual duration of a specific task. In this work, we introduce a new time{sensitive Bayesian aggregation method that simultaneously estimates a task's duration and obtains reliable aggregations of crowdsourced judgments. Our method builds on the key insight that the time taken by a worker to perform a task is an important indicator of the likely quality of the produced judgment. To capture this, our model uses latent variables to represent the uncertainty about the workers' completion time, the tasks' duration and the workers' accuracy. To relate the quality of a judgment to the time a worker spends on a task, our model assumes that each task is completed within a latent time window within which all workers with a propensity to valid labelling are expected to submit their judgments. In contrast, workers with a lower propensity to valid labelling, such as spammers, bots or lazy labellers, are assumed to perform tasks considerably faster or slower than the time required by normal workers. Specifically, we use efficient message- passing Bayesian inference to learn approximate posterior probabilities of (i) the confusion matrix of each worker, (ii) the propensity to valid labelling of each worker, (iii) the unbiased duration of each task and (iv) the true label of each task. Using two real-world public datasets for entity linking tasks, we show that our method produces up to 15% more accurate classifications and up to 100% more informative estimates of a task's duration compared to state{of{the{art methods.", "histories": [["v1", "Wed, 21 Oct 2015 16:42:55 GMT  (855kb,D)", "https://arxiv.org/abs/1510.06335v1", null], ["v2", "Mon, 18 Apr 2016 21:09:58 GMT  (2418kb,D)", "http://arxiv.org/abs/1510.06335v2", null]], "reviews": [], "SUBJECTS": "cs.AI cs.LG", "authors": ["matteo venanzi", "john guiver", "pushmeet kohli", "nick jennings"], "accepted": false, "id": "1510.06335"}, "pdf": {"name": "1510.06335.pdf", "metadata": {"source": "CRF", "title": "Time-Sensitive Bayesian Information Aggregation for Crowdsourcing Systems", "authors": ["Matteo Venanzi", "John Guiver", "Pushmeet Kohli", "Nicholas R. Jennings"], "emails": ["mavena@microsoft.com", "joguiver@microsoft.com", "pkohli@microsoft.com", "n.jennings@imperial.ac.uk"], "sections": [{"heading": "1. Introduction", "text": "This year, it has come to the point where you feel you can go to the top without being able to go to the top."}, {"heading": "2. Preliminaries", "text": "Consider a group of K workers who classify N objects into C possible classes - all our symbols are listed in Table 1. Suppose k workers file a judgment (k). (k) Suppose k workers (k workers) make a judgment (k workers). (k workers) Suppose it is the time taken by k workers (k workers) for the production of c workers (k workers). (k workers) Suppose it is the time taken by k workers for the production of c workers (k workers). (k workers) Suppose it is the time taken by k workers (k workers) for the production of c workers (k workers). (k workers) Suppose it is the time spent by the workers. (K workers) Assumptions that introduce key features of the BCC model that are relevant to our method. (Kim & Ghahramani, 2012), BCC is a method that includes multiple judgments produced by independent workers. (We now have several key features of the BCC model that are relevant to our BCC model, which includes Bhahi, 2012."}, {"heading": "3. Analysis of Workers\u2019 Time Spent on Judgments", "text": "In contrast to previous work in this area (Demartini, Difallah, & Cudre \u0301 -Mauroux, 2012; Wang, Faridani, & Ipeirotis, 2011), we extend our analysis of quality time responses to both specific task instances and the overall task, providing important insights for shaping our time-sensitive aggregation model. To this end, we are looking at two public datasets generated from a widespread NLP application by crowdsourcing companies that combine tasks."}, {"heading": "3.1 The Datasets", "text": "However, the dataset was collected using AMT, where each worker was asked to classify whether a single URI was either irrelevant (0) or relevant (1) to a single company. It includes the timestamps of acceptance and delivery of each judgment. In addition, gold standard labels were collected from expert editors for all tasks. No information was published regarding the limitations of the labor pool, even though all workers are known to live in India, and each worker was paid $0.01 per judgment. In total, 11,205 judgments were collected from a small pool of 25 workers, giving this dataset a moderately high number of judgments per employee, as detailed in Table 2. Figure 2 shows that the vast majority of the tasks received 5 judgments."}, {"heading": "3.2 Time Spent on Task versus Judgment Accuracy", "text": "We want to analyze the distribution of working time and the correctness of judgments in both countries. \"<.eb >.eu, we are focusing on the two sets of data, ZC-US and ZC-IN with binary names. In fact, the binary nature of these two sets of data allows us to analyze accuracy at a higher level of detail, i.e., in terms of the accuracy and recall of workers\" judgments and the time spent on their production. Specifically, Figure 2 shows the cumulative distribution of precision and the recall of judgments selected by a specific time threshold (x-axis) in relation to gold standard labels. Here is the precision of the rupture of true positive classifications over all returned positive + false positives) and the recall is the number of truly positive classifications divided by the number of positive samples. Similar to Demartini et al (2012), we find that the accuracy at the extremes of the distributions of time is lower."}, {"heading": "4. The BCCTime Model", "text": "Based on the above results of the time analysis of employee assessments, we observed that different types of quality time trends occur for certain tasks. However, in order to correct this, the standard BCC, as well as any other existing aggregation models that do not take this information into account, must not be able to draw conclusions about the likely duration of a task. To correct this, the standard BCC must be expanded to include these trends in the aggregation of mass assessments. To this end, the model must be flexible enough to identify workers who not only have imperfect skills but may not have the intention of making a valid attempt to complete a task. This further increases uncertainty about the reliability of the data. In this section, we describe our Bayesian classification combination model over time (BCCTime). Specifically, we describe the three components of the model with respect to (i) the representation of unknown workers \"propensity for valid labeling, (ii) the unpredictability of labor followed by (ii) the unpredictability of the assessments of the employee."}, {"heading": "4.1 Modelling Workers\u2019 Propensity To Valid Labelling", "text": "In view of the uncertainty as to an employee's intention to make valid judgments, we introduce the latent variable \"k\" (0, 1), which represents the employee's inclination to make a valid labelling attempt for a given task. In this way, the model is able to explain an employee's unreliability in a natural way, not only on the basis of his imperfect skills, but also on the basis of his attitude to the correct handling of a task. In contrast, \"k\" means close to zero that the employee tends to do his best to make valid judgments, which means that he behaves similarly to a spammer. Specifically, this means that only workers with a high propensity for valid labelling provide valid inputs that are valid for the task and valid for the duration of the task."}, {"heading": "4.2 Modelling Workers\u2019 Judgments", "text": "Here we describe the part of the model that deals with the generative process of mass judgments from the confusion matrix and worker inclination. Intuitively, only those judgments that are associated with valid labeling attempts should be taken into account in order to estimate the final labels, which means that any judgement can be generated from two different processes, depending on whether it comes from a valid labeling attempt or not. To capture this in the BCCTime generative model, a hybrid model is used to switch between these two cases, which are generated by v (k) i. For the first case of a valid labeling attempt, i.e. v (k) i = 1, the judgment is generated by the worker's confusion matrix according to the standard BCC model. Therefore, we assume that c (k) i is generated for the same model described for BCC (Equation 2), including v (k) times in the conditional (k) (i)."}, {"heading": "4.3 Modelling Workers\u2019 Completion Time", "text": "As shown in Section 3, the duration of a task can be defined as the interval at which workers are more likely to make high-quality assessments. However, due to the dependence of the duration on the characteristics of the task, the requirement is that such an interval does not have to be constant across all tasks. To model this, we define a lower or higher variability in the duration of the completion time, \u03bbi, for the period in which the duration of i is represented. Both thresholds per task are latent variables that need to be learned during the training period. Then, the tasks with a lower or higher variability in their duration can be represented by the values of their time thresholds. In this setting, all valid labeling attempts made by the workers are completed within the confusion interval detailed by these thresholds. Formally, we represent the probability that the completion time (k) is greater than the probability that the completion time is i."}, {"heading": "4.4 Probabilistic Inference", "text": "In order to draw Bayesian conclusions about all unknown quantities, we must provide previous distributions for the latent parameters of BCCTime (2001). Following the structure of the model, we can select conjugated distributions for all these parameters to allow a better traceable inference of their posterior probabilities. Therefore, the previous p-value funnel with hyperparameters p0: (true label prior) p-value Dir (p | p0) (10) The priors of s and \u03c0 (k) c are also funnel distributed with hyperparameters s0 and (k) c, 0 respectively: (spammer label prior) s-value Dir (s | s0) (conferix matrix prior) Dir (k) c-value Dir (k) c-value) c-value (k) c-value) (12) Then it has a Beta previous Beta-value \u03b20: (worker-value \u03b20)."}, {"heading": "5. Experimental Evaluation", "text": "Once we have described our model, we will test its performance in terms of classification accuracy and ability to learn the duration of tasks in real-world crowdsourcing experiments. Using the data sets described in Section 3, we will conduct experiments in the following experimental setup."}, {"heading": "5.1 Benchmarks", "text": "We look at a set of benchmarks consisting of three popular baselines (Majority voting, Vote distribution and Random) and three state-of-the-art aggregation methods (One coin, BCC and CBCC) commonly used in crowdsourcing applications. This method represents the accuracy of an employee with a single reliability parameter (or workforce), with the workforce returning the correct response with the probability given by the coin and the incorrect response with reverse probability. As a result, this method is applicable only to binary datasets. Crucially, this model represents the core mechanism of several existing methods, including Whitehill10. Alternative inference methods such as Gibbs sampling or Variational Bayes can be trivially applied to our model."}, {"heading": "5.2 Accuracy Metrics", "text": "We evaluate the classification accuracy of the methods tested, as measured by the Area Under the ROC Curve (AUC) for ZC-US and ZC-IN, as well as the average recall for WS-AMT. The former is a standard accuracy measure to evaluate the performance of binary classifiers over a range of discriminatory thresholds applied to their predictive class probabilities (Hanley & McNeil, 1982), which lends itself well to the two ZenCrowd binary datasets. The latter is the averaged recall across class categories (?), which is the most important measure to evaluate the likely methods used in the CrowdFlower task in 2013 on a dataset equivalent to WS-AMT (see Section 3.1)."}, {"heading": "5.3 Results", "text": "Table 3 reports the AUC of the seven algorithms on the ZenCrowd datasets. Specifically, it shows that BCCTime and BCCPropensity have the highest accuracy in both datasets: Their AUC is 11% higher in ZC-IN and 8% higher in ZC-US, respectively, compared to the other methods. Of the two, BCCTime is the best method, with an improvement of 13% in ZC-IN and 1% in ZC-US. Similarly, Table 4 reports the average recall of the methods in WS-AMT, which shows that BCCTime has the highest average recall rate, which is 2% higher than the second-best benchmark (vote distribution) and 4% higher than BCCPropensity. This means that the inferences to the time thresholds that already provide valuable information about the tasks extracted from the judgments adds an additional quality improvement to aggregated labels."}, {"heading": "6. Related Work", "text": "In recent years, much of the literature has focused on the development of intelligent data aggregation methods to help applicants combine assessments from multiple workers. In general, existing methods of assumptions and their complexity differ in modelling the various aspects of labeling noise. Interested readers can refer to the study of Sheshadri and Lease (2013) and the summary in Table 6, which lists the most common methods and their comparison with our approach. Some of these methods are able to handle both binary classification problems, i.e. when workers have to vote on objects between two possible classes, and multi-class classification problems, i.e. when workers have to vote on objects between more than two classes."}, {"heading": "7. Conclusions", "text": "We have it in our hands, as we have experienced in recent years. \"-\" We have made it. \"-\" We have made it. \"-\" We have made it. \"-\" We have made it. \"-\" We have made it. \"-\" We have made it. \"-\" We have made it. \"-\" We have made it. \"-\" We have made it. \"-\" We have made it. \"-\" We have made it. \"-\" We have made it. \"-\" We have made it. \"-\" We have made it. \"-\" We have made it. \"-\" We have made it. \"-\" We have made it. \"-\" We have made it. \"-\" We have made it. \""}], "references": [{"title": "Crowdsourcing for relevance evaluation", "author": ["O. Alonso", "D.E. Rose", "B. Stewart"], "venue": "In ACM SigIR Forum,", "citeRegEx": "Alonso et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Alonso et al\\.", "year": 2008}, {"title": "How to grade a test without knowing the answers\u2014a Bayesian graphical model for adaptive crowdsourcing and aptitude testing", "author": ["Y. Bachrach", "T. Graepel", "T. Minka", "J. Guiver"], "venue": "In Proceedings of the 29th International Conference on Machine Learning (ICML),", "citeRegEx": "Bachrach et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Bachrach et al\\.", "year": 2012}, {"title": "Soylent: a word processor with a crowd inside", "author": ["M. Bernstein", "G. Little", "R. Miller", "B. Hartmann", "M. Ackerman", "D. Karger", "D. Crowell", "K. Panovich"], "venue": "In Proceedings of the 23nd annual ACM symposium on User interface software and technology,", "citeRegEx": "Bernstein et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Bernstein et al\\.", "year": 2010}, {"title": "Learning to predict from crowdsourced data", "author": ["W. Bi", "L. Wang", "J.T. Kwok", "Z. Tu"], "venue": "In Proceedings of the 30th International Conference on Uncertainty in Artificial Intelligence (UAI)", "citeRegEx": "Bi et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Bi et al\\.", "year": 2014}, {"title": "Pattern recognition and machine learning, Vol", "author": ["C. Bishop"], "venue": "4. Springer New York.", "citeRegEx": "Bishop,? 2006", "shortCiteRegEx": "Bishop", "year": 2006}, {"title": "Crowdsourcing multi-label classification for taxonomy creation", "author": ["J. Bragg", "D.S. Weld"], "venue": "In First AAAI Conference on Human Computation and Crowdsourcing", "citeRegEx": "Bragg and Weld,? \\Q2013\\E", "shortCiteRegEx": "Bragg and Weld", "year": 2013}, {"title": "Maximum likelihood estimation of observer error-rates using the em algorithm", "author": ["A. Dawid", "A. Skene"], "venue": null, "citeRegEx": "Dawid and Skene,? \\Q1979\\E", "shortCiteRegEx": "Dawid and Skene", "year": 1979}, {"title": "Zencrowd: Leveraging probabilistic reasoning and crowdsourcing techniques for large-scale entity linking", "author": ["G. Demartini", "D.E. Difallah", "P. Cudr\u00e9-Mauroux"], "venue": "In Proceedings of the 21st international conference on World Wide Web (WWW),", "citeRegEx": "Demartini et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Demartini et al\\.", "year": 2012}, {"title": "Mechanical cheat: Spamming schemes and adversarial techniques on crowdsourcing platforms", "author": ["D.E. Difallah", "G. Demartini", "P. Cudr\u00e9-Mauroux"], "venue": "In CrowdSearch,", "citeRegEx": "Difallah et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Difallah et al\\.", "year": 2012}, {"title": "What\u2019s the right price? pricing tasks for finishing on time", "author": ["S. Faradani", "B. Hartmann", "P.G. Ipeirotis"], "venue": "In Human Computation, Vol. WS-11-11 of AAAI Workshops,", "citeRegEx": "Faradani et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Faradani et al\\.", "year": 2011}, {"title": "The meaning and use of the area under a receiver operating characteristic (roc", "author": ["J.A. Hanley", "B.J. McNeil"], "venue": "curve.. Radiology,", "citeRegEx": "Hanley and McNeil,? \\Q1982\\E", "shortCiteRegEx": "Hanley and McNeil", "year": 1982}, {"title": "Trueskill(tm): A Bayesian skill rating system", "author": ["R. Herbrich", "T. Minka", "T. Graepel"], "venue": "In Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "Herbrich et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Herbrich et al\\.", "year": 2007}, {"title": "Convex formulations of learning from crowds", "author": ["H. Kajino", "H. Kashima"], "venue": "Transactions of the Japanese Society for Artificial Intelligence,", "citeRegEx": "Kajino and Kashima,? \\Q2012\\E", "shortCiteRegEx": "Kajino and Kashima", "year": 2012}, {"title": "Combining human and machine intelligence in large-scale crowdsourcing", "author": ["E. Kamar", "S. Hacker", "E. Horvitz"], "venue": "In Proceedings of the 11th International Conference on Autonomous Agents and Multiagent Systems (AAMAS),", "citeRegEx": "Kamar et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Kamar et al\\.", "year": 2012}, {"title": "Identifying and accounting for task-dependent bias in crowdsourcing", "author": ["E. Kamar", "A. Kapoor", "E. Horvitz"], "venue": "In Third AAAI Conference on Human Computation and Crowdsourcing", "citeRegEx": "Kamar et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Kamar et al\\.", "year": 2015}, {"title": "Iterative learning for reliable crowdsourcing systems", "author": ["D. Karger", "S. Oh", "D. Shah"], "venue": "In Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "Karger et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Karger et al\\.", "year": 2011}, {"title": "In search of quality in crowdsourcing for search engine evaluation", "author": ["G. Kazai"], "venue": "Advances in information retrieval, pp. 165\u2013176. Springer.", "citeRegEx": "Kazai,? 2011", "shortCiteRegEx": "Kazai", "year": 2011}, {"title": "Bayesian classifier combination", "author": ["H. Kim", "Z. Ghahramani"], "venue": "In International Conference on Artificial Intelligence and Statistics,", "citeRegEx": "Kim and Ghahramani,? \\Q2012\\E", "shortCiteRegEx": "Kim and Ghahramani", "year": 2012}, {"title": "The wisdom of minority: discovering and targeting the right group of workers for crowdsourcing", "author": ["H. Li", "B. Zhao", "A. Fuxman"], "venue": "In Proceedings of the 23rd International Conference on World Wide Web (WWW),", "citeRegEx": "Li et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Li et al\\.", "year": 2014}, {"title": "The weighted majority algorithm", "author": ["N. Littlestone", "M.K. Warmuth"], "venue": "In 30th Annual Symposium on Foundations of Computer Science,", "citeRegEx": "Littlestone and Warmuth,? \\Q1989\\E", "shortCiteRegEx": "Littlestone and Warmuth", "year": 1989}, {"title": "Variational inference for crowdsourcing", "author": ["Q. Liu", "J. Peng", "A. Ihler"], "venue": "In Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "Liu et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Liu et al\\.", "year": 2012}, {"title": "Expectation propagation for approximate Bayesian inference", "author": ["T. Minka"], "venue": "Proceedings of the 17th Conference on Uncertainty in Artificial Intelligence (UAI), pp. 362\u2013369.", "citeRegEx": "Minka,? 2001", "shortCiteRegEx": "Minka", "year": 2001}, {"title": "A family of algorithms for approximate Bayesian inference", "author": ["T.P. Minka"], "venue": "Ph.D. thesis, Massachusetts Institute of Technology.", "citeRegEx": "Minka,? 2001", "shortCiteRegEx": "Minka", "year": 2001}, {"title": "Hac-er: a disaster response system based on human-agent collectives", "author": ["S.D. Ramchurn", "T.D. Huynh", "Y. Ikuno", "J. Flann", "F. Wu", "L. Moreau", "N.R. Jennings", "J.E. Fischer", "W. Jiang", "T Rodden"], "venue": "In 2015 International Conference on Autonomous Agents and Multiagent Systems,", "citeRegEx": "Ramchurn et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Ramchurn et al\\.", "year": 2015}, {"title": "Learning from crowds", "author": ["V. Raykar", "S. Yu", "L. Zhao", "G. Valadez", "C. Florin", "L. Bogoni", "L. Moy"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "Raykar et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Raykar et al\\.", "year": 2010}, {"title": "Gaussian process classification and active learning with multiple annotators", "author": ["F. Rodrigues", "F. Pereira", "B. Ribeiro"], "venue": "In Proceedings of the 31st International Conference on Machine Learning (ICML),", "citeRegEx": "Rodrigues et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Rodrigues et al\\.", "year": 2014}, {"title": "Get another label? Improving data quality and data mining using multiple, noisy labelers", "author": ["V. Sheng", "F. Provost", "P. Ipeirotis"], "venue": "In Proceedings of the 14th International Conference on Knowledge Discovery and Data Mining (SIGKDD),", "citeRegEx": "Sheng et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Sheng et al\\.", "year": 2008}, {"title": "Square: A benchmark for research on computing crowd consensus", "author": ["A. Sheshadri", "M. Lease"], "venue": "In Proceedings of the 1st AAAI Conference on Human Computation and Crowdsourcing (HCOMP),", "citeRegEx": "Sheshadri and Lease,? \\Q2013\\E", "shortCiteRegEx": "Sheshadri and Lease", "year": 2013}, {"title": "Dynamic bayesian combination of multiple imperfect classifiers", "author": ["E. Simpson", "S. Roberts", "I. Psorakis", "A. Smith"], "venue": "In Decision Making and Imperfection,", "citeRegEx": "Simpson et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Simpson et al\\.", "year": 2013}, {"title": "Language understanding in the wild: Combining crowdsourcing and machine learning", "author": ["E. Simpson", "M. Venanzi", "S. Reece", "P. Kohli", "J. Guiver", "S. Roberts", "N.R. Jennings"], "venue": "In 24th International World Wide Web Conference (WWW),", "citeRegEx": "Simpson et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Simpson et al\\.", "year": 2015}, {"title": "Combined Decision Making with Multiple Agents", "author": ["E. Simpson"], "venue": "Ph.D. thesis, University of Oxford.", "citeRegEx": "Simpson,? 2014", "shortCiteRegEx": "Simpson", "year": 2014}, {"title": "Efficient Budget Allocation with Accuracy Guarantees for Crowdsourcing Classification Tasks", "author": ["L. Tran-Thanh", "M. Venanzi", "A. Rogers", "N.R. Jennings"], "venue": "In The 12th International Conference on Autonomous Agents and Multi-Agent Systems (AAMAS),", "citeRegEx": "Tran.Thanh et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Tran.Thanh et al\\.", "year": 2013}, {"title": "Community-based bayesian aggregation models for crowdsourcing", "author": ["M. Venanzi", "J. Guiver", "G. Kazai", "P. Kohli", "M. Shokouhi"], "venue": "In 23rd International Conference on World Wide Web (WWW),", "citeRegEx": "Venanzi et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Venanzi et al\\.", "year": 2014}, {"title": "Estimating the completion time of crowdsourced tasks using survival analysis models", "author": ["J. Wang", "S. Faridani", "P. Ipeirotis"], "venue": "In Crowdsourcing for Search and Data Mining (CSDM),", "citeRegEx": "Wang et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2011}, {"title": "The multidimensional wisdom of crowds", "author": ["P. Welinder", "S. Branson", "S. Belongie", "P. Perona"], "venue": "In Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "Welinder et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Welinder et al\\.", "year": 2010}, {"title": "Whose vote should count more: Optimal integration of labels from labelers of unknown expertise", "author": ["J. Whitehill", "P. Ruvolo", "T. Wu", "J. Bergsma", "J.R. Movellan"], "venue": "In Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "Whitehill et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Whitehill et al\\.", "year": 2009}, {"title": "Modeling annotator expertise: Learning when everybody knows a bit of something", "author": ["Y. Yan", "R. Rosales", "G. Fung", "M. Schmidt", "G.H. Valadez", "L. Bogoni", "L. Moy", "J. Dy"], "venue": "In International Conference on Artificial Intelligence and Statistics,", "citeRegEx": "Yan et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Yan et al\\.", "year": 2010}, {"title": "Learning from the wisdom of crowds by minimax entropy", "author": ["D. Zhou", "S. Basu", "Y. Mao", "J. Platt"], "venue": "In Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "Zhou et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Zhou et al\\.", "year": 2012}, {"title": "A hidden markov model-based acoustic cicada detector for crowdsourced smartphone biodiversity monitoring", "author": ["D. Zilli", "O. Parson", "G.V. Merrett", "A. Rogers"], "venue": "Journal of Artificial Intelligence Research,", "citeRegEx": "Zilli et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Zilli et al\\.", "year": 2014}], "referenceMentions": [{"referenceID": 13, "context": "Flower3 have enabled a number of applications to hire pools of human workers to provide data to serve for training image annotation (Whitehill, Ruvolo, Wu, Bergsma, & Movellan, 2009; Welinder, Branson, Belongie, & Perona, 2010), galaxy classification4 (Kamar et al., 2012) and information retrieval systems (Alonso, Rose, & Stewart, 2008).", "startOffset": 252, "endOffset": 272}, {"referenceID": 16, "context": "When seeking to estimate this information, however, it is important to consider that some workers might not perform a task immediately and they might delay their submissions after accepting the task or, at the other extreme, they might submit a poor annotation in rapid time (Kazai, 2011).", "startOffset": 275, "endOffset": 288}, {"referenceID": 13, "context": "Such aggregated labels are often estimated in settings where the true answer of each task is never revealed, as this is the very quantity that the crowdsourcing process is trying to discover (Kamar et al., 2012).", "startOffset": 191, "endOffset": 211}, {"referenceID": 35, "context": "More sophisticated methods such as the one\u2013coin model (Karger, Oh, & Shah, 2011), GLAD (Whitehill et al., 2009), CUBAM (Welinder et al.", "startOffset": 87, "endOffset": 111}, {"referenceID": 34, "context": ", 2009), CUBAM (Welinder et al., 2010), DS (Dawid & Skene, 1979) and the Bayesian Classifier Combination (BCC) (Kim & Ghahramani, 2012) use probabilistic models that do take reliabilities into account, nor the potential labelling biases of the workers, e.", "startOffset": 15, "endOffset": 38}, {"referenceID": 29, "context": "Similarly, (Simpson et al., 2015) combined BCC with language modelling techniques for automated text sentiment analysis using crowd judgments.", "startOffset": 11, "endOffset": 33}, {"referenceID": 28, "context": "For example, in the galaxy zoo domain in which the workers classify images of celestial galaxies, the confusion matrices can detect workers who have low accuracy in classifying spiral galaxies or those who systematically classify every object as elliptical galaxies (Simpson et al., 2013).", "startOffset": 266, "endOffset": 288}, {"referenceID": 4, "context": "From this expression, it is possible to derive the predictive posterior distributions of each unobserved (latent) variable using standard integration rules for Bayesian inference (Bishop, 2006).", "startOffset": 179, "endOffset": 193}, {"referenceID": 30, "context": "However, it has been shown that, particularly for BCC models, it is possible to compute efficient approximations of these distributions using standard techniques such as Gibbs sampling (Kim & Ghahramani, 2012), variational Bayes (Simpson, 2014) and Expectation-Propagation (Venanzi et al.", "startOffset": 229, "endOffset": 244}, {"referenceID": 32, "context": "However, it has been shown that, particularly for BCC models, it is possible to compute efficient approximations of these distributions using standard techniques such as Gibbs sampling (Kim & Ghahramani, 2012), variational Bayes (Simpson, 2014) and Expectation-Propagation (Venanzi et al., 2014).", "startOffset": 273, "endOffset": 295}, {"referenceID": 32, "context": "Building on this, several extensions of BCC have been proposed for various crowdsourcing domains (Venanzi et al., 2014; Simpson et al., 2015, 2013).", "startOffset": 97, "endOffset": 147}, {"referenceID": 32, "context": "In particular, CBCC applies community\u2013based techniques to represent groups of workers with similar confusion matrices in the classifier combination process (Venanzi et al., 2014).", "startOffset": 156, "endOffset": 178}, {"referenceID": 7, "context": "1 The Datasets ZenCrowd - India (ZC-IN): contains a set of links between the names of entities extracted from news articles and uniform resource identifiers (URIs) describing the entity in Freebase7 and DBpedia8 (Demartini et al., 2012).", "startOffset": 212, "endOffset": 236}, {"referenceID": 7, "context": "ZenCrowd - USA (ZC-US): This dataset was also provided by Demartini et al. (2012) and contains judgements for the same set of tasks as ZC-IN, although the judgements were collected from AMT workers in the US.", "startOffset": 58, "endOffset": 82}, {"referenceID": 7, "context": "Similarly to Demartini et al. (2012), we find that the accuracy is lower at the extremes of the time distributions.", "startOffset": 13, "endOffset": 37}, {"referenceID": 35, "context": "Moreover, it empirically supports the theory of several existing data aggregation models (Kamar, Kapoor, & Horvitz, 2015; Whitehill et al., 2009; Bachrach, Graepel, Minka, & Guiver, 2012) that make use of these task\u2013specific features to achieve more accurate classifications in a number of crowdsourcing applications concerning, among others, galaxy classification (Kamar et al.", "startOffset": 89, "endOffset": 187}, {"referenceID": 14, "context": ", 2009; Bachrach, Graepel, Minka, & Guiver, 2012) that make use of these task\u2013specific features to achieve more accurate classifications in a number of crowdsourcing applications concerning, among others, galaxy classification (Kamar et al., 2015), image labelling (Whitehill et al.", "startOffset": 227, "endOffset": 247}, {"referenceID": 35, "context": ", 2015), image labelling (Whitehill et al., 2009) and problem solving (Bachrach et al.", "startOffset": 25, "endOffset": 49}, {"referenceID": 1, "context": ", 2009) and problem solving (Bachrach et al., 2012).", "startOffset": 28, "endOffset": 51}, {"referenceID": 6, "context": "Thus, this insight significantly extends the previous findings reported by Demartini et al. (2013) in which such a quality\u2013time trend was only observed across the entire task set.", "startOffset": 75, "endOffset": 99}, {"referenceID": 11, "context": ", the mean multiplied by the precision) to the posterior distribution of p(\u03c4 (k) i ), as shown in Table 1 in Herbrich et al. (2007). In a similar way, we model the probability of \u03c4 (k) i being greater than \u03bbi as:", "startOffset": 109, "endOffset": 132}, {"referenceID": 21, "context": "However, we can still compute approximations of such posterior distributions using standard techniques from the family of approximate Bayesian inference methods (Minka, 2001).", "startOffset": 161, "endOffset": 174}, {"referenceID": 21, "context": "In particular, we use the well-known EP algorithm (Minka, 2001) that has been shown to provide good quality approximations for BCC models (Venanzi et al.", "startOffset": 50, "endOffset": 63}, {"referenceID": 32, "context": "In particular, we use the well-known EP algorithm (Minka, 2001) that has been shown to provide good quality approximations for BCC models (Venanzi et al., 2014)10.", "startOffset": 138, "endOffset": 160}, {"referenceID": 28, "context": "It has been used in several crowdsourcing contexts including galaxy classification (Simpson et al., 2013), image annotation (Kim & Ghahramani, 2012) and disaster response (Ramchurn et al.", "startOffset": 83, "endOffset": 105}, {"referenceID": 23, "context": ", 2013), image annotation (Kim & Ghahramani, 2012) and disaster response (Ramchurn et al., 2015).", "startOffset": 73, "endOffset": 96}, {"referenceID": 32, "context": "This method has also been used in a number of crowdsourcing applications including web search evaluation and sentiment analysis (Venanzi et al., 2014).", "startOffset": 128, "endOffset": 150}, {"referenceID": 31, "context": "\u2022 Majority Voting: This is a simple yet very popular algorithm that estimates the aggregated label as the one that receives the most votes (Littlestone & Warmuth, 1989; Tran-Thanh et al., 2013).", "startOffset": 139, "endOffset": 193}, {"referenceID": 29, "context": "\u2022 Vote Distribution: This method estimates the true label based on the empirical probabilities of each class observed in the judgment set (Simpson et al., 2015).", "startOffset": 138, "endOffset": 160}, {"referenceID": 7, "context": "In particular, we refer to One coin as the unconstrained version of ZenCrowd (Demartini et al., 2012) without the two unicity and SameAs constraints defined in the original method.", "startOffset": 77, "endOffset": 101}, {"referenceID": 15, "context": "Specifically, (Karger et al., 2011) combines this model with a budget\u2013limited task allocation framework and provides strong theoretical guarantees on the asymptotical optimality of the inference of the workers\u2019 reliability and the worker-task matching.", "startOffset": 14, "endOffset": 35}, {"referenceID": 20, "context": "(Liu et al., 2012) uses a more general variational inference model that reduces to Karger et al.", "startOffset": 0, "endOffset": 18}, {"referenceID": 24, "context": "Then, these quantities may be inferred using logistic regression as in (Raykar et al., 2010) or maximum\u2013a\u2013posteriori approaches as in (Bragg & Weld, 2013).", "startOffset": 71, "endOffset": 92}, {"referenceID": 25, "context": "Alternatively, (Rodrigues et al., 2014) uses the two coin model embedded in a Gaussian process classification framework to compute the predictive probabilities of the aggregated labels and the workers\u2019 reliability using EP.", "startOffset": 15, "endOffset": 39}, {"referenceID": 35, "context": "Along the same lines, other models reason about the difficulty of a task that affects the quality of a judgment to improve the reliability of aggregated labels (Whitehill et al., 2009; Bachrach et al., 2012; Kajino & Kashima, 2012).", "startOffset": 160, "endOffset": 231}, {"referenceID": 1, "context": "Along the same lines, other models reason about the difficulty of a task that affects the quality of a judgment to improve the reliability of aggregated labels (Whitehill et al., 2009; Bachrach et al., 2012; Kajino & Kashima, 2012).", "startOffset": 160, "endOffset": 231}, {"referenceID": 35, "context": "In this area, (Whitehill et al., 2009) use a logistic regression model to incorporate the task\u2019s difficulty, together with the expertise of the worker for labelling images.", "startOffset": 14, "endOffset": 38}, {"referenceID": 1, "context": "In contrast, (Bachrach et al., 2012) use the difference between these two quantities to quantify the advantage that the worker may have in classifying the object within a joint difficulty-ability-response model.", "startOffset": 13, "endOffset": 36}, {"referenceID": 34, "context": "Additional factors, such as the worker\u2019s motivation or propensity for a particular task, are taken into account in more sophisticated models introduced by (Welinder et al., 2010; Yan, Rosales, Fung, Schmidt, Valadez, Bogoni, Moy, & Dy, 2010; Bi, Wang, Kwok, & Tu, 2014).", "startOffset": 155, "endOffset": 269}, {"referenceID": 20, "context": "The interested reader may refer to the survey by Sheshadri and Lease (2013), as well as to the summary in Table 6 that lists the most popular methods and their comparison with our approach.", "startOffset": 49, "endOffset": 76}, {"referenceID": 37, "context": "Then, (Zhou et al., 2012) extended this work to include a task\u2013specific latent matrix representing the confusability of a task as perceived by the workers.", "startOffset": 6, "endOffset": 25}, {"referenceID": 18, "context": "Similarly to CBCC, other methods leverage groups of workers with equivalent reliability to improve the quality of the aggregated labels with limited data (Li et al., 2014; Bi et al., 2014; Kajino & Kashima, 2012; Yan et al., 2010).", "startOffset": 154, "endOffset": 230}, {"referenceID": 3, "context": "Similarly to CBCC, other methods leverage groups of workers with equivalent reliability to improve the quality of the aggregated labels with limited data (Li et al., 2014; Bi et al., 2014; Kajino & Kashima, 2012; Yan et al., 2010).", "startOffset": 154, "endOffset": 230}, {"referenceID": 36, "context": "Similarly to CBCC, other methods leverage groups of workers with equivalent reliability to improve the quality of the aggregated labels with limited data (Li et al., 2014; Bi et al., 2014; Kajino & Kashima, 2012; Yan et al., 2010).", "startOffset": 154, "endOffset": 230}, {"referenceID": 33, "context": "Furthermore, earlier work introducing a method that predicts the duration of the task based on a number of available features (including the task\u2019s price, the creation time and the number of assignments) using a survival analysis model was presented by (Wang et al., 2011).", "startOffset": 253, "endOffset": 272}], "year": 2016, "abstractText": "Crowdsourcing systems commonly face the problem of aggregating multiple judgments provided by potentially unreliable workers. In addition, several aspects of the design of efficient crowdsourcing processes, such as defining worker\u2019s bonuses, fair prices and time limits of the tasks, involve knowledge of the likely duration of the task at hand. Bringing this together, in this work we introduce a new time\u2013sensitive Bayesian aggregation method that simultaneously estimates a task\u2019s duration and obtains reliable aggregations of crowdsourced judgments. Our method, called BCCTime, builds on the key insight that the time taken by a worker to perform a task is an important indicator of the likely quality of the produced judgment. To capture this, BCCTime uses latent variables to represent the uncertainty about the workers\u2019 completion time, the tasks\u2019 duration and the workers\u2019 accuracy. To relate the quality of a judgment to the time a worker spends on a task, our model assumes that each task is completed within a latent time window within which all workers with a propensity to genuinely attempt the labelling task (i.e., no spammers) are expected to submit their judgments. In contrast, workers with a lower propensity to valid labelling, such as spammers, bots or lazy labellers, are assumed to perform tasks considerably faster or slower than the time required by normal workers. Specifically, we use efficient message-passing Bayesian inference to learn approximate posterior probabilities of (i) the confusion matrix of each worker, (ii) the propensity to valid labelling of each worker, (iii) the unbiased duration of each task and (iv) the true label of each task. Using two real-world public datasets for entity linking tasks, we show that BCCTime produces up to 11% more accurate classifications and up to 100% more informative estimates of a task\u2019s duration compared to state\u2013of\u2013the\u2013art methods.", "creator": "TeX"}}}