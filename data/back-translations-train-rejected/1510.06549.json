{"id": "1510.06549", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "22-Oct-2015", "title": "Multi-GPU Distributed Parallel Bayesian Differential Topic Modelling", "abstract": "There is an explosion of data, documents, and other content, and people require tools to analyze and interpret these, tools to turn the content into information and knowledge. Topic modeling have been developed to solve these problems. Topic models such as LDA [Blei et. al. 2003] allow salient patterns in data to be extracted automatically. When analyzing texts, these patterns are called topics. Among numerous extensions of LDA, few of them can reliably analyze multiple groups of documents and extract topic similarities. Recently, the introduction of differential topic modeling (SPDP) [Chen et. al. 2012] performs uniformly better than many topic models in a discriminative setting.", "histories": [["v1", "Thu, 22 Oct 2015 09:40:54 GMT  (4984kb,D)", "http://arxiv.org/abs/1510.06549v1", null]], "reviews": [], "SUBJECTS": "cs.CL cs.DC cs.LG", "authors": ["aaron q li"], "accepted": false, "id": "1510.06549"}, "pdf": {"name": "1510.06549.pdf", "metadata": {"source": "CRF", "title": null, "authors": [], "emails": [], "sections": [{"heading": null, "text": "In fact, most of them will be able to play by the rules they have established in the past."}, {"heading": "1 Introduction 1", "text": "1.1............................................................................................................................................."}, {"heading": "2 Background 7", "text": "2.1 Scope. 23. 3. 4. 4. 5. 6. 7. 8. 8. 8. 8. 8. 5. 7. 7. 8. 8. 8. 9. 9. 9. 9. 9. 9. 9. 9. 9. 9. 9. 9. 9. 9. 9. 9. 9. 9. 9. 9. 3. 9. 9. 9. 9. 9. 9. 3. 9. 3. 3."}, {"heading": "3 Problems and Solutions 33", "text": "The personal data of each country. The personal data of each country. The personal data of each country. The personal data of each country. The personal data of each country. The personal data of each country. The personal data of each country. The personal data of each country. The personal data of each country. The personal data of each country. The personal data of each country. The personal data of each country. The personal data of each country. The personal data. The personal data of each country. The personal data of each country. The personal data of each country. The personal data of each country. The personal data of each country. The personal data of each country. The personal data of each country. The personal data of each country. The personal data of each country."}, {"heading": "4 Experiments 60", "text": "4.1. Experimental setup..........................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................."}, {"heading": "5 Conclusion 98", "text": "The superior role plays the superior role. The superior role plays the superior role. The superior role plays the superior role. The superior role plays the superior role. The superior role. The superior role plays the superior role. The superior role plays the superior role. The superior role plays the superior role. The superior role of the superior need. The superior role of the superior need. The superior role of the need. The superior role of the need. The superior role of the need. The superior role. The superior role. The superior role. The superior role. The superior role. The superior role of the need."}, {"heading": "Acknowledgments", "text": "I would like to thank Dr. Wray Buntine and Dr. Scott Sanner for their guidance and support throughout the year. Research is a journey into the unknown. Their advice and experiences have saved me many times through the journey when I was on the verge of giving up, and put me on the right path when I thought it was impossible to move forward. In particular, I would like to thank them for sharing their knowledge of many things, encouraging me to challenge myself with things others have never done, inspiring me with their research findings, and spending a great deal of their time helping me refine these theses. I would also like to thank Changyou Chen, who patiently explained his background in this field and generously shared his expertise in differential topic models. Without Changyou's tremendous amount of work previously done on different topic models, I would not be able to conduct the research with these many interesting outcomes. Finally, I would like to thank ANU and NICTA, who brought together the researchers, and have created and maintained the environment to keep things going."}, {"heading": "1.1 Overview", "text": "There is an explosion of data, documents and other content, and people need to analyse and interpret these problems in order to turn the content into information and knowledge."}, {"heading": "1.2 Introduction to Topic Modelling", "text": "Today, the amount of information available to us is much greater than our ability to process the information. The explosion of information has led to the rise of a new field of research: Information Access. People need tools to organize, search, summarize, and understand information, tools to turn information into knowledge [8]. Techniques such as theme modeling were invented to address these problems. Topic models reveal the underlying patterns in a collection of documents by analyzing the semantic content. Bayesian theme models are a class of theme models that contain multiple patterns in different exhibits, represented as Bayesian latent variables. When analyzing texts, these patterns are presented as a distribution of words, called \"Topics\" 0.25, \"\" C + + \"0.1,\". Python \"0.1,\" \""}, {"heading": "1.3 Introduction to Graphical Processing Units (GPUs)", "text": "A few years ago, the graphics processing unit (GPU) was considered just a dedicated device for rendering images or converting digital images into analog signals for monitors, most of which are used in high-end gaming PCs, by movie companies to create animations and special effects, or by large organizations to visualize their data. In recent years, people have discovered the potential computing power of GPUs ([16]). Nowadays, the GPU has already developed a programming system that allows programmers to transmit computer instructions to the GPU to harness the huge computing power within the GPU that has not previously been properly exploited ([16]). The GPU has become one of the most widely used parallel computer devices for many research and industry problems, due to its superior performance, cost efficiency, and retention efficiency for massive parallel transactions, with private organizations already having invested in the GU's top 500 list of GPU governments to have an enormous amount of money in June 2012."}, {"heading": "1.4 Motivation", "text": "Since LDA was released in the last decade, hundreds of enhancements have been made for many purposes, appearing in conferences such as ICML, NIPS, KDD, SIGIR, ICCV, and others. Many use sophisticated nonparametric statistics, and can be significantly slower than standard LDA. The problem is that the algorithms are too slow in practice. In analyzing millions of documents, supercomputers are required to get the result in a reasonable amount of time. Moreover, people prefer to get the result as quickly as possible, the runtime and cost efficiency can still make a crucial difference in many real-world situations. For example, if the analysis tool is provided as a service, it is not practical to use an expensive computing resource. In addition, people prefer to get the result as soon as possible, rather than wait in a queue for the analysis that is scheduled on supercomputers, then wait hours or days to get the result back."}, {"heading": "2.1 Scope", "text": "We assume that the reader has university-level background knowledge in general areas of computer science, statistics, and some related mathematics. In addition, to limit the length of our background chapter, we expect the reader to be familiar with standard machine learning, computer architecture, distributed and parallel computing, microprocessors, Bayesian statistics, parameter estimation, statistical inference, and Bayesian graphical models (see [20]). Wikipedia, for example, provides good coverage of these areas. In addition, we expect familiarity with basic topic models such as Latent Dirichlet Allocation (LDA) (see [1, 21]) and common performance metrics such as Perplexity (see Section 3.2.2.2), as we provide only brief explanations for these areas. While non-Bayesian topic models such as probabilistic latent semantic analysis (PLSA) [22] exist, the focus of modeling of the theoretical field has largely shifted to the topic area of Lyesida, which we use only for transferring Dilesida thematic models."}, {"heading": "2.2 Notation", "text": "Unless expressly stated otherwise, all random variables are discrete variables, all variables are non-negative real numbers or integers, and all probability distributions are discrete distributions. All Bayean graphical models such as Figure 2.3.1 use disk notation in which each rectangle represents repeating units, number of repetitions and range of repeating variables are given in the lower right corner."}, {"heading": "2.3 Existing Bayesian Topic Models", "text": "In this section I will give a brief overview of some existing models that are relevant to my research. Technical details such as model derivation, technical definitions, effects of hyperparameters, predictive probability and inference algorithm are left to the next section. A mathematical definition and conceptual description are given on the following models, sorted by their simplicity and the date on which they are created: \u2022 Latent Dirichlet Allocation (LDA) [1] \u2022 Pitman-Yor Topic Modelling (PYTM) [23] \u2022 Hierarchical Pitman-Yor Topic Modelling (HPYTM) [23] 9 \u2022 Differential Topic Modelling using Shadow Poisson Dirichlet Process (SPDP) [2] Most subject models are Unikram models (i.e. 1 gram models) - they only maintain the number of words occurring in documents and ignore the order of words."}, {"heading": "2.3.1 Latent Dirichlet Allocation (LDA)", "text": "The latent dirichlet allocation (LDA) [1] is one of the most popular models used in theme modeling due to its simplicity; the graphical model is shown in Figure 2.3.1. The generation process is as follows. ~ \u03c6k \u0445 dirichlet (~ \u03b2) used in theme modeling is: ~ K ~ \u03c9m (~ \u03b1). The graphical model is shown in Figure 2.3.1. The generation process is as follows: \u00b7 M, l = 1... Lm wm, l = 1... Multi (~ \u03b8m), l = 1... Lm is the total number of words in document m, subscript m denotes a theme index, k denotes a theme. The roles of other variables are: \u2022 ~ \u0445m: theme proportions, a vector used as a parameter of multinomial distribution in document m. It determines the probability of each theme in document m, subscript m denotes a theme index. The proportional roles of other than a document multinomial: a vector in document m."}, {"heading": "2.3.2 Pitman-Yor Topic Modelling (PYTM)", "text": "The PYTM assumes in each document that the words are drawn sequentially from a distribution derived from a Poisson Dirichlet Process (PDP) v Word Frequency Ranking (x-Axis in Log Scale) in Wikipedia (also known as the Pitman-Yor Process [3]). Topics11Figure 2.3.2: Word Frequency (y-Axis in Log Scale) v Word Frequency in Wikipedia (extracted from Wikipedia [3] under LGPL license) 12are not drawn before words - rather, the generation processes of a word and a topic are mixed together. PDP ensures that words in each document follow the characteristics of a power law that already appear before they appear exponentially more likely. Justification is based on Zipf's Law, which is developed in linguistic research that appears in large texts."}, {"heading": "2.3.3 Hierarchical Pitman-Yor Topic Modelling (HPYTM)", "text": "Hierarchical Pitman-Yor Topic Modelling (HPYTM) [23] has extended to PYTM by adopting the power law phenomenon not only in each document, but also within each topic. PDP word generation is now document-specific and not only document-specific, as is the case in PYTM. In this constellation, new words (type (2) words are no longer derived from discrete (~ \u03c6zm, l), but from a distribution generated by PDP for a specific topic. [26] The hierarchical Bayesian language model [26] still replaces some parts of PYTM with a more complicated structure, as in Figure 2.3.3 and 2.3.6. Note ~ \u03b2 and ~ digit in PYTM have been replaced by a two-tier hierarchical model. [26] The hierarchical Bayesian language model [26] still replaces some parts of PYTM by some features of LDA with a more complicated structure, as in Figure 2.6 and Figure 3.3."}, {"heading": "2.3.4 Differential Topic Modelling Using Shadow Poisson Dirich-", "text": "This is an example to illustrate the power of multiple models: we consider a situation where two different topic models must be combined, giving the model the opportunity to find similarities and differences in topics between multiple groups of documents. In this setup, the input documents are divided into several groups that share the same vocabulary. Topics are divided into all groups, but each group has its own representation of each topic. In addition, each group is allowed to have its own unique topics. In other words, all documents are assumed to be in the same group. This assumption simplifies the mathematical formulation of the topic and the predictive probabilities of the topic, but with such assumptions in place, topic models will not be able to extract information about the variation in popularity of topics and words among multiple sources of documents. The different information is important because it allows us to analyze subtle differences in opinions and perspectives across multiple documentaries."}, {"heading": "2.4 Model Derivation and Gibbs Sampler", "text": "All the topic models mentioned in the last section share some similarities in their derivatives and inference processes, because basically all of them are extensions of LDA. Gibbs \"collapsed sampler is most often chosen by authors of the above models to infer the multivariate latent variables. Although different models have different latent variables, we label them all by a latent parameter. The goal is to draw conclusions about the latent parameters (which include topic information) by estimating the latent parameters (for example, in LDA, word distributions x) and finding topic distributions x, with some data, according to Bayes rule: p (~ \u03b8 | Data) = p (~ Connection Data) p (Data), we will often apply this formula to an intractable probability distribution x. To see this, consider the simplest model of LDA."}, {"heading": "2.4.1 Notation", "text": "In this section, we use V to specify the size of the vocabulary, K to specify the total number of topics, M to specify the total number of documents, nmk: m, k to specify the number of times that the subject k has observed in document m, the first part of the subscript being a name to distinguish it from other counter variables that can also be named with n, and nkw: k, w to specify the number of times that the word w associates the subject k. We use nkw: k,. to specify the sumover dotted variables in the subscript, for example nkw: k,. = \u2211 V w = 1 nkw: k, w, and similar nmk: m,. = \u2211 K k k = 1 nmk: m, k."}, {"heading": "2.4.2 Latent Dirichlet Allocation (LDA)", "text": "The common distribution p (~ \u03b8, data) of LDA ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ p (~ \u03b2, ~ \u03b2 | \u03b1, ~ \u03b2) = p (~ w | z, ~ \u03b2) p (~ z, ~ ~ ~ ~ ~ ~ ~ \u03b22) Since ~ z does not depend on ~ \u03b2, the first term p (~ w) (~ w, ~ \u03b2) in 2.4.2 can be derived as: p (~ w, ~ z, ~ \u03b2) = p (~ w, ~ z, ~ p (~ z, ~) p (~ \u03b2) d ~ \u03c6 (\u03b1) d, p (\u03b1) p = 1 (~ \u03b2) V, w = 1) V, w + \u03b2w \u2212 nk \u2212 1 k, w \u2212 p = K (~ nk) p (~ nk) (~ \u03b2)) (2,4.3) Where nk), w \u00b2 k), w \u00b2, w \u00b2 k), p (\u00b2), p (\u00b2), p (\u00b2), b), b), z) (p)."}, {"heading": "2.4.3 Pitman-Yor Topic Modelling (PYTM)", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "2.4.3.1 Poisson Dirichlet Process", "text": "We will first give an overview of the Poisson Dirichlet process (PDP), as it forms the basis of PYTM. As mentioned in the last section, PDP generates a probability distribution from a base distribution H (.), concentration parameters \u03b3 and discount parameters d. The process is referred to by PDP (\u03b3, d, H (.). The analogy of the Chinese restaurant is as follows: In a strange Chinese restaurant that has an infinite number of tables, each table serves only one dish, and only if at least one customer is seated on that table. Waiters order each incoming customer to either sit at a table with dish j, share it among other people who are already seated there, or lead the customer to an empty table and serve a dish immediately. Waiters keep a record of the number of different dishes 1,..., J serves customers, the number of customers served with dish j, and the total number of customers who are designated by nj."}, {"heading": "2.4.3.2 Predictive Probability and Inference", "text": "Predictive probability can be derived by removing a word, similar to LDA. Here, we need both the predictive probability of the subject and the predictive probability of the word to proceed with the conclusion, because if we remove a word, we add it back later, and we have to rethink the seated arrangement when we add it back. Fortunately, the word predictive probability is given directly by CRP analogy, because in the above definition each word drawn depends on all previous words drawn: p (wm, l = j | W, Z, X) = nmw: m, j \u2212 d + nmw: m + \u03b3 + nmw: m K \u00b2 k = 1 nm, k + \u03b1m:. nkw: k, j + \u03b2k nkw: k,. + \u03b2k: k,. + \u03b2k: (2.4.6) Where 25mj times: 1 nm, k: 1 nm:., k: k: k, m \u00b2, m \u00b2, m: k,. + \u03b2k: (w: m), mj, j: w: the number of words is observed in this document, j: j: w \u00b2, m \u00b2)."}, {"heading": "2.4.4 Hierarchical Pitman-Yor Topic Modelling (HPYTM)", "text": "The derivative process is almost identical to that described in PYTM except that probabilities are calculated recursively. We will skip this section as the detail is not particularly related to SPDP. Readers should contact [23] if they are interested in the details."}, {"heading": "2.4.5 Shared Topic Modelling Using Shadow Poisson Dirichlet", "text": "In this model, the same method is not applicable, because the transformed basic measurement is no longer applied to a discrete (categorical or multinomial) distribution. To overcome this, we first introduce auxiliary variable ti, which we call multiplicity, representing the number of tables served in restaurant i (group i, theme k). It is shown by Corollary 17 in a \"restaurant.\""}, {"heading": "3.1 SPDP Computational Time Analysis", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "3.1.1 Theoretical Running Time Analysis", "text": "Suppose there are a total of N words in all documents in all groups. Suppose K represents the total number of topics to be extracted, and V represents the total number of words in the vocabulary. Suppose Gibbs sampler converges only after T iterations. Algorithm 2.1 shows for each word that all topics and all possible word connections must be scanned. Although Stirling numbers must be recursively calculated in the algorithm, it is possible to store almost all of them before the algorithm begins, which allows a constant query of complexity. In summary, the theoretical complexity of Algorithm 2.1 is a significant influence factor of total travel time. Since the transofrmation matrices P i define all possible word connections that must be scanned in the nested inner loop of Algorithm 2.1, it is a significant influence factor of total travel time. To reduce the complexity of the transmatrices P must be equal to the number of possible transmatrices in P."}, {"heading": "3.1.2 Practical Issues", "text": "In practice, a small collection of documents contains over 2000 documents in each group, consisting of over 500,000 words, and a vocabulary size of over 20,000 words. The number of topics we want to retrieve from such a collection ranges from about 30 to 100, and the collapsed Gibbs sampler needs about 2000 iterations to be able to converge. Multiplying these numbers together, we find that analyzing such a collection of documents may require at least constant times 1015 cycles if transformation matrix P is not economical. Suppose the matrix is sparse and each row or column contains no more than 20 non-zero entries. Compared to a non-sparse transformation matrix, the number of operations required is reduced to a constant time over 1012 cycles. The constant factor that takes into account memory instructions and arithmetic compilations in line 14.16, in algorithm 2.1, this could be very large, complicated data structure, for example, may exacerbate the accumulated problem of memory directives and arithmetic compilations in line 14.16."}, {"heading": "3.1.3 Parallelization Issues", "text": "Mathematically, the collapsed Gibbs sampler requires all words to be scanned sequentially, and the state of the collapsed Gibbs sampler (in our case, it is the vector p (W, Z | the rest) at each step when a word is scanned) forms a Markov chain. In other words, the probability distribution of topic and word association for the sample of the current word depends on the sample generated for previous words. Scanning multiple words in parallel with old state information breaks the rule of dependence, so mathematically there is no guarantee that the collapsed Gibbs sampler running in parallel would converge."}, {"heading": "3.2 The Goal", "text": "We are looking for a solution that addresses not only the problems mentioned above, but also fulfills a number of characteristics. It is preferred that the solution has flexible memory usage, is able to process large collections of documents without being slowed down by memory access; the solution should use parallelism so that it can be made scalable enough to run on multiple devices; the solution does not necessarily need to be accurate; a good approach that sacrifices a little accuracy but significantly improves runtime is sufficient for practical purposes; in order to maintain the balance between accuracy and speed, we also need a set of performance and quality requirements to measure the suitability of my approach, such as: \u2022 Scalability \u2022 Perplexity: a measure of the suitability of the model to the data \u2022 Topic quality and integrability: how good the topics are in terms of human understanding. In addition, it will be discussed why the popular PMI Score is not an adequate measurement of performance for our problem."}, {"heading": "3.2.1 Scalability", "text": "Since the algorithm is expected to process hundreds to millions of documents in real-world applications, it will be necessary to design a memory access architecture that provides effi-36cient access to the two dynamic variables t, n, m, q, v and the static constant variables P i (transformation matrices), Snm (Stirling numbers). If the memory requirements for storing these variables become too large, these variables can no longer be stored physically in main memory. Since algorithm 2.1 accesses the variable n and matrices the transformation matrices P i in a linear, consecutive pattern, they are the easiest to store. In comparison, there is no common pattern of how words and topics should appear within a document, giving more or less random access to the variables t, m, q, v.A good memory access architecture should divide variables into multiple regions and multiple levels of the access hierarchy, taking into account the current demand and frequency."}, {"heading": "3.2.2 Topic Performance Measure", "text": "Helplessness and mutual information value (PMI score) based on the Wikipedia corpus [31] are the two most popular metrics used in research to assess the quality of generated topic models. \u2022 PMI score based on the Wikipedia corpus calculates the meaningful relevance of the top ten words in each topic in terms of the frequency of simultaneous occurrence of corresponding words in the Wikipedia corpus. \u2022 Perplexity measures how well the generated model matches the test data. A good algorithm is expected to deliver results with a high PMI score and low perplexity. In practice, we have found in many situations that the PMI score may be unreliable, as we will soon show."}, {"heading": "3.2.2.1 PMI-Score Based on Wikipedia Corpus", "text": "Of many evaluation methods proposed in [31], the PMI score based on the Wikipedia corpus is the most consistent best performer in terms of the intrinsic semantic quality of learned topics. In [2] Chen et al. defined topic PMI score as PMIScore (~ w) = 1 45 \u2211 i < j PMI (wi, wj), where i, j {1,..., 10}, PMI (wi, wj) = topic P (wi, wj) P (wj) and P (wi, wj) is defined as word wi and wj appears in the same 10-word window, P (wi) and P (wj) the probability of occurrence of the word wi, wj) P (wi) P (wi) P (wi) P (wj)) and P (wi) is estimated as word frequency, as in the April 2011 Wikipedia dump."}, {"heading": "3.2.2.2 Perplexity", "text": "When measuring the quality of a subject model, helplessness is usually defined as: P (W | the rest) = I, i = 1, i = 1, i = 1, W = 1, where W represents all words among all examination documents in all groups. Exam documents are documents that are submitted in each group during the training phase. Di is the number of examination documents in group i, N is the total number of words in group i, I is the total number of words in examination document d, I is the total number of words in examination document d, I is the total number of words in group i, I is the total number of words in examination document d, I is the total number of examination documents in group i, I is the total number of words in examination document d, I is the total number of words in test document d, I is the total number of words in test document d, I is the total number of words in group 1, I is the total number of words in test document d, I is the total number of words in test document d, I is the total number of words in [21] we define p (~ d, total document is the words in group d, total document d = 1, the words in test document d, the rest of the words in test document, ~ k = p), as follows: P, ~ k = the words in the rest of k, p, p (k)."}, {"heading": "3.2.2.3 Topic Quality and Intepretability", "text": "If the topics produced do not provide people with coherent, integral information, they would not be useful in practice, even if they do well in both the PMI score and the helplessness. For example, the topic \"Barack Obama Apple iphone ipad health insurance\" might have a high PMI score because some pairs of words often appear in the Wikipedia corpus on this topic, but apparently this is not a good topic because it is a mixture of three topics. Likewise, the low helplessness just shows that the topic model has good ability to predict words in test documents, which does not necessarily mean that the topics are of good quality. The problem is best illustrated by an example in American political blogs, where a topic model simply places the greatest emphasis on the most common words, such as \"obama said Republican Democrats,\" or simply calculates the word frequency and distributes evenly across all topics."}, {"heading": "3.3 The Innovation: Speeding Up SPDP", "text": "In this section, I propose a number of ways to improve the running speed of SPDP. First, I will give a discussion of a basic trivial parallelization on line 11-17 of the algorithm 2.1. I will show that if the number of topics and the number of effective entries in the transformation matrices P i are small, such parallelization can cause significant costs for the creation and synchronization of threads, contrary to what people would expect from the outset. I will address the challenge that exact collapsed Gibbs sampling words must be scanned sequentially, so any parallelization can carry significant risks in terms of convergence and loss of accuracy. I will give a discussion of possible ways to overcome this obstacle, then I will propose a parallel approach that can not only be justified to work in theory, but can also be implemented and tested to work reasonably well in practice."}, {"heading": "3.3.1 Distributed Parallelization Proposals", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "3.3.1.1 Basic Parallelism: Over Topics and Word-Associations", "text": "Line 11-17 of algorithm 2.1 contains only independent operations. Suppose multiple complicated transformation matrices P i are used, K \u00d7 S threads can be output in parallel to calculate sampling probabilities of (k, v) pairs, where S is the maximum number of non-zero entries in each line and column of P i, k is a topic, and v is a word association. In most systems (especially on a CPU architecture) thread creation is a very expensive process. However, the cost of creating a thread can be greater than the benefit of having multiple threads computing these probabilities in parallel, and v represents a word association. 40 Typically, the cost of creating a thread is sufficient from 105 to 107 CPU cycles, depending on the system architecture. Regardless, the value is much greater than the number of cycles required to calculate the probabilities for each thread, unless the architecture has a pair of 104 that is supported (unless the idea of a pair v is in 102)."}, {"heading": "3.3.1.2 Parallel Word Sampling", "text": "The risk of divergence and loss of accuracy is unavoidable, but with appropriate parallelization methods, the loss can be minimized. Let's look at the convergence process of the collapsed Gibbs samplers in SPDP. In each iteration, the theme of a particular word is scanned based on the current execution numbers for all words except the word itself. Frequent themes and words have accumulated their counts quickly, while unusual themes and words also gradually lose their counts. As the algorithm progresses through many iterations, changes are slowed down and the counts stabilized until they converge into a stationary state. This behavior is similar to many chaotic systems where convergence is not dependent on baseline values, and the final stationary change in the positions of each body during the early stage of convergence is not sensitive."}, {"heading": "3.3.1.3 Parallelism With Improved Accuracy: Word Order Rearrangement", "text": "To reduce the risk, we can change the order of word selection when we scan it. Because our model is unique, we are allowed to scan words between all documents in the order we want. At a certain point in time, we should scan only one word from each document in parallel. As the number of processors is limited, we can simply rearrange the order of words before assigning them to the processors. Words in the same document should be kept as separate as possible so that each group of workgroups that are sampled at the same time consists largely of words from different documents. It can be done as follows: 1. Before executing the algorithm, create an empty array A to store the words; 2. Create a variable wind technique."}, {"heading": "3.3.1.4 Traditional Distributed Model: Dividing Documents", "text": "Many distributed models have already been proposed for LDA. Most related and influential are AD-LDA (Approximate Distributed LDA), proposed by Newman et al. in [32], and an improved version proposed by Smola and Narayanamurthy in [33].Newman's model distributes D-documents and counts in relation to these D-documents in P-processors (not necessarily on the same machine), and only synchronizes the number of counter variables after each iteration of the collapsed Gibbs sampling. Newman argued, however, that this model is a good approximation because the sampling process on multiple processors can hardly touch the same word and topic, so errors accumulated in counter variables is insignificant. 44Smola and Narayanamurthy's made an improvement over Newman's distributed framework. They proposed an architecture that assigns a dedicated processor to update and synchronize variables, globally, and distribute locally."}, {"heading": "3.3.2 All-in-one: Putting Everything Together", "text": ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"}, {"heading": "3.4 Implementation", "text": "A generalized framework for acceleration has been created, a few distributed and parallel techniques have been proposed, and their fitness under both the GPU and the CPU architecture has been studied; this section focuses exclusively on the GPU architecture and implementation issues; I show that for my algorithm, the GPU architecture has superior performance and superior cost-efficiency compared to the CPU architecture; there is a discussion about programming frameworks, namely OpenCL and CUDA, the two dominant GPU programming frameworks, and I argue that OpenCL is a better framework to start with; the architectural differences between two major GPU vendors, namely NVIDIA and AMD, are discussed; and a conclusion has been reached that there is no evidence for my algorithm that is better than others before optimization takes place."}, {"heading": "3.4.1 Parallelization Framework Comparisons", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "3.4.1.1 CPU v.s. GPU", "text": "The key success factors for the GPU over the CPU are overall higher computing power and energy efficiency, which is a combined result of higher memory bandwidth and a higher number of simple processors at lower operating frequencies. Table 3.1 shows a4950Comparison between a modern CPU and GPU, both designed by Advanced Micro Devices, Inc. (AMD), are available in the consumer market at approximately the same price ($100USD apiece as of September 30, 2012). A comparison between the NVIDIA GPU and Intel CPU is shown in Tables 3.2 and 3.The tables show the main differences between the CPU and the GPU are core frequency, instruction latency and the ability to run and plan large hardware threads in parallel. The GPU has an overwhelming advantage in the last measurement, which compensates for the minor deficits in the first two measurements."}, {"heading": "3.4.1.2 OpenCL v.s CUDA", "text": "In fact, most of them are able to survive by themselves if they do not play by the rules. (...) Most of them are able to survive by themselves. (...) Most of them are not able to survive by themselves. (...) Most of them are not able to survive by themselves. (...) Most of them are not able to survive by themselves. (...) Most of them are able to survive by themselves. (...) Most of them are not able to survive by themselves. (...) Most of them are not able to survive by themselves. (...) Most of them are not able to survive by themselves. (...) Most of them are able to survive by themselves. (...)"}, {"heading": "3.4.1.3 Hardware and Architectures", "text": "In fact, most of them will be able to orient themselves in a different direction than in a different direction, namely the direction in which they are moving."}, {"heading": "3.4.2 Implementation", "text": "Although an OpenCL kernel program must be written in C, the host program can be written in other languages. OpenCL's official specification uses C + + for native host code language, but wrappers have already been created for multiple languages such as Java, Objective-C, Python and many others. To achieve maximum portability without sacrificing efficiency, we have chosen Java for implementation and experimentation. If implemented correctly, Java has a comparable efficiency to C and C + +.Algorithm 3.2 outlines the implementation of pseudo-code for my algorithms MGPUDP-SPDP (Multi-GPU Distributed Parallel SPDP). As mentioned at the beginning of this section, we have set transformation matrices P that limit the number of word associations to 1 threads, and maximum number of threads that are needed to 1, and maximum number of threads at the topics level 2, typically less than two times the maximum number of threads."}, {"heading": "4.1 Experimental Setup", "text": "Three sets of data 1 are used for the experiments to show the effectiveness of the three levels of politics in the United States and its related countries, each an international framework established in Chapter 2, 1997. Words that have no semantic meaning are discarded for all documents, such as \"I,\" \"she,\" \"she,\" \"is,\" \"es,\" \"am,\" the so-called \"stop words,\" as they can perform different meanings in some situations: 1. Dataset \"RedState v. s DailyKos\": A blog containing 2276 blogs from the American political blogs RedState (www.redstate.com) and 2386 gs from DailyKos (www.dailykos.com) focusing on current political issues and government policies in the United States and its related countries."}, {"heading": "4.2 Experiments", "text": "The first experiment is performed on the original SPDP Java implementation, and the result is used as a benchmark to compare with the results from the other experiments performed on different versions of MGPU-DP-SPDP, which include some or all of the functions in the proposed three-stage distributed parallelism.Table 4.2 and Figure 4.2.1 shows the perplexity of the original SPDP algorithms with dataset \"RedState v.s DailyKos\" on both machines, where the number of topics is set to K = 32. The algorithm converts to approximately the same perplexity on both platforms, showing that although the algorithm is stochastical, the convergence speed is stable and platform-independent, it is not necessary to test my algorithm on different devices that belong to the same class. Although the perplexity changes little64Platform AMD FX8150 \"Bulldozer\" Intel Core i7 740QMPerplexity. \""}, {"heading": "4.2.2 Parallelization Over Words", "text": "The experiments will be conducted on both AMD Radeon HD5850 and NVIDIA GTX460M GPUs, with varying numbers of topics. This version will henceforth be provided as a GPU SPDP as a single column for comparison, achieving massive acceleration even though I have not optimized my code for the specific GPUs, as explained in Section 3.4.2. If the code is further optimized to take device-specific parameters into account, we can expect further improvement a few times better than the current result. With the implementation of Algorithms 3.2, only a fraction of the maximum local memory provided by both devices will be available."}, {"heading": "4.2.3 Effect of Re-ordering Words", "text": "As suggested in Section 3.3.1.3, the error amount is expected to be reduced by reordering the order of the words before they are sent to the GPU. In this experiment, this feature is added to the implementation, while everything else is the same as in the previous experiment. Figure 4.2.7, 4.2.8 and 4.2.9 show the convergence of the improved version compared to the convergence of the perplexity resulting from the GPU-SPDP and the original SPDP. Just as in the previous experiments, the theme qualities are slightly improved, which means that we now have an algorithm that is almost as good as the original SPDP algorithm. In fact, the perplexity is reduced to a value that is almost the same as the original SPDP algorithm, and the theme qualities are also slightly improved."}, {"heading": "4.2.4 Multi-GPU Distributed Parallelism", "text": "Finally, I implemented the ultimate algorithm MGPU-DP-SPDP, and tested it with the same configuration as in previous experiments, except all five Radeon HD5850 GPUs on the AMD machine are used this time. Runtime analysis is shown in Table 4.18. For the columns that represent the runtime of the original SPDP and GPU-SPDP, we used data from Table 4.10. Speed improvement over GPU-SPDP is sublinear and sometimes much lower than the maximum value (500%) for two reasons. First, for safety reasons, we used atomic locks to increment and reduce variables (i.e. using OpenCL primitives). For some reasons, this is necessary for AMD-GPUs when using multiple GPUs, otherwise the program would crash. We believe atomic locks are not necessary, and the result could be much more impressive if the experiment is performed on multiple professional VINUs, the second GPUs are not sufficient to improve the GDP. However, the second GPUs are not sufficient to have these problems."}, {"heading": "4.2.5 Multi-GPU Distributed Parallelism: Effect of Duplicating", "text": "In one of my early implementations of MGPU-DP-SPDP (which has a slightly worse error correction mechanism), it was found that duplication of training data is sometimes critical to obtaining topic probabilities that do not seem ridiculous. In my current version, duplication of training data is no longer necessary. I found that duplication of data once might slightly improve the correctness of topic probabilities and topic qualities, especially in a small collection of documents, which unsurprisingly leads to better perplexity. Due to hardware limitations, I cannot verify whether more than one duplicate is needed to improve the perplexity for a large number of GPUs, although I believe that with good implementation of error correction and optimization, the need to improve perplexity can be completely eliminated by duplicating the training data. Figure 4.2.13 shows the confusion once the data has been duplicated for the \"State Reds v. Dailyos\" dataset."}, {"heading": "4.2.6 Hellinger Distance", "text": "The Hellinger distance [40] is a popular measure of the similarity between two topic models. Here, two heatmaps are provided in Figure 4.2.14 and 4.2.15 to show the Hellinger distance between the original SPDP and the improved GPU-SPDP and the Hellinger distance between the original SPDP and the MGPU-DP-SPDP, both with the data set \"Reuters Disasters.\" The axis is the topic as it appeared in the improved GPU-SPDP, and the y axis is the topic as it appeared in the original SPDP, reordered to align with the topics in the x-axis. Lower values mean better agreement in topics. The results shown here look almost as good as the comparison of two passes of the original SPDP.96 Figure 4.2.14: Hellinger distance between the original SPDP and the improved GPU-SPDP with the data set \"Reuters Disasters\" 974.2.15: Hellinger distance between the original SPDP and the original SPDP."}, {"heading": "5.1 Conclusions and Key Contributions", "text": "I converted the state-of-the-art SPDP algorithm into a distributed parallel approach, where the running speed on a single GPU is greatly improved if only one GPU is used, and is sublinearly scalable to the number of available GPUs if multiple GPUs are used. \u2022 The single GPU algorithm improved the speed of the SPDP differential theme modeling algorithm by about 50 times on a single, inexpensive mid-range laptop GPU. \u2022 The multi-GPU algorithm MGPU-DP-SPDP uses the latest modern multi-GPU architecture, improves the running speed of SPDP and theme modeling on a homemade small GPU cluster with very low sacrifices in theme quality and only slightly inferior reasonably comparable perplexity. \u2022 SPDP is a representative of perhaps another hundred extensions of the LDA. My algorithm is generalized and leaves mathematical derivatives untouched."}, {"heading": "5.2 Future Work", "text": "Although the experiments were quite successful in my experiments, there are a few more improvements that could be made: \u2022 automated optimization tailored to the specifications and parameters of the GPU; \u2022 scalability and robustness of multi-GPU scanning; \u2022 full SPDP parallelization with matrices without identity transformation; \u2022 large-scale implementation; \u2022 adaptation of the three-stage distributed parallel frame to other LDA extensions."}, {"heading": "5.2.1 Automated Optimization", "text": "The OpenCL framework provides users with APIs to obtain information about the capacity and specification of available devices. It is possible to use this information to determine the preferred local workgroup size and adjust my algorithm to maximize performance; the number of threads per workgroup should be as close as possible to the preferred size; each thread should have the ability to calculate multiple sample probabilities; and the value should be determined by d NwordsNthreads K (S + 1) preferred e, where Nwords is the number of words that can be scanned in parallel with most C conflicts; Nthreads the maximum number of 100 parallel hardware threads that the device supports; K the number of topics; S the number of word connections (S = 1, if transformation matrix is P identity); Tpreferred is the preferred local workgroup size that is the preferred; C is a constant value that needs to be determined; the formula simply states that per topic we should determine the number of word connections (S = 1, if transformation matrix is P identity); C is the preferred value of the local workgroup size that needs to be determined; C is a constant value that needs to be determined; and C is the formula that we should determine the number of words per wave that should be matched to most C = 1 identity."}, {"heading": "5.2.2 Scalability And Robustness of Multi-GPU Sampling", "text": "The results of the experiments suggest that the current error correction mechanism works well with Word probabilities, but not so well with topic probabilities, thus increasing helplessness. A better error correction mechanism is required for multi-GPU scanning. In addition, sending and reading data from multiple GPUs can be inefficient, especially if the number of GPUs is large. It is possible to leave data on the GPUs, perform error correction, generate random numbers, and regenerate counters on GPUs. Complicated problems can occur, such as finding an efficient way to generate random numbers and spread to all GPU devices, and how not to correct errors at the same time. We can also consider using the model introduced by Smola et al. [33] to have a dedicated GPU to update the counters for all devices, and may not support some of the networks, however, the implementation of this model would be complicated and may require some network parameters."}, {"heading": "5.2.3 Full SPDP Parallelization With Non-identity Transforma-", "text": "For the sake of simplicity, we implemented MGPU-DP-SPDP in algorithms 3.1 and 3.2 assuming that the transformation matrices P i are identity matrices. As mentioned in Section 3.3.1.2, the difficulty in dealing with non-identity transformation matrices is in implementing a linked list v, which can be modified in parallel with high performance. A simple solution is to store the modifications in a separate array, which is to be merged into a linked list v after the execution of the GPU kernel. However, since a huge number of words are to be scanned on the GPU, this can result in significant delays in updating the linked list variable v and the counter variable q, which may cost the algorithm more than the benefit of non-identity transformation matrices 101 would bring. Another approach is to assign a fixed amount of memory to each local working group, and to assign the corresponding number of local words to the associated list, respectively."}, {"heading": "5.2.4 Large Scale Implementation", "text": "With a GPU cluster supercomputer, my algorithm has the potential to improve the running speed of SPDP and theme modeling to an unprecedented extent, potentially sublinear to the number of GPUs times number of cores available on a single GPU. Let's take this mid-sized GPU cluster supercomputer [41] in CSIRO, for example, a 64 Tesla S2050 GPU cluster containing 256 GPUs with 114688 streaming processors. Assuming that each GPU can only improve the running speed of SPDP by 100 times (which is a highly conservative underestimate, considering that these GPUs are all the highest professional scientific computing GPUs, and even my cheap laptop GPU is strong enough to improve the running speed of SPDP by 54 times before optimizing). Based on the evidence in my experiments, it is reasonable to estimate that my algorithm will run at a thousand times the speed of this GDP before the GPDP is large enough for the document collection to be optimized."}, {"heading": "5.2.5 Adapting the Three-level Distributed Parallel Framework", "text": "Although my three-level, distributed parallel framework for SPDP is proposed and implemented, it does not utilize many specific features of SPDP. Specifically, the mathematical derivative of SPDP is not modified, which allows the algorithm to be generalized and adapted to other LDA extensions or LDA itself. It requires few changes to the framework and little work to implement error fixes and other related parts. It is particularly advantageous for extensions with complex structures with many variables. In addition, the same approach can be used to parallelise and approximate Markov chains and solve a wider range of problems.102"}], "references": [{"title": "Latent Dirichlet Allocation", "author": ["D.M. Blei", "A.Y. Ng", "M.I. Jordan"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2003}, {"title": "Transformation Poisson-Dirichlet processes for differential topic modeling", "author": ["Wray Buntine"], "venue": "Technical report, ANU & NICTA,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2012}, {"title": "Efficient Parallel Graph Exploration on Multi-Core CPU and GPU", "author": ["Sungpack Hong", "Tayo Oguntebi", "Kunle Olukotun"], "venue": "IEEE Computer Society,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2011}, {"title": "Discovery in Text: Visualisation", "author": ["Wray Buntine"], "venue": "Topics and Statistics,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2011}, {"title": "Joint sentiment/topic model for sentiment analysis", "author": ["Chenghua Lin", "Yulan He"], "venue": "In Proceedings of the 18th ACM conference on Information and knowledge management,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2009}, {"title": "Dynamic Item Recommendation by Topic Modeling for Social Networks. In ITNG, pages 884\u2013889", "author": ["Sang Su Lee", "Tagyoung Chung", "Dennis McLeod"], "venue": "IEEE Computer Society,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2011}, {"title": "Investigating topic models for social media user recommendation", "author": ["Marco Pennacchiotti", "Siva Gurumurthy"], "venue": "In Proceedings of the 20th international conference companion on World wide web,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2011}, {"title": "Trend analysis model: trend consists of temporal words, topics, and timestamps", "author": ["Noriaki Kawamae"], "venue": null, "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2011}, {"title": "Investigating bias in traditional media through social media", "author": ["Arjumand Younus", "Muhammad Atif Qureshi", "Suneel Kumar Kingrani"], "venue": "WWW (Companion Volume),", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2012}, {"title": "The author-topic model for authors and documents", "author": ["Michal Rosen-Zvi", "Thomas Griffiths", "Mark Steyvers", "Padhraic Smyth"], "venue": "In Proceedings of the 20th conference on Uncertainty in artificial intelligence,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2004}, {"title": "Visualizing search results and document collections using topic maps", "author": ["David Newman", "Timothy Baldwin", "Lawrence Cavedon"], "venue": "J. Web Sem.,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2010}, {"title": "Pattern Recognition and Machine Learning", "author": ["Christopher M. Bishop"], "venue": null, "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2006}, {"title": "Parameter estimation for text analysis", "author": ["G. Heinrich"], "venue": "Technical report,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2009}, {"title": "Probabilistic Latent Semantic Analysis", "author": ["Thomas Hofmann"], "venue": null, "citeRegEx": "22", "shortCiteRegEx": "22", "year": 1999}, {"title": "Topic models with power-law using Pitman-Yor process", "author": ["Issei Sato", "Hiroshi Nakagawa"], "venue": null, "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2010}, {"title": "A Bayesian Review of the Poisson", "author": ["Wray L. Buntine", "Marcus Hutter"], "venue": "Dirichlet Process. CoRR,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2010}, {"title": "Gibbs Sampling Methods for Stick-Breaking Priors", "author": ["Hemant Ishwaran", "Lancelot F. James"], "venue": "Journal of the American Statistical Association,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2001}, {"title": "A Hierarchical Bayesian Language Model Based On Pitman-Yor Processes", "author": ["Yee Whye Teh"], "venue": "ACL. The Association for Computer Linguistics,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2006}, {"title": "Sampling Table Configurations for the Hierarchical Poisson-Dirichlet Process", "author": ["Changyou Chen", "Lan Du", "Wray Buntine"], "venue": "In Proceedings of the ECML/PKDD", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2011}, {"title": "Hierarchical Dirichlet processes", "author": ["Y.W. Teh", "M.I. Jordan", "M.J. Beal", "D.M. Blei"], "venue": "Journal of the American Statistical Association,", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2006}, {"title": "Monte Carlo Statistical Methods", "author": ["C.P. Robert", "G. Casella"], "venue": null, "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2004}, {"title": "The two-parameter Poisson-Dirichlet distribution derived from a stable subordinator", "author": ["J. Pitman", "M. Yor"], "venue": "Annals of Probability,", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 1997}, {"title": "Automatic Evaluation of Topic Coherence", "author": ["David Newman", "Jey Han Lau", "Karl Grieser", "Timothy Baldwin"], "venue": "In HLT-NAACL,", "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2010}, {"title": "Distributed Algorithms for Topic Models", "author": ["David Newman", "Arthur U. Asuncion", "Padhraic Smyth", "Max Welling"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "32", "shortCiteRegEx": "32", "year": 2009}, {"title": "An Architecture for Parallel", "author": ["Alexander J. Smola", "Shravan Narayanamurthy"], "venue": "Topic Models. PVLDB,", "citeRegEx": "33", "shortCiteRegEx": "33", "year": 2010}, {"title": "A Performance Comparison of CUDA and OpenCL", "author": ["Kamran Karimi", "Neil G. Dickson", "Firas Hamze"], "venue": "CoRR, abs/1005.2581,", "citeRegEx": "35", "shortCiteRegEx": "35", "year": 2010}, {"title": "A Comprehensive Performance Comparison of CUDA and OpenCL", "author": ["Jianbin Fang", "Ana Lucia Varbanescu", "Henk J. Sips"], "venue": null, "citeRegEx": "36", "shortCiteRegEx": "36", "year": 2011}, {"title": "Architecture Comparisons between Nvidia and ATI GPUs: Computation", "author": ["Ying Zhang", "Lu Peng", "Bin Li", "Jih-Kwon Peir", "Jianmin Chen"], "venue": "Parallelism and Data Communications,", "citeRegEx": "38", "shortCiteRegEx": "38", "year": 2012}, {"title": "RCV1: A New Benchmark Collection for Text Categorization Research", "author": ["D.D. Lewis", "Y. Yand", "T.G. Rose", "F. Li"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "39", "shortCiteRegEx": "39", "year": 2004}, {"title": "Modelling Sequential Text with an Adaptive Topic Model. In Empirical Methods in Natural Language Processing (EMNLP), page 9, Jeju /Korea", "author": ["Lan Du", "Wray Buntine", "Huidong Jin"], "venue": null, "citeRegEx": "40", "shortCiteRegEx": "40", "year": 2012}], "referenceMentions": [{"referenceID": 0, "context": "Bayesian topic models such as Latent Dirichlet Allocation (LDA) [1] allow salient patterns in large collection of documents to be extracted and analyzed automatically.", "startOffset": 64, "endOffset": 67}, {"referenceID": 1, "context": "Recently, the introduction of techniques for differential topic modelling, namely the Shadow Poisson Dirichlet Process model (SPDP) [2] performs uniformly better than many existing topic models in a discriminative setting.", "startOffset": 132, "endOffset": 135}, {"referenceID": 2, "context": "s NVIDIA GPU comparison (from [7]) .", "startOffset": 30, "endOffset": 33}, {"referenceID": 2, "context": "s NVIDIA GPU memory bandwidth comparison (from [7]) .", "startOffset": 47, "endOffset": 50}, {"referenceID": 0, "context": "Bayesian topic models such as Latent Dirichlet Allocation (LDA) [1] allow salient patterns in large collection of documents to be extracted and analyzed automatically.", "startOffset": 64, "endOffset": 67}, {"referenceID": 1, "context": "Recently, the introduction of techniques for differential topic modelling, namely the Shadow Poisson Dirichlet Process model (SPDP) [2] performs uniformly better than many existing topic models in a discriminative setting.", "startOffset": 132, "endOffset": 135}, {"referenceID": 3, "context": "People need tools to organize, search, summarize, and understand information, tools to turn information into knowledge [8].", "startOffset": 119, "endOffset": 122}, {"referenceID": 4, "context": "Today, there are many proposed applications of topic modelling to different industries: \u2022 Finance, media, governmental: Public sentiment analysis [9] \u2022 Social network companies: Content based social network recommendation systems [10, 11] \u2022 Media companies: Trend analysis[12], traditional media bias detection [13] \u2022 Military: Differential discovery on text collections [2], author detection[14] \u2022 Enterprises, consumers: Search engine and document processing [15]", "startOffset": 146, "endOffset": 149}, {"referenceID": 5, "context": "Today, there are many proposed applications of topic modelling to different industries: \u2022 Finance, media, governmental: Public sentiment analysis [9] \u2022 Social network companies: Content based social network recommendation systems [10, 11] \u2022 Media companies: Trend analysis[12], traditional media bias detection [13] \u2022 Military: Differential discovery on text collections [2], author detection[14] \u2022 Enterprises, consumers: Search engine and document processing [15]", "startOffset": 230, "endOffset": 238}, {"referenceID": 6, "context": "Today, there are many proposed applications of topic modelling to different industries: \u2022 Finance, media, governmental: Public sentiment analysis [9] \u2022 Social network companies: Content based social network recommendation systems [10, 11] \u2022 Media companies: Trend analysis[12], traditional media bias detection [13] \u2022 Military: Differential discovery on text collections [2], author detection[14] \u2022 Enterprises, consumers: Search engine and document processing [15]", "startOffset": 230, "endOffset": 238}, {"referenceID": 7, "context": "Today, there are many proposed applications of topic modelling to different industries: \u2022 Finance, media, governmental: Public sentiment analysis [9] \u2022 Social network companies: Content based social network recommendation systems [10, 11] \u2022 Media companies: Trend analysis[12], traditional media bias detection [13] \u2022 Military: Differential discovery on text collections [2], author detection[14] \u2022 Enterprises, consumers: Search engine and document processing [15]", "startOffset": 272, "endOffset": 276}, {"referenceID": 8, "context": "Today, there are many proposed applications of topic modelling to different industries: \u2022 Finance, media, governmental: Public sentiment analysis [9] \u2022 Social network companies: Content based social network recommendation systems [10, 11] \u2022 Media companies: Trend analysis[12], traditional media bias detection [13] \u2022 Military: Differential discovery on text collections [2], author detection[14] \u2022 Enterprises, consumers: Search engine and document processing [15]", "startOffset": 311, "endOffset": 315}, {"referenceID": 1, "context": "Today, there are many proposed applications of topic modelling to different industries: \u2022 Finance, media, governmental: Public sentiment analysis [9] \u2022 Social network companies: Content based social network recommendation systems [10, 11] \u2022 Media companies: Trend analysis[12], traditional media bias detection [13] \u2022 Military: Differential discovery on text collections [2], author detection[14] \u2022 Enterprises, consumers: Search engine and document processing [15]", "startOffset": 371, "endOffset": 374}, {"referenceID": 9, "context": "Today, there are many proposed applications of topic modelling to different industries: \u2022 Finance, media, governmental: Public sentiment analysis [9] \u2022 Social network companies: Content based social network recommendation systems [10, 11] \u2022 Media companies: Trend analysis[12], traditional media bias detection [13] \u2022 Military: Differential discovery on text collections [2], author detection[14] \u2022 Enterprises, consumers: Search engine and document processing [15]", "startOffset": 392, "endOffset": 396}, {"referenceID": 10, "context": "Today, there are many proposed applications of topic modelling to different industries: \u2022 Finance, media, governmental: Public sentiment analysis [9] \u2022 Social network companies: Content based social network recommendation systems [10, 11] \u2022 Media companies: Trend analysis[12], traditional media bias detection [13] \u2022 Military: Differential discovery on text collections [2], author detection[14] \u2022 Enterprises, consumers: Search engine and document processing [15]", "startOffset": 461, "endOffset": 465}, {"referenceID": 11, "context": "In addition, to limit the length of our background chapter, we expect the reader to be familiar with standard machine learning, computer architecture, distributed and parallel computing, micro-processors, Bayesian statistics, parameter estimation, statistical inference, and Bayesian graphical models (see [20]).", "startOffset": 306, "endOffset": 310}, {"referenceID": 0, "context": "Moreover, we expect familiarities with basic topic models such as Latent Dirichlet Allocation (LDA) (see [1, 21]) and common performance measure such as perplexity (see section 3.", "startOffset": 105, "endOffset": 112}, {"referenceID": 12, "context": "Moreover, we expect familiarities with basic topic models such as Latent Dirichlet Allocation (LDA) (see [1, 21]) and common performance measure such as perplexity (see section 3.", "startOffset": 105, "endOffset": 112}, {"referenceID": 13, "context": "While non-Bayesian topic models such as Probabilistic Latent Semantic Analysis (PLSA) [22] do exist, the focus of the topic modelling research field had been mostly shifted to Bayesian topic models since the advent of Latent Dirichlet Allocation (LDA) given their superior theoretical basis and good performance.", "startOffset": 86, "endOffset": 90}, {"referenceID": 0, "context": "A mathematical definition and a conceptional description are given on the following models, ordered by their simplicity and the dates they are created: \u2022 Latent Dirichlet Allocation (LDA) [1] \u2022 Pitman-Yor Topic Modelling (PYTM) [23] \u2022 Hierarchical Pitman-Yor Topic Modelling (HPYTM) [23]", "startOffset": 188, "endOffset": 191}, {"referenceID": 14, "context": "A mathematical definition and a conceptional description are given on the following models, ordered by their simplicity and the dates they are created: \u2022 Latent Dirichlet Allocation (LDA) [1] \u2022 Pitman-Yor Topic Modelling (PYTM) [23] \u2022 Hierarchical Pitman-Yor Topic Modelling (HPYTM) [23]", "startOffset": 228, "endOffset": 232}, {"referenceID": 14, "context": "A mathematical definition and a conceptional description are given on the following models, ordered by their simplicity and the dates they are created: \u2022 Latent Dirichlet Allocation (LDA) [1] \u2022 Pitman-Yor Topic Modelling (PYTM) [23] \u2022 Hierarchical Pitman-Yor Topic Modelling (HPYTM) [23]", "startOffset": 283, "endOffset": 287}, {"referenceID": 1, "context": "\u2022 Differential Topic Modelling using Shadow Poisson Dirichlet Process (SPDP) [2]", "startOffset": 77, "endOffset": 80}, {"referenceID": 0, "context": "1 Latent Dirichlet Allocation (LDA) Latent Dirichlet Allocation (LDA) [1] is one of the most popular models used in topic modelling because of its simplicity.", "startOffset": 70, "endOffset": 73}, {"referenceID": 14, "context": "2 Pitman-Yor Topic Modelling (PYTM) Pitman-Yor Topic Modelling (PYTM) [23] made a few improvements over LDA, hence achieved significantly better performance when measured in perplexity.", "startOffset": 70, "endOffset": 74}, {"referenceID": 15, "context": "The PYTM assumes in each document the words are sequentially drawn from a distribution generated by a Poisson Dirichlet Process (PDP)[24] (also known as Pitman-Yor Process[25]).", "startOffset": 133, "endOffset": 137}, {"referenceID": 16, "context": "The PYTM assumes in each document the words are sequentially drawn from a distribution generated by a Poisson Dirichlet Process (PDP)[24] (also known as Pitman-Yor Process[25]).", "startOffset": 171, "endOffset": 175}, {"referenceID": 14, "context": "d words from a multinomial distribution, hence is unable to capture the properties of a power-law [23].", "startOffset": 98, "endOffset": 102}, {"referenceID": 14, "context": "Below is another way of representing this process, similar to what is introduced in [23].", "startOffset": 84, "endOffset": 88}, {"referenceID": 14, "context": "The Hierarchical Pitman-Yor Topic Modelling (HPYTM) [23] made one extension to PYTM by assuming the power-law phenomenon not only exists in each document but also within each topic.", "startOffset": 52, "endOffset": 56}, {"referenceID": 17, "context": "The Hierarchical Bayesian Language Model [26] replaces some parts of PYTM still inheriting some features of LDA with a more complicated structure, as illustrated in Figure 2.", "startOffset": 41, "endOffset": 45}, {"referenceID": 1, "context": "4 Differential Topic Modelling Using Shadow Poisson Dirichlet Process This differential model [2] addresses the problem of comparing multiple groups of documents.", "startOffset": 94, "endOffset": 97}, {"referenceID": 15, "context": "Combining the essence of all models introduced above and results from other mathematics research and topic modelling research, especially the theoretical results in [24], the improved PDP table-configuration sampler in [27], and the Hierarchical Dirichlet Process model [28], the Differential Topic Model with Shadow Poisson Dirichlet Process (SPDP) is born.", "startOffset": 165, "endOffset": 169}, {"referenceID": 18, "context": "Combining the essence of all models introduced above and results from other mathematics research and topic modelling research, especially the theoretical results in [24], the improved PDP table-configuration sampler in [27], and the Hierarchical Dirichlet Process model [28], the Differential Topic Model with Shadow Poisson Dirichlet Process (SPDP) is born.", "startOffset": 219, "endOffset": 223}, {"referenceID": 19, "context": "Combining the essence of all models introduced above and results from other mathematics research and topic modelling research, especially the theoretical results in [24], the improved PDP table-configuration sampler in [27], and the Hierarchical Dirichlet Process model [28], the Differential Topic Model with Shadow Poisson Dirichlet Process (SPDP) is born.", "startOffset": 270, "endOffset": 274}, {"referenceID": 1, "context": "Although the superiority of this model has already been demonstrated in experiments, [2] does not provide an in depth explanation of the intuition of the model and derivation of the model.", "startOffset": 85, "endOffset": 88}, {"referenceID": 20, "context": "The proof can be found in most advanced statistics textbooks, such as [29].", "startOffset": 70, "endOffset": 74}, {"referenceID": 12, "context": "\u2206(~x) is the Dirichlet delta function, the normalizing constant, as defined in [21]:", "startOffset": 79, "endOffset": 83}, {"referenceID": 0, "context": "For more detailed explanation on LDA, readers should refer to [1].", "startOffset": 62, "endOffset": 65}, {"referenceID": 21, "context": ") coupled with probability weighting vector ~ p drawn from Poisson Dirichlet Distribution as stated in [30].", "startOffset": 103, "endOffset": 107}, {"referenceID": 15, "context": "A detailed Bayesian analysis of Poisson Dirichlet Process is given in [24] by Buntine and Hutter.", "startOffset": 70, "endOffset": 74}, {"referenceID": 14, "context": "Readers should refer to [23] if they are interested in the details.", "startOffset": 24, "endOffset": 28}, {"referenceID": 15, "context": "It is shown by Corollary 17 in [24] that in one \u201crestaurant\u201d:", "startOffset": 31, "endOffset": 35}, {"referenceID": 22, "context": "2 Topic Performance Measure Perplexity and pointwise mutual information score (PMI score) based on Wikipedia corpus[31] are the two most popular measures used in the research field to judge the quality of generated topic models.", "startOffset": 115, "endOffset": 119}, {"referenceID": 22, "context": "1 PMI-Score Based on Wikipedia Corpus Out of many evaluation methods proposed in [31], the PMI-score based on the Wikipedia corpus is the consistent best performer with respect to intrinsic semantic quality of learned topics.", "startOffset": 81, "endOffset": 85}, {"referenceID": 1, "context": "In [2] Chen et al.", "startOffset": 3, "endOffset": 6}, {"referenceID": 12, "context": "Similar to the definition in [21], we define p(~ wi,d| the rest) as:", "startOffset": 29, "endOffset": 33}, {"referenceID": 23, "context": "in [32], and an improved version proposed by Smola and Narayanamurthy in [33].", "startOffset": 3, "endOffset": 7}, {"referenceID": 24, "context": "in [32], and an improved version proposed by Smola and Narayanamurthy in [33].", "startOffset": 73, "endOffset": 77}, {"referenceID": 2, "context": "s NVIDIA GPU comparison (from [7])", "startOffset": 30, "endOffset": 33}, {"referenceID": 2, "context": "s NVIDIA GPU memory bandwidth comparison (from [7])", "startOffset": 47, "endOffset": 50}, {"referenceID": 25, "context": "Many performance analyses have been done for comparing these two frameworks [35, 36].", "startOffset": 76, "endOffset": 84}, {"referenceID": 26, "context": "Many performance analyses have been done for comparing these two frameworks [35, 36].", "startOffset": 76, "endOffset": 84}, {"referenceID": 27, "context": "A comperhensive academical study comparing a modern AMD GPU and NVIDIA GPU can be found in [38].", "startOffset": 91, "endOffset": 95}, {"referenceID": 28, "context": "The Reuters RCV1 corpus, Volume 1: English Language, 1996-08-20 to 1997-08-19, [39].", "startOffset": 79, "endOffset": 83}, {"referenceID": 1, "context": "These parameters are suggested as optimal parameters for blog data set by Chen [2], obtained by trial and error.", "startOffset": 79, "endOffset": 82}, {"referenceID": 29, "context": "6 Hellinger Distance Hellinger distance [40] is a popular measure of the similarity between two topic models.", "startOffset": 40, "endOffset": 44}, {"referenceID": 24, "context": "in [33], having a dedicated GPU to update counts for all devices.", "startOffset": 3, "endOffset": 7}, {"referenceID": 1, "context": "3 Full SPDP Parallelization With Non-identity Transformation Matrices Chen has showed in [2] that appropriate transformation matrices could uniformly improve the perplexity and the topic quality.", "startOffset": 89, "endOffset": 92}, {"referenceID": 24, "context": "did for LDA[33], which is the best speedup known so far to LDA.", "startOffset": 11, "endOffset": 15}], "year": 2015, "abstractText": "Multi-GPU Distributed Parallel Bayesian Differential Topic Modelling by Aaron(Qiaochu) Li Bachelor of Science(Advanced)(Honours) Research School of Computer Science Australian National University Doctor Wray Buntine, Supervisor Doctor Scott Sanner, Co-supervisor There is an explosion of data, documents, and other content, and people require tools to analyze and interpret these, tools to turn the content into information and knowledge. Topic modelling have been developed to solve these problems. Bayesian topic models such as Latent Dirichlet Allocation (LDA) [1] allow salient patterns in large collection of documents to be extracted and analyzed automatically. When analyzing texts, these patterns are called topics, represented as a distribution of words. Although numerous extensions of LDA have been created in academia in the last decade to address many problems, few of them can reliablily analyze multiple groups of documents and extract the similarities and differences in topics across these groups. Recently, the introduction of techniques for differential topic modelling, namely the Shadow Poisson Dirichlet Process model (SPDP) [2] performs uniformly better than many existing topic models in a discriminative setting. There is also a need to improve the running speed of algorithms for topic models. While some effort has been made for distributed algorithms, there is no work currently done using graphical processing units (GPU). Note the GPU framework has already become the most cost-efficient and popular parallel platform for many research and industry problems. In this thesis, I propose and implement a scalable multi-GPU distributed parallel framework which approximates SPDP, called MGPU-DP-SPDP, and a version running on a single GPU, Improved-GPU-SPDP. Through experiments, I have shown ImprovedGPU-SPDP improved the running speed of SPDP by about 50 times while being almost as accurate as SPDP, with only one single cheap laptop GPU. Furthermore, I have shown the speed improvement of MGPU-DP-SPDP is sublinearly scalable when multiple GPUs are used, while keeping the accuracy fairly comparable to SPDP. Therefore, on a mediumsized GPU cluster, the speed improvement could potentially reach a factor of a thousand. Note SPDP is just a representative of perhaps another hundred other extensions of LDA. Although my algorithm is implemented to work with SPDP, it is designed to be a general framework that can be extended to work with other LDA extensions and improve", "creator": "LaTeX with hyperref package"}}}