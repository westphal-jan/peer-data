{"id": "1510.06706", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "22-Oct-2015", "title": "ZNN - A Fast and Scalable Algorithm for Training 3D Convolutional Networks on Multi-Core and Many-Core Shared Memory Machines", "abstract": "Convolutional networks (ConvNets) have become a popular approach to computer vision. It is important to accelerate ConvNet training, which is computationally costly. We propose a novel parallel algorithm based on decomposition into a set of tasks, most of which are convolutions or FFTs. Applying Brent's theorem to the task dependency graph implies that linear speedup with the number of processors is attainable within the PRAM model of parallel computation, for wide network architectures. To attain such performance on real shared-memory machines, our algorithm computes convolutions converging on the same node of the network with temporal locality to reduce cache misses, and sums the convergent convolution outputs via an almost wait-free concurrent method to reduce time spent in critical sections. We implement the algorithm with a publicly available software package called ZNN. Benchmarking with multi-core CPUs shows that ZNN can attain speedup roughly equal to the number of physical cores. We also show that ZNN can attain over 90x speedup on a many-core CPU (Xeon Phi Knights Corner). These speedups are achieved for network architectures with widths that are in common use. The task parallelism of the ZNN algorithm is suited to CPUs, while the SIMD parallelism of previous algorithms is compatible with GPUs. Through examples, we show that ZNN can be either faster or slower than certain GPU implementations depending on specifics of the network architecture, kernel sizes, and density and size of the output patch. ZNN may be less costly to develop and maintain, due to the relative ease of general-purpose CPU programming.", "histories": [["v1", "Thu, 22 Oct 2015 18:14:42 GMT  (962kb,D)", "http://arxiv.org/abs/1510.06706v1", null]], "reviews": [], "SUBJECTS": "cs.NE cs.CV cs.DC cs.LG", "authors": ["aleksandar zlateski", "kisuk lee", "h sebastian seung"], "accepted": false, "id": "1510.06706"}, "pdf": {"name": "1510.06706.pdf", "metadata": {"source": "CRF", "title": "ZNN \u2013 A Fast and Scalable Algorithm for Training 3D Convolutional Networks on Multi-Core and Many-Core Shared Memory Machines", "authors": ["Aleksandar Zlateski", "Kisuk Lee", "H. Sebastian Seung"], "emails": ["zlateski@mit.edu,", "kisuklee@mit.edu", "sseung@princeton.edu"], "sections": [{"heading": null, "text": "I. INTRODUCTIONA standard formulation of supervised learning begins with a parameterized class of mappings, a training set of desired input-output pairs, and a loss function to measure the deviation of the actual output from the desired output. The goal of learning is to minimize the average loss within the training set. A popular minimization method is the origin of Convolutionary Networks (ConvNets). For each input subsequently, the parameters of mapping are updated less the direction of the loss in terms of parameters, which is a class of mappings known as Convolutionary Networks (ConvNets). Significant efforts have been made to parallelize ConvNet learning on GPUs, as in the popular software packages Caffe [1], Torch [2], and Theano [3]. ConvNet Learning has also been distributed across several machines [4]."}, {"heading": "II. COMPUTATION GRAPH", "text": "The aforementioned Cold War and World War II inspirations, which occurred in the 1930 \"s and 1930\" s, are indeed significant not only in the US, but also in other countries."}, {"heading": "A. Sliding window max-pooling ConvNet", "text": "In fact, it is so that the constellation in which we are in looks as if we were in a state in which we are in a state in which we are in a state in which we are in a state in which we are in a state in which we are in a state in which we are in a state in which we are in a state in which we are in a state in which we are in which we are in which we are in which we are in a state in which we are in which we are in which we are in which we are in a state in which we are in which we are in which we are in which we are in which we are in which we are in which we are in which we are in which we are in which we are in which we are in which we are in which we are in which we are in which we are in which we are in which we are in which we are in which we are in which we are in which we are in which we are in which we are in which we are in which we are in which we are in which we are in which we are in which we are in which we are in which we are in which we are in which we are in which we are in which we are in which we are in which we are in which we are in which we are in which we are in which we are in which we are in which we are in which we are in which we are in which we are in which we are in which we are in which we are in which we are in which we are in which we are in which we are in which we are in which we are in which we are in which we are in which we are in which we are in which we are in which we are in which we are in which we are in which we are in which we are in which we are in which we are in which we are in which we are in which we are in which we are in which we are in which we are in which we are in which we are in which we are in which we are in which we are in which we are in which we are in which we are in which we are in which we are in which we are in which we are in which we are in which we are in which we are in which we are in which we are in which we are in which we are in which we are in which we are in which we are in which we are in which we are in which we are"}, {"heading": "III. BACKPROPAGATION LEARNING", "text": "The reverse propagation algorithm is a method for calculating the gradient of the loss function in relation to the traceable parameters in a ConvNet, the cores and distortions. For each input, the calculation is made in several phases: 1) Obtain an input and desired output from the training set. 2) Forward run - Calculate the actual output of the ConvNet from the input image. 3) Calculate the gradient of the loss function in relation to the actual output. 4) Backward run - Calculate the gradient of the loss function in relation to the voxels of the output image at each node. 5) Weight update - Calculate the gradient of the loss function in relation to the cores and distortions and update these parameters towards minus the gradient. The forward run has already been described above. ZNN implements several options for the loss function, such as the Euclidean distance between the actual and the desired outputs."}, {"heading": "A. Backward pass", "text": "The output nodes of the forward graph become the input nodes of the reverse graph and are initialized with the gradient of the loss function in relation to the voxels of the output nodes of the forward graph. The nodes of the reverse graph are associated with their own images, which are different from the images associated with the nodes of the forward graph. Each edge in the reverse graph is multiplied by transposing the jacobic matrix of the operation, which is represented by the corresponding edge in the forward graph. The four edge operations in the forward diagram become the following four edge operations in the reverse diagram."}, {"heading": "B. Weight update", "text": "After the forward and backward transitions are completed, there are \"forward images\" at the nodes of the forward calculation graph and \"backwards images\" at the nodes of the backward calculation graph (except the input nodes) that are used to update the cores and distortions as follows. Kernel Update In a convolution that goes from node a to node b in the forward graph, the gradient of loss in relation to the core is calculated by entangling the reflected forward image on node a with the backward image on node b. A valid convolution is performed that results in an image of the size of the nucleus, which is multiplied by a small \"learning rate parameter\" and then subtracted by the kernel. Bias Update In a distortion at node a, the gradient of loss is calculated as the sum of all vowels in the backward image on node a. The scalar result is subtracted by a \"small learning rate\" and then distorted by the distortion."}, {"heading": "IV. DIRECT VS. FFT CONVOLUTION", "text": "For a single folding of an image of size n3 with a kernel of size k3, it is known that the FFT method (complexity O (n3 log n)) is more efficient than the direct method (complexity O (n3k3)), for sufficiently large kernel sizes. The crossover point of equal complexity complies with k3 protocol n. It is less known that the FFT direct crossover occurs for smaller kernel sizes for a ConvNet than for a single folding [5], [6]. This is because the FFT of an image on a node can be divided by edges on that node (see Table II).3 ZNN performs layerwise auto-tuning to choose between FFT-based or direct folding for each layer.Complexity can be further reduced by reducing the FFT of images and cores obtained during the transition to reuse during the decline and weight, in about [5]."}, {"heading": "V. TASK DEPENDENCY GRAPH", "text": "Each node represents one of the four forward operations (folding, maximizing / filtering, transfer function), four backward operations (jacobians) or two updating operations (kernel, bias) described above. Two additional tasks represent an interface with the training set. The data provider receives a training sample used for a single training session, and the loss gradient calculates the gradient of the loss in relation to the network output. The edges of the task dependency curve represent dependencies. The forward task of an edge e = (u, v) in the calculation curve depends on forward grade tasks of all edges3Note that our values differ from the values in [5], because we take into account the difference in complexity between complete and valid constellations (w, the same reverse curve)."}, {"heading": "A. Theoretically achievable speedup", "text": "Let's define TP as the time required for P processors to perform a learning iteration. We want a parallel algorithm that achieves a high acceleration SP = T1 / TP, and ideally one that approaches linear acceleration, SP = P. This should be possible for \"wide\" ConvNet architectures that contain many convolutions that can be performed in parallel. We formalize this intuition subsequently. According to Brent's theorem [17], if a calculation can be performed in T-time with an infinite number of processors, this amounts to a maximum number of tasks that can be performed simultaneously."}, {"heading": "VI. TASK SCHEDULING AND EXECUTION", "text": "s theorem requires no synchronization and no communication effort, we design our algorithm to minimize the synchronization effort and increase the time locality of the calculation to reduce cache errors. The central parameter in our algorithm is a global priority list that contains tasks that can be performed along with their priority."}, {"heading": "A. Priority queue", "text": "Tasks are put in a global queue when all non-upgradable dependencies are fulfilled (only tasks with update task as requirements are forward tasks).The basic idea behind this design choice is that if a forward task is provided to execute without the required update task being completed, we force the execution of the update task followed by the forward task requiring the result of the update, creating the storage place.The tasks on the queue are sorted by priority. Priorities are selected to increase the temporal locality of the calculation and minimize the latency of the calculation.We introduce two distinct strict arrangements of the nodes in the ConvNet calculation curve, which are based on the longest distance, in decreasing order, to each output or input node. Nodes with the same distance are uniquely ordered. The priority of the forward task of an edge (u) will be equal to the position of the v in the order it is performed in the node."}, {"heading": "B. Task Execution", "text": "The tasks are performed by the workers. Each employee takes a task and performs a task with the highest priority on the queue.Forward task algorithm is shown in the output node. The main function of the task is then to apply the corresponding FORWARD TRANSFORM to the specified input image I and accumulate the result to the sum stored in the output node. The task that the last image adds to the sum is then applied to the execution of all dependent tasks. The main functionality of the procedure is shown in the procedure DO-FORWARD. However, such a procedure can only be performed when the update task from the previous round has been completed. This is ensured by the creation of a new subtask with the main functionality for executing. algorithm 1 Execution of the procedure taskFORWARD task."}, {"heading": "VII. SYNCHRONIZATION ISSUES", "text": "It is important to minimize time spent on critical sections - portions of code that can only be executed by one thread at a time. The three most important points of the algorithm that need to be synchronized are memory management (allocation / deallocation), operations in the global task queue, and simultaneous sums."}, {"heading": "A. Queue operations", "text": "Activities in the global priority queue must be synchronized; the queue is implemented as a pile of lists that reduces the complexity of inserting and deleting from logN to logK, where N is the total number of tasks in the queue and K is the number of unique values for the priority of tasks within the queue. Depending on the network structure, this number can be much smaller than the total number of tasks in the queue, especially for large networks."}, {"heading": "B. Wait-free concurrent summation", "text": "The naive strategy of waiting until all other threads have added their images to the sum would result in a critical section time that scales linearly with the image size n3. We propose a new method that removes the dependence on the image size by performing only pointers within the critical section that work as follows. Suppose that multiple threads execute ADD-TO-SUM in algorithm 4. For each thread, v points to a different 3D image. We want the pointer to the sum of all of these images in object S when the computation ends. This is achieved by repeatedly attempting to reset the pointer to v for the sum stored in S.sum."}, {"heading": "C. Memory management", "text": "ZNN implements two custom memory allocators. These are designed to be faster than standard memory management routines, at the expense of using more memory. One custom allocator is dedicated to 3D images, which are usually large, and the other to small objects used in auxiliary data structures. Both allocators maintain 32 global memory pools of memory chunks. Each pool i, i, i, i, i. 31 contains chunks of size 2i. Lock-free queues, as described in [18] and implemented as part of the Boost [19] library, to implement memory operations. The only difference between the allocators is memory alignment - the 3D memory allocator ensures proper memory alignment for using SIMD commands. No memory is shared between the two allocators. If a chunk of memory is requested by the next pool to the appropriate size, the next available memory is first."}, {"heading": "VIII. SCALABILITY", "text": "We made measurements of acceleration through our proposed parallel algorithm in relation to the serial algorithm using the CPU systems listed in Table V. Amazon EC2 instances of 8 and 18 cores (c4.4xlarge and c4.8xlarge) were then chosen for benchmarking because they are readily available to everyone. A 4-way CPU system was included because it has 40 cores, although this is a relatively specialized piece of hardware. For an even larger number of cores, we also benchmarked the Xeon PhiTMKnights Corner. All measurements require the Intel compiler (version 15.0.2) with Intel MKL (version 11.2) libraries for FFTs and direct conversion. The 3D ConvNets included four fully connected 3 x 3 x 3 cores, each followed by a transfer function layer (T) with reactive function and two."}, {"heading": "IX. CPU VS. GPU", "text": "While the previous results seem inevitable, it is also important to know how the results will develop in terms of GPU production."}, {"heading": "A. Speed", "text": "Comparing 2D ConvNets is faster for sufficiently large cores (30 x 30 or larger) than Caffe and Theano. This makes sense because FFT convolution (ZNN) is more efficient for sufficiently large cores than direct convolution (Caffe and Theano). Such large cores are not generally used in practice, so ZNN may not be able to compete with GPU implementations for 2D networks. On the other hand, ZNN opens the possibility to train networks with large cores efficiently, and these could find practical application in the future. Comparing 3D ConvNets is comparable to Theano even for modest kernel sizes of 5 x 5 x 5 x 5 and surpasses Theano for kernel sizes of 7 x 7 x 7. Such kernel sizes are currently relevant for practical applications [21]."}, {"heading": "B. Memory", "text": "Given the limited amount of built-in GPU memory, we were unable to train Theano to train 3D networks with core sizes larger than 7 x 7 x 7. We also could not use Caffe to train many 2D networks (see missing bars in Fig. 8).ZNN enables the training of larger networks mainly because a typical CPU system has much more RAM than even a top GPU. Titan X, for example, only has 12 GB of onboard RAM. In addition, ZNN can achieve even higher speed by using additional RAM space, as in the case of FFT memorization. When using FFT-based convolutions with deactivated memorization, ZNN is more efficient in using RAM than the proposed GPU methods. The amount of memory required for the methods that suggest FFT memorization [6] could be very high, as it is proportional to the number of cores in a layer."}, {"heading": "XI. CONCLUSIONS", "text": "We expect an increase in the number of cores per chip (or Xeon PhiTMcard) in the future, which makes ZNN even more practical. In fact, we have already used ZNN to achieve state-of-the-art results in limit detection [23] and density calculation [24]. Since ZNN has a large amount of RAM, it can efficiently train very large ConvNets with large cores. ZNN allows simple extensions and can efficiently train a ConvNet with an arbitrary topology, which allows new research. Unlike the ZNN parallelization models, the current GPU implementations use SIMD parallelism to perform computing on an entire layer at the same time, limiting the network structure."}], "references": [{"title": "Caffe: Convolutional architecture for fast feature embedding", "author": ["Y. Jia", "E. Shelhamer", "J. Donahue", "S. Karayev", "J. Long", "R. Girshick", "S. Guadarrama", "T. Darrell"], "venue": "Proceedings of the ACM International Conference on Multimedia, pp. 675\u2013678, ACM, 2014.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2014}, {"title": "Torch7: A matlablike environment for machine learning", "author": ["R. Collobert", "K. Kavukcuoglu", "C. Farabet"], "venue": "BigLearn, NIPS Workshop, no. EPFL-CONF-192376, 2011.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 1923}, {"title": "Theano: a cpu and gpu math expression compiler", "author": ["J. Bergstra", "O. Breuleux", "F. Bastien", "P. Lamblin", "R. Pascanu", "G. Desjardins", "J. Turian", "D. Warde-Farley", "Y. Bengio"], "venue": "Proceedings of the Python for scientific computing conference (SciPy), vol. 4, p. 3, Austin, TX, 2010.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2010}, {"title": "Large scale distributed deep networks", "author": ["J. Dean", "G. Corrado", "R. Monga", "K. Chen", "M. Devin", "M. Mao", "A. Senior", "P. Tucker", "K. Yang", "Q.V. Le"], "venue": "Advances in Neural Information Processing Systems, pp. 1223\u20131231, 2012.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2012}, {"title": "Fast training of convolutional networks through ffts", "author": ["M. Mathieu", "M. Henaff", "Y. LeCun"], "venue": "International Conference on Learning Representations (ICLR2014), CBLS, April 2014.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2014}, {"title": "Fast convolutional nets with fbfft: A gpu performance evaluation", "author": ["N. Vasilache", "J. Johnson", "M. Mathieu", "S. Chintala", "S. Piantino", "Y. LeCun"], "venue": "arXiv preprint arXiv:1412.7580, 2014.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2014}, {"title": "A fast learning algorithm for image segmentation with max-pooling convolutional networks", "author": ["J. Masci", "A. Giusti", "D. Ciresan", "G. Fricout", "J. Schmidhuber"], "venue": "Image Processing (ICIP), 2013 20th IEEE International Conference on, pp. 2713\u20132717, IEEE, 2013.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2013}, {"title": "Fast image scanning with deep max-pooling convolutional neural networks", "author": ["A. Giusti", "D.C. Cire\u015fan", "J. Masci", "L.M. Gambardella", "J. Schmidhuber"], "venue": "arXiv preprint arXiv:1302.1700, 2013.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2013}, {"title": "Overfeat: Integrated recognition, localization and detection using convolutional networks", "author": ["P. Sermanet", "D. Eigen", "X. Zhang", "M. Mathieu", "R. Fergus", "Y. Le- Cun"], "venue": "arXiv preprint arXiv:1312.6229, 2013.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2013}, {"title": "The potential of the intel xeon phi for supervised deep learning", "author": ["A. Viebke", "S. Pllana"], "venue": "arXiv preprint arXiv:1506.09067, 2015.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2015}, {"title": "Training large scale deep neural networks on the intel xeon phi many-core coprocessor", "author": ["L. Jin", "Z. Wang", "R. Gu", "C. Yuan", "Y. Huang"], "venue": "Proceedings of the 2014 IEEE International Parallel & Distributed Processing Symposium Workshops, IPDPSW \u201914, (Washington, DC, USA), pp. 1622\u20131630, IEEE Computer Society, 2014.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2014}, {"title": "Imagenet classification with deep convolutional neural networks", "author": ["A. Krizhevsky", "I. Sutskever", "G.E. Hinton"], "venue": "Advances in neural information processing systems, pp. 1097\u20131105, 2012.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2012}, {"title": "Deep neural networks segment neuronal membranes in electron microscopy images", "author": ["D. Ciresan", "A. Giusti", "L.M. Gambardella", "J. Schmidhuber"], "venue": "Advances in neural information processing systems, pp. 2843\u20132851, 2012.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2012}, {"title": "Fully convolutional networks for semantic segmentation", "author": ["J. Long", "E. Shelhamer", "T. Darrell"], "venue": "The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2015.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2015}, {"title": "Locally scale-invariant convolutional neural networks", "author": ["A. Kanazawa", "A. Sharma", "D.W. Jacobs"], "venue": "arXiv preprint arXiv:1412.5104, 2014.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2014}, {"title": "Traffic sign recognition with multi-scale convolutional networks", "author": ["P. Sermanet", "Y. LeCun"], "venue": "Neural Networks (IJCNN), The 2011 International Joint Conference on, pp. 2809\u20132813, IEEE, 2011.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2011}, {"title": "Brents theorem", "author": ["J. Gustafson"], "venue": "Encyclopedia of Parallel Computing (D. Padua, ed.), pp. 182\u2013185, Springer US, 2011.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2011}, {"title": "Simple, fast, and practical nonblocking and blocking concurrent queue algorithms", "author": ["M.M. Michael", "M.L. Scott"], "venue": "Proceedings of the fifteenth annual ACM symposium on Principles of distributed computing, pp. 267\u2013275, ACM, 1996.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 1996}, {"title": "cudnn: Efficient primitives for deep learning", "author": ["S. Chetlur", "C. Woolley", "P. Vandermersch", "J. Cohen", "J. Tran", "B. Catanzaro", "E. Shelhamer"], "venue": "arXiv preprint arXiv:1410.0759, 2014.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2014}, {"title": "Connectomic reconstruction of the inner plexiform layer in the mouse retina", "author": ["M. Helmstaedter", "K.L. Briggman", "S.C. Turaga", "V. Jain", "H.S. Seung", "W. Denk"], "venue": "Nature, vol. 500, no. 7461, pp. 168\u2013174, 2013.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2013}, {"title": "Scheduling multithreaded computations by work stealing", "author": ["R.D. Blumofe", "C.E. Leiserson"], "venue": "Journal of the ACM (JACM), vol. 46, no. 5, pp. 720\u2013748, 1999.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 1999}, {"title": "Recursive training of 2d-3d convolutional networks for neuronal boundary detection", "author": ["K. Lee", "A. Zlateski", "A. Vishwanathan", "H.S. Seung"], "venue": "arXiv preprint arXiv:1508.04843, 2015.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2015}, {"title": "Automated computation of arbor densities: a step toward identifying neuronal cell types", "author": ["U. S\u00fcmb\u00fcl", "A. Zlateski", "A. Vishwanathan", "R.H. Masland", "H.S. Seung"], "venue": "Frontiers in neuroanatomy, vol. 8, 2014.", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2014}, {"title": "Dropout: A simple way to prevent neural networks from overfitting", "author": ["N. Srivastava", "G. Hinton", "A. Krizhevsky", "I. Sutskever", "R. Salakhutdinov"], "venue": "The Journal of Machine Learning Research, vol. 15, no. 1, pp. 1929\u20131958, 2014.", "citeRegEx": "25", "shortCiteRegEx": null, "year": 1929}], "referenceMentions": [{"referenceID": 0, "context": "Caffe [1], Torch [2] and Theano[3].", "startOffset": 6, "endOffset": 9}, {"referenceID": 1, "context": "Caffe [1], Torch [2] and Theano[3].", "startOffset": 17, "endOffset": 20}, {"referenceID": 2, "context": "Caffe [1], Torch [2] and Theano[3].", "startOffset": 31, "endOffset": 34}, {"referenceID": 3, "context": "ConvNet learning has also been distributed over multiple machines [4].", "startOffset": 66, "endOffset": 69}, {"referenceID": 4, "context": "FFT convolution was previously applied to 2D ConvNets running on GPUs [5], [6], and is even more advantageous for 3D ConvNets on CPUs.", "startOffset": 70, "endOffset": 73}, {"referenceID": 5, "context": "FFT convolution was previously applied to 2D ConvNets running on GPUs [5], [6], and is even more advantageous for 3D ConvNets on CPUs.", "startOffset": 75, "endOffset": 78}, {"referenceID": 6, "context": "As far as we know, ZNN is the first publicly available software that supports efficient training of sliding window max-pooling ConvNets, which have been studied by [7], [8], [9].", "startOffset": 164, "endOffset": 167}, {"referenceID": 7, "context": "As far as we know, ZNN is the first publicly available software that supports efficient training of sliding window max-pooling ConvNets, which have been studied by [7], [8], [9].", "startOffset": 169, "endOffset": 172}, {"referenceID": 8, "context": "As far as we know, ZNN is the first publicly available software that supports efficient training of sliding window max-pooling ConvNets, which have been studied by [7], [8], [9].", "startOffset": 174, "endOffset": 177}, {"referenceID": 9, "context": "There is related work on using Xeon PhiTMfor supervised deep learning [10].", "startOffset": 70, "endOffset": 74}, {"referenceID": 10, "context": "and unsupervised deep learning [11].", "startOffset": 31, "endOffset": 35}, {"referenceID": 11, "context": "A max-pooling ConvNet in the context of visual object recognition [12] is a special case of the definition given above.", "startOffset": 66, "endOffset": 70}, {"referenceID": 8, "context": "If localization and detection are desired as well as recognition, one can slide a window over a large image, and apply the max-pooling ConvNet at each location of the window [9].", "startOffset": 174, "endOffset": 177}, {"referenceID": 12, "context": "The sliding window max-pooling ConvNet is also useful in the context of boundary detection and image segmentation [13].", "startOffset": 114, "endOffset": 118}, {"referenceID": 8, "context": "This approach has been called skip-kernels [9] or filter rarefaction [14], and is equivalent in its results to max-fragmentation-pooling [8], [7].", "startOffset": 43, "endOffset": 46}, {"referenceID": 13, "context": "This approach has been called skip-kernels [9] or filter rarefaction [14], and is equivalent in its results to max-fragmentation-pooling [8], [7].", "startOffset": 69, "endOffset": 73}, {"referenceID": 7, "context": "This approach has been called skip-kernels [9] or filter rarefaction [14], and is equivalent in its results to max-fragmentation-pooling [8], [7].", "startOffset": 137, "endOffset": 140}, {"referenceID": 6, "context": "This approach has been called skip-kernels [9] or filter rarefaction [14], and is equivalent in its results to max-fragmentation-pooling [8], [7].", "startOffset": 142, "endOffset": 145}, {"referenceID": 14, "context": "It could be useful when implementing a \u201cscale-invariant\u201d ConvNet [15], where convolutions with shared kernel weights are performed at multiple scales to capture scale-invariant features.", "startOffset": 65, "endOffset": 69}, {"referenceID": 13, "context": "This is particularly beneficial to the multi-scale approach [14], [16], where images with multiple resolutions are combined together to construct the representation.", "startOffset": 60, "endOffset": 64}, {"referenceID": 15, "context": "This is particularly beneficial to the multi-scale approach [14], [16], where images with multiple resolutions are combined together to construct the representation.", "startOffset": 66, "endOffset": 70}, {"referenceID": 4, "context": "It is less well-known that the FFT-direct crossover occurs at smaller kernel sizes for a ConvNet than for a single convolution [5], [6].", "startOffset": 127, "endOffset": 130}, {"referenceID": 5, "context": "It is less well-known that the FFT-direct crossover occurs at smaller kernel sizes for a ConvNet than for a single convolution [5], [6].", "startOffset": 132, "endOffset": 135}, {"referenceID": 4, "context": "This possibility was previously noted in passing but not implemented due to limited onboard GPU memory [5], [6].", "startOffset": 103, "endOffset": 106}, {"referenceID": 5, "context": "This possibility was previously noted in passing but not implemented due to limited onboard GPU memory [5], [6].", "startOffset": 108, "endOffset": 111}, {"referenceID": 4, "context": "3Note that our values differ from the ones in [5] as we take into account the difference in complexity between full and valid convolutions.", "startOffset": 46, "endOffset": 49}, {"referenceID": 16, "context": "According to Brent\u2019s theorem [17], if a computation can be performed in T\u221e time with an infinite number of processors, then", "startOffset": 29, "endOffset": 33}, {"referenceID": 16, "context": "The only exception is that the complexity of a convolutional layer depends logarithmically on width (dlog2 fe), because summing the results of f convergent convolutions requires this amount of time using the binary collapse algorithm described in [17].", "startOffset": 247, "endOffset": 251}, {"referenceID": 17, "context": "Lock-free queues, as described in [18] and implemented as a part of the boost [19] library are used to implement the pool operations.", "startOffset": 34, "endOffset": 38}, {"referenceID": 0, "context": "Therefore, we benchmarked ZNN against Caffe [1]", "startOffset": 44, "endOffset": 47}, {"referenceID": 2, "context": "and Theano [3], two popular GPU implementations.", "startOffset": 11, "endOffset": 14}, {"referenceID": 18, "context": "For Caffe, both default and cuDNN[20] implementations were used.", "startOffset": 33, "endOffset": 37}, {"referenceID": 19, "context": "Such kernel sizes are currently relevant for practical applications [21].", "startOffset": 68, "endOffset": 72}, {"referenceID": 4, "context": "in [5], [6] could be very high as it is proportional to the number of kernels in a layer.", "startOffset": 3, "endOffset": 6}, {"referenceID": 5, "context": "in [5], [6] could be very high as it is proportional to the number of kernels in a layer.", "startOffset": 8, "endOffset": 11}, {"referenceID": 20, "context": "The repository also provides alternative scheduling strategies such as simple FIFO or LIFO as well as some more complex ones based on work stealing [22].", "startOffset": 148, "endOffset": 152}, {"referenceID": 21, "context": "In fact, we have already used ZNN to achieve state of the art results in boundary detection [23] and computation of dendritic arbor densities [24].", "startOffset": 92, "endOffset": 96}, {"referenceID": 22, "context": "In fact, we have already used ZNN to achieve state of the art results in boundary detection [23] and computation of dendritic arbor densities [24].", "startOffset": 142, "endOffset": 146}, {"referenceID": 23, "context": "ZNN\u2019s repository contains some sample extensions providing functionality of dropout [25] and multi-scale [14], [16] networks.", "startOffset": 84, "endOffset": 88}, {"referenceID": 13, "context": "ZNN\u2019s repository contains some sample extensions providing functionality of dropout [25] and multi-scale [14], [16] networks.", "startOffset": 105, "endOffset": 109}, {"referenceID": 15, "context": "ZNN\u2019s repository contains some sample extensions providing functionality of dropout [25] and multi-scale [14], [16] networks.", "startOffset": 111, "endOffset": 115}], "year": 2015, "abstractText": "Convolutional networks (ConvNets) have become a popular approach to computer vision. It is important to accelerate ConvNet training, which is computationally costly. We propose a novel parallel algorithm based on decomposition into a set of tasks, most of which are convolutions or FFTs. Applying Brent\u2019s theorem to the task dependency graph implies that linear speedup with the number of processors is attainable within the PRAM model of parallel computation, for wide network architectures. To attain such performance on real shared-memory machines, our algorithm computes convolutions converging on the same node of the network with temporal locality to reduce cache misses, and sums the convergent convolution outputs via an almost wait-free concurrent method to reduce time spent in critical sections. We implement the algorithm with a publicly available software package called ZNN. Benchmarking with multi-core CPUs shows that ZNN can attain speedup roughly equal to the number of physical cores. We also show that ZNN can attain over 90x speedup on a many-core CPU (Xeon PhiTMKnights Corner). These speedups are achieved for network architectures with widths that are in common use. The task parallelism of the ZNN algorithm is suited to CPUs, while the SIMD parallelism of previous algorithms is compatible with GPUs. Through examples, we show that ZNN can be either faster or slower than certain GPU implementations depending on specifics of the network architecture, kernel sizes, and density and size of the output patch. ZNN may be less costly to develop and maintain, due to the relative ease of general-purpose CPU programming.", "creator": "LaTeX with hyperref package"}}}