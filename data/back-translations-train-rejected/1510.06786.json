{"id": "1510.06786", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "22-Oct-2015", "title": "Freshman or Fresher? Quantifying the Geographic Variation of Internet Language", "abstract": "We present a new computational technique to detect and analyze statistically significant geographic variation in language. Our meta-analysis approach captures statistical properties of word usage across geographical regions and uses statistical methods to identify significant changes specific to regions. While previous approaches have primarily focused on lexical variation between regions, our method identifies words that demonstrate semantic and syntactic variation as well.", "histories": [["v1", "Thu, 22 Oct 2015 22:53:10 GMT  (1142kb,D)", "https://arxiv.org/abs/1510.06786v1", "11 pages : Submitted to WWW 2016"], ["v2", "Mon, 7 Mar 2016 14:40:36 GMT  (5166kb,D)", "http://arxiv.org/abs/1510.06786v2", "11 pages (updated submission)"]], "COMMENTS": "11 pages : Submitted to WWW 2016", "reviews": [], "SUBJECTS": "cs.CL cs.IR cs.LG", "authors": ["vivek kulkarni", "bryan perozzi", "steven skiena"], "accepted": false, "id": "1510.06786"}, "pdf": {"name": "1510.06786.pdf", "metadata": {"source": "CRF", "title": "Freshman or Fresher? Quantifying the Geographic Variation of Language in Online Social Media", "authors": ["Vivek Kulkarni", "Steven Skiena"], "emails": ["vvkulkarni@cs.stonybrook.edu}", "bperozzi@cs.stonybrook.edu}", "skiena@cs.stonybrook.edu}"], "sections": [{"heading": "1 Introduction", "text": "It is about the question of whether it is about a way, in which it is about a way, in which it is about a way, in which it is about a way, in which it is about a way, in which it is about a way, in which it is about a way, in which it is about a way, in which it is about a way, in which it is about a way, in which it is about a way, in which it is about a way, in which it is about a way, in which it is about a way, in which it is about a way, in which it is about a way, in which it is about a way, in which it is about a way, and in which it is about a way, in which it is about a way, in which it is about a way, in which it is about a way, in which it is about a way, in which it is about a way, in which it is about a way, in which it is about a way, in which it is about a way, in which it is about a way and in which it is about a way, in which it is about a way and in which it is about a way, in which it is about a way and in which it is about a way and in which it is about a way, in which it is about a way and in which it is about a way and in which it is about a way and in which it is about a way and in which it is about a way and in which it is about a way and in which it is about which it is about a way and in which it is about a way and in which it is about a way and in which it is about a way and in which it is about which it is about a way and in which it is about a way and in which it is about which it is about a way and in which it is about a way and in which it is about which it is about a way and in which it is about which it is about a way and in which it is about which it is about a way and in which it is about a way and in which it is about which it is about a way and in which it is about a way and in which it is about a way and in which it is about a way and in which it is about which it is about which it is about a way and in which it is about a way and in which it is"}, {"heading": "3 Methods", "text": "In this section, we discuss methods to model regional word usage."}, {"heading": "3.1 Baseline Methods", "text": "A standard method for determining which words vary in geographical regions is to track their frequency of use. Formally, we track the change in the probability of a word between regions, as described in [24]. To characterize the difference in the frequency of use of w between a regional pair (ri, rj), we calculate the ratio SCORE (w) = Pri (w) Prj (w) wherePri (w) is the probability of occurrence of w in the ri region. An example of the information we collect by tracking word frequencies across regions is shown in Figure 4 that touchdown (an American football term) is used much more frequently in the US than in the UK. While this naive method is easy to implement and identify words that differ in their usage patterns, limiting it to rare words is an overemphasis."}, {"heading": "3.2 Distributional Method: GEODIST", "text": "As we noted in the previous section, linguistic variation is not limited to syntactic variations. (To detect subtle semantic changes, we must infer the contextual use of a word.) To do this, we use distribution methods that indicate a latent semantic space that translates each word into a continuous vector space. (We differ from the region in which we embedding a specific word to detect random variation between regions and suggest a method to detect statistically significant changes.) We try to learn a regional word embedding. (V, Cr 7) Rd with a neural language model. For each word of the neural language model, we learn: 1. A global embedding (w) for the word embedded all region-specific cues."}, {"heading": "3.3 Statistical Significance of Changes", "text": "In this section, we outline our method to quantify whether an observed change given by SCORE (w) or \u03b2) is significant. If working on an entire population (or in the absence of stochastic processes), a fairly standardized method for determining outliers typically indicates the Z-value confidence test [1] (obtained by standardizing raw values) and the marking of samples whose Z-value is set to a threshold \u03b2 (typically at the 95th percentile) as an outlier. However, since in our method SCORE (w) confidence values may vary due to stochastic processes (even possibly pure coincidence), whether an observed score is significant or not dependent on two factors: (a) the magnitude of the observed score (effect magnitude) and (b) the probability to achieve a score more extreme than the observed number of points may vary (even be pure coincidence) whether an observed score is significant or not dependent on two factors: (a) the magnitude of the observed score (effect magnitude of observed or even two factors)."}, {"heading": "4 Datasets", "text": "Here we outline the details of two online datasets we are looking at - tweets from different geographic locations on Twitter and Google Books Ngram Corpus.Algorithm 1 SCORESIGNIFICANCE (C, B, \u03b1) Input: C: Corpus of text with R regions, B: Number ofbootstrap samples, \u03b1: Confidence Interval threshold Output: E: Computed effect sizes for each word w, CI: Computed confidence intervals for each word w / Estimate the NULL distribution.1: BS \u2190 \"Corpora from the NULL Distribution\" Output: E: Computed effect sizes for each word w, CI: Computed confidence intervalls for each word w / / Estimate the NULL distribution.1: BS \u2190 \"Corporate Corporate from the NULL Distribution\" (Corporate from the NULL Distribution}."}, {"heading": "5 Results and Analysis", "text": "In this section, we apply our methods to various data sets described above to identify words that are used differently in different geographical regions. Below, we describe the results of our experiments."}, {"heading": "5.1 Geographical Variation Analysis", "text": "In fact, it is as if most people who are in the United States are not able to play by the rules. (...) It is not as if they do not know the rules of the world. (...) It is not as if they do not know the rules of the world. (...) It is as if they do not know the rules of the world. (...) It is as if they do not know the rules of the world. (...) It is as if they do not know the rules of the world. (...) It is as if they do not know the rules of the world. (...) It is as if they do not know the rules of the world. \"(...) It is as if they do not know the rules of the world.\" (...)"}, {"heading": "6 Semantic Distance", "text": "In this section, we examine the following question: Do British and American English converge semantically or do they diverge semantically over time? To measure the semantic distance between languages over time, we propose a measure of the semantic distance between two language variants at a certain point t. Specifically, at a certain point in time t we obtain a corpus C and a pair of regions (ri, rj). Using our method (see Section 3.2), we calculate the standardized distance Zt (w) for each word w between regions at a certain point in time. Then, we construct the intersection of the word set W, which is assumed to have changed significantly at each point in time. We do this so that we focus (a) only on the words that significantly differ between the linguistic dialects at the time t and (b) that are stable over time as differently identified words, allowing us to track the use of the same set of divergent words over time."}, {"heading": "1900 1920 1940 1960 1980 2000", "text": "Prior to the 1950s, laws in British English were mainly used as a legal term (with regulations, ordinances, laws, etc.), whereas American English used acts to refer to acts (such as vandalism, acts of sabotage), but in the 1960s British English began to adopt American use. We suspect that this effect is due to globalization (the invention of radio, television, and the Internet), but leave a rigorous study of this phenomenon to future work. While our measure of the semantic distance between languages does not capture lexical variation, the introduction of new words, etc., our work opens the door for future research to develop better metrics for measuring semantic distances while taking other forms of variation into account."}, {"heading": "7 Related Work", "text": "In fact, it is so that most people are able to determine for themselves what they want and what they want. (...) It is not so that people are able to determine for themselves. (...) \"It is not so that they can do it.\" (...) \"It is so, as if.\" (...) \"It is so.\" (...) \"It is so, as if.\" (...) \"\" It is so, as if. \"(...)\" (...) \"\" (...) \"(...)\" (...) \"(...)\" (...) \"(...)\" (... \")\" (... \")\" (... \")\" (... \")\" (... \")\" (... \")\" (... \")\" (... \")\" (... \")\" (... \"()\" (). () (() ()."}, {"heading": "Acknowledgments", "text": "We thank David Bamman for passing on the code for training situated word embedding and Yingtao Tian for valuable comments."}], "references": [{"title": "Polyglot-ner: Massive multilingual named entity recognition", "author": ["R. Al-Rfou", "V. Kulkarni", "B. Perozzi", "S. Skiena"], "venue": null, "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2015}, {"title": "Polyglot: Distributed word representations for multilingual nlp", "author": ["R. Al-Rfou", "B. Perozzi", "S. Skiena"], "venue": "In Proceedings of the Seventeenth Conference on Computational Natural Language Learning", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2013}, {"title": "Gender identity and lexical variation in social media", "author": ["D Bamman"], "venue": "Journal of Sociolinguistics", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2014}, {"title": "Distributed representations of geographically situated language", "author": ["D. Bamman", "C. Dyer", "N.A. Smith"], "venue": "Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2014}, {"title": "Neural probabilistic language models", "author": ["Y. Bengio", "H. Schwenk", "J.-S. Senecal", "F. Morin", "J.-L. Gauvain"], "venue": "In Innovations in Machine Learning", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2006}, {"title": "Stochastic gradient learning in neural networks", "author": ["L. Bottou"], "venue": "In Proceedings of Neuro-N\u0131\u0302mes", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 1991}, {"title": "Analyzing discourse communities with distributional semantic models", "author": ["I. Brigadir", "D. Greene", "P. Cunningham"], "venue": "In ACM Web Science", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2015}, {"title": "The expressive power of word embeddings", "author": ["Y. Chen", "B. Perozzi", "R. Al-Rfou", "S. Skiena"], "venue": "arXiv preprint arXiv:1301.3226", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2013}, {"title": "Natural language processing (almost) from scratch. JMLR", "author": ["R. Collobert", "J. Weston", "L. Bottou", "M. Karlen", "K. Kavukcuoglu", "P. Kuksa"], "venue": null, "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2011}, {"title": "Measuring the semantic distance between languages from a statistical analysis of bilingual dictionaries", "author": ["M.C. Cooper"], "venue": "Journal of Quantitative Linguistics", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2008}, {"title": "Mapping dialectal variation by querying social media", "author": ["G. Doyle"], "venue": "In EACL", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2014}, {"title": "Confidence interval or p-value?: part 4 of a series on evaluation of scientific publications. Deutsches \u00c4rzteblatt International", "author": ["J.-B. du Prel", "G. Hommel", "B. R\u00f6hrig", "M. Blettner"], "venue": null, "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2009}, {"title": "A latent variable model for geographic lexical variation", "author": ["J. Eisenstein", "B. O\u2019Connor", "N.A. Smith", "E.P. Xing"], "venue": null, "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2010}, {"title": "Diffusion of lexical change in social media", "author": ["J. Eisenstein", "B. O\u2019Connor", "N.A. Smith", "E.P. Xing"], "venue": "PLoS ONE", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2014}, {"title": "Discovering sociolinguistic associations with structured sparsity", "author": ["J. Eisenstein", "N. A Smith"], "venue": "ACL-HLT", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2011}, {"title": "A dataset of syntacticngrams over time from a very large corpus of english books", "author": ["Y. Goldberg", "J. Orwant"], "venue": "*SEM", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2013}, {"title": "Crowdsourcing dialect characterization through twitter", "author": ["B. Gon\u00e7alves", "D. S\u00e1nchez"], "venue": null, "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2014}, {"title": "A distributional similarity approach to the detection of semantic change in the google books ngram corpus. In GEMS", "author": ["K. Gulordava", "M. Baroni"], "venue": null, "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2011}, {"title": "Learning distributed representations of concepts", "author": ["G.E. Hinton"], "venue": "In Proceedings of the eighth annual conference of the cognitive science society", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 1986}, {"title": "Ad hoc monitoring of vocabulary shifts over time", "author": ["T. Kenter", "M. Wevers", "P Huijnen"], "venue": "In CIKM. ACM", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2015}, {"title": "Temporal analysis of language through neural language models", "author": ["Y. Kim", "Y.-I. Chiu", "K. Hanaki", "D. Hegde", "S. Petrov"], "venue": null, "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2014}, {"title": "Statistically significant detection of linguistic change", "author": ["V. Kulkarni", "R. Al-Rfou", "B. Perozzi", "S. Skiena"], "venue": null, "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2015}, {"title": "Locating language in time and space / edited by William Labov", "author": ["W. Labov"], "venue": null, "citeRegEx": "25", "shortCiteRegEx": "25", "year": 1980}, {"title": "Syntactic annotations for the google books ngram corpus", "author": ["Y. Lin", "J.-B. Michel", "E. L Aiden"], "venue": "In Proceedings of the ACL 2012 system demonstrations", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2012}, {"title": "Quantitative analysis of culture using millions of digitized books", "author": ["Michel", "J.-B"], "venue": "Science", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2011}, {"title": "Linguistic regularities in continuous space word representations", "author": ["T Mikolov"], "venue": "In Proceedings of NAACL-HLT", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2013}, {"title": "Efficient estimation of word representations in vector space. arXiv preprint arXiv:1301.3781", "author": ["T. Mikolov", "K. Chen", "G. Corrado", "J. Dean"], "venue": null, "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2013}, {"title": "Distributed representations of words and phrases and their compositionality", "author": ["T. Mikolov", "I. Sutskever", "K. Chen", "G.S. Corrado", "J. Dean"], "venue": null, "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2013}, {"title": "Linguistic variation and change: on the historical sociolinguistics of English", "author": ["J. Milroy"], "venue": null, "citeRegEx": "31", "shortCiteRegEx": "31", "year": 1992}, {"title": "A scalable hierarchical distributed language model", "author": ["A. Mnih", "G.E. Hinton"], "venue": null, "citeRegEx": "32", "shortCiteRegEx": "32", "year": 2009}, {"title": "Distributional measures of semantic distance: A survey", "author": ["S.M. Mohammad", "G. Hirst"], "venue": "arXiv preprint arXiv:1203.1858", "citeRegEx": "33", "shortCiteRegEx": "33", "year": 2012}, {"title": "Hierarchical probabilistic neural network language model", "author": ["F. Morin", "Y. Bengio"], "venue": "In Proceedings of the international workshop on artificial intelligence and statistics", "citeRegEx": "34", "shortCiteRegEx": "34", "year": 2005}, {"title": "Discovering demographic language variation", "author": ["B. O\u2019Connor", "J. Eisenstein", "E.P. Xing", "N.A. Smith"], "venue": "In Proc. of NIPS Workshop on Machine Learning for Social Computing", "citeRegEx": "35", "shortCiteRegEx": "35", "year": 2010}, {"title": "Improved part-ofspeech tagging for online conversational text with word clusters. Association for Computational Linguistics", "author": ["O. Owoputi", "B O\u2019Connor"], "venue": null, "citeRegEx": "36", "shortCiteRegEx": "36", "year": 2013}, {"title": "Inducing language networks from continuous space word representations", "author": ["B. Perozzi", "R Al-Rfou"], "venue": "In Complex Networks V", "citeRegEx": "37", "shortCiteRegEx": "37", "year": 2014}, {"title": "Learning representations by back-propagating errors. Cognitive modeling 1:213", "author": ["D.E. Rumelhart", "G.E. Hinton", "R.J. Williams"], "venue": null, "citeRegEx": "38", "shortCiteRegEx": "38", "year": 2002}, {"title": "Using effect size-or why the p value is not enough", "author": ["G.M. Sullivan", "R. Feinn"], "venue": "Journal of graduate medical education", "citeRegEx": "39", "shortCiteRegEx": "39", "year": 2012}, {"title": "Analysing Sociolinguistic Variation", "author": ["S.A. Tagliamonte"], "venue": null, "citeRegEx": "40", "shortCiteRegEx": "40", "year": 2006}], "referenceMentions": [{"referenceID": 22, "context": "[25, 31, 40, 41]).", "startOffset": 0, "endOffset": 16}, {"referenceID": 28, "context": "[25, 31, 40, 41]).", "startOffset": 0, "endOffset": 16}, {"referenceID": 37, "context": "[25, 31, 40, 41]).", "startOffset": 0, "endOffset": 16}, {"referenceID": 2, "context": "prevalent in geographic regions [4, 13, 15, 16].", "startOffset": 32, "endOffset": 47}, {"referenceID": 10, "context": "prevalent in geographic regions [4, 13, 15, 16].", "startOffset": 32, "endOffset": 47}, {"referenceID": 12, "context": "prevalent in geographic regions [4, 13, 15, 16].", "startOffset": 32, "endOffset": 47}, {"referenceID": 13, "context": "prevalent in geographic regions [4, 13, 15, 16].", "startOffset": 32, "endOffset": 47}, {"referenceID": 21, "context": "Formally, we track the change in probability of a word across regions as described in [24].", "startOffset": 86, "endOffset": 90}, {"referenceID": 21, "context": "A method to capture syntactic variation in word usage through time was proposed by [24].", "startOffset": 83, "endOffset": 87}, {"referenceID": 21, "context": "Given a word w and a pair of regions (ri, rj) we adapt the method outlined in [24] and compute the JennsenremitUK remitUS curbUK curbUS wadUK wadUS 0.", "startOffset": 78, "endOffset": 82}, {"referenceID": 3, "context": "We differentiate ourselves from the closest related work to our method [5], by explicitly accounting for random variation between regions, and proposing a method to detect statistically significant changes.", "startOffset": 71, "endOffset": 74}, {"referenceID": 29, "context": "Therefore, we approximate this probability by using hierarchical soft-max [32, 34] which reduces the cost of computing the normalization factor from O(|V|) to O(log |V|).", "startOffset": 74, "endOffset": 82}, {"referenceID": 31, "context": "Therefore, we approximate this probability by using hierarchical soft-max [32, 34] which reduces the cost of computing the normalization factor from O(|V|) to O(log |V|).", "startOffset": 74, "endOffset": 82}, {"referenceID": 5, "context": "We optimize the model parameters using stochastic gradient descent [8], as \u03c6t(wi) = \u03c6t(wi)\u2212\u03b1\u00d7 \u2202J \u2202\u03c6t(wi) where \u03b1 is the learning rate.", "startOffset": 67, "endOffset": 70}, {"referenceID": 35, "context": "We calculate the derivatives using the back-propagation algorithm [38].", "startOffset": 66, "endOffset": 70}, {"referenceID": 14, "context": "We emphasize that these regional differences detected by GEODIST are inherently semantic, the result of a level of language understanding unattainable by methods which focus solely on lexical variation [17].", "startOffset": 202, "endOffset": 206}, {"referenceID": 11, "context": "Even though p-values have been traditionally used to report significance, recently researchers have argued against their use as p-values themselves do not indicate what the observed effect size was and hence even very small effects can be deemed statistically significant [14, 39].", "startOffset": 272, "endOffset": 280}, {"referenceID": 36, "context": "Even though p-values have been traditionally used to report significance, recently researchers have argued against their use as p-values themselves do not indicate what the observed effect size was and hence even very small effects can be deemed statistically significant [14, 39].", "startOffset": 272, "endOffset": 280}, {"referenceID": 24, "context": "The Google Books Ngram Corpus The Google Books Ngram Corpus corpus [27] contains frequencies of short phrases of text (ngrams) which were taken from books spanning eight languages over five centuries.", "startOffset": 67, "endOffset": 71}, {"referenceID": 15, "context": "We obtained the POS Distribution of each word in the above corpora using Google Syntactic Ngrams[18, 26].", "startOffset": 96, "endOffset": 104}, {"referenceID": 23, "context": "We obtained the POS Distribution of each word in the above corpora using Google Syntactic Ngrams[18, 26].", "startOffset": 96, "endOffset": 104}, {"referenceID": 33, "context": "In order to obtain part of speech tags, for the tweets we use the TweetNLP POS Tagger[36].", "startOffset": 85, "endOffset": 89}, {"referenceID": 2, "context": "Socio-variational linguistics A large body of work studies how language varies according to geography and time [4, 5, 15, 17, 19, 22\u201324].", "startOffset": 111, "endOffset": 136}, {"referenceID": 3, "context": "Socio-variational linguistics A large body of work studies how language varies according to geography and time [4, 5, 15, 17, 19, 22\u201324].", "startOffset": 111, "endOffset": 136}, {"referenceID": 12, "context": "Socio-variational linguistics A large body of work studies how language varies according to geography and time [4, 5, 15, 17, 19, 22\u201324].", "startOffset": 111, "endOffset": 136}, {"referenceID": 14, "context": "Socio-variational linguistics A large body of work studies how language varies according to geography and time [4, 5, 15, 17, 19, 22\u201324].", "startOffset": 111, "endOffset": 136}, {"referenceID": 16, "context": "Socio-variational linguistics A large body of work studies how language varies according to geography and time [4, 5, 15, 17, 19, 22\u201324].", "startOffset": 111, "endOffset": 136}, {"referenceID": 19, "context": "Socio-variational linguistics A large body of work studies how language varies according to geography and time [4, 5, 15, 17, 19, 22\u201324].", "startOffset": 111, "endOffset": 136}, {"referenceID": 20, "context": "Socio-variational linguistics A large body of work studies how language varies according to geography and time [4, 5, 15, 17, 19, 22\u201324].", "startOffset": 111, "endOffset": 136}, {"referenceID": 21, "context": "Socio-variational linguistics A large body of work studies how language varies according to geography and time [4, 5, 15, 17, 19, 22\u201324].", "startOffset": 111, "endOffset": 136}, {"referenceID": 6, "context": "While previous work like [7, 9, 20, 22, 23] focus on temporal analysis of language variation, our work centers on methods to detect and analyze linguistic variation according to geography.", "startOffset": 25, "endOffset": 43}, {"referenceID": 17, "context": "While previous work like [7, 9, 20, 22, 23] focus on temporal analysis of language variation, our work centers on methods to detect and analyze linguistic variation according to geography.", "startOffset": 25, "endOffset": 43}, {"referenceID": 19, "context": "While previous work like [7, 9, 20, 22, 23] focus on temporal analysis of language variation, our work centers on methods to detect and analyze linguistic variation according to geography.", "startOffset": 25, "endOffset": 43}, {"referenceID": 20, "context": "While previous work like [7, 9, 20, 22, 23] focus on temporal analysis of language variation, our work centers on methods to detect and analyze linguistic variation according to geography.", "startOffset": 25, "endOffset": 43}, {"referenceID": 21, "context": "Recently [24] proposed methods to detect statistically significant linguistic change over time that hinge on timeseries analysis.", "startOffset": 9, "endOffset": 13}, {"referenceID": 2, "context": "Several works on geographic variation [4, 13, 15, 35] focus on lexical variation.", "startOffset": 38, "endOffset": 53}, {"referenceID": 10, "context": "Several works on geographic variation [4, 13, 15, 35] focus on lexical variation.", "startOffset": 38, "endOffset": 53}, {"referenceID": 12, "context": "Several works on geographic variation [4, 13, 15, 35] focus on lexical variation.", "startOffset": 38, "endOffset": 53}, {"referenceID": 32, "context": "Several works on geographic variation [4, 13, 15, 35] focus on lexical variation.", "startOffset": 38, "endOffset": 53}, {"referenceID": 2, "context": "Bamman and others [4] study lexical variation in social media like Twitter based on gender identity.", "startOffset": 18, "endOffset": 21}, {"referenceID": 12, "context": "[15] describe a latent variable model to capture geographic lexical variation.", "startOffset": 0, "endOffset": 4}, {"referenceID": 13, "context": "[16] outline a model to capture diffusion of lexical variation in social media.", "startOffset": 0, "endOffset": 4}, {"referenceID": 3, "context": "The work that is most closely related to ours is that of Bamman, Dyer, and Smith [5].", "startOffset": 81, "endOffset": 84}, {"referenceID": 30, "context": "Measures of semantic distance have been developed for units of language (words, concepts etc) which [33] provide an excellent survey.", "startOffset": 100, "endOffset": 104}, {"referenceID": 9, "context": "Cooper [12] study the problem of measuring semantic distance between languages, by attempting to capture the relative difficulty of translating various pairs of languages using bi-lingual dictionaries.", "startOffset": 7, "endOffset": 11}, {"referenceID": 18, "context": "Word Embeddings The concept of using distributed representations to learn a mapping from symbolic data to continuous space dates back to Hinton [21].", "startOffset": 144, "endOffset": 148}, {"referenceID": 4, "context": "[6] proposed a neural language model to learn word embeddings and demonstrated that they outperform traditional n-gram based models.", "startOffset": 0, "endOffset": 3}, {"referenceID": 26, "context": "[29] proposed Skipgram models for learning word embeddings and demonstrated that they capture fine grained structures and linguistic regularities [28, 30].", "startOffset": 0, "endOffset": 4}, {"referenceID": 25, "context": "[29] proposed Skipgram models for learning word embeddings and demonstrated that they capture fine grained structures and linguistic regularities [28, 30].", "startOffset": 146, "endOffset": 154}, {"referenceID": 27, "context": "[29] proposed Skipgram models for learning word embeddings and demonstrated that they capture fine grained structures and linguistic regularities [28, 30].", "startOffset": 146, "endOffset": 154}, {"referenceID": 34, "context": "Also [37] induce language networks over word embeddings to reveal rich but varied community structure.", "startOffset": 5, "endOffset": 9}, {"referenceID": 0, "context": "Finally these embeddings have been demonstrated to be useful features for several NLP tasks [2, 3, 10, 11].", "startOffset": 92, "endOffset": 106}, {"referenceID": 1, "context": "Finally these embeddings have been demonstrated to be useful features for several NLP tasks [2, 3, 10, 11].", "startOffset": 92, "endOffset": 106}, {"referenceID": 7, "context": "Finally these embeddings have been demonstrated to be useful features for several NLP tasks [2, 3, 10, 11].", "startOffset": 92, "endOffset": 106}, {"referenceID": 8, "context": "Finally these embeddings have been demonstrated to be useful features for several NLP tasks [2, 3, 10, 11].", "startOffset": 92, "endOffset": 106}], "year": 2016, "abstractText": "In this paper we present a new computational technique to detect and analyze statistically significant geographic variation in language. While previous approaches have primarily focused on lexical variation between regions, our method identifies words that demonstrate semantic and syntactic variation as well. Our meta-analysis approach captures statistical properties of word usage across geographical regions and uses statistical methods to identify significant changes specific to regions. We extend recently developed techniques for neural language models to learn word representations which capture differing semantics across geographical regions. In order to quantify this variation and ensure robust detection of true regional differences, we formulate a null model to determine whether observed changes are statistically significant. Our method is the first such approach to explicitly account for random variation due to chance while detecting regional variation in word meaning. To validate our model, we study and analyze two different massive online data sets: millions of tweets from Twitter spanning not only four different countries but also fifty states, as well as millions of phrases contained in the Google Book Ngrams. Our analysis reveals interesting facets of language change at multiple scales of geographic resolution \u2013 from neighboring states to distant continents. Finally, using our model, we propose a measure of semantic distance between languages. Our analysis of British and American English over a period of 100 years reveals that semantic variation between these dialects is shrinking.", "creator": "LaTeX with hyperref package"}}}