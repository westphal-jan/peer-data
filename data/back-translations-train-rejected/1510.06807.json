{"id": "1510.06807", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "23-Oct-2015", "title": "Learning in the Rational Speech Acts Model", "abstract": "The Rational Speech Acts (RSA) model treats language use as a recursive process in which probabilistic speaker and listener agents reason about each other's intentions to enrich the literal semantics of their language along broadly Gricean lines. RSA has been shown to capture many kinds of conversational implicature, but it has been criticized as an unrealistic model of speakers, and it has so far required the manual specification of a semantic lexicon, preventing its use in natural language processing applications that learn lexical knowledge from data. We address these concerns by showing how to define and optimize a trained statistical classifier that uses the intermediate agents of RSA as hidden layers of representation forming a non-linear activation function. This treatment opens up new application domains and new possibilities for learning effectively from data. We validate the model on a referential expression generation task, showing that the best performance is achieved by incorporating features approximating well-established insights about natural language generation into RSA.", "histories": [["v1", "Fri, 23 Oct 2015 02:24:23 GMT  (700kb,D)", "http://arxiv.org/abs/1510.06807v1", "12 pages, 3 figures, 1 table. Preprint for Amsterdam Colloquium"]], "COMMENTS": "12 pages, 3 figures, 1 table. Preprint for Amsterdam Colloquium", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["will monroe", "christopher potts"], "accepted": false, "id": "1510.06807"}, "pdf": {"name": "1510.06807.pdf", "metadata": {"source": "CRF", "title": "Learning in the Rational Speech Acts Model", "authors": ["Will Monroe", "Christopher Potts"], "emails": ["wmonroe4@cs.stanford.edu,", "cgpotts@stanford.edu"], "sections": [{"heading": null, "text": "The Rational Speech Acts (RSA) model treats language usage as a recursive process in which the speaker and listener ponder each other's intentions in order to enrich the literal semantics of their language along broad Gricean lines. RSA has been shown to capture many types of conversation implications, but has been criticized as an unrealistic speaker model, and has so far required the manual specification of a semantic lexicon, preventing its use in natural language processing applications that learn lexical knowledge from data. We address these concerns by showing how to define and optimize a trained statistical classifier that uses RSA's intermediate agents as hidden layers of representation that form a nonlinear activation function. This treatment opens up new fields of application and new ways to effectively learn from data. We validate the model using a task to generate referential expressions and show that the best performance is achieved by incorporating traits that are well-established insights over natural language."}, {"heading": "1 Pragmatic language use", "text": "In the Grices view of language use [18], people are rational agents who can communicate efficiently and effectively by arguing about common communicative goals, the cost of production, previous expectations and beliefs of others. [11] The Rational Speech Acts (RSA) model [11] is a current reconstruction of Grice's core ideas. RSA and its extensions have shown that they capture many types of conversation implications and closely model the psycholinguistic data of children and adults. Both Grices theories and RSA have been criticized for predicting that people are more rational than they actually are. These criticisms have been particularly pronounced in the context of language production. It seems that their utterances are longer than they need to be."}, {"heading": "2 RSA as a speaker model", "text": "RSA is a descendant of the signal systems of [25] and refers to ideas from iterated best response (IBR) models [13, 20], iterated cautious response (ICR) models [21], and cognitive hierarchies [4] (see also [17, 31]). RSA models use language as a recursive process in which speaker and listener play a reference game in the context of the images in Figure 1 (a) to enrich the literal semantics of their language, increasing the efficiency and reliability of their communication compared to what purely literal agents can achieve. For example, suppose a speaker and listener play a reference game in the context of the images in Figure 1 (a)."}, {"heading": "3 The TUNA corpus", "text": "In Section 6, we evaluate RSA and learn RSA in the TUNA corpus [34, 15], a widely used resource for developing and testing models for the production of natural language. We are introducing the corpus now because it helps clarify the learning task facing our model, which we will define in the next section. In the TUNA corpus, participants were given a target speaker or speaker in the context of seven other distractors and were asked to describe the target (s). Experiments were conducted in two areas, furniture and people, each with a singular condition (describe a single unit) and a plural condition (describe two). Figure 2 provides a (slightly simplified) example from the singular furniture section, with the object identified by shade. In this case, the participant wrote the message \"blue fan small.\" All entities and messages are commented on with their semantic attributes, as shown here in simplified form (see the pictures)."}, {"heading": "4 Learned RSA", "text": "We formulate RSA as a machine learning model that can incorporate the quirks and constraints that characterize natural descriptions, while still presenting a unified model of pragmatic thinking. [17] This approach builds on the two-tiered, speaker-centered classifier of [17], but differs from them in that we directly optimize the performance of the pragmatic speaker in education, while [17] applying a recursive model of thinking based on a pre-trained classifier. Like RSA, the model can be generalized to allow additional intermediate agents, and it can be easily reformulated to begin with a literary listener. To build an agent that effectively learns from data, we must represent the items in our dataset in a way that accurately captures their key distinguishing features and allows robust generalizations to new items."}, {"heading": "5 Example", "text": "Figure 3 illustrates crucial aspects of how our model is optimized, elaborating on the concepts from the previous section. [The example also shows the ability of the trained S1 model to generate a specific implication without having observed a person in their data, while maintaining the ability to produce uninformative attributes when encouraged to do so by experience. However, as in our main experiments, we frame the learning task in terms of attribute selection with TUNA-like data. In this toy experiment, the agent is trained in two example contexts, consisting of a target speaker, a distraction speaker and a human-made expression. It is evaluated on a third test example. This small data model is given in the two lines of Figure 3. The expression on the test example is shown for comparison; it is not provided for the presentation of the data in the third series."}, {"heading": "6 Experiments", "text": "We focus on the singular part of the corpus used in 2008 and 2009 to refer to the expression Generation Challenges. We do not have access to the train / dev / test splits from these challenges, so we report five-fold cross-validation numbers. The singular part consists of 420 furniture tests with 176 different speakers and 360 people conducting studies with 228 different speakers. Primary evaluation metrics used in the attribute selection task with TUNA data are multisets Dice based on the attributes of the generated messages: 2 x x, D min (x), Za (mj), Attributes (x) | a (mj) | a (16) Here, a (m) is the multiset of message m, D is the non-multiset union of a (mi) and a (mj, x), x)."}, {"heading": "7 Conclusion", "text": "Our initial experiments show the utility of RSA as a trained classifier in generating referential expressions. The primary advantages of this version of RSA lie in the flexible way in which it can learn from available data, not only eliminating the need to manually define a complex semantic lexicon, but also providing the analytical freedom to create models that are sensitive to factors that guide the production of natural language and that are not naturally expressed in standard RSA. This basic presentation suggests a number of possible next steps. For example, it would be natural to apply the model to pragmatic interpretation (from the listener's perspective); this does not require significant formal changes to the model as defined in Section 4, and opens new avenues for evaluating pragmatic models in standard classification tasks such as mood analysis, topic prediction, and reasoning in relation to natural language. In addition, all versions of the model could consider incorporating additional hidden speaker and listener layers, incorporating the more pragmatic learning of the message into the broader spectrum, and more pragmatic learning of the phenomena."}], "references": [{"title": "Overspecification and the cost of pragmatic reasoning about referring expressions", "author": ["Peter Baumann", "Brady Clark", "Stefan Kaufmann"], "venue": "In Proceedings of the 36th Annual Meeting of the Cognitive Science Society. Cognitive Science Society,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2014}, {"title": "Pragmatic reasoning through semantic inference", "author": ["Leon Bergen", "Roger Levy", "Noah D. Goodman"], "venue": "Ms., MIT, UCSD, and Stanford,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2014}, {"title": "Large-scale machine learning with stochastic gradient descent", "author": ["L\u00e9on Bottou"], "venue": "Proceedings of the 19th International Conference on Computational Statistics,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2010}, {"title": "A cognitive hierarchy model of games", "author": ["Colin F. Camerer", "Teck-Hua Ho", "Juin-Kuan Chong"], "venue": "The Quarterly Journal of Economics,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2004}, {"title": "A Gaussian prior for smoothing maximum entropy models", "author": ["Stanley F. Chen", "Ronald Rosenfeld"], "venue": "Technical Report CMU-CS-99-108,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 1999}, {"title": "Computational interpretations of the Gricean maxims in the generation of referring expressions", "author": ["Robert Dale", "Ehud Reiter"], "venue": "Cognitive Science,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 1995}, {"title": "Optimal reasoning about referential expressions", "author": ["Judith Degen", "Michael Franke"], "venue": "Proceedings of the 16th Workshop on the Semantics and Pragmatics of Dialogue,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2012}, {"title": "A few useful things to know about machine learning", "author": ["Pedro Domingos"], "venue": "Communications of ACM,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2012}, {"title": "Adaptive subgradient methods for online learning and stochastic optimization", "author": ["John Duchi", "Elad Hazan", "Yoram Singer"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2011}, {"title": "Do speakers and listeners observe the Gricean maxim of quantity", "author": ["Paul E. Engelhardt", "Karl G.D. Bailey", "Fernanda Ferreira"], "venue": "Journal of Memory and Language,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2006}, {"title": "Predicting pragmatic reasoning in language", "author": ["Michael C. Frank", "Noah D. Goodman"], "venue": "games. Science,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2012}, {"title": "Inferring word meanings by assuming that speakers are informative", "author": ["Michael C. Frank", "Noah D. Goodman"], "venue": "Cognitive Psychology,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2014}, {"title": "Signal to Act: Game Theory in Pragmatics", "author": ["Michael Franke"], "venue": "ILLC Dissertation Series. Institute for Logic, Language and Computation,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2009}, {"title": "The TUNA Challenge 2008: Overview and evaluation results", "author": ["Albert Gatt", "Anja Belz", "Eric Kow"], "venue": "In Proceedings of the Fifth International Natural Language Generation Conference,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2008}, {"title": "Evaluating algorithms for the generation of referring expressions using a balanced corpus", "author": ["Albert Gatt", "Ielka van der Sluis", "Kees van Deemter"], "venue": "In Proceedings of the Eleventh European Workshop on Natural Language Generation,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2007}, {"title": "Are we Bayesian referring expression generators", "author": ["Albert Gatt", "Roger P.G. van Gompel", "Kees van Deemter", "Emiel Krahmer"], "venue": "In Proceedings of the Thirty-Fifth Annual Conference of the Cognitive Science Society,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2013}, {"title": "A game-theoretic approach to generating spatial descriptions", "author": ["Dave Golland", "Percy Liang", "Dan Klein"], "venue": "In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2010}, {"title": "Logic and conversation", "author": ["H. Paul Grice"], "venue": "Syntax and Semantics,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 1975}, {"title": "The Elements of Statistical Learning", "author": ["Trevor Hastie", "Robert Tibshirani", "Jerome Friedman"], "venue": null, "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2009}, {"title": "Game theory in semantics and pragmatics", "author": ["Gerhard J\u00e4ger"], "venue": "Semantics: An International Handbook of Natural Language Meaning,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2012}, {"title": "Formalizing the pragmatics of metaphor understanding", "author": ["Justine T. Kao", "Leon Bergen", "Noah D. Goodman"], "venue": "In Proceedings of the 36th Annual Meeting of the Cognitive Science Society,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2014}, {"title": "Nonliteral understanding of number words", "author": ["Justine T. Kao", "Jean Y. Wu", "Leon Bergen", "Noah D. Goodman"], "venue": "Proceedings of the National Academy of Sciences,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2014}, {"title": "Speaking: From Intention to Articulation, volume 1", "author": ["Willem JM Levelt"], "venue": "MIT press,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 1993}, {"title": "Convention", "author": ["David Lewis"], "venue": "Reprinted", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 1969}, {"title": "Bringing machine learning and compositional semantics together", "author": ["Percy Liang", "Christopher Potts"], "venue": "Annual Review of Linguistics,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2015}, {"title": "Generalized Linear Models", "author": ["Peter McCullagh", "John A. Nelder"], "venue": null, "citeRegEx": "27", "shortCiteRegEx": "27", "year": 1989}, {"title": "A Bayesian model of grounded color semantics", "author": ["Brian McMahan", "Matthew Stone"], "venue": "Transactions of the Association for Computational Linguistics,", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2015}, {"title": "Incremental speech production and referential overspecification", "author": ["Thomas Pechmann"], "venue": null, "citeRegEx": "29", "shortCiteRegEx": "29", "year": 1989}, {"title": "Embedded implicatures as pragmatic inferences under compositional lexical uncertainty", "author": ["Christopher Potts", "Daniel Lassiter", "Roger Levy", "Michael C. Frank"], "venue": "To appear in Journal of Semantics,", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2015}, {"title": "Speakers\u2019 and listeners\u2019 processes in a word communication", "author": ["Seymour Rosenberg", "Bertram D. Cohen"], "venue": "task. Science,", "citeRegEx": "31", "shortCiteRegEx": "31", "year": 1964}, {"title": "Learning internal representations by error propagation", "author": ["David E. Rumelhart", "Geoffrey E. Hinton", "Ronald J. Williams"], "venue": "Parallel Distributed Processing: Explorations in the Microstructure of Cognition,", "citeRegEx": "32", "shortCiteRegEx": "32", "year": 1986}, {"title": "Ad-hoc scalar implicature in adults and children", "author": ["Alex Stiller", "Noah D. Goodman", "Michael C. Frank"], "venue": "Proceedings of the 33rd Annual Meeting of the Cognitive Science Society,", "citeRegEx": "33", "shortCiteRegEx": "33", "year": 2011}, {"title": "Building a semantically transparent corpus for the generation of referring expressions", "author": ["Kees van Deemter", "Ielka van der Sluis", "Albert Gatt"], "venue": "In Proceedings of the Fourth International Natural Language Generation Conference,", "citeRegEx": "34", "shortCiteRegEx": "34", "year": 2006}, {"title": "Emergence of Gricean maxims from multi-agent decision theory. In Human Language Technologies: The 2013 Annual Conference of the North American Chapter of the Association for Computational Linguistics, pages 1072\u20131081", "author": ["Adam Vogel", "Max Bodoia", "Christopher Potts", "Dan Jurafsky"], "venue": "Association for Computational Linguistics,", "citeRegEx": "35", "shortCiteRegEx": "35", "year": 2013}, {"title": "Learning to reason pragmatically with cognitive limitations", "author": ["Adam Vogel", "Andr\u00e9s G\u00f3mez Emilsson", "Michael C. Frank", "Dan Jurafsky", "Christopher Potts"], "venue": "In Proceedings of the 36th Annual Meeting of the Cognitive Science Society,", "citeRegEx": "36", "shortCiteRegEx": "36", "year": 2014}], "referenceMentions": [{"referenceID": 17, "context": "In the Gricean view of language use [18], people are rational agents who are able to communicate efficiently and effectively by reasoning in terms of shared communicative goals, the costs of production, prior expectations, and others\u2019 belief states.", "startOffset": 36, "endOffset": 40}, {"referenceID": 10, "context": "The Rational Speech Acts (RSA) model [11] is a recent Bayesian reconstruction of these core Gricean ideas.", "startOffset": 37, "endOffset": 41}, {"referenceID": 6, "context": "RSA and its extensions have been shown to capture many kinds of conversational implicature and to closely model psycholinguistic data from children and adults [7, 2, 23, 30, 33].", "startOffset": 159, "endOffset": 177}, {"referenceID": 1, "context": "RSA and its extensions have been shown to capture many kinds of conversational implicature and to closely model psycholinguistic data from children and adults [7, 2, 23, 30, 33].", "startOffset": 159, "endOffset": 177}, {"referenceID": 21, "context": "RSA and its extensions have been shown to capture many kinds of conversational implicature and to closely model psycholinguistic data from children and adults [7, 2, 23, 30, 33].", "startOffset": 159, "endOffset": 177}, {"referenceID": 28, "context": "RSA and its extensions have been shown to capture many kinds of conversational implicature and to closely model psycholinguistic data from children and adults [7, 2, 23, 30, 33].", "startOffset": 159, "endOffset": 177}, {"referenceID": 31, "context": "RSA and its extensions have been shown to capture many kinds of conversational implicature and to closely model psycholinguistic data from children and adults [7, 2, 23, 30, 33].", "startOffset": 159, "endOffset": 177}, {"referenceID": 0, "context": "It seems that speakers often fall short: their utterances are longer than they need to be, underinformative, unintentionally ambiguous, obscure, and so forth [1, 10, 16, 24, 28, 29].", "startOffset": 158, "endOffset": 181}, {"referenceID": 9, "context": "It seems that speakers often fall short: their utterances are longer than they need to be, underinformative, unintentionally ambiguous, obscure, and so forth [1, 10, 16, 24, 28, 29].", "startOffset": 158, "endOffset": 181}, {"referenceID": 15, "context": "It seems that speakers often fall short: their utterances are longer than they need to be, underinformative, unintentionally ambiguous, obscure, and so forth [1, 10, 16, 24, 28, 29].", "startOffset": 158, "endOffset": 181}, {"referenceID": 22, "context": "It seems that speakers often fall short: their utterances are longer than they need to be, underinformative, unintentionally ambiguous, obscure, and so forth [1, 10, 16, 24, 28, 29].", "startOffset": 158, "endOffset": 181}, {"referenceID": 26, "context": "It seems that speakers often fall short: their utterances are longer than they need to be, underinformative, unintentionally ambiguous, obscure, and so forth [1, 10, 16, 24, 28, 29].", "startOffset": 158, "endOffset": 181}, {"referenceID": 27, "context": "It seems that speakers often fall short: their utterances are longer than they need to be, underinformative, unintentionally ambiguous, obscure, and so forth [1, 10, 16, 24, 28, 29].", "startOffset": 158, "endOffset": 181}, {"referenceID": 3, "context": "RSA can incorporate notions of bounded rationality [4, 13, 20], but it still sharply contrasts with views in the tradition of [6], in which speaker agents rely on heuristics and shortcuts to try to accurately describe the world while managing the cognitive demands of language production.", "startOffset": 51, "endOffset": 62}, {"referenceID": 12, "context": "RSA can incorporate notions of bounded rationality [4, 13, 20], but it still sharply contrasts with views in the tradition of [6], in which speaker agents rely on heuristics and shortcuts to try to accurately describe the world while managing the cognitive demands of language production.", "startOffset": 51, "endOffset": 62}, {"referenceID": 19, "context": "RSA can incorporate notions of bounded rationality [4, 13, 20], but it still sharply contrasts with views in the tradition of [6], in which speaker agents rely on heuristics and shortcuts to try to accurately describe the world while managing the cognitive demands of language production.", "startOffset": 51, "endOffset": 62}, {"referenceID": 5, "context": "RSA can incorporate notions of bounded rationality [4, 13, 20], but it still sharply contrasts with views in the tradition of [6], in which speaker agents rely on heuristics and shortcuts to try to accurately describe the world while managing the cognitive demands of language production.", "startOffset": 126, "endOffset": 129}, {"referenceID": 32, "context": "We validate the model on the task of attribute selection for referring expression generation with a widely-used corpus of referential descriptions (the TUNA corpus; [34, 15]), showing that it improves on heuristic-driven models and pure RSA by synthesizing the best aspects of both.", "startOffset": 165, "endOffset": 173}, {"referenceID": 14, "context": "We validate the model on the task of attribute selection for referring expression generation with a widely-used corpus of referential descriptions (the TUNA corpus; [34, 15]), showing that it improves on heuristic-driven models and pure RSA by synthesizing the best aspects of both.", "startOffset": 165, "endOffset": 173}, {"referenceID": 23, "context": "RSA is a descendent of the signaling systems of [25] and draws on ideas from iterated best response (IBR) models [13, 20], iterated cautious response (ICR) models [21], and cognitive hierarchies [4] (see also [17, 31]).", "startOffset": 48, "endOffset": 52}, {"referenceID": 12, "context": "RSA is a descendent of the signaling systems of [25] and draws on ideas from iterated best response (IBR) models [13, 20], iterated cautious response (ICR) models [21], and cognitive hierarchies [4] (see also [17, 31]).", "startOffset": 113, "endOffset": 121}, {"referenceID": 19, "context": "RSA is a descendent of the signaling systems of [25] and draws on ideas from iterated best response (IBR) models [13, 20], iterated cautious response (ICR) models [21], and cognitive hierarchies [4] (see also [17, 31]).", "startOffset": 113, "endOffset": 121}, {"referenceID": 3, "context": "RSA is a descendent of the signaling systems of [25] and draws on ideas from iterated best response (IBR) models [13, 20], iterated cautious response (ICR) models [21], and cognitive hierarchies [4] (see also [17, 31]).", "startOffset": 195, "endOffset": 198}, {"referenceID": 16, "context": "RSA is a descendent of the signaling systems of [25] and draws on ideas from iterated best response (IBR) models [13, 20], iterated cautious response (ICR) models [21], and cognitive hierarchies [4] (see also [17, 31]).", "startOffset": 209, "endOffset": 217}, {"referenceID": 29, "context": "RSA is a descendent of the signaling systems of [25] and draws on ideas from iterated best response (IBR) models [13, 20], iterated cautious response (ICR) models [21], and cognitive hierarchies [4] (see also [17, 31]).", "startOffset": 209, "endOffset": 217}, {"referenceID": 1, "context": "Models of this general form have been shown to capture a wide range of pragmatic behaviors [2, 12, 22, 23, 30] and to increase success in task-oriented dialogues [35, 36].", "startOffset": 91, "endOffset": 110}, {"referenceID": 11, "context": "Models of this general form have been shown to capture a wide range of pragmatic behaviors [2, 12, 22, 23, 30] and to increase success in task-oriented dialogues [35, 36].", "startOffset": 91, "endOffset": 110}, {"referenceID": 20, "context": "Models of this general form have been shown to capture a wide range of pragmatic behaviors [2, 12, 22, 23, 30] and to increase success in task-oriented dialogues [35, 36].", "startOffset": 91, "endOffset": 110}, {"referenceID": 21, "context": "Models of this general form have been shown to capture a wide range of pragmatic behaviors [2, 12, 22, 23, 30] and to increase success in task-oriented dialogues [35, 36].", "startOffset": 91, "endOffset": 110}, {"referenceID": 28, "context": "Models of this general form have been shown to capture a wide range of pragmatic behaviors [2, 12, 22, 23, 30] and to increase success in task-oriented dialogues [35, 36].", "startOffset": 91, "endOffset": 110}, {"referenceID": 33, "context": "Models of this general form have been shown to capture a wide range of pragmatic behaviors [2, 12, 22, 23, 30] and to increase success in task-oriented dialogues [35, 36].", "startOffset": 162, "endOffset": 170}, {"referenceID": 34, "context": "Models of this general form have been shown to capture a wide range of pragmatic behaviors [2, 12, 22, 23, 30] and to increase success in task-oriented dialogues [35, 36].", "startOffset": 162, "endOffset": 170}, {"referenceID": 15, "context": "RSA has been criticized on the grounds that it predicts unrealistic speaker behavior [16].", "startOffset": 85, "endOffset": 89}, {"referenceID": 32, "context": "In Section 6, we evaluate RSA and learned RSA in the TUNA corpus [34, 15], a widely used resource for developing and testing models of natural language generation.", "startOffset": 65, "endOffset": 73}, {"referenceID": 14, "context": "In Section 6, we evaluate RSA and learned RSA in the TUNA corpus [34, 15], a widely used resource for developing and testing models of natural language generation.", "startOffset": 65, "endOffset": 73}, {"referenceID": 16, "context": "This approach builds on the two-layer speaker-centric classifier of [17], but differs from theirs in that we directly optimize the performance of the pragmatic speaker in training, whereas [17] apply a recursive reasoning model on top of a pre-trained classifier.", "startOffset": 68, "endOffset": 72}, {"referenceID": 16, "context": "This approach builds on the two-layer speaker-centric classifier of [17], but differs from theirs in that we directly optimize the performance of the pragmatic speaker in training, whereas [17] apply a recursive reasoning model on top of a pre-trained classifier.", "startOffset": 189, "endOffset": 193}, {"referenceID": 7, "context": "To build an agent that learns effectively from data, we must represent the items in our dataset in a way that accurately captures their important distinguishing properties and permits robust generalization to new items [8, 26].", "startOffset": 219, "endOffset": 226}, {"referenceID": 24, "context": "To build an agent that learns effectively from data, we must represent the items in our dataset in a way that accurately captures their important distinguishing properties and permits robust generalization to new items [8, 26].", "startOffset": 219, "endOffset": 226}, {"referenceID": 18, "context": "Learned RSA is built on top of a log-linear model, standard in the machine learning literature and widely applied to classification tasks [19, 27].", "startOffset": 138, "endOffset": 146}, {"referenceID": 25, "context": "Learned RSA is built on top of a log-linear model, standard in the machine learning literature and widely applied to classification tasks [19, 27].", "startOffset": 138, "endOffset": 146}, {"referenceID": 30, "context": "In optimizing the performance of the pragmatic speaker S1 by adjusting the parameters to the simpler classifier S0, the RSA back-and-forth reasoning can be thought of as a non-linear function through which errors are propagated in training, similar to the activation functions in neural network models [32].", "startOffset": 302, "endOffset": 306}, {"referenceID": 4, "context": "Imposing this prior helps prevent overfitting to the training data and thereby damaging our ability to generalize well to new examples [5].", "startOffset": 135, "endOffset": 138}, {"referenceID": 2, "context": "The stochastic gradient descent (SGD) family of first-order optimization techniques [3] can be used to approximately maximize J(\u03b8) by obtaining noisy estimates of its gradient and \u201chillclimbing\u201d in the direction of the estimates.", "startOffset": 84, "endOffset": 87}, {"referenceID": 8, "context": "To find a good learning rate, we use AdaGrad [9], which sets the learning rate adaptively for each example based on an initial step size \u03b7 and gradient history.", "startOffset": 45, "endOffset": 48}, {"referenceID": 15, "context": "Pure RSA performs poorly for reasons predicted by [16]\u2014for example, it under-produces color terms and head nouns like desk, chair, and person.", "startOffset": 50, "endOffset": 54}, {"referenceID": 13, "context": "The performance of the learned RSA model on the people trials also compares favorably to the best dev set performance numbers from the 2008 Challenge [14], namely, .", "startOffset": 150, "endOffset": 154}, {"referenceID": 13, "context": "(In particular, the Accuracy values given in [14] are unfortunately not comparable with the values we present, as they reflect \u201cperfect match with at least one of the two reference outputs\u201d [emphasis in original].", "startOffset": 45, "endOffset": 49}], "year": 2015, "abstractText": "The Rational Speech Acts (RSA) model treats language use as a recursive process in which probabilistic speaker and listener agents reason about each other\u2019s intentions to enrich the literal semantics of their language along broadly Gricean lines. RSA has been shown to capture many kinds of conversational implicature, but it has been criticized as an unrealistic model of speakers, and it has so far required the manual specification of a semantic lexicon, preventing its use in natural language processing applications that learn lexical knowledge from data. We address these concerns by showing how to define and optimize a trained statistical classifier that uses the intermediate agents of RSA as hidden layers of representation forming a non-linear activation function. This treatment opens up new application domains and new possibilities for learning effectively from data. We validate the model on a referential expression generation task, showing that the best performance is achieved by incorporating features approximating well-established insights about natural language generation into RSA. 1 Pragmatic language use In the Gricean view of language use [18], people are rational agents who are able to communicate efficiently and effectively by reasoning in terms of shared communicative goals, the costs of production, prior expectations, and others\u2019 belief states. The Rational Speech Acts (RSA) model [11] is a recent Bayesian reconstruction of these core Gricean ideas. RSA and its extensions have been shown to capture many kinds of conversational implicature and to closely model psycholinguistic data from children and adults [7, 2, 23, 30, 33]. Both Grice\u2019s theories and RSA have been criticized for predicting that people are more rational than they actually are. These criticisms have been especially forceful in the context of language production. It seems that speakers often fall short: their utterances are longer than they need to be, underinformative, unintentionally ambiguous, obscure, and so forth [1, 10, 16, 24, 28, 29]. RSA can incorporate notions of bounded rationality [4, 13, 20], but it still sharply contrasts with views in the tradition of [6], in which speaker agents rely on heuristics and shortcuts to try to accurately describe the world while managing the cognitive demands of language production. In this paper, we offer a substantially different perspective on RSA by showing how to define it as a trained statistical classifier, which we call learned RSA. At the heart of learned RSA is the back-and-forth reasoning between speakers and listeners that characterizes RSA. However, whereas standard RSA requires a hand-built lexicon, learned RSA infers a lexicon from data. And whereas standard RSA makes predictions according to a fixed calculation, learned RSA seeks to optimize the likelihood of whatever examples it is trained on. Agents trained in this way exhibit the pragmatic behavior characteristic of RSA, but their behavior is governed by their training data and hence is only as rational as that experience supports. To the extent that the speakers who produced the data are pragmatic, learned RSA discovers that; to the extent that their behavior is governed by other factors, learned RSA picks up on that too. We validate the model on the task of attribute selection for referring expression generation with a widely-used corpus of referential descriptions (the TUNA corpus; [34, 15]), showing that it improves on heuristic-driven models and pure RSA by synthesizing the best aspects of both. 1 ar X iv :1 51 0. 06 80 7v 1 [ cs .C L ] 2 3 O ct 2 01 5 Learning in the Rational Speech Acts Model Monroe and Potts r1 r2 r3 (a) Simple reference game. be a rd gl a ss es ti e r1 .5 .5 0 r2 0 .5 .5 r3 0 0 1", "creator": "easychair.cls-3.4"}}}