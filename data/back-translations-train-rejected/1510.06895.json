{"id": "1510.06895", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "23-Oct-2015", "title": "Nonconvex Nonsmooth Low-Rank Minimization via Iteratively Reweighted Nuclear Norm", "abstract": "The nuclear norm is widely used as a convex surrogate of the rank function in compressive sensing for low rank matrix recovery with its applications in image recovery and signal processing. However, solving the nuclear norm based relaxed convex problem usually leads to a suboptimal solution of the original rank minimization problem. In this paper, we propose to perform a family of nonconvex surrogates of $L_0$-norm on the singular values of a matrix to approximate the rank function. This leads to a nonconvex nonsmooth minimization problem. Then we propose to solve the problem by Iteratively Reweighted Nuclear Norm (IRNN) algorithm. IRNN iteratively solves a Weighted Singular Value Thresholding (WSVT) problem, which has a closed form solution due to the special properties of the nonconvex surrogate functions. We also extend IRNN to solve the nonconvex problem with two or more blocks of variables. In theory, we prove that IRNN decreases the objective function value monotonically, and any limit point is a stationary point. Extensive experiments on both synthesized data and real images demonstrate that IRNN enhances the low-rank matrix recovery compared with state-of-the-art convex algorithms.", "histories": [["v1", "Fri, 23 Oct 2015 11:28:06 GMT  (15258kb,D)", "http://arxiv.org/abs/1510.06895v1", null]], "reviews": [], "SUBJECTS": "cs.LG cs.CV cs.NA", "authors": ["canyi lu", "jinhui tang", "shuicheng yan", "zhouchen lin"], "accepted": false, "id": "1510.06895"}, "pdf": {"name": "1510.06895.pdf", "metadata": {"source": "CRF", "title": "Nonconvex Nonsmooth Low-Rank Minimization via Iteratively Reweighted Nuclear Norm", "authors": ["Canyi Lu", "Jinhui Tang", "Shuicheng Yan", "Zhouchen Lin"], "emails": ["canyilu@gmail.com;", "eleyans@nus.edu.sg).", "jinhuitang@mail.njust.edu.cn).", "zlin@pku.edu.cn)."], "sections": [{"heading": null, "text": "This year, it is time to put yourself in a position to take the lead."}, {"heading": "II. NONCONVEX NONSMOOTH LOW-RANK MINIMIZATION", "text": "In this section we show how the general problem (3) can be solved. (3) A well-known example is the capped L1 standard, see Figure 1. (4). (4). (5). (7)..................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................."}, {"heading": "III. EXTENSIONS OF IRNN AND THE CONVERGENCE ANALYSIS", "text": "In this section, we expand IRNN to two types of problems that are more general than (3). The first is to solve some similar problems to (3), but with more general, non-convex penalties. The second is to solve the problem (5), the p + 2 blocks of variables.IRNN for problems with more generic nonconvex penaltiesIRNN can be extended to solve the following problem (X) + f (X), (23), where GIs are concavant and satisfy their supergradients (0)."}, {"heading": "IV. EXPERIMENTS", "text": "In this section we present several experiments to show that the models with a constant rate of 1,1 megapixels (we are a linear operator of the drainage of the drainage of the drainage of the drainage of the drainage of the drainage of the drainage of the drainage of the drainage of the drainage of the drainage of the drainage of the drainage of the drainage of the drainage of the drainage of the drainage of the drainage of the drainage of the drainage of the drainage of the drainage of the drainage of the drainage of the drainage of the drainage of the drainage of the drainage of the drainage of the drainage of the drainage of the drainage of the drainage of the drainage of the drainage of the drainage of the"}, {"heading": "V. CONCLUSIONS AND FUTURE WORK", "text": "This work, which aims at restoring the non-convex matrix by applying the non-convex surrogates of the L0 standard to the singular values to approximate the rank function, observed that all existing non-convex surrogates are concave and monotonous increasing to [0, \u221e), then we proposed a general solver IRNN to solve the non-convex minimization problem (3), we also extended IRNN to solve problem (5) with multiple variable blocks. In theory, we proved that each boundary point is a stationary point. Experiments with synthetic data and real data showed that IRNN usually outperforms the state-of-the-art convex algorithm. There are some interesting future works. First, it is still unclear which non-convex surrogate is the best."}, {"heading": "ACKNOWLEDGEMENTS", "text": "This research is supported by the Singapore National Research Foundation through its International Research Centre @ Singapore Funding Initiative and managed by the IDM Programme Office. Z. Lin is supported by the NSF of China (grant numbers 61272341, 61231002 and 61121002) and MSRA."}], "references": [{"title": "Generalized nonconvex nonsmooth low-rank minimization", "author": ["Canyi Lu", "Jinhui Tang", "Shuicheng Yan", "Zhouchen Lin"], "venue": "CVPR. IEEE, 2014, pp. 4130\u20134137.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2014}, {"title": "An introduction to compressive sampling", "author": ["Emmanuel J Cand\u00e8s", "Michael B Wakin"], "venue": "IEEE Signal Processing Magazine, vol. 25, no. 2, pp. 21\u201330, 2008.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2008}, {"title": "Robust face recognition via sparse representation", "author": ["J. Wright", "A.Y. Yang", "A. Ganesh", "S.S. Sastry", "Y. Ma"], "venue": "TPAMI, vol. 31, no. 2, pp. 210\u2013227, 2009.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2009}, {"title": "Locality-constrained linear coding for image classification", "author": ["Jinjun Wang", "Jianchao Yang", "Kai Yu", "Fengjun Lv", "Thomas Huang", "Yihong Gong"], "venue": "CVPR. IEEE, 2010, pp. 3360\u20133367.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2010}, {"title": "Image super-resolution via sparse representation", "author": ["Jianchao Yang", "John Wright", "Thomas S Huang", "Yi Ma"], "venue": "TIP, vol. 19, no. 11, pp. 2861\u20132873, 2010.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2010}, {"title": "Robust principal component analysis", "author": ["E.J. Cand\u00e8s", "X.D. Li", "Y. Ma", "J. Wright"], "venue": "Journal of the ACM, vol. 58, no. 3, 2011.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2011}, {"title": "Robust recovery of subspace structures by low-rank representation", "author": ["Guangcan Liu", "Zhouchen Lin", "Shuicheng Yan", "Ju Sun", "Yong Yu", "Yi Ma"], "venue": "TPAMI, 2013.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2013}, {"title": "Correlation adaptive subspace segmentation by trace lasso", "author": ["Canyi Lu", "Jiashi Feng", "Zhouchen Lin", "Shuicheng Yan"], "venue": "ICCV. IEEE, 2013, pp. 1345\u20131352.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2013}, {"title": "Exact matrix completion via convex optimization", "author": ["E.J. Cand\u00e8s", "B. Recht"], "venue": "Foundations of Computational mathematics, vol. 9, no. 6, pp. 717\u2013772, 2009.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2009}, {"title": "A fast iterative shrinkage-thresholding algorithm for linear inverse problems", "author": ["Amir Beck", "Marc Teboulle"], "venue": "SIAM Journal on Imaging Sciences, 2009.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2009}, {"title": "Fast solution of-norm minimization problems when the solution may be sparse", "author": ["David L Donoho", "Yaakov Tsaig"], "venue": "IEEE Transactions on Information Theory, vol. 54, no. 11, pp. 4789\u20134812, 2008.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2008}, {"title": "For most large underdetermined systems of linear equations the minimal `1-norm solution is also the sparsest solution", "author": ["David L Donoho"], "venue": "Communications on Pure and Applied Mathematics, vol. 59, no. 6, pp. 797\u2013829, 2006.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2006}, {"title": "A statistical view of some chemometrics regression tools", "author": ["LLdiko Frank", "Jerome Friedman"], "venue": "Technometrics, 1993.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 1993}, {"title": "Variable selection via nonconcave penalized likelihood and its oracle properties", "author": ["Jianqing Fan", "Runze Li"], "venue": "Journal of the American Statistical Association, 2001.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2001}, {"title": "Fast sparse regression and classification", "author": ["Jerome Friedman"], "venue": "International Journal of Forecasting, 2012.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2012}, {"title": "Nearly unbiased variable selection under minimax concave penalty", "author": ["Cunhui Zhang"], "venue": "The Annals of Statistics, 2010.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2010}, {"title": "Analysis of multi-stage convex relaxation for sparse regularization", "author": ["Tong Zhang"], "venue": "JMLR, 2010.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2010}, {"title": "A feasible nonconvex relaxation approach to feature selection", "author": ["Cuixia Gao", "Naiyan Wang", "Qi Yu", "Zhihua Zhang"], "venue": "AAAI, 2011.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2011}, {"title": "Nonlinear image recovery with half-quadratic regularization", "author": ["Donald Geman", "Chengda Yang"], "venue": "TIP, 1995.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 1995}, {"title": "Highly undersampled magnetic resonance image reconstruction via homotopic `0-minimization", "author": ["Joshua Trzasko", "Armando Manduca"], "venue": "TMI, 2009.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2009}, {"title": "Enhancing sparsity by reweighted `1 minimization", "author": ["E. Cand\u00e8s", "M.B. Wakin", "S.P. Boyd"], "venue": "Journal of Fourier Analysis and Applications, 2008.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2008}, {"title": "Improved iteratively reweighted least squares for unconstrained smoothed \\ell q minimization", "author": ["Ming-Jun Lai", "Yangyang Xu", "Wotao Yin"], "venue": "SIAM Journal on Numerical Analysis, vol. 51, no. 2, pp. 927\u2013957, 2013.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2013}, {"title": "Uncovering shared structures in multiclass classification", "author": ["Yonatan Amit", "Michael Fink", "Nathan Srebro", "Shimon Ullman"], "venue": "ICML, 2007.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2007}, {"title": "An accelerated proximal gradient algorithm for nuclear norm regularized linear least squares problems", "author": ["Kimchuan Toh", "Sangwoon Yun"], "venue": "Pacific Journal of Optimization, 2010.", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2010}, {"title": "Convex multi-task feature learning", "author": ["Andreas Argyriou", "Theodoros Evgeniou", "Massimiliano Pontil"], "venue": "Machine Learning, 2008.", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2008}, {"title": "An accelerated proximal gradient algorithm for nuclear norm regularized linear least squares problems", "author": ["K. Toh", "S. Yun"], "venue": "Pacific Journal of Optimization, 2010.", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2010}, {"title": "Distributed optimization and statistical learning via the alternating direction method of multipliers", "author": ["Stephen Boyd", "Neal Parikh", "Eric Chu", "Borja Peleato", "Jonathan Eckstein"], "venue": "Foundations and Trends\u00ae in Machine Learning, vol. 3, no. 1, pp. 1\u2013122, 2011.", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2011}, {"title": "Iterative reweighted algorithms for matrix rank minimization", "author": ["K. Mohan", "M. Fazel"], "venue": "JMLR, 2012.", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2012}, {"title": "Low-rank matrix recovery via iteratively reweighted least squares minimization", "author": ["Massimo Fornasier", "Holger Rauhut", "Rachel Ward"], "venue": "SIAM Journal on Optimization, vol. 21, no. 4, pp. 1614\u20131640, 2011.", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2011}, {"title": "An unconstrained `q minimization with 0 < q \u2264 1 for sparse solution of underdetermined linear systems", "author": ["Ming-Jun Lai", "Jingyue Wang"], "venue": "SIAM Journal on Optimization, vol. 21, no. 1, pp. 82\u2013101, 2011.", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2011}, {"title": "Fast and accurate matrix completion via truncated nuclear norm regularization", "author": ["Yao Hu", "Debing Zhang", "Jieping Ye", "Xuelong Li", "Xiaofei He"], "venue": "TPAMI, 2013.", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2013}, {"title": "Probabilistic low-rank matrix completion with adaptive spectral regularization algorithms", "author": ["Adrien Todeschini", "Fran\u00e7ois Caron", "Marie Chavent"], "venue": "NIPS, 2013, pp. 845\u2013853.", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2013}, {"title": "Compressive sensing via nonlocal low-rank regularization", "author": ["Weisheng Dong", "Guangming Shi", "Xin Li", "Yi Ma", "Feng Huang"], "venue": "TIP, vol. 23, no. 8, pp. 3618\u20133632, 2014.", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2014}, {"title": "Log-det heuristic for matrix rank minimization with applications to hankel and euclidean distance matrices", "author": ["Maryam Fazel", "Haitham Hindi", "Stephen P Boyd"], "venue": "American Control Conference. IEEE, 2003, vol. 3, pp. 2156\u20132162.", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2003}, {"title": "Generalized singular value thresholding", "author": ["Canyi Lu", "Changbo Zhu", "Chunyan Xu", "Shuicheng Yan", "Zhouchen Lin"], "venue": "AAAI, 2015.", "citeRegEx": "35", "shortCiteRegEx": null, "year": 2015}, {"title": "Latent low-rank representation for subspace segmentation and feature extraction", "author": ["Guangcan Liu", "Shuicheng Yan"], "venue": "ICCV. IEEE, 2011, pp. 1615\u20131622.", "citeRegEx": "36", "shortCiteRegEx": null, "year": 2011}, {"title": "Introductory lectures on convex optimization: A basic course, vol", "author": ["Yurii Nesterov"], "venue": null, "citeRegEx": "37", "shortCiteRegEx": "37", "year": 2004}, {"title": "The supergradient of a concave function", "author": ["KC Border"], "venue": "http://www.hss. caltech.edu/\u223ckcb/Notes/Supergrad.pdf, 2001, [Online].", "citeRegEx": "38", "shortCiteRegEx": null, "year": 2001}, {"title": "Reduced rank regression via adaptive nuclear norm penalization", "author": ["Kun Chen", "Hongbo Dong", "Kungsik Chan"], "venue": "Biometrika, 2013.", "citeRegEx": "39", "shortCiteRegEx": null, "year": 2013}, {"title": "A singular value thresholding algorithm for matrix completion", "author": ["Jianfeng Cai", "Emmanuel Cand\u00e8s", "Zuowei Shen"], "venue": "SIAM Journal on Optimization, 2010.", "citeRegEx": "40", "shortCiteRegEx": null, "year": 2010}, {"title": "Tensor decompositions and applications", "author": ["Tamara G Kolda", "Brett W Bader"], "venue": "SIAM Review, vol. 51, no. 3, pp. 455\u2013500, 2009.", "citeRegEx": "41", "shortCiteRegEx": null, "year": 2009}, {"title": "Nonsmooth analysis and optimization", "author": ["Frank Clarke"], "venue": "Proceedings of the International Congress of Mathematicians, 1983.", "citeRegEx": "42", "shortCiteRegEx": null, "year": 1983}, {"title": "Matrix completion with noise", "author": ["E.J. Cand\u00e8s", "Y. Plan"], "venue": "Proceedings of the IEEE, vol. 98, no. 6, pp. 925\u2013936, 2010.", "citeRegEx": "43", "shortCiteRegEx": null, "year": 2010}, {"title": "The augmented lagrange multiplier method for exact recovery of a corrupted low-rank matrices", "author": ["Z. Lin", "M. Chen", "L. Wu", "Y. Ma"], "venue": "UIUC Technical Report UILU-ENG-09-2215, Tech. Rep., 2009.", "citeRegEx": "44", "shortCiteRegEx": null, "year": 2009}, {"title": "Solving a low-rank factorization model for matrix completion by a nonlinear successive overrelaxation algorithm", "author": ["Zaiwen Wen", "Wotao Yin", "Yin Zhang"], "venue": "Mathematical Programming Computation, 2012.", "citeRegEx": "45", "shortCiteRegEx": null, "year": 2012}, {"title": "Robust subspace segmentation by low-rank representation", "author": ["G. Liu", "Z. Lin", "Y. Yu"], "venue": "ICML, 2010.", "citeRegEx": "46", "shortCiteRegEx": null, "year": 2010}, {"title": "Normalized cuts and image segmentation", "author": ["J.B. Shi", "J. Malik"], "venue": "TPAMI, vol. 22, no. 8, pp. 888\u2013905, 2000.  11", "citeRegEx": "47", "shortCiteRegEx": null, "year": 2000}], "referenceMentions": [{"referenceID": 1, "context": "INTRODUCTION BENEFITING from the success of Compressive Sensing (CS) [2], the sparse and low rank matrix structures have attracted considerable research interests from the computer vision and machine learning communities.", "startOffset": 69, "endOffset": 72}, {"referenceID": 2, "context": "For instance, sparse coding has been widely used for face recognition [3], image classification [4] and super-resolution [5], while low rank models are applied for background modeling [6], motion segmentation [7], [8] and collaborative filtering [9].", "startOffset": 70, "endOffset": 73}, {"referenceID": 3, "context": "For instance, sparse coding has been widely used for face recognition [3], image classification [4] and super-resolution [5], while low rank models are applied for background modeling [6], motion segmentation [7], [8] and collaborative filtering [9].", "startOffset": 96, "endOffset": 99}, {"referenceID": 4, "context": "For instance, sparse coding has been widely used for face recognition [3], image classification [4] and super-resolution [5], while low rank models are applied for background modeling [6], motion segmentation [7], [8] and collaborative filtering [9].", "startOffset": 121, "endOffset": 124}, {"referenceID": 5, "context": "For instance, sparse coding has been widely used for face recognition [3], image classification [4] and super-resolution [5], while low rank models are applied for background modeling [6], motion segmentation [7], [8] and collaborative filtering [9].", "startOffset": 184, "endOffset": 187}, {"referenceID": 6, "context": "For instance, sparse coding has been widely used for face recognition [3], image classification [4] and super-resolution [5], while low rank models are applied for background modeling [6], motion segmentation [7], [8] and collaborative filtering [9].", "startOffset": 209, "endOffset": 212}, {"referenceID": 7, "context": "For instance, sparse coding has been widely used for face recognition [3], image classification [4] and super-resolution [5], while low rank models are applied for background modeling [6], motion segmentation [7], [8] and collaborative filtering [9].", "startOffset": 214, "endOffset": 217}, {"referenceID": 8, "context": "For instance, sparse coding has been widely used for face recognition [3], image classification [4] and super-resolution [5], while low rank models are applied for background modeling [6], motion segmentation [7], [8] and collaborative filtering [9].", "startOffset": 246, "endOffset": 249}, {"referenceID": 9, "context": ", \u2016x \u20160 = #{xi 6= 0}, and the resulting convex problem can be solved by fast first-order solvers [10], [11].", "startOffset": 97, "endOffset": 101}, {"referenceID": 10, "context": ", \u2016x \u20160 = #{xi 6= 0}, and the resulting convex problem can be solved by fast first-order solvers [10], [11].", "startOffset": 103, "endOffset": 107}, {"referenceID": 11, "context": "Though for certain problems, the L1-minimization is equivalent to the L0-minimization under certain incoherence conditions [12], the obtained solution by L1-minimization is usually suboptimal to the original L0minimization since the L1-norm is a loose approximation of the L0-norm.", "startOffset": 123, "endOffset": 127}, {"referenceID": 0, "context": "This paper is an extended version of [1] published in CVPR 2014.", "startOffset": 37, "endOffset": 40}, {"referenceID": 12, "context": "Lp [13] \u03bb\u03b8 p { +\u221e, if \u03b8 = 0, \u03bbp\u03b8p\u22121, if \u03b8 > 0.", "startOffset": 3, "endOffset": 7}, {"referenceID": 13, "context": "SCAD [14] \uf8f1\uf8f4\uf8f2\uf8f4\uf8f4\uf8f3 \u03bb\u03b8, if \u03b8 \u2264 \u03bb, \u2212\u03b82+2\u03b3\u03bb\u03b8\u2212\u03bb2 2(\u03b3\u22121) , if \u03bb < \u03b8 \u2264 \u03b3\u03bb, \u03bb2(\u03b3+1) 2 , if \u03b8 > \u03b3\u03bb.", "startOffset": 5, "endOffset": 9}, {"referenceID": 14, "context": "Logarithm [15] \u03bb log(\u03b3+1) log(\u03b3\u03b8 + 1) \u03b3\u03bb (\u03b3\u03b8+1) log(\u03b3+1)", "startOffset": 10, "endOffset": 14}, {"referenceID": 15, "context": "MCP [16] \uf8f2\uf8f3\u03bb\u03b8 \u2212 \u03b8 2 2\u03b3 , if \u03b8 < \u03b3\u03bb,", "startOffset": 4, "endOffset": 8}, {"referenceID": 16, "context": "Capped L1 [17] { \u03bb\u03b8, if \u03b8 < \u03b3, \u03bb\u03b3, if \u03b8 \u2265 \u03b3.", "startOffset": 10, "endOffset": 14}, {"referenceID": 17, "context": "ETP [18] \u03bb 1\u2212exp(\u2212\u03b3) (1 \u2212 exp(\u2212\u03b3\u03b8)) \u03bb\u03b3 1\u2212exp(\u2212\u03b3) exp(\u2212\u03b3\u03b8) Geman [19] \u03bb\u03b8 \u03b8+\u03b3 \u03bb\u03b3 (\u03b8+\u03b3)2 Laplace [20] \u03bb(1 \u2212 exp(\u2212 \u03b8 \u03b3 )) \u03bb \u03b3 exp(\u2212 \u03b8 \u03b3 )", "startOffset": 4, "endOffset": 8}, {"referenceID": 18, "context": "ETP [18] \u03bb 1\u2212exp(\u2212\u03b3) (1 \u2212 exp(\u2212\u03b3\u03b8)) \u03bb\u03b3 1\u2212exp(\u2212\u03b3) exp(\u2212\u03b3\u03b8) Geman [19] \u03bb\u03b8 \u03b8+\u03b3 \u03bb\u03b3 (\u03b8+\u03b3)2 Laplace [20] \u03bb(1 \u2212 exp(\u2212 \u03b8 \u03b3 )) \u03bb \u03b3 exp(\u2212 \u03b8 \u03b3 )", "startOffset": 64, "endOffset": 68}, {"referenceID": 19, "context": "ETP [18] \u03bb 1\u2212exp(\u2212\u03b3) (1 \u2212 exp(\u2212\u03b3\u03b8)) \u03bb\u03b3 1\u2212exp(\u2212\u03b3) exp(\u2212\u03b3\u03b8) Geman [19] \u03bb\u03b8 \u03b8+\u03b3 \u03bb\u03b3 (\u03b8+\u03b3)2 Laplace [20] \u03bb(1 \u2212 exp(\u2212 \u03b8 \u03b3 )) \u03bb \u03b3 exp(\u2212 \u03b8 \u03b3 )", "startOffset": 94, "endOffset": 98}, {"referenceID": 12, "context": "nonconvex surrogates of L0-norm have been proposed, including Lp-norm (0 < p < 1) [13], Smoothly Clipped Absolute Deviation (SCAD) [14], Logarithm [15], Minimax Concave Penalty (MCP) [16], Capped L1 [17], ExponentialType Penalty (ETP) [18], Geman [19] and Laplace [20].", "startOffset": 82, "endOffset": 86}, {"referenceID": 13, "context": "nonconvex surrogates of L0-norm have been proposed, including Lp-norm (0 < p < 1) [13], Smoothly Clipped Absolute Deviation (SCAD) [14], Logarithm [15], Minimax Concave Penalty (MCP) [16], Capped L1 [17], ExponentialType Penalty (ETP) [18], Geman [19] and Laplace [20].", "startOffset": 131, "endOffset": 135}, {"referenceID": 14, "context": "nonconvex surrogates of L0-norm have been proposed, including Lp-norm (0 < p < 1) [13], Smoothly Clipped Absolute Deviation (SCAD) [14], Logarithm [15], Minimax Concave Penalty (MCP) [16], Capped L1 [17], ExponentialType Penalty (ETP) [18], Geman [19] and Laplace [20].", "startOffset": 147, "endOffset": 151}, {"referenceID": 15, "context": "nonconvex surrogates of L0-norm have been proposed, including Lp-norm (0 < p < 1) [13], Smoothly Clipped Absolute Deviation (SCAD) [14], Logarithm [15], Minimax Concave Penalty (MCP) [16], Capped L1 [17], ExponentialType Penalty (ETP) [18], Geman [19] and Laplace [20].", "startOffset": 183, "endOffset": 187}, {"referenceID": 16, "context": "nonconvex surrogates of L0-norm have been proposed, including Lp-norm (0 < p < 1) [13], Smoothly Clipped Absolute Deviation (SCAD) [14], Logarithm [15], Minimax Concave Penalty (MCP) [16], Capped L1 [17], ExponentialType Penalty (ETP) [18], Geman [19] and Laplace [20].", "startOffset": 199, "endOffset": 203}, {"referenceID": 17, "context": "nonconvex surrogates of L0-norm have been proposed, including Lp-norm (0 < p < 1) [13], Smoothly Clipped Absolute Deviation (SCAD) [14], Logarithm [15], Minimax Concave Penalty (MCP) [16], Capped L1 [17], ExponentialType Penalty (ETP) [18], Geman [19] and Laplace [20].", "startOffset": 235, "endOffset": 239}, {"referenceID": 18, "context": "nonconvex surrogates of L0-norm have been proposed, including Lp-norm (0 < p < 1) [13], Smoothly Clipped Absolute Deviation (SCAD) [14], Logarithm [15], Minimax Concave Penalty (MCP) [16], Capped L1 [17], ExponentialType Penalty (ETP) [18], Geman [19] and Laplace [20].", "startOffset": 247, "endOffset": 251}, {"referenceID": 19, "context": "nonconvex surrogates of L0-norm have been proposed, including Lp-norm (0 < p < 1) [13], Smoothly Clipped Absolute Deviation (SCAD) [14], Logarithm [15], Minimax Concave Penalty (MCP) [16], Capped L1 [17], ExponentialType Penalty (ETP) [18], Geman [19] and Laplace [20].", "startOffset": 264, "endOffset": 268}, {"referenceID": 20, "context": "Numerical studies [21], [22] have shown that the nonconvex sparse optimization usually outperforms convex models in the areas of signal recovery, error correction and image processing.", "startOffset": 18, "endOffset": 22}, {"referenceID": 21, "context": "Numerical studies [21], [22] have shown that the nonconvex sparse optimization usually outperforms convex models in the areas of signal recovery, error correction and image processing.", "startOffset": 24, "endOffset": 28}, {"referenceID": 22, "context": "The above low rank minimization problem arises in many computer vision tasks such as multiple category classification [23], matrix completion [24], multi-task learning [25] and low-rank representation with squared loss for subspace segmentation [7].", "startOffset": 118, "endOffset": 122}, {"referenceID": 23, "context": "The above low rank minimization problem arises in many computer vision tasks such as multiple category classification [23], matrix completion [24], multi-task learning [25] and low-rank representation with squared loss for subspace segmentation [7].", "startOffset": 142, "endOffset": 146}, {"referenceID": 24, "context": "The above low rank minimization problem arises in many computer vision tasks such as multiple category classification [23], matrix completion [24], multi-task learning [25] and low-rank representation with squared loss for subspace segmentation [7].", "startOffset": 168, "endOffset": 172}, {"referenceID": 6, "context": "The above low rank minimization problem arises in many computer vision tasks such as multiple category classification [23], matrix completion [24], multi-task learning [25] and low-rank representation with squared loss for subspace segmentation [7].", "startOffset": 245, "endOffset": 248}, {"referenceID": 25, "context": "The above convex problem can be efficiently solved by many known solvers [26], [27].", "startOffset": 73, "endOffset": 77}, {"referenceID": 26, "context": "The above convex problem can be efficiently solved by many known solvers [26], [27].", "startOffset": 79, "endOffset": 83}, {"referenceID": 12, "context": "(a) Lp Penalty [13] 0 2 4 6 0 0.", "startOffset": 15, "endOffset": 19}, {"referenceID": 13, "context": "(b) SCAD Penalty [14] 0 2 4 6 0 1 2 3", "startOffset": 17, "endOffset": 21}, {"referenceID": 14, "context": "(c) Logarithm Penalty [15] 0 2 4 6 0 0.", "startOffset": 22, "endOffset": 26}, {"referenceID": 15, "context": "(d) MCP Penalty [16]", "startOffset": 16, "endOffset": 20}, {"referenceID": 16, "context": "(e) Capped L1 Penalty [17] 0 2 4 6 0 0.", "startOffset": 22, "endOffset": 26}, {"referenceID": 17, "context": "(f) ETP Penalty [18] 0 2 4 6 0 0.", "startOffset": 16, "endOffset": 20}, {"referenceID": 18, "context": "(g) Geman Penalty [19] 0 2 4 6 0 0.", "startOffset": 18, "endOffset": 22}, {"referenceID": 19, "context": "(h) Laplace Penalty [20]", "startOffset": 20, "endOffset": 24}, {"referenceID": 0, "context": "In this paper, to achieve a better approximation of the rank function, we extend the nonconvex surrogates of L0-norm shown in Table I onto the singular values of the matrix, and show how to solve the following general nonconvex nonsmooth low rank minimization problem [1]", "startOffset": 268, "endOffset": 271}, {"referenceID": 27, "context": "The work [28], [29] extend the Lp-norm of a vector to the Schattenp norm (0 < p < 1) and use the iteratively reweighted least squares (IRLS) algorithm to solve the nonconvex rank minimization problem with affine constraint.", "startOffset": 9, "endOffset": 13}, {"referenceID": 28, "context": "The work [28], [29] extend the Lp-norm of a vector to the Schattenp norm (0 < p < 1) and use the iteratively reweighted least squares (IRLS) algorithm to solve the nonconvex rank minimization problem with affine constraint.", "startOffset": 15, "endOffset": 19}, {"referenceID": 29, "context": "IRLS is also applied for the unconstrained problem with the smoothed Schatten-p norm regularizer [30].", "startOffset": 97, "endOffset": 101}, {"referenceID": 30, "context": "Another nonconvex rank surrogate is the truncated nuclear norm [31].", "startOffset": 63, "endOffset": 67}, {"referenceID": 31, "context": "The nonconvex low rank matrix completion problem considered in [32] is a special case of our problem (3).", "startOffset": 63, "endOffset": 67}, {"referenceID": 32, "context": "The work [33] uses the nonconvex log-det heuristic in [34] for image recovery.", "startOffset": 9, "endOffset": 13}, {"referenceID": 33, "context": "The work [33] uses the nonconvex log-det heuristic in [34] for image recovery.", "startOffset": 54, "endOffset": 58}, {"referenceID": 34, "context": "A possible method to solve (3) is the proximal gradient algorithm [35], which requires to compute the proximal mapping of the nonconvex function g.", "startOffset": 66, "endOffset": 70}, {"referenceID": 34, "context": ", the convexity of \u2207g [35]), there does not exist a general solver for computing the proximal mapping of the general nonconvex g in assumption A1.", "startOffset": 22, "endOffset": 26}, {"referenceID": 35, "context": ", [36].", "startOffset": 2, "endOffset": 6}, {"referenceID": 36, "context": "3 in [37]).", "startOffset": 5, "endOffset": 9}, {"referenceID": 37, "context": "Inspired by (8), we can define the supergradient of concave g at the nonsmooth point x [38].", "startOffset": 87, "endOffset": 91}, {"referenceID": 20, "context": "The updating rule (16) can be regarded as an extension of the Iteratively Reweighted L1 (IRL1) algorithm [21] for the weighted L1-norm problem", "startOffset": 105, "endOffset": 109}, {"referenceID": 38, "context": "However, the weighted nuclear norm in (16) is nonconvex (it is convex if and only if w 1 \u2265 w 2 \u2265 \u00b7 \u00b7 \u00b7 \u2265 w m \u2265 0 [39]), while the weighted L1-norm in (17) is convex.", "startOffset": 113, "endOffset": 117}, {"referenceID": 39, "context": "Then WSVT reduces to the conventional Singular Value Thresholding (SVT) [40], which is an important subroutine in convex low rank optimization.", "startOffset": 72, "endOffset": 76}, {"referenceID": 9, "context": "The updating rule (20) then reduces to the known proximal gradient method [10].", "startOffset": 74, "endOffset": 78}, {"referenceID": 9, "context": "If the Lipschitz constant L(f) is not known or computable, the backtracking rule can be used to estimate \u03bc in each iteration [10].", "startOffset": 125, "endOffset": 129}, {"referenceID": 20, "context": ", [21], [30].", "startOffset": 2, "endOffset": 6}, {"referenceID": 29, "context": ", [21], [30].", "startOffset": 8, "endOffset": 12}, {"referenceID": 21, "context": ", [22], [31], [33], which target for some special nonconvex problems.", "startOffset": 2, "endOffset": 6}, {"referenceID": 30, "context": ", [22], [31], [33], which target for some special nonconvex problems.", "startOffset": 8, "endOffset": 12}, {"referenceID": 32, "context": ", [22], [31], [33], which target for some special nonconvex problems.", "startOffset": 14, "endOffset": 18}, {"referenceID": 30, "context": "The truncated nuclear norm ||X ||r = \u2211m i=r+1 \u03c3i(X) [31] is an interesting example.", "startOffset": 52, "endOffset": 56}, {"referenceID": 30, "context": "Compared with the alternating updating algorithm in [31], which require double loops, our IRNN will be more efficient and with stronger convergence guarantee.", "startOffset": 52, "endOffset": 56}, {"referenceID": 35, "context": "An example is the Latent Low Rank Representation (LatLRR) problem [36]", "startOffset": 66, "endOffset": 70}, {"referenceID": 40, "context": "where X \u2208 Rm1\u00d7\u00b7\u00b7\u00b7\u00d7mp is an p-way tensor and X \u00d7jPj denotes the j-mode product [41].", "startOffset": 78, "endOffset": 82}, {"referenceID": 6, "context": "TLRR is an extension of LRR [7] and LatLRR.", "startOffset": 28, "endOffset": 31}, {"referenceID": 42, "context": "The first two aim to examine the convergence behavior of IRNN for the matrix completion problem [43] on both synthetic data and real images.", "startOffset": 96, "endOffset": 100}, {"referenceID": 8, "context": "Low Rank Matrix Recovery on the Synthetic Data We first compare the low rank matrix recovery performances of nonconvex model (32) with the convex one by using nuclear norm [9] on the synthetic data.", "startOffset": 172, "endOffset": 175}, {"referenceID": 43, "context": "The Augmented Lagrange Multiplier (ALM) [44] method is used to solve the noise free problem", "startOffset": 40, "endOffset": 44}, {"referenceID": 23, "context": "We compare IRNN for (32) with convex Accelerated Proximal Gradient with Line search (APGL)3 [24] which solves the noisy problem", "startOffset": 92, "endOffset": 96}, {"referenceID": 30, "context": "We follow the experimental settings in [31].", "startOffset": 39, "endOffset": 43}, {"referenceID": 44, "context": "We compare IRNN with some stateof-the-art methods on this task, including APGL, Low-Rank Matrix Fitting (LMaFit)4 [45] and Truncated Nuclear Norm Regularization (TNNR)5 [31].", "startOffset": 114, "endOffset": 118}, {"referenceID": 30, "context": "We compare IRNN with some stateof-the-art methods on this task, including APGL, Low-Rank Matrix Fitting (LMaFit)4 [45] and Truncated Nuclear Norm Regularization (TNNR)5 [31].", "startOffset": 169, "endOffset": 173}, {"referenceID": 45, "context": "Tensor Low-Rank Representation In this section, we consider to use the Tensor Low-Rank Representation (TLRR) (27) for face clustering [46], [36].", "startOffset": 134, "endOffset": 138}, {"referenceID": 35, "context": "Tensor Low-Rank Representation In this section, we consider to use the Tensor Low-Rank Representation (TLRR) (27) for face clustering [46], [36].", "startOffset": 140, "endOffset": 144}, {"referenceID": 9, "context": "Problem (27) can be solved by the Accelerated Proximal Gradient (APG) [10] method with the optimal convergence rate O(1/K), where K is the number of iterations.", "startOffset": 70, "endOffset": 74}, {"referenceID": 45, "context": "After solving (27) or (36), we follow the settings in [46] to construct the affinity matrix by W = (|P3 |+|P3 |)/2.", "startOffset": 54, "endOffset": 58}, {"referenceID": 46, "context": "Finally, the Normalized Cuts (NCuts) [47] is applied based on W to segment the data into k groups.", "startOffset": 37, "endOffset": 41}, {"referenceID": 45, "context": "The performances of LRR and LatLRR are consistent with previous work [46], [36].", "startOffset": 69, "endOffset": 73}, {"referenceID": 35, "context": "The performances of LRR and LatLRR are consistent with previous work [46], [36].", "startOffset": 75, "endOffset": 79}], "year": 2015, "abstractText": "The nuclear norm is widely used as a convex surrogate of the rank function in compressive sensing for low rank matrix recovery with its applications in image recovery and signal processing. However, solving the nuclear norm based relaxed convex problem usually leads to a suboptimal solution of the original rank minimization problem. In this paper, we propose to perform a family of nonconvex surrogates of L0-norm on the singular values of a matrix to approximate the rank function. This leads to a nonconvex nonsmooth minimization problem. Then we propose to solve the problem by Iteratively Reweighted Nuclear Norm (IRNN) algorithm. IRNN iteratively solves a Weighted Singular Value Thresholding (WSVT) problem, which has a closed form solution due to the special properties of the nonconvex surrogate functions. We also extend IRNN to solve the nonconvex problem with two or more blocks of variables. In theory, we prove that IRNN decreases the objective function value monotonically, and any limit point is a stationary point. Extensive experiments on both synthesized data and real images demonstrate that IRNN enhances the low-rank matrix recovery compared with state-of-the-art convex algorithms.", "creator": "LaTeX with hyperref package"}}}