{"id": "1510.07099", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "24-Oct-2015", "title": "Combine CRF and MMSEG to Boost Chinese Word Segmentation in Social Media", "abstract": "In this paper, we propose a joint algorithm for the word segmentation on Chinese social media. Previous work mainly focus on word segmentation for plain Chinese text, in order to develop a Chinese social media processing tool, we need to take the main features of social media into account, whose grammatical structure is not rigorous, and the tendency of using colloquial and Internet terms makes the existing Chinese-processing tools inefficient to obtain good performance on social media.", "histories": [["v1", "Sat, 24 Oct 2015 02:24:54 GMT  (12kb)", "http://arxiv.org/abs/1510.07099v1", "5 pages, 5 tables"]], "COMMENTS": "5 pages, 5 tables", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["yao yushi", "huang zheng"], "accepted": false, "id": "1510.07099"}, "pdf": {"name": "1510.07099.pdf", "metadata": {"source": "CRF", "title": null, "authors": [], "emails": ["yys12345@sjtu.edu.cn", "huangzhengsjtu@126.com"], "sections": [{"heading": null, "text": "ar Xiv: 151 0.07 099v 1 [cs.C L] 24 Oct 201 5"}, {"heading": "1 Introduction", "text": "To get essential information, the first step is word processing. In general, we use natural language processing methods to analyze social media and extract information for other social media research. (Xiong el al., 2013) An English sentence uses spaces as a gap between words, but Chinese has no such word boundaries, so the Chinese natural language process includes a further step than English: word segmentation - defining the word boundaries. Word segmentation is the core step of all Chinese language processing, it is directly related to the accuracy of the entire Chinese natural language process. In addition, the difficulty of Chinese word segmentation lies in removing ambiguities and identifying OOV words. Ambiguity means that a sentence can lead to many possible word segmentation results, and each type of segmentation has different semantic meanings, while the meaning of OOV words refers to the word that is most commonly not included in the dictionary, i.e. the name and person."}, {"heading": "2 Related Work", "text": "Currently, research into the process of natural language for social media is still in the early stages, competing for the SIGHAN method of Chinese word segmentation, approaches based on sequence annotation have been widely used. Microsoft used conditional random fields and the size of the attribute window corresponds to one. (Huang and Zhao, 2007) It turns out that the characteristics were simplified, but the performance was still very good. DaLian Science and Technology University built two models, one based on the use of the word CRF model, the other based on the MMSM model. During our experiments we also used CRF algorithm, and it plays the key role in building the model. Our work mainly depends on improving conditional random field. Conditional Random Field (CRF) is a statistical sequence modeling framework that was first demonstrated by Lafferty J et al (2001) to have affty function in terms of speech processing (Lhere)."}, {"heading": "3 Methods", "text": "There are already a lot of sophisticated algorithms for word segmentation, such as: statistical methods (Hidden Markov Model HMM, CRF, etc.), lexicon-based algorithms (MMSEG), and rule-based algorithms. CRF performs better than lexicon-based models at OOV rates because CRF introduces additional features (Collins, 2002) that can be artificially added (Xiong et al., 2009), including character-level features and context-level features. In addition, CRF also retains the Markov features of the word (Wallach,), so we can eliminate word ambiguities by combining more features. Generally, there are several simple algorithms that are used in word segmentation. When using lexicon-based algorithms such as MSEG, we simply adapt words according to our lexicon. While for statistical algorithms such as CRF-based segmentation, we can combine the multi-segmentation of a CRF with a large number of words."}, {"heading": "4 Experiments", "text": "The files involved in the training process include the entire feature template file, training sets and lexicon files. To get the best results, we have taken a similar approach as the coordinates rise. We will divide the experiment into three phases, with each stage selecting a training material to optimize and repair the other two materials. (Razvan and Bunescu, 2008) At each stage, when a training material achieves the best performance on the test set, it is selected to be the last training material we would use. (Wallach, 2002)"}, {"heading": "4.1 Data and Tool", "text": "The training set for word segmentation comes from Backoff 2005. (Tseng et al., 2005) We must mention that the lexicon used by MMSEG is the Sougou lab Internet lexicon published in 2006, which contains a number of radio frequency words under the Internet environment and mmseg4j project lexicon. There are few test sets on social media, so we searched raw content from Sina weibo and then commented on it manually, the content comes from all possible areas and users. The size of the test set is about 190K and contains about 25,000 Chinese words. It is worth noting that our segmentation complies with the standard of MSR. During our experiment, we use PCRF to train the segmentation model and test the model on our test sets, PCRF is an open source implementation of (linear) conditional random fields (CRFs) for segmentation / labeling of follow-up data and the rebound format + the CRF is compatible with the functionality files and CRFs."}, {"heading": "4.2 Evaluation Metric", "text": "We evaluate system performance for each task. Three metrics are used to evaluate word segmentation: precision (P), callback (R), and F-score (F), defined by 2PR / (P + R), where precision is the percentage of correct words in the system output, and callback is the percentage of gold standard annotations that are correctly predicted."}, {"heading": "4.3 Feature Template", "text": "Since CRF is designed to calculate the conditional probability of sequence annotation, which is deduced by the observed sequence, and this probability can be used to describe a number of random variables related to the distribution of feature vectors. In each template, special% x [row, col] macro is used to specify a token in the input data, line indicates the relative position of the current focusing token, and col specifies the absolute position of the column. The first step of the experiment is to determine the feature vectors of our model, (Finkel et al., 2008) The original features are too simple, so we modify the feature template step by step and test the accuracy on our test set."}, {"heading": "4.4 Training Set", "text": "The next step is to change the training set and find out the impact of the training set on our model. We choose the training sets of MSR and PKU because the text of these two training sets comes mainly from the newspaper, which is similar to the content of some social media, because a lot of social media is informative, these social media behave like news, the only difference is that they come from the internet and general news mainly from news agencies. Table3 shows the results of word segmentation experiments with different training sets. From the results of the experiments we can see that the model trained by combining these two training sets did not exceed the model of the individual training set and the F score reversed, it is actually confusing, and the main reason is the different segmentation standard of the two training sets, which leads to the fact that deception during the training process, which will be discussed later."}, {"heading": "4.5 Lexicon", "text": "The final stage of our experiment is to select a suitable lexicon. We knew that the more words the lexicon contains, the better we can distinguish words that MMSEG has, as our goal is to achieve high performance on social media. We should add the Internet lexicon to our original lexicon, and we conducted another experiment in which we added a lexicon of a particular field for comparison purposes only. Table 4 shows the results of word segmentation experiments with different lexicon. The combination of the Internet lexicon has improved the overall performance of word segmentation by 0.1%, but if we only add the lexicon of a particular field, such as the medical science lexicon, it will have only a small positive effect on the test sentence, mainly due to the extent of overall coverage of terminology in certain areas in our test sentence."}, {"heading": "4.6 Result Comparison", "text": "For reference, we took a comparison between our best test result and other segmentation tools. We chose the model trained by backoff2005 MSR training set and Sougou Lab's Internet lexicon for comparison. We add a closed test result column for each tool to the table, LTP Cloud ranked second in the Chinese social media segmentation evaluation task in CLP 2012, suggesting that the results of its closed test have great authority for social media NLP tasks. These data represent only performance on a small test set. Our model may have a larger gap between these complex tools on larger test sets, but the results still guarantee our model's good adaptability to Chinese social media platforms."}, {"heading": "4.7 Error Analysis", "text": "The most surprising result of our experiments is the combination of MSR and PKU training sets. We compared the training sets of MSR and PKU and finally get the effect of different segmentation standards on the model. We find that there are many fundamental differences between the standards. For example, 613 is a word in the MSR training set, but is divided into 6 and 13 as two words in pku's. Also, a person's name is quite a big issue, MSR tends to treat the full name as a word, while PKU takes the first name as a word and the last name as another word. Overall, MSR generates more words than PKU."}, {"heading": "5 Conclusion", "text": "One of the most applicable algorithms is the conditional random field, which is widely used in word segmentation, language marking, and detection of named entities, and other aspects. CRF has a big advantage in labeling, and the MMSEG algorithm has a comparative advantage over other algorithms in vocabulary words, so we use MMSEG to segment first, and use the result of MMSEG segmentation as a new feature of the CRF. In the experiments, we continue to adjust the template file and use more correlation between Chinese characters after improving accuracy and callback by about 1.2% over the individual CRF model several times."}], "references": [{"title": "Chinese Segmentation and New Word Detection using Conditional Random Fields", "author": ["F McCallum et al.2010] Peng", "F Feng", "A. McCallum"], "venue": "Proceedings of the 20th international conference on Computational Linguistics:562", "citeRegEx": "Peng et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Peng et al\\.", "year": 2010}, {"title": "An Improved Chinese word Segmentation System with Conditional Random Field", "author": ["H Zhao et al.2006] Zhao", "N Huang C", "M. Li"], "venue": "Proceedings of the Fifth SIGHAN Workshop on Chinese Language Processing,", "citeRegEx": "Zhao et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Zhao et al\\.", "year": 2006}, {"title": "A Conditional Random Field Word Segmenter for Sighan Bakeoff", "author": ["H Tseng et al.2005] Tseng", "P Chang", "G Andrew"], "venue": "Proceedings of the Fourth SIGHAN Workshop on Chinese Language", "citeRegEx": "Tseng et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Tseng et al\\.", "year": 2005}, {"title": "Joint training and decoding using virtual nodes for cascaded segmentation and tagging tasks", "author": ["X Qian et al.2010] Qian", "Q Zhang", "Y Zhou"], "venue": "Proceedings of the 2010 Conference on Empirical Methods in Natural Language", "citeRegEx": "Qian et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Qian et al\\.", "year": 2010}, {"title": "Conditional Random Fields: Probabilistic Models for Segmenting and Labeling Sequence Data. Data Mining and Knowledge Discovery", "author": ["J Lafferty et al.2001] Lafferty", "A McCallum", "N. Pereira F C"], "venue": null, "citeRegEx": "Lafferty et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Lafferty et al\\.", "year": 2001}, {"title": "Semi-supervised learning of hidden conditional random fields for time-series classification", "author": ["M. Kim"], "venue": null, "citeRegEx": "Kim,? \\Q2013\\E", "shortCiteRegEx": "Kim", "year": 2013}, {"title": "Natural Language Processing (Almost) from Scratch, volume 1", "author": ["J Weston", "L Bottou"], "venue": "Journal of Machine Learning", "citeRegEx": "Collobert et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Collobert et al\\.", "year": 2011}, {"title": "Conditional ordinal random fields for structured ordinal-valued label prediction. Data mining and knowledge", "author": ["M. Kim"], "venue": null, "citeRegEx": "Kim,? \\Q2014\\E", "shortCiteRegEx": "Kim", "year": 2014}, {"title": "Minimum tag error for discriminative training of conditional random fields", "author": ["Y Xiong et al.2009] Xiong", "J Zhu", "H Huang"], "venue": null, "citeRegEx": "Xiong et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Xiong et al\\.", "year": 2009}, {"title": "Discriminative training methods for hidden markov models: Theory and experiments with perceptron algorithms", "author": ["Michael Collins"], "venue": "In Proceedings of EMNLP", "citeRegEx": "Collins.,? \\Q2002\\E", "shortCiteRegEx": "Collins.", "year": 2002}, {"title": "Efficient, feature-based, conditional random field parsing", "author": ["Alex Kleeman", "Christopher D"], "venue": "In Proceedings of ACL-08:HLT:959\u2013967", "citeRegEx": "Finkel et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Finkel et al\\.", "year": 2008}, {"title": "Conditional random fields: Probabilistic models for segmenting and labeling sequence data", "author": ["Andrew McCallum", "Fernando Pereira"], "venue": "In Proceedings of ICML 2001:282\u2013289", "citeRegEx": "Lafferty et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Lafferty et al\\.", "year": 2001}, {"title": "Civil Transportation Event Extraction from Chinese Microblog", "author": ["J Xiong el al.2013] Xiong", "Y Hao", "Z. Huang"], "venue": "Cloud Computing and Big Data (CloudCom-Asia),", "citeRegEx": "Xiong et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Xiong et al\\.", "year": 2013}, {"title": "Learning with probabilistic features for improved pipeline models", "author": ["Razvan", "Bunescu2008] Razvan C. Bunescu"], "venue": "InProceedings of EMNLP:670\u2013", "citeRegEx": "Razvan and Bunescu.,? \\Q2008\\E", "shortCiteRegEx": "Razvan and Bunescu.", "year": 2008}, {"title": "Chinese word segmentation: A decade review", "author": ["Huang", "Zhao2007] Changning Huang", "Hai Zhao"], "venue": "Journal of Chinese Information", "citeRegEx": "Huang et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Huang et al\\.", "year": 2007}, {"title": "Three new probabilistic models for dependency parsing: An exploration", "author": ["Jason M. Eisner"], "venue": "In Proceedings of the 16th Conference on Computational linguistics:340\u2013345", "citeRegEx": "Eisner.,? \\Q1996\\E", "shortCiteRegEx": "Eisner.", "year": 1996}, {"title": "Inducing features of random fields", "author": ["V. Della Pietra", "J. Lafferty"], "venue": "In Proceedings of the 16th Conference on Computational linguistics:380\u2013393", "citeRegEx": "Pietra et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Pietra et al\\.", "year": 1997}, {"title": "Inducing features of random fields", "author": ["H. Wallach"], "venue": "In Proc. 6th Annual CLUKResearch Colloquium", "citeRegEx": "Wallach.,? \\Q2002\\E", "shortCiteRegEx": "Wallach.", "year": 2002}, {"title": "Chinese statistical parsing", "author": ["Harper", "Huang2009] Mary Harper", "Zhongqiang Huang"], "venue": null, "citeRegEx": "Harper et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Harper et al\\.", "year": 2009}, {"title": "Improved inference for unlexicalized parsing", "author": ["Petrov", "Klein2007] Slav Petrov", "Dan Klein"], "venue": "In Proceedings of NAACL", "citeRegEx": "Petrov et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Petrov et al\\.", "year": 2007}, {"title": "Conditional Random Fields: An introduction", "author": ["M. Wallach] Wallach H"], "venue": "Technical Reports", "citeRegEx": "H,? \\Q2004\\E", "shortCiteRegEx": "H", "year": 2004}, {"title": "Shallow Parsing with Conditional Random Fields", "author": ["Sha", "Pereira2003] F. Sha", "F. Pereira"], "venue": "Proceedings of Human Language Technology,", "citeRegEx": "Sha et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Sha et al\\.", "year": 2003}], "referenceMentions": [{"referenceID": 6, "context": "(Collobert et al., 2011) In our approach, we combine CRF and MMSEG algorithm and extend features of traditional CRF algorithm to train the model for word segmentation, We use Internet lexicon in order to improve the performance of our model on Chinese social media.", "startOffset": 0, "endOffset": 24}, {"referenceID": 4, "context": "(Lafferty et al., 2001) Previous research showed that CRF has a good performance on word segmentation accuracy in the pipeline method.", "startOffset": 0, "endOffset": 23}, {"referenceID": 4, "context": "(Lafferty et al., 2001) Previous research showed that CRF has a good performance on word segmentation accuracy in the pipeline method. Tseng H et al(2005) and John Lafferty et al(2001) introduced a conditional random field se-", "startOffset": 1, "endOffset": 155}, {"referenceID": 4, "context": "(Lafferty et al., 2001) Previous research showed that CRF has a good performance on word segmentation accuracy in the pipeline method. Tseng H et al(2005) and John Lafferty et al(2001) introduced a conditional random field se-", "startOffset": 1, "endOffset": 185}, {"referenceID": 1, "context": "lated to ours is (Zhao et al., 2006), which also used assistant algorithm and added external lexicon, while they just add the output of assistant algorithm to the feature templates.", "startOffset": 17, "endOffset": 36}, {"referenceID": 9, "context": "CRF performs better than lexicon-based models on OOV rates because CRF introduce additional features, (Collins, 2002) which may be artificially added (Xiong et al.", "startOffset": 102, "endOffset": 117}, {"referenceID": 8, "context": "CRF performs better than lexicon-based models on OOV rates because CRF introduce additional features, (Collins, 2002) which may be artificially added (Xiong et al., 2009), including character level features and context level features, in addition, CRF also maintains the Markov characteristics of the word, (Wallach, ) thus we can remove word ambiguity by combining more features as well.", "startOffset": 150, "endOffset": 170}, {"referenceID": 3, "context": "(Qian et al., 2010)", "startOffset": 0, "endOffset": 19}, {"referenceID": 13, "context": "(Razvan and Bunescu, 2008) At each stage, if one training material gets the best performance on test set, it will be chosen to be the final training material that we would use.", "startOffset": 0, "endOffset": 26}, {"referenceID": 17, "context": "(Wallach, 2002)", "startOffset": 0, "endOffset": 15}, {"referenceID": 2, "context": "(Tseng et al., 2005) We have to mention that the lexicon used by MMSEG is the Sougou lab internet lexicon published in 2006, which contains a number of high-frequency words under internet environment, and mmseg4j project lexicon file.", "startOffset": 0, "endOffset": 20}, {"referenceID": 16, "context": "(Pietra et al., 1997) Each line in the template file of PCRF denotes one template.", "startOffset": 0, "endOffset": 21}, {"referenceID": 10, "context": "The first stage of the experiment is to determine the feature vectors of our model, (Finkel et al., 2008) the original features are too simple so we change the feature template step by step and test the accuracy on our test set.", "startOffset": 84, "endOffset": 105}], "year": 2015, "abstractText": "In this paper, we propose a joint algorithm for the word segmentation on Chinese social media. Previous work mainly focus on word segmentation for plain Chinese text, in order to develop a Chinese social media processing tool, we need to take the main features of social media into account, whose grammatical structure is not rigorous, and the tendency of using colloquial and Internet terms makes the existing Chinese-processing tools inefficient to obtain good performance on social media. (Collobert et al., 2011) In our approach, we combine CRF and MMSEG algorithm and extend features of traditional CRF algorithm to train the model for word segmentation, We use Internet lexicon in order to improve the performance of our model on Chinese social media. Our experimental result on Sina Weibo shows that our approach outperforms the stateof-the-art model.", "creator": "LaTeX with hyperref package"}}}