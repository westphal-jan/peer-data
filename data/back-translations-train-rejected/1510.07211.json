{"id": "1510.07211", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "25-Oct-2015", "title": "On End-to-End Program Generation from User Intention by Deep Neural Networks", "abstract": "This paper envisions an end-to-end program generation scenario using recurrent neural networks (RNNs): Users can express their intention in natural language; an RNN then automatically generates corresponding code in a characterby-by-character fashion. We demonstrate its feasibility through a case study and empirical analysis. To fully make such technique useful in practice, we also point out several cross-disciplinary challenges, including modeling user intention, providing datasets, improving model architectures, etc. Although much long-term research shall be addressed in this new field, we believe end-to-end program generation would become a reality in future decades, and we are looking forward to its practice.", "histories": [["v1", "Sun, 25 Oct 2015 06:52:45 GMT  (217kb,D)", "http://arxiv.org/abs/1510.07211v1", "Submitted to 2016 International Conference of Software Engineering \"Vision of 2025 and Beyond\" track"]], "COMMENTS": "Submitted to 2016 International Conference of Software Engineering \"Vision of 2025 and Beyond\" track", "reviews": [], "SUBJECTS": "cs.SE cs.LG", "authors": ["lili mou", "rui men", "ge li", "lu zhang", "zhi jin"], "accepted": false, "id": "1510.07211"}, "pdf": {"name": "1510.07211.pdf", "metadata": {"source": "CRF", "title": "On End-to-End Program Generation from User Intention by Deep Neural Networks", "authors": ["Lili Mou", "Rui Men", "Ge Li", "Lu Zhang", "Zhi Jin"], "emails": ["doublepower.mou@gmail.com,", "menruimr@gmail.com,", "lige@sei.pku.edu.cn", "zhanglu@sei.pku.edu.cn", "zhijin@sei.pku.edu.cn"], "sections": [{"heading": null, "text": "Categories and Theme Descriptions I.2.2 [Artificial Intelligence]: Automatic Programming - Program SynthesisGeneral Terms AlgorithmTags Deep Learning, Recursive Network, Program Generation"}, {"heading": "1. INTRODUCTION", "text": "Imagine the following scenario in software development: there is abundant high-quality source code, well commented and documented, in large software repositories. A very powerful machine (e.g. a deep neural network) learns to map natural descriptions of the source code. During development, its intention is expressed by a natural language (similar to a machine that automatically outputs the desired code), and the complete mapping of the source code is replaced by a digital copy."}, {"heading": "2. A CASE STUDY", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "2.1 The Model of Recurrent Networks", "text": "This year it is more than ever before."}, {"heading": "2.2 Result and Analysis", "text": "In fact, most of them will be able to abide by the rules that they have imposed on themselves, and they will be able to break the rules that they have imposed on themselves."}, {"heading": "3. PROSPECTIVES & ROAD MAP", "text": "In fact, most of them are able to determine for themselves what they want and what they want to do."}, {"heading": "4. RELATED WORK IN DEEP LEARNING FOR PROGRAM ANALYSIS", "text": "In recent years, we have witnessed the birth of a program analysis based on deep neural networks. In our previous work, we learn vector representations of programs that serve as a predictive phrase in deep learning [17]; we also propose tree-based revolutionary neural networks to classify programs by functionality and recognize source code of specific patterns [16]. Zaremba et al. use RNNs to estimate the performance of a limited python program [22]. Allamanis et al. use vector representations to suggest method names [1]. All of the above models are discriminatory, allowing us to consider the tasks as a classification problem. Karpathy et al. train an RNN-based language model on C-code that maximizes the common probability of a program [11]. Unlike the above studies, this paper examines whether neural models synthesize executable, functionally coherent programs, which can capture more of the needs of the intercoding structure of the program."}, {"heading": "5. CONCLUSIVE REMARKS", "text": "In this thesis, we trained a recursive neural network (RNN) to generate (almost) executable, functionally coherent source codes. Our initial work demonstrated the possibility of automatic end-to-end program generation. By analyzing the mechanism of RNN, we have designed several scenarios in which such techniques can be applied in software development tasks in future decades. To pursue this new direction of research, we are requesting studies from several disciplines."}, {"heading": "6. REFERENCES", "text": "[1] M. Allamanis, E. Barr, C. Bird, and C. Sutton.Suggesting exact method and class names. In ESEC / FSE, 2015. [2] K. Cho, B. van Merrie \u00c3 nboer, D. Bahdanau, and Y. Bengio. On the properties of neural machine translation: Encoder-decoder approaches. arXiv preprint, 1409.1259, 2014. [3] J. Chorowski, D. Bahdanau, D. Serdyuk, K. Cho, and Y. Bengio. Attention-based models for speech recognition. arXiv preprint, 1506.07503, 2015. [4] A. Cozzie and S. T. King: Writing programs with natural language and examples. Technical report, University of Illinois at Urbana-Champaign, 2012. [5] P. Flener and D. Partridge. Inductive programming. Engineering, 8 (2)."}], "references": [{"title": "Suggesting accurate method and class names", "author": ["M. Allamanis", "E. Barr", "C. Bird", "C. Sutton"], "venue": "ESEC/FSE", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2015}, {"title": "B", "author": ["K. Cho"], "venue": "van Merri\u00ebnboer, D. Bahdanau, and Y. Bengio. On the properties of neural machine translation: Encoder-decoder approaches. arXiv preprint, 1409.1259", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2014}, {"title": "Attention-based models for speech recognition", "author": ["J. Chorowski", "D. Bahdanau", "D. Serdyuk", "K. Cho", "Y. Bengio"], "venue": "arXiv preprint, 1506.07503", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2015}, {"title": "Macho: Writing programs with natural language and examples", "author": ["A. Cozzie", "S.T. King"], "venue": "Technical report, University of Illinois at Urbana-Champaign", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2012}, {"title": "Inductive programming", "author": ["P. Flener", "D. Partridge"], "venue": "Automated Softw. Engineering, 8(2):131\u2013137", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2001}, {"title": "Dimensions in program synthesis", "author": ["S. Gulwani"], "venue": "Proc. ACM SIGPLAN Symposium on Principles and Practice of Declarative Programming", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2010}, {"title": "General program synthesis benchmark suite", "author": ["T. Helmuth", "L. Spector"], "venue": "Proc. Genetic and Evol. Comput. Conf. ACM", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2015}, {"title": "Long short-term memory", "author": ["S. Hochreiter", "J. Schmidhuber"], "venue": "Neural Comput., 9(8):1735\u20131780", "citeRegEx": "8", "shortCiteRegEx": null, "year": 1997}, {"title": "Turing machines are recurrent neural networks", "author": ["H. Hy\u00f6tyniemi"], "venue": "Proc. STeP", "citeRegEx": "9", "shortCiteRegEx": null, "year": 1996}, {"title": "The unreasonable effectiveness of recurrent neural networks", "author": ["A. Karpathy"], "venue": "http://karpathy.github.io/ 2015/05/21/rnn-effectiveness/", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2015}, {"title": "Visualizing and understanding recurrent networks", "author": ["A. Karpathy", "J. Johnson", "F. Li"], "venue": "arXiv preprint, 1506.02078", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2015}, {"title": "Pegasus: First steps toward a naturalistic programming language", "author": ["R. Kn\u00f6ll", "M. Mezini"], "venue": "OOPSLA", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2006}, {"title": "et al", "author": ["A. Kumar", "O. Irsoy", "J. Su"], "venue": "Ask me anything: Dynamic memory networks for natural language processing. arXiv preprint, 1506.07285", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2015}, {"title": "Deep learning", "author": ["Y. LeCun", "Y. Bengio", "G. Hinton"], "venue": "Nature, 521(7553):436\u2013444", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2015}, {"title": "A deductive approach to program synthesis", "author": ["Z. Manna", "R. Waldinger"], "venue": "ACM Trans. Programming Languages and Syst., 2(1):90\u2013121", "citeRegEx": "15", "shortCiteRegEx": null, "year": 1980}, {"title": "TBCNN: A tree-based convolutional neural network for programming language processing", "author": ["L. Mou", "G. Li", "Z. Jin", "L. Zhang", "T. Wang"], "venue": "AAAI Workshop", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2015}, {"title": "Building program vector representations for deep learning", "author": ["L. Mou", "G. Li", "Y. Liu", "H. Peng", "Z. Jin", "Y. Xu", "L. Zhang"], "venue": "arXiv preprint, 1409.3358", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2014}, {"title": "On the difficulty of training recurrent neural networks", "author": ["R. Pascanu", "T. Mikolov", "Y. Bengio"], "venue": "arXiv preprint, 1211.5063", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2012}, {"title": "Sequence to sequence learning with neural networks", "author": ["I. Sutskever", "O. Vinyals", "Q. Le"], "venue": "NIPS", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2014}, {"title": "Automatically finding patches using genetic programming", "author": ["W. Weimer", "T. Nguyen", "C. Le Goues", "S. Forrest"], "venue": "ICSE", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2009}, {"title": "Learning to execute", "author": ["W. Zaremba", "I. Sutskever"], "venue": "arXiv preprint, 1410.4615", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2014}], "referenceMentions": [{"referenceID": 5, "context": ", algorithm discovery, programming assistance [6].", "startOffset": 46, "endOffset": 49}, {"referenceID": 14, "context": "propose deductive approaches [15], Flener et al.", "startOffset": 29, "endOffset": 33}, {"referenceID": 4, "context": "inductive approaches [5]; these methods require human-designed specifications.", "startOffset": 21, "endOffset": 24}, {"referenceID": 19, "context": "Program generation by genetic programming [20, 7] can automatically search the space of candidate programs (inefficiently), but carefully chosen mutation or crossover operations should also be provided.", "startOffset": 42, "endOffset": 49}, {"referenceID": 6, "context": "Program generation by genetic programming [20, 7] can automatically search the space of candidate programs (inefficiently), but carefully chosen mutation or crossover operations should also be provided.", "startOffset": 42, "endOffset": 49}, {"referenceID": 11, "context": "Natural language programming, emerged in the past decade, is much like \u201cpseudo-compiling,\u201d where the natural language is of low-level abstraction [12, 4].", "startOffset": 146, "endOffset": 153}, {"referenceID": 3, "context": "Natural language programming, emerged in the past decade, is much like \u201cpseudo-compiling,\u201d where the natural language is of low-level abstraction [12, 4].", "startOffset": 146, "endOffset": 153}, {"referenceID": 12, "context": "At the meantime, the natural language processing (NLP) community is witnessing significant breakthroughs and amazing results in various tasks including question answering [13], machine translation [19], or image-caption generation [21].", "startOffset": 171, "endOffset": 175}, {"referenceID": 18, "context": "At the meantime, the natural language processing (NLP) community is witnessing significant breakthroughs and amazing results in various tasks including question answering [13], machine translation [19], or image-caption generation [21].", "startOffset": 197, "endOffset": 201}, {"referenceID": 13, "context": "Among a variety of machine learning methods, the deep neural network (also known as deep learning) is among recent groundbreaking advances, featured by its ability of learning highly complicated features automatically [14].", "startOffset": 218, "endOffset": 222}, {"referenceID": 8, "context": "Theoretical analysis shows that recurrent neural networks are equivalent to Turing machines [9].", "startOffset": 92, "endOffset": 95}, {"referenceID": 17, "context": "However, training RNNs in early years was difficult because of the gradient blowup or vanishing problem [18].", "startOffset": 104, "endOffset": 108}, {"referenceID": 7, "context": "Long short term memory (LSTM) units [8], or gated units [2] are designed to balance between retaining the previous state and memorizing new information at the current time step, making RNNs much easier to train.", "startOffset": 36, "endOffset": 39}, {"referenceID": 1, "context": "Long short term memory (LSTM) units [8], or gated units [2] are designed to balance between retaining the previous state and memorizing new information at the current time step, making RNNs much easier to train.", "startOffset": 56, "endOffset": 59}, {"referenceID": 18, "context": "design an RNN model for sequence to sequence generation [19].", "startOffset": 56, "endOffset": 60}, {"referenceID": 9, "context": "Successful applications include generating texts, music, or even Linux-like C code [10].", "startOffset": 83, "endOffset": 87}, {"referenceID": 10, "context": ", parenthesis pairing, indentation, etc [11].", "startOffset": 40, "endOffset": 44}, {"referenceID": 18, "context": "Figure 1: A sequence to sequence recurrent neural network, adapted from [19].", "startOffset": 72, "endOffset": 76}, {"referenceID": 2, "context": "Attentionbased networks [3, 21], for example, are proposed recently to mitigate the problem of long input sequences that cannot be composed into a fixed-size vector.", "startOffset": 24, "endOffset": 31}, {"referenceID": 16, "context": "Our previous work learns programs\u2019 vector representations, serving as a pretraining phrase in deep learning [17]; we also propose treebased convolutional neural networks to classify programs by functionality and detect source code of certain patterns [16].", "startOffset": 108, "endOffset": 112}, {"referenceID": 15, "context": "Our previous work learns programs\u2019 vector representations, serving as a pretraining phrase in deep learning [17]; we also propose treebased convolutional neural networks to classify programs by functionality and detect source code of certain patterns [16].", "startOffset": 251, "endOffset": 255}, {"referenceID": 20, "context": "use RNNs to estimate the output of a restricted python program [22].", "startOffset": 63, "endOffset": 67}, {"referenceID": 0, "context": "leverage vector representations to suggest method names [1].", "startOffset": 56, "endOffset": 59}, {"referenceID": 10, "context": "train an RNN-based language model on C code, which maximizes the joint probability of a program [11].", "startOffset": 96, "endOffset": 100}], "year": 2015, "abstractText": "This paper envisions an end-to-end program generation scenario using recurrent neural networks (RNNs): Users can express their intention in natural language; an RNN then automatically generates corresponding code in a characterby-character fashion. We demonstrate its feasibility through a case study and empirical analysis. To fully make such technique useful in practice, we also point out several crossdisciplinary challenges, including modeling user intention, providing datasets, improving model architectures, etc. Although much long-term research shall be addressed in this new field, we believe end-to-end program generation would become a reality in future decades, and we are looking forward to its practice.", "creator": "LaTeX with hyperref package"}}}