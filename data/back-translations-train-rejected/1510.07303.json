{"id": "1510.07303", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "25-Oct-2015", "title": "A Framework for Distributed Deep Learning Layer Design in Python", "abstract": "In this paper, a framework for testing Deep Neural Network (DNN) design in Python is presented. First, big data, machine learning (ML), and Artificial Neural Networks (ANNs) are discussed to familiarize the reader with the importance of such a system. Next, the benefits and detriments of implementing such a system in Python are presented. Lastly, the specifics of the system are explained, and some experimental results are presented to prove the effectiveness of the system.", "histories": [["v1", "Sun, 25 Oct 2015 21:04:12 GMT  (2115kb,D)", "http://arxiv.org/abs/1510.07303v1", "8 pages, 7 figures"]], "COMMENTS": "8 pages, 7 figures", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["clay mcleod"], "accepted": false, "id": "1510.07303"}, "pdf": {"name": "1510.07303.pdf", "metadata": {"source": "META", "title": "A Framework for Distributed Deep Learning Layer Design in Python", "authors": ["Clay McLeod"], "emails": ["clmcleod@go.olemiss.edu"], "sections": [{"heading": "Introduction", "text": "The availability of this data has led researchers to publish large volumes of literature exploring how we can make good use of this data and how we can, in turn, handle this data in a computationally efficient manner [3], [4], [5], [6]. The best performing methods include a statistical technique called machine learning (ML), in which we attempt to build a model of the data by taking advantage of the high processing power of a machine [7], [8]. For a complete overview of ML and the algorithms involved, please refer [9] to a particularly good ML algorithm for working with large volumes of data."}, {"heading": "Machine Learning in Python", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "Benefits", "text": "Simplicity: First, Python is a good language because it is an easy-to-understand language to program with. Because the hurdle to start programming in Python is so low, it attracts a large audience - especially experts from other academic fields who have little programming experience but want to take advantage of advanced computing processes. Strong open source community: Partly due to the aforementioned simplicity, a strong community of scientists has formed around the open source community, allowing libraries to evolve and improve at a rapid pace, which is not useful when a small group of programmers is working on a closed source project. Therefore, many Python libraries use cutting-edge technologies and strong documentation pages. Many Python libraries also use non-restrictive licenses, which also induces companies to use them and return to the community.Mature scientific libraries: In addition, many experts in the scientific field want to reach as large a Python audience as possible in this system."}, {"heading": "Detriments", "text": "Global Interpreter Lock: One of the main obstacles to implementing massively parallel systems in Python is the existence of the Global Interpreter Lock (GIL) [22]. The GIL is necessary in the Python standard implementation because Python's memory management system is not thread-safe. Specifically, this means that no Python thread can access a Python object at the same time, which allows for simultaneity but no parallelism [23]. This restriction greatly reduces Python's ability to use multiple CPUs [24]. Furthermore, since the implementation of the GIL in Python, many other subsystems within the Python ecosystem have relied on its existence. Figure 1 shows that some of Python's central scientific libraries and their relationship with Thear Xiv are so limited: 151 0.07 303v 1 [cs.L G] 25 Oct 201 52 GIL. Although several transpilers, such as Python and Python do not have full Python support, the Python solution is not fully applied."}, {"heading": "Objectives", "text": "Before defining the system, the objectives of this paper and the system are discussed."}, {"heading": "Dependencies", "text": "In the following sections, the host refers to a single machine that coordinates the cluster of worker machines, including distributed jobs through the message queue (see \"RabbitMQ\"), the database (see \"MongoDB\"), and the HTTP report server (see \"Reporting Server\"). A worker machine is a dispensable machine that performs tasks as it receives them from the host machine. This system has only been tested on Python 2.7, but porting to Python 3.X should be trivial."}, {"heading": "Ecosystem", "text": "Docker: Docker is a lightweight Linux container system that allows you to compile a particular Linux image once and then effortlessly deploy that image to multiple destinations [29]. Specifically, we can compile an Ubuntu Linux image with the correct Python libraries installed, and then easily deploy our image on any of our Worker servers. Docker runs on most (if not all) major operating systems and takes little effort. For more information on configuring Docker or its capabilities, visit [30].CoreOS: CoreOS, however, is a lightweight, flexible operating system that runs containers [31]. CoreOS has automated clustering and the provision of workstations (so-called services) that have been built in. In addition, the CoreOS team has built a painlessly distributed key value store called etcd [32] that is vicd compatible with the operating system. Although many of these features may not be applicable on this paper, the Mongoose documentation may be one that is - and the only one that can be used on this paper."}, {"heading": "Reporting Server", "text": "The report server is built on a web platform and uses HTML5, CSS3 and JavaScript. Notable libraries and features include: \u2022 NGINX (http: / / www.nginx.com /): Serving static web pages and reverse proxy report servers. \u2022 Bootstrap (http: / / getbootstrap.com /): Twitter CSS styling library. \u2022 jQuery (http: / / jquery.com): DOM manipulation and communication with the backend REST server. \u2022 plot.ly (http: / / plot.ly): Plotting results, manipulating charts and exporting results. \u2022 Papa Parse (http: / / papaparse.com /): JS library for easy parsing CSV files."}, {"heading": "Python Libraries", "text": "In this system, he acts as an intermediary between the systems and the database, writing as much as possible about the entire system."}, {"heading": "System Design", "text": "For the purposes of this discussion, there are four atomic functional units that all work asynchronously with each other; these functional units include uploading the data (see the section called \"Data Uploading\"), initializing tasks to be performed for that data (see the section called \"Task Management\"), executing the tasks (see the section called \"Task Execution\"), and storing / visualizing the results (see the section called \"Results Storage and Visualization\"). Because these units work independently of each other, failure can occur at one stage of the pipeline without affecting the performance of another stage. Each stage is called \"Failure forward,\" meaning that in the event of an error, the system merely presents the corresponding error and carries out its next task instead of crashing. These are all principles that the reader should follow when creating his or her own systems."}, {"heading": "Data Upload", "text": "First, a static website for uploading data to the server was created to be processed. This server used a standard bootstrap template and suspended an interface for uploading a CSV file, as most datasets are easily represented as this type of file. Next, the Papa Parse library was used to analyze the CSV file into a multi-dimensional JavaScript array. If Papa Parse encountered an error in parsing the file (e.g. faulty format), it was prompted by an error notified to the user and the process was aborted. Note: Missing data was not considered an error due to the desired compatibility with sparse datasets. If the dataset was properly formatted, the user was prompted to choose a \"label\" (what value of the data we want to predict), and Papa Parse passed the data on to jQuery, which would attempt to upload the data to the Flask REST."}, {"heading": "Tasks Management", "text": "It has been suggested that the relationship between RabbitMQ and Celery is an integral part of this system. As already mentioned, setting up a RabbitMQ server is very simple with Docker - just run the default RabbitMQ container [43] with the web interface enabled. In this system, the RabbitMQ container was executed on the host machine for low latency communication with the Flask server. For the celery instance, a separate docker container was created on top of the standard Python 2.7 container, which takes over the operation of the Flask / Celery Python micro frame. As all these docker containers are executed on the same machine and the ports are assigned to the host machine, communication between Docker containers is seamless. Once this JavaScript array is passed to the Flask server, the user can prepare the data."}, {"heading": "Task Execution", "text": "As discussed in \"Task Management,\" this work unit simply implements a training function for Keras / PyBrain that trains the DNN based on the parameters provided by Celery. This is simply by writing a single method and letting Celery execute these training methods as soon as the workers are available. However, what is not trivial is the rapid deployment of new workers into the system. A Docker container was built on the basis of the standard Python 2.7 image [43], which was executed by an instance of a celery worker. In this way, whenever you apply this docker container to a work machine, another worker in the celery cluster is provided. The key to optimizing efficiency is to take advantage of: 1. Working machines with GPU acceleration. 2. Working machines with multiple workpieces. In order to take advantage of multiple CPU cores, simply place the \"THEANO _ FLAGS\" environment variable in the docker like a THANGS = a specific device can not be followed."}, {"heading": "Results Storage and Visualization", "text": "After processing by the Task Execution subsystem, the results are stored in the MongoDB instance. A standard MongoDB container [43] was installed on the host machine, which enabled low-latency communication with the Flask server and centralized data storage. To access the MongoDB results from the client, a RESTful API was used. Some information stored in the database includes the session ID, training time, model accuracy, and parameters used to train the model. Further intricacies of this setup are a web interface for monitoring both celery and RabbitMQ. Figures 5-7 show these reporting functions."}, {"heading": "Results", "text": "Discussion of the results is not the main focus of this paper, and research on what systematic approach to studying DNNs is still underway appears to be reaching a \"critical mass\" of information storage of about 500-700 hidden layers, with DNNs \"performance being flat for each number of layers larger than the critical mass point. This is most likely due to upgrading the deep neural network with the training data and not a significant finding, except that developers are encouraged to use as few layers as possible to avoid overfitting. Note: This premise has not been extensively tested against all types of layers and activation features, but a significant number of combinations have been tried. \u2022 DNN training increases roughly linearly with the amount of layers added to the network for a significant number of layer combinations (see Figure 5)."}, {"heading": "Improvements/Future Work", "text": "Several improvements are possible, in particular: \u2022 Asynchronous REST API: This focus of this experiment was to build a full-stack Python system up to 7 turn DNNs. However, this is generally not optimal for asynchronous applications such as a REST API. The reader is encouraged to look for alternatives outside the Python programming language such as NodeJS [45] or golang [46] to build their RESTful API, as Python's blocking nature poses major speed problems. \u2022 Granular web interface: Instead of directly programming the rules for selecting training parameters, it is conceivable that you can integrate this functionality into the web interface and allow DNN designers who are not familiar with a particular library to set these training rules via a GUI."}, {"heading": "Conclusion", "text": "This is potentially useful because the design of DNNs is an art rather than a science, so the more we can figure out how different hyperparameters / layers affect the accuracy of a deep neural network, the better. By using multiple deployment tools (Docker [29], CoreOS [31]), productive distributed solutions (RabbitMQ [33], MongoDB [36]), and the scientific python ecosystem (NumPy [17], SciPy [18], Theano [20], [21], Keras [42], PyBrain [41]), such a system can be easily implemented in a flexible way that prioritizes simplicity and modularity."}], "references": [{"title": "Big data: The next frontier for innovation, competition, and productivity", "author": ["J. Manyika", "M. Chui", "B. Brown", "J. Bughin", "R. Dobbs", "C. Roxburgh", "A.H. Byers"], "venue": "2011.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2011}, {"title": "Big data: A revolution that will transform how we live, work, and think", "author": ["V. Mayer-Sch\u00f6nberger", "K. Cukier"], "venue": null, "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2013}, {"title": "Big data analytics", "author": ["P. Russom", "others"], "venue": "TDWI Best Practices Report, Fourth Quarter, 2011.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2011}, {"title": "The hadoop distributed file system", "author": ["K. Shvachko", "H. Kuang", "S. Radia", "R. Chansler"], "venue": "Mass storage systems and technologies (mSST), 2010 iEEE 26th symposium on, 2010, pp. 1\u201310.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2010}, {"title": "Spark: Cluster computing with working sets", "author": ["M. Zaharia", "M. Chowdhury", "M.J. Franklin", "S. Shenker", "I. Stoica"], "venue": "Proceedings of the 2nd uSENIX conference on hot topics in cloud computing, 2010, vol. 10, p. 10.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2010}, {"title": "Genetic algorithms and machine learning", "author": ["D.E. Goldberg", "J.H. Holland"], "venue": "Machine learning, vol. 3, no. 2, pp. 95\u201399, 1988.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 1988}, {"title": "An overview of machine learning", "author": ["J.G. Carbonell", "R.S. Michalski", "T.M. Mitchell"], "venue": "Machine learning, Springer, 1983, pp. 3\u201323.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 1983}, {"title": "Mitchell,Machine learning: An artificial intelligence approach", "author": ["R.S. Michalski", "J.G. Carbonell", "T. M"], "venue": "Springer Science & Business Media,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2013}, {"title": "A comprehensive foundation", "author": ["S. Haykin", "N. Network"], "venue": "Neural Networks, vol. 2, no. 2004, 2004.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2004}, {"title": "Deep learning in neural networks: An overview", "author": ["J. Schmidhuber"], "venue": "CoRR, vol. abs/1404.7828, 2014.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2014}, {"title": "Unsupervised feature learning and deep learning: A review and new perspectives", "author": ["Y. Bengio", "A.C. Courville", "P. Vincent"], "venue": "CoRR, vol. abs/1206.5538, 2012.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2012}, {"title": "Deep learning", "author": ["Y. Bengio", "I.J. Goodfellow", "A. Courville"], "venue": "2015.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2015}, {"title": "Multilayer feedforward networks are universal approximators", "author": ["K. Hornik", "M. Stinchcombe", "H. White"], "venue": "Neural networks, vol. 2, no. 5, pp. 359\u2013366, 1989.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 1989}, {"title": "Multilayer feedforward networks with a nonpolynomial activation function can approximate any function", "author": ["M. Leshno", "V.Y. Lin", "A. Pinkus", "S. Schocken"], "venue": "Neural networks, vol. 6, no. 6, pp. 861\u2013867, 1993.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 1993}, {"title": "NumPy GitHub repository", "author": ["N. team"], "venue": "GitHub repository. GitHub, 2015.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2015}, {"title": "Data structures for statistical computing in python", "author": ["W. McKinney"], "venue": "Proceedings of the 9th python in science conference, 2010, pp. 51\u201356.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2010}, {"title": "Theano: New features and speed improvements.", "author": ["F. Bastien", "P. Lamblin", "R. Pascanu", "J. Bergstra", "I.J. Goodfellow", "A. Bergeron", "N. Bouchard", "Y. Bengio"], "venue": "Deep Learning and Unsupervised Feature Learning NIPS 2012 Workshop,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2012}, {"title": "Theano: A CPU and GPU math expression compiler", "author": ["J. Bergstra", "O. Breuleux", "F. Bastien", "P. Lamblin", "R. Pascanu", "G. Desjardins", "J. Turian", "D. Warde-Farley", "Y. Bengio"], "venue": "Proceedings of the python for scientific computing conference (SciPy), 2010.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2010}, {"title": "Global interpreter lock documentation", "author": ["P. Foundation"], "venue": "2015. [Online]. Available: https://wiki.python.org/ moin/GlobalInterpreterLock. [Accessed: 23-Oct-2015].", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2015}, {"title": "PyData and the gIL", "author": ["M. Rocklin"], "venue": "2015.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2015}, {"title": "Understanding the python gIL.", "author": ["D. Beazley"], "venue": null, "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2010}, {"title": "A practical guide to training restricted boltzmann machines", "author": ["G. Hinton"], "venue": "Momentum, vol. 9, no. 1, p. 926, 2010.  8", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2010}, {"title": "Practical recommendations for gradientbased training of deep architectures", "author": ["Y. Bengio"], "venue": "Neural networks: Tricks of the trade, Springer, 2012, pp. 437\u2013478.", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2012}, {"title": "Improving neural networks by preventing co-adaptation of feature detectors", "author": ["G.E. Hinton", "N. Srivastava", "A. Krizhevsky", "I. Sutskever", "R.R. Salakhutdinov"], "venue": "arXiv preprint arXiv:1207.0580, 2012.", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2012}, {"title": "GPU computing", "author": ["J.D. Owens", "M. Houston", "D. Luebke", "S. Green", "J.E. Stone", "J.C. Phillips"], "venue": "Proceedings of the IEEE, vol. 96, no. 5, pp. 879\u2013899, 2008.", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2008}, {"title": "Docker: Lightweight linux containers for consistent development and deployment", "author": ["D. Merkel"], "venue": "Linux J., vol. 2014, no. 239, Mar. 2014.", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2014}, {"title": "Docker documentation", "author": ["D. Team"], "venue": "2015. [Online]. Available: http://docs.docker.com. [Accessed: 24-Oct-2015].", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2015}, {"title": "CoreOS website", "author": ["C. Team"], "venue": "2015. [Online]. Available: https://coreos.com/. [Accessed: 24-Oct-2015].", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2015}, {"title": "CoreOS GitHub repository", "author": ["C. team"], "venue": "GitHub repository. https://github.com/coreos/etcd; GitHub, 2015.", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2015}, {"title": "RabbitMQ website", "author": ["R. Team"], "venue": "2015. [Online]. Available: https://www.rabbitmq.com/. [Accessed: 24-Oct- 2015].", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2015}, {"title": "Remote queues: Exposing message queues for optimization and atomicity", "author": ["E.A. Brewer", "F.T. Chong", "L.T. Liu", "S.D. Sharma", "J.D. Kubiatowicz"], "venue": "Proceedings of the seventh annual aCM symposium on parallel algorithms and architectures, 1995, pp. 42\u201353.", "citeRegEx": "34", "shortCiteRegEx": null, "year": 1995}, {"title": "Redis website", "author": ["R. Team"], "venue": "2015. [Online]. Available: https://www.redis.io/. [Accessed: 24-Oct-2015].", "citeRegEx": "35", "shortCiteRegEx": null, "year": 2015}, {"title": "MongoDB website", "author": ["M. Team"], "venue": "2015. [Online]. Available: https://www.mongodb.com/. [Accessed: 24-Oct- 2015].", "citeRegEx": "36", "shortCiteRegEx": null, "year": 2015}, {"title": "Cassandra vs mongoDB vs couchDB vs redis vs riak vs hBase vs couchbase vs orientDB vs aerospike vs neo4j vs hypertable vs elasticSearch vs accumulo vs voltDB vs scalaris vs rethinkDB comparison", "author": ["K. Kovacs"], "venue": "2015. [Online]. Available: http://kkovacs.eu/ cassandra-vs-mongodb-vs-couchdb-vs-redis. [Accessed: 24- Oct-2015].", "citeRegEx": "37", "shortCiteRegEx": null, "year": 2015}, {"title": "REST aPI design rulebook", "author": ["M. Masse"], "venue": null, "citeRegEx": "39", "shortCiteRegEx": "39", "year": 2011}, {"title": "PyBrain", "author": ["T. Schaul", "J. Bayer", "D. Wierstra", "Y. Sun", "M. Felder", "F. Sehnke", "T. R\u00fcckstie\u00df", "J. Schmidhuber"], "venue": "Journal of Machine Learning Research, vol. 11, pp. 743\u2013746, 2010.", "citeRegEx": "41", "shortCiteRegEx": null, "year": 2010}, {"title": "Keras website", "author": ["F. et a. Chollet"], "venue": "2015. [Online]. Available: http://keras.io. [Accessed: 24-Oct-2015].", "citeRegEx": "42", "shortCiteRegEx": null, "year": 2015}, {"title": "Docker hub", "author": ["D. Team"], "venue": "2015. [Online]. Available: http://hub.docker.com. [Accessed: 25-Oct-2015].", "citeRegEx": "43", "shortCiteRegEx": null, "year": 2015}, {"title": "Neural network studies. 1. comparison of overfitting and overtraining", "author": ["I.V. Tetko", "D.J. Livingstone", "A.I. Luik"], "venue": "Journal of chemical information and computer sciences, vol. 35, no. 5, pp. 826\u2013833, 1995.", "citeRegEx": "44", "shortCiteRegEx": null, "year": 1995}, {"title": "NodeJS website", "author": ["N. team"], "venue": "2015. [Online]. Available: http://nodejs.org/. [Accessed: 25-Oct-2015].", "citeRegEx": "45", "shortCiteRegEx": null, "year": 2015}, {"title": "Golang website", "author": ["Google"], "venue": "2015. [Online]. Available: http://golang.org/. [Accessed: 25-Oct-2015].", "citeRegEx": "46", "shortCiteRegEx": null, "year": 2015}], "referenceMentions": [{"referenceID": 0, "context": "In recent years, the amount of data that is produced annually has increased dramatically [1], [2].", "startOffset": 89, "endOffset": 92}, {"referenceID": 1, "context": "In recent years, the amount of data that is produced annually has increased dramatically [1], [2].", "startOffset": 94, "endOffset": 97}, {"referenceID": 2, "context": "The availability of this data has prompted researchers to publish large amounts of literature that study how we can make sense of this data, and, in turn, how can we handle this data in a computationally efficient manner [3], [4], [5], [6].", "startOffset": 221, "endOffset": 224}, {"referenceID": 3, "context": "The availability of this data has prompted researchers to publish large amounts of literature that study how we can make sense of this data, and, in turn, how can we handle this data in a computationally efficient manner [3], [4], [5], [6].", "startOffset": 231, "endOffset": 234}, {"referenceID": 4, "context": "The availability of this data has prompted researchers to publish large amounts of literature that study how we can make sense of this data, and, in turn, how can we handle this data in a computationally efficient manner [3], [4], [5], [6].", "startOffset": 236, "endOffset": 239}, {"referenceID": 5, "context": "high processing power [7], [8].", "startOffset": 22, "endOffset": 25}, {"referenceID": 6, "context": "high processing power [7], [8].", "startOffset": 27, "endOffset": 30}, {"referenceID": 7, "context": "For a complete overview of ML and the different algorithms involved, please refer to [9].", "startOffset": 85, "endOffset": 88}, {"referenceID": 8, "context": "One particularly good ML algorithm for working with large amounts of data is a specific subset of Artificial Neural Networks (ANNs) [10], [11], called Deep Neural Networks (DNNs) [12], [13], [14].", "startOffset": 132, "endOffset": 136}, {"referenceID": 9, "context": "One particularly good ML algorithm for working with large amounts of data is a specific subset of Artificial Neural Networks (ANNs) [10], [11], called Deep Neural Networks (DNNs) [12], [13], [14].", "startOffset": 179, "endOffset": 183}, {"referenceID": 10, "context": "One particularly good ML algorithm for working with large amounts of data is a specific subset of Artificial Neural Networks (ANNs) [10], [11], called Deep Neural Networks (DNNs) [12], [13], [14].", "startOffset": 185, "endOffset": 189}, {"referenceID": 11, "context": "One particularly good ML algorithm for working with large amounts of data is a specific subset of Artificial Neural Networks (ANNs) [10], [11], called Deep Neural Networks (DNNs) [12], [13], [14].", "startOffset": 191, "endOffset": 195}, {"referenceID": 12, "context": "DNNs work well with large sets of data because of their peculiar quality of being a universal approximator [15], [16], which means that theoretically speaking, they can model any real function.", "startOffset": 107, "endOffset": 111}, {"referenceID": 13, "context": "DNNs work well with large sets of data because of their peculiar quality of being a universal approximator [15], [16], which means that theoretically speaking, they can model any real function.", "startOffset": 113, "endOffset": 117}, {"referenceID": 14, "context": "Another key factor in Python\u2019s success in the DNN research space is due to it\u2019s portability and the large amount of scientific libraries that are actively being developed, such as Numpy [17], Scipy [18], Pandas [19], and Theano [20], [21].", "startOffset": 186, "endOffset": 190}, {"referenceID": 15, "context": "Another key factor in Python\u2019s success in the DNN research space is due to it\u2019s portability and the large amount of scientific libraries that are actively being developed, such as Numpy [17], Scipy [18], Pandas [19], and Theano [20], [21].", "startOffset": 211, "endOffset": 215}, {"referenceID": 16, "context": "Another key factor in Python\u2019s success in the DNN research space is due to it\u2019s portability and the large amount of scientific libraries that are actively being developed, such as Numpy [17], Scipy [18], Pandas [19], and Theano [20], [21].", "startOffset": 228, "endOffset": 232}, {"referenceID": 17, "context": "Another key factor in Python\u2019s success in the DNN research space is due to it\u2019s portability and the large amount of scientific libraries that are actively being developed, such as Numpy [17], Scipy [18], Pandas [19], and Theano [20], [21].", "startOffset": 234, "endOffset": 238}, {"referenceID": 18, "context": "Global Interpreter Lock: One of the major roadblocks for implementing massively parallel systems in Python in the existence of the Global Interpreter Lock (GIL) [22].", "startOffset": 161, "endOffset": 165}, {"referenceID": 19, "context": "Concretely, this means that means that no two Python threads can access a python object at the same time, which allows for concurrency, but not parallelism [23].", "startOffset": 156, "endOffset": 160}, {"referenceID": 20, "context": "This restriction greatly reduces Python\u2019s ability to utilize multiple CPUs [24].", "startOffset": 75, "endOffset": 79}, {"referenceID": 19, "context": "Python scientific libraries and their reliance on the GIL [23]", "startOffset": 58, "endOffset": 62}, {"referenceID": 21, "context": "\u2022 Best DNN layer design: Although some literature exists on the practical design of deep neural networks [25], [26], [27], designing deep neural networks remains a mix of following these guidelines and mastering the \u201cblack art\u201d of designing deep neural networks.", "startOffset": 105, "endOffset": 109}, {"referenceID": 22, "context": "\u2022 Best DNN layer design: Although some literature exists on the practical design of deep neural networks [25], [26], [27], designing deep neural networks remains a mix of following these guidelines and mastering the \u201cblack art\u201d of designing deep neural networks.", "startOffset": 111, "endOffset": 115}, {"referenceID": 23, "context": "\u2022 Best DNN layer design: Although some literature exists on the practical design of deep neural networks [25], [26], [27], designing deep neural networks remains a mix of following these guidelines and mastering the \u201cblack art\u201d of designing deep neural networks.", "startOffset": 117, "endOffset": 121}, {"referenceID": 3, "context": "If your dataset is larger than 1TB, the reader is encouraged to use a different system, such as Hadoop [5].", "startOffset": 103, "endOffset": 106}, {"referenceID": 24, "context": "\u2022 GPU acceleration: Although not true for every machine, some machines are equipped with graphic processing units (GPUs), which can be utilized to greatly decrease running time for training DNNs [28].", "startOffset": 195, "endOffset": 199}, {"referenceID": 25, "context": "Docker: Docker is a lightweight Linux container system that allows you to compile a specific Linux image once and deploy that image to multiple destinations with ease [29].", "startOffset": 167, "endOffset": 171}, {"referenceID": 26, "context": "For more information on configuring Docker or its capabilities, visit [30].", "startOffset": 70, "endOffset": 74}, {"referenceID": 27, "context": "CoreOS: CoreOS is a lightweight, flexible operating system that runs containers on top of it [31].", "startOffset": 93, "endOffset": 97}, {"referenceID": 28, "context": "In addition, the CoreOS team has built a painless distributed key-value store called etcd [32] that comes", "startOffset": 90, "endOffset": 94}, {"referenceID": 29, "context": "RabbitMQ: RabbitMQ is a robust message queue implementation that is easy, portable, and supports a number of messaging protocols, including the popular AMPQ protocol [33].", "startOffset": 166, "endOffset": 170}, {"referenceID": 30, "context": "Message queues [34] are crucial when building a distributed computing platform utilizing Python, because it provides a centralized pool of tasks for each of your worker machines to pull from.", "startOffset": 15, "endOffset": 19}, {"referenceID": 31, "context": "However, other similar task storing systems, such as Redis [35], are worth mentioning.", "startOffset": 59, "endOffset": 63}, {"referenceID": 32, "context": "MongoDB: MongoDB is a NoSQL database that offers easy storage and installation [36].", "startOffset": 79, "endOffset": 83}, {"referenceID": 33, "context": "A comprehensive comparison of databases and their trade offs can be found in [37].", "startOffset": 77, "endOffset": 81}, {"referenceID": 34, "context": "In this system, Flask acts as the middleman between the website and the database by exposing a REST api [39] (see \u201cSystem Design\u201d).", "startOffset": 104, "endOffset": 108}, {"referenceID": 14, "context": "NumPy, SciPy, Pandas: NumPy [17], SciPy [18], and Pandas [19] are the foundational scientific libraries upon which most other major Python libraries are built.", "startOffset": 28, "endOffset": 32}, {"referenceID": 15, "context": "NumPy, SciPy, Pandas: NumPy [17], SciPy [18], and Pandas [19] are the foundational scientific libraries upon which most other major Python libraries are built.", "startOffset": 57, "endOffset": 61}, {"referenceID": 16, "context": "Theano: Theano is a symbolic Python library that allows you to \u201cdefine, optimize, and evaluate mathematical expressions with multi-dimensional arrays efficiently\u201d [20], [21].", "startOffset": 163, "endOffset": 167}, {"referenceID": 17, "context": "Theano: Theano is a symbolic Python library that allows you to \u201cdefine, optimize, and evaluate mathematical expressions with multi-dimensional arrays efficiently\u201d [20], [21].", "startOffset": 169, "endOffset": 173}, {"referenceID": 35, "context": "PyBrain: PyBrain aims to offer a high performance neural network library without the complicated entry barrier that most users experience when trying to enter the field [41].", "startOffset": 169, "endOffset": 173}, {"referenceID": 36, "context": "Keras: Keras lives at the other end of the neural network library spectrum [42].", "startOffset": 75, "endOffset": 79}, {"referenceID": 37, "context": "Concretely, Docker has several precompiled containers available at [43] that you can base your image off of.", "startOffset": 67, "endOffset": 71}, {"referenceID": 37, "context": "As discussed earlier, setting up a RabbitMQ server is very easy using Docker - simply run the default RabbitMQ container provided at [43] with the web interface enabled.", "startOffset": 133, "endOffset": 137}, {"referenceID": 37, "context": "[43] that handles the running of the Flask/Celery Python microframework.", "startOffset": 0, "endOffset": 4}, {"referenceID": 21, "context": "1] [25].", "startOffset": 3, "endOffset": 7}, {"referenceID": 21, "context": "This creates a different output node in the DNN for each categorical answer and allows the DNN to output a probability that any given categorical answer is the correct one [25].", "startOffset": 172, "endOffset": 176}, {"referenceID": 38, "context": "Holding out some data for testing a DNN helps to alleviate overfitting [44].", "startOffset": 71, "endOffset": 75}, {"referenceID": 37, "context": "7 image [43] that ran an instance of a Celery worker.", "startOffset": 8, "endOffset": 12}, {"referenceID": 37, "context": "A default MongoDB container [43] was instantiated on the host machine,", "startOffset": 28, "endOffset": 32}, {"referenceID": 39, "context": "The reader is encouraged to look for alternatives outside of the Python programming language, such as NodeJS [45] or golang [46], to build their RESTful API, as Python\u2019s blocking nature presents major speed issues.", "startOffset": 109, "endOffset": 113}, {"referenceID": 40, "context": "The reader is encouraged to look for alternatives outside of the Python programming language, such as NodeJS [45] or golang [46], to build their RESTful API, as Python\u2019s blocking nature presents major speed issues.", "startOffset": 124, "endOffset": 128}, {"referenceID": 25, "context": "By utilizing several deployment tools (Docker [29], CoreOS [31]), production grade distributed solutions (RabbitMQ [33], MongoDB [36]), and the Python scientific ecosystem (NumPy [17], SciPy [18], Theano [20], [21], Keras [42], PyBrain [41]), such a system is easily implemented in a flexible way that prioritizes simplicity and modularity.", "startOffset": 46, "endOffset": 50}, {"referenceID": 27, "context": "By utilizing several deployment tools (Docker [29], CoreOS [31]), production grade distributed solutions (RabbitMQ [33], MongoDB [36]), and the Python scientific ecosystem (NumPy [17], SciPy [18], Theano [20], [21], Keras [42], PyBrain [41]), such a system is easily implemented in a flexible way that prioritizes simplicity and modularity.", "startOffset": 59, "endOffset": 63}, {"referenceID": 29, "context": "By utilizing several deployment tools (Docker [29], CoreOS [31]), production grade distributed solutions (RabbitMQ [33], MongoDB [36]), and the Python scientific ecosystem (NumPy [17], SciPy [18], Theano [20], [21], Keras [42], PyBrain [41]), such a system is easily implemented in a flexible way that prioritizes simplicity and modularity.", "startOffset": 115, "endOffset": 119}, {"referenceID": 32, "context": "By utilizing several deployment tools (Docker [29], CoreOS [31]), production grade distributed solutions (RabbitMQ [33], MongoDB [36]), and the Python scientific ecosystem (NumPy [17], SciPy [18], Theano [20], [21], Keras [42], PyBrain [41]), such a system is easily implemented in a flexible way that prioritizes simplicity and modularity.", "startOffset": 129, "endOffset": 133}, {"referenceID": 14, "context": "By utilizing several deployment tools (Docker [29], CoreOS [31]), production grade distributed solutions (RabbitMQ [33], MongoDB [36]), and the Python scientific ecosystem (NumPy [17], SciPy [18], Theano [20], [21], Keras [42], PyBrain [41]), such a system is easily implemented in a flexible way that prioritizes simplicity and modularity.", "startOffset": 179, "endOffset": 183}, {"referenceID": 16, "context": "By utilizing several deployment tools (Docker [29], CoreOS [31]), production grade distributed solutions (RabbitMQ [33], MongoDB [36]), and the Python scientific ecosystem (NumPy [17], SciPy [18], Theano [20], [21], Keras [42], PyBrain [41]), such a system is easily implemented in a flexible way that prioritizes simplicity and modularity.", "startOffset": 204, "endOffset": 208}, {"referenceID": 17, "context": "By utilizing several deployment tools (Docker [29], CoreOS [31]), production grade distributed solutions (RabbitMQ [33], MongoDB [36]), and the Python scientific ecosystem (NumPy [17], SciPy [18], Theano [20], [21], Keras [42], PyBrain [41]), such a system is easily implemented in a flexible way that prioritizes simplicity and modularity.", "startOffset": 210, "endOffset": 214}, {"referenceID": 36, "context": "By utilizing several deployment tools (Docker [29], CoreOS [31]), production grade distributed solutions (RabbitMQ [33], MongoDB [36]), and the Python scientific ecosystem (NumPy [17], SciPy [18], Theano [20], [21], Keras [42], PyBrain [41]), such a system is easily implemented in a flexible way that prioritizes simplicity and modularity.", "startOffset": 222, "endOffset": 226}, {"referenceID": 35, "context": "By utilizing several deployment tools (Docker [29], CoreOS [31]), production grade distributed solutions (RabbitMQ [33], MongoDB [36]), and the Python scientific ecosystem (NumPy [17], SciPy [18], Theano [20], [21], Keras [42], PyBrain [41]), such a system is easily implemented in a flexible way that prioritizes simplicity and modularity.", "startOffset": 236, "endOffset": 240}], "year": 2015, "abstractText": "In this paper, a framework for testing Deep Neural Network (DNN) design in Python is presented. First, big data, machine learning (ML), and Artificial Neural Networks (ANNs) are discussed to familiarize the reader with the importance of such a system. Next, the benefits and detriments of implementing such a system in Python are presented. Lastly, the specifics of the system are explained, and some experimental results are presented to prove the effectiveness of the system.", "creator": "LaTeX with hyperref package"}}}