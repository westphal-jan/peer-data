{"id": "1510.07526", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "26-Oct-2015", "title": "Empirical Study on Deep Learning Models for Question Answering", "abstract": "In this paper we explore deep learning models with memory component or attention mechanism for question answering task. We combine and compare three models, Neural Machine Translation, Neural Turing Machine, and Memory Networks for a simulated QA data set. This paper is the first one that uses Neural Machine Translation and Neural Turing Machines for solving QA tasks. Our results suggest that the combination of attention and memory have potential to solve certain QA problem.", "histories": [["v1", "Mon, 26 Oct 2015 16:03:27 GMT  (215kb,D)", "http://arxiv.org/abs/1510.07526v1", null], ["v2", "Tue, 27 Oct 2015 16:56:48 GMT  (215kb,D)", "http://arxiv.org/abs/1510.07526v2", null], ["v3", "Fri, 20 Nov 2015 15:36:56 GMT  (231kb,D)", "http://arxiv.org/abs/1510.07526v3", null]], "reviews": [], "SUBJECTS": "cs.CL cs.AI cs.LG", "authors": ["yang yu", "wei zhang", "chung-wei hang", "bing xiang", "bowen zhou"], "accepted": false, "id": "1510.07526"}, "pdf": {"name": "1510.07526.pdf", "metadata": {"source": "CRF", "title": "Empirical Study on Deep Learning Models for QA", "authors": ["Yang Yu", "Wei Zhang", "Chung-Wei Hang", "Bowen Zhou"], "emails": ["zhou}@us.ibm.com"], "sections": [{"heading": null, "text": "We combine and compare three models, Neural Machine Translation [1], Neural Turing Machine [5] and Memory Networks [15] for a simulated QA dataset [14]. This paper is the first to use Neural Machine Translation and Neural Turing Machines to solve QA tasks. Our results suggest that the combination of attention and memory has the potential to solve certain QA problems."}, {"heading": "1 Introduction", "text": "Answering questions (QA) is a task for processing natural language (NLP) that requires a deep understanding of semantic abstraction and reasoning about facts relevant to a question [6]. There are many different approaches to QA: constructing an NLP pipeline where each component is trained separately and then assembled [4], building large knowledge bases (KBs) [2] and reasoning with facts in it, and machine \"reading\" to understand questions and documents [6] that contain answers. Recently, various deep learning (DL) models have been proposed for different learning problems. DL models are usually distinguishable from end to end by gradient descent. They do not require any craftsmanship characteristics or separately tuned components. Therefore, we consider it important to study these models in order to address the QA problem. Implicitly or explicitly, solving QA problems can be divided into two steps. They do not require manual features or separately tuned components."}, {"heading": "2 Deep Learning Models for Question Answering", "text": "This year is the highest in the history of the country."}, {"heading": "3 Experiments", "text": "This year, the time has come for us to be able to look for a solution that is capable, that we are able to establish ourselves in the region."}, {"heading": "4 Conclusions", "text": "We examined several state-of-the-art DL models that have been proposed for different learning tasks to solve QA problems. Experimental results suggest that a good actor needs to remember and forget some facts when necessary and an external memory can be a choice. We also believe that adequate attention mechanisms to generate answers can be good. Therefore, we believe that a DL model that combines memory and attention mechanism has great potential in dealing with QA problems."}], "references": [{"title": "Neural machine translation by jointly learning to align and translate", "author": ["D. Bahdanau", "K. Cho", "Y. Bengio"], "venue": "arXiv preprint arXiv:1409.0473", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2014}, {"title": "Large-scale simple question answering with memory networks", "author": ["A. Bordes", "N. Usunier", "S. Chopra", "J. Weston"], "venue": "arXiv preprint arXiv:1506.02075", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2015}, {"title": "Question answering passage retrieval using dependency relations", "author": ["H. Cui", "R. Sun", "K. Li", "M.-Y. Kan", "T.-S. Chua"], "venue": "Proceedings of the 28th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, pages 400\u2013407, New York, NY, USA", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2005}, {"title": "Building watson: An overview of the deepqa project", "author": ["D.A. Ferrucci", "E.W. Brown", "J. Chu-Carroll", "J. Fan", "D. Gondek", "A. Kalyanpur", "A. Lally", "J.W. Murdock", "E. Nyberg", "J.M. Prager", "N. Schlaefer", "C.A. Welty"], "venue": "AI Magazine, 31(3):59\u201379", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2010}, {"title": "Neural turing machines", "author": ["A. Graves", "G. Wayne", "I. Danihelka"], "venue": "arXiv preprint arXiv:1410.5401", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2014}, {"title": "Teaching machines to read and comprehend", "author": ["K.M. Hermann", "T. Kocisk\u00fd", "E. Grefenstette", "L. Espeholt", "W. Kay", "M. Suleyman", "P. Blunsom"], "venue": "CoRR, abs/1506.03340", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2015}, {"title": "Recurrent continuous translation models", "author": ["N. Kalchbrenner", "P. Blunsom"], "venue": "EMNLP", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2013}, {"title": "Statistical phrase-based translation", "author": ["P. Koehn", "F.J. Och", "D. Marcu"], "venue": "Proceedings of the Human Language Technology Conference of the North American Chapter of the Association for Computational Linguistics, pages 48\u201354, Stroudsburg, PA, USA", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2003}, {"title": "Distributed representations of words and phrases and their compositionality", "author": ["T. Mikolov", "I. Sutskever", "K. Chen", "G.S. Corrado", "J. Dean"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2013}, {"title": "On the computational power of neural nets", "author": ["H.T. Siegelmann", "E.D. Sontag"], "venue": "Journal of Computer and System Sciences, 50(1):132\u2013150", "citeRegEx": "10", "shortCiteRegEx": null, "year": 1995}, {"title": "End-to-end memory networks", "author": ["S. Sukhbaatar", "A. Szlam", "J. Weston", "R. Fergus"], "venue": "arXiv preprint arXiv:1503.08895", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2015}, {"title": "Learning to rank answers to non-factoid questions from web collections", "author": ["M. Surdeanu", "M. Ciaramita", "H. Zaragoza"], "venue": "Computational Linguistics,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2011}, {"title": "Sequence to sequence learning with neural networks", "author": ["I. Sutskever", "O. Vinyals", "Q.V.V. Le"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2014}, {"title": "Towards AI-complete question answering: A set of prerequisite toy tasks", "author": ["J. Weston", "A. Bordes", "S. Chopra", "T. Mikolov"], "venue": "arXiv preprint arXiv:1502.05698", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2015}, {"title": "Memory networks", "author": ["J. Weston", "S. Chopra", "A. Bordes"], "venue": "arXiv preprint arXiv:1410.3916", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2014}], "referenceMentions": [{"referenceID": 0, "context": "We combine and compare three models, Neural Machine Translation [1], Neural Turing Machine [5], and Memory Networks [15] for a simulated QA data set [14].", "startOffset": 64, "endOffset": 67}, {"referenceID": 4, "context": "We combine and compare three models, Neural Machine Translation [1], Neural Turing Machine [5], and Memory Networks [15] for a simulated QA data set [14].", "startOffset": 91, "endOffset": 94}, {"referenceID": 14, "context": "We combine and compare three models, Neural Machine Translation [1], Neural Turing Machine [5], and Memory Networks [15] for a simulated QA data set [14].", "startOffset": 116, "endOffset": 120}, {"referenceID": 13, "context": "We combine and compare three models, Neural Machine Translation [1], Neural Turing Machine [5], and Memory Networks [15] for a simulated QA data set [14].", "startOffset": 149, "endOffset": 153}, {"referenceID": 5, "context": "Question Answering (QA) is a natural language processing (NLP) task that requires deep understanding of semantic abstraction and reasoning over facts that are relevant to a question [6].", "startOffset": 182, "endOffset": 185}, {"referenceID": 3, "context": "There are many different approaches to QA: constructing NLP pipeline where each component is separately trained and then assembled [4], building large knowledge bases (KBs) [2] and reasoning with facts therein, and machine \u201creading\u201d approach to comprehend question and documents [6] where answers are contained.", "startOffset": 131, "endOffset": 134}, {"referenceID": 1, "context": "There are many different approaches to QA: constructing NLP pipeline where each component is separately trained and then assembled [4], building large knowledge bases (KBs) [2] and reasoning with facts therein, and machine \u201creading\u201d approach to comprehend question and documents [6] where answers are contained.", "startOffset": 173, "endOffset": 176}, {"referenceID": 5, "context": "There are many different approaches to QA: constructing NLP pipeline where each component is separately trained and then assembled [4], building large knowledge bases (KBs) [2] and reasoning with facts therein, and machine \u201creading\u201d approach to comprehend question and documents [6] where answers are contained.", "startOffset": 279, "endOffset": 282}, {"referenceID": 14, "context": "MemNN MemNNs have been applied to QA [15, 11] and have shown promising results with different input transformation or model changes.", "startOffset": 37, "endOffset": 45}, {"referenceID": 10, "context": "MemNN MemNNs have been applied to QA [15, 11] and have shown promising results with different input transformation or model changes.", "startOffset": 37, "endOffset": 45}, {"referenceID": 14, "context": "The memory network described in [15] memorizes each fact in a memory slot and uses supporting facts for", "startOffset": 32, "endOffset": 36}, {"referenceID": 10, "context": "[11] described another version of the MemNN that can be trained end-to-end without knowing the supporting facts.", "startOffset": 0, "endOffset": 4}, {"referenceID": 2, "context": "Several previous works have used translation models to determine answers [3, 12].", "startOffset": 73, "endOffset": 80}, {"referenceID": 11, "context": "Several previous works have used translation models to determine answers [3, 12].", "startOffset": 73, "endOffset": 80}, {"referenceID": 6, "context": "NMT brings new approaches to machine translation, for example two recurrent neural network (RNN) models are proposed [7, 13].", "startOffset": 117, "endOffset": 124}, {"referenceID": 12, "context": "NMT brings new approaches to machine translation, for example two recurrent neural network (RNN) models are proposed [7, 13].", "startOffset": 117, "endOffset": 124}, {"referenceID": 7, "context": "[8]).", "startOffset": 0, "endOffset": 3}, {"referenceID": 0, "context": "[1] introduced an extension to the encoder-decoder model which uses bi-directional RNN and learns to align and translate jointly.", "startOffset": 0, "endOffset": 3}, {"referenceID": 4, "context": "NTM NTM [5] resembles Turing machines in that it could learn arbitrary procedure in theory.", "startOffset": 8, "endOffset": 11}, {"referenceID": 9, "context": "NTMs are essentially RNNs, which in turn are Turing-complete [10] and capable of encoding any computer program in theory, yet not always practical.", "startOffset": 61, "endOffset": 65}, {"referenceID": 8, "context": "NTM inputs are word distributed representations [9].", "startOffset": 48, "endOffset": 51}, {"referenceID": 13, "context": "The AI-Complete data [14] is a synthetic dataset designed for a set of AI tasks.", "startOffset": 21, "endOffset": 25}, {"referenceID": 14, "context": "MemNN [15] has shown promising results on AI-Complete data.", "startOffset": 6, "endOffset": 10}, {"referenceID": 13, "context": "We choose the MemNN with adaptive memory [14] as baseline (column a).", "startOffset": 41, "endOffset": 45}, {"referenceID": 10, "context": "Furthermore, when training data is sufficient, the accuracy is even comparable to the state-of-the-art accuracy (92%) from [11] where specialized features are added and tuned specifically for this data.", "startOffset": 123, "endOffset": 127}], "year": 2015, "abstractText": "In this paper we explore deep learning models with memory component or attention mechanism for question answering task. We combine and compare three models, Neural Machine Translation [1], Neural Turing Machine [5], and Memory Networks [15] for a simulated QA data set [14]. This paper is the first one that uses Neural Machine Translation and Neural Turing Machines for solving QA tasks. Our results suggest that the combination of attention and memory have potential to solve certain QA problem.", "creator": "LaTeX with hyperref package"}}}