{"id": "1510.07586", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "26-Oct-2015", "title": "Parser for Abstract Meaning Representation using Learning to Search", "abstract": "We develop a novel technique to parse English sentences into Abstract Meaning Representation (AMR) using SEARN, a Learning to Search approach, by modeling the concept and the relation learning in a unified framework. We evaluate our parser on multiple datasets from varied domains and show an absolute improvement of 2% to 6% over the state-of-the-art. Additionally we show that using the most frequent concept gives us a baseline that is stronger than the state-of-the-art for concept prediction. We plan to release our parser for public use.", "histories": [["v1", "Mon, 26 Oct 2015 18:34:16 GMT  (19kb)", "http://arxiv.org/abs/1510.07586v1", null]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["sudha rao", "yogarshi vyas", "hal daume iii", "philip resnik"], "accepted": false, "id": "1510.07586"}, "pdf": {"name": "1510.07586.pdf", "metadata": {"source": "CRF", "title": null, "authors": [], "emails": ["raosudha@cs.umd.edu,", "yogarshi@cs.umd.edu,", "hal@cs.umd.edu,", "resnik@umd.edu"], "sections": [{"heading": null, "text": "ar Xiv: 151 0.07 586v 1 [cs.C L] 26 Oct 201 5Parser for Abstract Meaning Representation using Learning to SearchSudha Rao1,3 \u0445, Yogarshi Vyas1,3 \u0445, Hal Daume \u0301 III1,3, Philip Resnik2,3 1Computer Science, 2Linguistics, 3UMIACSUniversity of Maryland raosudha @ cs.umd.edu, yogarshi @ cs.umd.edu, hal @ cs.umd.edu, resnik @ umd.eduAbstractWe are developing a novel technique for parsing English sentences in Abstract Meaning Representation (AMR) using SEARN, a Learning to Search approach, by modelling the concept and relationship learning in a uniform framework. We evaluate our parser using several sets of data from different areas and show an absolute improvement of 2% to 6% over the state of the art. Additionally, we show that the concept that most frequently provides us for publication is the state-of-art."}, {"heading": "1 Introduction", "text": "This is a rooted, directed, acyclic graph in which the nodes represent concepts (words, PropBank (Palmer et al., 2005) framesets or special keywords) and the edges represent relationships between these concepts. Figure 1 shows the complete AMR for a sample sentence. The key motivation behind the development of AMR was to have a comprehensive and diversified semantic formality that assembles the best insights from a variety of semantic annotations (such as named entities, co-references, semantic relationships, discourse connectivity, temporal entities, etc.) in a way that would allow the syntactic sentences to be merged into a variety of semantic annotations (such as named entities, semantic relationships, discourse-binding connectivities, time connectivities, etc.) in a way that would allow the time entities, time entities, etc., to be useful to their representation tasks (such as named entities, semantic relationships, discourse-binding connectivities, time-related entities, etc.)."}, {"heading": "2 Using SEARN", "text": "The task of generating an AMR in the face of a set is a structured prediction task in which the structure we are trying to predict is an individually rooted, connected diagram with concepts (nodes) and relationships (edges). In this work, we design an AMR parser that learns to predict this structure with SEARN. SEARN solves complex structured prediction problems by breaking them down into classification problems. It does this by dividing the structured output, y, into a sequence of decisions y1, y2, ym and then using a classifier to make predictions for each component, thereby enabling dependent predictions. We break down the AMR prediction problem into the three problems of predicting the AMR, predicting the root, and then predicting the relationships between the predicted concepts (explained in more detail in Section 3)."}, {"heading": "3 Methodology", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "3.1 Learning technique", "text": "We have the opportunity to be able to be able to be able to be able to be able to be able to show that we are able to behave, to put ourselves in a position, to put ourselves in a position, to put ourselves in a position, to put ourselves in a position, to put ourselves in a position, to put ourselves in a position, to put ourselves in a position, to put ourselves in a position, to put ourselves in a position, to put ourselves in a position, to put ourselves in a position, to put ourselves in a position, to put ourselves in a position, to put ourselves in a position where we are."}, {"heading": "3.3 Pruning the search space", "text": "In order to narrow the search space of our learning task and improve the quality of predictions, we use two observations about the nature of the edges of the AMR of a sentence and its dependency tree within our algorithm. First, we observe that a large fraction of the edges in the AMR of a sentence lies between concepts whose underlying ranges (more precisely, the words in these underlying ranges) lie within two edges of the respective sentence. This means that we refrain from invoking the predicted relationship function in algorithm 1 between the concepts ci and cj if each word in wi is three or more edges away from all the words in wj in the dependence tree of the sentence under consideration, and vice versa. This implies that there will be no relationship between the predicted AMR of this sentence and the predicted edge of this sentence. This does not affect the number of calls to predict the relationship in the worst case (n2 \u2212 n, for a sentence with fewer ranges), but rather that the number of calls in this sentence is necessarily large."}, {"heading": "3.4 Training on smaller sentences", "text": "For a set containing n spans, algorithm 1 must make n2 predictions at worst, and this can be inhibiting for large values of n. To deal with this, we use a parameter to specify a shortening of the length of a set (C), and use only sets whose length (number of spans) is less than or equal to C. This parameter can be varied based on the size of the training data and the distribution of the length of the sets in the training data. Setting a higher value of C causes the model to use more sets for training, but spend longer time, while lower values quickly train on fewer sets. In our experiments, a C value between 10 and 15 gave us the best balance between training time and the number of examples considered."}, {"heading": "4 Experiments and Results", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.1 Dataset and Method of Evaluation", "text": "We use the publicly available AMR Annotation Release 1.0 (LDC2014T12) corpus for our experiments. This corpus consists of datasets from various areas such as online discussion forums, blogs and newsgroups with about 13,000 AMR sentence pairs. Previous work has used only one of these datasets for evaluation (proxy), but we rate our parser on all of them. In addition, we also use the freely available AMRs for The Little Prince, (lp) 1, which comes from a more literary area. All datasets have a pre-defined training and test split (Table 4). As mentioned above (Sections 3.2 and 3.4), we use the entire training set to extract the candidate lists for concept prediction and relationship forecasting, but train our learning algorithm based on Jigs datasets (set-AMR pairs in the training data, which are achieved by selecting sets with less than a fixed number of spans)."}, {"heading": "4.2 Preprocessing", "text": "JAMR Aligner: The training data for AMR parsing consists of sentences paired with corresponding AMRs. To convert a raw sentence into a sequence of spans (as required by our algorithm), we get alignments between words in the sentence and concepts in AMR with the JAMR automatic aligner. The alignments obtained can be of three types (examples refer to Figure 1): \u2022 A single word aligned to a single concept: For example, the word \"read\" aligned to concept \"read01.\" \u2022 Span words aligned to a graph fragment: For example, span \"Stories from Nature\" aligned to the graph fragment \"name.\" This is usually done for named entities and multiword expressions such as those related to date and time. \u2022 A word aligned to a NULL concept: Most function words such as \"about,\" the, \"etc. are not aligned to a specific concept."}, {"heading": "4.3 Experiments", "text": "To train our model, we use SEARN as implemented in the Vowpal Wabbit Machine Learning Library (Langford et al., 2007; Daume \u0301 III et al., 2014).For each set of data, we conduct three types of experiments, which differ in how they obtain the concepts during the trial period. All use the approach described in Section 3.1 to predict relationships. \u2022 Oracle Concept - Use the true concept that matches each span. \u2022 1-Best Concept - Use the concept that matched the span most in the training data. \u2022 Fully Automatic - Use the concepts predicted with the approach described in Section 3.1."}, {"heading": "4.4 Connectivity", "text": "Algorithm 1 does not explicitly restrict the structure of AMR. Therefore, the predicted output may have separate components. Since we want the predicted AMR to be connected, we connect the separate components (if any) with the following heuristics. For each component, we find their roots (i.e. concepts without incoming relationships) and connect the components together by simply adding an edge from our predicted root to each of the component roots. To decide which edge between our predicted root and the root of a component should be used, we get the k-best list (as described in Section 3.2) between them and select the most common edge from this list."}, {"heading": "4.5 Acyclicity", "text": "The post-processing step described in the previous section ensures that the predicted AMRs are rooted, interconnected charts. However, an AMR is by definition also acyclic. We do not explicitly model this constraint within our learning framework. Nevertheless, we observe that only a very small number of AMRs predicted using our fully automatic approach have cycles in them. Of the total of 1,444 AMRs predicted in all test sets, less than 5% have cycles in them. Moreover, almost all the cycles predicted consist of only two nodes, i.e. both Rij and Rji have non-NO EDGE values for the ci and cj concepts. To obtain an acyclic graph, we can greedily select one of Rij or Rji without affecting parser performance."}, {"heading": "4.6 Results", "text": "Table 5 shows the result of the execution of our parser on all five sets of data. By executing our fully automated approach, we achieve an absolute improvement of about 2% to 6% on most sets of data compared to JAMR. Surprisingly, we see a large 21% improvement on the Online Discussion Forum Data Set (dfa). In all cases, our results indicate a more balanced output in terms of accuracy and recall compared to JAMR, with consistently higher recall rates. It should be noted that selecting the 1-best approach also delivers better results than JAMR. This indicates that the 1-best baseline is strong and may not be very easy to beat. To support this, we evaluate our concept predictions separately. The results are shown in Table 6. First, it should be noted that the transition from the fully learned concept prediction to the 1-best concept shows only a small (or in some cases no) decrease in performance, from an absolute 10% to a 2% improvement."}, {"heading": "5 Related work", "text": "Semantic representations and techniques for their analysis have a rich and diverse history. AMR itself is based on meaningful logic and neo-Davidsonian semantics (Davidson, 1967). AMR is not intended as an interlingua, but because of the various assumptions made in the creation of an AMR (dropping terms, functional words, morphology, etc.), it removes language-specific idiosyncrasies and interlingual representations (Dorr, 1992).The work of Zettlemoyer and Collins (Zettlemoyer and Collins, 2005) attempts various attempts to break sentences down into logical forms by commenting on raw sentences with such forms (Kate et al., 2005; Wong and Mooney, 2006).The work of Zettlemoyer and Collins (Zettlemoyer and Collins, 2005) attempts to map sentences in natural language in a lambdacalculus encoding manner."}, {"heading": "6 Conclusion and Future work", "text": "We have introduced a novel technique for parsing English sentences in AMR, using a search learning approach. We model the concept and relationship learning in a consistent framework using SEARN, which allows us to optimize the loss of the entire predicted output. We evaluate our parser on several sets of data from different areas and show that our parser performs better across all sets of data than the current state of the art. We also show that a simple method of selecting the most common concept provides us with a baseline that is better than the current state of the concept prediction. Currently, we are ensuring different characteristics of AMR, such as connectivity and activity using heuristics, and in the future we plan to include these as limitations in our learning technique."}], "references": [{"title": "Abstract meaning representation for sembanking", "author": ["Claire Bonial", "Shu Cai", "Madalina Georgescu", "Kira Griffitt", "Ulf Hermjakob", "Kevin Knight", "Philipp Koehn", "Martha Palmer", "Nathan Schneider"], "venue": null, "citeRegEx": "Banarescu et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Banarescu et al\\.", "year": 2013}, {"title": "Smatch: an evaluation metric for semantic", "author": ["Cai", "Knight2013] Shu Cai", "Kevin Knight"], "venue": null, "citeRegEx": "Cai et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Cai et al\\.", "year": 2013}, {"title": "Incremental parsing with the perceptron algorithm", "author": ["Collins", "Roark2004] Michael Collins", "Brian Roark"], "venue": "In Proceedings of the 42nd Annual Meeting on Association for Computational Linguistics,", "citeRegEx": "Collins et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Collins et al\\.", "year": 2004}, {"title": "Learning as search optimization: Approximate large margin methods for structured prediction", "author": ["III Daum\u00e9", "III Marcu2005] Hal Daum\u00e9", "Marcu. Daniel"], "venue": "In Proceedings of the 22nd international conference on Machine learning,", "citeRegEx": "Daum\u00e9 et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Daum\u00e9 et al\\.", "year": 2005}, {"title": "Search-based structured prediction", "author": ["John Langford", "Daniel Marcu"], "venue": "Machine learning,", "citeRegEx": "III et al\\.,? \\Q2009\\E", "shortCiteRegEx": "III et al\\.", "year": 2009}, {"title": "Efficient programmable learning to search. arXiv preprint arXiv:1406.1837", "author": ["John Langford", "Stephane Ross"], "venue": null, "citeRegEx": "III et al\\.,? \\Q2014\\E", "shortCiteRegEx": "III et al\\.", "year": 2014}, {"title": "The logical form of action sentences", "author": ["Donald Davidson"], "venue": null, "citeRegEx": "Davidson.,? \\Q1967\\E", "shortCiteRegEx": "Davidson.", "year": 1967}, {"title": "Output space search for structured prediction", "author": ["Alan Fern", "Prasad Tadepalli"], "venue": "arXiv preprint arXiv:1206.6460", "citeRegEx": "Doppa et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Doppa et al\\.", "year": 2012}, {"title": "The use of lexical semantics in interlingual machine translation", "author": ["Bonnie J Dorr"], "venue": "Machine Translation,", "citeRegEx": "Dorr.,? \\Q1992\\E", "shortCiteRegEx": "Dorr.", "year": 1992}, {"title": "A discriminative graph-based parser for the abstract meaning representation", "author": ["Sam Thomson", "Jaime G. Carbonell", "Chris Dyer", "Noah A. Smith"], "venue": "In Proceedings of the 52nd Annual Meeting of the Association", "citeRegEx": "Flanigan et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Flanigan et al\\.", "year": 2014}, {"title": "Semantic role labeling via framenet, verbnet and propbank", "author": ["Giuglea", "Moschitti2006] Ana-Maria Giuglea", "Alessandro Moschitti"], "venue": "ACL", "citeRegEx": "Giuglea et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Giuglea et al\\.", "year": 2006}, {"title": "Dynamic feature selection for dependency parsing", "author": ["He et al.2013] He He", "Hal Daum\u00e9 III", "Jason Eisner"], "venue": "In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing,", "citeRegEx": "He et al\\.,? \\Q2013\\E", "shortCiteRegEx": "He et al\\.", "year": 2013}, {"title": "Structured perceptron with inexact search", "author": ["Huang et al.2012] Liang Huang", "Suphan Fayong", "Yang Guo"], "venue": "In Proceedings of the 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Tech-", "citeRegEx": "Huang et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Huang et al\\.", "year": 2012}, {"title": "Learning to transform natural to formal languages", "author": ["Kate et al.2005] Rohit J. Kate", "Yuk Wah Wong", "Raymond J. Mooney"], "venue": "In Proceedings,", "citeRegEx": "Kate et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Kate et al\\.", "year": 2005}, {"title": "Conditional random fields: Probabilistic models for segmenting and labeling sequence data", "author": ["Andrew McCallum", "Fernando CN Pereira"], "venue": null, "citeRegEx": "Lafferty et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Lafferty et al\\.", "year": 2001}, {"title": "Vowpal wabbit online learning project", "author": ["Lihong Li", "Alexander Strehl"], "venue": null, "citeRegEx": "Langford et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Langford et al\\.", "year": 2007}, {"title": "Prune-andscore: Learning for greedy coreference resolution", "author": ["Ma et al.2014] Chao Ma", "Janardhan Rao Doppa", "J Walker Orr", "Prashanth Mannem", "Xiaoli Fern", "Tom Dietterich", "Prasad Tadepalli"], "venue": "In Proceedings of Conference on Empirical Methods", "citeRegEx": "Ma et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Ma et al\\.", "year": 2014}, {"title": "The proposition bank: An annotated corpus of semantic roles", "author": ["Palmer et al.2005] Martha Palmer", "Paul Kingsbury", "Daniel Gildea"], "venue": null, "citeRegEx": "Palmer et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Palmer et al\\.", "year": 2005}, {"title": "Boosting structured prediction for imitation learning", "author": ["David Bradley", "J Andrew Bagnell", "Joel Chestnutt"], "venue": "Robotics Institute,", "citeRegEx": "Ratliff et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Ratliff et al\\.", "year": 2007}, {"title": "Reinforcement and imitation learning via interactive no-regret learning", "author": ["Ross", "Bagnell2014] Stephane Ross", "J Andrew Bagnell"], "venue": "arXiv preprint arXiv:1406.5979", "citeRegEx": "Ross et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Ross et al\\.", "year": 2014}, {"title": "A reduction of imitation learning and structured prediction to no-regret online learning", "author": ["Ross et al.2011] St\u00e9phane Ross", "Geoff J. Gordon", "J. Andrew Bagnell"], "venue": "In Proceedings of the Workshop on Artificial Intelligence and Statistics (AIStats)", "citeRegEx": "Ross et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Ross et al\\.", "year": 2011}, {"title": "Verbnet: A broad-coverage, comprehensive verb lexicon", "author": ["Karin Kipper Schuler"], "venue": null, "citeRegEx": "Schuler.,? \\Q2005\\E", "shortCiteRegEx": "Schuler.", "year": 2005}, {"title": "A reduction from apprenticeship learning to classification", "author": ["Syed", "Schapire2010] Umar Syed", "Robert E Schapire"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Syed et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Syed et al\\.", "year": 2010}, {"title": "A transitionbased algorithm for amr parsing", "author": ["Wang et al.2015] Chuan Wang", "Nianwen Xue", "Sameer Pradhan"], "venue": null, "citeRegEx": "Wang et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2015}, {"title": "Learning for semantic parsing with statistical machine translation", "author": ["Wong", "Mooney2006] Yuk Wah Wong", "Raymond J. Mooney"], "venue": "In Human Language Technology Conference of the North American Chapter of the Association of Compu-", "citeRegEx": "Wong et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Wong et al\\.", "year": 2006}, {"title": "On learning linear ranking functions for beam search", "author": ["Xu", "Fern2007] Yuehua Xu", "Alan Fern"], "venue": "In Proceedings of the 24th international conference on Machine learning,", "citeRegEx": "Xu et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Xu et al\\.", "year": 2007}, {"title": "Discriminative learning of beamsearch heuristics for planning", "author": ["Xu et al.2007] Yuehua Xu", "Alan Fern", "Sung Wook Yoon"], "venue": "In IJCAI,", "citeRegEx": "Xu et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Xu et al\\.", "year": 2007}, {"title": "Learning to map sentences to logical form: Structured classification with probabilistic categorial grammars", "author": ["Zettlemoyer", "Collins2005] Luke S. Zettlemoyer", "Michael Collins"], "venue": "In UAI \u201905, Proceedings of the 21st Conference in Uncertainty", "citeRegEx": "Zettlemoyer et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Zettlemoyer et al\\.", "year": 2005}], "referenceMentions": [{"referenceID": 0, "context": "Abstract Meaning Representation (Banarescu et al., 2013) is a semantic representation which is a rooted, directed, acyclic graph where the nodes represent concepts (words, PropBank (Palmer et al.", "startOffset": 32, "endOffset": 56}, {"referenceID": 17, "context": ", 2013) is a semantic representation which is a rooted, directed, acyclic graph where the nodes represent concepts (words, PropBank (Palmer et al., 2005) framesets or special keywords) and the edges represent relations between these concepts.", "startOffset": 132, "endOffset": 153}, {"referenceID": 23, "context": "The other approach (Wang et al., 2015) uses a transition-based algorithm to convert the dependency representation of a sentence to its AMR.", "startOffset": 19, "endOffset": 38}, {"referenceID": 9, "context": "Flanigan et al. (2014) use a semi-Markov model to first identify the concepts, and then find a maximum spanning connected subgraph that defines the relations between these concepts.", "startOffset": 0, "endOffset": 23}, {"referenceID": 16, "context": ", 2014), and for even more complex structured prediction tasks like coreference resolution (Ma et al., 2014) and dependency parsing (He et al.", "startOffset": 91, "endOffset": 108}, {"referenceID": 11, "context": ", 2014) and dependency parsing (He et al., 2013).", "startOffset": 31, "endOffset": 48}, {"referenceID": 21, "context": "If si has not been seen in the training data, CL-CONsi consists of the lemmatized span, PropBank frames (for verbs) obtained using the Unified Verb Index (Schuler, 2005) and the NULL concept.", "startOffset": 154, "endOffset": 169}, {"referenceID": 14, "context": "First, we detect possible multiword spans corresponding to named entities, using a named entity recognizer (Lafferty et al., 2001).", "startOffset": 107, "endOffset": 130}, {"referenceID": 15, "context": "To train our model, we use SEARN as implemented in the Vowpal Wabbit machine learning library (Langford et al., 2007; Daum\u00e9 III et al., 2014).", "startOffset": 94, "endOffset": 141}, {"referenceID": 6, "context": "AMR itself is based on propositional logic and neoDavidsonian semantics (Davidson, 1967).", "startOffset": 72, "endOffset": 88}, {"referenceID": 8, "context": "), it does away with languagespecific idiosyncrasies and interlingual representations (Dorr, 1992) are thus, important predecessors to AMR.", "startOffset": 86, "endOffset": 98}, {"referenceID": 13, "context": "Like the task of AMR parsing, there have been various attempts to parse sentences into a logical form, given raw sentences annotated with such forms (Kate et al., 2005; Wong and Mooney, 2006).", "startOffset": 149, "endOffset": 191}, {"referenceID": 17, "context": "AMR aims to combine various semantic annotations to produce a unified annotation, but it mainly builds on top of PropBank (Palmer et al., 2005).", "startOffset": 122, "endOffset": 143}, {"referenceID": 12, "context": "Incremental structured perceptron (Collins and Roark, 2004; Huang et al., 2012), DAGGER (Ross et al.", "startOffset": 34, "endOffset": 79}, {"referenceID": 20, "context": ", 2012), DAGGER (Ross et al., 2011), AGGREVATE (Ross and Bagnell, 2014), etc.", "startOffset": 16, "endOffset": 35}, {"referenceID": 25, "context": "(Daum\u00e9 III and Marcu, 2005; Xu and Fern, 2007; Xu et al., 2007; Ratliff et al., 2007; Syed and Schapire, 2010; Doppa et al., 2012) are other algorithms that also belong to this family.", "startOffset": 0, "endOffset": 130}, {"referenceID": 18, "context": "(Daum\u00e9 III and Marcu, 2005; Xu and Fern, 2007; Xu et al., 2007; Ratliff et al., 2007; Syed and Schapire, 2010; Doppa et al., 2012) are other algorithms that also belong to this family.", "startOffset": 0, "endOffset": 130}, {"referenceID": 7, "context": "(Daum\u00e9 III and Marcu, 2005; Xu and Fern, 2007; Xu et al., 2007; Ratliff et al., 2007; Syed and Schapire, 2010; Doppa et al., 2012) are other algorithms that also belong to this family.", "startOffset": 0, "endOffset": 130}], "year": 2015, "abstractText": "We develop a novel technique to parse English sentences into Abstract Meaning Representation (AMR) using SEARN, a Learning to Search approach, by modeling the concept and the relation learning in a unified framework. We evaluate our parser on multiple datasets from varied domains and show an absolute improvement of 2% to 6% over the state-of-the-art. Additionally we show that using the most frequent concept gives us a baseline that is stronger than the state-of-the-art for concept prediction. We plan to release our parser for public use.", "creator": "LaTeX with hyperref package"}}}