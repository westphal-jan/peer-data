{"id": "1510.08418", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "28-Oct-2015", "title": "Fast k-best Sentence Compression", "abstract": "A popular approach to sentence compression is to formulate the task as a constrained optimization problem and solve it with integer linear programming (ILP) tools. Unfortunately, dependence on ILP may make the compressor prohibitively slow, and thus approximation techniques have been proposed which are often complex and offer a moderate gain in speed. As an alternative solution, we introduce a novel compression algorithm which generates k-best compressions relying on local deletion decisions. Our algorithm is two orders of magnitude faster than a recent ILP-based method while producing better compressions. Moreover, an extensive evaluation demonstrates that the quality of compressions does not degrade much as we move from single best to top-five results.", "histories": [["v1", "Wed, 28 Oct 2015 19:04:30 GMT  (86kb,D)", "http://arxiv.org/abs/1510.08418v1", "11 pages"]], "COMMENTS": "11 pages", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["katja filippova", "enrique alfonseca"], "accepted": false, "id": "1510.08418"}, "pdf": {"name": "1510.08418.pdf", "metadata": {"source": "CRF", "title": "Fast k-best Sentence Compression", "authors": ["Katja Filippova", "Enrique Alfonseca"], "emails": ["katjaf@google.com", "ealfonseca@google.com"], "sections": [{"heading": "1 Introduction", "text": "There has been an increase in the number of sentences in research over the last decade due to the promise it holds and the usefulness it brings in the age of small-screen mobile devices. Similar to text summaries, the question for other methods is how the terms can be deleted. Considering the complexity of the compression task (the number of possible outputs is exponential), the biggest challenge is to know what terms and expressions each term can be deleted without adversely affecting the sentence's content and grammar."}, {"heading": "2 The Top-down Approach", "text": "Our approach is syntax-oriented and operates on dependency trees (paragraph 2.1); the input tree is trimmed to obtain a valid partial tree from which to read a compression; the pruning decisions are made on the basis of the predictions of a maximum entropy classifier trained on a parallel corpus with a rich set of features (paragraph 2.2); Section 2.3 explains how to generate the individual top scoring compression; Section 2.4 extends the idea to any k."}, {"heading": "2.1 Preprocessing", "text": "Unsurprisingly, working with function words is much easier than deciding whether a syllable can be removed. Approaches that use a constituency parser and cut off constituent edges implicitly handle function words (Berg-Kirkpatrick et al., 2011; Wang et al., 2013). Approaches that use dependency representation either formulate hard constraints (Almeida & Martins, 2013) or collapse function words with their heads. We use the latter approach and transform each input tree (Nivre, 2006) according to Filippova & Strube (2008) and add edges from the dumb root to finite verbs. Finally, we introduce an entity tagger and collapse node referring to entities. Figure 1 provides an example of a transformed tree with additional edges from the dumb root node and an undirected counterpart for the following sentence we refer to in this section: The man was arrested on Friday."}, {"heading": "2.2 Estimating deletion probabilities", "text": "The monitored component of our system is a binary maximum entropy classifier (Berger et al., 1996), which is trained to estimate the probability of deleting an edge given its local context. In the following, we will refer to two probabilities, pdel (e) and pret (e): pdel (en, m) + pret (en, m) = 1, (1) where del stands for deletion, ret stands for holding the edge e from node n to node m, and pdel (en, m) is used with MaxEnt.The features we use are inspired by most recent work (Almeida & Martins, 2013; Filippova & Altun, 2013) and are as follows: syntactically: edge labels for the child and its siblings; NE types and PoS tags; lexical: head and child lemmas; negation; consolidation: consolidation of parent children's function and word length; we do not need a string length for words;"}, {"heading": "2.3 Obtaining top-scoring compression", "text": "In order to find the best results, we must give the best results to each node nwhose children areM = {m1, 2, 2, 3, 4, 5, 5, 5, 5, 5, 5, 5, 5, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 9, 9, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 9, 9, 9, 9, 9, 9, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11,"}, {"heading": "3 Adding a Node Subset Scorer", "text": "In the first run, the top-down compressor tries to find the best possible subset for each node by looking at each child individually and keeping or deleting the decisions independently of each other. How conservative or aggressive the algorithm is is is determined by a single parameter, but it sets a boundary between the two decisions. At lower class A values, a low probability of deletion (pdel) would be enough to remove a node child. Conversely, a higher class B value would mean that only children that the classifier is fairly certain about need to be deleted would be removed. Unsurprisingly, the class B value is to be optimized, as it is too low or too high depending on the node. While a child that might fall would not result in an unrammatic sentence that omitted an important argument that could render the compression incomprehensible."}, {"heading": "4 Evaluation", "text": "The purpose of the evaluation is to validate the following two hypotheses when comparing the new algorithm with a competitive ILP-based typesetting compressor (Filippova & Altun, 2013): 1. The top-down algorithm is designed to make local decisions on each node of the parse tree compared to global optimization by the ILP-based compressor; 2. We would like to verify whether the local model can achieve similar levels of accuracy or even exceed the global model, not only for the individual best, but for the best k results; 2. Automatic ILP optimization can be quite slow when the number of candidates that need to be evaluated for a given input is large; and 2. We want to quantify the speed that can be achieved without loss of accuracy by making simpler, local decisions in the input parse tree."}, {"heading": "4.1 Evaluation settings", "text": "The training set consists of 1,800,000 points, each of which consists of two elements: the first sentence of a news article and an extractive compression achieved by matching words from the sentence with those from the headline (see Filippova & Altun (2013) for technical details.) Part of this sentence was offered for evaluation and development of the classifiers; for testing, we use the data set from Filippova & Altun (2013) 1. This test set contains 10,000 points, each of which contains the original sentence and the extractive compression and the URL of the source document; from this sentence, we used only the first 1,000 points, with the remaining 9,000 points unseen, reserved for possible future experiments."}, {"heading": "19.5% 40.6% 31.2% 7.9% 1%", "text": "We trained the compressor with the same characteristics as our model (Sec. 2.2) on the same training data using an average perceptron (Collins, 2002). To make this system comparable to ours, we did not equip the ILP decoder with the oracle compression length during training, so the model learned to generate compression in the sense of the length parameter. Therefore, both methods accept the same Input1http: / / storage.googleapis.com / sentencecomp / compressiondata.jsonand are comparable."}, {"heading": "4.2 Automatic evaluation", "text": "In order to measure the quality of the two classifiers (MaxEnt from Sec. 2,2 and perceptron from Sec. 3), we performed an initial, direct evaluation of each of them on a small portion of the training set. The MaxEnt classifier predicts the probability of erasing an edge and gives a score between zero and one. Figure 3 shows precision, retrieval and F1 score at different thresholds. The highest F1 score is obtained at 0.45. Regarding the perctron classifier, which predicts the number of children we should keep for each node, its accuracy and precision per class and recall values are shown in Table 2. For an automatic evaluation of the quality of sentence compression, we followed the same approach as (Riezler et al., 2003; Filippova & Altun, 2013) and measured F1 score by comparing the trees of compression generated with the golden extractive compression. Table 3 shows the top variant results of the IL4P between the two top scores below."}, {"heading": "4.3 Manual evaluation", "text": "The first 100 items in the test data were manually evaluated by humans. We asked the rating agencies to rate both the legibility and informativeness of the compressions for the Golden Output, the Baseline and our systems.2 For both metrics, a 5-point Likert scale was used, and three ratings were collected for each item. Note that in a human assessment between ILP and TOP-DOWN (+ NSS), the baseline has an advantage because (1) it cuts less aggressively and therefore has a greater chance of producing grammatically correct and informative results, and (2) it receives an indication of the optimal compression length in the edges. We used the IntraClass correlation (ICC) (Shrout & Fleiss, 1979; 2The rating template and the rated sentences are also in the Supplementary Material.Cicchetti, 1994) as a measure of interpretation match."}, {"heading": "4.4 Efficiency", "text": "The average processing time per set is 32,074 microseconds (Intel Xeon 2.67 GHz CPU) for ILP, 929 for TOP-DOWN + NSS, and 678 for TOP-DOWN. This means that we have achieved almost a 50-fold increase in performance over ILP. Figure 4 shows the processing time for each of the 1,000 sets in the test set with the record length measured in tokens. Conversely, the average time for each of the top 5 results decreases to 143 microseconds when using TOP-DOWN, and 195 microseconds when using TOP-DOWN + NSS, which is a 300-fold improvement. Conversely, the average time for each of the top 5 results decreases to 143 microseconds when using TOP-DOWN, and 195 microseconds when using TOP-DOWN + NSS."}, {"heading": "5 Conclusions", "text": "We presented a fast and precise supervised algorithm for generating the k-best compression of a set. Compared to a competitive ILP-based system, our method is 50x faster in generating the best result and 300x faster in subsequent k-best compressions. It is better in terms of both readability and informativeness. Furthermore, an evaluation with human evaluators shows that the quality of the results for the top 5 results remains high."}], "references": [{"title": "Jointly learning to extract and compress", "author": ["T. Berg-Kirkpatrick", "D. Gillick", "D. Klein"], "venue": "In Proc. of ACL-11", "citeRegEx": "Berg.Kirkpatrick et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Berg.Kirkpatrick et al\\.", "year": 2011}, {"title": "A maximum entropy approach to natural language processing", "author": ["A. Berger", "S.A. Della Pietra", "V.J. Della Pietra"], "venue": null, "citeRegEx": "Berger et al\\.,? \\Q1996\\E", "shortCiteRegEx": "Berger et al\\.", "year": 1996}, {"title": "Guidelines, criteria, and rules of thumb for evaluating normed and standardized assessment instruments in psychology", "author": ["D.V. Cicchetti"], "venue": "Psychological Assessment,", "citeRegEx": "Cicchetti,? \\Q1994\\E", "shortCiteRegEx": "Cicchetti", "year": 1994}, {"title": "Constraint-based sentence compression: An integer programming approach", "author": ["J. Clarke", "M. Lapata"], "venue": "In Proc. of COLING-ACL-06 Poster Session,", "citeRegEx": "Clarke and Lapata,? \\Q2006\\E", "shortCiteRegEx": "Clarke and Lapata", "year": 2006}, {"title": "Global inference for sentence compression: An integer linear programming approach", "author": ["J. Clarke", "M. Lapata"], "venue": "Journal of Artificial Intelligence Research,", "citeRegEx": "Clarke and Lapata,? \\Q2008\\E", "shortCiteRegEx": "Clarke and Lapata", "year": 2008}, {"title": "Discriminative training methods for Hidden Markov Models: Theory and experiments with perceptron algorithms", "author": ["M. Collins"], "venue": "In Proc. of EMNLP-02,", "citeRegEx": "Collins,? \\Q2002\\E", "shortCiteRegEx": "Collins", "year": 2002}, {"title": "Overcoming the lack of parallel data in sentence compression", "author": ["K. Filippova", "Y. Altun"], "venue": "In Proc. of EMNLP-13,", "citeRegEx": "Filippova and Altun,? \\Q2013\\E", "shortCiteRegEx": "Filippova and Altun", "year": 2013}, {"title": "Dependency tree based sentence compression", "author": ["K. Filippova", "M. Strube"], "venue": "In Proc. of INLG08,", "citeRegEx": "Filippova and Strube,? \\Q2008\\E", "shortCiteRegEx": "Filippova and Strube", "year": 2008}, {"title": "An extractive supervised two-stage method for sentence compression", "author": ["D. Galanis", "I. Androutsopoulos"], "venue": "In Proc. of NAACL-HLT-10,", "citeRegEx": "Galanis and Androutsopoulos,? \\Q2010\\E", "shortCiteRegEx": "Galanis and Androutsopoulos", "year": 2010}, {"title": "Lexicalized Markov grammars for sentence compression", "author": ["M. Galley", "K.R. McKeown"], "venue": "In Proc. of NAACL-HLT-07,", "citeRegEx": "Galley and McKeown,? \\Q2007\\E", "shortCiteRegEx": "Galley and McKeown", "year": 2007}, {"title": "Producing intelligent telegraphic text reduction to provide an audio scanning service for the blind", "author": ["G. Grefenstette"], "venue": "In Working Notes of the Workshop on Intelligent Text Summarization, Palo Alto, Cal.,", "citeRegEx": "Grefenstette,? \\Q1998\\E", "shortCiteRegEx": "Grefenstette", "year": 1998}, {"title": "Better k-best parsing", "author": ["L. Huang", "D. Chiang"], "venue": "Technical Report MS-CIS-05-08: University of Pennsylvania", "citeRegEx": "Huang and Chiang,? \\Q2005\\E", "shortCiteRegEx": "Huang and Chiang", "year": 2005}, {"title": "Cut and paste based text summarization", "author": ["H. Jing", "K. McKeown"], "venue": "In Proc. of NAACL-00,", "citeRegEx": "Jing and McKeown,? \\Q2000\\E", "shortCiteRegEx": "Jing and McKeown", "year": 2000}, {"title": "Statistics-based summarization \u2013 step one: Sentence compression", "author": ["K. Knight", "D. Marcu"], "venue": "In Proc. of AAAI-00,", "citeRegEx": "Knight and Marcu,? \\Q2000\\E", "shortCiteRegEx": "Knight and Marcu", "year": 2000}, {"title": "Summarization with a joing model for sentence extraction and compression", "author": ["A.F.T. Martins", "N.A. Smith"], "venue": "In ILP for", "citeRegEx": "Martins and Smith,? \\Q2009\\E", "shortCiteRegEx": "Martins and Smith", "year": 2009}, {"title": "Discriminative sentence compression with soft syntactic evidence", "author": ["R. McDonald"], "venue": "In Proc. of EACL-06,", "citeRegEx": "McDonald,? \\Q2006\\E", "shortCiteRegEx": "McDonald", "year": 2006}, {"title": "Evaluating sentence compression: Pitfalls and suggested remedies", "author": ["C. Napoles", "C. Callison-Burch", "B. Van Durme"], "venue": "In Proceedings of the Workshop on Monolingual Text-to-text Generation,", "citeRegEx": "Napoles et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Napoles et al\\.", "year": 2011}, {"title": "Inductive Dependency Parsing", "author": ["J. Nivre"], "venue": null, "citeRegEx": "Nivre,? \\Q2006\\E", "shortCiteRegEx": "Nivre", "year": 2006}, {"title": "A comparison of model free versus model intensive approaches to sentence compression", "author": ["T. Nomoto"], "venue": "In Proc. of EMNLP-09,", "citeRegEx": "Nomoto,? \\Q2009\\E", "shortCiteRegEx": "Nomoto", "year": 2009}, {"title": "Fast joint compression and summarization via graph cuts", "author": ["X. Qian", "Y. Liu"], "venue": "In Proc. of EMNLP-13,", "citeRegEx": "Qian and Liu,? \\Q2013\\E", "shortCiteRegEx": "Qian and Liu", "year": 2013}, {"title": "Statistical sentence condensation using ambiguity packing and stochastic disambiguation methods for Lexical-Functional Grammar", "author": ["S. Riezler", "T.H. King", "R. Crouch", "A. Zaenen"], "venue": "In Proc. of HLT-NAACL-03,", "citeRegEx": "Riezler et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Riezler et al\\.", "year": 2003}, {"title": "Intraclass correlations: Uses in assessing rater reliability", "author": ["P.E. Shrout", "J.L. Fleiss"], "venue": "Psychological bulletin,", "citeRegEx": "Shrout and Fleiss,? \\Q1979\\E", "shortCiteRegEx": "Shrout and Fleiss", "year": 1979}, {"title": "Approximating strategies for multi-structure sentence compression", "author": ["K. Thadani"], "venue": "In Proc. of ACL-14,", "citeRegEx": "Thadani,? \\Q2014\\E", "shortCiteRegEx": "Thadani", "year": 2014}, {"title": "Sentence compression with joint structural inference", "author": ["K. Thadani", "K. McKeown"], "venue": "In Proc. of CoNLL-13,", "citeRegEx": "Thadani and McKeown,? \\Q2013\\E", "shortCiteRegEx": "Thadani and McKeown", "year": 2013}, {"title": "A Bayesian model for unsupervised semantic parsing", "author": ["I. Titov", "A. Klementiev"], "venue": "In Proc. of ACL-11,", "citeRegEx": "Titov and Klementiev,? \\Q2011\\E", "shortCiteRegEx": "Titov and Klementiev", "year": 2011}, {"title": "The Pythy summarization system: Microsoft Research at DUC", "author": ["K. Toutanova", "C. Brockett", "M. Gamon", "J. Jagarlamundi", "H. Suzuki", "L. Vanderwende"], "venue": "In Proc. of DUC-07", "citeRegEx": "Toutanova et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Toutanova et al\\.", "year": 2007}, {"title": "A sentence compression based framework to query-focused multidocument summarization", "author": ["L. Wang", "H. Raghavan", "V. Castelli", "R. Florian", "C. Cardie"], "venue": "In Proc. of ACL-13,", "citeRegEx": "Wang et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2013}, {"title": "Title generation with Quasi-Synchronous Grammar", "author": ["K. Woodsend", "Y. Feng", "M. Lapata"], "venue": "In Proc. of EMNLP-10,", "citeRegEx": "Woodsend et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Woodsend et al\\.", "year": 2010}, {"title": "Multiple aspect summarization using Integer Linear Programming", "author": ["K. Woodsend", "M. Lapata"], "venue": "In Proc. of EMNLP-12,", "citeRegEx": "Woodsend and Lapata,? \\Q2012\\E", "shortCiteRegEx": "Woodsend and Lapata", "year": 2012}], "referenceMentions": [{"referenceID": 0, "context": "Given the complexity of the compression task (the number of possible outputs is exponential), many systems frame it, sometimes combined with summarization, as an ILP problem which is then solved with off-the-shelf tools (Martins & Smith, 2009; Berg-Kirkpatrick et al., 2011; Thadani & McKeown, 2013).", "startOffset": 220, "endOffset": 299}, {"referenceID": 26, "context": "Importantly, approximate search techniques like beam search (Galanis & Androutsopoulos, 2010; Wang et al., 2013), are not required.", "startOffset": 60, "endOffset": 112}, {"referenceID": 26, "context": "While some methods rewrite the source tree and produce an alternative derivation at every consituent (Knight & Marcu, 2000; Galley & McKeown, 2007), others prune edges in the source tree (Filippova & Strube, 2008; Galanis & Androutsopoulos, 2010; Wang et al., 2013).", "startOffset": 187, "endOffset": 265}, {"referenceID": 27, "context": "Several text-to-text generation systems use ILP as an optimization tool to generate new sentences by combining pieces from the input (Clarke & Lapata, 2008; Martins & Smith, 2009; Woodsend et al., 2010; Filippova & Altun, 2013).", "startOffset": 133, "endOffset": 227}, {"referenceID": 0, "context": "While off-theshelf general purpose LP solvers are designed to be fast, in practice they may make the compressor prohibitively slow, in particular if compression is done jointly with summarization (Berg-Kirkpatrick et al., 2011; Qian & Liu, 2013; Thadani, 2014).", "startOffset": 196, "endOffset": 260}, {"referenceID": 22, "context": "While off-theshelf general purpose LP solvers are designed to be fast, in practice they may make the compressor prohibitively slow, in particular if compression is done jointly with summarization (Berg-Kirkpatrick et al., 2011; Qian & Liu, 2013; Thadani, 2014).", "startOffset": 196, "endOffset": 260}, {"referenceID": 0, "context": "Approaches which use a constituency parser and prune edges pointing to constituents, deal with function words implicitly (Berg-Kirkpatrick et al., 2011; Wang et al., 2013).", "startOffset": 121, "endOffset": 171}, {"referenceID": 26, "context": "Approaches which use a constituency parser and prune edges pointing to constituents, deal with function words implicitly (Berg-Kirkpatrick et al., 2011; Wang et al., 2013).", "startOffset": 121, "endOffset": 171}, {"referenceID": 17, "context": "We use the latter approach and transform every input tree (Nivre, 2006) following Filippova & Strube (2008) and also add edges from the dummy root to finite verbs.", "startOffset": 58, "endOffset": 71}, {"referenceID": 9, "context": "Comparison to related work Many compression systems have been introduced since the very first approaches by Grefenstette (1998), Jing & McKeown (2000) and Knight & Marcu (2000).", "startOffset": 108, "endOffset": 128}, {"referenceID": 9, "context": "Comparison to related work Many compression systems have been introduced since the very first approaches by Grefenstette (1998), Jing & McKeown (2000) and Knight & Marcu (2000).", "startOffset": 108, "endOffset": 151}, {"referenceID": 9, "context": "Comparison to related work Many compression systems have been introduced since the very first approaches by Grefenstette (1998), Jing & McKeown (2000) and Knight & Marcu (2000). Almost all of them make use of syntactic information (e.", "startOffset": 108, "endOffset": 177}, {"referenceID": 9, "context": "Comparison to related work Many compression systems have been introduced since the very first approaches by Grefenstette (1998), Jing & McKeown (2000) and Knight & Marcu (2000). Almost all of them make use of syntactic information (e.g., Clarke & Lapata (2006), McDonald (2006), Toutanova et al.", "startOffset": 108, "endOffset": 261}, {"referenceID": 9, "context": "Comparison to related work Many compression systems have been introduced since the very first approaches by Grefenstette (1998), Jing & McKeown (2000) and Knight & Marcu (2000). Almost all of them make use of syntactic information (e.g., Clarke & Lapata (2006), McDonald (2006), Toutanova et al.", "startOffset": 108, "endOffset": 278}, {"referenceID": 9, "context": "Comparison to related work Many compression systems have been introduced since the very first approaches by Grefenstette (1998), Jing & McKeown (2000) and Knight & Marcu (2000). Almost all of them make use of syntactic information (e.g., Clarke & Lapata (2006), McDonald (2006), Toutanova et al. (2007)), and our system is not an exception.", "startOffset": 108, "endOffset": 303}, {"referenceID": 9, "context": "Comparison to related work Many compression systems have been introduced since the very first approaches by Grefenstette (1998), Jing & McKeown (2000) and Knight & Marcu (2000). Almost all of them make use of syntactic information (e.g., Clarke & Lapata (2006), McDonald (2006), Toutanova et al. (2007)), and our system is not an exception. Like Nomoto (2009), Wang et al.", "startOffset": 108, "endOffset": 360}, {"referenceID": 9, "context": "Comparison to related work Many compression systems have been introduced since the very first approaches by Grefenstette (1998), Jing & McKeown (2000) and Knight & Marcu (2000). Almost all of them make use of syntactic information (e.g., Clarke & Lapata (2006), McDonald (2006), Toutanova et al. (2007)), and our system is not an exception. Like Nomoto (2009), Wang et al. (2013) we operate on syntactic trees provided by a stateof-the-art parser.", "startOffset": 108, "endOffset": 380}, {"referenceID": 0, "context": "While off-theshelf general purpose LP solvers are designed to be fast, in practice they may make the compressor prohibitively slow, in particular if compression is done jointly with summarization (Berg-Kirkpatrick et al., 2011; Qian & Liu, 2013; Thadani, 2014). Recent improvements to the ILP-based methods have been significant but not dramatic. For example, Thadani (2014) presents an approximation technique resulting in a 60% reduction in average inference time.", "startOffset": 197, "endOffset": 375}, {"referenceID": 0, "context": "While off-theshelf general purpose LP solvers are designed to be fast, in practice they may make the compressor prohibitively slow, in particular if compression is done jointly with summarization (Berg-Kirkpatrick et al., 2011; Qian & Liu, 2013; Thadani, 2014). Recent improvements to the ILP-based methods have been significant but not dramatic. For example, Thadani (2014) presents an approximation technique resulting in a 60% reduction in average inference time. Compared with this work, the main practical advantage of our system is that it is very fast without trading compression quality for speed improvements. On the modeling side, it demonstrates that local decisions are sufficient to produce an informative and grammatically correct sentence. Our recursive procedure of generating k best compressions at every node is partly inspired by frame semantics (Fillmore et al., 2003) and its extension from predicates to any node type (Titov & Klementiev, 2011). The core idea is that there are two components to a high-quality compression at every node in the tree: (1) it should keep all the essential arguments of that node; (2) these arguments should themselves be good compressions. This motivates an algorithm with a recursively defined scoring function which allows us to obtain k-best compressions nearly as fast as the single best one. In this respect our algorithm is similar to the k-best parsing algorithm by Huang & Chiang (2005). 2 The Top-down Approach Our approach is syntax-driven and operates on dependency trees (Sec.", "startOffset": 197, "endOffset": 1448}, {"referenceID": 0, "context": "While off-theshelf general purpose LP solvers are designed to be fast, in practice they may make the compressor prohibitively slow, in particular if compression is done jointly with summarization (Berg-Kirkpatrick et al., 2011; Qian & Liu, 2013; Thadani, 2014). Recent improvements to the ILP-based methods have been significant but not dramatic. For example, Thadani (2014) presents an approximation technique resulting in a 60% reduction in average inference time. Compared with this work, the main practical advantage of our system is that it is very fast without trading compression quality for speed improvements. On the modeling side, it demonstrates that local decisions are sufficient to produce an informative and grammatically correct sentence. Our recursive procedure of generating k best compressions at every node is partly inspired by frame semantics (Fillmore et al., 2003) and its extension from predicates to any node type (Titov & Klementiev, 2011). The core idea is that there are two components to a high-quality compression at every node in the tree: (1) it should keep all the essential arguments of that node; (2) these arguments should themselves be good compressions. This motivates an algorithm with a recursively defined scoring function which allows us to obtain k-best compressions nearly as fast as the single best one. In this respect our algorithm is similar to the k-best parsing algorithm by Huang & Chiang (2005). 2 The Top-down Approach Our approach is syntax-driven and operates on dependency trees (Sec. 2.1). The input tree is pruned to obtain a valid subtree from which a compression is read off. The pruning decisions are carried out based on predictions of a maximum entropy classifier which is trained on a parallel corpora with a rich feature set (Sec. 2.2). Section 2.3 explains how to generate the single, top-scoring compression; Section 2.4 extends the idea to arbitrary k. 2.1 Preprocessing Similar to previous work, we have a special treatment for function words like determiners, prepositions, auxiliary verbs. Unsurprisingly, dealing with function words is much easier than deciding whether a content word can be removed. Approaches which use a constituency parser and prune edges pointing to constituents, deal with function words implicitly (Berg-Kirkpatrick et al., 2011; Wang et al., 2013). Approaches which use a dependency representation either formulate hard constraints (Almeida & Martins, 2013), or collapse function words with their heads. We use the latter approach and transform every input tree (Nivre, 2006) following Filippova & Strube (2008) and also add edges from the dummy root to finite verbs.", "startOffset": 197, "endOffset": 2610}, {"referenceID": 1, "context": "2 Estimating deletion probabilities The supervised component of our system is a binary maximum entropy classifier (Berger et al., 1996) which is trained to estimate the probability of deleting an edge given its local context.", "startOffset": 114, "endOffset": 135}, {"referenceID": 26, "context": "The features we use are inspired by most recent work (Almeida & Martins, 2013; Filippova & Altun, 2013; Wang et al., 2013) and are as follows: syntactic: edge labels for the child and its siblings; NE type and PoS tags; lexical: head and child lemmas; negation; concatenation of parent lemmas and labels; numeric: depth; node length in words and characters; children count for the parent and the child.", "startOffset": 53, "endOffset": 122}, {"referenceID": 5, "context": "2) on the same training data using an averaged perceptron (Collins, 2002).", "startOffset": 58, "endOffset": 73}, {"referenceID": 20, "context": "For an automatic evaluation of the quality of the sentence compressions, we followed the same approach as (Riezler et al., 2003; Filippova & Altun, 2013) and measured F1-score by comparing the trees of the generated compressions to the golden, extractive compression.", "startOffset": 106, "endOffset": 153}], "year": 2015, "abstractText": "A popular approach to sentence compression is to formulate the task as a constrained optimization problem and solve it with integer linear programming (ILP) tools. Unfortunately, dependence on ILP may make the compressor prohibitively slow, and thus approximation techniques have been proposed which are often complex and offer a moderate gain in speed. As an alternative solution, we introduce a novel compression algorithm which generates k-best compressions relying on local deletion decisions. Our algorithm is two orders of magnitude faster than a recent ILPbased method while producing better compressions. Moreover, an extensive evaluation demonstrates that the quality of compressions does not degrade much as we move from single best to top-five results.", "creator": "TeX"}}}