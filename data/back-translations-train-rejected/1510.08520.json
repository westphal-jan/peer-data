{"id": "1510.08520", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "28-Oct-2015", "title": "Learning with $\\ell^{0}$-Graph: $\\ell^{0}$-Induced Sparse Subspace Clustering", "abstract": "$\\ell^{1}$-graph \\cite{YanW09,ChengYYFH10}, a sparse graph built by reconstructing each datum with all the other data using sparse representation, has been demonstrated to be effective in clustering high dimensional data and recovering independent subspaces from which the data are drawn. It is well known that $\\ell^{1}$-norm used in $\\ell^{1}$-graph is a convex relaxation of $\\ell^{0}$-norm for enforcing the sparsity. In order to handle general cases when the subspaces are not independent and follow the original principle of sparse representation, we propose a novel $\\ell^{0}$-graph that employs $\\ell^{0}$-norm to encourage the sparsity of the constructed graph, and develop a proximal method to solve the associated optimization problem with the proved guarantee of convergence. Extensive experimental results on various data sets demonstrate the superiority of $\\ell^{0}$-graph compared to other competing clustering methods including $\\ell^{1}$-graph.", "histories": [["v1", "Wed, 28 Oct 2015 22:48:09 GMT  (233kb)", "http://arxiv.org/abs/1510.08520v1", null], ["v2", "Wed, 18 Nov 2015 07:11:42 GMT  (233kb)", "http://arxiv.org/abs/1510.08520v2", null]], "reviews": [], "SUBJECTS": "cs.LG cs.CV", "authors": ["yingzhen yang", "jiashi feng", "jianchao yang", "thomas s huang"], "accepted": false, "id": "1510.08520"}, "pdf": {"name": "1510.08520.pdf", "metadata": {"source": "CRF", "title": null, "authors": [], "emails": [], "sections": [{"heading": null, "text": "ar Xiv: 151 0.08 520v 1 [cs.L G] 28 Oct 201 5"}, {"heading": "1. Introduction", "text": "This year, it will be able to put itself at the top of the group."}, {"heading": "1.1. Sparse Coding and \u21131-Graph for Clustering", "text": "The frugal coding methods represent an input signal by a linear combination of only a few atoms of a dictionary, which is usually overcomplete, and the frugal coefficients are referred to as frugal code. Frugal coding methods are commonly used in machine learning and signal processing, and frugal code is widely used as a discriminatory and robust property representation [20, 5, 21] with convincing performance for image classification and clustering. Denote the data of X = [x1, x2,.] located in the d-dimensional Euclidean space, and let the dictatorial matrix be D = [d1, d2, dp]."}, {"heading": "2. Formulation of \u21130-Graph", "text": "As already mentioned in the previous section, the method of linear optimisation [3] and the application of this method to learning based on 0 \u00b0 Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q"}, {"heading": "3. Experimental Results", "text": "In this section, the superior cluster performance of the first curve is shown on the basis of extensive experimental results. We compare our second curve with K averages (KM), spectral clusters (SC), 1-graph, sparse clusters, and embedding (SMCE). In addition, we derive the OMP graph, which builds the sparse graph in the same way as the sparse graph with 0-graph, which is a critical point for the function f if 0% f (x), where f (x) is the delimiting subdifferential of f at x. Please refer to a more detailed definition in [3], except that it solves the following optimization problem by Orthogonal Matching Pursuit (OMP) to obtain the sparse code: min \u03b1 xi \u2212 A\u03b1 i \u0445 2F s.t.: 1% 0 \u2264 T, \u03b1 i = 0, i = 0, i = 1, i = 1."}, {"heading": "3.1. Evaluation Metric", "text": "Two metrics are used to evaluate the performance of cluster methods, i.e. Accuracy and Normalized Reciprocal Information (NMI) [25]. Let the predicted marking of the date xi be the y-i generated by the cluster method, and yi is its basic truth marking. Accuracy is defined as Accuracy = 1IB (y-i) 6 = yin (14), where 1I is the indicator function, and ig is the best permutation mapping function by the Kuhn-Munkres algorithm [17]. The more predicted marks agree with the fundamentals, the more accuracy values will be attained.Let X specify the index set from the predicted markers {y-i} n = 1 and X from the basic truth markers {yi} n = 1. The more predicted markers agree with the fundamentals, the more accuracy values will be obtained."}, {"heading": "3.2. Clustering on UCI Data Sets and MNIST Handwritten Digits Database", "text": "In this section, we conduct experiments with two real data sets from the UCI Machine Learning Repository [1], i.e. the heart and ionosphere, and the MNIST database for handwritten digits. The three data sets are summarized in Table 5. The MNIST database for handwritten digits comprises a total of 70,000 samples for digits from 0 to 9. The digits are normalized and centered in a fixed-size image. For the MNIST data set, we randomly select 500 samples for each digit to obtain a subset of MNIST data consisting of 5,000 samples."}, {"heading": "3.3. Clustering On COIL-20 and COIL-100 Database", "text": "COIL-20 Database has 1440 images of 20 objects in which the background has been removed, and the size of each image is 32 x 32, so the dimension of this data is 1024. COIL-100 Database contains 100 objects with 72 images of the size 32 x 32 for each object. Images of each object were taken 5 degrees apart when the object was rotated on a turntable. The cluster results of these two data sets are in Table 2 and Table 3 respectively. We observe that the 0 graph consistently produces better results than all other competing methods. On the COIL-100 Database, SMCE delivers slightly better results than a 1 graph over the entire data due to its ability to model nonlinear manifolds."}, {"heading": "3.4. Clustering On Extended Yale Face Database B", "text": "The Extended Yale Face Database B contains facial images of 38 subjects with 64 frontal facial images taken under different lighting for each subject. Cluster results are shown in Table 4. We can see that the H0 graph achieves a significantly better cluster result than the H1 graph, which is the second best method based on this data."}, {"heading": "3.5. Parameter Setting", "text": "We use the sparse codes generated by a 1 graph with the weighting parameter 1 = 0.1 in (5) to initialize a 0 graph, and set \u03bb = 0.5 empirically for a 0 graph in all experiments of this section, and we observe that the average number of non-zero elements of the sparse code is about 3 for most datasets, the maximum iteration number M = 100 and the stop threshold \u03b5 = 10 \u2212 6. For the OMP graph, we adjust the parameter T in (13) to control the sparsity of the sparse codes generated so that the above average number of non-zero elements of the sparse code matches the 1 graph."}, {"heading": "4. Conclusion", "text": "Unlike the existing sparse graph methods, such as the \"1 graph,\" which uses the \"1 standard\" as a relaxation of the \"0 standard,\" the \"0 graph\" forces the sparseness of the constructed graph through the \"0 standard\" and optimizes the objective function using a proposed proximal method. The convergence of this proximal method is proven, and extensive experimental results on various real data sets demonstrate the effectiveness and superiority of the \"0 graph\" over other competing methods."}, {"heading": "5. Appendix", "text": "The proof for Theorem 1 is that if s are two times the maximum eigenvalue of ATA, then s is the Lipschitz constant for the course of the function Q. To see this, we have Q (Y) = 2 (ATAY \u2212 ATX) and Q (Y) \u2212 Q (Z) = 2 x A (Y \u2212 Z) = 0 x A (17) x max (A T A) \u00d7 x (Y \u2212 Z) x (T) x (T) x (T) x (18 x) x x (Q (T) x x x x) x (T) x x x x x x (T) x x x x x (T) x (T) x (t) x (t) x (t) x (t) x (t) x (x (T) x (18 x) x (T) x x (Q (T) x x x x) x x x x (T) x x x x x x x (T) x x x x x x x x) x x (T) x x x x (T) x x x (T) x x (T) x x x (T) x (T) x (T) x (t) x (t) x (t) x (t) x (t) x (t) x (t) x (t) x (t) x (t) x (t) x (t) x (t) x (t) x (t) x (t) x (t) x (t) x (t) x (t) x (t) x (t) x (t) x (t) x (t) x (t) x (t) x (t) x (t) x (t) x (t) x (t) x (t) x (t) x (t) x (t x (t) x (t) x (t) x (t) x (t) x (t) x x (t) x (t) x x x x x (t (t) x (t) x (t) x (t) x (t) x (t) x x (t) x x x x (t) x (t) x (t) x x x x x x (t) x (t) x x x (t) x ("}], "references": [{"title": "L0 norm based dictionary learning by proximal methods with global convergence", "author": ["C. Bao", "H. Ji", "Y. Quan", "Z. Shen"], "venue": "2014 IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2014, Columbus, OH, USA, June 23-28, 2014, pages 3858\u20133865", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2014}, {"title": "Proximal alternating linearized minimization for nonconvex and nonsmooth problems", "author": ["J. Bolte", "S. Sabach", "M. Teboulle"], "venue": "Math. Program.,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2014}, {"title": "Learning with l1-graph for image analysis", "author": ["B. Cheng", "J. Yang", "S. Yan", "Y. Fu", "T.S. Huang"], "venue": "IEEE Transactions on Image Processing, 19(4):858\u2013866", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2010}, {"title": "Sparse representation and learning in visual recognition: Theory and applications", "author": ["H. Cheng", "Z. Liu", "L. Yang", "X. Chen"], "venue": "Signal Process.,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2013}, {"title": "Pattern Classification (2Nd Edition)", "author": ["R.O. Duda", "P.E. Hart", "D.G. Stork"], "venue": "Wiley-Interscience", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2000}, {"title": "Sparse subspace clustering", "author": ["E. Elhamifar", "R. Vidal"], "venue": "CVPR, pages 2790\u20132797", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2009}, {"title": "Sparse manifold clustering and embedding", "author": ["E. Elhamifar", "R. Vidal"], "venue": "NIPS, pages 55\u201363", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2011}, {"title": "Sparse subspace clustering: Algorithm", "author": ["E. Elhamifar", "R. Vidal"], "venue": "theory, and applications. IEEE Trans. Pattern Anal. Mach. Intell., 35(11):2765\u20132781", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2013}, {"title": "Model-Based Clustering, Discriminant Analysis, and Density Estimation", "author": ["C. Fraley", "A.E. Raftery"], "venue": "Journal of the American Statistical Association,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2002}, {"title": "Clustering by passing messages between data points", "author": ["B.J. Frey", "D. Dueck"], "venue": "Science, 315:2007", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2007}, {"title": "Proximal methods for sparse hierarchical dictionary learning", "author": ["R. Jenatton", "J. Mairal", "F.R. Bach", "G.R. Obozinski"], "venue": "Proceedings of the 27th International Conference on Machine Learning (ICML-10), pages 487\u2013494", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2010}, {"title": "Online learning for matrix factorization and sparse coding", "author": ["J. Mairal", "F. Bach", "J. Ponce", "G. Sapiro"], "venue": "J. Mach. Learn. Res.,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2010}, {"title": "Supervised dictionary learning", "author": ["J. Mairal", "F.R. Bach", "J. Ponce", "G. Sapiro", "A. Zisserman"], "venue": "Advances in Neural Information Processing Systems 21, Proceedings of the Twenty-Second Annual Conference on Neural Information Processing Systems, Vancouver, British Columbia, Canada, December 8-11, 2008, pages 1033\u20131040", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2008}, {"title": "L0-norm-based sparse representation through alternate projections", "author": ["L. Mancera", "J. Portilla"], "venue": "In Image Processing,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2006}, {"title": "On spectral clustering: Analysis and an algorithm", "author": ["A.Y. Ng", "M.I. Jordan", "Y. Weiss"], "venue": "NIPS, pages 849\u2013856", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2001}, {"title": "Matching Theory", "author": ["D. Plummer", "L. Lov\u00e1sz"], "venue": "North- Holland Mathematics Studies. Elsevier Science", "citeRegEx": "17", "shortCiteRegEx": null, "year": 1986}, {"title": "Survey: Graph clustering", "author": ["S.E. Schaeffer"], "venue": "Comput. Sci. Rev.,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2007}, {"title": "Semi-supervised learning by sparse representation", "author": ["S. Yan", "H. Wang"], "venue": "SDM, pages 792\u2013801", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2009}, {"title": "Linear spatial pyramid matching using sparse coding for image classification", "author": ["J. Yang", "K. Yu", "Y. Gong", "T.S. Huang"], "venue": "CVPR, pages 1794\u20131801", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2009}, {"title": "Regularized l1-graph for data clustering", "author": ["Y. Yang", "Z. Wang", "J. Yang", "J. Han", "T. Huang"], "venue": "Proceedings of the British Machine Vision Conference. BMVA Press", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2014}, {"title": "Data clustering by laplacian regularized l1-graph", "author": ["Y. Yang", "Z. Wang", "J. Yang", "J. Wang", "S. Chang", "T.S. Huang"], "venue": "Proceedings of the Twenty-Eighth AAAI Conference on Artificial Intelligence, July 27 -31, 2014, Qu\u00e9bec City, Qu\u00e9bec, Canada., pages 3148\u20133149", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2014}, {"title": "Lowrank sparse coding for image classification", "author": ["T. Zhang", "B. Ghanem", "S. Liu", "C. Xu", "N. Ahuja"], "venue": "IEEE International Conference on Computer Vision, ICCV 2013, Sydney, Australia, December 1-8, 2013, pages 281\u2013288", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2013}, {"title": "Graph regularized sparse coding for image representation", "author": ["M. Zheng", "J. Bu", "C. Chen", "C. Wang", "L. Zhang", "G. Qiu", "D. Cai"], "venue": "IEEE Transactions on Image Processing, 20(5):1327\u2013 1336", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2011}, {"title": "Locality preserving clustering for image database", "author": ["X. Zheng", "D. Cai", "X. He", "W.-Y. Ma", "X. Lin"], "venue": "Proceedings of the 12th Annual ACM International Conference on Multimedia, MULTIMEDIA \u201904, pages 885\u2013891, New York, NY, USA", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2004}], "referenceMentions": [{"referenceID": 17, "context": "l-graph [19, 4], a sparse graph built by reconstructing each datum with all the other data using sparse representation, has been demonstrated to be effective in clustering high dimensional data and recovering independent subspaces from which the data are drawn.", "startOffset": 8, "endOffset": 15}, {"referenceID": 2, "context": "l-graph [19, 4], a sparse graph built by reconstructing each datum with all the other data using sparse representation, has been demonstrated to be effective in clustering high dimensional data and recovering independent subspaces from which the data are drawn.", "startOffset": 8, "endOffset": 15}, {"referenceID": 8, "context": "With Gaussian Mixture Model (GMM) as a representative, model-based clustering methods typically model the data by a mixture of parametric distributions, and the parameters of the distributions are estimated via fitting a statistical model to the data [10].", "startOffset": 251, "endOffset": 255}, {"referenceID": 4, "context": "For example, K-means [6] groups similar data together by a local minimum of sum of within-cluster dissimilarities.", "startOffset": 21, "endOffset": 24}, {"referenceID": 9, "context": "Affinity Propagation [11] uses the same principle and automatically determines the cluster number.", "startOffset": 21, "endOffset": 25}, {"referenceID": 14, "context": "Spectral Clustering [16] identifies clusters of complex shapes lying on some low dimensional manifolds by spectral embedding.", "startOffset": 20, "endOffset": 24}, {"referenceID": 16, "context": "Among various similarity-based clustering methods, graph-based methods [18] are important wherein the edge weight of the graph serving as the data similarity, and sparse graphs (where only a few edges have non-zero weights for each vertex) are demonstrated to be especially effective for clustering high dimensional data.", "startOffset": 71, "endOffset": 75}, {"referenceID": 17, "context": "Examples of sparse graph methods include l-graph [19, 4] and Sparse Subspace Clustering [9], which build the graph by reconstructing each datum with all the other data by sparse representation.", "startOffset": 49, "endOffset": 56}, {"referenceID": 2, "context": "Examples of sparse graph methods include l-graph [19, 4] and Sparse Subspace Clustering [9], which build the graph by reconstructing each datum with all the other data by sparse representation.", "startOffset": 49, "endOffset": 56}, {"referenceID": 7, "context": "Examples of sparse graph methods include l-graph [19, 4] and Sparse Subspace Clustering [9], which build the graph by reconstructing each datum with all the other data by sparse representation.", "startOffset": 88, "endOffset": 91}, {"referenceID": 19, "context": "l-graph is further extended to incorporate local manifold structure of the data in [21, 22].", "startOffset": 83, "endOffset": 91}, {"referenceID": 20, "context": "l-graph is further extended to incorporate local manifold structure of the data in [21, 22].", "startOffset": 83, "endOffset": 91}, {"referenceID": 17, "context": "To avoid the non-convex optimization problem incurred by l-norm, most of the sparse graph based methods [19, 4, 8, 9, 21, 22] replaces l-norm with l-norm so as to solve a convex optimization problem.", "startOffset": 104, "endOffset": 125}, {"referenceID": 2, "context": "To avoid the non-convex optimization problem incurred by l-norm, most of the sparse graph based methods [19, 4, 8, 9, 21, 22] replaces l-norm with l-norm so as to solve a convex optimization problem.", "startOffset": 104, "endOffset": 125}, {"referenceID": 6, "context": "To avoid the non-convex optimization problem incurred by l-norm, most of the sparse graph based methods [19, 4, 8, 9, 21, 22] replaces l-norm with l-norm so as to solve a convex optimization problem.", "startOffset": 104, "endOffset": 125}, {"referenceID": 7, "context": "To avoid the non-convex optimization problem incurred by l-norm, most of the sparse graph based methods [19, 4, 8, 9, 21, 22] replaces l-norm with l-norm so as to solve a convex optimization problem.", "startOffset": 104, "endOffset": 125}, {"referenceID": 19, "context": "To avoid the non-convex optimization problem incurred by l-norm, most of the sparse graph based methods [19, 4, 8, 9, 21, 22] replaces l-norm with l-norm so as to solve a convex optimization problem.", "startOffset": 104, "endOffset": 125}, {"referenceID": 20, "context": "To avoid the non-convex optimization problem incurred by l-norm, most of the sparse graph based methods [19, 4, 8, 9, 21, 22] replaces l-norm with l-norm so as to solve a convex optimization problem.", "startOffset": 104, "endOffset": 125}, {"referenceID": 10, "context": "In addition, l-norm has been widely used as a convex relaxation of l-norm for efficient sparse coding algorithms [12, 13, 14].", "startOffset": 113, "endOffset": 125}, {"referenceID": 11, "context": "In addition, l-norm has been widely used as a convex relaxation of l-norm for efficient sparse coding algorithms [12, 13, 14].", "startOffset": 113, "endOffset": 125}, {"referenceID": 12, "context": "In addition, l-norm has been widely used as a convex relaxation of l-norm for efficient sparse coding algorithms [12, 13, 14].", "startOffset": 113, "endOffset": 125}, {"referenceID": 7, "context": "[9] points out that in case that the data are drawn from linear independent subspaces, sparse representation by l-norm can recover the underlying subspaces.", "startOffset": 0, "endOffset": 3}, {"referenceID": 13, "context": "On the other hand, sparse representation methods [15] that directly optimize objective function involving l-norm demonstrate compelling performance compared to its lnorm counterpart.", "startOffset": 49, "endOffset": 53}, {"referenceID": 1, "context": "The proximal method is inspired by the proximal linearized method in [3].", "startOffset": 69, "endOffset": 72}, {"referenceID": 18, "context": "Sparse coding has been broadly applied in machine learning and signal processing, and sparse code is extensively used as a discriminative and robust feature representation [20, 5, 23, 21] with demonstrated convincing performance for image classification and clustering.", "startOffset": 172, "endOffset": 187}, {"referenceID": 3, "context": "Sparse coding has been broadly applied in machine learning and signal processing, and sparse code is extensively used as a discriminative and robust feature representation [20, 5, 23, 21] with demonstrated convincing performance for image classification and clustering.", "startOffset": 172, "endOffset": 187}, {"referenceID": 21, "context": "Sparse coding has been broadly applied in machine learning and signal processing, and sparse code is extensively used as a discriminative and robust feature representation [20, 5, 23, 21] with demonstrated convincing performance for image classification and clustering.", "startOffset": 172, "endOffset": 187}, {"referenceID": 19, "context": "Sparse coding has been broadly applied in machine learning and signal processing, and sparse code is extensively used as a discriminative and robust feature representation [20, 5, 23, 21] with demonstrated convincing performance for image classification and clustering.", "startOffset": 172, "endOffset": 187}, {"referenceID": 17, "context": "l-graph [19, 4] employed the idea of sparse coding to encode the intrinsic similarity between the data by the sparse codes.", "startOffset": 8, "endOffset": 15}, {"referenceID": 2, "context": "l-graph [19, 4] employed the idea of sparse coding to encode the intrinsic similarity between the data by the sparse codes.", "startOffset": 8, "endOffset": 15}, {"referenceID": 7, "context": "In sparse subspace clustering [9], the authors pointed out that such sparse representation for each datum recovers the underlying subspaces from which the data are generated.", "startOffset": 30, "endOffset": 33}, {"referenceID": 5, "context": ",xn] \u2208 IR, it is mentioned in [7, 9] that the following sparse representation for each data point by l-norm", "startOffset": 30, "endOffset": 36}, {"referenceID": 7, "context": ",xn] \u2208 IR, it is mentioned in [7, 9] that the following sparse representation for each data point by l-norm", "startOffset": 30, "endOffset": 36}, {"referenceID": 5, "context": "In the case that the subspaces from which the data are drawn are linear and independent, Wij is nonzero if and only if two points xi and xj are in the same subspace according to [7, 9].", "startOffset": 178, "endOffset": 184}, {"referenceID": 7, "context": "In the case that the subspaces from which the data are drawn are linear and independent, Wij is nonzero if and only if two points xi and xj are in the same subspace according to [7, 9].", "startOffset": 178, "endOffset": 184}, {"referenceID": 1, "context": "Inspired by recent advances in solving non-convex optimization problems by proximal linearized method [3] and the application of this method to l-norm based dictionary learning [2], we propose a proximal method to optimize (7) which is iterative.", "startOffset": 102, "endOffset": 105}, {"referenceID": 0, "context": "Inspired by recent advances in solving non-convex optimization problems by proximal linearized method [3] and the application of this method to l-norm based dictionary learning [2], we propose a proximal method to optimize (7) which is iterative.", "startOffset": 177, "endOffset": 180}, {"referenceID": 1, "context": "[3] shows that the l-norm function \u2016 \u00b7 \u20160 is a semi-algebraic function.", "startOffset": 0, "endOffset": 3}, {"referenceID": 1, "context": "The conclusions of this theorem directly follows from Theorem 1 in [3].", "startOffset": 67, "endOffset": 70}, {"referenceID": 1, "context": "Please refer to more detailed definition in [3].", "startOffset": 44, "endOffset": 47}, {"referenceID": 23, "context": "the accuracy and the Normalized Mutual Information(NMI) [25].", "startOffset": 56, "endOffset": 60}, {"referenceID": 15, "context": "where 1I is the indicator function, and \u03a9 is the best permutation mapping function by the Kuhn-Munkres algorithm [17].", "startOffset": 113, "endOffset": 117}, {"referenceID": 22, "context": "The accuracy and the normalized mutual information have been widely used for evaluating the performance of the clustering methods [24, 4, 25].", "startOffset": 130, "endOffset": 141}, {"referenceID": 2, "context": "The accuracy and the normalized mutual information have been widely used for evaluating the performance of the clustering methods [24, 4, 25].", "startOffset": 130, "endOffset": 141}, {"referenceID": 23, "context": "The accuracy and the normalized mutual information have been widely used for evaluating the performance of the clustering methods [24, 4, 25].", "startOffset": 130, "endOffset": 141}], "year": 2017, "abstractText": "l-graph [19, 4], a sparse graph built by reconstructing each datum with all the other data using sparse representation, has been demonstrated to be effective in clustering high dimensional data and recovering independent subspaces from which the data are drawn. It is well known that l-norm used in l-graph is a convex relaxation of lnorm for enforcing the sparsity. In order to handle general cases when the subspaces are not independent and follow the original principle of sparse representation, we propose a novel l-graph that employs l-norm to encourage the sparsity of the constructed graph, and develop a proximal method to solve the associated optimization problem with the proved guarantee of convergence. Extensive experimental results on various data sets demonstrate the superiority of l-graph compared to other competing clustering methods including l-graph.", "creator": "LaTeX with hyperref package"}}}