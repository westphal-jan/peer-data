{"id": "1510.08532", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "29-Oct-2015", "title": "The Singular Value Decomposition, Applications and Beyond", "abstract": "The singular value decomposition (SVD) is not only a classical theory in matrix computation and analysis, but also is a powerful tool in machine learning and modern data analysis. In this tutorial we first study the basic notion of SVD and then show the central role of SVD in matrices. Using majorization theory, we consider variational principles of singular values and eigenvalues. Built on SVD and a theory of symmetric gauge functions, we discuss unitarily invariant norms, which are then used to formulate general results for matrix low rank approximation. We study the subdifferentials of unitarily invariant norms. These results would be potentially useful in many machine learning problems such as matrix completion and matrix data classification. Finally, we discuss matrix low rank approximation and its recent developments such as randomized SVD, approximate matrix multiplication, CUR decomposition, and Nystrom approximation. Randomized algorithms are important approaches to large scale SVD as well as fast matrix computations.", "histories": [["v1", "Thu, 29 Oct 2015 00:59:53 GMT  (83kb)", "http://arxiv.org/abs/1510.08532v1", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["zhihua zhang"], "accepted": false, "id": "1510.08532"}, "pdf": {"name": "1510.08532.pdf", "metadata": {"source": "CRF", "title": null, "authors": [], "emails": ["zhihua@sjtu.edu.cn"], "sections": [{"heading": null, "text": "ar Xiv: 151 0.08 532v 1 [cs.L G] 29 Oct 201 5The Singular Value Decomposition, Applications and BeyondZhihua Zhang Shanghai Jiao Tong Universityzhihua @ sjtu.edu.cnContents"}, {"heading": "1 Introduction 2", "text": "1.1 Roadmap................................... 3 Notation and definitions................. 5"}, {"heading": "2 Preliminaries 7", "text": "2.1 Kronecker Products and Vectorization Operators..... 7 2.2 Majorization......................... 8 2.3 Derivatives and Optimality.............."}, {"heading": "3 The Singular Value Decomposition 12", "text": "3.1 Formulations.................................................................................................................................................................................................................................................................................................................................................."}, {"heading": "4 Applications of SVD: Case Studies 29", "text": "4.1 The Matrix MP Pseudoinverse............ 30 4.2 The Procrust Problem.................. 32 4.3 Subspace Methods: PCA, MDS, FDA and CCA...... 33"}, {"heading": "5 The QR and CUR Decompositions 37", "text": "5.1 QR factorization.................. 37iiii5.2 CUR decomposition..............."}, {"heading": "6 Variational Principles 41", "text": "6.1 Variation properties for eigenvalues........ 42 6.2 Variation properties for singular values......... 46 6.3 Appendix: Application of matrix differentials..... 48"}, {"heading": "7 Unitarily Invariant Norms 52", "text": ""}, {"heading": "8 Subdifferentials of Unitarily Invariant Norms 67", "text": "8.1 Distinguishing features............................................................."}, {"heading": "9 Matrix Low Rank Approximation 77", "text": "9.1 Basic results......................... 78 9.2 Approximate matrix multiplication........... 82"}, {"heading": "10 Large-Scale Matrix Approximation 86", "text": "It is not only a matter of time before it will be, but also of time until it will be, until it will be, \"he told the German Press Agency.\" I don't think we will be able to. \"He added,\" I don't think we will be able to do what we are going to do. \"He added,\" I don't think we will be able to do what we are going to do. \""}, {"heading": "1.1. Roadmap 3", "text": "The French mathematician L. Autonne, etc. Please refer to chapter 3 of Horn and Johnson [1991], in which the authors presented an excellent historical retrospective of the SVD or theory of singular values. There is a rich literature that includes singular values or SVD. Chapter 3 of Horn and Johnson [1991] provides comprehensive studies of inequalities of singular values as well as uniformly invariant norms, with the main focus on matrix theory. Books by Watkins [1991], Demmel [1997], Golub and Van Loan [2012], Trefethen and Bau III [1997] present a detailed introduction to SVD, the main focus of which is linear numerical algebra. This tutorial is motivated by the recent successful applications of SVD in machine learning and theoretical computer science [Hastie et al., 2001, Burges, 2010, Halko et al., 2011, Mahoney et al.]."}, {"heading": "1.1 Roadmap", "text": "This tutorial deals with matrix differential calculus, majorization theory, and symmetrical measurement functions. For them, the detailed materials can be found in Macnus and Neudecker [2000], Marshal et al. [2010], Schatten [1950], Bhatia [1997]. In Chapter 2, we discuss some preliminary work such as Kronecker Production and Vectorization Operators, Majorization Theory, and Derivatives."}, {"heading": "4 Introduction", "text": "In Chapter 3, we introduce the basic concepts of SVD, including its existence, construction and uniqueness. We then deduce some important matrix concepts and properties about SVD. We also examine generalized SVD problems related to the common decomposition of two matrices. In Chapter 4, we further illustrate the application of SVD in defining the matrix pseudo-inverse and solving the procrust analysis problem. We discuss the role that SVD plays in the subspace of machine learning methods. From the perspective of computation and modern data analysis, matrix factoring techniques should be the most important problem of matrices. In Table 1.1, we summarize matrix factoring methods categorized into three types. In particular, we consider polar decomposition, SVD and spectral decomposition to be the geometric representation of a data matrix standard, while the CX, CUR and Nystr\u00f6m compositions represent a compact representation of the positions themselves."}, {"heading": "1.2. Notation and Definitions 5", "text": "In Chapter 9, two important theorems on low-rank matrix approximation based on errors of uniformly invariant norms are presented: the first is an extension of the problem of ordinal estimation of the smallest squares; the second was proposed by Mirsky [1960] and is an extension of the novel Eckart-Young theorem [Eckart and Young, 1936]; we also discuss approximate matrix multiplication, which can be considered an inverse process of low-rank matrix approximation; and in Chapter 10, we examine randomized SVD, CUR approximation, and Nystr\u00f6m methods to make applications scalable; and the randomized SVD and CUR approximation can also be regarded as low-rank matrix approximation."}, {"heading": "1.2 Notation and Definitions", "text": "Everywhere in this tutorial, vectors and matrices are labeled with bold lowercase letters and bold uppercase letters. Rn + = {u = (u1,..., un) T-Rn: uj \u2265 0 for j = 1,..., n} and Rn + + = {u = (u1,..., un) T-Rn: uj > 0 for j = 1,..., n}. Furthermore, if u-Rn + (or u-Rn + +), we also label u-Rn = 0 (or u > 0). Faced with a vector x = (x1,., xn) T-Rn, we leave | x | = (x1 |,.) T, we leave the x-Rn-Rank (x-Rank) [x-Rank], the x-m-Rank [x-Rank], [x-Rank] and the x-Rank (m-Rank) [x-Rank]."}, {"heading": "6 Introduction", "text": "s), zero (A) is the zero space (i.e., zero (A) = {x: Ax = 0}), and p = min {m, n} dg (A) is the p vector with aii as ith element. Sometimes we also use the Matlab colon to represent a submatrix of A, and A: J consists of these columns of A in J. Leave the submatrix of A with I and the columns of A with positive products indexed by I and columns indexed by J, AI: consists of these columns of A in J."}, {"heading": "2.1 Kronecker Products and Vectorization Operators", "text": "Considering two matrices A-Rm \u00b7 n and B-Rp \u00b7 q, the Kronecker product of A and B is defined by A B, a11B \u00b7 \u00b7 a1nB...... am1B \u00b7 \u00b7 \u00b7 amnB, which is mp \u00b7 nq. The following properties are found in Muirhead [1982]. Sentence 2.1. The Kronecker product has the following properties. (a) (\u03b1A) (\u03b2B) = \u03b1\u03b2 (A B) for any scales \u03b1, \u03b2 R. (b) (A B) T = AT BT.7"}, {"heading": "8 Preliminaries", "text": "(c) (A B) C = A (B C). (d) If A and C are both m \u00b7 n and B p \u00b7 q, then (A + C) B = A B + C B and B (A + C) = B A + B C. (e) If A is m \u00b7 n, B is p \u00b7 q, C is n \u00b7 r, and D is q \u00b7 s, then (A B) (C D) = (AC) (BD). (f) If U and V are both orthogonal matrices, U is V. (g) If A and B are symmetrically positively semidefined (SPSD), A B.Kronecker products often work with vectorization operators. Leave Vec (A) = (a11,.,., amn) T Rvec, p Rvec,."}, {"heading": "2.2 Majorization", "text": "Given a vector x = (x1,.., xn) T-Rn, let x-i = (x-1,.., x-n) be such a permutation of x that x-y 1 \u2265 x-2 \u2265 \u00b7 \u00b7 \u2265 x-t. Given two vectors x and y-R n, x-y xi-yi means \u2265 0 for all i-y [n]. We say that x is majorised by y (denotes x-y) if i-i = 1 x-i \u2264 i-k = 1 y-i for k = 1,..., n-1 and 1 x-i = 1 x-i = 1 y-t. Likewise, x-y if i-i = 1 x-1 y-i-fork = 1,..., n-1 and 1-i = 1 x-i = 1 y-i-i."}, {"heading": "2.3. Derivatives and Optimality 9", "text": "We say that x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x"}, {"heading": "2.3 Derivatives and Optimality", "text": "First, f: X-Rn-R should be a continuous function. At x, the directional derivative of f in one direction u-X is defined as asf \"(x-x; u) = lim t \u2193 0 f (x-x + tu) \u2212 f (x-x) t."}, {"heading": "10 Preliminaries", "text": "If this boundary exists, then f (G\u00e2beaux) is differentiable at x (x). If f (x) is differentiable at any point in X, then we say f (G\u00e2beaux) is differentiable at x (x). If f (x) is differentiable at X, we consider a concept of subdifferentials. We say z is the subgradient of f (x), if it is satisfactory f (x), then we say f (x) \u2264 f (x)."}, {"heading": "2.3. Derivatives and Optimality 11", "text": "So we have this dfdX = (M + M T) X.In addition, Lemma 2.1 indicates that f (X) = vec (X) T (In M) vec (X). Therefore, we have fdvec (X) = vec (dfdX) = [In (M + MT)] vec (X), and hence d2f (X) dvec (X) dvec (X) T = In (M + MT).3 The Singular Value DecompositionThe Singular Value Decomposition (SVD) is a classic matrix theory and a computer tool. In modern data computation and analysis, SVD is becoming increasingly important. In this chapter, we will seek a systematic review of the basic principle of SVD. We will see that there are four approaches to SVD. The first approach is to distinguish itself from the spectral decomposition of a symmetric positivity."}, {"heading": "3.1. Formulations 13", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "3.1 Formulations", "text": "Considering a SPSD matrix unequal 0, but not PSD, the current eigenvalues are inevitably unequal (G = G = G = G = G = G = G = G = G = G = G = G = G = G = G = G = G = G = G = G = G = G = G = G = G = G = G = G = G = G = G = G = G = G = G = G = G = G = G = G = G = G = G = G = G = G = G = G = G = G = G = G = G = G = G = G = G = G = G = G = G = G = G = G = G = G = G = G = G = G = G = G = G = G = G = G = G = G = G = G = G = G = G = G = G G = G = G = G G G = G G = G G G = G G G = G G = G G G = G G = G G G = G G = G G = G G G = G G = G G G = G = G G = G G G = G = G G G = G G = G G = G G = G G = G = G G = G G = G = G G = G G = G G = G = G = G G G = G = G G = G = G = G G = G = G G = G = G = G G = G G = G G = G = G G G = G = G = G G = G = G G = G = G = G G G = G = G G G G = G = G G G = G = G G G = G = G = G G G = G G = G G G = G G = G G = G G G = G G = G = G G = G = G G = G G G G = G G G G G G = G G = G G G = G G = G G G = G G = G G G = G G G = G = G = G G G G G G G = G = G G G = G = G G G G = G G G G = G = G G = G G G G = G G G G = G G G G G G G G = G G"}, {"heading": "14 The Singular Value Decomposition", "text": "Suppose that \"r\" = \"diag\" (\u03bb1, \u03bb2,. > \"r\" = \"r\") and \"Ur\" = [u1, u2,. \"ur,\" where \"p\" = \"p\" \u00b7 \"p\" are the positive eigenvalues of \"AAT\" and \"u\" are the \"m \u00b7 r\" matrix of the corresponding eigenvectors, so that UTr Ur = \"p.\" It follows that the spectral decomposition \"UTr AA TUr =\" p. \"and\" r \"TU \u2212 r\" are from the last m \u2212 r columns of \"u.\" So we have ATU \u2212 r = 0. Leave Vr = [v1,., vr], ATUrr \u2212 1 / 2 r. Then it fulfills VTr Vr = Ir. Note: \"V \u2212 r\" p. \""}, {"heading": "3.1. Formulations 15", "text": "Theorem 3.1. In the face of an arbitrary A-Rm-n, however, the complete SVD defined in (3,4) is always present. Moreover, the singular values \u03c3i are clearly specified. Proof. IfA is zero, the result is trivial. Therefore, letA is a non-zero matrix. Define \u03c31, max-Rm-n = 1-Ax-2 = 1-Ax-2, which exists because x-Ax-V [V1-Ax-V] exists. Define V-x-2 = 1-Ax-2, which exists, and the quantity VUx-V-V is compact."}, {"heading": "16 The Singular Value Decomposition", "text": "Since the matrices [u1, U \u2212 1U] and [v1, V \u2212 1V] are orthonormal. (Regarding the uniqueness of the singular values, the \u03c32i eigenvalues of AAT are unique. Unfortunately, the left and right singular values Ur and Vr are not unique. However, let us consider the following result. Theorem 3,2. Let A = Ur\u0432 rV T r be a condensed SVD of A. Suppose there are different values among the unequal singular values \u03c31,.., \u0441r, with the respective multiplications ri (satisfactory multiplications ri = r). Then A = U \u03b4rV, T r is a condensed SVD, if and only ifU, r = Ur (Q1, Q2)."}, {"heading": "3.1. Formulations 17", "text": "It follows that Sii = Tii for i = 1,.. and T for orthonormal. Since S and T are now orthonormal, we have [SijS] for j = 1SijS T ij = \u03c1. Note that Sii = Tii for i = 1,...,. Since S and T are orthonormal, we have [SijS] for j = 1TijT T ij.D = 0. (3.6) Da 1 \u2212 SijT T = 1 SijT = 1 \u00b2 and SijST \u00b2 for TijS."}, {"heading": "18 The Singular Value Decomposition", "text": "P0-R (n \u2212 r) \u00b7 (n \u2212 r) are all orthonormal matrices. Then, A = U-V-T is a thin SVD if and only if U-R (n \u2212 r) is an orthonormal matrix in which Q = Q1-Q0 and P = Q1-Q1-Q1-Q1-P0 occurs. Currently, Q0-R (n \u2212 r) \u00b7 (n \u2212 r) is any orthonormal matrix. Obviously, Q1-Q1-Q1 and P = Q1-Q1-Hold.Theorem 3.2 and Korollar 3.3 are used in the derivation of subdifferentials of unitatively invariant standards (see chapter 8). If the matrix is in question."}, {"heading": "3.2. Matrix Properties via SVD 19", "text": "the singular values of A, where p = min {m, n}. For each k [p], then\u03c3k = min v1,..., vk \u2212 1, vk \u2212 1, Rnmax v-Rn, \u0435v-2 = 1vT [v1,..., vk \u2212 1] = 0, vk \u2212 2 = max v1,..., vn \u2212 k, Rnmin v-Rn, \u0394v-2 = 1vT [v1,.., vn \u2212 k] = 0, Av-2."}, {"heading": "3.2 Matrix Properties via SVD", "text": "Proposal 3.1. Let A = BVB VT be a complete SVD of m \u00b7 n matrix A, and A = Uri rV T r be a condensed SVD. Let p = min {m, n}. Then (1) the rank of A is equal to the number of unequal singular values of A. (2). (2) Range (AAT) = Range (Ur). (2). 1 is the spectral standard and vice versa (A)."}, {"heading": "20 The Singular Value Decomposition", "text": "Theorem 3.5. Faced with a matrix A-Rm \u00b7 n, leave H = [0 ATA 0]. IfA = Ur\u043frV T r is the condensed SVD, then H has 2r eigenvalues that are \u00b1 \u03c3i, with the corresponding orthonormal eigenvectors 1 \u221a 2 [vi \u00b1 ui], i = 1. \u2212 \u2212 \u2212 Conversely, H \u2212 r is the eigenvalue of H, with the corresponding eigenvector = [z (1) i (2) i], where z (1) i-Rn and z (2) i-Rm, then \u2212 \u03b3i is the eigenvalue of H, with the corresponding eigenvector zi = [z (1) i-z (2) i-i, where z (1) i-Rn and z (2) i-Rm, then \u2212 r is the eigenvalue of H."}, {"heading": "3.3. Matrix Concepts via SVD 21", "text": "Theorem 3.5 makes an interesting connection between the SVD of a general matrix and the EVD of a symmetric matrix. (The theorem also provides an alternative proof of the SVD problem of an arbitrary matrix. The following theorem shows that the polar decomposition of a matrix can be induced from its SVD. Note that SVD can also be inferred from the polar decomposition. (Here, we do not specify the details of this derivative. Theorem 3.6 (Polar decomposition) Let A-Rm \u00b7 n be a given matrix in which m-QT n is given. Then its polar decomposition exists; that is, there is a column, the V2S-V2quormal matrix Q, and a unique SPSD matrix S in which A-Q2 times n is given."}, {"heading": "3.3 Matrix Concepts via SVD", "text": "All matrices have SVD, so SVD plays a central role in matrix analysis and calculation. As we have seen in the previous section, many matrices"}, {"heading": "22 The Singular Value Decomposition", "text": "Definition 3.1. Definition: Definition: Definition: Definition: Definition: Definition: Definition: Definition: Definition: Definition: Definition: Definition: Definition: Definition: Definition: Definition: Definition: Definition: Definition: Definition: Definition: Definition: Definition: Definition: Definition: Definition: Definition: Definition: Definition: Definition: Definition: Definition: Definition: Definition: Definition: Definition: Definition: Definition: Definition: Definition: Definition: Definition: Definition: Definition: Definition: Definition: Definition: Definition: Definition: Definition: Definition:: Definition: Definition:: Definition:: Definition:: Definition:: Definition:: Definition:: Definition::: Definition::: Definition:::: Definition::: Definition:::: Definition:::: Definition:::: Definition:::: Definition::::"}, {"heading": "3.4. Generalized Singular Value Decomposition 23", "text": "The statistical lever [Hoaglin and Welsch, 1978] measures the extent to which the singular vectors of a matrix are correlated with the standard basis. Recently, it has found its usefulness in large-scale data analysis and in the analysis of randomized matrix algorithms [Drineas et al., 2008, Mahoney and Drineas, 2009, Ma et al., 2014]. A related term is matrix coherence, which was of interest for matrix completion and Nystr\u00f6m-based matrix approximation [Cand\u00e8s and Recht, 2009, Talwalkar and Rostamizadeh, 2010, Wang and Zhang, 2013, Nelson and Nguy\u00ean, 2013]."}, {"heading": "3.4 Generalized Singular Value Decomposition", "text": "Theorem 3.7 (GSVD). Suppose there are two matrices A, Rm, p and B, R, n, p and an invertable matrix X, Rp and p, such as UTAAX = diag (\u03b11,.., \u03b1q) and U T, BBX = diag (\u03b21,.., \u03b2p), where \u03b11 \u2265 \u00b7 \u00b7 \u03b1q \u2265 0 and 0 \u2264 \u03b21 \u2264 \u03b2 \u00b7 \u03b2p. The GSVD theorem was originally proposed by Loan [1976], in which n, \u2265 p (or m, p) is required. Later, Paige and Saunders [1981] developed a more general formula required only for the SVD and SVD in the SVD 1999."}, {"heading": "24 The Singular Value Decomposition", "text": "Theorem 3,8 (The CS composition) Let Q-R (m + > Q1) \u00b7 p be an orthonormal column matrix. Divide it as QT = [QT1, Q T 2], where Q1 and Q2 exist arem \u00b7 p and n \u00b7 p. Then there are orthonormal matrices U1 Rm \u00b7 m, U2 Rn \u00b7 n, and V1 p \u00b7 p so that UT1 Q1V1 = C and U T 2 Q2V1 = S, where C = r s p \u2212 r Ir 0 s 0 0 0 C1 0 m \u2212 s \u2212 r \u2212 s 0 S1 0 p \u2212 r \u2212 s 0 Ip \u2212 s, C1 = diag (1,."}, {"heading": "3.4. Generalized Singular Value Decomposition 25", "text": "This implies that W1 = 0 and WT2 W2 = Ip \u2212 r \u00b7 DTD = diag (1 \u2212 c2r + 1,. \u2212 c2p) are not singular. Let us now define si = \u221a 1 \u2212 c2i for i p. The last p \u2212 r columns from which z (1 \u2212 sr + 1,. \u2212 s, 1 \u2212 cr + s) is column orthonormal. We extend Z to an n \u2212 n orthonorthonormal matrix U2, the last p \u2212 r columns of which are u n. At setting 1 = cr + 1, \u00b7, \u03b1s = cr + s, we have UT2 Q1V1 = S.This is the theorem. Remarks It is worth pointing out that Q1 = U2SV T 1 is not certainly a complete SVD of Q1, because some of the non-elements of S are not on the diagonal in principle."}, {"heading": "26 The Singular Value Decomposition", "text": "What makes R-Rt \u00b7 t a positive diagonal matrix with its diagonal elements equal to the non-zero of the singular values of K, A = r s t \u2212 r \u2212 s r Ir 0 0 s 0 DA 0 m \u2212 r \u2212 s 0 0, (3,7) B = r s t \u2212 r \u2212 s n + r \u2212 t 0 0 s 0 DB 0 t \u2212 r \u2212 s 0 \u2212 r \u2212 s 0 \u2212 r \u2212 s (3,8) Here r and s depends on the context, DA = diag (\u03b1r + 1,., r + s) and DB = diag (1 \u2212 \u03b12r + 1,.,., 1 \u2212 \u03b12r + s), and 1 > \u03b1r + 1 \u2265 \u00b7 \u00b7 r + s > 0.Theorem 3.9 implies that UTAAX = [A, 0] and U T BBX = [P-P-1W-Ip \u2212 t)."}, {"heading": "3.4. Generalized Singular Value Decomposition 27", "text": "The application of Theorem 3.8 to P1 indicates that there are orthonormal matrices that are defined UA-Rm \u00b7 m, UB-Rn \u00b7 n and W-Rt \u00b7 t in such a way that [UTA 00 UTB] [P11 P21] W = [\u0421A-Rm \u00b7 B], where \"A\" and \"B\" are defined in (3.7) and (3.8). Consequently, [UTA 00 UTB] [AB] is V-AW TR 0-BW TR 0]. That is, UTAAV-A [W TR, 0] and UTBBV-B [W TR, 0]. In relation to Theorem 3.7, if \u03b2i 6 = 0, then the column xi is of \"X.\" ATAxi = \"iB TBxi,\" where \"Y-TBxi,\" where \"2\" i\u03b22 i. This implies that \"Up.\""}, {"heading": "28 The Singular Value Decomposition", "text": "We now have that AX = UUTAV (\u03a3 \u2212 1t VY Ip \u2212 t) = [UtVY, 0] = U (VY Im \u2212 t) (It 0) andBX = ZUUTAV (\u03a3 \u2212 1t VY Ip \u2212 t) = ZUt [VY, 0] = UY YV T Y (VY, 0] = UY (VTY Y Y, 0].Also, (VTY Im \u2212 t) UTAX = [It Y, 0].In this particular case, we only need to implement two SVDs on two matrices with smaller sizes. The diagonal elements of Y and the columns of VtA \u2212 1 t VY are the generalized eigenvalues and eigenvectors of the corresponding eigenvalues problematisis.Remarks assume that A Rm n and B Rm n have the same size."}, {"heading": "30 Applications of SVD: Case Studies", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.1 The Matrix MP Pseudoinverse", "text": "Considering a matrix A-Rm \u00b7 n and a vector b-Rm = A-Rm, we are also concerned with the least square estimation problem: x-Rp = argmin x-Rn-Ax \u2212 b-22. (4.1) The minimizer should meet the Karush-Kuhn-Tucker (KKT) condition: this is the solution of the following normal equation: ATAx = ATb. (4.2) Let A = Uri-RV T r be the condensed SVD of A. Then VrV T r x = Vrville T r-B. Define A-Rp-Rp \u2212 1 r-Rn \u00b7 m. Obviously, x-R-RV T is a minimizer. It is clear that if A-RV-T is invertable, then the minimizer x-A-1b is. Thus, A-Rp-1 is a generalization of \u2212 1 in the case that A-R.i.e matrix is any matrix."}, {"heading": "4.1. The Matrix MP Pseudoinverse 31", "text": "Similarly, BA = CA."}, {"heading": "B = BAB = BAC = CAC = C.", "text": "Let us see its application in solving generalized eigenproblems. If we consider two matrices M and N as q eigenpairs of the matrix pencil (M, N), then we refer to MX = diag (\u03bb1,..., \u03bbq) and X = [x1,.., xq] as q eigenpairs of the matrix pencil (M, N), if MX = NX\u0441; namely Mxi = \u03bbiNxi, for i = 1,..., q.The problem of finding eigenpairs of (M, N) is known as a generalized eigenpairs problem. If N = Im becomes the problem of conventional eigenvalues, we are usually interested in the problem with the nonzero problem of i = 1,..., q and refer to the problem of finding eigenpairs of (M, X) as eigenpairs of (M, N)."}, {"heading": "32 Applications of SVD: Case Studies", "text": "Proof. Let M = U1\u04411V T 1 and N = U2\u04412V T 2 be the condensed SVD of M and N. Furthermore, we have a range (M) = range (U1) and a range (N) = range (U2), which means that U1 can not only be expressed as U1 = U2Q, where Q is a matrix of appropriate order. Consequently, we have NN \u2020 M = U2U T 2 U2Q\u04351V T 1 = M.It is worth noting that the condition NN \u2020 M = M is not only necessary, but also sufficient for the range (M) range (N) is (N, X) the eigenpairs of N \u2020 M, then it is easy to see that the eigenpairs of Xpoem (M = M) and the eigenpairs of N = M (N), the eigenpairs of M = vertical."}, {"heading": "4.2 The Procrustes Problem", "text": "Orthogonal Procrust Analysis aims to shift Y relative by rotation in X [Gower and Dijksterhuis, 2004]. Specifically, the Procrust problem is defined as asmin Q-Rp-p-X-YQ-2F s.t. QTQ = Ip. (4.3) Theorem 4.3. Let the complete SVD of YTX be YTX = UVT. Then UVT is the minimizer of the Procrust problem in (4.3)."}, {"heading": "4.3. Subspace Methods: PCA, MDS, FDA, and CCA 33", "text": "Prove that the constants QTQ = Ip are equivalent to q T i qi = 1 for i = 1,.., p, and qTi qj = 0 for i 6 = j. Here, the Qi are the columns of Q. Therefore, the Qi function is istr (YTXQT) \u2212 1 2p \u2211 i = 1cii (q T i qi \u2212 1) \u2212 12 \u0445 i > jcij (q T i qj \u2212 0), which is written in the matrix form asL (Q, C) = tr (YTXQT) \u2212 2 \u2212 tr [C (QTQ \u2212 Ip] Q > jcij (q T i qj \u2212 0), which is in the matrix form asL (Q, C)."}, {"heading": "4.3 Subspace Methods: PCA, MDS, FDA, and CCA", "text": "Subspace methods such as Principal Component Analysis (PCA), Multidimensional Scaling (MDS), Fisher discriminant analysis (FDA) and Canonical Correlation Analysis (CCA) are a class of important machine learning methods. SVD plays a fundamental role in subspace learning methods."}, {"heading": "34 Applications of SVD: Case Studies", "text": "PCA [Jolliffe, 2002, Kittler and Young, 1973] and MDS [Cox and Cox, 2000] are two classic methods of dimension reduction. Let A = [a1,. \u00b7 an] T be a predefined data matrix in which each vector ai represents a data instance in R p. Let m = 1n \u2211 n = 1 ai = 1 nA T1n be the sample mean and Cn = In \u2212 1n1nITn be a so-called centered matrix. Pooled scatter matrix is defined as (omit a multiplier 1 / n) S = n \u00b2 (ai \u2212 m) (ai \u2212 m) T = ATCnA = ATCnA. It is well known that PCA calculates the spectral decomposition of S, while classical MDS or principal coordinate analysis (PCO) calculates the spectral decomposition of the class (PCO)."}, {"heading": "4.3. Subspace Methods: PCA, MDS, FDA, and CCA 35", "text": "In matrix form: SbX = SX\u0442, (4.4), where X = [x1,..., xq] (n \u00b7 q) and vice versa = diag (\u03bb1,..., \u03bbq) (q \u00b7 q). Then, Sb can be rewritten using the GSVD method [Loan, 1976, Paige and Saunders, 1981, Golub and Van Loan, 2012, Howland et al., 2003]. Furthermore, it is obvious that the area (Sb) area (ATCn) = area (S) can be solved. Theorem 4.2 therefore offers a solution if S is singular or almost singular. Furthermore, the method given in Theorem 3.10 is suitable for solving the primary area (ATCn) within the CCA group."}, {"heading": "4.3.1 Nonlinear Extensions", "text": "The reproduction of kernel theory [Aronszajn, 1950] provides an approach for nonlinear extensions of subspace methods. For example, Kernel PCA [Sch\u00f6lkopf et al., 1998], Kernel FDA [Baudat and Anouar, 2000, Mika et al., 2000, Roth and Steinhage, 2000], Kernel CCA [Akaho, 2001, Van Gestel et al., 2001, Bach and Jordan, 2002] have been successively proposed and widely applied in data analysis. Kernel methods work in a feature space F, which is related to the original input space X-Rp by mapping."}, {"heading": "36 Applications of SVD: Case Studies", "text": "To implement a kernel method without explicitly resorting to the so-called kernel trick [Sch\u00f6lkopf and Smola, 2002, Shawe-Taylor and Christianini, 2004]. L2 (X) is the quadratic integrable Hilbert space of functions whose elements are defined in X. It is a well-known result that if K is a reproducing kernel for Hilbert space L2 (X), then L2 (X) extends to the quadratic Hilbert space of functions whose functions are defined in X. It is a well-known result that if K is a reproducing kernel for Hilbert space L2 (X), then L2 (X) extends to the quadratic Hilbert space of functions."}, {"heading": "5.1 The QR Factorization", "text": "QR factorization is another decomposition method applicable to all matrices. In the face of a matrix A-Rm \u00b7 n, QR factorization is given by A = QR, where Q-Rm \u00b7 m is orthonormal and R-Rm \u00b7 n is upper triangle (or low triangle). Let D be a m-m diagonal matrix with diagonal elements either 1 or \u2212 1. Then A = (QD) (DR) is still a QR factorization of A. Therefore, we always assume that R has non-negative diagonal elements. 37"}, {"heading": "38 The QR and CUR Decompositions", "text": "Suppose matrix A also has a thin QR factorization: A = QR, where Q = Rm \u00b7 n is currently column orthonormally, and R \u0440Rn \u00b7 n is upper triangle with non-negative diagonal elements. If A is rank n, R is unambiguously determined. In this case, Q = AR \u2212 1 is also unambiguously determined. Asume A is rank r (\u2264 min {m, n}). Then there is a m \u00b7 m orthonormally matrix Q and a n \u00b7 n permutation matrix P such as QTAP = [R11 R120 0], where R11 is a r \u00b7 r upper triangular matrix with positive diagonal elements. This is called rank, which reveals QR factorization. Calculation of QR factorization can be arranged by the novel Gram-Schmidt orthogonalization process or the modified GramSchmidt matrix, which is numerically more stable than QR factorization."}, {"heading": "5.2 The CUR Decomposition", "text": "As we have seen, SVD leads us to a geometric representation, and QR factorization facilitates calculations, which have little concrete meaning, making it difficult to understand and interpret the data in question. Kuruvilla et al. [2002] claimed: \"It would be interesting to try to find base vectors for all experimental vectors, using actual experimental vectors and not artificial bases that offer little insight.\" Therefore, it is of great interest to represent a data matrix in relation to a small number of actual columns and / or actual rows of the matrix. Matrix column selection and CUR matrix decomposition provide such techniques."}, {"heading": "5.2. The CUR Decomposition 39", "text": "The CUR decomposition problem has been extensively discussed in the literature [Goreinov et al., 1997a, b, Stewart, 1999, Tyrtyshnikov, 2000, Berry et al., 2005, Drineas and Mahoney, 2005, Bien et al., 2010], and it has been shown to be very useful in high-dimensional data analysis.The CUR was originally referred to as skeletal substitution [Goreinov et al., 1997a]. Let us divide the A-Rm-n matrix of rank r. Then there is a non-ingular r submatrix in A. Without loss of universality, we assume that this non-ingular matrix is the first r-r submatrix of A. That is, A can be divided into the following form: A = [A11 A12 A21 Ajr-J = 21x."}, {"heading": "40 The QR and CUR Decompositions", "text": "The CUR decomposition is also an extension of the novel Nystr\u00f6m approximation to a general matrix. The Nystr\u00f6m method approaches a PLC matrix only using a subset of its columns, so we can only use a subset of its columns for the PLC matrix, so it can alleviate the costs of calculation and storage."}, {"heading": "42 Variational Principles", "text": "\u03bb (M) = (\u03bb1 (M),.., \u03bbn (M)) T stand for the eigenvalues of a n \u00b7 n real square matrix M, and \u03c3 (A) = (\u03c31 (A),.., \u03c3p (A)) T stand for the singular values of a m \u00b7 n real matrix A. Sometimes we also write \u03c3i or \u03bbi to them if they are explicit in the context of notation simplification."}, {"heading": "6.1 Variational Properties for Eigenvalues", "text": "In this section we will consider eigenvalue variation properties of a real symmetrical matrix. It is well known that for any symmetrical matrix its eigenvalues are all real; the following Cornerstone Theorem was originally developed by Neumann [1937]. Theorem 6.1 (von Neumann Theorem). Suppose the eigenvalues are all symmetrical. Thenn Theory i = 1\u03bbi (M) \u03bbi (N) = max QQT = Intr (QMQTN). Moreover, the second part immediately follows from the first part becausemin QQT = In tr (QMQTN) = \u2212 max QT = In tr (\u2212 N) = min QMQT = and QMdiM = (QM)."}, {"heading": "6.1. Variational Properties for Eigenvalues 43", "text": "Let us leave Q = [qij] = [q1,., qn] T =. Moreover, we now have VTR (Q\u0430MQ T\u043eN) = n \u2211 i = 1qTi \u0109Mqi\u03bbi (N) = n \u2212 1 \u2211 i = 1i \u2211 j = 1qTj \u0445 Mqj [\u03bbi (N) \u2212 \u03bbi + 1 (N)] + \u03bbn (N) n \u2211 j = 1qTj \u0445 Mqj = n \u2212 1 \u0445 i = 1 [\u03bbi (N) \u2212 \u03bbi (TMi + 1 (N)] i \u2211 j = 1n \u2211 k = 1q2jk\u03bbk (M) + \u03bbn (N).TMj W, [q2ij], which is double stochastical, and u = [u1,..., un] T, where uj = 1 q 2x.m (M). That is, u = W.m (Q2i.m)."}, {"heading": "44 Variational Principles", "text": "In the appendix we give a further proof based on the theory of matrix differentials. The von Neumann theorem describes the principle of variation of eigenvalues of a symmetrical matrix. On the basis of theorems 6.2 we have the following properties of variation: theorem 6.1. With two n \u00b7 n real symmetrical matrices M and N we have these (1) \u03bb (M + N) \u03bb (M) + \u03bb (N) and \u03bb (M) \u2212 \u03bb (N) \u03bb (M \u2212 N). (3) (m11.,., mnn). (1 (M + TMi = 1)."}, {"heading": "6.1. Variational Properties for Eigenvalues 45", "text": "Proposition 6.1- (3) is sometimes referred to as Schurs theorem. The second part of the following theorem is an extension of Schurs theorem.Proposition 6.2. Let M = [M11 M12 M21 M22] n \u00b7 n be real symmetrical. Here, M11 is k \u00b7 k. Then (1) \u03bbi (M).In addition, for each column orthonorthonormal matrix we have Q-k + i (M) for i = 1,.., k; and (2) (\u03bb (M11), \u03bb (M22)).In addition, for each column orthonorthonormal matrix we have Q-Rn \u00b7 k [3) \u03bbi (M). And (MTMQ-k + i (M) for i = 1,.."}, {"heading": "46 Variational Principles", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "6.2 Variational Properties for Singular Values", "text": "Theorems 6.1 and 6.2 can be extended to a general matrix. In this case, we examine singular values of the matrix instead. Theorems 6.3 and 6.4 correspond to theorems 6.1 and 6.2, respectively, theorem 6.3 (Ky Fan Theorem). In the face of two matrices A \u0440Rm \u00b7 n and B \u0440Rm \u00b7 n, A and B leave full SVDs A = UA\u0445 AVTA and B = UB\u041aBV T B, respectively p = min {m, n}. Consequently, i = 1\u03c3i (A) \u03c3i (B) \u03c3i (B) = max XT X = Im, YT = Im, YT Y = Intr (XTAYBT), which in the theorems T = UAU T B and Y = Theorp."}, {"heading": "6.2. Variational Properties for Singular Values 47", "text": "(1)......................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................"}, {"heading": "48 Variational Principles", "text": "If n = p = m, then equality applies to k = n. Andk \u2211 i = 1\u03c3i (AB) \u2264 k \u2211 i = 1\u03c3i (A) \u03c3i (B) \u2264 (k \u2211 i = 1\u03c3i (A)) (k \u2211 i = 1\u03c3i (B)). Proof. Let AB = U\u043fVT be a complete SVD of AB, and for k \u2264 q Uk and Vk the first k columns of U and V. Let us now take a polar decomposition of BVk as BVk = QS. Since S 2 = VTkB TBVk and through Proposition 6.3- (4), we get (S2) = det (VTkB TBVk) \u2264 k \u0445i = 1\u04452i (B) We have further thatk \u0445i = 1\u0445i (AB) = | det (UTkABVk) = det (S) = det (VTkAQ)."}, {"heading": "6.3 Appendix: Application of Matrix Differentials", "text": "Here we present alternative proofs for theorem 6.2 and theorem 6.4, which are based on matrix differentials. It aims to further illustrate the use of matrix differentials. The second proof for theorem 6.2. To solve the problem, we define the Lagrange function: L (Q, C) = tr (QTMQ) \u2212 tr (C (QTQ \u2212 Ik),"}, {"heading": "6.3. Appendix: Application of Matrix Differentials 49", "text": "Here C is a k \u00b7 k symmetrical matrix of Lagrangian multipliers (SincedL = tr (dQTMQ + QTMdQ) \u2212 tr (C (dQTQ + QTdQ))), this shows that dLdQ = 2MQ \u2212 2QC. The KKT condition is nowMQ \u2212 QC = 0, Sure, if C \u00b2, diag (\u03bb1,. \u2212 \u2212 k) and Q \u00b2 consist of the corresponding orthonormal eigenvectors, they are a solution of the above equation. In this setting we see that tr (Q \u00b2 TMQ \u00b2) = \u2211 ki = 1 \u00b2 \u2212 t.So we just have to prove that Q \u00b2 is actually the maximizer of the original problem. We now calculate the Hessian matrix of L \u00b2, THY \u00b2."}, {"heading": "50 Variational Principles", "text": "Here C1 and C2 are two k \u00b7 k \u2212 symmetrical matrix of the Lagrange multipliers. SincedL = tr (dXTAY) \u2212 1 2 tr (C1 (dX TX + XTdX)), dL = tr (XTAdY) \u2212 1 2 tr (C2 (dY TY + YTdY)), which lead to this dLdX = AY \u2212 XC1 and dLdY = XAT \u2212 YC2. The KT condition is now AY \u2212 XC1 = 0 and ATX \u2212 YC2 = 0, it follows from XTX \u2212 Ik that C1 = C2. We denote C, C1 \u2212 XY \u2212 XC = 0, ATX \u2212 YveY \u2212 Y."}, {"heading": "6.3. Appendix: Application of Matrix Differentials 51", "text": "So, for each ZT1 ZT1 Z1K and Z2 ZT2 ZT2 ZT2 ZT2 ZT2 ZT2 ZT2 ZT2 ZT2) ZT2 ZT2 ZT2 ZT2) ZT2 ZT2 ZT2) ZT2 ZT2 ZT2 ZT2) ZT2 (Z2) ZT2) ZT2 (Z2) ZT2 (Z2) ZT2) ZT2 (Z2) ZT2 (Z2) ZT2) ZT2 (ZT2) ZT2) ZT2 (ZT2) ZT2 (ZT2) ZT2 (Z2) Z2 (Z2) (Z2) (Z2) Z2) (Z2) (Z2) (Z2)."}, {"heading": "7.1. Matrix Norms 53", "text": "Secondly, it can include the majority theory. Accordingly, we specify some important properties of uniformly invariant norms."}, {"heading": "7.1 Matrix Norms", "text": "A function f: Rm \u00b7 n \u00b7 R is called a matrix norm if the following conditions are met: (1) f (A) > 0 for all matrix A-Rm \u00b7 n; (2) f (\u03b1A) = | \u03b1 | f (A) for all \u03b1-R and all A-Rm \u00b7 n; (3) f (A + B) \u2264 f (A) + f (B) for all A and B-Rm \u00b7 n.Moreover, when referring to a matrix norm, it means that it is consistent. If referring to a matrix norm for Rn \u00b7 n, it must be consistent. Here, we are not making this requirement. There is an equivalence between two norms. If referring to a matrix norm for Rn \u00b7 n, it must be consistent."}, {"heading": "54 Unitarily Invariant Norms", "text": "Sentence 7.1. The dual approach of matrix A has the following properties: (1) The dual approach is a standard. (2) (2) (3) tr (ABT) \u2264 | tr (ATB). (3) There are two approaches to defining a matrix standard. In the first approach, the standard of matrix A is defined by its vectorization standards to facilitate exposure. Note that the Frobenius standard is a vectorization standard that obviously meets conditions (1) - (3). We refer to this class of matrix standards as matrix vectorization norms."}, {"heading": "7.1. Matrix Norms 55", "text": "The x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x"}, {"heading": "56 Unitarily Invariant Norms", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "7.2 Symmetric Gauge Functions", "text": "Definition 7.3. A real function \u03c6: Rn \u2192 R is called a symmetric measuring function if it fulfils the following four conditions: (1) \u03c6 (u) > 0 for all unequal u-Rn. (2) \u03c6 (\u03b1u) = | \u03b1 | \u03c6 (u) for all constant \u03b1-R. (3) \u03c6 (u + v) \u2264 \u03c6 (u) + \u03c6 (v) for all u, v-Rn. (4) \u03c6 (Du\u03c0) = \u03c6 (u), where u\u03c01,.., u\u03c0n) with \u03c0 as permutation of [n] and D as n-shadow-n diagonal matrix with \u00b1 1 diagonal elements. Furthermore, the measuring function is normalized if it fulfils the condition."}, {"heading": "7.2. Symmetric Gauge Functions 57", "text": "??????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????"}, {"heading": "58 Unitarily Invariant Norms", "text": "It is worth mentioning that the proof of the theorem 7.4 means that an infinite unit results in an infinite family of standardization inequalities. Definition 7.4. The dual of a symmetrical measuring function \u03c6 on Rn is defined as \u03c6 (u), max {uTv: v-Rn, \u03c6 (v) = 1}. Proposal 7.2. Let \u03c6 be the dual of the symmetrical measuring function \u03c6. Then \u03c6 is also a symmetrical measuring function. Beyond that (????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????)???????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????"}, {"heading": "7.3 Unitarily Invariant Norms via SGFs", "text": "Theorem 7.5. If a uniformly invariant standard is given on Rm \u00b7 n, then there is a symmetrical measuring function \u03c6 on Rq, where q = min {m, n} is such that | | A | = \u03c6 (A) is for all A-Rm \u00b7 n.Conversely, if \u03c6 is a symmetrical measuring function on Rq, then | | A | |, \u03c6 (A) is a uniformly invariant standard on Rm \u00b7 n.Proof. In view of a uniformly invariant standard | | | | | | on Rm \u00b7 n and a vector x-Rq, we define \u03c6 (x), | | X | | | where X = xij] on Rm \u00b7 n is satisfactory that xii = xi is equal for all elements on Rm-n and a vector x-Rq."}, {"heading": "7.3. Unitarily Invariant Norms via SGFs 59", "text": "In the meantime, we have proved that \"A\" is a uniformly invariant standard. First, that \"A\" is for \"A\" = \"A = 0\" and \"B\" = \"A = 0\" = \"A = 0\" = \"A = 0\" = \"A = 0\" = \"A = 0\" = \"A\" = \"A = 0\" = \"A = 0\" = \"A = 0\" = \"A = 0.\" We now prove that \"A\" = \"A =\" 0 = 0 \"is a uniformly invariant standard."}, {"heading": "60 Unitarily Invariant Norms", "text": "So it is a symmetrical p standard for p \u2265 1, also known as the symmetrical p standard. Theorem 7.5 thus shows that | | | A | | E standard (A) for p \u2265 1 is a class of uniformly invariant standards. They are well known as the shadow p norms. Therefore, the \"A norm\" is the \"A norm\" for the \"A norm.\" The \"A norm\" for the \"A norm.\" The \"A norm\" for the \"A norm.\" The \"A norm\" for the \"A norm\" is the \"A norm.\" The \"A norm\" for the \"A norm.\" The \"A norm for the\" A norm \"is the\" A norm. \""}, {"heading": "7.4 Properties of Unitarily Invariant Norms", "text": "Theorem 7.5 opens an approach to the study of uniformly invariant norms by using symmetric measurement functions and majorization theory. We will see that this makes things more tractable. Theorem 7.7. However, if the norm is defined in Rm \u00b7 n, Theorem 6.6 cannot help to determine the consistency of the corresponding uniformly invariant norm. As a direct consequence of Theorem 7.5, we have the following result, which shows that uniformly invariant norms are monotonous. Theorem 7.8."}, {"heading": "7.4. Properties of Unitarily Invariant Norms 61", "text": "Proposition 7.3. In view of a matrix A-Rm \u00b7 n, you can obtain [A] r by replacing the last r rows and r columns of A with zeros, and < A-R > \u2212 R by replacing the last r rows or columns of A with zeros. Proof. Part (1) follows directly from Proposition 6.3, which shows that \"A\" (A) r \"w\" (< A > r) w \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\") \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p"}, {"heading": "62 Unitarily Invariant Norms", "text": "In this context, it should be noted that the theorem of Hoffman-Wielandt is still valid if A and B are normal. Theorem 7,9. Let us consider | | | | | | q | | | | | | | | | | |. (1) The latter result is widely known as the theorem of Hoffman-Wielandt. (1) Note that the theorem of Hoffman-Wielandt is still valid if A and B are normal. (2)"}, {"heading": "7.4. Properties of Unitarily Invariant Norms 63", "text": "Note that a norm that applies to Rm \u00b7 n seems to assign itself to V \u00b7 V \u00b7 V \u00b7 V \u00b7 V \u00b7 V \u00b7 V \u00b7 V \u00b7 V \u00b7 V \u00b7 V \u00b7 V \u00b7 V \u00b7 V \u00b7 V \u00b7 V \u00b7 V \u00b7 V \u00b7 V \u00b7 V \u00b7 V \u00b7 V \u00b7 V \u00b7 V \u00b7 V \u00b7 V \u00b7 V \u00b7 V \u00b7 V \u00b7 V \u00b7 V \u00b7 V \u00b7 V \u00b7 V \u00b7 V \u00b7 V \u00b7 V \u00b7 V \u00b7 V \u00b7 V \u00b7 V \u00b7 V \u00b7 V \u00b7 V \u00b7 V \u00b7 V \u00b7 V \u00b7 V \u00b7 V \u00b7 V \u00b7 V \u00b7 V \u00b7 V \u00b7 V \u00b7 V \u00b7 V \u00b7 V \u00b7 V \u00b7 V \u00b7 V \u00b7 V \u00b7 V \u00b7 V \u00b7 V \u00b7 V \u00b7 V \u00b7 V \u00b7 V \u00b7 V \u00b7 V \u00b7 V \u00b7 V \u00b7 V \u00b7 V \u00b7 V \u00b7 V \u00b7 V \u00b7 V \u00b7 V \u00b7 V \u00b7 V \u00b7 V \u00b7 V \u00b7 V \u00b7 V \u00b7 V \u00b7 V \u00b7 V \u00b7 V \u00b7 V \u00b7 V \u00b7 V \u00b7 V \u00b7 V \u00b7 V \u00b7 V \u00b7 V \u00b7 V \u00b7 V \u00b7 V \u00b7 V \u00b7 V \u00b7 V \u00b7 V \u00b7 V \u00b7 V \u00b7 V \u00b7 V \u00b7 V \u00b7 V \u00b7 V \u00b7 V \u00b7 V \u00b7 V \u00b7 V \u00b7 V \u00b7 V \u00b7 V \u00b7 V \u00b7 V \u00b7 V \u00b7 V \u00b7 V \u00b7 V \u00b7 V \u00b7 V \u00b7 V \u00b7 V \u00b7 V \u00b7 V \u00b7 V \u00b7 V \u00b7 V \u00b7 V \u00b7 V \u00b7 V \u00b7 V \u00b7 V \u00b7 V \u00b7 V \u00b7 V \u00b7 V \u00b7 V \u00b7 V \u00b7 V \u00b7 V \u00b7 V \u00b7 V \u00b7 V \u00b7 V \u00b7 V \u00b7 V \u00b7 V \u00b7 V \u00b7 V \u00b7 V \u00b7 V \u00b7 V \u00b7 V \u00b7 V \u00b7 V \u00b7 V \u00b7 V \u00b7 V \u00b7 V \u00b7 V \u00b7 V \u00b7 V \u00b7 V \u00b7 V \u00b7 V \u00b7 V \u00b7 V \u00b7 V \u00b7 V \u00b7 V \u00b7 V \u00b7 V \u00b7 V \u00b7 V \u00b7 V \u00b7 V \u00b7 V \u00b7 V \u00b7 V \u00b7 V \u00b7 V \u00b7 V \u00b7 V \u00b7 V \u00b7 V \u00b7 V \u00b7 V \u00b7 V \u00b7 V \u00b7 V \u00b7 V \u00b7 V \u00b7 V \u00b7 V \u00b7 V \u00b7 V \u00b7 V \u00b7 V \u00b7 V \u00b7 V \u00b7 V \u00b7 V \u00b7 V \u00b7 V \u00b7 V \u00b7 V \u00b7 V \u00b7 V \u00b7 V \u00b7 V \u00b7 V \u00b7 V \u00b7 V \u00b7 V \u00b7 V \u00b7 V \u00b7 V \u00b7 V \u00b7 V \u00b7 V \u00b7 V \u00b7 V \u00b7 V \u00b7 V \u00b7 V \u00b7 V \u00b7 V \u00b7 V \u00b7 V \u00b7 V \u00b7 V \u00b7 V \u00b7 V \u00b7 V \u00b7 V \u00b7 V \u00b7 V \u00b7 V \u00b7 V \u00b7 V V"}, {"heading": "64 Unitarily Invariant Norms", "text": "Proposition 7.5: The matrix A = Rm \u00b7 n, then the following points: \"A\" = \"A\" = \"A\" = \"A\" = \"A\" = \"A\" = \"A\" = \"A\" = \"A\" = \"A\" = \"A\" = \"A\" = \"A\" = \"A\" = \"A\" = \"A\" = \"X\" = \"X\" = \"X\" = \"X\" = \"X\" = \"X\" = \"X\" = \"X.\""}, {"heading": "7.4. Properties of Unitarily Invariant Norms 65", "text": "As a consequence of Proposition 7.5, the following proposal follows immediately. Furthermore, this proposal has been frequently used in matrix completion problems because an optimization problem regulated by the Frobenius standard is more easily solved than the problem regulated by the nuclear standard [Hastie et al., 2014]. Proposition 7.7. [Srebro et al., 2004, Mazumder et al.] In view of a matrix A-Rm \u00b7 n, the following applies: A-Rm = min X, Y: XYT = A1 2 [1-X-2F-Y-2F]. If rank (A) + k-Rm + n \u00b2, the above minimum is reached at a certain degree of decomposition. The following theorem shows that the Frobenius standard has a so-called ATrix-Pythagoras property."}, {"heading": "66 Unitarily Invariant Norms", "text": "In other words, the spectral standard is just a single satisfaction. (...) We are both able to differentiate ourselves. (...) We are both able to differentiate ourselves. (...) We are both able to show the relationship between ourselves and the world. (...) We are both able to show the relationship between ourselves and the world. (...) We are both able to show the relationship between ourselves and the world. (...) We are both able to show the relationship between ourselves and the world. (...) We are both able to recognize the relationship between ourselves and the world. (...) We are both able to recognize the relationship between ourselves and the world of the world. (...) We are both able to recognize the relationship between ourselves and the world of the world. (...) We are both able to recognize the relationship between ourselves and the world of the world. (...) We are both able to recognize the relationship between ourselves and the world of the world. (...) We are both able to recognize the relationship between ourselves and the world of the world of the world. (...) We are both able to recognize the relationship between ourselves and the world of the world of the world and the world of the world. (...) We are able to recognize the relationship between ourselves and the relationship between ourselves and the world of the world of the world of the world. (...) We are both able to recognize the relationship between ourselves and the relationship between ourselves and the world of the world of the world of the world of the world of the world. (...) We are able to recognize the relationship between us and the relationship between us. (... We are both able to recognize the relationship between us and the relationship between us and the relationship between us. (...) We are able to recognize the relationship between us. (...) We are both in the world of the relationship between the world of the world of the world of the world of the world of the world of the world of the world and the world of the world of the world of the world of the world of the world of the world, the world of the world of the world of the world, the world of the world of the world of the world of the world of the world, the world of the world of the world of the world, the world of the world of the world of the world of the world, in the world of the world of the world of the world of the world"}, {"heading": "68 Subdifferentials of Unitarily Invariant Norms", "text": "These are problems of estimating the smallest squares, the loss function of which is defined as a uniformly invariant standard."}, {"heading": "8.1 Subdifferentials", "text": "The subdifferential, a group of subgradients, is defined as [G] Rm \u00b7 n: [G] Rm \u00b7 n: [B] R + tr ((B \u2212 A) TG) for all B [Rm \u00b7 n] and denoted by [A]. If the norm [A] is differential, the subgradient degenerates into a gradient. That is, the subdifferential is a singleton. If, for example, you look at the square Frobenius norm [A] 2F = tr (ATA), the subgradient becomes a gradient."}, {"heading": "8.1. Subdifferentials 69", "text": "Here p = min., n =, T =, T =, V =, V =, V =, V =, V =, V =, V =, V =, V =, Z =, Z =, Z =, Z =, Z =, Z =, Z =, Z =, Z =, Z =, Z =, Z =, Z =, Z =, Z =, Z =, Z =, Z =, Z =, Z =, Z =, Z =, Z =, Z =, Z =, Z =, Z =, Z =, Z =, Z =, Z =, Z =, Z =, Z =, Z =, Z =, Z =, Z =, Z =, Z =, Z =, Z =, Z =, Z =, Z =, Z =, Z =, Z =, Z =, Z =, Z =, Z =, Z =, Z =, Z =, Z =."}, {"heading": "70 Subdifferentials of Unitarily Invariant Norms", "text": "The first inequality above results from \u03c3 (t) \u2212 tz \u00b2 w \u03c3 (A). The second inequality is based on the property of the undergradient of \u03c6 (t). Note that \u03c6 is a continuous function. On the basis of the definition of \u0445 \u03c6 (t), it is now directly confirmed that lim t \u2192 0 + d (t) \u2192 dT0 \u2264 (A) \u03c6 (A) dTz.This implies that the first equality also applies. Theorem 8.3. Let A \u00b2 Rm \u00b2 n have a complete SVD A = U\u0439VT, and let it happen."}, {"heading": "8.1. Subdifferentials 71", "text": "In addition, for each i, i, i, U (i) D (i) T (i)) T (i) T (i) T (i) T (i) T (i) D (i) D (i)) T (i) p (i) p (i) p (i) p (i) p (i) p (i) p (i) p (i) p (i) p (i) p (i) p (i) p (i) p (i) p (i) p)."}, {"heading": "72 Subdifferentials of Unitarily Invariant Norms", "text": "In this case, orthonormal matrices are defined in Korollar 3.3. Letters W, U \u2212 r [x] [x] [x] [x] [x] [x] [x] [x] [x] [x] [x] [x] [x] [x] [x] [x] [x] [x] [x] [x] [x] [x] [x] [x] [x] [x] [x] [x] [x] [x] [x] [x] [x] [x] [x] [x] [x] [x] [x] [x] [x] [x] [x] [x] [x] [x] [x] [x] [x] [x] [x] [x] [x] [x] [x] [x] [x] [x] [x] [x] [x] [x] [x] [x] [x] [x] [x] [x] [x] [x] [x] [x] [x] [x] [x] [x] [x] [x] [x] [x] [x] [x] [x] [x] [x] [x] [x] [x]] [x] [x] [x] [x] [x] [x]] [x] [x] [x] [x] [x] [x] [x]] [x] [x] [x] [x] [x]] [x] [x] [x] [x] [x]] [x] [x] [x] [x] [x] [x]] [x] [x] [x] [x]] [x] [x] [x] [x] [x] [x] [x] [x]] [x] [x] [x] [x]] [x] [x] [x] [x] [x]] [x] [x] [x] [x] [x] [x]"}, {"heading": "8.2. Applications 73", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "8.2 Applications", "text": "In this section, we present several examples to illustrate the application of the subdifferential of uniformly invariant standards in solving an optimization problem that is regulated by a uniformly invariant standard or is based on a uniformly invariant standard. Example 8.1. Let us consider the following optimization problem: min X-R \u00b7 nf (X), 12 x X-A-2F-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R-R"}, {"heading": "74 Subdifferentials of Unitarily Invariant Norms", "text": "It is a constant. Also, this problem is convex in X. Let A k unambiguous positive singularity values < 1 > \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _"}, {"heading": "8.2. Applications 75", "text": "As we have seen, the minimizer X has the same rank as A. Therefore, the problem in (8,5) cannot offer a low-grade solution. This problem makes the singular values of X \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 l better conditioned, because the uppermost singular values decay to \u03b4. Therefore, we call it a singular mean (SVA) operator.Example 8,3. In view of an unequal matrix A \u2212 \u2212 \u2212 Rm \u2212 n, we consider the following convex optimization problem: min X \u2212 Rm \u00b7 nf (X), \u0432 X \u2212 A \u00b2 X \u00b2 x \u00b2, (8,6), where it is a constant. \u2212 In the model above, the loss function and the regularization term are respectively defined as the spectral standard and the nuclear standard, which duplicate each other. Furthermore, this model can be considered a parallel version of the Dantzig selector [Cand\u00e8s and Tao, 2007]."}, {"heading": "76 Subdifferentials of Unitarily Invariant Norms", "text": "& & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & & # 10; & # 10; & & # 10; & & & & # 10; & & & & # 10; & & & # 10; & # 10; & # 10; & & & # 10; & & & # 10; & & & & # 10; & & & & # 10; & & & & & # 10; & & & # 10; & & & & # 10; & & & & # 10; & & # 10; & & & # 10; & & & # 10; & & & & # 10; & & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & & # 10; & & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & & # 10; & # 10; & # 10; & & # 10; & # 10; & # 10; & # 10; & & # 10; & # 10; & # 10; & # 10; & & & # 10; & # 10; & & & & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & & & & # 10; & # 10; & # 10; & & & & # 10; & # 10; & # 10; & & # 10; & # 10; & & & # 10; & & # 10; & & & &"}, {"heading": "78 Matrix Low Rank Approximation", "text": "In this chapter, we will first present some important theoretical results in low-level matrix approximation. Then, we will discuss approximate matrix multiplication. In the following chapter, we will deal with large-scale matrix approximation. We will examine randomized SVD and CUR approximation. They can also be inserted into low-level matrix approximation frameworks."}, {"heading": "9.1 Basic Results", "text": "In this chapter we present some basic results, some of which even apply to each uniformly invariant standard. Theorem 9.1. Let us therefore leave A-Rm \u00b7 n and C-Rm \u00b7 c. Then for each arbitrary X-Rc \u00b7 n and each uniformly invariant standard | | |, | A-CC \u2020 A-CX | |. In other words: C-Rm \u00b7 n and C-Rm \u00b7 c. Then for each X-Rc \u00b7 n and each uniformly invariant standard."}, {"heading": "9.1. Basic Results 79", "text": "Theorems 7.4 and 7.5 that extend the problem (9.1) to the smallest square problem (4.1) in Section 4.1. However, Theorem 9.1 shows that there is an identical solution. Theorem 9.2. Leave A-Rm \u00b7 n, C-Rm \u00b7 c, and R-Rr \u00b7 n. Then the following theorem shows the solution of a more complicated problem. Theorem 9.2. Leave A-Rm \u00b7 n, C-Rm \u00b7 c, and R-Rr \u00b7 n. Then applies to all X-Rc \u00b7 r, A-CC \u2020 R-F-CXR-R-R-R-R.F Equivalent is Theorem 9.2."}, {"heading": "80 Matrix Low Rank Approximation", "text": "Theorem 9,3 shows that the rank k truncated SVD produces the best k approximation. Originally proposed by Eckart and Young within the Frobenius norm, the theorem was generalized by Mirsky to all uniformly invariant norms. However, for each m-n real matrix B of rank k we can write it as a B-Q column, where Q-k-n matrix and C column are any k-n matrix. Therefore, \"A-B-B\" is the best complement to the SVK column."}, {"heading": "9.1. Basic Results 81", "text": "Proof: (A \u2212 QQQTA) T (QB \u2212 QTA) = 0, so that the result of the theorem is theorem 9.3.Theorem 9.4 - a variant of theorem 9.3 and theorem 9.1. Unfortunately, Bk could not be the solution to the above problem in every uniformly invariant norm, even in the spectral norm error. The reason for this is that the matrix Pythagorean identity is only applicable to the Frobenius norm hods (see theorem 7.11). However, Tropp [2015] pointed out that Frobenius norm error boundaries are not acceptable in most cases of practical interest. He even said: \"Theorem norm boundaries are generally vacant.\""}, {"heading": "82 Matrix Low Rank Approximation", "text": "It follows that theorem 6.5 results in theorem 6.5 + p = k + 1\u03c32j (A) leading to the result. Consider theorem 9.5 on theorem 9.4 to establish a spectral standard-bound margin of error. It follows that theorem 9.4, theorem 9.4, theorem 9.4, theorem 9.2 + p = k = k = k = k = k = k \u2212 QBk = F \u2264 A \u2212 QBk \u2012 A \u2212 QQQTAk \u2012 F \u2012 F \u2012 K \u2012 F \u2012 F \u2012 F \u2012 F. Consider that theorem 9.5 = K \u2212 QTAk \u2012 2F \u2012 (QQT) resulting from (A \u2212 QQQT), Q2K \u2212 V (Q2K), Q2K \u00b7 V (Q2K) and vice versa (Q2K \u2212 V)."}, {"heading": "9.2 Approximate Matrix Multiplication", "text": "Given the matrices A, Rn, d and B, Rn and p, it is known that the complexity of the calculation of ATB is O (dnp).The approximate matrix multiplication aims at obtaining a matrix C, Rd and p with o (dnp) time complexity, so that for a small \u03b5 > 0 the matrix A, B and B applies."}, {"heading": "9.2. Approximate Matrix Multiplication 83", "text": "This shows that approximate matrix multiplication can be considered an inverse process of the conventional matrix. Furthermore, approximate matrix multiplication is a potentially important approach for rapid matrix multiplication [Drineas et al., 2006a, Clarkson and Woodruff, 2009, Cohen and Lewis, 1999, Kane and Nelson, 2014, Drineas et al., 2011b, Nelson and Nguy\u00ean, 2013, Clarkson and Woodruff, 2013]. It is the basis of approximate minimal square methods and matrix low rank approximation methods [Sarlos, 2006, Halko et al., Kyrillidis et al, 2014, Martinsson et al., 2011, Woolfe et al., 2008, Magdon-Ismail, 2011, Magen and Zouzias, 2011, Cohen and Lewis, 1999, Kane and Nelson, 2014, Drineas et al."}, {"heading": "84 Matrix Low Rank Approximation", "text": "Recently Cohen et al. [2015] proved to be the optimal approximate matrix multiplication in terms of stable relationships between A and B by using subspace embedding [Batson et al., 2014].Theorem 9.6. [Cohen et al., 2015] Given that r is the maximum of the stable ranks of A and B. Then Kane and B introduced two conform matrices - ATB | | A | | B | holds for the 2r-dimensional subspace in which r is the maximum of the stable ranks of A and B. Then Kane and Nelson [2014] introduced the JL moment property."}, {"heading": "9.2. Approximate Matrix Multiplication 85", "text": "For low matrix approximation in the streaming model, Clarkson and Woodruff [2009] indicated the near-optimal spatial boundaries through the sketches. Liberty [2013] came up with a deterministic streaming algorithm, with an improved analysis examined by Ghashami and Phillips [2014], and Space lower bound obtained by Woodruff [2014a]. 10Large-scale matrix approximationIn this chapter, we discuss fast computation methods of SVD, kernel methods, and CUR decomposition using randomized approximation. The goal is to fill the factorization of the matrix with the use of large-scale data matrices. It is notoriously difficult to calculate SVD, because the exact SVD of a m \u00d7 n matrix O (mnmin {m, n}) takes time. Fortunately, many machine learning methods such as latent semantic indexation [Rerweal 1990 et learning clual, manipulation al and temporal] are sufficient."}, {"heading": "10.1. Randomized SVD 87", "text": "In contrast to randomised SVD, which is based on random projection, CUR approximation mainly uses column selection, which has been extensively studied in the communities of theoretical informatics (TCS) and numerical linear algebra (NLA).The work in TCS focuses mainly on the selection of good columns by randomised algorithms with detectable error limits [Frieze et al., 2004, Deshpande et al., 2006, Drineas et al., 2008, Deshpande and Rademacher, 2010, Boutsidis et al., 2014, Guruswami and Sinop, 2012].The emphasis in the NLA is then on deterministic algorithms, in particular the rank-revealing QR factorings that select columns by pivot rules [Foster, 1986, Chan, 1987, Stewart, 1999, Bischof and Hansen, 1991, Hong and Pan 1992, Chandraseat, Ipsean, and Eisenkarsen]."}, {"heading": "10.1 Randomized SVD", "text": "All randomized SVD algorithms have essentially the same idea: First they draw a random projection matrix, and finally they compute a rank-k matrix X-Rc \u00b7 n, so that the chart A-QX-Rm \u00b7 c is small compared to the chart A-Ak-Rm \u00b7 c and calculate their orthonormal bases Q-Rm \u00b7 c, and finally they compute a rank-k matrix X-Rc \u00b7 n. The following problem is the basis for the theoretical analysis of therandomized SVD [Halko et al., 2011, Gu, 2015]. Lemma 10.1. Let A-Rm \u00b7 n be a predetermined matrix, and Z-Rn \u00b7 k be a column orthonormal. Allow each matrix to have such a rank (Z-Rm) = k, and define C-Rm \u00b7 c-n-k, then the column orthonorthonormally."}, {"heading": "88 Large-Scale Matrix Approximation", "text": "For all matrices X-Rm \u00b7 n of rank most k in the column space of C. Obviously, C (ZTB) \u2020 ZT is such a matrix. Therefore, A-Rm \u00b7 n of rank most k in the column space of C. In addition, C (ZTB) \u2020 AZZT + AZZT \u2212 C (ZTB) \u2020 ZTB \u2212 C + (ZTB) is best because rank (ZTB) \u2212 A) is best (ZTB) \u2020 ZT \u00b2 E + EB (ZTB) \u2020 ZT \u00b2. Here we use the fact that ZTB (ZTB) is best because rank (ZTB) = k. Let's consider that EB (ZTB) is best."}, {"heading": "10.1. Randomized SVD 89", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "10.1.1 Randomized SVD: Frobenius Norm Bounds", "text": "In this subsection, we describe two randomized SVD algorithms that have (1 + 1) relative error limits. (Random Projection) To reduce computational effort, randomized algorithms [Frieze et al., 2004, Vempala, 2000] have been introduced to maintain abbreviated SVD and low approximation. (Random Calculation) Johnson & Lindenstrauss (JL) [Johnson and Lindenstrauss, 1984, Dasgupta and Gupta, 2003] is known to keep isometry in expectation or with high probability. (Random Calculation) Halko et al. (2011], Boutsidis et al. (2014] used the JL transform for sketching and showed relative error limits. (Random Calculation) The Gaussian test matrix is dense and cannot be applied efficiently to matrix. (Random Calculation) Various improvements have been proposed to make matrix matrix test matrix (Jrix matrix is relative matrix)."}, {"heading": "90 Large-Scale Matrix Approximation", "text": "Algorithm 1 Randomized SVD: The prototype algorithm 1: Input: a matrix A-Rm \u00b7 n with m \u2265 n, target rank k, the size of the diagram c, where 0 < k \u2264 c < n; 2: Draw a sketch matrix, e.g. a Gaussian test matrix or a counter diagram 3: Calculate C-Rm \u00b7 c and its orthonormal fundamentals Q-Rm \u00b7 c; 4: Calculate the rank-truncated SVD: QTA-Rk-kV-Tk; 5: Calculate U-k = QU-k-Rm \u00b7 k, V-K-K - an approximate rank-stump SVD of A.Transform the rank matrix. Then we solve the rank-Rk-K matrix (X-Rooff)."}, {"heading": "10.1.2 Randomized SVD: Spectral Norm Bounds", "text": "The previous section shows that the approximate truncated SVD can be calculated highly efficiently, with the (1 +) Frobenius relative error guaranteed. Frobenius standard limit says that the total elementary distance is small, but it does not inform us of the proximity of its singular vectors. Therefore, we need spectral standard limits or even stronger main angle limits; here we consider only the former. We are trying to find an m \u00b7 k column orthogonal matrix U-so that it forms the perpendicular matrix U-A-U-U-TA-22-A-Ak-22."}, {"heading": "10.1. Randomized SVD 91", "text": "In contrast to the Frobenius standard, the prototype algorithm is unlikely to reach a constant factor limit (i.e., it is independent of m, n), disregarding the 1 + 1 + 1 matrix or any column selection matrix. However, the order of the two lower limits must be at least n / c. We apply Gus Theorem [Gu, 2015] (Theorem 9.5) to obtain an O (n) factor spectral standard, and then introduce iterative algorithms with the (1 + 1) spectral standard. We apply Gus Theorem [Gu, 2015] (Theorem 9.5) to obtain an O (n) factor standard, and then introduce iterative algorithms with the (1 + 1) spectral standard."}, {"heading": "92 Large-Scale Matrix Approximation", "text": "Algorithm 2 Subspace Iteration Methods.1: Input: any matrix A-Rm \u00b7 n, the target rank k, the size of the diagram c, in which 0 < k \u2264 c < n; 2: Input: any matrix A-Rm \u00b7 n, the target rank k, the size of the diagram c, in which 0 < k \u2264 c < n; 2: Input: create a n \u00b7 c Gaussian test matrix and perform the diagram C (0) = ABA (0); 3: Input for i = 1 to t 4: Optional: Input C (i \u2212 1); 5: Compute C (i) = AATC (i \u2212 1); 6: End for 7: The power method: Input C (t) to get the quantity C (t) to get the quantity Rm \u00b7 c; 8: The Krylov Subspace Method: Orthalize (i \u2212 1); 6: Get the power method: Input C (t) to get the quantity C (\u00b7), C (C \u00b7 \u00b7), the quantity (C \u00b7)."}, {"heading": "10.2. Kernel Approximation 93", "text": "We show the Krylov subspace method in algorithm 2, which differs from the simultaneous potential iteration in only one line. It turns out that the Krylov subspace method converges much faster than the potential iteration [Saad, 2011]. Recently, Musco and Musco [2015] showed that with t = logn \u221a potentialization the spectral norm limit (10,3) is likely to be met. This result is obviously stronger than the simultaneous potential iteration. It is worth mentioning that the Krylov subspace method described in algorithm 2 is a simplified version and may be unstable if t is large. This is because the columns of C (0 \u00b7 thows) are applied in part or in practice to prevent instability (in algorithm 2) from recurring."}, {"heading": "10.2 Kernel Approximation", "text": "Kernel methods are important tools for machine learning, computer vision and data mining [Sch\u00f6lkopf and Smola, 2002, Shawe-Taylor and Christianini, 2004, Vapnik, 1998, Rasmussen and Williams, 2006]. For example, kernel ridge regression (KRR), Gaussian processes, Kernel Support Vector Machine (KSVM), spectral clustering and Kernel Principal Component Analysis (KPCA) are classic nonlinear models for regression, classification, clustering and dimensionality regression. Unfortunately, the lack of scalability has always been the biggest drawback of kernel methods. The three steps of most kernel methods - formation of the kernel matrix, training, generalization - can all be prohibitive in big data applications. Suppose that we get n training data and m test data, all in the d dimension. First, it takes PCO (n2d) time to form a kernel matrix, i.e., a kernel-second matrix, or nirix."}, {"heading": "94 Large-Scale Matrix Approximation", "text": "top k singular vectors of the (normalized) kernel matrix, where k is the number of classes or the target dimensionality. This costs O (n2k) time and O (n3) memory. Third, to generalize the trained model to the test data, core methods such as KRR, KSVM, KPCA cost O (nmd) time to form a n \u00b7 m cross kernel matrix between the training and test data. If m is as large as n, generating is as difficult as training.Low rank approximation is the most popular approach for scalable kernel approximation. If we get the low rank approximation K \u2248 CXCT, then the approximate eigenvalue decomposition can be obtained immediately. CXCXCT = UC (vTCXVCE) is the most popular approach to scalable kernel approximation."}, {"heading": "10.2. Kernel Approximation 95", "text": "Wang et al. [2014a] showed that by randomly sampling the K columns to form C by a particular algorithm, the approximation is highly precise: Wang et al., 2014a]. Unfortunately, the prototype algorithm has two obvious disadvantages. First, to calculate the intersection matrix of K, each entry of K \u2212 rik / n must be known. As is discussed, it takes O (n2d) time to form the core matrix K. Second, the matrix multiplication of C \u2020 K costs O (n2c) time. In sum, the prototype algorithm costs O (n2c + n2d) time."}, {"heading": "96 Large-Scale Matrix Approximation", "text": "The Nystr\u00f6m Method is the most popular approach of the Kernel Approximation. It was named after its inventor Nystr\u00f6m [1930] and gained its popularity in the Machine Learning Society after its application in the Gaussian Processional Regression [Williams and Seeger, 2001]. Let us be a column selection matrix, C = KS, and W = STKS. The Nystr\u00f6m Method approximates K by CW \u2020 CT. In fact, the Nystr\u00f6m Method is a special case of the faster SPSD matrix sketching where P and S are the same. This also indicates that the Nystr\u00f6m Method is an approximate solution to (10.4). Gittens and Mahoney [2013] offered comprehensive error analyses of the Nystr\u00f6m Method. The Nystr\u00f6m Method was used to solve millions of Kernel Methods [Talwalkar al., 2013], approximate, approximate, approximate, approximate, approximate, approximate, we cannot, approximate, approximate, quicker, approximate, approximate, approximate."}, {"heading": "10.3 The CUR Approximation", "text": "The decomposition of the CUR matrix is formed by selecting the c-columns of A to form a C-Rm-c matrix, r-rows to form an R-Rr-n matrix, and calculating an intersection matrix U-Rc-r so that CUR-A. In this section, we first discuss the motivations and then describe algorithms and error analyses. Motivations. we first continue the generalization problem of the core methods, which remains unsolved in the previous section. Suppose we get n training data and m-test data, all of the d dimension. To generalize the trained model to the test data, supervised core methods such as Gaussian processes and KRR must evaluate the core function of each tension and test data pair - that is, form an m-n matrix."}, {"heading": "10.3. The CUR Approximation 97", "text": "Due to the fast CUR algorithm, which is described later in this section, the approximation in d (m + n) time can be calculated linearly. In this way, the total time cost of generalization is in m +. Secondly, CUR forms a compressed representation of the data matrix, as well as the truncated SVD, and it can be very efficiently converted to the SVD-like form. \u2212 The total cost of generalization is linear in m +. Secondly, CUR forms a compressed representation of the data matrix, as well as the truncated SVD, and it can be very efficiently converted into the SVD-like form. \u2212 The total cost of generalization is linear in m +. Secondly, CUR forms a compressed representation of the data matrix, as well as the truncated SVD, and it can be very efficiently converted into the SVD-like form. \u2212 The CUR strategies are linear in m + The UCU data n.UTUCT = the original VB = VB (Here)."}, {"heading": "98 Large-Scale Matrix Approximation", "text": "Users just need to know that such column selection algorithms cannot be applied to accelerate the calculation. \u2212 r It remains an open problem whether there is a relatively flawed sampling algorithm that does not need to take the whole matrix A into account. In practice, users can simply select columns / rows without substitution, which is normally acceptable empirical performance. \u2212 r The intersection matrix. With the selected columns C and rows R at hand, we can easily calculate the intersection matrix. \u2212 r The intersection matrix."}, {"heading": "Acknowledgements", "text": "I would like to thank my doctoral students Cheng Chen, Luo Luo, Shusen Wang, Haishan Ye and Qiaomin Ye. Cheng Chen, Luo Luo and Qiaomin Ye in particular helped with the proofreading of the entire manuscript. Haishan Ye helped with the revision of Chapter 9.2 and Shusen Wang with the revision of Chapter 10. I would also like to thank other students who took my course \"Matrix Methods in Massive Data Analysis\" in the summer semester 2015."}], "references": [{"title": "Nystr\u00f6m approximation for large-scale determinantal processes", "author": ["Raja Hafiz Affandi", "Alex Kulesza", "Emily B. Fox", "Ben Taskar"], "venue": "In International Conference on Artificial Intelligence and Statistics (AISTATS),", "citeRegEx": "Affandi et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Affandi et al\\.", "year": 2013}, {"title": "A kernel method for canonical correlation analysis", "author": ["S. Akaho"], "venue": "In International Meeting of Psychometric Society,", "citeRegEx": "Akaho.,? \\Q2001\\E", "shortCiteRegEx": "Akaho.", "year": 2001}, {"title": "Spectral analysis of data", "author": ["Yossi Azar", "Amos Fiat", "Anna Karlin", "Frank McSherry", "Jared Saia"], "venue": "In Proceedings of the thirty-third annual ACM symposium on Theory of computing,", "citeRegEx": "Azar et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Azar et al\\.", "year": 2001}, {"title": "Kernel independent component analysis", "author": ["F.R. Bach", "M.I. Jordan"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Bach and Jordan.,? \\Q2002\\E", "shortCiteRegEx": "Bach and Jordan.", "year": 2002}, {"title": "Srivastave. Twice-Ramanujan sparsifiers", "author": ["J. Batson", "D. Spielman"], "venue": "SIAM Review,", "citeRegEx": "Batson et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Batson et al\\.", "year": 2014}, {"title": "Generalized discriminant analysis using a kernel approach", "author": ["G. Baudat", "F. Anouar"], "venue": "Neural Computation,", "citeRegEx": "Baudat and Anouar.,? \\Q2000\\E", "shortCiteRegEx": "Baudat and Anouar.", "year": 2000}, {"title": "Eigenfaces vs. Fisherfaces: Recognition using class specific linear projection", "author": ["P. Belhumeur", "J. Hespanha", "D. Kriegman"], "venue": "IEEE Trans. PAMI,", "citeRegEx": "Belhumeur et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Belhumeur et al\\.", "year": 1997}, {"title": "Laplacian eigenmaps for dimensionality reduction and data representation", "author": ["Mikhail Belkin", "Partha Niyogi"], "venue": "Neural computation,", "citeRegEx": "Belkin and Niyogi.,? \\Q2003\\E", "shortCiteRegEx": "Belkin and Niyogi.", "year": 2003}, {"title": "Generalized Inverses: Theory and Applications", "author": ["A. Ben-Israel", "T.N.E. Greville"], "venue": "Second Edition. Springer,", "citeRegEx": "Ben.Israel and Greville.,? \\Q2003\\E", "shortCiteRegEx": "Ben.Israel and Greville.", "year": 2003}, {"title": "Algorithm 844: computing sparse reduced-rank approximations to sparse matrices", "author": ["M.W. Berry", "S.A. Pulatova", "G.W. Stewart"], "venue": "ACM Transactions on Mathematical Software,", "citeRegEx": "Berry et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Berry et al\\.", "year": 2005}, {"title": "Tighter low-rank approximation via sampling the leveraged element", "author": ["Srinadh Bhojanapalli", "Prateek Jain", "Sujay Sanghavi"], "venue": "In Proceedings of the Twenty-Sixth Annual ACM-SIAM Symposium on Discrete Algorithms,", "citeRegEx": "Bhojanapalli et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Bhojanapalli et al\\.", "year": 2015}, {"title": "CUR from a sparse optimization viewpoint", "author": ["J. Bien", "Y. Xu", "M.W. Mahoney"], "venue": "In Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "Bien et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Bien et al\\.", "year": 2010}, {"title": "Structure-preserving and rank-revealing QRfactorizations", "author": ["C.H. Bischof", "P.C. Hansen"], "venue": "SIAM Journal on Scientific and Statistical Computing,", "citeRegEx": "Bischof and Hansen.,? \\Q1991\\E", "shortCiteRegEx": "Bischof and Hansen.", "year": 1991}, {"title": "Convex Analysis and Nonlinear Optimization: Theory and Examples", "author": ["Jonathan M. Borwein", "Adrian S. Lewis"], "venue": null, "citeRegEx": "Borwein and Lewis.,? \\Q2006\\E", "shortCiteRegEx": "Borwein and Lewis.", "year": 2006}, {"title": "Optimal CUR matrix decompositions", "author": ["Christos Boutsidis", "David P. Woodruff"], "venue": "STOC, pages 353\u2013362,", "citeRegEx": "Boutsidis and Woodruff.,? \\Q2014\\E", "shortCiteRegEx": "Boutsidis and Woodruff.", "year": 2014}, {"title": "Near-optimal column-based matrix reconstruction", "author": ["Christos Boutsidis", "Petros Drineas", "Malik Magdon-Ismail"], "venue": "SIAM Journal on Computing,", "citeRegEx": "Boutsidis et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Boutsidis et al\\.", "year": 2014}, {"title": "Dimension reduction: A guided tour", "author": ["Christopher J.C. Burges"], "venue": "Foundations and Trends in Machine Learning,", "citeRegEx": "Burges.,? \\Q2010\\E", "shortCiteRegEx": "Burges.", "year": 2010}, {"title": "A singular value thresholding algorithm for matrix completion", "author": ["Jian-Feng Cai", "Emmanuel J Cand\u00e8s", "Zuowei Shen"], "venue": "SIAM Journal on Optimization,", "citeRegEx": "Cai et al\\.,? \\Q1956\\E", "shortCiteRegEx": "Cai et al\\.", "year": 1956}, {"title": "Exact matrix completion via convex optimization", "author": ["Emmanuel J Cand\u00e8s", "B. Recht"], "venue": "Foundations of Computational Mathematics,", "citeRegEx": "Cand\u00e8s and Recht.,? \\Q2009\\E", "shortCiteRegEx": "Cand\u00e8s and Recht.", "year": 2009}, {"title": "The dantzig selector: Statistical estimation when p is much larger than n", "author": ["Emmanuel J Cand\u00e8s", "Terence Tao"], "venue": "The Annals of Statistics,", "citeRegEx": "Cand\u00e8s and Tao.,? \\Q2007\\E", "shortCiteRegEx": "Cand\u00e8s and Tao.", "year": 2007}, {"title": "Rank revealing QR factorizations", "author": ["T.F. Chan"], "venue": "Linear Algebra and Its Applications,", "citeRegEx": "Chan.,? \\Q1987\\E", "shortCiteRegEx": "Chan.", "year": 1987}, {"title": "On rank-revealing factorisations", "author": ["S. Chandrasekaran", "I.C.F. Ipsen"], "venue": "SIAM Journal on Matrix Analysis and Applications,", "citeRegEx": "Chandrasekaran and Ipsen.,? \\Q1994\\E", "shortCiteRegEx": "Chandrasekaran and Ipsen.", "year": 1994}, {"title": "Numerical linear algebra in the streaming model", "author": ["Kenneth L Clarkson", "David P Woodruff"], "venue": "In Proceedings of the forty-first annual ACM symposium on Theory of computing,", "citeRegEx": "Clarkson and Woodruff.,? \\Q2009\\E", "shortCiteRegEx": "Clarkson and Woodruff.", "year": 2009}, {"title": "Low rank approximation and regression in input sparsity time", "author": ["Kenneth L Clarkson", "David P Woodruff"], "venue": "In Proceedings of the forty-fifth annual ACM symposium on Theory of computing,", "citeRegEx": "Clarkson and Woodruff.,? \\Q2013\\E", "shortCiteRegEx": "Clarkson and Woodruff.", "year": 2013}, {"title": "Approximating matrix multiplication for pattern recognition tasks", "author": ["Edith Cohen", "David D Lewis"], "venue": "Journal of Algorithms,", "citeRegEx": "Cohen and Lewis.,? \\Q1999\\E", "shortCiteRegEx": "Cohen and Lewis.", "year": 1999}, {"title": "Dimensionality reduction for k-means clustering and low rank approximation", "author": ["Michael Cohen", "Sam Elder", "Cameron Musco", "Christopher Musco", "Madalina Persu"], "venue": "arXiv preprint arXiv:1410.6801,", "citeRegEx": "Cohen et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Cohen et al\\.", "year": 2014}, {"title": "Optimal approximate matrix product in terms of stable rank", "author": ["Michael B. Cohen", "Jelani Nelson", "David P. Woodruff"], "venue": "arXiv preprint arXiv:1507.02268,", "citeRegEx": "Cohen et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Cohen et al\\.", "year": 2015}, {"title": "An elementary proof of a theorem of Johnson and Lindenstrauss", "author": ["S. Dasgupta", "A. Gupta"], "venue": "Random Structure & Algorithms,", "citeRegEx": "Dasgupta and Gupta.,? \\Q2003\\E", "shortCiteRegEx": "Dasgupta and Gupta.", "year": 2003}, {"title": "Indexing by latent semantic analysis", "author": ["S. Deerwester", "S.T. Dumais", "G.W. Furnas", "T.K. Landauer", "R. Harshman"], "venue": "Journal of The American Society for Information Science,", "citeRegEx": "Deerwester et al\\.,? \\Q1990\\E", "shortCiteRegEx": "Deerwester et al\\.", "year": 1990}, {"title": "Applied Numerical Linear Algebra", "author": ["J. Demmel"], "venue": "SIAM, Philadelphia,", "citeRegEx": "Demmel.,? \\Q1997\\E", "shortCiteRegEx": "Demmel.", "year": 1997}, {"title": "Efficient volume sampling for row/column subset selection", "author": ["A. Deshpande", "L. Rademacher"], "venue": "In Proceedings of the 51st IEEE Annual Symposium on Foundations of Computer Science (FOCS),", "citeRegEx": "Deshpande and Rademacher.,? \\Q2010\\E", "shortCiteRegEx": "Deshpande and Rademacher.", "year": 2010}, {"title": "Matrix approximation and projective clustering via volume sampling", "author": ["A. Deshpande", "L. Rademacher", "S. Vempala", "G. Wang"], "venue": "Theory of Computing,", "citeRegEx": "Deshpande et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Deshpande et al\\.", "year": 2006}, {"title": "On the Nystr\u00f6m method for approximating a gram matrix for improved kernel-based learning", "author": ["P. Drineas", "M.W. Mahoney"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Drineas and Mahoney.,? \\Q2005\\E", "shortCiteRegEx": "Drineas and Mahoney.", "year": 2005}, {"title": "Relative-error CUR matrix decompositions", "author": ["P. Drineas", "Michael W. Mahoney", "S. Muthukrishnan"], "venue": "SIAM Journal on Matrix Analysis and Applications,", "citeRegEx": "Drineas et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Drineas et al\\.", "year": 2008}, {"title": "Fast monte carlo algorithms for matrices I: Approximating matrix multiplication", "author": ["Petros Drineas", "Ravi Kannan", "Michael W Mahoney"], "venue": "SIAM Journal on Computing,", "citeRegEx": "Drineas et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Drineas et al\\.", "year": 2006}, {"title": "Sampling algorithms for l2 regression and applications", "author": ["Petros Drineas", "Michael W. Mahoney", "S. Muthukrishnan"], "venue": "In Proceedings of the Seventeenth Annual ACM-SIAM Symposium on Discrete Algorithm,", "citeRegEx": "Drineas et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Drineas et al\\.", "year": 2006}, {"title": "Fast approximation of matrix coherence and statistical leverage", "author": ["Petros Drineas", "Malik Magdon-Ismail", "Michael W. Mahoney", "David P. Woodruff"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Drineas et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Drineas et al\\.", "year": 2011}, {"title": "Faster least squares approximation", "author": ["Petros Drineas", "Michael W Mahoney", "S Muthukrishnan", "Tam\u00e1s Sarl\u00f3s"], "venue": "Numerische Mathematik,", "citeRegEx": "Drineas et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Drineas et al\\.", "year": 2011}, {"title": "The approximation of one matrix by another of lower rank", "author": ["C. Eckart", "G. Young"], "venue": null, "citeRegEx": "Eckart and Young.,? \\Q1936\\E", "shortCiteRegEx": "Eckart and Young.", "year": 1936}, {"title": "A principal axis transformation for non-Hermitian matrices", "author": ["C. Eckart", "G. Young"], "venue": "Bulletin of the American Mathematical Society,", "citeRegEx": "Eckart and Young.,? \\Q1939\\E", "shortCiteRegEx": "Eckart and Young.", "year": 1939}, {"title": "Maximum properties and inequalities for the eigenvalues of completely continuous operators", "author": ["Ky Fan"], "venue": "Proc. Nat. Acad. Sci. USA,", "citeRegEx": "Fan.,? \\Q1951\\E", "shortCiteRegEx": "Fan.", "year": 1951}, {"title": "Turning big data into tiny data: Constant-size coresets for k-means, pca and projective clustering", "author": ["Dan Feldman", "Melanie Schmidt", "Christian Sohler"], "venue": "In Proceedings of the Twenty-Fourth Annual ACM-SIAM Symposium on Discrete Algorithms,", "citeRegEx": "Feldman et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Feldman et al\\.", "year": 2013}, {"title": "Rank and null space calculations using matrix decomposition without column interchanges", "author": ["L.V. Foster"], "venue": "Linear Algebra and its Applications,", "citeRegEx": "Foster.,? \\Q1986\\E", "shortCiteRegEx": "Foster.", "year": 1986}, {"title": "Spectral grouping using the Nystr\u00f6m method", "author": ["C. Fowlkes", "S. Belongie", "F. Chung", "J. Malik"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence,", "citeRegEx": "Fowlkes et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Fowlkes et al\\.", "year": 2004}, {"title": "Fast Monte Carlo algorithms for finding low-rank approximation", "author": ["A. Frieze", "K. Kannan", "Rademacher S. Vempala"], "venue": "Journal of the ACM,", "citeRegEx": "Frieze et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Frieze et al\\.", "year": 2004}, {"title": "Relative errors for deterministic low-rank matrix approximations", "author": ["Mina Ghashami", "Jeff M Phillips"], "venue": "In Proceedings of the Twenty-Fifth Annual ACMSIAM Symposium on Discrete Algorithms,", "citeRegEx": "Ghashami and Phillips.,? \\Q2014\\E", "shortCiteRegEx": "Ghashami and Phillips.", "year": 2014}, {"title": "Simultaneous diagonalization of rectangular complex matrices", "author": ["P.M. Gibson"], "venue": "Linear Algebra and Its Applications,", "citeRegEx": "Gibson.,? \\Q1974\\E", "shortCiteRegEx": "Gibson.", "year": 1974}, {"title": "Revisiting the Nystr\u00f6m method for improved large-scale machine learning", "author": ["A. Gittens", "M.W. Mahoney"], "venue": "In International Conference on Machine Learning (ICML),", "citeRegEx": "Gittens and Mahoney.,? \\Q2013\\E", "shortCiteRegEx": "Gittens and Mahoney.", "year": 2013}, {"title": "Molecular classification of cancer: class discovery and class prediction by gene expression monitoring", "author": ["T. Golub", "D. Slonim", "P. Tamayo", "C. Huard", "M. Gaasenbeek", "J. Mesirov", "H. Coller", "M. Loh", "J. Downing", "M. Caligiuri"], "venue": "Science, 286:531\u2013536,", "citeRegEx": "Golub et al\\.,? \\Q1999\\E", "shortCiteRegEx": "Golub et al\\.", "year": 1999}, {"title": "A theory of pseudoskeleton approximations", "author": ["S.A. Goreinov", "E.E. Tyrtyshnikov", "N.L. Zamarashkin"], "venue": "Linear Algebra and Its Applications,", "citeRegEx": "Goreinov et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Goreinov et al\\.", "year": 1997}, {"title": "Pseudo-skeleton approximations by matrices of maximal volume", "author": ["S.A. Goreinov", "N.L. Zamarashkin", "E.E. Tyrtyshnikov"], "venue": "Mathematical Notes,", "citeRegEx": "Goreinov et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Goreinov et al\\.", "year": 1997}, {"title": "Subspace iteration randomization and singular value problems", "author": ["Ming Gu"], "venue": "SIAM Journal on Scientific Computing,", "citeRegEx": "Gu.,? \\Q2015\\E", "shortCiteRegEx": "Gu.", "year": 2015}, {"title": "Efficient algorithms for computing a strong rank-revealing QR factorization", "author": ["Ming Gu", "S.C. Eisenstat"], "venue": "SIAM Journal on Scientific Computing,", "citeRegEx": "Gu and Eisenstat.,? \\Q1996\\E", "shortCiteRegEx": "Gu and Eisenstat.", "year": 1996}, {"title": "Optimal column-based low-rank matrix reconstruction", "author": ["V. Guruswami", "A.K. Sinop"], "venue": "In Proceedings of the 23rd Annual ACM-SIAM Symposium on Discrete Algorithms (SODA),", "citeRegEx": "Guruswami and Sinop.,? \\Q2012\\E", "shortCiteRegEx": "Guruswami and Sinop.", "year": 2012}, {"title": "Finding Structure with Randomness : Probabilistic Algorithms for Matrix Decompositions", "author": ["N Halko", "P G Martinsson", "J A Tropp"], "venue": "SIAM Review,", "citeRegEx": "Halko et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Halko et al\\.", "year": 2011}, {"title": "Canonical correlation analysis: An overview with application to learning methods", "author": ["D.R. Hardoon", "S. Szedmak", "J. Shawe-Taylor"], "venue": "Neural Computation,", "citeRegEx": "Hardoon et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Hardoon et al\\.", "year": 2004}, {"title": "The Elements of Statistical Learning: Data Mining, Inference, and Prediction", "author": ["T. Hastie", "R. Tibshirani", "J. Friedman"], "venue": null, "citeRegEx": "Hastie et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Hastie et al\\.", "year": 2001}, {"title": "Matrix completion and low-rank svd via fast alternating least squares", "author": ["Trevor Hastie", "Rahul Mazumder", "Jason Lee", "Reza Zadeh"], "venue": "arXiv preprint arXiv:1410.2596,", "citeRegEx": "Hastie et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Hastie et al\\.", "year": 2014}, {"title": "The hat matrix in regression and ANOVA", "author": ["D.C. Hoaglin", "R.E. Welsch"], "venue": "The American Statistician,", "citeRegEx": "Hoaglin and Welsch.,? \\Q1978\\E", "shortCiteRegEx": "Hoaglin and Welsch.", "year": 1978}, {"title": "Rank-revealing QR factorizations and the singular value decomposition", "author": ["Y.P. Hong", "C.T. Pan"], "venue": "Mathematics of Computation,", "citeRegEx": "Hong and Pan.,? \\Q1992\\E", "shortCiteRegEx": "Hong and Pan.", "year": 1992}, {"title": "On the singular values of a product of completely continuous operators", "author": ["A. Horn"], "venue": "Proc. Nat. Acad. Sci. USA,", "citeRegEx": "Horn.,? \\Q1951\\E", "shortCiteRegEx": "Horn.", "year": 1951}, {"title": "On the eigenvalues of a matrix with prescribed singular values", "author": ["A. Horn"], "venue": "Proc. Amer. Math. Soc.,", "citeRegEx": "Horn.,? \\Q1954\\E", "shortCiteRegEx": "Horn.", "year": 1954}, {"title": "Matrix Analysis", "author": ["Roger A. Horn", "Charles R. Johnson"], "venue": null, "citeRegEx": "Horn and Johnson.,? \\Q1985\\E", "shortCiteRegEx": "Horn and Johnson.", "year": 1985}, {"title": "Topics in Matrix Analysis", "author": ["Roger A. Horn", "Charles R. Johnson"], "venue": null, "citeRegEx": "Horn and Johnson.,? \\Q1991\\E", "shortCiteRegEx": "Horn and Johnson.", "year": 1991}, {"title": "Structure preserving dimension reduction for clustered text data based on the generalized singular value decomposition", "author": ["P. Howland", "M. Jeon", "H. Park"], "venue": "SIAM Journal on Matrix Analysis and Applications,", "citeRegEx": "Howland et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Howland et al\\.", "year": 2003}, {"title": "Improved bound for the Nystr\u00f6m method and its application to kernel classification", "author": ["R. Jin", "T. Yang", "M. Mahdavi", "Y.F. Li", "Z.H. Zhou"], "venue": "IEEE Transactions on Information Theory,", "citeRegEx": "Jin et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Jin et al\\.", "year": 2013}, {"title": "Extensions of Lipschitz mapping into a Hilbert space", "author": ["W.B. Johnson", "J. Lindenstrauss"], "venue": "Contemporary Mathematics,", "citeRegEx": "Johnson and Lindenstrauss.,? \\Q1984\\E", "shortCiteRegEx": "Johnson and Lindenstrauss.", "year": 1984}, {"title": "Principal component analysis. Springer, New York, second edition", "author": ["I.T. Jolliffe"], "venue": null, "citeRegEx": "Jolliffe.,? \\Q2002\\E", "shortCiteRegEx": "Jolliffe.", "year": 2002}, {"title": "Sparser johnson-lindenstrauss transforms", "author": ["Daniel M Kane", "Jelani Nelson"], "venue": "Journal of the ACM (JACM),", "citeRegEx": "Kane and Nelson.,? \\Q2014\\E", "shortCiteRegEx": "Kane and Nelson.", "year": 2014}, {"title": "Learning with whom to share in multi-task feature learning", "author": ["Zhuoliang Kang", "Kristen Grauman", "Fei Sha"], "venue": "In Proceedings of the 28th International Conference on Machine Learning", "citeRegEx": "Kang et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Kang et al\\.", "year": 2011}, {"title": "A new approach to feature selection based on the Karhunen-Lo\u00e8ve expansion", "author": ["J. Kittler", "P.C. Young"], "venue": "Pattern Recognition,", "citeRegEx": "Kittler and Young.,? \\Q1973\\E", "shortCiteRegEx": "Kittler and Young.", "year": 1973}, {"title": "Ensemble Nystr\u00f6m method", "author": ["S. Kumar", "M. Mohri", "A. Talwalkar"], "venue": "In Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "Kumar et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Kumar et al\\.", "year": 2009}, {"title": "Vector algebra in the analysis of genome-wide expression data", "author": ["F.G. Kuruvilla", "P.J. Park", "S.L. Schreiber"], "venue": "Genome Biology,", "citeRegEx": "Kuruvilla et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Kuruvilla et al\\.", "year": 2002}, {"title": "Approximate matrix multiplication with application to linear embeddings", "author": ["Anastasios Kyrillidis", "Michail Vlachos", "Anastasios Zouzias"], "venue": "In Information Theory (ISIT),", "citeRegEx": "Kyrillidis et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Kyrillidis et al\\.", "year": 2014}, {"title": "The mathematics of eigenvalue optimization", "author": ["Adrian S Lewis"], "venue": "Mathematical Programming,", "citeRegEx": "Lewis.,? \\Q2003\\E", "shortCiteRegEx": "Lewis.", "year": 2003}, {"title": "Simple and deterministic matrix sketching", "author": ["Edo Liberty"], "venue": "In Proceedings of the 19th ACM SIGKDD international conference on Knowledge discovery and data mining,", "citeRegEx": "Liberty.,? \\Q2013\\E", "shortCiteRegEx": "Liberty.", "year": 2013}, {"title": "Tensor completion for estimating missing values in visual data", "author": ["Ji Liu", "Przemyslaw Musialski", "Peter Wonka", "Jieping Ye"], "venue": "In Pattern Analysis and Machine Intelligence,", "citeRegEx": "Liu et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Liu et al\\.", "year": 2013}, {"title": "Generalizing the singular value decomposition", "author": ["C.F. Van Loan"], "venue": "SIAM Journal on numerical Analysis,", "citeRegEx": "Loan.,? \\Q1976\\E", "shortCiteRegEx": "Loan.", "year": 1976}, {"title": "Support matrix machines", "author": ["Luo Luo", "Yubo Xie", "Zhihua Zhang", "Wu-Jun Li"], "venue": "In The International Conference on Machine Learning (ICML),", "citeRegEx": "Luo et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Luo et al\\.", "year": 2015}, {"title": "A statistical perspective on algorithmic leveraging", "author": ["Ping Ma", "Michael Mahoney", "Bin Yu"], "venue": "In International Conference on Machine Learning (ICML),", "citeRegEx": "Ma et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Ma et al\\.", "year": 2014}, {"title": "Matrix Differential Calculus with Applications in Statistics and Econometrics", "author": ["Jan R. Macnus", "Heinz Neudecker"], "venue": null, "citeRegEx": "Macnus and Neudecker.,? \\Q2000\\E", "shortCiteRegEx": "Macnus and Neudecker.", "year": 2000}, {"title": "Using a non-commutative Bernstein bound to approximate some matrix algorithms in the spectral norm", "author": ["Malik Magdon-Ismail"], "venue": "arXiv preprint arXiv:1103.5453,", "citeRegEx": "Magdon.Ismail.,? \\Q2011\\E", "shortCiteRegEx": "Magdon.Ismail.", "year": 2011}, {"title": "Low rank matrix-valued chernoff bounds and approximate matrix multiplication", "author": ["Avner Magen", "Anastasios Zouzias"], "venue": "In Proceedings of the twenty-second annual ACM-SIAM symposium on Discrete Algorithms,", "citeRegEx": "Magen and Zouzias.,? \\Q2011\\E", "shortCiteRegEx": "Magen and Zouzias.", "year": 2011}, {"title": "CUR matrix decompositions for improved data analysis", "author": ["M.W. Mahoney", "P. Drineas"], "venue": "Proceedings of the National Academy of Sciences,", "citeRegEx": "Mahoney and Drineas.,? \\Q2009\\E", "shortCiteRegEx": "Mahoney and Drineas.", "year": 2009}, {"title": "Tensor-CUR decompositions for tensor-based data", "author": ["M.W. Mahoney", "M. Maggioni", "P. Drineas"], "venue": "SIAM Journal on Matrix Analysis and Applications,", "citeRegEx": "Mahoney et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Mahoney et al\\.", "year": 2008}, {"title": "Randomized algorithms for matrices and data", "author": ["Michael W. Mahoney"], "venue": "Foundations and Trends in Machine Learning,", "citeRegEx": "Mahoney.,? \\Q2011\\E", "shortCiteRegEx": "Mahoney.", "year": 2011}, {"title": "Multivariate Analysis", "author": ["K.V. Mardia", "J.T. Kent", "J.M. Bibby"], "venue": null, "citeRegEx": "Mardia et al\\.,? \\Q1979\\E", "shortCiteRegEx": "Mardia et al\\.", "year": 1979}, {"title": "Inequalities: Theory of Majorization and Its Applications", "author": ["Albert W. Marshal", "Ingram Olkin", "Barry C. Arnold"], "venue": null, "citeRegEx": "Marshal et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Marshal et al\\.", "year": 2010}, {"title": "A randomized algorithm for the decomposition of matrices", "author": ["Per-Gunnar Martinsson", "Vladimir Rokhlin", "Mark Tygert"], "venue": "Applied and Computational Harmonic Analysis,", "citeRegEx": "Martinsson et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Martinsson et al\\.", "year": 2011}, {"title": "Spectral regularization algorithms for learning large incomplete matrices", "author": ["Rahul Mazumder", "Trevor Hastie", "Robert Tibshirani"], "venue": "Journal of machine learning research,", "citeRegEx": "Mazumder et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Mazumder et al\\.", "year": 2010}, {"title": "Invariant feature extraction and classification in kernel space", "author": ["S. Mika", "G. R\u00e4tsch", "J. Weston", "B. Sch\u00f6lkopf", "A. Smola", "K.R. M\u00fcller"], "venue": "In Advances in Neural Information Processing Systems 12,", "citeRegEx": "Mika et al\\.,? \\Q2000\\E", "shortCiteRegEx": "Mika et al\\.", "year": 2000}, {"title": "Symmetric gauge functions and unitarily invariant norms", "author": ["L. Mirsky"], "venue": "Quarterly Journal of Mathemathics,", "citeRegEx": "Mirsky.,? \\Q1960\\E", "shortCiteRegEx": "Mirsky.", "year": 1960}, {"title": "Aspects of Multivariate Statistical Theory", "author": ["R.J. Muirhead"], "venue": null, "citeRegEx": "Muirhead.,? \\Q1982\\E", "shortCiteRegEx": "Muirhead.", "year": 1982}, {"title": "Singular value decomposition, eigenfaces, and 3 D reconstruction", "author": ["N. Muller", "L. Magaia", "B.M. Herbst"], "venue": "SIAM Review,", "citeRegEx": "Muller et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Muller et al\\.", "year": 2004}, {"title": "Stronger approximate singular value decomposition via the block lanczos and power methods", "author": ["Cameron Musco", "Christopher Musco"], "venue": "In Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "Musco and Musco.,? \\Q2015\\E", "shortCiteRegEx": "Musco and Musco.", "year": 2015}, {"title": "Osnap: Faster numerical linear algebra algorithms via sparser subspace embeddings", "author": ["Jelani Nelson", "Huy L Nguy\u00ean"], "venue": "In IEEE 54th Annual Symposium on Foundations of Computer Science (FOCS),", "citeRegEx": "Nelson and Nguy\u00ean.,? \\Q2013\\E", "shortCiteRegEx": "Nelson and Nguy\u00ean.", "year": 2013}, {"title": "Some matrix-inequalities and metrication of matrix-space", "author": ["J. von Neumann"], "venue": "Tomsk University Review,", "citeRegEx": "Neumann.,? \\Q1937\\E", "shortCiteRegEx": "Neumann.", "year": 1937}, {"title": "\u00dcber die praktische aufl\u00f6sung von integralgleichungen mit anwendungen auf randwertaufgaben", "author": ["Evert J. Nystr\u00f6m"], "venue": "Acta Mathematica,", "citeRegEx": "Nystr\u00f6m.,? \\Q1930\\E", "shortCiteRegEx": "Nystr\u00f6m.", "year": 1930}, {"title": "Towards a generalized singular value decomposition", "author": ["C.C. Paige", "M.A. Saunders"], "venue": "SIAM Journal on Numerical Analysis,", "citeRegEx": "Paige and Saunders.,? \\Q1981\\E", "shortCiteRegEx": "Paige and Saunders.", "year": 1981}, {"title": "Latent semantic indexing: A probabilistic analysis", "author": ["Christos H Papadimitriou", "Hisao Tamaki", "Prabhakar Raghavan", "Santosh Vempala"], "venue": "In Proceedings of the seventeenth ACM SIGACT-SIGMOD-SIGART symposium on Principles of database systems,", "citeRegEx": "Papadimitriou et al\\.,? \\Q1998\\E", "shortCiteRegEx": "Papadimitriou et al\\.", "year": 1998}, {"title": "Nonlinear discriminant analysis using kernel functions and the generalized singular value decomposition", "author": ["C.H. Park", "H. Park"], "venue": "SIAM Journal on Matrix Analysis and Applications,", "citeRegEx": "Park and Park.,? \\Q2005\\E", "shortCiteRegEx": "Park and Park.", "year": 2005}, {"title": "Trace norm regularization: reformulations, algorithms, and multi-task learning", "author": ["Ting Kei Pong", "Paul Tseng", "Shuiwang Ji", "Jieping Ye"], "venue": "SIAM Journal on Optimization,", "citeRegEx": "Pong et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Pong et al\\.", "year": 2010}, {"title": "Gaussian Processes for Machine Learning", "author": ["C.E. Rasmussen", "C.K.I. Williams"], "venue": null, "citeRegEx": "Rasmussen and Williams.,? \\Q2006\\E", "shortCiteRegEx": "Rasmussen and Williams.", "year": 2006}, {"title": "Convex Analysis", "author": ["T. Rockafellar"], "venue": null, "citeRegEx": "Rockafellar.,? \\Q1970\\E", "shortCiteRegEx": "Rockafellar.", "year": 1970}, {"title": "A randomized algorithm for principal component analysis", "author": ["V. Rokhlin", "A. Szlam", "M. Tygert"], "venue": "SIAM Journal on Matrix Analysis and Applications,", "citeRegEx": "Rokhlin et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Rokhlin et al\\.", "year": 2009}, {"title": "Nonlinear discriminant analysis using kernel functions", "author": ["V. Roth", "V. Steinhage"], "venue": "In Advances in Neural Information Processing Systems 12,", "citeRegEx": "Roth and Steinhage.,? \\Q2000\\E", "shortCiteRegEx": "Roth and Steinhage.", "year": 2000}, {"title": "Numerical methods for large eigenvalue problems. preparation", "author": ["Yousef Saad"], "venue": "Available from: http://www-users. cs. umn. edu/saad/books. html,", "citeRegEx": "Saad.,? \\Q2011\\E", "shortCiteRegEx": "Saad.", "year": 2011}, {"title": "Improved approximation algorithms for large matrices via random projections", "author": ["Tamas Sarlos"], "venue": "In Foundations of Computer Science,", "citeRegEx": "Sarlos.,? \\Q2006\\E", "shortCiteRegEx": "Sarlos.", "year": 2006}, {"title": "A Theory of Cross-Space", "author": ["Robert Schatten"], "venue": null, "citeRegEx": "Schatten.,? \\Q1950\\E", "shortCiteRegEx": "Schatten.", "year": 1950}, {"title": "Learning with Kernels", "author": ["B. Sch\u00f6lkopf", "A. Smola"], "venue": null, "citeRegEx": "Sch\u00f6lkopf and Smola.,? \\Q2002\\E", "shortCiteRegEx": "Sch\u00f6lkopf and Smola.", "year": 2002}, {"title": "Nonlinear component analysis as a kernel eigenvalue problem", "author": ["B. Sch\u00f6lkopf", "A. Smola", "K.-R. M\u00fcller"], "venue": "Neural Computation,", "citeRegEx": "Sch\u00f6lkopf et al\\.,? \\Q1998\\E", "shortCiteRegEx": "Sch\u00f6lkopf et al\\.", "year": 1998}, {"title": "Kernel Methods for Pattern Analysis", "author": ["J. Shawe-Taylor", "N. Cristianini"], "venue": null, "citeRegEx": "Shawe.Taylor and Cristianini.,? \\Q2004\\E", "shortCiteRegEx": "Shawe.Taylor and Cristianini.", "year": 2004}, {"title": "Normalized cuts and image segmentation", "author": ["Jianbo Shi", "Jitendra Malik"], "venue": "Pattern Analysis and Machine Intelligence, IEEE Transactions on,", "citeRegEx": "Shi and Malik.,? \\Q2000\\E", "shortCiteRegEx": "Shi and Malik.", "year": 2000}, {"title": "Memory efficient kernel approximation", "author": ["Si Si", "Cho-Jui Hsieh", "Inderjit Dhillon"], "venue": "In International Conference on Machine Learning (ICML),", "citeRegEx": "Si et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Si et al\\.", "year": 2014}, {"title": "Maximum-margin matrix factorization", "author": ["Nathan Srebro", "Jason Rennie", "Tommi S Jaakkola"], "venue": "In Advances in neural information processing systems,", "citeRegEx": "Srebro et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Srebro et al\\.", "year": 2004}, {"title": "Four algorithms for the efficient computation of truncated pivoted QR approximations to a sparse matrix", "author": ["G.W. Stewart"], "venue": "Numerische Mathematik,", "citeRegEx": "Stewart.,? \\Q1999\\E", "shortCiteRegEx": "Stewart.", "year": 1999}, {"title": "Matrix Perturbation Theory", "author": ["G.W. Stewart", "J.G. Sun"], "venue": null, "citeRegEx": "Stewart and Sun.,? \\Q1990\\E", "shortCiteRegEx": "Stewart and Sun.", "year": 1990}, {"title": "Matrix coherence and the Nystr\u00f6m method", "author": ["A. Talwalkar", "A. Rostamizadeh"], "venue": "Proceedings of the 26th Conference in Uncertainty in Artificial Intelligence,", "citeRegEx": "Talwalkar and Rostamizadeh.,? \\Q2010\\E", "shortCiteRegEx": "Talwalkar and Rostamizadeh.", "year": 2010}, {"title": "Large-scale manifold learning", "author": ["A. Talwalkar", "S. Kumar", "H. Rowley"], "venue": "In IEEE Conference on Computer Vision and Pattern Recognition (CVPR),", "citeRegEx": "Talwalkar et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Talwalkar et al\\.", "year": 2008}, {"title": "Largescale SVD and manifold learning", "author": ["Ameet Talwalkar", "Sanjiv Kumar", "Mehryar Mohri", "Henry Rowley"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Talwalkar et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Talwalkar et al\\.", "year": 2013}, {"title": "A global geometric framework for nonlinear dimensionality reduction", "author": ["Joshua B Tenenbaum", "Vin De Silva", "John C Langford"], "venue": null, "citeRegEx": "Tenenbaum et al\\.,? \\Q2000\\E", "shortCiteRegEx": "Tenenbaum et al\\.", "year": 2000}, {"title": "Regression shrinkage and selection via the lasso", "author": ["R. Tibshirani"], "venue": "Journal of the Royal Statistical Society, Series B,", "citeRegEx": "Tibshirani.,? \\Q1996\\E", "shortCiteRegEx": "Tibshirani.", "year": 1996}, {"title": "An introduction to matrix concentration inequalities", "author": ["Joel A Tropp"], "venue": "Foundations and Trends in Machine Learning,", "citeRegEx": "Tropp.,? \\Q2015\\E", "shortCiteRegEx": "Tropp.", "year": 2015}, {"title": "Making fisher discriminant analysis scalable", "author": ["Bojun Tu", "Zhihua Zhang", "ShusenWang", "Hui Qiani"], "venue": "In Proceedings of the 31th International Conference on Machine Learning", "citeRegEx": "Tu et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Tu et al\\.", "year": 2014}, {"title": "Face recognition using eigenfaces", "author": ["M.A. Turk", "A.P. Pentland"], "venue": "In Proceedings of IEEE International Conference on Computer Vision and Pattern Recognition,", "citeRegEx": "Turk and Pentland.,? \\Q1991\\E", "shortCiteRegEx": "Turk and Pentland.", "year": 1991}, {"title": "Incomplete cross approximation in the mosaic-skeleton", "author": ["E.E. Tyrtyshnikov"], "venue": "method. Computing,", "citeRegEx": "Tyrtyshnikov.,? \\Q2000\\E", "shortCiteRegEx": "Tyrtyshnikov.", "year": 2000}, {"title": "Kernel canonical correlation analysis and least squares support vector machines", "author": ["T. Van Gestel", "J.A.K. Suykens", "J. De Brabanter", "B. De Moor", "J. Vandewalle"], "venue": "In The International Conference on Artificial Neural Networks (ICANN),", "citeRegEx": "Gestel et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Gestel et al\\.", "year": 2001}, {"title": "Statistical Learning Theory", "author": ["V. Vapnik"], "venue": null, "citeRegEx": "Vapnik.,? \\Q1998\\E", "shortCiteRegEx": "Vapnik.", "year": 1998}, {"title": "The Random Projection Method", "author": ["Santosh S. Vempala"], "venue": "American Mathematical Society,", "citeRegEx": "Vempala.,? \\Q2000\\E", "shortCiteRegEx": "Vempala.", "year": 2000}, {"title": "Improving CUR matrix decomposition and the Nystr\u00f6m approximation via adaptive sampling", "author": ["Shusen Wang", "Zhihua Zhang"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Wang and Zhang.,? \\Q2013\\E", "shortCiteRegEx": "Wang and Zhang.", "year": 2013}, {"title": "Efficient algorithms and error analysis for the modified nystr\u00f6m method", "author": ["Shusen Wang", "Zhihua Zhang"], "venue": "In International Conference on Artificial Intelligence and Statistics (AISTATS),", "citeRegEx": "Wang and Zhang.,? \\Q2014\\E", "shortCiteRegEx": "Wang and Zhang.", "year": 2014}, {"title": "The modified Nystr\u00f6m method: Theories, algorithms, and extension", "author": ["Shusen Wang", "Luo Luo", "Zhihua Zhang"], "venue": "CoRR, abs/1406.5675,", "citeRegEx": "Wang et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2014}, {"title": "Improving the modified nystr\u00f6m method using spectral shifting", "author": ["Shusen Wang", "Chao Zhang", "Hui Qian", "Zhihua Zhang"], "venue": "In ACM SIGKDD Conference on Knowledge Discovery and Data Mining (KDD),", "citeRegEx": "Wang et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2014}, {"title": "Improved analyses of the randomized power method and block Lanczos method", "author": ["Shusen Wang", "Zhihua Zhang", "Tong Zhang"], "venue": null, "citeRegEx": "Wang et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2015}, {"title": "Towards more efficient symmetric matrix sketching and cur matrix decomposition", "author": ["Shusen Wang", "Zhihua Zhang", "Tong Zhang"], "venue": "arXiv preprint arXiv:1503.08395,", "citeRegEx": "Wang et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2015}, {"title": "Fundamentals of Matrix Computations", "author": ["D.S. Watkins"], "venue": null, "citeRegEx": "Watkins.,? \\Q1991\\E", "shortCiteRegEx": "Watkins.", "year": 1991}, {"title": "Characterization of the subdifferential of some matrix norms", "author": ["G.A. Watson"], "venue": "Linear Algebra and Its Applications,", "citeRegEx": "Watson.,? \\Q1992\\E", "shortCiteRegEx": "Watson.", "year": 1992}, {"title": "Using the Nystr\u00f6m method to speed up kernel machines", "author": ["C. Williams", "M. Seeger"], "venue": "In Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "Williams and Seeger.,? \\Q2001\\E", "shortCiteRegEx": "Williams and Seeger.", "year": 2001}, {"title": "Randomized algorithms for low-rank matrix factorizations: sharp performance", "author": ["Rafi Witten", "Emmanuel Cand\u00e8s"], "venue": "bounds. Algorithmica,", "citeRegEx": "Witten and Cand\u00e8s.,? \\Q2013\\E", "shortCiteRegEx": "Witten and Cand\u00e8s.", "year": 2013}, {"title": "Low rank approximation lower bounds in row-update streams", "author": ["David Woodruff"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Woodruff.,? \\Q2014\\E", "shortCiteRegEx": "Woodruff.", "year": 2014}, {"title": "Sketching as a tool for numerical linear algebra", "author": ["David P Woodruff"], "venue": "Foundations and Trends in Theoretical Computer Science,", "citeRegEx": "Woodruff.,? \\Q2014\\E", "shortCiteRegEx": "Woodruff.", "year": 2014}, {"title": "A fast randomized algorithm for the approximation of matrices", "author": ["Franco Woolfe", "Edo Liberty", "Vladimir Rokhlin", "Mark Tygert"], "venue": "Applied and Computational Harmonic Analysis,", "citeRegEx": "Woolfe et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Woolfe et al\\.", "year": 2008}, {"title": "Computational and theoretical analysis of null space and orthogonal linear discriminant analysis", "author": ["J. Ye", "T. Xiong"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Ye and Xiong.,? \\Q2006\\E", "shortCiteRegEx": "Ye and Xiong.", "year": 2006}, {"title": "Clustered Nystr\u00f6m method for large scale manifold learning and dimension reduction", "author": ["K. Zhang", "J.T. Kwok"], "venue": "IEEE Transactions on Neural Networks,", "citeRegEx": "Zhang and Kwok.,? \\Q2010\\E", "shortCiteRegEx": "Zhang and Kwok.", "year": 2010}, {"title": "Improved Nystr\u00f6m low-rank approximation and error analysis", "author": ["K. Zhang", "I.W. Tsang", "J.T. Kwok"], "venue": "In International Conference on Machine Learning (ICML),", "citeRegEx": "Zhang et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Zhang et al\\.", "year": 2008}, {"title": "The matrix ridge approximation: algorithms and applications", "author": ["Zhihua Zhang"], "venue": "Machine Learning,", "citeRegEx": "Zhang.,? \\Q2014\\E", "shortCiteRegEx": "Zhang.", "year": 2014}, {"title": "Regularized discriminant analysis, ridge regression and beyond", "author": ["Zhihua Zhang", "Guang Dai", "Congfu Xu", "Michael I. Jordan"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Zhang et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Zhang et al\\.", "year": 2010}, {"title": "Regularized matrix regression", "author": ["Hua Zhou", "Lexin Li"], "venue": "Journal of the Royal Statistical Society: Series B (Statistical Methodology),", "citeRegEx": "Zhou and Li.,? \\Q2014\\E", "shortCiteRegEx": "Zhou and Li.", "year": 2014}], "referenceMentions": [{"referenceID": 38, "context": "The first proof of the SVD for general m \u00d7 n matrices might be given by Eckart and Young [1939]. But the theory of singular values can date back to the 19th century when it had been studied by the Italian differential geometer E.", "startOffset": 72, "endOffset": 96}, {"referenceID": 55, "context": "Please refer to Chapter 3 of Horn and Johnson [1991] in which the authors presented an excellent historical retrospection about the SVD or theory of singular values.", "startOffset": 29, "endOffset": 53}, {"referenceID": 55, "context": "Please refer to Chapter 3 of Horn and Johnson [1991] in which the authors presented an excellent historical retrospection about the SVD or theory of singular values. There is a rich literature involving singular values or SVD. Chapter 3 of Horn and Johnson [1991] provides exhaustive studies about inequalities of singular values as well as unitarily invariant norms, and the primary focus is on the matrix theory.", "startOffset": 29, "endOffset": 264}, {"referenceID": 55, "context": "Please refer to Chapter 3 of Horn and Johnson [1991] in which the authors presented an excellent historical retrospection about the SVD or theory of singular values. There is a rich literature involving singular values or SVD. Chapter 3 of Horn and Johnson [1991] provides exhaustive studies about inequalities of singular values as well as unitarily invariant norms, and the primary focus is on the matrix theory. The books byWatkins [1991], Demmel [1997], Golub and Van Loan [2012], Trefethen and Bau III [1997] present a detailed introduction to SVD, the primary focus of which is on numerical linear algebra.", "startOffset": 29, "endOffset": 442}, {"referenceID": 28, "context": "The books byWatkins [1991], Demmel [1997], Golub and Van Loan [2012], Trefethen and Bau III [1997] present a detailed introduction to SVD, the primary focus of which is on numerical linear algebra.", "startOffset": 28, "endOffset": 42}, {"referenceID": 28, "context": "The books byWatkins [1991], Demmel [1997], Golub and Van Loan [2012], Trefethen and Bau III [1997] present a detailed introduction to SVD, the primary focus of which is on numerical linear algebra.", "startOffset": 28, "endOffset": 69}, {"referenceID": 28, "context": "The books byWatkins [1991], Demmel [1997], Golub and Van Loan [2012], Trefethen and Bau III [1997] present a detailed introduction to SVD, the primary focus of which is on numerical linear algebra.", "startOffset": 28, "endOffset": 99}, {"referenceID": 60, "context": "The preliminaries about matrices please refer to the book of Horn and Johnson [1985]. This tutorial involves matrix differential calculus, majorization theory, and symmetric gauge functions.", "startOffset": 61, "endOffset": 85}, {"referenceID": 60, "context": "The preliminaries about matrices please refer to the book of Horn and Johnson [1985]. This tutorial involves matrix differential calculus, majorization theory, and symmetric gauge functions. For them, the detailed materials can be found in Macnus and Neudecker [2000], Marshal et al.", "startOffset": 61, "endOffset": 268}, {"referenceID": 60, "context": "The preliminaries about matrices please refer to the book of Horn and Johnson [1985]. This tutorial involves matrix differential calculus, majorization theory, and symmetric gauge functions. For them, the detailed materials can be found in Macnus and Neudecker [2000], Marshal et al. [2010], Schatten [1950], Bhatia [1997].", "startOffset": 61, "endOffset": 291}, {"referenceID": 60, "context": "The preliminaries about matrices please refer to the book of Horn and Johnson [1985]. This tutorial involves matrix differential calculus, majorization theory, and symmetric gauge functions. For them, the detailed materials can be found in Macnus and Neudecker [2000], Marshal et al. [2010], Schatten [1950], Bhatia [1997].", "startOffset": 61, "endOffset": 308}, {"referenceID": 60, "context": "The preliminaries about matrices please refer to the book of Horn and Johnson [1985]. This tutorial involves matrix differential calculus, majorization theory, and symmetric gauge functions. For them, the detailed materials can be found in Macnus and Neudecker [2000], Marshal et al. [2010], Schatten [1950], Bhatia [1997]. In Chapter 2 we review some preliminaries such as Kronecker produces and vectorization operators, majorization theory, and derivatives.", "startOffset": 61, "endOffset": 323}, {"referenceID": 96, "context": "Specifically, we apply matrix differential calculus to rederive the von Neumann theorem [Neumann, 1937] and the Ky Fan theorem [Fan, 1951].", "startOffset": 88, "endOffset": 103}, {"referenceID": 40, "context": "Specifically, we apply matrix differential calculus to rederive the von Neumann theorem [Neumann, 1937] and the Ky Fan theorem [Fan, 1951].", "startOffset": 127, "endOffset": 138}, {"referenceID": 38, "context": "The second one was proposed by Mirsky [1960], which is an extension of the novel Eckart Young theorem [Eckart and Young, 1936].", "startOffset": 102, "endOffset": 126}, {"referenceID": 89, "context": "The second one was proposed by Mirsky [1960], which is an extension of the novel Eckart Young theorem [Eckart and Young, 1936].", "startOffset": 31, "endOffset": 45}, {"referenceID": 92, "context": "The following properties can be found in Muirhead [1982].", "startOffset": 41, "endOffset": 57}, {"referenceID": 13, "context": "The further details of these results can be found from Borwein and Lewis [2006]. The following lemma then shows the fundamental role of subgradients in optimization.", "startOffset": 55, "endOffset": 80}, {"referenceID": 58, "context": "The statistical leverage [Hoaglin and Welsch, 1978] measures the extent to which the singular vectors of a matrix are correlated with the standard basis.", "startOffset": 25, "endOffset": 51}, {"referenceID": 48, "context": "That is a so-called CS decomposition [Golub et al., 1999] given as follows.", "startOffset": 37, "endOffset": 57}, {"referenceID": 76, "context": "The GSVD theorem was originally proposed by Loan [1976], in which n \u2265 p (or m \u2265 p) is required.", "startOffset": 44, "endOffset": 56}, {"referenceID": 76, "context": "The GSVD theorem was originally proposed by Loan [1976], in which n \u2265 p (or m \u2265 p) is required. Later on, Paige and Saunders [1981] developed a more general formulation for GSVD in which matrix pencil A and B are required only to have the same number of columns.", "startOffset": 44, "endOffset": 132}, {"referenceID": 76, "context": "The GSVD theorem was originally proposed by Loan [1976], in which n \u2265 p (or m \u2265 p) is required. Later on, Paige and Saunders [1981] developed a more general formulation for GSVD in which matrix pencil A and B are required only to have the same number of columns. Paige and Saunders [1981] also studied a GSVD of submatrices of a column orthonormal matrix.", "startOffset": 44, "endOffset": 289}, {"referenceID": 63, "context": "Based on this observation, Howland et al. [2003], Park and Park [2005] applied GSVD for solving Fisher linear discriminant analysis (FLDA) and generalized Fisher discriminant analysis [Baudat and Anouar, 2000, Mika et al.", "startOffset": 27, "endOffset": 49}, {"referenceID": 63, "context": "Based on this observation, Howland et al. [2003], Park and Park [2005] applied GSVD for solving Fisher linear discriminant analysis (FLDA) and generalized Fisher discriminant analysis [Baudat and Anouar, 2000, Mika et al.", "startOffset": 27, "endOffset": 71}, {"referenceID": 46, "context": "Gibson [1974] proved that they have joint factorizations A = U\u03a3AV T and B = U\u03a3BV T if and only if ABT and BTA are both normal.", "startOffset": 0, "endOffset": 14}, {"referenceID": 2, "context": "For example, SVD is an important tool in spectral analysis [Azar et al., 2001], latent semantic indexing [Papadimitriou et al.", "startOffset": 59, "endOffset": 78}, {"referenceID": 99, "context": ", 2001], latent semantic indexing [Papadimitriou et al., 1998], spectral clustering, and projective clustering [Feldman et al.", "startOffset": 34, "endOffset": 62}, {"referenceID": 41, "context": ", 1998], spectral clustering, and projective clustering [Feldman et al., 2013].", "startOffset": 56, "endOffset": 78}, {"referenceID": 8, "context": "This leads us to the notion of the matrix Moore-Penrose (MP) pseudoinverse [Ben-Israel and Greville, 2003].", "startOffset": 75, "endOffset": 106}, {"referenceID": 144, "context": "However, when N is singular, Zhang et al. [2010] suggested to use a pseudoinverse eigenproblem:", "startOffset": 29, "endOffset": 49}, {"referenceID": 144, "context": "Moreover, Zhang et al. [2010] established a connection between the solutions of the generalized eigenproblem and its corresponding pseudoinverse eigenproblem.", "startOffset": 10, "endOffset": 30}, {"referenceID": 86, "context": "Fisher discriminant analysis (FDA) is a classical method for classification and dimension reduction simultaneously [Mardia et al., 1979].", "startOffset": 115, "endOffset": 136}, {"referenceID": 86, "context": "That is, there is a duality relationship between PCA and PCO [Mardia et al., 1979].", "startOffset": 61, "endOffset": 82}, {"referenceID": 55, "context": "CCA is another subspace learning model [Hardoon et al., 2004].", "startOffset": 39, "endOffset": 61}, {"referenceID": 110, "context": "For example, kernel PCA [Sch\u00f6lkopf et al., 1998], kernel FDA [Baudat and Anouar, 2000, Mika et al.", "startOffset": 24, "endOffset": 48}, {"referenceID": 59, "context": "Additionally, Gu and Eisenstat [1996] proposed efficient algorithms for computing a rank-revealing QR factorization [Hong and Pan, 1992].", "startOffset": 116, "endOffset": 136}, {"referenceID": 51, "context": "Additionally, Gu and Eisenstat [1996] proposed efficient algorithms for computing a rank-revealing QR factorization [Hong and Pan, 1992].", "startOffset": 14, "endOffset": 38}, {"referenceID": 51, "context": "Additionally, Gu and Eisenstat [1996] proposed efficient algorithms for computing a rank-revealing QR factorization [Hong and Pan, 1992]. Stewart [1999] devised efficient computational algorithms of truncated pivoted QR approximations to a sparse matrix.", "startOffset": 14, "endOffset": 153}, {"referenceID": 72, "context": "Kuruvilla et al. [2002] have claimed: \u201cit would be interesting to try to find basis vectors for all experiment vectors, using actual experiment vectors and not artificial bases that offer little insight.", "startOffset": 0, "endOffset": 24}, {"referenceID": 9, "context": "Alternatively, Stewart [1999] proposed a quasi Gram-Schmidt algorithm, obtaining a sparse column-row (SCA) approximation of the original matrix A [Berry et al., 2005].", "startOffset": 146, "endOffset": 166}, {"referenceID": 111, "context": "Alternatively, Stewart [1999] proposed a quasi Gram-Schmidt algorithm, obtaining a sparse column-row (SCA) approximation of the original matrix A [Berry et al.", "startOffset": 15, "endOffset": 30}, {"referenceID": 9, "context": "Alternatively, Stewart [1999] proposed a quasi Gram-Schmidt algorithm, obtaining a sparse column-row (SCA) approximation of the original matrix A [Berry et al., 2005]. The SCA approximation is of the form A \u2248 XTY, where X and Y consist of columns and rows of A, and T minimizes \u2016A\u2212XTY\u2016F . This algorithm is a deterministic peocedure but computationally expensive. The terminology of the CUR decomposition has been proposed by Drineas and Mahoney [2005], Mahoney et al.", "startOffset": 147, "endOffset": 453}, {"referenceID": 9, "context": "Alternatively, Stewart [1999] proposed a quasi Gram-Schmidt algorithm, obtaining a sparse column-row (SCA) approximation of the original matrix A [Berry et al., 2005]. The SCA approximation is of the form A \u2248 XTY, where X and Y consist of columns and rows of A, and T minimizes \u2016A\u2212XTY\u2016F . This algorithm is a deterministic peocedure but computationally expensive. The terminology of the CUR decomposition has been proposed by Drineas and Mahoney [2005], Mahoney et al. [2008]. They reformulated the idea based on random selection.", "startOffset": 147, "endOffset": 476}, {"referenceID": 137, "context": "For example, they have been applied to Gaussian processes [Williams and Seeger, 2001], kernel classification [Zhang et al.", "startOffset": 58, "endOffset": 85}, {"referenceID": 43, "context": ", 2013], spectral clustering [Fowlkes et al., 2004], kernel PCA and manifold learning [Talwalkar et al.", "startOffset": 29, "endOffset": 51}, {"referenceID": 0, "context": ", 2008, Zhang and Kwok, 2010], determinantal processes [Affandi et al., 2013], etc.", "startOffset": 55, "endOffset": 77}, {"referenceID": 116, "context": "Variational principles correspond to matrix perturbation theory [Stewart and Sun, 1990], which is the theoretical foundation to characterize stability or sensitivity of a matrix computation algorithm.", "startOffset": 64, "endOffset": 87}, {"referenceID": 96, "context": "The cornerstones are the novel von Neumann theorem [Neumann, 1937] and Ky Fan theorem [Fan, 1951].", "startOffset": 51, "endOffset": 66}, {"referenceID": 40, "context": "The cornerstones are the novel von Neumann theorem [Neumann, 1937] and Ky Fan theorem [Fan, 1951].", "startOffset": 86, "endOffset": 97}, {"referenceID": 96, "context": "The following cornerstone theorem was originally established by von Neumann [1937].", "startOffset": 68, "endOffset": 83}, {"referenceID": 62, "context": "The first result directly follows from the well known interlacing theorem [Horn and Johnson, 1985].", "startOffset": 74, "endOffset": 98}, {"referenceID": 108, "context": "[Schatten, 1950] Let u,v \u2208 Rn.", "startOffset": 0, "endOffset": 16}, {"referenceID": 40, "context": "[Fan, 1951] Given two nonnegative vectors u,v \u2208 R+, then u \u227aw v if and only if \u03c6(u) \u2264 \u03c6(v) for every symmetric gauge function \u03c6.", "startOffset": 0, "endOffset": 11}, {"referenceID": 121, "context": "Parallel with the l1-norm which is used as convex relaxation of the l0-norm [Tibshirani, 1996], the nuclear norm is a convex alternative of the matrix rank.", "startOffset": 76, "endOffset": 94}, {"referenceID": 116, "context": "Note that the Hoffman-Wielandt theorem still hods when A and B are normal [Stewart and Sun, 1990].", "startOffset": 74, "endOffset": 97}, {"referenceID": 57, "context": "Moreover, this proposition was widely used in matrix completion problems, because an optimization problem regularized by the Frobenius norm is solved more easily than that regularized by the nuclear norm [Hastie et al., 2014].", "startOffset": 204, "endOffset": 225}, {"referenceID": 60, "context": "35 of Horn and Johnson [1985]. As for Part (b), it is obvious that the Frobenius norm is both unitarily invariant and vectorization norm.", "startOffset": 6, "endOffset": 30}, {"referenceID": 13, "context": "But norm functions are convex and continuous, so we can resort to theory of subdifferentials [Rockafellar, 1970, Borwein and Lewis, 2006]. Indeed, the subdifferentials of unitarily invariant norms have been studied by Watson [1992] and Lewis [2003].", "startOffset": 113, "endOffset": 232}, {"referenceID": 13, "context": "But norm functions are convex and continuous, so we can resort to theory of subdifferentials [Rockafellar, 1970, Borwein and Lewis, 2006]. Indeed, the subdifferentials of unitarily invariant norms have been studied by Watson [1992] and Lewis [2003]. Using the properties of unitarily invariant norms and the SVD theory, we present directional derivatives and subdifferentials of unitarily invariant norms.", "startOffset": 113, "endOffset": 249}, {"referenceID": 19, "context": "Moreover, this model can be regarded as a parallel version of the Dantzig selector [Cand\u00e8s and Tao, 2007].", "startOffset": 83, "endOffset": 105}, {"referenceID": 122, "context": "However, Tropp [2015] pointed out that Frobenius-norm error bounds are not acceptable in most cases of practical interest.", "startOffset": 9, "endOffset": 22}, {"referenceID": 38, "context": "The theorem was originally proposed by Eckart and Young [1936] under the setting of the Frobenius norm, and generalized to any unitarily invariant norms by Mirsky [1960].", "startOffset": 39, "endOffset": 63}, {"referenceID": 38, "context": "The theorem was originally proposed by Eckart and Young [1936] under the setting of the Frobenius norm, and generalized to any unitarily invariant norms by Mirsky [1960].", "startOffset": 39, "endOffset": 170}, {"referenceID": 122, "context": "He even said \u201cFrobenius-norm error bounds are typically vacuous\u201d [Tropp, 2015].", "startOffset": 65, "endOffset": 78}, {"referenceID": 51, "context": "[Gu, 2015] Given any matrix A \u2208 Rm\u00d7n, let p = min{m,n} and B be a matrix with rank at most k such that", "startOffset": 0, "endOffset": 10}, {"referenceID": 121, "context": "However, Tropp [2015] pointed out that Frobenius-norm error bounds are not acceptable in most cases of practical interest.", "startOffset": 9, "endOffset": 22}, {"referenceID": 51, "context": "The following theorem was proposed by Gu [2015], which relates the approximation error in the Frobenius norm to that in the spectral norm.", "startOffset": 38, "endOffset": 48}, {"referenceID": 25, "context": "Moreover, it can be also used in large scalable k-means clustering [Cohen et al., 2014], approximate leverage scores [Drineas et al.", "startOffset": 67, "endOffset": 87}, {"referenceID": 4, "context": "[2015] proved optimal approximate matrix multiplication in terms of stable rank by using subspace embedding [Batson et al., 2014].", "startOffset": 108, "endOffset": 129}, {"referenceID": 24, "context": "Recently, Cohen et al. [2015] proved optimal approximate matrix multiplication in terms of stable rank by using subspace embedding [Batson et al.", "startOffset": 10, "endOffset": 30}, {"referenceID": 26, "context": "[Cohen et al., 2015] Given \u03b5, \u03b4 \u2208 (0, 1/2), let A and B be two conforming matrices, and \u03a0 be a (\u03b5, \u03b4) subspace embedding for the 2r\u0303-dimensional subspace, where r\u0303 is the maximum of the stable ranks of A and B.", "startOffset": 0, "endOffset": 20}, {"referenceID": 68, "context": "To analyze approximate matrix multiplication with the Frobenius error, Kane and Nelson [2014] introduced the JL-moment property.", "startOffset": 71, "endOffset": 94}, {"referenceID": 68, "context": "More specifically, they can be converted into each other [Kane and Nelson, 2014].", "startOffset": 57, "endOffset": 80}, {"referenceID": 67, "context": "More specifically, they can be converted into each other [Kane and Nelson, 2014]. There are other methods, which do not use subspace embedding matrices, in the literature. Magen and Zouzias [2011] gave a method based on columns selection.", "startOffset": 58, "endOffset": 197}, {"referenceID": 10, "context": "Bhojanapalli et al. [2015] proposed a new method with sampling and alternating minimization to directly compute a low-rank approximation to the product of two given matrices.", "startOffset": 0, "endOffset": 27}, {"referenceID": 22, "context": "For low-rank matrix approximation in the streaming model, Clarkson and Woodruff [2009] gave the near-optimal space bounds by the sketches.", "startOffset": 58, "endOffset": 87}, {"referenceID": 22, "context": "For low-rank matrix approximation in the streaming model, Clarkson and Woodruff [2009] gave the near-optimal space bounds by the sketches. Liberty [2013] came up with a deterministic streaming algorithm, with an improved analysis studied by Ghashami and Phillips [2014] and space lower bound obtained by Woodruff [2014a].", "startOffset": 58, "endOffset": 154}, {"referenceID": 22, "context": "For low-rank matrix approximation in the streaming model, Clarkson and Woodruff [2009] gave the near-optimal space bounds by the sketches. Liberty [2013] came up with a deterministic streaming algorithm, with an improved analysis studied by Ghashami and Phillips [2014] and space lower bound obtained by Woodruff [2014a].", "startOffset": 58, "endOffset": 270}, {"referenceID": 22, "context": "For low-rank matrix approximation in the streaming model, Clarkson and Woodruff [2009] gave the near-optimal space bounds by the sketches. Liberty [2013] came up with a deterministic streaming algorithm, with an improved analysis studied by Ghashami and Phillips [2014] and space lower bound obtained by Woodruff [2014a].", "startOffset": 58, "endOffset": 321}, {"referenceID": 28, "context": "Fortunately, many machine learning methods such as latent semantic indexing [Deerwester et al., 1990], spectral clustering [Shi and Malik, 2000], manifold learning [Tenenbaum et al.", "startOffset": 76, "endOffset": 101}, {"referenceID": 112, "context": ", 1990], spectral clustering [Shi and Malik, 2000], manifold learning [Tenenbaum et al.", "startOffset": 29, "endOffset": 50}, {"referenceID": 54, "context": "In particular, we will consider randomized SVD methods [Halko et al., 2011].", "startOffset": 55, "endOffset": 75}, {"referenceID": 15, "context": "2 are also fundamental in random column selection [Boutsidis et al., 2014].", "startOffset": 50, "endOffset": 74}, {"referenceID": 23, "context": "In particular, the count sketch [Clarkson and Woodruff, 2013] applies to A in only O(nnz(A)) time and exhibits very similar properties as the JL transform.", "startOffset": 32, "endOffset": 61}, {"referenceID": 24, "context": "The Johnson & Lindenstrauss (JL) transform [Johnson and Lindenstrauss, 1984, Dasgupta and Gupta, 2003] is known to keep isometry in expectation or with high probability. Halko et al. [2011], Boutsidis et al.", "startOffset": 77, "endOffset": 190}, {"referenceID": 15, "context": "[2011], Boutsidis et al. [2014] used the JL transform for sketching and showed relative-error bounds.", "startOffset": 8, "endOffset": 32}, {"referenceID": 15, "context": "[2011], Boutsidis et al. [2014] used the JL transform for sketching and showed relative-error bounds. However, the Gaussian test matrix is dense and cannot efficiently apply to matrices. Several improvements have been proposed to make the sketching matrix sparser; see the review [Woodruff, 2014b] for the complete list of the literature. In particular, the count sketch [Clarkson and Woodruff, 2013] applies to A in only O(nnz(A)) time and exhibits very similar properties as the JL transform. Specifically, Woodruff [2014b] showed that an m\u00d7O(k/\u01eb) sketch C = A\u03a9 can be obtained in O(nnz(A)) time and", "startOffset": 8, "endOffset": 526}, {"referenceID": 54, "context": "Halko et al. [2011] proposed to directly solve the left-hand side of (10.", "startOffset": 0, "endOffset": 20}, {"referenceID": 22, "context": "Clarkson and Woodruff [2013], Woodruff [2014b] showed that", "startOffset": 0, "endOffset": 29}, {"referenceID": 22, "context": "Clarkson and Woodruff [2013], Woodruff [2014b] showed that", "startOffset": 0, "endOffset": 47}, {"referenceID": 51, "context": "We apply Gu\u2019s theorem [Gu, 2015] (Theorem 9.", "startOffset": 22, "endOffset": 32}, {"referenceID": 51, "context": ", 2011, Gu, 2015]. The algorithm is described in Algorithm 2 and analyzed in the following. Let\u03a9 \u2208 Rn\u00d7c be a Gaussian test matrix or count sketch andB = (AAT )tA. Let us takeB instead of A as the input of the prototype algorithm 1 and obtain the approximate left singular vectors \u0168k. It is easy to verify that \u0168k is the same to the output of Algorithm 2. We will show that when t = O( logn \u01eb ), \u2225 \u2225A\u2212 \u0168k\u0168kA \u2225 \u2225 2 2 \u2264 (1 + \u01eb)\u2016A\u2212Ak\u20162. (10.3) To show this result, we need the lemma of Halko et al. [2011].", "startOffset": 8, "endOffset": 502}, {"referenceID": 106, "context": "It turns out that the Krylov subspace method converges much faster than the power iteration [Saad, 2011].", "startOffset": 92, "endOffset": 104}, {"referenceID": 106, "context": "In practice, re-orthogonalization or partial re-orthogonalization are employed to prevent the instability from happening [Saad, 2011].", "startOffset": 121, "endOffset": 133}, {"referenceID": 94, "context": "Very recently, Musco and Musco [2015] showed that with t = logn \u221a \u01eb power iteration, the 1+\u01eb spectral norm bound (10.", "startOffset": 15, "endOffset": 38}, {"referenceID": 120, "context": "For example, spectral clustering, KPCA, Isomap [Tenenbaum et al., 2000], and Laplacian eigenmaps [Belkin and Niyogi, 2003] compute the", "startOffset": 47, "endOffset": 71}, {"referenceID": 7, "context": ", 2000], and Laplacian eigenmaps [Belkin and Niyogi, 2003] compute the", "startOffset": 33, "endOffset": 58}, {"referenceID": 54, "context": "proposed by Halko et al. [2011] for approximating symmetric matrix.", "startOffset": 12, "endOffset": 32}, {"referenceID": 54, "context": "proposed by Halko et al. [2011] for approximating symmetric matrix. Wang et al. [2014a] showed that by randomly samplingO(k/\u01eb) columns of K to form C by a certain algorithm, the approximation is high accurate: \u2225 \u2225K\u2212CXC \u2225 \u2225 2 F \u2264 (1 + \u01eb) \u2225 \u2225K\u2212Kk \u2225 \u2225 2 F .", "startOffset": 12, "endOffset": 88}, {"referenceID": 54, "context": "proposed by Halko et al. [2011] for approximating symmetric matrix. Wang et al. [2014a] showed that by randomly samplingO(k/\u01eb) columns of K to form C by a certain algorithm, the approximation is high accurate: \u2225 \u2225K\u2212CXC \u2225 \u2225 2 F \u2264 (1 + \u01eb) \u2225 \u2225K\u2212Kk \u2225 \u2225 2 F . This upper bound matches the lower bound c \u2265 2k/\u01eb up to a constant factor [Wang et al., 2014a]. Unfortunately, the prototype algorithm has two obvious drawbacks. Firstly, to compute the intersection matrix X\u22c6, every entry of K must be known. As is discussed, it takes O(n2d) time to form the kernel matrix K. Secondly, the matrix multiplication C\u2020K costs O(n2c) time. In sum, the prototype algorithm costs O(n2c + n2d) time. Although it is substantially faster than the exact solution, the prototype algorithm has the same time complexity as the exact solution. Faster SPSD Matrix Sketching. Since C = KS has much more rows than columns, the optimization problem (10.4) is strongly overdetermined. Wang et al. [2015b] proposed to use sketching to approximately solve (10.", "startOffset": 12, "endOffset": 973}, {"referenceID": 131, "context": "Wang et al. [2015b] devised an algorithm that sets p = \u221a nc/ \u221a \u01eb and very efficiently forms the column selection matrix P; and the following error bound holds with high probability:", "startOffset": 0, "endOffset": 20}, {"referenceID": 141, "context": "Motivated by the matrix ridge approximation of Zhang [2014], Wang et al.", "startOffset": 47, "endOffset": 60}, {"referenceID": 131, "context": "Motivated by the matrix ridge approximation of Zhang [2014], Wang et al. [2014b] proposed a spectral shifting kernel approximation method.", "startOffset": 61, "endOffset": 81}, {"referenceID": 131, "context": "Motivated by the matrix ridge approximation of Zhang [2014], Wang et al. [2014b] proposed a spectral shifting kernel approximation method. When the spectrum of K decays slowly, the shifting term helps to improve the approximation accuracy and numerical stability. Wang et al. [2014a] also showed that the spectral shifting approach", "startOffset": 61, "endOffset": 284}, {"referenceID": 113, "context": "can be used to improve other kernel approximation models such as the memory efficient kernel approximation (MEKA) model [Si et al., 2014].", "startOffset": 120, "endOffset": 137}, {"referenceID": 137, "context": "It is named after its inventor Nystr\u00f6m [1930] and gained its popularity in the machine learning society after its application in Gaussian procession regression [Williams and Seeger, 2001].", "startOffset": 160, "endOffset": 187}, {"referenceID": 119, "context": "The Nystr\u00f6m method has been applied to solve million scale kernel methods [Talwalkar et al., 2013].", "startOffset": 74, "endOffset": 98}, {"referenceID": 129, "context": "The lower bound [Wang and Zhang, 2013] indicates that the Nystr\u00f6m method cannot attain (1+\u01eb) relativeerror bound unless it is willing to spend \u03a9(n2k/\u01eb) time.", "startOffset": 16, "endOffset": 38}, {"referenceID": 95, "context": "The Nystr\u00f6m Method is the most popular kernel approximation approach. It is named after its inventor Nystr\u00f6m [1930] and gained its popularity in the machine learning society after its application in Gaussian procession regression [Williams and Seeger, 2001].", "startOffset": 4, "endOffset": 116}, {"referenceID": 47, "context": "Gittens and Mahoney [2013] offered comprehensive error analysis of the Nystr\u00f6m method.", "startOffset": 0, "endOffset": 27}, {"referenceID": 33, "context": "Several different column selection strategies have been devised, among which the leverage score sampling [Drineas et al., 2008] and the adaptive sampling [Wang and Zhang, 2013, Boutsidis and Woodruff, 2014] attain relative error bounds.", "startOffset": 105, "endOffset": 127}, {"referenceID": 32, "context": "An example of Drineas et al. [2008] and Mahoney and Drineas [2009] has well shown this viewpoint; that is, the vector [(1/2)age \u2212 (1/ \u221a 2)height + (1/2)income], the sum of the significant uncorrelated features from a data set of people\u2019s features, is not particularly informative.", "startOffset": 14, "endOffset": 36}, {"referenceID": 32, "context": "An example of Drineas et al. [2008] and Mahoney and Drineas [2009] has well shown this viewpoint; that is, the vector [(1/2)age \u2212 (1/ \u221a 2)height + (1/2)income], the sum of the significant uncorrelated features from a data set of people\u2019s features, is not particularly informative.", "startOffset": 14, "endOffset": 67}, {"referenceID": 14, "context": ", 2008] and the adaptive sampling [Wang and Zhang, 2013, Boutsidis and Woodruff, 2014] attain relative error bounds. In particular, Boutsidis and Woodruff [2014] showed that with c = O(k/\u01eb) columns and r = O(k/\u01eb) rows selected by adaptive sampling to form C and R, min X \u2016A\u2212CXR\u2016F \u2264 (1 + \u01eb)\u2016A\u2212Ak\u2016F holds in expectation.", "startOffset": 57, "endOffset": 162}, {"referenceID": 114, "context": "This approach has been used by Stewart [1999], Wang and Zhang [2013], Boutsidis and Woodruff [2014].", "startOffset": 31, "endOffset": 46}, {"referenceID": 114, "context": "This approach has been used by Stewart [1999], Wang and Zhang [2013], Boutsidis and Woodruff [2014].", "startOffset": 31, "endOffset": 69}, {"referenceID": 14, "context": "This approach has been used by Stewart [1999], Wang and Zhang [2013], Boutsidis and Woodruff [2014]. This approach is very similar to the prototype SPSD matrix approximation method in the previous section, and it costs at least O(mn\u00b7min{c, r}) time and requires observing every entry of A.", "startOffset": 70, "endOffset": 100}, {"referenceID": 14, "context": "This approach has been used by Stewart [1999], Wang and Zhang [2013], Boutsidis and Woodruff [2014]. This approach is very similar to the prototype SPSD matrix approximation method in the previous section, and it costs at least O(mn\u00b7min{c, r}) time and requires observing every entry of A. Apparently, it cannot help speed up matrix computation. Wang et al. [2015a] proposed a more practical CUR decomposition method which solves (10.", "startOffset": 70, "endOffset": 366}], "year": 2015, "abstractText": "The singular value decomposition (SVD) is not only a classical theory in matrix computation and analysis, but also is a powerful tool in machine learning and modern data analysis. In this tutorial we first study the basic notion of SVD and then show the central role of SVD in matrices. Using majorization theory, we consider variational principles of singular values and eigenvalues. Built on SVD and a theory of symmetric gauge functions, we discuss unitarily invariant norms, which are then used to formulate general results for matrix low rank approximation. We study the subdifferentials of unitarily invariant norms. These results would be potentially useful in many machine learning problems such as matrix completion and matrix data classification. Finally, we discuss matrix low rank approximation and its recent developments such as randomized SVD, approximate matrix multiplication, CUR decomposition, and Nystr\u00f6m approximation. Randomized algorithms are important approaches to large scale SVD as well as fast matrix computations.", "creator": "LaTeX with hyperref package"}}}