{"id": "1510.08565", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "29-Oct-2015", "title": "Attention with Intention for a Neural Network Conversation Model", "abstract": "In a conversation or a dialogue process, attention and intention play intrinsic roles. This paper proposes a neural network based approach that models the attention and intention processes. It essentially consists of three recurrent networks. The encoder network is a word-level model representing source side sentences. The intention network is a recurrent network that models the dynamics of the intention process. The decoder network is a recurrent network produces responses to the input from the source side. It is a language model that is dependent on the intention and has an attention mechanism to attend to particular source side words, when predicting a symbol in the response. The model is trained end-to-end without labeling data. Experiments show that this model generates natural responses to user inputs.", "histories": [["v1", "Thu, 29 Oct 2015 05:31:28 GMT  (58kb,D)", "http://arxiv.org/abs/1510.08565v1", null], ["v2", "Mon, 2 Nov 2015 02:17:15 GMT  (58kb,D)", "http://arxiv.org/abs/1510.08565v2", null], ["v3", "Thu, 5 Nov 2015 07:26:01 GMT  (58kb,D)", "http://arxiv.org/abs/1510.08565v3", null]], "reviews": [], "SUBJECTS": "cs.NE cs.AI cs.HC cs.LG", "authors": ["kaisheng yao", "geoffrey zweig", "baolin peng"], "accepted": false, "id": "1510.08565"}, "pdf": {"name": "1510.08565.pdf", "metadata": {"source": "CRF", "title": "Attention with Intention for a Neural Network Conversation Model", "authors": ["Kaisheng Yao", "Geoffrey Zweig", "Baolin Peng"], "emails": ["kaisheny@microsoft.com", "gzweig@microsoft.com", "blpeng@se.cuhk.edu.hk"], "sections": [{"heading": "1 Introduction", "text": "A conversation process is a process of communicating thoughts through words. It can be regarded as a structural process that emphasizes the role of purpose and processing in discourse. [7] Essentially, the discourse structure is closely linked to two non-linguistic terms: intention and attention. In the processing of utterances, attention is drawn to certain words in a sentence. On the other hand, intention is higher than attention and has its primary role to explain discourse structures and coherence."}, {"heading": "2 Background", "text": "In the theory of discourse in [7], the discourse structure consists of three separate but interconnected components: the first is the linguistic structure that grasps the purposes relevant to the discourse that are expressed in each of the linguistic segments and in the relationships between them; the third is the state of attention that is dynamic and records the objects, characteristics and relationships that stand out at each point in the discourse; in many of the examples we observe, there is usually only one linguistic segment that consists of all utterances; therefore, in the following, we will consider a discourse with two structures: intention and attention. In Table 1, for example, there is a clear flow of intentions. The user indicates the problem, with the intention of the user to pass the problem on to the agent."}, {"heading": "3 The model", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "3.1 The attention with intention (AWI) model", "text": "Figure 1 illustrates the model. It shows three processing levels: encoder network, intention network and decoder network. The encoder network has inputs from the current source side input. Since the source page in the current turn is also dependent on the previous turn, the source side encoder network is linked to the output from the previous destination page. The encoder network generates a representation of the source page in the current turn. The intention network is dependent on its previous state, so it stores the history of the intentions. It is therefore a recursive network that takes over a representation of the source page in the current turn and updates its hidden state.The decoder is a recursive network for voice modeling that outputs an icon at any time. This output depends on the current intention of the target network."}, {"heading": "3.2 Encoder network", "text": "The encoder network reads the input set ~ x (s) and converts it into a representation of fixed length or variable length of the source side sequence. There are many ways to encode the source side. The approach we use is an RNN of such length (n) t = f (x (s) t, h (s) t-1) (1), where f (\u00b7) is an RNN. h (s) t is the hidden state at present t on the source side. The initial state h (s) t with t = 0 is a learned parameter vector. One form of the output of this encoder is the last hidden state activity c (s) T = h (s) T. This is used as the representation of the source side in the current turn to the intensity network. The other form is a variable length representation that is to be used in the attention model described in Sec. 3.4. A general variable description of the length representation is {swc = (t), where (t) could be (t)."}, {"heading": "3.3 Intention network", "text": "According to [7], the intention process is a dynamic process for modeling intrinsic conversation dynamics, in which an intention in one move depends on the intention in the previous move. This property could be modeled using a Markov model, but we choose an RNN. Interestingly, the hidden state of an RNN in a particular move can be considered a distributed representation of intention. Unlike the usual process of distributed representation of words in training [9], the distribution representation of intentions is trained using earlier phrases as context. We use a first order RNN model, in which a hidden state is explicitly dependent on its previous state.The intention model in AWI is therefore an RNN like the following h (i, k) = f (c (s) T, h (i, k-1)))) (3), where c (s) T is the fixed dimension representation of the source described in 2nd Sec."}, {"heading": "3.4 Decoder network", "text": "In this context, the decoder calculates the conditional probability asp (y (t) j | y (t) 1, \u00b7 \u00b7, y (t) j-1, ~ x (s) = g (y (t) j-1, h (t) j, c (t) j) (4), where the hidden state in the decoder is calculated using an RNNh (t) j = f (t) j-1, h (t) j-1, c (t) j (5).c (t) j is a vector to represent the context for generating y (t) j. It depends on the source-side attention asc (t) j = z (t) j (t) j-1, {c (s) j: t = {1, \u00b7 \u00b7, T} (6), if the source-side asc (t) is jmerable weight (j1)."}, {"heading": "3.5 Implementation details", "text": "The context vector c (s) t is an embedding vector of the source-side word in due course. The alignment model in Equation (8) follows the attention model in [1], in which ejt asejt = ~ v > tanh (W (ah) h (t) j-1 + W (ae) c (s) t) is calculated, (9) which is a neural network with a hidden layer of size A and a single output, parameterized by W (ae), W (ah), RA (H) and ~ v (RA) RA. H and A are the dimension of the hidden layer and the dimension of alignment."}, {"heading": "4 Evaluation", "text": "The training consists of 10000 dialogues with 96913 spins or conversations. The number of tokens is 2215047 on the source page and 2378950 on the landing page. The vocabulary consists of 9085 words from both sides. The data of the development data set have 1000 dialogues with 9971 spins. Test data sets have 500 dialogs with 5232 spins. We use SGD at the sentence level without dynamics. The learning rate is initialized to 0.1. The development set is used to control the learning rate. The learning rate halves if the perplexity about the development is increased. A training period has a run of the training data. The sequence of the training dialogs is randomly mixed at the beginning of each epoch. However, the sequence of the spins in the dialogue is maintained."}, {"heading": "4.1 Performances measure in perplexity", "text": "An objective comparison of different models for conversation is still an open question. We report perplexity (PPL), although it can have disadvantages to compare different models. Table 2 shows results in perplexity for two models with different hide-layer sizes. Results show that a larger model with 200 hide-layer dimensions has a lower PPL than the model with 50 dimensions."}, {"heading": "4.2 Examples of outputs from the trained model", "text": "Table 3 gives an example of the conversation process between a human being and the trained model. It has two levels of LSTMs and other constellations are the same as in Section 4.1. Similar to [11], the model generates natural reactions to user input. In this example, the flow of intentions is clearly visible."}, {"heading": "5 Related work", "text": "Our work refers to the recent work in [10, 12, 14] in which an encoder decoder framework is used to model conversations; the work in [10] is a model for one-turn conversations; the work in [14] is a simple encoder decoder procedure with a fixed representation of the source page; the work in [12] also uses a fixed representation of the source page, but has an additional RNN to model the dialogue context; this additional RNN is similar to the intention of RNN in the AWI model; however, the AWI model differs from [12] in that it incorporates the concept of attention and intention on the basis of the theory in [7]; therefore, the attention mechanism is indispensable for AWI; the model in [12] has no attention model; as it is not yet clear what objective measure should be used to compare different models, it is difficult to claim superiority over these models."}, {"heading": "6 Conclusions and discussions", "text": "We have presented a model that integrates attention and intention processes into a neural network model. Preliminary experiments show that this model generates natural responses to user input. Future work will include experiments with common datasets to compare different models and include objective functions such as goals."}], "references": [{"title": "Neural machine translation by jointly learning to align and translate", "author": ["D. Bahdanau", "K. Cho", "Y. Bengio"], "venue": "In Proceedings of the International Conference on Learning Representations (ICLR),", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2015}, {"title": "Joint-sequence models for grapheme-to-phoneme conversion", "author": ["M. Bisani", "H. Ney"], "venue": "Speech Communication,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2008}, {"title": "The ravenclaw dialog management framework: architecture and systems", "author": ["D. Bohus", "A.I. Rudnicky"], "venue": "Computer, Speech and Language,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2009}, {"title": "Listen, attend and spell", "author": ["W. Chan", "N. Jaitly", "Q.V. Le", "O. Vinyals"], "venue": "[cs.CL],", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2015}, {"title": "Hierarchical phrase-based translation", "author": ["D. Chiang"], "venue": "Computational Linguistics,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 1999}, {"title": "Fast and robust neural network joint models for statistical machine translation", "author": ["J. Devlin", "R. Zbib", "Z. Huang", "T. Lamar", "R. Schwartz", "J. Makhoul"], "venue": "In ACL,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2014}, {"title": "Attention, intentions, and the structure of discourse", "author": ["B.J. Grosz", "C.L. Sidner"], "venue": "Computational Linguistics,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 1986}, {"title": "Using recurrent neural networks for slot filling in spoken language understanding", "author": ["G. Mesnil", "Y. Dauphin", "K. Yao", "Y. Bengio", "L. Deng", "D. Hakkani-Tur", "X. He", "L. Heck", "G. Tur", "D. Yu", "G. Zweig"], "venue": "IEEE/ACM Transactions on Audio, Speech, and Language Processing,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2015}, {"title": "Efcient estimation of word representations in vector space", "author": ["T. Mikolov", "K. Chen", "G. Corrado", "J. Dean"], "venue": "In NIPS,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2013}, {"title": "Neural responding machine for short-text conversation", "author": ["L. Shang", "Z. Lu", "H. Li"], "venue": "In ACL,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2015}, {"title": "A hierarchical recurrent encoder-decoder for generative context-aware query suggestion", "author": ["A. Sordoni", "Y. Bengio", "H. Vahabi", "C. Lioma", "J.G. Simonsen", "J.-Y. Nie"], "venue": "[cd.NE],", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2015}, {"title": "A neural network approach to context-sensitive generation of conversation responses", "author": ["A. Sordoni", "M. Galley", "M. Auli", "C. Brockett", "Y. Ji", "M. Mitchell", "J.-Y. Nie", "J. Gao", "B. Dolan"], "venue": "In NAACL,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2015}, {"title": "Sequence to sequence learning with neural networks", "author": ["I. Sutskever", "O. Vinyals", "Q.V. Le"], "venue": "In Neural Information Processing Systems (NIPS),", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2014}, {"title": "A nerual converstion model", "author": ["O. Vinyals", "Q.V. Le"], "venue": "In ICML Deep Learning Workshop,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2015}, {"title": "Stochastic language generation in dialogue using recurrent neural networks with convoulutional sentence reranking", "author": ["T.-H. Wen", "M. Gasic", "D. Kim", "N. Mrksic", "P.-H. Su", "D. Vandyke", "S. Young"], "venue": "Technical report,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2015}, {"title": "Depth-gated LSTM", "author": ["K. Yao", "T. Cohn", "E. Vylomova", "K. Duh", "C. Dyer"], "venue": "[cs.NE],", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2015}, {"title": "Sequence-to-sequence neural net models for grapheme-to-phoneme conversion", "author": ["K. Yao", "G. Zweig"], "venue": "In INTERSPEECH,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2015}, {"title": "POMDP-based statistical spoken dialog systems: A review", "author": ["S. Young", "M. Gasic", "B. Thomson", "J.D. Williams"], "venue": "Proceedings of the IEEE,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2013}], "referenceMentions": [{"referenceID": 6, "context": "It may be considered as a structural process that stresses the role of purpose and processing in discourse [7].", "startOffset": 107, "endOffset": 110}, {"referenceID": 0, "context": "They have made significant progresses in machine translation [1,6,13], language understanding [8], and speech recognition [4].", "startOffset": 61, "endOffset": 69}, {"referenceID": 5, "context": "They have made significant progresses in machine translation [1,6,13], language understanding [8], and speech recognition [4].", "startOffset": 61, "endOffset": 69}, {"referenceID": 12, "context": "They have made significant progresses in machine translation [1,6,13], language understanding [8], and speech recognition [4].", "startOffset": 61, "endOffset": 69}, {"referenceID": 7, "context": "They have made significant progresses in machine translation [1,6,13], language understanding [8], and speech recognition [4].", "startOffset": 94, "endOffset": 97}, {"referenceID": 3, "context": "They have made significant progresses in machine translation [1,6,13], language understanding [8], and speech recognition [4].", "startOffset": 122, "endOffset": 125}, {"referenceID": 0, "context": "Among those neural network-based approaches, one particular approach, which is called encoder-decoder framework [1, 13], aims at relaxing much requirement on human labeling.", "startOffset": 112, "endOffset": 119}, {"referenceID": 12, "context": "Among those neural network-based approaches, one particular approach, which is called encoder-decoder framework [1, 13], aims at relaxing much requirement on human labeling.", "startOffset": 112, "endOffset": 119}, {"referenceID": 2, "context": "Conversation models have been typically designed to be domain specific with much knowledge such as rules [3,18].", "startOffset": 105, "endOffset": 111}, {"referenceID": 17, "context": "Conversation models have been typically designed to be domain specific with much knowledge such as rules [3,18].", "startOffset": 105, "endOffset": 111}, {"referenceID": 14, "context": "Recent methods [15] relax such requirement to some extent but their whole systems \u2217Presented at NIPS Workshop on Machine Learning for Spoken Language Understanding and Interaction 2015.", "startOffset": 15, "endOffset": 19}, {"referenceID": 9, "context": "Recent works in [10, 12, 14] are in this approach.", "startOffset": 16, "endOffset": 28}, {"referenceID": 11, "context": "Recent works in [10, 12, 14] are in this approach.", "startOffset": 16, "endOffset": 28}, {"referenceID": 13, "context": "Recent works in [10, 12, 14] are in this approach.", "startOffset": 16, "endOffset": 28}, {"referenceID": 16, "context": "For example, the alignment information between the source and target side is critical in grapheme-to-phoneme conversation [17] to outperform a strong baseline using n-gram models [2].", "startOffset": 122, "endOffset": 126}, {"referenceID": 1, "context": "For example, the alignment information between the source and target side is critical in grapheme-to-phoneme conversation [17] to outperform a strong baseline using n-gram models [2].", "startOffset": 179, "endOffset": 182}, {"referenceID": 5, "context": "In a neural network based machine translation system [6], the alignment information is used to outperform a strong phrase-based baseline [5].", "startOffset": 53, "endOffset": 56}, {"referenceID": 4, "context": "In a neural network based machine translation system [6], the alignment information is used to outperform a strong phrase-based baseline [5].", "startOffset": 137, "endOffset": 140}, {"referenceID": 6, "context": "In the theory of discourse in [7], discourse structure is composed of three separate but related components.", "startOffset": 30, "endOffset": 33}, {"referenceID": 6, "context": "Following [7], the intention process is a dynamic process to model the intrinsic dynamics of conversation, in which an intention in one turn is dependent on the intention in the previous turn.", "startOffset": 10, "endOffset": 13}, {"referenceID": 8, "context": "Different from the usual process of training distributed representation of words [9], the distribution representation of intentions are trained with previous turns as their context.", "startOffset": 81, "endOffset": 84}, {"referenceID": 0, "context": "The weight is computed using a content-based alignment model [1] that produces high scores if the target side hidden state in previous time h j-1 and c (s) t are similar.", "startOffset": 61, "endOffset": 64}, {"referenceID": 15, "context": "All of the recurrent networks are implemented using a recently proposed depth-gated long-shortterm memory (LSTM) network [16].", "startOffset": 121, "endOffset": 125}, {"referenceID": 0, "context": "(8) follows the attention model in [1], in which ejt is calculated as ejt = ~v > tanh ( W(ah)h (t) j-1 +W (ae)c (s) t ) , (9)", "startOffset": 35, "endOffset": 38}, {"referenceID": 10, "context": "Similarly as observed in [11], the model produces natural responses to user inputs.", "startOffset": 25, "endOffset": 29}, {"referenceID": 9, "context": "Our work is related to the recent work in [10, 12, 14], which uses an encoder-decoder framework to model conversation.", "startOffset": 42, "endOffset": 54}, {"referenceID": 11, "context": "Our work is related to the recent work in [10, 12, 14], which uses an encoder-decoder framework to model conversation.", "startOffset": 42, "endOffset": 54}, {"referenceID": 13, "context": "Our work is related to the recent work in [10, 12, 14], which uses an encoder-decoder framework to model conversation.", "startOffset": 42, "endOffset": 54}, {"referenceID": 9, "context": "The work in [10] is a model for single turn conversation.", "startOffset": 12, "endOffset": 16}, {"referenceID": 13, "context": "The work in [14] is a simple encoder-decoder method using a fixed-dimension representation of the source side.", "startOffset": 12, "endOffset": 16}, {"referenceID": 11, "context": "The work in [12] also uses a fixed-dimension representaiton of the source side but has an additional RNN to model dialogue context.", "startOffset": 12, "endOffset": 16}, {"referenceID": 11, "context": "However, AWI model differs from [12] in that it incorprates the concept of attention and intention based on the theory in [7].", "startOffset": 32, "endOffset": 36}, {"referenceID": 6, "context": "However, AWI model differs from [12] in that it incorprates the concept of attention and intention based on the theory in [7].", "startOffset": 122, "endOffset": 125}, {"referenceID": 11, "context": "The model in [12] doesn\u2019t have an attention model.", "startOffset": 13, "endOffset": 17}, {"referenceID": 11, "context": "We believe AWI model is an alternative to the models in [12, 14].", "startOffset": 56, "endOffset": 64}, {"referenceID": 13, "context": "We believe AWI model is an alternative to the models in [12, 14].", "startOffset": 56, "endOffset": 64}], "year": 2017, "abstractText": "In a conversation or a dialogue process, attention and intention play intrinsic roles. This paper proposes a neural network based approach that models the attention and intention processes. It essentially consists of three recurrent networks. The encoder network is a word-level model representing source side sentences. The intention network is a recurrent network that models the dynamics of the intention process. The decoder network is a recurrent network produces responses to the input from the source side. It is a language model that is dependent on the intention and has an attention mechanism to attend to particular source side words, when predicting a symbol in the response. The model is trained end-to-end without labeling data. Experiments show that this model generates natural responses to user inputs.", "creator": "LaTeX with hyperref package"}}}