{"id": "1510.08660", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "29-Oct-2015", "title": "RATM: Recurrent Attentive Tracking Model", "abstract": "This work presents an attention mechanism-based neural network approach for tracking objects in video. A recurrent neural network is trained to predict the position of an object at time t+1 given a series of selective glimpses into video frames at time steps 1 to t. The proposed recurrent attentive tracking model can be trained using simple gradient-based training. Various settings are explored in experiments on artificial data to justify design choices.", "histories": [["v1", "Thu, 29 Oct 2015 12:06:56 GMT  (596kb,D)", "http://arxiv.org/abs/1510.08660v1", null], ["v2", "Fri, 20 Nov 2015 21:04:30 GMT  (871kb,D)", "http://arxiv.org/abs/1510.08660v2", "ICLR 2016 submission"], ["v3", "Thu, 26 Nov 2015 13:47:19 GMT  (907kb,D)", "http://arxiv.org/abs/1510.08660v3", "Under review as a conference paper at ICLR 2016"], ["v4", "Thu, 28 Apr 2016 07:32:03 GMT  (4670kb,D)", "http://arxiv.org/abs/1510.08660v4", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["samira ebrahimi kahou", "vincent michalski", "roland memisevic"], "accepted": false, "id": "1510.08660"}, "pdf": {"name": "1510.08660.pdf", "metadata": {"source": "CRF", "title": "RATM: Recurrent Attentive Tracking Model", "authors": ["Samira Ebrahimi Kahou", "Vincent Michalski", "Roland Memisevic"], "emails": ["samira.ebrahimi-kahou@polymtl.ca", "vincent.michalski@umontreal.ca", "roland.memisevic@umontreal.ca"], "sections": [{"heading": "1 Introduction", "text": "Traditional object tracking work uses different scanning methods to select and evaluate multiple window candidates on the basis of similarity in the attribute space. [1] In this work, we present a tracking method that predicts the position of an object at a given time, and not only on the basis of previous information, i.e. without taking into account the contents of the current frame.Recently, a fully differentiated attention mechanism was introduced that can selectively read and write in images, and used to create a model for sequential image generation [2]. Interestingly, in the experiments of [2] on handwritten numerals, the reading mechanism uses contours and the writing mechanism that generates digits in a continuous movement of the writing window, indicating the potential of this method for tracking applications [2]. Interestingly, the proposed recursive attention-oriented tracking model (RATM) uses a modified version of this reading mechanism to fully train the earlier use of attention networks."}, {"heading": "2 Method", "text": "The proposed model essentially consists of two components: an attention mechanism that extracts patches from the input images, and a recurrent neural network (RNN) that predicts attention parameters, i.e. predicts where to look in the next frame. Sections 2.1 and 2.2 respectively describe the attention mechanism or RNN."}, {"heading": "2.1 Attention", "text": "The attention mechanism introduced in [2] extracts insights from the input data by applying a set of 2D Gaussian window filters (one pixel of view each). Taking advantage of the separability of 2D Gaussian windows, this attention mechanism constructs an N \u00b7 N grid of two-dimensional Gaussian filters by applying a sequential set of line filters and a set of column filters. \u2212 P value: 151 0.08 660v 1 [cs.L G] 29 O (a) layout of the Recurrent Attentive Tracking Model. The black arrows indicate that the hidden states affect the patch extraction in the next step. \u2212 P value: A complete image, bottom: The selected attention patch displayed as a network input.Filter. The grid has 4 parameters 1, the grid X, grid points, Y, and the standard gotrope between the input points."}, {"heading": "2.2 Recurrent Neural Network", "text": "Advances in the optimization of RNNs enable their successful application in a wide range of tasks [5, 6, 7]. The simplest RNN consists of an input, a hidden, and an output level. t calculates the network based on the input frame xt and the previous hidden state ht \u2212 1 the new hidden state ht = \u03c3 (Winxt + Wrecht \u2212 1), (5) which is a non-linear activation function. t is the input and Wrec is the recursive weight matrix. While the application of the more complex short-term memories [7] and gated recurrent units [6] is very common in recent work, we rely on a simpler variant of recursive networks called IRNN [8], where the IRN is rescaled with a recalibrated version of the identity and identity of the IRN [6]."}, {"heading": "2.3 Recurrent Attentive Tracking Model", "text": "The purpose of tracking is to map a sequence of inputs X = (x1, x2,.., xT) to a sequence of locations Y = (y1, y2,.., yT). Given that the trajectory (y1,.., yt \u2212 1) usually contains highly relevant contextual information for predicting the position of an object at the time t, it is appropriate to choose a hidden state model capable of creating a representation of this trajectory. The proposed RATM is then a fully differentiable neural network consisting of the described attention mechanism and an RNN. The hidden state vector of the RNN is displayed on window parameters according to equations 2 and 3. The resulting image patch, returned by the attention mechanism, is then passed on to the RNN as input xt + 1, which stems + 1 according to equation 5. A sketch of the model of an image patch 1a is used in the NNNN model."}, {"heading": "3 Experiments", "text": "In this section, we present some of the experiments with RATM and comment on qualitative results. Unless otherwise noted, the initial parameters of the attention mechanism are set so that the resulting first patch covers roughly the entire image."}, {"heading": "3.1 Bouncing Balls", "text": "For our first experiment, we used the 32-frame bouncing ball dataset [10]. In the objective function, we merely minimized the mean error square between the selected patch in the last frame and a target patch, which is simply a cropped white ball image, since the shape and color of the object is constant throughout the dataset. Dynamics in this dataset are very limited, and although the tracking looks accurate, it is explained by the potential of the RNNs to remember the amount of possible trajectories, which is why we experimented with more sophisticated datasets. Figure 1 shows the results of tracking a ball in a test sequence in the top row."}, {"heading": "3.2 MNIST", "text": "Single-digit: To better understand the learning ability of RATM, we generated a new data set by drawing a number randomly from MNIST [11]. We respected the same data split for training and testing as MNIST, i.e. numbers were drawn from the training split to generate training sequences and test splits for tests. the videos produced are 100 x 100 in size and show a number that moves in a random step with dynamics against a black background. This experiment is different from ball tracking because the numbers come from different classes that the model has to recognize. As an objective function, we used the average classification error provided by a trained flat CNN that classified the patches of each time step. This CNN is trained on the MNIST dataset and the model parameters remained fixed during the RATM training. The model did not learn to change the boundary between the boundary and the vertex of the boundary, in order to first learn the truth."}, {"heading": "4 Conclusion and Future Work", "text": "We experimented with RATM on the Caltech Pedestrian Dataset [12, 13], but the number of long annotated training sequences containing the same subject is quite low for low learning standards. Furthermore, the frame rate is very limited, so the model has learned to predict almost a static window. Results were inconclusive and are not presented here. Currently, we are experimenting with other data sets that do not only contain pedestrians, trying to provide quantitative results as a basis for comparison. As a long-term project goal, we plan to explore multi-object tracking and the representation learned through the network. In particular, we will focus on the model's ability to handle occlusions and compress object identity and trajectory in its hidden state."}, {"heading": "Acknowledgments", "text": "The authors thank the developers of Theano [14, 15] and Jo \ufffd rg Bornschein for the helpful discussions. This work was supported by the NSERC Discovery Award and the BMBF, Project 01GQ0841. We also thank the Canadian Foundation for Innovation (CFI) for their support within the framework of the Leaders Programme."}], "references": [{"title": "Visual tracking: An experimental survey", "author": ["Arnold W M Smeulders", "Dung M Chu", "Rita Cucchiara", "Simone Calderara", "Afshin Dehghan", "Mubarak Shah"], "venue": "Pattern Analysis and Machine Intelligence, IEEE Transactions on,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2014}, {"title": "DRAW: A recurrent neural network for image generation", "author": ["Karol Gregor", "Ivo Danihelka", "Alex Graves", "Daan Wierstra"], "venue": "CoRR, abs/1502.04623,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2015}, {"title": "Learning where to attend with deep architectures for image tracking", "author": ["Misha Denil", "Loris Bazzani", "Hugo Larochelle", "Nando de Freitas"], "venue": "CoRR, abs/1109.3737,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2011}, {"title": "Rectified linear units improve restricted boltzmann machines", "author": ["Vinod Nair", "Geoffrey E Hinton"], "venue": "In Proceedings of the 27th International Conference on Machine Learning", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2010}, {"title": "Sequence to sequence learning with neural networks", "author": ["Ilya Sutskever", "Oriol Vinyals", "Quoc VV Le"], "venue": "In Advances in neural information processing systems,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2014}, {"title": "Learning phrase representations using rnn encoderdecoder for statistical machine translation", "author": ["Kyunghyun Cho", "Bart Van Merri\u00ebnboer", "Caglar Gulcehre", "Dzmitry Bahdanau", "Fethi Bougares", "Holger Schwenk", "Yoshua Bengio"], "venue": "arXiv preprint arXiv:1406.1078,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2014}, {"title": "Long short-term memory", "author": ["Sepp Hochreiter", "J\u00fcrgen Schmidhuber"], "venue": "Neural computation,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 1997}, {"title": "A simple way to initialize recurrent networks of rectified linear units", "author": ["Quoc V Le", "Navdeep Jaitly", "Geoffrey E Hinton"], "venue": "arXiv preprint arXiv:1504.00941,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2015}, {"title": "Recurrent neural networks for emotion recognition in video", "author": ["Samira Ebrahimi Kahou", "Vincent Michalski", "Kishore Konda", "Roland Memisevic", "Christopher Pal"], "venue": "In Proceedings of the 17th ACM on International Conference on Multimodal Interaction, ICMI \u201915,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2015}, {"title": "The recurrent temporal restricted boltzmann machine", "author": ["Ilya Sutskever", "Geoffrey E Hinton", "Graham W Taylor"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2009}, {"title": "Gradient-based learning applied to document recognition", "author": ["Yann Lecun", "Leon Bottou", "Yoshua Bengio", "Patrick Haffner"], "venue": "Proceedings of the IEEE,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 1998}, {"title": "Pedestrian detection: A benchmark", "author": ["Piotr Doll\u00e1r", "Christian Wojek", "Bernt Schiele", "Pietro Perona"], "venue": "In CVPR,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2009}, {"title": "Pedestrian detection: An evaluation of the state of the art", "author": ["Piotr Doll\u00e1r", "Christian Wojek", "Bernt Schiele", "Pietro Perona"], "venue": null, "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2012}, {"title": "Theano: new features and speed improvements", "author": ["Fr\u00e9d\u00e9ric Bastien", "Pascal Lamblin", "Razvan Pascanu", "James Bergstra", "Ian Goodfellow", "Arnaud Bergeron", "Nicolas Bouchard", "David Warde-Farley", "Yoshua Bengio"], "venue": "arXiv preprint arXiv:1211.5590,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2012}, {"title": "Theano: a cpu and gpu math expression compiler", "author": ["James Bergstra", "Olivier Breuleux", "Fr\u00e9d\u00e9ric Bastien", "Pascal Lamblin", "Razvan Pascanu", "Guillaume Desjardins", "Joseph Turian", "David Warde-Farley", "Yoshua Bengio"], "venue": "In Proceedings of the Python for scientific computing conference (SciPy),", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2010}], "referenceMentions": [{"referenceID": 0, "context": "[1].", "startOffset": 0, "endOffset": 3}, {"referenceID": 1, "context": "Recently, a fully differentiable attention mechanism, that can selectively read and write in images was introduced and used to build a model for sequential generation of images [2].", "startOffset": 177, "endOffset": 180}, {"referenceID": 1, "context": "Interestingly, in the experiments of [2] on handwritten digits, the read mechanism learned to trace contours and the write mechanism generated digits in a continuous motion of the write window, hinting at the potential of this method for tracking applications.", "startOffset": 37, "endOffset": 40}, {"referenceID": 2, "context": "Previous work has investigated the application of attention mechanisms for tracking with neural networks [3].", "startOffset": 105, "endOffset": 108}, {"referenceID": 1, "context": "The attention mechanism introduced in [2] extracts glimpses from the input data by applying a set of 2D Gaussian window filters.", "startOffset": 38, "endOffset": 41}, {"referenceID": 1, "context": "In contrast to the original implementation in [2], we use the absolute value function to ensure positivity of \u03b4 and \u03c3.", "startOffset": 46, "endOffset": 49}, {"referenceID": 3, "context": "Piecewise linear activation functions have been shown to benefit optimization [4] and the absolute value function is a convenient trade-off between the harsh zeroing of all negative inputs of the Rectified Linear Unit (ReLU) and the extreme saturation for highly negative inputs of the exponential function.", "startOffset": 78, "endOffset": 81}, {"referenceID": 4, "context": "Advances in the optimization of RNNs enable their successful application in a wide range of tasks [5, 6, 7].", "startOffset": 98, "endOffset": 107}, {"referenceID": 5, "context": "Advances in the optimization of RNNs enable their successful application in a wide range of tasks [5, 6, 7].", "startOffset": 98, "endOffset": 107}, {"referenceID": 6, "context": "Advances in the optimization of RNNs enable their successful application in a wide range of tasks [5, 6, 7].", "startOffset": 98, "endOffset": 107}, {"referenceID": 1, "context": "The original read mechanism from [2] also adds a scalar intensity parameter, that is multiplied to filter responses.", "startOffset": 33, "endOffset": 36}, {"referenceID": 6, "context": "While the application of the more sophisticated long-short-term memory [7] and gated recurrent units [6] is very common in recent work, we rely on a simpler variant of recurrent networks, called IRNN [8], in which Wrec is initialized with a scaled version of the identity matrix and \u03c3 in Equation 5 corresponds to the element-wise ReLU activation function [4]", "startOffset": 71, "endOffset": 74}, {"referenceID": 5, "context": "While the application of the more sophisticated long-short-term memory [7] and gated recurrent units [6] is very common in recent work, we rely on a simpler variant of recurrent networks, called IRNN [8], in which Wrec is initialized with a scaled version of the identity matrix and \u03c3 in Equation 5 corresponds to the element-wise ReLU activation function [4]", "startOffset": 101, "endOffset": 104}, {"referenceID": 7, "context": "While the application of the more sophisticated long-short-term memory [7] and gated recurrent units [6] is very common in recent work, we rely on a simpler variant of recurrent networks, called IRNN [8], in which Wrec is initialized with a scaled version of the identity matrix and \u03c3 in Equation 5 corresponds to the element-wise ReLU activation function [4]", "startOffset": 200, "endOffset": 203}, {"referenceID": 3, "context": "While the application of the more sophisticated long-short-term memory [7] and gated recurrent units [6] is very common in recent work, we rely on a simpler variant of recurrent networks, called IRNN [8], in which Wrec is initialized with a scaled version of the identity matrix and \u03c3 in Equation 5 corresponds to the element-wise ReLU activation function [4]", "startOffset": 356, "endOffset": 359}, {"referenceID": 8, "context": "In our experiments we use the IRNN implementation of [9].", "startOffset": 53, "endOffset": 56}, {"referenceID": 9, "context": "For our initial experiment, we used the bouncing balls data set [10] with 32 frames.", "startOffset": 64, "endOffset": 68}, {"referenceID": 10, "context": "Single-Digit: To better understand the learning ability of RATM, we generated a new data set by moving one digit randomly drawn from MNIST [11].", "startOffset": 139, "endOffset": 143}, {"referenceID": 9, "context": "Thus, we generated new sequences by modifying the bouncing balls script [10].", "startOffset": 72, "endOffset": 76}, {"referenceID": 11, "context": "We experimented with RATM on the Caltech Pedestrian Dataset [12, 13], but the number of long annotated training sequences which contain the same subject is quite low for deep learning standards.", "startOffset": 60, "endOffset": 68}, {"referenceID": 12, "context": "We experimented with RATM on the Caltech Pedestrian Dataset [12, 13], but the number of long annotated training sequences which contain the same subject is quite low for deep learning standards.", "startOffset": 60, "endOffset": 68}, {"referenceID": 13, "context": "The authors would like to thank the developers of Theano [14, 15] and J\u00f6rg Bornschein for helpful discussions.", "startOffset": 57, "endOffset": 65}, {"referenceID": 14, "context": "The authors would like to thank the developers of Theano [14, 15] and J\u00f6rg Bornschein for helpful discussions.", "startOffset": 57, "endOffset": 65}], "year": 2015, "abstractText": "This work presents an attention mechanism-based neural network approach for tracking objects in video. A recurrent neural network is trained to predict the position of an object at time t + 1 given a series of selective glimpses into video frames at time steps 1 to t. The proposed recurrent attentive tracking model can be trained using simple gradient-based training. Various settings are explored in experiments on artificial data to justify design choices.", "creator": "LaTeX with hyperref package"}}}