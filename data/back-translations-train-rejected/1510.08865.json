{"id": "1510.08865", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "29-Oct-2015", "title": "Mixed Robust/Average Submodular Partitioning: Fast Algorithms, Guarantees, and Applications to Parallel Machine Learning and Multi-Label Image Segmentation", "abstract": "We investigate two novel mixed robust/average-case submodular data partitioning problems that we collectively call \\emph{Submodular Partitioning}. These problems generalize purely robust instances of the problem, namely \\emph{max-min submodular fair allocation} (SFA) and \\emph{min-max submodular load balancing} (SLB), and also average-case instances, that is the \\emph{submodular welfare problem} (SWP) and \\emph{submodular multiway partition} (SMP). While the robust versions have been studied in the theory community, existing work has focused on tight approximation guarantees, and the resultant algorithms are not generally scalable to large real-world applications. This is in contrast to the average case, where most of the algorithms are scalable. In the present paper, we bridge this gap, by proposing several new algorithms (including greedy, majorization-minimization, minorization-maximization, and relaxation algorithms) that not only scale to large datasets but that also achieve theoretical approximation guarantees comparable to the state-of-the-art. We moreover provide new scalable algorithms that apply to additive combinations of the robust and average-case objectives. We show that these problems have many applications in machine learning (ML), including data partitioning and load balancing for distributed ML, data clustering, and image segmentation. We empirically demonstrate the efficacy of our algorithms on real-world problems involving data partitioning for distributed optimization (of convex and deep neural network objectives), and also purely unsupervised image segmentation.", "histories": [["v1", "Thu, 29 Oct 2015 20:07:32 GMT  (3652kb,D)", "https://arxiv.org/abs/1510.08865v1", "To appear NIPS 2015"], ["v2", "Tue, 16 Aug 2016 04:00:41 GMT  (4171kb,D)", "http://arxiv.org/abs/1510.08865v2", null]], "COMMENTS": "To appear NIPS 2015", "reviews": [], "SUBJECTS": "cs.DS cs.DM cs.LG", "authors": ["kai wei", "rishabh iyer", "shengjie wang", "wenruo bai", "jeff bilmes"], "accepted": false, "id": "1510.08865"}, "pdf": {"name": "1510.08865.pdf", "metadata": {"source": "CRF", "title": "Mixed Robust/Average Submodular Partitioning: Fast Algorithms, Guarantees, and Applications to Parallel Machine Learning and Multi-Label Image Segmentation", "authors": ["Kai Wei", "Rishabh Iyer", "Shengjie Wang"], "emails": ["kaiwei@uw.edu", "rkiyer@uw.edu", "wangsj@uw.edu", "wrbai@uw.edu", "bilmes@uw.edu"], "sections": [{"heading": null, "text": "ar Xiv: 151 0.Partitioning and load balancing for distributed machine algorithms on parallel machines; 2) data clustering; and 3) multi-label image segmentation with (only) Boolean submodular functions via pixel partitioning. We demonstrate empirically the effectiveness of our algorithms on real problems with data partitioning for distributed optimization of standard targets for machine learning (including convex and deep targets for neural networks) and also on purely unattended (i.e. no supervised or semi-supervised learning and no interactive segmentation) image segmentation. Keywords: submodular optimization, submodular partitioning, data partitioning, parallel computing, greedy algorithm, multi-label image segmentation, data science"}, {"heading": "1. Introduction", "text": "Most cluster problems are based on an optimization that is either an average sum or an average sum of costs. Given a set of V items, an m partition \u03c0 = (A\u03c01, A \u03c0 2,..., A \u03c0 m) is a set of sub-ranges of V (called blocks) that are not intersecting (i.e., A\u03c0i, A\u03c0j =) that is based on an aggregation of judgments about the internal quality of the resulting blocks. The goal of a partitioning is to produce a partitioning that is measurably good in some way, often based on an aggregation of the internal quality of the resulting blocks. In data science and machine learning applications, partitioning is almost always the end result of clustering (although in some cases clustering could allow an intersecting subset) in which V is divided into m clusters (which, in this paper, we refer to block building)."}, {"heading": "1.1 Data Partitioning for Parallel Machine Learning", "text": "Many of today's statistical learning methods can benefit from the vast and unprecedented amount of training data that exists today and that is readily available because, in fact, there is \"no data like more data.\" On the other hand, big data poses significant computational challenges to machine learning because, as big data grows, we are expected to be nearing the end of Moore's Thompson and Parthasarathy law (2006), and single-threaded computing speed has unfortunately not improved significantly since around 2003. Therefore, as the flood of data continues to grow, it is imperative to develop efficient and scalable methods for the formation of large-scale statistical models. One strategy is to develop smarter and more efficient algorithms, and indeed this is eagerly pursued in the machine learning community. Another natural and complementary strategy that is also widely pursued is through parallel and distributed computing. 1. Similar sub-categories have been referred to as \"non-uniform\" cases of vitality and \"flesh in 2008."}, {"heading": "1.2 Multi-label Image Segmentation", "text": "Segmentation of images into different regions is one of the more important problems in computer vision. Historically, the problem has been addressed by finding a MAP solution for a Markov Random Field (MRF) where a distribution p (y, x) = 1Z exp (y, x)) is built. (...) Here, y = (y1, y2,.) is a set of pixel images for a series of pixels V, x) = (x) 1, x (2, x) | V |) is a set of pixel values, and where E (y, x) is an energy division function. For binary image segmentation, yi [0, 1] and for multi-label instances, yi [0, 1], m \u2212 1}, and the image caption maxy p (y]."}, {"heading": "1.3 Sub-categorizations and Related Previous Work", "text": "Problem 1: A further approach of problem 1 has already appeared in the literature. Problem 1 with \u03bb = 0 is called submodular fair allocation (SFA), which has been largely investigated in the heterogeneous constellation. Problem 1 with \u03bb = 0 is called submodular fair allocation (SFA), which has been largely investigated in the heterogeneous constellation. If the problem is applied only approximately to 1 / 2 + for each individual golovin (2005), the well-being of an LP solution that O (1 / (2005) has achieved is not sufficient. Golovin (2005) gives a matching-based algorithm with a factor of 1 / (n \u2212 m + 1) approximation, which works poorly if m n. Khot and Ponnuswami (2007) suggest a binary search for an improved factor of 1 / (2m \u2212 1). Another approach approaches each submodular function by its soid approximation (not)."}, {"heading": "1.4 Our Contributions", "text": "In contrast to problems 1 and 2 in the average case (i.e., \u03bb = 1), existing algorithms for the worst case (\u03bb = 0) are not scalable. This paper closes this gap by proposing three new classes of algorithms to solve SFA and SLB: (1) greedy algorithms; (2) semigradient-based algorithms; and (3) a Lova extension based on relaxation algorithms. (2012) For general m, if we formulate the problem as a non-monotonous submodular maximization that can be approximated to a factor of 1 / 2 with a functionality of O (n). (2012) We give a simple and scalable greedy algorithms (GreedMax), and show a factor of 1 / m in the homogeneous setting that improves the state of the art factor of 1 / 2."}, {"heading": "2. Robust Submodular Partitioning (Problems 1 and 2 when \u03bb = 0)", "text": "Notation: We define f (j | S), f (j | S) \u2212 f (S) as the amplification of j \u0445V in the context of S V. Then f is submodular, if and only if f (j | S) \u2265 f (j | T) for all S T and j / \u0445T. Besides, f is monotonous iff f (j | S) \u2265 0, \u0435j / \u0441S, S V. We assume that the fundamental set V = {1, 2, \u00b7 \u00b7, n}."}, {"heading": "2.1 Approximation Algorithms for SFA (Problem 1 with \u03bb = 0)", "text": "We will first investigate a specific case of SFA with a narrow factor of 1 / 2. If m = 2, then the problem will not be solved. (A), (4), where g (A), (A), (A), (A), (A), (A), (A), (A), (A), (A), (A), (A), (A), (A), (A), (A), (A), (A), (A), (A), (A), (A), (A), (A), (A), (A), (A), (A), (A), (A), (A), (A), (A), (A), (A), (A), (A), (A), (A), (A), (A), (A, (A), (A), (A), (A)."}, {"heading": "2.2 Approximation Algorithms for SLB (Problem 2 with \u03bb = 0)", "text": "In this section, we will examine the problem of submodular load distribution (SLB). It is a specific case of problem 2 with the SLB (SLB). We will first analyze the hardness of the SLB (SLB). We will then show that the SLB (SLB) -based algorithms come with a guarantee that corresponds to the problem hardness. Finally, we will describe a more efficient supergradient-based algorithm analysis (SLB) in their analyses. In most of the applications of the SLB, we find that the parameters m and Fleischer can be treated as a constant w.r.t. nFor this purpose, we offer a more general hardness analysis that directly depends on m.Theorem 6."}, {"heading": "3. General Submodular Partitioning (Problems 1 and 2 when 0 < \u03bb < 1)", "text": "We use the proposed algorithms for the specific cases of problems 1 and 2 as building blocks to design algorithms for the general scenarios (0 < \u03bb < 1). We first propose a simple and generic scheme that provides a performance guarantee in relation to both problems. We then generalize the proposed GreedSat to obtain a practical algorithm for problem 1. For problem 2, we generalize Lova \"sz Round to obtain a relativization of based algorithms."}, {"heading": "4. Experiments", "text": "In this section, we evaluate empirically the algorithms proposed for problems 1 and 2. First, we compare the performance of the various algorithms discussed in this paper using a synthetic dataset. Then, we evaluate some of the scalable algorithms proposed for problems 1 and 2 for large-scale real-world data partitioning applications, including distributed ADMM, distributed neural network training, and finally unattended image segmentation tasks."}, {"heading": "4.1 Experiments on Synthetic Data", "text": "In this section, we separately evaluate four different cases: Problem 1 with \u03bb = 0 (SFA), Problem 2 with \u03bb = 0 (SLB), Problem 1 with 0 < and Problem 2 with 0 < Min < 1. Because some of the algorithms, such as the Ellipsoidal Approximations Goemans et al. (2009) and Lova \u0301 sz relaxation algorithms, we are limited to only 40 instances of data, i.e., we only evaluate the simplicity we apply to the homogeneous setting (fi's are identical). In each case, we test two types of submodular functions: the setup location function and the set-cover function is defined as follows: ffac (A) = v-V-V-A, a, where the similarity between element v and symmetric is."}, {"heading": "4.2 Problem 1 for Distributed Training", "text": "In this section, we will focus on applications of problem 1 to real-wold machine learning problems. In particular, we will examine how a partition obtained by solving problem 1 with specific instances of submodular functions can be solved in the way that the distribution of data for distributed convex optimization is solved. We will evaluate distributed convex optimization on a text categorization task. We will use 20 newsgroup record 4, which consists of 18,774 articles divided almost evenly over 20 classes. The text categorization task is to classify an article into a newsgroup (of twenty) for which it was posted. We will randomly use 2 / 3 of all the data as training and test data. The task is solved as a multi-class classification problem, which we formulate as \"2 regulated logistic regression.\" We will solve this convex optimization problem in a distributed way where the data is distributed."}, {"heading": "4.3 Problem 2 for Unsupervised Image Segmentation", "text": "In our experiments, the image segmentation task is solved as unsupervised clustering of pixels, where the goal is to get a division of pixels so that the majority of pixels in each block either achieve the same goal or optimize."}, {"heading": "5. Conclusions", "text": "In this paper, we looked at two novel mixed robust / average submodular partitioning problems that generalize four well-known problems: submodular fair allocation (SFA), submodular load balancing (SLB), submodular welfare problems (SFA), and submodular reusable partitions (SMP). While the average case problems, i.e. SWP and SMP, allow efficient and narrow algorithms, existing approaches to the worst problems, i.e. SFA and SLB, are generally not scalable. We close this gap by providing several new algorithms that not only scale to large data sets, but also provide comparable theoretical guarantees. In addition, we provide a set of efficient frameworks to solve the general mixed robust / average submodular partition problems. We also show that submodular partitioning is not applicable to a range of machine learning problems, which include load monitoring, optimization, and balancing."}, {"heading": "Acknowledgments", "text": "This material is based on work supported by the National Science Foundation under grant number IIS-1162606, the National Institutes of Health under grant number R01GM103544, and a research award from Google, Microsoft, and Intel. R. Iyer acknowledges the support of a Microsoft Research Ph.D Fellowship. This work has been partially supported by TerraSwarm, one of six STARnet centers, a program sponsored by the Semiconductor Research Corporation, MARCO and DARPA."}, {"heading": "Appendix", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "Proof for Theorem 1", "text": "Theorem If f1 and f2 are monotonous submodularity, min {f1 (A), f2 (V\\ A) is also submodular. To prove the theorem, we show a more general result: Let f and h be submodular, and f \u2212 h is either monotonous (S), then g (S) = min {f (S), h (S)} is also submodular. The theorem follows from this result, since f (S) = f1 (S) and h (S) = f2 (V\\ S) are both submodular, and f (S) \u2212 h (S) \u2212 f1 (S) \u2212 f2 (V\\ S) is monotonous ing.To show that g (S) is submodular, we show that g (S) f (S) f + f (S) + g (S), h (S)."}, {"heading": "Proof for Theorem 2", "text": "Theorem Under the homogeneous constellation (fi = f for all i), GreedMax is guaranteed that there will be a division between the two. (36) Proof that the warranty of 1 / m actually applies to a streaming version of the greedy algorithm (StreamGreed, see Alg. 10). In particular, we show that StreamGreed provides a factor of 1 / m for SFA under the homogeneous constellation. Theorem 2 then follows, since GreedMax can be considered as StreamGreed with a specific order. Algorithm 10: StreamGreedInput: V = {v1, v2,., m, ultimately f."}, {"heading": "Proof for Theorem 3", "text": "Given theorem, \u03b1 and any other 0 < \u03b4 < \u03b1, GreedSat finds a partition in such a way that at least dm (\u03b1 \u2212 \u03b4) e blocks are useful, but at least get a Cmin solution, so that the returned solution yields a Cmax solution that meets a Cmin solution. < \u03b1cmax. The gap between cmax and cmin is limited by, i.e., cmax \u2212 cmin blocks. Next, we prove that there are no Cmax blocks that satisfy a Cmax solution. (A) The Cmax blocks are so large that the Cmax blocks have a good solution. (A) We prove that there are no Cmax blocks that satisfy the Cmax blocks. (A), the Cmax blocks satisfy the Cmax blocks. (A), the Cmax blocks."}, {"heading": "Proof for Theorem 4", "text": "Theorem MMax achieves a worst-case guarantee of O (mini 1 + (| hi-fi) i-fi-i-fi-i-fi-i-fi-i-fi-i-fi-i-fi-i-i-i-i-fi-i-fi-i-fi-i-fi-i-i-fi-i-i-i-i-fi-i-i-fi-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-"}, {"heading": "Proof for Theorem 5", "text": "Theorem Suppose there is an algorithm for solving the modular version of SFA with an approximation factor \u03b1 \u2264 1, then we have the following: min i fi (A \u03c0t i) \u2265 \u03b1mini fi (A \u03c0t \u2212 1 i). (57) Proof Consider the following: min i fi (A \u03c0t \u2212 1 i) = mini hi (A \u03c0t \u2212 1 i) / / tightness of the modular lower limit. (58) \u2264 \u03b1min i hi (A \u03c0t \u2212 i) / / approximation factor of the modular SFA. (59) \u2264 \u03b1hj (A\u03c0tj) / / j \u0432argmin i fi (A \u03c0t i) (60) \u2264 \u03b1fj (A\u03c0tj) / / hj (A \u03c0t \u2212 1 j) upper limits fj everywhere. (61) = \u03b1min i fi (A\u03c0t i) (62)"}, {"heading": "Proof for Theorem 6", "text": "Theorem for each > 0, SLB cannot be based on a factor of (1 \u2212) m \u03b2 = (1 > LB \u03b2 =) m = o (\u221a n / log n) with a polynomial number of queries even under the homogeneous adjustment. Proof We use the same evidence techniques as in Svitkina and Fleischer (2008). Consider two submodular functions: f1 (S) = min {S |, \u03b1}; (63) f2 (S) = min {m \u2211 i = 1 min {\u03b2, Vi}, \u03b1}; (64) where {Vi} mi = 1 is a uniformly random division of V into m blocks, \u03b1 = nm and \u03b2 = n m2 (1 \u2212). To be more precise about the uniform random division, we assign each element to one of the m blocks with the probability of 1 / m. It is easy to verify that OPT1 = mindness = maxi f1 (A) = n / m and Tfxi = 2 (2)."}, {"heading": "Proof for Theorem 7", "text": "Theorem Lova \u0301 szRound is guaranteed to find a partition that limits the loss of performance in the step of rounding the fracture solution {x-i-mi = 1 or equivalent as follows: max i-f-i (x-i) \u2265 1m max i-fi (Ai), (67) where {Ai-mi = 1 is the resulting division after rounding. To show Eqn 67, it is sufficient to show that f-i (x-i) \u2265 1mfi (Ai) for all i = 1,..... Next we consider the following: fi (Ai) = f-i (1Ai) = mf-i (1m 1Ai) / / positive homogeneity of the Lova-sz extension (68) For each element vj-Ai we have x-moni (j-i) = 1-oti (1-i)."}, {"heading": "Proof for Theorem 8", "text": "Theorem MMin achieves a worst-case guarantee of (2 maxi | A\u03c0 1, \u00b7 \u00b7, A\u03c0 1 + (| A\u03c0 i | \u2212 1) (1 \u2212 \u03bafi (A \u2264 i))), where\u03c0 1 = (A\u03c0 1, \u00b7 \u00b7, A\u03c0 1) denotes the optimal partition. Proof that \u03b1 = 2 is the approximation factor of the algorithm for solving the modular version of problem 2 Lenstra et al. (1990). To simplify the notation, we write \u03c0 = (A \u04321,.,., A \u0441m) as the resulting partition after the first iteration of MMin, and \u03c0 = (A \u0445 1,.,., A \u0445 m) as the optimal solution. Again, the first iteration is sufficient to obtain the performance guarantee, and the following iterations are designed to improve empirical performance. Since the supergradients for each function used for the first iteration are the simple form of headfi-Hi:"}, {"heading": "Proof for Theorem 9", "text": "Theorem Suppose there is an algorithm for solving the modular version of SLB with an approximation factor \u03b1 \u2265 1, so for each iteration t we have that max i fi (A \u03c0t i) \u2264 \u03b1maxi fi (A \u03c0t \u2212 1 i). (74) Proof The proof is symmetrical to that for theorem 5."}, {"heading": "Proof for Theorem 10", "text": "(1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1)) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1 (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1 (1) (1) (1 (1) (1) (1) (1) (1) (1) (1) (1 (1) (1 (1) (1) (1) (1 (1) (1 (1) (1) (1) (1 (1) (1) (1) (1) (1 (1 (1) (1) (1) (1) (1 (1) (1) (1 (1) (1) (1"}, {"heading": "Proof for Theorem 11", "text": "Theorem Given, Proof Denote Intermediate F Given, Proof Denote Intermediate F Given, Proof Denote Given, and, 0 \u2264 Given, given, given, given, given, given, given, given, given, given, given, given, given, given, given, given, given, given, given, given, given, given, given, given, given, given, given, given, given, given, given, given, given, given, given, given, given, given, given, given, given, given, given, given, given, given, given, given, given, given, given, given, given, given, given, given, given, given, given, given, given, given, given, given, given, given, given, given, given, given, given, given, given, given, given, given, given, given, given, given, given,"}, {"heading": "Proof for Theorem 12", "text": "Theorem Define F \u03bb (zipated) = zipp = zipp = zipp = zipp = zipp = zipp = zipp = psp = psp = psp = psp = psp = psp = psp = psp = psp = psp = psp = psp = psp = psp = psp = psp = psp = psp = psp = psp = psp = psp = psp = psp sp = psp = psp sp sp = psp psp = psp = psp psp = psp psp = psp = psp = psp = psp = psp = psp = psp = psp psp = psp = psp = sp = sp = sp = sp = sp = sp = sp = sp = sp = psp = sp = sp = sp = sp = psp = psp = sp = sp = sp = psp = psp = psp = psp = psp = psp = psp = psp = psp = psp = psp = psp = psp = psp = psp = psp = psp = psp = psp = psp = psp = psp = psp = psp = psp = psp = psp = psp = psp = psp = psp = psp = psp = psp = psp = psp = psp = psp = psp = psp = psp = psp = psp = psp = psp = psp = psp = psp = psp = psp = psp = psp = psp = psp = psp = psp = psp = psp = psp = psp = psp = psp = psp = psp = psp = psp = psp = psp = psp = psp = psp = psp = psp = psp = psp = psp = psp = psp = psp = psp = psp = psp = psp = psp = psp = psp = psp = psp = psp = psp = psp = psp = psp = psp = psp = psp = psp = psp = psp = psp = psp = psp"}], "references": [{"title": "Streaming min-max hypergraph partitioning", "author": ["Dan Alistarh", "Jennifer Iglesias", "Milan Vojnovic"], "venue": "Advances in Neural Information Processing Systems", "citeRegEx": "Alistarh et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Alistarh et al\\.", "year": 2015}, {"title": "k-means++: The advantages of careful seeding", "author": ["David Arthur", "Sergei Vassilvitskii"], "venue": "In SODA,", "citeRegEx": "Arthur and Vassilvitskii.,? \\Q2007\\E", "shortCiteRegEx": "Arthur and Vassilvitskii.", "year": 2007}, {"title": "An approximation algorithm for max-min fair allocation of indivisible goods", "author": ["Arash Asadpour", "Amin Saberi"], "venue": "In SICOMP,", "citeRegEx": "Asadpour and Saberi.,? \\Q2010\\E", "shortCiteRegEx": "Asadpour and Saberi.", "year": 2010}, {"title": "Matlab wrapper for graph cut", "author": ["Shai Bagon"], "venue": "Foundations and Trends in Machine Learning,", "citeRegEx": "Bagon.,? \\Q2006\\E", "shortCiteRegEx": "Bagon.", "year": 2006}, {"title": "Handbook of Mathematical Models in Computer Vision, chapter Graph Cuts in Vision and Graphics: Theories and Applications", "author": ["Y. Boykov", "O. Veksler"], "venue": null, "citeRegEx": "Boykov and Veksler.,? \\Q2006\\E", "shortCiteRegEx": "Boykov and Veksler.", "year": 2006}, {"title": "An experimental comparison of min-cut/max-flow algorithms for energy minimization in vision", "author": ["Yuri Boykov", "Vladimir Kolmogorov"], "venue": "IEEE transactions on Pattern Analysis and Machine Intelligence,", "citeRegEx": "Boykov and Kolmogorov.,? \\Q2004\\E", "shortCiteRegEx": "Boykov and Kolmogorov.", "year": 2004}, {"title": "Efficient approximate energy minimization via graph cuts", "author": ["Yuri Boykov", "Olga Veksler", "Ramin Zabih"], "venue": "IEEE transactions on Pattern Analysis and Machine Intelligence,", "citeRegEx": "Boykov et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Boykov et al\\.", "year": 2001}, {"title": "Partitions of graphs into one or two independent sets and cliques", "author": ["Andreas Brandst\u00e4dt"], "venue": "Discrete Mathematics,", "citeRegEx": "Brandst\u00e4dt.,? \\Q1996\\E", "shortCiteRegEx": "Brandst\u00e4dt.", "year": 1996}, {"title": "A tight linear time (1/2)-approximation for unconstrained submodular maximization", "author": ["Niv Buchbinder", "Moran Feldman", "Joseph Naor", "Roy Schwartz"], "venue": "In FOCS,", "citeRegEx": "Buchbinder et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Buchbinder et al\\.", "year": 2012}, {"title": "Approximation algorithms for submodular multiway partition", "author": ["Chandra Chekuri", "Alina Ene"], "venue": "In FOCS,", "citeRegEx": "Chekuri and Ene.,? \\Q2011\\E", "shortCiteRegEx": "Chekuri and Ene.", "year": 2011}, {"title": "Submodular cost allocation problem and applications", "author": ["Chandra Chekuri", "Alina Ene"], "venue": "In Automata, Languages and Programming,", "citeRegEx": "Chekuri and Ene.,? \\Q2011\\E", "shortCiteRegEx": "Chekuri and Ene.", "year": 2011}, {"title": "Submodular meets spectral: Greedy algorithms for subset selection, sparse approximation and dictionary selection", "author": ["A. Das", "D. Kempe"], "venue": "arXiv preprint arXiv:1102.3975,", "citeRegEx": "Das and Kempe.,? \\Q2011\\E", "shortCiteRegEx": "Das and Kempe.", "year": 2011}, {"title": "A deterministic algorithm for maximizing submodular functions", "author": ["Shahar Dobzinski", "Ami Mor"], "venue": "arXiv preprint arXiv:1507.07237,", "citeRegEx": "Dobzinski and Mor.,? \\Q2015\\E", "shortCiteRegEx": "Dobzinski and Mor.", "year": 2015}, {"title": "Local distribution and the symmetry gap: Approximability of multiway partitioning problems", "author": ["Alina Ene", "Jan Vondr\u00e1k", "Yi Wu"], "venue": "In SODA,", "citeRegEx": "Ene et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Ene et al\\.", "year": 2013}, {"title": "Complexity of graph partition problems", "author": ["Tomas Feder", "Pavol Hell", "Sulamita Klein", "Rajeev Motwani"], "venue": "In Proceedings of the thirty-first annual ACM symposium on Theory of computing,", "citeRegEx": "Feder et al\\.,? \\Q1999\\E", "shortCiteRegEx": "Feder et al\\.", "year": 1999}, {"title": "Maximizing non-monotone submodular functions", "author": ["Uriel Feige", "Vahab Mirrokni", "Jan Vondr\u00e1k"], "venue": "SIAM J. COMPUT.,", "citeRegEx": "Feige et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Feige et al\\.", "year": 2011}, {"title": "An analysis of approximations for maximizing submodular set functions\u2014II", "author": ["M.L. Fisher", "G.L. Nemhauser", "L.A. Wolsey"], "venue": "In Polyhedral combinatorics,", "citeRegEx": "Fisher et al\\.,? \\Q1978\\E", "shortCiteRegEx": "Fisher et al\\.", "year": 1978}, {"title": "Submodular functions and optimization, volume 58", "author": ["Satoru Fujishige"], "venue": null, "citeRegEx": "Fujishige.,? \\Q2005\\E", "shortCiteRegEx": "Fujishige.", "year": 2005}, {"title": "A review of robust clustering methods", "author": ["L.A. Gar\u0107\u0131a-Escudero", "A. Gordaliza", "C. Matr\u00e1n", "A. Mayo-Iscar"], "venue": "Advances in Data Analysis and Classification,", "citeRegEx": "Gar\u0107\u0131a.Escudero et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Gar\u0107\u0131a.Escudero et al\\.", "year": 2010}, {"title": "Approximating submodular functions everywhere", "author": ["Michel Goemans", "Nicholas Harvey", "Satoru Iwata", "Vahab Mirrokni"], "venue": "In SODA,", "citeRegEx": "Goemans et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Goemans et al\\.", "year": 2009}, {"title": "Improved approximation algorithms for maximum cut and satisfiability problems using semidefinite programming", "author": ["Michel X Goemans", "David P Williamson"], "venue": "Journal of the ACM (JACM),", "citeRegEx": "Goemans and Williamson.,? \\Q1995\\E", "shortCiteRegEx": "Goemans and Williamson.", "year": 1995}, {"title": "Max-min fair allocation of indivisible goods", "author": ["Daniel Golovin"], "venue": "Technical Report CMU-CS-05144,", "citeRegEx": "Golovin.,? \\Q2005\\E", "shortCiteRegEx": "Golovin.", "year": 2005}, {"title": "Partitioning chordal graphs into independent sets and cliques", "author": ["Pavol Hell", "Sulamita Klein", "Loana Tito Nogueira", "F\u00e1bio Protti"], "venue": "Discrete Applied Mathematics,", "citeRegEx": "Hell et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Hell et al\\.", "year": 2004}, {"title": "Efficient visual exploration and coverage with a micro aerial vehicle in unknown environments", "author": ["L. Heng", "A. Gotovos", "A. Krause", "M. Pollefeys"], "venue": "In IEEE Int. Conf. on Robotics and Automation (ICRA),", "citeRegEx": "Heng et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Heng et al\\.", "year": 2015}, {"title": "A polynomial approximation scheme for scheduling on uniform processors: Using the dual approximation approach", "author": ["Dorit S Hochbaum", "David B Shmoys"], "venue": "SICOMP,", "citeRegEx": "Hochbaum and Shmoys.,? \\Q1988\\E", "shortCiteRegEx": "Hochbaum and Shmoys.", "year": 1988}, {"title": "The submodular Bregman and Lov\u00e1sz-Bregman divergences with applications", "author": ["R. Iyer", "J. Bilmes"], "venue": "In NIPS,", "citeRegEx": "Iyer and Bilmes.,? \\Q2012\\E", "shortCiteRegEx": "Iyer and Bilmes.", "year": 2012}, {"title": "Algorithms for approximate minimization of the difference between submodular functions, with applications", "author": ["R. Iyer", "J. Bilmes"], "venue": "In Uncertainty in Artificial Intelligence (UAI),", "citeRegEx": "Iyer and Bilmes.,? \\Q2012\\E", "shortCiteRegEx": "Iyer and Bilmes.", "year": 2012}, {"title": "Submodular Optimization with Submodular Cover and Submodular Knapsack Constraints", "author": ["R. Iyer", "J. Bilmes"], "venue": "In NIPS,", "citeRegEx": "Iyer and Bilmes.,? \\Q2013\\E", "shortCiteRegEx": "Iyer and Bilmes.", "year": 2013}, {"title": "Fast semidifferential based submodular function optimization", "author": ["R. Iyer", "S. Jegelka", "J. Bilmes"], "venue": "In ICML,", "citeRegEx": "Iyer et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Iyer et al\\.", "year": 2013}, {"title": "Curvature and Efficient Approximation Algorithms for Approximation and Minimization of Submodular Functions", "author": ["Rishabh Iyer", "Stefanie Jegelka", "Jeff Bilmes"], "venue": null, "citeRegEx": "Iyer et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Iyer et al\\.", "year": 2013}, {"title": "Monotone closure of relaxed constraints in submodular optimization: Connections between minimization and maximization", "author": ["Rishabh Iyer", "Stefanie Jegelka", "Jeff Bilmes"], "venue": "In Uncertainty in Artificial Intelligence (UAI),", "citeRegEx": "Iyer et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Iyer et al\\.", "year": 2014}, {"title": "Submodularity beyond submodular energies: coupling edges in graph cuts", "author": ["S. Jegelka", "J. Bilmes"], "venue": "In CVPR,", "citeRegEx": "Jegelka and Bilmes.,? \\Q2011\\E", "shortCiteRegEx": "Jegelka and Bilmes.", "year": 2011}, {"title": "Approximation algorithms for the max-min allocation problem", "author": ["Subhash Khot", "Ashok Ponnuswami"], "venue": "In APPROX,", "citeRegEx": "Khot and Ponnuswami.,? \\Q2007\\E", "shortCiteRegEx": "Khot and Ponnuswami.", "year": 2007}, {"title": "A principled deep random field for image segmentation", "author": ["P. Kohli", "A. Osokin", "S. Jegelka"], "venue": "In IEEE Conf. on Computer Vision and Pattern Recognition (CVPR),", "citeRegEx": "Kohli et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Kohli et al\\.", "year": 2013}, {"title": "Near-optimal sensor placements in Gaussian processes: Theory, efficient algorithms and empirical studies", "author": ["A. Krause", "A. Singh", "C. Guestrin"], "venue": "In JMLR,", "citeRegEx": "Krause et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Krause et al\\.", "year": 2008}, {"title": "Robust submodular observation selection", "author": ["Andreas Krause", "Brendan McMahan", "Carlos Guestrin", "Anupam Gupta"], "venue": "In JMLR,", "citeRegEx": "Krause et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Krause et al\\.", "year": 2008}, {"title": "Approximation algorithms for scheduling unrelated parallel machines", "author": ["Jan Karel Lenstra", "David B Shmoys", "\u00c9va Tardos"], "venue": "In Mathematical programming,", "citeRegEx": "Lenstra et al\\.,? \\Q1990\\E", "shortCiteRegEx": "Lenstra et al\\.", "year": 1990}, {"title": "Graph partitioning via parallel submodular approximation to accelerate distributed machine learning", "author": ["Mu Li", "Dave Andersen", "Alexander Smola"], "venue": "In arXiv preprint arXiv:1505.04636,", "citeRegEx": "Li et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Li et al\\.", "year": 2015}, {"title": "Least squares quantization in pcm", "author": ["S. Lloyd"], "venue": "IEEE Transactions on IT,", "citeRegEx": "Lloyd.,? \\Q1982\\E", "shortCiteRegEx": "Lloyd.", "year": 1982}, {"title": "Accelerated greedy algorithms for maximizing submodular set functions", "author": ["M. Minoux"], "venue": "In Optimization Techniques,", "citeRegEx": "Minoux.,? \\Q1978\\E", "shortCiteRegEx": "Minoux.", "year": 1978}, {"title": "Minimum average cost clustering", "author": ["Kiyohito Nagano", "Yoshinobu Kawahara", "Satoru Iwata"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Nagano et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Nagano et al\\.", "year": 2010}, {"title": "A faster strongly polynomial time algorithm for submodular function minimization", "author": ["J.B. Orlin"], "venue": "Mathematical Programming,", "citeRegEx": "Orlin.,? \\Q2009\\E", "shortCiteRegEx": "Orlin.", "year": 2009}, {"title": "Parallel training of deep neural networks with natural gradient and parameter averaging", "author": ["Daniel Povey", "Xiaohui Zhang", "Sanjeev Khudanpur"], "venue": "arXiv preprint arXiv:1410.7455,", "citeRegEx": "Povey et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Povey et al\\.", "year": 2014}, {"title": "Combinatorial optimization: polyhedra and efficiency, volume 24", "author": ["Alexander Schrijver"], "venue": null, "citeRegEx": "Schrijver.,? \\Q2003\\E", "shortCiteRegEx": "Schrijver.", "year": 2003}, {"title": "Communal cuts: Sharing cuts across images", "author": ["E. Shelhamer", "S. Jegelka", "T. Darrell"], "venue": "In NIPS workshop on Discrete Optimization in Machine Learning,", "citeRegEx": "Shelhamer et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Shelhamer et al\\.", "year": 2014}, {"title": "A contour completion model for augmenting surface reconstructions", "author": ["Nathan Silberman", "Lior Shapira", "Ran Gal", "Pushmeet Kohli"], "venue": "In Europ. Conf. on Computer Vision (ECCV),", "citeRegEx": "Silberman et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Silberman et al\\.", "year": 2014}, {"title": "Anticlustering: maximizing the variance criterion", "author": ["H Sp\u00e4th"], "venue": "Control and Cybernetics,", "citeRegEx": "Sp\u00e4th.,? \\Q1986\\E", "shortCiteRegEx": "Sp\u00e4th.", "year": 1986}, {"title": "Efficient minimization of decomposable submodular functions", "author": ["P. Stobbe", "A. Krause"], "venue": "In NIPS,", "citeRegEx": "Stobbe and Krause.,? \\Q2010\\E", "shortCiteRegEx": "Stobbe and Krause.", "year": 2010}, {"title": "Submodular approximation: Sampling-based algorithms and lower bounds", "author": ["Z. Svitkina", "L. Fleischer"], "venue": "In FOCS,", "citeRegEx": "Svitkina and Fleischer.,? \\Q2008\\E", "shortCiteRegEx": "Svitkina and Fleischer.", "year": 2008}, {"title": "Submodular approximation: Sampling-based algorithms and lower bounds", "author": ["Zoya Svitkina", "Lisa Fleischer"], "venue": "SIAM Journal on Computing,", "citeRegEx": "Svitkina and Fleischer.,? \\Q2011\\E", "shortCiteRegEx": "Svitkina and Fleischer.", "year": 2011}, {"title": "Superdifferential cuts for binary energies", "author": ["T. Taniai", "Y. Matsushita", "T. Naemura"], "venue": "In IEEE Conf. on Computer Vision and Pattern Recognition (CVPR),", "citeRegEx": "Taniai et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Taniai et al\\.", "year": 2015}, {"title": "Moore\u2019s law: the future of si microelectronics", "author": ["Scott E Thompson", "Srivatsan Parthasarathy"], "venue": "materials today,", "citeRegEx": "Thompson and Parthasarathy.,? \\Q2006\\E", "shortCiteRegEx": "Thompson and Parthasarathy.", "year": 2006}, {"title": "Learning mixtures of submodular functions for image collection summarization", "author": ["Sebastian Tschiatschek", "Rishabh K Iyer", "Haochen Wei", "Jeff A Bilmes"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Tschiatschek et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Tschiatschek et al\\.", "year": 2014}, {"title": "Set partition principles", "author": ["Venceslav Valev"], "venue": "In Transactions of the Ninth Prague Conference on Information Theory, Statistical Decision Functions, and Random Processes,(Prague,", "citeRegEx": "Valev.,? \\Q1982\\E", "shortCiteRegEx": "Valev.", "year": 1982}, {"title": "Set partition principles revisited", "author": ["Ventzeslav Valev"], "venue": null, "citeRegEx": "Valev.,? \\Q1983\\E", "shortCiteRegEx": "Valev.", "year": 1983}, {"title": "A tutorial on spectral clustering", "author": ["Ulrike Von Luxburg"], "venue": "Statistics and computing,", "citeRegEx": "Luxburg.,? \\Q2007\\E", "shortCiteRegEx": "Luxburg.", "year": 2007}, {"title": "Optimal approximation for the submodular welfare problem in the value oracle model", "author": ["Jan Vondr\u00e1k"], "venue": "In STOC,", "citeRegEx": "Vondr\u00e1k.,? \\Q2008\\E", "shortCiteRegEx": "Vondr\u00e1k.", "year": 2008}, {"title": "Using document summarization techniques for speech data subset selection", "author": ["Kai Wei", "Yuzong Liu", "Katrin Kirchhoff", "Jeff Bilmes"], "venue": "In North American Chapter of the Association for Computational Linguistics/Human Language Technology Conference", "citeRegEx": "Wei et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Wei et al\\.", "year": 2013}, {"title": "Fast multi-stage submodular maximization", "author": ["Kai Wei", "Rishabh Iyer", "Jeff Bilmes"], "venue": "In Proceedings of the 31st International Conference on Machine Learning", "citeRegEx": "Wei et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Wei et al\\.", "year": 2014}, {"title": "Submodular subset selection for large-scale speech training data", "author": ["Kai Wei", "Yuzong Liu", "Katrin Kirchhoff", "Chris Bartels", "Jeff Bilmes"], "venue": "In Proc. IEEE Intl. Conf. on Acoustics, Speech, and Signal Processing, Florence,", "citeRegEx": "Wei et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Wei et al\\.", "year": 2014}, {"title": "Unsupervised submodular subset selection for speech data", "author": ["Kai Wei", "Yuzong Liu", "Katrin Kirchhoff", "Jeff Bilmes"], "venue": "In IEEE International Conference on Acoustics, Speech, and Signal Processing (ICASSP), Florence,", "citeRegEx": "Wei et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Wei et al\\.", "year": 2014}, {"title": "Submodularity in data subset selection and active learning", "author": ["Kai Wei", "Rishabh Iyer", "Jeff Bilmes"], "venue": "In ICML,", "citeRegEx": "Wei et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Wei et al\\.", "year": 2015}, {"title": "On generalized greedy splitting algorithms for multiway partition problems", "author": ["Liang Zhao", "Hiroshi Nagamochi", "Toshihide Ibaraki"], "venue": "Discrete applied mathematics,", "citeRegEx": "Zhao et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Zhao et al\\.", "year": 2004}, {"title": "Submodular attribute selection for action recognition in video", "author": ["Jingjing Zheng", "Zhuolin Jiang", "Rama Chellappa", "Jonathon P Phillips"], "venue": "In NIPS,", "citeRegEx": "Zheng et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Zheng et al\\.", "year": 2014}], "referenceMentions": [{"referenceID": 18, "context": "These problems generalize both purely robust instances of the problem (namely max-min submodular fair allocation (SFA) Golovin (2005) and min-max submodular load balancing (SLB) Svitkina and Fleischer (2008)) and also generalize average-case instances (that is the submodular welfare problem (SWP) Vondr\u00e1k (2008) and submodular multiway partition (SMP) Chekuri and Ene (2011a)).", "startOffset": 119, "endOffset": 134}, {"referenceID": 18, "context": "These problems generalize both purely robust instances of the problem (namely max-min submodular fair allocation (SFA) Golovin (2005) and min-max submodular load balancing (SLB) Svitkina and Fleischer (2008)) and also generalize average-case instances (that is the submodular welfare problem (SWP) Vondr\u00e1k (2008) and submodular multiway partition (SMP) Chekuri and Ene (2011a)).", "startOffset": 119, "endOffset": 208}, {"referenceID": 18, "context": "These problems generalize both purely robust instances of the problem (namely max-min submodular fair allocation (SFA) Golovin (2005) and min-max submodular load balancing (SLB) Svitkina and Fleischer (2008)) and also generalize average-case instances (that is the submodular welfare problem (SWP) Vondr\u00e1k (2008) and submodular multiway partition (SMP) Chekuri and Ene (2011a)).", "startOffset": 119, "endOffset": 313}, {"referenceID": 9, "context": "These problems generalize both purely robust instances of the problem (namely max-min submodular fair allocation (SFA) Golovin (2005) and min-max submodular load balancing (SLB) Svitkina and Fleischer (2008)) and also generalize average-case instances (that is the submodular welfare problem (SWP) Vondr\u00e1k (2008) and submodular multiway partition (SMP) Chekuri and Ene (2011a)).", "startOffset": 353, "endOffset": 377}, {"referenceID": 9, "context": "These problems generalize both purely robust instances of the problem (namely max-min submodular fair allocation (SFA) Golovin (2005) and min-max submodular load balancing (SLB) Svitkina and Fleischer (2008)) and also generalize average-case instances (that is the submodular welfare problem (SWP) Vondr\u00e1k (2008) and submodular multiway partition (SMP) Chekuri and Ene (2011a)). While the robust versions have been studied in the theory community Goemans et al. (2009); Golovin (2005); Khot and Ponnuswami (2007); Svitkina and Fleischer (2008); Vondr\u00e1k (2008), existing work has focused on tight approximation guarantees, and the resultant algorithms are not, in general, scalable to very large real-world applications.", "startOffset": 353, "endOffset": 469}, {"referenceID": 9, "context": "These problems generalize both purely robust instances of the problem (namely max-min submodular fair allocation (SFA) Golovin (2005) and min-max submodular load balancing (SLB) Svitkina and Fleischer (2008)) and also generalize average-case instances (that is the submodular welfare problem (SWP) Vondr\u00e1k (2008) and submodular multiway partition (SMP) Chekuri and Ene (2011a)). While the robust versions have been studied in the theory community Goemans et al. (2009); Golovin (2005); Khot and Ponnuswami (2007); Svitkina and Fleischer (2008); Vondr\u00e1k (2008), existing work has focused on tight approximation guarantees, and the resultant algorithms are not, in general, scalable to very large real-world applications.", "startOffset": 353, "endOffset": 485}, {"referenceID": 9, "context": "These problems generalize both purely robust instances of the problem (namely max-min submodular fair allocation (SFA) Golovin (2005) and min-max submodular load balancing (SLB) Svitkina and Fleischer (2008)) and also generalize average-case instances (that is the submodular welfare problem (SWP) Vondr\u00e1k (2008) and submodular multiway partition (SMP) Chekuri and Ene (2011a)). While the robust versions have been studied in the theory community Goemans et al. (2009); Golovin (2005); Khot and Ponnuswami (2007); Svitkina and Fleischer (2008); Vondr\u00e1k (2008), existing work has focused on tight approximation guarantees, and the resultant algorithms are not, in general, scalable to very large real-world applications.", "startOffset": 353, "endOffset": 513}, {"referenceID": 9, "context": "These problems generalize both purely robust instances of the problem (namely max-min submodular fair allocation (SFA) Golovin (2005) and min-max submodular load balancing (SLB) Svitkina and Fleischer (2008)) and also generalize average-case instances (that is the submodular welfare problem (SWP) Vondr\u00e1k (2008) and submodular multiway partition (SMP) Chekuri and Ene (2011a)). While the robust versions have been studied in the theory community Goemans et al. (2009); Golovin (2005); Khot and Ponnuswami (2007); Svitkina and Fleischer (2008); Vondr\u00e1k (2008), existing work has focused on tight approximation guarantees, and the resultant algorithms are not, in general, scalable to very large real-world applications.", "startOffset": 353, "endOffset": 544}, {"referenceID": 9, "context": "These problems generalize both purely robust instances of the problem (namely max-min submodular fair allocation (SFA) Golovin (2005) and min-max submodular load balancing (SLB) Svitkina and Fleischer (2008)) and also generalize average-case instances (that is the submodular welfare problem (SWP) Vondr\u00e1k (2008) and submodular multiway partition (SMP) Chekuri and Ene (2011a)). While the robust versions have been studied in the theory community Goemans et al. (2009); Golovin (2005); Khot and Ponnuswami (2007); Svitkina and Fleischer (2008); Vondr\u00e1k (2008), existing work has focused on tight approximation guarantees, and the resultant algorithms are not, in general, scalable to very large real-world applications.", "startOffset": 353, "endOffset": 560}, {"referenceID": 33, "context": "Most clustering problems are based on optimizing an objective that is either a sum, or an average-case, utility where the goal is to optimize the sum of individual cluster costs \u2014 this includes the ubiquitous k-means procedure Lloyd (1982); Arthur and Vassilvitskii (2007) where the goal is to construct a partitioning based on choosing a set of cluster centers that minimizes the total sum of squared distances between each point and its closest center.", "startOffset": 227, "endOffset": 240}, {"referenceID": 1, "context": "Most clustering problems are based on optimizing an objective that is either a sum, or an average-case, utility where the goal is to optimize the sum of individual cluster costs \u2014 this includes the ubiquitous k-means procedure Lloyd (1982); Arthur and Vassilvitskii (2007) where the goal is to construct a partitioning based on choosing a set of cluster centers that minimizes the total sum of squared distances between each point and its closest center.", "startOffset": 241, "endOffset": 273}, {"referenceID": 1, "context": "Most clustering problems are based on optimizing an objective that is either a sum, or an average-case, utility where the goal is to optimize the sum of individual cluster costs \u2014 this includes the ubiquitous k-means procedure Lloyd (1982); Arthur and Vassilvitskii (2007) where the goal is to construct a partitioning based on choosing a set of cluster centers that minimizes the total sum of squared distances between each point and its closest center. More rarely, clustering algorithms may be based on robust objective functions Gar\u0107\u0131a-Escudero et al. (2010), where the goal is to optimize the worst-case internal cluster cost (i.", "startOffset": 241, "endOffset": 563}, {"referenceID": 1, "context": "Most clustering problems are based on optimizing an objective that is either a sum, or an average-case, utility where the goal is to optimize the sum of individual cluster costs \u2014 this includes the ubiquitous k-means procedure Lloyd (1982); Arthur and Vassilvitskii (2007) where the goal is to construct a partitioning based on choosing a set of cluster centers that minimizes the total sum of squared distances between each point and its closest center. More rarely, clustering algorithms may be based on robust objective functions Gar\u0107\u0131a-Escudero et al. (2010), where the goal is to optimize the worst-case internal cluster cost (i.e., quality of the clustering is based solely on the quality of the worst internal cluster cost within the clustering). The average case vs. worst case cluster cost assessment distinction are extremes along a continuum, although all existing algorithms operate only at, rather than in between, these extremes. There is another way of categorizing clustering algorithms, and that based on the goal of each resultant cluster. Most of the time, clustering algorithms attempt to produce clusters containing items similar to or near each other (e.g., with k-means, a cluster consists of a centroid and a set of nearby points), and dissimilarity exists, ideally, only between different clusters. An alternate possible goal of clustering is to have each block contains as diverse a set of items as possible, where similarity exist between rather than within clusters. For example, partitioning the vertices of a graph into a set of k independent (or, equivalently, stable) sets would fall into this later category, assuming the graph has edges only between similar vertices. This subsumes graph k-colorability problems, one of the most well-known of the NP-complete problems, although in some special cases it is solvable in polynomial time Brandst\u00e4dt (1996); Feder et al.", "startOffset": 241, "endOffset": 1886}, {"referenceID": 1, "context": "Most clustering problems are based on optimizing an objective that is either a sum, or an average-case, utility where the goal is to optimize the sum of individual cluster costs \u2014 this includes the ubiquitous k-means procedure Lloyd (1982); Arthur and Vassilvitskii (2007) where the goal is to construct a partitioning based on choosing a set of cluster centers that minimizes the total sum of squared distances between each point and its closest center. More rarely, clustering algorithms may be based on robust objective functions Gar\u0107\u0131a-Escudero et al. (2010), where the goal is to optimize the worst-case internal cluster cost (i.e., quality of the clustering is based solely on the quality of the worst internal cluster cost within the clustering). The average case vs. worst case cluster cost assessment distinction are extremes along a continuum, although all existing algorithms operate only at, rather than in between, these extremes. There is another way of categorizing clustering algorithms, and that based on the goal of each resultant cluster. Most of the time, clustering algorithms attempt to produce clusters containing items similar to or near each other (e.g., with k-means, a cluster consists of a centroid and a set of nearby points), and dissimilarity exists, ideally, only between different clusters. An alternate possible goal of clustering is to have each block contains as diverse a set of items as possible, where similarity exist between rather than within clusters. For example, partitioning the vertices of a graph into a set of k independent (or, equivalently, stable) sets would fall into this later category, assuming the graph has edges only between similar vertices. This subsumes graph k-colorability problems, one of the most well-known of the NP-complete problems, although in some special cases it is solvable in polynomial time Brandst\u00e4dt (1996); Feder et al. (1999); Hell et al.", "startOffset": 241, "endOffset": 1907}, {"referenceID": 1, "context": "Most clustering problems are based on optimizing an objective that is either a sum, or an average-case, utility where the goal is to optimize the sum of individual cluster costs \u2014 this includes the ubiquitous k-means procedure Lloyd (1982); Arthur and Vassilvitskii (2007) where the goal is to construct a partitioning based on choosing a set of cluster centers that minimizes the total sum of squared distances between each point and its closest center. More rarely, clustering algorithms may be based on robust objective functions Gar\u0107\u0131a-Escudero et al. (2010), where the goal is to optimize the worst-case internal cluster cost (i.e., quality of the clustering is based solely on the quality of the worst internal cluster cost within the clustering). The average case vs. worst case cluster cost assessment distinction are extremes along a continuum, although all existing algorithms operate only at, rather than in between, these extremes. There is another way of categorizing clustering algorithms, and that based on the goal of each resultant cluster. Most of the time, clustering algorithms attempt to produce clusters containing items similar to or near each other (e.g., with k-means, a cluster consists of a centroid and a set of nearby points), and dissimilarity exists, ideally, only between different clusters. An alternate possible goal of clustering is to have each block contains as diverse a set of items as possible, where similarity exist between rather than within clusters. For example, partitioning the vertices of a graph into a set of k independent (or, equivalently, stable) sets would fall into this later category, assuming the graph has edges only between similar vertices. This subsumes graph k-colorability problems, one of the most well-known of the NP-complete problems, although in some special cases it is solvable in polynomial time Brandst\u00e4dt (1996); Feder et al. (1999); Hell et al. (2004). This general idea, where the clusters are as internally diverse as possible using some given measure of diversity, has been called anticlustering Valev (1983, 1998); Sp\u00e4th (1986) in the past and, as can be seen from the above, comprises in general some difficult computational problems.", "startOffset": 241, "endOffset": 1927}, {"referenceID": 1, "context": "Most clustering problems are based on optimizing an objective that is either a sum, or an average-case, utility where the goal is to optimize the sum of individual cluster costs \u2014 this includes the ubiquitous k-means procedure Lloyd (1982); Arthur and Vassilvitskii (2007) where the goal is to construct a partitioning based on choosing a set of cluster centers that minimizes the total sum of squared distances between each point and its closest center. More rarely, clustering algorithms may be based on robust objective functions Gar\u0107\u0131a-Escudero et al. (2010), where the goal is to optimize the worst-case internal cluster cost (i.e., quality of the clustering is based solely on the quality of the worst internal cluster cost within the clustering). The average case vs. worst case cluster cost assessment distinction are extremes along a continuum, although all existing algorithms operate only at, rather than in between, these extremes. There is another way of categorizing clustering algorithms, and that based on the goal of each resultant cluster. Most of the time, clustering algorithms attempt to produce clusters containing items similar to or near each other (e.g., with k-means, a cluster consists of a centroid and a set of nearby points), and dissimilarity exists, ideally, only between different clusters. An alternate possible goal of clustering is to have each block contains as diverse a set of items as possible, where similarity exist between rather than within clusters. For example, partitioning the vertices of a graph into a set of k independent (or, equivalently, stable) sets would fall into this later category, assuming the graph has edges only between similar vertices. This subsumes graph k-colorability problems, one of the most well-known of the NP-complete problems, although in some special cases it is solvable in polynomial time Brandst\u00e4dt (1996); Feder et al. (1999); Hell et al. (2004). This general idea, where the clusters are as internally diverse as possible using some given measure of diversity, has been called anticlustering Valev (1983, 1998); Sp\u00e4th (1986) in the past and, as can be seen from the above, comprises in general some difficult computational problems.", "startOffset": 241, "endOffset": 2107}, {"referenceID": 17, "context": ", fi(S) \u2264 fi(T ) whenever S \u2286 T ), normalized (fi(\u2205) = 0), and submodular Fujishige (2005) (i.", "startOffset": 74, "endOffset": 91}, {"referenceID": 52, "context": "Submodularity is a natural property in many real-world ML applications Wei et al. (2013); Zheng et al.", "startOffset": 71, "endOffset": 89}, {"referenceID": 52, "context": "Submodularity is a natural property in many real-world ML applications Wei et al. (2013); Zheng et al. (2014); Nagano et al.", "startOffset": 71, "endOffset": 110}, {"referenceID": 36, "context": "(2014); Nagano et al. (2010); Wei et al.", "startOffset": 8, "endOffset": 29}, {"referenceID": 36, "context": "(2014); Nagano et al. (2010); Wei et al. (2014b); Krause et al.", "startOffset": 8, "endOffset": 49}, {"referenceID": 32, "context": "(2014b); Krause et al. (2008a); Jegelka and Bilmes (2011); Das and Kempe (2011); Krause et al.", "startOffset": 9, "endOffset": 31}, {"referenceID": 30, "context": "(2008a); Jegelka and Bilmes (2011); Das and Kempe (2011); Krause et al.", "startOffset": 9, "endOffset": 35}, {"referenceID": 11, "context": "(2008a); Jegelka and Bilmes (2011); Das and Kempe (2011); Krause et al.", "startOffset": 36, "endOffset": 57}, {"referenceID": 11, "context": "(2008a); Jegelka and Bilmes (2011); Das and Kempe (2011); Krause et al. (2008b); Wei et al.", "startOffset": 36, "endOffset": 80}, {"referenceID": 11, "context": "(2008a); Jegelka and Bilmes (2011); Das and Kempe (2011); Krause et al. (2008b); Wei et al. (2015). When minimizing, submodularity naturally model notions of interacting costs and complexity, while when maximizing it readily models notions of diversity, summarization quality, and information.", "startOffset": 36, "endOffset": 99}, {"referenceID": 51, "context": "\u201d On the other hand, big data presents significant computational challenges to machine learning since, while big data is still getting bigger, it is expected that we are nearing the end of Moore\u2019s law Thompson and Parthasarathy (2006), and single threaded computing speed has unfortunately not significantly improved since about 2003.", "startOffset": 201, "endOffset": 235}, {"referenceID": 47, "context": "the \u201cnon-uniform\u201d case in the past Svitkina and Fleischer (2008); Goemans et al.", "startOffset": 35, "endOffset": 65}, {"referenceID": 19, "context": "the \u201cnon-uniform\u201d case in the past Svitkina and Fleischer (2008); Goemans et al. (2009), but we utilize the names homogeneous and heterogeneous to avoid implying that we desire the final set of submodular function valuations be uniform, which they do not need to be in our case.", "startOffset": 66, "endOffset": 88}, {"referenceID": 42, "context": "(2011) and distributed neural network training Povey et al. (2014), to name only a few.", "startOffset": 47, "endOffset": 67}, {"referenceID": 57, "context": "For example, Wei et al. (2015) show that the utility functions of data subsets for training certain machine learning classifiers can be derived as submodular functions.", "startOffset": 13, "endOffset": 31}, {"referenceID": 36, "context": ", locality maximization via bipartite graphs, where the goal is to organize the data so as to improve data locality, as in the work of Li et al. (2015); Alistarh et al.", "startOffset": 135, "endOffset": 152}, {"referenceID": 0, "context": "(2015); Alistarh et al. (2015) where a random partition is quite likely to perform very poorly.", "startOffset": 8, "endOffset": 31}, {"referenceID": 4, "context": "In many instances Boykov et al. (2001); Boykov and Kolmogorov (2004); Boykov and Veksler (2006), it was found that performing this maximization can be performed by a graph cut algorithm, in particular when the energy function takes the form E(y, x\u0304) = \u2211 v\u2208V \u03c6v(yv, x\u0304v) + \u2211 C\u2208C \u03c6C(yC) where C are the set of cliques of a graph.", "startOffset": 18, "endOffset": 39}, {"referenceID": 4, "context": "(2001); Boykov and Kolmogorov (2004); Boykov and Veksler (2006), it was found that performing this maximization can be performed by a graph cut algorithm, in particular when the energy function takes the form E(y, x\u0304) = \u2211 v\u2208V \u03c6v(yv, x\u0304v) + \u2211 C\u2208C \u03c6C(yC) where C are the set of cliques of a graph.", "startOffset": 8, "endOffset": 37}, {"referenceID": 4, "context": "(2001); Boykov and Kolmogorov (2004); Boykov and Veksler (2006), it was found that performing this maximization can be performed by a graph cut algorithm, in particular when the energy function takes the form E(y, x\u0304) = \u2211 v\u2208V \u03c6v(yv, x\u0304v) + \u2211 C\u2208C \u03c6C(yC) where C are the set of cliques of a graph.", "startOffset": 38, "endOffset": 64}, {"referenceID": 4, "context": "(2001); Boykov and Kolmogorov (2004); Boykov and Veksler (2006), it was found that performing this maximization can be performed by a graph cut algorithm, in particular when the energy function takes the form E(y, x\u0304) = \u2211 v\u2208V \u03c6v(yv, x\u0304v) + \u2211 C\u2208C \u03c6C(yC) where C are the set of cliques of a graph. When the order of the cliques is two, and when the \u03c6C functions are submodular, the MAP inference problem can be solved via a minimum graph cut algorithm, and if the order if \u03c6C is larger, then general submodular function minimization Fujishige (2005) (which runs in polynomial time in |V |) may be used.", "startOffset": 38, "endOffset": 548}, {"referenceID": 4, "context": "(2001); Boykov and Kolmogorov (2004); Boykov and Veksler (2006), it was found that performing this maximization can be performed by a graph cut algorithm, in particular when the energy function takes the form E(y, x\u0304) = \u2211 v\u2208V \u03c6v(yv, x\u0304v) + \u2211 C\u2208C \u03c6C(yC) where C are the set of cliques of a graph. When the order of the cliques is two, and when the \u03c6C functions are submodular, the MAP inference problem can be solved via a minimum graph cut algorithm, and if the order if \u03c6C is larger, then general submodular function minimization Fujishige (2005) (which runs in polynomial time in |V |) may be used. This corresponds, however, only to the binary image segmentation problem. In order to perform multi-label image segmentation, there have been various elaborate extensions of the energy functions to support this (e.g., Jegelka and Bilmes (2011); Shelhamer et al.", "startOffset": 38, "endOffset": 845}, {"referenceID": 4, "context": "(2001); Boykov and Kolmogorov (2004); Boykov and Veksler (2006), it was found that performing this maximization can be performed by a graph cut algorithm, in particular when the energy function takes the form E(y, x\u0304) = \u2211 v\u2208V \u03c6v(yv, x\u0304v) + \u2211 C\u2208C \u03c6C(yC) where C are the set of cliques of a graph. When the order of the cliques is two, and when the \u03c6C functions are submodular, the MAP inference problem can be solved via a minimum graph cut algorithm, and if the order if \u03c6C is larger, then general submodular function minimization Fujishige (2005) (which runs in polynomial time in |V |) may be used. This corresponds, however, only to the binary image segmentation problem. In order to perform multi-label image segmentation, there have been various elaborate extensions of the energy functions to support this (e.g., Jegelka and Bilmes (2011); Shelhamer et al. (2014); Heng et al.", "startOffset": 38, "endOffset": 870}, {"referenceID": 4, "context": "(2001); Boykov and Kolmogorov (2004); Boykov and Veksler (2006), it was found that performing this maximization can be performed by a graph cut algorithm, in particular when the energy function takes the form E(y, x\u0304) = \u2211 v\u2208V \u03c6v(yv, x\u0304v) + \u2211 C\u2208C \u03c6C(yC) where C are the set of cliques of a graph. When the order of the cliques is two, and when the \u03c6C functions are submodular, the MAP inference problem can be solved via a minimum graph cut algorithm, and if the order if \u03c6C is larger, then general submodular function minimization Fujishige (2005) (which runs in polynomial time in |V |) may be used. This corresponds, however, only to the binary image segmentation problem. In order to perform multi-label image segmentation, there have been various elaborate extensions of the energy functions to support this (e.g., Jegelka and Bilmes (2011); Shelhamer et al. (2014); Heng et al. (2015); Taniai et al.", "startOffset": 38, "endOffset": 890}, {"referenceID": 4, "context": "(2001); Boykov and Kolmogorov (2004); Boykov and Veksler (2006), it was found that performing this maximization can be performed by a graph cut algorithm, in particular when the energy function takes the form E(y, x\u0304) = \u2211 v\u2208V \u03c6v(yv, x\u0304v) + \u2211 C\u2208C \u03c6C(yC) where C are the set of cliques of a graph. When the order of the cliques is two, and when the \u03c6C functions are submodular, the MAP inference problem can be solved via a minimum graph cut algorithm, and if the order if \u03c6C is larger, then general submodular function minimization Fujishige (2005) (which runs in polynomial time in |V |) may be used. This corresponds, however, only to the binary image segmentation problem. In order to perform multi-label image segmentation, there have been various elaborate extensions of the energy functions to support this (e.g., Jegelka and Bilmes (2011); Shelhamer et al. (2014); Heng et al. (2015); Taniai et al. (2015); Kohli et al.", "startOffset": 38, "endOffset": 912}, {"referenceID": 4, "context": "(2001); Boykov and Kolmogorov (2004); Boykov and Veksler (2006), it was found that performing this maximization can be performed by a graph cut algorithm, in particular when the energy function takes the form E(y, x\u0304) = \u2211 v\u2208V \u03c6v(yv, x\u0304v) + \u2211 C\u2208C \u03c6C(yC) where C are the set of cliques of a graph. When the order of the cliques is two, and when the \u03c6C functions are submodular, the MAP inference problem can be solved via a minimum graph cut algorithm, and if the order if \u03c6C is larger, then general submodular function minimization Fujishige (2005) (which runs in polynomial time in |V |) may be used. This corresponds, however, only to the binary image segmentation problem. In order to perform multi-label image segmentation, there have been various elaborate extensions of the energy functions to support this (e.g., Jegelka and Bilmes (2011); Shelhamer et al. (2014); Heng et al. (2015); Taniai et al. (2015); Kohli et al. (2013); Silberman et al.", "startOffset": 38, "endOffset": 933}, {"referenceID": 4, "context": "(2001); Boykov and Kolmogorov (2004); Boykov and Veksler (2006), it was found that performing this maximization can be performed by a graph cut algorithm, in particular when the energy function takes the form E(y, x\u0304) = \u2211 v\u2208V \u03c6v(yv, x\u0304v) + \u2211 C\u2208C \u03c6C(yC) where C are the set of cliques of a graph. When the order of the cliques is two, and when the \u03c6C functions are submodular, the MAP inference problem can be solved via a minimum graph cut algorithm, and if the order if \u03c6C is larger, then general submodular function minimization Fujishige (2005) (which runs in polynomial time in |V |) may be used. This corresponds, however, only to the binary image segmentation problem. In order to perform multi-label image segmentation, there have been various elaborate extensions of the energy functions to support this (e.g., Jegelka and Bilmes (2011); Shelhamer et al. (2014); Heng et al. (2015); Taniai et al. (2015); Kohli et al. (2013); Silberman et al. (2014) to name just a few), but in all cases the energy function E(y, x\u0304) has to be extended to, at the very least, a non-pseudo Boolean function.", "startOffset": 38, "endOffset": 958}, {"referenceID": 2, "context": "When fi\u2019s are all modular, the tightest algorithm, so far, is to iteratively round an LP solution achieving O(1/( \u221a m logm)) approximation Asadpour and Saberi (2010), whereas the problem is NP-hard to 1/2 + approximate for any > 0 Golovin (2005).", "startOffset": 139, "endOffset": 166}, {"referenceID": 2, "context": "When fi\u2019s are all modular, the tightest algorithm, so far, is to iteratively round an LP solution achieving O(1/( \u221a m logm)) approximation Asadpour and Saberi (2010), whereas the problem is NP-hard to 1/2 + approximate for any > 0 Golovin (2005). When fi\u2019s are submodular, Golovin (2005) gives a matching-based algorithm with a factor 1/(n\u2212m+ 1) approximation that performs poorly when m n.", "startOffset": 139, "endOffset": 246}, {"referenceID": 2, "context": "When fi\u2019s are all modular, the tightest algorithm, so far, is to iteratively round an LP solution achieving O(1/( \u221a m logm)) approximation Asadpour and Saberi (2010), whereas the problem is NP-hard to 1/2 + approximate for any > 0 Golovin (2005). When fi\u2019s are submodular, Golovin (2005) gives a matching-based algorithm with a factor 1/(n\u2212m+ 1) approximation that performs poorly when m n.", "startOffset": 139, "endOffset": 288}, {"referenceID": 2, "context": "When fi\u2019s are all modular, the tightest algorithm, so far, is to iteratively round an LP solution achieving O(1/( \u221a m logm)) approximation Asadpour and Saberi (2010), whereas the problem is NP-hard to 1/2 + approximate for any > 0 Golovin (2005). When fi\u2019s are submodular, Golovin (2005) gives a matching-based algorithm with a factor 1/(n\u2212m+ 1) approximation that performs poorly when m n. Khot and Ponnuswami (2007) propose a binary search algorithm yielding an improved factor of 1/(2m \u2212 1).", "startOffset": 139, "endOffset": 418}, {"referenceID": 2, "context": "When fi\u2019s are all modular, the tightest algorithm, so far, is to iteratively round an LP solution achieving O(1/( \u221a m logm)) approximation Asadpour and Saberi (2010), whereas the problem is NP-hard to 1/2 + approximate for any > 0 Golovin (2005). When fi\u2019s are submodular, Golovin (2005) gives a matching-based algorithm with a factor 1/(n\u2212m+ 1) approximation that performs poorly when m n. Khot and Ponnuswami (2007) propose a binary search algorithm yielding an improved factor of 1/(2m \u2212 1). Another approach approximates each submodular function by its ellipsoid approximation (non-scalable) and reduces SFA to its modular version leading to an approximation factor of O( \u221a nm1/4 log n logm). These approaches are theoretically interesting, but they either do not fully exploit the problem structure or cannot scale to large problems. On the other hand, Problem 1 for \u03bb = 1 is called submodular welfare. This problem has been extensively studied in the literature and can be equivalently formulated as submodular maximization under a partition matroid constraint Vondr\u00e1k (2008). It", "startOffset": 139, "endOffset": 1082}, {"referenceID": 29, "context": "Problem 1 (Max-(Min+Avg)) Approximation factor \u03bb = 0, BinSrch Khot and Ponnuswami (2007) 1/(2m\u2212 1) \u03bb = 0, Matching Golovin (2005) 1/(n\u2212m+ 1) \u03bb = 0, Ellipsoid Goemans et al.", "startOffset": 62, "endOffset": 89}, {"referenceID": 19, "context": "Problem 1 (Max-(Min+Avg)) Approximation factor \u03bb = 0, BinSrch Khot and Ponnuswami (2007) 1/(2m\u2212 1) \u03bb = 0, Matching Golovin (2005) 1/(n\u2212m+ 1) \u03bb = 0, Ellipsoid Goemans et al.", "startOffset": 115, "endOffset": 130}, {"referenceID": 18, "context": "Problem 1 (Max-(Min+Avg)) Approximation factor \u03bb = 0, BinSrch Khot and Ponnuswami (2007) 1/(2m\u2212 1) \u03bb = 0, Matching Golovin (2005) 1/(n\u2212m+ 1) \u03bb = 0, Ellipsoid Goemans et al. (2009) O( \u221a nm1/4 log n logm) \u03bb = 1, GreedWelfare Fisher et al.", "startOffset": 158, "endOffset": 180}, {"referenceID": 16, "context": "(2009) O( \u221a nm1/4 log n logm) \u03bb = 1, GreedWelfare Fisher et al. (1978) 1/2 \u03bb = 0, GreedSat\u2217 (1/2\u2212 \u03b4, \u03b4 1/2+\u03b4 ) \u03bb = 0, MMax\u2217 O(min i 1+(|Ai |\u22121)(1\u2212\u03bafi (A \u03c0\u0302 i )) |Ai | \u221a m logm ) \u03bb = 0, GreedMax\u2020\u2217 1/m 0 < \u03bb < 1, GeneralGreedSat\u2217 \u03bb/2 0 < \u03bb < 1, CombSfaSwp\u2217 max{ \u03b2\u03b1 \u03bb\u0304\u03b2+\u03b1 , \u03bb\u03b2} 0 < \u03bb < 1, CombSfaSwp\u2020\u2217 max{min{\u03b1, 1 m}, \u03b2\u03b1 \u03bb\u0304\u03b2+\u03b1 , \u03bb\u03b2} \u03bb = 0, Hardness 1/2 Golovin (2005) \u03bb = 1, Hardness 1\u2212 1/e Vondr\u00e1k (2008) Table 1: Summary of our contributions and existing work on Problem 1.", "startOffset": 50, "endOffset": 71}, {"referenceID": 16, "context": "(2009) O( \u221a nm1/4 log n logm) \u03bb = 1, GreedWelfare Fisher et al. (1978) 1/2 \u03bb = 0, GreedSat\u2217 (1/2\u2212 \u03b4, \u03b4 1/2+\u03b4 ) \u03bb = 0, MMax\u2217 O(min i 1+(|Ai |\u22121)(1\u2212\u03bafi (A \u03c0\u0302 i )) |Ai | \u221a m logm ) \u03bb = 0, GreedMax\u2020\u2217 1/m 0 < \u03bb < 1, GeneralGreedSat\u2217 \u03bb/2 0 < \u03bb < 1, CombSfaSwp\u2217 max{ \u03b2\u03b1 \u03bb\u0304\u03b2+\u03b1 , \u03bb\u03b2} 0 < \u03bb < 1, CombSfaSwp\u2020\u2217 max{min{\u03b1, 1 m}, \u03b2\u03b1 \u03bb\u0304\u03b2+\u03b1 , \u03bb\u03b2} \u03bb = 0, Hardness 1/2 Golovin (2005) \u03bb = 1, Hardness 1\u2212 1/e Vondr\u00e1k (2008) Table 1: Summary of our contributions and existing work on Problem 1.", "startOffset": 50, "endOffset": 366}, {"referenceID": 16, "context": "(2009) O( \u221a nm1/4 log n logm) \u03bb = 1, GreedWelfare Fisher et al. (1978) 1/2 \u03bb = 0, GreedSat\u2217 (1/2\u2212 \u03b4, \u03b4 1/2+\u03b4 ) \u03bb = 0, MMax\u2217 O(min i 1+(|Ai |\u22121)(1\u2212\u03bafi (A \u03c0\u0302 i )) |Ai | \u221a m logm ) \u03bb = 0, GreedMax\u2020\u2217 1/m 0 < \u03bb < 1, GeneralGreedSat\u2217 \u03bb/2 0 < \u03bb < 1, CombSfaSwp\u2217 max{ \u03b2\u03b1 \u03bb\u0304\u03b2+\u03b1 , \u03bb\u03b2} 0 < \u03bb < 1, CombSfaSwp\u2020\u2217 max{min{\u03b1, 1 m}, \u03b2\u03b1 \u03bb\u0304\u03b2+\u03b1 , \u03bb\u03b2} \u03bb = 0, Hardness 1/2 Golovin (2005) \u03bb = 1, Hardness 1\u2212 1/e Vondr\u00e1k (2008) Table 1: Summary of our contributions and existing work on Problem 1.", "startOffset": 50, "endOffset": 404}, {"referenceID": 20, "context": "It is worth noting that the homogeneous instance of the submodular welfare withm = 2 generalizes the well-known max-cut problem (Goemans and Williamson, 1995), where the submodular objective is defined as the submodular neighborhood function (Iyer and Bilmes, 2013; Schrijver, 2003).", "startOffset": 128, "endOffset": 158}, {"referenceID": 27, "context": "It is worth noting that the homogeneous instance of the submodular welfare withm = 2 generalizes the well-known max-cut problem (Goemans and Williamson, 1995), where the submodular objective is defined as the submodular neighborhood function (Iyer and Bilmes, 2013; Schrijver, 2003).", "startOffset": 242, "endOffset": 282}, {"referenceID": 43, "context": "It is worth noting that the homogeneous instance of the submodular welfare withm = 2 generalizes the well-known max-cut problem (Goemans and Williamson, 1995), where the submodular objective is defined as the submodular neighborhood function (Iyer and Bilmes, 2013; Schrijver, 2003).", "startOffset": 242, "endOffset": 282}, {"referenceID": 16, "context": "admits a scalable greedy algorithm that achieves a 1/2 approximation Fisher et al. (1978). More recently a multi-linear extension based algorithm nicely solves the submodular welfare problem with a factor of (1\u22121/e) matching the hardness of this problem Vondr\u00e1k (2008).", "startOffset": 69, "endOffset": 90}, {"referenceID": 16, "context": "admits a scalable greedy algorithm that achieves a 1/2 approximation Fisher et al. (1978). More recently a multi-linear extension based algorithm nicely solves the submodular welfare problem with a factor of (1\u22121/e) matching the hardness of this problem Vondr\u00e1k (2008). It is worth noting that the homogeneous instance of the submodular welfare withm = 2 generalizes the well-known max-cut problem (Goemans and Williamson, 1995), where the submodular objective is defined as the submodular neighborhood function (Iyer and Bilmes, 2013; Schrijver, 2003).", "startOffset": 69, "endOffset": 269}, {"referenceID": 24, "context": "In the homogeneous setting, Hochbaum and Shmoys (1988) give a PTAS scheme ((1+ )-approximation algorithm which runs in polynomial time for any fixed ), while an LP relaxation algorithm provides a 2-approximation for the heterogeneous setting Lenstra et al.", "startOffset": 28, "endOffset": 55}, {"referenceID": 24, "context": "In the homogeneous setting, Hochbaum and Shmoys (1988) give a PTAS scheme ((1+ )-approximation algorithm which runs in polynomial time for any fixed ), while an LP relaxation algorithm provides a 2-approximation for the heterogeneous setting Lenstra et al. (1990). When the objectives are submodular, the problem becomes much harder.", "startOffset": 28, "endOffset": 264}, {"referenceID": 24, "context": "In the homogeneous setting, Hochbaum and Shmoys (1988) give a PTAS scheme ((1+ )-approximation algorithm which runs in polynomial time for any fixed ), while an LP relaxation algorithm provides a 2-approximation for the heterogeneous setting Lenstra et al. (1990). When the objectives are submodular, the problem becomes much harder. Even in the homogeneous setting, Svitkina and Fleischer (2008) show that the problem is information theoretically hard to approximate within o( \u221a n/ log n).", "startOffset": 28, "endOffset": 397}, {"referenceID": 43, "context": "Problem 2 (Min-(Max+Avg)) Approximation factor \u03bb = 0, Balanced\u2020 Svitkina and Fleischer (2008) min{m,n/m} \u03bb = 0, Sampling Svitkina and Fleischer (2008) O( \u221a n log n) \u03bb = 0, Ellipsoid Goemans et al.", "startOffset": 64, "endOffset": 94}, {"referenceID": 43, "context": "Problem 2 (Min-(Max+Avg)) Approximation factor \u03bb = 0, Balanced\u2020 Svitkina and Fleischer (2008) min{m,n/m} \u03bb = 0, Sampling Svitkina and Fleischer (2008) O( \u221a n log n) \u03bb = 0, Ellipsoid Goemans et al.", "startOffset": 64, "endOffset": 151}, {"referenceID": 16, "context": "Problem 2 (Min-(Max+Avg)) Approximation factor \u03bb = 0, Balanced\u2020 Svitkina and Fleischer (2008) min{m,n/m} \u03bb = 0, Sampling Svitkina and Fleischer (2008) O( \u221a n log n) \u03bb = 0, Ellipsoid Goemans et al. (2009) O( \u221a n log n) \u03bb = 1, GreedSplit\u2020 Zhao et al.", "startOffset": 182, "endOffset": 204}, {"referenceID": 16, "context": "Problem 2 (Min-(Max+Avg)) Approximation factor \u03bb = 0, Balanced\u2020 Svitkina and Fleischer (2008) min{m,n/m} \u03bb = 0, Sampling Svitkina and Fleischer (2008) O( \u221a n log n) \u03bb = 0, Ellipsoid Goemans et al. (2009) O( \u221a n log n) \u03bb = 1, GreedSplit\u2020 Zhao et al. (2004); Narasimhan et al.", "startOffset": 182, "endOffset": 256}, {"referenceID": 16, "context": "Problem 2 (Min-(Max+Avg)) Approximation factor \u03bb = 0, Balanced\u2020 Svitkina and Fleischer (2008) min{m,n/m} \u03bb = 0, Sampling Svitkina and Fleischer (2008) O( \u221a n log n) \u03bb = 0, Ellipsoid Goemans et al. (2009) O( \u221a n log n) \u03bb = 1, GreedSplit\u2020 Zhao et al. (2004); Narasimhan et al. (2005) 2 \u03bb = 1, Relax Chekuri and Ene (2011a) O(log n) \u03bb = 0, MMin\u2217 max i 2|A\u03c0 i | 1+(|A\u03c0 i |\u22121)(1\u2212\u03bafi (A \u03c0\u2217 i )) \u03bb = 0, Lov\u00e1sz Round\u2217 m 0 < \u03bb < 1 GeneralLov\u00e1sz Round\u2217 m 0 < \u03bb < 1, CombSlbSmp\u2217 min{ m\u03b1 m\u03bb\u0304+\u03bb , \u03b2(m\u03bb\u0304+ \u03bb)} 0 < \u03bb < 1, CombSlbSmp\u2020\u2217 min{m, m\u03b1 m\u03bb\u0304+\u03bb , \u03b2(m\u03bb\u0304+ \u03bb)} \u03bb = 0, Hardness\u2217 m \u03bb = 1, Hardness 2\u2212 2/m Ene et al.", "startOffset": 182, "endOffset": 282}, {"referenceID": 9, "context": "(2005) 2 \u03bb = 1, Relax Chekuri and Ene (2011a) O(log n) \u03bb = 0, MMin\u2217 max i 2|A\u03c0 i | 1+(|A\u03c0 i |\u22121)(1\u2212\u03bafi (A \u03c0\u2217 i )) \u03bb = 0, Lov\u00e1sz Round\u2217 m 0 < \u03bb < 1 GeneralLov\u00e1sz Round\u2217 m 0 < \u03bb < 1, CombSlbSmp\u2217 min{ m\u03b1 m\u03bb\u0304+\u03bb , \u03b2(m\u03bb\u0304+ \u03bb)} 0 < \u03bb < 1, CombSlbSmp\u2020\u2217 min{m, m\u03b1 m\u03bb\u0304+\u03bb , \u03b2(m\u03bb\u0304+ \u03bb)} \u03bb = 0, Hardness\u2217 m \u03bb = 1, Hardness 2\u2212 2/m Ene et al.", "startOffset": 22, "endOffset": 46}, {"referenceID": 9, "context": "(2005) 2 \u03bb = 1, Relax Chekuri and Ene (2011a) O(log n) \u03bb = 0, MMin\u2217 max i 2|A\u03c0 i | 1+(|A\u03c0 i |\u22121)(1\u2212\u03bafi (A \u03c0\u2217 i )) \u03bb = 0, Lov\u00e1sz Round\u2217 m 0 < \u03bb < 1 GeneralLov\u00e1sz Round\u2217 m 0 < \u03bb < 1, CombSlbSmp\u2217 min{ m\u03b1 m\u03bb\u0304+\u03bb , \u03b2(m\u03bb\u0304+ \u03bb)} 0 < \u03bb < 1, CombSlbSmp\u2020\u2217 min{m, m\u03b1 m\u03bb\u0304+\u03bb , \u03b2(m\u03bb\u0304+ \u03bb)} \u03bb = 0, Hardness\u2217 m \u03bb = 1, Hardness 2\u2212 2/m Ene et al. (2013) Table 2: Summary of our contributions and existing work on Problem 23.", "startOffset": 22, "endOffset": 333}, {"referenceID": 9, "context": "(2005) 2 \u03bb = 1, Relax Chekuri and Ene (2011a) O(log n) \u03bb = 0, MMin\u2217 max i 2|A\u03c0 i | 1+(|A\u03c0 i |\u22121)(1\u2212\u03bafi (A \u03c0\u2217 i )) \u03bb = 0, Lov\u00e1sz Round\u2217 m 0 < \u03bb < 1 GeneralLov\u00e1sz Round\u2217 m 0 < \u03bb < 1, CombSlbSmp\u2217 min{ m\u03b1 m\u03bb\u0304+\u03bb , \u03b2(m\u03bb\u0304+ \u03bb)} 0 < \u03bb < 1, CombSlbSmp\u2020\u2217 min{m, m\u03b1 m\u03bb\u0304+\u03bb , \u03b2(m\u03bb\u0304+ \u03bb)} \u03bb = 0, Hardness\u2217 m \u03bb = 1, Hardness 2\u2212 2/m Ene et al. (2013) Table 2: Summary of our contributions and existing work on Problem 23. computation Orlin (2009), where \u03b3 is the cost of a function valuation.", "startOffset": 22, "endOffset": 429}, {"referenceID": 9, "context": "(2005) 2 \u03bb = 1, Relax Chekuri and Ene (2011a) O(log n) \u03bb = 0, MMin\u2217 max i 2|A\u03c0 i | 1+(|A\u03c0 i |\u22121)(1\u2212\u03bafi (A \u03c0\u2217 i )) \u03bb = 0, Lov\u00e1sz Round\u2217 m 0 < \u03bb < 1 GeneralLov\u00e1sz Round\u2217 m 0 < \u03bb < 1, CombSlbSmp\u2217 min{ m\u03b1 m\u03bb\u0304+\u03bb , \u03b2(m\u03bb\u0304+ \u03bb)} 0 < \u03bb < 1, CombSlbSmp\u2020\u2217 min{m, m\u03b1 m\u03bb\u0304+\u03bb , \u03b2(m\u03bb\u0304+ \u03bb)} \u03bb = 0, Hardness\u2217 m \u03bb = 1, Hardness 2\u2212 2/m Ene et al. (2013) Table 2: Summary of our contributions and existing work on Problem 23. computation Orlin (2009), where \u03b3 is the cost of a function valuation. Similar to Submodular Fair Allocation, Goemans et al. (2009) applies the same ellipsoid approximation techniques leading to a factor of O( \u221a n log n) Goemans et al.", "startOffset": 22, "endOffset": 536}, {"referenceID": 9, "context": "(2005) 2 \u03bb = 1, Relax Chekuri and Ene (2011a) O(log n) \u03bb = 0, MMin\u2217 max i 2|A\u03c0 i | 1+(|A\u03c0 i |\u22121)(1\u2212\u03bafi (A \u03c0\u2217 i )) \u03bb = 0, Lov\u00e1sz Round\u2217 m 0 < \u03bb < 1 GeneralLov\u00e1sz Round\u2217 m 0 < \u03bb < 1, CombSlbSmp\u2217 min{ m\u03b1 m\u03bb\u0304+\u03bb , \u03b2(m\u03bb\u0304+ \u03bb)} 0 < \u03bb < 1, CombSlbSmp\u2020\u2217 min{m, m\u03b1 m\u03bb\u0304+\u03bb , \u03b2(m\u03bb\u0304+ \u03bb)} \u03bb = 0, Hardness\u2217 m \u03bb = 1, Hardness 2\u2212 2/m Ene et al. (2013) Table 2: Summary of our contributions and existing work on Problem 23. computation Orlin (2009), where \u03b3 is the cost of a function valuation. Similar to Submodular Fair Allocation, Goemans et al. (2009) applies the same ellipsoid approximation techniques leading to a factor of O( \u221a n log n) Goemans et al. (2009). When \u03bb = 1, Problem 2 becomes the submodular multiway partition (SMP) for which one can obtain a relaxation based 2approximation Chekuri and Ene (2011a) in the homogeneous case.", "startOffset": 22, "endOffset": 647}, {"referenceID": 9, "context": "(2005) 2 \u03bb = 1, Relax Chekuri and Ene (2011a) O(log n) \u03bb = 0, MMin\u2217 max i 2|A\u03c0 i | 1+(|A\u03c0 i |\u22121)(1\u2212\u03bafi (A \u03c0\u2217 i )) \u03bb = 0, Lov\u00e1sz Round\u2217 m 0 < \u03bb < 1 GeneralLov\u00e1sz Round\u2217 m 0 < \u03bb < 1, CombSlbSmp\u2217 min{ m\u03b1 m\u03bb\u0304+\u03bb , \u03b2(m\u03bb\u0304+ \u03bb)} 0 < \u03bb < 1, CombSlbSmp\u2020\u2217 min{m, m\u03b1 m\u03bb\u0304+\u03bb , \u03b2(m\u03bb\u0304+ \u03bb)} \u03bb = 0, Hardness\u2217 m \u03bb = 1, Hardness 2\u2212 2/m Ene et al. (2013) Table 2: Summary of our contributions and existing work on Problem 23. computation Orlin (2009), where \u03b3 is the cost of a function valuation. Similar to Submodular Fair Allocation, Goemans et al. (2009) applies the same ellipsoid approximation techniques leading to a factor of O( \u221a n log n) Goemans et al. (2009). When \u03bb = 1, Problem 2 becomes the submodular multiway partition (SMP) for which one can obtain a relaxation based 2approximation Chekuri and Ene (2011a) in the homogeneous case.", "startOffset": 22, "endOffset": 801}, {"referenceID": 9, "context": "(2005) 2 \u03bb = 1, Relax Chekuri and Ene (2011a) O(log n) \u03bb = 0, MMin\u2217 max i 2|A\u03c0 i | 1+(|A\u03c0 i |\u22121)(1\u2212\u03bafi (A \u03c0\u2217 i )) \u03bb = 0, Lov\u00e1sz Round\u2217 m 0 < \u03bb < 1 GeneralLov\u00e1sz Round\u2217 m 0 < \u03bb < 1, CombSlbSmp\u2217 min{ m\u03b1 m\u03bb\u0304+\u03bb , \u03b2(m\u03bb\u0304+ \u03bb)} 0 < \u03bb < 1, CombSlbSmp\u2020\u2217 min{m, m\u03b1 m\u03bb\u0304+\u03bb , \u03b2(m\u03bb\u0304+ \u03bb)} \u03bb = 0, Hardness\u2217 m \u03bb = 1, Hardness 2\u2212 2/m Ene et al. (2013) Table 2: Summary of our contributions and existing work on Problem 23. computation Orlin (2009), where \u03b3 is the cost of a function valuation. Similar to Submodular Fair Allocation, Goemans et al. (2009) applies the same ellipsoid approximation techniques leading to a factor of O( \u221a n log n) Goemans et al. (2009). When \u03bb = 1, Problem 2 becomes the submodular multiway partition (SMP) for which one can obtain a relaxation based 2approximation Chekuri and Ene (2011a) in the homogeneous case. In the non-homogeneous case, the guarantee is O(log n) Chekuri and Ene (2011b). Similarly, Zhao et al.", "startOffset": 22, "endOffset": 905}, {"referenceID": 9, "context": "(2005) 2 \u03bb = 1, Relax Chekuri and Ene (2011a) O(log n) \u03bb = 0, MMin\u2217 max i 2|A\u03c0 i | 1+(|A\u03c0 i |\u22121)(1\u2212\u03bafi (A \u03c0\u2217 i )) \u03bb = 0, Lov\u00e1sz Round\u2217 m 0 < \u03bb < 1 GeneralLov\u00e1sz Round\u2217 m 0 < \u03bb < 1, CombSlbSmp\u2217 min{ m\u03b1 m\u03bb\u0304+\u03bb , \u03b2(m\u03bb\u0304+ \u03bb)} 0 < \u03bb < 1, CombSlbSmp\u2020\u2217 min{m, m\u03b1 m\u03bb\u0304+\u03bb , \u03b2(m\u03bb\u0304+ \u03bb)} \u03bb = 0, Hardness\u2217 m \u03bb = 1, Hardness 2\u2212 2/m Ene et al. (2013) Table 2: Summary of our contributions and existing work on Problem 23. computation Orlin (2009), where \u03b3 is the cost of a function valuation. Similar to Submodular Fair Allocation, Goemans et al. (2009) applies the same ellipsoid approximation techniques leading to a factor of O( \u221a n log n) Goemans et al. (2009). When \u03bb = 1, Problem 2 becomes the submodular multiway partition (SMP) for which one can obtain a relaxation based 2approximation Chekuri and Ene (2011a) in the homogeneous case. In the non-homogeneous case, the guarantee is O(log n) Chekuri and Ene (2011b). Similarly, Zhao et al. (2004); Narasimhan et al.", "startOffset": 22, "endOffset": 936}, {"referenceID": 9, "context": "(2005) 2 \u03bb = 1, Relax Chekuri and Ene (2011a) O(log n) \u03bb = 0, MMin\u2217 max i 2|A\u03c0 i | 1+(|A\u03c0 i |\u22121)(1\u2212\u03bafi (A \u03c0\u2217 i )) \u03bb = 0, Lov\u00e1sz Round\u2217 m 0 < \u03bb < 1 GeneralLov\u00e1sz Round\u2217 m 0 < \u03bb < 1, CombSlbSmp\u2217 min{ m\u03b1 m\u03bb\u0304+\u03bb , \u03b2(m\u03bb\u0304+ \u03bb)} 0 < \u03bb < 1, CombSlbSmp\u2020\u2217 min{m, m\u03b1 m\u03bb\u0304+\u03bb , \u03b2(m\u03bb\u0304+ \u03bb)} \u03bb = 0, Hardness\u2217 m \u03bb = 1, Hardness 2\u2212 2/m Ene et al. (2013) Table 2: Summary of our contributions and existing work on Problem 23. computation Orlin (2009), where \u03b3 is the cost of a function valuation. Similar to Submodular Fair Allocation, Goemans et al. (2009) applies the same ellipsoid approximation techniques leading to a factor of O( \u221a n log n) Goemans et al. (2009). When \u03bb = 1, Problem 2 becomes the submodular multiway partition (SMP) for which one can obtain a relaxation based 2approximation Chekuri and Ene (2011a) in the homogeneous case. In the non-homogeneous case, the guarantee is O(log n) Chekuri and Ene (2011b). Similarly, Zhao et al. (2004); Narasimhan et al. (2005) propose a greedy splitting 2-approximation algorithm for the homogeneous setting.", "startOffset": 22, "endOffset": 962}, {"referenceID": 8, "context": "For SFA, when m = 2, we formulate the problem as non-monotone submodular maximization, which can be approximated up to a factor of 1/2 with O(n) function evaluations Buchbinder et al. (2012). For general m, we give a simple and scalable greedy algorithm (GreedMax), and show a factor of 1/m in the homogeneous setting, improving the state-ofthe-art factor of 1/(2m\u2212 1) under the heterogeneous setting Khot and Ponnuswami (2007).", "startOffset": 166, "endOffset": 191}, {"referenceID": 8, "context": "For SFA, when m = 2, we formulate the problem as non-monotone submodular maximization, which can be approximated up to a factor of 1/2 with O(n) function evaluations Buchbinder et al. (2012). For general m, we give a simple and scalable greedy algorithm (GreedMax), and show a factor of 1/m in the homogeneous setting, improving the state-ofthe-art factor of 1/(2m\u2212 1) under the heterogeneous setting Khot and Ponnuswami (2007). For the heterogeneous setting, we propose a \u201csaturate\u201d greedy algorithm (GreedSat) that iteratively solves instances of submodular welfare problems.", "startOffset": 166, "endOffset": 428}, {"referenceID": 48, "context": "the hardness result in Svitkina and Fleischer (2008) and show that it is hard to approximate better than m for any m = o( \u221a n/ log n) even in the homogeneous setting.", "startOffset": 23, "endOffset": 53}, {"referenceID": 8, "context": "This problem has been well studied in the literature Buchbinder et al. (2012); Dobzinski and Mor (2015); Feige et al.", "startOffset": 53, "endOffset": 78}, {"referenceID": 8, "context": "This problem has been well studied in the literature Buchbinder et al. (2012); Dobzinski and Mor (2015); Feige et al.", "startOffset": 53, "endOffset": 104}, {"referenceID": 8, "context": "This problem has been well studied in the literature Buchbinder et al. (2012); Dobzinski and Mor (2015); Feige et al. (2011). A simple bi-directional randomized greedy algorithm Buchbinder et al.", "startOffset": 53, "endOffset": 125}, {"referenceID": 8, "context": "This problem has been well studied in the literature Buchbinder et al. (2012); Dobzinski and Mor (2015); Feige et al. (2011). A simple bi-directional randomized greedy algorithm Buchbinder et al. (2012) solves Eqn 4 with a tight factor of 1/2.", "startOffset": 53, "endOffset": 203}, {"referenceID": 32, "context": "By assuming the homogeneity of the fi\u2019s, we obtain a very simple 1/m-approximation algorithm improving upon the state-of-the-art factor 1/(2m \u2212 1) Khot and Ponnuswami (2007). Thanks to the lazy evaluation trick as described in Minoux (1978), Line 5 in Alg.", "startOffset": 147, "endOffset": 174}, {"referenceID": 32, "context": "By assuming the homogeneity of the fi\u2019s, we obtain a very simple 1/m-approximation algorithm improving upon the state-of-the-art factor 1/(2m \u2212 1) Khot and Ponnuswami (2007). Thanks to the lazy evaluation trick as described in Minoux (1978), Line 5 in Alg.", "startOffset": 147, "endOffset": 241}, {"referenceID": 26, "context": "Similar in flavor to the one proposed in Krause et al. (2008b) GreedSat defines an intermediate objective F\u0304 c(\u03c0) = \u2211m i=1 f c i (A \u03c0 i ), where f c i (A) = 1 m min{fi(A), c} (Line 2).", "startOffset": 41, "endOffset": 63}, {"referenceID": 16, "context": "In this work, we solve Line 6 using the greedy algorithm as described in Alg 3, which attains a constant 1/2-approximation Fisher et al. (1978). Moreover the lazy evaluation trick also applies for Alg 3 enabling the wrapper algorithm GreedSat scalable to large data sets.", "startOffset": 123, "endOffset": 144}, {"referenceID": 16, "context": "In this work, we solve Line 6 using the greedy algorithm as described in Alg 3, which attains a constant 1/2-approximation Fisher et al. (1978). Moreover the lazy evaluation trick also applies for Alg 3 enabling the wrapper algorithm GreedSat scalable to large data sets. One can also use a more computationally expensive multi-linear relaxation algorithm as given in Vondr\u00e1k (2008) to solve Line 6 with a tight factor \u03b1 = (1\u22121/e).", "startOffset": 123, "endOffset": 383}, {"referenceID": 16, "context": "In this work, we solve Line 6 using the greedy algorithm as described in Alg 3, which attains a constant 1/2-approximation Fisher et al. (1978). Moreover the lazy evaluation trick also applies for Alg 3 enabling the wrapper algorithm GreedSat scalable to large data sets. One can also use a more computationally expensive multi-linear relaxation algorithm as given in Vondr\u00e1k (2008) to solve Line 6 with a tight factor \u03b1 = (1\u22121/e). Setting the input argument \u03b1 as the approximation factor for Line 6, the essential idea of GreedSat is to perform a binary search over the parameter c to find the largest c\u2217 such that the returned solution \u03c0\u0302c \u2217 for the instance of SWP satisfies F\u0304 c \u2217 (\u03c0\u0302c \u2217 ) \u2265 \u03b1c\u2217. GreedSat terminates after solving O(log(i fi(V ) )) instances of SWP. Theorem 3 gives a bi-criterion optimality guarantee. Theorem 3 Given > 0, 0 \u2264 \u03b1 \u2264 1 and any 0 < \u03b4 < \u03b1, GreedSat finds a partition such that at least dm(\u03b1\u2212 \u03b4)e blocks receive utility at least \u03b4 1\u2212\u03b1+\u03b4 (max\u03c0\u2208\u03a0 mini fi(Ai )\u2212 ). For any 0 < \u03b4 < \u03b1 Theorem 3 ensures that the top dm(\u03b1\u2212 \u03b4)e valued blocks in the partition returned by GreedSat are (\u03b4/(1\u2212 \u03b1+ \u03b4)\u2212 )-optimal. \u03b4 controls the trade-off between the number of top valued blocks to bound and the performance guarantee attained for these blocks. The smaller \u03b4 is, the more top blocks are bounded, but with a weaker guarantee. We set the input argument \u03b1 = 1/2 (or \u03b1 = 1\u2212 1/e) as the worst-case performance guarantee for solving SWP so that the above theoretical analysis follows. However, the worst-case is often achieved only by very contrived submodular functions. For the ones used in practice, the greedy algorithm often leads to near-optimal solution (Krause et al. (2008b) and our own observations).", "startOffset": 123, "endOffset": 1699}, {"referenceID": 16, "context": "In this work, we solve Line 6 using the greedy algorithm as described in Alg 3, which attains a constant 1/2-approximation Fisher et al. (1978). Moreover the lazy evaluation trick also applies for Alg 3 enabling the wrapper algorithm GreedSat scalable to large data sets. One can also use a more computationally expensive multi-linear relaxation algorithm as given in Vondr\u00e1k (2008) to solve Line 6 with a tight factor \u03b1 = (1\u22121/e). Setting the input argument \u03b1 as the approximation factor for Line 6, the essential idea of GreedSat is to perform a binary search over the parameter c to find the largest c\u2217 such that the returned solution \u03c0\u0302c \u2217 for the instance of SWP satisfies F\u0304 c \u2217 (\u03c0\u0302c \u2217 ) \u2265 \u03b1c\u2217. GreedSat terminates after solving O(log(i fi(V ) )) instances of SWP. Theorem 3 gives a bi-criterion optimality guarantee. Theorem 3 Given > 0, 0 \u2264 \u03b1 \u2264 1 and any 0 < \u03b4 < \u03b1, GreedSat finds a partition such that at least dm(\u03b1\u2212 \u03b4)e blocks receive utility at least \u03b4 1\u2212\u03b1+\u03b4 (max\u03c0\u2208\u03a0 mini fi(Ai )\u2212 ). For any 0 < \u03b4 < \u03b1 Theorem 3 ensures that the top dm(\u03b1\u2212 \u03b4)e valued blocks in the partition returned by GreedSat are (\u03b4/(1\u2212 \u03b1+ \u03b4)\u2212 )-optimal. \u03b4 controls the trade-off between the number of top valued blocks to bound and the performance guarantee attained for these blocks. The smaller \u03b4 is, the more top blocks are bounded, but with a weaker guarantee. We set the input argument \u03b1 = 1/2 (or \u03b1 = 1\u2212 1/e) as the worst-case performance guarantee for solving SWP so that the above theoretical analysis follows. However, the worst-case is often achieved only by very contrived submodular functions. For the ones used in practice, the greedy algorithm often leads to near-optimal solution (Krause et al. (2008b) and our own observations). Setting \u03b1 as the actual performance guarantee for SWP (often very close to 1) can improve the empirical bound, and we, in practice, typically set \u03b1 = 1 to good effect. MMax: In parallel to GreedSat, we also introduce a semi-gradient based approach for solving SFA under the heterogeneous setting. We call this algorithm minorizationmaximization (MMax, see Alg. 4). Similar to the ones proposed in Iyer et al. (2013a); Iyer and Bilmes (2013, 2012b), the idea is to iteratively maximize tight lower bounds of the submodular functions.", "startOffset": 123, "endOffset": 2143}, {"referenceID": 16, "context": "In this work, we solve Line 6 using the greedy algorithm as described in Alg 3, which attains a constant 1/2-approximation Fisher et al. (1978). Moreover the lazy evaluation trick also applies for Alg 3 enabling the wrapper algorithm GreedSat scalable to large data sets. One can also use a more computationally expensive multi-linear relaxation algorithm as given in Vondr\u00e1k (2008) to solve Line 6 with a tight factor \u03b1 = (1\u22121/e). Setting the input argument \u03b1 as the approximation factor for Line 6, the essential idea of GreedSat is to perform a binary search over the parameter c to find the largest c\u2217 such that the returned solution \u03c0\u0302c \u2217 for the instance of SWP satisfies F\u0304 c \u2217 (\u03c0\u0302c \u2217 ) \u2265 \u03b1c\u2217. GreedSat terminates after solving O(log(i fi(V ) )) instances of SWP. Theorem 3 gives a bi-criterion optimality guarantee. Theorem 3 Given > 0, 0 \u2264 \u03b1 \u2264 1 and any 0 < \u03b4 < \u03b1, GreedSat finds a partition such that at least dm(\u03b1\u2212 \u03b4)e blocks receive utility at least \u03b4 1\u2212\u03b1+\u03b4 (max\u03c0\u2208\u03a0 mini fi(Ai )\u2212 ). For any 0 < \u03b4 < \u03b1 Theorem 3 ensures that the top dm(\u03b1\u2212 \u03b4)e valued blocks in the partition returned by GreedSat are (\u03b4/(1\u2212 \u03b1+ \u03b4)\u2212 )-optimal. \u03b4 controls the trade-off between the number of top valued blocks to bound and the performance guarantee attained for these blocks. The smaller \u03b4 is, the more top blocks are bounded, but with a weaker guarantee. We set the input argument \u03b1 = 1/2 (or \u03b1 = 1\u2212 1/e) as the worst-case performance guarantee for solving SWP so that the above theoretical analysis follows. However, the worst-case is often achieved only by very contrived submodular functions. For the ones used in practice, the greedy algorithm often leads to near-optimal solution (Krause et al. (2008b) and our own observations). Setting \u03b1 as the actual performance guarantee for SWP (often very close to 1) can improve the empirical bound, and we, in practice, typically set \u03b1 = 1 to good effect. MMax: In parallel to GreedSat, we also introduce a semi-gradient based approach for solving SFA under the heterogeneous setting. We call this algorithm minorizationmaximization (MMax, see Alg. 4). Similar to the ones proposed in Iyer et al. (2013a); Iyer and Bilmes (2013, 2012b), the idea is to iteratively maximize tight lower bounds of the submodular functions. Submodular functions have tight modular lower bounds, which are related to the subdifferential \u2202f (Y ) of the submodular set function f at a set Y \u2286 V , which is defined Fujishige (2005) as:", "startOffset": 123, "endOffset": 2446}, {"referenceID": 2, "context": "In other words, at iteration t+ 1, for each block i, we approximate fi with its modular lower bound tight at A\u03c0 t i and solve a modular version of Problem 1 (Line 7), which admits efficient approximation algorithms Asadpour and Saberi (2010). MMax is initialized with a partition \u03c00, which is obtained by solving Problem 1, where each fi is replaced with a simple modular function f \u2032 i(A) = \u2211 a\u2208A fi(a).", "startOffset": 215, "endOffset": 242}, {"referenceID": 48, "context": "Existing hardness for SLB is shown to be o( \u221a n/ log n) Svitkina and Fleischer (2008). However it is independent of m, and Svitkina and Fleischer (2008) assumes m = \u0398( \u221a n/ log n) in their analysis.", "startOffset": 56, "endOffset": 86}, {"referenceID": 48, "context": "Existing hardness for SLB is shown to be o( \u221a n/ log n) Svitkina and Fleischer (2008). However it is independent of m, and Svitkina and Fleischer (2008) assumes m = \u0398( \u221a n/ log n) in their analysis.", "startOffset": 56, "endOffset": 153}, {"referenceID": 48, "context": "Though the proof technique for Theorem 6 mostly carries over from Svitkina and Fleischer (2008), the result strictly generalizes the analysis in Svitkina and Fleischer (2008).", "startOffset": 66, "endOffset": 96}, {"referenceID": 48, "context": "Though the proof technique for Theorem 6 mostly carries over from Svitkina and Fleischer (2008), the result strictly generalizes the analysis in Svitkina and Fleischer (2008). For any choice of m = o( \u221a n/ log n) Theorem 6 implies that it is information theoretically hard to approximate SLB better than m even for the homogeneous setting.", "startOffset": 66, "endOffset": 175}, {"referenceID": 28, "context": "The algorithm proceeds as follows: (1) apply the Lov\u00e1sz extension of submodular functions to relax SLB to a convex program, which is exactly solved to a fractional solution (Line 2); (2) map the fractional solution to a partition using the \u03b8-rounding technique as proposed in Iyer et al. (2014) (Line 3 - 6).", "startOffset": 276, "endOffset": 295}, {"referenceID": 31, "context": "Here, we iteratively choose modular upper bounds, which are defined via superdifferentials \u2202f (Y ) of a submodular function Jegelka and Bilmes (2011) at Y : \u2202 (Y ) = {y \u2208 R : f(X)\u2212 y(X) \u2264 f(Y )\u2212 y(Y ); for all X \u2286 V }.", "startOffset": 124, "endOffset": 150}, {"referenceID": 25, "context": "Moreover, there are specific supergradients Iyer and Bilmes (2012a); Iyer et al.", "startOffset": 44, "endOffset": 68}, {"referenceID": 25, "context": "Moreover, there are specific supergradients Iyer and Bilmes (2012a); Iyer et al. (2013a) that define the following two modular upper bounds (when referring to either one, we use mfX): mfX,1(Y ) , f(X)\u2212 \u2211", "startOffset": 44, "endOffset": 89}, {"referenceID": 36, "context": "At iteration t+ 1, for each block i, MMin replaces fi with a choice of its modular upper bound mi tight at A \u03c0t i and solves a modular version of Problem 2 (Line 7), for which there exists an efficient LP relaxation based algorithm Lenstra et al. (1990). Similar to MMax, the initial partition \u03c00 is obtained by solving Problem 2, where each fi is substituted with f \u2032 i(A) = \u2211 a\u2208A fi(a).", "startOffset": 232, "endOffset": 254}, {"referenceID": 19, "context": "Since some of the algorithms, such as the Ellipsoidal Approximations Goemans et al. (2009) and Lov\u00e1sz relaxation algorithms, are computationally intensive, we restrict ourselves to only 40 data instances, i.", "startOffset": 69, "endOffset": 91}, {"referenceID": 19, "context": ", SFA, we compare among 6 algorithms: GreedMax, GreedSat, MMax, Balanced Partition (BP), Ellipsoid Approximation (EA) Goemans et al. (2009), and Binary Search algorithm (BS) Khot and Ponnuswami (2007).", "startOffset": 118, "endOffset": 140}, {"referenceID": 19, "context": ", SFA, we compare among 6 algorithms: GreedMax, GreedSat, MMax, Balanced Partition (BP), Ellipsoid Approximation (EA) Goemans et al. (2009), and Binary Search algorithm (BS) Khot and Ponnuswami (2007). Balanced Partition method simply partitions the ground set V into m blocks such that the size of each block is balanced and is either d |V | m e or b |V | m c.", "startOffset": 118, "endOffset": 201}, {"referenceID": 19, "context": ", SLB, we compare among 5 algorithms: Lov\u00e1sz Round, MMin, GeneralGreedMin, Ellipsoid Approximation (EA) Goemans et al. (2009), and Balanced Partition Svitkina and Fleischer (2011).", "startOffset": 104, "endOffset": 126}, {"referenceID": 19, "context": ", SLB, we compare among 5 algorithms: Lov\u00e1sz Round, MMin, GeneralGreedMin, Ellipsoid Approximation (EA) Goemans et al. (2009), and Balanced Partition Svitkina and Fleischer (2011). We implement GeneralGreedMin with the input argument \u03bb = 0.", "startOffset": 104, "endOffset": 180}, {"referenceID": 52, "context": "(2014b, 2015); Tschiatschek et al. (2014), which has the form:", "startOffset": 15, "endOffset": 42}, {"referenceID": 47, "context": "ffea is in the form of a sum of concave over modular functions, hence is monotone submodular Stobbe and Krause (2010). The class of feature-based submodular function has been widely applied to model the utility of a data subset on a number of tasks, including speech data subset selection Wei et al.", "startOffset": 93, "endOffset": 118}, {"referenceID": 47, "context": "ffea is in the form of a sum of concave over modular functions, hence is monotone submodular Stobbe and Krause (2010). The class of feature-based submodular function has been widely applied to model the utility of a data subset on a number of tasks, including speech data subset selection Wei et al. (2014b,c), and image summarization Tschiatschek et al. (2014). Moreover ffea has been shown in Wei et al.", "startOffset": 93, "endOffset": 362}, {"referenceID": 47, "context": "ffea is in the form of a sum of concave over modular functions, hence is monotone submodular Stobbe and Krause (2010). The class of feature-based submodular function has been widely applied to model the utility of a data subset on a number of tasks, including speech data subset selection Wei et al. (2014b,c), and image summarization Tschiatschek et al. (2014). Moreover ffea has been shown in Wei et al. (2015) to model the log-likelihood of a data subset for a N\u00e4\u0131ve Bayes classifier.", "startOffset": 93, "endOffset": 413}, {"referenceID": 42, "context": "Note that this distributed training scheme is similar to the ones presented in Povey et al. (2014). The submodular partitioning for both tasks is obtained by solving the homogeneous case of Problem 1 (\u03bb = 0) using GreedMax on a form of clustered facility location, as proposed and used in Wei et al.", "startOffset": 79, "endOffset": 99}, {"referenceID": 42, "context": "Note that this distributed training scheme is similar to the ones presented in Povey et al. (2014). The submodular partitioning for both tasks is obtained by solving the homogeneous case of Problem 1 (\u03bb = 0) using GreedMax on a form of clustered facility location, as proposed and used in Wei et al. (2015). The function is defined as follows:", "startOffset": 79, "endOffset": 307}, {"referenceID": 57, "context": "Wei et al. (2015) show that fc-fac models the log-likelihood of a data subset for a Nearest Neighbor classifier.", "startOffset": 0, "endOffset": 18}, {"referenceID": 54, "context": "sparse Wei et al. (2014a). In the experiment, we instantiate ffac by a sparse 10-nearest neighbor sparse graph, where each item v is connected only to its 10 closest neighbors.", "startOffset": 7, "endOffset": 26}, {"referenceID": 4, "context": "A number of unsupervised methods are tested as baselines in the experiment, including k-means, k-medoids, graph cuts Boykov and Kolmogorov (2004) and spectral clustering Von Luxburg (2007).", "startOffset": 117, "endOffset": 146}, {"referenceID": 4, "context": "A number of unsupervised methods are tested as baselines in the experiment, including k-means, k-medoids, graph cuts Boykov and Kolmogorov (2004) and spectral clustering Von Luxburg (2007). We use the RBF kernel sparse similarity matrix as the input for spectral clustering.", "startOffset": 117, "endOffset": 189}, {"referenceID": 3, "context": "For graph cuts, we use the MATLAB implementation Bagon (2006), which has a smoothness parameter \u03b1.", "startOffset": 49, "endOffset": 62}, {"referenceID": 2, "context": "Proof We assume the approximation factor of the algorithm for solving the modular version of Problem 1 is \u03b1 = O( 1 \u221a m logm ) Asadpour and Saberi (2010). For notation simplicity, we write \u03c0\u0302 = (\u00c21, .", "startOffset": 126, "endOffset": 153}, {"referenceID": 2, "context": "Proof We assume the approximation factor of the algorithm for solving the modular version of Problem 1 is \u03b1 = O( 1 \u221a m logm ) Asadpour and Saberi (2010). For notation simplicity, we write \u03c0\u0302 = (\u00c21, . . . , \u00c2m) as the resulting partition after the first iteration of MMax, and \u03c0\u2217 = (A1, . . . , A \u2217 m) as its optimal solution. Note that first iteration suffices to yield the performance guarantee, and the subsequent iterations are designed so as to improve the empirical performance. Since the proxy function for each function fi used for the first iteration are the simple modular upper bound with the form: hi(X) = \u2211 j\u2208X fi(j). Given the curvature of each submodular function fi, we can tightly bound a submodular function fi in the following form Iyer et al. (2013b):", "startOffset": 126, "endOffset": 770}, {"referenceID": 48, "context": "Proof We use the same proof techniques as in Svitkina and Fleischer (2008). Consider two submodular functions:", "startOffset": 45, "endOffset": 75}, {"referenceID": 48, "context": "As shown in Svitkina and Fleischer (2008), P{f1(S) > f2(S)} is maximized when |S| = \u03b1.", "startOffset": 12, "endOffset": 42}, {"referenceID": 36, "context": "Proof Let \u03b1 = 2 be the approximation factor of the algorithm for solving the modular version of Problem 2 Lenstra et al. (1990). For notation simplicity, we write \u03c0\u0302 = (\u00c21, .", "startOffset": 106, "endOffset": 128}], "year": 2016, "abstractText": "We study two mixed robust/average-case submodular partitioning problems that we collectively call Submodular Partitioning. These problems generalize both purely robust instances of the problem (namely max-min submodular fair allocation (SFA) Golovin (2005) and min-max submodular load balancing (SLB) Svitkina and Fleischer (2008)) and also generalize average-case instances (that is the submodular welfare problem (SWP) Vondr\u00e1k (2008) and submodular multiway partition (SMP) Chekuri and Ene (2011a)). While the robust versions have been studied in the theory community Goemans et al. (2009); Golovin (2005); Khot and Ponnuswami (2007); Svitkina and Fleischer (2008); Vondr\u00e1k (2008), existing work has focused on tight approximation guarantees, and the resultant algorithms are not, in general, scalable to very large real-world applications. This is in contrast to the average case, where most of the algorithms are scalable. In the present paper, we bridge this gap, by proposing several new algorithms (including those based on greedy, majorization-minimization, minorization-maximization, and relaxation algorithms) that not only scale to large sizes but that also achieve theoretical approximation guarantees close to the state-of-the-art, and in some cases achieve new tight bounds. We also provide new scalable algorithms that apply to additive combinations of the robust and average-case extreme objectives. We show that these problems have many applications in machine learning (ML). This includes: 1) data 1 ar X iv :1 51 0. 08 86 5v 2 [ cs .D S] 1 6 A ug 2 01 6 Wei, Iyer, Wang, Bai, Bilmes partitioning and load balancing for distributed machine algorithms on parallel machines; 2) data clustering; and 3) multi-label image segmentation with (only) Boolean submodular functions via pixel partitioning. We empirically demonstrate the efficacy of our algorithms on real-world problems involving data partitioning for distributed optimization of standard machine learning objectives (including both convex and deep neural network objectives), and also on purely unsupervised (i.e., no supervised or semi-supervised learning, and no interactive segmentation) image segmentation.", "creator": "LaTeX with hyperref package"}}}