{"id": "1510.08949", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "30-Oct-2015", "title": "Testing Visual Attention in Dynamic Environments", "abstract": "We investigate attention as the active pursuit of useful information. This contrasts with attention as a mechanism for the attenuation of irrelevant information. We also consider the role of short-term memory, whose use is critical to any model incapable of simultaneously perceiving all information on which its output depends. We present several simple synthetic tasks, which become considerably more interesting when we impose strong constraints on how a model can interact with its input, and on how long it can take to produce its output. We develop a model with a different structure from those seen in previous work, and we train it using stochastic variational inference with a learned proposal distribution.", "histories": [["v1", "Fri, 30 Oct 2015 01:39:54 GMT  (1320kb,D)", "http://arxiv.org/abs/1510.08949v1", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["philip bachman", "david krueger", "doina precup"], "accepted": false, "id": "1510.08949"}, "pdf": {"name": "1510.08949.pdf", "metadata": {"source": "CRF", "title": "Testing Visual Attention in Dynamic Environments", "authors": ["Philip Bachman", "David Krueger", "Doina Precup"], "emails": ["phil.bachman@gmail.com", "dkrueger@email.com", "dprecup@cs.mcgill.ca"], "sections": [{"heading": "1 Introduction", "text": "Attention, which is considered a behavioral phenomenon, can be interpreted as a necessary adaptation to intrinsic limitations of perception. For example, if an agent exists in an environment where 100 bits of information per clock are available, but the agent is only able to observe 10 bits per clock, then the agent must be careful how he directs his perceptual capacity around the environment while capturing what bits he can. Agent attention is not generated by avoiding noise, but by tracking signals.We present multiple tasks to test the capabilities of models that combine visual attention mechanisms and sequential decision-making. Despite their simple structure, these tasks become difficult if we impose severe limitations on how a model can interact with its input and how many steps it may take to produce its output. Inputs and outputs in these tasks are either time-variable sequences or multiple presentations of a fixed value."}, {"heading": "2 Task and Model Descriptions", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "2.1 Our Tasks", "text": "In the static input task, which we call \"hasty copying,\" the model reconstructs an input based on a sequence of readings received from its sensor. To make the task more difficult, we limit the number of readings, T, and the dimension k, so that the product kT is significantly smaller than the input dimension. We encourage the model to continuously refine its prediction by simulating the reconstruction at an end time determined by a toxic random variable. We also introduce a family of sequential observation and prediction tasks, all of which involve traceability and copying objects from an input video. In these tasks, the model attempts to reconstruct the sequence of a particular object (or objects in the present)."}, {"heading": "2.2 Our Model", "text": "This year, it is more than ever before in the history of the city."}, {"heading": "3.1 Discussion", "text": "We presented a series of tasks that provide a test bed for models that combine visual attention with sequential prediction. Although our detection and tracking tasks are simple in form, they require sophisticated behavior to achieve consistent success. For example, to track multiple objects in the presence of deflectors, attention needs to be divided among targets while using short-term memory and knowledge of environmental dynamics to estimate the location of objects that are not currently being looked after. We presented a suitable model and some preliminary empirical results that show that our tasks are within the reach of current methods, but offer plenty of room for growth."}], "references": [{"title": "Learning wake-sleep recurrent attention models", "author": ["Jimmy Ba", "Roger Grosse", "Ruslan Salakhutdinov", "Brendan Frey"], "venue": "In NIPS,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2015}, {"title": "Multiple object recognition with visual attention", "author": ["Jimmy Ba", "Volodomyr Mnih", "Koray Kavucuoglu"], "venue": "In ICLR,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2015}, {"title": "Data generation as sequential decision making", "author": ["Philip Bachman", "Doina Precup"], "venue": "In NIPS,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2015}, {"title": "Draw: A recurrent neural network for image generation", "author": ["Karol Gregor", "Ivo Danihelka", "Alex Graves", "Daan Wierstra"], "venue": "In ICML,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2015}, {"title": "Gradient-based learning applied to document recognition", "author": ["Y. LeCun", "L. Bottou", "Y. Bengio", "P. Haffner"], "venue": "Proceedings of the IEEE,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 1998}, {"title": "Guided policy search", "author": ["Sergey Levine", "Vladlen Koltun"], "venue": "In ICML,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2013}, {"title": "Recurrent models of visual attention", "author": ["Volodomyr Mnih", "Nicolas Heess", "Alex Graves", "Koray Kavucuoglu"], "venue": "In NIPS,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2014}, {"title": "The toronto face database", "author": ["Joshua Susskind", "Adam Anderson", "Geoffrey E Hinton"], "venue": null, "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2010}], "referenceMentions": [{"referenceID": 0, "context": "[1, 2, 7], by considering time-varying inputs/outputs, and by putting \u201ctime-pressure\u201d on output construction (when working with a fixed input).", "startOffset": 0, "endOffset": 9}, {"referenceID": 1, "context": "[1, 2, 7], by considering time-varying inputs/outputs, and by putting \u201ctime-pressure\u201d on output construction (when working with a fixed input).", "startOffset": 0, "endOffset": 9}, {"referenceID": 6, "context": "[1, 2, 7], by considering time-varying inputs/outputs, and by putting \u201ctime-pressure\u201d on output construction (when working with a fixed input).", "startOffset": 0, "endOffset": 9}, {"referenceID": 5, "context": "One could also think of this training method as Guided Policy Search [6] \u2013 see [3] for more on this view.", "startOffset": 69, "endOffset": 72}, {"referenceID": 2, "context": "One could also think of this training method as Guided Policy Search [6] \u2013 see [3] for more on this view.", "startOffset": 79, "endOffset": 82}, {"referenceID": 0, "context": "We add uniform noise in [0, 1] to each pixel independently with probability .", "startOffset": 24, "endOffset": 30}, {"referenceID": 0, "context": "Finally, we clip all pixel values to remain within the [0, 1] interval.", "startOffset": 55, "endOffset": 61}, {"referenceID": 3, "context": "For the read op, we use a moveable 2x2 grid of differentiable Gaussian filters, as in the DRAW model from [4].", "startOffset": 106, "endOffset": 109}, {"referenceID": 2, "context": "See [3] for more discussion of this view.", "startOffset": 4, "endOffset": 7}, {"referenceID": 7, "context": "We present results on \u201churried copying\u201d with TFD [8] and MNIST [5], and on our detection/tracking tasks with synthetic video data.", "startOffset": 49, "endOffset": 52}, {"referenceID": 4, "context": "We present results on \u201churried copying\u201d with TFD [8] and MNIST [5], and on our detection/tracking tasks with synthetic video data.", "startOffset": 63, "endOffset": 66}], "year": 2015, "abstractText": "We investigate attention as the active pursuit of useful information. This contrasts with attention as a mechanism for the attenuation of irrelevant information. We also consider the role of short-term memory, whose use is critical to any model incapable of simultaneously perceiving all information on which its output depends. We present several simple synthetic tasks, which become considerably more interesting when we impose strong constraints on how a model can interact with its input, and on how long it can take to produce its output. We develop a model with a different structure from those seen in previous work, and we train it using stochastic variational inference with a learned proposal distribution.", "creator": "LaTeX with hyperref package"}}}