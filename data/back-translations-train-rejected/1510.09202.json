{"id": "1510.09202", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "30-Oct-2015", "title": "Generating Text with Deep Reinforcement Learning", "abstract": "We introduce a novel schema for sequence to sequence learning with a Deep Q-Network (DQN), which decodes the output sequence iteratively. The aim here is to enable the decoder to first tackle easier portions of the sequences, and then turn to cope with difficult parts. Specifically, in each iteration, an encoder-decoder Long Short-Term Memory (LSTM) network is employed to, from the input sequence, automatically create features to represent the internal states of and formulate a list of potential actions for the DQN. Take rephrasing a natural sentence as an example. This list can contain ranked potential words. Next, the DQN learns to make decision on which action (e.g., word) will be selected from the list to modify the current decoded sequence. The newly modified output sequence is subsequently used as the input to the DQN for the next decoding iteration. In each iteration, we also bias the reinforcement learning's attention to explore sequence portions which are previously difficult to be decoded. For evaluation, the proposed strategy was trained to decode ten thousands natural sentences. Our experiments indicate that, when compared to a left-to-right greedy beam search LSTM decoder, the proposed method performed competitively well when decoding sentences from the training set, but significantly outperformed the baseline when decoding unseen sentences, in terms of BLEU score obtained.", "histories": [["v1", "Fri, 30 Oct 2015 19:02:53 GMT  (79kb)", "http://arxiv.org/abs/1510.09202v1", "Accepted to the NIPS2015 Deep Reinforcement Learning Workshop"]], "COMMENTS": "Accepted to the NIPS2015 Deep Reinforcement Learning Workshop", "reviews": [], "SUBJECTS": "cs.CL cs.LG cs.NE", "authors": ["hongyu guo"], "accepted": false, "id": "1510.09202"}, "pdf": {"name": "1510.09202.pdf", "metadata": {"source": "CRF", "title": "Generating Text with Deep Reinforcement Learning", "authors": ["Hongyu Guo"], "emails": ["hongyu.guo@nrc-cnrc.gc.ca"], "sections": [{"heading": null, "text": "ar Xiv: 151 0.09 202v 1 [cs.C L] 30 Oct 2"}, {"heading": "1 Introduction", "text": "This year, it is more than ever in the history of the city, where it is so far that it is a place, where it is a place, where it is a place, where it is a place."}, {"heading": "2 Background", "text": "This model-free technique is used to learn optimal behavior. Q-Q is a commonly used framework for controlling learning processes through a computer algorithm called an agent that interacts with its environment. [1] There are a number of internal states in which a series of predefined actions A = a1,.. ak, the agent initiates an action by following certain guidelines or rules that lead to a new state and receive a reward. The agent's goal is to maximize some cumulative rewards through a sequence of actions. Each such action forms a transitional stamp (s, a, s,) of the Decision Process (MDP). Practically speaking, the environment is unknown or partially observed, and a sequence of state transition tuples can be used to formulate the environment.Q-Learning [34] is a popular form of RL learning processes."}, {"heading": "3 Generating Sequence with Deep Q-Network", "text": "We use an encoder decoder LSTM network, as shown in [30], to automatically generate informative attributes for a DQN, so that the DQN can learn a Q-value function to approximately achieve its long-term rewards; the learning algorithm is shown in Figure 1 and Algorithm 1."}, {"heading": "3.1 Generating State Representations with LSTMs", "text": "Encoder encoder encoder encoder encoder encoder encoder encoder encoder encoder encoder encoder encoder encoder encoder encoder encoder encoder encoder encoder encoder encoder encoder encoder encoder encoder encoder encoder encoder encoder encoder encoder encoder encoder encoder encoder encoder encoder encoder encoder encoder encoder encoder encoder encoder encoder encoder encoder encoder encoder encoder encoder encoder encoder encoder encoder encoder encoder encoder encoder encoder encoder encoder encoder encoder encoder encoder encoder encoder encoder encoder encoder encoder encoder encoder encoder encoder encoder encoder encoder encoder encoder encoder encoder encoder encoder encoder encoder encoder encoder encoder encoder encoder encoder encoder encoder encoder encoder encoder encoder encoder encoder encoder encoder encoder encoder encoder encoder encoder encoder encoder encoder encoder encoder encoder encoder encoder encoder encoder encoder encoder encoder encoder encoder encoder encoder encoder encoder encoder encoder encoder encoder encoder encoder encoder encoder encoder encoder encoder encoder encoder encoder encoder encoder encoder encoder encoder encoder encoder encoder encoder encoder encoder encoder encoder encoder encoder encoder encoder encoder encoder encoder encoder encoder encoder encoder encoder encoder encoder encoder"}, {"heading": "3.2 Iteratively Decoding Sequence with Deep Q-Network", "text": "The DQN considers the sentence pair, namely the EnSeni and DeSeni. (i.e., A, B, C > and < Ai, Bi, Ci >, respectively, in Figure 1) as its internal state. (i.e., A, B, B, 2) The list of hidden words in the DeLSTM is also fed into a neural network (represented as DQN in Figure 1 and graphically illustrated as DQN in Figure 1) (discussed further in Section 3.3). Learning these neural networks is DQN's current state, which contains the EnSeni and DeSeni, as well as the word probability list at each step of DeQM."}, {"heading": "3.3 Bidirectional LSTMs for DQN", "text": "When decoding, we want the DQN to receive information about the entire input sequence, i.e. < Ai, Bi, Ci > in Figure 1. To achieve this goal, we use a bi-directional LSTM [12]. Specifically, a bi-directional LSTM [12] allows the hidden states for a specific time step t of a given sequence to summarize the past and future of the time step t in the sequence. In each of these steps, the hidden state of the bidirectional LSTM is the concatenation of the forward and backward hidden state and is then transmitted to the same output layer, i.e. Equation 2 for the DQN is implemented as follows (shown in the left subfigure above) \u2212 \u2212 \u2192 (ht \u2212 W \u2192 h \u2192 b \u2212 forward)"}, {"heading": "3.4 BLEU Score for DQN Reward", "text": "The reward is calculated based on the proximity between the target sentence < y1, y2,..., yT > and the decrypted output sentence (i.e. DeSen) after the DQN has become active. We calculate the similarity of this sentence pair using the popular score metric in the statistical translation. Specifically, we get a BLEU [25] score between these two sentences. We measure the score difference between the current iteration and the previous iteration. If the difference is positive, a reward of + 1 is assigned; if it is negative, then -1 as a reward; otherwise it is zero. Since we are performing a sentence level comparison here, we adopt the smoothed version of BLEU [17]. Unlike the BLEU, the smoothed BLEU avoids a zero score, even if there are no 4 gram matches in the sentence pair."}, {"heading": "3.5 Empirical Observations on Model Design", "text": "This year it is more than ever before in the history of the city."}, {"heading": "4 Experiments", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.1 Task and Dataset", "text": "Our experimental task is to train a network to regenerate natural sentences, that is, when a sentence is given as input, the network first compresses it into a fixed vector, and then this vector is used to decrypt the input sentence. In other words, X and Y in algorithm 1 are the same. In our experiment, we randomly select 12,000 sentences with a maximum length of 30 from the billion-word corpus [7]. We train our model with 10,000 sentences and then select the best model with the validation data consisting of 1000 sentences. Then, we test our model with 1000 seen sentences and 1000 unseen sentences. The seen test sentence is randomly sampled from the training set."}, {"heading": "4.2 Training and Testing Detail", "text": "For this reason, we used a single-layer LSTM for the encoder decoder LSTM and the backward-facing LSTM in the DQN, both with 100 memory cells and 100 dimensional word embedding. We used a softmax over 10,000 words (that is the size of the word we used in the experiments) at each output (i.e., time step) of the DeLSTM. We initialized all LSTM parameters with the uniform distribution between -0.15 and + 0.15, including the word vectors. We used Adaptive Stochastic Gradient Descent (AdaSGD) without dynamics, with a starting learning rate of 0.05. Although LSTMs tend not to suffer from the disappearing gradient problem, we may have exploding gradients. Thus, we deal with the gradient clip technique [26] with a threshold of 15."}, {"heading": "4.3 Experimental Results", "text": "The development of the training for the state generation function StateGF and DQN are shown in Figure 2, and the most important test results are shown in Table 1. From the training curves for the two encoder decoders LSTMs and DQN as shown in Figure 2, we can see that both trainings converge very well. For the LSTM training, the average cost was steadily decreasing, and the average BLEU score gradually increased. Both curves then stabilized after 20 iterations. Also, the correct subfigure in Figure 2 indicates that the reward by the DQN was negative at the beginning of the training and then gradually shifted to the positive response zone."}, {"heading": "5 Related Work", "text": "Recently, it has been shown that the Deep Q Network (DQN) is able to play Atari games successfully [14, 21, 24]. Trained with a variant of Q-Learning [34], the DQN learns control strategies using deep neural networks. The basic idea is to use deep learning to automatically generate informative characteristics to depict the inner states of the environment in which the software agent lives, and then to learn a nonlinear control police function for the learning agent to take action. In addition to playing video games, the use of reinforcement learning to learn control strategies from text has also been studied. Applications include interpreting user manuals [6], navigating instructions [2, 16, 18, 33] and playing text-based games [5, 10, 23]. In addition, DQN has recently been used to learn memory access patterns and to rearrange a series of pre-defined words from the above [35]."}, {"heading": "6 Conclusion and Future Work", "text": "We use a Deep Q Network (DQN) to pursue an iterative decoding strategy for the sequence for sequence learning. To this end, we use an encoder decoder LSTM network to automatically approximate internal states and formulate potential actions for the DQN. In addition, we integrate an attention mechanism into the exploration strategy of reinforcement learning. This exploration allows the decoding network to intuitively learn from many synthetic sequence texts generated during the decoding phase. We evaluate the proposed method with a sentence regeneration task. Our experiments demonstrate the promising performance of our approach, especially in decoding invisible sentences in terms of the BLEU score achieved. This paper also presents several empirical observations in terms of model design to successfully decode sequence text with DQN."}], "references": [{"title": "High-level reinforcement learning in strategy games", "author": ["C. Amato", "G. Shani"], "venue": "AAMAS \u201910, pages 75\u201382", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2010}, {"title": "Weakly supervised learning of semantic parsers for mapping instructions to actions", "author": ["Y. Artzi", "L. Zettlemoyer"], "venue": "TACL, 1:49\u201362", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2013}, {"title": "Neural machine translation by jointly learning to align and translate", "author": ["D. Bahdanau", "K. Cho", "Y. Bengio"], "venue": "CoRR, abs/1409.0473", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2014}, {"title": "Learning to win by reading manuals in a monte-carlo framework", "author": ["S.R.K. Branavan", "D. Silver", "R. Barzilay"], "venue": "ACL \u201911, pages 268\u2013277", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2011}, {"title": "Learning to win by reading manuals in a monte-carlo framework", "author": ["S.R.K. Branavan", "D. Silver", "R. Barzilay"], "venue": "CoRR, abs/1401.5390", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2014}, {"title": "Reading between the lines: Learning to map highlevel instructions to commands", "author": ["S.R.K. Branavan", "L.S. Zettlemoyer", "R. Barzilay"], "venue": "ACL \u201910, pages 1268\u20131277, Stroudsburg, PA, USA", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2010}, {"title": "One billion word benchmark for measuring progress in statistical language modeling", "author": ["C. Chelba", "T. Mikolov", "M. Schuster", "Q. Ge", "T. Brants", "P. Koehn", "T. Robinson"], "venue": "INTERSPEECH 2014, pages 2635\u20132639", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2014}, {"title": "B", "author": ["K. Cho"], "venue": "van Merrienboer, \u00c7. G\u00fcl\u00e7ehre, F. Bougares, H. Schwenk, and Y. Bengio. Learning phrase representations using RNN encoder-decoder for statistical machine translation. CoRR, abs/1406.1078", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2014}, {"title": "Adaptive subgradient methods for online learning and stochastic optimization", "author": ["J. Duchi", "E. Hazan", "Y. Singer"], "venue": "J. Mach. Learn. Res.,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2011}, {"title": "Reading to learn: Constructing features from semantic abstracts", "author": ["J. Eisenstein", "J. Clarke", "D. Goldwasser", "D. Roth"], "venue": "EMNLP \u201909, pages 958\u2013967, Stroudsburg, PA, USA", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2009}, {"title": "Finding structure in time", "author": ["J.L. Elman"], "venue": "COGNITIVE SCIENCE, 14(2):179\u2013211", "citeRegEx": "11", "shortCiteRegEx": null, "year": 1990}, {"title": "Speech recognition with deep recurrent neural networks", "author": ["A. Graves", "A. Mohamed", "G.E. Hinton"], "venue": "CoRR, abs/1303.5778", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2013}, {"title": "DRAW: A recurrent neural network for image generation", "author": ["K. Gregor", "I. Danihelka", "A. Graves", "D. Wierstra"], "venue": "CoRR, abs/1502.04623", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2015}, {"title": "Deep recurrent q-learning for partially observable mdps", "author": ["M.J. Hausknecht", "P. Stone"], "venue": "CoRR, abs/1507.06527", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2015}, {"title": "Chapter 25 - serial order: A parallel distributed processing approach", "author": ["M.I. Jordan"], "venue": "J. W. Donahoe and V. P. Dorsel, editors, Neural-Network Models of CognitionBiobehavioral Foundations, volume 121 of Advances in Psychology, pages 471 \u2013 495. North-Holland", "citeRegEx": "15", "shortCiteRegEx": null, "year": 1997}, {"title": "Toward understanding natural language directions", "author": ["T. Kollar", "S. Tellex", "D. Roy", "N. Roy"], "venue": "Proceedings of the 5th ACM/IEEE International Conference on Human-robot Interaction, HRI \u201910, pages 259\u2013266, Piscataway, NJ, USA", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2010}, {"title": "Automatic evaluation of machine translation quality using longest common subsequence and skip-bigram statistics", "author": ["C.-Y. Lin", "F.J. Och"], "venue": "ACL \u201904", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2004}, {"title": "Learning to parse natural language commands to a robot control system", "author": ["C. Matuszek", "E. Herbst", "L. Zettlemoyer", "D. Fox"], "venue": "Experimental Robotics, pages 403\u2013415. Springer International Publishing", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2013}, {"title": "Recurrent neural network based language model", "author": ["T. Mikolov", "M. Karafi\u00e1t", "L. Burget", "J. Cernock\u1ef3", "S. Khudanpur"], "venue": "INTERSPEECH 2010, pages 1045\u20131048", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2010}, {"title": "Recurrent models of visual attention", "author": ["V. Mnih", "N. Heess", "A. Graves", "K. Kavukcuoglu"], "venue": "CoRR, abs/1406.6247", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2014}, {"title": "Human-level control through deep reinforcement learning", "author": ["V. Mnih", "K. Kavukcuoglu", "D. Silver", "A.A. Rusu", "J. Veness", "M.G. Bellemare", "A. Graves", "M. Riedmiller", "A.K. Fidjeland", "G. Ostrovski", "S. Petersen", "C. Beattie", "A. Sadik", "I. Antonoglou", "H. King", "D. Kumaran", "D. Wierstra", "S. Legg", "D. Hassabis"], "venue": "Nature, 518(7540):529\u2013533,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2015}, {"title": "Prioritized sweeping: Reinforcement learning with less data and less time", "author": ["A.W. Moore", "C.G. Atkeson"], "venue": "Machine Learning, pages 103\u2013130", "citeRegEx": "22", "shortCiteRegEx": null, "year": 1993}, {"title": "Language understanding for text-based games using deep reinforcement learning", "author": ["K. Narasimhan", "T. Kulkarni", "R. Barzilay"], "venue": "CoRR, abs/1506.08941", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2015}, {"title": "Action-conditional video prediction using deep networks in atari games", "author": ["J. Oh", "X. Guo", "H. Lee", "R.L. Lewis", "S.P. Singh"], "venue": "CoRR, abs/1507.08750", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2015}, {"title": "Bleu: A method for automatic evaluation of machine translation", "author": ["K. Papineni", "S. Roukos", "T. Ward", "W.-J. Zhu"], "venue": "ACL \u201902, pages 311\u2013318", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2002}, {"title": "On the difficulty of training recurrent neural networks", "author": ["R. Pascanu", "T. Mikolov", "Y. Bengio"], "venue": null, "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2013}, {"title": "Reinforcement learning of local shape in the game of go", "author": ["D. Silver", "R. Sutton", "M. M\u00fcller"], "venue": "IJCAI\u201907, pages 1053\u20131058, San Francisco, CA, USA", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2007}, {"title": "Weakly supervised memory networks", "author": ["S. Sukhbaatar", "A. Szlam", "J. Weston", "R. Fergus"], "venue": "CoRR, abs/1503.08895", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2015}, {"title": "Lstm neural networks for language modeling", "author": ["M. Sundermeyer", "R. Schl\u00fcter", "H. Ney"], "venue": "INTER- SPEECH, pages 194\u2013197", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2012}, {"title": "Sequence to sequence learning with neural networks", "author": ["I. Sutskever", "O. Vinyals", "Q.V. Le"], "venue": "CoRR, abs/1409.3215", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2014}, {"title": "Introduction to Reinforcement Learning", "author": ["R.S. Sutton", "A.G. Barto"], "venue": "MIT Press, Cambridge, MA, USA, 1st edition", "citeRegEx": "31", "shortCiteRegEx": null, "year": 1998}, {"title": "Show and tell: A neural image caption generator", "author": ["O. Vinyals", "A. Toshev", "S. Bengio", "D. Erhan"], "venue": "CoRR, abs/1411.4555", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2014}, {"title": "Learning to follow navigational directions", "author": ["A. Vogel", "D. Jurafsky"], "venue": "ACL \u201910, pages 806\u2013814, Stroudsburg, PA, USA", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2010}, {"title": "Q-learning", "author": ["C.J.C.H. Watkins", "P. Dayan"], "venue": "Machine Learning, pages 279\u2013292", "citeRegEx": "34", "shortCiteRegEx": null, "year": 1992}, {"title": "Reinforcement learning neural turing machines", "author": ["W. Zaremba", "I. Sutskever"], "venue": "CoRR, abs/1505.00521", "citeRegEx": "35", "shortCiteRegEx": null, "year": 2015}], "referenceMentions": [{"referenceID": 7, "context": "To address that, an encoder-decoder Long Short-Term Memory (LSTM) architecture has been recently shown to be very effective [8, 30].", "startOffset": 124, "endOffset": 131}, {"referenceID": 29, "context": "To address that, an encoder-decoder Long Short-Term Memory (LSTM) architecture has been recently shown to be very effective [8, 30].", "startOffset": 124, "endOffset": 131}, {"referenceID": 18, "context": "The decoding process is essentially a recurrent neural network language model [19, 29].", "startOffset": 78, "endOffset": 86}, {"referenceID": 28, "context": "The decoding process is essentially a recurrent neural network language model [19, 29].", "startOffset": 78, "endOffset": 86}, {"referenceID": 2, "context": "Inspired by the recent success of attention mechanisms [3, 13, 20, 28], we here also bias the reinforcement", "startOffset": 55, "endOffset": 70}, {"referenceID": 12, "context": "Inspired by the recent success of attention mechanisms [3, 13, 20, 28], we here also bias the reinforcement", "startOffset": 55, "endOffset": 70}, {"referenceID": 19, "context": "Inspired by the recent success of attention mechanisms [3, 13, 20, 28], we here also bias the reinforcement", "startOffset": 55, "endOffset": 70}, {"referenceID": 27, "context": "Inspired by the recent success of attention mechanisms [3, 13, 20, 28], we here also bias the reinforcement", "startOffset": 55, "endOffset": 70}, {"referenceID": 24, "context": "Our experimental studies indicate that the proposed method performed competitively well for decoding sentences from the training set, when compared to a left-to-right greedy beam search decoder with LSTMs, but significantly outperformed the baseline when decoding unseen sentences, in terms of BLEU [25] score obtained.", "startOffset": 299, "endOffset": 303}, {"referenceID": 0, "context": "Reinforcement Learning and Deep Q-Network Reinforcement Learning (RL) is a commonly used framework for learning control policies by a computer algorithm, the so-called agent, through interacting with its environment \u039e [1, 27].", "startOffset": 218, "endOffset": 225}, {"referenceID": 26, "context": "Reinforcement Learning and Deep Q-Network Reinforcement Learning (RL) is a commonly used framework for learning control policies by a computer algorithm, the so-called agent, through interacting with its environment \u039e [1, 27].", "startOffset": 218, "endOffset": 225}, {"referenceID": 33, "context": "Q-Learning [34] is a popular form of RL.", "startOffset": 11, "endOffset": 15}, {"referenceID": 3, "context": "The parameter \u03b8 is often learned by features generalized over the states and actions of the environment [4, 31].", "startOffset": 104, "endOffset": 111}, {"referenceID": 30, "context": "The parameter \u03b8 is often learned by features generalized over the states and actions of the environment [4, 31].", "startOffset": 104, "endOffset": 111}, {"referenceID": 20, "context": "[21] introduced the Deep Q-Network (DQN).", "startOffset": 0, "endOffset": 4}, {"referenceID": 30, "context": "The iterative updates are derived from the Bellman equation, where the expectation E is often computed over all transition tuples that involved the agent taking action a in state s [31]:", "startOffset": 181, "endOffset": 185}, {"referenceID": 20, "context": "For playing video games, one can infer state representations directly from raw pixels of screens using a convolutional network [21].", "startOffset": 127, "endOffset": 131}, {"referenceID": 7, "context": "These additional cells enable the LSTM to preserve state over long periods of time [8, 12, 30, 32].", "startOffset": 83, "endOffset": 98}, {"referenceID": 11, "context": "These additional cells enable the LSTM to preserve state over long periods of time [8, 12, 30, 32].", "startOffset": 83, "endOffset": 98}, {"referenceID": 29, "context": "These additional cells enable the LSTM to preserve state over long periods of time [8, 12, 30, 32].", "startOffset": 83, "endOffset": 98}, {"referenceID": 31, "context": "These additional cells enable the LSTM to preserve state over long periods of time [8, 12, 30, 32].", "startOffset": 83, "endOffset": 98}, {"referenceID": 29, "context": "We employ an encoder-decoder LSTM network, as presented in [30], to automatically generate informative features for a DQN, so that the DQN can learn a Q-value function to approximate its long term rewards.", "startOffset": 59, "endOffset": 63}, {"referenceID": 10, "context": "We here describe the commonly used Elman-type RNNs [11]; other variants such as Jordan-type RNNs [15] are also available in the community.", "startOffset": 51, "endOffset": 55}, {"referenceID": 14, "context": "We here describe the commonly used Elman-type RNNs [11]; other variants such as Jordan-type RNNs [15] are also available in the community.", "startOffset": 97, "endOffset": 101}, {"referenceID": 29, "context": "A straight forward and effective method for this decoding search, as suggested by [30], is to deploy a simple left-to-right beam search.", "startOffset": 82, "endOffset": 86}, {"referenceID": 29, "context": "As suggested by [30], a beam size of 1 works well.", "startOffset": 16, "endOffset": 20}, {"referenceID": 24, "context": ", yT > and the current decoded sentence DeSeni+1 is evaluated by a BLEU metric [25], which then assigns a reward ri to the action of selecting \u0177 t.", "startOffset": 79, "endOffset": 83}, {"referenceID": 30, "context": "That is, through following an \u01ebgreedy policy, the agent can perform a random action with probability \u01eb [31].", "startOffset": 103, "endOffset": 107}, {"referenceID": 2, "context": "Inspired by the recent success of attention mechanisms [3, 13, 20, 28], we here bias the reinforcement learning\u2019s attention to explore the sequence portions which are difficult to be decoded.", "startOffset": 55, "endOffset": 70}, {"referenceID": 12, "context": "Inspired by the recent success of attention mechanisms [3, 13, 20, 28], we here bias the reinforcement learning\u2019s attention to explore the sequence portions which are difficult to be decoded.", "startOffset": 55, "endOffset": 70}, {"referenceID": 19, "context": "Inspired by the recent success of attention mechanisms [3, 13, 20, 28], we here bias the reinforcement learning\u2019s attention to explore the sequence portions which are difficult to be decoded.", "startOffset": 55, "endOffset": 70}, {"referenceID": 27, "context": "Inspired by the recent success of attention mechanisms [3, 13, 20, 28], we here bias the reinforcement learning\u2019s attention to explore the sequence portions which are difficult to be decoded.", "startOffset": 55, "endOffset": 70}, {"referenceID": 11, "context": "To attain this goal, we deploy a bidirectional LSTMs [12].", "startOffset": 53, "endOffset": 57}, {"referenceID": 11, "context": "Specifically, for a specific time step t of a given sequence, a Bidirectional LSTM [12] enables the hidden states to summarize time step t\u2019s past and future in the sequence.", "startOffset": 83, "endOffset": 87}, {"referenceID": 24, "context": "Specifically, we obtain a BLEU [25] score between these two sentences.", "startOffset": 31, "endOffset": 35}, {"referenceID": 16, "context": "Note that, since we here conduct a sentence level comparison, we adopt the smoothed version of BLEU [17].", "startOffset": 100, "endOffset": 104}, {"referenceID": 21, "context": "For example, one can use the priority sampling of replay technique [22].", "startOffset": 67, "endOffset": 71}, {"referenceID": 6, "context": "In our experiment, we randomly select 12000 sentences, with max length of 30, from the Billion Word Corpus [7].", "startOffset": 107, "endOffset": 110}, {"referenceID": 8, "context": "We used Adaptive Stochastic Gradient Descent (AdaSGD) [9] without momentum, with a starting learning rate of 0.", "startOffset": 54, "endOffset": 57}, {"referenceID": 25, "context": "Thus we employ the gradient norm clip technique [26] with a threshold of 15.", "startOffset": 48, "endOffset": 52}, {"referenceID": 29, "context": "We compared our strategy with an encoder-decoder LSTM network used in [30] for machine translation.", "startOffset": 70, "endOffset": 74}, {"referenceID": 29, "context": "As suggested by [30], a beam size of 1 worked well.", "startOffset": 16, "endOffset": 20}, {"referenceID": 13, "context": "Recently, the Deep Q-Network (DQN) has been shown to be able to successfully play Atari games [14, 21, 24].", "startOffset": 94, "endOffset": 106}, {"referenceID": 20, "context": "Recently, the Deep Q-Network (DQN) has been shown to be able to successfully play Atari games [14, 21, 24].", "startOffset": 94, "endOffset": 106}, {"referenceID": 23, "context": "Recently, the Deep Q-Network (DQN) has been shown to be able to successfully play Atari games [14, 21, 24].", "startOffset": 94, "endOffset": 106}, {"referenceID": 33, "context": "Trained with a variant of Q-learning [34], the DQN learns control strategies using deep neural networks.", "startOffset": 37, "endOffset": 41}, {"referenceID": 5, "context": "Applications include interpreting user manuals [6], navigating directions [2, 16, 18, 33] and playing text-based games [5, 10, 23].", "startOffset": 47, "endOffset": 50}, {"referenceID": 1, "context": "Applications include interpreting user manuals [6], navigating directions [2, 16, 18, 33] and playing text-based games [5, 10, 23].", "startOffset": 74, "endOffset": 89}, {"referenceID": 15, "context": "Applications include interpreting user manuals [6], navigating directions [2, 16, 18, 33] and playing text-based games [5, 10, 23].", "startOffset": 74, "endOffset": 89}, {"referenceID": 17, "context": "Applications include interpreting user manuals [6], navigating directions [2, 16, 18, 33] and playing text-based games [5, 10, 23].", "startOffset": 74, "endOffset": 89}, {"referenceID": 32, "context": "Applications include interpreting user manuals [6], navigating directions [2, 16, 18, 33] and playing text-based games [5, 10, 23].", "startOffset": 74, "endOffset": 89}, {"referenceID": 4, "context": "Applications include interpreting user manuals [6], navigating directions [2, 16, 18, 33] and playing text-based games [5, 10, 23].", "startOffset": 119, "endOffset": 130}, {"referenceID": 9, "context": "Applications include interpreting user manuals [6], navigating directions [2, 16, 18, 33] and playing text-based games [5, 10, 23].", "startOffset": 119, "endOffset": 130}, {"referenceID": 22, "context": "Applications include interpreting user manuals [6], navigating directions [2, 16, 18, 33] and playing text-based games [5, 10, 23].", "startOffset": 119, "endOffset": 130}, {"referenceID": 34, "context": "Also, DQN has recently been employed to learn memory access patterns and rearrange a set of given words [35].", "startOffset": 104, "endOffset": 108}], "year": 2015, "abstractText": "We introduce a novel schema for sequence to sequence learning with a Deep QNetwork (DQN), which decodes the output sequence iteratively. The aim here is to enable the decoder to first tackle easier portions of the sequences, and then turn to cope with difficult parts. Specifically, in each iteration, an encoder-decoder Long Short-Term Memory (LSTM) network is employed to, from the input sequence, automatically create features to represent the internal states of and formulate a list of potential actions for the DQN. Take rephrasing a natural sentence as an example. This list can contain ranked potential words. Next, the DQN learns to make decision on which action (e.g., word) will be selected from the list to modify the current decoded sequence. The newly modified output sequence is subsequently used as the input to the DQN for the next decoding iteration. In each iteration, we also bias the reinforcement learning\u2019s attention to explore sequence portions which are previously difficult to be decoded. For evaluation, the proposed strategy was trained to decode ten thousands natural sentences. Our experiments indicate that, when compared to a left-to-right greedy beam search LSTM decoder, the proposed method performed competitively well when decoding sentences from the training set, but significantly outperformed the baseline when decoding unseen sentences, in terms of BLEU score obtained.", "creator": "LaTeX with hyperref package"}}}