{"id": "1511.00043", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "30-Oct-2015", "title": "Learning Adversary Behavior in Security Games: A PAC Model Perspective", "abstract": "Recent applications of Stackelberg Security Games (SSG), from green crime to urban crime, have employed machine learning tools to learn and predict adversary behavior using available data about defender-adversary interaction.We commit to an approach of directly learning the response function of the adversary and initiate a formal study of the learnability guarantees of the approach. We make three main contributions: (1) we formulate our approach theoretically in the PAC learning framework, (2) we analyze the PAC learnability of the known parametric SUQR model of bounded rationality and (3) we propose the first non-parametric class of response functions for SSGs and analyze its PAC learnability. Finally, we conduct experiments and report the real world performance of the learning methods.", "histories": [["v1", "Fri, 30 Oct 2015 22:27:25 GMT  (4146kb,D)", "http://arxiv.org/abs/1511.00043v1", null], ["v2", "Wed, 18 Nov 2015 17:51:17 GMT  (3598kb,D)", "http://arxiv.org/abs/1511.00043v2", null], ["v3", "Fri, 20 Nov 2015 08:34:43 GMT  (2999kb,D)", "http://arxiv.org/abs/1511.00043v3", null]], "reviews": [], "SUBJECTS": "cs.AI cs.GT cs.LG", "authors": ["arunesh sinha", "debarun kar", "milind tambe"], "accepted": false, "id": "1511.00043"}, "pdf": {"name": "1511.00043.pdf", "metadata": {"source": "CRF", "title": "Learning Adversary Behavior in Security Games: A PAC Model Perspective", "authors": [], "emails": [], "sections": [{"heading": "1 Introduction", "text": "This year, it has reached the stage where it will be able to take the lead."}, {"heading": "2 SSG Preliminaries", "text": "This section presents the background and preliminary notations for SSGs. A summary of the notations used in this paper is given in Table 1. A SSG is a two-player game between a defender (leader) and an opponent (follower) (Paruchuri et al. 2008). The defender wants to protect T targets with a limited number of security resources K < T). For simplicity, we will limit ourselves to the scenario without time limitations (see Korzhyk et al. (2010). The defender's pure strategy is to allocate each resource to a target."}, {"heading": "3 Learning Framework for SSG", "text": "First, we present some notations: given two n-dimensional points o and o \u00b2, the lp distance dlp = > Y = > distance dlp between the two points is: dlp (o, o \u2032) = | o-o \u2032 | p = (\u2211 n i = 1 | oi \u2212 o \"p\" p \"p\" p \"p\" p \"p.\" Also dl \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" n. \"KL refers to the Kullback-Leibler divergence. We use Haussler's (1992) learning framework, which includes an instance space X\" and result space X \"ltx\" x \"p\" p. \"In our context, the space of defenders is mixed strategies x.\" Outcome space Y is defined as the space of all possible categorical decisions on a number of T targets (i.e., choice of target) for the opponent: ti = & lt."}, {"heading": "4 Sample Complexity", "text": "In this section, we derive the example complexity for the parametric and NPL cases (< K = > FS = > FS = = =) < K (K) K (K) K (K) K (K) K (K) K (K) K (K) K (K) K (K) K (K) K (K) K (K) K (K) K (K) K (K) K (K) K (K) K (K) K (K) K (K) K) K (K) K (K) K (K) K (K) K (K) K (K) K (K) K (K) K (K) K) K (K) K (K) K) K (K) K) K (K) K (K) K (K) K (K) K (K) K) K (K) K) K (K) K) K (K) K) K (K) K (K) K) K (K) K (K) K) K (K) K (K) K) K (K) K (K) K) K (K) K (K) K) K (K) K) K (K) K (K) K) K (K) K) K (K) K) K (K) K (K) K) K (K) K (K) K) K (K) K) K (K) K) K (K) K) K (K) K (K) K) K (K) K (K) K) K (K) K (K) K (K) K) K (K) K) K (K) K (K) K (K) K (K) K (K) K (K) K) K (K (K) K) K (K (K) K (K) K (K) K (K) K) K (S S S (K) K (K (K) K (K) K (K) K (K) K (K) K (K) K (K) K (K) K (K) K (K (K) K) K (K) K (K) K (K) K (K (K) K (K) K) K (K"}, {"heading": "5 Empirical Risk Minimizer", "text": "As already mentioned, our loss function has been designed so that empirical risk minimization is the same as maximizing the probability of data. In fact, in the parametric case, the standard MLE approach can be used as it is and has been used in the literature. However, in the first step, we estimate the most likely value for hi (x) (for each i) for each x in the training data, ensuring that for each x, x \"in the training data an x\" x \"-\" i \"-\" i \"-\" i \"-\" i \"-\" i \"-\" -. \"-\" - \"-.\" - \"-\" - \"-\" - \"i\" - \"-.\" - \"- - - -.\" - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -"}, {"heading": "6 Utility Bounds", "text": "Next, we have the difference between the optimal benefit and the benefit derived from planning, using the learned h. The usage binding is the same for the parametric and the NPL case. Since the PAC Learning Guarantee only limits the risk between h and h to correlate qp and qh, the problem goes below a limited KL divergence between qh and qp. Lemma 6. Suppose E [KL (x) | qh."}, {"heading": "7 Experimental Results", "text": "We show experimental results on two sets of data: (i) real poaching data from QENP (obtained from (Nguyen et al. 2015); (ii) data from human subjects to AMT (obtained from (Kar et al. 2015) to estimate actual errors and the amount of data required to reduce the error for both the parametric and NPL learning settings; and we compare the NPL approach with the standard SUQR approach and show that the NPL approach is carried out while computationally slow attacks on SUQR are carried out for Uganda data; we compare with SUQR as it is widely used in literature; for completeness we also have a SUQR model in application for which the performance between the NPL and standard SUQR.For each dataset we perform four experiments with 25%, 75% and 100% of the original data."}, {"heading": "8 Conclusion", "text": "We presented a PAC analysis of the SUQR model and the first non-parametric learning model for SSGs with a PAC analysis of the same. Our experiments with poaching data and simulated data show that NPL surpasses SUQR with increasing data, but in mathematical terms 5In the appendix we show that \u03b1 is actually approaching zero on simulated data and exceeds NPL SUQR with enough samples. Finally, we hope that we have created fertile ground for interesting future research."}, {"heading": "A Proofs", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "Proof of Theorem 1", "text": "The proof: First, Haussler uses the following pseudo measurement variable on A, which using the loss function l = 76,000 (a, b) = 76,000 (a, b) = 76,000 (a) \u2212 \u2212 l (y, p) \u2212 l (y, b) \u2212. To start with Haussler's result, we show Pr (a) h (~ z) \u2212 rh (p) | < \u03b13) 1 \u2212 4C (\u03b1 48, H, p) e \u2212 2m 576M2If we choose \u03b1 = \u03b1 / 4M and \u03bd = 2M in Theorem 9 of (Haussler 1992) \u2212 rh (3) (Section 2.2, (Haussler 1992)) of dv, we get \u2212 s \u2212 s whenever dv (r, s) \u2264 a). Using this directly in Theorem 9 of Haussler (1992), we get the desired result above it."}, {"heading": "Proof of Lemma 1", "text": "The proof. Attach any probability distribution over X, say P. For the abbreviation, we type k instead of T \u2212 1. Consider a / 2 coverage Ui for each Fi; also let V have a / 2 coverage for G. We claim that U1 \u00b7,. Uk \u00b7 V is a coverage for G \u00b7 iFi. Thus, the size of coverage for G \u00b2 \u00b7 iFi is limited by | V | i \u2032. With this claim, we get N (, G \u00b2 iFi, dL1 (P, dl \u00b2 1)) < N (/ 2, G, dL1 (P, dl \u00b2 1). < k i = 1N (/ 2, Fi, dL1 (P, dl1))))) Assuming the probability distributions, we get our desired result. Now we prove our assertion about coverage. Let's take any function h = & ltfi = < g1 + fk + fk >."}, {"heading": "Proof of Lemma 2", "text": "First, prove that xiT = xi \u2212 xT lies between [\u2212 1, 1] due to the constraints on xi, xT. Then we have the following result for any two functions g, g \u00b2 G: dL1 (P, dl \u00b2 1) (g, g \u00b2) = x 1 T \u2212 1 T \u2212 1 T = 1 dl1 (w (xi \u2212 xT), w \u00b2 (xi \u2212 xT)))) dP (x) = x 1 T \u2212 1 x (w \u2212 w \u00b2) (xi \u2212 xT) (xi \u2212 xT) | dP (x) \u2264 X 1 T \u2212 1 T \u00b2 i = 1 (w \u2212 w \u00b2) | dP (x) = (w \u2212 w \u00b2) | (w \u2212 w \u00b2) | (w \u2212 w \u00b2) | In addition, note that the range of any g = w (xi \u2212 xT) | dP (x) \u2264 X 1 T \u2212 1 x \u00b2 i = 1 (w \u00b2) | dP (x) = (w \u2212 w \u00b2) | argument is that the range of an arbitrary g = xi w \u2212 xT (xxi) \u2212 T \u2212 \u2212 \u2212 T."}, {"heading": "Proof of Lemma 3", "text": "Proof. First, the space of the functionsH = {h / K, Hi} Lipschitz with constants \u2264 1 and | hi (x) | \u2264 M / 2K. Clearly N (, Hi, dl) \u2264 N (/ K, H, dl). The following result is (Tikhomirov and Kolmogorov 1993): for each Lipschitz real functional space H with constants 1, each positive integer s and each distance d N (, H, dl) \u2264 (2 M (s + 1) 2K + 1) \u00b7 (s + 1) N (s + 1, X, d) Then we get the limit to N (/ K, H, dl) by selecting s = 1 and d = dl \u00b2 and thus getting the desired limit to N (, Hi, dl)."}, {"heading": "Proof of Lemma 4", "text": "Proof. To simplify the notation, let us perform the proof with k for K + 1. Let us leave Yi = Ui \u2212 0.5, then | Y | \u2264 1 / 2 and ST \u2212 0.5T = \u2211 i Yi. Let us use Amber's inequality with the fact that E [Y 2i] = 1 / 12P (\u2211 i Yi = ST \u2212 0.5T \u2264 \u2212 t) \u2264 e \u2212 0.5t2 T / 12 + t / 6Thus P (ST \u2264 0.5T \u2212 t) \u2264 e 0.5t2T / 12 + t / 6. Let us take k = 0.5T \u2212 t and thus t = 0.5T \u2212 k = T (0.5 \u2212 k / T). This results in P (ST \u2264 k) \u2264 e \u2212 3T (0.5 \u2212 k / T) 2 1 \u2212 k / T"}, {"heading": "Proof of Theorem 3", "text": "Considering the results of Lemma 3, we get the sample complexity of hi | hi | j | | | j | | | | | | | | | | Therefore, we would like to point out that if K / T is a constant, then the O (e \u2212 T) in Lemma 4 is also swamped with the K-T term. In practice, however, this term for fixed T delivers a lower actual complexity than what is specified by the Order.Proof of Lemma 5Proof. Note that any solution for MinLip will have a Lipschitz constant of K. Therefore, it is sufficient to show that the Lipschitz constant of hi is a K to prove that hi is a solution of MinLip. Let's take two x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x."}, {"heading": "Proof of Lemma 6", "text": "Prove that pX is the limit of p (x, y) for room X. Define the expected entropy E [H (x)] = \u00b2 pX (x) \u2211 T i = 1 Iy = tiq p i (x) dx dy. This is the same as \u2212 x) log q p (x) p i = 1 Iy = \u2212 p (x, y) \u2211 T i = 1 Iy = ti log q h (x) dx dy. This is the same as \u2212 x) log q p i (x) p i = 1 Iy = tiq p i (x) p (x) p = ti log q h i (x) \u2212 p (x). This is reduced to \u2212 pX (x): p i = 1 Iy = tiq p (x) p (x) q h (x) qh (x)."}, {"heading": "Proof of Lemma 7", "text": "We know that the x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x"}, {"heading": "Proof of Theorem 4", "text": "x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x"}, {"heading": "B Extension to Stackelberg Games", "text": "Our technique extends to Stackelberg games by pointing out that the individual case K = 1 with the objectives T \u2212 1 results in the probability of playing an action. With this constellation, the security game is now a standard Stackelberg game, but where the leader has T actions and the successor has T \u2212 1 actions. So to grasp the general Stakelberg game, we assume for the opponent N actions (instead of T \u2212 1 above). Then, similar to security games q1, qN denotes the probability of the opponent to play an action. Therefore, the function h now outputs vectors of size N \u2212 1 (instead of O (T)), i.e., A is a subset of N \u2212 1 dimensional Euclidean space. The model of the game within the framework of the PSD results in both technology and safety N \u2212 1."}, {"heading": "C Analysis of Standard SUQR form", "text": "For SUQR, the rewards and penalties are paid and fixed. < r = < r1,., rT > (each), rmax, rmax > 0), and the penalties are p = < p1,., pT > (each), pT > (1, pmin], pmin < 0), the performance of h (x) = < w1x1T + w2r1T + w2r1T,., w1xT \u2212 1T \u2212 w2rT \u2212 1T \u2212 1T >, where riT = ri \u2212 rT and the same for piT \u2212 w2x1T + w2r1T G. (x), we can consider the same weights."}, {"heading": "D Experimental Results", "text": "Here we provide additional experimental results on Uganda, AMT and simulated data sets. First, in Fig. 2 (a) and 2 (b) we provide results on Uganda data set for the general parametric case we are analyzing in our work (i.e., generalized SUQR data) for both fine-grained and coarse-grained settings. The AMT data set consists of 32 unique mixed strategies, 16 of which were used for one payout structure and the remaining 16 for another. In the main paper we provided results on AMT data for payout structure 1. Here we show results on AMT data in Fig. 2 (c) and 2 (d), 16 of which are used for parametric (SUQR) and the remaining 16 for another."}], "references": [], "referenceMentions": [], "year": 2017, "abstractText": "Recent applications of Stackelberg Security Games (SSG), from green crime to urban crime, have employed machine learning tools to learn and predict adversary behavior using available data about defender-adversary interaction. We commit to an approach of directly learning the response function of the adversary and initiate a formal study of the learnability guarantees of the approach. We make three main contributions: (1) we formulate our approach theoretically in the PAC learning framework, (2) we analyze the PAC learnability of the known parametric SUQR model of bounded rationality and (3) we propose the first non-parametric class of response functions for SSGs and analyze its PAC learnability. Finally, we conduct experiments and report the real world performance of the learning methods.", "creator": "LaTeX with hyperref package"}}}