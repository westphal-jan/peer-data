{"id": "1511.00215", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "1-Nov-2015", "title": "A Unified Tagging Solution: Bidirectional LSTM Recurrent Neural Network with Word Embedding", "abstract": "Bidirectional Long Short-Term Memory Recurrent Neural Network (BLSTM-RNN) has been shown to be very effective for modeling and predicting sequential data, e.g. speech utterances or handwritten documents. In this study, we propose to use BLSTM-RNN for a unified tagging solution that can be applied to various tagging tasks including part-of-speech tagging, chunking and named entity recognition. Instead of exploiting specific features carefully optimized for each task, our solution only uses one set of task-independent features and internal representations learnt from unlabeled text for all tasks.Requiring no task specific knowledge or sophisticated feature engineering, our approach gets nearly state-of-the-art performance in all these three tagging tasks.", "histories": [["v1", "Sun, 1 Nov 2015 07:59:48 GMT  (474kb)", "http://arxiv.org/abs/1511.00215v1", "Rejected by EMNLP 2015, score: 4,3,3 (full is 5)"]], "COMMENTS": "Rejected by EMNLP 2015, score: 4,3,3 (full is 5)", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["peilu wang", "yao qian", "frank k soong", "lei he", "hai zhao"], "accepted": false, "id": "1511.00215"}, "pdf": {"name": "1511.00215.pdf", "metadata": {"source": "CRF", "title": "A Unified Tagging Solution: Bidirectional LSTM Recurrent Neural Network with Word Embedding", "authors": ["Peilu Wang", "Yao Qian", "Frank K. Soong", "Lei He", "Hai Zhao"], "emails": ["helei}@microsoft.com,", "zhaohai@cs.sjtu.edu.cn,", "yqian@ets.org"], "sections": [{"heading": null, "text": "ar Xiv: 151 1.00 215v 1 [cs.C L] 1N ov2 01"}, {"heading": "1 Introduction", "text": "In fact, it is a very strange but unpredictable development that has developed in recent years, \"he said in an interview with the New York Times:\" It is not as if we are able to sow seeds to change the world. \""}, {"heading": "2 Bidirectional LSTM Architecture", "text": "The recurrent neural world (RNN) is a kind of artificial neural network that can be accessed because it is a system, in which it is a system, in which it is a system, in which it is a system, in which it is a system, in which it is a system, in which it is a structure, in which it is two levels (e.g. Wxhxt and hidden layers), in which it is a vector of the hidden states, in which two layers are connected (e.g. Wxhxt is the layer between input and hidden layer), in which it is bias vector of the hidden layer of the activation function of the hidden layer. Note that it gives information from previous steps, and thus can use all the input history."}, {"heading": "3 Tagging System", "text": "The schematic diagram of the BLSTM-RNN-based tagging system is shown in Figure 3. Givena sentence w1, w2,..., wn with the tags y1, y2,..., yn, BLSTM-RNN is first used to predict the tag probability distribution o (wi) of each word, then a decryption algorithm is proposed to generate the final predicted tags y \u2032 1, y \u2032 2,..., y \u2032 n."}, {"heading": "3.1 BLSTM-RNN for tagging", "text": "The use of BLSTM RNN is illustrated in Figure 4. Here, wi is the only hot representation of the current word, which is a binary vector with the dimension | V |, where V is the vocabulary. To decrease | V |, each letter of the input word is transferred to its lowercase. uppercase information is maintained by introducing a three-dimensional binary vector f (wi) to indicate whether wi is a complete lowercase, complete uppercase or leading word. The input vector Ii of the network is calculated as: Ii = W1wi + W2f (wi), where W1 and W2 are weight matrices that connect two levels. W1wi is also known as word embedding of wi, which is a real vector with a much smaller dimension than wi. In practice, W1wi is implemented as a reference table to reduce the arithmetic cost of the word that is embedded in the W1wi."}, {"heading": "3.2 Decoding", "text": "According to BLSTM-RNN, the obtained probability distribution of each step is independent of each other. However, in order to use this type of labeling constraints, we introduce a transition matrix A between the output of each step, as shown in Figure 5. Each circle represents the day probability predicted by BLSTM-RNN. Aij stores the point number of the transition from day ti to tj. The point number is determined very easily that if tj appears in the training corpus immediately after ti, Aij is 1, otherwise 0. It implies that tag bigrams that do not appear in the training corpus are invalid and would not appear in the test case, regardless of whether they are actually valid. The point number of a set w1, w2,..., wn ([w] n1 for short) along a path of day yn, y2,..., yn (n) (for the transition from BLY = 1 MRY = 1 (MRY = 1) = 1 (1) (1 = 1)."}, {"heading": "4 Word Embedding", "text": "As a neural network, BLSTM-RNN can easily take over pre-formed word embeddings by initializing W1 (shown in Figure 4) with these external embeddings. Currently, many word embeddings trained on very large corpora are available online. To address this deficiency, we also propose a new method to train with BLSTM-RNN word embeddings on unmarked data. In this method, BLSTM-RNN is used to perform a tagging task with only two types of tags to predict: wrong / correct. Input is a sequence of words that is a normal sentence in which some words are replaced by randomly selected words from the vocabulary. Words that are replaced are selected from the sentence. In these replaced words, the neural words (incorrect / correct) are also replaced by words that are considered erroneous in the sample."}, {"heading": "5 Experiments", "text": "All our approaches are based on CURRENT (Weninger et al., 2014), an open source GPU-based toolkit from BLSTM-RNN. To build and train the neural network, we follow the default setting of CURRENT: the input layer and hidden layer activation functions are logistical functions, while the output layer uses Softmax functions for multiclassification. Neural networks are trained using statistical gradient descend algorithms with constant learning rate. In all experiments, consecutive digits are offset within a word with the symbol \"#.\" For example, both the words \"Tel192\" and \"Tel6\" are converted to \"Tel #.\" The vocabulary we use is the most common 100,000 words in the North American news corpus (Graff, 2008), plus a single \"UNK\" symbol to replace all words from the vocabulary."}, {"heading": "5.1 Tasks", "text": "In this section we briefly introduce three typical tagging tasks and their experimental structure, by means of which we evaluate the performance of the proposed approach: Part-of-speech tagging (POS), chunking (CHUNK) and named entity recognition (NER).POS is the task of labeling each word with its part of the speech, e.g. nouns, verbs, adjectives, etc. Our POS tagging experiment is performed using data from the Wall Street Journal from Penn Treebank III (Marcus et al., 1993). Training, development and test sentences are divided by setup in (Collins, 2002). Table 1 lists statistical information of the three datasets. Performance is divided into phrases containing syntactically related words, such as the noun phrase NIOL (NP), the verb phrase Nousse (Nousse), the Nousse (Nousse) To identify the phrase Nousse, we use a phrase Nousse (Nousse), the Nousse (Nousse), the Nousse (Nousse), the Nousse (Nousse), the phrase we use frequently."}, {"heading": "5.2 Network Structure", "text": "In all experiments, without a specific description, the size of the input layer is set to 100 and the size of the output layer is set as the number of mark types according to the specific marking task. In this experiment, we evaluate different sizes of the hidden layer in BLSTM-RNN to achieve the best size for later experiments.Performances for three tasks are shown in Figure 7. It shows that the hidden layer size has a limited impact on performance if it is large enough. To maintain a good compromise between accuracy, model size and training time, we select 100, which is the smallest layer size to achieve \"adequate\" performance as a hidden layer size in all subsequent experiments.In addition, we also evaluate the deep structure that uses multiple BLSTM layers. It has been reported that this deep BLSTM achieves significantly better performance than a single layer BLSTM in various applications, such as speech synthesis (Fan et al., 2014 Fernanet al., 2014), speech recognition (BB), speech recognition (BB) and STM only (BLM) are sufficient."}, {"heading": "5.3 Decoder", "text": "In this experiment, we test the effect of decoders. The performance of BLSTM tagging systems without and with decoders is shown in Table 6. Without decoders, the predicted tag y \u00b2 is determined by directly selecting the tag with the highest probability of network output o (wi). Results show that decoders significantly improve the performance of CHUNK and NER tasks, but no help for POS. We offer a possible explanation for this difference in improvement. In tasks such as CHUNK and NER that use the IOBES tagging scheme, tags are highly dependent on their previous tags. For example, I-X can only have 42 tag types behind B-X. CHUNK task that can combine 42 x 42 x 42 x 42 = 1764 tag bigrams, but only 252 (14.3%) of them actually appear in the training body. In NER task, 78 (27.0%) of a total of 289 day bigrams have occurred more than once."}, {"heading": "5.4 Word Embedding", "text": "In this experiment, we evaluate BLSTM-RNN tagging approach with different word embeddings derived from the proposed approach in Section 4 as well as four types of embeddings.To trade word embeddings using our approach, we are able to construct the unfilled data that are able to assess the generated information that are able to be able to be able to be able to be able to be able to be able to be able to be able to be."}, {"heading": "5.5 Comparison with Previous Systems", "text": "In this section we compare our approach with previous state-of-the-art systems of POS, CHUNK and NER tasks. Table 7 lists the related work of these three tasks. BLSTM represents the BLSTMRNN tagging approach using BLSTMWE (all) + (Collobert, 2011) word embedding. POS: (Huang et al., 2012) reports the highest accuracy of WSJ test sets (97.35%). In addition (Moore, 2014) (Shen et al., 2007) (97.33%) also achieves accuracy of over 97.3%. These three systems are considered state-of-the-art systems in POS tagging. (Toutanova et al., 2003) is one of the most commonly used approaches, also known as Stanford taggers. All of these methods use rich morphological features proposed in Ratnaparkhi, 1996."}, {"heading": "6 Related Works", "text": "(Collobert et al., 2011) is the most similar work to ours. However, it is a unified tagging solution based on neural networks that also uses simple task-independent features and word embeddings learned from unlabeled text. The main difference is that (Collobert et al., 2011) instead of BLSTM-RNN, the uni-certified tagging system of (Collobert et al., 2011) and NN + WE is this system that uses word embeddings trained by their approach. Without word embeddings, BLSTM NN outperforms NN in all three tasks. It is consistent with observations in previous work that BLSTM-RNN is a more powerful model for sequential labeling than word embeddings trained by their approach."}, {"heading": "7 Conclusions", "text": "In this article, we propose a unified tagging solution based on BLSTM-RNN. This system avoids task-specific features, but uses text embedding that is automatically learned from unlabeled text. Without relying on feature engineering or prior knowledge, this approach can easily be applied to different tagging tasks. Experiments are performed on three typical tagging tasks: POS tagging, chunking, and named entity recognition. With simple task-independent input functions, our approach achieves nearly state-of-the-art results for all three tasks. Our results suggest that BLSTM-RNN with word embedding is an effective unified tagging solution that is worth exploring further."}], "references": [], "referenceMentions": [], "year": 2015, "abstractText": "Bidirectional Long Short-Term Memory Recurrent Neural Network (BLSTMRNN) has been shown to be very effective for modeling and predicting sequential data, e.g. speech utterances or handwritten documents. In this study, we propose to use BLSTM-RNN for a unified tagging solution that can be applied to various tagging tasks including partof-speech tagging, chunking and named entity recognition. Instead of exploiting specific features carefully optimized for each task, our solution only uses one set of task-independent features and internal representations learnt from unlabeled text for all tasks. Requiring no task specific knowledge or sophisticated feature engineering, our approach gets nearly state-ofthe-art performance in all these three tagging tasks.", "creator": "dvips(k) 5.991 Copyright 2011 Radical Eye Software"}}}