{"id": "1511.00360", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "2-Nov-2015", "title": "Automatic Prosody Prediction for Chinese Speech Synthesis using BLSTM-RNN and Embedding Features", "abstract": "Prosody affects the naturalness and intelligibility of speech. However, automatic prosody prediction from text for Chinese speech synthesis is still a great challenge and the traditional conditional random fields (CRF) based method always heavily relies on feature engineering. In this paper, we propose to use neural networks to predict prosodic boundary labels directly from Chinese characters without any feature engineering. Experimental results show that stacking feed-forward and bidirectional long short-term memory (BLSTM) recurrent network layers achieves superior performance over the CRF-based method. The embedding features learned from raw text further enhance the performance.", "histories": [["v1", "Mon, 2 Nov 2015 02:34:12 GMT  (657kb,D)", "http://arxiv.org/abs/1511.00360v1", "5 pages, 4 figures, ASRU 2015"]], "COMMENTS": "5 pages, 4 figures, ASRU 2015", "reviews": [], "SUBJECTS": "cs.CL cs.SD", "authors": ["chuang ding", "lei xie", "jie yan", "weini zhang", "yang liu"], "accepted": false, "id": "1511.00360"}, "pdf": {"name": "1511.00360.pdf", "metadata": {"source": "CRF", "title": "AUTOMATIC PROSODY PREDICTION FOR CHINESE SPEECH SYNTHESIS USING BLSTM-RNN AND EMBEDDING FEATURES", "authors": ["Chuang Ding", "Lei Xie", "Jie Yan", "Weini Zhang", "Yang Liu"], "emails": ["yangliu}@nwpu-aslp.org"], "sections": [{"heading": null, "text": "Index terms - automatic prosody prediction, speech synthesis, neural network, BLSTM, embedding functions"}, {"heading": "1. INTRODUCTION", "text": "This year, it is as far as ever in the history of the city, where it is as far as never before."}, {"heading": "2. THE PROPOSED APPROACH", "text": "In order to make predictive models more independent from feature engineering, we use a variant of the architecture of neural networks proposed by [16] for the probabilistic language model. This architecture was then used for CWS and POS tagging [12]. As shown in Fig. 2, the architecture takes raw text as input and maps each Chinese character into a basic feature vector. The following layers are two types of neural networks, FFNN and BLSTM-RNN, which recognize several layers of feature representations from the basic feature vectors. The output layer is a graph through which tag inference is achieved by the Viterbi algorithm."}, {"heading": "2.1. Feature Vectors", "text": "Typically, a character dictionary D of size | D | is extracted from the training set and assigns unknown characters to a special symbol that is not used elsewhere. Each Chinese character can typically be represented by a uniform vector whose size is | D |, and all dimensions are marked with 0, except for the position of the character in D, which is marked as 1. However, the uniform high-dimensional representation is assumed not to model the semantic similarity between the ideographic characters. In contrast, the distributed representation or embedding function in the form of a low-dimensional, continuously valuable vector, which was learned in a completely unattended manner using neural networks, carries important syntactical and semantic information [17]."}, {"heading": "2.2. Network Structures and Training", "text": "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _"}, {"heading": "2.3. Tag Inference", "text": "In order to model the tag dependence and derive the tag sequence globally, a transitional ScoreSab is introduced for a set of tags G = {B, NB, O} in order to jump from day a-G to day b-G. For the input string of a sentence c [1: T] with a tag sequence tag [1: T], a sentence score is then given by the sum of the transition and network scores [12, 23]: l (c [1: T], day [1: T], \u03b8) = T-t = 1 (Stopped \u2212 1tagt + f\u03b8 (tagt | ct), where f\u0442 (tagt | ct) specifies the score output for day on the t-th character through the networks. In view of a sentence c [1: T], we can find the best tag path tag [1: T] by maximizing the sentence score: tag [1: T] max arg = arg."}, {"heading": "3. EXPERIMENTS", "text": "This year, it is only a matter of time before there is a result in which there is a result."}, {"heading": "4. CONCLUSION AND FUTURE WORK", "text": "In this paper, we propose to use neural network architectures to predict prosodic boundary markers directly from Chinese characters without feature engineering. We show that stacking feed-forward and bi-directional short-term memory (BLSTM) recurring layers achieves superior performance. We obtain useful character embedding capabilities from raw text. Both objective and subjective assessments show that the proposed neural network authorization achieves superior performance over the CRF-based approach and the use of embedding capabilities can further enhance performance. It is promising for future work to predict PW, PH and IPH labels in a unified neural network, and features of embedding n-gram characters can be further investigated."}, {"heading": "5. ACKNOWLEDGEMENTS", "text": "This work was supported by the National Natural Science Foundation of China (61175018 and 61571363)."}, {"heading": "6. REFERENCES", "text": "[1] Yao Qian, Zhizheng Bangalore, \"Xuezhe Ma, and Frank Soong,\" Automatic prosody prediction and detection with conditional random field (crf) models, \"in Proceedings of ISCSLP, 2010, pp. 135-138. [2] Jingwei Sun, Jing Yang, Jianping Zhang, and Yonghong Yan,\" Chinese prosody structure prediction based on conditional random fields, \"in Proceedings of ICNC, 2009, vol. 3, pp. 602-606. [3] Je Hun Jeon and Yang Liu,\" Automatic prosodic events using syllable-based acoustic features, \"in Proceedings of ICASSP, 2009, pp. 4565-4568. [4] Vivek Rangarajan, Shrikanth Narayanan, and Srinikanth Bangalore."}], "references": [{"title": "Automatic prosody prediction and detection with conditional random field (crf) models", "author": ["Yao Qian", "Zhizheng Wu", "Xuezhe Ma", "Frank Soong"], "venue": "Proceedings of ISCSLP, 2010, pp. 135\u2013138.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2010}, {"title": "Chinese prosody structure prediction based on conditional random fields", "author": ["Jingwei Sun", "Jing Yang", "Jianping Zhang", "Yonghong Yan"], "venue": "Proceedings of ICNC, 2009, vol. 3, pp. 602\u2013606.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2009}, {"title": "Automatic prosodic events detection using syllable-based acoustic and syntactic features", "author": ["Je Hun Jeon", "Yang Liu"], "venue": "Proceedings of ICASSP, 2009, pp. 4565\u2013 4568.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2009}, {"title": "Exploiting acoustic and syntactic features for prosody labeling in a maximum entropy framework", "author": ["Vivek Rangarajan", "Shrikanth Narayanan", "Srinivas Bangalore"], "venue": "Proceedings of NAACL HLT, 2007, pp. 1\u20138.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2007}, {"title": "Improving intonational phrasing with syntactic information", "author": ["Philipp Koehn", "Steven Abney", "Julia Hirschberg", "Michael Collins"], "venue": "Proceedings of ICASSP, 2000, pp. 1289\u20131290.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2000}, {"title": "Locating boundaries for prosodic constituents in unrestricted mandarin texts", "author": ["Min Chu", "Yao Qian"], "venue": "Computational linguistics and Chinese language processing, pp. 61\u201382, 2001.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2001}, {"title": "Automatic phrase break prediction in chinese sentences", "author": ["Xin Nie", "Zuo-ying Wang"], "venue": "Journal of Chinese information Processing, pp. 39\u201344, 2003.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2003}, {"title": "Chinese prosody phrase break prediction based on maximum entropy model", "author": ["Jian-Feng Li", "Guoping Hu", "Ren-hua Wang"], "venue": "Proceedings of INTER- SPEECH, 2004, pp. 729\u2013732.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2004}, {"title": "Automatic prosodic labeling with conditional random fields and rich acoustic features", "author": ["Gina-Anne Levow"], "venue": "Proceedings of IJCNLP, 2008, pp. 217\u2013224.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2008}, {"title": "Conditional random fields: Probabilistic models for segmenting and labeling sequence data", "author": ["John D. Lafferty", "Andrew McCallum", "Fernando C.N. Pereira"], "venue": "Proceedings of 18th ICML, 2001, pp. 282\u2013289.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2001}, {"title": "Learning rules for chinese prosodic phrase prediction", "author": ["Zhao Sheng", "Tao Jianhua", "Cai Lianhong"], "venue": "Proceedings of ACL, 2002, pp. 1\u20137.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2002}, {"title": "Deep learning for chinese word segmentation and pos tagging", "author": ["Xiaoqing Zheng", "Hanyang Chen", "Tianyu Xu"], "venue": "Proceedings of EMNLP, 2013, pp. 647\u2013657.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2013}, {"title": "Maxmargin tensor neural network for chinese word segmentation", "author": ["Wenzhe Pei", "Tao Ge", "Chang Baobao"], "venue": "Proceedings of ACL, 2014, pp. 293\u2013303.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2014}, {"title": "Bidirectional recurrent neural networks", "author": ["Mike Schuster", "Kuldip K Paliwal"], "venue": "IEEE Transactions on Signal Processing, vol. 45, no. 11, pp. 2673\u20132681, 1997.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 1997}, {"title": "Long shortterm memory", "author": ["Sepp Hochreiter", "J\u00fcrgen Schmidhuber"], "venue": "Neural computation, vol. 9, no. 8, pp. 1735\u20131780, 1997.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 1997}, {"title": "A neural probabilistic language model", "author": ["Yoshua Bengio", "R\u00e9jean Ducharme", "Pascal Vincent", "Christian Janvin"], "venue": "The Journal of Machine Learning Research, vol. 3, pp. 1137\u20131155, 2003.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2003}, {"title": "Distributed representations of words and phrases and their compositionality", "author": ["Tomas Mikolov", "Ilya Sutskever", "Kai Chen", "Greg S Corrado", "Jeff Dean"], "venue": "Proceedings of NIPS, 2013, pp. 3111\u20133119.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2013}, {"title": "Linguistic regularities in continuous space word representations", "author": ["Tomas Mikolov", "Wen-tau Yih", "Geoffrey Zweig"], "venue": "Proceedings of HLT-NAACL, 2013, pp. 746\u2013751.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2013}, {"title": "Feature-based neural language model and chinese word segmentation", "author": ["Mairgup Mansur", "Wenzhe Pei", "Baobao Chang"], "venue": "Proceedings of 6th IJCNLP, vol. 1, no. 2.3, pp. 2\u20133, 2013.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2013}, {"title": "On fuzzy modeling using fuzzy neural networks with the back-propagation algorithm", "author": ["Shin-ichi Horikawa", "Takeshi Furuhashi", "Yoshiki Uchikawa"], "venue": "IEEE transactions on Neural Networks, vol. 3, no. 5, pp. 801\u2013 806, 1992.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 1992}, {"title": "Gradient-based learning algorithms for recurrent networks and their computational complexity", "author": ["Ronald J Williams", "David Zipser"], "venue": "Back-propagation: Theory, architectures and applications, pp. 433\u2013486, 1995.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 1995}, {"title": "Hybrid speech recognition with deep bidirectional lstm", "author": ["Alex Graves", "Navdeep Jaitly", "A-R Mohamed"], "venue": "Proceedings of ASRU, 2013, pp. 273\u2013278.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2013}, {"title": "Natural language processing (almost) from scratch", "author": ["Ronan Collobert", "Jason Weston", "L\u00e9on Bottou", "Michael Karlen", "Koray Kavukcuoglu", "Pavel Kuksa"], "venue": "The Journal of Machine Learning Research, vol. 12, pp. 2493\u2013 2537, 2011.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2011}, {"title": "Introducing currennt\u2013the munich opensource cuda recurrent neural network toolkit", "author": ["Felix Weninger", "Johannes Bergmann", "Bj\u00f6rn Schuller"], "venue": "Journal of Machine Learning Research, vol. 15, 2014.", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2014}], "referenceMentions": [{"referenceID": 0, "context": "It is well known that speech prosody plays an important perceptual role in human speech communication [1].", "startOffset": 102, "endOffset": 105}, {"referenceID": 1, "context": "In Chinese speech synthesis systems, typical prosody boundary labels consist of prosodic word (PW), prosodic phrase (PPH) and intonational phrase (IPH), which construct a threelayer prosody structure tree [2], as shown in Fig.", "startOffset": 205, "endOffset": 208}, {"referenceID": 2, "context": "Some syntactic cues like part-of-speech (POS), syllable identity, syllable stress and their contextual counterparts are commonly used for prosody boundary prediction [3, 4, 5].", "startOffset": 166, "endOffset": 175}, {"referenceID": 3, "context": "Some syntactic cues like part-of-speech (POS), syllable identity, syllable stress and their contextual counterparts are commonly used for prosody boundary prediction [3, 4, 5].", "startOffset": 166, "endOffset": 175}, {"referenceID": 4, "context": "Some syntactic cues like part-of-speech (POS), syllable identity, syllable stress and their contextual counterparts are commonly used for prosody boundary prediction [3, 4, 5].", "startOffset": 166, "endOffset": 175}, {"referenceID": 5, "context": "Many statistical methods have been investigated to model speech prosody, including classification and regression tree [6], hidden Markov model [7], maximum entropy model [8] and conditional ranIPH IPH", "startOffset": 118, "endOffset": 121}, {"referenceID": 6, "context": "Many statistical methods have been investigated to model speech prosody, including classification and regression tree [6], hidden Markov model [7], maximum entropy model [8] and conditional ranIPH IPH", "startOffset": 143, "endOffset": 146}, {"referenceID": 7, "context": "Many statistical methods have been investigated to model speech prosody, including classification and regression tree [6], hidden Markov model [7], maximum entropy model [8] and conditional ranIPH IPH", "startOffset": 170, "endOffset": 173}, {"referenceID": 8, "context": "dom fields (CRF) [9].", "startOffset": 17, "endOffset": 20}, {"referenceID": 0, "context": "To our knowledge, the best reported results were achieved with CRF due to its ability of relaxing strong model independence assumption and solving the label bias problem [1, 10].", "startOffset": 170, "endOffset": 177}, {"referenceID": 9, "context": "To our knowledge, the best reported results were achieved with CRF due to its ability of relaxing strong model independence assumption and solving the label bias problem [1, 10].", "startOffset": 170, "endOffset": 177}, {"referenceID": 10, "context": "First, it heavily relies on the performances of Chinese word segmentation (CWS) and POS tagging [11].", "startOffset": 96, "endOffset": 100}, {"referenceID": 11, "context": "Moreover, the choice of effective features, from a broad set of feature templates, is critical to the success of such systems [12].", "startOffset": 126, "endOffset": 130}, {"referenceID": 11, "context": "[12] applied neural networks to CWS and POS tagging and proposed a perceptron-style algorithm to speed up the training process with negligible loss in performance.", "startOffset": 0, "endOffset": 4}, {"referenceID": 12, "context": "[13] proposed a max-margin tensor neural network for CWS to model interactions between tags and context characters by exploiting tag embeddings and tensor-based transformation.", "startOffset": 0, "endOffset": 4}, {"referenceID": 13, "context": "Instead, bidirectional recurrent neural networks (BRNN) are able to incorporate contextual information from both past and future inputs [14].", "startOffset": 136, "endOffset": 140}, {"referenceID": 14, "context": "Specifically, BRNN with long short-term memory (LSTM) cells, namely BLSTM-RNN, has become a popular model [15].", "startOffset": 106, "endOffset": 110}, {"referenceID": 15, "context": "In order to make the prediction models less dependent on the feature engineering, we choose to use a variant of the neural network architecture proposed by [16] for probabilistic language model.", "startOffset": 156, "endOffset": 160}, {"referenceID": 11, "context": "This architecture was subsequently used for CWS and POS tagging [12].", "startOffset": 64, "endOffset": 68}, {"referenceID": 16, "context": "In contrast, the distributed representation or embedding feature, in form of a low dimensional continuous-valued vector learned using neural networks from raw text in a fully unsupervised way, is assumed to carry important syntactic and semantic information [17] [18].", "startOffset": 258, "endOffset": 262}, {"referenceID": 17, "context": "In contrast, the distributed representation or embedding feature, in form of a low dimensional continuous-valued vector learned using neural networks from raw text in a fully unsupervised way, is assumed to carry important syntactic and semantic information [17] [18].", "startOffset": 263, "endOffset": 267}, {"referenceID": 18, "context": "[19] have shown superior performance in Chinese word segmentation by the use of embedding features based on a neural language model [16].", "startOffset": 0, "endOffset": 4}, {"referenceID": 15, "context": "[19] have shown superior performance in Chinese word segmentation by the use of embedding features based on a neural language model [16].", "startOffset": 132, "endOffset": 136}, {"referenceID": 15, "context": "Besides [16], Mikolov et al.", "startOffset": 8, "endOffset": 12}, {"referenceID": 16, "context": "[17] proposed a faster skip-gram model called word2vec1.", "startOffset": 0, "endOffset": 4}, {"referenceID": 19, "context": "FFNN, trained with a backpropagation learning algorithm [20], is widely used in many practical applications.", "startOffset": 56, "endOffset": 60}, {"referenceID": 20, "context": "The weight gradients are computed over the entire utterance [21].", "startOffset": 60, "endOffset": 64}, {"referenceID": 21, "context": "The deep architecture is able to build up progressively higher level representations of the input data, which is a crucial factor of the recent success of hybrid systems [22].", "startOffset": 170, "endOffset": 174}, {"referenceID": 11, "context": "For the input character sequence of a sentence c[1:T ] with a tag sequence tag[1:T ], a sentence-level score is then given by the sum of transition and network scores [12, 23]:", "startOffset": 167, "endOffset": 175}, {"referenceID": 22, "context": "For the input character sequence of a sentence c[1:T ] with a tag sequence tag[1:T ], a sentence-level score is then given by the sum of transition and network scores [12, 23]:", "startOffset": 167, "endOffset": 175}, {"referenceID": 23, "context": "That is to say, three separate neural network models were trained independently for PW, PPH and IPH using the CURRENNT toolkit [24].", "startOffset": 127, "endOffset": 131}], "year": 2015, "abstractText": "Prosody affects the naturalness and intelligibility of speech. However, automatic prosody prediction from text for Chinese speech synthesis is still a great challenge and the traditional conditional random fields (CRF) based method always heavily relies on feature engineering. In this paper, we propose to use neural networks to predict prosodic boundary labels directly from Chinese characters without any feature engineering. Experimental results show that stacking feed-forward and bidirectional long short-term memory (BLSTM) recurrent network layers achieves superior performance over the CRF-based method. The embedding features learned from raw text further enhance the performance.", "creator": "LaTeX with hyperref package"}}}