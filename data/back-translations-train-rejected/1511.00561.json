{"id": "1511.00561", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "2-Nov-2015", "title": "SegNet: A Deep Convolutional Encoder-Decoder Architecture for Image Segmentation", "abstract": "We present a novel and practical deep fully convolutional neural network architecture for semantic pixel-wise segmentation termed SegNet. This core trainable segmentation engine consists of an encoder network, a corresponding decoder network followed by a pixel-wise classification layer. The architecture of the encoder network is topologically identical to the 13 convolutional layers in the VGG16 network . The role of the decoder network is to map the low resolution encoder feature maps to full input resolution feature maps for pixel-wise classification. The novelty of SegNet lies is in the manner in which the decoder upsamples its lower resolution input feature map(s). Specifically, the decoder uses pooling indices computed in the max-pooling step of the corresponding encoder to perform non-linear upsampling. This eliminates the need for learning to upsample. The upsampled maps are sparse and are then convolved with trainable filters to produce dense feature maps. We compare our proposed architecture with the fully convolutional network (FCN) architecture and its variants. This comparison reveals the memory versus accuracy trade-off involved in achieving good segmentation performance. The design of SegNet was primarily motivated by road scene understanding applications. Hence, it is efficient both in terms of memory and computational time during inference. It is also significantly smaller in the number of trainable parameters than competing architectures and can be trained end-to-end using stochastic gradient descent. We also benchmark the performance of SegNet on Pascal VOC12 salient object segmentation and the recent SUN RGB-D indoor scene understanding challenge. We show that SegNet provides competitive performance although it is significantly smaller than other architectures. We also provide a Caffe implementation of SegNet and a webdemo at", "histories": [["v1", "Mon, 2 Nov 2015 15:51:03 GMT  (3861kb,D)", "http://arxiv.org/abs/1511.00561v1", null], ["v2", "Tue, 8 Dec 2015 13:56:56 GMT  (3860kb,D)", "http://arxiv.org/abs/1511.00561v2", null], ["v3", "Mon, 10 Oct 2016 21:11:59 GMT  (2225kb,D)", "http://arxiv.org/abs/1511.00561v3", null]], "reviews": [], "SUBJECTS": "cs.CV cs.LG cs.NE", "authors": ["vijay badrinarayanan", "alex kendall", "roberto cipolla"], "accepted": false, "id": "1511.00561"}, "pdf": {"name": "1511.00561.pdf", "metadata": {"source": "CRF", "title": "SegNet: A Deep Convolutional Encoder-Decoder Architecture for Image Segmentation", "authors": ["Vijay Badrinarayanan", "Alex Kendall", "Roberto Cipolla"], "emails": [], "sections": [{"heading": null, "text": "Index Terms - Deep Convolutional Neural Networks, Semantic Pixel-Wise Segmentation, Encoder, Decoder, Pooling, Upsampling.F"}, {"heading": "1 INTRODUCTION", "text": "In fact, most of them are able to survive on their own when they move to another world."}, {"heading": "2 LITERATURE REVIEW", "text": "This year, it is as far as ever in the history of the city, where it is as far as never before in the history of the city."}, {"heading": "3 ARCHITECTURE", "text": "This year, we have reached the point where we feel we are able to live in a country where most people are able to live in a country where they are able to move and where they are able to flourish."}, {"heading": "3.1 Decoder Variants", "text": "There is only one reason why it has to come to this point in order to analyze the performance of Segnet and make its performance comparable to that of Segnet. \"Blessed,\" according to the title, \"is also a smaller version of SegNet, i.e. SegNet-Basic 1, which has 4 encoders and 4 decoders.\" All encoders in SegNet-Basic are able to make their performance comparable to that of SegNet-Basics. \"The encoders in SegNet-Basic have the same way that SegNet-Basic has their performance and the subsampling variants of SegNet, which upsample their inputs with the received decoders.\" Batch normalization is used after each convolutional layer in the encoder encoder and decoder network. No biases according to ReLU are used. \""}, {"heading": "3.2 Training", "text": "In fact, most of them will be able to move in a direction in which they have gone."}, {"heading": "3.3 Analysis", "text": "This year it is more than ever before."}, {"heading": "4 BENCHMARKING", "text": "We quantify the performance of SegNet on three different benchmarks with our Caffe implementation 3. Through this process, we demonstrate the effectiveness of SegNet for various scene segmentation tasks that have practical application. In the first experiment, we test the performance of SegNet on the road Dataset (see Sec. 3.2 for more information on these data). We use this result to compare SegNet with several methods, including Random Forests [25], Boosting [27] in combination with CRF-based methods [28]. We have also collected SegNet on a larger dataset of the various publicly available datasets, [53] and show that this leads to a big improvement in accuracy."}, {"heading": "4.1 CamVid Road Scenes", "text": "This year, it is only a matter of time before an agreement is reached."}, {"heading": "4.2 SUN RGB-D Indoor Scenes", "text": "This year, it will be able to take the measures mentioned in order to defuse and improve the situation."}, {"heading": "4.3 Pascal VOC12 Segmentation Challenge", "text": "This year, it has reached the stage where it will be able to take the lead."}, {"heading": "5 DISCUSSION AND FUTURE WORK", "text": "The success of the project is mainly due to the availability of massive data sets, to a lesser extent than the use of models developed in real time."}, {"heading": "6 CONCLUSION", "text": "The main motivation behind SegNet was the need to design an efficient architecture for understanding street scenes that is efficient in terms of both memory and computing time. We analyzed SegNet and compared it with other important variants to show the trade-offs that occur when designing architectures for segmentation. Those who store the full length of the encoder network perform best, but use more memory during the inference time. SegNet, on the other hand, is more efficient because it stores only the max pooling indexes of the function cards and uses them in its decoder network to achieve good performance. However, the inference time increases because the decoder network needs to be larger. On large and well-known datasets, SegNet performs competitively in challenges such as understanding indoor scenes and Pascal VOC12. It sets a new benchmark for understanding street scenes when it is trained on a large dataset to estimate future segment performance well as its architecture is efficient and competitive with its architecture."}], "references": [{"title": "Very deep convolutional networks for large-scale image recognition", "author": ["K. Simonyan", "A. Zisserman"], "venue": "arXiv preprint arXiv:1409.1556, 2014.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2014}, {"title": "Fully convolutional networks for semantic segmentation", "author": ["J. Long", "E. Shelhamer", "T. Darrell"], "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 3431\u20133440, 2015.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2015}, {"title": "Semantic object classes in video: A high-definition ground truth database", "author": ["G. Brostow", "J. Fauqueur", "R. Cipolla"], "venue": "PRL, vol. 30(2), pp. 88\u2013 97, 2009.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2009}, {"title": "Going deeper with convolutions", "author": ["C. Szegedy", "W. Liu", "Y. Jia", "P. Sermanet", "S. Reed", "D. Anguelov", "D. Erhan", "V. Vanhoucke", "A. Rabinovich"], "venue": "CoRR, vol. abs/1409.4842, 2014.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2014}, {"title": "Very deep convolutional networks for large-scale image recognition", "author": ["K. Simonyan", "A. Zisserman"], "venue": "CoRR, vol. abs/1409.1556, 2014.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2014}, {"title": "Learning hierarchical features for scene labeling", "author": ["C. Farabet", "C. Couprie", "L. Najman", "Y. LeCun"], "venue": "IEEE PAMI, vol. 35, no. 8, pp. 1915\u20131929, 2013.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 1915}, {"title": "Fast semantic segmentation of rgb-d scenes with gpu-accelerated deep neural networks", "author": ["N. Hft", "H. Schulz", "S. Behnke"], "venue": "KI 2014: Advances in Artificial Intelligence (C. Lutz and M. Thielscher, eds.), vol. 8736 of  14 Lecture Notes in Computer Science, pp. 80\u201385, Springer International Publishing, 2014.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2014}, {"title": "Parsing natural scenes and natural language with recursive neural networks", "author": ["R. Socher", "C.C. Lin", "C. Manning", "A.Y. Ng"], "venue": "ICML, pp. 129\u2013 136, 2011.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2011}, {"title": "Learning deconvolution network for semantic segmentation", "author": ["H. Noh", "S. Hong", "B. Han"], "venue": "CoRR, vol. abs/1505.04366, 2015.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2015}, {"title": "Conditional random fields as recurrent neural networks", "author": ["S. Zheng", "S. Jayasumana", "B. Romera-Paredes", "V. Vineet", "Z. Su", "D. Du", "C. Huang", "P. Torr"], "venue": "arXiv preprint arXiv:1502.03240, 2015.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2015}, {"title": "Parsenet: Looking wider to see better", "author": ["W. Liu", "A. Rabinovich", "A.C. Berg"], "venue": "CoRR, vol. abs/1506.04579, 2015.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2015}, {"title": "Segnet: A deep convolutional encoder-decoder architecture for robust semantic pixel-wise labelling", "author": ["V. Badrinarayanan", "A. Handa", "R. Cipolla"], "venue": "CoRR, vol. abs/1505.07293, 2015.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2015}, {"title": "Predicting depth, surface normals and semantic labels with a common multi-scale convolutional architecture", "author": ["D. Eigen", "R. Fergus"], "venue": "arXiv preprint arXiv:1411.4734, 2014.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2014}, {"title": "Semantic image segmentation with deep convolutional nets and fully connected crfs", "author": ["L. Chen", "G. Papandreou", "I. Kokkinos", "K. Murphy", "A.L. Yuille"], "venue": "CoRR, vol. abs/1412.7062, 2014.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2014}, {"title": "Large-scale machine learning with stochastic gradient descent", "author": ["L. Bottou"], "venue": "Proceedings of COMPSTAT\u20192010, pp. 177\u2013186, Springer, 2010.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2010}, {"title": "Decoupled deep neural network for semisupervised semantic segmentation", "author": ["S. Hong", "H. Noh", "B. Han"], "venue": "CoRR, vol. abs/1506.04924, 2015.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2015}, {"title": "Unsupervised learning of invariant feature hierarchies with applications to object recognition", "author": ["M. Ranzato", "F.J. Huang", "Y. Boureau", "Y. LeCun"], "venue": "CVPR, 2007.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2007}, {"title": "The role of context for object detection and semantic segmentation in the wild", "author": ["R. Mottaghi", "X. Chen", "X. Liu", "N.-G. Cho", "S.-W. Lee", "S. Fidler", "R. Urtasun"], "venue": "Computer Vision and Pattern Recognition (CVPR), 2014 IEEE Conference on, pp. 891\u2013898, IEEE, 2014.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2014}, {"title": "The pascal visual object classes challenge: A retrospective", "author": ["M. Everingham", "S.A. Eslami", "L. Van Gool", "C.K. Williams", "J. Winn", "A. Zisserman"], "venue": "International Journal of Computer Vision, vol. 111, no. 1, pp. 98\u2013136.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 0}, {"title": "Semantic contours from inverse detectors", "author": ["B. Hariharan", "P. Arbel\u00e1ez", "L. Bourdev", "S. Maji", "J. Malik"], "venue": "Computer Vision (ICCV), 2011 IEEE International Conference on, pp. 991\u2013998, IEEE, 2011.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2011}, {"title": "Sun rgb-d: A rgb-d scene understanding benchmark suite", "author": ["S. Song", "S.P. Lichtenberg", "J. Xiao"], "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 567\u2013576, 2015.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2015}, {"title": "Edge boxes: Locating object proposals from edges", "author": ["C.L. Zitnick", "P. Doll\u00e1r"], "venue": "Computer Vision\u2013ECCV 2014, pp. 391\u2013405, Springer, 2014.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2014}, {"title": "Indoor segmentation and support inference from rgbd images", "author": ["N. Silberman", "D. Hoiem", "P. Kohli", "R. Fergus"], "venue": "ECCV, pp. 746\u2013760, Springer, 2012.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2012}, {"title": "Are we ready for autonomous driving? the KITTI vision benchmark suite", "author": ["A. Geiger", "P. Lenz", "R. Urtasun"], "venue": "CVPR, pp. 3354\u20133361, 2012.", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2012}, {"title": "Semantic texton forests for image categorization and segmentation", "author": ["J. Shotton", "M. Johnson", "R. Cipolla"], "venue": "CVPR, 2008.", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2008}, {"title": "Segmentation and recognition using structure from motion point clouds", "author": ["G. Brostow", "J.J. Shotton", "R. Cipolla"], "venue": "ECCV, Marseille, 2008.", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2008}, {"title": "Combining appearance and structure from motion features for road scene understanding", "author": ["P. Sturgess", "K. Alahari", "L. Ladicky", "P.H.S.Torr"], "venue": "BMVC, 2009.", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2009}, {"title": "What, where and how many? combining object detectors and crfs", "author": ["L. Ladicky", "P. Sturgess", "K. Alahari", "C. Russell", "P.H.S. Torr"], "venue": "ECCV, pp. 424\u2013437, 2010.", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2010}, {"title": "Structured class-labels in random forests for semantic image labelling", "author": ["P. Kontschieder", "S.R. Bulo", "H. Bischof", "M. Pelillo"], "venue": "ICCV, pp. 2190\u20132197, IEEE, 2011.", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2011}, {"title": "Semantic segmentation of urban scenes using dense depth maps", "author": ["C. Zhang", "L. Wang", "R. Yang"], "venue": "ECCV, pp. 708\u2013721, Springer, 2010.", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2010}, {"title": "Superparsing", "author": ["J. Tighe", "S. Lazebnik"], "venue": "IJCV, vol. 101, no. 2, pp. 329\u2013 349, 2013.", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2013}, {"title": "Rgb-(d) scene labeling: Features and algorithms", "author": ["X. Ren", "L. Bo", "D. Fox"], "venue": "CVPR, pp. 2759\u20132766, IEEE, 2012.", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2012}, {"title": "Dense 3D Semantic Mapping of Indoor Scenes from RGB-D Images", "author": ["A. Hermans", "G. Floros", "B. Leibe"], "venue": "ICRA, 2014.", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2014}, {"title": "Perceptual organization and recognition of indoor scenes from rgb-d images", "author": ["S. Gupta", "P. Arbelaez", "J. Malik"], "venue": "CVPR, pp. 564\u2013571, IEEE, 2013.", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2013}, {"title": "Scene parsing with multiscale feature learning, purity trees, and optimal covers", "author": ["C. Farabet", "C. Couprie", "L. Najman", "Y. LeCun"], "venue": "ICML, 2012.", "citeRegEx": "35", "shortCiteRegEx": null, "year": 2012}, {"title": "Deep convolutional networks for scene parsing", "author": ["D. Grangier", "L. Bottou", "R. Collobert"], "venue": "ICML Workshop on Deep Learning, 2009.", "citeRegEx": "36", "shortCiteRegEx": null, "year": 2009}, {"title": "Unrolling loopy top-down semantic feedback in convolutional deep networks", "author": ["C. Gatta", "A. Romero", "J. van de Weijer"], "venue": "CVPR Workshop on Deep Vision, 2014.", "citeRegEx": "37", "shortCiteRegEx": null, "year": 2014}, {"title": "Recurrent convolutional neural networks for scene labeling", "author": ["P. Pinheiro", "R. Collobert"], "venue": "ICML, pp. 82\u201390, 2014.", "citeRegEx": "38", "shortCiteRegEx": null, "year": 2014}, {"title": "ImageNet Large Scale Visual Recognition Challenge", "author": ["O. Russakovsky", "J. Deng", "H. Su", "J. Krause", "S. Satheesh", "S. Ma", "Z. Huang", "A. Karpathy", "A. Khosla", "M. Bernstein", "A.C. Berg", "L. Fei-Fei"], "venue": "International Journal of Computer Vision (IJCV), pp. 1\u201342, April 2015.", "citeRegEx": "39", "shortCiteRegEx": null, "year": 2015}, {"title": "Microsoft coco: Common objects in context", "author": ["T.-Y. Lin", "M. Maire", "S. Belongie", "J. Hays", "P. Perona", "D. Ramanan", "P. Doll\u00e1r", "C.L. Zitnick"], "venue": "Computer Vision\u2013ECCV 2014, pp. 740\u2013755, Springer, 2014.", "citeRegEx": "40", "shortCiteRegEx": null, "year": 2014}, {"title": "Fully connected deep structured networks", "author": ["A.G. Schwing", "R. Urtasun"], "venue": "arXiv preprint arXiv:1503.02351, 2015.", "citeRegEx": "41", "shortCiteRegEx": null, "year": 2015}, {"title": "Efficient piecewise training of deep structured models for semantic segmentation", "author": ["G. Lin", "C. Shen", "I. Reid"], "venue": "arXiv preprint arXiv:1504.01013, 2015.", "citeRegEx": "42", "shortCiteRegEx": null, "year": 2015}, {"title": "Hypercolumns for object segmentation and fine-grained localization", "author": ["B. Hariharan", "P.A. Arbel\u00e1ez", "R.B. Girshick", "J. Malik"], "venue": "CoRR, vol. abs/1411.5752, 2014.", "citeRegEx": "43", "shortCiteRegEx": null, "year": 2014}, {"title": "Feedforward semantic segmentation with zoom-out features", "author": ["M. Mostajabi", "P. Yadollahpour", "G. Shakhnarovich"], "venue": "arXiv preprint arXiv:1412.0774, 2014.", "citeRegEx": "44", "shortCiteRegEx": null, "year": 2014}, {"title": "Deconvolutional networks", "author": ["M.D. Zeiler", "D. Krishnan", "G.W. Taylor", "R. Fergus"], "venue": "CVPR, pp. 2528\u20132535, IEEE, 2010.", "citeRegEx": "45", "shortCiteRegEx": null, "year": 2010}, {"title": "Learning convolutional feature hierarchies for visual recognition", "author": ["K. Kavukcuoglu", "P. Sermanet", "Y. Boureau", "K. Gregor", "M. Mathieu", "Y. LeCun"], "venue": "NIPS, pp. 1090\u20131098, 2010.", "citeRegEx": "46", "shortCiteRegEx": null, "year": 2010}, {"title": "Learning a deep convolutional network for image super-resolution", "author": ["C. Dong", "C.C. Loy", "K. He", "X. Tang"], "venue": "ECCV, pp. 184\u2013199, Springer, 2014.", "citeRegEx": "47", "shortCiteRegEx": null, "year": 2014}, {"title": "Depth map prediction from a single image using a multi-scale deep network", "author": ["D. Eigen", "C. Puhrsch", "R. Fergus"], "venue": "arXiv preprint arXiv:1406.2283, 2014.", "citeRegEx": "48", "shortCiteRegEx": null, "year": 2014}, {"title": "Batch normalization: Accelerating deep network training by reducing internal covariate shift", "author": ["S. Ioffe", "C. Szegedy"], "venue": "CoRR, vol. abs/1502.03167, 2015.", "citeRegEx": "49", "shortCiteRegEx": null, "year": 2015}, {"title": "What is the best multi-stage architecture for object recognition", "author": ["K. Jarrett", "K. Kavukcuoglu", "M. Ranzato", "Y. LeCun"], "venue": "ICCV, pp. 2146\u2013 2153, 2009.", "citeRegEx": "50", "shortCiteRegEx": null, "year": 2009}, {"title": "Delving deep into rectifiers: Surpassing human-level performance on imagenet classification", "author": ["K. He", "X. Zhang", "S. Ren", "J. Sun"], "venue": "arXiv preprint arXiv:1502.01852, 2015.", "citeRegEx": "51", "shortCiteRegEx": null, "year": 1852}, {"title": "Caffe: Convolutional architecture for fast feature embedding", "author": ["Y. Jia", "E. Shelhamer", "J. Donahue", "S. Karayev", "J. Long", "R. Girshick", "S. Guadarrama", "T. Darrell"], "venue": "arXiv preprint arXiv:1408.5093, 2014.", "citeRegEx": "52", "shortCiteRegEx": null, "year": 2014}, {"title": "Labelme: online image annotation and applications", "author": ["A. Torralba", "B.C. Russell", "J. Yuen"], "venue": "tech. rep., MIT CSAIL Technical Report., 2009.", "citeRegEx": "53", "shortCiteRegEx": null, "year": 2009}, {"title": "Vision-based offline-online perception paradigm for autonomous driving", "author": ["G. Ros", "S. Ramos", "M. Granados", "A. Bakhtiary", "D. Vazquez", "A. Lopez"], "venue": "WACV, 2015.", "citeRegEx": "54", "shortCiteRegEx": null, "year": 2015}, {"title": "Decomposing a scene into geometric and semantically consistent regions", "author": ["S. Gould", "R. Fulton", "D. Koller"], "venue": "ICCV, pp. 1\u20138, IEEE, 2009.", "citeRegEx": "55", "shortCiteRegEx": null, "year": 2009}, {"title": "Labelme: a database and web-based tool for image annotation", "author": ["B.C. Russell", "A. Torralba", "K.P. Murphy", "W.T. Freeman"], "venue": "IJCV, vol. 77, no. 1-3, pp. 157\u2013173, 2008.", "citeRegEx": "56", "shortCiteRegEx": null, "year": 2008}, {"title": "Efficient inference in fully connected crfs with gaussian edge potentials", "author": ["V. Koltun"], "venue": "In: NIPS (2011, 2011.", "citeRegEx": "57", "shortCiteRegEx": null, "year": 2011}, {"title": "Object detectors emerge in deep scene cnns", "author": ["B. Zhou", "A. Khosla", "A. Lapedriza", "A. Oliva", "A. Torralba"], "venue": "arXiv preprint arXiv:1412.6856, 2014.", "citeRegEx": "58", "shortCiteRegEx": null, "year": 2014}, {"title": "Neural decision forests for semantic image labelling", "author": ["Bulo", "S. Rota", "P. Kontschieder"], "venue": "CVPR, 2014.", "citeRegEx": "60", "shortCiteRegEx": null, "year": 2014}, {"title": "Local label descriptor for example based semantic image labeling", "author": ["Y. Yang", "Z. Li", "L. Zhang", "C. Murphy", "J. Ver Hoeve", "H. Jiang"], "venue": "ECCV, pp. 361\u2013375, Springer, 2012.", "citeRegEx": "61", "shortCiteRegEx": null, "year": 2012}, {"title": "Sift flow: Dense correspondence across different scenes", "author": ["C. Liu", "J. Yuen", "A. Torralba", "J. Sivic"], "venue": "ECCV, Marseille, 2008.", "citeRegEx": "62", "shortCiteRegEx": null, "year": 2008}, {"title": "Dropout as a bayesian approximation: Insights and applications", "author": ["Y. Gal", "Z. Ghahramani"], "venue": "Deep Learning Workshop, ICML, 2015.", "citeRegEx": "63", "shortCiteRegEx": null, "year": 2015}], "referenceMentions": [{"referenceID": 0, "context": "VGG16 network [1].", "startOffset": 14, "endOffset": 17}, {"referenceID": 1, "context": "network [2] architecture and its variants.", "startOffset": 8, "endOffset": 11}, {"referenceID": 3, "context": "In particular, deep learning has seen huge success lately in handwritten digit recognition, speech, categorising whole images and detecting objects in images [4], [5].", "startOffset": 158, "endOffset": 161}, {"referenceID": 4, "context": "In particular, deep learning has seen huge success lately in handwritten digit recognition, speech, categorising whole images and detecting objects in images [4], [5].", "startOffset": 163, "endOffset": 166}, {"referenceID": 5, "context": "Now there is an active interest for semantic pixel-wise labelling [6] [7], [8], [2], [9], [10], [11], [12], [13], [14].", "startOffset": 66, "endOffset": 69}, {"referenceID": 6, "context": "Now there is an active interest for semantic pixel-wise labelling [6] [7], [8], [2], [9], [10], [11], [12], [13], [14].", "startOffset": 70, "endOffset": 73}, {"referenceID": 7, "context": "Now there is an active interest for semantic pixel-wise labelling [6] [7], [8], [2], [9], [10], [11], [12], [13], [14].", "startOffset": 75, "endOffset": 78}, {"referenceID": 1, "context": "Now there is an active interest for semantic pixel-wise labelling [6] [7], [8], [2], [9], [10], [11], [12], [13], [14].", "startOffset": 80, "endOffset": 83}, {"referenceID": 8, "context": "Now there is an active interest for semantic pixel-wise labelling [6] [7], [8], [2], [9], [10], [11], [12], [13], [14].", "startOffset": 85, "endOffset": 88}, {"referenceID": 9, "context": "Now there is an active interest for semantic pixel-wise labelling [6] [7], [8], [2], [9], [10], [11], [12], [13], [14].", "startOffset": 90, "endOffset": 94}, {"referenceID": 10, "context": "Now there is an active interest for semantic pixel-wise labelling [6] [7], [8], [2], [9], [10], [11], [12], [13], [14].", "startOffset": 96, "endOffset": 100}, {"referenceID": 11, "context": "Now there is an active interest for semantic pixel-wise labelling [6] [7], [8], [2], [9], [10], [11], [12], [13], [14].", "startOffset": 102, "endOffset": 106}, {"referenceID": 12, "context": "Now there is an active interest for semantic pixel-wise labelling [6] [7], [8], [2], [9], [10], [11], [12], [13], [14].", "startOffset": 108, "endOffset": 112}, {"referenceID": 13, "context": "Now there is an active interest for semantic pixel-wise labelling [6] [7], [8], [2], [9], [10], [11], [12], [13], [14].", "startOffset": 114, "endOffset": 118}, {"referenceID": 5, "context": "However, some of these recent approaches have tried to directly adopt deep architectures designed for category prediction to pixel-wise labelling [6].", "startOffset": 146, "endOffset": 149}, {"referenceID": 13, "context": "The results, although very encouraging, appear coarse [14].", "startOffset": 54, "endOffset": 58}, {"referenceID": 14, "context": "It must also be able to train end-to-end in order to jointly optimise all the weights in the network using an efficient weight update technique such as stochastic gradient descent (SGD) [15].", "startOffset": 186, "endOffset": 190}, {"referenceID": 1, "context": "Networks that are trained end-to-end or equivalently those that do not use multi-stage training [2] or other supporting aids such as region proposals [9] help establish benchmarks that are more easily repeatable.", "startOffset": 96, "endOffset": 99}, {"referenceID": 8, "context": "Networks that are trained end-to-end or equivalently those that do not use multi-stage training [2] or other supporting aids such as region proposals [9] help establish benchmarks that are more easily repeatable.", "startOffset": 150, "endOffset": 153}, {"referenceID": 0, "context": "The encoder network in SegNet is topologically identical to the convolutional layers in VGG16 [1].", "startOffset": 94, "endOffset": 97}, {"referenceID": 1, "context": "We remove the fully connected layers of VGG16 which makes the SegNet encoder network significantly smaller and easier to train than many other recent architectures [2], [9], [11], [16].", "startOffset": 164, "endOffset": 167}, {"referenceID": 8, "context": "We remove the fully connected layers of VGG16 which makes the SegNet encoder network significantly smaller and easier to train than many other recent architectures [2], [9], [11], [16].", "startOffset": 169, "endOffset": 172}, {"referenceID": 10, "context": "We remove the fully connected layers of VGG16 which makes the SegNet encoder network significantly smaller and easier to train than many other recent architectures [2], [9], [11], [16].", "startOffset": 174, "endOffset": 178}, {"referenceID": 15, "context": "We remove the fully connected layers of VGG16 which makes the SegNet encoder network significantly smaller and easier to train than many other recent architectures [2], [9], [11], [16].", "startOffset": 180, "endOffset": 184}, {"referenceID": 16, "context": "This idea was inspired from an architecture designed for unsupervised feature learning [17].", "startOffset": 87, "endOffset": 91}, {"referenceID": 2, "context": "[3].", "startOffset": 0, "endOffset": 3}, {"referenceID": 1, "context": "advantages; (i) it improves boundary delineation , (ii) it reduces the number of parameters enabling end-to-end training, and (iii) this form of upsampling can be incorporated into any encoder-decoder architecture such as [2], [10] with only a little modification.", "startOffset": 222, "endOffset": 225}, {"referenceID": 9, "context": "advantages; (i) it improves boundary delineation , (ii) it reduces the number of parameters enabling end-to-end training, and (iii) this form of upsampling can be incorporated into any encoder-decoder architecture such as [2], [10] with only a little modification.", "startOffset": 227, "endOffset": 231}, {"referenceID": 1, "context": "One of the main contributions of this paper is our analysis of the SegNet decoding technique and the widely used Fully Convolutional Network (FCN) [2].", "startOffset": 147, "endOffset": 150}, {"referenceID": 8, "context": "Another common feature is they have trainable parameters in the order of hundreds of millions and thus encounter difficulties in performing end-toend training [9].", "startOffset": 159, "endOffset": 162}, {"referenceID": 1, "context": "The difficulty of training these networks has led to multi-stage training [2], appending networks to a pre-trained core segmentation engine such as FCN [10], use of supporting aids such as region proposals for inference [9], disjoint training of classification and segmentation networks [16] and use of additional training data for pre-training [11] [18] or for full training [10].", "startOffset": 74, "endOffset": 77}, {"referenceID": 9, "context": "The difficulty of training these networks has led to multi-stage training [2], appending networks to a pre-trained core segmentation engine such as FCN [10], use of supporting aids such as region proposals for inference [9], disjoint training of classification and segmentation networks [16] and use of additional training data for pre-training [11] [18] or for full training [10].", "startOffset": 152, "endOffset": 156}, {"referenceID": 8, "context": "The difficulty of training these networks has led to multi-stage training [2], appending networks to a pre-trained core segmentation engine such as FCN [10], use of supporting aids such as region proposals for inference [9], disjoint training of classification and segmentation networks [16] and use of additional training data for pre-training [11] [18] or for full training [10].", "startOffset": 220, "endOffset": 223}, {"referenceID": 15, "context": "The difficulty of training these networks has led to multi-stage training [2], appending networks to a pre-trained core segmentation engine such as FCN [10], use of supporting aids such as region proposals for inference [9], disjoint training of classification and segmentation networks [16] and use of additional training data for pre-training [11] [18] or for full training [10].", "startOffset": 287, "endOffset": 291}, {"referenceID": 10, "context": "The difficulty of training these networks has led to multi-stage training [2], appending networks to a pre-trained core segmentation engine such as FCN [10], use of supporting aids such as region proposals for inference [9], disjoint training of classification and segmentation networks [16] and use of additional training data for pre-training [11] [18] or for full training [10].", "startOffset": 345, "endOffset": 349}, {"referenceID": 17, "context": "The difficulty of training these networks has led to multi-stage training [2], appending networks to a pre-trained core segmentation engine such as FCN [10], use of supporting aids such as region proposals for inference [9], disjoint training of classification and segmentation networks [16] and use of additional training data for pre-training [11] [18] or for full training [10].", "startOffset": 350, "endOffset": 354}, {"referenceID": 9, "context": "The difficulty of training these networks has led to multi-stage training [2], appending networks to a pre-trained core segmentation engine such as FCN [10], use of supporting aids such as region proposals for inference [9], disjoint training of classification and segmentation networks [16] and use of additional training data for pre-training [11] [18] or for full training [10].", "startOffset": 376, "endOffset": 380}, {"referenceID": 13, "context": "In addition, performance boosting post-processing techniques [14] have also been popular.", "startOffset": 61, "endOffset": 65}, {"referenceID": 18, "context": "Although all these factors improve performance on challenging benchmarks [19], it is unfortunately difficult from their quantitative results to disentangle the key design factors necessary to achieve good performance.", "startOffset": 73, "endOffset": 77}, {"referenceID": 1, "context": "We therefore analysed the decoding process used in some of these approaches [2], [9] and reveal their pros and cons.", "startOffset": 76, "endOffset": 79}, {"referenceID": 8, "context": "We therefore analysed the decoding process used in some of these approaches [2], [9] and reveal their pros and cons.", "startOffset": 81, "endOffset": 84}, {"referenceID": 18, "context": "We evaluate the performance of SegNet on PascalVOC12 salient object(s) segmentation [19], [20] and scene understanding challenges such as CamVid road scene segmentation [3] and SUN RGB-D indoor scene segmentation [21].", "startOffset": 84, "endOffset": 88}, {"referenceID": 19, "context": "We evaluate the performance of SegNet on PascalVOC12 salient object(s) segmentation [19], [20] and scene understanding challenges such as CamVid road scene segmentation [3] and SUN RGB-D indoor scene segmentation [21].", "startOffset": 90, "endOffset": 94}, {"referenceID": 2, "context": "We evaluate the performance of SegNet on PascalVOC12 salient object(s) segmentation [19], [20] and scene understanding challenges such as CamVid road scene segmentation [3] and SUN RGB-D indoor scene segmentation [21].", "startOffset": 169, "endOffset": 172}, {"referenceID": 20, "context": "We evaluate the performance of SegNet on PascalVOC12 salient object(s) segmentation [19], [20] and scene understanding challenges such as CamVid road scene segmentation [3] and SUN RGB-D indoor scene segmentation [21].", "startOffset": 213, "endOffset": 217}, {"referenceID": 18, "context": "Pascal VOC12 [19] is the benchmark for segmentation due to its size and challenges, but the majority of this task has one or two foreground classes surrounded by a highly varied background.", "startOffset": 13, "endOffset": 17}, {"referenceID": 15, "context": "This implicitly favours techniques used for detection as shown by the recent work on a decoupled classification-segmentation network [16] where the classification network can be trained with a large set of weakly labelled data and the independent segmentation network performance is improved.", "startOffset": 133, "endOffset": 137}, {"referenceID": 13, "context": "The method of [14] also use the feature maps of the classification network with an independent CRF post-processing technique to perform segmentation.", "startOffset": 14, "endOffset": 18}, {"referenceID": 8, "context": "The performance can also be boosted by the use additional inference aids such as region proposals [9], [22].", "startOffset": 98, "endOffset": 101}, {"referenceID": 21, "context": "The performance can also be boosted by the use additional inference aids such as region proposals [9], [22].", "startOffset": 103, "endOffset": 107}, {"referenceID": 2, "context": "Semantic pixel-wise segmentation is an active topic of research, fuelled by challenging datasets [3], [19], [21], [23], [24].", "startOffset": 97, "endOffset": 100}, {"referenceID": 18, "context": "Semantic pixel-wise segmentation is an active topic of research, fuelled by challenging datasets [3], [19], [21], [23], [24].", "startOffset": 102, "endOffset": 106}, {"referenceID": 20, "context": "Semantic pixel-wise segmentation is an active topic of research, fuelled by challenging datasets [3], [19], [21], [23], [24].", "startOffset": 108, "endOffset": 112}, {"referenceID": 22, "context": "Semantic pixel-wise segmentation is an active topic of research, fuelled by challenging datasets [3], [19], [21], [23], [24].", "startOffset": 114, "endOffset": 118}, {"referenceID": 23, "context": "Semantic pixel-wise segmentation is an active topic of research, fuelled by challenging datasets [3], [19], [21], [23], [24].", "startOffset": 120, "endOffset": 124}, {"referenceID": 24, "context": "Forest [25], [26] or Boosting [27], [28] to predict the class probabilities of the center pixel.", "startOffset": 7, "endOffset": 11}, {"referenceID": 25, "context": "Forest [25], [26] or Boosting [27], [28] to predict the class probabilities of the center pixel.", "startOffset": 13, "endOffset": 17}, {"referenceID": 26, "context": "Forest [25], [26] or Boosting [27], [28] to predict the class probabilities of the center pixel.", "startOffset": 30, "endOffset": 34}, {"referenceID": 27, "context": "Forest [25], [26] or Boosting [27], [28] to predict the class probabilities of the center pixel.", "startOffset": 36, "endOffset": 40}, {"referenceID": 24, "context": "Features based on appearance [25] or SfM and appearance [26], [27], [28] have been explored for the CamVid road scene understanding test [3].", "startOffset": 29, "endOffset": 33}, {"referenceID": 25, "context": "Features based on appearance [25] or SfM and appearance [26], [27], [28] have been explored for the CamVid road scene understanding test [3].", "startOffset": 56, "endOffset": 60}, {"referenceID": 26, "context": "Features based on appearance [25] or SfM and appearance [26], [27], [28] have been explored for the CamVid road scene understanding test [3].", "startOffset": 62, "endOffset": 66}, {"referenceID": 27, "context": "Features based on appearance [25] or SfM and appearance [26], [27], [28] have been explored for the CamVid road scene understanding test [3].", "startOffset": 68, "endOffset": 72}, {"referenceID": 2, "context": "Features based on appearance [25] or SfM and appearance [26], [27], [28] have been explored for the CamVid road scene understanding test [3].", "startOffset": 137, "endOffset": 140}, {"referenceID": 26, "context": "These per-pixel noisy predictions (often called unary terms) from the classifiers are then smoothed by using a pair-wise or higher order CRF [27], [28] to improve the accuracy.", "startOffset": 141, "endOffset": 145}, {"referenceID": 27, "context": "These per-pixel noisy predictions (often called unary terms) from the classifiers are then smoothed by using a pair-wise or higher order CRF [27], [28] to improve the accuracy.", "startOffset": 147, "endOffset": 151}, {"referenceID": 28, "context": "This improves the results of Random Forest based unaries [29] but thin structured classes are classfied poorly.", "startOffset": 57, "endOffset": 61}, {"referenceID": 29, "context": "Dense depth maps computed from the CamVid video have also been used as input for classification using Random Forests [30].", "startOffset": 117, "endOffset": 121}, {"referenceID": 30, "context": "Another approach argues for the use of a combination of popular hand designed features and spatio temporal super-pixelization to obtain higher accuracy [31].", "startOffset": 152, "endOffset": 156}, {"referenceID": 27, "context": "The best performing technique on the CamVid test [28] addresses the imbalance among label frequencies by combining object detection outputs with classifier predictions in a CRF framework.", "startOffset": 49, "endOffset": 53}, {"referenceID": 22, "context": "Indoor RGBD pixel-wise semantic segmentation has also gained popularity since the release of the NYU dataset [23].", "startOffset": 109, "endOffset": 113}, {"referenceID": 31, "context": "Improvements were made using a richer feature set including LBP and region segmentation to obtain higher accuracy [32] followed by a CRF.", "startOffset": 114, "endOffset": 118}, {"referenceID": 22, "context": "In more recent work [23], both class segmentation and support relationships are inferred together using a combination of RGB and depth based cues.", "startOffset": 20, "endOffset": 24}, {"referenceID": 32, "context": "Another approach focusses on real-time joint reconstruction and semantic segmentation, where Random Forests are used as the classifier [33].", "startOffset": 135, "endOffset": 139}, {"referenceID": 33, "context": "[34] use boundary detection and hierarchical grouping before performing category segmentation.", "startOffset": 0, "endOffset": 4}, {"referenceID": 5, "context": "There have also been attempts to apply networks designed for object categorization to segmentation, particularly by replicating the deepest layer features in blocks to match image dimensions [6], [35], [36], [37].", "startOffset": 191, "endOffset": 194}, {"referenceID": 34, "context": "There have also been attempts to apply networks designed for object categorization to segmentation, particularly by replicating the deepest layer features in blocks to match image dimensions [6], [35], [36], [37].", "startOffset": 196, "endOffset": 200}, {"referenceID": 35, "context": "There have also been attempts to apply networks designed for object categorization to segmentation, particularly by replicating the deepest layer features in blocks to match image dimensions [6], [35], [36], [37].", "startOffset": 202, "endOffset": 206}, {"referenceID": 36, "context": "There have also been attempts to apply networks designed for object categorization to segmentation, particularly by replicating the deepest layer features in blocks to match image dimensions [6], [35], [36], [37].", "startOffset": 208, "endOffset": 212}, {"referenceID": 35, "context": "However, the resulting classification is blocky [36].", "startOffset": 48, "endOffset": 52}, {"referenceID": 37, "context": "Another approach using recurrent neural networks [38] merges several low resolution predictions to create input image resolution predictions.", "startOffset": 49, "endOffset": 53}, {"referenceID": 5, "context": "These techniques are already an improvement over hand engineered features [6] but their ability to delineate boundaries is poor.", "startOffset": 74, "endOffset": 77}, {"referenceID": 1, "context": "Newer deep architectures [2], [9], [10], [13], [16] particularly designed for segmentation have advanced the state-of-the-art by learning to decode or map low resolution image representations to pixel-wise predictions.", "startOffset": 25, "endOffset": 28}, {"referenceID": 8, "context": "Newer deep architectures [2], [9], [10], [13], [16] particularly designed for segmentation have advanced the state-of-the-art by learning to decode or map low resolution image representations to pixel-wise predictions.", "startOffset": 30, "endOffset": 33}, {"referenceID": 9, "context": "Newer deep architectures [2], [9], [10], [13], [16] particularly designed for segmentation have advanced the state-of-the-art by learning to decode or map low resolution image representations to pixel-wise predictions.", "startOffset": 35, "endOffset": 39}, {"referenceID": 12, "context": "Newer deep architectures [2], [9], [10], [13], [16] particularly designed for segmentation have advanced the state-of-the-art by learning to decode or map low resolution image representations to pixel-wise predictions.", "startOffset": 41, "endOffset": 45}, {"referenceID": 15, "context": "Newer deep architectures [2], [9], [10], [13], [16] particularly designed for segmentation have advanced the state-of-the-art by learning to decode or map low resolution image representations to pixel-wise predictions.", "startOffset": 47, "endOffset": 51}, {"referenceID": 0, "context": "The encoder network which produces these low resolution representations in all of these architectures is the VGG16 classification network [1] which has 13 convolutional layers and 3 fully connected layers.", "startOffset": 138, "endOffset": 141}, {"referenceID": 38, "context": "This encoder network weights are typically pre-trained on the large ImageNet object classification dataset [39].", "startOffset": 107, "endOffset": 111}, {"referenceID": 1, "context": "Each decoder in the Fully Convolutional Network (FCN) architecture [2] learns to upsample its input feature map(s) and combines them with the corresponding encoder feature map to produce the input to the next decoder.", "startOffset": 67, "endOffset": 70}, {"referenceID": 8, "context": "This growth is stopped after three decoders thus ignoring high resolution feature maps can certainly lead to loss of edge information [9].", "startOffset": 134, "endOffset": 137}, {"referenceID": 9, "context": "We study this network in more detail as it the core of other recent architectures [10], [11].", "startOffset": 82, "endOffset": 86}, {"referenceID": 10, "context": "We study this network in more detail as it the core of other recent architectures [10], [11].", "startOffset": 88, "endOffset": 92}, {"referenceID": 9, "context": "The predictive performance of FCN has been improved further by appending the FCN with a recurrent neural network (RNN) [10] and fine-tuning them on large datasets [19], [40].", "startOffset": 119, "endOffset": 123}, {"referenceID": 18, "context": "The predictive performance of FCN has been improved further by appending the FCN with a recurrent neural network (RNN) [10] and fine-tuning them on large datasets [19], [40].", "startOffset": 163, "endOffset": 167}, {"referenceID": 39, "context": "The predictive performance of FCN has been improved further by appending the FCN with a recurrent neural network (RNN) [10] and fine-tuning them on large datasets [19], [40].", "startOffset": 169, "endOffset": 173}, {"referenceID": 40, "context": "The fact that joint training helps is also shown in other recent results [41], [42].", "startOffset": 73, "endOffset": 77}, {"referenceID": 41, "context": "The fact that joint training helps is also shown in other recent results [41], [42].", "startOffset": 79, "endOffset": 83}, {"referenceID": 8, "context": "Interestingly, the deconvolutional network [9] performs significantly better than FCN although at the cost of a more complex training and inference.", "startOffset": 43, "endOffset": 46}, {"referenceID": 12, "context": "Multi-scale deep architectures are also being pursued [13], [42].", "startOffset": 54, "endOffset": 58}, {"referenceID": 41, "context": "Multi-scale deep architectures are also being pursued [13], [42].", "startOffset": 60, "endOffset": 64}, {"referenceID": 42, "context": "They come in two flavours, (i) those which use input images at a few scales and corresponding deep feature extraction networks, and (ii) those which combine feature maps from different layers of a single deep architecture [43] [11].", "startOffset": 222, "endOffset": 226}, {"referenceID": 10, "context": "They come in two flavours, (i) those which use input images at a few scales and corresponding deep feature extraction networks, and (ii) those which combine feature maps from different layers of a single deep architecture [43] [11].", "startOffset": 227, "endOffset": 231}, {"referenceID": 43, "context": "The common idea is to use features extracted at multiple scales to provide both local and global context [44] and the using feature maps of the early encoding layers retain more high frequency detail leading to sharper class boundaries.", "startOffset": 105, "endOffset": 109}, {"referenceID": 12, "context": "Some of these architectures are difficult to train due to their parameter size [13].", "startOffset": 79, "endOffset": 83}, {"referenceID": 41, "context": "Others [42] append a CRF to their multi-scale network and jointly train them.", "startOffset": 7, "endOffset": 11}, {"referenceID": 8, "context": "Several of the recently proposed deep architectures for segmentation are not feed-forward in inference time [9], [14], [16].", "startOffset": 108, "endOffset": 111}, {"referenceID": 13, "context": "Several of the recently proposed deep architectures for segmentation are not feed-forward in inference time [9], [14], [16].", "startOffset": 113, "endOffset": 117}, {"referenceID": 15, "context": "Several of the recently proposed deep architectures for segmentation are not feed-forward in inference time [9], [14], [16].", "startOffset": 119, "endOffset": 123}, {"referenceID": 41, "context": "They require either MAP inference over a CRF [42], [41] or aids such as region proposals [9] for inference.", "startOffset": 45, "endOffset": 49}, {"referenceID": 40, "context": "They require either MAP inference over a CRF [42], [41] or aids such as region proposals [9] for inference.", "startOffset": 51, "endOffset": 55}, {"referenceID": 8, "context": "They require either MAP inference over a CRF [42], [41] or aids such as region proposals [9] for inference.", "startOffset": 89, "endOffset": 92}, {"referenceID": 8, "context": "The recently proposed Deconvolutional Network [9] and its semi-supervised variant the Decoupled network [16] use the max locations of the encoder feature maps (pooling indices) to perform non-linear upsampling in the decoder network.", "startOffset": 46, "endOffset": 49}, {"referenceID": 15, "context": "The recently proposed Deconvolutional Network [9] and its semi-supervised variant the Decoupled network [16] use the max locations of the encoder feature maps (pooling indices) to perform non-linear upsampling in the decoder network.", "startOffset": 104, "endOffset": 108}, {"referenceID": 11, "context": "CVPR 2015 [12]), proposed this idea of decoding in the decoder network.", "startOffset": 10, "endOffset": 14}, {"referenceID": 13, "context": "Another recent method [14] shows the benefit of reducing the number of parameters significantly without sacrificing performance, reducing memory consumption and improving inference time.", "startOffset": 22, "endOffset": 26}, {"referenceID": 16, "context": "[17].", "startOffset": 0, "endOffset": 4}, {"referenceID": 44, "context": "A somewhat similar decoding technique is used for visualizing trained convolutional networks [45] for classification.", "startOffset": 93, "endOffset": 97}, {"referenceID": 45, "context": "[46] to accept full image sizes as input to learn hierarchical encoders.", "startOffset": 0, "endOffset": 4}, {"referenceID": 46, "context": "Other applications where pixel wise predictions are made using deep networks are image super-resolution [47] and depth map prediction from a single image [48].", "startOffset": 104, "endOffset": 108}, {"referenceID": 47, "context": "Other applications where pixel wise predictions are made using deep networks are image super-resolution [47] and depth map prediction from a single image [48].", "startOffset": 154, "endOffset": 158}, {"referenceID": 47, "context": "The authors in [48] discuss the need for learning to upsample from low resolution feature maps which is the central topic of this paper.", "startOffset": 15, "endOffset": 19}, {"referenceID": 0, "context": "The encoder network consists of 13 convolutional layers which correspond to the first 13 convolutional layers in the VGG16 network [1] designed for object classification.", "startOffset": 131, "endOffset": 134}, {"referenceID": 38, "context": "We can therefore initialize the training process from weights trained for classification on large datasets [39].", "startOffset": 107, "endOffset": 111}, {"referenceID": 1, "context": "7M) as compared to other recent architectures [2], [9] (see.", "startOffset": 46, "endOffset": 49}, {"referenceID": 8, "context": "7M) as compared to other recent architectures [2], [9] (see.", "startOffset": 51, "endOffset": 54}, {"referenceID": 48, "context": "These are then batch normalized [49]).", "startOffset": 32, "endOffset": 36}, {"referenceID": 1, "context": "Many segmentation architectures [2], [9], [14] share the same encoder network and they only vary in the form of their decoder network.", "startOffset": 32, "endOffset": 35}, {"referenceID": 8, "context": "Many segmentation architectures [2], [9], [14] share the same encoder network and they only vary in the form of their decoder network.", "startOffset": 37, "endOffset": 40}, {"referenceID": 13, "context": "Many segmentation architectures [2], [9], [14] share the same encoder network and they only vary in the form of their decoder network.", "startOffset": 42, "endOffset": 46}, {"referenceID": 1, "context": "Of these we choose to compare the SegNet decoding technique with the widely used Fully Convolutional Network (FCN) decoding technique [2], [10].", "startOffset": 134, "endOffset": 137}, {"referenceID": 9, "context": "Of these we choose to compare the SegNet decoding technique with the widely used Fully Convolutional Network (FCN) decoding technique [2], [10].", "startOffset": 139, "endOffset": 143}, {"referenceID": 11, "context": "SegNet-Basic was earlier termed SegNet in a archival version of this paper [12] compresses the encoder feature maps which are then used in the corresponding decoders.", "startOffset": 75, "endOffset": 79}, {"referenceID": 1, "context": "The upsampling kernels are initialized using bilinear interpolation weights [2].", "startOffset": 76, "endOffset": 79}, {"referenceID": 5, "context": "We also tried other generic variants where feature maps are simply upsampled by replication [6], or by using a fixed (and sparse) array of indices for upsampling.", "startOffset": 92, "endOffset": 95}, {"referenceID": 49, "context": "We perform local contrast normalization [50] to the RGB input.", "startOffset": 40, "endOffset": 44}, {"referenceID": 50, "context": "[51].", "startOffset": 0, "endOffset": 4}, {"referenceID": 14, "context": "9 [15] using our Caffe implementation of SegNet-Basic [52].", "startOffset": 2, "endOffset": 6}, {"referenceID": 51, "context": "9 [15] using our Caffe implementation of SegNet-Basic [52].", "startOffset": 54, "endOffset": 58}, {"referenceID": 1, "context": "An illustration of SegNet and FCN [2] decoders.", "startOffset": 34, "endOffset": 37}, {"referenceID": 1, "context": "We use the cross-entropy loss [2] as the objective function for training the network.", "startOffset": 30, "endOffset": 33}, {"referenceID": 12, "context": "We use median frequency balancing [13] where the weight assigned to a class in the loss function is the ratio of the median of class frequencies computed on the entire training set divided by the class frequency.", "startOffset": 34, "endOffset": 38}, {"referenceID": 18, "context": "To compare the quantitative performance of the different decoder variants, we use three commonly used performance measures: global accuracy (G) which measures the percentage of pixels correctly classified in the dataset, class average accuracy (C) is the mean of the predictive accuracy over all classes and mean intersection over union (I/U) over all classes as used in the Pascal VOC12 challenge [19].", "startOffset": 398, "endOffset": 402}, {"referenceID": 8, "context": "This is also supported by experimental evidence gathered by other authors when comparing FCN with SegNet-type decoding techniques [9].", "startOffset": 130, "endOffset": 133}, {"referenceID": 24, "context": "We use this result to compare SegNet with several methods including Random Forests [25], Boosting [25], [27] in combination with CRF based methods [28].", "startOffset": 83, "endOffset": 87}, {"referenceID": 24, "context": "We use this result to compare SegNet with several methods including Random Forests [25], Boosting [25], [27] in combination with CRF based methods [28].", "startOffset": 98, "endOffset": 102}, {"referenceID": 26, "context": "We use this result to compare SegNet with several methods including Random Forests [25], Boosting [25], [27] in combination with CRF based methods [28].", "startOffset": 104, "endOffset": 108}, {"referenceID": 27, "context": "We use this result to compare SegNet with several methods including Random Forests [25], Boosting [25], [27] in combination with CRF based methods [28].", "startOffset": 147, "endOffset": 151}, {"referenceID": 2, "context": "We also trained SegNet on a larger dataset of road scenes collected from various publicly available datasets [3], [53], [54] and show that this leads to a large improvement in accuracy.", "startOffset": 109, "endOffset": 112}, {"referenceID": 52, "context": "We also trained SegNet on a larger dataset of road scenes collected from various publicly available datasets [3], [53], [54] and show that this leads to a large improvement in accuracy.", "startOffset": 114, "endOffset": 118}, {"referenceID": 53, "context": "We also trained SegNet on a larger dataset of road scenes collected from various publicly available datasets [3], [53], [54] and show that this leads to a large improvement in accuracy.", "startOffset": 120, "endOffset": 124}, {"referenceID": 20, "context": "SUN RGB-D [21] is a very challenging and large dataset of indoor scenes with 5285 training and 5050 testing images.", "startOffset": 10, "endOffset": 14}, {"referenceID": 1, "context": "Using the depth modality would necessitate architectural modifications/redesign [2].", "startOffset": 80, "endOffset": 83}, {"referenceID": 22, "context": "We also note that an earlier benchmark dataset NYUv2 [23] is included as part of this dataset.", "startOffset": 53, "endOffset": 57}, {"referenceID": 18, "context": "Pascal VOC12 [19] is a RGB dataset for segmentation with 12031 combined training and validation images of indoor and outdoor scenes.", "startOffset": 13, "endOffset": 17}, {"referenceID": 2, "context": "A number of outdoor scene datasets are available for semantic parsing [3], [24], [55], [56].", "startOffset": 70, "endOffset": 73}, {"referenceID": 23, "context": "A number of outdoor scene datasets are available for semantic parsing [3], [24], [55], [56].", "startOffset": 75, "endOffset": 79}, {"referenceID": 54, "context": "A number of outdoor scene datasets are available for semantic parsing [3], [24], [55], [56].", "startOffset": 81, "endOffset": 85}, {"referenceID": 55, "context": "A number of outdoor scene datasets are available for semantic parsing [3], [24], [55], [56].", "startOffset": 87, "endOffset": 91}, {"referenceID": 2, "context": "Of these we choose to benchmark SegNet using the CamVid dataset [3] as it contains video sequences.", "startOffset": 64, "endOffset": 67}, {"referenceID": 25, "context": "This enables us to compare our proposed architecture with those which use motion and structure [26], [27], [28] and video segments [31].", "startOffset": 95, "endOffset": 99}, {"referenceID": 26, "context": "This enables us to compare our proposed architecture with those which use motion and structure [26], [27], [28] and video segments [31].", "startOffset": 101, "endOffset": 105}, {"referenceID": 27, "context": "This enables us to compare our proposed architecture with those which use motion and structure [26], [27], [28] and video segments [31].", "startOffset": 107, "endOffset": 111}, {"referenceID": 30, "context": "This enables us to compare our proposed architecture with those which use motion and structure [26], [27], [28] and video segments [31].", "startOffset": 131, "endOffset": 135}, {"referenceID": 56, "context": "More dense CRF models [57] can be better but with additional cost of inference.", "startOffset": 22, "endOffset": 26}, {"referenceID": 10, "context": "Another explanation could be that the size of the receptive fields of the deepest layer feature units are smaller than their theoretical estimates [11], [58] and hence unable to group all the side-walk pixels into one class.", "startOffset": 147, "endOffset": 151}, {"referenceID": 57, "context": "Another explanation could be that the size of the receptive fields of the deepest layer feature units are smaller than their theoretical estimates [11], [58] and hence unable to group all the side-walk pixels into one class.", "startOffset": 153, "endOffset": 157}, {"referenceID": 11, "context": "We also find that SegNetBasic [12] trained in a layer-wise manner using L-BFGS [59] also performs competitively and is better than SegNet-Basic trained with SGD (see Sec.", "startOffset": 30, "endOffset": 34}, {"referenceID": 20, "context": "Some test samples from the recent SUN RGB-D dataset [21] are shown in Fig.", "startOffset": 52, "endOffset": 56}, {"referenceID": 24, "context": "The evolution of results from various patch based predictions [25], [26], then combined with CRF smoothing models [27], [28].", "startOffset": 62, "endOffset": 66}, {"referenceID": 25, "context": "The evolution of results from various patch based predictions [25], [26], then combined with CRF smoothing models [27], [28].", "startOffset": 68, "endOffset": 72}, {"referenceID": 26, "context": "The evolution of results from various patch based predictions [25], [26], then combined with CRF smoothing models [27], [28].", "startOffset": 114, "endOffset": 118}, {"referenceID": 27, "context": "The evolution of results from various patch based predictions [25], [26], then combined with CRF smoothing models [27], [28].", "startOffset": 120, "endOffset": 124}, {"referenceID": 10, "context": "Large sized side-walk is not smoothly segmented possibly because the empirical size of the receptive fields of deep feature units is not large enough [11], [58].", "startOffset": 150, "endOffset": 154}, {"referenceID": 57, "context": "Large sized side-walk is not smoothly segmented possibly because the empirical size of the receptive fields of deep feature units is not large enough [11], [58].", "startOffset": 156, "endOffset": 160}, {"referenceID": 25, "context": "SfM+Appearance [26] 46.", "startOffset": 15, "endOffset": 19}, {"referenceID": 26, "context": "1 n/a Boosting [27] 61.", "startOffset": 15, "endOffset": 19}, {"referenceID": 29, "context": "4 n/a Dense Depth Maps [30] 85.", "startOffset": 23, "endOffset": 27}, {"referenceID": 28, "context": "1 n/a Structured Random Forests [29] n/a 51.", "startOffset": 32, "endOffset": 36}, {"referenceID": 58, "context": "5 n/a Neural Decision Forests [60] n/a 56.", "startOffset": 30, "endOffset": 34}, {"referenceID": 59, "context": "1 n/a Local Label Descriptors [61] 80.", "startOffset": 30, "endOffset": 34}, {"referenceID": 30, "context": "6 n/a Super Parsing [31] 87.", "startOffset": 20, "endOffset": 24}, {"referenceID": 11, "context": "3 SegNet-Basic (layer-wise training [12]) 75.", "startOffset": 36, "endOffset": 40}, {"referenceID": 26, "context": "1 CRF based approaches Boosting + pairwise CRF [27] 70.", "startOffset": 47, "endOffset": 51}, {"referenceID": 26, "context": "8 n/a Boosting+Higher order [27] 84.", "startOffset": 28, "endOffset": 32}, {"referenceID": 27, "context": "8 n/a Boosting+Detectors+CRF [28] 81.", "startOffset": 29, "endOffset": 33}, {"referenceID": 2, "context": "TABLE 2 Quantitative results on CamVid [3] consisting of 11 road scene categories.", "startOffset": 39, "endOffset": 42}, {"referenceID": 18, "context": "Other challenges such as Pascal VOC12 [19] salient object segmentation have occupied researchers more, but indoor scene segmentation is more challenging and has more practical applications such as in robotics.", "startOffset": 38, "endOffset": 42}, {"referenceID": 31, "context": "The existing top performing method [32] relies on hand engineered features using colour, gradients and surface normals for describing super-pixels and then smooths super-pixel labels with a CRF.", "startOffset": 35, "endOffset": 39}, {"referenceID": 60, "context": "[62] n/a 9.", "startOffset": 0, "endOffset": 4}, {"referenceID": 60, "context": "[62] n/a 10.", "startOffset": 0, "endOffset": 4}, {"referenceID": 31, "context": "al [32] n/a 36.", "startOffset": 3, "endOffset": 7}, {"referenceID": 31, "context": "SegNet RGB based predictions have a high global accuracy and also matches the RGB-D based predictions [32] in terms of class average accuracy.", "startOffset": 102, "endOffset": 106}, {"referenceID": 18, "context": "The Pascal VOC12 segmentation challenge [19] consists of segmenting a few salient object classes from a widely varying background class.", "startOffset": 40, "endOffset": 44}, {"referenceID": 1, "context": "Our efforts in this benchmarking experiment have not been diverted towards attaining the top rank by either using multi-stage training [2], other datasets for pre-training such as MS-COCO [10], [40], training and inference aids such as object proposals [9], [22] or post-processing using CRF based methods [9], [14].", "startOffset": 135, "endOffset": 138}, {"referenceID": 9, "context": "Our efforts in this benchmarking experiment have not been diverted towards attaining the top rank by either using multi-stage training [2], other datasets for pre-training such as MS-COCO [10], [40], training and inference aids such as object proposals [9], [22] or post-processing using CRF based methods [9], [14].", "startOffset": 188, "endOffset": 192}, {"referenceID": 39, "context": "Our efforts in this benchmarking experiment have not been diverted towards attaining the top rank by either using multi-stage training [2], other datasets for pre-training such as MS-COCO [10], [40], training and inference aids such as object proposals [9], [22] or post-processing using CRF based methods [9], [14].", "startOffset": 194, "endOffset": 198}, {"referenceID": 8, "context": "Our efforts in this benchmarking experiment have not been diverted towards attaining the top rank by either using multi-stage training [2], other datasets for pre-training such as MS-COCO [10], [40], training and inference aids such as object proposals [9], [22] or post-processing using CRF based methods [9], [14].", "startOffset": 253, "endOffset": 256}, {"referenceID": 21, "context": "Our efforts in this benchmarking experiment have not been diverted towards attaining the top rank by either using multi-stage training [2], other datasets for pre-training such as MS-COCO [10], [40], training and inference aids such as object proposals [9], [22] or post-processing using CRF based methods [9], [14].", "startOffset": 258, "endOffset": 262}, {"referenceID": 8, "context": "Our efforts in this benchmarking experiment have not been diverted towards attaining the top rank by either using multi-stage training [2], other datasets for pre-training such as MS-COCO [10], [40], training and inference aids such as object proposals [9], [22] or post-processing using CRF based methods [9], [14].", "startOffset": 306, "endOffset": 309}, {"referenceID": 13, "context": "Our efforts in this benchmarking experiment have not been diverted towards attaining the top rank by either using multi-stage training [2], other datasets for pre-training such as MS-COCO [10], [40], training and inference aids such as object proposals [9], [22] or post-processing using CRF based methods [9], [14].", "startOffset": 311, "endOffset": 315}, {"referenceID": 20, "context": "Qualitative assessment of SegNet predictions on RGB indoor test scenes from the recently released SUN RGB-D dataset [21].", "startOffset": 116, "endOffset": 120}, {"referenceID": 13, "context": "We also specify when a method reports the performance of its core engine on the smaller validation set of 346 images [14].", "startOffset": 117, "endOffset": 121}, {"referenceID": 8, "context": "From the results in Table 5 we can see that the best performing networks are either very large (and slow) [9] and/or they use a CRF [10].", "startOffset": 106, "endOffset": 109}, {"referenceID": 9, "context": "From the results in Table 5 we can see that the best performing networks are either very large (and slow) [9] and/or they use a CRF [10].", "startOffset": 132, "endOffset": 136}, {"referenceID": 9, "context": "This is shown in the experiments using CRF-RNN [10] wherein the core FCN-8 model predictions are less accurate without extra training data.", "startOffset": 47, "endOffset": 51}, {"referenceID": 13, "context": "It is interesting that the DeepLab [14] architecture which is simply upsampling the FCN encoder features using bilinear interpolation performs reasonably well (on the validation set).", "startOffset": 35, "endOffset": 39}, {"referenceID": 8, "context": "Methods using object proposals during training and/or inference [9], [43] are very slow in inference time and it is hard to measure their true performance.", "startOffset": 64, "endOffset": 67}, {"referenceID": 42, "context": "Methods using object proposals during training and/or inference [9], [43] are very slow in inference time and it is hard to measure their true performance.", "startOffset": 69, "endOffset": 73}, {"referenceID": 8, "context": "These aids are necessitated by the very large size of their deep network [9] and also because the Pascal data can also be processed by a detect and segment approach.", "startOffset": 73, "endOffset": 76}, {"referenceID": 0, "context": "In comparison, SegNet is smaller by virtue of discarding the fully connected layers in the VGG16 [1].", "startOffset": 97, "endOffset": 100}, {"referenceID": 13, "context": "The authors of DeepLab [14] have also reported little loss in performance by reducing the size of the fully connected layers.", "startOffset": 23, "endOffset": 27}, {"referenceID": 39, "context": "We believe more training data [40] can help improve the performance of SegNet.", "startOffset": 30, "endOffset": 34}, {"referenceID": 61, "context": "We are also interested in experimenting with Dropout [63] during training and testing.", "startOffset": 53, "endOffset": 57}, {"referenceID": 18, "context": "Qualitative assessment of SegNet predictions on test samples from Pascal VOC12 [19] dataset.", "startOffset": 79, "endOffset": 83}, {"referenceID": 57, "context": "This can be perhaps be attributed to the smaller empirical size of the receptive field of the feature units in the deepest encoder layer size [58].", "startOffset": 142, "endOffset": 146}, {"referenceID": 13, "context": "DeepLab [14] (validation set) n/a n/a < 134.", "startOffset": 8, "endOffset": 12}, {"referenceID": 1, "context": "5 58 n/a n/a FCN-8 [2] (multi-stage training) 134 0.", "startOffset": 19, "endOffset": 22}, {"referenceID": 42, "context": "2 210 n/a Hypercolumns [43] (object proposals) n/a n/a > 134.", "startOffset": 23, "endOffset": 27}, {"referenceID": 8, "context": "6 n/a n/a DeconvNet [9] (object proposals) 138.", "startOffset": 20, "endOffset": 23}, {"referenceID": 9, "context": "26 (\u00d7 50) CRF-RNN [10] (multi-stage training) n/a n/a > 134.", "startOffset": 18, "endOffset": 22}], "year": 2015, "abstractText": "We present a novel and practical deep fully convolutional neural network architecture for semantic pixel-wise segmentation termed SegNet. This core trainable segmentation engine consists of an encoder network, a corresponding decoder network followed by a pixel-wise classification layer. The architecture of the encoder network is topologically identical to the 13 convolutional layers in the VGG16 network [1]. The role of the decoder network is to map the low resolution encoder feature maps to full input resolution feature maps for pixel-wise classification. The novelty of SegNet lies is in the manner in which the decoder upsamples its lower resolution input feature map(s). Specifically, the decoder uses pooling indices computed in the max-pooling step of the corresponding encoder to perform non-linear upsampling. This eliminates the need for learning to upsample. The upsampled maps are sparse and are then convolved with trainable filters to produce dense feature maps. We compare our proposed architecture with the fully convolutional network [2] architecture and its variants. This comparison reveals the memory versus accuracy trade-off involved in achieving good segmentation performance. The design of SegNet was primarily motivated by road scene understanding applications. Hence, it is efficient both in terms of memory and computational time during inference. It is also significantly smaller in the number of trainable parameters than competing architectures and can be trained end-to-end using stochastic gradient descent without complex training protocols. We also benchmark the performance of SegNet on Pascal VOC12 salient object segmentation and the recent SUN RGB-D indoor scene understanding challenge. These quantitative assessments show that SegNet provides competitive performance although it is significantly smaller than other architectures. We also provide a Caffe implementation of SegNet and a webdemo at http://mi.eng.cam.ac.uk/projects/segnet/.", "creator": "LaTeX with hyperref package"}}}