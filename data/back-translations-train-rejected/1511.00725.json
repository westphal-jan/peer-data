{"id": "1511.00725", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "2-Nov-2015", "title": "Galaxy-X: A Novel Approach for Multi-class Classification in an Open Universe", "abstract": "Classification is a fundamental task in machine learning and artificial intelligence. Existing classification methods are designed to classify unknown instances within a set of previously known classes that are seen in training. Such classification takes the form of prediction within a closed-set. However, a more realistic scenario that fits the ground truth of real world applications is to consider the possibility of encountering instances that do not belong to any of the classes that are seen in training, $i.e.$, an open-set classification. In such situation, existing closed-set classification methods will assign a training label to these instances resulting in a misclassification. In this paper, we introduce Galaxy-X, a novel multi-class classification method for open-set problem. For each class of the training set, Galaxy-X creates a minimum bounding hyper-sphere that encompasses the distribution of the class by enclosing all of its instances. In such manner, our method is able to distinguish instances resembling previously seen classes from those that are of unseen classes. Experimental results on benchmark datasets show the efficiency of our approach in classifying novel instances from known as well as unknown classes. We also introduce a novel evaluation procedure to adequately evaluate open-set classification.", "histories": [["v1", "Mon, 2 Nov 2015 22:04:00 GMT  (741kb,D)", "https://arxiv.org/abs/1511.00725v1", null], ["v2", "Mon, 25 Jan 2016 02:27:55 GMT  (1332kb,D)", "http://arxiv.org/abs/1511.00725v2", null]], "reviews": [], "SUBJECTS": "cs.LG cs.AI cs.DB cs.IR", "authors": ["wajdi dhifli", "abdoulaye banir\\'e diallo"], "accepted": false, "id": "1511.00725"}, "pdf": {"name": "1511.00725.pdf", "metadata": {"source": "CRF", "title": "Galaxy-X: A Novel Approach for Multi-class Classification in an Open Universe", "authors": ["Wajdi Dhifli", "Abdoulaye Banir\u00e9 Diallo"], "emails": ["dhifli.wajdi@courrier.uqam.ca,", "diallo.abdoulaye@uqam.ca"], "sections": [{"heading": null, "text": "Classification is a fundamental task in the field of machine learning and artificial intelligence. Existing classification methods are designed to classify unknown instances within a series of previously known classes seen in training. Such classification takes the form of predictions within a closed set. However, a more realistic scenario, which corresponds to the basic truth of applications in the real world, is to consider the possibility of encountering instances that do not belong to one of the classes seen in training, i.e. an open classification. In such a situation, existing closed classification methods assign a training label to instances, leading to a misclassification. In this essay, we present Galaxy-X, a novel multi-class classification method for open problems. For each class of the training set, Galaxy-X creates a minimal limiting hypersphere that encompasses the distribution of the class by encompassing all its instances. In such a way, our method is unable to differentiate from those seen previously in cases that resemble classes."}, {"heading": "1. Introduction", "text": "In fact, most of them are able to determine for themselves what they want to do and what they want to do."}, {"heading": "2. Related Work", "text": "In fact, the fact is that you will be able to go to another world, where you have to go to another world, where you are able to discover another world, where you can go to another world, where you can go to another world, where you can go to another world, where you are able to create a new world, where you are able to create a new world, where you are able to create a new world, where you are able to create a new world."}, {"heading": "3. Galaxy-X", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "3.1. Preliminaries and Problem Definition", "text": "Let D be a learning group of n instances, and L be the group of possible names in D, D = {(x1, l1),..., (xn, ln), where li-L and xi are defined by a vector in d-dimensional space, i.e., lx-L [1, n]. In this constellation, the classifier must be able to assign a label lx to a test instance x that is known to lx-L or that is unknown, i.e., lx-L-unknown. \"In this constellation, it is necessary to define a boundary wrapper for each class to distinguish it from other unknown possibilities. Defining such a boundary is hard and sensitive, since the delimited classroom should reflect the class distribution by enclosing as much of its instances as possible, while keeping the other instances out as much as possible."}, {"heading": "3.2. The Training Process", "text": "Algorithm 1 describes the training phase in Galaxy-X. Starting from a training set D and a training set L over D, we create a model Ml for each class L, which consists of the minimum intermediate point of the hypersphere and the radius rl according to equations 1, 2 and 3. Algorithm 1: Galaxy-X: The training process Data: D: Training set, L: Training set Results: M: Set of class models cl and radius rl, as in equations 1, 2 and 3. Algorithm 1: Galaxy-X: The training process Data: D: Training set, L: Training set Results: M: Set of class models cl and radius rl, as in equations 1, 2 and 3. Algorithm 3 for each class (l, L) do 4 cl \u2190 Zenteroid (Dl) 5 rl \u2190 Boundary (Dl, rl) 6 Ml (cl, rl) 7 M \u2190 M, Ml \u2190"}, {"heading": "3.3. Acceptance of Instances", "text": "In the open-set classification, the classifier should be able to distinguish between instances of the different known classes and reject those of the unknown classes. Therefore, we define an assessment of the acceptance of an instance by a class depending on its position on the class boundary. Definition 1. (Acceptance Score) Acceptance of an instance x by a class of label l is defined as follows: AcceptanceScore (x, l) = 1 \u2212 Distance (x, cl) rl (4), where distance is an appropriate distance measure, cl is the center of class l, and rl is its radius. The acceptance score is defined in] \u2212 \u221e, 1] (i.e. AcceptanceScore-Score-IR \u2264 1) and allows to decide whether an instance is accepted or rejected by a class. The score is interpreted as follows: \u2022 SceptanceScore-Score-Class [0, 1]: AcceptanceScore is acceptable."}, {"heading": "3.4. The Classification Process", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "3.4.1. Filtering Prediction Candidate labels", "text": "Based on the query result, it is possible to filter for a given query instance x the subset of candidate names Lx, Lx L. The latter is the subset of the remaining possible candidates, so that at Lx 6 = \u2205, then the prediction name lx is an element of it, lx Lx. The general algorithm of filtering candidate class names is described in Algorithm 2. It starts with an empty series of candidate names. In view of the composition of the training class models, it is checked whether the query instance x is accepted or rejected by each training class according to Definition 1. In fact, all class names where the query instance does not match the class distribution, i.e. if x is outside the class boundary, are rejected. Only the subset of names of accepted classes is considered as possible candidate names for the prediction. Algorithm 2: Galaxy-X: The label filters process data: M: Set of class models, Examination instance: Lancx reserved for each candidate (Preliminary instance: 1)."}, {"heading": "3.4.2. Handling Class Overlapping", "text": "In such a case, when a query instance x is circumscribed by a hypersphere, x assumes the class label of the latter, otherwise x is considered an unknown class. However, in real cases, the hyperspheres may overlap mainly in the presence of a high degree of similarity between classes. Indeed, the overlapping space between classes resembles a locally closed classification within an open classification context. In this case, a local closed classifier is formed only by the overlapping classes, which are then used only for classifying query instances located within the overlapping space, i.e. instances accepted by several classes in algorithm 2, | Lx | > 1."}, {"heading": "3.4.3. The Classification Process", "text": "Algorithm 3 describes the classification process of Galaxy-X. After training, the first step in prediction is filtering candidate labels according to algorithm 2. If the predicted set of candidate labels is an empty set FLx = \u2205, then the query instance x does not match one of the distributions of the training class. As it is an open classification, the predicted set lx is set to \"Unknown.\" If | FLx | = 1, x is only accepted by one training class, in which case the predicted label is the only filtered option lx \u2190 FLx. In the case where | FLx | > 1 x has a degree of similarity to more than one class and is located in the overlap area between the hyperspheres of the retained class labels, since this situation is a conventional closed classification, a closed Classified E is trained locally only on the retained classes of FLx, then the class label is used to predict E."}, {"heading": "3.5. Softening Class Boundaries", "text": "To add flexibility to the models, we introduce a softening parameter that allows distortion of the class boundary. In fact, it allows to add more generalization or specialization to the classification models - as a trade-off between sensitivity (recall) and specificity. Figure 1 shows examples of positive and negative softening of a class boundary in each case. In Figure 1 (a), a positive softening expands the radius of the minimum boundary hypersphere, which allows to add more generalization to the class model. Extending the class boundary can help detect test cases that are of the same class but slightly different from the training instances. In Figure 1 (b), a negative softening is performed that shrinks the radius of the hypersphere and adds further specialization of the class model."}, {"heading": "4. Experimental Evaluation", "text": "The evaluation of open, multi-level learning methods requires the definition of appropriate measures and protocols."}, {"heading": "4.1. How Open is an Open-set Classification?", "text": "We propose openness as a measure to quantify the openness of a classification scenario (SD). Definition 3. (Openness) It measures the ratio of labels that are not seen in training but occur in the prediction of all labels of dataset D. Openness is defined as follows: Openness (SD) = | UnseenLabels | | L | (9) Openness is defined in IR +. An openness value of 0 means that it is a closed classification scenario, otherwise it is an open classification. Theoretically, the value of openness can even be + \u221e, which means an infinite number of possibilities. In practical cases, however, the number of test labels can usually be delimited. In our experiments, openness is specified as 0, 1 [because the open classification is simulated using a benchmark dataset, i.e., all possible labels are known, i.e. L | = training labels correspond to this context."}, {"heading": "4.2. Evaluation Technique", "text": "Traditional evaluation techniques such as leave-validation, Random-P-ClassOut-CrossValidation are not suitable for open-set classification. They were originally designed for open-set classification and therefore do not represent sufficient limitations for labels to simulate an open-set classification. We propose Leave-P-ClassOut-CrossValidation, a novel evaluation technology for open-set classification, which allows to simulate an open-set classification that is similar to real-life applications where we do not have knowledge of all classes in education. The general procedure of leave-P-Class-Out-CrossValidation is described in Algorithm4. Firstly, all possible combinations C of the P-labels from L are calculated."}, {"heading": "4.3. Evaluation Measures", "text": "The natural method of evaluating the classification is to use the measure of accuracy, which refers to the amount of correctly classified instances from the dataset. In multi-class classification, accuracy is averaged across all classes of the dataset. However, the negative rate can exceed the positive rate, which inflates the classification results, leading to an overestimation of the performance of the classifier. Furthermore, the number of test classes is (at least in theory) undefined. The F measure (also referred to as the f-score), the harmonious means of precision and recall, is a good alternative to open classification. Formally, it is defined as: F measure = 2 x precision \u00d7 Recall Precision + Recall (10) Where precision is TPTP + FN and Recall is TPTP + FP. We use the weighted version of the f measure as a rating metric for our experiments. F measure is calculated for each label, the results are then weighted by each label and each label."}, {"heading": "4.4. Experimental Protocol and Settings", "text": "In order to ensure an even participation of all used attributes in the classification, a min-max normalization is applied to each attribute independently so that no attribute dominates in the prediction (xnormalized = x \u2212 min max \u2212 min, where x is an attribute value, min and max are the minimum and maximum values for the attribute vector).In each experiment, we use Galaxy-X to classify a considered dataset in a simulated open-set classification using the Leave-PClass-Out-CrossValidation valuation.The maximum number of iterations is set to 10 and the number of cross-validations in each iteration is 5. We evaluate the classification performance in terms of weighted f-measure with the incremental values of openness. Classification results are compared with the gold standard multi-class classification strategy One-vs.-v.-rest [6] with a linear SVM-class SVM (SVM) classifier."}, {"heading": "5. Results and Discussion", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "5.1. Evaluation on Classification of Handwriting Digits", "text": "The dataset consists of 1797 cases divided into 10 classes representing the Arabic numerals. Each instance is an 8x8 image of a handwritten numeral, and is therefore represented by a vector of 64 characteristics of values between 0 and 16. Since the dataset is multidimensional, we use multiple learning methods, a non-linear dimensionality reduction, to visualize the distribution of the datasets. We use the t-distributed stochastic classes embedding (t-SNE) to minimize the similarities between the data to common probabilities and attempts."}, {"heading": "5.2. Evaluation on Face Recognition", "text": "We evaluate Galaxy-SVM for facial recognition with the Olivetti Faces Dataset from AT & T Laboratories Cambridge2. This dataset consists of a series of 400 images, 10 images of 40 individuals each. Images were taken at different times, with lighting, facial expressions (open / closed eyes, smiles, etc.) and facial expressions (with / without glasses, etc.) SVS-S each image is 64x64 in size resulting from a feature vector of 4096 grayscale values. Figure 5a shows a2http: / www.cl.ac.uk / research / dtg / facedatabase.htmlsample from the datasets representing 8 individuals. The task is to identify the identity of the individuals depicted. Figure 5b shows a t-SNE visualization of the dataset, where each data point is colored according to the affiliation to the basic class."}, {"heading": "5.3. Evaluation on Synthetic Datasets", "text": "We also evaluate the classification performance of H-Galaxy-SVM, GalaxySVM (\u03b4 = -0.3) on the classification of multiple synthetically generated data sets. We also compare the results obtained with those of OvS-SVM, OCSVM + OvRSVM in the open classification of the same data sets with different disclosure values from 0 to 0.8. The data sets are composed of different classes ranging from 10 to 50 classes. For all data sets, each class is distributed and composed of 500 instances, so that in each cross-validation evaluation 100 instances of each class are used and 400 instances in training. Each instance consists of a two-dimensional numerical vector and the standard deviation for each class distribution is 1. The number of repetitions of experiments is 10 at each classification."}, {"heading": "6. Conclusion", "text": "In this paper, we addressed a fundamental problem of machine learning and artificial intelligence, namely open-set classification, where it is possible to anticipate classes that have not been seen in training and that are better suited to many applications in the real world. In open-set classification, it is necessary to define a decision limit for each class that surrounds its instances and resembles their distribution, and the definition of such a class limit is difficult, as it should offer the best trade-off between generalizing to detect such unknown instances that belong to a training class but differ slightly from their distribution, and specializing to reject unknown instances that do not resemble a known class distribution. In many real-world applications where the closed-world hypothesis does not hold, it is important to recognize such unknown instances and draw the attention of experts to them in order to prevent misclassification."}], "references": [{"title": "Imagenet classification with deep convolutional neural networks", "author": ["A. Krizhevsky", "I. Sutskever", "G.E. Hinton"], "venue": "in: 26th Annual Conference on Neural Information Processing Systems (NIPS\u201912), Curran Associates, Inc.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2012}, {"title": "Machine learning applications in genetics and genomics", "author": ["M.W. Libbrecht", "W.S. Noble"], "venue": "Nature Reviews Genetics 16 ", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2015}, {"title": "Simplifying decision trees", "author": ["J.R. Quinlan"], "venue": "International Journal of Man- Machine Studies 27 (3) ", "citeRegEx": "3", "shortCiteRegEx": null, "year": 1987}, {"title": "Support-vector networks", "author": ["C. Cortes", "V. Vapnik"], "venue": "Machine Learning 20 (3) ", "citeRegEx": "4", "shortCiteRegEx": null, "year": 1995}, {"title": "One-class classification: Concept learning in the absence of counterexamples", "author": ["D. Tax"], "venue": "Ph.D. thesis, Technische Universiteit Delft ", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2001}, {"title": "Multiclass from binary: Expanding one-versusall", "author": ["A. Rocha", "S. Goldenstein"], "venue": "one-versus-one and ecoc-based approaches, IEEE Transactions on Neural Networks and Learning Systems 25 (2) ", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2014}, {"title": "Extreme re-balancing for svms: A case study", "author": ["B. Raskutti", "A. Kowalczyk"], "venue": "ACM SIGKDD Explorations Newsletter 6 (1) ", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2004}, {"title": "Toward open set recognition", "author": ["W. Scheirer", "A. Rocha", "A. Sapkota", "T. Boult"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence 35 (7) ", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2013}, {"title": "Unbiased look at dataset bias", "author": ["A. Torralba", "A.A. Efros"], "venue": "in: Proceedings of the 2011 IEEE Conference on Computer Vision and Pattern Recognition (CVPR\u201911), IEEE Computer Society", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2011}, {"title": "Introduction to Semi- Supervised Learning", "author": ["X. Zhu", "A.B. Goldberg", "R. Brachman", "T. Dietterich"], "venue": "Morgan and Claypool Publishers", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2009}, {"title": "Optimising two-stage recognition systems", "author": ["T. Landgrebe", "P. Pac\u013a\u0131k", "D.M.J. Tax", "R.P.W. Duin"], "venue": "in: Proceedings of the 6th International Conference on Multiple Classifier Systems, MCS\u201905, Springer-Verlag", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2005}, {"title": "Growing a multi-class classifier with a reject option", "author": ["D.M.J. Tax", "R.P.W. Duin"], "venue": "Pattern Recognition Letters 29 (10) ", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2008}, {"title": "Accelerating t-sne using tree-based algorithms", "author": ["L. Van Der Maaten"], "venue": "Journal of Machine Learning Research", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2014}], "referenceMentions": [{"referenceID": 0, "context": "It has an extremely large number of domains of application ranging from aid decision systems in finance and marketing, to bioinformatics and computer vision [1][2].", "startOffset": 157, "endOffset": 160}, {"referenceID": 1, "context": "It has an extremely large number of domains of application ranging from aid decision systems in finance and marketing, to bioinformatics and computer vision [1][2].", "startOffset": 160, "endOffset": 163}, {"referenceID": 2, "context": "the training set, then to predict the class label of unknown instances within the same set of already seen classes [3][4][5][6].", "startOffset": 115, "endOffset": 118}, {"referenceID": 3, "context": "the training set, then to predict the class label of unknown instances within the same set of already seen classes [3][4][5][6].", "startOffset": 118, "endOffset": 121}, {"referenceID": 4, "context": "the training set, then to predict the class label of unknown instances within the same set of already seen classes [3][4][5][6].", "startOffset": 121, "endOffset": 124}, {"referenceID": 5, "context": "the training set, then to predict the class label of unknown instances within the same set of already seen classes [3][4][5][6].", "startOffset": 124, "endOffset": 127}, {"referenceID": 6, "context": "Some attempts have emerged trying to remedy such situation, mainly based on the sampling of a subset of representatives from the negatives [7].", "startOffset": 139, "endOffset": 142}, {"referenceID": 1, "context": "In bioinformatics [2], the advances in sequencing technology have made the acquisition of genomic sequences fast and easy.", "startOffset": 18, "endOffset": 21}, {"referenceID": 7, "context": "Another example of domains of application is computer vision [8].", "startOffset": 61, "endOffset": 64}, {"referenceID": 7, "context": "presented a formalization of open-set classification and showed its importance in real world applications [8].", "startOffset": 106, "endOffset": 109}, {"referenceID": 7, "context": "They showed how recognition accuracies are inflated in closed-set scenarios, leading to an over-estimated confidence in the evaluated approaches [8, 9].", "startOffset": 145, "endOffset": 151}, {"referenceID": 8, "context": "They showed how recognition accuracies are inflated in closed-set scenarios, leading to an over-estimated confidence in the evaluated approaches [8, 9].", "startOffset": 145, "endOffset": 151}, {"referenceID": 7, "context": "-set SVM [8], which defines an additional hyper-plane for each class such that the latter becomes delimited by two hyper-planes in feature space.", "startOffset": 9, "endOffset": 12}, {"referenceID": 9, "context": "Semi-supervised classification [10] have addressed open-set classification to some extent, where part of the dataset is unlabeled and the goal is to label as much as possible of the unlabeled data then use them in training to enhance the performance.", "startOffset": 31, "endOffset": 35}, {"referenceID": 4, "context": "The most known technique is one-class SVM [5] where the classifier is trained only on a single positive class and the aim is to define a contour that encloses it from the rest of the classification universe.", "startOffset": 42, "endOffset": 45}, {"referenceID": 5, "context": "-rest [6] are popular techniques for multi-class classification.", "startOffset": 6, "endOffset": 9}, {"referenceID": 10, "context": "Based on [11] and [12], it is possible to build a simple open-set multi-class classifier using a combination of a one-class classifier and a multi-class classifier.", "startOffset": 9, "endOffset": 13}, {"referenceID": 11, "context": "Based on [11] and [12], it is possible to build a simple open-set multi-class classifier using a combination of a one-class classifier and a multi-class classifier.", "startOffset": 18, "endOffset": 22}, {"referenceID": 0, "context": "\u2022 AcceptanceScore \u2208 [0, 1]: the query instance x is accepted by the class l:", "startOffset": 20, "endOffset": 26}, {"referenceID": 0, "context": "where F \u2208 [0, 1] is a binary function that is defined as follows:", "startOffset": 10, "endOffset": 16}, {"referenceID": 0, "context": "F (x, l) = \uf8f4\uf8f2\uf8f4\uf8f3 1, if AcceptanceScore(x, l) \u2208 [0, 1] and x belongs to l, or", "startOffset": 46, "endOffset": 52}, {"referenceID": 5, "context": "-Rest [6] using a linear SVM as the baseline classifier (OvR-SVM), and with the open-set multi-class classifier One-vs.", "startOffset": 6, "endOffset": 9}, {"referenceID": 7, "context": "[8].", "startOffset": 0, "endOffset": 3}, {"referenceID": 10, "context": "We also build a two-step open-set multi-class classifier, termed OCSVM+OvR-SVM, based on [11] and [12] as discussed in related work (Section2).", "startOffset": 89, "endOffset": 93}, {"referenceID": 11, "context": "We also build a two-step open-set multi-class classifier, termed OCSVM+OvR-SVM, based on [11] and [12] as discussed in related work (Section2).", "startOffset": 98, "endOffset": 102}, {"referenceID": 12, "context": "We use the t-distributed Stochastic Neighbor Embedding (t-SNE) [13] which converts similarities between data points to joint probabilities and tries to minimize the Kullback-Leibler divergence between the joint probabilities of the low-dimensional embedding and the high-dimensional data.", "startOffset": 63, "endOffset": 67}], "year": 2016, "abstractText": "Classification is a fundamental task in machine learning and artificial intelligence. Existing classification methods are designed to classify unknown instances within a set of previously known classes that are seen in training. Such classification takes the form of prediction within a closed-set. However, a more realistic scenario that fits the ground truth of real world applications is to consider the possibility of encountering instances that do not belong to any of the classes that are seen in training, i.e., an open-set classification. In such situation, existing closed-set classification methods will assign a training label to these instances resulting in a misclassification. In this paper, we introduce Galaxy-X, a novel multi-class classification method for open-set problem. For each class of the training set, Galaxy-X creates a minimum bounding hyper-sphere that encompasses the distribution of the class by enclosing all of its instances. In such manner, our method is able to distinguish instances resembling previously seen classes from those that are of unseen classes. To adequately evaluate open-set classification, we introduce a novel evaluation procedure. Experimental results on benchmark datasets as well as on synthetic datasets show the efficiency of our approach in classifying novel instances from known as well as unknown classes.", "creator": "LaTeX with hyperref package"}}}