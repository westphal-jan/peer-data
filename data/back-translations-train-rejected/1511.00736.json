{"id": "1511.00736", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "2-Nov-2015", "title": "ProtNN: Fast and Accurate Nearest Neighbor Protein Function Prediction based on Graph Embedding in Structural and Topological Space", "abstract": "Studying the function of proteins is important for understanding the molecular mechanisms of life. The number of publicly available protein structures has increasingly become extremely large. Still, the determination of the function of a protein structure remains a difficult, costly, and time consuming task. The difficulties are often due to the essential role of spatial and topological structures in the determination of protein functions in living cells. In this paper, we propose ProtNN, a novel approach for protein function prediction. Given an unannotated protein structure and a set of annotated proteins, ProtNN finds the nearest neighbor annotated structures based on protein graph pairwise similarities. ProtNN assigns to the query protein the function with the highest number of votes across the set of $k$ nearest neighbor reference proteins, where $k$ is a user defined parameter. Experimental evaluation demonstrates that ProtNN is able to accurately classify several datasets in an extremely fast runtime compared to state-of-the-art approaches.", "histories": [["v1", "Mon, 2 Nov 2015 23:02:48 GMT  (971kb,D)", "http://arxiv.org/abs/1511.00736v1", null], ["v2", "Mon, 25 Jan 2016 01:55:45 GMT  (1369kb)", "http://arxiv.org/abs/1511.00736v2", null]], "reviews": [], "SUBJECTS": "cs.LG cs.SI", "authors": ["wajdi dhifli", "abdoulaye banir\\'e diallo"], "accepted": false, "id": "1511.00736"}, "pdf": {"name": "1511.00736.pdf", "metadata": {"source": "CRF", "title": "ProtNN: Fast and Accurate Nearest Neighbor Protein Function Prediction based on Graph Embedding in Structural and Topological Space", "authors": ["Wajdi Dhifli", "Abdoulaye Banir\u00e9 Diallo"], "emails": ["dhifli.wajdi@courrier.uqam.ca,", "diallo.abdoulaye@uqam.ca"], "sections": [{"heading": null, "text": "Keywords: protein structures, function prediction, graph classification"}, {"heading": "1 Introduction", "text": "This year, it is so far that it only takes one year to get there like never before."}, {"heading": "2 Methods", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "2.1 Graph Representation of Protein 3D Structures", "text": "In this context, a protein-3D structure can be regarded as a series of elements (amino acids and atoms) interconnected by chemical interactions [4,9,11]. Figure 1 shows a real example of the human hemoglobin protein and its graph representation. The figure clearly shows that the graph representation preserves the overall structure of the protein and its constituents. Protein-graph model G is a graph consisting of a series of nodes V and edges E. L is a marking function that assigns a label l to each node in V. Each node of G represents an amino acid from the 3D structure and is labelled with its corresponding amino acid type. Let it be a function that equals the euclidean distance between the nodes (u, v), V, V and Jom (each below the threshold V) and the threshold between the nodes (V, V and V)."}, {"heading": "2.2 Structural and Topological Embedding of Protein Graphs", "text": "Graph Embedding In ProtNN, each protein 3D structure is represented by a graph according to Equation 1. Then, each graph is embedded in a vector of structural and topological characteristics, assuming that structurally similar graphs should yield similar structural and topological characteristic vectors. In this way, ProtNN guarantees accuracy and computational efficiency. It should be noted that although structurally similar graphs should have similar structural and topological characteristics, ProtNN similarity should not necessarily yield the same results of structure matching (as in structural alignment), but it should enrich them, as ProtNN even takes into account hidden similarities (such as diagram density and energy) that are not taken into account in structural matching. Structural and topological attributes In ProtNN, the pairwise similarity between two protein diagrams (protein properties) is measured by the distance between their vector representations."}, {"heading": "2.3 ProtNN: Nearest Neighbor Protein Functional Classification", "text": "The general classification pipeline of ProtNN can be described as follows: First, pre-processing is performed on the reference protein database, in which a graph model GP is created for each reference protein P, VP, according to Equation 1. For each graph model GP, a structural and topological description vector VP is created by calculating the corresponding values for each of the structural and topological attributes described in Section 2.2. To ensure an even participation of all attributes used in the classification, a min-max normalization (xnormalized = x \u2212 min max \u2212 min, where x is an attribute value, min and max are the minimum and maximum values for the attribute vector) is applied independently to each attribute of MVP, so that no attribute will dominate in the prediction."}, {"heading": "3 Experiments", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "3.1 Datasets", "text": "We use six benchmark datasets of protein structures previously used in [10,15,16,17]. Each dataset consists of positive protein examples Dataset SCOP ID Family name Pos. Neg.DS1 48623 Vertebrate phospholipase A2 29 DS2 52592 G proteins 33 33 DS3 48942 C1 set domains 38 38 DS4 56437 C-type lectin domains 38 38 DS5 56251 Proteasome subunits 35 35 DS6 88854 Protein kinases, catalytic subunits 41 41 coming from a selected protein family, and negative protein samples randomly taken from the PDB [1]. The selected positive protein families are vertebrate phospholipase A2, G protein families, C1-set domains, C-type lectin domains and protein kinases, catalytic subunits. Table 1 summarizes the features of the six datasets."}, {"heading": "3.2 Protocol and Settings", "text": "Experiments were conducted on CentOS Linux workstations with Intel Core i7 CPU at 3.40 GHz and 16.00 GB RAM. To convert proteins into graphs, we used a \u03b4 value of 7A. Assessment yardstick is classification accuracy, and the evaluation technology is leave-one-out (LOO), whereby each dataset is used to create N classification scenarios, where N is the number of proteins in the dataset. In each scenario, one reference protein is used as a query instance and the rest of the dataset as a reference. The goal is to accurately predict the class of the query protein. Classification accuracy for each dataset is averaged over the results for all N evaluations."}, {"heading": "4 Results and Discussion", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.1 ProtNN Classification Results", "text": "This year it has come to the point that it will only be a matter of time before it will happen, until it does."}, {"heading": "4.2 Scalability and Runtime Analysis", "text": "In this section we analyze the cost of calculating ProtNN and FatCat, the most competitive approach. We analyze the variation in runtime for both approaches with a higher number of protein 3D structures ranging from 10 to 100 proteins with a step size of 10. In Figure 4 we report on runtime results in seconds (left) and in log10 scale (right). There is a large gap between the runtime of ProtNN and that of FatCat. The average runtime of the graph transformation of ProtNN was 0.8 seconds and that of the calculation of attributes was 0.6 seconds for each protein. The total time of similarity search and function prediction of ProtNN in this experiment was 47 times more than FatCat. The average runtime of the graph transformation of ProtNN was 0.8 seconds and that of the calculation of attributes was 0.6 seconds for each protein."}, {"heading": "5 Conclusion", "text": "In this paper, we proposed ProtNN, a new fast and precise approach for predicting protein functions. We defined a graph transformation and embedding model that contains both explicit and hidden structural and topological properties of the 3D structure of proteins. We successfully implemented the proposed model and demonstrated experimentally that it is possible to detect similarities and efficiently predict the function of protein 3D structures. Empirical results from our experiments showed that taking structural information into account is an important advantage for the correct identification of protein functions, and that alignment-based classification and subgraph-based classification are very competitive approaches. However, as the number of comparisons between protein pairs and the size of the data set increases enormously, the results of more detailed models would incur enormous computational costs. ProtNN showed that it is able to classify multiple benchmark data sets from literature with very low computational costs."}, {"heading": "Appendix: Structural and Topological Attributes", "text": "Following is the list of structural and topological attributes used in ProtNN: A1- Number of knots is the effective number of knots (1). - It represents the effective number of knots (1). - It represents the effective number of knots (2). - It represents the effective number of knots (2). - The average number of knots is the average number of knots. - It measures how many edges in E compared to the maximum possible number of edges between knots V | V | i = 1 degree of radiality (ui). - The density of a graph G = (V, E) measures how many edges in E are compared to the maximum possible number of edges between knots V. Formally: the (G) = 2 | E | (V | V | 1)."}], "references": [{"title": "The protein data bank", "author": ["H.M. Berman", "J.D. Westbrook", "Z. Feng", "G. Gilliland", "T.N. Bhat", "H. Weissig", "I.N. Shindyalov", "P.E. Bourne"], "venue": "Nucleic Acids Research 28(1)", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2000}, {"title": "Data growth and its impact on the scop database: new developments", "author": ["A. Andreeva", "D. Howorth", "J.M. Chandonia", "S.E. Brenner", "T.J.P. Hubbard", "C. Chothia", "A.G. Murzin"], "venue": "Nucleic Acids Research 36(1)", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2008}, {"title": "CATH: comprehensive structural and functional annotations for genome sequences", "author": ["I. Sillitoe", "T.E. Lewis", "A.L. Cuff", "S. Das", "P. Ashford", "N.L. Dawson", "N. Furnham", "R.A. Laskowski", "D. Lee", "J.G. Lees", "S. Lehtinen", "R.A. Studer", "J.M. Thornton", "C.A. Orengo"], "venue": "Nucleic Acids Research 43(Database-Issue)", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2015}, {"title": "Mining the entire protein databank for frequent spatially cohesive amino acid patterns", "author": ["P. Meysman", "C. Zhou", "B. Cule", "B. Goethals", "K. Laukens"], "venue": "BioData Mining 8", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2015}, {"title": "Basic local alignment search tool", "author": ["S. Altschul", "W. Gish", "W. Miller", "E. Myers", "D. Lipman"], "venue": "Journal of Molecular Biology 215", "citeRegEx": "5", "shortCiteRegEx": null, "year": 1990}, {"title": "Protein structure alignment by incremental combinatorial extension of the optimum path", "author": ["I.N. Shindyalov", "P.E. Bourne"], "venue": "Protein Engineering 11(9)", "citeRegEx": "6", "shortCiteRegEx": null, "year": 1998}, {"title": "Protein structure alignment using environmental profiles", "author": ["J. Jung", "B. Lee"], "venue": "Protein Engineering 13", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2000}, {"title": "Flexible structure alignment by chaining aligned fragment pairs allowing twists", "author": ["Y. Ye", "A. Godzik"], "venue": "Bioinformatics 19", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2003}, {"title": "Protein function prediction via graph kernels", "author": ["K.M. Borgwardt", "C.S. Ong", "S. Sch\u00f6nauer", "S.V.N. Vishwanathan", "A.J. Smola", "H. Kriegel"], "venue": "Proceedings Thirteenth International Conference on Intelligent Systems for Molecular Biology 2005, Detroit, MI, USA, 25-29 June 2005.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2005}, {"title": "Graph classification: a diversified discriminative feature selection approach", "author": ["Y. Zhu", "J.X. Yu", "H. Cheng", "L. Qin"], "venue": "21st ACM International Conference on Information and Knowledge Management, ACM", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2012}, {"title": "Smoothing 3D protein structure motifs through graph mining and amino-acids similarities", "author": ["W. Dhifli", "R. Saidi", "E. Mephu Nguifo"], "venue": "Journal of Computational Biology 21(2)", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2014}, {"title": "No free lunch theorems for optimization", "author": ["D. Wolpert", "W.G. Macready"], "venue": "IEEE Transactions on Evolutionary Computation 1(1)", "citeRegEx": "12", "shortCiteRegEx": null, "year": 1997}, {"title": "Graphs over time: densification laws, shrinking diameters and possible explanations", "author": ["J. Leskovec", "J. Kleinberg", "C. Faloutsos"], "venue": "eleventh ACM SIGKDD international conference on Knowledge discovery in data mining, ACM", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2005}, {"title": "Effective graph classification based on topological and label attributes", "author": ["G. Li", "M. Semerci", "B. Yener", "M.J. Zaki"], "venue": "Statistical Analysis and Data Mining 5(4)", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2012}, {"title": "Boosting with structure information in the functional space: an application to graph classification", "author": ["H. Fei", "J. Huan"], "venue": "ACM knowledge discovery and data mining conference (KDD).", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2010}, {"title": "Graph classification based on pattern co-occurrence", "author": ["N. Jin", "C. Young", "W. Wang"], "venue": "ACM International Conference on Information and Knowledge Management.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2009}, {"title": "GAIA: graph classification using evolutionary computation", "author": ["N. Jin", "C. Young", "W. Wang"], "venue": "Proceedings of the 2010 ACM SIGMOD International Conference on Management of data, ACM", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2010}, {"title": "Learning SciPy for Numerical and Scientific Computing - Second Edition", "author": ["Sergio J. Rojas G.", "Erik A Christensen", "F.J.B.S."], "venue": "Community experience distilled. Packt Publishing", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2015}, {"title": "Gene selection for cancer classification using support vector machines", "author": ["I. Guyon", "J. Weston", "S. Barnhill", "V. Vapnik"], "venue": "Machine Learning 46(1-3)", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2002}], "referenceMentions": [{"referenceID": 0, "context": "In fact, the number of proteins in the Protein Data Bank (PDB) [1] has more than tripled over the last decade.", "startOffset": 63, "endOffset": 66}, {"referenceID": 1, "context": "Alternative databases such as SCOP [2] and CATH [3] are undergoing the same trend.", "startOffset": 35, "endOffset": 38}, {"referenceID": 2, "context": "Alternative databases such as SCOP [2] and CATH [3] are undergoing the same trend.", "startOffset": 48, "endOffset": 51}, {"referenceID": 3, "context": "contacts that support it [4].", "startOffset": 25, "endOffset": 28}, {"referenceID": 4, "context": "Blast [5], .", "startOffset": 6, "endOffset": 9}, {"referenceID": 5, "context": "Combinatorial Extention [6], Sheba [7], FatCat [8], .", "startOffset": 24, "endOffset": 27}, {"referenceID": 6, "context": "Combinatorial Extention [6], Sheba [7], FatCat [8], .", "startOffset": 35, "endOffset": 38}, {"referenceID": 7, "context": "Combinatorial Extention [6], Sheba [7], FatCat [8], .", "startOffset": 47, "endOffset": 50}, {"referenceID": 8, "context": "This classification strategy is based on the hypothesis that structurally similar proteins could share a common ancestor [9].", "startOffset": 121, "endOffset": 124}, {"referenceID": 9, "context": "Such motifs could be discriminative [10], representative [11], cohesive [4], etc.", "startOffset": 36, "endOffset": 40}, {"referenceID": 10, "context": "Such motifs could be discriminative [10], representative [11], cohesive [4], etc.", "startOffset": 57, "endOffset": 61}, {"referenceID": 3, "context": "Such motifs could be discriminative [10], representative [11], cohesive [4], etc.", "startOffset": 72, "endOffset": 75}, {"referenceID": 11, "context": "However, such consideration makes these methods subject to the \u201dno free lunch\u201d principle [12], where the gain in accuracy comes with an offset of computational cost.", "startOffset": 89, "endOffset": 93}, {"referenceID": 3, "context": "this context, a protein 3D structure can be seen as a set of elements (amino acids and atoms) that are interconnected through chemical interactions [4,9,11].", "startOffset": 148, "endOffset": 156}, {"referenceID": 8, "context": "this context, a protein 3D structure can be seen as a set of elements (amino acids and atoms) that are interconnected through chemical interactions [4,9,11].", "startOffset": 148, "endOffset": 156}, {"referenceID": 10, "context": "this context, a protein 3D structure can be seen as a set of elements (amino acids and atoms) that are interconnected through chemical interactions [4,9,11].", "startOffset": 148, "endOffset": 156}, {"referenceID": 12, "context": "In order to avoid the loss of structural information in the embedding, and to guarantee ProtNN accuracy, we use a set of structural and topological attributes from the literature that have shown to be interesting and efficient in describing connected graphs [13,14].", "startOffset": 258, "endOffset": 265}, {"referenceID": 13, "context": "In order to avoid the loss of structural information in the embedding, and to guarantee ProtNN accuracy, we use a set of structural and topological attributes from the literature that have shown to be interesting and efficient in describing connected graphs [13,14].", "startOffset": 258, "endOffset": 265}, {"referenceID": 9, "context": "We use six benchmark datasets of protein structures that have previously been used in [10,15,16,17].", "startOffset": 86, "endOffset": 99}, {"referenceID": 14, "context": "We use six benchmark datasets of protein structures that have previously been used in [10,15,16,17].", "startOffset": 86, "endOffset": 99}, {"referenceID": 15, "context": "We use six benchmark datasets of protein structures that have previously been used in [10,15,16,17].", "startOffset": 86, "endOffset": 99}, {"referenceID": 16, "context": "We use six benchmark datasets of protein structures that have previously been used in [10,15,16,17].", "startOffset": 86, "endOffset": 99}, {"referenceID": 0, "context": "that are from a selected protein family, and negative protein examples that are randomly sampled from the PDB [1].", "startOffset": 110, "endOffset": 113}, {"referenceID": 17, "context": "See [18] for a formal definition of these measures.", "startOffset": 4, "endOffset": 8}, {"referenceID": 0, "context": "Results Using Different Numbers of Nearest Neighbors In the following, we evaluate the classification accuracy of ProtNN on each of the six benchmark datasets using different numbers of nearest neighbors k \u2208 [1,10].", "startOffset": 208, "endOffset": 214}, {"referenceID": 9, "context": "Results Using Different Numbers of Nearest Neighbors In the following, we evaluate the classification accuracy of ProtNN on each of the six benchmark datasets using different numbers of nearest neighbors k \u2208 [1,10].", "startOffset": 208, "endOffset": 214}, {"referenceID": 0, "context": "For simplicity, we plot the average value of classification accuracy for each value of k \u2208 [1,10] over the six datasets using each of the top-five measures.", "startOffset": 91, "endOffset": 97}, {"referenceID": 9, "context": "For simplicity, we plot the average value of classification accuracy for each value of k \u2208 [1,10] over the six datasets using each of the top-five measures.", "startOffset": 91, "endOffset": 97}, {"referenceID": 0, "context": "Tendancy of average accuracy of ProtNN for each value of k \u2208 [1,10] over the six datasets and using each of the top-five distance measures.", "startOffset": 61, "endOffset": 67}, {"referenceID": 9, "context": "Tendancy of average accuracy of ProtNN for each value of k \u2208 [1,10] over the six datasets and using each of the top-five distance measures.", "startOffset": 61, "endOffset": 67}, {"referenceID": 18, "context": "follow the Recursive Feature Elimination (RFE) approach [19] with ProtNN as the classifier.", "startOffset": 56, "endOffset": 60}, {"referenceID": 4, "context": "Comparison with Other Classification Techniques We compare our approach with multiple state-of-the-art approaches for protein function prediction namely: sequence alignment-based classification (using Blast [5]), structural alignment-based classification (using Combinatorial Extension (CE) [6], Sheba [7], and FatCat [8]), and substructure(subgraph)-based classification (using GAIA [17], LPGBCMP [15], and D&D [10]).", "startOffset": 207, "endOffset": 210}, {"referenceID": 5, "context": "Comparison with Other Classification Techniques We compare our approach with multiple state-of-the-art approaches for protein function prediction namely: sequence alignment-based classification (using Blast [5]), structural alignment-based classification (using Combinatorial Extension (CE) [6], Sheba [7], and FatCat [8]), and substructure(subgraph)-based classification (using GAIA [17], LPGBCMP [15], and D&D [10]).", "startOffset": 291, "endOffset": 294}, {"referenceID": 6, "context": "Comparison with Other Classification Techniques We compare our approach with multiple state-of-the-art approaches for protein function prediction namely: sequence alignment-based classification (using Blast [5]), structural alignment-based classification (using Combinatorial Extension (CE) [6], Sheba [7], and FatCat [8]), and substructure(subgraph)-based classification (using GAIA [17], LPGBCMP [15], and D&D [10]).", "startOffset": 302, "endOffset": 305}, {"referenceID": 7, "context": "Comparison with Other Classification Techniques We compare our approach with multiple state-of-the-art approaches for protein function prediction namely: sequence alignment-based classification (using Blast [5]), structural alignment-based classification (using Combinatorial Extension (CE) [6], Sheba [7], and FatCat [8]), and substructure(subgraph)-based classification (using GAIA [17], LPGBCMP [15], and D&D [10]).", "startOffset": 318, "endOffset": 321}, {"referenceID": 16, "context": "Comparison with Other Classification Techniques We compare our approach with multiple state-of-the-art approaches for protein function prediction namely: sequence alignment-based classification (using Blast [5]), structural alignment-based classification (using Combinatorial Extension (CE) [6], Sheba [7], and FatCat [8]), and substructure(subgraph)-based classification (using GAIA [17], LPGBCMP [15], and D&D [10]).", "startOffset": 384, "endOffset": 388}, {"referenceID": 14, "context": "Comparison with Other Classification Techniques We compare our approach with multiple state-of-the-art approaches for protein function prediction namely: sequence alignment-based classification (using Blast [5]), structural alignment-based classification (using Combinatorial Extension (CE) [6], Sheba [7], and FatCat [8]), and substructure(subgraph)-based classification (using GAIA [17], LPGBCMP [15], and D&D [10]).", "startOffset": 398, "endOffset": 402}, {"referenceID": 9, "context": "Comparison with Other Classification Techniques We compare our approach with multiple state-of-the-art approaches for protein function prediction namely: sequence alignment-based classification (using Blast [5]), structural alignment-based classification (using Combinatorial Extension (CE) [6], Sheba [7], and FatCat [8]), and substructure(subgraph)-based classification (using GAIA [17], LPGBCMP [15], and D&D [10]).", "startOffset": 412, "endOffset": 416}], "year": 2017, "abstractText": "Studying the function of proteins is important for understanding the molecular mechanisms of life. The number of publicly available protein structures has increasingly become extremely large. Still, the determination of the function of a protein structure remains a difficult, costly, and time consuming task. The difficulties are often due to the essential role of spatial and topological structures in the determination of protein functions in living cells. In this paper, we propose ProtNN, a novel approach for protein function prediction. Given an unannotated protein structure and a set of annotated proteins, ProtNN finds the nearest neighbor annotated structures based on protein graph pairwise similarities. ProtNN assigns to the query protein the function with the highest number of votes across the set of k nearest neighbor reference proteins, where k is a user defined parameter. Experimental evaluation demonstrates that ProtNN is able to accurately classify several datasets in an extremely fast runtime compared to state-of-the-art approaches.", "creator": "LaTeX with hyperref package"}}}