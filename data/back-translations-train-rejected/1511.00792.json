{"id": "1511.00792", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "3-Nov-2015", "title": "Fast Collaborative Filtering from Implicit Feedback with Provable Guarantees", "abstract": "Building recommendation algorithms is one of the most challenging tasks in Machine Learning. Although there has been significant progress in building recommendation systems when explicit feedback is available from the users in the form of rating or text, most of the applications do not receive such feedback. Here we consider the recommendation task where the available data is the record of the items selected by different users over time for subscription or purchase. This is known as implicit feedback recommendation. Such data are usually available as large amount of user logs stored over massively distributed storage systems such as Hadoop. Therefore it is essential to have a highly scalable algorithm to build a recommender system for such applications. Here we propose a probabilistic algorithm that takes only two to three passes through the entire dataset to extract the model parameters during the training phase. We demonstrate the competitive performance of our algorithm in several empirical measures as well as the computation time in comparison with the existing algorithms on various publicly available datasets.", "histories": [["v1", "Tue, 3 Nov 2015 06:43:54 GMT  (290kb,D)", "http://arxiv.org/abs/1511.00792v1", null], ["v2", "Wed, 4 Nov 2015 06:26:04 GMT  (290kb,D)", "http://arxiv.org/abs/1511.00792v2", null], ["v3", "Thu, 5 Nov 2015 13:06:07 GMT  (291kb,D)", "http://arxiv.org/abs/1511.00792v3", null], ["v4", "Tue, 10 Nov 2015 12:05:40 GMT  (291kb,D)", "http://arxiv.org/abs/1511.00792v4", null], ["v5", "Mon, 23 Nov 2015 10:05:40 GMT  (291kb,D)", "http://arxiv.org/abs/1511.00792v5", null], ["v6", "Thu, 24 Dec 2015 20:33:13 GMT  (293kb,D)", "http://arxiv.org/abs/1511.00792v6", null], ["v7", "Fri, 15 Jan 2016 15:44:30 GMT  (292kb,D)", "http://arxiv.org/abs/1511.00792v7", null], ["v8", "Sat, 6 Feb 2016 10:28:25 GMT  (146kb,D)", "http://arxiv.org/abs/1511.00792v8", null], ["v9", "Wed, 8 Jun 2016 16:33:38 GMT  (161kb,D)", "http://arxiv.org/abs/1511.00792v9", null], ["v10", "Sun, 21 Aug 2016 08:00:15 GMT  (130kb,D)", "http://arxiv.org/abs/1511.00792v10", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["sayantan dasgupta"], "accepted": false, "id": "1511.00792"}, "pdf": {"name": "1511.00792.pdf", "metadata": {"source": "CRF", "title": null, "authors": ["Sayantan Dasgupta"], "emails": ["sayantad@uci.edu"], "sections": [{"heading": "1 INTRODUCTION", "text": "In fact, it is so that most of them are able to abide by the rules which they have imposed on themselves. (...) Indeed, it is so that they are able to determine for themselves what they want to do. (...) It is not as if they do it. (...) It is as if they do it. (...) It is as if they do it. (...) It is as if they do it. (...) \"It is as if they do it.\" (...). \"(.).\" (.). \"(.).\" (.). \"(.\" (.). \"(.).\" (.). \"(.).\" (.). \"(.).\" (.). \"(.).\" (. \"(.).\" (.). \"(.\" (.). \"(.).\" (.). \"(.\" (.). \"(.).\" (.). \"(.\" (.). \"(.).\" (.). \"(.).\" (.). \"(.\" (.). \"(.).\" (.). \"(.).\" (.).). \"(.).\" (. \").\" (.).). \"(.\" (.). \"(.).\" (.).). \"(.\" (.).). \"(.).\" (.). \"(.).\" (.).). \"(.).\" (. \"(.).\" (.).). \"(.).\" (.). \"(.).\" (.).). \"(.).\" (. \"(.\" (.).).). \"(.).\" (.). \"(.).\" (.). \"(.\"). \"(.).\" (.).). \"(.).\" (.). \").\"). \"(.\" (. \"(.).).\" (.). \"(.\").). \").\" (.). \")."}, {"heading": "2 LATENT VARIABLE MODEL FOR METHOD OF MOMENTS", "text": "In this section we will outline the generative latent model for the method of moments and derive the equations to extract the parameters of the model. We will also show that the method provides unique model parameters. Suppose that there are U-users and D-items, and the latent variable h can be derived from K-items. For each user x-items we can first select a latent state of h-items from the discrete distribution P (h-x), then we select an item y-items, y1, y2. yD) from the discrete distribution P (y-II). The generative process is as follows, h-items of discretion (P-X) y-items (y-h) (1) Let us denote the probability of the latent variable h-items."}, {"heading": "3 EXPERIMENTAL RESULTS", "text": "We show the implementation of our model on three publicly available datasets, so that the results can be reproduced as needed. The different attributes of the datasets are described in Table 1. If we treat the dataset as a user item matrix, we can define the density of elements that are not zero as follows: Density = number of < users, item > tuples Number of users (19) We use K = 50 for all models in our experiments; WRMF passes the memory for higher values of K. We use the implementation of BPR and WRMF from the MyMediaLite library, available at http: / / www.mymedialite.net /, developed by the authors of Rendle et al. (2009). For each dataset, we calculate the Precision @ GDP, Recall @ \u0442 and WRMF from the MyMediaLite library, which is available at http: / / www.mymedialite.net, developed by the authors of Rendal (2009)."}, {"heading": "3.1 TA-FENG DATASET", "text": "The Ta-Feng data set consists of online purchase records for the months of January, February, November and December of 2001. We combine the records of January and November into a training set of approximately 24,000 users and 21,000 products and approximately 470,000 sales records. The records of February and December form the test set. BPR achieves the highest MAP of all, but MoM delivers the best precision recall curve with slightly worse MAP than BPR. We also compare our result for Precison @ 5 with that achieved with the Unified Boltzman Machine (UBM) in Gunawardana & Meek (2009) in Table 2."}, {"heading": "3.2 MILLION SONG DATASET", "text": "Millions of song records contain the logs of one million users who listen to 385,000 song tracks with 48 million observations, using a subset of data consisting of 100,000 users and about 165,000 song tracks with about 1.45 million observations published in Kaggle. MoM provides the best MAP and precision recall for lower values \u03c4, and it trains in a similar time frame as BPR. We also compare our result for MAP @ 500 with that of content-based music recommendation using Deep Convolutional Neural Network (DCNN) in Van den Oord et al. (2013) for 9330 most popular songs with 20,000 users in Table 3. Our method is mathematically far less elaborate than Deep Convolutional Neural Network for large datasets."}, {"heading": "3.3 YANDEX SEARCH LOG DATASET", "text": "Yandex dataset was recently published in Kaggle and contains the search logs of 27 days for 5.7 million users and 70.3 million URLs. However, only 12 million URLs have at least one click. We selected a subset of 150,000 users and 624,000 URLs and used the data of the first 14 days as a training set, and the last 13 days as a test set. The WRMF performs best on Yandex dataset, but compared to MoM or BPR it takes about 10 times to train. MoM, on the other hand, performs relatively well, but only takes a fraction of the time the WRMF takes."}, {"heading": "4 CONCLUSION AND FUTURE WORKS", "text": "Here we have introduced a generative model based on the method of moment for implicit feedback recommendations, and established its competitive performance on three sets of data. BPR delivers the best results for the Ta Feng dataset, but its performance deteriorates greatly as the size and accuracy of the data increases. WRMF, on the other hand, performs worse for the Ta Feng dataset, but performance improves as the dataset becomes larger and more economical. However, its computational time is significantly higher than that of the other two algorithms, and for the Yandex dataset it is almost 10 times slower than BPR or MoM. MoM provides competitive performance for all datasets, but takes a similar computational time to BPR. The competitive computational aspects of the MoM can be even more obvious when applied to larger amounts of data stored across multiple nodes in a distributed ecosystem."}], "references": [{"title": "A spectral algorithm for latent dirichlet allocation", "author": ["Anandkumar", "Anima", "Liu", "Yi-kai", "Hsu", "Daniel J", "Foster", "Dean P", "Kakade", "Sham M"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Anandkumar et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Anandkumar et al\\.", "year": 2012}, {"title": "Tensor decompositions for learning latent variable models", "author": ["Anandkumar", "Animashree", "Ge", "Rong", "Hsu", "Daniel", "Kakade", "Sham M", "Telgarsky", "Matus"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Anandkumar et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Anandkumar et al\\.", "year": 2014}, {"title": "Spectral experts for estimating mixtures of linear regressions", "author": ["Chaganty", "Arun Tejasvi", "Liang", "Percy"], "venue": "arXiv preprint arXiv:1306.3729,", "citeRegEx": "Chaganty et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Chaganty et al\\.", "year": 2013}, {"title": "Spectral learning of latent-variable pcfgs: algorithms and sample complexity", "author": ["Cohen", "Shay B", "Stratos", "Karl", "Collins", "Michael", "Foster", "Dean P", "Ungar", "Lyle H"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Cohen et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Cohen et al\\.", "year": 2014}, {"title": "Google news personalization: scalable online collaborative filtering", "author": ["Das", "Abhinandan S", "Datar", "Mayur", "Garg", "Ashutosh", "Rajaram", "Shyam"], "venue": "In Proceedings of the 16th international conference on World Wide Web,", "citeRegEx": "Das et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Das et al\\.", "year": 2007}, {"title": "Spectral dependency parsing with latent variables", "author": ["Dhillon", "Paramveer S", "Rodu", "Jordan", "Collins", "Michael", "Foster", "Dean P", "Ungar", "Lyle H"], "venue": "In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning,", "citeRegEx": "Dhillon et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Dhillon et al\\.", "year": 2012}, {"title": "A unified approach to building hybrid recommender systems", "author": ["Gunawardana", "Asela", "Meek", "Christopher"], "venue": "In Proceedings of the Third ACM Conference on Recommender Systems,", "citeRegEx": "Gunawardana et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Gunawardana et al\\.", "year": 2009}, {"title": "Latent semantic models for collaborative filtering", "author": ["Hofmann", "Thomas"], "venue": "ACM Trans. Inf. Syst.,", "citeRegEx": "Hofmann and Thomas.,? \\Q2004\\E", "shortCiteRegEx": "Hofmann and Thomas.", "year": 2004}, {"title": "A spectral algorithm for learning hidden markov models", "author": ["Hsu", "Daniel", "Kakade", "Sham M", "Zhang", "Tong"], "venue": "Journal of Computer and System Sciences,", "citeRegEx": "Hsu et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Hsu et al\\.", "year": 2012}, {"title": "Collaborative filtering for implicit feedback datasets", "author": ["Hu", "Yifan", "Koren", "Yehuda", "Volinsky", "Chris"], "venue": "In Proceedings of the 2008 Eighth IEEE International Conference on Data Mining,", "citeRegEx": "Hu et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Hu et al\\.", "year": 2008}, {"title": "Shifted power method for computing tensor eigenpairs", "author": ["Kolda", "Tamara G", "Mayo", "Jackson R"], "venue": "SIAM Journal on Matrix Analysis and Applications,", "citeRegEx": "Kolda et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Kolda et al\\.", "year": 2011}, {"title": "Using probabilistic latent semantic analysis for personalized web search", "author": ["Lin", "Chenxi", "Xue", "Gui-Rong", "Zeng", "Hua-Jun", "Yu", "Yong"], "venue": "In Web Technologies Research and Development-APWeb", "citeRegEx": "Lin et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Lin et al\\.", "year": 2005}, {"title": "Bpr: Bayesian personalized ranking from implicit feedback", "author": ["Rendle", "Steffen", "Freudenthaler", "Christoph", "Gantner", "Zeno", "Schmidt-Thieme", "Lars"], "venue": "In Proceedings of the Twenty-Fifth Conference on Uncertainty in Artificial Intelligence,", "citeRegEx": "Rendle et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Rendle et al\\.", "year": 2009}, {"title": "Hilbert space embeddings of hidden markov models", "author": ["Song", "Le", "Boots", "Byron", "Siddiqi", "Sajid M", "Gordon", "Geoffrey J", "Smola", "Alex J"], "venue": "In Proceedings of the 27th international conference on machine learning", "citeRegEx": "Song et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Song et al\\.", "year": 2010}, {"title": "Spectral methods for indian buffet process inference", "author": ["Tung", "Hsiao-Yu", "Smola", "Alex J"], "venue": "In Advances in Neural Information Processing Systems 27: Annual Conference on Neural Information Processing Systems", "citeRegEx": "Tung et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Tung et al\\.", "year": 2014}, {"title": "Deep content-based music recommendation", "author": ["Van den Oord", "Aaron", "Dieleman", "Sander", "Schrauwen", "Benjamin"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Oord et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Oord et al\\.", "year": 2013}], "referenceMentions": [{"referenceID": 4, "context": "Examples of such Implicit recommendation systems are music recommendation, personalised search result recommendation or personalised news recommendation; for example, in a personalised news recommender system Das et al. (2007), we always know the articles a user read or liked from his click logs, but will never come to know the articles he did not like (Figure 1).", "startOffset": 209, "endOffset": 227}, {"referenceID": 4, "context": "Examples of such Implicit recommendation systems are music recommendation, personalised search result recommendation or personalised news recommendation; for example, in a personalised news recommender system Das et al. (2007), we always know the articles a user read or liked from his click logs, but will never come to know the articles he did not like (Figure 1). Please note that the feedback available in the form of like or unlike tag is an explicit binary feedback, and must not be confused with implicit feedback systems, where we have only the knowledge of what the users liked before. Majority of the literature of recommender systems focus on cases where explicit feedback are available.There have been a few previous attempts to build recommendation systems based on implicit feedback, e.g. in personalized ranking of search results Lin et al. (2005) or personalized news recommendation Das et al.", "startOffset": 209, "endOffset": 863}, {"referenceID": 4, "context": "Examples of such Implicit recommendation systems are music recommendation, personalised search result recommendation or personalised news recommendation; for example, in a personalised news recommender system Das et al. (2007), we always know the articles a user read or liked from his click logs, but will never come to know the articles he did not like (Figure 1). Please note that the feedback available in the form of like or unlike tag is an explicit binary feedback, and must not be confused with implicit feedback systems, where we have only the knowledge of what the users liked before. Majority of the literature of recommender systems focus on cases where explicit feedback are available.There have been a few previous attempts to build recommendation systems based on implicit feedback, e.g. in personalized ranking of search results Lin et al. (2005) or personalized news recommendation Das et al. (2007). These recommendation algorithms usually rely on Probabilistic Latent Semantic Algorithm (PLSI) Hofmann (2004).", "startOffset": 209, "endOffset": 917}, {"referenceID": 4, "context": "Examples of such Implicit recommendation systems are music recommendation, personalised search result recommendation or personalised news recommendation; for example, in a personalised news recommender system Das et al. (2007), we always know the articles a user read or liked from his click logs, but will never come to know the articles he did not like (Figure 1). Please note that the feedback available in the form of like or unlike tag is an explicit binary feedback, and must not be confused with implicit feedback systems, where we have only the knowledge of what the users liked before. Majority of the literature of recommender systems focus on cases where explicit feedback are available.There have been a few previous attempts to build recommendation systems based on implicit feedback, e.g. in personalized ranking of search results Lin et al. (2005) or personalized news recommendation Das et al. (2007). These recommendation algorithms usually rely on Probabilistic Latent Semantic Algorithm (PLSI) Hofmann (2004). PLSI deploys EM algorithm for training, and therefore suffers from local maxima problem.", "startOffset": 209, "endOffset": 1028}, {"referenceID": 4, "context": "Also, most of the literature on implicit feedback, such as Lin et al. (2005) and Das et al.", "startOffset": 59, "endOffset": 77}, {"referenceID": 1, "context": "(2005) and Das et al. (2007) are published using proprietary datasets, and lack of publicly available dataset has also been a major impediment for academic contribution in this segment.", "startOffset": 11, "endOffset": 29}, {"referenceID": 1, "context": "(2005) and Das et al. (2007) are published using proprietary datasets, and lack of publicly available dataset has also been a major impediment for academic contribution in this segment. Hu et al. (2008) proposes a weighted matrix factorization (WRMF) for implicit feedback recommendation.", "startOffset": 11, "endOffset": 203}, {"referenceID": 1, "context": "(2005) and Das et al. (2007) are published using proprietary datasets, and lack of publicly available dataset has also been a major impediment for academic contribution in this segment. Hu et al. (2008) proposes a weighted matrix factorization (WRMF) for implicit feedback recommendation. The algorithm scans through the entire dataset for every iteration until convergence, and it may prove computationally very expensive for large amount of user logs stored across multiple nodes in a distributed ecosystem. Bayesian Personalized Ranking (BPR) inRendle et al. (2009) uses stochastic approach to train from a small batch size, and reduces the computation time significantly.", "startOffset": 11, "endOffset": 569}, {"referenceID": 1, "context": "(2005) and Das et al. (2007) are published using proprietary datasets, and lack of publicly available dataset has also been a major impediment for academic contribution in this segment. Hu et al. (2008) proposes a weighted matrix factorization (WRMF) for implicit feedback recommendation. The algorithm scans through the entire dataset for every iteration until convergence, and it may prove computationally very expensive for large amount of user logs stored across multiple nodes in a distributed ecosystem. Bayesian Personalized Ranking (BPR) inRendle et al. (2009) uses stochastic approach to train from a small batch size, and reduces the computation time significantly. These two methods are shown to outperform others such as similarity or neighbourhood based methods in Rendle et al. (2009), and hence we limit our discussion on BPR and WRMF in this article.", "startOffset": 11, "endOffset": 799}, {"referenceID": 0, "context": "The method can be proven to be globally convergent using PAC style proof (see Anandkumar et al. (2014)), and has been successfully applied for Hidden Markov Model in Hsu et al.", "startOffset": 78, "endOffset": 103}, {"referenceID": 0, "context": "The method can be proven to be globally convergent using PAC style proof (see Anandkumar et al. (2014)), and has been successfully applied for Hidden Markov Model in Hsu et al. (2012) and Song et al.", "startOffset": 78, "endOffset": 184}, {"referenceID": 0, "context": "The method can be proven to be globally convergent using PAC style proof (see Anandkumar et al. (2014)), and has been successfully applied for Hidden Markov Model in Hsu et al. (2012) and Song et al. (2010), for Topic Modeling in Anandkumar et al.", "startOffset": 78, "endOffset": 207}, {"referenceID": 0, "context": "The method can be proven to be globally convergent using PAC style proof (see Anandkumar et al. (2014)), and has been successfully applied for Hidden Markov Model in Hsu et al. (2012) and Song et al. (2010), for Topic Modeling in Anandkumar et al. (2012), for various Natural Language Processing applications in Cohen et al.", "startOffset": 78, "endOffset": 255}, {"referenceID": 0, "context": "The method can be proven to be globally convergent using PAC style proof (see Anandkumar et al. (2014)), and has been successfully applied for Hidden Markov Model in Hsu et al. (2012) and Song et al. (2010), for Topic Modeling in Anandkumar et al. (2012), for various Natural Language Processing applications in Cohen et al. (2014), Dhillon et al.", "startOffset": 78, "endOffset": 332}, {"referenceID": 0, "context": "The method can be proven to be globally convergent using PAC style proof (see Anandkumar et al. (2014)), and has been successfully applied for Hidden Markov Model in Hsu et al. (2012) and Song et al. (2010), for Topic Modeling in Anandkumar et al. (2012), for various Natural Language Processing applications in Cohen et al. (2014), Dhillon et al. (2012) and for Spectral Experts of Linear Regression in Chaganty & Liang (2013).", "startOffset": 78, "endOffset": 355}, {"referenceID": 0, "context": "The method can be proven to be globally convergent using PAC style proof (see Anandkumar et al. (2014)), and has been successfully applied for Hidden Markov Model in Hsu et al. (2012) and Song et al. (2010), for Topic Modeling in Anandkumar et al. (2012), for various Natural Language Processing applications in Cohen et al. (2014), Dhillon et al. (2012) and for Spectral Experts of Linear Regression in Chaganty & Liang (2013). Here we propose a generative model for implicit feedback recommendation system based on Method of Moments.", "startOffset": 78, "endOffset": 428}, {"referenceID": 0, "context": "We use the tensor decomposition algorithm from Anandkumar et al. (2014), which computes the eigen-vectors of M\u03033 through power iteration followed by deflation (Algorithm 2).", "startOffset": 47, "endOffset": 72}, {"referenceID": 12, "context": "net/ , developed by the authors of Rendle et al. (2009). For every dataset, we compute the Precision@\u03c4 , Recall@\u03c4 , and Mean Average Precision (MAP@\u03c4 ) for \u03c4 \u2208 {5, 10, 20, 40, 60, 80, 100, 200, 300, 400, 500}.", "startOffset": 35, "endOffset": 56}, {"referenceID": 12, "context": "net/ , developed by the authors of Rendle et al. (2009). For every dataset, we compute the Precision@\u03c4 , Recall@\u03c4 , and Mean Average Precision (MAP@\u03c4 ) for \u03c4 \u2208 {5, 10, 20, 40, 60, 80, 100, 200, 300, 400, 500}. The Precision-Recall curves as well as MAP@\u03c4 is shown in Figure 2. We also show the computation time averaged over 10 executions for each method in Figure 2. We carry out our experiments on Unix Platform on a single machine with Intel i5 Processor (2.4GHz) and 8GB memory, and no multi-threading or other performance enhancement technique is used in the code. 3.1 TA-FENG DATASET Ta-Feng dataset consists of online grocery purchase records for the months of January, February, November and December in 2001.We combine the records of January and November resulting in a training set consisting of around 24,000 users and 21,000 products, and around 470,000 sales records. The records of February and December are combined to form the test set. BPR achieves the highest MAP of all, but MoM yields the best Precision-Recall curve with slightly worse MAP than BPR. We also compare our result for Precison@5 with that obtained using Unified Boltzman Machine (UBM) in Gunawardana & Meek (2009) in Table 2.", "startOffset": 35, "endOffset": 1198}, {"referenceID": 15, "context": "We also compare our result for MAP@500 with those of content based Music Recommendation using Deep Convolutional Neural Network (DCNN) in Van den Oord et al. (2013) for 9330 most popular songs with 20,000 users in Table 3.", "startOffset": 146, "endOffset": 165}], "year": 2017, "abstractText": "Building recommendation algorithms is one of the most challenging tasks in Machine Learning. Although there has been significant progress in building recommendation systems when explicit feedback is available from the users in the form of rating or text, most of the applications do not receive such feedback. Here we consider the recommendation task where the available data is the record of the items selected by different users over time for subscription or purchase. This is known as implicit feedback recommendation. Such data are usually available as large amount of user logs stored over massively distributed storage systems such as Hadoop. Therefore it is essential to have a highly scalable algorithm to build a recommender system for such applications. Here we propose a probabilistic algorithm that takes only two to three passes through the entire dataset to extract the model parameters during the training phase. We demonstrate the competitive performance of our algorithm in several empirical measures as well as the computation time in comparison with the existing algorithms on various publicly available datasets.", "creator": "LaTeX with hyperref package"}}}