{"id": "1511.00971", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "3-Nov-2015", "title": "Data Stream Classification using Random Feature Functions and Novel Method Combinations", "abstract": "Big Data streams are being generated in a faster, bigger, and more commonplace. In this scenario, Hoeffding Trees are an established method for classification. Several extensions exist, including high-performing ensemble setups such as online and leveraging bagging. Also, $k$-nearest neighbors is a popular choice, with most extensions dealing with the inherent performance limitations over a potentially-infinite stream.", "histories": [["v1", "Tue, 3 Nov 2015 16:29:57 GMT  (1653kb,D)", "http://arxiv.org/abs/1511.00971v1", "20 pages, journal"]], "COMMENTS": "20 pages, journal", "reviews": [], "SUBJECTS": "cs.LG cs.NE", "authors": ["diego marr\\'on", "jesse read", "albert bifet", "nacho navarro"], "accepted": false, "id": "1511.00971"}, "pdf": {"name": "1511.00971.pdf", "metadata": {"source": "CRF", "title": "Data Stream Classification using Random Feature Functions and Novel Method Combinations", "authors": ["Diego Marr\u00f3n", "Jesse Read", "Albert Bifet", "Nacho Navarro"], "emails": ["dmarron@ac.upc.edu", "jesse.read@aalto.fi", "albert.bifet@telecom-paristech.fr", "nacho@ac.upc.edu"], "sections": [{"heading": null, "text": "In this scenario, the Hoeffding Trees are an established method of classification. Several extensions exist, including powerful ensemble constellations such as online constellations and leveraging bagging. In addition, k-next neighbors is a popular choice, with most extensions managing the inherent performance constraints over a potentially infinite stream. At the same time, methods of gradient descent are becoming increasingly popular, partly due to the successes of deep learning. Although deep neural networks can learn gradually, they have so far proved too sensitive to hyperparameter options and starting conditions to be considered effective \"off-theself\" data streams. In this paper, we look at combinations of Hoeffding trees, the closest neighbor, and gradient descent methods with a streaming pre-processing approach in the form of a random function filter for additional predictive power."}, {"heading": "1. Introduction", "text": "There is a trend towards working with large and dynamic data sources, which is becoming a key area of data collection, and this trend is evident both in the real world and in academic literature. (There are a variety of application and acceleration methods in the real world that are not only found in the real world, but also in the real world and in the real world. (There are also a variety of methods that are used in the real world and in the real world). (However, there are also a variety of methods that are used in the real world and in the real world: 151 1.00 971v 1 [cs.L G].Many modern data sources are not only dynamic, but need to be classified in real time. Such contexts can be found in sensor applications (e.g. tracking and activity monitoring), demand forecasting (e.g. electricity), manufacturing processes, email feeds, news feeds and social feeds."}, {"heading": "2. Related Work", "text": "However, these trees are conservative in many situations, and in many situations naive Bayes method happens to surpass the standard Hoeffding tree at first, even though it is ultimately more accurate than the majority class. Given the widespread acceptance, this is the standard method in MOA, and we simply denounce this method in the experimental section as HT. In fact, naive Bayes classification is for free authors, as it can be done using the same statistics that are already collected by the majority class. Other established examples include the use of major component analyses (also reviewed in [9]) for this transformation."}, {"heading": "3. Tree Based Random Feature Functions", "text": "The transformation of the attribute space prior to learning and classification is an established idea in the literature on statistical and machine learning [9], for example with basic (or attribute) functions. Suppose the input instance is x-lang. This vector is transformed into a new space z = \u03c6 (x) via the function \u03c6, creating a new vector z of length h. Any commercially available model treats z as if it were input. The functions can either be appropriately selected by a domain expert or simply selected to achieve a more dimensioned representation of the input. Polynomials and splines are a typical selection. With regard to the HTs with additional algorithms in the sheets (as described in section 2), this filter can be placed either in front of the HT or in front of the method in the sheets, or both. In this essay, we adapt this methodology to deal with other classifiers in a similar way, namely kN and a GSbased group of sheets, rather than the T-based method (when T is used in BayG and T)."}, {"heading": "3.1. Ensembles in Data Streams", "text": "It is an ensemble method used to improve the accuracy of classification methods. Non-Streaming Bagging [23] builds a series of M base models that form each model with a bootstrap sample of size N. This binomial distribution for large values of N tends to be a Poisson (\u03bb = 1) distribution in which Poisson (\u03bb = 1) / k! Using this fact, Oza and Russell [24, 25] have proposed online bagging, an online method that instead of sampling with substitution of each example gives a weight to Poisson (1). ADWIN Bagging [26] is an adaptive version of online bagging that uses a change detector to decide when ensemble models are discarded."}, {"heading": "4. Neural Networks with Random Projections for Data Streams", "text": "This means that the statistical distribution of the data we are interested in may change. The idea behind the random layer is to improve the data localization throughout the space that the trained layer sees. Imagine the instance is a tiny luminous point on the space, with enough random neurons that act as mirrors, we hope that the trained layer can better capture the data movement. In this work, the strategy applied by the random projection layer is the one shown in Figure 2Except for the fact that it is never formed, the random layer is a normal layer and needs its activation functions, in this work, sigmoid, ReLU incremental and a radial base function that is used when the default function is used, except for the fact that it is never formed, the random layer is a normal layer and needs its activation functions, in this work sigmoid, ReLU incremental, LU incremental, incremental function that is used when the default function is used, except for the fact that it is never formed, the random layer is a normal layer and needs its activation functions, in this work sigmoid, ReLU incremental, LU is the radial function, and we use the base function = \u2212 11."}, {"heading": "5. Random Feature Function Evaluation", "text": "In fact, most of them are able to trump themselves, and they are able to trump themselves. (...) Most of them are not able to trump themselves. (...) Most of them are not able to trump themselves. (...) Most of them are not able to trump themselves. (...) Most of them are not able to trump themselves. (...) Most of them are not able to trump themselves. (...) Most of them are not able to trump themselves. (...) Most of them are not able to trump themselves. (...) Most of them are not able to trump themselves. (...) Most of them are able to trump themselves. (...)"}, {"heading": "6. GPU Extended Evaluation", "text": "In the previous evaluations, we noticed that the SGD methods have the strongest advantage from random functions that we apply in three layers. We noted the increasing popularity of DL methods, we chose this strategy for further investigation and experimentation in this section. A natural choice for implementing DNNs is the use of GPUs to speed up the selection of the network. Our experiments were evaluated on an NVIDIA Tesla K40c with 12GB of RAM, 15 SMX and up to 2880 simultaneous threads and CUDA 7.Another motivation to speed up the selection of network hyperparameters by using cross-validation: for each dataset and activation functions, different configurations are tested and best executed is chosen. This was a high number of combinations and a way to speed up the process. The random projection layer is implemented with a standard feed-forward network."}, {"heading": "6.1. Electricity Dataset", "text": "Table 5 summarizes the best results for the individual activation functions and their configurations. As shown earlier in this dataset, the sigmoid activation function was better than the others. Second, we find ReLU and ReLU Inc activation functions that returned similar results, and slightly worse than the sigmoid. In terms of RBF, all the configurations we tried performed worse than Sigmoid, ReLU, and ReLU inc, but very similar for the different gammas. In Figure 7, we can see how accuracy changes with different parameters that we tested."}, {"heading": "6.2. CoverType Dataset", "text": "Table 6 gives us the best results for the COVT dataset for each activation function. We see a similar pattern to the ELEC evaluation, SIG, ReLU and ReLU inc performed much better than the RBFs, and all three can exceed the results shown in Table 2. This time, the best result is achieved with the ReLU activation function on a random level. In Figure 8, we see the activation curves. Although we have achieved the best result with ReLU, the sigmoid has a better learning curve and is very close to ReLU accuracy. ReLU inc has a very similar learning curve to the standard ReLU. The different RBFs for the same momentum and learning with different sizes provide very similar (if not equal) results, so we have chosen the smaller sizes."}, {"heading": "6.3. SUSY Dataset", "text": "Table 7 shows the best results for the SUSY dataset and the activation function, and Figure 9 the learning curves. The most striking effect is sigmoid, ReLU and ReLU inc the learning stop very soon, with only 20 random neurons ReLU reaching its maximum peak with 74.85%. The RBFs that performed poorly in previous evaluations are the ones with the best results here. A curious result we can see is that the RBFs are about 7x% powerful in all three evaluations. Although 2 of the 3 results are not very good, they do not seem to be very sensitive to the different datasets, and somehow the results are stable across different data distributions."}, {"heading": "7. Conclusions", "text": "In this paper, we investigated combinations of Hoeffding trees, the closest neighbor, and gradient descend methods by adding a layer based on a random function filter. We found that this random layer can turn a simple gradient descend learner into a competitive method for real-time data analysis. In this first experiment, we even improved the current state-of-the-art algorithms and achieved the best and second best results for two out of three data sets tested. However, like Hoeffding Trees and adjacent methods, the random layer works in contrast to many other descend-based methods without intensive parameter tuning. We have successfully expanded and implemented the GPUs, achieving powerful prediction results, suggesting that the use of GPUs for data stream degradation is a promising research topic in order to obtain new fast and adaptable methods of machine learning."}, {"heading": "Acknowledgment", "text": "This work was partially supported by the research program of Aalto University AEF. http: / / energyefficiency.aalto.fi / en /, by NVIDIA through the UPC / BSC GPU Center of Excellence, and the Spanish Ministry of Science and Technology through the TIN2012-34557. [1] W. Qu, Y. Zhu, J. Zhu, Q. Qiu, Mining multi-label concept-drifting data streams using dynamic classifier ensemble, in: Asian Conference on Machine Learning, Vol. 5828 of Lecture Notes in Computer Science, Springer, 2009, pp. 308-321. [2] P. Domingos, G. Hulten, Mining high-speed data streams, in: Proceedings of the 6th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, 2000, pp."}], "references": [{"title": "Mining multi-label concept-drifting data streams using dynamic classifier ensemble", "author": ["W. Qu", "Y. Zhang", "J. Zhu", "Q. Qiu"], "venue": "in: Asian Conference on Machine Learning, Vol. 5828 of Lecture Notes in Computer Science, Springer", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2009}, {"title": "Mining high-speed data streams", "author": ["P. Domingos", "G. Hulten"], "venue": "in: Proceedings of the 6th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2000}, {"title": "Leveraging bagging for evolving data streams", "author": ["A. Bifet", "G. Holmes", "B. Pfahringer"], "venue": "in: ECML PKDD\u201910, Springer-Verlag, Berlin, Heidelberg", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2010}, {"title": "Instance-based classification and regression on data streams", "author": ["A. Shaker", "E. H\u00fcllermeier"], "venue": "in: Learning in Non-Stationary Environments, Springer New York", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2012}, {"title": "Batch-incremental versus instance-incremental learning in dynamic and evolving data", "author": ["J. Read", "A. Bifet", "B. Pfahringer", "G. Holmes"], "venue": "in: 11th Int. Symposium on Intelligent Data Analysis", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2012}, {"title": "What are extreme learning machines? filling the gap between frank rosenblatt\u2019s dream and john von neumann\u2019s puzzle", "author": ["G. Huang"], "venue": "Cognitive Computation 7 (3) ", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2015}, {"title": "Stress-testing Hoeffding trees", "author": ["G. Holmes", "R. Kirkby", "B. Pfahringer"], "venue": "in: 9th European Conference on Principles and Practice of Knowledge Discovery in Databases (PKDD \u201905)", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2005}, {"title": "The Elements of Statistical Learning", "author": ["T. Hastie", "R. Tibshirani", "J. Friedman"], "venue": "Springer Series in Statistics, Springer New York Inc., New York, NY, USA", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2001}, {"title": "Reducing the dimensionality of data with neural networks", "author": ["G. Hinton", "R. Salakhutdinov"], "venue": "Science 313 (5786) ", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2006}, {"title": "Rectified linear units improve restricted boltzmann machines", "author": ["V. Nair", "G.E. Hinton"], "venue": "in: Proceedings of the 27th International Conference on Machine Learning (ICML-10), June 21-24, 2010, Haifa, Israel", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2010}, {"title": "Training products of experts by minimizing contrastive divergence", "author": ["G. Hinton"], "venue": "Neural Computation 14 (8) ", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2000}, {"title": "Deep learning in multi-label data-streams", "author": ["J. Read", "F. Perez-Cruz", "A. Bifet"], "venue": "in: Symposium on Applied Computing, ACM", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2015}, {"title": "Extreme learning machines: a survey", "author": ["G.-B. Huang", "D. Wang", "Y. Lan"], "venue": "International Journal of Machine Learning and Cybernetics 2 (2) ", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2011}, {"title": "Universal approximation using incremental constructive feedforward networks with random hidden nodes", "author": ["G.-B. Huang", "L. Chen", "C.-K. Siew"], "venue": "Neural Networks, IEEE Transactions on 17 (4) ", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2006}, {"title": "Incremental extreme learning machine with fully complex hidden nodes", "author": ["G. bin Huang", "M. bin Li", "L. Chen", "C. kheong Siew"], "venue": null, "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2007}, {"title": "Random forests of very fast decision trees on gpu for mining evolving big data streams", "author": ["D. Marron", "A. Bifet", "G.D.F. Morales"], "venue": "in: 21st European Conference on Artificial Intelligence 2014", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2014}, {"title": "D", "author": ["H. Grahn", "N. Lavesson", "M.H. Lapajne"], "venue": "Slat, Cudarf: A cuda-based implementation of random forests., in: H. J. Siegel, A. El-Kadi (Eds.), AICCSA, IEEE Computer Society", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2011}, {"title": "CURFIL: random forests for image labeling on GPU", "author": ["H. Schulz", "B. Waldvogel", "R. Sheikh", "S. Behnke"], "venue": "in: VISAPP 2015 - Proceedings of the 10th International Conference on Computer Vision Theory and Applications, Volume 2, Berlin, Germany, 11-14 March, 2015.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2015}, {"title": "Fast k nearest neighbor search using gpu", "author": ["V. Garcia", "E. Debreuve", "M. Barlaud"], "venue": "in: 2008 IEEE Computer Society Conference on Computer Vision and Pattern Recognition Workshops", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2008}, {"title": "An implementation of high performance parallel knn algorithm based on gpu", "author": ["L. Huang", "Z. Liu", "Z. Yan", "P. Liu", "Q. Cai"], "venue": "in: Networking and Distributed Computing (ICNDC), 2012 Third International Conference on", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2012}, {"title": "Pudiannao: A polyvalent machine learning accelerator", "author": ["D. Liu", "T. Chen", "S. Liu", "J. Zhou", "S. Zhou", "O. Teman", "X. Feng", "X. Zhou", "Y. Chen"], "venue": "in: Proceedings of the Twentieth International Conference on Architectural Support for Programming Languages and Operating Systems, ASPLOS \u201915", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2015}, {"title": "Bagging predictors", "author": ["L. Breiman"], "venue": "Mach. Learn. 24 (2) ", "citeRegEx": "23", "shortCiteRegEx": null, "year": 1996}, {"title": "Experimental comparisons of online and batch versions of bagging and boosting", "author": ["N.C. Oza", "S.J. Russell"], "venue": "in: KDD", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2001}, {"title": "Online bagging and boosting", "author": ["N. Oza", "S. Russell"], "venue": "in: Artificial Intelligence and Statistics 2001, Morgan Kaufmann", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2001}, {"title": "New ensemble methods for evolving data streams", "author": ["A. Bifet", "G. Holmes", "B. Pfahringer", "R. Kirkby", "R. Gavald\u00e0"], "venue": "in: ACM SIGKDD international conference on Knowledge discovery and data mining (KDD \u201909)", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2009}, {"title": "A deep interpretation of classifier chains", "author": ["J. Read", "J. Hollm\u00e9n"], "venue": "in: Advances in Intelligent Data Analysis XIII - 13th International Symposium, IDA 2014", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2014}, {"title": "Learning from time-changing data with adaptive windowing", "author": ["A. Bifet", "R. Gavald\u00e0"], "venue": "in: SIAM International Conference on Data Mining", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2007}], "referenceMentions": [{"referenceID": 0, "context": ", [1]), where batches are constantly gathered over time, and newer models replace older ones as memory fills up.", "startOffset": 2, "endOffset": 5}, {"referenceID": 1, "context": "Nevertheless, incremental methods remain strongly preferred in the data streams literature, and particularly the Hoeffding tree (HT) and its variations [2, 3], k-nearest neighbors (kNN) [4].", "startOffset": 152, "endOffset": 158}, {"referenceID": 2, "context": "Nevertheless, incremental methods remain strongly preferred in the data streams literature, and particularly the Hoeffding tree (HT) and its variations [2, 3], k-nearest neighbors (kNN) [4].", "startOffset": 152, "endOffset": 158}, {"referenceID": 3, "context": "Nevertheless, incremental methods remain strongly preferred in the data streams literature, and particularly the Hoeffding tree (HT) and its variations [2, 3], k-nearest neighbors (kNN) [4].", "startOffset": 186, "endOffset": 189}, {"referenceID": 4, "context": "Support for these options is given by large-scale empirical comparisons [5], where it is also found that methods such as naive Bayes and stochastic gradient descent-based (SGD) are relatively poor performers.", "startOffset": 72, "endOffset": 75}, {"referenceID": 5, "context": "In recent years, Extreme Learning Machines [6] (ELMs) have emerged as a popular framework in Machine Learning.", "startOffset": 43, "endOffset": 46}, {"referenceID": 1, "context": "Hoeffding trees [2] are state-of-the-art in classification for data streams and they predict by choosing the majority class at each leaf.", "startOffset": 16, "endOffset": 19}, {"referenceID": 6, "context": "However, these trees may be conservative at first and in many situations naive Bayes method outperforms the standard Hoeffding tree initially, although it is eventually overtaken [8].", "startOffset": 179, "endOffset": 182}, {"referenceID": 6, "context": "A proposed hybrid adaptive method (by [8]) is a Hoeffding tree with naive Bayes at the leaves, i.", "startOffset": 38, "endOffset": 41}, {"referenceID": 7, "context": "Other established examples include using principal component analysis (reviewed also in [9]) for this transformation, and also Restricted Boltzmann Machines (RBMs) [10].", "startOffset": 88, "endOffset": 91}, {"referenceID": 8, "context": "Other established examples include using principal component analysis (reviewed also in [9]) for this transformation, and also Restricted Boltzmann Machines (RBMs) [10].", "startOffset": 164, "endOffset": 168}, {"referenceID": 9, "context": "In this case, z = \u03c6(x) = f(W>x) for some non-linearity f : a sigmoid function is typical, but more recently rectified linear units (ReLUs, [11]) have fallen into favour.", "startOffset": 139, "endOffset": 143}, {"referenceID": 10, "context": "The weight matrix W is learned with gradient-based methods [12], and the projected output should provide a better feature representation for a neural network or any off-the-shelf method.", "startOffset": 59, "endOffset": 63}, {"referenceID": 11, "context": "already in [13], but concluded that the sensitivity to hyper-parameters and initial conditions prevented good \u2018out-of-the-box\u2019 deployment in data streams.", "startOffset": 11, "endOffset": 15}, {"referenceID": 12, "context": "Approaches such as the so-called extreme learning machines (ELMs) [14] avoid tricky parametrizations by simply using random functions (indeed, ELMs are basically linear learners on top of non-linear data transformations).", "startOffset": 66, "endOffset": 70}, {"referenceID": 13, "context": "Despite the hidden layer weights being random , it has been proven that ELMs is still capable of universal approximation of any non-constant piecewise contiuous function [15].", "startOffset": 170, "endOffset": 174}, {"referenceID": 14, "context": "Also an incremental version of ELMs is proposed in [16].", "startOffset": 51, "endOffset": 55}, {"referenceID": 15, "context": "The only work we are aware of regarding to HT in the context of online realtime data streams mining is[17], were the authors present a parallel implementation of HT and Random Forests for binary trees and data streams achieving goods speedups, but with limitations on the size and with high memory consumption.", "startOffset": 102, "endOffset": 106}, {"referenceID": 16, "context": "More generic HT implementation of Random Forests is presented in [18].", "startOffset": 65, "endOffset": 69}, {"referenceID": 17, "context": "In [19] the authors introduced an open source library, available at github, to predict images labelling using random forests.", "startOffset": 3, "endOffset": 7}, {"referenceID": 18, "context": "Also, kNN has already been successfully ported to GPUs [20].", "startOffset": 55, "endOffset": 59}, {"referenceID": 19, "context": "kNN is also used in business intelligence [21] and has also its implementation on the GPU.", "startOffset": 42, "endOffset": 46}, {"referenceID": 20, "context": "The same way as with HT, a tool for machine learning (including kNN) is described in [22].", "startOffset": 85, "endOffset": 89}, {"referenceID": 7, "context": "Transforming the feature space prior to learning and classification is an established idea in the statistical and machine learning literature [9], for example with basis (or feature-) functions.", "startOffset": 142, "endOffset": 145}, {"referenceID": 21, "context": "Non-streaming bagging [23] builds a set of M base models, training each model with a bootstrap sample of size N created by drawing random samples with replacement from the original training set.", "startOffset": 22, "endOffset": 26}, {"referenceID": 22, "context": "Using this fact, Oza and Russell [24, 25] proposed Online Bagging, an online method that instead of sampling with replacement, gives each example a weight according to Poisson(1).", "startOffset": 33, "endOffset": 41}, {"referenceID": 23, "context": "Using this fact, Oza and Russell [24, 25] proposed Online Bagging, an online method that instead of sampling with replacement, gives each example a weight according to Poisson(1).", "startOffset": 33, "endOffset": 41}, {"referenceID": 24, "context": "ADWIN Bagging [26] is an adaptive version of Online Bagging that uses a change detector to decide when to discard under-performing ensemble models.", "startOffset": 14, "endOffset": 18}, {"referenceID": 2, "context": "Leveraging Bagging (LB, [3]) improves ADWIN Bagging, increasing the weights of this resampling using a larger value \u03bb to compute the value of the Poisson distribution.", "startOffset": 24, "endOffset": 27}, {"referenceID": 12, "context": "The random feature used in these evaluations are basically ELMs [14].", "startOffset": 64, "endOffset": 68}, {"referenceID": 4, "context": "A thorough description of most of the datasets is given in [5].", "startOffset": 59, "endOffset": 62}, {"referenceID": 25, "context": "Of the others, LOC1 and LOC2 are datasets dealing with classifying the location of an object in a grid of 5 \u00d7 5 and 50 \u00d7 50 pixels respectively, described in [27].", "startOffset": 158, "endOffset": 162}, {"referenceID": 4, "context": "As noticed earlier [5], SGD is a poor performer compared to HTs, however, working in a feature space of random ReLUs, SGD-F actually reaches HT performance (on SUSY, and looks promising under ELEC) with similar time complexity.", "startOffset": 19, "endOffset": 22}, {"referenceID": 26, "context": ", [29].", "startOffset": 2, "endOffset": 6}, {"referenceID": 8, "context": "Sizes tested: [10, 100] increment of 10, [100, 1000] increment of 100, and two more sizes: 1500, 2000.", "startOffset": 14, "endOffset": 23}, {"referenceID": 0, "context": "[1] W.", "startOffset": 0, "endOffset": 3}, {"referenceID": 1, "context": "[2] P.", "startOffset": 0, "endOffset": 3}, {"referenceID": 2, "context": "[3] A.", "startOffset": 0, "endOffset": 3}, {"referenceID": 3, "context": "[4] A.", "startOffset": 0, "endOffset": 3}, {"referenceID": 4, "context": "[5] J.", "startOffset": 0, "endOffset": 3}, {"referenceID": 5, "context": "[6] G.", "startOffset": 0, "endOffset": 3}, {"referenceID": 6, "context": "[8] G.", "startOffset": 0, "endOffset": 3}, {"referenceID": 7, "context": "[9] T.", "startOffset": 0, "endOffset": 3}, {"referenceID": 8, "context": "[10] G.", "startOffset": 0, "endOffset": 4}, {"referenceID": 9, "context": "[11] V.", "startOffset": 0, "endOffset": 4}, {"referenceID": 10, "context": "[12] G.", "startOffset": 0, "endOffset": 4}, {"referenceID": 11, "context": "[13] J.", "startOffset": 0, "endOffset": 4}, {"referenceID": 12, "context": "[14] G.", "startOffset": 0, "endOffset": 4}, {"referenceID": 13, "context": "[15] G.", "startOffset": 0, "endOffset": 4}, {"referenceID": 14, "context": "[16] G.", "startOffset": 0, "endOffset": 4}, {"referenceID": 15, "context": "[17] D.", "startOffset": 0, "endOffset": 4}, {"referenceID": 16, "context": "[18] H.", "startOffset": 0, "endOffset": 4}, {"referenceID": 17, "context": "[19] H.", "startOffset": 0, "endOffset": 4}, {"referenceID": 18, "context": "[20] V.", "startOffset": 0, "endOffset": 4}, {"referenceID": 19, "context": "[21] L.", "startOffset": 0, "endOffset": 4}, {"referenceID": 20, "context": "[22] D.", "startOffset": 0, "endOffset": 4}, {"referenceID": 21, "context": "[23] L.", "startOffset": 0, "endOffset": 4}, {"referenceID": 22, "context": "[24] N.", "startOffset": 0, "endOffset": 4}, {"referenceID": 23, "context": "[25] N.", "startOffset": 0, "endOffset": 4}, {"referenceID": 24, "context": "[26] A.", "startOffset": 0, "endOffset": 4}, {"referenceID": 25, "context": "[27] J.", "startOffset": 0, "endOffset": 4}, {"referenceID": 26, "context": "[29] A.", "startOffset": 0, "endOffset": 4}], "year": 2015, "abstractText": "Big Data streams are being generated in a faster, bigger, and more commonplace. In this scenario, Hoeffding Trees are an established method for classification. Several extensions exist, including high-performing ensemble setups such as online and leveraging bagging. Also, k-nearest neighbors is a popular choice, with most extensions dealing with the inherent performance limitations over a potentially-infinite stream. At the same time, gradient descent methods are becoming increasingly popular, owing in part to the successes of deep learning. Although deep neural networks can learn incrementally, they have so far proved too sensitive to hyperparameter options and initial conditions to be considered an effective \u2018off-theshelf\u2019 data-streams solution. In this work, we look at combinations of Hoeffding-trees, nearest neighbour, and gradient descent methods with a streaming preprocessing approach in the form of a random feature functions filter for additional predictive power. We further extend the investigation to implementing methods on GPUs, which we test on some large real-world datasets, and show the benefits of using GPUs for data-stream learning due to their high scalability. Our empirical evaluation yields positive results for the novel approaches that we experiment with, highlighting important issues, and shed light on promising future directions in approaches to data-stream classification.", "creator": "LaTeX with hyperref package"}}}