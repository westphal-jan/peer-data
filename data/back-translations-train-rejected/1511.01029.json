{"id": "1511.01029", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "3-Nov-2015", "title": "Understanding symmetries in deep networks", "abstract": "Recent works have highlighted scale invariance or symmetry present in the weight space of a typical deep network and the adverse effect it has on the Euclidean gradient based stochastic gradient descent optimization. In this work, we show that a commonly used deep network, which uses convolution, batch normalization, reLU, max-pooling, and sub-sampling pipeline, possess more complex forms of symmetry arising from scaling-based reparameterization of the network weights. We propose to tackle the issue of the weight space symmetry by constraining the filters to lie on the unit-norm manifold. Consequently, training the network boils down to using stochastic gradient descent updates on the unit-norm manifold. Our empirical evidence based on the MNIST dataset shows that the proposed updates improve the test performance beyond what is achieved with batch normalization and without sacrificing the computational efficiency of the weight updates.", "histories": [["v1", "Tue, 3 Nov 2015 18:50:03 GMT  (1040kb,D)", "http://arxiv.org/abs/1511.01029v1", "Accepted at the 8th NIPS Workshop on Optimization for Machine Learning (OPT2015) to be held at Montreal, Canada on December 11, 2015"]], "COMMENTS": "Accepted at the 8th NIPS Workshop on Optimization for Machine Learning (OPT2015) to be held at Montreal, Canada on December 11, 2015", "reviews": [], "SUBJECTS": "cs.LG cs.AI cs.CV", "authors": ["vijay badrinarayanan", "bamdev mishra", "roberto cipolla"], "accepted": false, "id": "1511.01029"}, "pdf": {"name": "1511.01029.pdf", "metadata": {"source": "CRF", "title": "Understanding symmetries in deep networks", "authors": ["Vijay Badrinarayanan", "Bamdev Mishra", "Roberto Cipolla"], "emails": ["vb292@cam.ac.uk", "bamdevm@amazon.com", "cipolla@cam.ac.uk"], "sections": [{"heading": null, "text": "Recent work has highlighted the scale invariance or symmetry present in the weight space of a typical deep network, and the adverse effects it has on the Euclidean gradient-based stochastic gradient descend optimization. In this work, we show that a commonly used deep network that uses folding, batch normalization, ReLU, max pooling and sub-sampling pipelines has more complex forms of symmetry resulting from the scale-based repair measurement of network weights. We propose to address the problem of weight space symmetry by forcing filters to lie on the diversity of the unit standard. As a result, the network is trained to use stochastic gradient descend updates on the unit standard. Our empirical evidence, based on the MNIST test set, shows that the proposed data updates will improve the normalization beyond what is proposed with the batch updating."}, {"heading": "1 Introduction", "text": "Most known forms use Euclidean gradients with different learning rates to optimize weights. In this regard, the recent work [2] has brought to light the invariance characteristics in the weight space, often used by deep networks. These symmetries or invariances for weight repair imply that, while the loss function remains unchanged, the Euclidean gradient variations are based on the parameterization chosen. Consequently, the optimization curves may vary significantly for different repair measurements [2]. The precursors of these methods are the early work of Amari [3], which suggested the use of natural gradients to manage weight symmetries in neural networks."}, {"heading": "2 Architecture and symmetry analysis", "text": "A two-layer architecture, ArchBN, is shown in Figure 1. Each layer in ArchBN has typical components commonly found in Convolutionary Neural Networks [16], such as multiplication with a traceable weight matrix (W1, W2), a batch normalization layer (b1, b2) [17], elemental ReLU, 2 \u00b7 1 max-pooling with increment 2, and sub-sampling. The last layer is a soft-max classification layer (b1, b2). The network is trained with a cross-entropy loss. Lines of the weight matrices W1 and W2 correspond to filters in layers 1 and 2, respectively. The dimension of each row corresponds to the input dimension of the layer of the dataset. For the MNIST digits, the input is a 784 dimensional vector."}, {"heading": "3 Resolving symmetry issues using manifold optimization", "text": "An efficient way to solve the symmetries existing in ArchBN is to restrict the weight vectors (filters) in W1 and W2 to lie on the oblique manifold [8, 15], i.e., each filter in the fully connected layers is forced to have the standard (abbreviated UN). To this end, we consider the constraints diag (W1W T 1) = 1 and diag (W2W T 2) = 1, where diag (\u00b7) is an operator extracting the diagonal elements of the argument matrix. To this end, we consider a weight vector w (W1W T) with the constraint wTw = 1 and diag (W2W T T T 2) = 1, where diag (\u00b7) is an operator extracting the diagonal elements of the argument matrix."}, {"heading": "4 Experiments and results", "text": "We train two and four layers of deep ArchBN to perform digital classifications on the MNIST dataset (60K training and 10K test images). We use 64 features per layer. The digit images are converted into a 784-dimensional vector as input for the network (s). We use SGD-based optimization and use the base learning rate from the set 10 \u2212 p for p, 4, 5 units for each training. Class vectors are also normalized from a standard Gaussian and unit. We use SGD-based optimization and use the base learning rate from the set 10 \u2212 p for p."}, {"heading": "5 Application to image segmentation", "text": "We are applying SGD with the proposed UN weight updates in Table 1 to train SegNet, a deep revolutionary network proposed for the segmentation of streetscapes into several classes [21]. Although convolutional, this network has the same symmetries as those analyzed for ArchBN in (1).The network is trained for 100 epochs on the CamVid [22] training set of 367 images.The predictions for some test images from CamVid are shown in Figure 2. These qualitative results point to the usefulness of symmetry invariant weight updates for larger networks that occur in practice."}, {"heading": "6 Conclusion", "text": "We have highlighted the symmetries that exist in the weight space of deep neural network architectures that are currently popular, and these symmetries can be absorbed into the gradient descent by applying a unit of measurement for the filter weights, taking into account the diverse structure on which the weights of the network are based. Empirical results show that our proposed weight update technique can improve test performance on a modern architecture. As a future direction of research, we would like to explore other efficient symmetrically invariant weight update techniques and use them for deep revolutionary neural networks that are used in practical applications."}, {"heading": "Acknowledgments", "text": "Bamdev Mishra was supported as a research fellow of the FNRS (Belgian Fund for Scientific Research), the scientific responsibility lies with the authors."}], "references": [{"title": "Large-scale machine learning with stochastic gradient descent", "author": ["L. Bottou"], "venue": "International Conference on Computational Statistics (COMPSTAT), pages 177\u2013186", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2010}, {"title": "Path-sgd: Path-normalized optimization in deep neural networks", "author": ["B. Neyshabur", "R. Salakhutdinov", "N. Srebro"], "venue": "Advances in Neural Information Processing Systems 29 (NIPS)", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2015}, {"title": "Natural gradient works efficiently in learning", "author": ["S.-I. Amari"], "venue": "Neural computation, 10(2):251\u2013276", "citeRegEx": "3", "shortCiteRegEx": null, "year": 1998}, {"title": "Revisiting natural gradient for deep networks", "author": ["R. Pascanu", "Y. Bengio"], "venue": "Technical report, arXiv:1301.3584", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2013}, {"title": "Natural neural networks", "author": ["G. Desjardins", "K. Simonyan", "R. Pascanu", "K. Kavukcuoglu"], "venue": "Technical report, arXiv:1507.00210", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2015}, {"title": "Riemannian metrics for neural networks I: Feedforward networks", "author": ["Y. Ollivier"], "venue": "Information and Inference, 4(2):108\u2013153", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2015}, {"title": "Riemannian metrics for neural networks II: Recurrent networks and learning symbolic data sequences", "author": ["Y. Ollivier"], "venue": "Information and Inference, 4(2):154\u2013193", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2015}, {"title": "Optimization Algorithms on Matrix Manifolds", "author": ["P.-A. Absil", "R. Mahony", "R. Sepulchre"], "venue": "Princeton University Press, Princeton, NJ", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2008}, {"title": "Riemannian preconditioning", "author": ["B. Mishra", "R. Sepulchre"], "venue": "Technical report, arXiv:1405.6055", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2014}, {"title": "Low-rank matrix completion via preconditioned optimization on the Grassmann manifold", "author": ["N. Boumal", "P.-A. Absil"], "venue": "Linear Algebra and its Applications, 475:200\u2013239", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2015}, {"title": "Low-rank optimization on the cone of positive semidefinite matrices", "author": ["M. Journ\u00e9e", "F. Bach", "P.-A. Absil", "R. Sepulchre"], "venue": "SIAM Journal on Optimization, 20(5):2327\u20132351", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2010}, {"title": "Riemannian geometry of Grassmann manifolds with a view on algorithmic computation", "author": ["P.-A. Absil", "R. Mahony", "R. Sepulchre"], "venue": "Acta Applicandae Mathematicae, 80(2):199\u2013220", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2004}, {"title": "The geometry of algorithms with orthogonality constraints", "author": ["A. Edelman", "T.A. Arias", "S.T. Smith"], "venue": "SIAM Journal on Matrix Analysis and Applications, 20(2):303\u2013353", "citeRegEx": "13", "shortCiteRegEx": null, "year": 1998}, {"title": "Optimization algorithms exploiting unitary constraints", "author": ["J.H Manton"], "venue": "IEEE Transactions on Signal Processing, 50(3):635\u2013650", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2002}, {"title": "Manopt", "author": ["N. Boumal", "B. Mishra", "P.-A. Absil", "R. Sepulchre"], "venue": "a matlab toolbox for optimization on manifolds. The Journal of Machine Learning Research, 15(1):1455\u20131459", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2014}, {"title": "Deep learning", "author": ["Y. LeCun", "Y. Bengio", "G. Hinton"], "venue": "Nature, 521(7553):436\u2013444", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2015}, {"title": "Batch normalization: Accelerating deep network training by reducing internal covariate shift", "author": ["S. Ioffe", "C. Szegedy"], "venue": "International Conference on Machine learning (ICML)", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2015}, {"title": "Stochastic gradient descent on Riemannian manifolds", "author": ["S. Bonnabel"], "venue": "IEEE Transactions on Automatic Control, 58(9):2217\u20132229", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2013}, {"title": "Lecture notes", "author": ["G. Hinton"], "venue": "Technical report, University of Toronto", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2008}, {"title": "Do deep nets really need to be deep? In Advances in Neural Information Processing Systems 28 (NIPS)", "author": ["J. Ba", "R. Caruana"], "venue": "pages 2654\u20132662", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2014}, {"title": "Segnet: A deep convolutional encoder-decoder architecture for robust semantic pixel-wise labelling", "author": ["V. Badrinarayanan", "A. Handa", "R. Cipolla"], "venue": "Technical report, arXiv:1505.07293", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2015}, {"title": "Semantic object classes in video: A high-definition ground truth database", "author": ["G. Brostow", "J. Fauqueur", "R. Cipolla"], "venue": "Pattern Recognition Letters, 30(2):88\u201397", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2009}], "referenceMentions": [{"referenceID": 0, "context": "Stochastic gradient descent (SGD) has been the workhorse for optimization of deep networks [1].", "startOffset": 91, "endOffset": 94}, {"referenceID": 1, "context": "In this regard, the recent work [2] has brought to light scale invariance properties in the weight space which commonly used deep networks possess.", "startOffset": 32, "endOffset": 35}, {"referenceID": 1, "context": "Consequently, optimization trajectories can vary significantly for different reparameterizations [2].", "startOffset": 97, "endOffset": 100}, {"referenceID": 2, "context": "Although these issues have been raised recently, the precursor to these methods is the early work of Amari [3], who proposed the use of natural gradients to tackle weight space symmetries in neural networks.", "startOffset": 107, "endOffset": 110}, {"referenceID": 3, "context": "The idea is to compute the steepest descent direction for the weight update on the manifold defined by these symmetries and use this direction to update the weights [4, 5, 6, 7].", "startOffset": 165, "endOffset": 177}, {"referenceID": 4, "context": "The idea is to compute the steepest descent direction for the weight update on the manifold defined by these symmetries and use this direction to update the weights [4, 5, 6, 7].", "startOffset": 165, "endOffset": 177}, {"referenceID": 5, "context": "The idea is to compute the steepest descent direction for the weight update on the manifold defined by these symmetries and use this direction to update the weights [4, 5, 6, 7].", "startOffset": 165, "endOffset": 177}, {"referenceID": 6, "context": "The idea is to compute the steepest descent direction for the weight update on the manifold defined by these symmetries and use this direction to update the weights [4, 5, 6, 7].", "startOffset": 165, "endOffset": 177}, {"referenceID": 7, "context": "On the other hand, optimization over a manifold with symmetries or invariances has been a topic of much research and provides guidance to other simpler metric constructions [8, 9, 10, 11, 12, 13, 14].", "startOffset": 173, "endOffset": 199}, {"referenceID": 8, "context": "On the other hand, optimization over a manifold with symmetries or invariances has been a topic of much research and provides guidance to other simpler metric constructions [8, 9, 10, 11, 12, 13, 14].", "startOffset": 173, "endOffset": 199}, {"referenceID": 9, "context": "On the other hand, optimization over a manifold with symmetries or invariances has been a topic of much research and provides guidance to other simpler metric constructions [8, 9, 10, 11, 12, 13, 14].", "startOffset": 173, "endOffset": 199}, {"referenceID": 10, "context": "On the other hand, optimization over a manifold with symmetries or invariances has been a topic of much research and provides guidance to other simpler metric constructions [8, 9, 10, 11, 12, 13, 14].", "startOffset": 173, "endOffset": 199}, {"referenceID": 11, "context": "On the other hand, optimization over a manifold with symmetries or invariances has been a topic of much research and provides guidance to other simpler metric constructions [8, 9, 10, 11, 12, 13, 14].", "startOffset": 173, "endOffset": 199}, {"referenceID": 12, "context": "On the other hand, optimization over a manifold with symmetries or invariances has been a topic of much research and provides guidance to other simpler metric constructions [8, 9, 10, 11, 12, 13, 14].", "startOffset": 173, "endOffset": 199}, {"referenceID": 13, "context": "On the other hand, optimization over a manifold with symmetries or invariances has been a topic of much research and provides guidance to other simpler metric constructions [8, 9, 10, 11, 12, 13, 14].", "startOffset": 173, "endOffset": 199}, {"referenceID": 16, "context": "Figure 1: ArchBN: a two layer deep architecture for classification with batch normalization [17].", "startOffset": 92, "endOffset": 96}, {"referenceID": 14, "context": "The stochastic gradient descent algorithms with the proposed updates are implemented in Matlab and Manopt [15].", "startOffset": 106, "endOffset": 110}, {"referenceID": 15, "context": "Each layer in ArchBN has typical components commonly found in convolutional neural networks [16] such as multiplication with a trainable weight matrix (W1,W2), a batch normalization layer (b1, b2) [17], element-wise rectification ReLU, 2\u00d7 1 max-pooling with stride 2, and sub-sampling.", "startOffset": 92, "endOffset": 96}, {"referenceID": 16, "context": "Each layer in ArchBN has typical components commonly found in convolutional neural networks [16] such as multiplication with a trainable weight matrix (W1,W2), a batch normalization layer (b1, b2) [17], element-wise rectification ReLU, 2\u00d7 1 max-pooling with stride 2, and sub-sampling.", "startOffset": 197, "endOffset": 201}, {"referenceID": 16, "context": "The batch normalization [17] layer normalizes each feature (element) in the h1 and h2 layers to have zero-mean unit variance over each mini-batch.", "startOffset": 24, "endOffset": 28}, {"referenceID": 16, "context": "Empirical results in [17] show that this significantly improves convergence and our experiments also support this claim.", "startOffset": 21, "endOffset": 25}, {"referenceID": 1, "context": "It should be stressed that our analysis differs from [2], where the authors deal with a simpler case of \u03b1 = \u03b10, \u03b2 = 1 \u03b10 , and \u03b10 is a non-zero scalar.", "startOffset": 53, "endOffset": 56}, {"referenceID": 1, "context": "Unfortunately, the Euclidean gradient of the weights (used in standard SGD) is not invariant to reparameterizations of the weights [2].", "startOffset": 131, "endOffset": 134}, {"referenceID": 14, "context": "It is defined as \u03a0W (Z) = Z \u2212Diag(diag((ZW ))W [15].", "startOffset": 47, "endOffset": 51}, {"referenceID": 7, "context": "An efficient way to resolve the symmetries that exist in ArchBN is to constrain the weight vectors (filters) in W1 and W2 to lie on the oblique manifold [8, 15], i.", "startOffset": 153, "endOffset": 160}, {"referenceID": 14, "context": "An efficient way to resolve the symmetries that exist in ArchBN is to constrain the weight vectors (filters) in W1 and W2 to lie on the oblique manifold [8, 15], i.", "startOffset": 153, "endOffset": 160}, {"referenceID": 0, "context": "The convergence analysis of SGD on manifolds follows the developments in [1, 18].", "startOffset": 73, "endOffset": 80}, {"referenceID": 17, "context": "The convergence analysis of SGD on manifolds follows the developments in [1, 18].", "startOffset": 73, "endOffset": 80}, {"referenceID": 20, "context": "Figure 2: SGD with the proposed UN weight updates, shown in Table 1, for training SegNet [21].", "startOffset": 89, "endOffset": 93}, {"referenceID": 18, "context": "For each full dataset training run, we use the bold-driver protocol [19] to anneal the learning rate.", "startOffset": 68, "endOffset": 72}, {"referenceID": 19, "context": "This raises the question for future research as to whether some deep networks necessarily have to be that deep or it can be made shallower (and efficient) by better optimization [20].", "startOffset": 178, "endOffset": 182}, {"referenceID": 20, "context": "We apply SGD with the proposed UN weight updates in Table 1 for training SegNet, a deep convolutional network proposed for road scene image segmentation into multiple classes [21].", "startOffset": 175, "endOffset": 179}, {"referenceID": 21, "context": "The network is trained for 100 epochs on the CamVid [22] training set of 367 images.", "startOffset": 52, "endOffset": 56}], "year": 2015, "abstractText": "Recent works have highlighted scale invariance or symmetry present in the weight space of a typical deep network and the adverse effect it has on the Euclidean gradient based stochastic gradient descent optimization. In this work, we show that a commonly used deep network, which uses convolution, batch normalization, reLU, max-pooling, and sub-sampling pipeline, possess more complex forms of symmetry arising from scaling-based reparameterization of the network weights. We propose to tackle the issue of the weight space symmetry by constraining the filters to lie on the unit-norm manifold. Consequently, training the network boils down to using stochastic gradient descent updates on the unit-norm manifold. Our empirical evidence based on the MNIST dataset shows that the proposed updates improve the test performance beyond what is achieved with batch normalization and without sacrificing the computational efficiency of the weight updates.", "creator": "LaTeX with hyperref package"}}}