{"id": "1511.01158", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "3-Nov-2015", "title": "Distributed Deep Learning for Question Answering", "abstract": "This paper is an empirical study of the distributed deep learning for a question answering subtask: answer selection. Comparison studies of SGD, MSGD, DOWNPOUR and EASGD/EAMSGD algorithms have been presented. Experimental results show that the message passing interface based distributed framework can accelerate the convergence speed at a sublinear scale. This paper demonstrates the importance of distributed training: with 120 workers, an 83x speedup is achievable and running time is decreased from 107.9 hours to 1.3 hours, which will benefit the productivity significantly.", "histories": [["v1", "Tue, 3 Nov 2015 23:18:35 GMT  (41kb)", "http://arxiv.org/abs/1511.01158v1", null], ["v2", "Fri, 13 May 2016 15:41:54 GMT  (42kb)", "http://arxiv.org/abs/1511.01158v2", null], ["v3", "Thu, 4 Aug 2016 16:41:37 GMT  (32kb)", "http://arxiv.org/abs/1511.01158v3", "This paper will appear in the Proceeding of The 25th ACM International Conference on Information and Knowledge Management (CIKM 2016), Indianapolis, USA"]], "reviews": [], "SUBJECTS": "cs.LG cs.CL cs.DC", "authors": ["minwei feng", "bing xiang", "bowen zhou"], "accepted": false, "id": "1511.01158"}, "pdf": {"name": "1511.01158.pdf", "metadata": {"source": "CRF", "title": "Distributed Deep Learning for Answer Selection", "authors": ["Minwei Feng"], "emails": ["<mfeng@us.ibm.com", "bingxia@us.ibm.com", "zhou>@us.ibm.com"], "sections": [{"heading": null, "text": "ar Xiv: 151 1,01 158v 1 [cs.L G] 3N ov2 01"}, {"heading": "1 Introduction", "text": "A practical challenge of deep learning is the very time-consuming training process. It is not uncommon to see the reported training time in the order of days or even weeks in research work, but this is rarely acceptable for practical commercial use (e.g. training as a service in the cloud), where rapid response time is expected from customers. Even in research environments, the long time calculation could discourage scientists from conducting as many experiments as necessary and slowing down the research and development cycle. Therefore, distributed deep learning training has become a critical research direction, along with the advancement of deep learning itself on the algorithm side. Various infrastructures and experimental results have recently been published, and many of these systems come from leading IT companies. Most of this work reports on the results of Computervision benchmark tasks such as CIFAR10 or ImageNet."}, {"heading": "2 Related Works", "text": "One of the pioneering works is Google's Distbelief System [3], in which DOWNPOUR was proposed. The system has multiple parameter servers and clients, and most of the other work follows the same spirit of DOWNPOUR. Adam [2] is another similar system that has many technical features such as reduced memory copies and mitigating the effects of slow machines. IBM's Rudra System [5] is aA Q HLQA CNNQAP + RP + R SimilarityFigure 1: Architecture. HL is the hidden layer with tanh activation function. CNN is the revolutionary neural network. P stands for maxpooling and R for the ReLU activation function. QA means that the weights of the corresponding layer are shared by Q and A.master-client."}, {"heading": "3 Answer Selection Task", "text": "This section describes the benchmark test used in this thesis. Unlike many previous papers, we are working on a QVA task: Q selection. The paper [4] has created an open task (including the released corpus) that serves as a benchmark for comparison purposes. For a detailed description of the data and task, please refer to [4]. A summary is given here to capture the work itself. Selecting the answer is an important subtask of the QA \u2212 a question and an answer candidate pool {a1, a2,..., as} for this question (s is the pool size) the goal is to find the best answer candidate ak, 1 \u2264 k \u2264 s. If the selected answer ak is within the basic truth set (a question may have more than one correct answer) of question q, the question q is considered to be answered correctly, otherwise it is answered incorrectly (s is the pool size)."}, {"heading": "4 MPI-based Framework", "text": "We have opted for Message Passing Interface (MPI) technology to allow master and client to communicate with each other. Message Passing is designed for applications such as deep learning, where frequent communication is required and is reported to be more efficient than other higher-level toolkits such as Spark [8]. Figure 2 demonstrates our MPI framework. There are two types of processes: client and server, which are distributed over the high-performance computing clusters. In this paper, the clients perform the forward / reverse flow of the neural network and send the update messages to servers. The servers maintain a central model. They receive messages from clients and update the central model and send the latest model back to the clients. In this paper, we always use 1https: / / github.com / dmlc / mxnet the non-blocking communication (MPI _ ISend / MPI _ IRecv) to increase the overall speed. To reduce the communication overhead, we always use 1https: / / github.com / dmlc / mxnet to handle the non-blocking communication (MPI _ ISend / MPI _ IRecv) in order to increase the overall speed."}, {"heading": "5 Distributed Training Algorithms", "text": "We have compared state-of-the-art algorithms: Stochastic Gradient Descent (SGD) [1], Dynamics stochastic Gradient Descent (MSGD) [9], DOWNPOUR \u2212 3], Elastic Average stochastic Gradient Descent (EASGD) [11] and its variation dynamics EASGD (EAMSGD) [11]. The DOWNPOUR and EASGD algorithms are given in Tables 1 and 2. In both tables, the loop is endless because the purpose of this work is to compare the convergence speed. In practice, it should be valid forever until the criteria. x is the central model maintained by the customer. x is the model the customer holds i. gi ti (xi) is the local gradient calculation by the customer i in due time."}, {"heading": "6 Experimental Results", "text": "Table 3 shows the comparative results of convergence acceleration. However, for the method of SGD and MSGD, we run the training for 5 days. For other methods that use multiple servers and clients, the runtime limit is then set at 12 hours. This means saving computing resources so that more experiments can be planned. Also, in practice, it is much less useful if runtime is still prohibitive when large amounts of computing resources are used. As shown in [4], the accuracy on the Test1 corpus is about 63%. Therefore, we define the convergence speed as the time required for the accuracy of the Test1 corpus to climb to 63%. Table 3a shows the performance of different methods under different learning rates and it is clear that the learning rate makes a decisive influence on the convergence speed."}, {"heading": "7 Conclusions", "text": "We have carried out an empirical study for the selection task, which is a crucial component of the task. We form the framework with the MPI. State-of-the-art algorithms have been compared, including SGD / MSGD, DOWNPOUR and EASGD / EAMSGD. To the best of our knowledge, this is the first time that the experimental results for distributed deep learning have been reported on a QA task. This work proves the importance of distributed training. 83-fold acceleration is achievable with the use of 120 workers, which is a huge gain for practical productivity. We realize that due to the lack of a solid mathematical basis, distributed deep learning is still an experimental method. Our experience shows that the coordination of hyperparameters can play a crucial role in convergence speed. On the other hand, the task itself could change performance."}, {"heading": "Acknowledgments", "text": "The authors thank Sixin Zhang for implementation aids, helpful discussions and valuable feedback."}], "references": [{"title": "Online learning in neural networks. chapter Online Learning and Stochastic Approximations", "author": ["L\u00e9on Bottou"], "venue": null, "citeRegEx": "1", "shortCiteRegEx": "1", "year": 1998}, {"title": "Project adam: Building an efficient and scalable deep learning training system", "author": ["Trishul Chilimbi", "Yutaka Suzue", "Johnson Apacible", "Karthik Kalyanaraman"], "venue": "In Proceedings of the 11th USENIX Symposium on Operating Systems Design and Implementation (OSDI", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2014}, {"title": "Large scale distributed deep networks", "author": ["Jeffrey Dean", "Greg Corrado", "Rajat Monga", "Kai Chen", "Matthieu Devin", "Quoc V. Le", "Mark Z. Mao", "Marc\u2019Aurelio Ranzato", "Andrew W. Senior", "Paul A. Tucker", "Ke Yang", "Andrew Y. Ng"], "venue": "In Advances in Neural Information Processing Systems 25: 26th Annual Conference on Neural Information Processing Systems", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2012}, {"title": "Applying deep learning to answer selection: A study and an open task", "author": ["Minwei Feng", "Bing Xiang", "Michael R. Glass", "Lidan Wang", "Bowen Zhou"], "venue": "In Proceedings of the 2015 IEEE Automatic Speech Recognition and Understanding Workshop (ASRU", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2015}, {"title": "Model Accuracy and Runtime Tradeoff in Distributed Deep Learning", "author": ["S. Gupta", "W. Zhang", "J. Milthorpe"], "venue": "ArXiv e-prints,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2015}, {"title": "Scaling distributed machine learning with the parameter server", "author": ["Mu Li", "David G. Andersen", "Jun Woo Park", "Alexander J. Smola", "Amr Ahmed", "Vanja Josifovski", "James Long", "Eugene J. Shekita", "Bor-Yiing Su"], "venue": "In 11th USENIX Symposium on Operating Systems Design and Implementation (OSDI", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2014}, {"title": "Large-scale logistic regression and linear support vector machines using spark", "author": ["Chieh-Yen Lin", "Cheng-Hao Tsai", "Ching-Pei Lee", "Chih-Jen Lin"], "venue": "IEEE International Conference on Big Data,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2014}, {"title": "On the importance of initialization and momentum in deep learning", "author": ["Ilya Sutskever", "James Martens", "George E. Dahl", "Geoffrey E. Hinton"], "venue": "In Proceedings of the 30th International Conference on Machine Learning (ICML-13),", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2013}, {"title": "Petuum: A new platform for distributed machine learning on big data", "author": ["Eric P. Xing", "Qirong Ho", "Wei Dai", "Jin-Kyu Kim", "Jinliang Wei", "Seunghak Lee", "Xun Zheng", "Pengtao Xie", "Abhimanu Kumar", "Yaoliang Yu"], "venue": "In Proceedings of the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2015}, {"title": "Deep learning with elastic averaging SGD", "author": ["Sixin Zhang", "Anna Choromanska", "Yann LeCun"], "venue": "In Proceedings of the 2015 Conference on Neural Information Processing Systems", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2015}], "referenceMentions": [{"referenceID": 0, "context": "We have compared the latest technologies, including SGD [1] , MSGD [9] , DOWNPOUR [3] and EASGD/EAMSGD [11].", "startOffset": 56, "endOffset": 59}, {"referenceID": 7, "context": "We have compared the latest technologies, including SGD [1] , MSGD [9] , DOWNPOUR [3] and EASGD/EAMSGD [11].", "startOffset": 67, "endOffset": 70}, {"referenceID": 2, "context": "We have compared the latest technologies, including SGD [1] , MSGD [9] , DOWNPOUR [3] and EASGD/EAMSGD [11].", "startOffset": 82, "endOffset": 85}, {"referenceID": 9, "context": "We have compared the latest technologies, including SGD [1] , MSGD [9] , DOWNPOUR [3] and EASGD/EAMSGD [11].", "startOffset": 103, "endOffset": 107}, {"referenceID": 2, "context": "One of the pioneering work is Google\u2019s Distbelief system [3] in which DOWNPOUR has been proposed.", "startOffset": 57, "endOffset": 60}, {"referenceID": 1, "context": "The system Adam [2] is another similar framework which has many engineering features like reduced memory copies and mitigating the impact of slow machines.", "startOffset": 16, "endOffset": 19}, {"referenceID": 4, "context": "IBM\u2019s Rudra system [5] is a", "startOffset": 19, "endOffset": 22}, {"referenceID": 5, "context": "A parameter server framework is proposed in [7] that supports flexible consistency models, elastic scalability and continuous fault tolerance.", "startOffset": 44, "endOffset": 47}, {"referenceID": 5, "context": "[7] provides the APIs so that other framework like MXNet1 can utilize it.", "startOffset": 0, "endOffset": 3}, {"referenceID": 8, "context": "The platform Petuum [10] supports a synchronization model with bounded staleness.", "startOffset": 20, "endOffset": 24}, {"referenceID": 3, "context": "The paper [4] created an open task (including the released corpus) which serves as a benchmark for comparison purpose.", "startOffset": 10, "endOffset": 13}, {"referenceID": 3, "context": "For the detailed description of the data and task please refer to [4].", "startOffset": 66, "endOffset": 69}, {"referenceID": 6, "context": "Message passing is designed for application like deep learning where frequent communications need to be conducted and has been reported to be more efficient than other higher level toolkit like Spark [8] .", "startOffset": 200, "endOffset": 203}, {"referenceID": 0, "context": "We have compared state-of-the-art algorithms: stochastic gradient descent (SGD) [1] , momentum stochastic gradient descent (MSGD) [9], DOWNPOUR [3] , elastic averaging stochastic gradient descent (EASGD) [11] and its variation momentum EASGD (EAMSGD) [11] .", "startOffset": 80, "endOffset": 83}, {"referenceID": 7, "context": "We have compared state-of-the-art algorithms: stochastic gradient descent (SGD) [1] , momentum stochastic gradient descent (MSGD) [9], DOWNPOUR [3] , elastic averaging stochastic gradient descent (EASGD) [11] and its variation momentum EASGD (EAMSGD) [11] .", "startOffset": 130, "endOffset": 133}, {"referenceID": 2, "context": "We have compared state-of-the-art algorithms: stochastic gradient descent (SGD) [1] , momentum stochastic gradient descent (MSGD) [9], DOWNPOUR [3] , elastic averaging stochastic gradient descent (EASGD) [11] and its variation momentum EASGD (EAMSGD) [11] .", "startOffset": 144, "endOffset": 147}, {"referenceID": 9, "context": "We have compared state-of-the-art algorithms: stochastic gradient descent (SGD) [1] , momentum stochastic gradient descent (MSGD) [9], DOWNPOUR [3] , elastic averaging stochastic gradient descent (EASGD) [11] and its variation momentum EASGD (EAMSGD) [11] .", "startOffset": 204, "endOffset": 208}, {"referenceID": 9, "context": "We have compared state-of-the-art algorithms: stochastic gradient descent (SGD) [1] , momentum stochastic gradient descent (MSGD) [9], DOWNPOUR [3] , elastic averaging stochastic gradient descent (EASGD) [11] and its variation momentum EASGD (EAMSGD) [11] .", "startOffset": 251, "endOffset": 255}, {"referenceID": 3, "context": "As shown in [4], the accuracy on the test1 corpus is around 63%.", "startOffset": 12, "endOffset": 15}, {"referenceID": 9, "context": "For example, in [11] the EASGD/EAMSGD work better than DOWNPOUR for the image classification tasks while for this answer selection task DOWNPOUR performs better.", "startOffset": 16, "endOffset": 20}], "year": 2015, "abstractText": "This paper is an empirical study of the distributed deep learning for a question answering subtask: answer selection. Comparison studies of SGD, MSGD, DOWNPOUR and EASGD/EAMSGD algorithms have been presented. Experimental results show that the message passing interface based distributed framework can accelerate the convergence speed at a sublinear scale. This paper demonstrates the importance of distributed training: with 120 workers, an 83x speedup is achievable and running time is decreased from 107.9 hours to 1.3 hours, which will benefit the productivity significantly.", "creator": "gnuplot 5.0 patchlevel 0"}}}