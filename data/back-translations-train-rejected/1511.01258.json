{"id": "1511.01258", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "4-Nov-2015", "title": "Learn on Source, Refine on Target:A Model Transfer Learning Framework with Random Forests", "abstract": "We propose novel model transfer-learning methods that refine a decision forest model M learned within a \"source\" domain using a training set sampled from a \"target\" domain, assumed to be a variation of the source. We present two random forest transfer algorithms. The first algorithm searches greedily for locally optimal modifications of each tree structure by trying to locally expand or reduce the tree around individual nodes. The second algorithm does not modify structure, but only the parameter (thresholds) associated with decision nodes. We also propose to combine both methods by considering an ensemble that contains the union of the two forests. The proposed methods exhibit impressive experimental results over a range of problems.", "histories": [["v1", "Wed, 4 Nov 2015 09:41:12 GMT  (499kb,D)", "https://arxiv.org/abs/1511.01258v1", "2 columns, 14 pages, TPAMI submitted"], ["v2", "Sun, 8 Nov 2015 06:56:12 GMT  (499kb,D)", "http://arxiv.org/abs/1511.01258v2", "2 columns, 14 pages, TPAMI submitted"]], "COMMENTS": "2 columns, 14 pages, TPAMI submitted", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["noam segev", "maayan harel", "shie mannor", "koby crammer", "ran el-yaniv"], "accepted": false, "id": "1511.01258"}, "pdf": {"name": "1511.01258.pdf", "metadata": {"source": "CRF", "title": "LEARN ON SOURCE, REFINE ON TARGET: A MODEL TRANSFER LEARNING FRAMEWORK WITH RANDOM FORESTS 1 Learn on Source, Refine on Target: A Model Transfer Learning Framework with Random Forests", "authors": ["Noam Segev", "Maayan Harel", "Shie Mannor", "Koby Crammer", "Ran El-Yaniv"], "emails": ["rani]@cs.technion.ac.il", "maayanga@ee.technion.ac.il", "shie@ee.technion.ac.il", "koby@ee.technion.ac.il"], "sections": [{"heading": null, "text": "Index Terms - Transfer Learning, Model Transfer, Random Forest, Decision Tree F"}, {"heading": "1 INTRODUCTION", "text": "In fact, it is such that most people will be able to move into a different world, in which they are able to live, in which they are able to live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they, in which they live, in which they, in which they live, in which they live, in which they, in which they live, in which they live, in which they live."}, {"heading": "2 PRELIMINARY DEFINITIONS", "text": "A domain D = (X, Y, P) consists of an r-dimensional attribute space X, an identifier space Y and a probability distribution P (x, y), where x-X is the attribute vector and y-Y. In supervised model transfer learning, we get two domains: a source domain, DS = (XS, YS, PS), and a target domain, DT = (XT, YT, PT). Faced with a loss function \": Y \u00d7 Y \u2192 R +, a source prediction function f: XS \u2192 YS and (typically limited) a target training that ST is drawn from DT, our goal is to have a function f: FT: XT \u2192 YT with low risk R (f) = EPT (x, y) {'(f (x), y)}, a source domain that refers to the target domain vi (typically), a different transfer learning models have different constraints on the relationship between the source domain and the target domain."}, {"heading": "2.1 Random Forests Models:", "text": "Our algorithms are based on standardized Random Forests (RFs) [12]. For a tree in the forest, we use the following notations: Each tree node v has an outer degree d (v) and its children are called v1,..., vd (v). A leaf node v is associated with a single decision value in Y, which is called y (v). An internal (non-leaf) node v is associated with a single attribute \u03c6 (v), and for a numerical attribute it is also associated with a numerical threshold \u03c4 (v). The classification of a sample is based on the leaf that reaches the sample, i.e. the leaf at the end of the path in the tree that the sample will follow, and for each node u in this way we say that x \"arrives\" at u."}, {"heading": "3 ALGORITHMS", "text": "We now describe our two algorithms, SER and STRUT, to convert trees into the target domain."}, {"heading": "3.1 Structure Expansion/Reduction", "text": "In expansion transformation, we specialize in rules that are induced to the target data via the source data. In reduction, we perform the reverse operation, i.e. we generalize rules induced via the source data. First, a random forest is induced using the source data SS. The SER algorithm begins by calculating for each node v the sentence STv of all marked points in the target data S T that reaches v. Then, each leaf v is expanded to a full tree in relation to the sample set STv. Finally, the algorithm - working from the bottom up on the tree - attempts to perform a structure reduction as follows. Two error types are defined for the node v in relation to STv. The first, the sub-tree error, is the empirical error of the sub-tree whose root is v. The second, the leaf error, is defined on the basis of the error present when this error is incorporated into the first step of the leaf tree (when the leaf error is inserted)."}, {"heading": "3.1.1 Visual Illustration of the SER Algorithm", "text": "The SER algorithm applies two local transformations to a given decision tree. To gain some intuition about how it works, we illustrate these transformations. In Figures (1a, 1b), we present two simple domains (classification problems). The DTs learned for each domain are illustrated in Figure (1c). A standard DT induced via (1a) leading to the LHS tree in Figure (1c) can easily be transferred to a domain (1b) using the expansion operation (applied by SER), resulting in the RHS tree in Figure (1c). Similarly, the tree induced for (1b) can be transferred to (1a) using the reverse reduction operation. It is obvious that a single classifier can describe two identical domains."}, {"heading": "3.1.2 Logical Regularization in SER", "text": "In this section, we explain the reasons for this construction and argue that it serves as a kind of regulation that keeps the resulting target model closer to the source model than it would be if the reduction then preceded expansion. It is well known that a decision tree corresponds to a disjunctive normal form (DNF) in which a single rule representing a root-to-leaf path is equivalent to a conjunction of letters [13]. Let u = u0,. To be a root toleaf path in a tree before executing the SER algorithms, with a rule corresponding to that path. Let u \u2032 S = u \u2032 0 be a root-to-leaf path in a tree after executing the SER algorithms, with rules corresponding to that path."}, {"heading": "3.2 Structure Transfer", "text": "The Structure Transfer (STRUT) algorithm is motivated by the observation that decision trees for similar problems should be structurally similar, for example, consider the similar problems of detecting online fraud in two major banks in two different geo-demographic environments (say, one threshold is in the US and the other is in India), both of which can be modeled to share many of the features and their dependencies (e.g., the characteristics \"typical customer transaction size\" should appear in both models), but the scale of such features and associated decision thresholds is likely to be between the problems.The STRUT algorithm adapts to the source samples by discarding all numerical thresholds in the tree and selecting a new threshold (v) for a node-v with a numerical feature that uses STv, the subset of targets that reach the STRUT. If the algorithm encounters a node-v, it is considered empty for TV, it is considered insufficient."}, {"heading": "3.2.1 On the Use of IG and DG", "text": "As discussed above, the STRUT algorithm relies on both the DG and IG measures to optimize the adjusted thresholds. Here, we explain the motivation for using this combination of measures. The IG is effective in quantifying the \"informativeness\" of a threshold. However, IG first ignores the dependencies imposed by the given structure. In contrast, DG is a global regulatory measure that does not take local profits into account. The following examples show that each of these measures does not, on its own, select an appropriate threshold. First, let us consider the application of IG. Let us define two simple areas in which the X feature space is the range [0, 1], and our label space Y = {\u00b1 1}. The source and target distributions, Ps and Pt, are taken into Ps (x) = {1 0.4 < x < x < x &ltx < 0.7 \u2212 1 otherwise; Pt (x) = {1 0.3 < < < < < < < < < < < < < < < &ltx < < < < < < &ltx < < < < < < < < < < < < < < < < < < < < &ltx < < < < < < < < < < < < < < < < < < < < x < < < < < < < x < < < < < x; < < < < x; < < < < > < < < > < < < > < < < < <"}, {"heading": "3.3 A MIX Approach", "text": "Our solution is to create two forests with both SER and STRUT and then define MIX as a voting group whose underlying model is to unite all the trees in those forests. MIX is therefore a simple majority decision that is applied to all decision trees transferred from either SER or STRUT. As you can see below, the resulting MIX ensemble often exceeds both of its components. An intuitive explanation of its excellent performance can be found in Section 5."}, {"heading": "3.4 Numerical Examples and Intuition", "text": "To gain an intuition about the relative strengths and weaknesses of the SER and STRUT algorithms, as well as the MIX solution, we will consider a number of small synthetic transfer challenges, each representing a controlled transformation between the source and target domains. We will present two of these challenges. Appendix A. There are eight additional synthetic examples available, each synthetic example consisting of 1,000 independent attempts. \"Movable Source Transformation\" demonstrates a source concept that is shifted in the target domain, but retains its geometry between the domains. Constructively, we expect STRUT to stand out in this case. In the \"mixed boxes\" transformation, the source concept is composed of a 50-50 mix of slightly shifted boxes, and the target concept consists of one of these boxes. This problem models a case where the target concept is some kind of refinement of the source concept. SER can be expected to stand out in this problem, and we present the STUT concepts in the table 2."}, {"heading": "4 EMPIRICAL EVALUATION", "text": "We evaluated the SER, STRUT and MIX transfer learning algorithms over a number of challenges, first comparing our results with non-transferable learning techniques and trivial tree-based transfer learning baselines, and finally competing with other transfer algorithms. We used the SrcOnly baseline as our first benchmark. As it is a trivial approach to transfer learning that does not use target data, the model was trained only on the source data. Our second benchmark was the TgtOnly baseline, where we only build the target model using target data. Generally, any useful transfer learning method should outperform the SrcOnly baseline. Traditionally, the TgtOnly benchmark is considered to be the skyline representing the best possible performance. However, effective transfer learning methods can sometimes outperform the skyline, as we cleverly exploit both the source and target examples, giving us a larger training sample than the one we have based on the transfer model."}, {"heading": "4.1 Datasets", "text": "In fact, it is a very rare disease, which is only a very rare disease, \"he said in an interview with the Deutsche Presse-Agentur.\" It is very likely that it is a very rare disease, \"he told the Deutsche Presse-Agentur."}, {"heading": "4.2 Experiments and Results", "text": "In fact, it is the case that most of us are in a position to search for new paths that they wish to take. \"(S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S."}, {"heading": "4.3 Comparing to Instance Transfer Algorithms", "text": "In this sense, it is interesting and important to understand the advantages and limitations of model transmission methods, and that is why we have conducted a comparative study of our model transmission methods to transfer algorithms. In this section, we briefly mention our comparison of the MIX algorithm against the instance transmission algorithms. The first is TradaBoost, which is applied with random decision trees. Our tests show that the use of random decisions is much better than that of instance transmission algorithms."}, {"heading": "5 CAN WE EXPLAIN THE ADVANTAGE OF MIX?", "text": "Our empirical results indicate that the MIX algorithm performs well even if only one of its components Q = Q Q = Q delivers good results and can also exceed each of its components. We attribute this behavior to diversity and correlation among ensemble members, while a particular tree transformed by the SER algorithm is likely to be different in size from the original tree, since the expansion step will add the tree depth and the reduction step will reduce the size of some of the branches, while the same tree transformed by the STRUT algorithm is likely to maintain its original size, but with different thresholds. Thus, the paired correlation in the MIX forest between two trees transformed from the same original tree is likely to have a low correlation and the result in a more diverse forest."}, {"heading": "6 RELATED WORK", "text": "The generic title \"transfers\" encompasses a whole range of paradigms. As Levy and Markovitch point out, such paradigms are motivated by (implicit or explicit) modeling or process assumptions. For example, some paradigms, such as \"feature transfer,\" are motivated by assumptions about the link between source and target (for example, the properties of the target achieved by certain mappings)."}, {"heading": "6.1 Transfer Learning using Decision Trees", "text": "Most of the early transfer learning methods were based on neural networks, and while SVMs and ensemble techniques have become increasingly important in this area, DT models are still under-researched in this field. Of the few tree-based techniques studied, only the work of Won et al. works in our model transfer setting. In particular, Won et al. proposed a simple technique to update an existing tree that was trained only on source samples using target samples. [76] Their approach resembled an iterative learning method based solely on iterative expansion steps, which does not take into account refitting numerical thresholds. Adaptive DTs and stream sub-sampling were used in data streams to handle massive, fast streams [77], [78] as well as concept drift [80], [81], [82] and modifies the DT when new samples arrive. However, there are no explicit source and target distributions in this setting, but rather multiple changes in the setting over time, requiring multiple sets."}, {"heading": "7 CONCLUDING REMARKS", "text": "Taking advantage of the modularity and flexibility of decision trees, we designed two model transfer learning algorithms that use a model trained over the source domain and adapt it effectively to the target domain by making local adjustments to the tree parameters and / or its architecture. Our experiments with synthetic data suggest that the effectiveness of the algorithms varies according to the transformation type. Our final MIX algorithm combines the proposed base algorithms and often exceeds their underlying components. It also achieves a performance that is superior to leading model transfer algorithms and, moreover, competitive with instance transfer algorithms and even surpasses some of them. An attractive feature of the proposed method (and any effective model transfer algorithm) is that the source data can be discarded over the source domain after training, and the transfer of the models to the target domain can be calculated later, whenever it is necessary to develop a variety of sources from this beautiful application."}, {"heading": "APPENDIX A OTHER NUMERICAL EXAMPLES ON SYNTHETIC DATA", "text": "These two examples were selected from a series of 10 problems that we synthetically designed to capture simple transformations between the source and the target problems. Here, we define all 10 of these problems, present the test performance of the algorithms on these problems, and provide details of the experimental protocol we use. Each challenge is defined by a defined binary concept of the source and target source, and a transformation that maps the concept to the target domain. Concepts and transformations are defined below for each challenge. All challenges are defined so that X is the positive unit of the quadrant in R3 (using numerical characteristics). In all experiments we maintain the relationship that the SS concept maps and adopts to the target domain | ST | = 64. In all cases, P (x), the marginal distribution over X, is the uniform distribution over X. Each challenge was randomly repeated."}], "references": [{"title": "Real-time human pose recognition in parts from single depth images", "author": ["J. Shotton", "T. Sharp", "A. Kipman", "A. Fitzgibbon", "M. Finocchio"], "venue": "Communications of the ACM, vol. 56, no. 1, pp. 116\u2013124, 2013.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2013}, {"title": "Logistic regression with an auxiliary data source", "author": ["X. Liao", "Y. Xue", "L. Carin"], "venue": "Proceedings of the 22nd International Conference on Machine Learning. ACM, 2005, pp. 505\u2013512.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2005}, {"title": "Personalized handwriting recognition via biased regularization", "author": ["W. Kienzle", "K. Chellapilla"], "venue": "Proceedings of the 24th International Conference on Machine Learning (ICML). ACM, 2006, pp. 457\u2013464.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2006}, {"title": "Cross-domain video concept detection using adaptive svms", "author": ["J. Yang", "R. Yan", "A.G. Hauptmann"], "venue": "Proceedings of the 15th International Conference on Multimedia. ACM, 2007, pp. 188\u2013197.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2007}, {"title": "Domain adaptation from multiple sources via auxiliary classifiers", "author": ["L. Duan", "I.W. Tsang", "D. Xu", "T.S. Chua"], "venue": "Proceedings of the 26th Annual International Conference on Machine Learning, 2009.  LEARN ON SOURCE, REFINE ON TARGET: A MODEL TRANSFER LEARNING FRAMEWORK WITH RANDOM FORESTS  13", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2009}, {"title": "Boosting expert ensembles for rapid concept recall", "author": ["A. Rettinger", "M. Zinkevich", "M. Bowling"], "venue": "Proceedings of the 21st National Conference on Artificial Intelligence, vol. 21, no. 1. AAAI, 2006, p. 464.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2006}, {"title": "Transfer learning from multiple source domains via consensus regularization", "author": ["P. Luo", "F. Zhuang", "H. Xiong", "Y. Xiong", "Q. He"], "venue": "Proceedings of the 17th ACM Conference on Information and Knowledge Management. ACM, 2008, pp. 103\u2013112.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2008}, {"title": "Kernel-based inductive transfer", "author": ["U. R\u00fcckert", "S. Kramer"], "venue": "Machine Learning and Knowledge Discovery in Databases. Springer, 2008, pp. 220\u2013233.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2008}, {"title": "Mdl-based decision tree pruning", "author": ["M. Mehta", "J. Rissanen", "R. Agrawal"], "venue": "Proceedings of the 1st International Conference on Knowledge Discovery and Data Mining (KDD). AAAI, 1995, pp. 216\u2013221.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 1995}, {"title": "A survey on transfer learning", "author": ["S.J. Pan", "Q. Yang"], "venue": "IEEE Transactions on Knowledge and Data Engineering, vol. 22, no. 10, pp. 1345\u20131359, 2010.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2010}, {"title": "Random forests", "author": ["L. Breiman"], "venue": "Machine Learning, vol. 45, no. 1, pp. 5\u201332, 2001.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2001}, {"title": "Divergence measures based on the shannon entropy", "author": ["J. Lin"], "venue": "IEEE Transactions on Information Theory, vol. 37, no. 1, pp. 145\u2013151, 1991.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 1991}, {"title": "Programs for Machine Learning", "author": ["J.R. Quinlan"], "venue": null, "citeRegEx": "16", "shortCiteRegEx": "16", "year": 1993}, {"title": "Domain adaptation from multiple sources: A domain-dependent regularization approach", "author": ["L. Duan", "D. Xu", "I.W. Tsang"], "venue": "IEEE Transactions on Neural Networks and Learning Systems, vol. 23, no. 3, pp. 504\u2013518, 2012.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2012}, {"title": "Boosting for transfer learning", "author": ["W. Dai", "Q. Yang", "G.R. Xue", "Y. Yu"], "venue": "Proceedings of the 24th International Conference on Machine Learning, 2007.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2007}, {"title": "UCI machine learning repository", "author": ["K. Bache", "M. Lichman"], "venue": "2013. [Online]. Available: http://archive.ics.uci.edu/ml", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2013}, {"title": "Modeling wine preferences by data mining from physicochemical properties", "author": ["P. Cortez", "A. Cerdeira", "F. Almeida", "T. Matos", "J. Reis"], "venue": "Decision Support Systems, vol. 47, no. 4, pp. 547\u2013553, 2009.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2009}, {"title": "The MNIST database of handwritten digits", "author": ["Y. Lecun", "C. Cortes"], "venue": "1998. [Online]. Available: http://yann.lecun.com/exdb/ mnist/", "citeRegEx": "21", "shortCiteRegEx": null, "year": 1998}, {"title": "A database for handwritten text recognition research", "author": ["J.J. Hull"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 16, no. 5, pp. 550\u2013554, 1994.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 1994}, {"title": "Recognizing activities and spatial context using wearable sensors", "author": ["A. Subramanya", "A. Raj", "J.A. Bilmes", "D. Fox"], "venue": "Proceedings of the 22nd Annual Conference on Uncertainty in Artificial Intelligence (UAI). AUAI Press, 2006, pp. 494\u2013502.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2006}, {"title": "Learning from multiple outlooks", "author": ["M. Harel", "S. Mannor"], "venue": "Proceedings of the 28th International Conference on Machine Learning (ICML). ACM, 2011, pp. 401\u2013408.", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2011}, {"title": "Understanding variable importances in forests of randomized trees", "author": ["G. Louppe", "L. Wehenkel", "A. Sutera", "P. Geurts"], "venue": "Advances in Neural Information Processing Systems 26, C. Burges, L. Bottou, M. Welling, Z. Ghahramani, and K. Weinberger, Eds. Curran Associates, Inc., 2013, pp. 431\u2013439.", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2013}, {"title": "A review on ensembles for the class imbalance problem: bagging-, boosting-, and hybrid-based approaches", "author": ["M. Galar", "A. Fernandez", "E. Barrenechea", "H. Bustince", "F. Herrera"], "venue": "IEEE Transactions on Systems, Man, and Cybernetics, Part C: Applications and Reviews, vol. 42, no. 4, pp. 463\u2013484, 2012.", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2012}, {"title": "Trbagg: A simple transfer learning method and its application to personalization in collaborative tagging", "author": ["T. Kamishima", "M. Hamasaki", "S. Akaho"], "venue": "Proceedings of the 9th International Conference on Data Mining (ICDM). IEEE, 2009, pp. 219\u2013228.", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2009}, {"title": "Frustratingly easy domain adaptation", "author": ["III H. Daum\u00e9"], "venue": "Conference of the Association for Computational Linguistics (ACL), 2007.", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2007}, {"title": "Transfer learning decision forests for gesture recognition", "author": ["N.A. Goussies", "S. Ubalde", "M. Mejail"], "venue": "Journal of Machine Learning Research, vol. 15, pp. 3667\u20133690, 2014.", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2014}, {"title": "Pac-bayes & margins", "author": ["J. Shawe-Taylor", "J. Langford"], "venue": "Proceedings of the 17th Neural Information Processing Systems (NIPS). MIT Press, 2003, pp. 439\u2013446.", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2003}, {"title": "Simplified pac-bayesian margin bounds", "author": ["D. McAllester"], "venue": "Learning Theory and Kernel Machines. Springer, 2003, pp. 203\u2013215.", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2003}, {"title": "Pacbayesian learning of linear classifiers", "author": ["P. Germain", "A. Lacasse", "F. Laviolette", "M. Marchand"], "venue": "Proceedings of the 26th Annual International Conference on Machine Learning. ACM, 2009, pp. 353\u2013360.", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2009}, {"title": "Risk bounds for the majority vote: From a pac-bayesian analysis to a learning algorithm", "author": ["P. Germain", "A. Lacasse", "F. Laviolette", "M. Marchand", "J.-F. Roy"], "venue": "Journal of Machine Learning Research, vol. 16, pp. 787\u2013860, 2015.", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2015}, {"title": "Boosting the margin: A new explanation for the effectiveness of voting methods", "author": ["R.E. Schapire", "Y. Freund", "P. Bartlett", "W.S. Lee"], "venue": "The Annals of Statistics, vol. 26, no. 5, pp. 1651\u20131686, 1998.", "citeRegEx": "34", "shortCiteRegEx": null, "year": 1998}, {"title": "Teaching machines to learn by metaphors", "author": ["O. Levy", "S. Markovitch"], "venue": "Proceedings of the 26th Conference on Artificial Intelligence. AAAI, 2012, pp. 991\u2013997.", "citeRegEx": "35", "shortCiteRegEx": null, "year": 2012}, {"title": "Learning with few examples using a constrained gaussian prior on randomized trees", "author": ["E. Rodner", "J. Denzler"], "venue": "Proceedings of the Vision, Modeling, and Visualization Conference (VMV). Pro Universitate, 2008, pp. 159\u2013168.", "citeRegEx": "36", "shortCiteRegEx": null, "year": 2008}, {"title": "Safety in numbers: Learning categories from few examples with multi model knowledge transfer", "author": ["T. Tommasi", "F. Orabona", "B. Caputo"], "venue": "Proceedings of the 23rd Conference on Computer Vision and Pattern Recognition (CVPR). IEEE, 2010, pp. 3081\u20133088.", "citeRegEx": "37", "shortCiteRegEx": null, "year": 2010}, {"title": "Learning with few examples for binary and multiclass classification using regularization of randomized trees", "author": ["E. Rodner", "J. Denzler"], "venue": "Pattern Recognition Letters, vol. 32, no. 2, pp. 244\u2013251, 2011.", "citeRegEx": "38", "shortCiteRegEx": null, "year": 2011}, {"title": "A model of inductive bias learning", "author": ["J. Baxter"], "venue": "J. Artif. Intell. Res.(JAIR), vol. 12, pp. 149\u2013198, 2000.", "citeRegEx": "39", "shortCiteRegEx": null, "year": 2000}, {"title": "Direct transfer of learned information among neural networks", "author": ["L.Y. Pratt", "J. Mostow", "C.A. Kamm", "A.A. Kamm"], "venue": "Proceedings of the 9th National Conference on Artificial Intelligence. AAAI, 1991, pp. 584\u2013589.", "citeRegEx": "40", "shortCiteRegEx": null, "year": 1991}, {"title": "Learning one more thing", "author": ["S. Thrun", "T.M. Mitchell"], "venue": "DTIC Document, Tech. Rep., 1994.", "citeRegEx": "41", "shortCiteRegEx": null, "year": 1994}, {"title": "Modeling transfer relationships between learning tasks for improved inductive transfer", "author": ["E. Eaton", "M. desJardins", "T. Lane"], "venue": "Machine Learning and Knowledge Discovery in Databases, pp. 317\u2013 332, 2008.", "citeRegEx": "42", "shortCiteRegEx": null, "year": 2008}, {"title": "Learning with few examples by transferring feature relevance", "author": ["E. Rodner", "J. Denzler"], "venue": "Pattern Recognition. Springer, 2009, pp. 252\u2013261.", "citeRegEx": "44", "shortCiteRegEx": null, "year": 2009}, {"title": "Boosting for regression transfer", "author": ["D. Pardoe", "P. Stone"], "venue": "Proceedings of the 27th International Conference on Machine learning (ICML). ACM, 2010, pp. 863\u2013870.", "citeRegEx": "45", "shortCiteRegEx": null, "year": 2010}, {"title": "Boosting for transfer learning with multiple sources", "author": ["Y. Yao", "G. Doretto"], "venue": "Proceedings of the 23rd Conference on Computer Vision and Pattern Recognition (CVPR). IEEE, 2010, pp. 1855\u20131862.", "citeRegEx": "46", "shortCiteRegEx": null, "year": 2010}, {"title": "Selective transfer learning for cross domain recommendation", "author": ["Z. Lu", "W. Pan", "E.W. Xiang", "Q. Yang", "L. Zhao", "E. Zhong"], "venue": "Proceedings of the 2013 SIAM International Conference on Data Mining. SDM, 2013, pp. 641\u2013649.", "citeRegEx": "47", "shortCiteRegEx": null, "year": 2013}, {"title": "Instance weighting for domain adaptation in NLP", "author": ["J. Jiang", "C. Zhai"], "venue": "ACL, vol. 7, 2007, pp. 264\u2013271.", "citeRegEx": "48", "shortCiteRegEx": null, "year": 2007}, {"title": "Improving SVM accuracy by training on auxiliary data sources", "author": ["P. Wu", "T.G. Dietterich"], "venue": "Proceedings of the 21st International Conference on Machine Learning (ICML). ACM, 2004, pp. 110\u2013117.", "citeRegEx": "49", "shortCiteRegEx": null, "year": 2004}, {"title": "Adapting visual category models to new domains", "author": ["K. Saenko", "B. Kulis", "M. Fritz", "T. Darrell"], "venue": "Proceedings of the European Conference on Computer Vision (ECCV). Springer, 2010, pp. 213\u2013 226.", "citeRegEx": "50", "shortCiteRegEx": null, "year": 2010}, {"title": "Self-taught learning: transfer learning from unlabeled data", "author": ["R. Raina", "A. Battle", "H. Lee", "B. Packer", "A.Y. Ng"], "venue": "Proceedings of the 24th International Conference on Machine Learning (ICML). ACM, 2007, pp. 759\u2013766.", "citeRegEx": "51", "shortCiteRegEx": null, "year": 2007}, {"title": "Multi-task feature learning", "author": ["A. Evgeniou", "M. Pontil"], "venue": "Advances in Neural Information Processing Systems, vol. 19, p. 41, 2007.", "citeRegEx": "52", "shortCiteRegEx": null, "year": 2007}, {"title": "Transfer learning via dimensionality reduction", "author": ["S.J. Pan", "J.T. Kwok", "Q. Yang"], "venue": "Proceedings of the 23rd Conference on Artificial Intelligence. AAAI, 2008, pp. 677\u2013682.", "citeRegEx": "53", "shortCiteRegEx": null, "year": 2008}, {"title": "Domain adaptation for statistical classifiers.", "author": ["H. Daum\u00e9 III", "D. Marcu"], "venue": "J. Artif. Intell. Res.(JAIR),", "citeRegEx": "54", "shortCiteRegEx": "54", "year": 2006}, {"title": "A theory of learning from different domains", "author": ["S. Ben-David", "J. Blitzer", "K. Crammer", "A. Kulesza", "F. Pereira", "J.W. Vaughan"], "venue": "Machine Learning, vol. 79, no. 1-2.", "citeRegEx": "55", "shortCiteRegEx": null, "year": 0}, {"title": "A literature survey on domain adaptation of statistical classifiers", "author": ["J. Jiang"], "venue": "URL: http://sifaka.cs.uiuc.edu/jiang4/domainadaptation/survey, 2008.", "citeRegEx": "56", "shortCiteRegEx": null, "year": 2008}, {"title": "Regularized multi\u2013task learning", "author": ["T. Evgeniou", "M. Pontil"], "venue": "Proceedings of the 10th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, 2004.", "citeRegEx": "58", "shortCiteRegEx": null, "year": 2004}, {"title": "Multi-domain learning by confidence-weighted parameter combination", "author": ["M. Dredze", "A. Kulesza", "K. Crammer"], "venue": "Machine Learning, vol. 79, no. 1-2, pp. 123\u2013149, 2010.", "citeRegEx": "59", "shortCiteRegEx": null, "year": 2010}, {"title": "Multiclass transfer learning from unconstrained priors", "author": ["L. Jie", "T. Tommasi", "B. Caputo"], "venue": "International Conference on Computer Vision (ICCV). IEEE, 2011, pp. 1863\u20131870.", "citeRegEx": "60", "shortCiteRegEx": null, "year": 2011}, {"title": "Domain adaptation with structural correspondence learning", "author": ["J. Blitzer", "R. McDonald", "F. Pereira"], "venue": "Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing, pp. 120\u2013128.", "citeRegEx": "61", "shortCiteRegEx": null, "year": 2006}, {"title": "Cross-domain learning methods for high-level visual concept classification", "author": ["W. Jiang", "E. Zavesky", "S.-F. Chang", "A. Loui"], "venue": "Proceedings of the 15th IEEE International Conference on Image Processing (ICIP), 2008.", "citeRegEx": "62", "shortCiteRegEx": null, "year": 2008}, {"title": "Exploiting web images for event recognition in consumer videos: A multiple source domain adaptation approach", "author": ["L. Duan", "D. Xu", "S.-F. Chang"], "venue": "Computer Vision and Pattern Recognition (CVPR). IEEE, 2012, pp. 1338\u20131345.", "citeRegEx": "63", "shortCiteRegEx": null, "year": 2012}, {"title": "Unsupervised visual domain adaptation using subspace alignment", "author": ["B. Fernando", "A. Habrard", "M. Sebban", "T. Tuytelaars"], "venue": "International Conference on Computer Vision (ICCV). IEEE, 2013, pp. 2960\u20132967.", "citeRegEx": "64", "shortCiteRegEx": null, "year": 2013}, {"title": "Convex multi-task feature learning", "author": ["A. Argyriou", "T. Evgeniou", "M. Pontil"], "venue": "Machine Learning, vol. 73, no. 3, pp. 243\u2013272, 2008.", "citeRegEx": "65", "shortCiteRegEx": null, "year": 2008}, {"title": "A framework for learning predictive structures from multiple tasks and unlabeled data", "author": ["R.K. Ando", "T. Zhang"], "venue": "The Journal of Machine Learning Research, vol. 6, pp. 1817\u20131853, 2005.", "citeRegEx": "66", "shortCiteRegEx": null, "year": 1817}, {"title": "Taking advantage of sparsity in multi-task learning", "author": ["K. Lounici", "M. Pontil", "A. Tsybakov", "S. Geer"], "venue": "Proceedings of the 22nd Annual Conference on Learning Theory (COLT). ACL, 2009.", "citeRegEx": "67", "shortCiteRegEx": null, "year": 2009}, {"title": "Unsupervised domain adaptation by domain invariant projection", "author": ["M. Baktashmotlagh", "M.T. Harandi", "B.C. Lovell", "M. Salzmann"], "venue": "International Conference on Computer Vision (ICCV). IEEE, 2013, pp. 769\u2013776.", "citeRegEx": "68", "shortCiteRegEx": null, "year": 2013}, {"title": "Learning to learn with the informative vector machine", "author": ["N.D. Lawrence", "J.C. Platt"], "venue": "Proceedings of the 21st International Conference on Machine Learning (ICML). ACM, 2004, p. 65.", "citeRegEx": "69", "shortCiteRegEx": null, "year": 2004}, {"title": "Hierarchical bayesian modelling with gaussian processes", "author": ["A. Schwaighofer", "V. Tresp", "K. Yu"], "venue": "Advances in Neural Information Processing Systems 17, 2005.", "citeRegEx": "70", "shortCiteRegEx": null, "year": 2005}, {"title": "Learning gaussian processes from multiple tasks", "author": ["K. Yu", "V. Tresp", "A. Schwaighofer"], "venue": "Proceedings of the 22nd International Conference on Machine Learning (ICML). ACM, 2005, pp. 1012\u2013 1019.", "citeRegEx": "71", "shortCiteRegEx": null, "year": 2005}, {"title": "Multi-task Gaussian process prediction.", "author": ["E.V. Bonilla", "K.M. Chai", "C.K. Williams"], "venue": "in NIPS,", "citeRegEx": "72", "shortCiteRegEx": "72", "year": 2007}, {"title": "Domain transfer svm for video concept detection", "author": ["L. Duan", "I.W. Tsang", "D. Xu", "S.J. Maybank"], "venue": "Computer Vision and Pattern Recognition (CVPR). IEEE, 2009, pp. 1375\u20131381.", "citeRegEx": "73", "shortCiteRegEx": null, "year": 2009}, {"title": "Domain transfer multiple kernel learning", "author": ["L. Duan", "I.W. Tsang", "D. Xu"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 34, no. 3, pp. 465\u2013479, 2012.", "citeRegEx": "74", "shortCiteRegEx": null, "year": 2012}, {"title": "Visual event recognition in videos by learning from web data", "author": ["L. Duan", "D. Xu", "I.-H. Tsang", "J. Luo"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 34, no. 9, pp. 1667\u20131680, 2012.", "citeRegEx": "75", "shortCiteRegEx": null, "year": 2012}, {"title": "Transfer learning in decision trees", "author": ["J. won Lee", "C. Giraud-Carrier"], "venue": "Proceedings of the International Joint Conference on Neural Networks (IJCNN). IEEE, 2007, pp. 726\u2013731.", "citeRegEx": "76", "shortCiteRegEx": null, "year": 2007}, {"title": "Mining high-speed data streams", "author": ["P. Domingos", "G. Hulten"], "venue": "Proceedings of the 6th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, 2000, pp. 71\u201380.", "citeRegEx": "77", "shortCiteRegEx": null, "year": 2000}, {"title": "Efficient decision tree construction on streaming data", "author": ["R. Jin", "G. Agrawal"], "venue": "Proceedings of the 9th International Conference on Knowledge Discovery and Data Mining (SIGKDD). ACM, 2003, pp. 571\u2013576.", "citeRegEx": "78", "shortCiteRegEx": null, "year": 2003}, {"title": "Mining massive data streams", "author": ["G. Hulten", "P. Domingos", "L. Spencer"], "venue": "The Journal of Machine Learning Research, 2005.", "citeRegEx": "79", "shortCiteRegEx": null, "year": 2005}, {"title": "Mining time-changing data streams", "author": ["G. Hulten", "L. Spencer", "P. Domingos"], "venue": "Proceedings of the 7th International Conference on Knowledge Discovery and Data Mining (SIGKDD). ACM, 2001, pp. 97\u2013106.", "citeRegEx": "80", "shortCiteRegEx": null, "year": 2001}, {"title": "Decision trees for mining data streams", "author": ["J. Gama", "R. Fernandes", "R. Rocha"], "venue": "Intelligent Data Analysis, vol. 10, no. 1, pp. 23\u201345, 2006.", "citeRegEx": "81", "shortCiteRegEx": null, "year": 2006}, {"title": "Learning in environments with unknown dynamics: Towards more robust concept learners", "author": ["M. N\u00fa\u00f1ez", "R. Fidalgo", "R. Morales"], "venue": "The Journal of Machine Learning Research, vol. 8, pp. 2595\u20132628, 2007.", "citeRegEx": "82", "shortCiteRegEx": null, "year": 2007}, {"title": "Boosting multi-task weak learners with applications to textual and social data", "author": ["J.B. Faddoul", "B. Chidlovskii", "F. Torre", "R. Gilleron"], "venue": "Proceedings of the 9th International Conference on Machine Learning and Applications (ICMLA). IEEE, 2010, pp. 367\u2013372.", "citeRegEx": "83", "shortCiteRegEx": null, "year": 2010}, {"title": "Learning multiple tasks with boosted decision trees", "author": ["J.B. Faddoul", "B. Chidlovskii", "R. Gilleron", "F. Torre"], "venue": "Machine Learning and Knowledge Discovery in Databases. Springer, 2012, pp. 681\u2013696.", "citeRegEx": "84", "shortCiteRegEx": null, "year": 2012}], "referenceMentions": [{"referenceID": 0, "context": "For example, Microsoft\u2019s Kinect performs human pose recognition using random forests [1], which can be improved with user-specific training data to accommodate environmental changes (e.", "startOffset": 85, "endOffset": 88}, {"referenceID": 1, "context": ", by using a biased regularizer [2], [3], [4], [5], or aggregating multiple source-target predictors [6], [7], [8].", "startOffset": 32, "endOffset": 35}, {"referenceID": 2, "context": ", by using a biased regularizer [2], [3], [4], [5], or aggregating multiple source-target predictors [6], [7], [8].", "startOffset": 37, "endOffset": 40}, {"referenceID": 3, "context": ", by using a biased regularizer [2], [3], [4], [5], or aggregating multiple source-target predictors [6], [7], [8].", "startOffset": 42, "endOffset": 45}, {"referenceID": 4, "context": ", by using a biased regularizer [2], [3], [4], [5], or aggregating multiple source-target predictors [6], [7], [8].", "startOffset": 47, "endOffset": 50}, {"referenceID": 5, "context": ", by using a biased regularizer [2], [3], [4], [5], or aggregating multiple source-target predictors [6], [7], [8].", "startOffset": 101, "endOffset": 104}, {"referenceID": 6, "context": ", by using a biased regularizer [2], [3], [4], [5], or aggregating multiple source-target predictors [6], [7], [8].", "startOffset": 106, "endOffset": 109}, {"referenceID": 7, "context": ", by using a biased regularizer [2], [3], [4], [5], or aggregating multiple source-target predictors [6], [7], [8].", "startOffset": 111, "endOffset": 114}, {"referenceID": 8, "context": "The two common techniques to regularize DTs are pruning and voting ensembles [9], [10].", "startOffset": 77, "endOffset": 80}, {"referenceID": 9, "context": "This setting is quite common, both in research literature and in real world applications of transfer learning, but is only one of a few existing approaches [11].", "startOffset": 156, "endOffset": 160}, {"referenceID": 10, "context": "Our algorithms are based on standard Random Forests (RFs) [12].", "startOffset": 58, "endOffset": 62}, {"referenceID": 11, "context": "DG relies on the (symmetric) Jensen-Shannon divergence given in Equation (2), where DKL(\u00b7) is the familiar Kullback-Leibler divergence and M is the mean distribution, M = 1 2 (P +Q) [14].", "startOffset": 182, "endOffset": 186}, {"referenceID": 0, "context": "Define two simple domains where the feature space, X , is the range [0, 1], and our label space is Y = {\u00b11}.", "startOffset": 68, "endOffset": 74}, {"referenceID": 0, "context": "We keep the same feature space as in the previous example (the range [0, 1]) as well as the same label space (Y = {\u00b11}).", "startOffset": 69, "endOffset": 75}, {"referenceID": 12, "context": "5 algorithm [16].", "startOffset": 12, "endOffset": 16}, {"referenceID": 3, "context": "The first is Adaptive SVM (ASVM) [4], which uses target examples to regularize an SVM model with a Gaussian kernel, trained using source examples only.", "startOffset": 33, "endOffset": 36}, {"referenceID": 4, "context": "While ASVM has several extensions, these usually rely on a large set of unlabeled target training data, without which these techniques are similar to ASVM [5], [17].", "startOffset": 155, "endOffset": 158}, {"referenceID": 13, "context": "While ASVM has several extensions, these usually rely on a large set of unlabeled target training data, without which these techniques are similar to ASVM [5], [17].", "startOffset": 160, "endOffset": 164}, {"referenceID": 6, "context": "The second algorithm is consensus regularization [7], which attempts to decrease the classification error by minimizing an entropy based disagreement measure among a set of source-only and target-only models.", "startOffset": 49, "endOffset": 52}, {"referenceID": 14, "context": "These approaches are common practice in dataset construction for validating transfer models [18].", "startOffset": 92, "endOffset": 96}, {"referenceID": 15, "context": "Mushroom: This is a publicly available dataset from the UCI Repository [19].", "startOffset": 71, "endOffset": 75}, {"referenceID": 14, "context": "[18], and the resulting partition is such that the source mushrooms belong to different species than the target mushrooms.", "startOffset": 0, "endOffset": 4}, {"referenceID": 16, "context": "Wine: Publicly available wine quality dataset [20], already partitioned into two; the source domain consists of white wines and the target domain consists of red wines.", "startOffset": 46, "endOffset": 50}, {"referenceID": 17, "context": "Inversion: The task concerns hand-written digit recognition from the MNIST digit database [21].", "startOffset": 90, "endOffset": 94}, {"referenceID": 18, "context": "USPS: Another example of hand-written digit recognition collected under different conditions [22].", "startOffset": 93, "endOffset": 97}, {"referenceID": 19, "context": "[23], is a recording of a customized wearable sensor system.", "startOffset": 0, "endOffset": 4}, {"referenceID": 20, "context": "We performed the same preprocessing on the data as that performed by Harel and Mannor [24] and treated each ordered pair of users as a source-target pair, totaling 30 possible pairs.", "startOffset": 86, "endOffset": 90}, {"referenceID": 21, "context": "[25]).", "startOffset": 0, "endOffset": 4}, {"referenceID": 22, "context": "Moreover, with the severe class imbalance exhibited, error (or accuracy) is no longer an appropriate measurement and can lead to erroneous conclusions [26].", "startOffset": 151, "endOffset": 155}, {"referenceID": 14, "context": "The first is TradaBoost [18], which is applied with random decision trees as the weak learners.", "startOffset": 24, "endOffset": 28}, {"referenceID": 23, "context": "The second algorithm tested was TrBagg [27], which initially trains classifiers on bootstrapped bags sampled with replacements from T \u222a T and regularizes the ensemble by filtering out classifiers which are overly biased towards the target domain.", "startOffset": 39, "endOffset": 43}, {"referenceID": 24, "context": "The third algorithm we compared against is the Frustratingly Easy Domain Adaptation (FEDA) [28] meta-algorithm.", "startOffset": 91, "endOffset": 95}, {"referenceID": 25, "context": "Finally, we test the Mixed-Entropy (ME) [29] algorithm, a state-of-the-art forest-specific technique which combines source and target training samples using a weighted information gain measure.", "startOffset": 40, "endOffset": 44}, {"referenceID": 26, "context": ", [30], [31], [32]), Germain et al.", "startOffset": 2, "endOffset": 6}, {"referenceID": 27, "context": ", [30], [31], [32]), Germain et al.", "startOffset": 8, "endOffset": 12}, {"referenceID": 28, "context": ", [30], [31], [32]), Germain et al.", "startOffset": 14, "endOffset": 18}, {"referenceID": 29, "context": "1 (Corollary 16 [33]).", "startOffset": 16, "endOffset": 20}, {"referenceID": 29, "context": "is a measure of expected joint error [33].", "startOffset": 37, "endOffset": 41}, {"referenceID": 30, "context": "2 (Theorem 1 [34]).", "startOffset": 13, "endOffset": 17}, {"referenceID": 31, "context": "As noted by Levy and Markovitch [35], such paradigms are motivated by (implicit or explicit) modeling or process assumptions.", "startOffset": 32, "endOffset": 36}, {"referenceID": 9, "context": "[11] identifies the following settings, which are not mutually exclusive.", "startOffset": 0, "endOffset": 4}, {"referenceID": 2, "context": "Present model transfer model methods rely on a biased regularizer [3], [4], [36], [37], [38], on aggregating multiple source-target predictors [6], [7], [8], [39], utilizing model parameter transfer as priors [40], [41], [42], or by feature weight estimation [43], [44].", "startOffset": 66, "endOffset": 69}, {"referenceID": 3, "context": "Present model transfer model methods rely on a biased regularizer [3], [4], [36], [37], [38], on aggregating multiple source-target predictors [6], [7], [8], [39], utilizing model parameter transfer as priors [40], [41], [42], or by feature weight estimation [43], [44].", "startOffset": 71, "endOffset": 74}, {"referenceID": 32, "context": "Present model transfer model methods rely on a biased regularizer [3], [4], [36], [37], [38], on aggregating multiple source-target predictors [6], [7], [8], [39], utilizing model parameter transfer as priors [40], [41], [42], or by feature weight estimation [43], [44].", "startOffset": 76, "endOffset": 80}, {"referenceID": 33, "context": "Present model transfer model methods rely on a biased regularizer [3], [4], [36], [37], [38], on aggregating multiple source-target predictors [6], [7], [8], [39], utilizing model parameter transfer as priors [40], [41], [42], or by feature weight estimation [43], [44].", "startOffset": 82, "endOffset": 86}, {"referenceID": 34, "context": "Present model transfer model methods rely on a biased regularizer [3], [4], [36], [37], [38], on aggregating multiple source-target predictors [6], [7], [8], [39], utilizing model parameter transfer as priors [40], [41], [42], or by feature weight estimation [43], [44].", "startOffset": 88, "endOffset": 92}, {"referenceID": 5, "context": "Present model transfer model methods rely on a biased regularizer [3], [4], [36], [37], [38], on aggregating multiple source-target predictors [6], [7], [8], [39], utilizing model parameter transfer as priors [40], [41], [42], or by feature weight estimation [43], [44].", "startOffset": 143, "endOffset": 146}, {"referenceID": 6, "context": "Present model transfer model methods rely on a biased regularizer [3], [4], [36], [37], [38], on aggregating multiple source-target predictors [6], [7], [8], [39], utilizing model parameter transfer as priors [40], [41], [42], or by feature weight estimation [43], [44].", "startOffset": 148, "endOffset": 151}, {"referenceID": 7, "context": "Present model transfer model methods rely on a biased regularizer [3], [4], [36], [37], [38], on aggregating multiple source-target predictors [6], [7], [8], [39], utilizing model parameter transfer as priors [40], [41], [42], or by feature weight estimation [43], [44].", "startOffset": 153, "endOffset": 156}, {"referenceID": 35, "context": "Present model transfer model methods rely on a biased regularizer [3], [4], [36], [37], [38], on aggregating multiple source-target predictors [6], [7], [8], [39], utilizing model parameter transfer as priors [40], [41], [42], or by feature weight estimation [43], [44].", "startOffset": 158, "endOffset": 162}, {"referenceID": 36, "context": "Present model transfer model methods rely on a biased regularizer [3], [4], [36], [37], [38], on aggregating multiple source-target predictors [6], [7], [8], [39], utilizing model parameter transfer as priors [40], [41], [42], or by feature weight estimation [43], [44].", "startOffset": 209, "endOffset": 213}, {"referenceID": 37, "context": "Present model transfer model methods rely on a biased regularizer [3], [4], [36], [37], [38], on aggregating multiple source-target predictors [6], [7], [8], [39], utilizing model parameter transfer as priors [40], [41], [42], or by feature weight estimation [43], [44].", "startOffset": 215, "endOffset": 219}, {"referenceID": 38, "context": "Present model transfer model methods rely on a biased regularizer [3], [4], [36], [37], [38], on aggregating multiple source-target predictors [6], [7], [8], [39], utilizing model parameter transfer as priors [40], [41], [42], or by feature weight estimation [43], [44].", "startOffset": 221, "endOffset": 225}, {"referenceID": 39, "context": "Present model transfer model methods rely on a biased regularizer [3], [4], [36], [37], [38], on aggregating multiple source-target predictors [6], [7], [8], [39], utilizing model parameter transfer as priors [40], [41], [42], or by feature weight estimation [43], [44].", "startOffset": 265, "endOffset": 269}, {"referenceID": 14, "context": "Boosting based instance weighing is common practice in this category [18], [45], [46], [47], as is instance elimination (and sub-sampling) [27], [48], but other techniques exist for utilizing the source information in different ways [28], [29], [49], [50].", "startOffset": 69, "endOffset": 73}, {"referenceID": 40, "context": "Boosting based instance weighing is common practice in this category [18], [45], [46], [47], as is instance elimination (and sub-sampling) [27], [48], but other techniques exist for utilizing the source information in different ways [28], [29], [49], [50].", "startOffset": 75, "endOffset": 79}, {"referenceID": 41, "context": "Boosting based instance weighing is common practice in this category [18], [45], [46], [47], as is instance elimination (and sub-sampling) [27], [48], but other techniques exist for utilizing the source information in different ways [28], [29], [49], [50].", "startOffset": 81, "endOffset": 85}, {"referenceID": 42, "context": "Boosting based instance weighing is common practice in this category [18], [45], [46], [47], as is instance elimination (and sub-sampling) [27], [48], but other techniques exist for utilizing the source information in different ways [28], [29], [49], [50].", "startOffset": 87, "endOffset": 91}, {"referenceID": 23, "context": "Boosting based instance weighing is common practice in this category [18], [45], [46], [47], as is instance elimination (and sub-sampling) [27], [48], but other techniques exist for utilizing the source information in different ways [28], [29], [49], [50].", "startOffset": 139, "endOffset": 143}, {"referenceID": 43, "context": "Boosting based instance weighing is common practice in this category [18], [45], [46], [47], as is instance elimination (and sub-sampling) [27], [48], but other techniques exist for utilizing the source information in different ways [28], [29], [49], [50].", "startOffset": 145, "endOffset": 149}, {"referenceID": 24, "context": "Boosting based instance weighing is common practice in this category [18], [45], [46], [47], as is instance elimination (and sub-sampling) [27], [48], but other techniques exist for utilizing the source information in different ways [28], [29], [49], [50].", "startOffset": 233, "endOffset": 237}, {"referenceID": 25, "context": "Boosting based instance weighing is common practice in this category [18], [45], [46], [47], as is instance elimination (and sub-sampling) [27], [48], but other techniques exist for utilizing the source information in different ways [28], [29], [49], [50].", "startOffset": 239, "endOffset": 243}, {"referenceID": 44, "context": "Boosting based instance weighing is common practice in this category [18], [45], [46], [47], as is instance elimination (and sub-sampling) [27], [48], but other techniques exist for utilizing the source information in different ways [28], [29], [49], [50].", "startOffset": 245, "endOffset": 249}, {"referenceID": 45, "context": "Boosting based instance weighing is common practice in this category [18], [45], [46], [47], as is instance elimination (and sub-sampling) [27], [48], but other techniques exist for utilizing the source information in different ways [28], [29], [49], [50].", "startOffset": 251, "endOffset": 255}, {"referenceID": 20, "context": "Standard techniques to address this problem include norm optimization [24], [51], [52] and manipulating and combining features [35], [42], [53]", "startOffset": 70, "endOffset": 74}, {"referenceID": 46, "context": "Standard techniques to address this problem include norm optimization [24], [51], [52] and manipulating and combining features [35], [42], [53]", "startOffset": 76, "endOffset": 80}, {"referenceID": 47, "context": "Standard techniques to address this problem include norm optimization [24], [51], [52] and manipulating and combining features [35], [42], [53]", "startOffset": 82, "endOffset": 86}, {"referenceID": 31, "context": "Standard techniques to address this problem include norm optimization [24], [51], [52] and manipulating and combining features [35], [42], [53]", "startOffset": 127, "endOffset": 131}, {"referenceID": 38, "context": "Standard techniques to address this problem include norm optimization [24], [51], [52] and manipulating and combining features [35], [42], [53]", "startOffset": 133, "endOffset": 137}, {"referenceID": 48, "context": "Standard techniques to address this problem include norm optimization [24], [51], [52] and manipulating and combining features [35], [42], [53]", "startOffset": 139, "endOffset": 143}, {"referenceID": 49, "context": "Domain Adaptation (DA) and Multi-Task Learning (MTL): In domain adaptation the difference between the domains is the result of different feature and labeling spaces; however, DA is typically considered within a semisupervised context where an abundance of unlabeled data is available as well [54], [55], [56], while in MTL the goal is to produce a good hypothesis for several related learning problems simultaneously [57].", "startOffset": 292, "endOffset": 296}, {"referenceID": 50, "context": "Domain Adaptation (DA) and Multi-Task Learning (MTL): In domain adaptation the difference between the domains is the result of different feature and labeling spaces; however, DA is typically considered within a semisupervised context where an abundance of unlabeled data is available as well [54], [55], [56], while in MTL the goal is to produce a good hypothesis for several related learning problems simultaneously [57].", "startOffset": 298, "endOffset": 302}, {"referenceID": 51, "context": "Domain Adaptation (DA) and Multi-Task Learning (MTL): In domain adaptation the difference between the domains is the result of different feature and labeling spaces; however, DA is typically considered within a semisupervised context where an abundance of unlabeled data is available as well [54], [55], [56], while in MTL the goal is to produce a good hypothesis for several related learning problems simultaneously [57].", "startOffset": 304, "endOffset": 308}, {"referenceID": 24, "context": "Some notable approaches for these settings are based on similarity to a common predictor [28], [58], [59], [60], finding a shared representation [50], [61], [62], [63], [64] or a shared subspace [65], [66], [67], [68], as well as probabilistic approaches [69], [70], [71], [72].", "startOffset": 89, "endOffset": 93}, {"referenceID": 52, "context": "Some notable approaches for these settings are based on similarity to a common predictor [28], [58], [59], [60], finding a shared representation [50], [61], [62], [63], [64] or a shared subspace [65], [66], [67], [68], as well as probabilistic approaches [69], [70], [71], [72].", "startOffset": 95, "endOffset": 99}, {"referenceID": 53, "context": "Some notable approaches for these settings are based on similarity to a common predictor [28], [58], [59], [60], finding a shared representation [50], [61], [62], [63], [64] or a shared subspace [65], [66], [67], [68], as well as probabilistic approaches [69], [70], [71], [72].", "startOffset": 101, "endOffset": 105}, {"referenceID": 54, "context": "Some notable approaches for these settings are based on similarity to a common predictor [28], [58], [59], [60], finding a shared representation [50], [61], [62], [63], [64] or a shared subspace [65], [66], [67], [68], as well as probabilistic approaches [69], [70], [71], [72].", "startOffset": 107, "endOffset": 111}, {"referenceID": 45, "context": "Some notable approaches for these settings are based on similarity to a common predictor [28], [58], [59], [60], finding a shared representation [50], [61], [62], [63], [64] or a shared subspace [65], [66], [67], [68], as well as probabilistic approaches [69], [70], [71], [72].", "startOffset": 145, "endOffset": 149}, {"referenceID": 55, "context": "Some notable approaches for these settings are based on similarity to a common predictor [28], [58], [59], [60], finding a shared representation [50], [61], [62], [63], [64] or a shared subspace [65], [66], [67], [68], as well as probabilistic approaches [69], [70], [71], [72].", "startOffset": 151, "endOffset": 155}, {"referenceID": 56, "context": "Some notable approaches for these settings are based on similarity to a common predictor [28], [58], [59], [60], finding a shared representation [50], [61], [62], [63], [64] or a shared subspace [65], [66], [67], [68], as well as probabilistic approaches [69], [70], [71], [72].", "startOffset": 157, "endOffset": 161}, {"referenceID": 57, "context": "Some notable approaches for these settings are based on similarity to a common predictor [28], [58], [59], [60], finding a shared representation [50], [61], [62], [63], [64] or a shared subspace [65], [66], [67], [68], as well as probabilistic approaches [69], [70], [71], [72].", "startOffset": 163, "endOffset": 167}, {"referenceID": 58, "context": "Some notable approaches for these settings are based on similarity to a common predictor [28], [58], [59], [60], finding a shared representation [50], [61], [62], [63], [64] or a shared subspace [65], [66], [67], [68], as well as probabilistic approaches [69], [70], [71], [72].", "startOffset": 169, "endOffset": 173}, {"referenceID": 59, "context": "Some notable approaches for these settings are based on similarity to a common predictor [28], [58], [59], [60], finding a shared representation [50], [61], [62], [63], [64] or a shared subspace [65], [66], [67], [68], as well as probabilistic approaches [69], [70], [71], [72].", "startOffset": 195, "endOffset": 199}, {"referenceID": 60, "context": "Some notable approaches for these settings are based on similarity to a common predictor [28], [58], [59], [60], finding a shared representation [50], [61], [62], [63], [64] or a shared subspace [65], [66], [67], [68], as well as probabilistic approaches [69], [70], [71], [72].", "startOffset": 201, "endOffset": 205}, {"referenceID": 61, "context": "Some notable approaches for these settings are based on similarity to a common predictor [28], [58], [59], [60], finding a shared representation [50], [61], [62], [63], [64] or a shared subspace [65], [66], [67], [68], as well as probabilistic approaches [69], [70], [71], [72].", "startOffset": 207, "endOffset": 211}, {"referenceID": 62, "context": "Some notable approaches for these settings are based on similarity to a common predictor [28], [58], [59], [60], finding a shared representation [50], [61], [62], [63], [64] or a shared subspace [65], [66], [67], [68], as well as probabilistic approaches [69], [70], [71], [72].", "startOffset": 213, "endOffset": 217}, {"referenceID": 63, "context": "Some notable approaches for these settings are based on similarity to a common predictor [28], [58], [59], [60], finding a shared representation [50], [61], [62], [63], [64] or a shared subspace [65], [66], [67], [68], as well as probabilistic approaches [69], [70], [71], [72].", "startOffset": 255, "endOffset": 259}, {"referenceID": 64, "context": "Some notable approaches for these settings are based on similarity to a common predictor [28], [58], [59], [60], finding a shared representation [50], [61], [62], [63], [64] or a shared subspace [65], [66], [67], [68], as well as probabilistic approaches [69], [70], [71], [72].", "startOffset": 261, "endOffset": 265}, {"referenceID": 65, "context": "Some notable approaches for these settings are based on similarity to a common predictor [28], [58], [59], [60], finding a shared representation [50], [61], [62], [63], [64] or a shared subspace [65], [66], [67], [68], as well as probabilistic approaches [69], [70], [71], [72].", "startOffset": 267, "endOffset": 271}, {"referenceID": 66, "context": "Some notable approaches for these settings are based on similarity to a common predictor [28], [58], [59], [60], finding a shared representation [50], [61], [62], [63], [64] or a shared subspace [65], [66], [67], [68], as well as probabilistic approaches [69], [70], [71], [72].", "startOffset": 273, "endOffset": 277}, {"referenceID": 1, "context": "Semi-supervised Transfer: source and target domains are the same and source data includes only unlabeled examples [2], [5], [17], [51], [62], [73], [74], [75].", "startOffset": 114, "endOffset": 117}, {"referenceID": 4, "context": "Semi-supervised Transfer: source and target domains are the same and source data includes only unlabeled examples [2], [5], [17], [51], [62], [73], [74], [75].", "startOffset": 119, "endOffset": 122}, {"referenceID": 13, "context": "Semi-supervised Transfer: source and target domains are the same and source data includes only unlabeled examples [2], [5], [17], [51], [62], [73], [74], [75].", "startOffset": 124, "endOffset": 128}, {"referenceID": 46, "context": "Semi-supervised Transfer: source and target domains are the same and source data includes only unlabeled examples [2], [5], [17], [51], [62], [73], [74], [75].", "startOffset": 130, "endOffset": 134}, {"referenceID": 56, "context": "Semi-supervised Transfer: source and target domains are the same and source data includes only unlabeled examples [2], [5], [17], [51], [62], [73], [74], [75].", "startOffset": 136, "endOffset": 140}, {"referenceID": 67, "context": "Semi-supervised Transfer: source and target domains are the same and source data includes only unlabeled examples [2], [5], [17], [51], [62], [73], [74], [75].", "startOffset": 142, "endOffset": 146}, {"referenceID": 68, "context": "Semi-supervised Transfer: source and target domains are the same and source data includes only unlabeled examples [2], [5], [17], [51], [62], [73], [74], [75].", "startOffset": 148, "endOffset": 152}, {"referenceID": 69, "context": "Semi-supervised Transfer: source and target domains are the same and source data includes only unlabeled examples [2], [5], [17], [51], [62], [73], [74], [75].", "startOffset": 154, "endOffset": 158}, {"referenceID": 9, "context": "For a comprehensive review of these fields the reader is referred to the works of Pan and Yang [11] and Jiang [56].", "startOffset": 95, "endOffset": 99}, {"referenceID": 51, "context": "For a comprehensive review of these fields the reader is referred to the works of Pan and Yang [11] and Jiang [56].", "startOffset": 110, "endOffset": 114}, {"referenceID": 70, "context": "proposed a simple technique to update an existing tree trained only on source samples using target samples [76].", "startOffset": 107, "endOffset": 111}, {"referenceID": 71, "context": "Adaptive DTs and stream sub-sampling have been used in data streams to handle massive, high-speed streams [77], [78], [79] as well as concept drift [80], [81], [82], modifying the DT as new samples arrive.", "startOffset": 106, "endOffset": 110}, {"referenceID": 72, "context": "Adaptive DTs and stream sub-sampling have been used in data streams to handle massive, high-speed streams [77], [78], [79] as well as concept drift [80], [81], [82], modifying the DT as new samples arrive.", "startOffset": 112, "endOffset": 116}, {"referenceID": 73, "context": "Adaptive DTs and stream sub-sampling have been used in data streams to handle massive, high-speed streams [77], [78], [79] as well as concept drift [80], [81], [82], modifying the DT as new samples arrive.", "startOffset": 118, "endOffset": 122}, {"referenceID": 74, "context": "Adaptive DTs and stream sub-sampling have been used in data streams to handle massive, high-speed streams [77], [78], [79] as well as concept drift [80], [81], [82], modifying the DT as new samples arrive.", "startOffset": 148, "endOffset": 152}, {"referenceID": 75, "context": "Adaptive DTs and stream sub-sampling have been used in data streams to handle massive, high-speed streams [77], [78], [79] as well as concept drift [80], [81], [82], modifying the DT as new samples arrive.", "startOffset": 154, "endOffset": 158}, {"referenceID": 76, "context": "Adaptive DTs and stream sub-sampling have been used in data streams to handle massive, high-speed streams [77], [78], [79] as well as concept drift [80], [81], [82], modifying the DT as new samples arrive.", "startOffset": 160, "endOffset": 164}, {"referenceID": 77, "context": "presented a variation of AdaBoost with decision stumps fitted to multiple tasks [83].", "startOffset": 80, "endOffset": 84}, {"referenceID": 78, "context": "The same authors later applied boosting with DTs while using a modified information gain (IG) criterion [84].", "startOffset": 104, "endOffset": 108}], "year": 2015, "abstractText": "We propose novel model transfer-learning methods that refine a decision forest model M learned within a \u201csource\u201d domain using a training set sampled from a \u201ctarget\u201d domain, assumed to be a variation of the source. We present two random forest transfer algorithms. The first algorithm searches greedily for locally optimal modifications of each tree structure by trying to locally expand or reduce the tree around individual nodes. The second algorithm does not modify structure, but only the parameter (thresholds) associated with decision nodes. We also propose to combine both methods by considering an ensemble that contains the union of the two forests. The proposed methods exhibit impressive experimental results over a range of problems.", "creator": "TeX"}}}