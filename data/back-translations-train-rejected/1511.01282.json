{"id": "1511.01282", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "4-Nov-2015", "title": "Factorizing LambdaMART for cold start recommendations", "abstract": "Recommendation systems often rely on point-wise loss metrics such as the mean squared error. However, in real recommendation settings only few items are presented to a user. This observation has recently encouraged the use of rank-based metrics. LambdaMART is the state-of-the-art algorithm in learning to rank which relies on such a metric. Despite its success it does not have a principled regularization mechanism relying in empirical approaches to control model complexity leaving it thus prone to overfitting.", "histories": [["v1", "Wed, 4 Nov 2015 10:49:15 GMT  (50kb)", "http://arxiv.org/abs/1511.01282v1", null]], "reviews": [], "SUBJECTS": "cs.LG cs.IR", "authors": ["phong nguyen", "jun wang", "alexandros kalousis"], "accepted": false, "id": "1511.01282"}, "pdf": {"name": "1511.01282.pdf", "metadata": {"source": "CRF", "title": "Factorizing LambdaMART for cold start recommendations", "authors": ["Phong Nguyen", "Jun Wang", "Alexandros Kalousis"], "emails": ["Phong.Nguyen@unige.ch", "jwang1@expedia.com", "Alexandros.Kalousis@hesge.ch"], "sections": [{"heading": null, "text": "ar Xiv: 151 1.01 282v 1 [cs.L G] 4N ov2 01 Motivated by the fact that the descriptions of users and articles as well as the preference behavior can very often be well summarized by a small number of hidden factors, we propose a novel algorithm, LambdaMARTMatrix Factorization (LambdaMART-MF), which learns a latent representation of users and articles with gradient-enhanced trees. The algorithm factorizes lambdaMART by defining relevance values as an inner product of the learned representations of users and articles. The lower rank is essentially a model complexity controller; furthermore, we propose additional regulators to limit the learned latent representations reflecting the user and article manifolds, since these are defined by their original characteristics, which are based on descriptors and the preference behavior, and we propose to relate the articles to DG, since they are very different from the ones used by MG and DG, and we suggest a similar penalty for the calculation systems."}, {"heading": "1. INTRODUCTION", "text": "Most receiver algorithms minimize a punctual loss such as the mean square error or the mean average error between the predicted and actual user preferences. For example, in matrix factorization, a learning paradigm that is very popular in receiver problems. Such cost functions are clearly not suitable for receiver problems as they determine the order of preferences rather than absolute values."}, {"heading": "2. PRELIMINARIES", "text": "We get a (sparse) preference matrix, Y, of size n \u00b7 m. The (i, j) missing input of Y represents the preference value of the ith user for the jth element in recommendation problems or the relevance value of the jth document for the ith element when learning problems, the larger the value of the (i, j) entry, the greater the relevance or preference. In addition to the preference matrix, we also have the descriptions of the users and items. Similarly, we use ci = (ci1,., cid) to denote T-Rd, the d-dimensional description of the ith user, and with C the n \u00d7 d user description matrix, whose ith line is given by the cTi. Similarly, we use dj = (dj1,.,., djl) to denote the T-dimensional description of the ith element as the preference value, but the l-dimensional description of the cTi is given by the cTi."}, {"heading": "2.1 Evaluation Metric", "text": "In real-world applications of preference learning, such as recommendation, information gathering, etc., users are ultimately shown only a few points with the highest score. Consequently, appropriate benchmarks for preference learning focus on the correctness of the points with the highest score. One such very common metric is the Discounted Cumulative Gain (DCG) [10], which is defined as follows: DCG (r, y) @ k = M \u2211 i = 12yi \u2212 1log2 (ri + 1) I (ri \u2264 k) (1), where k is the truncation level at which DCG is calculated, and I the indicator function that returns 1 if its argument is otherwise 0. y is dimensional ground relevance vector and r is a ranking vector that we will learn. The DCG score measures the match between the given ranking vector r and the ranking vector of the relevance vector."}, {"heading": "2.2 LambdaMART", "text": "The main difficulty in learning preferences is that the sequence functions are not continuous and that the combination complexity in each case is as high as it is. Therefore, most common will be the sequence of the sequence of the sequence of the sequence of the sequence of the sequence of the sequence of the sequence of the sequence of the sequence of the sequence of the sequence of the sequence of the sequence of the sequence of the sequence of the sequence of the sequence of the sequence of the sequence of the sequence of the sequence of the sequence of the sequence of the sequence of the sequence of the sequence of the sequence of the sequence of the sequence of the sequence of the sequence of the sequence of the sequence of the sequence of the sequence of the sequence of the sequence of the sequence of the sequence of the sequence of the sequence of the sequence of the sequence of the sequence of the sequence of the sequence of the sequence of the sequence of the sequence of the sequence of the sequence of the sequence of the sequence of the sequence of the sequence of the sequence of the sequence of the sequence of the sequence of the sequence of the sequence of the sequence of the sequence of the sequence of the sequence of the sequence of the sequence of the sequence of the sequence of the sequence of the sequence of the sequence of the sequence of the sequence of the sequence of the sequence of the sequence of the sequence of the sequence of the sequence of the sequence of the sequence of the sequence of the sequence of the sequence of the sequence of the sequence of the sequence of the sequence of the sequence of the sequence of the sequence of the sequence of the sequence of the sequence of the sequence of the sequence of the sequence of the sequence of the sequence of the sequence of the sequence of the sequence of the sequence of the sequence of the sequence of the sequence of the sequence of the sequence of the sequence of the sequence of the sequence of the sequence of the sequence of the sequence of the sequence of the"}, {"heading": "3. FACTORIZED LAMBDA-MART", "text": "In order to address the new representation of users and terms, the dimensioning of r is a small number of users and terms that have the added advantage of introducing structural regulations in the learned preference. (D) We will define the relevance of users and terms of users and terms we use in the learned preference matrix. Specifically, we will define the relevance of users and terms of y, where ui and vj are the rdimensional users and descriptions. (D) We denote U: n \u00b7 r The new representation of users and terms of users. (D) We are the rdimensional users and descriptions of users. (D) We denote U: n \u00b7 r The new representation of users and terms. (D) We are the rdimensional users and descriptions. (D) We are the rdimensional descriptions of users and descriptions. (D)"}, {"heading": "4. REGULARIZATION", "text": "In this section, we will describe a number of different control methods to meaningfully limit the learning of the user and item-latent profile. We will do this by incorporating various control mechanisms into the color gradient enhancement algorithm to avoid overmatching while learning the Fu and FV functions."}, {"heading": "4.1 Input-Output Space Regularization", "text": "We will address the question of how far we will be able to reflect the respective similarities of their users. (D) We will also define the learned latent representations through the input-space similarities, what we will call output-space similarities, which we will call output-space similarities. (D) We will not consider the input-space similarities, which we will call output-space similarities, as output-space similarities of users who want to recognize the respective similarities of their preference vectors. (D) We will use the input-space similarities, which we will consider as input / output-space similarities, to track the input-space similarities. (D) We will define the input-space similarities of users and items. (D) We will use the input-space similarities of users and items that we will use as input / output-space similarities of users and items. (D)"}, {"heading": "4.2 Weighted NDCG Cost", "text": "In addition to the laplac regulation of the latent profiles, we offer a soft variant of the NDCG loss used in LambdaMART. Remember that the loss in the NDCG is determined by the pair difference that occurs when we exchange the position of two items j and k for a specific user. This loss can be disproportionately large, even if the two items are similar in terms of the similarities defined above. A consequence of such large penalties will be a large deviation of the inclined trees under which similar items no longer fall into the same sheet node. To alleviate this problem, we introduce a weighted NDCG difference that takes into account the input and output similarities of the items, which we define as follows: SV = \u00b51SVin + \u00b52SVout (24) \u2022 WNDCGijk = NDCG i jk (1 \u2212 sVjk) Under the weighted variant, if two Itej are similar, the loss is caused by very small trees."}, {"heading": "4.3 Regularized LambdaMART-MF", "text": "It is not the first time that we will be able to realize the results. (1) It is the first time that we are able to realize the results. (1) It is the second time that we are able to realize the results. (2) It is the second time that we are able to realize the results. (2) It is the second time that we are able to realize the results. (2) It is the first time that we are able to realize the results. (2) It is the second time that we are able to realize the results. (2) It is the second time that we are able to realize the results. (2) It is the first time that we are trying to realize the results. (2) It is the second time that we are trying to realize the results. (2) It is the second time that we are trying to realize the results. (2) It is the second time that we are trying to realize the results. (2) It is the second time that we are trying to realize the results. (2) It is the second time that we are trying to realize the results."}, {"heading": "5.1 Recommendation Tasks", "text": "We will evaluate the performance of the algorithms presented above in two different variants of the cold start problem. In the first variant, which we will call User Cold Start, we will evaluate the quality of the recommendations they make to invisible users when the set of elements about which we make recommendations is specified. In the second variant, which we will call Full Cold Start, we will make suggestions for both invisible users and items. In addition, we will evaluate the performance of our algorithms in a Matrix Com-2http: / / grouplens.org / datasets / movielens / pletion setting; here we will randomly remove items from users \"preference lists and predict the preferences of the removed items based on a model learned from the remaining observed ratings. In model-based collaborative filtering, the matrix completion will usually be performed by low-level matrix factoring algorithms that do not want to evaluate side information, whether we see any improvements in the side performance."}, {"heading": "5.2 Comparison Baselines", "text": "As a first baseline, we use LambdaMART (LM), [4] in both cold start variants as well as in matrix completion. To ensure a fair comparison of our methods against LamdaMART, we will all train with the same values about the hyperparameters they share, namely the learning rate, the size of the regression trees and the number of iterations. In matrix completion, we will also add CofiRank (CR) as baseline, a state-of-the-matrix factorization algorithm in collaborative ranking [13]. CR minimizes an upper limit of NDCG and uses the l2 standard to regulate the latent factors. Its objective function is therefore similar to that of methods and LambdaMART, but it directly learns the latent factors by using the side information."}, {"heading": "5.3 Meta-Mining", "text": "This year it is so far that it will only be a matter of time before it is ready, until it is ready."}, {"heading": "5.4 MovieLens", "text": "This year it is more than ever before."}, {"heading": "6. CONCLUSION", "text": "Since in real referral systems only top items can be observed by users, we believe that ranking loss functions that focus on the accuracy of the top items are better suited to this type of problem. We are investigating the use of a state-of-the-art learning algorithm, LambdaMART, in a referral framework focused on the cold start problem, one of the most difficult problems in referral systems. However, LambdaMART has a certain number of limitations, stemming in particular from the fact that there is no principled way to control overfitting based on ad hoc approaches. We proposed a number of ways to deal with these limitations. Most importantly, we consider learning the problem as learning a low-level matrix factorization; our underlying assumption here is that the descriptions of users and items and their preferential behavior are strongly influenced by some latent factors. We are now putting user-learning preferences together as metrical elements in addition to user-learning preferences, and very much influencing the regulatory ones."}, {"heading": "7. REFERENCES", "text": "[1] J. Abernethy, F. Bach, T. Evgeniou, and J.-P. Vert.P. Burges M. 2001. Low Intelligence Mining Factorization with attributes. arXiv preprint cs / 0611124, 2006. [2] D. Agarwal and B.-C. Chen. flda: Matrix Factorization through latent dirichlet allocation. In Proceedings of the Third ACM international conference on Web search and data mining, WSDM '10, pp. 91-100, New York, NY, USA, 2010. [3] R. M. Bell and Y. Koren. Scalable collaborative filtering with the third ACM international interpolation weights. In Data Mining, 2007. ICDM2007. Seventh IEEE International Conference on, pp. 43-52. IEEE, 2007. [4] C. J. Burges. From ranknet to lambdarank to lambdamart. Learning."}], "references": [{"title": "Low-rank matrix factorization with attributes", "author": ["J. Abernethy", "F. Bach", "T. Evgeniou", "J.-P. Vert"], "venue": "arXiv preprint cs/0611124,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2006}, {"title": "flda: matrix factorization through latent dirichlet allocation", "author": ["D. Agarwal", "B.-C. Chen"], "venue": "In Proceedings of the third ACM international conference on Web search and data mining,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2010}, {"title": "Scalable collaborative filtering with jointly derived neighborhood interpolation weights", "author": ["R.M. Bell", "Y. Koren"], "venue": "In Data Mining,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2007}, {"title": "From ranknet to lambdarank to lambdamart: An overview", "author": ["C.J. Burges"], "venue": "Learning, 11:23\u2013581,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2010}, {"title": "Learning to rank using an ensemble of lambda-gradient models", "author": ["C.J. Burges", "K.M. Svore", "P.N. Bennett", "A. Pastusiak", "Q. Wu"], "venue": "Journal of Machine Learning Research-Proceedings Track,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2011}, {"title": "On the local optimality of lambdarank", "author": ["P. Donmez", "K.M. Svore", "C.J. Burges"], "venue": "In Proceedings of the 32nd international ACM SIGIR conference on Research and development in information retrieval,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2009}, {"title": "Greedy function approximation: a gradient boosting machine", "author": ["J.H. Friedman"], "venue": "Annals of Statistics,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2001}, {"title": "Ontology-based meta-mining of knowledge discovery workflows", "author": ["M. Hilario", "P. Nguyen", "H. Do", "A. Woznica", "A. Kalousis"], "venue": "Meta-Learning in Computational Intelligence. Springer,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2011}, {"title": "Ir evaluation methods for retrieving highly relevant documents", "author": ["K. J\u00e4rvelin", "J. Kek\u00e4l\u00e4inen"], "venue": "In Proceedings of the 23rd annual international ACM SIGIR conference on Research and development in information retrieval,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2000}, {"title": "Learning heterogeneous similarity measures for hybrid-recommendations in meta-mining", "author": ["P. Nguyen", "J. Wang", "M. Hilario", "A. Kalousis"], "venue": "In IEEE 12th International Conference on Data Mining (ICDM),", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2012}, {"title": "Maximum-margin matrix factorization", "author": ["N. Srebro", "J.D.M. Rennie", "T.S. Jaakkola"], "venue": "Advances in Neural Information Processing Systems", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2005}, {"title": "Maximum margin matrix factorization for collaborative ranking", "author": ["M. Weimer", "A. Karatzoglou", "Q.V. Le", "A. Smola"], "venue": "Advances in neural information processing systems,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2007}, {"title": "On using simultaneous perturbation stochastic approximation for ir measures, and the empirical optimality of lambdarank", "author": ["Y. Yue", "C. Burges"], "venue": "In NIPS Machine Learning for Web Search Workshop,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2007}], "referenceMentions": [{"referenceID": 10, "context": "For instance in matrix factorization, a learning paradigm very popular in recommendation problems, stateof-the-art approaches such as [12, 1, 2] minimize the squared error between the inner product of the learned low-rank representations of users and items and the respective true preference scores.", "startOffset": 134, "endOffset": 144}, {"referenceID": 0, "context": "For instance in matrix factorization, a learning paradigm very popular in recommendation problems, stateof-the-art approaches such as [12, 1, 2] minimize the squared error between the inner product of the learned low-rank representations of users and items and the respective true preference scores.", "startOffset": 134, "endOffset": 144}, {"referenceID": 1, "context": "For instance in matrix factorization, a learning paradigm very popular in recommendation problems, stateof-the-art approaches such as [12, 1, 2] minimize the squared error between the inner product of the learned low-rank representations of users and items and the respective true preference scores.", "startOffset": 134, "endOffset": 144}, {"referenceID": 11, "context": "Cofirank [13], a collaborative ranking algorithm, does maximum-margin matrix factorization by optimizing an upper bound of NDCG measure, a ranking-based loss function.", "startOffset": 9, "endOffset": 13}, {"referenceID": 3, "context": "Probably the best known example of such algorithms is LambdaMART, [4], the state-of-the-art learning to rank algorithm.", "startOffset": 66, "endOffset": 69}, {"referenceID": 8, "context": "One such, very often used, metric is the Discounted Cumulative Gain (DCG)[10], which is defined as follows:", "startOffset": 73, "endOffset": 77}, {"referenceID": 3, "context": "LambdaMART is one of the most popular algorithms for preference learning which follows exactly this idea, [4].", "startOffset": 106, "endOffset": 109}, {"referenceID": 12, "context": "[14] have shown empiricially that solving this problem also optimizes the NDCG metric of the learned model.", "startOffset": 0, "endOffset": 4}, {"referenceID": 6, "context": "LambdaMART uses Multiple Additive Regression Trees (MART) [7] to solve its optimization problem.", "startOffset": 58, "endOffset": 61}, {"referenceID": 4, "context": "g [5, 6].", "startOffset": 2, "endOffset": 8}, {"referenceID": 5, "context": "g [5, 6].", "startOffset": 2, "endOffset": 8}, {"referenceID": 10, "context": "Existing matrix factorization algorithms used in collaborative filtering recommendation learn the low-rank representation of users and items in order to complete the sparse preference matrix Y, [12, 13, 1, 2], however these approaches cannot address the cold start problem.", "startOffset": 194, "endOffset": 208}, {"referenceID": 11, "context": "Existing matrix factorization algorithms used in collaborative filtering recommendation learn the low-rank representation of users and items in order to complete the sparse preference matrix Y, [12, 13, 1, 2], however these approaches cannot address the cold start problem.", "startOffset": 194, "endOffset": 208}, {"referenceID": 0, "context": "Existing matrix factorization algorithms used in collaborative filtering recommendation learn the low-rank representation of users and items in order to complete the sparse preference matrix Y, [12, 13, 1, 2], however these approaches cannot address the cold start problem.", "startOffset": 194, "endOffset": 208}, {"referenceID": 1, "context": "Existing matrix factorization algorithms used in collaborative filtering recommendation learn the low-rank representation of users and items in order to complete the sparse preference matrix Y, [12, 13, 1, 2], however these approaches cannot address the cold start problem.", "startOffset": 194, "endOffset": 208}, {"referenceID": 7, "context": "Meta-mining [9] applies the idea of meta-learning or learning to learn to the whole DM process.", "startOffset": 12, "endOffset": 15}, {"referenceID": 9, "context": "Recently [11] have proposed tackling the problem as a hybrid recommendation problem: dataset-workflow pairs can be seen as user-item pairs which are related by the relative performance achieved by the workflows applied on the datasets.", "startOffset": 9, "endOffset": 13}, {"referenceID": 9, "context": "We propose here to go one step beyond [11] and consider the meta-mining as a learning to rank problem.", "startOffset": 38, "endOffset": 42}, {"referenceID": 3, "context": "As first baseline we will use LambdaMART (LM), [4], in both cold start variants as well as in the matrix completion setting.", "startOffset": 47, "endOffset": 50}, {"referenceID": 11, "context": "In the matrix completion we will also add CofiRank (CR) as a baseline, a state-of-the-art matrix factorization algorithm in collaborative ranking [13].", "startOffset": 146, "endOffset": 150}, {"referenceID": 2, "context": "For the two cold start evaluation variants, we will also have as a second baseline a memory-based approach, one of the most common approaches used for cold start recommendations [3].", "startOffset": 178, "endOffset": 181}, {"referenceID": 9, "context": "The meta-mining problem we will consider is the one provided by [11].", "startOffset": 64, "endOffset": 68}, {"referenceID": 9, "context": "In [11], the authors used four feature selection algorithms: Information Gain, IG, Chisquare, CHI, ReliefF, RF, and recursive feature elimination with SVM, SVMRFE; they fixed the number of selected features to ten.", "startOffset": 3, "endOffset": 7}, {"referenceID": 9, "context": "Finally, to describe the datasets and the data mining workflows we use the same characteristics that were used in [11].", "startOffset": 114, "endOffset": 118}, {"referenceID": 4, "context": "We choose N in the range [5, 10, 15].", "startOffset": 25, "endOffset": 36}, {"referenceID": 8, "context": "We choose N in the range [5, 10, 15].", "startOffset": 25, "endOffset": 36}, {"referenceID": 11, "context": "For this baseline we used the default parameters as these are provided in [13].", "startOffset": 74, "endOffset": 78}, {"referenceID": 0, "context": "g [1, 2].", "startOffset": 2, "endOffset": 8}, {"referenceID": 1, "context": "g [1, 2].", "startOffset": 2, "endOffset": 8}], "year": 2015, "abstractText": "Recommendation systems often rely on point-wise loss metrics such as the mean squared error. However, in real recommendation settings only few items are presented to a user. This observation has recently encouraged the use of rankbased metrics. LambdaMART is the state-of-the-art algorithm in learning to rank which relies on such a metric. Despite its success it does not have a principled regularization mechanism relying in empirical approaches to control model complexity leaving it thus prone to overfitting. Motivated by the fact that very often the users\u2019 and items\u2019 descriptions as well as the preference behavior can be well summarized by a small number of hidden factors, we propose a novel algorithm, LambdaMARTMatrix Factorization (LambdaMART-MF), that learns a low rank latent representation of users and items using gradient boosted trees. The algorithm factorizes lambdaMART by defining relevance scores as the inner product of the learned representations of the users and items. The low rank is essentially a model complexity controller; on top of it we propose additional regularizers to constraint the learned latent representations that reflect the user and item manifolds as these are defined by their original feature based descriptors and the preference behavior. Finally we also propose to use a weighted variant of NDCG to reduce the penalty for similar items with large rating discrepancy. We experiment on two very different recommendation datasets, meta-mining and movies-users, and evaluate the performance of LambdaMART-MF, with and without regularization, in the cold start setting as well as in the simpler matrix completion setting. In both cases it outperforms in a significant manner current state of the art algorithms.", "creator": "LaTeX with hyperref package"}}}