{"id": "1511.01442", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "4-Nov-2015", "title": "Low-Rank Approximation of Weighted Tree Automata", "abstract": "We describe a technique to minimize weighted tree automata (WTA), a powerful formalisms that subsumes probabilistic context-free grammars (PCFGs) and latent-variable PCFGs. Our method relies on a singular value decomposition of the underlying Hankel matrix defined by the WTA. Our main theoretical result is an efficient algorithm for computing the SVD of an infinite Hankel matrix implicitly represented as a WTA. We provide an analysis of the approximation error induced by the minimization, and we evaluate our method on real-world data originating in newswire treebank. We show that the model achieves lower perplexity than previous methods for PCFG minimization, and also is much more stable due to the absence of local optima.", "histories": [["v1", "Wed, 4 Nov 2015 19:17:18 GMT  (401kb,D)", "http://arxiv.org/abs/1511.01442v1", "Submitted to AISTATS 2016"], ["v2", "Thu, 24 Dec 2015 08:39:49 GMT  (431kb,D)", "http://arxiv.org/abs/1511.01442v2", "To appear in AISTATS 2016"]], "COMMENTS": "Submitted to AISTATS 2016", "reviews": [], "SUBJECTS": "cs.LG cs.FL", "authors": ["guillaume rabusseau", "borja balle", "shay b cohen"], "accepted": false, "id": "1511.01442"}, "pdf": {"name": "1511.01442.pdf", "metadata": {"source": "CRF", "title": "Weighted Tree Automata Approximation by Singular Value Truncation", "authors": ["Guillaume Rabusseau", "Borja Balle", "Shay B. Cohen"], "emails": [], "sections": [{"heading": "1 Introduction", "text": "In fact, most of these algorithms are able to outdo themselves, and the complexity of these algorithms depends on the size of the grammar in which they move. As a rule, they are determined by the number of rules and states in which they move."}, {"heading": "1.1 Notation", "text": "For an integer n, we write [n] = {1,.., n}. We use bold lowercase letters (or symbols) for vectors (e.g. v-Rd1), bold uppercase letters for matrices (e.g. M-Rd1-d2), and bold calligraphic letters for third-order tensors (e.g. T-Rd1-d2-d3). Without explicit vectors, vectors are column vectors by default. The identity matrix is written as I.: i1-d1), i2-d2 [d2], i3-d3 [d3], we use v (i1), M (i1, i2), and T (i2, i3, i3) to mark the corresponding entries. The ith series (or column) of a matrix M is called M (i-d3) (resp. M (:, i)."}, {"heading": "2 Approximate Minimization of WTA and SVD of Hankel Matrices", "text": "In this section, we present the first contribution of the paper: namely, the existence of a canonical form for weighted tree automats that induces the singular depreciation of the infinite Hankel matrix associated with the automaton. We begin by referring to several definitions and well-known facts about WTA used in the rest of the essay, and then proceed to establish the existence of the canonical form that we call the singular value-tree automaton. Finally, we show how the elimination of the states in this canonical form corresponding to the smallest singular values of the Hankel matrix leads to an effective process of model reduction in the WTA."}, {"heading": "2.1 Weighted Tree Automata", "text": "Let's be a finite alphabet; that is, the depth of a tree is defined by the size (t) and defined recursively by the size (t). (t1, t2) The size of a tree is defined by the size (t). (t1, t2) The size of a tree is defined by the size (t1, t2). (t1, t2) The size of a tree is defined by the size (t). (t1, t2) The size of a tree is defined by the size (t). (t1, t2) The size (t2) The size (t1) + size (t2) The size (t2) + size (t2) The number of internal nodes in the tree is defined by the size (t). (t) The depth of a tree is defined by the size (t1, t2)."}, {"heading": "2.2 Rank Factorizations of Hankel Matrices", "text": "If the rank of Hf is indeed finite - say rank (Hf) = n - one can refine theorem 1 by showing that if f is rational, the set of all possible rank factorizations of Hf in direct agreement with the set of minimum WTA calculations f. The first step is to show that any minimum WTA value A = < \u03b1, T, {phenomena} > calculation f can cause a rank factorization Hf = PASA. We build SA by setting the column corresponding to a tree t to SA (:, t). To define PA, we must all introduce a new rank factorization Hf = PASA."}, {"heading": "2.3 Approximate Minimization with the Singular Value Tree Automaton", "text": "The equation (1) can be interpreted as meaning that each state is, as far as possible, disconnected from each other."}, {"heading": "3 Computing the Singular Value WTA", "text": "In this section we can show that if we have an arbitrary minimum of WTA A for f, then we can transform the calculation of SVD into the corresponding SVTA efficiency. In other words, given a representation of Hf as WTA, we can calculate its SVD without the need to operate on infinite matrices. The key observation is to reduce the calculation of SVD from Hf to the spectral properties of the gram matrices GC = P > P and GT = SS > associated with the rank factoring Hf = PS from some minimal WTA calculation f."}, {"heading": "4 Approximation Error of an SVTA Truncation", "text": "In this section we analyze the approximation error caused by circumcision of an SVTA."}, {"heading": "5 Experiments", "text": "In this section, we evaluate the performance of our method using a model derived from real data, using a PCFG that we learned from a text corpus as the starting model. Before presenting our experimental setup and results, we remember the standard mapping between WCFG and WTA."}, {"heading": "5.1 Converting WCFG to WTA", "text": "A weighted context-free grammar (WCFG) in Chomsky's normal form is a tuple G = < N, \u03a3, R, weight > where N is the finite set of non-terminal symbols, \u03a3 is the finite set of words, where \u03a3 N = \u2205, R is a set of rules having the form (a \u2192 bc), (a \u2192 x) or (\u2192 a) for a, b, c, c, x, and weight: R \u2192 R is the weight function extended to the set of all possible rules by assigning the weight (\u03b4) = 0 for all rules \u03b4 6 \u0440R.A WCFG G G to each derivative tree given by the weight."}, {"heading": "5.2 Experimental Setup and Results", "text": "It is not as if we get involved in the way in which we engage in the way in which we apply it. (It is not that we get involved in the way in which we apply it.) (It is not that we get involved in the way in which we apply it.) (It is not that we get involved in the way in which we apply it.) We compare our method with the methods described in (Cohen et al., 2013a), which evolve in the way in which we apply it. (Chi and Kolda, 2012) around the types of an underlying PCFG.2We used three evaluation measures: \"2 distance between the functions.\""}, {"heading": "6 Conclusion", "text": "Our main algorithm is based on a singular value decomposition of an infinite Hankel matrix induced by the WTA. We gave theoretical guarantees for the error caused by our minimization method. Our experiments with real analytical data show that the minimized WTA, depending on the number of individual values used, approximates the original WTA well in three metrics: perplexity, clamping accuracy and '2 distance of tree weights. Our work has links to spectral learning techniques for WTA and exhibits properties similar to those algorithms, such as the lack of local optima. In future work, we plan to investigate the application of our approach to the design and analysis of improved spectral learning algorithms for WTA."}, {"heading": "A Proof of Theorem 2", "text": "Theorem. Let f: T \u2192 R be rational. If Hf = PS is a rank factorization, then there is a minimum WTA factorization f so that PA = P \u00b2 and SA = S.Proof. Let n = rank (f). Let B have an arbitrary minimum WTA factorization f. Suppose B induces the rank factorization Hf = P \u00b2 S \u00b2. Since the columns of P and P \u00b2 are the basis for the column range of Hf, there must be a change in the base Q \u00b2 Rn \u00b7 n between P and P \u00b2. That is, Q is an invertable matrix so that P \u00b2 Q = P. Furthermore, since P \u00b2 S \u00b2 = Hf = PS \u00b2 QS and P \u00b2 has a full column rank, we must have S \u00b2 = QS, or equivalent, Q \u2212 1S \u00b2 (S \u00b2 = S). Thus, let A = BQ \u2212 C show what fA = B immediately confirms."}, {"heading": "B Proof of Theorem 3", "text": "The result will follow if we show that Hf is the matrix of a compact operator in a Hilbert space (Conway, 1990). The main obstacle to this approach is that the rows and columns of Hf are indexed by different objects (trees vs. contexts), so we will have to see Hf as an operator in a larger space containing both objects. All we have is that the rows and columns of Hf are indexed."}, {"heading": "C Proof of Theorem 4", "text": "Theorem. Let F: Rn2 \u2192 Rn2 be the mapping defined by F (v) = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p p p p = p = p = p p p = p = p = p p p = p = p = p p"}, {"heading": "D Proof of Theorem 5", "text": "Theorem. There are 0 < p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 \u2212 \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212"}, {"heading": "E Proof of Theorem 6", "text": "E \"E\" E \"E\" E \"E\" E \"E\" E \"E\" E \"E\" E \"E\" E \"E\" E \"E\" E \"E\" E \"E\" E \"E\" E \"E\" E \"E\" E \"E\" E \"E\" E \"E\" E \"E\" E \"E\" E \"E\" E \"E\" E \"E\" E \"E\" E \"E\" E \"E\" E \"E\" E \"E\" E \"E\" E \"E\" E \"E\" E \"E\" E \"E\" E \"E\" E \"E\" E \"E\" E \"E\" E \"E\" E \"E\" E \"E\" E \"E\" E \"E\" E \"E\" E \"E\" E \"E\" E \"E\" E \"E\" E \"E\" E \"E\" E \"E\" E \"E\" E \"E\" E \"E\" E \"E\" E \"E\" E \"E\" E \"E\" E \"E\" E \"E\" E \"E\" E \"E\" E \"E\" E \"E\" E \"E\" E \"E\" E \"E\" E \"E\" E \"E\" E \"E\" E \"E\" E \"E\" E \"E\" E \"E\" E \"E\" E \"E\" E \"E\" E \"E\" E \"E\" E \"E\" E \"E\" E \"E\" E \"E\" E \"E\" E \"E\" E \"E\" E \"E\" E \"E\" E \"E\" E \"E\" E \"E\" E \"E\" E \"E\" E \"E\" E \"is E\" E \"E\" E \"E\" E \"E\" E \"E\" E \"E\" E \"E\" E \"E\" E \"E\" E \"E\" E \"E\" E \"E\" E \"E\" E \"E\" E \"E\" E \"E\" E \"E\" E \"E\" E \"E\" E \"E\" E \"E\" E \"E\" E \"E\" E \"E\" E \"E\" E \"E\" is E \"E\" E \"E\" E \"E\" E \"E\" E \"E\" E \"E\" E \"E\" E \"E\" E \"E\""}, {"heading": "F Proof of Theorem 7", "text": "To prove that the BTA is a type of kit, we will first introduce a different type of context than the one presented in Section 2, in which each leaf of a binary tree is outlined by a specific tree. & # 8222; & # 8220; We will then demonstrate a fundamental relationship between the components of this first tensor and its truncation. & # 8222; We will ultimately use this problem to introduce the absolute faultiness of an SVTA (Propositions 3 and 4). & # 8220; We will introduce a different type of context from that in Section 2. & # 8222; Each leaf of a binary tree is marked by the special symbol (which still functions as a placeholder). & # 8220; & # 8220;"}], "references": [{"title": "Grammatical inference as a principal component analysis problem", "author": ["R. Bailly", "F. Denis", "L. Ralaivola"], "venue": "In Proceedings of ICML", "citeRegEx": "Bailly et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Bailly et al\\.", "year": 2009}, {"title": "A spectral approach for probabilistic grammatical inference on trees", "author": ["R. Bailly", "A. Habrard", "F. Denis"], "venue": "In Proceedings of ALT", "citeRegEx": "Bailly et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Bailly et al\\.", "year": 2010}, {"title": "Spectral learning of weighted automata: A forward-backward perspective", "author": ["B. Balle", "X. Carreras", "F. Luque", "A. Quattoni"], "venue": "Machine Learning", "citeRegEx": "Balle et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Balle et al\\.", "year": 2014}, {"title": "A canonical form for weighted automata and applications to approximate minimization", "author": ["B. Balle", "P. Panangaden", "D. Precup"], "venue": "In Proceedings of LICS", "citeRegEx": "Balle et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Balle et al\\.", "year": 2015}, {"title": "Recognizable formal power series on trees", "author": ["J. Berstel", "C. Reutenauer"], "venue": "Theoretical Computer Science", "citeRegEx": "Berstel and Reutenauer,? \\Q1982\\E", "shortCiteRegEx": "Berstel and Reutenauer", "year": 1982}, {"title": "Closing the learning planning loop with predictive state representations", "author": ["B. Boots", "S. Siddiqi", "G. Gordon"], "venue": "International Journal of Robotics Research", "citeRegEx": "Boots et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Boots et al\\.", "year": 2011}, {"title": "The rank of a formal tree power series", "author": ["S. Bozapalidis", "O. Louscou-Bozapalidou"], "venue": "Theoretical Computer Science", "citeRegEx": "Bozapalidis and Louscou.Bozapalidou,? \\Q1983\\E", "shortCiteRegEx": "Bozapalidis and Louscou.Bozapalidou", "year": 1983}, {"title": "On tensors, sparsity, and nonnegative factorizations", "author": ["E.C. Chi", "T.G. Kolda"], "venue": "SIAM Journal on Matrix Analysis and Applications", "citeRegEx": "Chi and Kolda,? \\Q2012\\E", "shortCiteRegEx": "Chi and Kolda", "year": 2012}, {"title": "Tensor decomposition for fast parsing with latentvariable PCFGs", "author": ["S.B. Cohen", "M. Collins"], "venue": "In Proceedings of NIPS", "citeRegEx": "Cohen and Collins,? \\Q2012\\E", "shortCiteRegEx": "Cohen and Collins", "year": 2012}, {"title": "Approximate PCFG parsing using tensor decomposition", "author": ["S.B. Cohen", "G. Satta", "M. Collins"], "venue": "In Proceedings of NAACL", "citeRegEx": "Cohen et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Cohen et al\\.", "year": 2013}, {"title": "Experiments with spectral learning of latent-variable PCFGs", "author": ["S.B. Cohen", "K. Stratos", "M. Collins", "D.P. Foster", "L. Ungar"], "venue": "In Proceedings of NAACL", "citeRegEx": "Cohen et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Cohen et al\\.", "year": 2013}, {"title": "Spectral learning of latent-variable PCFGs: Algorithms and sample complexity", "author": ["S.B. Cohen", "K. Stratos", "M. Collins", "D.P. Foster", "L. Ungar"], "venue": "Journal of Machine Learning Research", "citeRegEx": "Cohen et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Cohen et al\\.", "year": 2014}, {"title": "A course in functional analysis", "author": ["J.B. Conway"], "venue": null, "citeRegEx": "Conway,? \\Q1990\\E", "shortCiteRegEx": "Conway", "year": 1990}, {"title": "Inversion error, condition number, and approximate inverses of uncertain matrices. Linear algebra and its applications", "author": ["L. El Ghaoui"], "venue": null, "citeRegEx": "Ghaoui,? \\Q2002\\E", "shortCiteRegEx": "Ghaoui", "year": 2002}, {"title": "Parsing algorithms and metrics", "author": ["J. Goodman"], "venue": "In Proceedings of ACL", "citeRegEx": "Goodman,? \\Q1996\\E", "shortCiteRegEx": "Goodman", "year": 1996}, {"title": "A spectral algorithm for learning hidden Markov models", "author": ["D. Hsu", "S.M. Kakade", "T. Zhang"], "venue": "Journal of Computer and System Sciences", "citeRegEx": "Hsu et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Hsu et al\\.", "year": 2012}, {"title": "Minimisation of Multiplicity Tree Automata", "author": ["S. Kiefer", "I. Marusic", "J. Worrell"], "venue": null, "citeRegEx": "Kiefer et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Kiefer et al\\.", "year": 2015}, {"title": "Low-rank spectral learning with weighted loss functions", "author": ["A. Kulesza", "N. Jiang", "S. Singh"], "venue": "In Proceedings of AISTATS", "citeRegEx": "Kulesza et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Kulesza et al\\.", "year": 2015}, {"title": "Low-Rank Spectral Learning", "author": ["A. Kulesza", "N.R. Rao", "S. Singh"], "venue": "In Proceedings of AISTATS", "citeRegEx": "Kulesza et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Kulesza et al\\.", "year": 2014}, {"title": "Numerical analysis: a second course. Siam", "author": ["J.M. Ortega"], "venue": null, "citeRegEx": "Ortega,? \\Q1990\\E", "shortCiteRegEx": "Ortega", "year": 1990}, {"title": "An annotation scheme for free word order languages", "author": ["W. Skut", "B. Krenn", "T. Brants", "H. Uszkoreit"], "venue": "In Conference on Applied Natural Language Processing", "citeRegEx": "Skut et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Skut et al\\.", "year": 1997}, {"title": "T \u2016F \u2016v\u2016 and \u2016T (I,v, I)\u2016F \u2264 \u2016T \u2016F \u2016v\u2016. Let \u03b4 = \u2016E\u2212 \u00ca\u2016 and let \u03c3 be the smallest nonzero eigenvalue of the matrix I\u2212 E. It follows from (El Ghaoui, 2002, Equation (7.2)) that if \u03b4 < \u03c3 then \u2016(I\u2212 E)\u22121 \u2212 (I\u2212 \u00ca)\u22121\u2016 \u2264 \u03b4/(\u03c3(\u03c3 \u2212 \u03b4))", "author": ["v)\u2016F"], "venue": "Since \u03b4 = O(\u03b5)", "citeRegEx": "I and \u2264,? \\Q2002\\E", "shortCiteRegEx": "I and \u2264", "year": 2002}], "referenceMentions": [{"referenceID": 15, "context": "Our techniques are inspired by recent developments in spectral learning algorithms for different classes of models on sequences (Hsu et al., 2012; Bailly et al., 2009; Boots et al., 2011; Balle et al., 2014) and trees (Bailly et al.", "startOffset": 128, "endOffset": 207}, {"referenceID": 0, "context": "Our techniques are inspired by recent developments in spectral learning algorithms for different classes of models on sequences (Hsu et al., 2012; Bailly et al., 2009; Boots et al., 2011; Balle et al., 2014) and trees (Bailly et al.", "startOffset": 128, "endOffset": 207}, {"referenceID": 5, "context": "Our techniques are inspired by recent developments in spectral learning algorithms for different classes of models on sequences (Hsu et al., 2012; Bailly et al., 2009; Boots et al., 2011; Balle et al., 2014) and trees (Bailly et al.", "startOffset": 128, "endOffset": 207}, {"referenceID": 2, "context": "Our techniques are inspired by recent developments in spectral learning algorithms for different classes of models on sequences (Hsu et al., 2012; Bailly et al., 2009; Boots et al., 2011; Balle et al., 2014) and trees (Bailly et al.", "startOffset": 128, "endOffset": 207}, {"referenceID": 1, "context": ", 2014) and trees (Bailly et al., 2010; Cohen et al., 2014), and subsequent investigations into low-rank spectral learning for predictive state representations (Kulesza et al.", "startOffset": 18, "endOffset": 59}, {"referenceID": 11, "context": ", 2014) and trees (Bailly et al., 2010; Cohen et al., 2014), and subsequent investigations into low-rank spectral learning for predictive state representations (Kulesza et al.", "startOffset": 18, "endOffset": 59}, {"referenceID": 3, "context": ", 2014, 2015) and approximate minimization of weighted automata (Balle et al., 2015).", "startOffset": 64, "endOffset": 84}, {"referenceID": 8, "context": "The idea of speeding up parsing with (L)PCFG by approximating the original model with a smaller one was recently studied in (Cohen and Collins, 2012; Cohen et al., 2013a), where a tensor decomposition technique was used in order to obtain the minimized model.", "startOffset": 124, "endOffset": 170}, {"referenceID": 8, "context": "It was observed in (Cohen and Collins, 2012; Cohen et al., 2013a) that a side-effect of reducing the size of a grammar learned from data was a slight improvement in parsing performance.", "startOffset": 19, "endOffset": 65}, {"referenceID": 8, "context": "It is important to remark that in contrast with the tensor decompositions in (Cohen and Collins, 2012; Cohen et al., 2013a) which are susceptible to local optima problems, our approach resembles a power-method approach to SVD, which yields efficient globally convergent algorithms.", "startOffset": 77, "endOffset": 123}, {"referenceID": 1, "context": "While WTA are usually defined over arbitrary ranked trees, only considering binary trees does not lead to any loss of generality since WTA on ranked trees are equivalent to WTA on binary trees (see (Bailly et al., 2010) for references).", "startOffset": 198, "endOffset": 219}, {"referenceID": 6, "context": "Theorem 1 ((Bozapalidis and Louscou-Bozapalidou, 1983)).", "startOffset": 11, "endOffset": 54}, {"referenceID": 3, "context": "In the case of weighted automata on strings, (Balle et al., 2015) recently showed a polynomial time algorithm for computing the Gram matrices of a string Hankel matrix by solving a system of linear equations.", "startOffset": 45, "endOffset": 65}, {"referenceID": 16, "context": "Thus, we shall resort to a different 1If the WTA given to the algorithm is not minimal, a pre-processing step can be used to minimize the input using the algorithm from (Kiefer et al., 2015).", "startOffset": 169, "endOffset": 190}, {"referenceID": 4, "context": "It is shown in (Berstel and Reutenauer, 1982) that A\u2297 computes the function fA\u2297(t) = f(t)2.", "startOffset": 15, "endOffset": 45}, {"referenceID": 20, "context": "2 Experimental Setup and Results In our experiments, we used the annotated corpus of german newspaper texts NEGRA (Skut et al., 1997).", "startOffset": 114, "endOffset": 133}, {"referenceID": 7, "context": "2013a), who use tensor decomposition algorithms (Chi and Kolda, 2012) to decompose the tensors of an underlying PCFG.", "startOffset": 48, "endOffset": 69}, {"referenceID": 14, "context": "For parsing we use minimum Bayes risk decoding, maximizing the sum of the marginals for the nonterminals in the grammar, essentially choosing the best tree topology given a string (Goodman, 1996).", "startOffset": 180, "endOffset": 195}], "year": 2017, "abstractText": "We describe a technique to minimize weighted tree automata (WTA), a powerful formalisms that subsumes probabilistic context-free grammars (PCFGs) and latent-variable PCFGs. Our method relies on a singular value decomposition of the underlying Hankel matrix defined by the WTA. Our main theoretical result is an efficient algorithm for computing the SVD of an infinite Hankel matrix implicitly represented as a WTA. We provide an analysis of the approximation error induced by the minimization, and we evaluate our method on real-world data originating in newswire treebank. We show that the model achieves lower perplexity than previous methods for PCFG minimization, and also is much more stable due to the absence of local optima.", "creator": "TeX"}}}