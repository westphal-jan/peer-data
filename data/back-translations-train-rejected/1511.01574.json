{"id": "1511.01574", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "5-Nov-2015", "title": "Multinomial Loss on Held-out Data for the Sparse Non-negative Matrix Language Model", "abstract": "We describe Sparse Non-negative Matrix (SNM) language model estimation using multinomial loss on held-out data.", "histories": [["v1", "Thu, 5 Nov 2015 01:45:29 GMT  (13kb)", "https://arxiv.org/abs/1511.01574v1", null], ["v2", "Mon, 22 Feb 2016 19:15:19 GMT  (14kb)", "http://arxiv.org/abs/1511.01574v2", null]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["ciprian chelba", "fernando pereira"], "accepted": false, "id": "1511.01574"}, "pdf": {"name": "1511.01574.pdf", "metadata": {"source": "CRF", "title": "Multinomial Loss on Held-out Data for the Sparse Non-negative Matrix Language Model", "authors": ["Ciprian Chelba"], "emails": ["ciprianchelba@google.com", "pereira@google.com"], "sections": [{"heading": null, "text": "ar Xiv: 151 1.01 574v 2 [cs.C L] 22 FeWe describe the estimation of the Sparse Non-negative Matrix (SNM) language model using multinomial losses on held data. The ability to train on held data is important in practical situations where the training data does not normally match the held / tested data. It is also less restricted than the previous training algorithm based on held data: it allows the use of more abundant meta activities in the adaptation model, such as the diversity counts used by Kneser-Ney, which would be difficult to handle in leave-one-out training. In experiments with the one billion word language modeling benchmark [3], we are able to slightly improve previous results reported in [11] - [11a]."}, {"heading": "1 Introduction", "text": "A statistical language model estimates the probability values P (W) for word strands W in a Vocabulary V, the size of which runs into tens of thousands, hundreds of thousands and sometimes even millions. Typically, the string W is broken down into sentences or other segments, such as expressions in automatic speech recognition, which are often considered conditionally independent; we assume that W is such a segment or such a string. Since the parameter range of P (wk | w1, w2,..., wk \u2212 1) is too large, the language model is forced to classify the context Ww1 = w2, wwk \u2212 1 into an equivalence class determined by a function (Wk \u2212 1). Consequently, the language model is forced to classify the context Ww1 = w2, wwwk \u2212 1 into an equivalence class determined by a function (Wk \u2212 1)."}, {"heading": "2 Notation and Modeling Assumptions", "text": "In fact, it is a way in which most people are able, in which they are able to move, and in which they are able, in which they are able to move, and in which they are able to be able to be in a position, to be in a position, to be in a position, to be in a position, to be in a position, to be in a position, to be in a position, to be in a position, to be in the position in which they are in, in which they are in."}, {"heading": "3 Multinomial Loss for the Sparse Non-negative Matrix Language Model", "text": "17 The sparse non-negative matrix (SNM) is explained by a linear model on a metaphor. (11) - [11] - [11] - [11] - [11] - [11] - [11] - [11] - [11] - [11] - [11] - [11] - [11] - [11] - [11] - [11] - [11] - [13] - [13] - [11] - [14] - [14] - [14] - [14] - [14] - [16] - [16] - [16] - [16] - [16] - [13] - [13] - [13] - [13] - [13] - [14] - [14] - [14] - [14] - [14] - [16] - [16] - [16] - [16 - [16] - [16 - 17 - 17 - 17 - 17 - 17 - 17 - 17 - 17 - 17 - 17 - 17 - 17 - 17 - 17 - 17 - 17 - 17 - 17 - 17 - 17] - 17 - 17 - 17 - [16 - 17] - 17 - 17 - 17 - 17 - 17 - 17 - 17 - 17 - [13] - 17 - 17 - 17 - 17 - 17 - 17 - 17 - [13] - 17 - 17 - 17 - 17 - 17 - 17 - 17 - [13] - 17 - 17 - 17 - 17 - 17 - 17 - 17 - 17 - 17] - 17 - [13 - 17] - 17 - 17 - 17 - 17] - 17 - 17 - 17 - 17 - 17 - 17 - 17 - 17 - 17 - 17 - 17 - 17] - 17 - 17 - 17 - 17 - 17 - 17 - 17 - 17 - 17 - 17 - 17 - 17 - 17 - 17 - 17 - 17 - 17] - 17 - 17 - 17 - 17 - 17 - 17 - 17 - 17 - 17 - 17 - 17 - 17 - 17 - 17 - 17 - 17 - 17 - 17 - 17 - 17 - 17 - 17 - 17 - 17 - 17 - 17 - 17 - 17 - 17 - 17 - 17 - 17 - 17 - 17 - 17 - 17 - 17 - 17 - 17 - 17 - 17 - 17 - 17 - 17 - 17 - 17 - 17 - 17 -"}, {"heading": "3.1 Implementation Notes", "text": "From a mathematical point of view, the two main problems with a simple gradient descent parameter update (either on-line or batch) are: 1. the second term on the right (RHS) of Eq. (5) is an update that must be applied to all words in the vocabulary, whether they occur at a particular training event or not; 2. maintaining the model after an Mfw parameter update means recalculating all Mf. (F) normalization coefficients. For mini / batch updates, model renoralization is performed at the end of each training period / iteration, and it is no longer a problem."}, {"heading": "3.2 Meta-features extraction", "text": "The process of breaking down the original characteristics into meta-characteristics and recombining these characteristics enables similar characteristics, i.e. characteristics that differ only in some of their basic components, to be divided into weights, thus improving generalisation.In the light of an event, the 4-gram characteristic for predicting the target fox would be broken down into the following elementary meta-characteristics: \u2022 Characteristic identity, e.g. [the fast brown] \u2022 Characteristic type, e.g. 3-gram \u2022 Characteristic identity, e.g. Fox \u2022 Characteristic identity CfwElementary meta-characteristics of different types are then combined with others to form more complex meta-characteristics, as best described by the pseudo-code in Appendix C; note that the apparent lack of characteristic identity is represented by the combination of the characteristic identity and the target identity. Since meta-characteristics of the same order of magnitude, we group them so that they can divide the weights."}, {"heading": "4 Experiments", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.1 Experiments on the One Billion Words Language Modeling Benchmark", "text": "In fact, it is such that it is a matter of a way in which most people are able to put themselves into the world in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they, in which they live, in which they, in which they, in which they live, in which they, in which they live, in which they live, in which they, in which they live."}, {"heading": "4.2 Experiments on 10B Words of Burmese Data in Statistical Machine Translation Language Modeling Setup", "text": "In a separate set of data from 1900 to 1900, the number of people residing in a country increased by more than 50%."}, {"heading": "4.3 Experiments on 35B Words of Italian Data in Language Modeling Setup for Automatic Speech Recognition", "text": "The aforementioned hreeaJnln nvo nlrlteeaJnh nvo nlrf\u00fc ide nlrrlgteeaeaeVnln ni red nlrlllteeaeaeaeFnlrh-eSrteeoiuiuiaeaeaeaeaeaeaeaeaeJnln ni nlrteeaeaeaeFnlrh-eSrteeaeaeaeaeaeaeaeaeaeaeaeaeaeaeaeaeSrmnlrllllllllc-etec-eceteerc-Srrrrrtec-Srrteoc Srteoc-lllltec-Srteoc-Srteoc-Srteoc-Srteoc"}, {"heading": "5 Conclusions and Future Work", "text": "The main conclusion is that training the adaptation model on held data by means of multinomial losses has many advantages, whereas the previous results in [11] agree: as observed in [12], Section 2, a binary probability model is expected to produce the same model as a multinomial probability model. Correcting the deficiency in [11] is caused by using a Poisson model for each binary random variable and does not seem to make any difference in the quality of the estimated model. Ability to train on held data is very important in practical situations where the training data normally does not match the held / tested data."}, {"heading": "6 Acknowledgments", "text": "Thanks to Yoram Singer for clarifying the correct mini-batch variant of AdaGrad, Noam Shazeer for help in understanding its implementation of adaptation function estimation, Diamantino Caseiro for code evaluations, Kunal Talwar, Amir Globerson and Diamantino Caseiro for useful discussions and Anton Andryev for providing the SMT training / hold-out / test records. Last but not least we are grateful to our former summer trainee Joris Pelemans for suggestions in preparing the final version of the paper. A Appendix: 5-gram feature extraction configuration / / / / / Test data set c o n-a-a-g a-g a-a-g a-a-a-a-a-a-a-a-a-a-a-a-a-a-a-a-a-a-a-a-a-a-a-a-a-a-a-a-a-a-a-a-a-a-a-a-a-a-a-a-a-a-a-a-a-a-a-a-a-a-a-a-a-a-a-a-a-a-a-a-a-a-a-a-a-a-a-a-a-a-a-a-a-a-a-a-a-a-a-a-a-a-a-a-a-a-a-a-a-a-a-a-a-a-a-a-a-a-a-a-a-a-a-a-a-a-a-a-a-a-a-a-a-a-a-a-a-a-a-a-a-a-a-a-a-a-a-a-a-a-a-a-a-a-a-a-a-a-a-a-a-a-a-a-a-a-a-a-a-a-a-a-a-a-a-a-a-a-a-a-a-a-a-a-a-a-a-a-a-a-a-a-a-a-a-a-a-"}, {"heading": "C Appendix: Meta-features Extraction Pseudo-code", "text": "/ / Concat (Metafeatures, end pos, mf new) concatenates metafeatures mf new / / with all existing metafeatures up to end-pos. function COMPUTEMETAFEATURES (FeatureTargetPair pair) / / feature-related metafeatures \u2190 (fingerprint (pair.feature.id ()), 1.0) Metafeatures \u2190 (fingerprint (pair.feature.type (), 1.0) ln count = log (pair.feature.count () / log (2) bucketetetetetetet1 = floor (ln count) bucketetetetetetetet2 = ceil (ln count) weight1 = bucket2 - (ln count), ln count (ln count) (bucketats count) (.bucketbucketbucketbucket1 = bucketsize (ln count) (mecketetetetetetetetetetet.metetetetetetetetetetetetetetetetetcount (2, ltetetetetetetetetetetettttcount) (count) (count (2, ln) (count (mecketetetetetats count), ln (1, ln) (count). / / / / / Concat (Metafeatures, end pos, end pos, mf new) concatenates metafeatures mf new / / / / / / with all existing metafeatures up to end-pos."}], "references": [{"title": "Bayesian Language Model Interpolation for Mobile Speech Input", "author": ["Cyril Allauzen", "Michael Riley"], "venue": "Proceedings of Interspeech, 1429\u20131432, 2011.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2011}, {"title": "Bigtable: A distributed storage system for structured data", "author": ["Chang"], "venue": "ACM Transactions on Computer Systems, vol. 26, pp. 1\u201326, num. 2, 2008.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2008}, {"title": "One Billion Word Benchmark for Measuring Progress in Statistical Language Modeling", "author": ["Ciprian Chelba", "Tom\u00e1\u0161 Mikolov", "Mike Schuster", "Qi Ge", "Thorsten Brants", "Phillipp Koehn", "Tony Robinson"], "venue": "Proceedings of Interspeech, 2635\u20132639, 2014.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2014}, {"title": "Sparse Non-negative Matrix Language Modeling for Geo-annotated Query Session Data", "author": ["Ciprian Chelba", "Noam Shazeer"], "venue": "ASRU, to appear, 2015.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2015}, {"title": "Adaptive Subgradient Methods for Online Learning and Stochastic Optimization", "author": ["John Duchi", "Elad Hazan", "Yoram Singer"], "venue": "Journal of Machine Learning Research, 12, 2121\u20132159, 2011.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2011}, {"title": "Small statistical models by random feature mixing", "author": ["Kuzman Ganchev", "Mark Dredze"], "venue": "Proceedings of the ACL-2008 Workshop on Mobile Language Processing, Association for Computational Linguistics, 2008.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2008}, {"title": "MapReduce: Simplified data processing on large clusters", "author": ["Sanjay Ghemawat", "Jeff Dean"], "venue": "Proceedings of OSDI, 2004.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2004}, {"title": "Statistical Methods for Speech Recognition", "author": ["Frederick Jelinek"], "venue": "1997. MIT Press, Cambridge, MA, USA.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 1997}, {"title": "Strategies for training large scale neural network language models", "author": ["Tom\u00e1\u0161 Mikolov", "Anoop Deoras", "Daniel Povey", "Luk\u00e1s Burget", "Jan Cernock\u00fd"], "venue": "Proceedings of ASRU, 196\u2013201, 2011.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2011}, {"title": "Pruning Sparse Non-negative Matrix N-gram Language Models", "author": ["Joris Pelemans", "Noam M. Shazeer", "Ciprian Chelba"], "venue": "Proceedings of Interspeech, 1433\u20131437, 2015.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2015}, {"title": "Sparse Non-negative Matrix Language Modeling For Skip-grams", "author": ["Noam Shazeer", "Joris Pelemans", "Ciprian Chelba. \u201cSkip-gram Language Modeling Using Sparse Non-negative Matrix Probability Estimation", "\u201d CoRR", "abs/1412.1454", "2014. [Online]. Available: http://arxiv.org/abs/1412.1454. [11a] Noam Shazeer", "Joris Pelemans", "Ciprian Chelba"], "venue": "Proceedings of Interspeech, 1428\u20131432, 2015.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2015}, {"title": "Efficient Subsampling for Training Complex Language Models", "author": ["Puyang Xu", "A. Gunawardana", "S. Khudanpur"], "venue": "Proceedings of EMNLP, 2011.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2011}, {"title": "Feature Hashing for Large Scale Multitask Learning", "author": ["Weinberger"], "venue": "Proceedings of the 26th Annual International Conference on Machine Learning, ACM, pp. 1113-1120, 2009. 14", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2009}], "referenceMentions": [{"referenceID": 2, "context": "In experiments on the one billion words language modeling benchmark [3], we are able to slightly improve on previous results reported in [11]-[11a] which uses a different loss function, and employs leave-one-out training on a subset of the main training set.", "startOffset": 68, "endOffset": 71}, {"referenceID": 10, "context": "In experiments on the one billion words language modeling benchmark [3], we are able to slightly improve on previous results reported in [11]-[11a] which uses a different loss function, and employs leave-one-out training on a subset of the main training set.", "startOffset": 137, "endOffset": 141}, {"referenceID": 7, "context": "For an excellent discussion on the use of perplexity in statistical language modeling, as well as various estimates for the entropy of English the reader is referred to [8], Section 8.", "startOffset": 169, "endOffset": 172}, {"referenceID": 3, "context": "A simple extension that leverages context beyond the current sentence, as well as other categorical features such as geo-location is presented and evaluated in [4].", "startOffset": 160, "endOffset": 163}, {"referenceID": 10, "context": "The sparse non-negative matrix (SNM) language model (LM) [11]-[11a] assigns probability to a word by applying the equivalence classification function \u03a6(W ) to the context of the prediction, as explained in the previous section, and then using a matrix M, where Mfw is indexed by feature f \u2208 F and word w \u2208 V .", "startOffset": 57, "endOffset": 61}, {"referenceID": 5, "context": "In order to control the model size we use the hashing technique in [6],[13].", "startOffset": 67, "endOffset": 70}, {"referenceID": 12, "context": "In order to control the model size we use the hashing technique in [6],[13].", "startOffset": 71, "endOffset": 75}, {"referenceID": 4, "context": "Rather than using a single fixed learning rate \u03b7, we use AdaGrad [5] which uses a separate adaptive learning rate \u03b7k,B for each weight \u03b8k,B: \u03b7k,B = \u03b3 \u221a \u22060 + \u2211B b=1 [ \u2211 e\u2208b \u2202 logP (e) \u2202\u03b8k ]2 (7)", "startOffset": 65, "endOffset": 68}, {"referenceID": 1, "context": "The [(f,w)\u21d2Mfw] map is initialized with relative frequencies c(w|f) computed from the training data; on disk they are stored in an SSTable [2] keyed by (f,w), with f and w represented as plain strings.", "startOffset": 139, "endOffset": 142}, {"referenceID": 6, "context": "A MapReduce [7] with two inputs extracts and intersects the features encountered on development data with the features collected on the main training data\u2014where the relative frequencies c(w|f) were also computed.", "startOffset": 12, "endOffset": 15}, {"referenceID": 5, "context": "To control memory usage, we employ a feature hashing technique [6],[13] where we store the meta-feature weights in a flat hash table of predefined size; strings are fingerprinted, counts are hashed and the resulting integer mapped to an index k in \u03b8 by taking its value modulo the pre-defined size(\u03b8).", "startOffset": 63, "endOffset": 66}, {"referenceID": 12, "context": "To control memory usage, we employ a feature hashing technique [6],[13] where we store the meta-feature weights in a flat hash table of predefined size; strings are fingerprinted, counts are hashed and the resulting integer mapped to an index k in \u03b8 by taking its value modulo the pre-defined size(\u03b8).", "startOffset": 67, "endOffset": 71}, {"referenceID": 2, "context": "1 Experiments on the One Billion Words Language Modeling Benchmark Our first experimental setup used the One Billion Word Benchmark corpus2 made available by [3].", "startOffset": 158, "endOffset": 161}, {"referenceID": 10, "context": "We conducted experiments using two feature extraction configurations identical to those used in [11]: 5-gram and skip-10-gram, see Appendix A and B.", "startOffset": 96, "endOffset": 100}, {"referenceID": 10, "context": "In summary, training and evaluating in exactly the same training/test setup as the one in [11] we find that:", "startOffset": 90, "endOffset": 94}, {"referenceID": 10, "context": "8 reported in [11], Table 1, and very close to the Kneser-Ney PPL of 67.", "startOffset": 14, "endOffset": 18}, {"referenceID": 10, "context": "3 reported in [11], Table 3, respectively.", "startOffset": 14, "endOffset": 18}, {"referenceID": 10, "context": "Because of the mismatch between the training and the held-out/test data, the PPL of the un-adjusted SNM 5-gram LM is significantly lower than that of the SNM adjusted using leave-one-out [11] on a subset of the shuffled training set: 710 versus 1285.", "startOffset": 187, "endOffset": 191}, {"referenceID": 10, "context": "The main conclusion is that training the adjustment model on held-out data using multinomial loss introduces many advantages while matching the previous results reported in [11]: as observed in [12], Section 2, using a binary probability model is expected to yield the same model as a multinomial probability model.", "startOffset": 173, "endOffset": 177}, {"referenceID": 11, "context": "The main conclusion is that training the adjustment model on held-out data using multinomial loss introduces many advantages while matching the previous results reported in [11]: as observed in [12], Section 2, using a binary probability model is expected to yield the same model as a multinomial probability model.", "startOffset": 194, "endOffset": 198}, {"referenceID": 10, "context": "Correcting the deficiency in [11] induced by using a Poisson model for each binary random variable does not seem to make a difference in the quality of the estimated model.", "startOffset": 29, "endOffset": 33}, {"referenceID": 0, "context": "\u2022 ability to mix various data sources based on how relevant they are to a given held-out set, thus providing an alternative to Bayesian mixing algorithms such as [1],", "startOffset": 162, "endOffset": 165}, {"referenceID": 9, "context": "\u2022 excellent pruning properties relative to entropy pruning of Katz and Kneser-Ney models [10],", "startOffset": 89, "endOffset": 93}, {"referenceID": 9, "context": "\u2022 conversion to standard ARPA back-off format [10],", "startOffset": 46, "endOffset": 50}, {"referenceID": 3, "context": "\u2022 effortless incorporation of richer features such as skip-n-grams and geo-tags [4],", "startOffset": 80, "endOffset": 83}], "year": 2016, "abstractText": "We describe Sparse Non-negative Matrix (SNM) language model estimation using multinomial loss on held-out data. Being able to train on held-out data is important in practical situations where the training data is usually mismatched from the held-out/test data. It is also less constrained than the previous training algorithm using leave-one-out on training data: it allows the use of richer meta-features in the adjustment model, e.g. the diversity counts used by Kneser-Ney smoothing which would be difficult to deal with correctly in leave-one-out training. In experiments on the one billion words language modeling benchmark [3], we are able to slightly improve on previous results reported in [11]-[11a] which uses a different loss function, and employs leave-one-out training on a subset of the main training set. Surprisingly, an adjustment model with meta-features that discard all lexical information can perform as well as lexicalized meta-features. We find that fairly small amounts of held-out data (on the order of 30-70 thousand words) are sufficient for training the adjustment model. In a real-life scenario where the training data is a mix of data sources that are imbalanced in size, and of different degrees of relevance to the held-out and test data, taking into account the data source for a given skip-/n-gram feature and combining them for best performance on held-out/test data improves over skip-/n-gram SNM models trained on pooled data by about 8% in the SMT setup, or as much as 15% in the ASR/IME setup. The ability to mix various data sources based on how relevant they are to a mismatched held-out set is probably the most attractive feature of the new estimation method for SNM LM.", "creator": "LaTeX with hyperref package"}}}