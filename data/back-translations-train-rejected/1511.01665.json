{"id": "1511.01665", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "5-Nov-2015", "title": "An Empirical Study on Sentiment Classification of Chinese Review using Word Embedding", "abstract": "In this article, how word embeddings can be used as features in Chinese sentiment classification is presented. Firstly, a Chinese opinion corpus is built with a million comments from hotel review websites. Then the word embeddings which represent each comment are used as input in different machine learning methods for sentiment classification, including SVM, Logistic Regression, Convolutional Neural Network (CNN) and ensemble methods. These methods get better performance compared with N-gram models using Naive Bayes (NB) and Maximum Entropy (ME). Finally, a combination of machine learning methods is proposed which presents an outstanding performance in precision, recall and F1 score. After selecting the most useful methods to construct the combinational model and testing over the corpus, the final F1 score is 0.920.", "histories": [["v1", "Thu, 5 Nov 2015 09:25:21 GMT  (535kb,D)", "http://arxiv.org/abs/1511.01665v1", "The 29th Pacific Asia Conference on Language, Information and Computing"]], "COMMENTS": "The 29th Pacific Asia Conference on Language, Information and Computing", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["yiou lin", "hang lei", "jia wu", "xiaoyu li"], "accepted": false, "id": "1511.01665"}, "pdf": {"name": "1511.01665.pdf", "metadata": {"source": "CRF", "title": "An Empirical Study on Sentiment Classification of Chinese Review using Word Embedding", "authors": ["Yiou Lin", "Hang Lei", "Jia Wu", "Xiaoyu Li"], "emails": ["lyoshiwo@gmail.com", "xiaoyuuestc}@uestc.edu.cn"], "sections": [{"heading": "1 Introduction", "text": "Sentiment analysis or opinion mining is the computational study of people's opinions, appraisals, attitudes, and emotion towards entities, individuals, issues, events, themes and their attributes (Liu and Zhang, 2012). The task of sentiment analysis is technically challenging and practically very useful. For example, companies always want to find public or consumer opinions about their products and services. Consumers also need a sounding board rather than thinking alone while making decisions. With the development of the Internet, opinionated texts from social media (e.g. reviews, blogs, and microblogs) are often used for decision making, making automated sentiment analysis techniques increasingly important. These tasks of sentiment analysis include classifying the polarity of certain texts. Much work has been done in recent years to improve the English polarity classification. There are two categories of such work. One is referred to as \"machine learning,\" which is first proposed to determine whether a review is positive or negative by classifying the English polarity classification, including SVME and SVME-65P classes."}, {"heading": "2 Related work", "text": "According to Liu and Zhang (2012), sentiment analysis research mainly began in early 2000 by Turney (2002) and Pang et al. (2002). Turney (2002) initially used a few semantic words (e.g., excellent and bad) to mark other phrases with search engine queries, and then the researchers also proposed several custom techniques specifically for sentiment classification, such as the score function, which relies on words in positive and negative ratings (Dave et al., 2003) and feature weighting schemes that are used to improve classification accuracy (Paltoglou and Thelwall, 2010). Also, the other situation of entity analysis is to represent texts by vectors that appear these words in the text but do not preserve the word order. And, a machine learning approach is ultimately used for classification. In such a way, Pang et al al. (2002) consider the technique of classifying documents by default learning entification by making them more technical."}, {"heading": "3 Methodology", "text": "This section presents the methodology used in our experiment."}, {"heading": "3.1 Feature selection methods", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "3.1.1 Sentiment lexicon and CHI", "text": "A sentiment lexicon that takes sentiment words into account plays an important role in sentiment analysis, resulting in a combination of two Chinese sentiment lexicographs (Hownet (Dong and Dong, 2006) and DLLEX (Xu et al., 2008), including a total of 30406 words. After removing the words that do not appear in the corpus, 10444 sentiment words are preserved. After several experiments, CHI (Galavotti et al., 2000) is selected to gather information. Finally, 150 most valuable words are included in the new lexicon."}, {"heading": "3.1.2 Word2vec", "text": "Word2vec (Mikolov et al., 2013a) has gained traction today. As its name indicates, it translates words into vectors called word embeddings, i.e. it maintains the vector representations of words. Gensim1, a python tool, is used to obtain the word2vec module. The method for creating the word2vec model is unattended learning and 300 is set as quantity1http: / radimrehurek.com / gensim / of the vector dimension. Table 1 shows the word embeddings of a Chinese hotel rating, which means that the room is very clean and tidy. For convenient representation, each value of the dimension is multiplied by 10,000 and given by di (i = 1,..., 300)."}, {"heading": "3.2 Traditional methods", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "3.2.1 Naive Bayes Classification", "text": "Naive Bayes (NB) is often used in the sentiment classification, which is used to assign a specific overview document d to class c * = argmaxcP (c | d). According to Bayes \"rule, P (cj | d) = P (cj) P (d | cj) P (d), where cj is a kind of class and P (d) plays no role in selecting c *. Let's mark f1, f2,... fm as the feature set that occurs in all reviews, and let's set ni (d) as the number of times fi appears in d. Usually, ni (d) is set to 1 if fi appears more than once. Then, a formulation can be rounded off as P (cj | d) = P (cj)."}, {"heading": "3.2.2 Maximum Entropy Classification", "text": "The maximum entropy classification follows the principle of maximum entropy (Jaynes, 1957), which means that, subject to precisely specified prior data (e.g. a set expressing verifiable information), the probability distribution that best represents the current state of knowledge is the one with the greatest entropy. Thus, the estimate of P (cj | d) is indicated as follows: P (cj | d) = 1\u03c0 (d) exp (\u2211 m i = 1 \u03bbi, cjFi, cj (d, cj))) Fi, cj (d, x) = {1 if ni > 0 and x = cj 0 elsewhere \u03c0 (d) is a normalization function, and \u03bbi, cj is the weight of fj in the maximum entropy cj. The other parameters are defined in the same way as Section 3.2.1. After fifteen iterations of the improved scaling algorithm, the parameters are adjusted to Bird Toxial (Piexal), the Bird Toxial (1997), the parameters are paramended."}, {"heading": "3.2.3 Support Vector Machines", "text": "Support Vector Machines (SVM) is a very effective method of machine learning, first introduced by (Cortes and Vapnik, 1995). SVM constructs a hyperplane or series of hyperplanes in a high-dimensional space represented by ~ w. The larger the margin, the smaller the error of the classifier after the training, the greater the distance of the support vector to the nearest training data point in each class. Then the problem of maximizing the margin in argmin ~ w, b1 2 | | w | | 2where yi (~ w \u00b7 xi \u2212 b) \u2265 1 and its unlimited dual form presents itself as the following optimization problem: maximizing the L-kit (\u03b1), whereas L-Kit (\u03b1) = n-code i \u2212 1 \u2212 2 \u0445 i, j-jyiyjk (xi, xj) = n-jyiyjk (xi, xj-jn = 1), jn-jn-jjj = xi, xxi = 1X-T."}, {"heading": "3.3 Ensemble methods", "text": "Ensemble methods (Dietterich, 2000; Friedman, 2001; Ridgeway, 2007) are supervised learning algorithms that typically combine multiple hypotheses to form a better one. There are two families of ensemble methods, averages and boosting methods. In averaging methods, several estimators are built to make their predictions average. It is a kind of coordination, on average. The combined estimator is usually better than any of the basic estimators because its variance is reduced (e.g. dredging methods and forests of randomized trees). In contrast, boosting methods build fundamental estimators sequentially and each tries to reduce the bias of the combined estimator. The idea behind this is to combine several weak models to generate a more powerful ensemble model (e.g. AdaBoost and Gradient Tree Boosting). The modules of the ensemble method are selected from kit-learning, tree-boosting and edge-boosting."}, {"heading": "3.4 CNN methods", "text": "CNN is an abbreviation for Convolutional Neural Networks. Its key module is to calculate the folding between input and output. Just as CNN is used in computer vision, a matrix is needed as CNN's input. After several experiments, we set D = 60 as the dimensional set of word embeddings for CNN. If there are L-words in a sentence, we combine their word embeddings to construct a matrix of size L x D, as specified in Figure 1. L = 60, since fixed L is needed, and this means that only 60 words are preserved from the beginning of a verification. On the other hand, if the length of word embeddings is less than 60, the matrix is filled with used vectors by repeating them. At the end, each review is seen through a matrix of size 60 \u00d7 60. Formally, in computer vision, n images (Xl, l = 1, n) of size r \u00d7 c, kernel size xx."}, {"heading": "4 Experiment results", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.1 Corpuses", "text": "Unlike English corpus reviews, Chinese corpses are relatively small and tend to focus on POS reviews (Mingqin et al., 2003), analysis (Xue et al., 2005) and translation (Xiao, 2010). In the Chinese sentiment classification, the most popular corpus is ChnSentiCorp (Tan and Zhang, 2008) with 7,000 positive reviews and 3,000 negative reviews. Since the amount of data collected by previous Chinese NLP researchers is too small for our work, we form a new corpus, MioChnCorp, with one million Chinese hotel reviews. The corpus is public and can be downloaded directly. 6 The reviews are filtered and filtered by the Website7, which have coarse-grained reviews (5-star scale) for each rating. We post the 3-star reviews, which can be ambiguous, and highlight the 5-star reviews on Bayang.7 The last ratings are all the same."}, {"heading": "4.2 The performance measure", "text": "The F1 score (also known as the F measure) is a measure of the accuracy of a test that combines memory and precision as follows: F1 = 2 \u00b7 Precision \u00b7 Recall Precision + RecallRecall = Correct Positive Predictions Amount = Correct Positive Predictions Amount Positive Predictions Amount Since MioChnCorp has two categories (positive and negative), Macro F1 is used to evaluate the performance of the classification method over the CorpusMacro F1 = Positive F1 + Negative F12 in the rest of this article."}, {"heading": "4.3 Experimental design", "text": "NB and ME use 10543 words (the sentimental words and CHI words) LinearSVC use unique and bigrams. Five methods (SVC, LR, AdaBoost, Gradient Tree Boosting (GBT) and Random Forests (RF)) use the average vectors of word embeddings and CHI words (expanding the dimension size to 450). CNN uses the matrix constructed by word embeddings from words in a review as a feature. Although all of these models are effective, the combination of different machine learning methods is intended to achieve a better F1 score. There are two ways to combine these methods: first, the idea is simple, \"the minority is subordinate to the majority\" (marked as the voice of all), and the other way is to exceed validation in the validation set. Add another number for validation into these tree folds."}, {"heading": "4.4 Comparison and analysis", "text": "Table 3, Table 4 and Table 5 show the performance of the various methods of machine learning. Subjected to hardware recurrence (RAM: 8G = ME i Storing) There are enough resource reviews (RAM: 8G, CPU: Intel I5, GPU: GTX960M), the experiments are explored via tree-sized corpses: 40,000, 80,000 and 120,000. Each corpus is divided into four folds: Output: For each classifier Ci, Train Model of the Combination Model Input: The experimental set of designated samples: Train Set, Validation Set and Storing Predication; n Machine Learning Attribute Classifier (labeled Ci, i = 1, n) with standard parameters Output Parameters Output: For each classifier Ci, Train Model Set and Storing Predication as a validation list i; 2: Use logistic regression to determine the validation attributes based on the laboratory attributes."}, {"heading": "5 Conclusion and Future Work", "text": "To conduct this experiment, a Chinese corpus, MioChnCorp10, is collected with one million Chinese hotel reviews. MioChnCorp trains a word2vec model to represent distributed representations of words and phrases in Chinese hotel reviews, and the experimental results then suggest that the more data we use, the better performance we achieve. And 60,000 or more reviews (e.g. 120,000) of reviews are sufficient for sentiment analysis of Chinese hotel reviews. In addition, we use word embedding as input features without sentiment lexicographs and find such features good by using the Ensemble10http: / / pan.baidu.com / s / 1dDo9s8hmethods, LR, SVM and CNN. In terms of these learning methods, SVM works best. Although CNN does not perform as well as expected, it still performs better than NB and ME. The roles we use for constructing the F1 model is an excellent Chinese exploration method."}, {"heading": "6 Acknowledgements", "text": "Financial support for this work is provided by the Basic Research Fund for Central Universities, No. ZYGX2014J065."}], "references": [{"title": "Nltk: the natural language toolkit", "author": ["Steven Bird"], "venue": "In Proceedings of the COLING/ACL on Interactive presentation sessions,", "citeRegEx": "Bird.,? \\Q2006\\E", "shortCiteRegEx": "Bird.", "year": 2006}, {"title": "Natural language processing (almost) from scratch", "author": ["Jason Weston", "L\u00e9on Bottou", "Michael Karlen", "Koray Kavukcuoglu", "Pavel Kuksa"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "Collobert et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Collobert et al\\.", "year": 2011}, {"title": "Mining the peanut gallery: Opinion extraction and semantic classification of product reviews", "author": ["Dave et al.2003] Kushal Dave", "Steve Lawrence", "David M Pennock"], "venue": "In Proceedings of the 12th international conference on World Wide Web,", "citeRegEx": "Dave et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Dave et al\\.", "year": 2003}, {"title": "Ensemble methods in machine learning", "author": ["Thomas G Dietterich"], "venue": "In Multiple classifier systems,", "citeRegEx": "Dietterich.,? \\Q2000\\E", "shortCiteRegEx": "Dietterich.", "year": 2000}, {"title": "HowNet and the Computation of Meaning", "author": ["Dong", "Dong2006] Zhendong Dong", "Qiang Dong"], "venue": "World Scientific", "citeRegEx": "Dong et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Dong et al\\.", "year": 2006}, {"title": "Greedy function approximation: a gradient boosting machine", "author": ["Jerome H Friedman"], "venue": "Annals of statistics,", "citeRegEx": "Friedman.,? \\Q2001\\E", "shortCiteRegEx": "Friedman.", "year": 2001}, {"title": "Feature selection and negative evidence in automated text categorization", "author": ["Fabrizio Sebastiani", "Maria Simi"], "venue": "In Proceedings of KDD", "citeRegEx": "Galavotti et al\\.,? \\Q2000\\E", "shortCiteRegEx": "Galavotti et al\\.", "year": 2000}, {"title": "Effective use of word order for text categorization with convolutional neural networks. arXiv preprint arXiv:1412.1058", "author": ["Johnson", "Zhang2014] Rie Johnson", "Tong Zhang"], "venue": null, "citeRegEx": "Johnson et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Johnson et al\\.", "year": 2014}, {"title": "A convolutional neural network for modelling sentences", "author": ["Edward Grefenstette", "Phil Blunsom"], "venue": "arXiv preprint arXiv:1404.2188", "citeRegEx": "Kalchbrenner et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Kalchbrenner et al\\.", "year": 2014}, {"title": "Gradientbased learning applied to document recognition", "author": ["LeCun et al.1998] Yann LeCun", "L\u00e9on Bottou", "Yoshua Bengio", "Patrick Haffner"], "venue": "Proceedings of the IEEE,", "citeRegEx": "LeCun et al\\.,? \\Q1998\\E", "shortCiteRegEx": "LeCun et al\\.", "year": 1998}, {"title": "Experimental study on sentiment classification of chinese review using machine learning techniques", "author": ["Li", "Sun2007] Jun Li", "Maosong Sun"], "venue": "In Natural Language Processing and Knowledge Engineering,", "citeRegEx": "Li et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Li et al\\.", "year": 2007}, {"title": "A survey of opinion mining and sentiment analysis", "author": ["Liu", "Zhang2012] Bing Liu", "Lei Zhang"], "venue": "In Mining text data,", "citeRegEx": "Liu et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Liu et al\\.", "year": 2012}, {"title": "Distributed representations of words and phrases and their compositionality", "author": ["Ilya Sutskever", "Kai Chen", "Greg S Corrado", "Jeff Dean"], "venue": "In Advances in neural information processing systems,", "citeRegEx": "Mikolov et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Linguistic regularities in continuous space word representations", "author": ["Wen-tau Yih", "Geoffrey Zweig"], "venue": "In HLTNAACL,", "citeRegEx": "Mikolov et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Building a large chinese corpus annotated with semantic dependency", "author": ["Mingqin et al.2003] Li Mingqin", "Li Juanzi", "Dong Zhendong", "Wang Zuoying", "Lu Dajin"], "venue": "In Proceedings of the second SIGHAN workshop on Chinese language processing-Volume", "citeRegEx": "Mingqin et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Mingqin et al\\.", "year": 2003}, {"title": "A study of information retrieval weighting schemes for sentiment analysis", "author": ["Paltoglou", "Thelwall2010] Georgios Paltoglou", "Mike Thelwall"], "venue": "In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics,", "citeRegEx": "Paltoglou et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Paltoglou et al\\.", "year": 2010}, {"title": "Opinion mining and sentiment analysis", "author": ["Pang", "Lee2008] Bo Pang", "Lillian Lee"], "venue": "Foundations and trends in information retrieval,", "citeRegEx": "Pang et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Pang et al\\.", "year": 2008}, {"title": "Thumbs up?: sentiment classification using machine learning techniques", "author": ["Pang et al.2002] Bo Pang", "Lillian Lee", "Shivakumar Vaithyanathan"], "venue": "In Proceedings of the ACL-02 conference on Empirical methods in natural language processing-Volume", "citeRegEx": "Pang et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Pang et al\\.", "year": 2002}, {"title": "Scikit-learn: Machine learning in python", "author": ["Ga\u00ebl Varoquaux", "Alexandre Gramfort", "Vincent Michel", "Bertrand Thirion", "Olivier Grisel", "Mathieu Blondel", "Peter Prettenhofer", "Ron Weiss", "Vincent Dubourg"], "venue": null, "citeRegEx": "Pedregosa et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Pedregosa et al\\.", "year": 2011}, {"title": "Inducing features of random fields", "author": ["Vincent Della Pietra", "John Lafferty"], "venue": "Pattern Analysis and Machine Intelligence, IEEE Transactions", "citeRegEx": "Pietra et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Pietra et al\\.", "year": 1997}, {"title": "Generalized boosted models: A guide to the gbm", "author": ["Greg Ridgeway"], "venue": "package. Update,", "citeRegEx": "Ridgeway.,? \\Q2007\\E", "shortCiteRegEx": "Ridgeway.", "year": 2007}, {"title": "An empirical study of sentiment analysis for chinese documents", "author": ["Tan", "Zhang2008] Songbo Tan", "Jin Zhang"], "venue": "Expert Systems with Applications,", "citeRegEx": "Tan et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Tan et al\\.", "year": 2008}, {"title": "Thumbs up or thumbs down?: semantic orientation applied to unsupervised classification of reviews", "author": ["Peter D Turney"], "venue": "In Proceedings of the 40th annual meeting on association for computational linguistics,", "citeRegEx": "Turney.,? \\Q2002\\E", "shortCiteRegEx": "Turney.", "year": 2002}, {"title": "How different is translated chinese from native chinese? a corpus-based study of translation universals", "author": ["Richard Xiao"], "venue": "International Journal of Corpus Linguistics,", "citeRegEx": "Xiao.,? \\Q2010\\E", "shortCiteRegEx": "Xiao.", "year": 2010}, {"title": "Convolutional neural network based triangular crf for joint intent detection and slot filling", "author": ["Xu", "Sarikaya2013] Puyang Xu", "Ruhi Sarikaya"], "venue": "In Automatic Speech Recognition and Understanding (ASRU),", "citeRegEx": "Xu et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Xu et al\\.", "year": 2013}, {"title": "Construction and analysis of emotional corpus", "author": ["Xu et al.2008] LH Xu", "HF Lin", "Jing Zhao"], "venue": "Journal of Chinese information processing,", "citeRegEx": "Xu et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Xu et al\\.", "year": 2008}, {"title": "The penn chinese treebank: Phrase structure annotation of a large corpus", "author": ["Xue et al.2005] Naiwen Xue", "Fei Xia", "Fu-Dong Chiou", "Marta Palmer"], "venue": "Natural language engineering,", "citeRegEx": "Xue et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Xue et al\\.", "year": 2005}, {"title": "Sentiment classification for chinese reviews: A comparison between svm and semantic approaches", "author": ["Ye et al.2005] Qiang Ye", "Bin Lin", "Yi-Jun Li"], "venue": "In Machine Learning and Cybernetics,", "citeRegEx": "Ye et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Ye et al\\.", "year": 2005}, {"title": "Exploiting effective features for chinese sentiment classification", "author": ["Zhai et al.2011] Zhongwu Zhai", "Hua Xu", "Bada Kang", "Peifa Jia"], "venue": "Expert Systems with Applications,", "citeRegEx": "Zhai et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Zhai et al\\.", "year": 2011}], "referenceMentions": [{"referenceID": 17, "context": "learning\u201d which is firstly proposed to determine whether a review is positive or negative by using three machine learning methods, including NB, ME and SVM (Pang et al., 2002).", "startOffset": 156, "endOffset": 175}, {"referenceID": 22, "context": "And an overall score is calculated to assign the review to a specific class (Turney, 2002).", "startOffset": 76, "endOffset": 90}, {"referenceID": 20, "context": "According to Liu and Zhang (2012), the sentiment analysis research mainly started from early 2000 by Turney (2002) and Pang et al.", "startOffset": 101, "endOffset": 115}, {"referenceID": 16, "context": "According to Liu and Zhang (2012), the sentiment analysis research mainly started from early 2000 by Turney (2002) and Pang et al. (2002). Turney", "startOffset": 119, "endOffset": 138}, {"referenceID": 2, "context": "the score function based on words in positive and negative reviews (Dave et al., 2003) and feature weighting schemes used to enhance classification accuracy (Paltoglou and Thelwall, 2010).", "startOffset": 67, "endOffset": 86}, {"referenceID": 16, "context": "In such way, Pang et al. (2002) considered classifying documents according to standard machine learning techniques.", "startOffset": 13, "endOffset": 32}, {"referenceID": 27, "context": "However, compared to English sentiment analysis, there are relatively few investigations conducted on Chinese sentiment classification until 2005 (Ye et al., 2005).", "startOffset": 146, "endOffset": 163}, {"referenceID": 27, "context": "However, compared to English sentiment analysis, there are relatively few investigations conducted on Chinese sentiment classification until 2005 (Ye et al., 2005). Li and Sun (2007) presented a study on comparison of different machine learning approaches under different text representation schemes and feature weighting schemes.", "startOffset": 147, "endOffset": 183}, {"referenceID": 27, "context": "However, compared to English sentiment analysis, there are relatively few investigations conducted on Chinese sentiment classification until 2005 (Ye et al., 2005). Li and Sun (2007) presented a study on comparison of different machine learning approaches under different text representation schemes and feature weighting schemes. They found that SVM achieved the best performance. After that, Tan and Zhang (2008) found 6,000 or bigger for the size of features would be sufficient for Chinese", "startOffset": 147, "endOffset": 415}, {"referenceID": 1, "context": ", Collobert et al. (2011), Johnson and Zhang (2014)) deviated from traditional methods and tried to train neural networks such as Convolutional Neural Networks (CNN) for NLP tasks (e.", "startOffset": 2, "endOffset": 26}, {"referenceID": 1, "context": ", Collobert et al. (2011), Johnson and Zhang (2014)) deviated from traditional methods and tried to train neural networks such as Convolutional Neural Networks (CNN) for NLP tasks (e.", "startOffset": 2, "endOffset": 52}, {"referenceID": 1, "context": ", Collobert et al. (2011), Johnson and Zhang (2014)) deviated from traditional methods and tried to train neural networks such as Convolutional Neural Networks (CNN) for NLP tasks (e.g., named entity recognition and sentiment analysis). Among them, Xu and Sarikaya (2013) and Kalchbrenner et al.", "startOffset": 2, "endOffset": 272}, {"referenceID": 1, "context": ", Collobert et al. (2011), Johnson and Zhang (2014)) deviated from traditional methods and tried to train neural networks such as Convolutional Neural Networks (CNN) for NLP tasks (e.g., named entity recognition and sentiment analysis). Among them, Xu and Sarikaya (2013) and Kalchbrenner et al. (2014) got some state-of-the-art performance.", "startOffset": 2, "endOffset": 303}, {"referenceID": 1, "context": ", Collobert et al. (2011), Johnson and Zhang (2014)) deviated from traditional methods and tried to train neural networks such as Convolutional Neural Networks (CNN) for NLP tasks (e.g., named entity recognition and sentiment analysis). Among them, Xu and Sarikaya (2013) and Kalchbrenner et al. (2014) got some state-of-the-art performance. But the work of Collobert et al. (2011) was paid most attention for describing a unified architecture for NLP tasks which learned features by training a deep neural network even when being given very limited prior knowledge.", "startOffset": 2, "endOffset": 382}, {"referenceID": 25, "context": "A combination of two Chinese sentiment lexicons (Hownet (Dong and Dong, 2006) and DLLEX (Xu et al., 2008)) is constructed, including 30406 words in total.", "startOffset": 88, "endOffset": 105}, {"referenceID": 6, "context": "After several experiments, CHI (Galavotti et al., 2000) is chosen for information gain.", "startOffset": 31, "endOffset": 55}, {"referenceID": 19, "context": "After fifteen iterations of the improved iterative scaling algorithm (Pietra et al., 1997) implemented in Natural Language Toolkit (Bird, 2006), the parameters of \u03bbi,cj are adjusted to maximize the entropy of distribution of training data.", "startOffset": 69, "endOffset": 90}, {"referenceID": 0, "context": ", 1997) implemented in Natural Language Toolkit (Bird, 2006), the parameters of \u03bbi,cj are adjusted to maximize the entropy of distribution of training data.", "startOffset": 48, "endOffset": 60}, {"referenceID": 18, "context": "For SVM models, python tool scikit-learn (Pedregosa et al., 2011) is chosen for training and testing.", "startOffset": 41, "endOffset": 65}, {"referenceID": 3, "context": "Ensemble methods (Dietterich, 2000; Friedman, 2001; Ridgeway, 2007) are supervised learning algorithm which commonly combine multiple hypotheses to form a better one.", "startOffset": 17, "endOffset": 67}, {"referenceID": 5, "context": "Ensemble methods (Dietterich, 2000; Friedman, 2001; Ridgeway, 2007) are supervised learning algorithm which commonly combine multiple hypotheses to form a better one.", "startOffset": 17, "endOffset": 67}, {"referenceID": 20, "context": "Ensemble methods (Dietterich, 2000; Friedman, 2001; Ridgeway, 2007) are supervised learning algorithm which commonly combine multiple hypotheses to form a better one.", "startOffset": 17, "endOffset": 67}, {"referenceID": 9, "context": "Extending the implementation 4 of the lenet5 (LeCun et al., 1998), the convolutional layer", "startOffset": 45, "endOffset": 65}, {"referenceID": 14, "context": "atively small and usually focus on POS tagging (Mingqin et al., 2003), parsing (Xue et al.", "startOffset": 47, "endOffset": 69}, {"referenceID": 26, "context": ", 2003), parsing (Xue et al., 2005) and translating (Xiao, 2010).", "startOffset": 17, "endOffset": 35}, {"referenceID": 23, "context": ", 2005) and translating (Xiao, 2010).", "startOffset": 24, "endOffset": 36}, {"referenceID": 28, "context": "Zhai et al. (2011) tried to get a believable result using the average value from 30", "startOffset": 0, "endOffset": 19}], "year": 2015, "abstractText": "In this article, how word embeddings can be used as features in Chinese sentiment classification is presented. Firstly, a Chinese opinion corpus is built with a million comments from hotel review websites. Then the word embeddings which represent each comment are used as input in different machine learning methods for sentiment classification, including SVM, Logistic Regression, Convolutional Neural Network (CNN) and ensemble methods. These methods get better performance compared with N-gram models using Naive Bayes (NB) and Maximum Entropy (ME). Finally, a combination of machine learning methods is proposed which presents an outstanding performance in precision, recall and F1 score. After selecting the most useful methods to construct the combinational model and testing over the corpus, the final F1 score is 0.920.", "creator": "LaTeX with hyperref package"}}}