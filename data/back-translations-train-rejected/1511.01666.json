{"id": "1511.01666", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "5-Nov-2015", "title": "Comparing Writing Styles using Word Embedding and Dynamic Time Warping", "abstract": "The development of plot or story in novels is reflected in the content and the words used. The flow of sentiments, which is one aspect of writing style, can be quantified by analyzing the flow of words. This study explores literary works as signals in word embedding space and tries to compare writing styles of popular classic novels using dynamic time warping.", "histories": [["v1", "Thu, 5 Nov 2015 09:25:41 GMT  (322kb,D)", "http://arxiv.org/abs/1511.01666v1", null]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["abhinav tushar", "abhinav dahiya"], "accepted": false, "id": "1511.01666"}, "pdf": {"name": "1511.01666.pdf", "metadata": {"source": "CRF", "title": "Comparing Writing Styles using Word Embedding and Dynamic Time Warping Comparing Writing Styles using Word Embedding and Dynamic Time Warping", "authors": ["Abhinav Tushar", "Abhinav Dahiya"], "emails": ["a)abhinav.tushar.vs@gmail.com", "b)iit.abhinav.dahiya@gmail.com"], "sections": [{"heading": null, "text": "The flow of emotions, which is an aspect of writing style, can be quantified by analyzing the flow of words. I. INTRODUCTIONThe writing style of a novel is based on many factors, including the author's personal style and the nature of the novel itself, and thrillers by the same author tend to resemble each other in the way the story develops. Apart from the specific factual details, literary works tend to have a defined statistical flow of time that defines the subjective feeling of work as a whole, and this flow can be compared with the classical watering down and analysis of series of texts by comparing word processing models."}, {"heading": "II. BACKGROUND", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "A. Word Embedding", "text": "Word embedding refers to techniques that make it possible to represent words in vectors of real numbers in a continuous space.The vector representation can be learned with many techniques and is based on the use of the information based on the context, a word is available.We are using a model proposed by Mikolov et al. (2013a, b) that uses Continuous Bag-of-Words and Skip-Gram model to learn these continuous representations.a) abhinav.tushar.vs @ gmail.com b) iit.abhinav.dahiya @ gmail.com. Continuous Bag-of-Words model (Figure 1) tries out a context of the size defined by c from text by a word and tries to learn a projection into vector space that correctly predicts the middle word. The summary of words represented as hot vectors w (i), where c of the text is defined by w (w), w (w), w (w), w (w) \u2212 1."}, {"heading": "B. Dynamic Time Warping", "text": "Dynamic Time Distortion (DTW) is a technique for measuring the similarity between two time series. DTW treats the difference in speed and time between signals and has been used in applications such as voice recognition and signature matching, where the difference in signal velocity should not affect the final result. It is based on optimal match and the algorithm will output a value corresponding to the separation of the two time series. A major advantage of DTW is that the two series need not be of equal length, which is the case here, since two different books will have different number of words and thus different length. The aim of this algorithm is to calculate a distance measurement for a given pair of time series that can represent the similarity / dissimilarity between these two series by determining a path W that minimizes the cumulative euclidean distance between the elements of the two series."}, {"heading": "III. APPROACH", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "A. Novels to Signal", "text": "Our increasing approach to converting a novel into a signal uses sentence vectors by taking the mean of word vectors in a sentence. A word embedding model is trained using 1000 free e-books from the Gutenberg project 1. The model is done using Word2Vec2.Training with a feature size (word vector) of 100 and a context window size of 10 words. Once trained, a novel with Nw words leads to a 100-dimensional time series of length Nw.To reduce the dimension of data word vectors in a sentence, they are averaged, resulting in a time series matrix of the form Ns \u00d7 100, where Ns is the number of sentences in the novel. To reduce the line dimensions, we find cosine similarity of each line with few anchor points in the word space. This provides a measure of the movement of the signal contained in the entire vocabulary filter Ns \u00d7 100, where Ns is the number of sentences in the novel."}, {"heading": "B. Comparing Signals", "text": "Once generated, the signals are compared with FastDTW (Salvador and Chan, 2004), which implements a faster version of the DTW algorithm. Vanilla DTW requires O (n2) calculation, while FastDTW calculates in linear time. We use 24 classic novels collected from the Gutenberg project (listed in Table I) and generate a distance matrix for the articles. A scatter plot of books using multi-dimensional scaling is shown in Figure 5. Although the similarity measurement itself is a useful benchmark, the scatter plot also groups authors according to font styles and confirms the hypothesis that comparing the flow in the time series of word vectors can have subjective predictions."}, {"heading": "V. CONCLUSIONS AND FUTURE WORKS", "text": "This study provides an opportunity to explore literary works as signals in the text embedding space. Author clusters formed as a result of the analysis provide encouraging support for this method as a text analysis technique at the highest level. A more accurate study can be carried out by building on the current method to identify the main components involved in shaping the overall image of a book while not knowing factual details. Text representation can be improved by paragraph (sentence) vectors instead of word vectors. Anchor points can be improved due to the effects and number of points. Frequency-based analysis of signals can provide a better insight."}], "references": [{"title": "Distributed Representations of Words and Phrases and their Compositionality", "author": ["T. Mikolov", "K. Chen", "G. Corrado", "J. Dean"], "venue": "NIPS, Pp. 1\u20139.", "citeRegEx": "Mikolov et al\\.,? 2013a", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Efficient Estimation of Word Representations in Vector Space", "author": ["T. Mikolov", "G. Corrado", "K. Chen", "J. Dean"], "venue": "Proceedings of the International Conference on Learning Representations (ICLR 2013), Pp. 1\u201312.", "citeRegEx": "Mikolov et al\\.,? 2013b", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "FastDTW: Toward accurate dynamic time warping in linear time and space", "author": ["S. Salvador", "P. Chan"], "venue": "KDD Workshop on Mining Temporal and Sequential Data, Pp. 70\u201380.", "citeRegEx": "Salvador and Chan,? 2004", "shortCiteRegEx": "Salvador and Chan", "year": 2004}], "referenceMentions": [{"referenceID": 2, "context": "Once generated, the signals are compared using FastDTW (Salvador and Chan, 2004) which implements a faster version of DTW algorithm.", "startOffset": 55, "endOffset": 80}], "year": 2015, "abstractText": "This flow can be quantified and compared by analyzing the text using natural language processing techniques. This study uses word embedding models to generate time series for novels and then compare the resulting series using dynamic time warping to find similarities. Considering time series analysis rather than a pure statistical one can capture the flow of the works and thus the similarities generated will provide a metric for comparing the novels based on the progression.", "creator": "LaTeX with hyperref package"}}}