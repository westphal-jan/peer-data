{"id": "1511.01754", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "5-Nov-2015", "title": "Symmetry-invariant optimization in deep networks", "abstract": "Recent works have highlighted scale invariance or symmetry that is present in the weight space of a typical deep network and the adverse effect that it has on the Euclidean gradient based stochastic gradient descent optimization. In this work, we show that these and other commonly used deep networks, such as those which use a max-pooling and sub-sampling layer, possess more complex forms of symmetry arising from scaling based reparameterization of the network weights. We then propose two symmetry-invariant gradient based weight updates for stochastic gradient descent based learning. Our empirical evidence based on the MNIST dataset shows that these updates improve the test performance without sacrificing the computational efficiency of the weight updates. We also show the results of training with one of the proposed weight updates on an image segmentation problem.", "histories": [["v1", "Thu, 5 Nov 2015 14:17:40 GMT  (1798kb,D)", "https://arxiv.org/abs/1511.01754v1", "Submitted to ICLR 2016. arXiv admin note: text overlap witharXiv:1511.01029"], ["v2", "Sat, 7 Nov 2015 19:01:03 GMT  (1798kb,D)", "http://arxiv.org/abs/1511.01754v2", "Submitted to ICLR 2016. arXiv admin note: text overlap witharXiv:1511.01029"]], "COMMENTS": "Submitted to ICLR 2016. arXiv admin note: text overlap witharXiv:1511.01029", "reviews": [], "SUBJECTS": "cs.LG cs.AI cs.CV", "authors": ["vijay badrinarayanan", "bamdev mishra", "roberto cipolla"], "accepted": false, "id": "1511.01754"}, "pdf": {"name": "1511.01754.pdf", "metadata": {"source": "CRF", "title": null, "authors": ["Vijay Badrinarayanan", "Bamdev Mishra"], "emails": ["vb292@cam.ac.uk", "bamdevm@amazon.com", "roberto@cam.ac.uk"], "sections": [{"heading": null, "text": "Recent work has highlighted the scale invariance or symmetry present in the weight space of a typical deep network, and the adverse effects it has on Euclidean gradient-based stochastic gradient descendant optimization. In this work, we show that these and other commonly used deep networks, such as those using a max-pooling and sub-sampling layer, exhibit more complex forms of symmetry resulting from the scaling-based repair ametrization of network weights. Subsequently, we propose two symmetry-invariant gradient-based weight updates for stochastic gradient descension learning. Our empirical evidence based on the MNIST dataset shows that these updates improve test performance without sacrificing the computing efficiency of weight actualizations. We also show the results of training with a proposed weighting segment."}, {"heading": "1 INTRODUCTION", "text": "In fact, most of them are able to outdo themselves; most of them have seen themselves outdo themselves; most of them have outdone themselves; most of them have outdone themselves; most of them have outdone themselves; most of them have outdone themselves; most of them have outdone themselves; most of them have outdone themselves; most of them have outdone themselves; most of them have outdone themselves; most of them have outdone themselves; most of them have outdone themselves; most of them have outdone themselves; most of them have outdone themselves; most of them have outlined themselves; most of themselves; most of themselves; most of themselves; most of themselves; most of themselves; most of themselves; most of themselves; most of themselves; most of themselves; most of themselves; most of the others; most of themselves; the others; the others..................."}, {"heading": "2 ARCHITECTURES AND SYMMETRY ANALYSIS", "text": "To keep the exposure simple, we consider a two-layer architecture shown in Figure 1. Each layer in Arch1 has typical components that are often found in Convolutionary Neural Networks (LeCun et al., 2015), such as multiplication with a trainable weight matrix (e.g. W1 and W2) that predicts the probabilities of the relevant K classes. Arch2 has an additional batch normalization layer when compared with Arch1 (Ioffe & Szegedy, 2015).The analysis in this section covers deeper architectures that use the same components as Arch1 and Arch2. The rows of the weight matrices W1 and W2 correspond to the layers in layers 1 and 2. The dimensions of each layer correspond to the input dimension of the layer."}, {"heading": "3 RESOLVING SYMMETRY ISSUES USING MANIFOLD OPTIMIZATION", "text": "We propose two ways to solve the symmetries that arise in the depths of the system. First, we follow the approach of (Amari, 1998; Edelman et al., 1998; Absil et al., 2008) to equip the search space with a new, non-Euclidean metric to solve the symmetries. Second, we break up the symmetries by extending the filter weights to the unity-norm multiplicity. In both cases, our updates are easy to implement in a stochastic gradient setting on manifolds (Bonnabel, 2013). The proposed updates are deep for two layers. However, the updates can be extended to deeper architecture."}, {"heading": "4 EXPERIMENTAL SETUP", "text": "We have two and four years to unfold until we are able to unfold."}, {"heading": "5 RESULTS AND ANALYSIS", "text": "The aforementioned cerebral consecrated cerebral consecrated cerebral consecrated cerebral consecrated cerebral consecrated cerebral consecrated cerebral consecrated cerebral consecrated cerebral consecrated cerebral consecrated cerebral consecrated."}, {"heading": "6 APPLICATION TO IMAGE SEGMENTATION", "text": "We are applying SGD with the proposed UN weight updates in Table 1 to train SegNet, a deep revolutionary network proposed for the segmentation of streetscapes into several classes (Badrinarayanan et al., 2015a), which, although convolutional, has the same symmetries as those analyzed for Arch2 in (3).The network is trained for 100 epochs on the 367-image training set of CamVid (Brostow et al., 2009).The predictions of the trained SegNet on some sample test images from the dataset can be seen in Figure 3. These qualitative results point to the usefulness of our analysis and symmetric-invariant weight updates for larger networks that occur in practice."}, {"heading": "7 CONCLUSION", "text": "We have shown that these symmetries can be well managed in the framework of stochastic gradient descent optimization, either by designing a suitable non-euclidean metric or by imposing a unit of measurement on the filter weights. Both strategies take into account the manifold structure on which the weights of the network rest and lead to symmetrically invariant weight updates. Empirical results show that our proposed symmetrically invariant weight updates can improve test performance even on modern architectures. In future research, we would use these techniques for deep convolutionary neural networks used in practical applications."}, {"heading": "ACKNOWLEDGMENTS", "text": "Vijay Badrinarayanan and Roberto Cipolla were supported by a sponsorship from Toyota Motor Europe, Belgium. Bamdev Mishra was supported as a research fellow of the FNRS (Belgian Fund for Scientific Research). The scientific responsibility lies with the authors."}], "references": [{"title": "Riemannian geometry of Grassmann manifolds with a view on algorithmic computation", "author": ["Absil", "P.-A", "R. Mahony", "R. Sepulchre"], "venue": "Acta Applicandae Mathematicae,", "citeRegEx": "Absil et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Absil et al\\.", "year": 2004}, {"title": "Optimization Algorithms on Matrix Manifolds", "author": ["Absil", "P.-A", "R. Mahony", "R. Sepulchre"], "venue": null, "citeRegEx": "Absil et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Absil et al\\.", "year": 2008}, {"title": "Natural gradient works efficiently in learning", "author": ["Amari", "S.-I"], "venue": "Neural computation,", "citeRegEx": "Amari and S..I.,? \\Q1998\\E", "shortCiteRegEx": "Amari and S..I.", "year": 1998}, {"title": "SegNet: a deep convolutional encoder-decoder architecture for robust semantic pixel-wise labelling", "author": ["V. Badrinarayanan", "A. Handa", "R. Cipolla"], "venue": "Technical report,", "citeRegEx": "Badrinarayanan et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Badrinarayanan et al\\.", "year": 2015}, {"title": "Understanding symmetries in deep networks", "author": ["V. Badrinarayanan", "B. Mishra", "R. Cipolla"], "venue": "Technical report,", "citeRegEx": "Badrinarayanan et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Badrinarayanan et al\\.", "year": 2015}, {"title": "Stochastic gradient descent on Riemannian manifolds", "author": ["S. Bonnabel"], "venue": "IEEE Transactions on Automatic Control,", "citeRegEx": "Bonnabel,? \\Q2013\\E", "shortCiteRegEx": "Bonnabel", "year": 2013}, {"title": "Large-scale machine learning with stochastic gradient descent", "author": ["L. Bottou"], "venue": "In International Conference on Computational Statistics (COMPSTAT),", "citeRegEx": "Bottou,? \\Q2010\\E", "shortCiteRegEx": "Bottou", "year": 2010}, {"title": "Low-rank matrix completion via preconditioned optimization on the Grassmann manifold", "author": ["N. Boumal", "Absil", "P.-A"], "venue": "Linear Algebra and its Applications,", "citeRegEx": "Boumal et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Boumal et al\\.", "year": 2015}, {"title": "Manopt, a matlab toolbox for optimization on manifolds", "author": ["N. Boumal", "B. Mishra", "Absil", "P.-A", "R. Sepulchre"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "Boumal et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Boumal et al\\.", "year": 2014}, {"title": "Semantic object classes in video: A high-definition ground truth database", "author": ["G. Brostow", "J. Fauqueur", "R. Cipolla"], "venue": "Pattern Recognition Letters,", "citeRegEx": "Brostow et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Brostow et al\\.", "year": 2009}, {"title": "The geometry of algorithms with orthogonality constraints", "author": ["A. Edelman", "T.A. Arias", "S.T. Smith"], "venue": "SIAM Journal on Matrix Analysis and Applications,", "citeRegEx": "Edelman et al\\.,? \\Q1998\\E", "shortCiteRegEx": "Edelman et al\\.", "year": 1998}, {"title": "URL https://www. cs.toronto.edu/ \u0303hinton/csc2515/notes/lec6tutorial.pdf. Ioffe, S. and Szegedy, C. Batch normalization: Accelerating deep network training by reducing internal covariate shift", "author": ["G. Hinton"], "venue": "In International Conference on Machine learning (ICML),", "citeRegEx": "Hinton,? \\Q2008\\E", "shortCiteRegEx": "Hinton", "year": 2008}, {"title": "Low-rank optimization on the cone of positive semidefinite matrices", "author": ["M. Journ\u00e9e", "F. Bach", "Absil", "P.-A", "R. Sepulchre"], "venue": "SIAM Journal on Optimization,", "citeRegEx": "Journ\u00e9e et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Journ\u00e9e et al\\.", "year": 2010}, {"title": "Optimization algorithms exploiting unitary constraints", "author": ["J.H. Manton"], "venue": "IEEE Transactions on Signal Processing,", "citeRegEx": "Manton,? \\Q2002\\E", "shortCiteRegEx": "Manton", "year": 2002}, {"title": "Path-sgd: Path-normalized optimization in deep neural networks", "author": ["B. Neyshabur", "R. Salakhutdinov", "N. Srebro"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "Neyshabur et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Neyshabur et al\\.", "year": 2015}, {"title": "Riemannian metrics for neural networks I: Feedforward networks", "author": ["Y. Ollivier"], "venue": "Information and Inference,", "citeRegEx": "Ollivier,? \\Q2015\\E", "shortCiteRegEx": "Ollivier", "year": 2015}, {"title": "Riemannian metrics for neural networks II: Recurrent networks and learning symbolic data sequences", "author": ["Y. Ollivier"], "venue": "Information and Inference,", "citeRegEx": "Ollivier,? \\Q2015\\E", "shortCiteRegEx": "Ollivier", "year": 2015}, {"title": "Revisiting natural gradient for deep networks", "author": ["R. Pascanu", "Y. Bengio"], "venue": "Technical report,", "citeRegEx": "Pascanu and Bengio,? \\Q2013\\E", "shortCiteRegEx": "Pascanu and Bengio", "year": 2013}], "referenceMentions": [{"referenceID": 6, "context": "Stochastic gradient descent (SGD) has been the workhorse for optimization of deep networks (Bottou, 2010).", "startOffset": 91, "endOffset": 105}, {"referenceID": 14, "context": "In this regard, the recent work (Neyshabur et al., 2015) has brought to light simple scale invariance properties or symmetries in the weight space, which commonly used deep networks possess.", "startOffset": 32, "endOffset": 56}, {"referenceID": 14, "context": "In particular, the Euclidean gradient scales inversely to the scaling of the variable (Neyshabur et al., 2015).", "startOffset": 86, "endOffset": 110}, {"referenceID": 14, "context": "This leads to very different trajectories for different reparameterizations of the weights during the training process (Neyshabur et al., 2015).", "startOffset": 119, "endOffset": 143}, {"referenceID": 1, "context": "On the other hand, optimization over a manifold with symmetries has been a topic of much research and provides guidance to other simpler metric choices as we show in this paper (Absil et al., 2008; Mishra & Sepulchre, 2014; Boumal & Absil, 2015; Journ\u00e9e et al., 2010; Absil et al., 2004; Edelman et al., 1998; Manton, 2002).", "startOffset": 177, "endOffset": 323}, {"referenceID": 12, "context": "On the other hand, optimization over a manifold with symmetries has been a topic of much research and provides guidance to other simpler metric choices as we show in this paper (Absil et al., 2008; Mishra & Sepulchre, 2014; Boumal & Absil, 2015; Journ\u00e9e et al., 2010; Absil et al., 2004; Edelman et al., 1998; Manton, 2002).", "startOffset": 177, "endOffset": 323}, {"referenceID": 0, "context": "On the other hand, optimization over a manifold with symmetries has been a topic of much research and provides guidance to other simpler metric choices as we show in this paper (Absil et al., 2008; Mishra & Sepulchre, 2014; Boumal & Absil, 2015; Journ\u00e9e et al., 2010; Absil et al., 2004; Edelman et al., 1998; Manton, 2002).", "startOffset": 177, "endOffset": 323}, {"referenceID": 10, "context": "On the other hand, optimization over a manifold with symmetries has been a topic of much research and provides guidance to other simpler metric choices as we show in this paper (Absil et al., 2008; Mishra & Sepulchre, 2014; Boumal & Absil, 2015; Journ\u00e9e et al., 2010; Absil et al., 2004; Edelman et al., 1998; Manton, 2002).", "startOffset": 177, "endOffset": 323}, {"referenceID": 13, "context": "On the other hand, optimization over a manifold with symmetries has been a topic of much research and provides guidance to other simpler metric choices as we show in this paper (Absil et al., 2008; Mishra & Sepulchre, 2014; Boumal & Absil, 2015; Journ\u00e9e et al., 2010; Absil et al., 2004; Edelman et al., 1998; Manton, 2002).", "startOffset": 177, "endOffset": 323}, {"referenceID": 1, "context": "Even though the focus of the paper is on SGD algorithms, it should be noted that the updates proposed in Table 1 can readily be extended to first and second order batch algorithms (Absil et al., 2008).", "startOffset": 180, "endOffset": 200}, {"referenceID": 8, "context": "The stochastic gradient descent algorithms with the proposed updates are implemented in Matlab and Manopt (Boumal et al., 2014).", "startOffset": 106, "endOffset": 127}, {"referenceID": 14, "context": "It should be noted that our analysis differs from (Neyshabur et al., 2015), where the authors deal with a simpler case wherein \u03b2 = 1 \u03b10 is a scalar and \u03b8 is reparameterization free.", "startOffset": 50, "endOffset": 74}, {"referenceID": 10, "context": "First, we follow the approach of (Amari, 1998; Edelman et al., 1998; Absil et al., 2008) to equip the search space with a new non-Euclidean metric to resolve the symmetries present.", "startOffset": 33, "endOffset": 88}, {"referenceID": 1, "context": "First, we follow the approach of (Amari, 1998; Edelman et al., 1998; Absil et al., 2008) to equip the search space with a new non-Euclidean metric to resolve the symmetries present.", "startOffset": 33, "endOffset": 88}, {"referenceID": 5, "context": "In both these case, our updates are simple to implement in a stochastic gradient descent setting on manifolds (Bonnabel, 2013).", "startOffset": 110, "endOffset": 126}, {"referenceID": 1, "context": "Another way to resolve the symmetries that exist in Arch1 and Arch2 is to constrain the weight vectors (filters) in W1 and W2 to lie on the oblique manifold (Absil et al., 2008; Boumal et al., 2014), i.", "startOffset": 157, "endOffset": 198}, {"referenceID": 8, "context": "Another way to resolve the symmetries that exist in Arch1 and Arch2 is to constrain the weight vectors (filters) in W1 and W2 to lie on the oblique manifold (Absil et al., 2008; Boumal et al., 2014), i.", "startOffset": 157, "endOffset": 198}, {"referenceID": 8, "context": "Specifically, it is defined as \u03a0W (Z) = Z\u2212Diag(diag((ZW ))W (Boumal et al., 2014), where Diag(\u00b7) is an operator which creates a diagonal matrix with its argument placed along the diagonal and diag(\u00b7) is an operator which extracts the diagonal elements of the argument matrix.", "startOffset": 60, "endOffset": 81}, {"referenceID": 6, "context": "The convergence analysis of SGD on manifolds follows the developments in (Bottou, 2010; Bonnabel, 2013).", "startOffset": 73, "endOffset": 103}, {"referenceID": 5, "context": "The convergence analysis of SGD on manifolds follows the developments in (Bottou, 2010; Bonnabel, 2013).", "startOffset": 73, "endOffset": 103}, {"referenceID": 14, "context": "BSGD is also studied as a benchmark algorithm in (Neyshabur et al., 2015).", "startOffset": 49, "endOffset": 73}, {"referenceID": 11, "context": "For training each network, we use the two well known protocols for annealing or decaying the learning rate; the bold-driver (annealing) protocol (Hinton, 2008) and the exponential decay protocol.", "startOffset": 145, "endOffset": 159}, {"referenceID": 9, "context": "The network is trained for 100 epochs on the CamVid (Brostow et al., 2009)", "startOffset": 52, "endOffset": 74}], "year": 2015, "abstractText": "Recent works have highlighted scale invariance or symmetry that is present in the weight space of a typical deep network and the adverse effect that it has on the Euclidean gradient based stochastic gradient descent optimization. In this work, we show that these and other commonly used deep networks, such as those which use a max-pooling and sub-sampling layer, possess more complex forms of symmetry arising from scaling based reparameterization of the network weights. We then propose two symmetry-invariant gradient based weight updates for stochastic gradient descent based learning. Our empirical evidence based on the MNIST dataset shows that these updates improve the test performance without sacrificing the computational efficiency of the weight updates. We also show the results of training with one of the proposed weight updates on an image segmentation problem.", "creator": "LaTeX with hyperref package"}}}