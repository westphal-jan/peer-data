{"id": "1511.02024", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "6-Nov-2015", "title": "Towards a Better Understanding of Predict and Count Models", "abstract": "In a recent paper, Levy and Goldberg pointed out an interesting connection between prediction-based word embedding models and count models based on pointwise mutual information. Under certain conditions, they showed that both models end up optimizing equivalent objective functions. This paper explores this connection in more detail and lays out the factors leading to differences between these models. We find that the most relevant differences from an optimization perspective are (i) predict models work in a low dimensional space where embedding vectors can interact heavily; (ii) since predict models have fewer parameters, they are less prone to overfitting.", "histories": [["v1", "Fri, 6 Nov 2015 10:29:26 GMT  (199kb,D)", "http://arxiv.org/abs/1511.02024v1", "17 pages"]], "COMMENTS": "17 pages", "reviews": [], "SUBJECTS": "cs.LG cs.CL", "authors": ["s sathiya keerthi", "tobias schnabel", "rajiv khanna"], "accepted": false, "id": "1511.02024"}, "pdf": {"name": "1511.02024.pdf", "metadata": {"source": "CRF", "title": "Towards a Better Understanding of Predict and Count Models", "authors": ["S. Sathiya Keerthi", "Tobias Schnabel", "Rajiv Khanna"], "emails": [], "sections": [{"heading": null, "text": "Motivated by the findings of our analysis, we show how counter models can be regulated in principle and offer closed solutions for L1 and L2 regularization. Finally, we propose a new embedding model with a convex target and the additional advantage of comprehensibility."}, {"heading": "1 Introduction", "text": "In recent years, however, we have managed to improve models such as the classical language model, and a recent evaluation study suggests that the models are superior in terms of the number of words."}, {"heading": "2 Notations", "text": "Consider a corpus D = {wt} | D | t = 1, which is a sequence of words above a vocabulary V. We will use w when referring to the index of a target word, and c when referring to the index of a context word. We will define the exact term of the context in Section 2.1. Note that w and c are integers, the values from 1 to | V |. Each word and context word has a vectorial representation. We will use w, Rm, and c to denote the word vector and context vector corresponding to the word w and context word c. For the vector x, xi will denote the i-th component of x. Let {w} and {c} specify the set of all word and context vectors. You can also think of {w} and {c} as matrices W and C, whose w-te and c-te are specified by w and c-te."}, {"heading": "2.1 Defining context", "text": "The definition of context determines which co-occurrences are taken into account when creating a model (along with the meaning of individual co-occurrences). A common method of defining context is to select a window around each occurrence of a word w in D. There are many options to customize this window (e.g., it can be either symmetrical or asymmetrical around w. There may also be weights (e.g. depending on the relative positions of w and c) that coincide with each (w, c) occurrence in D. You can also include weighing common words, possibly both the current word w and the context words c1. All of these decisions determine the value of # (w, c). Note that the weights can be real, # (w, c), # (w, \u00b7, c), and # mmm."}, {"heading": "3 Background", "text": "Remember that the purpose of this paper is to examine the differences between counting and prediction models. To do this, we first summarize two specific cases of counting and prediction models: We first present PMI models as representatives of counting models in Section 3.1 and then discuss the Skip Gram Negative Sample Model (SGNS) in Section 3.2."}, {"heading": "3.1 PMI models", "text": "PMI models have been extensively used in distributional semantic models [12] to calculate similarities between words. As the name implies, the key parameter in these models is Pointwise Mutual Information (PMI). Their definition and MLE estimate based on data is given by PMI (w, c) = log P (w, c) P (c) \u2248 log # (w, c) \u00b7 | D | # (w, \u00b7) \u00b7 # (\u00b7, c), (4), where # (w, c) is the simple counting function. For a given word w, PMI w represents a vector whose components are PMI (w, c) for all c-V. Note that PMI is not defined if # (w, c) = 0. To avoid this problem, positive PMI (PPMI) (PPMI) (PPMI) replaces all negative values, i.e. PPMI (w, c) = max (PMI), c) all dimensions are shifted by PMI."}, {"heading": "3.2 Skip-Gram with Negative Sampling (SGNS)", "text": "\"It's as if he's been able to get to the top,\" he says."}, {"heading": "3.2.1 Remarks on symmetry", "text": "If we start from symmetrical windows as outlined in Section 2.1 for the counting of co-events, we can exchange word and context vectors at optimality. Observation 2 Let's assume (W \u0445, C \u0445) is a minimizer of. \"Then (C \u0445, W \u0445) is also a minimizer of\"; in other words, with optimality, if the word vectors and context word vectors are completely interchanged, optimality holds. Depending on how the numerical optimization of (7) was initialized, W \u0445 and C \u0445 can be completely interchanged. Similar swap properties can be given for other models in the coevent format, e.g. Glove [9]. Observation 3 Note that Observation 2 does not mean that W = C. If \"a convex function would have been, then the convex function, which is based on the fact that any convex combination of optimal solutions is optimal, can not be coupled with the existence of an optimal function of an optimum NS (we normally call the existence of an optimal W = 7)."}, {"heading": "4 Connecting count and predict models", "text": "In this section, we return to the idea that Levy and Goldberg [2] linked and extended PMI models with SGNS. The central step is to explicitly solve SGNS in a closed form; we can then see that the explicit solution of SGNS corresponds to the vectors constructed in a shifted PMI model."}, {"heading": "4.1 Solving SGNS in closed form", "text": "As Levy and Goldberg have shown, the closed-form solution of the SGNS object turns out to correspond to a solution constructed by the displaced PMI model. In contrast to Levy and Goldberg, we apply the approximate square analysis to the SGNS lens to give a more detailed view of a broad class of loss functions. The central idea of the analysis is to assume that, given a sufficient number of dimensions, any score x = w \u00b7 c can be minimized independently of any other results. Let's define c (x) = \"w, c (w), c (w \u00b7 c), c (w \u00b7 c), c (w \u00b7 c), c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, etc."}, {"heading": "4.2 Connecting SGNS and Shifted PMI", "text": "One of the most important results of Levy and Goldberg [2] is that the vectors generated by Shifting PMI are a solution to the SGNS target. Let's use the quadratic analysis of Section 4.1 to say this cleaner. Let's call it the unit vector in R | V | whose i-th component is 1 and all other components zero. Let's say we use a uniform representation for context vectors, i.e. we fix c = ec for all C. Observation 7 Let's say we fix each context vector for the more uniform representation given above. Then only W remains as a variable set and the following hold. (a) The minimizer W of (8) is given by w = x \u0445 w, where x \u00b2 w is a vector with {x \u00b2 w, c} c as components."}, {"heading": "5 Count and predict models: differences", "text": "Empirically, there seems to be evidence that predictive and counting models work differently (e.g. [1]), which is interesting because they all take into account the same input data - namely the simultaneous occurrence of words in the text. What are the reasons for this? Although the previous section indicated a strong link between SGNS- and PMI-methods (observation 7 even indicates an approximate equivalence), we believe that there are two main differences between the two methods, which (a) relate to the dimension of embedding and (b) C are fixed as a uniform representation. Let's expand on the two factors. Observation 8 Dimension of embedding. As already mentioned in observation 5, the small embedding dimension used by SGNS for words can be crucial for learning good word vectors. For example, co-occurring words can influence each other."}, {"heading": "5.1 Further differences in Shifted PPMI", "text": "Remember that the solution for SGNS given by Shifted PMI is useless in practice, since we have entries that read \u2212 \u221e. Besides, the Shifted PMI solution has a high number of dimensions, and this might not make sense in practice. Levy and Goldberg present two heuristics that fix these problems. First, they propose to omit negative terms in the objective function in order to make a solution feasible. Second, they propose a subsequent SVD step to reduce the dimensionality of the Shifted PPMI matrix."}, {"heading": "5.1.1 Omitting terms", "text": "Since in practice we cannot work with vectors that have \u2212 \u221e entries, Goldberg and Levy propose to remedy this problem by shifting the PPMI instead of shifting the PMI. Different PPMI corresponds to the omission of all (w, c) terms that exhibit negative shifts in PMI values. (7) This corresponds to a modified SGNS method in which negative sample samples that do not meet the PosCondition, i.e. those with PMI \u2212 log k < 0, are left out. There are two problems with the above approach. Firstly, we can no longer guarantee optimality for this solution. Secondly, this also seems incompatible with the basic idea behind negative sample, namely a sample of unobserved (w, c) pairs. Levy and Goldberg [2] make the following statement in the second paragraph of their paper: \"We note that shifting the PPMI does indeed represent an almost perfect approximation of the optimal solution, even though it does not represent an optimal approximation of the set in terms.\""}, {"heading": "5.1.2 Applying SVD", "text": "Levy and Goldberg also propose to apply SVD to the SPPMI matrix to obtain low-dimensional embedding. If we follow this step, we lose some of the optimality we had with the postponed PMI solution. Formally, we consider an SVPMI matrix M whose (w, c)'th term is max (0, x, w, c). To make word vectors of dimension d lower than | V |, Goldberg and Levy suggest applying SVD to matrix M. Let M = UVV T denote the SVD. If one is interested in embedding dimension d < | V |, the stranded d approximation of M, UdV T d is used. To form word vectors, one can use either W SVDd = UDD or the \"symmetrical version.\""}, {"heading": "5.2 Summary", "text": "Figure 1 gives a rough and fast overview of the different morphings of SGNS into different PMI methods. The use of a small embedding dimension by SGNS as opposed to the full-dimensional uniform representation used by PMI methods is probably the most important difference. The omission of certain negative examples, the approximation of the original nonlinear lens by a square, and the omission of the \u03b1w, c factors from the square lens are additional differences. Carefully designed experiments are necessary to understand the individual effects of each difference on the quality of the resulting word vectors. The various differences also imply that the methods at the two left ends of the figure, while incorporating a low-dimensional embedding, may differ significantly due to the various reasons given in this section."}, {"heading": "6 Extensions", "text": "In the following two subsections, we propose two extensions to the basic PMI model. First, we show how to add regularization to PMI models and offer explicit solutions for L2 and L1 regularization. We hope that regularization will help improve problems with data sparseness. Then, we propose a new convex model for word embedding that is not only easy to learn, but also provides intuitive word vectors, as each dimension corresponds exactly to a context word."}, {"heading": "6.1 Adding regularization to PMI methods", "text": "Let us continue with the formulation of Section 4.1 and add regularization. If overadjustment is one of the causes of the poorer performance of PMI methods compared to SGNS, then regularization should help improve performance. Consider the decoupled determination of x = WC. Our modified goal is nowargmin x \u03c1w, c (x) + Rw, c (x), where Rw, c is the regularization term. As we argued in Section 4.1, each WC is decoupled from other variables. Therefore, we can solve each one-dimensional optimization problem in isolation - even when regularization is added. We now derive solutions in closed form for the L2 and L1 regularization."}, {"heading": "6.1.1 L2 regularization", "text": "We can weigh common words and contextual words and add a standard L2 regulator as follows: Rw, c (x) = \u03bb # (w, \u00b7) \u00b7 # (\u00b7, c) | D | R (x) = 1 2 x2 (17) We will discuss the L1 regulator R (x) = | x | later. Let's also just look at the logistical loss. Divide by # (w, \u00b7) \u00b7 # (\u00b7 (, c) | D | we get c = arg min x ePMI log (1 + e \u2212 x) + k log (1 + ex) + \u03bb 2 x2 (18) Let's set the derivative of the target to zero and simplify w \u0445 c to be the solution: x = h (x), where h (x) = ePMI \u2212 kex1 + ex. (19) The line from A to B in Figure 2 approaches h (x) nicely in the region where the optimal is."}, {"heading": "6.1.2 L1 regularization", "text": "Similarly, it is easy to find closed form expressions for w \u00b2 c for the L1 regularization. In this case, many of the optimal values go back to zero: the greater the value of \u03bb, the more the number of zeros. For the L1 regularization, three cases have to be taken into account: The value h (0) = ePMI \u2212 k 2 plays a key role. Case 1. \u2212 \u03bb \u2264 h (0) \u2264 \u03bb. In this case, it is easy to verify that at x = 0 the left-sided derivative of the object in (18) is not positive and the right-sided derivative is not negative. Therefore, w \u00b2 c = 0 is the optimal solution. Case 2. h (0) > \u03bb. In this case, the right-sided derivative in x = 0 is negative and the optimal value is positive. The optimum is found by solving h (x) = \u03bb, which in turn results in c \u2212 log ePMI \u2212 k + permanently (3)."}, {"heading": "6.1.3 Discussion", "text": "Adding regulatory models to counting models has two advantages: First, the unregulated solution (see Table 2; same as SPMI) is useless because x \u0445 w, c leads to extremes when # (w, c) = 0, but regulated solutions are always well defined. Second, we expect regulatory action to help when over-matching impairs performance with PMI models. Future work is needed to empirically investigate and validate the usefulness of regulation in counting models."}, {"heading": "6.2 A convex formulation for word vectors", "text": "It is not only the way in which the individual words and their contexts are defined, but also the way in which the individual words and contexts are defined. Also, the way in which the objective function of the individual words is defined makes our objective function convective. Another advantage is that this leads to intelligible models - each vector entry refers to a context word. Also, to get compact representations, we use L1 regulations on the word vectors instead of using them. Another advantage is that these results can be used in intelligible models - each vector entry refers to a context word. Also, to get compact representations, we use L1 regulations on the word vectors instead of learning systems that refer to sub-dimensional spaces, as traditional predictions modele.Let us describe the formulation in some detail, let us use the dimension of the context."}, {"heading": "7 Conclusion", "text": "In this report, we demonstrated how to explicitly solve the SGNS target for a wide range of loss functions. In addition, we pointed out two important differences between the shifted PMI model and the SGNS model. Firstly, the SGNS model uses much less embedding in practice, which makes the SGNS model less susceptible to overfits. Secondly, the context vectors are fixed in the PMI model and therefore there is no interaction between context vectors and word vectors. Finally, we proposed two extensions to existing models. Firstly, we showed how we can integrate regulation into PMI models in order to overfitting.Secondly, we presented a new embedding model that not only has a convex target, but also leads to intelligent embedding."}], "references": [{"title": "Don\u2019t count, predict! A systematic comparison of context-counting vs. context-predicting semantic vectors", "author": ["M. Baroni", "G. Dinu", "G. Kruszewski"], "venue": null, "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2014}, {"title": "Neural word embedding as implicit factorization", "author": ["O. Levy", "Y. Goldberg"], "venue": null, "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2014}, {"title": "Linguistic Regularities in Sparse and Explicit Word Representations", "author": ["O. Levy", "Y. Goldberg"], "venue": null, "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2014}, {"title": "Improving distributional similarity with lessons learned from word embeddings", "author": ["O. Levy", "Y. Goldberg", "I. Dagan"], "venue": "TACL,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2015}, {"title": "Efficient estimation of word representations in vector space", "author": ["T. Mikolov", "K. Chen", "G. Corrado", "J. Dean"], "venue": "CoRR, abs/1301.3781,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2013}, {"title": "Distributed representations of words and phrases and their compositionality", "author": ["T. Mikolov", "I. Sutskever", "K. Chen", "G.S. Corrado", "J. Dean"], "venue": null, "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2013}, {"title": "A scalable hierarchical distributed language model", "author": ["A. Mnih", "G. Hinton"], "venue": null, "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2009}, {"title": "Learning word embeddings efficiently with noise-contrastive estimation", "author": ["A. Mnih", "K. Kavukcuoglu"], "venue": null, "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2013}, {"title": "GloVe: Global vectors for word representation", "author": ["J. Pennington", "R. Socher", "C.D. Manning"], "venue": "EMNLP", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2014}, {"title": "Linking Glove with word2vec.arXiv: 1411.5595v2", "author": ["T. Shi", "Z. Liu"], "venue": null, "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2014}, {"title": "Model-based Word Embeddings from Decompositions of Count Matrices", "author": ["K. Stratos", "M. Collins", "D. Hsu"], "venue": null, "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2015}, {"title": "From frequency to meaning: Vector space models of semantics", "author": ["P. Turney", "P. Pantel"], "venue": "JAIR,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2010}], "referenceMentions": [{"referenceID": 1, "context": "Abstract In a recent paper, Levy and Goldberg [2] pointed out an interesting connection between prediction-based word embedding models and count models based on pointwise mutual information.", "startOffset": 46, "endOffset": 49}, {"referenceID": 11, "context": "Distributional semantic models [12], also called count models [1] have been popular in the computational linguistics literature for several decades.", "startOffset": 31, "endOffset": 35}, {"referenceID": 0, "context": "Distributional semantic models [12], also called count models [1] have been popular in the computational linguistics literature for several decades.", "startOffset": 62, "endOffset": 65}, {"referenceID": 4, "context": "In the last few years, however, predict models such as the Skip-Gram model and Continuous Bag-Of-Words model have become the de facto standard for word modeling [5, 6].", "startOffset": 161, "endOffset": 167}, {"referenceID": 5, "context": "In the last few years, however, predict models such as the Skip-Gram model and Continuous Bag-Of-Words model have become the de facto standard for word modeling [5, 6].", "startOffset": 161, "endOffset": 167}, {"referenceID": 6, "context": "These methods have origins in neural language modeling [7], where the goal was to improve classic language models.", "startOffset": 55, "endOffset": 58}, {"referenceID": 0, "context": "A recent evaluation study [1] suggested that the predict models are superior to the count models on a range of word similarity tasks.", "startOffset": 26, "endOffset": 29}, {"referenceID": 3, "context": "hyperparameter optimization [4].", "startOffset": 28, "endOffset": 31}, {"referenceID": 1, "context": "To this end, Levy and Goldberg [2] made an interesting connection between two key models: a traditional count model based on pointwise mutual information (PMI) and a predict model, namely the skip-gram model with negative sampling (SGNS).", "startOffset": 31, "endOffset": 34}, {"referenceID": 5, "context": "For example, Mikolov et al [6] suggest the following down-sampling probability:", "startOffset": 27, "endOffset": 30}, {"referenceID": 5, "context": "Let P3(i) be the weight associated with the position of the context word w in the context 1Mikolov et al [6] used down-weighting of common words.", "startOffset": 105, "endOffset": 108}, {"referenceID": 1, "context": "It is unclear if Levy and Goldberg [2] used it.", "startOffset": 35, "endOffset": 38}, {"referenceID": 11, "context": "1 PMI models PMI models have been used extensively in distributional semantic models [12] to compute similarities between words.", "startOffset": 85, "endOffset": 89}, {"referenceID": 1, "context": "More recently, Levy and Goldberg [2] have defined shifted variants of PMI and the PPMI metrics.", "startOffset": 33, "endOffset": 36}, {"referenceID": 5, "context": "2 Skip-Gram with Negative Sampling (SGNS) SGNS [6] is a popular predict model that aims to predict which word w occurs with a context word c.", "startOffset": 47, "endOffset": 50}, {"referenceID": 5, "context": "One can derive the SGNS [6] model from a binary classification setting where the target variable y specifies whether a word w occurs with context word c.", "startOffset": 24, "endOffset": 27}, {"referenceID": 1, "context": "Levy and Goldberg [2] showed that, by accumulating data over co-occurrences of words and context words, the objective function in (6) can be rewritten as:", "startOffset": 18, "endOffset": 21}, {"referenceID": 1, "context": "We consider general loss functions, L(x, y) (here y \u2208 {1,\u22121} is the target variable) with the logistic loss 2Levy and Goldberg [2] write the problem as the maximization of likelihood; hence the ` here and the one in [2] are negatives of each other.", "startOffset": 127, "endOffset": 130}, {"referenceID": 1, "context": "We consider general loss functions, L(x, y) (here y \u2208 {1,\u22121} is the target variable) with the logistic loss 2Levy and Goldberg [2] write the problem as the maximization of likelihood; hence the ` here and the one in [2] are negatives of each other.", "startOffset": 216, "endOffset": 219}, {"referenceID": 4, "context": "Observation 1 Unlike SGNS, not all models can be reduced to the co-occurrence format - an example is the CBOW model [5].", "startOffset": 116, "endOffset": 119}, {"referenceID": 6, "context": "In fact, even with the skip-gram model, the reduction to the co-occurrence format is not possible if we use the traditional softmax or hierarchical softmax [7] approach to speed up training.", "startOffset": 156, "endOffset": 159}, {"referenceID": 0, "context": "That property makes them have close relations with count models [1, 2, 10].", "startOffset": 64, "endOffset": 74}, {"referenceID": 1, "context": "That property makes them have close relations with count models [1, 2, 10].", "startOffset": 64, "endOffset": 74}, {"referenceID": 9, "context": "That property makes them have close relations with count models [1, 2, 10].", "startOffset": 64, "endOffset": 74}, {"referenceID": 8, "context": "Another model that can be expressed in co-occurrence form is Glove [9].", "startOffset": 67, "endOffset": 70}, {"referenceID": 8, "context": ", Glove [9].", "startOffset": 8, "endOffset": 11}, {"referenceID": 5, "context": "[6] derive (7) starting from the problem of predicting a context word given a word.", "startOffset": 0, "endOffset": 3}, {"referenceID": 6, "context": "It is useful to note that this comment does not hold if traditional softmax or hierarchical softmax [7] or noise-contrastive estimation [8] is used to model probabilities with the associated negative likelihood loss.", "startOffset": 100, "endOffset": 103}, {"referenceID": 7, "context": "It is useful to note that this comment does not hold if traditional softmax or hierarchical softmax [7] or noise-contrastive estimation [8] is used to model probabilities with the associated negative likelihood loss.", "startOffset": 136, "endOffset": 139}, {"referenceID": 1, "context": "In this section, we revisit the idea that Levy and Goldberg [2] used to connect PMI models with SGNS and extend it.", "startOffset": 60, "endOffset": 63}, {"referenceID": 1, "context": "1 Solving SGNS in closed form Here, we extend the analysis of Levy and Goldberg [2].", "startOffset": 80, "endOffset": 83}, {"referenceID": 1, "context": "2 Connecting SGNS and Shifted PMI One of the key results of Levy and Goldberg [2] is that the vectors created by Shifted PMI are a solution to the SGNS objective.", "startOffset": 78, "endOffset": 81}, {"referenceID": 1, "context": "Though Levy and Goldberg [2] do not mention the above construction, this is a simple observation that easily follows from their analysis.", "startOffset": 25, "endOffset": 28}, {"referenceID": 0, "context": ", [1]).", "startOffset": 2, "endOffset": 5}, {"referenceID": 1, "context": "Levy and Goldberg [2] make the following statement in the second paragraph of Section 5.", "startOffset": 18, "endOffset": 21}, {"referenceID": 1, "context": "Observation 10 Levy and Goldberg [2] propose that SVD is done on M or one of its variants.", "startOffset": 33, "endOffset": 36}, {"referenceID": 1, "context": "However, if remaining faithful to SGNS is the aim, (13) 3Levy and Goldberg [2] recommend using the matrix corresponding to Shifted PPMI.", "startOffset": 75, "endOffset": 78}, {"referenceID": 1, "context": "Levy and Goldberg [2] recommend Shifted PPMI and refer to the spectral word vectors for it as SVD and Symmetric SVD.", "startOffset": 18, "endOffset": 21}, {"referenceID": 2, "context": "Depending on the purpose for which the word vectors are developed we can also use dependency-tree based context features [3].", "startOffset": 121, "endOffset": 124}, {"referenceID": 6, "context": "We can employ: (a) proper softmax over all words; (b) hierarchical softmax [7]; (c) Noise-contrastive estimation (NCE) [8]; or, (d) negative sampling like in SGNS [5].", "startOffset": 75, "endOffset": 78}, {"referenceID": 7, "context": "We can employ: (a) proper softmax over all words; (b) hierarchical softmax [7]; (c) Noise-contrastive estimation (NCE) [8]; or, (d) negative sampling like in SGNS [5].", "startOffset": 119, "endOffset": 122}, {"referenceID": 4, "context": "We can employ: (a) proper softmax over all words; (b) hierarchical softmax [7]; (c) Noise-contrastive estimation (NCE) [8]; or, (d) negative sampling like in SGNS [5].", "startOffset": 163, "endOffset": 166}], "year": 2015, "abstractText": "In a recent paper, Levy and Goldberg [2] pointed out an interesting connection between prediction-based word embedding models and count models based on pointwise mutual information. Under certain conditions, they showed that both models end up optimizing equivalent objective functions. This paper explores this connection in more detail and lays out the factors leading to differences between these models. We find that the most relevant differences from an optimization perspective are (i) predict models work in a low dimensional space where embedding vectors can interact heavily; (ii) since predict models have fewer parameters, they are less prone to overfitting. Motivated by the insight of our analysis, we show how count models can be regularized in a principled manner and provide closed-form solutions for L1 and L2 regularization. Finally, we propose a new embedding model with a convex objective and the additional benefit of being intelligible.", "creator": "LaTeX with hyperref package"}}}