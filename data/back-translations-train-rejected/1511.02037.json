{"id": "1511.02037", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "6-Nov-2015", "title": "ALOJA: A Framework for Benchmarking and Predictive Analytics in Big Data Deployments", "abstract": "This article presents the ALOJA project and its analytics tools, which leverages machine learning to interpret Big Data benchmark performance data and tuning. ALOJA is part of a long-term collaboration between BSC and Microsoft to automate the characterization of cost-effectiveness on Big Data deployments, currently focusing on Hadoop. Hadoop presents a complex run-time environment, where costs and performance depend on a large number of configuration choices. The ALOJA project has created an open, vendor-neutral repository, featuring over 40,000 Hadoop job executions and their performance details. The repository is accompanied by a test-bed and tools to deploy and evaluate the cost-effectiveness of different hardware configurations, parameters and Cloud services. Despite early success within ALOJA, a comprehensive study requires automation of modeling procedures to allow an analysis of large and resource-constrained search spaces. The predictive analytics extension, ALOJA-ML, provides an automated system allowing knowledge discovery by modeling environments from observed executions. The resulting models can forecast execution behaviors, predicting execution times for new configurations and hardware choices. That also enables model-based anomaly detection or efficient benchmark guidance by prioritizing executions. In addition, the community can benefit from ALOJA data-sets and framework to improve the design and deployment of Big Data applications.", "histories": [["v1", "Fri, 6 Nov 2015 11:27:29 GMT  (608kb,D)", "http://arxiv.org/abs/1511.02037v1", "Submitted to IEEE Transactions on Emerging Topics in Computing (TETC). Part of the Aloja Project. Partially funded by European Research Council (ERC) under the European Union's Horizon 2020 research and innovation programme (grant agreement No 639595) - HiEST Project. arXiv admin note: substantial text overlap witharXiv:1511.02030"]], "COMMENTS": "Submitted to IEEE Transactions on Emerging Topics in Computing (TETC). Part of the Aloja Project. Partially funded by European Research Council (ERC) under the European Union's Horizon 2020 research and innovation programme (grant agreement No 639595) - HiEST Project. arXiv admin note: substantial text overlap witharXiv:1511.02030", "reviews": [], "SUBJECTS": "cs.LG cs.DC", "authors": ["josep ll berral", "nicolas poggi", "david carrera", "aaron call", "rob reinauer", "daron green"], "accepted": false, "id": "1511.02037"}, "pdf": {"name": "1511.02037.pdf", "metadata": {"source": "CRF", "title": "ALOJA: A Framework for Benchmarking and Predictive Analytics in Big Data Deployments", "authors": ["Josep Ll. Berral", "Nicolas Poggi", "David Carrera", "Aaron Call", "Rob Reinauer", "Daron Green"], "emails": [], "sections": [{"heading": null, "text": "Index Terms - Data Center Management, Hadoop, Benchmarks, Modelling and Prediction, Machine Learning, Execution Experiences"}, {"heading": "1 INTRODUCTION", "text": "In fact, it is in such a way that we see ourselves in a position to go into another world, in which we go into another world, in which we go into another world, in which we go into another world, in which we go into another world, in which we go into a world, in which we go into another world, in which we go into which we are in which we go into another world, in which we go into which we are in which we are in which we are in which we are in which we, in which we, in which we, in which we, in which we, in which we, in which we, in which we, in which we, in which we, in which we, in which we, in which we, in which we, in which we, in which we, we, in which we, we, in which we, we, in which we, in which we, in which we, in which we, we, we, in which we, we, we, in which we, we, we, in which we, we, we, in which we, we, in which we, we, we, in which we, we, in which we, we, in which we, we, in which we, in which we, we, in which we, in which we, in which we, we, in which we, we, in which we, in which we, in which we, we, in which we, in which we, we, in which we, in which we, in which we, we, in which we, in which we, we, in which we, in which we, we, we, in which, in which we, we, in which, we, we, in which, we, we, we, in which, in which, we, we, in which, we, we, in which, we, in which, we, we, in which, we, we, in which, we, we, in which, we, in which, we, in which, we, we, in which, we, we, in which, we, in which, we, we, in which, we, we, we, we, we, in which, we, we, we, we, in which, we, in which, we, in which, we, we, we, we, in which, we, we, in which, we"}, {"heading": "1.1 Motivation", "text": "In recent years, most of the industry's efforts have focused on building scalable data-processing frameworks like Hadoop and its derivative services, but these efforts have focused on corporate adoption of Hadoop, development of map / application reduction, and lately on tuning the performance of such deployments and managing their data. Studies show that Hadoop's execution performance can be up to three times better than the default configuration for most deployments [8]. Hadoop is currently dominated by multiple vendors, each offering their tailored distribution with patches and changes to the standard apache distribution, thanks to the fact that Hadoop itself is open source. Such changes are rarely pushed into the main distribution, so that the state of standard and standard is maintained, so that no speedups or performance tweaks can be executed [8]. There is also evidence that Hadoop performance is fairly poorly distributed to new or scalable hardware."}, {"heading": "1.2 Contribution", "text": "The ALOJA Big Data Benchmarking Framework focuses on providing researchers and developers with a set of tools and visualizations to characterize Hadoop configurations and performance issues; the framework is available to the entire community to be used with their own Hadoop dataset execution; users and researchers can also implement or expand this set of tools by adding, comparing, or predicting data from observed tasks; and / or adding new analysis or visualization tools, even new machine learning or anomaly detection algorithms. In addition, all datasets collected for ALOJA are public and can be explored through our framework or used as datasets in other platforms. Finally, in this work we share our experience in analyzing Hadoop execution datasets. We present results on early results using our visualization tools; and we present results for modeling and predicting suite execution times for Hadoop 11."}, {"heading": "2 STATE OF THE ART", "text": "Here we present the background of the ALOJA project and the current state of the art in Hadoop and the performance-oriented prediction analysis of distributed systems."}, {"heading": "2.1 Background", "text": "The work presented here focuses on the ALOJA project, an initiative of the Barcelona Supercomputing Center (BSC), which has been developing Hadoop-related computing literacy through its Azure4Research program for more than 7 years. [3] This project is partially supported by Microsoft Corporation, which contributes technically through product teams, financially, and by providing resources and infrastructure. The initial approach was to create a comprehensive open and public Hadoop benchmarking repository, which currently includes more than 40,000 Hadoop benchmarks and is used as the main dataset for this work. At this point, the tool compares software configuration parameters, emerging hardware and cloud services, and the cost of each type of featured setup and the resulting performance for each workload. We expect that our growing repository and analysis tools will help Hadoop community meet their big data application needs."}, {"heading": "2.1.1 Benchmarking", "text": "Due to the large number of possible Hadoop configurations, each of which influences execution in different ways, we characterize Hadoop with comprehensive benchmarking. Hadoop distributions include jobs that can be used to benchmark performance, commonly referred to as micro-benchmarks, each representing a specific type of workload. However, ALOJA currently includes versions of the open source benchmark suite Intel HiBench [11] that can be more realistic and comprehensive than the included examples in Hadoop distributions."}, {"heading": "2.1.2 Current Platform and Tools", "text": "The current ALOJA platform is open source software and datasets available to the community for downloading, use and enhancement, with the goal of automated benchmarking of big data deployments either locally or in the cloud. It includes a set of scripts to automate cluster and node definitions that describe cluster orchestration and settings, as well as scripts to execute selected benchmarks and collect results and related information. Figure 2 shows how the main components of the ALOJA platform feed each other in a continuous loop: benchmarks are executed and the information is collected in the online repository so that users can explore the imported versions and their details, and then decide which new implementations or benchmarks will follow."}, {"heading": "2.2 Related Work", "text": "As mentioned above, execution performance on most deployments can be improved by at least three times the standard Hadoop configuration [8], and the emergence of Hadoop in the industry has led to several attempts to optimize performance, new schemes for proper data distribution or partition, and adjustments in hardware configurations to increase scalability or reduce running costs. Characterizing these deployments is a critical challenge in finding optimal configuration options. One option to accelerate computing systems would be to scale or add new (and thus improved) hardware, but unfortunately, there is evidence that Hadoop performs poorly in such situations, and server scaling also improves performance, but at the increased cost of infrastructure, performance, and storage required [8]. Previous research, such as the Starfish Project by H.Herodotou et al. [10], focuses on the need to optimize work requirements to meet Hadoop configurations."}, {"heading": "3 THE ALOJA FRAMEWORK", "text": "The ALOJA project arose in an attempt to find solutions to an important problem facing the Hadoop community, namely the lack of understanding of which parameters, either software or hardware, determine the performance and cost of Hadoop workloads. Hadoop implementations can also be found in a variety of operational environments, from low-level raw material clusters to high-end data devices, and include all kinds of cloud-based solutions on a large scale, leading to a growing need to understand how different operational parameters, such as VM sizes used in cloud implementations, affect the cost-effectiveness of Hadoop workloads.The project is structured in three phases and aims to 1) create the benchmarking ALOJA platform, 2) deploy visualization and analysis tools in an online web application, and 3) develop performance models derived from the collected data."}, {"heading": "3.1 Methodology and Road-Map", "text": "The roadmap of the ALOJA project is structured into three execution phases: first phase: the first phase consists of a systematic study of performance results across a number of selected modern and novel hardware components, software parameters configuring Hadoop implementations, and solution implementation patterns; second phase: the second phase introduces models and methods for analyzing Hadoop implementations. The ALOJA repository is an accumulated data set of results that allows us to model and predict performance characteristics that are marked as anomalous, performance characteristics, hardware components, software configurations, and application characteristics. By predicting these performance and efficiency results based on the performance characteristics, the system can also make decisions about current and future implementations, such as the rejection of performance characteristics that are marked as anomalous, or the recommendation of the implementing tools to be carried out in the most comprehensive manner, such as the rejection of performance characteristics that are suitable for the implementation phase and the recommendation of the next batch of the implementing tools."}, {"heading": "3.2 The platform", "text": "The ALOJA platform implements the aforementioned repository and the tools available on the ALOJA website. [1] The relevant components of this implementation are benchmark management, data analysis and flow, deep instrumentation and testing infrastructure."}, {"heading": "3.2.1 Benchmarking Components", "text": "The configuration management scripts are responsible for setting up the servers (on-site or in the cloud), the configuration and profile tools of OS and JVM, the Hadoop deployment, and the capture of benchmark starts and metrics, as well as the cleanup of the environment. Parameter selection and queue form the batch of workloads that can be executed by the user options introduced by the ALOJA web application. Finally, the metric profiling includes the system metrics for each process and each execution (CPU, memory, IO, memory), including a log parser to match the collected hadoop metrics with system metrics. Finally, the benchmarks available for selection include the previously cited Intel HiBench suite, which characterizes 4 categories of workload: \u00b5 benchmarks, web crash, machine learning, and HDFS. The benchmarks, which are currently generated by the following terabytes, are terabytes, and terabytes, which are a large site."}, {"heading": "3.2.2 Data Analysis and Flow", "text": "After executing a benchmark, the result is sent to the repository. Performance metrics are extracted, analyzed, and linked to the Hadoop metrics for each job, and then imported into a relational database, where each benchmark is assigned a unique ID associated with its configuration (SW and HW), system metrics, and Hadoop results. Once in the database, the platform provides filtering and visualization for the stored data, allowing different runs, parameter selections, or deployment options to be compared."}, {"heading": "3.2.3 Deep instrumentation", "text": "ALOJA provides a mechanism to easily compare direct results of different Hadoop executions, but in some cases it will be necessary to perform analyses at a lower level. ALOJA uses low-profiling tools for such analyses: \u2022 PARAVER [21], a performance visualization tool for execution histories developed at BSC and widely used to analyze HPC applications, but also Web applications, and now for Hadoop workloads \u2022 The Hadoop Analysis Toolkit, including the Java Instrumentation Suite (JIS), which uses Aspect Oriented Programming to avoid recompilation of Hadoop, and produces a file for execution traces.5 \u2022 Advanced analysis tools such as DBSCAN [22] to detect different algorithm phases as well as regions of different subroutines with similar behavior within Hadoop jobs. This method is linked to the data flow to improve system metrics."}, {"heading": "3.2.4 Initial Testing Infrastructure", "text": "Since ALOJA aims to create a vendor-neutral repository, the test execution environment is not a closed specification, and the platform is designed to support multiple deployment environments. However, to test the platform, a few initial deployments were added and guided through benchmarks: 1) A high-end cluster: an on-site cluster with 2 to 16 data nodes per execution plus a head node with two 6-core sandbridge Intel processors and 64 GB RAM machines; 6 SATA2 SSD drives as RAID 0 memory (1.6 GB / s read, 1 GB / s write) and local 3 TB SATA hard disk space per node; network interfaces with 4 Gigabit Ethernet and 2 FRD InfiniBand ports per node, which provide a bandwidth peak of up to 56 Gbps; and infinition per tape-48 instances each with a Microsoft AzaBit cloud-limited volume: one Aza48 instances."}, {"heading": "4 MODELING BENCHMARKS", "text": "In the second and third phases, the ALOJA project aims to integrate data mining techniques into the analysis of Hadoop performance data. Modeling Hadoop performance versus execution makes it possible to predict such execution results (e.g. execution time or resource consumption) based on input information such as software and hardware configurations. In addition, such models can be applied to anomaly detection methods by comparing actual execution to predicted results and labeling tasks whose runtime is significantly beyond their machine-learned prediction as anomalous. In addition, data mining techniques can help us determine which data from our repository is more important by determining what minimum set of execution operations is required to characterize a specific Hadoop deployment and then recommending which executions should be performed to add a new or updated role."}, {"heading": "4.1 Data-Sets and Workloads", "text": "The ALOJA repository provides all the data for learning models of Hadoop execution and environments. As indicated in Sections 2.1 and 3.2.1, the dataset currently contains up to 40,000 Hadoop executions of 8 different benchmarks from the Intel HiBench suite. Each execution consists of a prepared Hadoop job that generates the data and an appropriate benchmark, such as Teragen and Terasort. Although the job of interest is generally the correct benchmark (i.e. Terasort), the preparation jobs are also valid for training models. This allows us to calculate over 80,000 executions from which we learn. Each benchmark is executed with different configurations, including clusters and VMs, storage drives, internal algorithms, and other Hadoop parameters. Table 1 summarizes the different characteristics of the data system."}, {"heading": "4.2 The Learning Process", "text": "The learning method is a 3-way model of training, validation and exam; see Figure 5: The data set is divided (at this time by random selection) and two subsets are used to develop and validate a model; the third part is used to test the best of the parameters; all learning algorithms are compared by the same test substance; at this point, the ALOJA user can choose between four learning methods."}, {"heading": "4.3 Modeling Results", "text": "Predicting the execution time for a given benchmark and configuration is the first application of the Predictive Analytics toolset. Knowing the expected execution time for a number of possible experiments helps in deciding which new tasks to start, prioritizing them, or simply calculating the cost of resource leasing in advance if it depends on resources per time. Figure 7 shows the learning and prediction data flow. 1. https: / / github.com / Aloja / aloja-ml7"}, {"heading": "4.3.1 Comparing Modeling Algorithms", "text": "Since executions cost money and time, we want to perform as few jobs as possible to model and predict the rest of the possible executions. This makes deciding on the best sizes for such splits a challenge, as we try to require as few benchmark executions as possible to maintain good predictive accuracy. This means building an accurate model from the minimum number of observations. At this point, we will review the accuracy of predictors that provide different sizes of training sets, also compare the different selected algorithms, observe how much we can generalize the model to all workloads, and also how ML improves prediction from other thumb techniques that are applied in the field. As mentioned above, we have several predictive algorithms from which we can build our models, as well as various parameters and choices on the training type. Figure 8 shows the average errors, absolute and relative, for validation and testing the process, using a 25% learning algorithm for each training type (25%)."}, {"heading": "4.3.2 Generalization of Models", "text": "Another central concern was whether a learned model could be generalized by using data from all observed benchmarks, or whether each type of execution / benchmark would require its own specific model. A motivation to create a single general model was to reduce the total number of executions and to generate a comprehensive understanding of all workloads. Furthermore, there was an expectation that our selected algorithms would be able to distinguish the main differences between them (e.g. a regression tree can branch out different trees to use different benchmarks). On the other hand, we knew that different benchmarks can behave very differently and generalization could affect the accuracy of the model. Figure 10 shows the RAE for exceeding each benchmark both by a general model and by a model that was created using only its type of observations. In almost all cases, the specific model fits similar to or better than the general model, and the different algorithms show the same trends, but are much more neural in networks."}, {"heading": "4.3.3 Applications", "text": "The ALOJA framework includes these predictive capabilities in several tools. One of these is the prediction of the performance of known benchmarks on a new computing cluster, as far as we are able to describe this new cluster, so that the new executions for such a cluster are automatically decided. In addition, the hardware configuration of such a cluster allows us to find the best software configurations for our benchmark executions. In the case of a new benchmark entering the system, we can try to check whether one of the existing models for specific benchmarks matches the new one, and then treat the new benchmark as the known one, or expand or train a new model for that benchmark. Other important uses integrated into the ALOJA framework unfold the space of possible configurations and fill it with predicted values for these configurations without any observed execution. Then, note the expected importance of a given parameter or a range of values based on a given parameter."}, {"heading": "5 USE CASES", "text": "In this section, we present some specific applications for the predictive model included in the ALOJA platform, which is a high priority in the ALOJA project roadmap: 1) detecting anomalies by detecting faulty executions by comparing their execution times with those predicted; 2) identifying which executions would best model a particular Hadoop scenario or benchmark by pooling execution observations and using the resulting cluster centers as recommended configurations; and 3) identifying which features are most relevant to each benchmark or hardware infrastructure to speed up execution."}, {"heading": "5.1 Anomaly Detection Mechanisms", "text": "One application of our predictive analysis toolkit is to detect and automatically flag abnormal designs or designs that need to be checked by an operator. Automatic designs that are prone to failure or even designs that have not been properly modeled can save users time that need to review each design or require less human intervention in establishing rules of thumb to decide which designs should be discarded. If we model benchmark behaviors, we can apply model-based anomaly detection methods. After validating a learned model, this can be considered the \"rule that explains the system,\" and any observation that does not fit the model (i.e., the difference between the observed value and the predicted value is greater than expected) is considered an anomaly. Here, we mark abnormal dataset entries as warnings and outliers: 1) a warning that is a copy of an error that is similar to an observation in relation to an observation in n)."}, {"heading": "5.1.1 Validation and Comparisons", "text": "The aforementioned suggestions from the Cold War period and the expulsion from the Cold War period to the Cold War period, when the world was not yet in order, have been taken into the hands of the US and the EU."}, {"heading": "5.1.2 Use cases", "text": "Automatic detection of failed execution saves users time, but also informs administrators if elements of the system are incorrect or faulty, or have unexpectedly changed. Furthermore, compilations of failed execution with common configuration parameters indicate that it is not an appropriate configuration for such a benchmark; or failure to use certain hardware indicates that such hardware should be avoided for those execution. In addition, highlighting anomalous execution simplifies data analysis, especially if such a number of execution is present in the repository, as well as the + 600,000 other performance traces associated with the execution in the repository."}, {"heading": "5.2 Recommending Executions", "text": "When modeling a benchmark, a set of configurations, or a new hardware configuration, some executions need to be performed to observe their new behavior. However, since these executions cost money and time, we want to perform as few as possible. This means that we execute the minimum set of executions that defines the system with sufficient accuracy. From a model of our executions, we can try to model which of these executions are most suitable to be executed on a new system, and use the results to model it; or we can use the model to determine which executions, visible or invisible in our data set, can be executed and used to model. The ALOJA dataset obtained from random or serial executions can include similar executions, introduce redundancy or noise; and find out which minimal executions are the ones that minimize the amount of training data is a combinatory problem based on a large set of data that we can accomplish with a large set of observations."}, {"heading": "5.2.1 Validation and Comparisons", "text": "For each iteration, looking for k clusters, we calculate the error of the resulting model based on the reference dataset or model, also we can estimate the cost of executing these executions in the tested clusters, with average execution costs of $6.85 / hour. Note that the estimated execution time comes from the seen dataset, and applying these configurations to new clusters or invisible components can increase or decrease the values of such estimates, and it should be treated as more than a random value. Figure 15 shows the evolution of the error and execution cost per each k group of recommendations from our ALOJA dataset. See that more executions imply more accuracy in modeling and predicting, but more cost and execution time. To test the method against a new cluster addition, we have prepared a new setup (under the premise that 8 data nodes core 12, ALK 10, ALK 64, some data disk 6 and some recommendations will be executed)."}, {"heading": "5.2.2 Use cases", "text": "Finding the minimum number of executions to be able to define the behavior of our Hadoop environment helps save time and / or money. Also, the operator wants to prioritize executions by first executing those that provide more information about the system, and then executing the rest in descending order of relevance, useful when testing or comparing our environment for modifications, health checks, or validation of cloning inserts. Also, when adding new benchmarks or resources, it is common for the new benchmark to resemble another previously seen benchmark in behavior, or for one hardware component to resemble another in behavior. Instead of testing it based on random executions, we could use the most important executions for the most similar environments seen to test it, and although the results may not match well with previous models (in fact, the new environment may be different), we use the observed results as a starting point to build a new model for selecting joop close to other platforms."}, {"heading": "5.3 Ranking Features", "text": "Knowing how variables affect execution is important, and it can be retrieved from studying the learned model, but the ranking of variables from fast to slow, and detecting which variables generate such a gradient, can be very handy if the model is not easy to interpret (such as neural networks or nearest neighbors). Selecting a subspace of configurations (for practical reasons), we can predict all configurations that are tested and not tested, and the configurations from slower to faster. Then, we can find which variables cause the biggest changes in such a ranking, which indicate which generate the largest gap, and so on for each variable choice.We can use several methods for ranking variables, such as the Gini factor against the execution time. Also, we can use another method that separates the ranking configurations in a dichotomous manner (algorithm 1), after determining which variables better separates the slower configurations."}, {"heading": "6 CONCLUSION", "text": "In this article, we have described ALOJA-ML, a set of tools for automated modeling and prediction tasks via benchmarking data repositories, part of the ALOJA project. ALOJAML identifies key features of the workload through machine learning, in this case from the Hadoop ecosystem, to predict performance characteristics for a workload execution on a given set of deployment parameters that have not previously been studied in the test infrastructure. The presented work includes some selected application techniques of the ALOJA-ML tool within the framework of the ALOJA platform. One of the techniques presented is used to guide and select the most representative runs of an application that need to be characterized in a deployment in order to reduce the number of samples required. Another technique is to identify anomalies on large groups of job executions in order to automatically filter failed runs. The last presented technique is to determine the order of work available from the configuration parameters to determine a new one of available resources."}, {"heading": "ACKNOWLEDGMENTS", "text": "This project was funded by the European Research Council (ERC) under the European Union's Horizon 2020 research and innovation programme (grant agreement no. 639595) and is partially supported by the Spanish Ministry of Economy under the contracts TIN2012-34557 and 2014SGR1051."}], "references": [{"title": "Instance-based learning algorithms", "author": ["D. Aha", "D. Kibler"], "venue": "Machine Learning,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 1991}, {"title": "Scale-up vs scale-out for hadoop: Time to rethink", "author": ["R. Appuswamy", "C. Gkantsidis", "D. Narayanan", "O. Hodson", "A. Rowstron"], "venue": "In Proceedings of the 4th Annual Symposium on Cloud Computing,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2013}, {"title": "Power-aware multi-data center management using machine learning", "author": ["J.L. Berral", "R. Gavald\u00e0", "J. Torres"], "venue": "In 42nd Intl. Conf. on Parallel Processing,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2013}, {"title": "The weka data mining software: An update", "author": ["M. Hall", "E. Frank", "G. Holmes", "B. Pfahringer", "P. Reutemann", "I.H. Witten"], "venue": "SIGKDD Explor. Newsl.,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2009}, {"title": "Hadoop Performance Tuning", "author": ["D. Heger"], "venue": "https: //hadoop-toolkit.googlecode.com/files/White% 20paper-HadoopPerformanceTuning.pdf", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2015}, {"title": "Hadoop Performance Tuning - A pragmatic & iterative approach", "author": ["D. Heger"], "venue": "DH Technologies,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2013}, {"title": "Starfish: A self-tuning system for big data analytics", "author": ["H. Herodotou", "H. Lim", "G. Luo", "N. Borisov", "L. Dong", "F.B. Cetin", "S. Babu"], "venue": "In CIDR,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2011}, {"title": "The HiBench benchmark suite: Characterization of the MapReduce-based data analysis", "author": ["S. Huang", "J. Huang", "J. Dai", "T. Xie", "B. Huang"], "venue": "Data Engineering Workshops, 22nd Intl. Conf. on,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2010}, {"title": "Global Hadoop Market", "author": ["L. Person"], "venue": "Allied market research,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2014}, {"title": "ALOJA: A systematic study of hadoop deployment variables to enable automated characterization of cost-effectiveness", "author": ["N. Poggi", "D. Carrera", "A. Call", "S. Mendoza", "Y. Becerra", "J. Torres", "E. Ayguad\u00e9", "F. Gagliardi", "J. Labarta", "R. Reinauer", "N. Vujic", "D. Green", "J. Blakeley"], "venue": "IEEE Intl. Conf. on Big Data,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2014}, {"title": "Learning with continuous classes", "author": ["R.J. Quinlan"], "venue": "Australian Joint Conference on Artificial Intelligence, Singapore,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 1992}, {"title": "A Language and Environment for Statistical Computing", "author": ["R R Core Team"], "venue": "R Foundation for Statistical Computing,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2014}, {"title": "Active and accelerated learning of cost models for optimizing scientific applications", "author": ["P. Shivam", "S. Babu", "J. Chase"], "venue": "In Proceedings of the 32nd Intl. Conf. on Very Large Data Bases,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2006}, {"title": "Modern Applied Statistics with S", "author": ["W.N. Venables", "B.D. Ripley"], "venue": null, "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2002}, {"title": "Induction of model trees for predicting continuous classes", "author": ["Y. Wang", "I.H. Witten"], "venue": "In Poster papers of the 9th European Conference on Machine Learning. Springer,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 1997}, {"title": "Hadoop: The Definitive Guide", "author": ["T. White"], "venue": "O\u2019Reilly Media Inc.,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2009}, {"title": "Machine learning for on-line hardware reconfiguration", "author": ["J. Wildstrom", "P. Stone", "E. Witchel", "M. Dahlin"], "venue": "In 20th Intl. Joint Conference on Artifical Intelligence, IJCAI\u201907,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2007}, {"title": "Density-Based Clustering in Spatial Databases: The Algorithm GDBSCAN and Its Applications", "author": ["J. Sander", "M. Ester", "H. Kriegel", "X. Xu"], "venue": "In Data Mining Knowledge Discovery,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 1998}, {"title": "Vagrant, a tool for building complete development environments", "author": ["M. Hashimoto"], "venue": "http://www.vagrantup.com/ (May", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2015}], "referenceMentions": [{"referenceID": 8, "context": "DURING the last years Hadoop has emerged as the main framework for Big Data processing, and its adoption continues at a compound annual growth rate of 58% [12].", "startOffset": 155, "endOffset": 159}, {"referenceID": 5, "context": "Hardware component choices and tunable software parameters, from both Hadoop and the Java run-time deployments, have a high impact on performance [9], [10].", "startOffset": 146, "endOffset": 149}, {"referenceID": 6, "context": "Hardware component choices and tunable software parameters, from both Hadoop and the Java run-time deployments, have a high impact on performance [9], [10].", "startOffset": 151, "endOffset": 155}, {"referenceID": 9, "context": "On-premise deployments or cloud services, produce different behavior patterns during the execution, adding another level of complexity [13].", "startOffset": 135, "endOffset": 139}, {"referenceID": 4, "context": "Studies show that Hadoop execution performance can improve up to three times at least from the default configuration for most deployments [8].", "startOffset": 138, "endOffset": 141}, {"referenceID": 4, "context": "Such changes are rarely pushed into the main distribution, maintaining its condition of standard and default, so running without speedups or performance tweaks [8].", "startOffset": 160, "endOffset": 163}, {"referenceID": 1, "context": "There is also evidence that Hadoop performance is quite poor on new or scale-up hardware [5], also scaling out in number of servers tends to improve performance but paying extra running costs like power, also storage space [5].", "startOffset": 89, "endOffset": 92}, {"referenceID": 1, "context": "There is also evidence that Hadoop performance is quite poor on new or scale-up hardware [5], also scaling out in number of servers tends to improve performance but paying extra running costs like power, also storage space [5].", "startOffset": 223, "endOffset": 226}, {"referenceID": 7, "context": "Also we present results on modeling and prediction of execution times for a set of Hadoop deployments and the HiBench benchmarking suite [11].", "startOffset": 137, "endOffset": 141}, {"referenceID": 7, "context": "However, ALOJA currently features executions from the Intel HiBench open-source benchmark suite [11], which can be more realistic and comprehensive than the supplied examples in Hadoop distributions.", "startOffset": 96, "endOffset": 100}, {"referenceID": 18, "context": "The platform includes a Vagrant virtual machine [23] with a sand-box environment and sample executions used for development and early experiments, for users to create their own repositories or data analysis.", "startOffset": 48, "endOffset": 52}, {"referenceID": 9, "context": "In the project site [1] there is more technical documentation for further usage and tool development, also preliminary versions of this work and project were presented in [25] and [13].", "startOffset": 180, "endOffset": 184}, {"referenceID": 4, "context": "As previously said, for most deployments, execution performance can be improved by at least 3 times from the default Hadoop configuration [8], and the emergence of Hadoop in the industry has led to several attempts at tuning towards performance optimization, new schemes for proper data distribution or partition, and adjustments in hardware configurations to increase scalability or reduce running costs.", "startOffset": 138, "endOffset": 141}, {"referenceID": 4, "context": "An option to speed-up computing systems would be to scale-up or add new (and thus improved) hardware, but unfortunately there is evidence that Hadoop performs poorly in such situations, also scaling out in number of servers improve performance but at the increased costs of infrastructure, power and required storage [8].", "startOffset": 317, "endOffset": 320}, {"referenceID": 6, "context": "[10], focus on the need for tuning Hadoop configurations to match specific workload requirements.", "startOffset": 0, "endOffset": 4}, {"referenceID": 16, "context": "Wildstrom [20] proposed modeling system behaviors vs.", "startOffset": 10, "endOffset": 14}, {"referenceID": 12, "context": "Shivam [16]) modeled computational-science applications allowing prediction of their execution time in grid infrastructures.", "startOffset": 7, "endOffset": 11}, {"referenceID": 17, "context": "\u2022 Advanced analysis tools like DBSCAN [22] to detect different algorithm phases as well as regions of different subroutines with similar behavior inside Hadoop jobs.", "startOffset": 38, "endOffset": 42}, {"referenceID": 10, "context": "The selected methods are explained as follows: Regression tree algorithm: we use the M5P [14], [18] from the RWeka toolkit.", "startOffset": 89, "endOffset": 93}, {"referenceID": 14, "context": "The selected methods are explained as follows: Regression tree algorithm: we use the M5P [14], [18] from the RWeka toolkit.", "startOffset": 95, "endOffset": 99}, {"referenceID": 0, "context": "Nearest neighbor algorithm: we use the IBk [4], also from the RWeka toolkit.", "startOffset": 43, "endOffset": 46}, {"referenceID": 13, "context": "Neural networks: we use a 1-hidden-layer FFANN from nnet R package [17] with pre-tuned parameters as the complexity of parameter tuning in neural nets require enough error and retrial to not provide a proper usage of the rest of tools of the framework.", "startOffset": 67, "endOffset": 71}, {"referenceID": 11, "context": "Polynomial regression: a baseline method for prediction, from the R core package [15].", "startOffset": 81, "endOffset": 85}, {"referenceID": 2, "context": "cluster allows the system to schedule jobs better, improving consolidation and de-consolidation processes [6], and reduce resource consumptions by maintaining any quality of service or job deadline preservation.", "startOffset": 106, "endOffset": 109}, {"referenceID": 11, "context": ", apply the kmeans algorithm [15]), obtain for each cluster a representative observation (i.", "startOffset": 29, "endOffset": 33}], "year": 2017, "abstractText": "This article presents the ALOJA project and its analytics tools, which leverages machine learning to interpret Big Data benchmark performance data and tuning. ALOJA is part of a long-term collaboration between BSC and Microsoft to automate the characterization of cost-effectiveness on Big Data deployments, currently focusing on Hadoop. Hadoop presents a complex run-time environment, where costs and performance depend on a large number of configuration choices. The ALOJA project has created an open, vendor-neutral repository, featuring over 40,000 Hadoop job executions and their performance details. The repository is accompanied by a test-bed and tools to deploy and evaluate the cost-effectiveness of different hardware configurations, parameters and Cloud services. Despite early success within ALOJA, a comprehensive study requires automation of modeling procedures to allow an analysis of large and resource-constrained search spaces. The predictive analytics extension, ALOJA-ML, provides an automated system allowing knowledge discovery by modeling environments from observed executions. The resulting models can forecast execution behaviors, predicting execution times for new configurations and hardware choices. That also enables model-based anomaly detection or efficient benchmark guidance by prioritizing executions. In addition, the community can benefit from ALOJA data-sets and framework to improve the design and deployment of Big Data applications.", "creator": "LaTeX with hyperref package"}}}