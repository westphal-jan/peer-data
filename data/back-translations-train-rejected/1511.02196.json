{"id": "1511.02196", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "6-Nov-2015", "title": "Evaluating Protein-protein Interaction Predictors with a Novel 3-Dimensional Metric", "abstract": "In order for the predicted interactions to be directly adopted by biologists, the ma- chine learning predictions have to be of high precision, regardless of recall. This aspect cannot be evaluated or numerically represented well by traditional metrics like accuracy, ROC, or precision-recall curve. In this work, we start from the alignment in sensitivity of ROC and recall of precision-recall curve, and propose an evaluation metric focusing on the ability of a model to be adopted by biologists. This metric evaluates the ability of a machine learning algorithm to predict only new interactions, meanwhile, it eliminates the influence of test dataset. In the experiment of evaluating different classifiers with a same data set and evaluating the same predictor with different datasets, our new metric fulfills the evaluation task of our interest while two widely recognized metrics, ROC and precision-recall curve fail the tasks for different reasons.", "histories": [["v1", "Fri, 6 Nov 2015 19:14:09 GMT  (710kb)", "http://arxiv.org/abs/1511.02196v1", "This article is an extended version of a poster presented in AMIA TBI 2015"]], "COMMENTS": "This article is an extended version of a poster presented in AMIA TBI 2015", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["haohan wang", "madhavi k ganapathiraju"], "accepted": false, "id": "1511.02196"}, "pdf": {"name": "1511.02196.pdf", "metadata": {"source": "CRF", "title": "Evaluating Protein-protein Interaction Predictors with a Novel 3-Dimensional Metric", "authors": ["Haohan Wang", "Madhavi K. Ganapathiraju"], "emails": [], "sections": [{"heading": null, "text": "For predicted interactions to be directly adopted by biologists, the predictions of machine learning must be highly accurate, regardless of recall. This aspect cannot be evaluated or numerically well represented by traditional metrics such as accuracy, ROC, or precision recall curve. In this work, we start from the alignment of ROC sensitivity and recall of the precision recall curve, and propose an evaluation metric that focuses on the ability of a model to be adopted by biologists. This metric evaluates the ability of a machine learning algorithm to predict only new interactions while eliminating the influence of test data sets. In the experiment to evaluate different classifiers with the same dataset and evaluate the same predictor with different datasets, our new metric fulfills the evaluation task of our interest, while two widely recognized metrics, ROC and the precision recall curve do not fulfill the tasks for different reasons 1."}, {"heading": "1. Introduction", "text": "This year, it has come to the point where we are in a time, we are in a time, we are in a time, we are in a time, we are in a time, we are in a time, we are in a time, we are in a time, we are in a time, we are in a time, we are able to change the world, \"he said."}, {"heading": "2. Different Evaluation Metrics and their Constraints in Evaluating Real", "text": "In this section, we will talk about various existing benchmarks and the reasons for their disadvantages. We will first focus on the three most common metrics: Accuracy, ROC and Precision Recall, then briefly talk about other metrics. Confusion Matrix2, however, is used repeatedly in these metrics. Accuracy is one of the most widely used benchmarks for predicting tasks. It can simply be explained as the percentage of correctly predicted labels, regardless of whether they are positive or negative. It is called (tp + fn)."}, {"heading": "3. New Evaluation Metric", "text": "In this sector, we are able to correct the mistakes that have been mentioned, in the way that we have done them and in the way that we have done them, the way that we have done them, the way that we have done them, the way that we have done them."}, {"heading": "4. Evaluation on Simple Probabilistic Partial Oracle Predictor and Toy Data", "text": "In this section, we will first present the simple predictors we are designing, and then we will compare our new yardsticks with \u03b2-metrics, which will be compared with the two most widely accepted metrics, ROC and the precision callback curve on these simple predictors, with some data we have designed. However, we will illustrate the advantage of our evaluation metrics with these simple examples, and create a less certain prediction for positive data. First, let us define P (\u03b1, \u03b2) as our predictor, which predicts a probability for each instance, such as the probability of being a positive instance. It predicts horribly correct labeling for all positive data with the probability \u03b1, and generates a less certain prediction for positive results and generates a less certain prediction for positive results. A deterministic prediction is a prediction with a confidence value of 1 for positive data and 0 for negative data."}, {"heading": "5. Conclusion", "text": "In this work, we begin with the goal of machine learning models to accelerate laboratory biological research, propose an assessment metric that can describe the ability of a predictor to be adopted by biology researchers. We emphasize that a machine learning model should predict positive interactions with very high reliability, thus saving laboratory effort. We propose a new assessment metric that focuses mainly on the true positive and false positive rate, releasing the 8 effect of a positive share of data, taking into account false positive / true negative rates and the recall of a classifier that can select the best predictors of positive interactions. We evaluate our assessment metric with some toy models of machine learning and show that the test result is consistent with what we believe. In the future, we will use our assessment metric to evaluate some real protein predictors and make a better comparison between these predictors for use in biological research."}, {"heading": "Acknowledgment", "text": "This work was funded by the Biobehavioral Research Awards for Innovative New Scientists (BRAINS) R01MH094564, awarded to MKG by the National Institute of Mental Health of National Institutes of Health (NIMH / NIH) of the USA."}, {"heading": "APPENDIX A: High Resolution Version of Figures.", "text": "Figure 1 (a) Figure 1 (b) 12Figure 2 (a) Figure 2 (b) 14Figure 3 (a) Figure 3 (b)"}], "references": [{"title": "Measuring classification performance: the hmeasure package", "author": ["C. Anagnostopoulos"], "venue": null, "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2012}, {"title": "Selecting and interpreting measures of thematic classification accuracy", "author": ["Stehman", "Stephen V"], "venue": "Remote sensing of Environment", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 1997}, {"title": "The meaning and use of the area under a receiver operating characteristic (ROC) curve", "author": ["Hanley", "James A", "Barbara J. McNeil"], "venue": null, "citeRegEx": "3", "shortCiteRegEx": "3", "year": 1982}, {"title": "An introduction to ROC analysis", "author": ["Fawcett", "Tom"], "venue": "Pattern recognition", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2006}, {"title": "Evaluation: from precision, recall and F-measure to ROC, informedness, markedness and correlation", "author": ["Powers", "David Martin"], "venue": null, "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2011}, {"title": "Evaluating diagnostic tests: the area under the ROC curve and the balance of errors", "author": ["Hand", "David J"], "venue": "Statistics in Medicine", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2010}, {"title": "The relationship between Precision-Recall and ROC curves", "author": ["Davis", "Jesse", "Mark Goadrich"], "venue": "Proceedings of the 23rd international conference on Machine learning", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2006}, {"title": "Area Under the Precision-Recall Curve: Point Estimates and Confidence Intervals. Machine Learning and Knowledge Discovery in Databases", "author": ["Boyd", "Kendrick", "Kevin H. Eng", "C. David Page"], "venue": null, "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2013}, {"title": "Cost curves: An improved method for visualizing classifier performance", "author": ["Drummond", "Chris", "Robert C. Holte"], "venue": null, "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2006}, {"title": "What ROC Curves Can't Do (and Cost Curves Can)", "author": ["Drummond", "Chris", "Robert C. Holte"], "venue": "ROCAI", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2004}, {"title": "Measuring classifier performance: a coherent alternative to the area under the ROC curve. Machine learning", "author": ["Hand", "David J"], "venue": null, "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2009}, {"title": "A coherent interpretation of AUC as a measure of aggregated classification performance", "author": ["Ferri", "C\u00e8sar", "Jos\u00e9 Hern\u00e1ndez-Orallo", "Peter A. Flach"], "venue": "Pro ceedings of the 28th International Conference on Machine Learning", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2011}, {"title": "The AUK: A simple alternative to the AUC", "author": ["Kaymak", "Uzay", "Arie Ben-David", "Rob Potharst"], "venue": "Engineering Applications of Artificial Intelligence", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2012}, {"title": "Beyond kappa: A review of interrater agreement measures", "author": ["Banerjee", "Mousumi"], "venue": "Canadian Journal of Statistics", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 1999}], "referenceMentions": [{"referenceID": 2, "context": "Most of them are found to be promising with one or several traditional evaluation metrics, like accuracy, ROC [3-4], or precision recall curve [5].", "startOffset": 110, "endOffset": 115}, {"referenceID": 3, "context": "Most of them are found to be promising with one or several traditional evaluation metrics, like accuracy, ROC [3-4], or precision recall curve [5].", "startOffset": 110, "endOffset": 115}, {"referenceID": 4, "context": "Most of them are found to be promising with one or several traditional evaluation metrics, like accuracy, ROC [3-4], or precision recall curve [5].", "startOffset": 143, "endOffset": 146}, {"referenceID": 0, "context": "Besides these two main goals, we also consider several general criteria for an evaluation metric mentioned by Anagnostopous et al [1]: 1) Simple intuition.", "startOffset": 130, "endOffset": 133}, {"referenceID": 5, "context": "However, in the most widely recognized interpretation of ROC applied to evaluating protein interaction model, it evaluates regardless of the costs of two types of errors [6], emphasizing both the abilities of predicting new interaction and excluding non-interaction equivalently.", "startOffset": 170, "endOffset": 173}, {"referenceID": 6, "context": "Area under precision recall curve is not very widely, but this numeric value is still used in some work comparing ROC and precision recall curve [7-8].", "startOffset": 145, "endOffset": 150}, {"referenceID": 7, "context": "Area under precision recall curve is not very widely, but this numeric value is still used in some work comparing ROC and precision recall curve [7-8].", "startOffset": 145, "endOffset": 150}, {"referenceID": 8, "context": "4 Cost curve [9-10] is a point/line duality of ROC curve.", "startOffset": 13, "endOffset": 19}, {"referenceID": 9, "context": "4 Cost curve [9-10] is a point/line duality of ROC curve.", "startOffset": 13, "endOffset": 19}, {"referenceID": 10, "context": "H-measure [11] is a special case of cost curve, what basically shows the same ability and constraints of cost curve.", "startOffset": 10, "endOffset": 14}, {"referenceID": 11, "context": "[12]", "startOffset": 0, "endOffset": 4}, {"referenceID": 12, "context": "AUK (Area Under Kappa) [13] is another metric that focuses on the Kappa statistics [14].", "startOffset": 23, "endOffset": 27}, {"referenceID": 13, "context": "AUK (Area Under Kappa) [13] is another metric that focuses on the Kappa statistics [14].", "startOffset": 83, "endOffset": 87}], "year": 2015, "abstractText": "In order for the predicted interactions to be directly adopted by biologists, the machine learning predictions have to be of high precision, regardless of recall. This aspect cannot be evaluated or numerically represented well by traditional metrics like accuracy, ROC, or precision-recall curve. In this work, we start from the alignment in sensitivity of ROC and recall of precision-recall curve, and propose an evaluation metric focusing on the ability of a model to be adopted by biologists. This metric evaluates the ability of a machine learning algorithm to predict only new interactions, meanwhile, it eliminates the influence of test dataset. In the experiment of evaluating different classifiers with a same data set and evaluating the same predictor with different datasets, our new metric fulfills the evaluation task of our interest while two widely recognized metrics, ROC and precision-recall curve fail the tasks for different reasons. 1", "creator": "Word"}}}