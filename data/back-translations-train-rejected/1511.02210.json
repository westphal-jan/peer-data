{"id": "1511.02210", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "6-Nov-2015", "title": "Learning Optimized Or's of And's", "abstract": "Or's of And's (OA) models are comprised of a small number of disjunctions of conjunctions, also called disjunctive normal form. An example of an OA model is as follows: If ($x_1 = $ `blue' AND $x_2=$ `middle') OR ($x_1 = $ `yellow'), then predict $Y=1$, else predict $Y=0$. Or's of And's models have the advantage of being interpretable to human experts, since they are a set of conditions that concisely capture the characteristics of a specific subset of data. We present two optimization-based machine learning frameworks for constructing OA models, Optimized OA (OOA) and its faster version, Optimized OA with Approximations (OOAx). We prove theoretical bounds on the properties of patterns in an OA model. We build OA models as a diagnostic screening tool for obstructive sleep apnea, that achieves high accuracy with a substantial gain in interpretability over other methods.", "histories": [["v1", "Fri, 6 Nov 2015 19:55:59 GMT  (127kb,D)", "http://arxiv.org/abs/1511.02210v1", null]], "reviews": [], "SUBJECTS": "cs.AI", "authors": ["tong wang", "cynthia rudin"], "accepted": false, "id": "1511.02210"}, "pdf": {"name": "1511.02210.pdf", "metadata": {"source": "CRF", "title": "Learning Optimized Or\u2019s of And\u2019s", "authors": ["Tong Wang", "Cynthia Rudin"], "emails": ["tongwang@mit.edu", "rudin@mit.edu"], "sections": [{"heading": null, "text": "An example of an open access model is the following: If (x1 = \"blue\" AND x2 = \"medium\") OR (x1 = \"yellow\"), then we predict Y = 1, otherwise we predict Y = 0. Or the open access model has the advantage that it is interpretable by human experts because it is a set of conditions that concisely capture the characteristics of a certain subset of data. We present two optimization-based machine learning frameworks for the creation of open access models, Optimized OA (OOA) and its faster version, Optimized OA with Approximations (OOAx). We prove theoretical limitations regarding the characteristics of patterns in an open access model. We build open access models as a diagnostic screening tool for obstructive sleep apnea that achieves high accuracy with a significant gain in interpretability over other methods."}, {"heading": "1 Introduction", "text": "In fact, it is so that most of them are able to survive themselves, and that they are able to survive themselves, \"he said.\" But it is not so that they are able to survive themselves. \"Indeed,\" it is not so that they are able to survive themselves, but that they are able to survive themselves. \"Indeed,\" it is not so that they are able to survive themselves. \""}, {"heading": "2 Optimized Or\u2019s of And\u2019s", "text": "We work with a dataset S = {(Xn, Yn)} Nn, which consists of N examples with J attributes of mixed type. Yn-1, \u2212 1} represents the labels. Numerical attributes are indexed by the index set Jn and categorical attributes are indexed by Jc. The j attribute of the n example is called Xnj. An open access classifier consists of a series of patterns that characterize a single class, here the positive class. Each pattern is a conjunction of conditions (literals), and the number of conditions is called the length of a pattern. For example: The length of the pattern \"age \u2265 30 AND has hypertension AND is female\" is 3. Let z denote a pattern, and 1z (X) indicates whether X satisfies the pattern. A stands for a series of patterns. An OA classification based on A (1) is as follows: A (1)."}, {"heading": "2.1 MIP Formulation", "text": "We formulate a mixed integer program to generate a pattern set with numerical and categorical attributes. The MIP uses the following target L (A) to minimize the training error while maintaining the smallness of the model.L (A) = # Error (A) N + C1 # Literale (A) + C2 # Pattern (A). (2) The first term in the target is the loss function that counts the number of classification errors. The two terms are scaled by the parameters C1 and C2 to punish the complexity of the model.C1 represents the percentage of training errors in A, and 2) the total number of patterns in A that are called # Patterns (A).The two terms are scaled by the parameters C1 and C2 to punish the complexity of the model.C1 represents the percentage of training errors that the user is willing to deal with."}, {"heading": "2.1.1 Literals for Numerical Attributes", "text": "For a numerical attribute, a literal j (for simplicity, if we refer to a literal j) has the form \"lkj \u2264 X \u00b7 j \u2264 ukj,\" where ukj and lkj represent the upper and lower boundaries of the range in the letter j. k is a pattern index. For each example, Xn indicates whether Xnj meets the upper boundary of the letter j. k is a pattern index. That is, u, nkj = 1, if Xnj \u2212 ukj = 1, and l \u2212 kj = 1, if Xnj meets the lower boundary of the letter lkj."}, {"heading": "2.1.2 Literals for Categorical Attributes", "text": "For a categorical attribute, a literal j has the form \"X \u00b7 j = the v-th category,\" where v = 1,... Vj} is an index for categories of the attribute j and Vj is the total number of categories. To determine whether Xn meets the condition in the literal j, we specify whether Xnj corresponds to the category contained in that letter. To determine whether Xnj meets the v-th category of the attribute j, we specify whether Xnj is equal to the category contained in that letter. We convert the binary code Xnj to Xnjv so that Xnjv = 1, if Xnj takes the v-th category of the attribute j."}, {"heading": "2.1.3 Counting Classification Errors", "text": "For categorical attributes, we consider both cases in which a literal is substantial, i.e., if the literal is substantial, the MIP must check whether a data point satisfies both upper and lower boundaries of the range, i.e. whether the upper boundaries of the range are specified by u-nkj and l-nkj. If the literal is not substantial, i.e. ukj = Uj and lkj = Lj, then the MIP must satisfy both upper and lower boundaries of the range, i.e., if the letter is non-substantial, i.e. ukj = Uj and lkj = Lj, then we must u-nkj = 1 for all Xn. Using categorical nk-nk-n-nk-n-n-n-n-n-n-n-n-n-n-n-n-n-n-n-n-n-n-n-n-n-n-n-n-n-n-category situations, we are able to specify whether Xn is."}, {"heading": "2.1.4 The Objective", "text": "The MIP minimizes the number of MIP variables between 1N N, N, N, N, 1, N, C1, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K"}, {"heading": "3 Optimized Or\u2019s of And\u2019s with Approximations", "text": "To speed up the learning process, we suggest Optimized Or's of And's with Approximations (OOAx), which decouple from the optimization process, the first two aspects of complexity mentioned above. OOAx uses a pre-mining approach and then selects an approach. It uses sophisticated pattern mining techniques to generate a set of patterns, then applies a secondary criterion to further examine the rules to form a candidate pattern group. Finally, a holistic linear program (ILP) searches within these patterns for an optimal group. This method consists of the following three steps: Pattern Mining, Pattern Screening, and Pattern Selection."}, {"heading": "3.1 Pattern Mining", "text": "There are many commonly used pattern mining methods, such as FP growth (Han, Pei and Yin, 2000), Apriori (Agrawal, Srikant and others, 1994), Eclat (Zaki et al., 1997), etc. In our implementation, we use FP growth in Pythons (Borgelt, 2005), which uses binary encoded data and user-specified minimum support and maximum length to create patterns that meet both requirements; the algorithm runs sufficiently fast (usually less than a second for thousands of observations); since the FP growth algorithm handles binary data, we discredit the numerical attributes by manually selecting the thresholds for containers."}, {"heading": "3.2 Pattern Screening", "text": "In the pattern mining step, the number of patterns generated, even for a medium-sized dataset, is usually overwhelming. For example, millions of patterns are generated for the sleep apnea dataset (which we will discuss in detail in the experiment sections) of 1192 patients and 112 binary-coded attributes, when the maximum length is 3 and the minimum support is 5%. Ideally, we want the pattern set to contain thousands of patterns for the calculation, so we use a secondary criterion to investigate the patterns further. Result (z) = InfoGain (S | z) \u2212 \u03b3lz. (22) This criterion takes into account the classification power of a pattern, measured by information gain InfoGain (S | z), and the sparseness, measured by the length of the pattern lz information gain on data S is InfoGain (S | z) \u2212 \u03b3lz = H (S) \u2212 H (S | z), where the entropy of the pattern is K."}, {"heading": "3.3 Pattern Selecting", "text": "The previous two steps considerably reduce the calculation burden by feeding the last step with a series of high-quality candidate samples. Now, our goal is to select an optimal group A from the candidate group P. We formulate an ILP using the same objective (2) and present it below. (min) We formulate an ILP with the same objective (2) and present it. (min) K1N N N N N N = 1 + C1 KP KP KP + C2 KP K + KP = 1 perspecnk.k, (24) KP K + KP K = 1 perspecnk.k + (23) KP \u041ak = 1 perspecnk.k, (24) \u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u043d\u043d\u043d\u043d\u043d\u043d\u0435\u043d\u0435\u043d\u0435\u043d\u0435\u043d\u0435\u043d\u0435\u043d\u0435\u043d\u0435\u043d\u0435\u043d\u0435\u043d\u0435\u043d\u0435\u043d\u0435\u043d\u0435\u043d\u0435\u043d\u0435\u043d\u0435\u043d\u0435\u043d\u0435\u043d\u0435\u043d\u0435\u043d\u0435\u043d\u0435\u043d\u0435\u043d\u0435\u043d\u0435\u043d\u0435\u043d\u0435\u043d\u0435\u043d\u0435\u043d\u0435\u043d\u0435\u043d\u0435\u043d\u0435\u043d\u0435\u043d\u0435\u043d\u0435\u043d\u0435\u043d\u0435\u043d\u0435\u043d\u0435\u043d\u0435\u043d\u0435\u043d\u0435\u043d\u0435\u043d\u043d\u0435\u043d\u0435\u043d\u0435\u043d\u0435\u043d\u0435\u043d\u0435\u043d\u0435\u043d\u0435\u043d\u0435\u043d\u0435\u043d\u0435\u043d\u0435\u043d\u0435\u043d\u0435\u043d\u0435\u043d\u0435\u043d\u043d\u0435\u043d\u0435\u043d\u0435\u043d\u0435\u043d\u0435\u043d\u0435\u043d\u0435\u043d\u0435\u043d\u0435\u043d\u0435\u043d\u0435\u043d\u0435\u043d\u0435\u043d\u0435\u043d\u0435\u043d\u0435\u043d\u0435\u043d\u0435\u043d\u0435\u043dneynicnicnicnicnicnicnicnicnicnicnicnicnicnicnicnicnicnicnicnicnicnicnicnicnicnicnicnicnicnicnicnicnicnicnicnicnicnicnicnicnicnicnicnicnicnicnicnicnicnicnicnicnicnicnicnicnicnicnicni"}, {"heading": "4 Analysis on Patterns and OA Models", "text": "In this section we discuss the quality of patterns in an OA classifier. Certain properties of patterns improve computational complexity. We also show the VC dimension of OA models and compare OA classifiers with other discrete classifiers (decision trees and random forests)."}, {"heading": "4.1 Bounds on Patterns", "text": "Define the support of pattern z via dataset S asIS (z) = {X | 1z (X) pattern then sure A pattern (X-S), (26) and the support of pattern z via dataset S assuppS (z) = | IS (z) |. (27) suppS + (z) pattern is called the positive support of z, which is the number of positive examples in IS (z), and suppS \u2212 (z) is called the negative support of z, which is the number of negative examples in IS (z). An OA classifier is essentially an interplay of weaker classifiers, patterns. The inclusion of patterns with a low quality is expensive, and as we will prove unnecessary. First, in Theorem 1, we show that the optimal solution is never a pattern with a high negative support.Theorem 1 Let's take an OA model with regulation saving parameters C1 and C2. The OA model is trained on a dataset S, consisting of positive examples + N."}, {"heading": "4.2 VC Dimension of an OA classifier", "text": "We consider the VC dimension of the hypotheses categories as a series of patterns selected from a prefabricated pattern. There are some results for k-DNF (max.) max. (max.) max. (max.) max. (max.) max. (max.) max. (max.) max. (max.) max. (max.) max. (max.) max. (max.) max. (max.) max. (max.) max. (max.) max. (max.) max. (max.) max. (max.) max. (max.) max. (max.) max. (max.) max. (max.). (max.). (max.). (max.). (max.). (max.). (max.). (max.). (max.). (max.). (max.). (max.). (max.). (max.). (max.)."}, {"heading": "4.3 Comparing with Other Discrete Classifiers", "text": "Like OA classifiers, we simply collect patterns that are positively associated with leaves, in this case, sheets 4 and 6, in a decision that is gray. Like OA classifiers, decision trees and random forests, we also discredit the input patterns and assign a label to each sub-space. We prove that for these models always equivalent OA classifiers exist. These theorems are simple, but for those who have not thought about it.We present the definition of two classifiers that are equivalent. Definition 2 Two classifiers f1, f2 are equivalent if for each input X, f1 (X) = f2 (X).In a decision tree, the leaves divide the input space into areas with different labels, which will be the predicted result for all data ending in that range. A path from the root to a leaf is a conjunction of words, i.e. a pattern see Figure A, and therefore the patterns are equal to a fictitious tree."}, {"heading": "5 Experiments", "text": "Our experiments include the use of open access models for the diagnosis of obstructive sleep apnea (OSA) and experiments with 9 public records from the UCI Machine Learning Repository (Lichman, 2013). In order to create simple open access models for interpretation purposes, we set the maximum number of patterns in all experiments to 5. (In an open access framework, we set K = 5; in an open access framework, we add a restriction that the sum of open access models is less than or equal to 5.) Since we have severely limited the properties of open access models, we expect to lose predictive accuracy over unrestricted basic methods. In many experiments we have conducted, we found that open access models do not lose performance, and most of the time are the most powerful models, while we achieve a significant gain in interpretability."}, {"heading": "5.1 Diagnosing Obstructive Sleep Apnea", "text": "We analyzed polysomnography and self-reported clinical information from 1,922 patients tested in the Massachusetts General Hospital Clinical Sleep Lab, initially analyzed in 2015 (Ustun and Rudin, 2015; Ustun et al., 2015). The goal is to classify which patients entering the MGH Sleep Lab have OSA based on a survey completed at the time of admission. We produce predictive models for OSA screening using attributes from self-reported symptoms and self-reported medical information. Attributes include detailed information such as age, gender, BMI, drowsiness when the patient snores, when the patient wakes during sleep, when he or she slightly falls asleep, the level of fatigue, etc. The dataset was encoded into 112 attributes. Due to the size of these datasets, we chose OOAx for faster compilation."}, {"heading": "5.2 Performance on UCI Datasets", "text": "We applied OOA and OOAx to several UCI data sets and compared them with 5 previously mentioned interpretable models and 2 black box models, random forests and SVM. In the experimental setup, we set a time limit for the MIP within the OOA framework to ensure that there is a solution in due time. Table 2 shows the mean and standard deviation from the sample over 5 folders. We observed that OA classifiers achieve a very competitive performance. For the four categorical data sets in Table 2, OA classifiers always perform better than other models. In particular, for Otic-Tac-Toe and Monks, where there are correct models that correctly classify all examples, OA models are able to detect the correct patterns and achieve 100% accuracy. For numerical and mixed data sets, OA performance models are on par with other learning models, sometimes unexplained by other methods."}, {"heading": "6 Conclusion", "text": "OA models have a long history; they are particularly useful as (i) interpretable screening mechanisms, where much of the data from another round of modelling is not taken into account, and (ii) marketing consideration sets, which are created by humans to reduce cognitive stress in order to make a decision. We introduced two optimization-based frameworks for learning Or's of And's. The first framework, OOA, uses a MIP to build patterns directly from data, and can handle both categorical and numerical data without pre-processing; the second framework, OOAx, reduces computation by pre-mining patterns; and we have set limits on the support for patterns that guarantee that the pattern space can be safely reduced; both methods can produce high-quality OA classifiers, as experiments have shown."}], "references": [], "referenceMentions": [], "year": 2015, "abstractText": "Or\u2019s of And\u2019s (OA) models are comprised of a small number of disjunctions of conjunctions, also called disjunctive normal form. An example of an OA model is as follows: If (x1 = \u2018blue\u2019 AND x2 = \u2018middle\u2019) OR (x1 = \u2018yellow\u2019), then predict Y = 1, else predict Y = 0. Or\u2019s of And\u2019s models have the advantage of being interpretable to human experts, since they are a set of conditions that concisely capture the characteristics of a specific subset of data. We present two optimization-based machine learning frameworks for constructing OA models, Optimized OA (OOA) and its faster version, Optimized OA with Approximations (OOAx). We prove theoretical bounds on the properties of patterns in an OA model. We build OA models as a diagnostic screening tool for obstructive sleep apnea, that achieves high accuracy with a substantial gain in interpretability over other methods.", "creator": "LaTeX with hyperref package"}}}