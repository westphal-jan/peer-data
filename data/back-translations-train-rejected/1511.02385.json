{"id": "1511.02385", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "7-Nov-2015", "title": "Review-Level Sentiment Classification with Sentence-Level Polarity Correction", "abstract": "We propose an effective technique to solving review-level sentiment classification problem by using sentence-level polarity correction. Our polarity correction technique takes into account the consistency of the polarities (positive and negative) of sentences within each product review before performing the actual machine learning task. While sentences with inconsistent polarities are removed, sentences with consistent polarities are used to learn state-of-the-art classifiers. The technique achieved better results on different types of products reviews and outperforms baseline models without the correction technique. Experimental results show an average of 82% F-measure on four different product review domains.", "histories": [["v1", "Sat, 7 Nov 2015 18:38:22 GMT  (302kb,D)", "http://arxiv.org/abs/1511.02385v1", "15 pages. This paper is based on the same sentence-level technique proposed in Orimaye, S. O., Alhashmi, S. M., and Siew, E. G. Buy it-dont buy it: sentiment classification on Amazon reviews using sentence polarity shift. In PRICAI 2012: Trends in Artificial Intelligence, pp. 386-399. Springer Berlin Heidelberg"]], "COMMENTS": "15 pages. This paper is based on the same sentence-level technique proposed in Orimaye, S. O., Alhashmi, S. M., and Siew, E. G. Buy it-dont buy it: sentiment classification on Amazon reviews using sentence polarity shift. In PRICAI 2012: Trends in Artificial Intelligence, pp. 386-399. Springer Berlin Heidelberg", "reviews": [], "SUBJECTS": "cs.CL cs.AI cs.LG", "authors": ["sylvester olubolu orimaye", "saadat m alhashmi", "eu-gene siew", "sang jung kang"], "accepted": false, "id": "1511.02385"}, "pdf": {"name": "1511.02385.pdf", "metadata": {"source": "CRF", "title": "Review-Level Sentiment Classification with Sentence-Level Polarity Correction", "authors": ["Sylvester Olubolu Orimaye", "Saadat M. Alhashmi", "Sang Jung Kang"], "emails": ["sylvester.orimaye@monash.edu)", "salhashmi@sharjah.ac.ae)", "siew.eu-gene@monash.edu)", "sjkan2@student.monash.edu)"], "sections": [{"heading": null, "text": "Tags: Sentiment Analysis, Review-Level Classification, Polarity Correction, Data Mining, Machine Learning"}, {"heading": "1 Introduction", "text": "In fact, most of them will be able to move to another world, where they will be able to move to another world, where they will be able to move to another world, where they will be able to move, where they will be able to move, where they will be able to move."}, {"heading": "2 Related Work", "text": "In fact, most of them are able to survive on their own."}, {"heading": "3 Training Set Correction", "text": "In fact, it is very likely that the negative feelings are expressed within the first part of the review and are then followed by positive feelings. [5] This could lead to reviewers tending to emphasize the negative aspects of a product in the form of positive feelings in the later part of the review that produce some satisfaction. [6] Reviewers tend to emphasize the negative aspects of a product, and in some cases the positive feelings regarding the product that gives them some satisfaction."}, {"heading": "4 Sentence-Level Polarity Correction", "text": "Following the training sentence correction in Section 3, we propose sentence-level polarity correction = to further reduce the misclassification in both \"training\" and \"tests.\" More importantly, the bag-of-words approach has rarely improved the accuracy of a sentient, [3, 4] a sentence-level approach could bring a better improvement, since most feelings are expressed anyway at the sentence level. [38] However, in Section 3, we have pointed out that many review documents tend to contain both positive and negative sentences, regardless of their individual categories (i.e. positive or negative). While the consistent sentence polarities of both categories could be helpful for the classification task, it would be better to remove sentences with exhaustive polarities that cause inconsistencies by using a polarity correction approach. [39, 40, 3] that we remove the inconsistency with an example in Section 1, The idea of polarity correction consists of the sentence-level polarity correction."}, {"heading": "5 Experiment and Results", "text": "We conducted several experiments with our correction method and compared performance with state-of-the-art classifiers with and without our polarity correction techniques. The classifiers include the variant of the Minimum Sequential Optimization (SMO) from Support Vector Machines (SVM), [41] and Na \ufffd \u0131ve Bayes (NB) classifiers. [42] We used SVM and NB on the WEKA machine learning platform [43] with word bag, unigram and word bigram characteristics. We did not consider word trigram characteristics, as both word unigram and word bigram characteristics were examined to improve the tasks of mood classification. [1, 22, 3] We performed a performance evaluation of 80% to 20% to compare them with the baseline on each dataset domain. To select the best parameters for the word unigram and word ram characteristics, we performed hyper-validation of the car parameter search with [44 MB]."}, {"heading": "5.1 Dataset and Baseline", "text": "Our dataset is the multi-domain sentiment dataset constructed by Blitzer et al. [5], first used in 2007 and consisting of online product ratings from Amazon across four different product areas, including beauty products, books, software, and the kitchen. Each product domain has 1,000 positive ratings and 1,000 negative ratings, based on customers \"star ratings according to Blitzer et al. [5] For each domain, we separated 800 documents per category as a training kit and used the remaining 200 documents as an invisible test set. We extracted the review text and performed sentence boundary identification by optimizing the output of the MedlineSentenceModel, which is available as part of the LingPipe library. 2As a starting point, we implemented a sentence-level sensing classifier using a technique similar to Pang and Lee [22] on the same dataset, but without our correction technique."}, {"heading": "5.2 Evaluation", "text": "We used three evaluation variables consisting of precision, recall and F-measure or F-1: Precision is calculated as TP / (TP + FP), recall as TP / (TP + FN) and F-measure as (2 \u0445 precision \u0445 recall) / (precision + recall). Note that TP, TN, FP and FN are defined as true positives, true negatives, false positives or false negatives. All results are based on a confidence interval of 95%."}, {"heading": "5.3 Results and Discussion", "text": "We present the results in Tables 2 - 5, where the model is the type of classifier, Pr. is the precision, Rc. is the recall, and F-1 is the F measure, respectively. We identify the models using our correction technique with \"cor\" after the model names. For1 http: / / www.cs.jhu.edu / mdredze / sentiments / 2 http: / / alias-i.com / lingpipe / docs / api / aliasi / sentences / MedlineSentenceModel.htmlexample \"SVM-Unigram-Cor\" shows a model with unigrammable features and our correction techniques. Standard models are identified by the algorithm name and the feature used. Baseline models are identified with \"baseline,\" we identify our best performing model with the baseline and the comparable performance with standard models."}, {"heading": "6 Conclusions", "text": "In this thesis, we have proposed a training set and a sentence-level polarity correction for the task of sentiment classification of assessment documents. We have conducted experiments in various Amazon product rating areas and have shown that a sentiment classifier with training sets and sentence-level polarity corrections performed better and outperformed a state-of-the-art sentiment classification in all rating areas. Our correction methods first eliminate polarity distortions from the training set and then inconsistent sentence-level polarity corrections from training and testing sets. Given the difficulty of the task of sentiment classification [3], we believe that the improvement shown by the correction method is promising and could lead to the construction of a more accurate sentiment classification. In the future, we will integrate training techniques and polarity correction at the sentence level as part of an independent sentiment recognition algorithm [3] and integrate larger experiments on McEdit web data sets such as Amazed.AP Data Auko.html: Amazon / 3 have been prepared."}], "references": [{"title": "Thumbs up?: sentiment classification us- ing machine learning techniques,\u201d in Proceedings of the ACL-02 conference on Em- pirical methods in natural language processing", "author": ["B. Pang", "L. Lee", "S. Vaithyanathan"], "venue": "Association for Computational Linguistics,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2002}, {"title": "Facet-based opinion retrieval from blogs,", "author": ["O. Vechtomova"], "venue": "Information Processing & Management,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2010}, {"title": "Sentiment analysis and opinion mining,", "author": ["B. Liu"], "venue": "Synthesis Lectures on Human Language Technologies,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2012}, {"title": "New avenues in opinion mining and sentiment analysis,", "author": ["E. Cambria", "B. Schuller", "Y. Xia", "C. Havasi"], "venue": "IEEE Intelligent Systems,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2013}, {"title": "Biographies, bollywood, boom-boxes and blenders: Domain adaptation for sentiment classification,", "author": ["J. Blitzer", "M. Dredze", "F. Pereira"], "venue": "(Association of Compu- tational Linguistics (ACL)),", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2007}, {"title": "G\u00f3mez-Rod\u0155\u0131guez, \u201cA syntactic approach for opinion mining on spanish reviews,", "author": ["D. Vilares", "M.A. Alonso"], "venue": "Natural Language Engineering,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2015}, {"title": "What computing with words means to me [discussion fo- rum],", "author": ["J. Mendel", "L. Zadeh", "E. Trillas", "R. Yager", "J. Lawry", "H. Hagras", "S. Guadarrama"], "venue": "Computational Intelligence Magazine, IEEE,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2010}, {"title": "A hierarchical approach to mood classification in blogs,", "author": ["F. Keshtkar", "D. Inkpen"], "venue": "Natural Language Engineering,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2012}, {"title": "On the difficulty of automatically detecting irony: beyond a simple case of negation,", "author": ["A. Reyes", "P. Rosso"], "venue": "Knowledge and Information Systems,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2014}, {"title": "Selection of correction candidates for the normalization of spanish user-generated content,", "author": ["M. Melero", "M. Costa-Juss\u00e0", "P. Lambert", "M. Quixal"], "venue": "Natural Lan- guage Engineering,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2014}, {"title": "Building a concept-level sentiment dictionary based on commonsense knowledge,", "author": ["A. Tsai", "R. Tsai", "J. Hsu"], "venue": "IEEE Intelligent Systems,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2013}, {"title": "Enhanced sen- ticnet with affective labels for concept-based opinion mining,", "author": ["S. Poria", "A. Gelbukh", "A. Hussain", "D. Das", "S. Bandyopadhyay"], "venue": "IEEE Intelligent Systems,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2013}, {"title": "Sentiment and behaviour annotation in a corpus of dialogue summaries,", "author": ["N.T. Roman", "P. Piwek", "A.M.B.R. Carvalho", "A.R. Alvares"], "venue": "Journal of Universal Computer Science,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2015}, {"title": "Opinion mining and sentiment analysis,", "author": ["B. Pang", "L. Lee"], "venue": "Foundations and Trends in Information Retrieval,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2008}, {"title": "Sentiment polarity identification in financial news: A cohesion-based approach,", "author": ["A. Devitt", "K. Ahmad"], "venue": "Proceedings of the 45th Annual Meeting of the Asso- ciation of Computational Linguistics,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2007}, {"title": "An introduction to evolutionary com- putation in finance,", "author": ["A. Brabazon", "M. O\u2019Neill", "I. Dempsey"], "venue": "Computational Intelligence Magazine, IEEE,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2008}, {"title": "Evaluating and un- derstanding text-based stock price prediction models,", "author": ["E.J. Fortuny", "T.D. Smedt", "D. Martens", "W. Daelemans"], "venue": "Information Processing & Management,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2014}, {"title": "Extraction of unexpected sen- tences: A sentiment classification assessed approach,", "author": ["D. Li", "A. Laurent", "P. Poncelet", "M. Roche"], "venue": "Intelligent Data Analysis,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2010}, {"title": "The effect of negation on sentiment analysis and retrieval effectiveness,", "author": ["L. Jia", "C. Yu", "W. Meng"], "venue": "Proceeding of the 18th ACM conference on Information and knowledge management,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2009}, {"title": "A sentimental education: sentiment analysis using subjec- tivity summarization based on minimum cuts,", "author": ["B. Pang", "L. Lee"], "venue": "Proceedings of the 42nd Annual Meeting on Association for Computational Linguistics, (Barcelona,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2004}, {"title": "Recognizing contextual polarity in phrase- level sentiment analysis,", "author": ["T. Wilson", "J. Wiebe", "P. Hoffmann"], "venue": "Proceedings of the conference on Human Language Technology and Empirical Methods in Natural Language Processing, (Vancouver, British Columbia,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2005}, {"title": "Learning with compositional semantics as structural in- ference for subsentential sentiment analysis,", "author": ["Y. Choi", "C. Cardie"], "venue": "Proceedings of the Conference on Empirical Methods in Natural Language Processing, (Honolulu,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2008}, {"title": "Annotating expressions of opinions and emo- tions in language,", "author": ["J. Wiebe", "T. Wilson", "C. Cardie"], "venue": "Language Resources and Evaluation,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2005}, {"title": "Access: news and blog analysis for the social sciences,", "author": ["M. Bautin", "C.B. Ward", "A. Patil", "S.S. Skiena"], "venue": "Proceedings of the 19th international conference on World wide web,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2010}, {"title": "Mining the blogosphere for top news stories identification,", "author": ["Y. Lee", "H.-y. Jung", "W. Song", "J.-H. Lee"], "venue": "Proceeding of the 33rd international ACM SIGIR confer- ence on Research and development in information retrieval,", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2010}, {"title": "A survey on sentiment detection of reviews,", "author": ["H. Tang", "S. Tan", "X. Cheng"], "venue": "Expert Systems with Applications,", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2009}, {"title": "Self-training from labeled features for sentiment analysis,", "author": ["Y. He", "D. Zhou"], "venue": "Information Processing & Management,", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2011}, {"title": "From bias to opin- ion: a transfer-learning approach to real-time sentiment analysis,", "author": ["P.H. Calais Guerra", "A. Veloso", "W. Meira Jr.", "V. Almeida"], "venue": "Proceedings of the 17th ACM SIGKDD international conference on Knowledge discovery and data mining,", "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2011}, {"title": "Comparative experiments on sentiment classifi- cation for online product reviews,", "author": ["H. Cui", "V. Mittal", "M. Datar"], "venue": "(American Association for Artificial Intelligence (AAAI)),", "citeRegEx": "32", "shortCiteRegEx": "32", "year": 2006}, {"title": "Montecinos, \u201cRobust classification of imbalanced data us- ing one-class and two-class svm-based multiclassifiers,", "author": ["C.S. Maldonado"], "venue": "Intelligent Data Analysis,", "citeRegEx": "33", "shortCiteRegEx": "33", "year": 2014}, {"title": "Adapting naive bayes to domain adap- tation for sentiment analysis,", "author": ["S. Tan", "X. Cheng", "Y. Wang", "H. Xu"], "venue": "Advances in Information Retrieval,", "citeRegEx": "34", "shortCiteRegEx": "34", "year": 2009}, {"title": "Feature ensemble plus sample selec- tion: A comprehensive approach to domain adaptation for sentiment classification,", "author": ["R. Xia", "C. Zong", "X. Hu", "E. Cambria"], "venue": "IEEE Intelligent Systems, vol. 28,", "citeRegEx": "36", "shortCiteRegEx": "36", "year": 2013}, {"title": "Naive bayes models for probability estimation,", "author": ["D. Lowd", "P. Domingos"], "venue": "Proceedings of the 22nd international conference on Machine learning,", "citeRegEx": "37", "shortCiteRegEx": "37", "year": 2005}, {"title": "Sentence-level sentiment polarity clas- sification using a linguistic approach,\u201d Digital Libraries: For Cultural Heritage, Knowledge Dissemination, and Future Creation", "author": ["L. Tan", "J. Na", "Y. Theng", "K. Chang"], "venue": null, "citeRegEx": "38", "shortCiteRegEx": "38", "year": 2011}, {"title": "Recognizing contextual polarity: An ex- ploration of features for phrase-level sentiment analysis,", "author": ["T. Wilson", "J. Wiebe", "P. Hoffmann"], "venue": "Computational Linguis- tics,", "citeRegEx": "39", "shortCiteRegEx": "39", "year": 2009}, {"title": "Sentiment classifica- tion and polarity shifting,", "author": ["S. Li", "S.Y.M. Lee", "Y. Chen", "C.-R. Huang", "G. Zhou"], "venue": "Proceedings of the 23rd International Conference on Computational Linguistics, (Beijing,", "citeRegEx": "40", "shortCiteRegEx": "40", "year": 2010}, {"title": "Sequential minimal optimization: A fast algorithm for training support vector machines,", "author": ["J. Platt"], "venue": "Tech. Rep. MSR-TR-98-14, Microsoft Research,", "citeRegEx": "41", "shortCiteRegEx": "41", "year": 1998}, {"title": "An empirical study of the naive bayes classifier,", "author": ["I. Rish"], "venue": "IJCAI 2001 workshop on empirical methods in artificial intelligence,", "citeRegEx": "42", "shortCiteRegEx": "42", "year": 2001}, {"title": "The weka data mining software: an update,", "author": ["M. Hall", "E. Frank", "G. Holmes", "B. Pfahringer", "P. Reutemann", "I.H. Witten"], "venue": "ACM SIGKDD explorations newslet- ter, vol. 11,", "citeRegEx": "43", "shortCiteRegEx": "43", "year": 2009}, {"title": "Auto-weka: Com- bined selection and hyperparameter optimization of classification algorithms,", "author": ["C. Thornton", "F. Hutter", "H.H. Hoos", "K. Leyton-Brown"], "venue": "Proceedings of the 19th ACM SIGKDD international conference on Knowledge dis- covery and data mining,", "citeRegEx": "44", "shortCiteRegEx": "44", "year": 2013}, {"title": "Hidden factors and hidden topics: understanding rating dimensions with review text,", "author": ["J. McAuley", "J. Leskovec"], "venue": "Proceedings of the 7th ACM conference on Recommender systems,", "citeRegEx": "45", "shortCiteRegEx": "45", "year": 2013}], "referenceMentions": [{"referenceID": 0, "context": "The most prominent in the literature is Pang et al,[1] which employed supervised machine learning techniques to classify positive and negative sentiments in movie reviews.", "startOffset": 51, "endOffset": 54}, {"referenceID": 1, "context": "[2, 3, 4] Practical benefits also emerged as a result of automatic recommendation of movies and products by using the sentiments expressed in the related review.", "startOffset": 0, "endOffset": 9}, {"referenceID": 2, "context": "[2, 3, 4] Practical benefits also emerged as a result of automatic recommendation of movies and products by using the sentiments expressed in the related review.", "startOffset": 0, "endOffset": 9}, {"referenceID": 3, "context": "[2, 3, 4] Practical benefits also emerged as a result of automatic recommendation of movies and products by using the sentiments expressed in the related review.", "startOffset": 0, "endOffset": 9}, {"referenceID": 4, "context": "[5, 3, 4] This is also applicable to business intelligence applications which rely on customers\u2019 reviews to extract \u2018satisfaction\u2019 patterns that may improve profitability.", "startOffset": 0, "endOffset": 9}, {"referenceID": 2, "context": "[5, 3, 4] This is also applicable to business intelligence applications which rely on customers\u2019 reviews to extract \u2018satisfaction\u2019 patterns that may improve profitability.", "startOffset": 0, "endOffset": 9}, {"referenceID": 3, "context": "[5, 3, 4] This is also applicable to business intelligence applications which rely on customers\u2019 reviews to extract \u2018satisfaction\u2019 patterns that may improve profitability.", "startOffset": 0, "endOffset": 9}, {"referenceID": 5, "context": "[6, 7] While the number of reviews has continued to grow, and sentiments are expressed in a subtle manner, it is important to develop more effective sentiment classification techniques that can correctly classify sentiments despite natural language ambiguities, which include the use of irony.", "startOffset": 0, "endOffset": 6}, {"referenceID": 6, "context": "[8, 9, 10, 11] In this work, we classify sentiments expressed on individual product types by learning a language model classifier.", "startOffset": 0, "endOffset": 14}, {"referenceID": 7, "context": "[8, 9, 10, 11] In this work, we classify sentiments expressed on individual product types by learning a language model classifier.", "startOffset": 0, "endOffset": 14}, {"referenceID": 8, "context": "[8, 9, 10, 11] In this work, we classify sentiments expressed on individual product types by learning a language model classifier.", "startOffset": 0, "endOffset": 14}, {"referenceID": 9, "context": "[8, 9, 10, 11] In this work, we classify sentiments expressed on individual product types by learning a language model classifier.", "startOffset": 0, "endOffset": 14}, {"referenceID": 1, "context": "[2, 7] This enables the reviewer to express a substantial level of sentiments on the particular product alone without necessarily splitting the opinions between different products.", "startOffset": 0, "endOffset": 6}, {"referenceID": 5, "context": "[2, 7] This enables the reviewer to express a substantial level of sentiments on the particular product alone without necessarily splitting the opinions between different products.", "startOffset": 0, "endOffset": 6}, {"referenceID": 10, "context": "[13, 14, 15] The application of sentiment classification is important to the ordinary users of opinion mining and sentiment analysis systems.", "startOffset": 0, "endOffset": 12}, {"referenceID": 11, "context": "[13, 14, 15] The application of sentiment classification is important to the ordinary users of opinion mining and sentiment analysis systems.", "startOffset": 0, "endOffset": 12}, {"referenceID": 12, "context": "[13, 14, 15] The application of sentiment classification is important to the ordinary users of opinion mining and sentiment analysis systems.", "startOffset": 0, "endOffset": 12}, {"referenceID": 13, "context": "[16, 2, 3] This is because the different categories of sentiments (e.", "startOffset": 0, "endOffset": 10}, {"referenceID": 1, "context": "[16, 2, 3] This is because the different categories of sentiments (e.", "startOffset": 0, "endOffset": 10}, {"referenceID": 2, "context": "[16, 2, 3] This is because the different categories of sentiments (e.", "startOffset": 0, "endOffset": 10}, {"referenceID": 14, "context": "[17, 18, 19] Sentiment classification on product reviews can be challenging,[16, 3, 4] which is why it is still a very active area of research.", "startOffset": 0, "endOffset": 12}, {"referenceID": 15, "context": "[17, 18, 19] Sentiment classification on product reviews can be challenging,[16, 3, 4] which is why it is still a very active area of research.", "startOffset": 0, "endOffset": 12}, {"referenceID": 16, "context": "[17, 18, 19] Sentiment classification on product reviews can be challenging,[16, 3, 4] which is why it is still a very active area of research.", "startOffset": 0, "endOffset": 12}, {"referenceID": 13, "context": "[17, 18, 19] Sentiment classification on product reviews can be challenging,[16, 3, 4] which is why it is still a very active area of research.", "startOffset": 76, "endOffset": 86}, {"referenceID": 2, "context": "[17, 18, 19] Sentiment classification on product reviews can be challenging,[16, 3, 4] which is why it is still a very active area of research.", "startOffset": 76, "endOffset": 86}, {"referenceID": 3, "context": "[17, 18, 19] Sentiment classification on product reviews can be challenging,[16, 3, 4] which is why it is still a very active area of research.", "startOffset": 76, "endOffset": 86}, {"referenceID": 17, "context": "More importantly, sentiments expressed in each product review sometimes include ambiguous and unexpected sentences, [20] and are often alternated between the two different positive and negative polarities.", "startOffset": 116, "endOffset": 120}, {"referenceID": 0, "context": "[1, 16, 3] As such, the bag-of-words approach is not sufficient alone.", "startOffset": 0, "endOffset": 10}, {"referenceID": 13, "context": "[1, 16, 3] As such, the bag-of-words approach is not sufficient alone.", "startOffset": 0, "endOffset": 10}, {"referenceID": 2, "context": "[1, 16, 3] As such, the bag-of-words approach is not sufficient alone.", "startOffset": 0, "endOffset": 10}, {"referenceID": 2, "context": "[3, 4] We emphasize that most negative reviews contain positive sentences and often express negative sentiments by using just a few negative sentences.", "startOffset": 0, "endOffset": 6}, {"referenceID": 3, "context": "[3, 4] We emphasize that most negative reviews contain positive sentences and often express negative sentiments by using just a few negative sentences.", "startOffset": 0, "endOffset": 6}, {"referenceID": 18, "context": "[21] We show an example as follows:", "startOffset": 0, "endOffset": 4}, {"referenceID": 19, "context": "Pang and Lee,[22] proposed a subjectivity summarization technique that is based on minimum cuts to classify sentiment polarities in IMDb movie reviews.", "startOffset": 13, "endOffset": 17}, {"referenceID": 19, "context": "Thus Pang and Lee,[22] showed that minimum cuts in graph put such sentences in", "startOffset": 18, "endOffset": 22}, {"referenceID": 19, "context": "Thus, contrary to Pang and Lee,[22] our work has the ability to effectively learn sentiments by identifying the likely subjective sentences with consistent sentiments.", "startOffset": 31, "endOffset": 35}, {"referenceID": 2, "context": "[3, 4] Consider, for example, the following excerpt from a \u2018positive-labelled\u2019 movie review:", "startOffset": 0, "endOffset": 6}, {"referenceID": 3, "context": "[3, 4] Consider, for example, the following excerpt from a \u2018positive-labelled\u2019 movie review:", "startOffset": 0, "endOffset": 6}, {"referenceID": 20, "context": "Similarly, Wilson et al,[23] used instances of polar words to detect contextual polarity of phrases from the MPQA corpus.", "startOffset": 24, "endOffset": 28}, {"referenceID": 2, "context": "[3] Earlier in Section 1, we have illustrated some example sentences to that effect.", "startOffset": 0, "endOffset": 3}, {"referenceID": 20, "context": "To this extent, Wilson et al,[23] performed manual annotation of contextual polarities in the MPQA corpus to train a classifier with a combination of ten features resulting to 65.", "startOffset": 29, "endOffset": 33}, {"referenceID": 21, "context": "Choi and Cardie,[24] proposed a compositional semantics approach to learn the polarity of sentiments from the sub-sentential level of opinionated expres-", "startOffset": 16, "endOffset": 20}, {"referenceID": 22, "context": "Interestingly, on the Multi-Perspective Question Answering (MPQA) corpus created by Wiebe et al,[25] this combination yielded a performance of 90.", "startOffset": 96, "endOffset": 100}, {"referenceID": 21, "context": "The performance achieved by Choi and Cardie,[24] is understandable given that the MPQA corpus contains well \u2018structured\u2019 news articles which are mostly well written on certain topics.", "startOffset": 44, "endOffset": 48}, {"referenceID": 14, "context": "[17, 26, 27] For example, it is more likely that a negative news \u2018event\u2019 such as \u2018Disaster unfolds as Tsunami rocks Japan\u2019 will attract \u2018persistent\u2019 negative expressions and sentiments in news articles.", "startOffset": 0, "endOffset": 12}, {"referenceID": 23, "context": "[17, 26, 27] For example, it is more likely that a negative news \u2018event\u2019 such as \u2018Disaster unfolds as Tsunami rocks Japan\u2019 will attract \u2018persistent\u2019 negative expressions and sentiments in news articles.", "startOffset": 0, "endOffset": 12}, {"referenceID": 24, "context": "[17, 26, 27] For example, it is more likely that a negative news \u2018event\u2019 such as \u2018Disaster unfolds as Tsunami rocks Japan\u2019 will attract \u2018persistent\u2019 negative expressions and sentiments in news articles.", "startOffset": 0, "endOffset": 12}, {"referenceID": 21, "context": "It would be interesting to know the performance of the heuristics used by Choi and Cardie,[24] on standard product review datasets such as Amazon online product review datasets.", "startOffset": 90, "endOffset": 94}, {"referenceID": 25, "context": "[28] Our main contribution to the sentiment classification task is to do training set correction and further detect inter-sentence polarity consistency that could improve a sentiment classifier.", "startOffset": 0, "endOffset": 4}, {"referenceID": 14, "context": "[17] More importantly, we believe every sentence in the review may not necessarily contribute to the classification of the review to the appropriate class.", "startOffset": 0, "endOffset": 4}, {"referenceID": 19, "context": "[22] We say that certain sequential sentences with consistent sentiment polarities could be sufficient to represent and distinguish between the sentiment classes of a review.", "startOffset": 0, "endOffset": 4}, {"referenceID": 26, "context": "[29, 30] We emphasize that our approach is promising and can be easily integrated by any sentiment classification system regardless of the sentiment detection technique employed.", "startOffset": 0, "endOffset": 8}, {"referenceID": 2, "context": "[3] In a negative-labeled product review for example, it is more likely that negative sentiments will be expressed within the first few portion of the review and then followed by positive sentiments in the later portion of the review on some of the aspects of the product that gave some satisfactions.", "startOffset": 0, "endOffset": 3}, {"referenceID": 4, "context": "[5, 3] This could be because reviewers tend to emphasize on the negative aspects of a product than the positive aspects, and in some cases, both polarities are expressed alternately, which we will discuss in Section 4.", "startOffset": 0, "endOffset": 6}, {"referenceID": 2, "context": "[5, 3] This could be because reviewers tend to emphasize on the negative aspects of a product than the positive aspects, and in some cases, both polarities are expressed alternately, which we will discuss in Section 4.", "startOffset": 0, "endOffset": 6}, {"referenceID": 19, "context": "[22, 31] As such, we propose a promising approach to reduce the bias in the training set by first learning a \u2018n\u00e4\u0131ve\u2019 sentence-level classifier on all sentences from both the positive and negative categories.", "startOffset": 0, "endOffset": 8}, {"referenceID": 27, "context": "[22, 31] As such, we propose a promising approach to reduce the bias in the training set by first learning a \u2018n\u00e4\u0131ve\u2019 sentence-level classifier on all sentences from both the positive and negative categories.", "startOffset": 0, "endOffset": 8}, {"referenceID": 19, "context": "unigram or bag-of-words),[22, 32] without necessarily performing sophisticated features engineering since the final sentiment classifier will be constructed with more fine-grained features.", "startOffset": 25, "endOffset": 33}, {"referenceID": 28, "context": "unigram or bag-of-words),[22, 32] without necessarily performing sophisticated features engineering since the final sentiment classifier will be constructed with more fine-grained features.", "startOffset": 25, "endOffset": 33}, {"referenceID": 29, "context": "[33] For example, one could learn the popular N\u00e4\u0131ve Bayes classifier with only unigram features.", "startOffset": 0, "endOffset": 4}, {"referenceID": 19, "context": "[22, 34, 35] It is also possible to use a more complexly constructed classifier at the expense of efficiency.", "startOffset": 0, "endOffset": 12}, {"referenceID": 30, "context": "[22, 34, 35] It is also possible to use a more complexly constructed classifier at the expense of efficiency.", "startOffset": 0, "endOffset": 12}, {"referenceID": 31, "context": "While this technique may result to a meta classification,[36] we propose to include the technique as part of the training process of the final sentiment classifier.", "startOffset": 57, "endOffset": 61}, {"referenceID": 32, "context": "[37] We compute the Joint-Log-Probability as follows:", "startOffset": 0, "endOffset": 4}, {"referenceID": 2, "context": "More importantly, because the bag-of-words approach has seldom improve the accuracy of a sentiment classifier,[3, 4] a sentence-level approach could give better improvement since most sentiments are expressed at sentencelevel anyway.", "startOffset": 110, "endOffset": 116}, {"referenceID": 3, "context": "More importantly, because the bag-of-words approach has seldom improve the accuracy of a sentiment classifier,[3, 4] a sentence-level approach could give better improvement since most sentiments are expressed at sentencelevel anyway.", "startOffset": 110, "endOffset": 116}, {"referenceID": 33, "context": "[38] However, we have indicated in Section 3 that many review documents have the tendency to contain both positive and negative sentences, regardless of their individual categories (i.", "startOffset": 0, "endOffset": 4}, {"referenceID": 34, "context": "[39, 40, 3] Note that we have motivated the inconsistency problem with an example in Section 1.", "startOffset": 0, "endOffset": 11}, {"referenceID": 35, "context": "[39, 40, 3] Note that we have motivated the inconsistency problem with an example in Section 1.", "startOffset": 0, "endOffset": 11}, {"referenceID": 2, "context": "[39, 40, 3] Note that we have motivated the inconsistency problem with an example in Section 1.", "startOffset": 0, "endOffset": 11}, {"referenceID": 0, "context": "[1, 40, 3] As such, a given polarity is expressed consistently over a number of sentences and at a certain point deviate to the other polarity, and continues over a number of sentences alternately.", "startOffset": 0, "endOffset": 10}, {"referenceID": 35, "context": "[1, 40, 3] As such, a given polarity is expressed consistently over a number of sentences and at a certain point deviate to the other polarity, and continues over a number of sentences alternately.", "startOffset": 0, "endOffset": 10}, {"referenceID": 2, "context": "[1, 40, 3] As such, a given polarity is expressed consistently over a number of sentences and at a certain point deviate to the other polarity, and continues over a number of sentences alternately.", "startOffset": 0, "endOffset": 10}, {"referenceID": 14, "context": "[17, 21] Note that this technique is different from intra-sentence polarity detection as studied in Li et al.", "startOffset": 0, "endOffset": 8}, {"referenceID": 18, "context": "[17, 21] Note that this technique is different from intra-sentence polarity detection as studied in Li et al.", "startOffset": 0, "endOffset": 8}, {"referenceID": 35, "context": "[40] An additional thing we did was to performed negation tagging by tagging 1 to 3 words after a negation word in each sentence.", "startOffset": 0, "endOffset": 4}, {"referenceID": 36, "context": "The classifiers comprises of the Sequential Minimum Optimization (SMO) variant of Support Vector Machines (SVM),[41] and N\u00e4\u0131ve Bayes (NB) classifier.", "startOffset": 112, "endOffset": 116}, {"referenceID": 37, "context": "[42] We used SVM and NB on the WEKA machine learning platform,[43] with bag-of-words,unigram, and word bigram features.", "startOffset": 0, "endOffset": 4}, {"referenceID": 38, "context": "[42] We used SVM and NB on the WEKA machine learning platform,[43] with bag-of-words,unigram, and word bigram features.", "startOffset": 62, "endOffset": 66}, {"referenceID": 0, "context": "[1, 22, 3] We conducted 80%-20% performance evaluation for comparison with the baselines on each dataset domain.", "startOffset": 0, "endOffset": 10}, {"referenceID": 19, "context": "[1, 22, 3] We conducted 80%-20% performance evaluation for comparison with the baselines on each dataset domain.", "startOffset": 0, "endOffset": 10}, {"referenceID": 2, "context": "[1, 22, 3] We conducted 80%-20% performance evaluation for comparison with the baselines on each dataset domain.", "startOffset": 0, "endOffset": 10}, {"referenceID": 39, "context": "For selecting the best parameters for the baseline algorithms, we performed hyperparameters search using Auto-Weka,[44] with cross-validation and the Sequential Model-based Algorithm Configuration (SMAC) optimization algorithm, which is an Bayesian optimization method proposed as part of Auto-Weka.", "startOffset": 115, "endOffset": 119}, {"referenceID": 39, "context": "[44] We performed the search by using the unigram features on the training set of each domain.", "startOffset": 0, "endOffset": 4}, {"referenceID": 0, "context": "[1, 22, 3]", "startOffset": 0, "endOffset": 10}, {"referenceID": 19, "context": "[1, 22, 3]", "startOffset": 0, "endOffset": 10}, {"referenceID": 2, "context": "[1, 22, 3]", "startOffset": 0, "endOffset": 10}, {"referenceID": 4, "context": "[5] The dataset was first used in year 2007 and consists of Amazon online", "startOffset": 0, "endOffset": 3}, {"referenceID": 4, "context": "[5] For each domain, we separated 800 documents per category as training set and used the remaining 200 documents as unseen testing set.", "startOffset": 0, "endOffset": 3}, {"referenceID": 19, "context": "As our baseline, we implemented a sentence-level sentiment classifier using a technique similar to Pang and Lee,[22] on the same dataset but without our correction technique.", "startOffset": 112, "endOffset": 116}, {"referenceID": 2, "context": "We also like to emphasize that any reasonable sentence-level polarity identification technique,[3] used in place of the \u2018n\u00e4\u0131ve\u2019 classifier in the correction processes, is likely to work just fine and give improved results for the overall sentiment classification task.", "startOffset": 95, "endOffset": 98}, {"referenceID": 2, "context": "Given the difficulty of the sentiment classification task [3], we believe that the improvement shown by the correction technique is promising and could lead to building a more accurate sentiment classifier.", "startOffset": 58, "endOffset": 61}, {"referenceID": 40, "context": "[45]", "startOffset": 0, "endOffset": 4}], "year": 2015, "abstractText": "We propose an effective technique to solving review-level sentiment classification problem by using sentence-level polarity correction. Our polarity correction technique takes into account the consistency of the polarities (positive and negative) of sentences within each product review before performing the actual machine learning task. While sentences with inconsistent polarities are removed, sentences with consistent polarities are used to learn state-of-the-art classifiers. The technique achieved better results on different types of products reviews and outperforms baseline models without the correction technique. Experimental results show an average of 82% F-measure on four different product review domains.", "creator": "TeX"}}}