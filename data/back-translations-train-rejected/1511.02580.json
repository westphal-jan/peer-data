{"id": "1511.02580", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "9-Nov-2015", "title": "How far can we go without convolution: Improving fully-connected networks", "abstract": "We propose ways to improve the performance of fully connected networks. We found that two approaches in particular have a strong effect on performance: linear bottleneck layers and unsupervised pre-training using autoencoders without hidden unit biases. We show how both approaches can be related to improving gradient flow and reducing sparsity in the network. We show that a fully connected network can yield approximately 70% classification accuracy on the permutation-invariant CIFAR-10 task, which is much higher than the current state-of-the-art. By adding deformations to the training data, the fully connected network achieves 78% accuracy, which is just 10% short of a decent convolutional network.", "histories": [["v1", "Mon, 9 Nov 2015 06:56:24 GMT  (493kb,D)", "http://arxiv.org/abs/1511.02580v1", "10 pages, 11 figures, submitted for ICLR 2016"]], "COMMENTS": "10 pages, 11 figures, submitted for ICLR 2016", "reviews": [], "SUBJECTS": "cs.LG cs.NE", "authors": ["zhouhan lin", "roland memisevic", "kishore konda"], "accepted": false, "id": "1511.02580"}, "pdf": {"name": "1511.02580.pdf", "metadata": {"source": "CRF", "title": null, "authors": ["Zhouhan Lin", "Roland Memisevic"], "emails": ["roland.memisevic}@umontreal.ca", "konda.kishorereddy@gmail.com"], "sections": [{"heading": "1 INTRODUCTION", "text": "Convolutionary neural networks (CNN) have achieved tremendous success since Krizhevsky et al. (2012), especially in computer vision applications. Unfortunately, the basic arithmetical idea behind these, weight distribution, is not biologically plausible and cannot be well mapped to simple, closely parallel hardware designs that can one day provide less energy and more efficient ways to operate neural networks, in other words, a neural network that learns from \"permutation invariant\" classification tasks. We use these as a test bed for studying alternative architectural design decisions alongside conversion, which is biologically plausible and potentially more hardware-friendly. Studying these design decisions is also relevant because many tasks and sub-invariant tasks are not mutually complementary, which allows them to overlay each other."}, {"heading": "1.1 SPARSITY IN NEURAL NETWORKS", "text": "In fact, it is that we are able to assert ourselves, that we are able, that we are able, that we are able, that we are able, that we are able, that we are able, that we are able, that we are able."}, {"heading": "2 LINEAR BOTTLENECK LAYERS", "text": "One disadvantage of scarcity in a deep network is that it amounts to data scarcity: a weight whose post-synaptic unit is switched off 80% of the time effectively only gets to see 20% of the training data. In the lower layers of a Convolutionary Network, this problem is compensated by weight distribution, increasing the effective number of training examples (in this case patches) per weight by a factor of several thousand. In a fully connected layer (more precisely, a layer without weight distribution), such compensation is not possible and the only way to counteract this effect would be to increase the training size. Scarcity is a common and stable effect in neural networks that contain ReLU or sigmoid units, and it may be related to the fact that the distortions are driven to zero in response to regulation (e.g. Konda et al. (2014)). Figure 1 (left) shows the scarcity achieved after training with LLU-MLPs 2000 or the additional sigmoid 2000 units showing the presence of the AR and the AR-1000 units respectively."}, {"heading": "2.1 LINEAR LAYERS AS DISTRIBUTION RESHAPING", "text": "It is well known that various activation functions work best when input is on a reasonable scale, which is the motivation for using preprocessions through medium subtraction and scaling, for example. Furthermore, inclusion in hidden unit activation can bring about further performance improvements (Ioffe & Szegedy, 2015). Here, we focus on distribution over the outputs of a unit whose output is a linear neuron Y with inputs ~ X = (X1, X2,..., XN) is given by Y = N i wiXi + b, where wi is the entry into the weight vector corresponding to the input vector i, b is the bias of linear neurons, and N denotes the input dimension. Assume the data fed into each input node Xi is independent and has a finite mean \u00b5i and variance 2i."}, {"heading": "2.2 DERIVATIVES IN THE PRESENCE OF LINEAR LAYERS", "text": "We now look at the derivatives of a ReLU network. Activation of the i-th layer is given by: ~ Hi + 1 = R (wi ~ Hi + ~ bi), (5) where R () is the elementary activation function. The backward propagated updates to wi will be: \u2206 wi = \u03b4 \u043c R \"(~ Hi + 1) \u00b7 ~ Hi (6), where \u03b4 stands for the downward flowing error signal emanating from the upper layer, \u0445 stands for elementary multiply, and \u00b7 is the product of a vertical vector and a horizontal vector. As discussed in Section 2.1, at least 50% (usually much more in practice due to negative distortions) of the values in the representation Hi and Hi + 1 are typically zero (cf., Figure 1, left), resulting in a large fraction of the inputs of Lwi per training case zero. If we insert a linear layer ~ + two layers ~ Lbl = Hwi l (Hwi)."}, {"heading": "2.3 REDUCING PARAMETERS", "text": "Suppose a linear layer with L units were inserted between two nonlinear layers, each with N units, and the total number of parameters would be 2NL + L + N. This is much less than the number of parameters that would result from the direct connection of the two nonlinear layers, which would correspond to N2 + N parameters. Convolutionary network also reduces the number of parameters by folding cores. Note that for each trained folding network, we can always find a fully connected network with the same accuracy by extending the folding cores."}, {"heading": "3 PRE-TRAINING AND ZERO-BIAS ACTIVATIONS", "text": "It has been suggested by Saxe et al. (2013) that the benefit of unattended pre-discharge of a network of RBMs or autoencoders can lead to weight matrices that are closer to orthogonal and therefore less affected by disappearing progression problems. However, it is interesting that the practical success of these pre-discharge programs is conditioned by comparison with fully supervised learning processes and accordingly is not orthogonized. (Konda et al., 2014) This could be one of the reasons why the practical success of these pre-discharge programs is quite limited to monitor reverse learning. It is also important that the number of active units in a layer is often smaller than the same number in the layer below, so that the \"effective\" weight matrix for a particular input example is out of the question."}, {"heading": "4 EXPERIMENTS", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.1 CIFAR-10", "text": "In fact, the majority of them will be able to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to fight, to move, to move, to move, to fight, to move, to move, to move, to fight, to move, to move, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move"}, {"heading": "4.2 THE HIGGS DATASET", "text": "The HIGGS dataset includes 11 million samples with 28 dimensions. The first 21 characteristics are kinematic properties measured by particle detectors in a particle accelerator, and the remaining 7 characteristics are functions of the first 21. Thus, the dataset itself is permutation invariant. The task is to decide whether a given sample corresponds to a Higgs boson or not. We have tried both ReLU and ZAE with / without linear bottleneck layers on this dataset. Similarly, for each model PCA whitening with 99% of the variance is used for pre-processing (corresponding to 27 major components). We use the same model size for four different models that we compare (Zero-Bias ReLU, Z-Lin, ReLU and ReLU-Lin). The structure is 27 \u2212 800 \u2212 100 \u2212 100 \u2212 100 \u2212 100 \u2212 100 \u2212 2 for all, so the models differ by using different activation functions. We train all models with dynamics, and learning rates are not effective in the 4c units."}, {"heading": "4.3 REDUCING PARAMETERS", "text": "We define a network by stacking 3 Z-LIN pairs plus a ZAE layer and a logistics regression classifier. Each ZAE layer has 4000 hidden units. We reduce the linear layer size from 1000 to 100 units. Training includes dropouts, pre-training and fine-tuning. Results are shown in Figure 5. We observe that even with a hidden layer of size 100, which has only 1 / 20 of the parameters, the network still works reasonably well and does not lose too much accuracy. A 7-layer network with 4000 units per shift has about 112 million parameters. With linear layers, the smallest model in Figure 5 has only about 2.44 million parameters. In comparison, a typical Convolutionary network gives an accuracy of over 80% on CIFAR10."}, {"heading": "4.4 ACHIEVING STATE-OF-THE-ART ON PERMUTATION INVARIANT CIFAR-10", "text": "We compare the performance of the Z-Lin model with other published methods at CIFAR-10. For our method, the pre-processing steps are identical to Section 4.1. For the Z-Lin network, each ZAE layer is 4000 hidden units, and each linear autoencoder has 1000 hidden units. We use logistic regression on the last ZAE layer for classification. According to Konda et al. (2014), the threshold of all ZAEs is fixed to 1.0 during the lecture and set on the subsequent layers and fine-tune. As before, we subtract the mean and normalize the activations to have a standard deviation of 1.0 in all layers."}, {"heading": "5 RELATED WORK", "text": "The idea of a linear bottleneck layer is very old and was already used in the context of autoencoders as early as the 1980s (e.g. Baldi & Hornik (1989); more recently, Ba & Caruana (2014) used a linear bottleneck layer to factorise a single-layer network and showed that this accelerated learning; the application of a linear bottleneck layer in the last layer of a neural network to manage high-dimensional outputs is described in Sainath et al. (2013) and Xue et al. (2013). Unlike our work, none of these methods aims to alleviate disappearing gradients and deal with scarcity, and (accordingly) they only use a single bottleneck layer in the network.The transformation of distributions via activations using linear layers is also related to the recently introduced stack normalization trick (Ioffe & Szegedy, 2015), since it is also linear, and it is a way that only one layer fits, but only one layer."}, {"heading": "6 DISCUSSION", "text": "It is well known that a single-layer neural network can model any nonlinear function under mild conditions (Funahashi, 1989; Cybenko, 1989). The intuition behind this observation is that the hidden layer divides space into hemispheres or \"tiles,\" and the subsequent linear layer composes the nonlinear function by combining different linear regions to produce the output. Interestingly, however, the practical usefulness of this result is limited, since approaching a given function would require an exponentially large number of hidden units. In practice, this is a motivation for the use of multi-layer networks, which calculate a sequence of consequently more limited but tractable nonlinear functions."}, {"heading": "ACKNOWLEDGMENTS", "text": "The authors would like to thank the developers of Theano (Bastien et al., 2012) and the following research funding and computer support agencies: Samsung, NSERC, Calcul Que \u0301 bec, Compute Canada, the Canada Research Chairs and CIFAR."}], "references": [{"title": "Do deep nets really need to be deep", "author": ["Ba", "Jimmy", "Caruana", "Rich"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Ba et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Ba et al\\.", "year": 2014}, {"title": "Neural networks and principal component analysis: Learning from examples without local minima", "author": ["P. Baldi", "K. Hornik"], "venue": "Neural networks,", "citeRegEx": "Baldi and Hornik,? \\Q1989\\E", "shortCiteRegEx": "Baldi and Hornik", "year": 1989}, {"title": "Theano: new features and speed improvements", "author": ["Bastien", "Fr\u00e9d\u00e9ric", "Lamblin", "Pascal", "Pascanu", "Razvan", "Bergstra", "James", "Goodfellow", "Ian J", "Bergeron", "Arnaud", "Bouchard", "Nicolas", "Bengio", "Yoshua"], "venue": "Deep Learning and Unsupervised Feature Learning NIPS 2012 Workshop,", "citeRegEx": "Bastien et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Bastien et al\\.", "year": 2012}, {"title": "Approximation by superpositions of a sigmoidal function", "author": ["Cybenko", "George"], "venue": "Mathematics of control, signals and systems,", "citeRegEx": "Cybenko and George.,? \\Q1989\\E", "shortCiteRegEx": "Cybenko and George.", "year": 1989}, {"title": "On the approximate realization of continuous mappings by neural networks", "author": ["Funahashi", "Ken-Ichi"], "venue": "Neural networks,", "citeRegEx": "Funahashi and Ken.Ichi.,? \\Q1989\\E", "shortCiteRegEx": "Funahashi and Ken.Ichi.", "year": 1989}, {"title": "Delving deep into rectifiers: Surpassing human-level performance on imagenet classification", "author": ["He", "Kaiming", "Zhang", "Xiangyu", "Ren", "Shaoqing", "Sun", "Jian"], "venue": "arXiv preprint arXiv:1502.01852,", "citeRegEx": "He et al\\.,? \\Q2015\\E", "shortCiteRegEx": "He et al\\.", "year": 2015}, {"title": "Gradient flow in recurrent nets: the difficulty of learning", "author": ["Hochreiter", "Sepp", "Bengio", "Yoshua", "Frasconi", "Paolo", "Schmidhuber", "J\u00fcrgen"], "venue": "long-term dependencies,", "citeRegEx": "Hochreiter et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Hochreiter et al\\.", "year": 2001}, {"title": "Batch normalization: Accelerating deep network training by reducing internal covariate shift", "author": ["Ioffe", "Sergey", "Szegedy", "Christian"], "venue": "arXiv preprint arXiv:1502.03167,", "citeRegEx": "Ioffe et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Ioffe et al\\.", "year": 2015}, {"title": "Zero-bias autoencoders and the benefits of co-adapting", "author": ["Konda", "Kishore", "Memisevic", "Roland", "Krueger", "David"], "venue": "features. stat,", "citeRegEx": "Konda et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Konda et al\\.", "year": 2014}, {"title": "Learning multiple layers of features from tiny images", "author": ["Krizhevsky", "Alex", "Hinton", "Geoffrey"], "venue": "Computer Science Department,", "citeRegEx": "Krizhevsky et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Krizhevsky et al\\.", "year": 2009}, {"title": "Imagenet classification with deep convolutional neural networks. In Advances in neural information processing", "author": ["Krizhevsky", "Alex", "Sutskever", "Ilya", "Hinton", "Geoffrey E"], "venue": null, "citeRegEx": "Krizhevsky et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Krizhevsky et al\\.", "year": 2012}, {"title": "Fastfood - approximating kernel expansions in loglinear time", "author": ["Le", "Quoc", "Sarlos", "Tamas", "Smola", "Alex"], "venue": "In 30th International Conference on Machine Learning (ICML),", "citeRegEx": "Le et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Le et al\\.", "year": 2013}, {"title": "Low-rank matrix factorization for deep neural network training with high-dimensional output targets", "author": ["Sainath", "Tara N", "Kingsbury", "Brian", "Sindhwani", "Vikas", "Arisoy", "Ebru", "Ramabhadran", "Bhuvana"], "venue": "In Acoustics, Speech and Signal Processing (ICASSP),", "citeRegEx": "Sainath et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Sainath et al\\.", "year": 2013}, {"title": "Exact solutions to the nonlinear dynamics of learning in deep linear neural networks", "author": ["Saxe", "Andrew M", "McClelland", "James L", "Ganguli", "Surya"], "venue": "arXiv preprint arXiv:1312.6120,", "citeRegEx": "Saxe et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Saxe et al\\.", "year": 2013}, {"title": "Dropout: A simple way to prevent neural networks from overfitting", "author": ["Srivastava", "Nitish", "Hinton", "Geoffrey", "Krizhevsky", "Alex", "Sutskever", "Ilya", "Salakhutdinov", "Ruslan"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "Srivastava et al\\.,? \\Q1929\\E", "shortCiteRegEx": "Srivastava et al\\.", "year": 1929}, {"title": "80 million tiny images: A large data set for nonparametric object and scene recognition", "author": ["Torralba", "Antonio", "Fergus", "Robert", "Freeman", "William T"], "venue": "Pattern Analysis and Machine Intelligence, IEEE Transactions on,", "citeRegEx": "Torralba et al\\.,? \\Q1958\\E", "shortCiteRegEx": "Torralba et al\\.", "year": 1958}, {"title": "Restructuring of deep neural network acoustic models with singular value decomposition", "author": ["Xue", "Jian", "Li", "Jinyu", "Gong", "Yifan"], "venue": null, "citeRegEx": "Xue et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Xue et al\\.", "year": 2013}], "referenceMentions": [{"referenceID": 9, "context": "Convolutional neural networks (CNN) have had huge successes since Krizhevsky et al. (2012), especially in computer vision applications.", "startOffset": 66, "endOffset": 91}, {"referenceID": 5, "context": "In the PReLU activation function (He et al., 2015), for example, the zero-derivative regime is replaced by a learned, and typically small-slope, linear activation.", "startOffset": 33, "endOffset": 50}, {"referenceID": 13, "context": "Sparsity also plays a crucial role in unsupervised learning (which can also be viewed as a way to help the optimization (Saxe et al., 2013)), where these activations have accordingly never been successfully applied.", "startOffset": 120, "endOffset": 139}, {"referenceID": 5, "context": ", Hochreiter et al. (2001)) this means that for a ReLU activation many Jacobians are diagonal matrices containing many zeros along the diagonal.", "startOffset": 2, "endOffset": 27}, {"referenceID": 5, "context": "In the PReLU activation function (He et al., 2015), for example, the zero-derivative regime is replaced by a learned, and typically small-slope, linear activation. Another approach is the Maxout activation (Goodfellow et al., 2013), which is defined as the maximum of several linear activations. Accordingly, preventing zero-derivatives this way was shown to improve the optimization and the performance of multilayer networks. Unfortunately, these methods solve the problem by giving up on sparsity altogether, which raises the question whether sparsity is simply not as desirable as widely assumed or whether the benefit of the optimization outweigh any detrimental effect on sparsity. Sparsity also plays a crucial role in unsupervised learning (which can also be viewed as a way to help the optimization (Saxe et al., 2013)), where these activations have accordingly never been successfully applied. This view of sparsity motivates us to revisit a simple, but as we shall show surprisingly effective, approach to retaining the benefits of sparsity without preventing gradient-flow. The idea is to interleave linear, low-dimensional layers with the sparse, high-dimensional layers containing ReLU or sigmoid activations. We show that this approach outperforms equivalent PReLU and Maxout networks on the fully supervised, permutation invariant CIFAR-10 task. A second detrimental side-effect of sparsity, discussed in more detail in Konda et al. (2014), is that for a ReLU or sigmoid unit to become sparse it typically learns strongly negative bias-terms.", "startOffset": 34, "endOffset": 1456}, {"referenceID": 5, "context": "In the PReLU activation function (He et al., 2015), for example, the zero-derivative regime is replaced by a learned, and typically small-slope, linear activation. Another approach is the Maxout activation (Goodfellow et al., 2013), which is defined as the maximum of several linear activations. Accordingly, preventing zero-derivatives this way was shown to improve the optimization and the performance of multilayer networks. Unfortunately, these methods solve the problem by giving up on sparsity altogether, which raises the question whether sparsity is simply not as desirable as widely assumed or whether the benefit of the optimization outweigh any detrimental effect on sparsity. Sparsity also plays a crucial role in unsupervised learning (which can also be viewed as a way to help the optimization (Saxe et al., 2013)), where these activations have accordingly never been successfully applied. This view of sparsity motivates us to revisit a simple, but as we shall show surprisingly effective, approach to retaining the benefits of sparsity without preventing gradient-flow. The idea is to interleave linear, low-dimensional layers with the sparse, high-dimensional layers containing ReLU or sigmoid activations. We show that this approach outperforms equivalent PReLU and Maxout networks on the fully supervised, permutation invariant CIFAR-10 task. A second detrimental side-effect of sparsity, discussed in more detail in Konda et al. (2014), is that for a ReLU or sigmoid unit to become sparse it typically learns strongly negative bias-terms. In other words, while sparse activations can be useful in terms of the learning objective (for example, by allowing the network to carve up space into small regions) it forces the network to utilize bias terms that are not optimal for the representation encoded in hidden layer activations. Konda et al. (2014), for example, suggest bias-terms equal to zero to be optimal and propose a way to train an autoencoder with zero-bias hidden units.", "startOffset": 34, "endOffset": 1870}, {"referenceID": 8, "context": ", Konda et al. (2014)).", "startOffset": 2, "endOffset": 22}, {"referenceID": 8, "context": "The observation that biases tend to zero motivated Konda et al. (2014) to introduce a \u201czero-bias\u201d activation function which uses a fixed threshold followed by linear activation.", "startOffset": 51, "endOffset": 71}, {"referenceID": 8, "context": "It is interesting to note, however, that the sparsityinducing negative biases yield reconstructions that are affine not linear and accordingly may not orthogonalize weights after all (Konda et al., 2014).", "startOffset": 183, "endOffset": 203}, {"referenceID": 12, "context": "It was suggested by Saxe et al. (2013) that the benefit of unsupervised pre-training of a network using RBMs or autoencoders may result in weight matrices that are closer to orthogonal and thus less affected by vanishing gradients problems.", "startOffset": 20, "endOffset": 39}, {"referenceID": 8, "context": "It is interesting to note, however, that the sparsityinducing negative biases yield reconstructions that are affine not linear and accordingly may not orthogonalize weights after all (Konda et al., 2014). This may be one of the reasons why the practical success of these pre-training schemes has been quite limited by comparison to fully supervised learning using back propagation. It is also important to note that due to sparsity, the number of active units in a layer is often smaller than that the same number in the layer below, so the \u201ceffective\u201d weight matrix for a given input example is not a square matrix. Rather than simply orthogonal weight matrices, we should be looking for networks where hidden units which tend to be active on the same inputs have weights that tend to be orthogonal to one another. In other words, we should be looking for \u201corthogonal active paths\u201d through the network rather than overall orthogonal weight matrices. In order to obtain hidden units with linear not affine encodings Konda et al. (2014) introduce \u201czerobias autoencoders\u201d (ZAE) whose activations are threshold-ed when pre-trained by minimizing the autoencoder reconstruction error, and whose thresholds are removed when the weights are used for classification or for initializing a feed forward network.", "startOffset": 184, "endOffset": 1036}, {"referenceID": 8, "context": "It is interesting to note, however, that the sparsityinducing negative biases yield reconstructions that are affine not linear and accordingly may not orthogonalize weights after all (Konda et al., 2014). This may be one of the reasons why the practical success of these pre-training schemes has been quite limited by comparison to fully supervised learning using back propagation. It is also important to note that due to sparsity, the number of active units in a layer is often smaller than that the same number in the layer below, so the \u201ceffective\u201d weight matrix for a given input example is not a square matrix. Rather than simply orthogonal weight matrices, we should be looking for networks where hidden units which tend to be active on the same inputs have weights that tend to be orthogonal to one another. In other words, we should be looking for \u201corthogonal active paths\u201d through the network rather than overall orthogonal weight matrices. In order to obtain hidden units with linear not affine encodings Konda et al. (2014) introduce \u201czerobias autoencoders\u201d (ZAE) whose activations are threshold-ed when pre-trained by minimizing the autoencoder reconstruction error, and whose thresholds are removed when the weights are used for classification or for initializing a feed forward network. As discussed in Konda et al. (2014) minimizing squared error under the linear encoding should encourage weights corresponding to hidden units that tend to be \u201con\u201d together to orthogonalize (because the orthogonal projection is what minimizes reconstruction error).", "startOffset": 184, "endOffset": 1338}, {"referenceID": 8, "context": "It is interesting to note, however, that the sparsityinducing negative biases yield reconstructions that are affine not linear and accordingly may not orthogonalize weights after all (Konda et al., 2014). This may be one of the reasons why the practical success of these pre-training schemes has been quite limited by comparison to fully supervised learning using back propagation. It is also important to note that due to sparsity, the number of active units in a layer is often smaller than that the same number in the layer below, so the \u201ceffective\u201d weight matrix for a given input example is not a square matrix. Rather than simply orthogonal weight matrices, we should be looking for networks where hidden units which tend to be active on the same inputs have weights that tend to be orthogonal to one another. In other words, we should be looking for \u201corthogonal active paths\u201d through the network rather than overall orthogonal weight matrices. In order to obtain hidden units with linear not affine encodings Konda et al. (2014) introduce \u201czerobias autoencoders\u201d (ZAE) whose activations are threshold-ed when pre-trained by minimizing the autoencoder reconstruction error, and whose thresholds are removed when the weights are used for classification or for initializing a feed forward network. As discussed in Konda et al. (2014) minimizing squared error under the linear encoding should encourage weights corresponding to hidden units that tend to be \u201con\u201d together to orthogonalize (because the orthogonal projection is what minimizes reconstruction error). Konda et al. (2014) reported decent classification performance in various supervised tasks, but found only a weak improvement over standard autoencoders when used to initialize a single-hidden-layer MLP.", "startOffset": 184, "endOffset": 1587}, {"referenceID": 8, "context": "We compare two different activation functions: ReLU and zero-bias ReLU (Konda et al., 2014).", "startOffset": 71, "endOffset": 91}, {"referenceID": 14, "context": "The CIFAR-10 dataset is a subset of the 80 Million Tiny Images Torralba et al. (2008), and contains 10 balanced classes.", "startOffset": 63, "endOffset": 86}, {"referenceID": 5, "context": "We also compare the bottleneck layer structure with other related modifications like PReLU (He et al., 2015) and Maxout (Goodfellow et al.", "startOffset": 91, "endOffset": 108}, {"referenceID": 8, "context": "Following Konda et al. (2014), the threshold of all ZAEs are fixed at 1.", "startOffset": 10, "endOffset": 30}, {"referenceID": 11, "context": "1% (Le et al., 2013), and 63.", "startOffset": 3, "endOffset": 20}, {"referenceID": 8, "context": "9% (Konda et al., 2014).", "startOffset": 3, "endOffset": 23}, {"referenceID": 8, "context": "9% (Konda et al., 2014). By adding dropout (Srivastava et al., 2014) during pre-training and fine-tuning, and using a very deep Z-LIN network (3 Z-LIN pairs, plus a ZAE layer and logistic regression classifier, i.e., 4000Z\u2212 1000Lin\u22124000Z\u22121000Lin\u22124000Z\u22121000Lin\u22124000Z\u221210) the performance improves to 69.62%, which exceeds the current state-of-the-art on permutation invariant CIFAR10 by a very large margin. If we give up on permutation-invariance by using data augmentation (eg., Krizhevsky et al. (2012)) but retain the use of a fully connected network, the performance improves much further.", "startOffset": 4, "endOffset": 504}, {"referenceID": 11, "context": "They are (from left to right): 1) Logistic Regression on whitened data; 2) Pure backprop on a 782-10000-10 network; 3) Pure backprop on a 78210000-10000-10 network; 4) RBM with 2 hidden layers of 10000 hidden units each, plus a logistic regression; 5) RBM with 10000 hiddens plus logistic regression; 6) \u201dFastfood FFT\u201d model (Le et al., 2013); 7) Zerobias autoencoder of 4000 hidden units with logistic regression (Konda et al.", "startOffset": 325, "endOffset": 342}, {"referenceID": 8, "context": ", 2013); 7) Zerobias autoencoder of 4000 hidden units with logistic regression (Konda et al., 2014); 8) 782-4000-1000-4000-10 Z-Lin network trained without dropout; 9) 782-4000-1000-4000-10004000-1000-4000-10 Z-Lin network, trained with dropout 10) Z-Lin network the same as (8) but trained with dropout and data augmentation; Results (1)-(5) are from Krizhevsky & Hinton (2009).", "startOffset": 79, "endOffset": 99}, {"referenceID": 8, "context": ", 2013); 7) Zerobias autoencoder of 4000 hidden units with logistic regression (Konda et al., 2014); 8) 782-4000-1000-4000-10 Z-Lin network trained without dropout; 9) 782-4000-1000-4000-10004000-1000-4000-10 Z-Lin network, trained with dropout 10) Z-Lin network the same as (8) but trained with dropout and data augmentation; Results (1)-(5) are from Krizhevsky & Hinton (2009). The final one is distinguished with a grey background because it uses data augmentation.", "startOffset": 80, "endOffset": 379}, {"referenceID": 12, "context": "An application of a linear bottleneck layer in the last layer of a neural network for dealing with highdimensional outputs is described in Sainath et al. (2013) and Xue et al.", "startOffset": 139, "endOffset": 161}, {"referenceID": 12, "context": "An application of a linear bottleneck layer in the last layer of a neural network for dealing with highdimensional outputs is described in Sainath et al. (2013) and Xue et al. (2013). In contrast to our work, in none of these methods is the goal to alleviate vanishing gradients and deal with sparsity, and (accordingly) they use just a single bottleneck layer in the network.", "startOffset": 139, "endOffset": 183}], "year": 2015, "abstractText": "We propose ways to improve the performance of fully connected networks. We found that two approaches in particular have a strong effect on performance: linear bottleneck layers and unsupervised pre-training using autoencoders without hidden unit biases. We show how both approaches can be related to improving gradient flow and reducing sparsity in the network. We show that a fully connected network can yield approximately 70% classification accuracy on the permutationinvariant CIFAR-10 task, which is much higher than the current state-of-the-art. By adding deformations to the training data, the fully connected network achieves 78% accuracy, which is just 10% short of a decent convolutional network.", "creator": "LaTeX with hyperref package"}}}