{"id": "1511.02583", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "9-Nov-2015", "title": "Batch-normalized Maxout Network in Network", "abstract": "This paper reports a novel deep architecture referred to as Maxout network In Network (MIN), which can enhance model discriminability and facilitate the process of information abstraction within the receptive field. The proposed network adopts the framework of the recently developed Network In Network structure, which slides a universal approximator, multilayer perceptron (MLP) with rectifier units, to exact features. Instead of MLP, we employ maxout MLP to learn a variety of piecewise linear activation functions and to mediate the problem of vanishing gradients that can occur when using rectifier units. Moreover, batch normalization is applied to reduce the saturation of maxout units by pre-conditioning the model and dropout is applied to prevent overfitting. Finally, average pooling is used in all pooling layers to regularize maxout MLP in order to facilitate information abstraction in every receptive field while tolerating the change of object position. Because average pooling preserves all features in the local patch, the proposed MIN model can enforce the suppression of irrelevant information during training. Our experiments demonstrated the state-of-the-art classification performance when the MIN model was applied to MNIST, CIFAR-10, and CIFAR-100 datasets and comparable performance for SVHN dataset.", "histories": [["v1", "Mon, 9 Nov 2015 07:09:57 GMT  (334kb,D)", "http://arxiv.org/abs/1511.02583v1", null]], "reviews": [], "SUBJECTS": "cs.CV cs.LG", "authors": ["jia-ren chang", "yong-sheng chen"], "accepted": false, "id": "1511.02583"}, "pdf": {"name": "1511.02583.pdf", "metadata": {"source": "CRF", "title": "Batch-normalized Maxout Network in Network", "authors": ["Jia-Ren Chang", "Yong-Sheng Chen"], "emails": ["followwar.cs00g@nctu.edu.tw", "yschen@cs.nctu.edu.tw"], "sections": [{"heading": "1. Introduction", "text": "In fact, most of them are able to survive on their own."}, {"heading": "2. Design of Deep Architecture", "text": "This section presents previous work related to the proposed MIN structure, including NIN, Maxout Network and batch normalization, followed by the design of the MIN ar architecture."}, {"heading": "2.1. NIN", "text": "The NIN model [18] uses the universal approximator MLP to extract features from local patches. Compared to CNN, the MLP, which uses a ReLU as an activation function, introduced the abstraction of information that is more representative of the latent concepts. The NIN model introduced the mlpconv layer, which consists of a linear revolutionary layer and a two-layer MLP. The calculation performed by the mlpconv layer is as follows: f1i, j, n1 = max (w1n1 T xi, j + bn1, 0), f2i, j, n2 = max (w2n2 T f1i, j + bn2, 0), f3i, j, n3 = max (w3n3 T f2i, j + bn3, 0), (1), where (i, j) the pixel index in the function charts is compressed."}, {"heading": "2.2. Maxout Network", "text": "Maxout MLP has so far proved to be a universal approximator [7], where a maxout unit is implemented by the following function: fi, j, n = max m [1, k] (wTnmxi, j + bnm), (2) where (i, j) is the pixel index in the feature maps, xi, j represents the site-centered input patch (i, j), and n is used to index the channels of feature maps fi, j, n that are constructed by taking the maximum over k of hidden parts. From another perspective, maxout unit is equivalent to a cross-channel max pooling layer on a revolutionary layer. The cross-channel max pooling layer selects the maximum power to be fed into the next layer. The maxout unit is helpful to solve the problem of gradient disappearance, as the gradient is able to flow through each unit."}, {"heading": "2.3. Batch Normalization", "text": "Batch normalization [11] is used to independently normalize each channel in the direction of the zero mean and unit variance: x-i, j, n = xi, j, n-E [xn] \u221a Var [xn], (3) to which the normalized value is scaled and shifted: fi, j, n = \u03b3nx, i, j, n + \u03b2n. (4) Here xi, j stands for the value at the location (i, j), n is used to index the channels of the characteristic scale, and scaling and shifting of the parameters \u03b3n, \u03b2n are new parameters that combine in network training. The batch normalization layer can be applied directly before the activation function to a convective network, such as ReLU or maxout. In this case, the non-linear units tend to generate an activation with a stable distribution that reduces saturation."}, {"heading": "2.4. Proposed MIN Architecture", "text": "As described in Section 2.1, NIN uses ReLU as an activation function in the mlpconv layer. In addition, we have replaced the ReLU activation functions in the two-layer MLP in NIN with the maxout units to overcome the disappearance of the gradient problem that often occurs when using ReLU. Furthermore, we have applied batch normalization immediately after the Convolutionary Calculation to avoid the covariant shift problem caused by changes in the data distribution. Specifically, we have removed the activation function of the Convolutionary Layer, making it a pure feature extractor. The architecture of the proposed MIN model is shown in Figure 1. Feature cards in a MIN block are calculated as follows: f1i, j, n1 = BN (w1n1 T xi, j + b 1 nj), 2 = maxm."}, {"heading": "3. Experiments", "text": "In the following experiments, the proposed method was evaluated using four benchmark datasets: MNIST [14], CIFAR-10 [12], CIFAR-100 [12], and SVHN [22]. The proposed model consists of three stacked MIN blocks followed by a Softmax layer. A MIN block comprises a revolutionary layer, a two-layer maxout MLP, and a spatial pooling layer. For regulation, dropout is applied between MIN blocks. Table 1 describes the parameter settings that match those of NIN [18] for fair comparison. The network was implemented with the MatConvNet toolbox [26] in the Matlab environment."}, {"heading": "3.1. MNIST", "text": "The MNIST dataset [14] consists of handwritten numerical images, 28 x 28 pixels in size, divided into 10 classes (0 to 9) with 60,000 training sessions and 10,000 test samples. Tests on this dataset were conducted without data magnification. Table 2 compares the results obtained in this study with those of previous work. Despite the fact that many methods can achieve very low error rates for the MNIST dataset, we achieved a test error rate of 0.24%, resulting in a new state-of-the-art performance without data magnification."}, {"heading": "3.2. CIFAR-10", "text": "The CIFAR-10 dataset [12] consists of natural color images measuring 32 x 32 pixels, 10 classes of 50,000 training images and 10,000 test images. For this dataset, we applied global contrast normalization and brightening according to the methods outlined in [7]. To allow comparison with previous work, the dataset was expanded by 2 pixels on each page, resulting in images measuring 36 x 36 pixels. Subsequently, we performed random corner sections measuring 32 x 32 pixels and random flipping during training. Table 3 compares our results with those of previous work. We achieved an error rate of 7.85% with no data augmentation and 6.75% with data augmentation. These are the best results to our knowledge."}, {"heading": "3.3. CIFAR-100", "text": "The CIFAR-100 dataset [12] has the same size and format as the CIFAR-10, but contains 100 classes. Therefore, the number of images per class is only one-tenth of that of CIFAR-10. Consequently, this dataset is much more sophisticated. We applied the hyperparameters used for CIFAR-10, but reset the learning rate, resulting in an error rate of 28.86% without data augmentation, which is the most advanced performance. Table 4 presents a summary of the best results obtained in previous work and current work."}, {"heading": "3.4. SVHN", "text": "The SVHN dataset consists of color images of house numbers (32 x 32 pixels) collected by Google Street View. There are 73,257 and 531,131 digits respectively in the training and additional digits. In accordance with previous work [7], we selected 400 samples per class from the training set and 200 samples per class from the additional set for validation. The remaining 598,388 images were used for training. In addition, there are 26,032 digits in the test set. For the SVHN dataset, we applied the hyperparameters as in the above-mentioned experiments, but corrected the learning rate. We also processed the dataset using local contrast normalization according to the method outlined by Goodfellow et al. [7]. Without data augmentation, we achieved a test error rate of 1.81%, comparable to the best result of previous work."}, {"heading": "3.5. Model capacity", "text": "According to Montufar et al. [20], a deep neural network with ReLU activation function at n0 input and hidden L layers of width n \u2265 n0 can have linear ranges. A deep maxout network with L layers of width n and k maxout layers can calculate functions in at least kL \u2212 1kn linear regions. This theoretical result suggests that the number of linear regions in a maxout network increases with the number of maxout layers. Furthermore, the number of linear regions in both ReLU and maxout networks increases exponentially with the number of layers. From this perspective, the maxout network model (MIM) [17] is actually more complex than the proposed method. However, the maxout network is susceptible to upgrading the training dataset without model regulation, due to the fact that the maxout network pieces in the MIM layer are the most valuable representations in the tested ARX and not improved."}, {"heading": "3.6. Regularization of average pooling in MIN", "text": "In this study, the MIN block is able to abstract representative information from each local patch in such a way that more differentiated information is embedded in the characteristic map. Thus, we can use the average spatial pool layer in each pool layer to aggregate local spatial information. We then compared the results by comparing the average pool layer in the first two pool layers with the maximum pool layer, while the last pool layer was fixed to the global average pool layer. Table 6 shows the test error associated with different pooling methods. The use of the average pool layer in all pool layers proved to be particularly effective. Irrelevant information in the input image can be inhibited by the average pool layer, but can be discarded by the maximum pool layer."}, {"heading": "3.7. Visualization of learned features", "text": "In fact, most of the people who are able to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to move, to move, to move, to move, to move, to move, to move, to move, to move, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance"}, {"heading": "4. Conclusions", "text": "This paper presents a novel, deep architecture, MIN. A MIN block, consisting of a revolutionary layer and a two-layer maxout MLP, is used to unify the input, and average pooling is applied in all pooling layers. From a neuroscientific perspective, the proposed architecture resembles the mechanism of the visual system in the brain. The proposed method outperforms others due to the following improvements: the MIN block facilitates the information abstraction of local patches, stack normalization prevents covariant shifts, and average pooling acts as a spatial regulator that tolerates changes in object position. Our experiments showed that the MIN method achieves state-of-the-art or comparable performance on the MNIST, CIFAR-10, CIFAR-100, and SVHN datasets. Furthermore, the extracted feature maps show the effectiveness of categorical representation by using the MIN method and its potential to detect multiple objects."}], "references": [{"title": "Normalization as a canonical neural computation", "author": ["M. Carandini", "D.J. Heeger"], "venue": "Nature Reviews Neuroscience, 13(1):51\u201362", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2012}, {"title": "Imagenet: A large-scale hierarchical image database", "author": ["J. Deng", "W. Dong", "R. Socher", "L.-J. Li", "K. Li", "L. Fei- Fei"], "venue": "CVPR09, pages 248\u2013255. IEEE", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2009}, {"title": "Neural mechanisms of selective visual attention", "author": ["R. Desimone", "J. Duncan"], "venue": "Annual review of neuroscience, 18(1):193\u2013 222", "citeRegEx": "3", "shortCiteRegEx": null, "year": 1995}, {"title": "Effects of noise letters upon the identification of a target letter in a nonsearch task", "author": ["B.A. Eriksen", "C.W. Eriksen"], "venue": "Perception & psychophysics, 16(1):143\u2013149", "citeRegEx": "4", "shortCiteRegEx": null, "year": 1974}, {"title": "Information processing in visual search: A continuous flow conception and experimental results", "author": ["C.W. Eriksen", "D.W. Schultz"], "venue": "Perception & Psychophysics, 25(4):249\u2013263", "citeRegEx": "5", "shortCiteRegEx": null, "year": 1979}, {"title": "Understanding the difficulty of training deep feedforward neural networks", "author": ["X. Glorot", "Y. Bengio"], "venue": "International conference on artificial intelligence and statistics, pages 249\u2013256", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2010}, {"title": "Maxout networks", "author": ["I.J. Goodfellow", "D. Warde-Farley", "M. Mirza", "A.C. Courville", "Y. Bengio"], "venue": "Proceedings of the 30th International Conference on Machine Learning  ", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2013}, {"title": "Pre-and poststimulus activation of response channels: a psychophysiological analysis", "author": ["G. Gratton", "M.G. Coles", "E.J. Sirevaag", "C.W. Eriksen", "E. Donchin"], "venue": "Journal of Experimental Psychology: Human perception and performance, 14(3):331", "citeRegEx": "8", "shortCiteRegEx": null, "year": 1988}, {"title": "Parametric reverse correlation reveals spatial linearity of retinotopic human v1 bold response", "author": ["K.A. Hansen", "S.V. David", "J.L. Gallant"], "venue": "Neuroimage, 23(1):233\u2013241", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2004}, {"title": "Improving neural networks by preventing co-adaptation of feature detectors", "author": ["G.E. Hinton", "N. Srivastava", "A. Krizhevsky", "I. Sutskever", "R. Salakhutdinov"], "venue": "CoRR, abs/1207.0580", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2012}, {"title": "Batch normalization: Accelerating deep network training by reducing internal covariate shift", "author": ["S. Ioffe", "C. Szegedy"], "venue": "ICML, volume 37 of JMLR Proceedings, pages 448\u2013456. JMLR.org", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2015}, {"title": "Learning multiple layers of features from tiny images", "author": ["A. Krizhevsky", "G. Hinton"], "venue": null, "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2009}, {"title": "Imagenet classification with deep convolutional neural networks", "author": ["A. Krizhevsky", "I. Sutskever", "G.E. Hinton"], "venue": "Advances in neural information processing systems, pages 1097\u20131105", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2012}, {"title": "Gradientbased learning applied to document recognition", "author": ["Y. LeCun", "L. Bottou", "Y. Bengio", "P. Haffner"], "venue": "Proceedings of the IEEE, 86(11):2278\u20132324", "citeRegEx": "14", "shortCiteRegEx": null, "year": 1998}, {"title": "Deeplysupervised nets", "author": ["C.-Y. Lee", "S. Xie", "P. Gallagher", "Z. Zhang", "Z. Tu"], "venue": "Proceedings of AISTATS 2015", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2015}, {"title": "Recurrent convolutional neural network for object recognition", "author": ["M. Liang", "X. Hu"], "venue": "In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR),", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2015}, {"title": "On the importance of normalisation layers in deep learning with piecewise linear activation units", "author": ["Z. Liao", "G. Carneiro"], "venue": "arXiv preprint arXiv:1508.00330", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2015}, {"title": "Network in network", "author": ["M. Lin", "Q. Chen", "S. Yan"], "venue": "International Conference on Learning Representations, abs/1312.4400", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2014}, {"title": "Rectifier nonlinearities improve neural network acoustic models", "author": ["A.L. Maas", "A.Y. Hannun", "A.Y. Ng"], "venue": "Proc. ICML, volume 30", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2013}, {"title": "On the number of linear regions of deep neural networks", "author": ["G.F. Montufar", "R. Pascanu", "K. Cho", "Y. Bengio"], "venue": "Advances in Neural Information Processing Systems, pages 2924\u20132932", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2014}, {"title": "Rectified linear units improve restricted boltzmann machines", "author": ["V. Nair", "G.E. Hinton"], "venue": "Proceedings of the 27th International Conference on Machine Learning ", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2010}, {"title": "Reading digits in natural images with unsupervised feature learning", "author": ["Y. Netzer", "T. Wang", "A. Coates", "A. Bissacco", "B. Wu", "A.Y. Ng"], "venue": "NIPS workshop on deep learning and unsupervised feature learning, volume 2011, page 5. Granada, Spain", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2011}, {"title": "Convolutional neural networks applied to house numbers digit classification", "author": ["P. Sermanet", "S. Chintala", "Y. LeCun"], "venue": "Proceedings of the 21th International Conference on Pattern Recognition ", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2012}, {"title": "Improving predictive inference under covariate shift by weighting the log-likelihood function", "author": ["H. Shimodaira"], "venue": "Journal of statistical planning and inference, 90(2):227\u2013244", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2000}, {"title": "Dropout: A simple way to prevent neural networks from overfitting", "author": ["N. Srivastava", "G. Hinton", "A. Krizhevsky", "I. Sutskever", "R. Salakhutdinov"], "venue": "The Journal of Machine Learning Research, 15(1):1929\u20131958", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2014}, {"title": "Matconvnet-convolutional neural networks for matlab", "author": ["A. Vedaldi", "K. Lenc"], "venue": "arXiv preprint arXiv:1412.4564", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2014}, {"title": "Regularization of neural networks using dropconnect", "author": ["L. Wan", "M. Zeiler", "S. Zhang", "Y.L. Cun", "R. Fergus"], "venue": "Proceedings of the 30th International Conference on Machine Learning ", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2013}, {"title": "From maxout to channel-out: Encoding information on sparse pathways", "author": ["Q. Wang", "J. JaJa"], "venue": "Artificial Neural Networks and Machine Learning\u2013ICANN 2014, pages 273\u2013280. Springer", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2014}, {"title": "Stochastic pooling for regularization of deep convolutional neural networks", "author": ["M.D. Zeiler", "R. Fergus"], "venue": "arXiv preprint arXiv:1301.3557", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2013}, {"title": "Visualizing and understanding convolutional networks", "author": ["M.D. Zeiler", "R. Fergus"], "venue": "Computer Vision\u2013ECCV 2014, pages 818\u2013833. Springer", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2014}], "referenceMentions": [{"referenceID": 12, "context": "Deep convolutional neural networks (CNNs) [13] have recently been applied to large image datasets, such as MNIST [14], CIFAR-10/100 [12], SVHN [22], and ImageNet [2] for image classification [10].", "startOffset": 42, "endOffset": 46}, {"referenceID": 13, "context": "Deep convolutional neural networks (CNNs) [13] have recently been applied to large image datasets, such as MNIST [14], CIFAR-10/100 [12], SVHN [22], and ImageNet [2] for image classification [10].", "startOffset": 113, "endOffset": 117}, {"referenceID": 11, "context": "Deep convolutional neural networks (CNNs) [13] have recently been applied to large image datasets, such as MNIST [14], CIFAR-10/100 [12], SVHN [22], and ImageNet [2] for image classification [10].", "startOffset": 132, "endOffset": 136}, {"referenceID": 21, "context": "Deep convolutional neural networks (CNNs) [13] have recently been applied to large image datasets, such as MNIST [14], CIFAR-10/100 [12], SVHN [22], and ImageNet [2] for image classification [10].", "startOffset": 143, "endOffset": 147}, {"referenceID": 1, "context": "Deep convolutional neural networks (CNNs) [13] have recently been applied to large image datasets, such as MNIST [14], CIFAR-10/100 [12], SVHN [22], and ImageNet [2] for image classification [10].", "startOffset": 162, "endOffset": 165}, {"referenceID": 9, "context": "Deep convolutional neural networks (CNNs) [13] have recently been applied to large image datasets, such as MNIST [14], CIFAR-10/100 [12], SVHN [22], and ImageNet [2] for image classification [10].", "startOffset": 191, "endOffset": 195}, {"referenceID": 29, "context": "In [30], Zeiler et al.", "startOffset": 3, "endOffset": 7}, {"referenceID": 29, "context": "Despite the advances that have been made in the development of this technology, many issues related to deep learning remain, including: (1) model discriminability and the robustness of learned features in early layers [30]; (2) the vanishing gradients and saturation of activation units during training [6]; and (3) limited training data, which may lead to overfitting [25].", "startOffset": 218, "endOffset": 222}, {"referenceID": 5, "context": "Despite the advances that have been made in the development of this technology, many issues related to deep learning remain, including: (1) model discriminability and the robustness of learned features in early layers [30]; (2) the vanishing gradients and saturation of activation units during training [6]; and (3) limited training data, which may lead to overfitting [25].", "startOffset": 303, "endOffset": 306}, {"referenceID": 24, "context": "Despite the advances that have been made in the development of this technology, many issues related to deep learning remain, including: (1) model discriminability and the robustness of learned features in early layers [30]; (2) the vanishing gradients and saturation of activation units during training [6]; and (3) limited training data, which may lead to overfitting [25].", "startOffset": 369, "endOffset": 373}, {"referenceID": 17, "context": "For enhancing model discriminability, the Network In Network (NIN) [18] model uses a sliding micro neural network, multilayer perceptron (MLP), to increase the nonlinearity of local patches in order to enable the abstraction of greater quantities of information within the receptive fields.", "startOffset": 67, "endOffset": 71}, {"referenceID": 14, "context": "Similarly, Deeply Supervised Nets (DSN) [15] provides companion objective functions to constrain hidden layers, such that robust features can be learned in the early layers of a deep CNN structure.", "startOffset": 40, "endOffset": 44}, {"referenceID": 12, "context": "Current solutions involve the use of rectified linear units (ReLU) to prevent vanishing gradients [13, 19, 21] because ReLU activates above 0 and its partial derivative is 1.", "startOffset": 98, "endOffset": 110}, {"referenceID": 18, "context": "Current solutions involve the use of rectified linear units (ReLU) to prevent vanishing gradients [13, 19, 21] because ReLU activates above 0 and its partial derivative is 1.", "startOffset": 98, "endOffset": 110}, {"referenceID": 20, "context": "Current solutions involve the use of rectified linear units (ReLU) to prevent vanishing gradients [13, 19, 21] because ReLU activates above 0 and its partial derivative is 1.", "startOffset": 98, "endOffset": 110}, {"referenceID": 6, "context": "Recently, the maxout network [7] provided a remedy to this problem.", "startOffset": 29, "endOffset": 32}, {"referenceID": 23, "context": "This phenomenon is referred to as internal covariate shift [24].", "startOffset": 59, "endOffset": 63}, {"referenceID": 10, "context": "[11] addressed this problem by applying batch normalization to the input of each layer.", "startOffset": 0, "endOffset": 4}, {"referenceID": 24, "context": "Dropout [25] and Dropconnect [27] techniques are widely used to regularize deep networks in order to prevent overfitting.", "startOffset": 8, "endOffset": 12}, {"referenceID": 26, "context": "Dropout [25] and Dropconnect [27] techniques are widely used to regularize deep networks in order to prevent overfitting.", "startOffset": 29, "endOffset": 33}, {"referenceID": 6, "context": "The idea of the technique is to randomly drop units or connections to prevent units from co-adapting, which has been shown to improve classification accuracy in numerous studies [7, 15, 18, 25].", "startOffset": 178, "endOffset": 193}, {"referenceID": 14, "context": "The idea of the technique is to randomly drop units or connections to prevent units from co-adapting, which has been shown to improve classification accuracy in numerous studies [7, 15, 18, 25].", "startOffset": 178, "endOffset": 193}, {"referenceID": 17, "context": "The idea of the technique is to randomly drop units or connections to prevent units from co-adapting, which has been shown to improve classification accuracy in numerous studies [7, 15, 18, 25].", "startOffset": 178, "endOffset": 193}, {"referenceID": 24, "context": "The idea of the technique is to randomly drop units or connections to prevent units from co-adapting, which has been shown to improve classification accuracy in numerous studies [7, 15, 18, 25].", "startOffset": 178, "endOffset": 193}, {"referenceID": 17, "context": "[18] proposed a strategy referred to as global average pooling for the replacement of fully-connected layers to enable the summing of spatial information of feature maps, thereby making it more robust to spatial translation.", "startOffset": 0, "endOffset": 4}, {"referenceID": 8, "context": "This strategy is close to human visual process, in which retinotopic response can be predicted by linear spatial summation [9].", "startOffset": 123, "endOffset": 126}, {"referenceID": 2, "context": "Within the same receptive field, the features of objects compete with each other in the object recognition process [3].", "startOffset": 115, "endOffset": 118}, {"referenceID": 17, "context": "Based on the NIN [18] structure, we employ a maxout MLP for feature extraction and refer to the proposed model as Maxout network In Network (MIN).", "startOffset": 17, "endOffset": 21}, {"referenceID": 17, "context": "The NIN model [18] uses the universal approximator MLP for the extraction of features from local patches.", "startOffset": 14, "endOffset": 18}, {"referenceID": 6, "context": "Maxout MLP has previously been proven as a universal approximator [7], wherein a maxout unit is implemented by the following function:", "startOffset": 66, "endOffset": 69}, {"referenceID": 10, "context": "of network parameters during training [11].", "startOffset": 38, "endOffset": 42}, {"referenceID": 10, "context": "Batch normalization [11] is used to independently normalize each channel toward zero mean and unit variance:", "startOffset": 20, "endOffset": 24}, {"referenceID": 0, "context": "Normalization also exists in biological neural network, which is a canonical neural computation well-studied in neuroscience field [1].", "startOffset": 131, "endOffset": 134}, {"referenceID": 17, "context": "The NIN [18] has capability to abstract representative features within the receptive field and thereby achieve good results in image classification.", "startOffset": 8, "endOffset": 12}, {"referenceID": 17, "context": "[18] demonstrated that the complexity of maxout networks increases with the number of maxout pieces or layers.", "startOffset": 0, "endOffset": 4}, {"referenceID": 27, "context": "The MIN block reduces saturation by applying batch normalization and makes it possible to encode information on pathways or in the activation patterns of maxout pieces [28].", "startOffset": 168, "endOffset": 172}, {"referenceID": 13, "context": "In the following experiments, the proposed method was evaluated using four benchmark datasets: MNIST [14], CIFAR-10 [12], CIFAR-100 [12], and SVHN [22].", "startOffset": 101, "endOffset": 105}, {"referenceID": 11, "context": "In the following experiments, the proposed method was evaluated using four benchmark datasets: MNIST [14], CIFAR-10 [12], CIFAR-100 [12], and SVHN [22].", "startOffset": 116, "endOffset": 120}, {"referenceID": 11, "context": "In the following experiments, the proposed method was evaluated using four benchmark datasets: MNIST [14], CIFAR-10 [12], CIFAR-100 [12], and SVHN [22].", "startOffset": 132, "endOffset": 136}, {"referenceID": 21, "context": "In the following experiments, the proposed method was evaluated using four benchmark datasets: MNIST [14], CIFAR-10 [12], CIFAR-100 [12], and SVHN [22].", "startOffset": 147, "endOffset": 151}, {"referenceID": 17, "context": "Table 1 details the parameter settings which, for the sake of a fair comparison, are the same as those used in NIN [18].", "startOffset": 115, "endOffset": 119}, {"referenceID": 25, "context": "The network was implemented using the MatConvNet [26] toolbox in the Matlab environment.", "startOffset": 49, "endOffset": 53}, {"referenceID": 6, "context": "[7] to determine the hyper-parameters of the model, such as momentum, weight decay, and learning rate decay.", "startOffset": 0, "endOffset": 3}, {"referenceID": 13, "context": "The MNIST dataset [14] consists of handwritten digit images, 28 x 28 pixels in size, organized into 10 classes (0 to 9) with 60,000 training and 10,000 test samples.", "startOffset": 18, "endOffset": 22}, {"referenceID": 11, "context": "The CIFAR-10 dataset [12] consists of color natural images, 32 x 32 pixels in size, from 10 classes with 50,000 training and 10,000 test images.", "startOffset": 21, "endOffset": 25}, {"referenceID": 6, "context": "For this dataset, we applied global contrast normalization and whitening in accordance with the methods outlined in [7].", "startOffset": 116, "endOffset": 119}, {"referenceID": 11, "context": "The CIFAR-100 dataset [12] is the same size and format as the CIFAR-10; however, it contains 100 classes.", "startOffset": 22, "endOffset": 26}, {"referenceID": 6, "context": "In accordance with previous works [7], we selected 400 samples per class from the train-", "startOffset": 34, "endOffset": 37}, {"referenceID": 28, "context": "Method Error (%) Stochastic pooling [29] 0.", "startOffset": 36, "endOffset": 40}, {"referenceID": 6, "context": "47 Maxout network (k=2) [7] 0.", "startOffset": 24, "endOffset": 27}, {"referenceID": 17, "context": "47 NIN [18] 0.", "startOffset": 7, "endOffset": 11}, {"referenceID": 14, "context": "45 DSN [15] 0.", "startOffset": 7, "endOffset": 11}, {"referenceID": 16, "context": "39 MIM (k=2) [17] 0.", "startOffset": 13, "endOffset": 17}, {"referenceID": 15, "context": "03 RCNN-96 [16] 0.", "startOffset": 11, "endOffset": 15}, {"referenceID": 6, "context": "[7].", "startOffset": 0, "endOffset": 3}, {"referenceID": 28, "context": "Comparison of test errors on CIFAR-10 dataset Method Error (%) No data augmentation Stochastic pooling [29] 15.", "startOffset": 103, "endOffset": 107}, {"referenceID": 6, "context": "13 Maxout network (k=2) [7] 11.", "startOffset": 24, "endOffset": 27}, {"referenceID": 17, "context": "68 NIN [18] 10.", "startOffset": 7, "endOffset": 11}, {"referenceID": 14, "context": "41 DSN [15] 9.", "startOffset": 7, "endOffset": 11}, {"referenceID": 15, "context": "69 RCNN-160 [16] 8.", "startOffset": 12, "endOffset": 16}, {"referenceID": 16, "context": "69 MIM (k=2) [17] 8.", "startOffset": 13, "endOffset": 17}, {"referenceID": 6, "context": "85 Data augmentation Maxout network (k=2) [7] 9.", "startOffset": 42, "endOffset": 45}, {"referenceID": 17, "context": "38 NIN [18] 8.", "startOffset": 7, "endOffset": 11}, {"referenceID": 14, "context": "81 DSN [15] 8.", "startOffset": 7, "endOffset": 11}, {"referenceID": 15, "context": "22 RCNN-160 [16] 7.", "startOffset": 12, "endOffset": 16}, {"referenceID": 28, "context": "Comparison of test errors on CIFAR-100 dataset Method Error (%) Stochastic pooling [29] 42.", "startOffset": 83, "endOffset": 87}, {"referenceID": 6, "context": "51 Maxout network (k=2) [7] 38.", "startOffset": 24, "endOffset": 27}, {"referenceID": 17, "context": "57 NIN [18] 35.", "startOffset": 7, "endOffset": 11}, {"referenceID": 14, "context": "68 DSN [15] 34.", "startOffset": 7, "endOffset": 11}, {"referenceID": 15, "context": "57 RCNN-160 [16] 31.", "startOffset": 12, "endOffset": 16}, {"referenceID": 16, "context": "75 MIM (k=2) [17] 29.", "startOffset": 13, "endOffset": 17}, {"referenceID": 26, "context": "Note that Dropconnet [27] uses data augmentation and multiple model voting Method Error (%) Maxout network (k=2) [7] 2.", "startOffset": 21, "endOffset": 25}, {"referenceID": 6, "context": "Note that Dropconnet [27] uses data augmentation and multiple model voting Method Error (%) Maxout network (k=2) [7] 2.", "startOffset": 113, "endOffset": 116}, {"referenceID": 17, "context": "47 NIN [18] 2.", "startOffset": 7, "endOffset": 11}, {"referenceID": 22, "context": "35 Human performance [23] 2.", "startOffset": 21, "endOffset": 25}, {"referenceID": 16, "context": "00 MIM (k=2) [17] 1.", "startOffset": 13, "endOffset": 17}, {"referenceID": 26, "context": "08 Dropconnect [27] 1.", "startOffset": 15, "endOffset": 19}, {"referenceID": 14, "context": "94 DSN [15] 1.", "startOffset": 7, "endOffset": 11}, {"referenceID": 15, "context": "81 RCNN-192 [16] 1.", "startOffset": 12, "endOffset": 16}, {"referenceID": 19, "context": "[20], a deep neural network using ReLU activation function with n0 input and L hidden layers of width n \u2265 n0 can have \u03a9 ( (n/n0) 0nn0 )", "startOffset": 0, "endOffset": 4}, {"referenceID": 16, "context": "From this perspective, the Maxout network In Maxout network (MIM) model [17] is indeed more complex than the proposed method.", "startOffset": 72, "endOffset": 76}, {"referenceID": 3, "context": "In human visual system, the competing process of distractors has been investigated by the Eriksen flanker task [4].", "startOffset": 111, "endOffset": 114}, {"referenceID": 7, "context": "Psychophysiological analysis [8] supported the theory indicating that the flankers activate the incorrect response competing with the correct response [5].", "startOffset": 29, "endOffset": 32}, {"referenceID": 4, "context": "Psychophysiological analysis [8] supported the theory indicating that the flankers activate the incorrect response competing with the correct response [5].", "startOffset": 151, "endOffset": 154}], "year": 2015, "abstractText": "This paper reports a novel deep architecture referred to as Maxout network In Network (MIN), which can enhance model discriminability and facilitate the process of information abstraction within the receptive field. The proposed network adopts the framework of the recently developed Network In Network structure, which slides a universal approximator, multilayer perceptron (MLP) with rectifier units, to exact features. Instead of MLP, we employ maxout MLP to learn a variety of piecewise linear activation functions and to mediate the problem of vanishing gradients that can occur when using rectifier units. Moreover, batch normalization is applied to reduce the saturation of maxout units by pre-conditioning the model and dropout is applied to prevent overfitting. Finally, average pooling is used in all pooling layers to regularize maxout MLP in order to facilitate information abstraction in every receptive field while tolerating the change of object position. Because average pooling preserves all features in the local patch, the proposed MIN model can enforce the suppression of irrelevant information during training. Our experiments demonstrated the state-of-the-art classification performance when the MIN model was applied to MNIST, CIFAR-10, and CIFAR-100 datasets and comparable performance for SVHN dataset.", "creator": "LaTeX with hyperref package"}}}