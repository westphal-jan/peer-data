{"id": "1511.02841", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "9-Nov-2015", "title": "Symmetries and control in generative neural nets", "abstract": "We study generative nets which can control and modify observations, after being trained on real-life datasets. In order to zoom-in on an object, some spatial, color and other attributes are learned by classifiers and attention nets. Plugging these symmetry statistics in the generative layers of auto-classifiers-encoders (ACE) appears to be the most direct way to simultaneously: i) generate new observations with arbitrary attributes, from a given class , ii) describe the true \"style\", i.e., the low-dimensional latent manifold encoding the \"essence\" of the data, after superfluous attributes are factored out, and iii) organically control, i.e., move or modify objects within given observations. We demonstrate the sharp improvement of the generative qualities of shallow ACE with spatial symmetry statistics, on the distorted MNIST and CIFAR10 datasets.", "histories": [["v1", "Mon, 9 Nov 2015 20:49:03 GMT  (1842kb,D)", "https://arxiv.org/abs/1511.02841v1", null], ["v2", "Mon, 16 Nov 2015 17:49:51 GMT  (1097kb,D)", "http://arxiv.org/abs/1511.02841v2", null], ["v3", "Fri, 8 Apr 2016 21:38:31 GMT  (1097kb,D)", "http://arxiv.org/abs/1511.02841v3", null]], "reviews": [], "SUBJECTS": "cs.CV cs.LG", "authors": ["galin georgiev"], "accepted": false, "id": "1511.02841"}, "pdf": {"name": "1511.02841.pdf", "metadata": {"source": "CRF", "title": "SYMMETRIES AND CONTROL IN GENERATIVE NEURAL NETS", "authors": ["Galin Georgiev"], "emails": ["galin.georgiev@gammadynamics.com"], "sections": [{"heading": "1 INTRODUCTION", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "1.1 GENERATIVITY AND CONTROL.", "text": "Generating plausible but hitherto invisible observations seems to have been, at least chronologically, one of the most difficult challenges for artificial neural networks. A generative network can \"dream\" new observations, each a vector in a high-dimensional space RN, scanning from a probability density of white noise p (z). This model density is based on a preferably low-dimensional space of latent variables z = {z (N)} Nlat\u0443 = 1. In order to generate plausible new observations, the latent manifold must encode the complexity of the set of P-training observations {x\u00b5} P\u00b5 = 1% RN. Generativity has much more to offer than \"dreaming-up\" new random observations. It is the centerpiece of the control capabilities of a neural network. Visual biological networks, for example, capture existential motor information such as location / shape and other attributes of an object and can act on it in a natural way, by moving it to or modifying an object."}, {"heading": "1.2 LEARNING FROM REAL-LIFE DATA.", "text": "From the recent crop of generative networks, Section 2, only one seems to offer this desirable reconstruction of a low-dimensional latent diversity: the varying auto-encoders (UAE) Kingma & Welling (2014), Rezende et al. (2014) Their subset, called Gibbs machines, also has far-reaching roots in information geometry and thermodynamics, which are very useful. They work well with idealized visual datasets such as MNIST LeCun et al. (1998). Unfortunately, like the other generative networks, they do not cope well with more realistic images when objects are spatially varied or when there is severe disorder in the background. These features are simulated in the Rotated Translated-Scaled (RTS) ar Xiv: 151 1.02 841v 3 [cs.C] 8A pr2 016MNIST and Translated-Warped (TC) MNIST, Annex B. We highlight the deficits in the basic class of networks."}, {"heading": "1.3 \u201cA HORSE, A HORSE! MY KINGDOM FOR A HORSE!\u201d 2", "text": "In fact, it is such that most of them will be able to move to another world, in which they are able to move to another world, in which they are able to move to another world, in which they are able to move, in which they move, in which they move, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they, in which they live, in which they, in which they live, in which they live, in which they, in which they live, in which they, in which they live, in which they, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they, in which they live, in which they live, in which they live, in which they live, in which they live, in which they, in which they live, in which they live, in which they, in which they live, in which they, in which they, in which they, in which they, in which they, in which they, in which they live, in which they, in which they, in which they, in which they, in which they, in which they, in which they, live, in which they, in which they, in which they, live, in which they, in which they, in which they, live, in which they, in which they, in which they, in which they, in which they, in which they, in which they, live, in which they, in which they, in which they, in fact, in fact, in fact, are able to put themselves, are able to move,"}, {"heading": "2 GENERATIVE NETS AND THE LATENT MANIFOLD.", "text": "There are two scenarios: a) the network has reconstruction capabilities, therefore q (x) can in theory be evaluated on the formation and testing of observations, the aim being to minimize the so-called cross-entropy or negative log probability, i.e., the expectation E (\u2212 log q), where there is an expectation of the empirical density r (). (Recently proposed reconstructive generative networks are: i) the generalized denoization of the auto-encoder (DAE) Bengio al. (2013), ii) the empirical density r."}, {"heading": "3 THE THEORY. CONNECTIONS WITH INFORMATION GEOMETRY AND THERMODYNAMICS.", "text": "A theoretical framework for universal networks was recently outlined in Georgiev (2015). Some of the constructs there, such as the ACE architecture, appeared optional and driven exclusively by universality requirements. We summarize and generalize the framework in the current context, arguing that the ACE architecture or its variations are indispensable for generative reconstructive networks. It follows from the probability or variation theory of the Pythagorean Theorem, Chentsov (1968), which underlies modern estimation theory, and is pervasive in information geometry, Amari & Nagaoka (2000)."}, {"heading": "4 ACE WITH SYMMETRY STATISTICS.", "text": "The ACE architecture with symmetry statistics is shown in Figure 5. As in the basic ACE statistics, training is monitored, i.e. labels are used in the auto encoder and each class has a dedicated decoder, with unimodal sampling in the generative layer of each class. Instead, sampling during the test is done using a mixture of densities, with mixing weights {\u03c9\u00b5, c} NCc = 1 for the \u00b5-th observation, for the class c generated by the classifier. Posterior density from Section 2 yields 5: p (z | x\u00b5) = NC \u2211 c = 1 spec, cp (z | x\u00b5, c). (4.1) 4 Laplacian density is not in the exponential class, but is a sum of two exponential densities located in the exponential class."}, {"heading": "5 OPEN PROBLEMS.", "text": "1. Test experimentally deep convolutionary ACE-s with (common) feature maps, both in the classifier and in the encoder. From feature maps at different depths, generate corresponding generative latent variables. Add latent variables at different depths symmetry statistics. 2. Create separate symmetry statistics for separate feature maps in generative networks, in the spirit of Hinton et al. (2011)."}, {"heading": "ACKNOWLEDGMENTS", "text": "We appreciate conversations with Nikola Toshev, Stefan Petrov and their help with CIFAR10."}, {"heading": "B DISTORTED MNIST.", "text": "The two distorted MNIST datasets replicate Jadeberg et al. (2015), Appendix A.3, although different random seeds and implementation details may cause differences. The rotated-translated-scaled (RTS) MNIST is located on 42x42 canvases with random + / - 45 \u00b0 rotations, + / - 7 pixel translations, and 1.2 / 0.7 scales. In the translated-cluttered (TC) MNIST, the original image is randomly translated across a canvas of 60x60, with 6 clutter of the size 6x6 randomly extracted from randomly selected other images and added to the background."}], "references": [{"title": "Multiple object recognition with visual attention", "author": ["Ba", "Jimmy", "Mnih", "Volodymyr", "Kavukcuoglu", "Koray"], "venue": "In ICLR,", "citeRegEx": "Ba et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Ba et al\\.", "year": 2014}, {"title": "Generalized denoising autoencoders as generative models", "author": ["Bengio", "Yoshua", "Yao", "Li", "Alain", "Guillaume", "Vincent", "Pascal"], "venue": "In NIPS,", "citeRegEx": "Bengio et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Bengio et al\\.", "year": 2013}, {"title": "Deep generative stochastic networks trainable by backprop", "author": ["Bengio", "Yoshua", "Thibodeau-Laufer", "Eric", "Yosinski", "Jason"], "venue": "In ICML,", "citeRegEx": "Bengio et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Bengio et al\\.", "year": 2014}, {"title": "A backward progression of attentional effects in the ventral stream", "author": ["E.A. Buffalo", "P. Fries", "R. Landman", "H. Liang", "R. Desimone"], "venue": "Proc. Nat. Acad. Sci.,", "citeRegEx": "Buffalo et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Buffalo et al\\.", "year": 2010}, {"title": "Nonsymmetrical distance between probability distributions, entropy and the theorem of Pythagoras", "author": ["N.N. Chentsov"], "venue": "Mathematical notes of the Academy of Sciences of the USSR,", "citeRegEx": "Chentsov,? \\Q1968\\E", "shortCiteRegEx": "Chentsov", "year": 1968}, {"title": "Multi-column deep neural networks for image classification", "author": ["Ciresan", "Dan", "Meier", "Ueli", "Schmidhuber", "Juergen"], "venue": null, "citeRegEx": "Ciresan et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Ciresan et al\\.", "year": 2012}, {"title": "Deep generative image models using a laplacian pyramid of adversarial networks", "author": ["Denton", "Emily", "Chintala", "Soumith", "Szlam", "Arthur", "Fergus", "Rob"], "venue": null, "citeRegEx": "Denton et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Denton et al\\.", "year": 2015}, {"title": "NICE: Non-linear independent components estimation", "author": ["Dinh", "Laurent", "Krueger", "David", "Bengio", "Yoshua"], "venue": null, "citeRegEx": "Dinh et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Dinh et al\\.", "year": 2014}, {"title": "Towards universal neural nets: Gibbs machines and ACE", "author": ["Georgiev", "Galin"], "venue": null, "citeRegEx": "Georgiev and Galin.,? \\Q2015\\E", "shortCiteRegEx": "Georgiev and Galin.", "year": 2015}, {"title": "DRAW: A recurrent neural network for image generation", "author": ["Gregor", "Karol", "Danihelka", "Ivo", "Graves", "Alex", "Wierstra", "Daan"], "venue": null, "citeRegEx": "Gregor et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Gregor et al\\.", "year": 2015}, {"title": "Transforming auto-encoders", "author": ["Hinton", "Geoffrey", "Krizhevsky", "Alex", "S. Wang"], "venue": "In ICANN,", "citeRegEx": "Hinton et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Hinton et al\\.", "year": 2011}, {"title": "Adam: A method for stochastic optimization", "author": ["Kingma", "Diederik", "Ba", "Jimmy"], "venue": "In ICLR,", "citeRegEx": "Kingma et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Kingma et al\\.", "year": 2015}, {"title": "Auto-encoding variational Bayes", "author": ["Kingma", "Durk P", "Welling", "Max"], "venue": "In ICLR,", "citeRegEx": "Kingma et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Kingma et al\\.", "year": 2014}, {"title": "Semi-supervised learning with deep generative models", "author": ["Kingma", "Durk P", "Rezende", "Danilo J", "Mohamed", "Shakir", "Welling", "Max"], "venue": null, "citeRegEx": "Kingma et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Kingma et al\\.", "year": 2014}, {"title": "Learning multiple layers of features from tiny images", "author": ["Krizhevsky", "Alex"], "venue": "Technical report,", "citeRegEx": "Krizhevsky and Alex.,? \\Q2009\\E", "shortCiteRegEx": "Krizhevsky and Alex.", "year": 2009}, {"title": "Imagenet classification with deep convolutional neural networks", "author": ["Krizhevsky", "Alex", "Sutskever", "Ilya", "Hinton", "Geoffrey"], "venue": "In NIPS,", "citeRegEx": "Krizhevsky et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Krizhevsky et al\\.", "year": 2012}, {"title": "Statistical Physics, Part 1, 3rd edition", "author": ["L.D. Landau", "E.M. Lifshitz"], "venue": null, "citeRegEx": "Landau and Lifshitz,? \\Q1980\\E", "shortCiteRegEx": "Landau and Lifshitz", "year": 1980}, {"title": "The stochastic group", "author": ["Poole", "David"], "venue": "American Mathematical Monthly,", "citeRegEx": "Poole and David.,? \\Q1995\\E", "shortCiteRegEx": "Poole and David.", "year": 1995}, {"title": "Variational inference with normalizing flows", "author": ["Rezende", "Danilo J", "Mohamed", "Shakir"], "venue": null, "citeRegEx": "Rezende et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Rezende et al\\.", "year": 2015}, {"title": "Stochastic backpropagation and approximate inference in deep generative models", "author": ["Rezende", "Danilo J", "Mohamed", "Shakir", "Wierstra", "Daan"], "venue": "In JMLR,", "citeRegEx": "Rezende et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Rezende et al\\.", "year": 2014}, {"title": "A generative process for sampling contractive auto-encoders", "author": ["Rifai", "Salah", "Bengio", "Yoshua", "Dauphin", "Yann", "Vincent", "Pascal"], "venue": "In ICML,", "citeRegEx": "Rifai et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Rifai et al\\.", "year": 2012}, {"title": "Overfeat: Integrated recognition, localization and detection using convolutional networks", "author": ["Sermanet", "Pierre", "Eigen", "David", "Zhang", "Xiang", "Mathieu", "Micha\u00ebl", "Fergus", "Rob", "LeCun", "Yann"], "venue": null, "citeRegEx": "Sermanet et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Sermanet et al\\.", "year": 2014}, {"title": "Thalamic relays and cortical functioning", "author": ["Sherman", "Murray"], "venue": "Progress in Brain Research,", "citeRegEx": "Sherman and Murray.,? \\Q2005\\E", "shortCiteRegEx": "Sherman and Murray.", "year": 2005}, {"title": "Transformation invariance in pattern recognition: Tangent distance and propagation", "author": ["P. Simard", "Cun", "Y. Le", "J. Denker", "B. Victorri"], "venue": "Int. J. Imag. Syst. Tech.,", "citeRegEx": "Simard et al\\.,? \\Q2000\\E", "shortCiteRegEx": "Simard et al\\.", "year": 2000}, {"title": "Deep unsupervised learning using nonequilibrium thermodynamics", "author": ["Sohl-Dickstein", "Jascha", "Weiss", "Eric", "Maheswaranathan", "Niru", "Ganguli", "Surya"], "venue": "In ICML,", "citeRegEx": "Sohl.Dickstein et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Sohl.Dickstein et al\\.", "year": 2015}, {"title": "Visualizing and understanding convolutional networks", "author": ["Zeiler", "Matthew D", "Fergus", "Rob"], "venue": "In ECCV,", "citeRegEx": "Zeiler et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Zeiler et al\\.", "year": 2014}, {"title": "Optimizer is Adam, Kingma & Ba (2015), stochastic gradient descent back-propagation, learning rate = 0.0015 for MNIST and 0.0005 for CIFAR10, decay = 50 epochs, batch size = 250. We used only one standard set of hyper-parameters per dataset and have not done hyper-parameter optimizations. Convolutional weights are initialized uniformly in (\u22121, 1) and normalized by square root of the product", "author": ["Theano platform", "Bastien"], "venue": null, "citeRegEx": "platform and Bastien,? \\Q2012\\E", "shortCiteRegEx": "platform and Bastien", "year": 2012}, {"title": "2015), produces six affine spatial symmetry statistics (box 0.2 in Figure 5). This net has 2 convolutional hidden layers, with 20 5x5 filters each, with 2x2 max-poolings between layers, and a fully-connected layer of size 50. Figure 3: Layer sizes 3072-2048-2048(2x10)-(2048x10)-(2048x10)-(3072x10) for the auto-encoder branch, same classifier as in Fig 2. The symmetry statistics net has 2 convolutional hidden layers, with 32-64 3x3 filters", "author": ["Jadeberg"], "venue": null, "citeRegEx": "Jadeberg,? \\Q2048\\E", "shortCiteRegEx": "Jadeberg", "year": 2048}], "referenceMentions": [{"referenceID": 18, "context": "From the recent crop of generative nets, section 2, only one appears to offer this desirable reconstruction via a low-dimensional latent manifold: the variational auto-encoders (VAE) Kingma & Welling (2014), Rezende et al. (2014). Their subset called Gibbs machines, has also far-reaching roots into information geometry and thermodynamics, which come in very handy.", "startOffset": 208, "endOffset": 230}, {"referenceID": 18, "context": "From the recent crop of generative nets, section 2, only one appears to offer this desirable reconstruction via a low-dimensional latent manifold: the variational auto-encoders (VAE) Kingma & Welling (2014), Rezende et al. (2014). Their subset called Gibbs machines, has also far-reaching roots into information geometry and thermodynamics, which come in very handy. They perform well on idealized visual data sets like MNIST LeCun et al. (1998). Unfortunately, like the other generative nets, they do not cope well with more realistic images, when objects are spatially varied or, if there is heavy clutter in the background.", "startOffset": 208, "endOffset": 446}, {"referenceID": 21, "context": "We have horse-like creatures, which morph into giraffes as one moves up the grid! The first successful application of Lie algebra symmetries to neural nets was in Simard et al. (2000). The recent crop of spatial attention nets Jadeberg et al.", "startOffset": 163, "endOffset": 184}, {"referenceID": 21, "context": "We have horse-like creatures, which morph into giraffes as one moves up the grid! The first successful application of Lie algebra symmetries to neural nets was in Simard et al. (2000). The recent crop of spatial attention nets Jadeberg et al. (2015), Gregor et al.", "startOffset": 163, "endOffset": 250}, {"referenceID": 9, "context": "(2015), Gregor et al. (2015), Sermanet et al.", "startOffset": 8, "endOffset": 29}, {"referenceID": 23, "context": "(2014), or Figure 3 (d) in Sohl-Dickstein et al. (2015). The improvement in Denton et al.", "startOffset": 27, "endOffset": 56}, {"referenceID": 6, "context": "The improvement in Denton et al. (2015) is due to so-called Laplacian pyramids, and can be overlayed on any core generative model.", "startOffset": 19, "endOffset": 40}, {"referenceID": 0, "context": "(2014), Ba et al. (2014) optimize spatial symmetry statistics, corresponding to a given object inside an observation.", "startOffset": 8, "endOffset": 25}, {"referenceID": 0, "context": "(2014), Ba et al. (2014) optimize spatial symmetry statistics, corresponding to a given object inside an observation. An efficient calculation of symmetry statistics, for multiple objects, requires a classifier. Hence, generation and reconstruction on real-life datasets lead to an auto-encoder/classifier combo like ACE. Supplementing auto-encoders with affine transforms was first proposed in Hinton et al. (2011), where spatial symmetry statistics were referred to as \u201ccapsules\u201d.", "startOffset": 8, "endOffset": 416}, {"referenceID": 3, "context": "Feedback loops for attention data, vaguely reminiscent of Figure 5, have been identified between higher- and lower-level visual areas of the brain, Sherman (2005), Buffalo et al. (2010). For colored images, one also needs the color symmetry statistics, forming a semigroup of nonnegative 3x3 matrices in the stochastic group3 S(3,R).", "startOffset": 164, "endOffset": 186}, {"referenceID": 17, "context": "Latent manifold learning was pioneered for modern nets in Rifai et al. (2012). When a latent sample z\u03bd is chosen from a model density p(z), a generative net decodes it into a simulated observation x\u0302\u03bd , from a corresponding model density q(x\u0302).", "startOffset": 58, "endOffset": 78}, {"referenceID": 1, "context": "Recently proposed reconstructive generative nets are: i) the generalized denoising auto-encoders (DAE) Bengio et al. (2013), ii) the generative stochastic networks (GSN) Bengio et al.", "startOffset": 103, "endOffset": 124}, {"referenceID": 1, "context": "Recently proposed reconstructive generative nets are: i) the generalized denoising auto-encoders (DAE) Bengio et al. (2013), ii) the generative stochastic networks (GSN) Bengio et al. (2014), iii) the variational auto-encoders introduced above, iv) the non-linear independent component estimation (NICE) Dinh et al.", "startOffset": 103, "endOffset": 191}, {"referenceID": 1, "context": "Recently proposed reconstructive generative nets are: i) the generalized denoising auto-encoders (DAE) Bengio et al. (2013), ii) the generative stochastic networks (GSN) Bengio et al. (2014), iii) the variational auto-encoders introduced above, iv) the non-linear independent component estimation (NICE) Dinh et al. (2014), and v) Sohl-Dickstein et al.", "startOffset": 103, "endOffset": 323}, {"referenceID": 1, "context": "Recently proposed reconstructive generative nets are: i) the generalized denoising auto-encoders (DAE) Bengio et al. (2013), ii) the generative stochastic networks (GSN) Bengio et al. (2014), iii) the variational auto-encoders introduced above, iv) the non-linear independent component estimation (NICE) Dinh et al. (2014), and v) Sohl-Dickstein et al. (2015). Except The subgroup of matrices \u2208 GL(3,R), with entries in each row adding up to one, Poole (1995).", "startOffset": 103, "endOffset": 360}, {"referenceID": 1, "context": "Recently proposed reconstructive generative nets are: i) the generalized denoising auto-encoders (DAE) Bengio et al. (2013), ii) the generative stochastic networks (GSN) Bengio et al. (2014), iii) the variational auto-encoders introduced above, iv) the non-linear independent component estimation (NICE) Dinh et al. (2014), and v) Sohl-Dickstein et al. (2015). Except The subgroup of matrices \u2208 GL(3,R), with entries in each row adding up to one, Poole (1995).", "startOffset": 103, "endOffset": 460}, {"referenceID": 4, "context": "It follows from the probabilistic or variational Pythagorean theorem, Chentsov (1968), which underlies modern estimation theory, and is pervasive in information geometry, Amari & Nagaoka", "startOffset": 70, "endOffset": 86}, {"referenceID": 11, "context": "Using a similar mixture of posterior densities, but different architecturally conditional VAEs, were proposed in the context of semi-supervised learning in Kingma et al. (2014).", "startOffset": 156, "endOffset": 177}, {"referenceID": 5, "context": "External augmentation is known to improve significantly a net\u2019s classification performance Ciresan et al. (2012), Krizhevsky et al.", "startOffset": 91, "endOffset": 113}, {"referenceID": 5, "context": "External augmentation is known to improve significantly a net\u2019s classification performance Ciresan et al. (2012), Krizhevsky et al. (2012). This in turn improves the quality of the symmetry statistics and creates a virtuous feedback cycle.", "startOffset": 91, "endOffset": 139}, {"referenceID": 10, "context": "Produce separate symmetry statistics for separate feature maps in generative nets, in the spirit of Hinton et al. (2011).", "startOffset": 100, "endOffset": 121}], "year": 2016, "abstractText": "We study generative nets which can control and modify observations, after being trained on real-life datasets. In order to zoom-in on an object, some spatial, color and other attributes are learned by classifiers in specialized attention nets. In fieldtheoretical terms, these learned symmetry statistics form the gauge group of the data set. Plugging them in the generative layers of auto-classifiers-encoders (ACE) appears to be the most direct way to simultaneously: i) generate new observations with arbitrary attributes, from a given class; ii) describe the low-dimensional manifold encoding the \u201cessence\u201d of the data, after superfluous attributes are factored out; and iii) organically control, i.e., move or modify objects within given observations. We demonstrate the sharp improvement of the generative qualities of shallow ACE, with added spatial and color symmetry statistics, on the distorted MNIST and CIFAR10 datasets.", "creator": "LaTeX with hyperref package"}}}