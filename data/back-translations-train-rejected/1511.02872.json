{"id": "1511.02872", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "9-Nov-2015", "title": "Visual Language Modeling on CNN Image Representations", "abstract": "Measuring the naturalness of images is important to generate realistic images or to detect unnatural regions in images. Additionally, a method to measure naturalness can be complementary to Convolutional Neural Network (CNN) based features, which are known to be insensitive to the naturalness of images. However, most probabilistic image models have insufficient capability of modeling the complex and abstract naturalness that we feel because they are built directly on raw image pixels. In this work, we assume that naturalness can be measured by the predictability on high-level features during eye movement. Based on this assumption, we propose a novel method to evaluate the naturalness by building a variant of Recurrent Neural Network Language Models on pre-trained CNN representations. Our method is applied to two tasks, demonstrating that 1) using our method as a regularizer enables us to generate more understandable images from image features than existing approaches, and 2) unnaturalness maps produced by our method achieve state-of-the-art eye fixation prediction performance on two well-studied datasets.", "histories": [["v1", "Mon, 9 Nov 2015 21:00:08 GMT  (3587kb,D)", "http://arxiv.org/abs/1511.02872v1", null]], "reviews": [], "SUBJECTS": "cs.CV cs.AI cs.LG", "authors": ["hiroharu kato", "tatsuya harada"], "accepted": false, "id": "1511.02872"}, "pdf": {"name": "1511.02872.pdf", "metadata": {"source": "CRF", "title": "Visual Language Modeling on CNN Image Representations", "authors": ["Hiroharu Kato", "Tatsuya Harada"], "emails": ["kato@mi.t.u-tokyo.ac.jp", "harada@mi.t.u-tokyo.ac.jp"], "sections": [{"heading": "1. Introduction", "text": "Measuring the naturalness of images is an important problem. By measuring naturalness, one can generate realistic images or recognize unnatural regions in images. Convolutional Neural Networks (CNNs) are an extremely important component in computer vision. However, they are known to be insensitive to the naturalness of images. Images generated by them appear strange and unrealistic [40]. Furthermore, they are prone to unnatural noise or artificial fabrication [16, 41, 51]. Therefore, a method of measuring naturalness can be complementary to CNN. Despite the importance of measuring the naturalness of images, few alternative methods exist. Although manyprobabilistic image models have been proposed, most are only applicable to small images, they are generally not preferred for large images."}, {"heading": "2. Related work", "text": "This section presents a brief overview of existing approaches related to the modeling of the naturalness of images. Additionally, we describe the reconstruction of images based on characteristics and the prediction of eye fixation."}, {"heading": "2.1. Image modeling", "text": "Many probabilistic image models have been proposed [1, 15, 19, 29, 33, 45, 15], but most are applicable only to small and fixed image fields of simple content. To solve this question, Gregor et al. [18] used attention mechanisms and generated very realistic images. Denton et al. [9] applied hierarchically generative adversarial networks [15]. Theis and Bethge [53] proposed a scalable image model using multidimensional LSTMs [17] that predict pixel values of certain locations from previous pixels. We also use RNN for predictions like theirs. However, to gather more information at a higher level, we train RNN on CNN representations, not on raw pixels. Several vision papers explicitly use LM. Wu et al. [58] and Tirilly et al. [54] trained LMs on quantified local descriptors or learning them on a small scale, although their approach to measurement is similar to the 42-year visual."}, {"heading": "2.2. Image reconstruction from features", "text": "The reconstruction of an image from its representation is an important task in understanding the characteristics of representation. Many works have addressed this problem for craft representations [8, 28, 38, 56, 57] and deep representations [11, 38, 47, 59].Mahendran and Vedaldi [38] showed that an image can be reconstructed by gradient descent if the representation is extracted by differentiable functions. They also demonstrated that a \"natural image prediction\" is necessary to reconstruct interpretable images. They regulated reconstructed images to eliminate spikes in raw pixels and to be within the natural RGB range. Simonyan et al. [47] took a similar approach and used the L2 regulation on images. Dosovitskiy and Brox [11] inverted CNN features by directly learning a CNN that translates features into images. They demonstrated that colors and rough compositions of the original image can be reconstructed."}, {"heading": "2.3. Eye fixation prediction", "text": "Modeling visual attention is essential for the efficient processing of massive real-world data. In particular, a task to predict human fixation points has been extensively studied [3]. Bruce and Tsotsos [5] demonstrated that eye fixation points can be predicted using Shannon's \"self-information,\" an information-theoretical view that has been adopted for many research efforts [3]. Our method also uses a form of self-discovery. Many newer methods are based on supervised training on a fixation dataset [25, 27, 31, 32, 36, 55]. Our model is also a portable flexible model. However, since it is trained in an unattended manner, it requires images of the target domain, but no data for eye fixation. Since creating a dataset is a tedious task, it is a favorable property for practical applications."}, {"heading": "3. Visual language modeling", "text": "As shown in Figure 1, we measure the naturalness of an image using RNNLM and CNN. The pipeline of our method and its sections are explained below. (Section 3.2.) 3. Run RNNLM forward and backward along the x-axis and y-axis to obtain prediction maps and predictive error maps. (Section 3.1.2, 3.3.) 4. Enumerate them and output them as an unnatural value. (Section 3.3.) RNNLM must be trained in advance on natural images. The training method is described in Section 3.4.We apply our method of reconstructing features and eye fixation predictions. We describe the details of two applications in Section 3.5 and Section 3.6."}, {"heading": "3.1. CNN and RNN", "text": "First, we introduce CNN and RNN briefly and describe their configuration in this work."}, {"heading": "3.1.1 CNN", "text": "CNN is a state-of-the-art neural network in image classification [30, 48]. To extract image representations, CNN hierarchically applies 2D folding, non-linear activation and downsampling. The weights of folding cores are learned from data to minimize classification errors. Mid-level CNNs trained on a large generic image classification dataset have been proven to function as a powerful generic image trait [10, 48] and have become de facto the standard image trait in recent years. We use the outputs immediately after the folding layers of AlexNet [30] and VGGNet [48] to extract mid-level representations. Table 1 shows the layer name in the lectured caffe model [24] and the output size of its folding layers."}, {"heading": "3.1.2 RNN", "text": "RNN is a neural network used to predict a sequence. For each time step t, the hidden unit ht receives information from the input xt and the previous hidden unit ht \u2212 1. Then ht passes information to the output yt. Since ht and ht \u2212 1 are connected, yt depends on x1, x2,..., xt. In fact, RNN can make predictions in the context of infinite length. The most basic type of recursive layer is formalized as following. ht = tanh (Wxxt + Whht \u2212 1 + b). (1) However, it cannot learn long-term dependencies because it disappears in the process of flowing through many hidden connections. To overcome this problem, a variant of the recursive layer called Long-Short Term Memory (LSTM) has been proposed [20]. We used a recursive layer LSTM \u2212 bxt as a prediction of below.it =."}, {"heading": "3.2. Preprocessing", "text": "For example, the representation according to the convector1 layer of AlexNet includes 55 x 55 vectors with 96 dimensions. We normalize each dimension of such vectors to have zero and unit variance. After normalization, we apply Principal Component Analysis (PCA) to orthogonize the dimensions and reduce them by half. Normalization and PCA parameters are learned from the training set of the ILSVRC 2012 image classification data set [44]."}, {"heading": "3.3. Combination of CNN and RNNLM", "text": "The LMs divide the probability p (s) into p (s1) p (s2 | s1) p (s3 | s2)... p (s3 | s2)... p (sT | s1, s2,..., sT \u2212 1). An intuitive interpretation of this is that the probability is determined by how the next vector from past vectors is predictable. Frequent LMs deal with the first part of a hot vector representing a word and assume that the ambiguous distribution on p (st \u2212 s1, s1). In contrast, we use dense real value vector st and assume that the Gaussian distribution on p (st | s1, st \u2212 1). Specifically, we assume that the mean value on Wul is determined."}, {"heading": "3.4. Training of RNNLM", "text": "To calculate naturalness, we need to train RNNLM in advance by reducing many natural images to a minimum. We use the ILSVRC 2012 image classification dataset [44] for training. We train RNNLM at medium levels for AlexNet [30] and VGGNet [48]. For AlexNet, the learning rate and dynamics of the SGD are set to 10 and 0.9, respectively. Minibatch size is set to 16. For VGGNet, the learning rate and dynamics of the SGD are set to 20 and 0.9, respectively. Minibatch size is set to a factor of 0.1 per 5000 iterations for both networks. We stop learning after 20,000 iterations."}, {"heading": "3.5. Image reconstruction from features", "text": "Mahendran and Vedaldi [38] showed that image characteristics can be inverted into the original image by gradient descent (GD) if the function of feature extraction includes differentiable elements. Their key technique is to introduce the \"natural image before\" R (i) into its objective function. We call the original image i and the function of feature extraction \u03c6 (i). Then, they used the weight of the regularizer to reconstruct the reconstructed image i-isi = argmin i (i) \u2212?????????????????? (12) They used R-i, which keeps pixel values in the natural range and punishes strong intensity changes in adjacent pixels. Instead, we set R-i = ui?. Since ui-i is differentiable, objective function can be minimized by GD."}, {"heading": "3.6. Eye fixation prediction", "text": "It has been suggested that people look at places where Shannon's \"self-information\" is high [5]. Since self-information is identical with the negative logarithm of probability, unnatural map ul can be considered a kind of information map that predicts prominent places. We use an unnatural map ul as an emphasis map. Additionally, we apply Gaussian blur of a size \u03c3 to ul according to common practice [32, 36, 60]. Before blurring, we take the root of ul to prevent excessive expansion of peak values. An example of unnatural map or highlighting map ul is shown in Figure 2."}, {"heading": "4. Experiments", "text": "In this section we present the evaluation of the effectiveness of our proposed method by applying it to two tasks: image reconstruction from characteristics and prediction of eye fixation."}, {"heading": "4.1. Image reconstruction from features", "text": "We then combine our method with the work of Dosovitskiy and Brox [11] and present further improved results. Finally, we examine the role of each layer by imposing the regulator on the target layer. Our model has the following hyperparameters: the amount of layers l used for intermediate level representations, the weight of the unnatural layer of AlexNet, the weight of the first hundred images in the validation set of the classification dataset ILSVRC 2012 [44]. Our model has the following hyperparameters: the amount of layers l used for intermediate level representations, the weight of the unnatural map of Higl, the weight of the Higgs regulation dataset and the learning rate and dynamics of GD. In this section, unless otherwise specified, we present the values 1, 2, convective, 4, convective, convective values of G4 = 4, convective, convective values of G5 = 10, and learning rate and dynamics of GD = 10 \u2212."}, {"heading": "4.1.1 Evaluation method", "text": "The quantitative assessment of whether or not a reconstructed image is similar to the original image is not easy. Previous reports on the lithography [8, 57] do not provide quantitative analysis. Since the use of some kind of image characteristic can cause an unfair comparison, a square error [11, 28, 38], or a correlation coefficient [56] between reconstructed images and original images, correlation coefficients [56] have been used to date. Vondrick et al. [56] evaluated reconstructed images by asking people to classify them, and reported that the correlation coefficient did not always agree with the judgments of people. Therefore, in this work we determine the similarity of images by interviewing people. We provide them with the original image and corresponding reconstructed images, and then select the image from reconstructed images that is most similar to the original image. We interviewed one hundred people on CrowdFlower1."}, {"heading": "4.1.2 Results of reconstruction", "text": "Figure 3 shows original images and reconstructed images using our method and two existing methods [11, 38]. The results of our method have clear edges and not the results of Mahendran and Vedaldi [38], because their regulators prevent large intensity changes in adjacent pixels.1http: / / www.crowdflower.com / The method presented by Dosovitskiy and Brox [11] reconstructs general shapes and colors well, although the details are lost because their method produces an average of possible solutions.1Our results seem to be most similar to the original images. They are clear and understandable, which helps us interpret the properties of the image. Table 2 presents results of quantitative evaluation by crowdsourcing. Of 100 images, 94 of our images are the most similar images to the original images. Our results received 67.22% of the total votes of 100 people, clearly indicating the superiority of our methods. Figure 4 shows six cases where our results were subordinated to natural or other methods."}, {"heading": "4.1.3 Better initialization", "text": "It is known that the initial solution of the GD strongly influences the result, especially in neural networks [13, 50]. Indeed, the key to breakthroughs in deep networks is the intelligent initialization of weights [19]. Since the results of the method of Dosovitskiy and Brox can be interpreted as an average of possible solutions, they can be good starting solutions. Figure 5 and the last line of Figure 4 show reconstructed images initialized with the results of the method presented by Dosovitskiy and Brox. These results have been considerably improved compared to previous results. In some images, the absolute positions and colors of objects are corrected, suggesting that the limitations of our method are largely due to the initialization strategy and that they can be compensated by the method presented by Dosovitskiy and Brox."}, {"heading": "4.1.4 Analysis of layers", "text": "Figure 6 shows images that have been reconstructed using our method, in which case the regulator is imposed on a particular layer or raw image pixels. Results that have been regulated on raw pixels that are perfectly understandable show how important it is to model the naturalness of high-level characteristics to produce realistic images, not raw image pixels. Results that are regulated on convect2 or convect3 are less clear than the result on convect1, which implies that the information contained in lower layers affects the naturalness of images. Higher layers can regulate more abstract information, but they alone are not sufficient to produce images."}, {"heading": "4.2. Eye fixation prediction", "text": "In this section we evaluate our method for predicting eye fixation proposed in Section 3.6. We describe details of the data sets and evaluation metrics. Then we present the results. Our model has two hyperparameters: the amount of layers l used for intermediate level imaging and the core value of Gaussian blur \u03c3. We use a folding layer of VGGNet [48] as l and set \u03c3 to 0.030."}, {"heading": "4.2.1 Dataset", "text": "Many datasets are used to predict eye fixation, and we evaluate our method using two popular datasets, called MIT1003 [27] and Toronto [5]. In addition, we evaluate ours based on the MIT Saliency Benchmark [6, 26], which consists of two additional datasets: MIT300 and CAT2000. The MIT Saliency Benchmark is an online benchmarking service. The evaluation is commissioned. MIT1003 MIT1003 dataset [27] includes 1003 images of natural interior and exterior scenes and corresponding maps for eye fixation. It includes 779 landscape images and 228 portrait images. Toronto [5] includes 200 images of outdoor and interior shots and corresponding maps for eye fixation. MIT300 MIT300 [6, 26] includes 300 images of natural interior and exterior scenes and corresponding maps for eye fixation. Corresponding maps for fixation are not provided, as the protocol for capturing these datasets is the same as MIT03, MIT106, MIT1026, MIT03, and MIT03."}, {"heading": "4.2.2 Evaluation metrics", "text": "The task of eye fixation can be interpreted as a detection task to identify the eye's fixation point from an image. Therefore, an area below the curve is often used to evaluate the ROC curve [5]. However, since people tend to look around in the center of the image, this measurement gives a high value to centred highlighting points. To solve this problem, it has been suggested to mix the AUC values [52, 61]. The AUC does not calculate this measurement uniformly on all pixels, but on centred fixation points of other images. The mixed AUC value of centred Gaussian is about 0.5. We use this measurement for evaluation."}, {"heading": "4.2.3 Results of benchmark dataset", "text": "Figure 7 shows the mixed AUC value obtained with our method on the data sets MIT1003 and Toronto. Various l and \u03c3 are being tested. The best setting is l = conven5 4, \u03c3 = 0.030 for MIT1003 and l = conven5 1, \u03c3 = 0.010 for Toronto. This value is not published online but is included in their papers.Table 3 presents our results and existing results on the data sets MIT1003 and Toronto. We compare ours with Mr-CNN [36], AWS [12], BMS [60], CA [14], eDN [55], HFT [35], ICL [22], IS [21], JUDD [27], LG [2] and QDCT [46]. We have reached the state of the art on these well-studied data sets. Table 4 presents our results on MIT300 and CAT2000. We compare our results with SALICON [25], Deep Fix [31], Deconvalence [12], SALWCA], SALWAP = 12."}, {"heading": "4.2.4 Discussion", "text": "It is noteworthy that higher layers provide better results, as shown in Figure 7. These results suggest that the fixation points of the eyes are determined solely on the basis of higher signals. Table 5 presents our score and the average of results available on the scoreboard of each CAT2000 dataset. Compared to other methods, ours are better in the areas of social, caricature and affective. Presumably, this is true because the images in these categories contain more high-level content such as faces or pedestrians. Ours are worse in terms of patterns, low resolution and fractal, because our images are trained on natural images. Images in these categories do not seem to be natural. Prediction accuracy can be improved by training RNLM on images in these categories."}, {"heading": "5. Conclusion", "text": "In this work, which is based on the assumption that naturalness can be measured by the predictability of high-level images during eye movement, we proposed a new method of measuring the naturalness of an image by building a variant of RNLMs on the CNN features. We used it as a regularization tool in the reconstruction of images from image characteristics. The results of the experiments show that this regularization mechanism contributes to producing more viable images than existing approaches. We found that the naturalness of lower layers is important to produce natural images. In addition, we rated \"unnaturality maps\" of images as saliency maps. This was motivated by the assumption that the highlighting of images is based on the self-discovery of each location. We showed that the proposed \"unnaturality map\" reaches the state of the art."}], "references": [], "referenceMentions": [], "year": 2015, "abstractText": "Measuring the naturalness of images is important to generate realistic images or to detect unnatural regions in images. Additionally, a method to measure naturalness can be complementary to Convolutional Neural Network (CNN) based features, which are known to be insensitive to the naturalness of images. However, most probabilistic image models have insufficient capability of modeling the complex and abstract naturalness that we feel because they are built directly on raw image pixels. In this work, we assume that naturalness can be measured by the predictability on high-level features during eye movement. Based on this assumption, we propose a novel method to evaluate the naturalness by building a variant of Recurrent Neural Network Language Models on pre-trained CNN representations. Our method is applied to two tasks, demonstrating that 1) using our method as a regularizer enables us to generate more understandable images from image features than existing approaches, and 2) unnaturalness maps produced by our method achieve state-of-the-art eye fixation prediction performance on two well-studied datasets.", "creator": "LaTeX with hyperref package"}}}