{"id": "1511.02909", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "9-Nov-2015", "title": "Efficient Construction of Local Parametric Reduced Order Models Using Machine Learning Techniques", "abstract": "Reduced order models are computationally inexpensive approximations that capture the important dynamical characteristics of large, high-fidelity computer models of physical systems. This paper applies machine learning techniques to improve the design of parametric reduced order models. Specifically, machine learning is used to develop feasible regions in the parameter space where the admissible target accuracy is achieved with a predefined reduced order basis, to construct parametric maps, to chose the best two already existing bases for a new parameter configuration from accuracy point of view and to pre-select the optimal dimension of the reduced basis such as to meet the desired accuracy. By combining available information using bases concatenation and interpolation as well as high-fidelity solutions interpolation we are able to build accurate reduced order models associated with new parameter settings. Promising numerical results with a viscous Burgers model illustrate the potential of machine learning approaches to help design better reduced order models.", "histories": [["v1", "Mon, 9 Nov 2015 22:12:14 GMT  (581kb,D)", "http://arxiv.org/abs/1511.02909v1", "28 pages, 15 figures, 6 tables"]], "COMMENTS": "28 pages, 15 figures, 6 tables", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["azam moosavi", "razvan stefanescu", "adrian sandu"], "accepted": false, "id": "1511.02909"}, "pdf": {"name": "1511.02909.pdf", "metadata": {"source": "CRF", "title": null, "authors": ["Azam Moosavi", "Razvan Stefanescu", "Adrian Sandu"], "emails": ["rstefan@ncsu.edu"], "sections": [{"heading": null, "text": "Keywords reduced job models, high-fidelity models, data adaptation, machine learning, workable region of parameters, local reduced job models."}, {"heading": "1 Introduction", "text": "Many physical phenomena are mathematically described by partial differential equations (PDE) and, after applying appropriate discretization schemes, are simulated on a computer. PDE-based models often require calibration and parameter adjustment to provide realistic simulation results. Recent developments in uncertainty quantification [52, 85, 18] provide the necessary tools to validate such models, even in the context of variability and lack of knowledge about the input parameters. While uncertainty factors such as Markov methods [15, 17] can be measured the effects of uncertain parameters on some quantities of interest, they often become unfeasible due to the large number of model realizations. Similar difficulties arise when Bayesian inference problems are solved, as sampling problems are requested from the posterior distribution."}, {"heading": "2 Parametrized Reduced Order Modeling", "text": "In this paper we look at discrete inner products (Euclidean Dot products), although continuous products can also be used."}, {"heading": "3 Problem Description and Solution Methodology", "text": "This paper deals with the following problems in the construction of models of reduced order: the design of the parametric map, the selection of the two best already existing bases for a new parameter configuration from the point of view of accuracy, and the selection of the optimal dimension of the reduced basis. We will formulate them in detail below."}, {"heading": "3.1 Designing the parametric map", "text": "Problem 1 (Efficient Approximate ROMs): For any parameter configuration, we can construct a reduced job model (2) that provides an accurate and efficient approximation of the high-precision solution (1): The snapshots used to generate the base U can be obtained with any parametric configuration that belongs to P-1. A simple solution is to solve the high-precision model for the specific configuration U-0 and then create the corresponding reduced job model. However, this approach is computationally expensive.Our methodology suggests choosing a small, calculable subset U-1."}, {"heading": "3.2 Selecting the best two already existing bases for a new parameter configuration", "text": "This approach is inspired by the construction of local reduced order models in which the time domain is divided into several regions (70, 66), thus keeping the reduced base dimension small so that fast online simulations are possible. I's cardinality is inversely proportional to the prescribed accuracy threshold. As the desired error threshold is lowered, the map changes, since the radii rj usually become smaller, and more spheres are needed to cover the level of the parametric domain, i.e. M is increased. The construction of the parametric map (6) using local reduced order models requires the following components: 1. The ability to study the environment of microj \"P\" and efficiently estimate the level of the defective domain = max i = 1,.., Nt \"x\" (\u00b50, ti) \u2212 U\u00b5j x \"(0, ti) \u2212 P.\""}, {"heading": "3.2 Selecting the best two already existing bases for a new parameter configuration", "text": "Since the error of the reduced job solution at a new parameter position \u00b50 with the reduction of the distance d (\u00b50, \u00b5j) is not necessarily smaller, the following more general problem arises: Problem 2 (selection of the best fundamentals). For a new parameter configuration, the \u00b50 find the best available fundamentals (among the existing ones) that offer the most accurate solution of the reduced order model. The ability of the already proposed probability models can be used to estimate the error \u03b5 = x (\u00b50, ti) \u2212 U\u00b5j x (\u00b50, \u00b5j, ti) 2, i = 1,.., Nt, (9) for all available U\u00b5j, j = 1, 2,.. in the database, and the fundamentals that lead to the smallest estimated errors are selected. This approach is discussed in Section 6.3."}, {"heading": "3.3 Selecting the dimension of the reduced basis", "text": "Problem 3 (Optimal Base Dimension): Find the optimum dimension of the base parameter U\u00b5 for the parametric configuration \u00b5 so that the error is lower than the prescribed threshold \u0435x (\u00b5, ti) \u2212 U\u00b5 x (\u00b5, \u00b5, ti) \u2022 2 \u2264 \u03b5, i = 1,.., Nt. (10) By optimum we mean the smallest dimension that allows the reduced job model to meet the error limitation (10). The base dimension is one of the most important features of a reduced job model. Reduced diversity directly affects both the online complexity of the reduced job model and its accuracy [51, 39, 31]. Increasing the size of the base reduces the projection error and the accuracy of the reduced job model. Consequently, the spectrum of this mapping matrix provides guidance regarding the choice of the reduced base size when some of the reduced job model's prescribed errors are desired, but the accuracy also depends on this integration."}, {"heading": "4 Supervised Machine Learning Techniques", "text": "Do we see a random vector in Neuronale and gauze???????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????"}, {"heading": "4.1 Gaussian process kernel method", "text": "A Gaussian process is a collection of random variables, each finite number having a common Gaussian distribution [71]. A Gaussian process is fully described by its mean and covariance functions \u03c6 (z) \u0445 gp (m (z), k (zi, zj)), (12), where [71] m (z) = E [\u03c6 (z), k (zi, zj) = E [(zi) \u2212 m (zi))))) (\u03c6 (zj) \u2212 m (zj) (zj))]]]. In this work, we use the frequently used square-exponential covariance core Gaussian with [71]: k (zi, zj) = 2 \u03c6 exp (\u2212 (zi \u2212 zj) 22 '2) + \u0441i, j, (13), where zi and zj are the pairs of data points in formation or test samples."}, {"heading": "4.2 Artificial Neural networks", "text": "After viewing the data, the predictive distribution represents the back of the head [11] and is given by: p (y, z, y, z) \u0445 N (k (z, z) (k (z, z) + \u03c32nI) \u2212 1 y, k (z, z) \u2212 k (z, z) (k (z, z) + \u03c32nI) \u2212 1 k (z, z))). (17) The prediction of the Gaussian process depends on the choice of mean and covariance functions and their hyperparameters. Hyperparameters can be derived from the data by minimizing the marginal negative log probability."}, {"heading": "4.2 Artificial Neural networks", "text": "The study of artificial neural networks (ANNs) begins in the 1910s to indicate the biological structure of the human brain. \u00b7 Pioneered by Rosenblatt, who proposed a three-layer network structure, the Perceptron [37]. ANNs recognize the data pattern by discovering the input-output relationships. Applications include the approximation of functions, regression analysis, time series prediction, pattern recognition, and speech synthesis and recognition [46, 7].The architecture of ANNs is schematically represented in Figure 1. ANNs consist of neurons and connections between neurons (weights).The neurons are organized in layers where at least three layers of neurons (an input layer, a hidden layer, and an output layer) are required to build a neural network.The input layer distributes the input signals to x, z2, zn, and zn to the hidden layer."}, {"heading": "5 Combining Available Information for Accurate ROMs at New Parametric Configurations", "text": "The POD method generates an orthogonal base, which roughly covers the state solution space of the model for a particular parameter configuration. To move away from the original parametric configuration, the construction of new bases is necessary, since the original reduced job model is no longer accurate. However, if states continuously depend on parameters, the POD base constructed for a parameter configuration can approximately cover the solution space for different parametric settings in local environments. In the literature, several methods have been proposed to combine the available information to generate more precise reduced job models for new parameter configurations. One is the interpolation of the available reduced job bases U\u00b5j, which were calculated for the parametric configuration."}, {"heading": "5.1 Basis interpolation", "text": "Configurations \u00b5j, j = 1,.., M. The dependence of the bases on the parameters was modeled with different linear and nonlinear spatially dependent interpolants. Here, we compare the performance of different strategies, which include Lagrange interpolation of bases in the matrix space and in the tangent space of the Grassmann manifold. Furthermore, we propose to concatenate the available reduced bases, followed by an orthogonization process, and to interpolate the solutions of the high-fidelity model in order to derive the reduced order basis for a new parameter configuration."}, {"heading": "5.1 Basis interpolation", "text": "A continuous and linear dependence with respect to the parametric space is shown, and if M discrete bases U\u00b5j = U (\u00b5j) already exist for different parametric configurations \u00b5j, j = 1, 2,.., M, then a base corresponding to the new configuration # 0 can be achieved using the interpolation formula U\u00b50 = M [M] j = 1 U\u00b5jLj (\u00b5) = 1, 6 = j \u00b5 \u2212 \u00b5j \u2212 \u00b5i the new configuration # 0 \u2212 0 -0. Disadvantages of this approach include the orthogonisation-related requirement for the resulting interpolated base vectors and the lack of linear variations in the angles between the subspace planes [54] provided by the reduced bases U\u00b5j. Differential geometry results can be applied to mitigate these deficits."}, {"heading": "5.2 Basis concatenation", "text": "The idea of basic concatenation was introduced in [93] and arose from the idea of a global base [89, 80]. In global strategy, existing high-fidelity snapshots that correspond to different parameter configurations are collected in a single snapshot matrix and then a matrix factorization is performed to extract the most energetic singular vectors. On this global basis, reduced ordering models for parameter values that were not available in the first snapshots are then created."}, {"heading": "5.3 Interpolation of high-fidelity model solutions", "text": "Assuming that x1, x2, x2, x2, x2, x2, x2, x2, x2, x2, x2, x2, x2, x2, x2, x2, x2, x2, x2, x2, x2, x2, x2, x2, x2, x2, x2, x2, x2, x2, x2, x2, x2, x2, x2, x2, x2, x2, x2, x2, x2, x2, x2, x2, x2, x2, x2, x2, x2, x2, x2, x2, x2, x2, x2, x2, x2, x2, x2, x2, 4, 4, 4, 4, 4, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5"}, {"heading": "5.3 Interpolation of high-fidelity model solutions", "text": "The method discussed here assumes that the model solution is continuously dependent on the parameter settings. Therefore, it is obvious to consider constructing the basis for a new parameter configuration by interpolating the existing high-fidelity model solutions associated with various parameter settings, and then performing SVD factoring of the interpolated results. (For example, the Lagrange solution is interpolated by interpolating the interpolated solution. (18) A new basis is constructed from the interpolated model solution (26), where x\u00b5j-RNstate \u00b7 Nt is the model solution corresponding to the parameter \u00b5j and the interpolation polynomials are defined in (18). A new basis is constructed from the interpolated model solution (26) for the new parametric configuration."}, {"heading": "6 Numerical Experiments", "text": "We illustrate the application of the proposed machine learning methods to the construction of error models for the solutions of reduced order models for a one-dimensional Burgers model and their subsequent use. The model proposed here is characterized by two scalar parameters, but the proposed parametric map is only intended to cover the variation of the viscosity coefficient of space. For each of the problems presented in Section 3 (creation of the parametric map, selection of the best two already existing foundations for a new parameter configuration from the point of view of accuracy and selection of the dimension of the reduced basis), we present in detail the proposed solution approaches and the corresponding numerical results. To assess the performance of probability models, we use various cross-validation tests. The dimensions of training and test data sets are selected empirically based on the number of samples. In the case of artificial neural network models, the number of hidden layers and neurons in each type of hidden problems varies for each layer to be examined."}, {"heading": "6.1 One-dimensional Burgers\u2019 equation", "text": "Burger's equation is an important partial differential equation from fluid mechanics [14]. The evolution of the velocity u of a liquid develops according to \u2202 u \u2202 t + \u03bdu \u2202 x = \u00b5 \u2202 2u \u2202 x2, x-0 [0, L], t-0 (0, tf], (31) with tf = 1 and L = 1. Here is the viscosity coefficient. The parameter \u03bd has no physical meaning and is used to control the nonlinear effect of the advection date. The model has homogeneous dirichlet boundary conditions u (0, t) = u (L, t) = 0, t-0 (0, tf]. A smooth initial condition is used, described by a seventh degree polynomial and represented in Figure 2."}, {"heading": "6.2 Designing the parametric map", "text": "The discretization uses a spatial grid of Ns = 201 equidistant points on [0, L], with \u2206 x = L / (Ns \u2212 1). A uniform temporal grid with Nt = 301 points covers the interval [0, tf], with \u2206 t = tf / (Nt \u2212 1). The discrete velocity vector is u (tj) \u2248 [u (xi, tj)] i = 1,2,.., Nstate RNstate, N = j = 1, 2,.. Nt, where Nstate = Ns \u2212 2 (the known boundaries are removed).The semi-discrete version of the model (31) is u \u2032 = \u2212 \u03bdu Axu + \u00b5Axxu, (32), where u \u2032 is the time derivative of u, andAx, Axx-RNstate \u00b7 Nstate \u00b7 Nstate are the central differential quantities of first order and second order."}, {"heading": "6.2 Designing the parametric map", "text": "We try to create a parameter map for the 1D Burgers model for the viscosity domain, which consists of the interval [0.01, 1]. The non-physical parameter \u03bd is set to 1. As discussed in Section 3, we proceed in the following steps: First, we construct probability models to approximate the error of a reduced order solution. Second, we identify \"\u00b5-practicable\" intervals [d ', dr] in the parameter space, so that the local reduced order model, which depends only on the high accuracy curve at \u00b5, is accurate within the prescribed threshold for each \u00b50 solution. Finally, a greedy algorithm generates the parametric map by covering the parameter space with a union of \u00b5i-practicable intervals. [0.01, 1] \u0445M i = 1 [di', d i r], (33) where each \u00b5i-practicable interval is characterized by an error threshold (from one to another)."}, {"heading": "6.2.1 Error estimation of ROM solutions", "text": "We first construct probabilistic models \u03c6: z \u2192 \u03b5, (34), in which the input characteristics z include a new viscosity parameter value \u00b50, a parameter value \u00b5, which is linked to the complete model run that generated the base U\u00b5, and the dimension of the reduced manifold. Goal \u03b5 is the estimated error of the solution of the reduced order model at \u00b50 on the basis U\u00b5 and the corresponding reduced operators, which are calculated using the Frobenius standard \u03b5 = \u0445 [x (\u00b50, ti)] i = 1,.., Nt \u2212 U\u00b5 [x (\u00b50, \u00b5, ti)] i = 1,.., Nt \u0445 F. (35)."}, {"heading": "6.2 Designing the parametric map", "text": "The training dataset includes equally distributed values of \u00b5 and \u00b50 over the entire interval, \u00b5 (0.1, 0.2,.., 0.9, 1) and \u00b50 (0.01, 0.02,.., 0.99, 1). In addition, reduced baseline measurements that include the interval (4, 5,.., 14, 15] and the reduced system error are included. The entire training dataset contains nearly 12,000 samples, and a high-precision model solution is calculated for each sample. Figure 3 (b) shows isocontours of the reduced order model errors for viscosity parameter values \u00b50 and various POD base dimensions. The design of the reduced order models is based on the high fidelity curve for \u00b5 = 0.8. Since target values vary widely (from 300 to 10 \u2212 6), we consider the logarithms of the error protocol to reduce the predicted results."}, {"heading": "6.2 Designing the parametric map", "text": "Figure 6 illustrates the average error rate in predictions across five different ANN and GP configurations. In each configuration, the machine is trained to randomly split 80% of the data set and tested against the selected test data specified in Figure 6."}, {"heading": "6.2 Designing the parametric map", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "6.2.2 Construction of a \u00b5\u2212feasible interval", "text": "We saw in the previous subsection that probability models can accurately estimate the error \u03b5 (35) in the context of reduced order models, so we can use them to establish a range of viscosity parameters around \u00b5, so that the reduced job solutions fulfill a certain desired level of accuracy depending on U\u00b5. Specifically, starting from parameters \u00b5, a fixed POD base dimension, and a tolerance error protocol (\u03b5), we look for an interval [dl, dr] so that the estimated predictive force l-og (\u03b5) of the true error protocol (35) logs the required error requirement l-og (\u03b5) < log the requirement profile-og-og (\u03b5), 0-dr]. (37) Our proposed strategy uses a simple incremental approach by scanning the environment of \u00b5 to log the estimated errors og (\u03b5), take into account the estimated error (< r)."}, {"heading": "6.2 Designing the parametric map", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "6.2.3 The parametric map as a reunion of \u00b5\u2212feasible intervals", "text": "The reunion of the different mikBurgers model solutions with lower viscosity parameters is not possible. (i.e.) The reunion of the different mikBurgers model solutions with lower viscosity parameters is not possible. (i.e.) The reunion of the different mikBurgers model solutions with lower viscosity parameters (i.e.) is not possible. (i.e.) The reunion of the different mikBurgers model solutions with lower viscosity parameters (i.e.) is not possible. (i.e.) The reunion of the different mikBurgers model solutions with lower viscosity threshold (i.e.) is not possible. (i.e.) The reunion of the different mikBurgers model solutions with lower viscosity threshold (i.e.) is not possible."}, {"heading": "6.2 Designing the parametric map", "text": "For our experiments, we set A = 0.01, B = 1, \u03b5 = 1, e \u2212 2, \u0445 s = 5, e \u2212 3, r0 = 0.5, dim = 9, \u03b21 = 1.2, \u03b22 = 0.9 and \u03b23 = 1.4. We initiate the algorithm by setting \u00b50 = 0.87, and we get the first realizable interval [0.7700, 1.0500]. (Next, the algorithm selects \u00b51 = 0.73 with the corresponding range of [0.6700, 0.8250] using the same initial threshold level. Since we cover the parametric range from right to left, i.e. we select smaller and smaller parameters, the algorithm increases the current threshold. (Otherwise, the reduced order models would not meet the initial precision.) We continue this process until we reach the threshold of 6.25 with models of \u00b532 = 0.021 and the associated radius."}, {"heading": "6.3 Select the best already existing ROMs for a new parameter value \u00b50", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "6.3 Select the best already existing ROMs for a new parameter value \u00b50", "text": "An important and practical concern in the context of reduced order models is their lack of robustness in relation to parameter changes. Here, we propose to solve another practical problem, i.e. to provide a collection of reduced bases calculated at different locations in the parameter space. Find the one that proposes the most accurate reduced solution for a new viscosity parameter \u00b50. We will rely on similar probabilistic models built in subsection 6.2.1. The input characteristics for the GP and ANN are the new parameter \u00b50, a parameter \u00b5 whose corresponding path is used as a snapshot for generating base U\u00b5 and the dimension of reduced manifoldness. The target variable y-a-a-a-a is the estimated error protocol of the reduced order model when using base U\u00b5, a parameter \u00b5 whose corresponding path is used as a snapshot for generating base U\u00b5 and the dimension of reduced manifold."}, {"heading": "6.4 Combining Available Information for Accurate ROMs at New Parametric Configurations", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "6.4 Combining Available Information for Accurate ROMs at New Parametric Configurations", "text": "The experiments in subsection 6.3 revealed the potential of the probability models to select a hierarchy of reduced multiplicities that produces higher exact solutions. Figure 9 shows the accuracy of reduced order models for micro _ 0 = 0.35,.. Figure 9 shows that the most accurate reduced solutions for micro _ 0 = 0.35 are produced, constructing only 10 existing POD subspaces for parameters equally distributed along the interval [0.1, 1], \u00b5 = 0.2, 0.3 and 0.4. The numerical experiments described here focus on the construction of a POD base for micro _ 0 = 0.35 by combining the available data for micro _ 1 = 0.3 and mikro _ 2 = 0.4. The performances of the methods discussed (concatenating bases, lagrange interpolation of bases in the matrix space and in the tangential space of multiplicity) are."}, {"heading": "6.5 Optimal size of reduced order model", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "6.5 Optimal size of reduced order model", "text": "Here we propose two alternative approaches to select the reduced base size, which takes into account specified levels of accuracy in the reduced job model solutions. These techniques use the construction of probability models \u03c6: z \u2192 y-via neural network and Gaussian process, as in Section 4. The input characteristics z for this problem consist of the viscosity parameter \u00b5 [0.01, 1] and the protocol of the Frobenius standard of error between the high fidelity and reduced job models log (\u03b5) (35). The output sought is the dimension of the reduced manifold. For the training phase, the data set is generated using multiple runs of the 1D Burgers model with different viscosity parameters \u00b5, different base sizes d and the protocol of the Frobenius norms of discrepancies between the full and projected reduced job solution log."}, {"heading": "6.5 Optimal size of reduced order model", "text": "The results are obtained by conventional validation, using 80% of the sample size for training data and the remaining 20% for the test data. The formula used is described in Equation (11a).Figures 13 and 14 show the corresponding estimation errors on 100 and 1000 training samples for both ANN and GP models. These histograms can, as already mentioned, assess the validity of GP assumptions. The shape of the data distributions is closer to a Gaussian profile than in the case of the data distribution discussed in Section 6.2.1. The 5 results from the folds are averaged and displayed in Table 6. To assess the accuracy of the probability models, the data set is randomly divided into 5 equally large subsamples and 5 \u2212 fold cross-validation tests.The 5 results from the folds are averaged and they are displayed in Table 6. The neural network model estimates the size of the reduced Gaussian deficiency in comparison to the Gaussian process in 87% of the correct size of the selected POD cases."}, {"heading": "6.5 Optimal size of reduced order model", "text": "Intrinsic value estimation is the standard method for selecting the optimal reduced multiple dimension when seeking a prescribed level of accuracy of the reduced solution, whereby the desired accuracy \u03b5 is set to 10 \u2212 3. The discrepancies between the predicted and actual dimensions are shown in Figure 15. Predicted values are the averages across five different models, in which each model is trained by ANN and GP for random 80% division of the data set and tested on the firmly selected 20% test data. We note that the snapshot matrix spectrum underestimates the true size of the multiple as expected, as errors due to integration into the reduced space are not taken into account. Neural network predictions were extremely accurate for most samples, while the Gaussian process usually overestimated the reduced multiple dimensions."}, {"heading": "7 Conclusions", "text": "This paper demonstrates the value of machine learning approaches to guide the construction of parametric spatial distributions for the use of efficient and precise local order models. While current methods are defined in the sense of voronoi tessellation and are based on K-mean algorithms [5, 6, 47], our approach differs from the subregions of parametric space in the use of Gaussian processes and artificial neural networks for the errors of reduced order models and parametric domain collections. The machine learning models used differ from those proposed in them, such as reduced subspace dimensions, and are specifically designed for the precise prediction of reduced order models."}, {"heading": "Acknowledgements", "text": "This work was partially supported by the NSF CCF 1218454 Award and the Computational Science Laboratory at Virginia Tech."}], "references": [], "referenceMentions": [], "year": 2015, "abstractText": "Reduced order models are computationally inexpensive approximations that capture the important dynamical characteristics of large, high-fidelity computer models of physical systems. This paper applies machine learning techniques to improve the design of parametric reduced order models. Specifically, machine learning is used to develop feasible regions in the parameter space where the admissible target accuracy is achieved with a predefined reduced order basis, to construct parametric maps, to chose the best two already existing bases for a new parameter configuration from accuracy point of view and to pre-select the optimal dimension of the reduced basis such as to meet the desired accuracy. By combining available information using bases concatenation and interpolation as well as high-fidelity solutions interpolation we are able to build accurate reduced order models associated with new parameter settings. Promising numerical results with a viscous Burgers model illustrate the potential of machine learning approaches to help design better reduced order models. key words reduced order models, high-fidelity models, data fitting, machine learning, feasible region of parameters, local reduced order models.", "creator": "LaTeX with hyperref package"}}}