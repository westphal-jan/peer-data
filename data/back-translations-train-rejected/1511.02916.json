{"id": "1511.02916", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "9-Nov-2015", "title": "Spectral-Spatial Classification of Hyperspectral Image Using Autoencoders", "abstract": "Hyperspectral image (HSI) classification is a hot topic in the remote sensing community. This paper proposes a new framework of spectral-spatial feature extraction for HSI classification, in which for the first time the concept of deep learning is introduced. Specifically, the model of autoencoder is exploited in our framework to extract various kinds of features. First we verify the eligibility of autoencoder by following classical spectral information based classification and use autoencoders with different depth to classify hyperspectral image. Further in the proposed framework, we combine PCA on spectral dimension and autoencoder on the other two spatial dimensions to extract spectral-spatial information for classification. The experimental results show that this framework achieves the highest classification accuracy among all methods, and outperforms classical classifiers such as SVM and PCA-based SVM.", "histories": [["v1", "Mon, 9 Nov 2015 22:29:13 GMT  (718kb)", "http://arxiv.org/abs/1511.02916v1", "Accepted as a conference paper at ICICS 2013, an updated version. Codes published. 9 pages, 6 figures"]], "COMMENTS": "Accepted as a conference paper at ICICS 2013, an updated version. Codes published. 9 pages, 6 figures", "reviews": [], "SUBJECTS": "cs.CV cs.AI cs.LG", "authors": ["zhouhan lin", "yushi chen", "xing zhao", "gang wang"], "accepted": false, "id": "1511.02916"}, "pdf": {"name": "1511.02916.pdf", "metadata": {"source": "CRF", "title": "Spectral-Spatial Classification of Hyperspectral Image Using Autoencoders", "authors": ["Zhouhan Lin", "Yushi Chen", "Xing Zhao", "Gang Wang"], "emails": ["lin.zhouhan@gmail.com,", "chenyushi@hit.edu.cn,", "xintongzhaoxing@126.com", "wanggang@ntu.edu.sg"], "sections": [{"heading": null, "text": "This year, the time has come for an agreement to be reached, and it will only take a few days."}, {"heading": "A. Hyperspectral Image Classification: Description", "text": "The classification of hyperspectral images is slightly different from the traditional image classification, so we have to work out the task we face here. A typical hyperspectral scene spans several square kilometers of land and has hundreds of spectral channels instead of just 3 RGB channels, as shown in Fig. 1. As a result, the task of classifying hyperspectral images has unique characteristics. The most unique is that the task of hyperspectral image classification, rather than giving a caption to an entire image, is pixel-based: We assign a label for each pixel in the scene according to the spectral information provided by its hundreds of spectral channels."}, {"heading": "B. Autoencoders (AE) and Stacked autoencoders (SAE)", "text": "Recent years have shown that we are able to meet people's needs and that we are able to meet people's needs."}, {"heading": "A. Classifying with shallow spectral representation", "text": "In the second half of the second quarter there was an open fight between the two teams."}, {"heading": "B. Classifying with deep spectral representation", "text": "There are some motivations to extract more robust deep spectral representations. First, because the complex situation of lighting in the large scene, objects in the same class exhibit different spectral characters in different places. For example, a lawn that is exposed to direct sunlight shows different spectral characters from the same lawn, but obscured by the sun through a tall building. Also, the scattering of other peripheral ground objects tends the spectra of the lawn and also changes its characters. Other factors include rotations of the sensor, different atmospheric scattering states and so on. According to these factors, the probability distribution of a particular class is difficult to be one-sided hot and has deviations across multiple directions in scope. These complex variations of spectra make it hopeless to analyze pixel by pixel as they are influenced by their entangled pixels."}, {"heading": "C. Classifying with spectral-spatial representation", "text": "Unlike other HSI spectral information methods, which use only the 4 or 8 tangent neighbors and simple filtering, our deep frame takes into account all the pixels in a flat neighboring region and lets the autoencoders learn the representation of themselves. The overall picture of our proposed method is detailed in Fig 4. First, a 7x7 neighboring region of a particular pixel is extracted from the original image. Due to the hundreds of channels along the spectral dimension, the data on this initial layer always have tens of thousands of dimensions. Such a large neighboring region will result in too large an input dimension for the classifier and contain too much redundancy."}, {"heading": "A. Single-layer spectral representation", "text": "This part compares our proposed AE-SVM classification scheme with ordinary SVM and SVM, with PCA inserted as a feature extraction step (PCA-SVM).1) Reconstructed Spectrum Since the single-layer autoencoder is an important building block of other methods proposed in this essay, we first check how well the input is reconstructed through epochs before giving the resulting classification accuracy by analyzing a sample of different training epoches.2) Classification accuracy Fig. 6 shows the classification performance of 8 autoencoders with different sizes of hidden layers ranging from 20 to 140 on KSC data and 60 to 180 on Pavia. Although the hidden layer size, unlike the corresponding autoencoders, exceeds the performance of eight autoencoders with different sizes of hidden layers ranging from 20 to 140 on KSC data, and 60 to 180 on Pavia."}, {"heading": "B. Classifying with deep spectral representation", "text": "We have tried several stacked autoencoders with different depths and compared the results with other traditional methods. For KSC data, they have 176 spectral channels, and each hidden layer size is set to 100, plus a logistical regression layer over the stacked autoencoder. Thus, the neural networks are as 176-100... -100-13. For Pavia data set with 103 spectral channels and 9 classes, the performance is best when 140 is used as the hidden layer size. Neural networks are like 103-140-... -140-9. \"Hidden layer numbers\" in Table I correspond to the number of 100 or 140 layers in the deep neural network. Experiments show that depth helps with the decreasing classification error rate. For comparison, we perform two traditional methods, SVM and SVM with PCA, which are attached as feature extraction on the same data."}, {"heading": "C. Classifying with spectral-spatial representation", "text": "If we apply SVM directly to spectral spatial information, the error rate will be unacceptably high, as shown in Table II. Our methods confirm that this type of representation also provides abundant information for classification. We only use the previous 3 basic components of a 7x7 neighboring region to achieve an error rate as low as 4,000% on KSC data and 14.355% on Pavia data. [However, these traditional methods fail in their accuracy, but our proposed method succeeds in finding correct properties in the datasets and yields the highest accuracy. In this paper, we propose a hyperspectival image classification using both spectral and spatial information provided by stacked autocoders.Our experiments first confirm that autocode-extracted representations of the SVM class help to lower a classified error rate."}], "references": [{"title": "Introduction to hyperspectral imaging", "author": ["R.B. Smith"], "venue": "Aug 2006, available at http://www.microimages.com/getstart/hyprspec.htm.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2006}, {"title": "An active learning approach to hyperspectral data classification,", "author": ["S. Rajan", "J. Ghosh", "M.M. Crawford"], "venue": "IEEE Trans. Geosci. Remote Sens., vol. 46,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2008}, {"title": "A relative evaluation of multiclass image classification by support vector machines", "author": ["Foody", "Giles M.", "Ajay Mathur"], "venue": "IEEE  Trans.  Geosci.  Remote Sens., vol. 42, no. 6, pp. 1335-1343, 2004.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2004}, {"title": "Kernel-based methods for hyperspectral image classification", "author": ["G. Camps-Valls", "L. Bruzzone"], "venue": "IEEE Trans. Geosci. Remote Sens.,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2005}, {"title": "Hyperspectral Feature Extraction using Selective PCA based on Genetic Algorithm with Subgroups", "author": ["Liu Ying", "Gu Yanfeng", "Zhang Ye"], "venue": "Innovative Computing, Information and Control, First International Conference on, 2006.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2006}, {"title": "Classification of hyperspectral remote sensing images with support vector machines.", "author": ["Melgani", "Farid", "Lorenzo Bruzzone"], "venue": "IEEE Trans. Geosci. Remote Sens., vol. 42,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2004}, {"title": "Classification and Feature Extraction of Remote Sensing Images from Urban Areas based on Morphological Approaches", "author": ["J.A. Benediktsson et. al."], "venue": "IEEE Trans. Geosci. Remote Sens., vol. 41, no. 10, pp. 1940-1949, 2003.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 1940}, {"title": "A fast learning algorithm for deep belief nets", "author": ["G. Hinton", "S. Osindero", "Y Teh"], "venue": "Neural Computation, vol. 18, pp. 1527-1554, 2006.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2006}, {"title": "Learning in the deep structured conditional random fields", "author": ["D. Yu", "L. Deng", "S. Wang"], "venue": "Proc. NIPS Workshop, Dec. 2009.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2009}, {"title": "Deep belief networks for phone recognition,\u201dProc.NIPS", "author": ["A. Mohamed", "G. Dahl", "G. Hinton"], "venue": null, "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2009}, {"title": "Greedy layer-wise training of deep networks", "author": ["Y. Bengio", "P. Lamblin", "D. Popovici", "H. Larochelle"], "venue": "Neural Information Processing Systems 19 (NIPS\u201906), pp. 153-160, 2007.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2007}, {"title": "Stacked denoising autoencoders", "author": ["P. Vincent", "H. Larochelle", "I. Lajoie", "Y. Bengio", "P. Manzagol"], "venue": "J. Machine Learning Res.,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2010}, {"title": "A practical guide to training restricted Boltzmann machines", "author": ["E. Hinton G."], "venue": "Technical Report UTML TR2010-003, Department of Computer Science, University of Toronto, 2010.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2010}, {"title": "Backpropagation applied to handwritten zip code recognition", "author": ["Y. LeCun", "B. Boser", "et.al. Denker J.S."], "venue": "Neural Computation, vol. 1, no. 4, pp. 541-551, 1989.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 1989}, {"title": "Neocognitron: A self-organizing neural network model for a mechanism of pattern recognition unaffected by shift in position", "author": ["K Fukushima"], "venue": "Biological Cybernetics, vol. 36, pp. 193-202, 1980.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 1980}, {"title": "Representation learning: A review and new perspectives", "author": ["Y. Bengio", "A. Courville", "P. Vincent"], "venue": "arXiv:1206.5538, 2012.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2012}, {"title": "Theano: A CPU and GPU Math Expression Compiler", "author": ["J. Bergstra", "O. Breuleux", "et. al."], "venue": "Proc. of the Python for Scientific Computing Conference (SciPy), Austin, TX, June 30 - July 3, 2010.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2010}], "referenceMentions": [{"referenceID": 0, "context": "Hyperspectral imagery is becoming a valuable tool for monitoring the Earth\u2019s surface [1].", "startOffset": 85, "endOffset": 88}, {"referenceID": 1, "context": "If successfully exploited, the hyperspectral image can yield higher classification accuracies and more detailed class taxonomies [2].", "startOffset": 129, "endOffset": 132}, {"referenceID": 2, "context": "Traditional HSI classification methods uses spectral information only, and the classification algorithms typically include parallelepiped classification, k-nearest-neighbors, maximum likelihood, minimum distance and logistic regression [3].", "startOffset": 236, "endOffset": 239}, {"referenceID": 3, "context": "\u201d[4] To tackle this problem, scholars of the remote sensing community try to introduce variety of feature extraction methods like PCA, ICA, sequential forward floating search and wavelet analysis [5].", "startOffset": 1, "endOffset": 4}, {"referenceID": 4, "context": "\u201d[4] To tackle this problem, scholars of the remote sensing community try to introduce variety of feature extraction methods like PCA, ICA, sequential forward floating search and wavelet analysis [5].", "startOffset": 196, "endOffset": 199}, {"referenceID": 2, "context": "In most cases, SVM based methods can obtain better classification accuracy than other widely used pattern recognition techniques on HSI data [3].", "startOffset": 141, "endOffset": 144}, {"referenceID": 5, "context": "So after SVM is introduced in this field [6], feature extraction is", "startOffset": 41, "endOffset": 44}, {"referenceID": 6, "context": "Recently there are some methods proposed for incorporating spatial information, like mathematical morphology [7].", "startOffset": 109, "endOffset": 112}, {"referenceID": 7, "context": "On the other thread, recent advantages in training multilayer neural networks have refurbished best records in a wide variety of machine learning problems including classification or regression tasks that involve processing image [8], language [9] and speech [10].", "startOffset": 230, "endOffset": 233}, {"referenceID": 8, "context": "On the other thread, recent advantages in training multilayer neural networks have refurbished best records in a wide variety of machine learning problems including classification or regression tasks that involve processing image [8], language [9] and speech [10].", "startOffset": 244, "endOffset": 247}, {"referenceID": 9, "context": "On the other thread, recent advantages in training multilayer neural networks have refurbished best records in a wide variety of machine learning problems including classification or regression tasks that involve processing image [8], language [9] and speech [10].", "startOffset": 259, "endOffset": 263}, {"referenceID": 10, "context": "Typical deep neural network architectures include Deep Belief Networks [11], Deep Boltzmann Machines [12], Stacked Autoencoders [13] and Stacked Denoising Autoencoders [14].", "startOffset": 101, "endOffset": 105}, {"referenceID": 11, "context": "Typical deep neural network architectures include Deep Belief Networks [11], Deep Boltzmann Machines [12], Stacked Autoencoders [13] and Stacked Denoising Autoencoders [14].", "startOffset": 128, "endOffset": 132}, {"referenceID": 12, "context": "Typical deep neural network architectures include Deep Belief Networks [11], Deep Boltzmann Machines [12], Stacked Autoencoders [13] and Stacked Denoising Autoencoders [14].", "startOffset": 168, "endOffset": 172}, {"referenceID": 13, "context": "The layer-wise training framework has a bunch of alternatives like Restricted Boltzmann Machines [15], Pooling Units [16], Convolutional Neural Networks [17] and Autoencoders [13].", "startOffset": 97, "endOffset": 101}, {"referenceID": 14, "context": "The layer-wise training framework has a bunch of alternatives like Restricted Boltzmann Machines [15], Pooling Units [16], Convolutional Neural Networks [17] and Autoencoders [13].", "startOffset": 117, "endOffset": 121}, {"referenceID": 15, "context": "The layer-wise training framework has a bunch of alternatives like Restricted Boltzmann Machines [15], Pooling Units [16], Convolutional Neural Networks [17] and Autoencoders [13].", "startOffset": 153, "endOffset": 157}, {"referenceID": 11, "context": "The layer-wise training framework has a bunch of alternatives like Restricted Boltzmann Machines [15], Pooling Units [16], Convolutional Neural Networks [17] and Autoencoders [13].", "startOffset": 175, "endOffset": 179}, {"referenceID": 15, "context": "It is believed that deep architectures can potentially lead to progressively more abstract features at higher layers of representation, and more abstract features are generally invariant to most local changes of the input [17].", "startOffset": 222, "endOffset": 226}, {"referenceID": 0, "context": "All data are regularized on to an interval of [0, 1] before feeding into classifiers.", "startOffset": 46, "endOffset": 52}, {"referenceID": 16, "context": "The codes are implemented using Theano [18], a Python library for symbolic computation.", "startOffset": 39, "endOffset": 43}], "year": 2015, "abstractText": "Hyperspectral image (HSI) classification is a hot topic in the remote sensing community. This paper proposes a new framework of spectral-spatial feature extraction for HSI classification, in which for the first time the concept of deep learning is introduced. Specifically, the model of autoencoder is exploited in our framework to extract various kinds of features. First we verify the eligibility of autoencoder by following classical spectral information based classification and use autoencoders with different depth to classify hyperspectral image. Further in the proposed framework, we combine PCA on spectral dimension and autoencoder on the other two spatial dimensions to extract spectral-spatial information for classification. The experimental results show that this framework achieves the highest classification accuracy among all methods, and outperforms classical classifiers such as SVM and PCA-based SVM. Keywords-autoencoders; deep learning; hyperspectral; image classification; neural networks; stacked autoencoders", "creator": "Microsoft\u00ae Word 2013"}}}