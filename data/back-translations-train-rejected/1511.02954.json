{"id": "1511.02954", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "10-Nov-2015", "title": "Reducing the Training Time of Neural Networks by Partitioning", "abstract": "This paper presents a new method for pre-training neural networks that can decrease the total training time for a neural network while maintaining the final performance, which motivates its use on deep neural networks. By partitioning the training task in multiple training subtasks with sub-models, which can be performed independently and in parallel, it is shown that the size of the sub-models reduces almost quadratically with the number of subtasks created, quickly scaling down the sub-models used for the pre-training. The sub-models are then merged to provide a pre-trained initial set of weights for the original model. The proposed method is independent of the other aspects of the training, such as architecture of the neural network, training method, and objective, making it compatible with a wide range of existing approaches. The speedup without loss of performance is validated experimentally on MNIST and on CIFAR10 data sets, also showing that even performing the subtasks sequentially can decrease the training time. Moreover, we show that larger models may present higher speedups and conjecture about the benefits of the method in distributed learning systems.", "histories": [["v1", "Tue, 10 Nov 2015 01:20:51 GMT  (77kb)", "http://arxiv.org/abs/1511.02954v1", "Figure 2b has lower quality due to file size constraints"], ["v2", "Sun, 3 Jan 2016 17:18:06 GMT  (79kb)", "http://arxiv.org/abs/1511.02954v2", "Figure 2b has lower quality due to file size constraints"]], "COMMENTS": "Figure 2b has lower quality due to file size constraints", "reviews": [], "SUBJECTS": "cs.NE cs.LG", "authors": ["conrado s miranda", "fernando j von zuben"], "accepted": false, "id": "1511.02954"}, "pdf": {"name": "1511.02954.pdf", "metadata": {"source": "CRF", "title": null, "authors": [], "emails": ["contact@conradomiranda.com,", "vonzuben@dca.fee.unicamp.br"], "sections": [{"heading": null, "text": "ar Xiv: 151 1.02 954v 1 [cs.N E] 10 Nov 201 5"}, {"heading": "1 INTRODUCTION", "text": "This year, it has reached the point where it will be able to put itself at the top of the list, in the same way as it has done in recent years."}, {"heading": "2 PARTITIONING NEURAL NETWORKS", "text": "This section is divided into three parts. Section 2.1 further explains the problem to be solved and how the solution relates to existing methods in the literature. Section 2.2 describes the method itself, and Section 2.3 analyses the possible benefits that can be achieved by the method, some of which are confirmed in the experiments carried out."}, {"heading": "2.1 MOTIVATION", "text": "Large neural networks are able to perform better than smaller networks, but are much more expensive to learn and use; they require special training methods when they do not fit into a single unit of computation (Dean et al., 2012) and can be used after training to provide guidance on how to improve smaller networks (Hinton et al., 2015). Therefore, another common solution for improving the performance of smaller neural networks by ensembles (Hansen & Salamon, 1990), where predictions of multiple neural networks are combined to provide more accurate results, is still important to guide the improvement of smaller networks. It is important to stress that ensembles only work because the different models provide different predictions for the same data (Perrone & Cooper, 1992)."}, {"heading": "2.2 PARTITIONING METHOD", "text": "This year is the highest in the history of the country."}, {"heading": "2.3 ANALYSIS OF THE METHOD", "text": "From the description of the partitioning method, it is clear that each sub-model of the network has fewer parameters to train on. After the partition, there are three ways to change the number of parameters: 1) keep the same number as the original network, which happens with the output distortions; 2) reduce linearly what happens with all other biases and output weights, since the number of neurons is copied more linear and the number of inputs and outputs; and 3) reduce square what happens with the weights between the internal neurons. Since most of the parameters are concentrated in deep neural networks, we can expect an almost quadratic reduction in the parameters."}, {"heading": "3 EXPERIMENTAL RESULTS", "text": "In these experiments, a GeForce GTX 760 was used, running on neural networks small enough to fill the memory of the GPU, including parameters, intermediate values, and stacks of data, but large enough to prevent the entire layer-by-layer processing from being fully parallel, thereby avoiding overheads due to communication between nodes, which may be specific to the method of performing model parallelism, while the proposed method may have its improvements. We present two experiments to evaluate the method proposed in this paper: the first uses a small network to classify digits on MNIST (LeCun et al., 1998) and focuses on analyzing the impact of the number of partitions created on training time and performance; the second uses a larger network to classify the images on CIFAR10 (Krizhevsky, 2009) and takes into account different epochs for the time before training."}, {"heading": "3.1 MNIST", "text": "This year, it has reached the stage where it will be able to take the lead."}, {"heading": "3.2 CIFAR10", "text": "The neural networks and parameters used in these experiments are the same as those used by Krizhevsky et al. (2012) in the CUDA-ConvNet library, which consists of two revolutionary layers with ReLU activation, each characterized by normalization and pooling layers. Afterwards, two locally connected layers with ReLU activation and a fully connected layer complete the network. Details of the network and the parameters can be found in the source code of the library. The neural network has more than 2.1 million parameters, and we fix the number of partitions to 2 and vary the iteration in which the submodels are merged. As the datapath is presented in the original network normalization layers, it is not possible to apply the pre-education directly, as these layers require communication between the submodels."}, {"heading": "4 CONCLUSION", "text": "In this paper, we have introduced a method for pre-training a neural network by dividing it into smaller neural networks and training them on the original learning task. Submodel size is reduced almost quadratically with the number of sub-models generated, which reduces the number of computational units required during pre-training."}, {"heading": "ACKNOWLEDGMENTS", "text": "The authors thank CNPq for the financial support."}], "references": [{"title": "cuDNN: Efficient primitives for deep learning", "author": ["S. Chetlur", "C. Woolley", "P. Vandermersch", "J. Cohen", "J. Tran", "B. Catanzaro", "E. Shelhamer"], "venue": null, "citeRegEx": "Chetlur et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Chetlur et al\\.", "year": 2014}, {"title": "The loss surface of multilayer networks", "author": ["A. Choromanska", "M. Henaff", "M. Mathieu", "G.B. Arous", "Y. LeCun"], "venue": null, "citeRegEx": "Choromanska et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Choromanska et al\\.", "year": 2014}, {"title": "Deep learning with COTS HPC systems", "author": ["A. Coates", "B. Huval", "T. Wang", "D.J. Wu", "A.Y. Ng", "B. Catanzaro"], "venue": "In Proceedings of the 30th International Conference on Machine Learning,", "citeRegEx": "Coates et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Coates et al\\.", "year": 2013}, {"title": "Identifying and attacking the saddle point problem in high-dimensional non-convex optimization", "author": ["Y.N. Dauphin", "R. Pascanu", "C. Gulcehre", "K. Cho", "S. Ganguli", "Y. Bengio"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Dauphin et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Dauphin et al\\.", "year": 2014}, {"title": "Large scale distributed deep networks", "author": ["J. Dean", "G.S. Corrado", "R. Monga", "K. Chen", "M. Devin", "Q.V. Le", "M.Z. Mao", "M. Ranzato", "A. Senior", "P. Tucker", "K. Yang", "A.Y. Ng"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Dean et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Dean et al\\.", "year": 2012}, {"title": "Why does unsupervised pretraining help deep learning", "author": ["D. Erhan", "Y. Bengio", "A. Courville", "P. Manzagol", "P. Vincent", "S. Bengio"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "Erhan et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Erhan et al\\.", "year": 2010}, {"title": "Understanding the difficulty of training deep feedforward neural networks", "author": ["X. Glorot", "Y. Bengio"], "venue": "In International Conference on Artificial Intelligence and Statistics,", "citeRegEx": "Glorot and Bengio,? \\Q2010\\E", "shortCiteRegEx": "Glorot and Bengio", "year": 2010}, {"title": "Deep learning with limited numerical precision", "author": ["S. Gupta", "A. Agrawal", "K. Gopalakrishnan", "P. Narayanan"], "venue": null, "citeRegEx": "Gupta et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Gupta et al\\.", "year": 2015}, {"title": "Neural network ensembles", "author": ["L.K. Hansen", "P. Salamon"], "venue": "IEEE Transactions on Pattern Analysis & Machine Intelligence,", "citeRegEx": "Hansen and Salamon,? \\Q1990\\E", "shortCiteRegEx": "Hansen and Salamon", "year": 1990}, {"title": "Distilling the knowledge in a neural network", "author": ["G. Hinton", "O. Vinyals", "J. Dean"], "venue": null, "citeRegEx": "Hinton et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Hinton et al\\.", "year": 2015}, {"title": "Learning multiple layers of features from tiny images", "author": ["A. Krizhevsky"], "venue": "Technical report,", "citeRegEx": "Krizhevsky,? \\Q2009\\E", "shortCiteRegEx": "Krizhevsky", "year": 2009}, {"title": "One weird trick for parallelizing convolutional neural networks", "author": ["A. Krizhevsky"], "venue": null, "citeRegEx": "Krizhevsky,? \\Q2014\\E", "shortCiteRegEx": "Krizhevsky", "year": 2014}, {"title": "Imagenet classification with deep convolutional neural networks", "author": ["A. Krizhevsky", "I. Sutskever", "G.E. Hinton"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Krizhevsky et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Krizhevsky et al\\.", "year": 2012}, {"title": "Gradient-based learning applied to document recognition", "author": ["Y. LeCun", "L. Bottou", "Y. Bengio", "P. Haffner"], "venue": "Proceedings of the IEEE,", "citeRegEx": "LeCun et al\\.,? \\Q1998\\E", "shortCiteRegEx": "LeCun et al\\.", "year": 1998}, {"title": "On the saddle point problem for non-convex optimization", "author": ["R. Pascanu", "Y.N. Dauphin", "S. Ganguli", "Y. Bengio"], "venue": null, "citeRegEx": "Pascanu et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Pascanu et al\\.", "year": 2014}, {"title": "When networks disagree: Ensemble methods for hybrid neural networks", "author": ["M.P. Perrone", "L.N. Cooper"], "venue": "World Scientific,", "citeRegEx": "Perrone and Cooper,? \\Q1992\\E", "shortCiteRegEx": "Perrone and Cooper", "year": 1992}, {"title": "Large-scale deep unsupervised learning using graphics processors", "author": ["R. Raina", "A. Madhavan", "A.Y. Ng"], "venue": "In Proceedings of the 26th annual International Conference on Machine Learning,", "citeRegEx": "Raina et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Raina et al\\.", "year": 2009}, {"title": "On the importance of initialization and momentum in deep learning", "author": ["I. Sutskever", "J. Martens", "G. Dahl", "G. Hinton"], "venue": "In Proceedings of the 30th International Conference on Machine Learning,", "citeRegEx": "Sutskever et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Sutskever et al\\.", "year": 2013}, {"title": "Fast convolutional nets with fbfft: A GPU performance evaluation", "author": ["N. Vasilache", "J. Johnson", "M. Mathieu", "S. Chintala", "S. Piantino", "Y. LeCun"], "venue": null, "citeRegEx": "Vasilache et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Vasilache et al\\.", "year": 2014}], "referenceMentions": [{"referenceID": 16, "context": ", 2015) and advances in computing power, such as the use of graphic processing units (GPUs) (Raina et al., 2009).", "startOffset": 92, "endOffset": 112}, {"referenceID": 7, "context": "Some researches have focused on speeding up deep neural networks in general, including proposals based on hardware, such as using limited numerical precision (Gupta et al., 2015), which could increase the number of computing units on the hardware, and software, such as using Fourier transform to compute a convolution (Vasilache et al.", "startOffset": 158, "endOffset": 178}, {"referenceID": 18, "context": ", 2015), which could increase the number of computing units on the hardware, and software, such as using Fourier transform to compute a convolution (Vasilache et al., 2014) and developing a base library for deep neural networks (Chetlur et al.", "startOffset": 148, "endOffset": 172}, {"referenceID": 0, "context": ", 2014) and developing a base library for deep neural networks (Chetlur et al., 2014).", "startOffset": 63, "endOffset": 85}, {"referenceID": 4, "context": "DistBelief (Dean et al., 2012) is another framework to speedup the training of large neural network by exploiting parallelism, but it focuses on clusters of computers.", "startOffset": 11, "endOffset": 30}, {"referenceID": 0, "context": ", 2014) and developing a base library for deep neural networks (Chetlur et al., 2014). In this paper, we focus on existing research interested in decreasing the training time, as these approaches are closer to the proposed method. However, we highlight that these improvements are not mutually exclusive and can be used together. Krizhevsky (2014) proposed a mixture of data and model parallelism over GPUs in a single machine based on the type of the layer, exploiting their particularities for increased speed.", "startOffset": 64, "endOffset": 348}, {"referenceID": 5, "context": "A well known example of this is unsupervised pre-training (Erhan et al., 2010), where the network is greedly trained layer-wise to reconstruct its input.", "startOffset": 58, "endOffset": 78}, {"referenceID": 2, "context": "This framework is extended in Coates et al. (2013) to use GPUs as computing units.", "startOffset": 30, "endOffset": 51}, {"referenceID": 4, "context": "They may require special methods for training if they do not fit in a single computing unit (Dean et al., 2012) and can be used after trained to provide guidance to improve smaller networks (Hinton et al.", "startOffset": 92, "endOffset": 111}, {"referenceID": 9, "context": ", 2012) and can be used after trained to provide guidance to improve smaller networks (Hinton et al., 2015).", "startOffset": 86, "endOffset": 107}, {"referenceID": 17, "context": "Since the performance achieved by a neural network may depend on its initialization, there has been a search for good initialization methods (Glorot & Bengio, 2010; Sutskever et al., 2013).", "startOffset": 141, "endOffset": 188}, {"referenceID": 3, "context": "Nonetheless, neural networks seem to be able to achieve good and diverse local minima or saddle points (Dauphin et al., 2014; Choromanska et al., 2014; Pascanu et al., 2014), so they can easily be used as components of ensembles to improve performance.", "startOffset": 103, "endOffset": 173}, {"referenceID": 1, "context": "Nonetheless, neural networks seem to be able to achieve good and diverse local minima or saddle points (Dauphin et al., 2014; Choromanska et al., 2014; Pascanu et al., 2014), so they can easily be used as components of ensembles to improve performance.", "startOffset": 103, "endOffset": 173}, {"referenceID": 14, "context": "Nonetheless, neural networks seem to be able to achieve good and diverse local minima or saddle points (Dauphin et al., 2014; Choromanska et al., 2014; Pascanu et al., 2014), so they can easily be used as components of ensembles to improve performance.", "startOffset": 103, "endOffset": 173}, {"referenceID": 12, "context": "In this partition, one filter of a convolutional layer corresponds to an atomic unit, since all the computed activations share the same parameters, and any layer that has internal parameters or whose activation depends on the individual input values instead of their aggregate, such as normalization layers (Krizhevsky et al., 2012), must be replaced by multiple similar, parallel layers.", "startOffset": 307, "endOffset": 332}, {"referenceID": 13, "context": "The first uses a small network to classify digits on MNIST (LeCun et al., 1998) and focuses on analysing the effects of the number of partitions created on the training time and performance.", "startOffset": 59, "endOffset": 79}, {"referenceID": 10, "context": "The second uses a larger network to classify the images on CIFAR10 (Krizhevsky, 2009) and considers different number of epochs for the pre-training.", "startOffset": 67, "endOffset": 85}, {"referenceID": 10, "context": "The neural network and parameters used for these experiments are the same used by Krizhevsky et al. (2012) in the CUDA-ConvNet library1, which is composed of two convolutional layers with ReLU activation, each one followed by normalization and pooling layers.", "startOffset": 82, "endOffset": 107}], "year": 2017, "abstractText": "This paper presents a new method for pre-training neural networks that can decrease the total training time for a neural network while maintaining the final performance, which motivates its use on deep neural networks. By partitioning the training task in multiple training subtasks with sub-models, which can be performed independently and in parallel, it is shown that the size of the sub-models reduces almost quadratically with the number of subtasks created, quickly scaling down the sub-models used for the pre-training. The sub-models are then merged to provide a pre-trained initial set of weights for the original model. The proposed method is independent of the other aspects of the training, such as architecture of the neural network, training method, and objective, making it compatible with a wide range of existing approaches. The speedup without loss of performance is validated experimentally on MNIST and on CIFAR10 data sets, also showing that even performing the subtasks sequentially can decrease the training time. Moreover, we show that larger models may present higher speedups and conjecture about the benefits of the method in distributed learning systems.", "creator": "LaTeX with hyperref package"}}}