{"id": "1511.03034", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "10-Nov-2015", "title": "Learning with a Strong Adversary", "abstract": "In this paper, we propose a method, learning with adversary, to learn a robust network. Our method takes finding adversarial examples as its mediate step. A new and simple way of finding adversarial examples are presented and experimentally shown to be more `efficient'. Lastly, experimental results shows our learning method greatly improves the robustness of the learned network.", "histories": [["v1", "Tue, 10 Nov 2015 09:44:33 GMT  (142kb,D)", "http://arxiv.org/abs/1511.03034v1", null], ["v2", "Thu, 12 Nov 2015 21:19:21 GMT  (143kb,D)", "http://arxiv.org/abs/1511.03034v2", null], ["v3", "Wed, 18 Nov 2015 19:56:40 GMT  (127kb,D)", "http://arxiv.org/abs/1511.03034v3", null], ["v4", "Tue, 5 Jan 2016 07:34:40 GMT  (127kb,D)", "http://arxiv.org/abs/1511.03034v4", null], ["v5", "Thu, 7 Jan 2016 05:47:33 GMT  (238kb,D)", "http://arxiv.org/abs/1511.03034v5", null], ["v6", "Sat, 16 Jan 2016 01:44:18 GMT  (244kb,D)", "http://arxiv.org/abs/1511.03034v6", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["ruitong huang", "bing xu", "dale schuurmans", "csaba szepesvari"], "accepted": false, "id": "1511.03034"}, "pdf": {"name": "1511.03034.pdf", "metadata": {"source": "CRF", "title": null, "authors": ["Ruitong Huang", "Bing Xu", "Dale Schuurmans"], "emails": ["szepesva}@ualberta.ca"], "sections": [{"heading": "1 INTRODUCTION", "text": "This year, it has reached the stage where it will be able to take the lead, in the same way as it has done in the past."}, {"heading": "1.1 NOTATIONS", "text": "Let's call the samples Z = {(x1, y1),..., (xN, yN)}. Let's let K be the number of classes in our classification problem. The loss function used for training is called \".\" In the face of a norm, the duel norm is called \".\" Let's call the duel norm. \"u,\" that is,. \"u\" = max. \"v.\" < u, v >. Let's call the network \"N,\" the last layer of which is a softmax layer g (x), \u03b1 = (\u03b11,..., \u03b1K)."}, {"heading": "2 FINDING ADVERSARIAL EXAMPLES", "text": "Consider a network that uses softmax as its final level for classification, referred to by J. If we return a sample (x, y) (x, 2,., T), so that N (x) = y is where y is the true name for x. Our goal is to find a small error. N (X + r) 6 = N (X) Our simple method to find such an error is based on the linear approximation of g (x), g (x + r) = J. (x), where T = x is the derived matrix. Consider the following question: for a fixed index j = J, what is the minimum r (j) satisfactory r (j)."}, {"heading": "3 TOWARD THE ROBUSTNESS OF NEURAL NETWORK", "text": "The problem is that the problem (2) must be solved by the use of SGD. (2) The hyperparameters c, which control the magnitude of the disorder, must be tuned. (2) The objective function is the error that led to the aberrations. (3) The objective function is the error that led to the aberrations. (4) It is a replacement loss that is distinguishable and smooth. (4) Let Li (3) the need to calculate the derivatives. (4) The problem is that the problem f = argminf. (4).To solve the problem (2)."}, {"heading": "3.1 COMPUTING THE PERTURBATION", "text": "We propose two methods based on two different principles: Our method, similar to that of Goodfellow et al., 2014, does not require a solution to an optimization problem. Experimental results show that our method is more efficient compared to the method proposed in Goodfellow et al., 2014 in that the performance of the network is worse with the same disturbance than with our adverse examples."}, {"heading": "3.1.1 LIKELIHOOD BASED LOSS", "text": "Let's assume the loss function '(x, y) = h (\u03b1y), where h is a non-negative decreasing function. One of the typical examples would be the logistic regression model. In fact, most network models use a softmax layer as the last layer and a cross-entropy lens function. All of these networks can fit into this type of loss function. Let's remember that we like to findr = arg max. < c (xi + r (i))) yi. This problem can still be difficult in general. We suggest an approximate solution based on the linear approximation of the function g. Since h decreases, r: g (xi + r (i)) yi replaces."}, {"heading": "3.1.2 MISCLASSIFICATION BASED LOSS", "text": "In the case where the loss function is \"a substitute loss for the misclassification rate, it is reasonable in Equation (3) to continue using the misclassification rate as a loss function.\" Equation (3) is therefore to find an error that leads to a misclassification of N. In order to achieve a good approximation in practice, c is a small value and therefore cannot be large enough to force the misclassification of N. An intuitive possibility is to have r in the same direction as the one found in Section 2, since such a direction can be an \"efficient\" direction for the error."}, {"heading": "4 EXPERIMENTAL RESULTS", "text": "We use MNIST (LeCun et al., 1998b) and CIFAR-10 to test our methods to find conflicting examples and train robust networkers.The MNIST dataset contains handwritten grayscale images in the format 28x28. We randomly select 50,000 images for training and 10,000 for testing. We normalize each pixel into the range [0, 1] by dividing 256. \"Introduction to CIFAR-10\": Data Distribution"}, {"heading": "4.1 FINDING ADVERSARIAL EXAMPLES", "text": "We test various fault methods at MNIST, including: 1. Malfunction based on \u03b1 with \"2 standard as shown in section 2 (Adv Alpha); 2. Malfunction based on loss function with loss function as shown in section 3.1 (Adv Loss); 3. Malfunction based on loss function with loss function using the standard shown in section 3.1 (Adv Loss Sign);. Specifically, a standard steering is trained on the MNIST, with training and validation accuracy being 100% and 99.1%. Based on the learned network, different validation sets are then generated by interfering with the original data using different fault methods. Orders of malfunctions range from 0.0 to 4.0 in\" 2 standard. Classification accuracies on differently malfunctioning data sets are shown in Figure 1."}, {"heading": "Is it reasonable to use `2 norm to measure the magnitude?", "text": "One might be concerned if the following case occurs: A small disturbance in the \"2 standard has the most weight at a certain position and therefore alters the image significantly. One of these examples shows up as shown in Figure * * * * *. However, the above example is artificially generated and we do not observe such a phenomenon in our experiments. Disadvantage of using \u03b1 to find disturbances Note that the difference in disturbance efficiency between the use of \u03b1 and the use of the loss function is small. On the other hand, to calculate the disturbance based on \u03b1, one must calculate \u03b1 x, which is actually D times the calculation complexity of the method that uses the loss function, and therefore only has to calculate Medicare'x. Here d is the number of classes."}, {"heading": "4.2 LEARNING WITH ADVERSARY", "text": "We tested our method on both sides of the Atlantic and on the other side of the Atlantic. We first tested different learning methods on a 2-layer neural network that has 1000 hidden nodes for each hidden layer: 1. Normal reverse propagation methods (Normal); 2. Normal reverse propagation layers with drop-outs; 3. The method in (Goodfellow et al., 2014) (Goodfellow's method); 4. Learning with raw data (LWA); 5. Learning with opponents in the representative layer (LWA Rep); 5. The robustness of each classification is measured in different opposites."}, {"heading": "ACKNOWLEDGMENTS", "text": "We thank Naiyan Wang and Ian Goodfellow for meaningful discussions. This work was supported by Alberta Innovates Technology Futures and NSERC."}], "references": [{"title": "Analysis of classifiers\u2019 robustness to adversarial perturbations", "author": ["Alhussein Fawzi", "Omar Fawzi", "Pascal Frossard"], "venue": "arXiv preprint arXiv:1502.02590,", "citeRegEx": "Fawzi et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Fawzi et al\\.", "year": 2015}, {"title": "Explaining and harnessing adversarial examples", "author": ["Ian J Goodfellow", "Jonathon Shlens", "Christian Szegedy"], "venue": "arXiv preprint arXiv:1412.6572,", "citeRegEx": "Goodfellow et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Goodfellow et al\\.", "year": 2014}, {"title": "Deep neural networks for acoustic modeling in speech recognition: The shared views of four research groups", "author": ["Geoffrey Hinton", "Li Deng", "Dong Yu", "George E Dahl", "Abdel-rahman Mohamed", "Navdeep Jaitly", "Andrew Senior", "Vincent Vanhoucke", "Patrick Nguyen", "Tara N Sainath"], "venue": "Signal Processing Magazine, IEEE,", "citeRegEx": "Hinton et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Hinton et al\\.", "year": 2012}, {"title": "Imagenet classification with deep convolutional neural networks. In Advances in neural information processing", "author": ["Alex Krizhevsky", "Ilya Sutskever", "Geoffrey E Hinton"], "venue": null, "citeRegEx": "Krizhevsky et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Krizhevsky et al\\.", "year": 2012}, {"title": "Gradient-based learning applied to document recognition", "author": ["Yann LeCun", "L\u00e9on Bottou", "Yoshua Bengio", "Patrick Haffner"], "venue": "Proceedings of the IEEE,", "citeRegEx": "LeCun et al\\.,? \\Q1998\\E", "shortCiteRegEx": "LeCun et al\\.", "year": 1998}, {"title": "The mnist database of handwritten digits", "author": ["Yann LeCun", "Corinna Cortes", "Christopher JC Burges"], "venue": null, "citeRegEx": "LeCun et al\\.,? \\Q1998\\E", "shortCiteRegEx": "LeCun et al\\.", "year": 1998}, {"title": "Distributional smoothing by virtual adversarial examples", "author": ["Takeru Miyato", "Shin-ichi Maeda", "Masanori Koyama", "Ken Nakae", "Shin Ishii"], "venue": "arXiv preprint arXiv:1507.00677,", "citeRegEx": "Miyato et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Miyato et al\\.", "year": 2015}, {"title": "Apprenticeship learning using inverse reinforcement learning and gradient methods", "author": ["Gergely Neu", "Csaba Szepesv\u00e1ri"], "venue": "arXiv preprint arXiv:1206.5264,", "citeRegEx": "Neu and Szepesv\u00e1ri.,? \\Q2012\\E", "shortCiteRegEx": "Neu and Szepesv\u00e1ri.", "year": 2012}, {"title": "Improving back-propagation by adding an adversarial gradient", "author": ["Arild N\u00f8kland"], "venue": "arXiv preprint arXiv:1510.04189,", "citeRegEx": "N\u00f8kland.,? \\Q2015\\E", "shortCiteRegEx": "N\u00f8kland.", "year": 2015}, {"title": "Intriguing properties of neural networks", "author": ["Christian Szegedy", "Wojciech Zaremba", "Ilya Sutskever", "Joan Bruna", "Dumitru Erhan", "Ian Goodfellow", "Rob Fergus"], "venue": "arXiv preprint arXiv:1312.6199,", "citeRegEx": "Szegedy et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Szegedy et al\\.", "year": 2013}, {"title": "Exploring the space of adversarial images", "author": ["Pedro Tabacof", "Eduardo Valle"], "venue": "arXiv preprint arXiv:1510.05328,", "citeRegEx": "Tabacof and Valle.,? \\Q2015\\E", "shortCiteRegEx": "Tabacof and Valle.", "year": 2015}], "referenceMentions": [{"referenceID": 3, "context": "(Krizhevsky et al., 2012; Hinton et al., 2012) Part of reason is believed to be its high expressiveness from the deep architecture.", "startOffset": 0, "endOffset": 46}, {"referenceID": 2, "context": "(Krizhevsky et al., 2012; Hinton et al., 2012) Part of reason is believed to be its high expressiveness from the deep architecture.", "startOffset": 0, "endOffset": 46}, {"referenceID": 9, "context": "Following the paper of (Szegedy et al., 2013), more and more attentions have been attracted toward such curious \u2018adversary phenomenon\u2019 in the deep learning community.", "startOffset": 23, "endOffset": 45}, {"referenceID": 9, "context": "(Szegedy et al., 2013; Goodfellow et al., 2014; Fawzi et al., 2015; Miyato et al., 2015; N\u00f8kland, 2015; Tabacof and Valle, 2015).", "startOffset": 0, "endOffset": 128}, {"referenceID": 1, "context": "(Szegedy et al., 2013; Goodfellow et al., 2014; Fawzi et al., 2015; Miyato et al., 2015; N\u00f8kland, 2015; Tabacof and Valle, 2015).", "startOffset": 0, "endOffset": 128}, {"referenceID": 0, "context": "(Szegedy et al., 2013; Goodfellow et al., 2014; Fawzi et al., 2015; Miyato et al., 2015; N\u00f8kland, 2015; Tabacof and Valle, 2015).", "startOffset": 0, "endOffset": 128}, {"referenceID": 6, "context": "(Szegedy et al., 2013; Goodfellow et al., 2014; Fawzi et al., 2015; Miyato et al., 2015; N\u00f8kland, 2015; Tabacof and Valle, 2015).", "startOffset": 0, "endOffset": 128}, {"referenceID": 8, "context": "(Szegedy et al., 2013; Goodfellow et al., 2014; Fawzi et al., 2015; Miyato et al., 2015; N\u00f8kland, 2015; Tabacof and Valle, 2015).", "startOffset": 0, "endOffset": 128}, {"referenceID": 10, "context": "(Szegedy et al., 2013; Goodfellow et al., 2014; Fawzi et al., 2015; Miyato et al., 2015; N\u00f8kland, 2015; Tabacof and Valle, 2015).", "startOffset": 0, "endOffset": 128}, {"referenceID": 8, "context": "While in (N\u00f8kland, 2015), as a specific case of the method in (Goodfellow et al.", "startOffset": 9, "endOffset": 24}, {"referenceID": 1, "context": "While in (N\u00f8kland, 2015), as a specific case of the method in (Goodfellow et al., 2014), it is suggested to only use the objective function that is defined on the perturbed data.", "startOffset": 62, "endOffset": 87}, {"referenceID": 0, "context": "Recently a theoretical exploration about the robustness of classifiers in (Fawzi et al., 2015) suggest that, as expected, there is a trade-off between the expressive power and the robustness.", "startOffset": 74, "endOffset": 94}, {"referenceID": 0, "context": ", 2012; Hinton et al., 2012) Part of reason is believed to be its high expressiveness from the deep architecture. Even though the misclassification rate is the main performance metric used to evaluate classifiers, robustness is also a highly desirable property. In particular, a classifier is expected to be \u2018smooth\u2019 so that a small perturbation of a datapoint does not change the prediction of the model. However, a recent intriguing discovery suggests that DNN models do not have such property of robustness. Szegedy et al. (2013) A well performed DNN model may misclassify most of the datapoints because of a human-indistinguishable perturbation on the original dataset.", "startOffset": 8, "endOffset": 533}, {"referenceID": 0, "context": ", 2014; Fawzi et al., 2015; Miyato et al., 2015; N\u00f8kland, 2015; Tabacof and Valle, 2015). Goodfellow et al. (2014) suggests that the reason that cause the existence of adversarial examples may be its linearity of the model in high dimension.", "startOffset": 8, "endOffset": 115}, {"referenceID": 0, "context": ", 2014; Fawzi et al., 2015; Miyato et al., 2015; N\u00f8kland, 2015; Tabacof and Valle, 2015). Goodfellow et al. (2014) suggests that the reason that cause the existence of adversarial examples may be its linearity of the model in high dimension. Further exploration is conduct by Tabacof and Valle (2015), showing that in an image domain, adversarial images inhabit large \u201dadversarial pockets\u201d in the pixel space.", "startOffset": 8, "endOffset": 301}, {"referenceID": 0, "context": ", 2014; Fawzi et al., 2015; Miyato et al., 2015; N\u00f8kland, 2015; Tabacof and Valle, 2015). Goodfellow et al. (2014) suggests that the reason that cause the existence of adversarial examples may be its linearity of the model in high dimension. Further exploration is conduct by Tabacof and Valle (2015), showing that in an image domain, adversarial images inhabit large \u201dadversarial pockets\u201d in the pixel space. Based on these observations, different ways of finding adversarial examples are proposed, among which the most relevant one is proposed in the paper of Goodfellow et al. (2014) where a linear approximation is used and thus it does not require to solve an optimization problem.", "startOffset": 8, "endOffset": 587}, {"referenceID": 0, "context": ", 2014; Fawzi et al., 2015; Miyato et al., 2015; N\u00f8kland, 2015; Tabacof and Valle, 2015). Goodfellow et al. (2014) suggests that the reason that cause the existence of adversarial examples may be its linearity of the model in high dimension. Further exploration is conduct by Tabacof and Valle (2015), showing that in an image domain, adversarial images inhabit large \u201dadversarial pockets\u201d in the pixel space. Based on these observations, different ways of finding adversarial examples are proposed, among which the most relevant one is proposed in the paper of Goodfellow et al. (2014) where a linear approximation is used and thus it does not require to solve an optimization problem. In this paper, we further investigate this problem, and proposed another simple way of finding adversarial examples. Experimental results suggest that our method is more efficient in the sense that DNN has worse performance under same magnitude of perturbation. The main contribution of this paper is learn a robust classifier that can still maintain a high classification performance. Goodfellow et al. (2014) suggest using a new objective function that is a combination of the original one and the one after the datapoints are perturbed to improve the robustness of the network.", "startOffset": 8, "endOffset": 1098}, {"referenceID": 8, "context": "We observe that our learning procedure turns out to be very similar to the one proposed in (N\u00f8kland, 2015), while both works are conduct totally independently from different understandings of this problem.", "startOffset": 91, "endOffset": 106}, {"referenceID": 9, "context": "This problem is first investigated in (Szegedy et al., 2013) which proposes the following learning procedure: given x,", "startOffset": 38, "endOffset": 60}, {"referenceID": 7, "context": "Thus, by Proposition 2 of (Neu and Szepesv\u00e1ri, 2012), \u2202f \u2202v (u \u2217, v0) also belongs to the subderivative of L.", "startOffset": 26, "endOffset": 52}, {"referenceID": 9, "context": "The uniformly Lipschitz-continuous of neural networks was also discussed in the paper of Szegedy et al. (2013). It still remains to compute u\u2217 in Proposition 3.", "startOffset": 89, "endOffset": 111}, {"referenceID": 1, "context": "Our method, similar to that of (Goodfellow et al., 2014), does not require to solve an optimization problem.", "startOffset": 31, "endOffset": 56}, {"referenceID": 1, "context": "Experimental results show that our method, compared to the method proposed in (Goodfellow et al., 2014), is more efficient in that under the same magnitude of perturbation, the performance of the network is worse on our adversarial examples.", "startOffset": 78, "endOffset": 103}, {"referenceID": 1, "context": "Note that the second item here is exactly the method suggested in (Goodfellow et al., 2014).", "startOffset": 66, "endOffset": 91}, {"referenceID": 1, "context": "The method in (Goodfellow et al., 2014) (Goodfellow\u2019s method); 4.", "startOffset": 14, "endOffset": 39}, {"referenceID": 1, "context": "Overall, it achieves fair comparable performance to Goodfellow\u2019s method (Goodfellow et al., 2014).", "startOffset": 72, "endOffset": 97}], "year": 2017, "abstractText": "In this paper, we propose a method, learning with adversary, to learn a robust network. Our method takes finding adversarial examples as its mediate step. A new and simple way of finding adversarial examples are presented and experimentally shown to be more \u2018efficient\u2019. Lastly, experimental results shows our learning method greatly improves the robustness of the learned network.", "creator": "LaTeX with hyperref package"}}}