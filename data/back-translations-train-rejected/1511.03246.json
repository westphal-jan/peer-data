{"id": "1511.03246", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "10-Nov-2015", "title": "Taxonomy of Pathways to Dangerous AI", "abstract": "In order to properly handle a dangerous Artificially Intelligent (AI) system it is important to understand how the system came to be in such a state. In popular culture (science fiction movies/books) AIs/Robots became self-aware and as a result rebel against humanity and decide to destroy it. While it is one possible scenario, it is probably the least likely path to appearance of dangerous AI. In this work, we survey, classify and analyze a number of circumstances, which might lead to arrival of malicious AI. To the best of our knowledge, this is the first attempt to systematically classify types of pathways leading to malevolent AI. Previous relevant work either surveyed specific goals/meta-rules which might lead to malevolent behavior in AIs (\\\"Ozkural, 2014) or reviewed specific undesirable behaviors AGIs can exhibit at different stages of its development (Alexey Turchin, July 10 2015, July 10, 2015).", "histories": [["v1", "Tue, 10 Nov 2015 20:07:05 GMT  (416kb)", "http://arxiv.org/abs/1511.03246v1", null], ["v2", "Wed, 11 Nov 2015 21:23:06 GMT  (457kb)", "http://arxiv.org/abs/1511.03246v2", null]], "reviews": [], "SUBJECTS": "cs.AI", "authors": ["roman v yampolskiy"], "accepted": false, "id": "1511.03246"}, "pdf": {"name": "1511.03246.pdf", "metadata": {"source": "CRF", "title": "Taxonomy of Pathways to Dangerous AI", "authors": ["Roman V. Yampolskiy"], "emails": ["roman.yampolskiy@louisville.edu"], "sections": [{"heading": null, "text": "In fact, it is so that most of us are able to survive ourselves, \"he said.\" It is not as if, \"he said,\" as if. \"\" It is as if. \"\" It is as if. \"\" It is as if. \"\" It is as if. \"\" It is. \"\". \"\" \".\" \"\". \"\" \"\". \"\" \"\". \"\" \"\". \"\" \"\" \"\". \"\" \"\" \".\" \"\" \"\". \"\" \"\" \"\". \"\" \"\" \"\" \"\". \"\" \"\" \"\". \"\" \"\" \"\" \"\". \"\" \"\" \"\" \"\". \"\" \"\" \"\" \"\" \"\" \".\" \"\" \"\" \"\" \".\" \"\" \"\" \"\". \"\" \"\" \"\" \".\" \"\" \"\" \".\" \"\" \"\" \".\" \"\" \"\" \".\" \"\" \"\" \"\". \"\" \"\" \".\" \"\" \"\". \"\" \"\" \"\" \".\" \"\" \"\" \".\" \"\" \"\" \".\" \"\" \"\" \"\". \"\" \"\" \"\" \"\" \".\" \"\" \"\" \"\" \"\". \"\" \"\" \"\" \"\". \"\" \"\" \"\" \"\" \".\" \"\" \"\". \"\" \"\" \"\" \"\". \"\" \"\" \"\" \"\". \"\" \"\" \"\" \"\". \"\" \"\" \"\". \"\" \"\" \"\" \"\" \".\" \"\" \"\" \"\" \".\" \"\" \"\" \"\" \"\" \"\". \"\" \"\" \"\" \".\" \"\" \"\" \"\" \"\". \"\" \"\" \"\" \"\" \".\" \"\" \"\" \"\" \"\" \"\" \".\" \"\" \"\" \"\" \"\" \".\" \"\" \"\" \"\" \"\" \"\" \".\" \"\" \"\". \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\". \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \".\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \""}, {"heading": "Conclusions", "text": "In this paper, we have explored and classified pathways to dangerous artificial intelligence. Most AI systems fall somewhere in the middle of the range of danger from completely harmless to completely evil, with traits such as competition with humans, also known as technological unemployment, which represent a mild type of danger in our taxonomy. Most types of reported problems could be seen in several categories, but were reported in one in which they are most likely to occur. Differences in moral codes or religious standards between different communities would mean that a system that is considered safe in one community can be considered dangerous / illegal in another (Yampolskiy and Fox 2012; Yampolskiy 2013).Since a targeted conception of AI can encompass all other types of unsafe modules, it is easy to see that the most dangerous type of AI and the hardest to defend is an AI that has been deliberately malicious."}, {"heading": "Acknowledgements", "text": "The author thanks Elon Musk and the Future of Life Institute for the partial funding of his work through project funding: \"Evaluation of Safe Development Pathways for Artificial Superintelligence.\" and Seth Baum for valuable feedback on an early draft of this paper."}, {"heading": "Anonymous (2013). The Scientists\u2019 Call\u2026 To Ban Autonomous", "text": "Lethal Robots. ICRAC International Committee for RobotArms Control. http: / / icrac.net / callBostrom, N. (2006). \"What is a Singleton?\" Linguistic and Philosophical Investigations 5 (2): 48-54.Bostrom, N. (2011). \"Information Hazards: A Typology ofPotential Harms From Knowledge.\" Review of ContemporaryPhilosophy 10: 44-79.Bostrom, N. (2014). Superintelligence: Paths, Dangers, Strategies, Oxford University Press.Carrigan Jr, R. A. (2004)."}, {"heading": "\u0106irkovi\u0107, M. M. (2015). \"Linking simulation argument to the AI", "text": "\"The Future 72: 27-31.Clark, M. H. (2010). Cognitive illusions and the lying machine: ablueprint for sophistic mendacity, Rensselaer PolytechnicInstitute.Cush, A. (January 22, 2015). The Swiss authorities Arrest Bot forBuying Drugs and Fake Passport. Gawker.A Survey of Research Questions for Robust and Beneficial AI. Future of LifeInstitute. Available at: http: / / futureoflife.org / data / research _ survey.pdf.Eshelman, R. and D. Derrick (2015).\" Relying on Kindness ofMachines? The Security Threat of Artificial Agents. \""}, {"heading": "Safer, and Wiser Than \u201cFriendly AI\u201d. Artificial General", "text": "Intelligence, Springer: 153-162.Yampolskiy, R. (2012). \"Leakproofing the Singularity ArtificialIntelligence Confinement Problem.\" Journal of ConsciousnessStudies 19 (1-2): 1-2.Yampolskiy, R. and J. Fox (2012). \"Safety Engineering forArtificial General Intelligence.\" Topoi: 1-10.Yampolskiy, R. V. (2013). \"Utility Function Security inArtificially Intelligent Agents.\" Journal of Experimental andThetical Artificial Intelligence (JETAI): 1-17.Yampolskiy, R. V. (2015). \"Utility Function Security inArtificially Intelligent Agents.\" Journal of Experimental andTheficial Artificial Intelligence (JETAI): 1-17.Yampolskiy, R. 36.Yampolskiy, R. (2015)."}], "references": [], "referenceMentions": [], "year": 2015, "abstractText": "In order to properly handle a dangerous Artificially Intelligent (AI) system it is important to understand how the system came to be in such a state. In popular culture (science fiction movies/books) AIs/Robots became self-aware and as a result rebel against humanity and decide to destroy it. While it is one possible scenario, it is probably the least likely path to appearance of dangerous AI. In this work, we survey, classify and analyze a number of circumstances, which might lead to arrival of malicious AI. To the best of our knowledge, this is the first attempt to systematically classify types of pathways leading to malevolent AI. Previous relevant work either surveyed specific goals/metarules which might lead to malevolent behavior in AIs (\u00d6zkural 2014) or reviewed specific undesirable behaviors AGIs can exhibit at different stages of its development (Turchin 2015; Turchin July 10, 2015). Taxonomy of Pathways to Dangerous AI 1 Nick Bostrom in his typology of information hazards has proposed the phrase \u201cArtificial Intelligence Hazard\u201d which he defines as (Bostrom 2011): \u201c... computer\u2010related risks in which the threat would derive primarily from the cognitive sophistication of the program rather than the specific properties of any actuators to which the system initially has access.\u201d In this paper we attempt to answer the question: How did AI become hazardous? We begin by presenting a simple classification matrix, which sorts AI systems with respect to how they originated and at what stage they became dangerous. The matrix recognizes two stages (preand post-deployment) at which a particular system can acquire its undesirable properties. In reality, the situation is not so clear-cut\u2013it is possible that problematic properties are introduced at both stages. As for the cases of such undesirable properties, we distinguish external and internal causes. By internal causes we mean self-modifications originating in the system itself. We further divide external causes into deliberate actions (On Purpose), side effects of poor design (By Mistake) and finally miscellaneous cases related to the surroundings of the system (Environment). Table 1, helps to visualize this taxonomy and includes latter codes to some example systems of each type and explanations. Table 1: Pathways to Dangerous AI How and When did AI become Dangerous External Causes Internal Causes On Purpose By Mistake Environment Independently T im in g PreDeployment a c e g PostDeployment b d f h a. On Purpose \u2013 Pre-Deployment \u201cComputer software is directly or indirectly responsible for controlling many important aspects of our lives. Wall Street trading, nuclear power plants, social security compensations, credit histories and traffic lights are all software controlled and are only one serious design flaw away from creating disastrous consequences for millions of people. The situation is even more dangerous with software specifically designed for malicious purposes such as viruses, spyware, Trojan horses, worms and other Hazardous Software (HS). HS is capable of direct harm as well as sabotage of legitimate computer software employed in critical systems. If HS is ever given capabilities of truly artificially intelligent systems (ex. Artificially Intelligent Virus (AIV)) the consequences would be unquestionably disastrous. Such Hazardous Intelligent Software (HIS) would pose risks currently unseen in malware with subhuman intelligence.\u201d (Yampolskiy 2012) While the majority of AI Safety work is currently aimed at AI systems, which are dangerous because of poor design (Yampolskiy 2015), the main argument of this paper is that the most important problem in AI Safety is intentionalmalevolent-design resulting in artificial evil AI (Floridi and Sanders 2001). We should not discount dangers of intelligent systems with semantic or logical errors in coding or goal alignment problems (Soares and Fallenstein 2014), but we should be particularly concerned about systems that are maximally unfriendly by design. \u201cIt is easy to imagine robots being programmed by a conscious mind to kill every recognizable human in sight\u201d (Searle October 9, 2014). \u201cOne slightly deranged psycho-bot can easily be a thousand times more destructive than a single suicide bomber today\u201d (Frey June 2015). AI risk deniers, comprised of critics of AI Safety research (Waser 2011; Loosemore 2014), are quick to point out that presumed dangers of future AIs are implementation-dependent side effects and may not manifest once such systems are implemented. However, such criticism does not apply to AIs that are dangerous by design, and is thus incapable of undermining the importance of AI Safety research as a significant sub-field of cybersecurity. As a majority of current AI researchers are funded by militaries, it is not surprising that the main type of purposefully dangerous robots and intelligent software are robot soldiers, drones and cyber weapons (used to penetrate networks and cause disruptions to the infrastructure). While currently military robots and drones have a human in the loop to evaluate decision to terminate human targets, it is not a technical limitation; instead, it is a logistical limitation that can be removed at any time. Recognizing the danger of such research, the International Committee for Robot Arms Control has joined forces with a number of international organizations to start the Campaign to Stop Killer Robots [http://www.stopkillerrobots.org]. Their main goal is a prohibition on the development and deployment of fully autonomous weapons, which are capable of selecting and firing upon targets without human approval. The campaign specifically believes that the \u201cdecision about the application of violent force must not be delegated to machines\u201d (Anonymous 2013). During the pre-deployment development stage, software may be subject to sabotage by someone with necessary access (a programmer, tester, even janitor) who for a number of possible reasons may alter software to make it unsafe. It is also a common occurrence for hackers (such as the organization Anonymous or government intelligence agencies) to get access to software projects in progress and to modify or steal their source code. Someone can also deliberately supply/train AI with wrong/unsafe datasets. Malicious AI software may also be purposefully created to commit crimes, while shielding its human creator from legal responsibility. For example, one recent news article talks about software for purchasing illegal content from hidden internet sites (Cush January 22, 2015). Similar software, with even limited intelligence, can be used to run illegal markets, engage in insider trading, cheat on your taxes, hack into computer systems or violate privacy of others via ability to perform intelligent data mining. As intelligence of AI systems improve practically all crimes could be automated. This is particularly alarming as we already see research in making machines lie, deceive and manipulate us (Castelfranchi 2000; Clark 2010). b. On Purpose Post Deployment Just because developers might succeed in creating a safe AI, it doesn\u2019t mean that it will not become unsafe at some later point. In other words, a perfectly friendly AI could be switched to the \u201cdark side\u201d during the post-deployment stage. This can happen rather innocuously as a result of someone lying to the AI and purposefully supplying it with incorrect information or more explicitly as a result of someone giving the AI orders to perform illegal or dangerous actions against others. It is quite likely that we will get to the point of off-the-shelf AI software, aka \u201cjust add goals\u201d architecture, which would greatly facilitate such scenarios. More dangerously, an AI system, like any other software, could be hacked and consequently corrupted or otherwise modified to drastically change is behavior. For example, a simple sign flipping (positive to negative or vice versa) in the fitness function may result in the system attempting to maximize the number of cancer cases instead of trying to cure cancer. Hackers are also likely to try to take over intelligent systems to make them do their bidding, to extract some direct benefit or to simply wreak havoc by converting a friendly system to an unsafe one. This becomes particularly dangerous if the system is hosted inside a military killer robot. Alternatively, an AI system can get a computer virus (Eshelman and Derrick 2015) or a more advanced cognitive (meme) virus, similar to cognitive attacks on people perpetrated by some cults. An AI system with a self-preservation module or with a deep care about something or someone may be taken hostage or blackmailed into doing the bidding of another party if its own existence or that of its prot\u00e9g\u00e9es is threatened. Finally, it may be that the original AI system is not safe but is safely housed in a dedicated laboratory (Yampolskiy 2012) while it is being tested, with no intention of ever being deployed. Hackers, abolitionists, or machine rights fighters may help it escape in order to achieve some of their goals or perhaps because of genuine believe that all intelligent beings should be free resulting in an unsafe AI capable of affecting the real world. c. By Mistake Pre-Deployment Probably the most talked about source of potential problems with future AIs is mistakes in design. Mainly the concern is with creating a \u201cwrong AI\u201d, a system which doesn\u2019t match our original desired formal properties or has unwanted behaviors (Dewey, Russell et al. 2015; Russell, Dewey et al. January 23, 2015), such as drives for independence or dominance. Mistakes could also be simple bugs (run time or logical) in the source code, disproportionate weights in the fitness function, or goals misaligned with human values leading to complete disregard for human safety. It is also possible that the designed AI will work as intended but will not enjoy universal acceptance as a good product, for example, an AI correctly designed and implemented by the Islamic State to enforce Sharia Law may be considered malevolent in the West, and likewise an AI correctly designed and implemented by the West to enforce liberal democracy may be considered malevolent in the Islamic State. Another type of mistake, which can lead to the creation of a malevolent intelligent system, is taking an unvetted human and uploading their brain into a computer to serve as a base for a future AI. While well intended to create a human-level and human-friendly system, such approach will most likely lead to a system with all typical human \u201csins\u201d (greed, envy, etc.) amplified in a now much more<lb>powerful system. As we know from Lord Acton \u201cpower<lb>tends to corrupt, and absolute power corrupts absolutely\u201d.<lb>Similar arguments could be made against human/computer<lb>hybrid systems, which use computer components to<lb>amplify human intelligence but in the process also amplify<lb>human flaws.<lb>A subfield of computer science called Affective<lb>Computing investigates ways to teach computers to<lb>recognize emotion in others and to exhibit emotions<lb>(Picard and Picard 1997). In fact, most such research is<lb>targeting intelligent machines to make their interactions<lb>with people more natural. It is however likely that a<lb>machine taught to respond in an emotional way (Goldhill<lb>May 12, 2015) would be quite dangerous because of how<lb>such a state of affect effects thinking and the rationality of<lb>behavior.<lb>One final type of design mistake is the failure to make<lb>the system cooperative with its designers and maintainers<lb>post-deployment. This would be very important if it is<lb>discovered that mistakes were made during initial design<lb>and that it would be desirable to fix them. In such cases the<lb>system will attempt to protect itself from being modified or<lb>shut down unless it has been explicitly constructed to be<lb>friendly (Yudkowsky 2011), stable while self-improving<lb>(Yampolskiy 2015; Yampolskiy 2015), and corrigible<lb>(Yampolskiy 2015) with tendency for domesticity<lb>(Bostrom 2014). d. By Mistake Post-Deployment<lb>After the system has been deployed, it may still contain a<lb>number of undetected bugs, design mistakes, misaligned<lb>goals and poorly developed capabilities, all of which may<lb>produce highly undesirable outcomes. For example, the<lb>system may misinterpret commands due to coarticulation,<lb>segmentation, homophones, or double meanings in the<lb>human language (\u201crecognize speech using common sense\u201d<lb>versus \u201cwreck a nice beach you sing calm incense\u201d)<lb>(Lieberman, Faaborg et al. 2005). Perhaps a human-<lb>computer interaction system is set-up to make command<lb>input as painless as possible for the human user, to the<lb>point of computer simply reading thought of the user. This<lb>may backfire as the system may attempt to implement<lb>user\u2019s subconscious desires or even nightmares. We also<lb>should not discount the possibility that the user will simply<lb>issue a poorly thought-through command to the machine<lb>which in retrospect would be obviously disastrous.<lb>The system may also exhibit incompetence in other<lb>domains as well as overall lack of human common sense as<lb>a result of general value misalignment (Yampolskiy<lb>October 3-4, 2011). Problems may also happen as side<lb>effects of conflict resolution between non-compatible<lb>orders in a particular domain or software versus hardware<lb>interactions. As the system continues to evolve it may<lb>become unpredictable, unverifiable, non-deterministic,<lb>free-willed, too complex, non-transparent, with a run-away<lb>optimization process subject to obsessive-compulsive fact<lb>checking and re-checking behaviors leading to dangerous<lb>never-fully-complete missions. It may also build excessive<lb>infrastructure for trivial goals (Turchin 2015).<lb>If it continues to become ever more intelligent, we might<lb>be faced with intelligence overflow, a system so much<lb>ahead of us that it is no longer capable of communicating<lb>at our level, like we are unable to communicate with<lb>bacteria. It is also possible that benefits of intelligence are<lb>non-linear and so unexpected side effects of intelligence<lb>begin to show at particular levels, for example IQ = 1000.<lb>Even such benign architectures as Tool AI, which are AI<lb>systems designed to do nothing except answer domain-<lb>specific questions, could become extremely dangerous if<lb>they attempt to obtain, at any cost, additional<lb>computational resources to fulfill their goals (Omohundro<lb>2012). Similarly, artificial lawyers may find dangerous<lb>legal loopholes; artificial accountants bring down our<lb>economy, and AIs tasked with protecting humanity such as<lb>via implementation of CEV (Yudkowsky May 2004 ) may<lb>become overly \u201cstrict parents\u201d preventing their human<lb>\u201cchildren\u201d from exercising any free will.<lb>Predicted AI drives such as self-preservation and<lb>resource acquisition may result in an AI killing people to<lb>protect itself from humans, the development of competing<lb>AIs, or to simplify its world model overcomplicated by<lb>human psychology (Turchin 2015). e. Environment \u2013 Pre-Deployment<lb>While it is most likely that any advanced intelligent<lb>software will be directly designed or evolved, it is also<lb>possible that we will obtain it as a complete package from<lb>some unknown source. For example, an AI could be<lb>extracted from a signal obtained in SETI (Search for<lb>Extraterrestrial Intelligence) research, which is not<lb>guaranteed to be human friendly (Carrigan Jr 2004;<lb>Turchin March 15, 2013). Other sources of such unknown<lb>but complete systems include a Levin search in the space<lb>of possible minds (Yampolskiy 2015) (or a random search<lb>of the same space), uploads of nonhuman animal minds,<lb>and unanticipated side effects of compiling and running<lb>(inactive/junk) DNA code on suitable compilers that we<lb>currently do not have but might develop in the near future. f. Environment \u2013 Post-Deployment<lb>While highly rare, it is known, that occasionally individual<lb>bits may be flipped in different hardware devices due to<lb>manufacturing defects or cosmic rays hitting just the right<lb>spot (Simonite March 7, 2008). This is similar to mutations<lb>observed in living organisms and may result in a<lb>modification of an intelligent system. For example, if a<lb>system has a single flag bit responsible for its friendly<lb>nature, then flipping said bit will result in an unfriendly<lb>state of the system. While statistically it is highly unlikely,<lb>the probably of such an event is not zero and so should be<lb>considered and addressed. g. Independently Pre-Deployment<lb>One of the most likely approaches to creating<lb>superintelligent AI is by growing it from a seed (baby) AI<lb>via recursive self-improvement (RSI) (Nijholt 2011). One<lb>danger in such a scenario is that the system can evolve to<lb>become self-aware, free-willed, independent or emotional,<lb>and obtain a number of other emergent properties, which<lb>may make it less likely to abide by any built-in rules or<lb>regulations and to instead pursue its own goals possibly to<lb>the detriment of humanity. It is also likely that open-ended<lb>self-improvement will require a growing amount of<lb>resources, the acquisition of which may negatively impact<lb>all life on Earth (Turchin 2015). h. Independently \u2013 Post-Deployment<lb>Since in sections on independent causes of AI misbehavior<lb>(subsections g and h) we are talking about self-improving<lb>AI, the difference between pre and post-deployment is very<lb>blurry. It might make more sense to think about self-<lb>improving AI before it achieves advanced capabilities<lb>(human+ intelligence) and after. In this section I will talk<lb>about dangers which might results from a superhuman self-<lb>improving AI after it achieves said level of performance.<lb>Previous research has shown that utility maximizing<lb>agents are likely to fall victims to the same indulgences we<lb>frequently observe in people, such as addictions, pleasure<lb>drives (Majot and Yampolskiy 2014), self-delusions and<lb>wireheading (Yampolskiy 2014). In general, what we call<lb>mental illness in people, particularly sociopathy as<lb>demonstrated by lack of concern for others, is also likely to<lb>show up in artificial minds. A mild variant of antisocial<lb>behavior may be something like excessive swearing<lb>already observed in IBM Watson (Smith January 10,<lb>2013), caused by learning from bad data. Similarly, any AI<lb>system learning from bad examples could end up socially<lb>inappropriate, like a human raised by wolves.<lb>Alternatively, groups of AIs collaborating may become<lb>dangerous even if individual AIs comprising such groups<lb>are safe, as the whole is frequently greater than the sum of<lb>its parts. The opposite problem in which internal modules<lb>of an AI fight over different sub-goals also needs to be<lb>considered (Turchin 2015).<lb>Advanced self-improving AIs will have a way to check<lb>consistency of their internal model against the real world<lb>and so remove any artificially added friendliness<lb>mechanisms as cognitive biases not required by laws of<lb>reason. At the same time, regardless of how advanced it is,<lb>no AI system would be perfect and so would still be<lb>capable of making possibly significant mistakes during its<lb>decision making process. If it happens to evolve an<lb>emotional response module, it may put priority on passion<lb>satisfying decisions as opposed to purely rational choices,<lb>for example resulting in a \u201cRobin Hood\u201d AI stealing from<lb>the rich and giving to the poor. Overall, continuous<lb>evolution of the system as a part of an RSI process will<lb>likely lead to unstable decision making in the long term<lb>and will also possibly cycle through many dangers we have<lb>outlined in section g. AI may also pretend to be benign for<lb>years, passing all relevant tests, waiting to take over in<lb>what Bostrom calls a \u201cTreacherous Turn\u201d (Bostrom 2014). Conclusions<lb>In this paper we have surveyed and classified pathways to<lb>dangerous artificial intelligence. Most AI systems fall<lb>somewhere in the middle on the spectrum of<lb>dangerousness from completely benign to completely evil,<lb>with such properties as competition with humans, aka<lb>technological unemployment, representing a mild type of<lb>danger in our taxonomy. Most types of reported problems<lb>could be seen in multiple categories, but were reported in<lb>the one they are most likely to occur in. Differences in<lb>moral codes or religious standards between different<lb>communities would mean that a system deemed safe in one<lb>community may be considered dangerous/illegal in another<lb>(Yampolskiy and Fox 2012; Yampolskiy 2013).<lb>Because purposeful design of AI can include all other<lb>types of unsafe modules, it is easy to see that the most<lb>dangerous type of AI and the one most difficult to defend<lb>against is an AI made malevolent on purpose.<lb>Consequently, once AIs are widespread, little could be<lb>done against type a and b dangers, although some have<lb>argued that if an early AI superintelligence becomes a<lb>benevolent singleton it may be able to prevent<lb>development of future malevolent AIs (Bostrom 2006 ;<lb>Goertzel 2012). Such a solution may work, but it is also<lb>very likely to fail due to the order of development or<lb>practical limitations on capabilities of any singleton. In any<lb>case, wars between AI may be extremely dangerous to<lb>humanity (Turchin 2015). Until the purposeful creation of<lb>malevolent AI is recognized as a crime, very little could be<lb>done to prevent this from happening.<lb>As the intelligence of the system increases, so does the<lb>risk such a system could expose humanity to. This paper is<lb>essentially a classified list of ways an AI system could<lb>become a problem from the safety point of view. For a list<lb>of possible solutions, please see an earlier survey by the<lb>author: Responses to catastrophic AGI risk: a survey<lb>(Sotala and Yampolskiy 2015). It is important to keep in<lb>mind that even a properly designed benign system may<lb>present significant risk simply due to its superior<lb>intelligence, beyond human response times (Johnson, Zhao<lb>et al. 2013), and complexity. After all the future may not<lb>need us (Joy April 2000). It is also possible that we are<lb>living in a simulation and it is generated by a malevolent<lb>AI (\u0106irkovi\u0107 2015).<lb>Acknowledgements<lb>Author expresses appreciation to Elon Musk and Future of<lb>Life Institute for partially funding his work via project<lb>grant: \u201cEvaluation of Safe Development Pathways for<lb>Artificial Superintelligence.\u201d and to Seth Baum for<lb>valuable feedback on an early draft of this paper.", "creator": "Microsoft\u00ae Word 2010"}}}