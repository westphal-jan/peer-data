{"id": "1511.03292", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "10-Nov-2015", "title": "From Images to Sentences through Scene Description Graphs using Commonsense Reasoning and Knowledge", "abstract": "In this paper we propose the construction of linguistic descriptions of images. This is achieved through the extraction of scene description graphs (SDGs) from visual scenes using an automatically constructed knowledge base. SDGs are constructed using both vision and reasoning. Specifically, commonsense reasoning is applied on (a) detections obtained from existing perception methods on given images, (b) a \"commonsense\" knowledge base constructed using natural language processing of image annotations and (c) lexical ontological knowledge from resources such as WordNet. Amazon Mechanical Turk(AMT)-based evaluations on Flickr8k, Flickr30k and MS-COCO datasets show that in most cases, sentences auto-constructed from SDGs obtained by our method give a more relevant and thorough description of an image than a recent state-of-the-art image caption based approach. Our Image-Sentence Alignment Evaluation results are also comparable to that of the recent state-of-the art approaches.", "histories": [["v1", "Tue, 10 Nov 2015 21:14:51 GMT  (6165kb,D)", "http://arxiv.org/abs/1511.03292v1", null]], "reviews": [], "SUBJECTS": "cs.CV cs.AI cs.CL", "authors": ["somak aditya", "yezhou yang", "chitta baral", "cornelia fermuller", "yiannis aloimonos"], "accepted": false, "id": "1511.03292"}, "pdf": {"name": "1511.03292.pdf", "metadata": {"source": "CRF", "title": "From Images to Sentences through Scene Description Graphs using Commonsense Reasoning and Knowledge", "authors": ["Somak Aditya", "Yezhou Yang", "Chitta Baral", "Cornelia Fermuller", "Yiannis Aloimonos"], "emails": [], "sections": [{"heading": "1. Introduction", "text": "In fact, most people who are able to survive themselves are not able to survive themselves, \"he told the German Press Agency in an interview with\" Welt am Sonntag \":\" I don't think that we will be able to save the world, and that we will be able to save it. \""}, {"heading": "2. Related Works", "text": "As [21] suggests, such works can be categorized into 1) dense image annotations, 2) textual descriptions, 3) natural language in images, and 4) neural networks in visual and linguistic areas. In accordance with the above categorization, we share our roots with the work to create textual descriptions. This includes the work that generates sentences such as [19], [12], [34], [40] descriptions by joining annotations or applying templates to established image contents. [10], [24], [25], [47], [48] are some of the works that have produced descriptions by stitching annotations or applying templates to established image contents. Several works have shown promising efforts to acquire and apply common sense in various aspects of scene analysis. [52] uses abstraction to discover sectarian-like illustrations. [7] All of the scenarios we automatically suggest relate to certain variations, 37] all of which we learn automatically relate."}, {"heading": "3. State-of-the-art Visual Detections", "text": "The recent development of deep neural networks is based on a revolutionary detection method. Unlike traditional craftsmanship, a multi-layered neural network architecture is efficient, describing the raw data [1], which demonstrates superior performance in terms of standard detection [2]. In this paper, we use three image data sets popularly referred to as Flickr 8k. Flickr 30k and Coco datasets [3]. These three data sets have a range of 8.0 and more than 160K. The images from these data sets are generally referred to as Flickr 8k."}, {"heading": "4. Constructing SDGs from Noisy Visual Detections", "text": "In order to gain a better understanding of this complex system, we provide an architectural diagram that provides the reasoning process for an exemplary image in Figure 3.As shown above, the perception system produces objects, scenes and constitutive recognition studs for each image. Each recognition is provided with a trust relationship in which the results for each individual image are provided. However, most of these recognition systems are quite noisy. We develop an ingenious thought pattern to construct SDGs from such noisy recognition systems, using pre-processed background knowledge. 4All the phrases and their corresponding frequencies are made available to the public in the final version. At this stage, we collect ontological information about object categories in object categories Meta classes. We collect meta classes in Scene Metadata (OT) and Scene classes in Scene Metadata."}, {"heading": "4.2. Reasoning Framework", "text": "Equipped with the background knowledge given in the form of (Kb, Bn, SM, OT) mixing reliability, we process the objects, scenes and constituent detections for an image to construct a suitable SDG in the following ways: i) We then colonize synonyms, hypernims, hypernims of objects and synonyms, AVCs, AVCs (with priors) of scenes; ii) (Scene Constituents:) we extract entities and events from each constituency. Just as, the constituent person wears short sentences in an event costume with two edges: a designated agent joins the person and another designated recipient joins the entity. iiii) We select the AVCs iteratively so that we maximize the conditional probability that we give high trusted objects; iv) (Objects) for low-scoring objects."}, {"heading": "4.4. Conditional Probability Estimation", "text": "In this subsection we describe the type of conditional probabilities we estimate and the Bayesian Network we learn to estimate such probabilities. We use conditional probability calculations in two steps of our approach: to derive the most likely capture of abstract visual concepts and to correct faulty objects with low scoring. To derive the most likely capture of AVCs, we first compile a list (Cfreq) of all common AVCs (with frequency > 2 in our experiments) from all scenes detected for a test image. We then follow algorithm 1 to obtain the set of derived concepts, the Cinf from the series of high scores (Score > \u03b1h8) entities Oimg and the set of Simg scenes detected for a test image. We iterate until entropy continues to decrease."}, {"heading": "4.4.1 Learning the Bayesian Network Bn", "text": "To gather knowledge of the naturally occurring units and to abstract visual concepts, we learn a Bayesian network that represents the dependencies between them. We generate the training data D, which represents a series of tuples T = (ti) i, i, 1,.., N, where N is the total number of units and AVCs. Each term ti is binary and denotes 1 when the ith entity (or AVC) occurs in the tupel. Then we use the Tabu Search (tabu) algorithm to learn the structure and then populate the conditional probability tables using the R-bnlearn8The hyperparameter (or AVC) is determined on the basis of validation data. Package [38] A subgraph of the learned Bayesian network is shown in Figure 5.To create the training data, we process each training image (in itr) to automatically detect the entities and AVC- scenes (we then only detect the airport state)."}, {"heading": "4.5. Ranking and Inferring Final Concepts", "text": "This year is the highest in the history of the country."}, {"heading": "5. Experiments and Results", "text": "This year it is more than ever before."}, {"heading": "6. Conclusion", "text": "This paper introduced an argumentation module to generate textual descriptions from images by first constructing a new semantic intermediate representation, the Scene Description Diagram (SDG), which will later be used to generate sentences. The argumentation module uses an automatically constructed knowledge base made from text to capture \"healthy\" knowledge. After building the Knowledge Base, we proposed a method to obtain such SDGs from loud labels using our prediction system. The SDG is a representation of the scene in the field of view that integrates direct visual knowledge (objects and their locations in the scene) with a healthy background of sound knowledge. In addition, the SDGs have a structure that resembles semantic representations of sentences, facilitating the interaction between vision and natural language. The presentation of the SDG has great potential. Here, we have the SDG for the automatic creation of sentences that are used to describe the scene."}], "references": [{"title": "Representation learning: A review and new perspectives", "author": ["Y. Bengio", "A. Courville", "P. Vincent"], "venue": "Pattern Analysis and Machine Intelligence, IEEE Transactions on, 35(8):1798\u20131828,", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2013}, {"title": "Enabling experts to build knowledge bases from science textbooks", "author": ["V.K. Chaudhri", "B.E. John", "S. Mishra", "J. Pacheco", "B. Porter", "A. Spaulding"], "venue": "Proceedings of the 4th International Conference on Knowledge Capture, K-CAP \u201907, pages 159\u2013166, New York, NY, USA,", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2007}, {"title": "Learning a recurrent visual representation for image caption generation", "author": ["X. Chen", "C.L. Zitnick"], "venue": "arXiv preprint arXiv:1411.5654,", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2014}, {"title": "Km-the knowledge machine 2.0: Users manual", "author": ["P. Clark", "B. Porter", "B.P. Works"], "venue": "Department of Computer Science, University of Texas at Austin,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2004}, {"title": "Histograms of oriented gradients for human detection", "author": ["N. Dalal", "B. Triggs"], "venue": "Computer Vision and Pattern Recognition, 2005. CVPR 2005. IEEE Computer Society Conference on, volume 1, pages 886\u2013893. IEEE,", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2005}, {"title": "Commonsense reasoning and commonsense knowledge in artificial intelligence", "author": ["E. Davis", "G. Marcus"], "venue": "Commun. ACM, 58(9):92\u2013103, Aug.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2015}, {"title": "Learning everything about anything: Webly-supervised visual concept learning", "author": ["S.K. Divvala", "A. Farhadi", "C. Guestrin"], "venue": "2014 IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2014, Columbus, OH, USA, June 23-28, 2014, pages 3270\u20133277,", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2014}, {"title": "Long-term recurrent convolutional networks for visual recognition and description", "author": ["J. Donahue", "L.A. Hendricks", "S. Guadarrama", "M. Rohrbach", "S. Venugopalan", "K. Saenko", "T. Darrell"], "venue": "arXiv preprint arXiv:1411.4389,", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2014}, {"title": "Decaf: A deep convolutional activation feature for generic visual recognition", "author": ["J. Donahue", "Y. Jia", "O. Vinyals", "J. Hoffman", "N. Zhang", "E. Tzeng", "T. Darrell"], "venue": "Proceedings of the 31st International Conference on Machine Learning (ICML-14), pages 647\u2013655,", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2014}, {"title": "Image description using visual dependency representations", "author": ["D. Elliott", "F. Keller"], "venue": "Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, EMNLP 2013, 18-21 October 2013, Grand Hyatt Seattle, Seattle, Washington, USA, A meeting of SIGDAT, a Special Interest Group of the ACL, pages 1292\u20131302,", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2013}, {"title": "Describing objects by their attributes", "author": ["A. Farhadi", "I. Endres", "D. Hoiem", "D. Forsyth"], "venue": "Computer Vision and Pattern Recognition, 2009. CVPR 2009. IEEE Conference on, pages 1778\u20131785. IEEE,", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2009}, {"title": "Every picture tells a story: Generating sentences from images", "author": ["A. Farhadi", "M. Hejrati", "M.A. Sadeghi", "P. Young", "C. Rashtchian", "J. Hockenmaier", "D. Forsyth"], "venue": "Proceedings of the 11th European Conference on Computer Vision: Part IV, ECCV\u201910, pages 15\u201329, Berlin, Heidelberg,", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2010}, {"title": "A discriminatively trained, multiscale, deformable part model", "author": ["P. Felzenszwalb", "D. McAllester", "D. Ramanan"], "venue": "Computer Vision and Pattern Recognition, 2008. CVPR 2008. IEEE Conference on, pages 1\u20138. IEEE,", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2008}, {"title": "Simplenlg: A realisation engine for practical applications", "author": ["A. Gatt", "E. Reiter"], "venue": "Proceedings of the 12th European Workshop on Natural Language Generation, ENLG \u201909, pages 90\u201393, Stroudsburg, PA, USA,", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2009}, {"title": "Rich feature hierarchies for accurate object detection and semantic segmentation", "author": ["R. Girshick", "J. Donahue", "T. Darrell", "J. Malik"], "venue": "Computer Vision and Pattern Recognition,", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2014}, {"title": "Objects in action: An approach for combining action understanding and object perception", "author": ["A. Gupta", "L.S. Davis"], "venue": "Computer Vision and Pattern Recognition, 2007. CVPR\u201907. IEEE Conference on, pages 1\u20138. IEEE,", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2007}, {"title": "Conceptnet 3: a flexible, multilingual semantic network for common sense knowledge", "author": ["C. Havasi", "R. Speer", "J. Alonso"], "venue": "Recent advances in natural language processing, pages 27\u201329. Citeseer,", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2007}, {"title": "Framing image description as a ranking task: Data, models and evaluation metrics", "author": ["M. Hodosh", "P. Young", "J. Hockenmaier"], "venue": "Journal of Artificial Intelligence Research, pages 853\u2013899,", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2013}, {"title": "Image retrieval using scene graphs", "author": ["J. Johnson", "R. Krishna", "M. Stark", "J. Li", "M. Bernstein", "L. Fei-Fei"], "venue": "IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2015}, {"title": "Deep visual-semantic alignments for generating image descriptions", "author": ["A. Karpathy", "F.-F. Li"], "venue": "arXiv preprint arXiv:1412.2306,", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2014}, {"title": "Unifying visual-semantic embeddings with multimodal neural language models", "author": ["R. Kiros", "R. Salakhutdinov", "R.S. Zemel"], "venue": "arXiv preprint arXiv:1411.2539,", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2014}, {"title": "Imagenet classification with deep convolutional neural networks", "author": ["A. Krizhevsky", "I. Sutskever", "G. Hinton"], "venue": "NIPS 2012,", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2013}, {"title": "Baby talk: Understanding and generating image descriptions", "author": ["G. Kulkarni", "V. Premraj", "S. Dhar", "S. Li", "Y. Choi", "A.C. Berg", "T.L. Berg"], "venue": "Proceedings of the 24th CVPR,", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2011}, {"title": "Collective generation of natural image descriptions", "author": ["P. Kuznetsova", "V. Ordonez", "A.C. Berg", "T.L. Berg", "Y. Choi"], "venue": "Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics: Long Papers - Volume 1, ACL \u201912, pages 359\u2013368, Stroudsburg, PA, USA,", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2012}, {"title": "Learning to detect unseen object classes by between-class attribute transfer", "author": ["C.H. Lampert", "H. Nickisch", "S. Harmeling"], "venue": "Computer Vision and Pattern Recognition, 2009. CVPR 2009. IEEE Conference on, pages 951\u2013958. IEEE,", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2009}, {"title": "On space-time interest points", "author": ["I. Laptev"], "venue": "International Journal of Computer Vision, 64(2-3):107\u2013123,", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2005}, {"title": "Cyc: A large-scale investment in knowledge infrastructure", "author": ["D.B. Lenat"], "venue": "Commun. ACM, 38(11):33\u201338, Nov.", "citeRegEx": "28", "shortCiteRegEx": null, "year": 1995}, {"title": "An information-theoretic definition of similarity", "author": ["D. Lin"], "venue": "ICML, volume 98, pages 296\u2013304,", "citeRegEx": "29", "shortCiteRegEx": null, "year": 1998}, {"title": "Object recognition from local scale-invariant features", "author": ["D.G. Lowe"], "venue": "Computer vision, 1999. The proceedings of the seventh IEEE international conference on, volume 2, pages 1150\u20131157. Ieee,", "citeRegEx": "30", "shortCiteRegEx": null, "year": 1999}, {"title": "Explain images with multimodal recurrent neural networks", "author": ["J. Mao", "W. Xu", "Y. Yang", "J. Wang", "A.L. Yuille"], "venue": "arXiv preprint arXiv:1410.1090,", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2014}, {"title": "Activity recognition using the velocity histories of tracked keypoints", "author": ["R. Messing", "C. Pal", "H. Kautz"], "venue": "Computer Vision, 2009 IEEE 12th International Conference on, pages 104\u2013111. IEEE,", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2009}, {"title": "Viewinvariant modeling and recognition of human actions using grammars", "author": ["A.S. Ogale", "A. Karapurkar", "Y. Aloimonos"], "venue": "R. Vidal, A. Heyden, and Y. Ma, editors, WDV, volume 4358 of Lecture Notes in Computer Science, pages 115\u2013126. Springer,", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2006}, {"title": "Im2text: Describing images using 1 million captioned photographs", "author": ["V. Ordonez", "G. Kulkarni", "T.L. Berg"], "venue": "J. Shawe-Taylor, R. S. Zemel, P. L. Bartlett, F. C. N. Pereira, and K. Q. Weinberger, editors, NIPS, pages 1143\u20131151,", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2011}, {"title": "Cnn features off-the-shelf: an astounding baseline for recognition", "author": ["A.S. Razavian", "H. Azizpour", "J. Sullivan", "S. Carlsson"], "venue": "Computer Vision and Pattern Recognition Workshops (CVPRW), 2014 IEEE Conference on, pages 512\u2013519. IEEE,", "citeRegEx": "35", "shortCiteRegEx": null, "year": 2014}, {"title": "Common-Sense Knowledge for a Computer Vision System for Human Action Recognition", "author": ["M. Santofimia", "J. Martinez-del Rincon", "J.-C. Nebel"], "venue": "J. Bravo, R. Herv\u00e1s, and M. Rodr\u0131\u0301guez, editors, Ambient Assisted Living and Home Care, volume 7657 of Lecture Notes in Computer Science, pages 159\u2013166. Springer Berlin Heidelberg,", "citeRegEx": "36", "shortCiteRegEx": null, "year": 2012}, {"title": "Generating semantically precise scene graphs from textual descriptions for improved image retrieval", "author": ["S. Schuster", "R. Krishna", "A. Chang", "L. Fei-Fei", "C.D. Manning"], "venue": "Proceedings of the Fourth Workshop on Vision and Language, pages 70\u201380, Lisbon, Portugal, September", "citeRegEx": "37", "shortCiteRegEx": null, "year": 2015}, {"title": "Learning bayesian networks with the bnlearn R package", "author": ["M. Scutari"], "venue": "Journal of Statistical Software, 35(3):1\u201322,", "citeRegEx": "38", "shortCiteRegEx": null, "year": 2010}, {"title": "Towards addressing the winograd schema challenge - building and using a semantic parser and a knowledge hunting module", "author": ["A. Sharma", "N.H. Vo", "S. Aditya", "C. Baral"], "venue": "Proceedings of the Twenty-Fourth International Joint Conference on Artificial Intelligence, IJCAI 2015, Buenos Aires, Argentina, July 25-31, 2015, pages 1319\u20131325,", "citeRegEx": "39", "shortCiteRegEx": null, "year": 2015}, {"title": "Grounded compositional semantics for finding and describing images with sentences", "author": ["R. Socher", "A. Karpathy", "Q.V. Le", "C.D. Manning", "A.Y. Ng"], "venue": "TACL, 2:207\u2013218,", "citeRegEx": "40", "shortCiteRegEx": null, "year": 2014}, {"title": "HAL\u2019s Legacy: 2001\u2019s Computer as Dream and Reality", "author": ["D.G. Stork"], "venue": "MIT Press,", "citeRegEx": "41", "shortCiteRegEx": null, "year": 1998}, {"title": "A gestaltist approach to contour-based object recognition: Combining bottom-up and top-down cues", "author": ["C.L. Teo", "C. Ferm\u00fcller", "Y. Aloimonos"], "venue": "The International Journal of Robotics Research, page 0278364914558493,", "citeRegEx": "42", "shortCiteRegEx": null, "year": 2015}, {"title": "Embedding high-level information into low level vision: Efficient object search in clutter", "author": ["C.L. Teo", "A. Myers", "C. Fermuller", "Y. Aloimonos"], "venue": "Robotics and Automation (ICRA), 2013 IEEE International Conference on, pages 126\u2013 132. IEEE,", "citeRegEx": "43", "shortCiteRegEx": null, "year": 2013}, {"title": "Show and tell: A neural image caption generator", "author": ["O. Vinyals", "A. Toshev", "S. Bengio", "D. Erhan"], "venue": "arXiv preprint arXiv:1411.4555,", "citeRegEx": "44", "shortCiteRegEx": null, "year": 2014}, {"title": "Action recognition by dense trajectories", "author": ["H. Wang", "A. Klaser", "C. Schmid", "C.-L. Liu"], "venue": "Computer Vision and Pattern Recognition (CVPR), 2011 IEEE Conference on, pages 3169\u20133176. IEEE,", "citeRegEx": "45", "shortCiteRegEx": null, "year": 2011}, {"title": "A cognitive system for understanding human manipulation actions", "author": ["Y. Yang", "C. Ferm\u00fcller", "Y. Aloimonos", "A. Guha"], "venue": "Advances in Cognitive Systems, 3:67\u201386,", "citeRegEx": "46", "shortCiteRegEx": null, "year": 2014}, {"title": "Corpus-guided sentence generation of natural images", "author": ["Y. Yang", "C.L. Teo", "III H. Daum\u00e9", "Y. Aloimonos"], "venue": "In Proceedings of the Conference on Empirical Methods in Natural Language Processing,", "citeRegEx": "47", "shortCiteRegEx": "47", "year": 2011}, {"title": "I2t: Image parsing to text description", "author": ["B.Z. Yao", "X. Yang", "L. Lin", "M.W. Lee", "S.C. Zhu"], "venue": "Proceedings of the IEEE, 98(8):1485\u20131508,", "citeRegEx": "48", "shortCiteRegEx": null, "year": 2010}, {"title": "Attribute-based transfer learning for object categorization with zero/one training example", "author": ["X. Yu", "Y. Aloimonos"], "venue": "Computer Vision\u2013ECCV 2010, pages 127\u2013140. Springer Berlin Heidelberg,", "citeRegEx": "49", "shortCiteRegEx": null, "year": 2010}, {"title": "Active scene recognition with vision and language", "author": ["X. Yu", "C. Fermuller", "C.L. Teo", "Y. Yang", "Y. Aloimonos"], "venue": "Computer Vision (ICCV), 2011 IEEE International Conference on, pages 810\u2013817. IEEE,", "citeRegEx": "50", "shortCiteRegEx": null, "year": 2011}, {"title": "Learning deep features for scene recognition using places database", "author": ["B. Zhou", "A. Lapedriza", "J. Xiao", "A. Torralba", "A. Oliva"], "venue": "NIPS,", "citeRegEx": "51", "shortCiteRegEx": null, "year": 2014}, {"title": "Bringing semantics into focus using visual abstraction", "author": ["C.L. Zitnick", "D. Parikh"], "venue": "CVPR, pages 3009\u20133016. IEEE,", "citeRegEx": "52", "shortCiteRegEx": null, "year": 2013}], "referenceMentions": [{"referenceID": 39, "context": "Rosenfeld [41], one of the founders of the field of Computer Vision, pointed to the fundamental problem of generating semantics of visual scenes.", "startOffset": 10, "endOffset": 14}, {"referenceID": 5, "context": "1Commonsense reasoning and commonsense knowledge can be of many types [6].", "startOffset": 70, "endOffset": 73}, {"referenceID": 16, "context": "Commonsense knowledge can belong to different levels of abstraction [18, 28].", "startOffset": 68, "endOffset": 76}, {"referenceID": 26, "context": "Commonsense knowledge can belong to different levels of abstraction [18, 28].", "startOffset": 68, "endOffset": 76}, {"referenceID": 28, "context": "In this methodology, scenes are recognized by detecting the inside objects [30, 5, 23], objects are recognized by detecting their parts or attributes [13, 26, 11, 49, 42, 43, 50] and activities are recognized by detecting the motions, objects and contexts involved in the activities [27, 32, 45, 17, 33, 46].", "startOffset": 75, "endOffset": 86}, {"referenceID": 4, "context": "In this methodology, scenes are recognized by detecting the inside objects [30, 5, 23], objects are recognized by detecting their parts or attributes [13, 26, 11, 49, 42, 43, 50] and activities are recognized by detecting the motions, objects and contexts involved in the activities [27, 32, 45, 17, 33, 46].", "startOffset": 75, "endOffset": 86}, {"referenceID": 21, "context": "In this methodology, scenes are recognized by detecting the inside objects [30, 5, 23], objects are recognized by detecting their parts or attributes [13, 26, 11, 49, 42, 43, 50] and activities are recognized by detecting the motions, objects and contexts involved in the activities [27, 32, 45, 17, 33, 46].", "startOffset": 75, "endOffset": 86}, {"referenceID": 12, "context": "In this methodology, scenes are recognized by detecting the inside objects [30, 5, 23], objects are recognized by detecting their parts or attributes [13, 26, 11, 49, 42, 43, 50] and activities are recognized by detecting the motions, objects and contexts involved in the activities [27, 32, 45, 17, 33, 46].", "startOffset": 150, "endOffset": 178}, {"referenceID": 24, "context": "In this methodology, scenes are recognized by detecting the inside objects [30, 5, 23], objects are recognized by detecting their parts or attributes [13, 26, 11, 49, 42, 43, 50] and activities are recognized by detecting the motions, objects and contexts involved in the activities [27, 32, 45, 17, 33, 46].", "startOffset": 150, "endOffset": 178}, {"referenceID": 10, "context": "In this methodology, scenes are recognized by detecting the inside objects [30, 5, 23], objects are recognized by detecting their parts or attributes [13, 26, 11, 49, 42, 43, 50] and activities are recognized by detecting the motions, objects and contexts involved in the activities [27, 32, 45, 17, 33, 46].", "startOffset": 150, "endOffset": 178}, {"referenceID": 47, "context": "In this methodology, scenes are recognized by detecting the inside objects [30, 5, 23], objects are recognized by detecting their parts or attributes [13, 26, 11, 49, 42, 43, 50] and activities are recognized by detecting the motions, objects and contexts involved in the activities [27, 32, 45, 17, 33, 46].", "startOffset": 150, "endOffset": 178}, {"referenceID": 40, "context": "In this methodology, scenes are recognized by detecting the inside objects [30, 5, 23], objects are recognized by detecting their parts or attributes [13, 26, 11, 49, 42, 43, 50] and activities are recognized by detecting the motions, objects and contexts involved in the activities [27, 32, 45, 17, 33, 46].", "startOffset": 150, "endOffset": 178}, {"referenceID": 41, "context": "In this methodology, scenes are recognized by detecting the inside objects [30, 5, 23], objects are recognized by detecting their parts or attributes [13, 26, 11, 49, 42, 43, 50] and activities are recognized by detecting the motions, objects and contexts involved in the activities [27, 32, 45, 17, 33, 46].", "startOffset": 150, "endOffset": 178}, {"referenceID": 48, "context": "In this methodology, scenes are recognized by detecting the inside objects [30, 5, 23], objects are recognized by detecting their parts or attributes [13, 26, 11, 49, 42, 43, 50] and activities are recognized by detecting the motions, objects and contexts involved in the activities [27, 32, 45, 17, 33, 46].", "startOffset": 150, "endOffset": 178}, {"referenceID": 25, "context": "In this methodology, scenes are recognized by detecting the inside objects [30, 5, 23], objects are recognized by detecting their parts or attributes [13, 26, 11, 49, 42, 43, 50] and activities are recognized by detecting the motions, objects and contexts involved in the activities [27, 32, 45, 17, 33, 46].", "startOffset": 283, "endOffset": 307}, {"referenceID": 30, "context": "In this methodology, scenes are recognized by detecting the inside objects [30, 5, 23], objects are recognized by detecting their parts or attributes [13, 26, 11, 49, 42, 43, 50] and activities are recognized by detecting the motions, objects and contexts involved in the activities [27, 32, 45, 17, 33, 46].", "startOffset": 283, "endOffset": 307}, {"referenceID": 43, "context": "In this methodology, scenes are recognized by detecting the inside objects [30, 5, 23], objects are recognized by detecting their parts or attributes [13, 26, 11, 49, 42, 43, 50] and activities are recognized by detecting the motions, objects and contexts involved in the activities [27, 32, 45, 17, 33, 46].", "startOffset": 283, "endOffset": 307}, {"referenceID": 15, "context": "In this methodology, scenes are recognized by detecting the inside objects [30, 5, 23], objects are recognized by detecting their parts or attributes [13, 26, 11, 49, 42, 43, 50] and activities are recognized by detecting the motions, objects and contexts involved in the activities [27, 32, 45, 17, 33, 46].", "startOffset": 283, "endOffset": 307}, {"referenceID": 31, "context": "In this methodology, scenes are recognized by detecting the inside objects [30, 5, 23], objects are recognized by detecting their parts or attributes [13, 26, 11, 49, 42, 43, 50] and activities are recognized by detecting the motions, objects and contexts involved in the activities [27, 32, 45, 17, 33, 46].", "startOffset": 283, "endOffset": 307}, {"referenceID": 44, "context": "In this methodology, scenes are recognized by detecting the inside objects [30, 5, 23], objects are recognized by detecting their parts or attributes [13, 26, 11, 49, 42, 43, 50] and activities are recognized by detecting the motions, objects and contexts involved in the activities [27, 32, 45, 17, 33, 46].", "startOffset": 283, "endOffset": 307}, {"referenceID": 29, "context": "Current developments [31, 22, 8, 21, 44, 3] in Computer Vision have shown that deep neural nets can be trained to generate a caption for an arbitrary scene with decent success.", "startOffset": 21, "endOffset": 43}, {"referenceID": 20, "context": "Current developments [31, 22, 8, 21, 44, 3] in Computer Vision have shown that deep neural nets can be trained to generate a caption for an arbitrary scene with decent success.", "startOffset": 21, "endOffset": 43}, {"referenceID": 7, "context": "Current developments [31, 22, 8, 21, 44, 3] in Computer Vision have shown that deep neural nets can be trained to generate a caption for an arbitrary scene with decent success.", "startOffset": 21, "endOffset": 43}, {"referenceID": 19, "context": "Current developments [31, 22, 8, 21, 44, 3] in Computer Vision have shown that deep neural nets can be trained to generate a caption for an arbitrary scene with decent success.", "startOffset": 21, "endOffset": 43}, {"referenceID": 42, "context": "Current developments [31, 22, 8, 21, 44, 3] in Computer Vision have shown that deep neural nets can be trained to generate a caption for an arbitrary scene with decent success.", "startOffset": 21, "endOffset": 43}, {"referenceID": 2, "context": "Current developments [31, 22, 8, 21, 44, 3] in Computer Vision have shown that deep neural nets can be trained to generate a caption for an arbitrary scene with decent success.", "startOffset": 21, "endOffset": 43}, {"referenceID": 19, "context": "Figure 1: Examples from [21]: (a) Positive example annotation: construction worker in orange safety vest is working on road, (b) Negative example annotation: a bunch of bananas are hanging from a ceiling.", "startOffset": 24, "endOffset": 28}, {"referenceID": 1, "context": "2Throughout this paper, we follow definition of Entities and Events from [16, 2].", "startOffset": 73, "endOffset": 80}, {"referenceID": 19, "context": "As [21] suggests, such works can be categorized into 1) dense image annotations, 2) generating textual descriptions, 3) grounding natural language in images and 4) neural networks in visual and language domains.", "startOffset": 3, "endOffset": 7}, {"referenceID": 17, "context": "This includes the works that retrieves and ranks sentences from training sets given an image such as [19], [12],[34], [40].", "startOffset": 101, "endOffset": 105}, {"referenceID": 11, "context": "This includes the works that retrieves and ranks sentences from training sets given an image such as [19], [12],[34], [40].", "startOffset": 107, "endOffset": 111}, {"referenceID": 32, "context": "This includes the works that retrieves and ranks sentences from training sets given an image such as [19], [12],[34], [40].", "startOffset": 112, "endOffset": 116}, {"referenceID": 38, "context": "This includes the works that retrieves and ranks sentences from training sets given an image such as [19], [12],[34], [40].", "startOffset": 118, "endOffset": 122}, {"referenceID": 9, "context": "[10], [24], [25], [47], [48] are some of the works that have generated descriptions by stitching together annotations or applying templates on detected image content.", "startOffset": 0, "endOffset": 4}, {"referenceID": 22, "context": "[10], [24], [25], [47], [48] are some of the works that have generated descriptions by stitching together annotations or applying templates on detected image content.", "startOffset": 6, "endOffset": 10}, {"referenceID": 23, "context": "[10], [24], [25], [47], [48] are some of the works that have generated descriptions by stitching together annotations or applying templates on detected image content.", "startOffset": 12, "endOffset": 16}, {"referenceID": 45, "context": "[10], [24], [25], [47], [48] are some of the works that have generated descriptions by stitching together annotations or applying templates on detected image content.", "startOffset": 18, "endOffset": 22}, {"referenceID": 46, "context": "[10], [24], [25], [47], [48] are some of the works that have generated descriptions by stitching together annotations or applying templates on detected image content.", "startOffset": 24, "endOffset": 28}, {"referenceID": 50, "context": "[52] uses abstraction to discover semantically similar images.", "startOffset": 0, "endOffset": 4}, {"referenceID": 6, "context": "[7] proposes to learn all variations pertaining to all concepts and [36] uses common-sense to learn actions.", "startOffset": 0, "endOffset": 3}, {"referenceID": 34, "context": "[7] proposes to learn all variations pertaining to all concepts and [36] uses common-sense to learn actions.", "startOffset": 68, "endOffset": 72}, {"referenceID": 18, "context": "Recently, [20] introduced scene graphs to describe scenes and [37] creates scene graphs from descriptions.", "startOffset": 10, "endOffset": 14}, {"referenceID": 35, "context": "Recently, [20] introduced scene graphs to describe scenes and [37] creates scene graphs from descriptions.", "startOffset": 62, "endOffset": 66}, {"referenceID": 3, "context": "However, we automatically construct the graph from an image, and we believe, due to the event-entity-attribute based representation and meaningful edge-labels (borrowed from KM-ontology[4]) , SDGs are more equipped to facilitate symbolic-level reasoning.", "startOffset": 185, "endOffset": 188}, {"referenceID": 0, "context": "Different from the traditional hand-crafted features, a multilayer neural network architecture efficiently captures sophisticated hierarchies describing the raw data [1], which has shown superior performance on standard scene recognition [51], object recognition [15] and image captioning [21] benchmarks.", "startOffset": 166, "endOffset": 169}, {"referenceID": 49, "context": "Different from the traditional hand-crafted features, a multilayer neural network architecture efficiently captures sophisticated hierarchies describing the raw data [1], which has shown superior performance on standard scene recognition [51], object recognition [15] and image captioning [21] benchmarks.", "startOffset": 238, "endOffset": 242}, {"referenceID": 14, "context": "Different from the traditional hand-crafted features, a multilayer neural network architecture efficiently captures sophisticated hierarchies describing the raw data [1], which has shown superior performance on standard scene recognition [51], object recognition [15] and image captioning [21] benchmarks.", "startOffset": 263, "endOffset": 267}, {"referenceID": 19, "context": "Different from the traditional hand-crafted features, a multilayer neural network architecture efficiently captures sophisticated hierarchies describing the raw data [1], which has shown superior performance on standard scene recognition [51], object recognition [15] and image captioning [21] benchmarks.", "startOffset": 289, "endOffset": 293}, {"referenceID": 17, "context": "Image Dataset: In this paper, we use three image data sets, which are popularly referred to as Flickr 8k, Flickr 30k and Coco datasets [19].", "startOffset": 135, "endOffset": 139}, {"referenceID": 19, "context": "For all datasets, we used the train-test splits from [21] and the 4000 testing images (1000 each from Flickr 8k and 30k; 2000 from MS-COCO validation set; denoted as I) serve as the testing bed for our reasoning experiments.", "startOffset": 53, "endOffset": 57}, {"referenceID": 14, "context": "Deep Object Recognition: We use the trained bottom-up region proposals and convolutional neural networks(CNN) object detection method from [15].", "startOffset": 139, "endOffset": 143}, {"referenceID": 49, "context": "Deep Scene Recognition: We use the trained CNN scene classification method from [51].", "startOffset": 80, "endOffset": 84}, {"referenceID": 8, "context": "Recent empirical results from a diverse range of visual recognition tasks indicate that the generic descriptors extracted from the CNN are very powerful [9, 35].", "startOffset": 153, "endOffset": 160}, {"referenceID": 33, "context": "Recent empirical results from a diverse range of visual recognition tasks indicate that the generic descriptors extracted from the CNN are very powerful [9, 35].", "startOffset": 153, "endOffset": 160}, {"referenceID": 21, "context": "In this work, we use a pre-trained CNN from [23].", "startOffset": 44, "endOffset": 48}, {"referenceID": 8, "context": "For each image in I , we use this pre-trained model to extract a 4096 dimensional feature vector using [9, 23].", "startOffset": 103, "endOffset": 110}, {"referenceID": 21, "context": "For each image in I , we use this pre-trained model to extract a 4096 dimensional feature vector using [9, 23].", "startOffset": 103, "endOffset": 110}, {"referenceID": 37, "context": "In Figure 4, we describe how we construct Kb from a set of Image Annotations (Ad) using the Stanford Parser and K-Parser [39].", "startOffset": 121, "endOffset": 125}, {"referenceID": 1, "context": "6The idea behind the representation of a Concept is inspired by that of a Process in AURA [2].", "startOffset": 90, "endOffset": 93}, {"referenceID": 3, "context": "The K-parser then maps these dependency labels using a set of rules to a set of meaningful labels from KM-Ontology[4] and the resulting graph is further augmented using ontological and semantic information from different sources (more details on kparser.", "startOffset": 114, "endOffset": 117}, {"referenceID": 36, "context": "package [38].", "startOffset": 8, "endOffset": 12}, {"referenceID": 13, "context": "Template Based Sentence Generation: We generate textual descriptions from the SDG using the SimpleNLG[14] package.", "startOffset": 101, "endOffset": 105}, {"referenceID": 19, "context": "We perform a qualitative evaluation (\u201crelevance\u201d and \u201cthoroughness\u201d) of the textual descriptions generated from SDGs with the sentences generated by [21] using the Amazon Mechanical Turkers (AMT).", "startOffset": 149, "endOffset": 153}, {"referenceID": 19, "context": "For comparison purposes, we use the implementation from [21] to generate a textual caption S for each testing image.", "startOffset": 56, "endOffset": 60}, {"referenceID": 19, "context": "Subsequently, we compared the gold-standard with entity-event set from [21] and the SDG output from our system for each image.", "startOffset": 71, "endOffset": 75}, {"referenceID": 19, "context": "Type Accuracy-SDG(%) Precision-SDG(%) Accuracy(%)[21] Precision(%)[21]", "startOffset": 49, "endOffset": 53}, {"referenceID": 19, "context": "Type Accuracy-SDG(%) Precision-SDG(%) Accuracy(%)[21] Precision(%)[21]", "startOffset": 66, "endOffset": 70}, {"referenceID": 19, "context": "For comparison, we asked the AMTs to also judge one random gold-standard description and the output from [21], a state-of-the-art image captioning system.", "startOffset": 105, "endOffset": 109}, {"referenceID": 19, "context": "Experiment [21] Our Method Gold Standard R \u00b1 D(8k) 2.", "startOffset": 11, "endOffset": 15}, {"referenceID": 19, "context": "Table 2: Sentence generation relevance (R) and thoroughness (T) human evaluation results with gold standard and [21] on Flickr 8k, 30k and MS-COCO datasets.", "startOffset": 112, "endOffset": 116}, {"referenceID": 19, "context": "Image-Sentence Alignment Evaluation: Similar to the experiments in [21, 20], we also evaluate the imagesentence alignment quality using ranking experiments.", "startOffset": 67, "endOffset": 75}, {"referenceID": 18, "context": "Image-Sentence Alignment Evaluation: Similar to the experiments in [21, 20], we also evaluate the imagesentence alignment quality using ranking experiments.", "startOffset": 67, "endOffset": 75}, {"referenceID": 27, "context": ") is WordNet-Lin Similarity [29] between two words and Jaccard(.", "startOffset": 28, "endOffset": 32}, {"referenceID": 19, "context": "Flickr8k Model R@1 R@5 R@10 Med r [21] BRNN 11.", "startOffset": 34, "endOffset": 38}, {"referenceID": 19, "context": "5 Flickr30k [21] BRNN 15.", "startOffset": 12, "endOffset": 16}, {"referenceID": 19, "context": "MS-COCO [21] BRNN (1k) 20.", "startOffset": 8, "endOffset": 12}, {"referenceID": 18, "context": "We should also mention that the concept-level modeling provided by SDGs is what separates this work from other recent approaches [20].", "startOffset": 129, "endOffset": 133}], "year": 2015, "abstractText": "In this paper we propose the construction of linguistic descriptions of images. This is achieved through the extraction of scene description graphs (SDGs) from visual scenes using an automatically constructed knowledge base. SDGs are constructed using both vision and reasoning. Specifically, commonsense reasoning1 is applied on (a) detections obtained from existing perception methods on given images, (b) a \u201ccommonsense\u201d knowledge base constructed using natural language processing of image annotations and (c) lexical ontological knowledge from resources such as WordNet. Amazon Mechanical Turk(AMT)-based evaluations on Flickr8k, Flickr30k and MS-COCO datasets show that in most cases, sentences auto-constructed from SDGs obtained by our method give a more relevant and thorough description of an image than a recent state-of-the-art image caption based approach. Our Image-Sentence Alignment Evaluation results are also comparable to that of the recent state-of-the art approaches.", "creator": "LaTeX with hyperref package"}}}