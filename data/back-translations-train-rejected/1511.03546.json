{"id": "1511.03546", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "11-Nov-2015", "title": "Hierarchical Latent Semantic Mapping for Automated Topic Generation", "abstract": "Much of information sits in an unprecedented amount of text data. Managing allocation of these large scale text data is an important problem for many areas. Topic modeling performs well in this problem. The traditional generative models (PLSA,LDA) are the state-of-the-art approaches in topic modeling and most recent research on topic generation has been focusing on improving or extending these models. However, results of traditional generative models are sensitive to the number of topics K, which must be specified manually. The problem of generating topics from corpus resembles community detection in networks. Many effective algorithms can automatically detect communities from networks without a manually specified number of the communities. Inspired by these algorithms, in this paper, we propose a novel method named Hierarchical Latent Semantic Mapping (HLSM), which automatically generates topics from corpus. HLSM calculates the association between each pair of words in the latent topic space, then constructs a unipartite network of words with this association and hierarchically generates topics from this network. We apply HLSM to several document collections and the experimental comparisons against several state-of-the-art approaches demonstrate the promising performance.", "histories": [["v1", "Wed, 11 Nov 2015 15:58:30 GMT  (513kb,D)", "https://arxiv.org/abs/1511.03546v1", "9 pages, 3 figures, Under Review as a conference at ICLR 2016. arXiv admin note: text overlap witharXiv:1010.0431,arXiv:0812.1242by other authors"], ["v2", "Mon, 16 Nov 2015 13:47:53 GMT  (513kb,D)", "http://arxiv.org/abs/1511.03546v2", "9 pages, 3 figures, Under Review as a conference at ICLR 2016"], ["v3", "Tue, 17 Nov 2015 05:23:58 GMT  (388kb,D)", "http://arxiv.org/abs/1511.03546v3", "9 pages, 3 figures, Under Review as a conference at ICLR 2016"], ["v4", "Thu, 26 Nov 2015 01:35:58 GMT  (388kb,D)", "http://arxiv.org/abs/1511.03546v4", "9 pages, 3 figures, Under Review as a conference at ICLR 2016"]], "COMMENTS": "9 pages, 3 figures, Under Review as a conference at ICLR 2016. arXiv admin note: text overlap witharXiv:1010.0431,arXiv:0812.1242by other authors", "reviews": [], "SUBJECTS": "cs.LG cs.CL cs.IR", "authors": ["guorui zhou", "guang chen"], "accepted": false, "id": "1511.03546"}, "pdf": {"name": "1511.03546.pdf", "metadata": {"source": "CRF", "title": null, "authors": ["Guorui Zhou", "Guang Chen"], "emails": ["chenguang}@bupt.edu.cn"], "sections": [{"heading": "1 INTRODUCTION", "text": "This year, it is more than ever before in the history of the city in which we find ourselves."}, {"heading": "2 HIERARCHICAL LATENT SEMANTIC MAPPING", "text": "Hierarchical Latin semantic mapping (HLSM) is a networking approach to topic modeling. Similar to the known topic models, each document is presented as a mixture of latent topics. The key feature that distinguishes the HLSM model from existing topic models is that HLSM directly clusters words and defines each cluster as a topic, then refines these initial topics, so HLSM estimates the probability distributions p (word | topic) in a novel process. The HLSM model includes topics such as the following steps: Step 1. Construct the universal network. We calculate the association between each word pair that occurs in at least one document. Then we construct the universal network in which words are associated with the association above the threshold. Step 2. Clusters of words in the hierarchy summarize the words in the universal structure."}, {"heading": "2.1 CONSTRUCT THE UNIPARTITE NETWORK", "text": "The association between words must be closely linked to the themes in order to ensure the validity of cluster words q > q pairs based on this network, but the themes are latent, and all observations are the words collected in documents. < If we artificially assign themes to documents with prior human knowledge, we can find that documents share the same themes, or rather share some words. Of course, we can believe that the words in many documents share the same theme, in another word these words are more similar in the latent theme space. To calculate the association between words in the latent theme space, the core idea of latent semantic analysis (LSI) is that we assign words to a vector space of reduced dimensionality based on a singular value decomposition (SVD) in the latent theme space."}, {"heading": "2.2 CLUSTERING WORDS HIERARCHICALLY", "text": "In fact, in a country where most people are able to understand the world, it is so that one sees oneself in a position to change, to change, to change, to change, to change, to change, to change, to change, to change, to change, to change, to change, to widen, to change, to change, to change, to change, to change, to change, to change, to change, to change, to change, to change, to change, to change, to change, to change, to change, to change, to change, to change, to change, to change, to change, to change, to change, to change, to change, to change, to change, to change, to change, to change, to change, to change."}, {"heading": "2.3 REFINE THE PRIOR GUESS", "text": "It is only a matter of time until there is an agreement. (...) It is only a matter of time until there is an agreement. (...) It is only a matter of time until there is an agreement. (...) It is a matter of time until there is an agreement. (...) It is a matter of time until we reach an agreement. (...) It is a matter of time until we find a solution. (...) It is a matter of time until we find a solution. (...) It is a matter of time until we find a solution. (...) It is a matter of time until we find a solution. (...) It is a matter of time until we find a solution. (...) It is a matter of time until we find a solution. (...) It is a question of time until we find a solution. (...) It is a question of time until we find a solution. (...) It is a question of time until we find a solution. (...) It is a question of time until we find a solution."}, {"heading": "3 EXPERIMENTAL EVALUATIONS", "text": "HLSM is a theme model for text collections that can be applied to many applications such as classification, clustering, filtering, information gathering, and related fields. Drawing on Ley's idea Lead & Jordan (2003), we examine two important applications in this section: document modeling and document classification."}, {"heading": "3.1 DOCUMENT MODELING", "text": "The goal of document modeling is to generalize the trained model from the training data set DA into a new data set 3878. The documents in the corpora are not labeled, our goal is to estimate the density, so we would like to obtain a high probability on a pre-set test. In particular, we have calculated the perplexity of a pre-set test to evaluate the models. Models that provide a lower perplexity are considered in order to achieve a better generalization performance, since the model is less surprised by a part of the data sets that the model has never seen before. Formally, the perplexity for a test set of M documents is defined as follows: Perplexity (Dtest) = exp {\u2212 imp M i = 1 logp (di) Theme of Disease M i = 1 Li} (10) We perform this experiment on a subset of the 20 newsgroups data sets that we use."}, {"heading": "3.2 DOCUMENT CLASSIFICATION", "text": "The selection of attributes is a difficult aspect of the document classification problem. By representing the documents in terms of latent theme space, the topic models can generate the probabilities p (t | d). To test the effectiveness of HLSM, we compared them with the following representative topic models and chose AC as the evaluation basis. PLSA, symmetrical LDA, asymmetrical LDA, TopicMapping. We generated six cross-domain text records from 20 newsgroups using their described structure. There are 4 fields in each data set, Table 3 summarizes the data sets generated from 20 newsgroups. To make the task more effective and convincing, the task was defined as multiplicative classification. (The choice of attributes is a challenging aspect of the document classification problem.)"}, {"heading": "4 CONCLUSION", "text": "We apply the HLSM model to multiple document collections for document modeling and document clustering, and experimental comparisons with modern approaches show the promising performance. Especially in the area of community detection, a considerable amount of work has been done on stochastic block models that attempt to adapt a model for detecting community structures in networks. We believe that this work, which is similar in spirit to the theme model, would provide new insights into topic modeling."}], "references": [{"title": "Cross-domain text classification using semantic based approach", "author": ["B.U.A. Barathi"], "venue": "In Sustainable Energy and Intelligent Systems (SEISCON", "citeRegEx": "Barathi,? \\Q2011\\E", "shortCiteRegEx": "Barathi", "year": 2011}, {"title": "Latent dirichlet allocation", "author": ["D.M. Blei", "A.Y. Ng", "M.I. Jordan"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Blei et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Blei et al\\.", "year": 2003}, {"title": "Probabilistic topic models", "author": ["D. Blei", "L. Carin", "D. Dunson"], "venue": "Signal Processing Magazine,", "citeRegEx": "Blei et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Blei et al\\.", "year": 2010}, {"title": "Dynamic topic models", "author": ["Blei", "David M", "Lafferty", "John D"], "venue": "In Proceedings of the 23rd International Conference on Machine Learning,", "citeRegEx": "Blei et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Blei et al\\.", "year": 2006}, {"title": "The nested chinese restaurant process and bayesian nonparametric inference of topic hierarchies", "author": ["Blei", "David M", "Griffiths", "Thomas L", "Jordan", "Michael I"], "venue": "J. ACM,", "citeRegEx": "Blei et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Blei et al\\.", "year": 2010}, {"title": "Using topic keyword clusters for automatic document clustering", "author": ["Chang", "Hsi-Cheng", "Hsu", "Chiun-Chieh"], "venue": "In Information Technology and Applications,", "citeRegEx": "Chang et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Chang et al\\.", "year": 2005}, {"title": "Finding scientific topics", "author": ["Griffiths", "Thomas L", "Steyvers", "Mark"], "venue": "Proceedings of the National Academy of Sciences,", "citeRegEx": "Griffiths et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Griffiths et al\\.", "year": 2004}, {"title": "Probabilistic latent semantic indexing", "author": ["T. Hofmann"], "venue": "SIGIR, pp", "citeRegEx": "Hofmann,? \\Q1999\\E", "shortCiteRegEx": "Hofmann", "year": 1999}, {"title": "High-reproducibility and high-accuracy method for automated topic classification", "author": ["Lancichinetti", "Andrea", "Sirer", "M. Irmak", "Wang", "Jane X", "Acuna", "Daniel", "K\u00f6rding", "Konrad", "Amaral", "Lu\u0131\u0301s A. Nunes"], "venue": "Phys. Rev. X,", "citeRegEx": "Lancichinetti et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Lancichinetti et al\\.", "year": 2015}, {"title": "Parallelized variational em for latent dirichlet allocation: An experimental evaluation of speed and scalability", "author": ["Nallapati", "Ramesh", "Cohen", "William", "J. Lafferty"], "venue": "In Data Mining Workshops,", "citeRegEx": "Nallapati et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Nallapati et al\\.", "year": 2007}, {"title": "Multilevel compression of random walks on networks reveals hierarchical organization in large integrated systems", "author": ["Rosvall", "Martin", "Bergstrom", "Carl T"], "venue": "PLoS ONE, 6(4):e18209,", "citeRegEx": "Rosvall et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Rosvall et al\\.", "year": 2011}, {"title": "Rethinking lda: Why priors matter", "author": ["Wallach", "Hanna M", "Mimno", "David M", "McCallum", "Andrew"], "venue": "Advances in Neural Information Processing Systems", "citeRegEx": "Wallach et al\\.,? \\Q1981\\E", "shortCiteRegEx": "Wallach et al\\.", "year": 1981}], "referenceMentions": [{"referenceID": 7, "context": "LDA and PLSI both aim to estimate the values of these probabilities with the highest likelihood of generating the corpus (Hofmann, 1999; Blei & Jordan, 2003; Griffiths & Steyvers, 2004; Nallapati et al., 2007).", "startOffset": 121, "endOffset": 209}, {"referenceID": 9, "context": "LDA and PLSI both aim to estimate the values of these probabilities with the highest likelihood of generating the corpus (Hofmann, 1999; Blei & Jordan, 2003; Griffiths & Steyvers, 2004; Nallapati et al., 2007).", "startOffset": 121, "endOffset": 209}, {"referenceID": 0, "context": "Based on these topics we can solve problems on cross-domain text classification Barathi (2011), understanding text clustering Chang & Hsu (2005), text recommendation, and other related text data applications.", "startOffset": 80, "endOffset": 95}, {"referenceID": 0, "context": "Based on these topics we can solve problems on cross-domain text classification Barathi (2011), understanding text clustering Chang & Hsu (2005), text recommendation, and other related text data applications.", "startOffset": 80, "endOffset": 145}, {"referenceID": 0, "context": "Based on these topics we can solve problems on cross-domain text classification Barathi (2011), understanding text clustering Chang & Hsu (2005), text recommendation, and other related text data applications. There has been an exceptional amount of research on topic-model algorithms. PLSA and LDA are highly modular and can therefore be easily extended. Since LDA\u2019s introduction, there is much research based on it. The Correlated Topic Model Advances Blei & Lafferty (2006) follows this approach, inducing a correlation structure between topics by using the logistic normal distribution instead of the Dirichlet.", "startOffset": 80, "endOffset": 476}, {"referenceID": 0, "context": "Based on these topics we can solve problems on cross-domain text classification Barathi (2011), understanding text clustering Chang & Hsu (2005), text recommendation, and other related text data applications. There has been an exceptional amount of research on topic-model algorithms. PLSA and LDA are highly modular and can therefore be easily extended. Since LDA\u2019s introduction, there is much research based on it. The Correlated Topic Model Advances Blei & Lafferty (2006) follows this approach, inducing a correlation structure between topics by using the logistic normal distribution instead of the Dirichlet. Another extension is the hierarchical LDA Blei et al. (2010b), where topics are joined together in a hierarchy by using the nested Chinese restaurant process.", "startOffset": 80, "endOffset": 677}, {"referenceID": 8, "context": "A research on the validity of LDA optimization algorithms for inferring topic models also proposed that current implementations of LDA had low validity (Lancichinetti et al., 2015).", "startOffset": 152, "endOffset": 180}, {"referenceID": 1, "context": "likelihood,as has been previously reported Blei et al. (2010a); Wallach et al.", "startOffset": 43, "endOffset": 63}, {"referenceID": 1, "context": "likelihood,as has been previously reported Blei et al. (2010a); Wallach et al. (2009). A research on the validity of LDA optimization algorithms for inferring topic models also proposed that current implementations of LDA had low validity (Lancichinetti et al.", "startOffset": 43, "endOffset": 86}], "year": 2015, "abstractText": "Much of information sits in an unprecedented amount of text data. Managing allocation of these large scale text data is an important problem for many areas. Topic modeling performs well in this problem. The traditional generative models (PLSA,LDA) are the state-of-the-art approaches in topic modeling and most recent research on topic generation has been focusing on improving or extending these models. However, results of traditional generative models are sensitive to the number of topics K, which must be specified manually and determines the rank of solution space for topic generation. The problem of generating topics from corpus resembles community detection in networks. Many effective algorithms can automatically detect communities from networks without a manually specified number of the communities. Inspired by these algorithms, in this paper, we propose a novel method named Hierarchical Latent Semantic Mapping (HLSM), which automatically generates topics from corpus. HLSM calculates the association between each pair of words in the latent topic space, then constructs a unipartite network of words with this association and hierarchically generates topics from this network. We apply HLSM to several document collections and the experimental comparisons against several state-of-the-art approaches demonstrate the promising performance.", "creator": "LaTeX with hyperref package"}}}