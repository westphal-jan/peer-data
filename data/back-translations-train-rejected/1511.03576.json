{"id": "1511.03576", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "11-Nov-2015", "title": "DataGrinder: Fast, Accurate, Fully non-Parametric Classification Approach Using 2D Convex Hulls", "abstract": "It has been a long time, since data mining technologies have made their ways to the field of data management. Classification is one of the most important data mining tasks for label prediction, categorization of objects into groups, advertisement and data management. In this paper, we focus on the standard classification problem which is predicting unknown labels in Euclidean space. Most efforts in Machine Learning communities are devoted to methods that use probabilistic algorithms which are heavy on Calculus and Linear Algebra. Most of these techniques have scalability issues for big data, and are hardly parallelizable if they are to maintain their high accuracies in their standard form. Sampling is a new direction for improving scalability, using many small parallel classifiers. In this paper, rather than conventional sampling methods, we focus on a discrete classification algorithm with O(n) expected running time. Our approach performs a similar task as sampling methods. However, we use column-wise sampling of data, rather than the row-wise sampling used in the literature. In either case, our algorithm is completely deterministic. Our algorithm, proposes a way of combining 2D convex hulls in order to achieve high classification accuracy as well as scalability in the same time. First, we thoroughly describe and prove our O(n) algorithm for finding the convex hull of a point set in 2D. Then, we show with experiments our classifier model built based on this idea is very competitive compared with existing sophisticated classification algorithms included in commercial statistical applications such as MATLAB.", "histories": [["v1", "Wed, 11 Nov 2015 17:06:35 GMT  (697kb,D)", "http://arxiv.org/abs/1511.03576v1", null]], "reviews": [], "SUBJECTS": "cs.DB cs.CG cs.LG", "authors": ["mohammad khabbaz"], "accepted": false, "id": "1511.03576"}, "pdf": {"name": "1511.03576.pdf", "metadata": {"source": "CRF", "title": "DataGrinder: Fast, Accurate, Fully non-Parametric Classification Approach Using 2D Convex Hulls", "authors": ["Mohammad Khabbaz"], "emails": ["mohammmad@gmail.com"], "sections": [{"heading": "1. INTRODUCTION", "text": "We have a small example where we have points in a level, each of which has a different category. A classified model is given data in two categories. We have a small example where we have a series of points in a level, each of which has a different category. We have a classified model. We have a small example where we have a small example where we have a series of points in a level, each of which has a different category. We have an example that there is another category. We have a lot of work on classifications as one of the most important techniques for supervised learning. Figure 1, shows a small example where we have points in a level, each of which has a different category."}, {"heading": "2. CONVEX HULL BACKGROUND", "text": "In fact, most people who are able to realize themselves also do it as if they were doing it. (...) It's not as if they were doing it. (...) It's as if they were doing it. (...) It's as if they were doing it. (...) It's as if they were doing it. (...) It's as if they were doing it. \"(...) It's as if they were doing it. (...) It's as if they were doing it. (...) It's as if they were doing it. (...) It's as if they were doing it.\" (...) It's as if they were doing it. \"(...) It's as if they were doing it.\" (...) It's as if they were doing it."}, {"heading": "3. FINDING CONVEX HULL BY CANDIDATE ELIMINATION", "text": "We know that, in the worst-case scenario, we cannot do better in order to achieve a better runtime. In this section, we describe a process called \"candidate elimination,\" which we use instead of sorting. We use candidate elimination along with existing FindUpperHull procedures to resolve the problem. The idea is to avoid sorting to maintain candidate lists for various parts of the convex hull and find the next minimum from a smaller candidate list, rather than paying O (hull) for sorting in the initial phase. Figure 5, divides the plane as well as the convex hull-hull-hull-hull-hull-hull-hull-hull-hull-hull-hull-hull-hull-hull-hull-hull-hull-hull-hull-hull-hull-hull-hull-hull-hull-hull-hull-hull-hull-hull-hull-hull-hull-hull-hull-hull-hull-hull-hull-sorting-hull-hull-hull-hull-hull-hull-hull-hull-hull-hull-hull-hull-hull-hull-hull-hull-hull-hull-hull-hull-hull-hull-hull-hull-hull-hull-hull-hull-hull-hull-hull-hull-hull-hull-hull-hull-hull-hull-hull-hull-hull-hull-hull-hull-hull-hull-hull-hull-elimination-hull-elimination-hull-elimination-elimination-hullingly-elimination-elimination-elimination-elimination-elimination-elimination-elimination-elimination-elimination-elimination-elimination-elimination-elimination-elimination-elimination-elimination-elimination-elimination-elimination-elimination-elimination-elimination-elimination-elimination-elimination-elimination-elimination-elimination-elimination-elimination-elimination-elim"}, {"heading": "3.1 Convex Hull Algorithm", "text": "The first step to eliminating candidates reduces the expected number of candidates by half. Although this is a good heuristics, we still need to eliminate more candidates and find the right convex hull. As described above, we do this in 4 smaller steps for UpperLeftHull, UpperRightHull, LowerLeftHull and LowerRightHull separately. Here, we only describe the process for UpperLeftHull, and we know that the rest is symmetrical for the other three quarters of the convex hull. Algorithm 2 takes as input the list of upper left candidates after the first candidate elimination, which are on or above the line from Xmin to Ymax. Please note that the list of candidates is no longer sorted by x coordinates. The idea is to avoid sorting the candidate list. Instead, we continue to find the next smallest x, NextX, and repeat the candidate elimination with NextX."}, {"heading": "4. RUNNING TIME ANALYSIS", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.1 Worst Case Running Time", "text": "There are three main steps in each iteration of determining the convex hull by eliminating the candidate: \u2022 Search for NextX, total O (| candidate |) \u2022 Elimination of the candidate, O (| candidate |) \u2022 Attachment of the upper hull, C (constant) In the worst case, it is possible that all points in P belong to the convex hull. In this case, elimination of the candidate results in no candidates being removed and an O (n) process being repeated n times, resulting in a runtime of O (n2) of the worst-case problem. In the current classification problem, the worst-case scenario rarely happens."}, {"heading": "4.2 Expected Running Time", "text": "Since there are 4 quarters and the expected number of candidates is n / 2 after the initial candidate elimination, there is an expected number of n / 8 candidates in each triangle. It is worth noting that we can use the product of the expected values of two random variables as the expected value of their product, since all random variables are independent. This is a natural assumption widely used in our examples. We can then define 0 < 1, as the elimination ratio in iteration 1 and 0 < 2, as the elimination ratio in iteration 2 < 1, as the elimination ratio in iteration 2 < 1, as the elimination ratio in iteration 2 < 1, as the elimination ratio in iteration 2 < 1, as the elimination ratio in iteration 2 < 2, as the elimination ratio in iteration 2 < 1, as the elimination ratio in iteration 2 < 2."}, {"heading": "5. CLASSIFICATION ALGORITHM", "text": "Figure 8 shows the same distribution of data in classes as Figure 1. It also shows how classes are separated using their 2-dimensional (2D) convex hulls. Each new missing-label sample is checked against these three convex hulls. It is also the case that when we try to separate these classes using different decision boundaries, we face the same problem. Since convex hulls are tightly wrapped around the points of each class, it reduces the chances of misclassification using its narrow boundaries. There may be cases where the point falls outside of all convex hulls. In such cases, we can assign a point to a class in which we use proximity in Euclidean space. In the rest of this paper, we only deal with classification problems where there are typically d > 2 characteristics."}, {"heading": "5.1 Parallelization for Training and Testing", "text": "We can easily achieve parallelization for both training and test phases by partitioning by either classes or 2DAspects. This can be done in a simple way using a divide-and-conquer approach. For example, we can divide the data into different classes or partitions according to the indexes of (f1, f2) combinations. As this is trivial, we only describe a simple divide-and-conquer algorithm for determining the 2D convex hull of a point P to conclude this section. It is worth noting that in Section CH we have already shown theoretically and empirically that our convex skull algorithm reads only O (s) the expected number of points during its execution. Parallelizing the same algorithm by dividing and conquering the strategy obviously does not increase runtime. In fact, there is no reason for parallelization in many scenarios x. In cases where we want to build classifiers on demand for millions of points, it is convenient to parallelizing video points."}, {"heading": "6. EXPERIMENTAL ANALYSIS", "text": "In this section, we already report on our results in terms of accuracy in various cases. First, we propose a random class approach, through which we can control the difficulty of the classification problem instance. We only generate data using the uniform distribution. However, it is known that we can convert other distributions into uniform even before classification, in order to leave more room for future work, using the key factors in the classification of the classes DataGrinder and efficient algorithms. We will shortly show how DataGrinder (DGR) achieves high accuracy even in its simplest form, as described in this paper."}, {"heading": "6.1 Existing Classification Datasets", "text": "We use two standard datasets, which are also used as examples in machine learning textbooks for the classification problem, iris and wine. We get these datasets from the UCI Data Mining Repository 2. Both datasets have less than 1000 samples and 3 classes. We use 10x cross validation for training and testing, which means that we divide the dataset into 10 partitions and use the average of 10 experiments. In each experiment, we use 9 partitions for training and 1 for testing. All algorithms achieve acceptable accuracy on iris datasets > 90%, and close to 1 (Figure 11). In the case of grinder datasets, Grinder dataset shows that DiscriminantClassifier is slightly better than DataGrinder and DecisionTree. The NearestNeighbor method is2http: / archive.ics.uci.edu / ml / datasets.htmlFigure 12: Using 100 \"filters,\" filter for 1, the best way to classify... 0.02,..."}, {"heading": "7. RELATED WORK", "text": "In this section, we look at recent work in the literature, scalable data mining algorithms and frameworks similar to DataGrinder, in motivation and in technical implementation. In [2] this section, authors propose an \"exact indexing\" approach. They propose to index strategies in the kernel area that are used for the exact processing of data when used for ranking. Considering an SVM model, authors use properties of the kernel space such as ranking stability. They provide an excellent background of support engines, and their relevance to databases, top query processing and ranking."}, {"heading": "8. CONCLUSIONS AND FUTURE WORK", "text": "We propose an algorithm based on a well-known historical algorithm, with an expected runtime of O (n). We propose a simpler and shorter proof than previous work, and we also calculate a constant that serves as an upper limit for the expected linear runtime. We conduct experiments to substantiate all our arguments. We conduct several experiments and show that DataGrinder is comparable to the most reliable commercial classification packages in MATLAB, surpassing many, while maintaining its extremely detectable scalability. We show how to achieve multiple levels of parallelization while maintaining the correctness of our classification algorithm. We intend to focus on more detailed geometric studies of the problem in order to partition the data more accurately."}, {"heading": "9. REFERENCES", "text": "This year, it has come to the point where there is only one person who is able to come out on top."}], "references": [{"title": "Automatic Discovery of Attributes in Relational Databases", "author": ["Zhang"], "venue": "ACM SIGMOD,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2011}, {"title": "Exact Indexing for Support Vector Machines", "author": ["Yu"], "venue": "In ACM SIGMOD,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2011}, {"title": "Local Graph Sparsification for Scalable Clustering", "author": ["Satuluri"], "venue": "In ACM SIGMOD,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2011}, {"title": "Efficient Parallel Skyline Processing using Hyperplane Projections", "author": ["Kohler"], "venue": "In ACM SIGMOD,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2011}, {"title": "Latent OLAP: Data Cubes over Latent Variables", "author": ["Agarwal"], "venue": "In ACM SIGMOD,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2011}, {"title": "Fast Personalized PageRank on MapReduce", "author": ["Bahmani"], "venue": "In ACM SIGMOD,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2011}, {"title": "ArrayStore: A Storage Manager for Complex Parallel Array Processing", "author": ["Emad"], "venue": "In ACM SIGMOD,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2011}, {"title": "Advancing Data Clustering via Projective Clustering Ensembles", "author": ["Gullo"], "venue": "In ACM SIGMOD,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2011}, {"title": "Nearest-Neighbor Searching Under Uncertainty", "author": ["K. Agarwal"], "venue": "In PODS,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2012}, {"title": "On the Complexity of Package Recommendation Problems", "author": ["Deng"], "venue": "In PODS,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2012}, {"title": "The Complexity of Mining Maximal Frequent Subgraphs", "author": ["Kimelfeld"], "venue": "In PODS,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2013}, {"title": "Spanners: A Formal Framework for Information Extraction", "author": ["Fagin"], "venue": "In PODS,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2013}, {"title": "I/O-Efficient Planar Range Skyline and Attrition Priority Queues", "author": ["Kejlberg-Rasmussen"], "venue": "In PODS,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2013}, {"title": "The Fine Classification of Conjunctive Queries and Parameterized Logarithmic Space Complexity", "author": ["Chen"], "venue": "In PODS,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2013}, {"title": "GAIA: Graph Classification Using Evolutionary Computation", "author": ["Jin"], "venue": "In ACM SIGMOD,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2010}, {"title": "Recsplorer: Recommendation Algorithms Based on Precedence Mining", "author": ["Parameswaran"], "venue": "In ACM SIGMOD,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2010}, {"title": "K-Nearest Neighbor Search for Fuzzy Objects", "author": ["Zheng"], "venue": "ACM SIGMOD,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2010}, {"title": "ERACER: A Database Approach for Statistical Inference and Data Cleaning", "author": ["Mayfield"], "venue": "In ACM SIGMOD,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2010}, {"title": "A computational pipeline for the development of multi-marker bio-signature panels and ensemble classifiers", "author": ["Gunter"], "venue": "BMC Bioinformatics,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2012}, {"title": "Predicting Completion Times of Batch Query Workloads Using Interaction-aware Models and Simulation", "author": ["Ahmad"], "venue": "In EDBT,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2011}, {"title": "A Bayesian Approach to Online Performance Modeling for Database Appliances using Gaussian Models", "author": ["Bilal Sheikh"], "venue": "In ICAC,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2011}, {"title": "BSkyTree: Scalable Skyline Computation Using A Balanced Pivot Selection", "author": ["Lee"], "venue": "In EDBT,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2010}, {"title": "newblock TopRecs: Top-k Algorithms for Item-based Collaborative Filtering", "author": ["Khabbaz"], "venue": "newblock In EDBT,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2011}, {"title": "newblock Employing Structural and Textual Feature Extraction for Semistructured Document Classification. newblock", "author": ["Khabbaz"], "venue": "In IEEE SMC-C,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2012}, {"title": "newblock Semi-Supervised Dynamic Classification for Intrusion Detection. newblock", "author": ["Koochakzadeh"], "venue": "In Int. J. Soft. Eng. Knowl. Eng", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2010}, {"title": "Cluster Forests", "author": ["Yan"], "venue": "In Computational Statistics and Data Analysis", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2013}, {"title": "Divide-and-conquer and statistical inference for big data", "author": ["Jordan"], "venue": "In ACM SIGKDD,", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2012}, {"title": "Divide-and-Conquer Matrix Factorization", "author": ["Jordan"], "venue": "In NIPS,", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2011}, {"title": "Regularizedlinear discriminant analysis and its application in microarrays", "author": ["Guo"], "venue": "In Biostatistics,", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2007}, {"title": "Yahoo! music recommendations: modeling music ratings with temporal dynamics and item taxonomy", "author": ["Koenigstein"], "venue": "In ACM RecSys,", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2011}, {"title": "Maximizing Product Adoption in Social Networks", "author": ["Bhagat"], "venue": "In, ACM WSDM,", "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2012}, {"title": "A Data-Based Approach to Social Influence Maximization", "author": ["Goyal"], "venue": "In, PVLDB,", "citeRegEx": "32", "shortCiteRegEx": "32", "year": 2011}, {"title": "Integrating classification and association rule mining", "author": ["Liu"], "venue": "In, ACM SIGKDD,", "citeRegEx": "33", "shortCiteRegEx": "33", "year": 1998}, {"title": "Accurate and efficient classification based on multiple class-association rules. In", "author": ["Li"], "venue": "IEEE ICDM,", "citeRegEx": "34", "shortCiteRegEx": "34", "year": 2001}, {"title": "Classification based on predictive association", "author": ["Yin"], "venue": "SIAM SDM,", "citeRegEx": "35", "shortCiteRegEx": "35", "year": 2003}, {"title": "Fast Algorithms for Mining Association Rules in Large Databases", "author": ["R. Agrawal"], "venue": null, "citeRegEx": "36", "shortCiteRegEx": "36", "year": 1994}, {"title": "Mining frequent patterns without candidate generation", "author": ["Han"], "venue": null, "citeRegEx": "37", "shortCiteRegEx": "37", "year": 2000}, {"title": "BIRCH: An Efficient Data Clustering Method for Very Large Databases", "author": ["Zhang"], "venue": "In, ACM SIGMOD,", "citeRegEx": "38", "shortCiteRegEx": "38", "year": 1996}, {"title": "Pattern Recognition and Machine Learning", "author": ["C.M. Bishop"], "venue": "In, Springer,", "citeRegEx": "39", "shortCiteRegEx": "39", "year": 2006}, {"title": "Computational Geometry, Algorithms and Applications, Third Edition", "author": ["Mark de Berg"], "venue": "In Springer,", "citeRegEx": "40", "shortCiteRegEx": "40", "year": 2008}, {"title": "A Note on Linear Expected Time Algorithms for Finding Convex Hulls", "author": ["Devroye"], "venue": "In Computing,", "citeRegEx": "41", "shortCiteRegEx": "41", "year": 1981}], "referenceMentions": [{"referenceID": 0, "context": "Data mining topics such as classification [1, 2, 14, 15, 19, 24, 25, 29], clustering [38, 28, 27, 26, 24, 8, 7, 5, 3], frequent pattern mining [37, 36, 11], frequent sub-structure mining [11, 24], regression [21, 20], data cleaning [18, 8, 1], ranking [6], data warehousing [5], recommender systems [23, 16, 10], bio-informatics [19], outlier detection [22, 13, 4], nearest neighbors [17, 9] and social networks [31, 32], have been widely discussed in data management and prediction.", "startOffset": 42, "endOffset": 72}, {"referenceID": 1, "context": "Data mining topics such as classification [1, 2, 14, 15, 19, 24, 25, 29], clustering [38, 28, 27, 26, 24, 8, 7, 5, 3], frequent pattern mining [37, 36, 11], frequent sub-structure mining [11, 24], regression [21, 20], data cleaning [18, 8, 1], ranking [6], data warehousing [5], recommender systems [23, 16, 10], bio-informatics [19], outlier detection [22, 13, 4], nearest neighbors [17, 9] and social networks [31, 32], have been widely discussed in data management and prediction.", "startOffset": 42, "endOffset": 72}, {"referenceID": 13, "context": "Data mining topics such as classification [1, 2, 14, 15, 19, 24, 25, 29], clustering [38, 28, 27, 26, 24, 8, 7, 5, 3], frequent pattern mining [37, 36, 11], frequent sub-structure mining [11, 24], regression [21, 20], data cleaning [18, 8, 1], ranking [6], data warehousing [5], recommender systems [23, 16, 10], bio-informatics [19], outlier detection [22, 13, 4], nearest neighbors [17, 9] and social networks [31, 32], have been widely discussed in data management and prediction.", "startOffset": 42, "endOffset": 72}, {"referenceID": 14, "context": "Data mining topics such as classification [1, 2, 14, 15, 19, 24, 25, 29], clustering [38, 28, 27, 26, 24, 8, 7, 5, 3], frequent pattern mining [37, 36, 11], frequent sub-structure mining [11, 24], regression [21, 20], data cleaning [18, 8, 1], ranking [6], data warehousing [5], recommender systems [23, 16, 10], bio-informatics [19], outlier detection [22, 13, 4], nearest neighbors [17, 9] and social networks [31, 32], have been widely discussed in data management and prediction.", "startOffset": 42, "endOffset": 72}, {"referenceID": 18, "context": "Data mining topics such as classification [1, 2, 14, 15, 19, 24, 25, 29], clustering [38, 28, 27, 26, 24, 8, 7, 5, 3], frequent pattern mining [37, 36, 11], frequent sub-structure mining [11, 24], regression [21, 20], data cleaning [18, 8, 1], ranking [6], data warehousing [5], recommender systems [23, 16, 10], bio-informatics [19], outlier detection [22, 13, 4], nearest neighbors [17, 9] and social networks [31, 32], have been widely discussed in data management and prediction.", "startOffset": 42, "endOffset": 72}, {"referenceID": 23, "context": "Data mining topics such as classification [1, 2, 14, 15, 19, 24, 25, 29], clustering [38, 28, 27, 26, 24, 8, 7, 5, 3], frequent pattern mining [37, 36, 11], frequent sub-structure mining [11, 24], regression [21, 20], data cleaning [18, 8, 1], ranking [6], data warehousing [5], recommender systems [23, 16, 10], bio-informatics [19], outlier detection [22, 13, 4], nearest neighbors [17, 9] and social networks [31, 32], have been widely discussed in data management and prediction.", "startOffset": 42, "endOffset": 72}, {"referenceID": 24, "context": "Data mining topics such as classification [1, 2, 14, 15, 19, 24, 25, 29], clustering [38, 28, 27, 26, 24, 8, 7, 5, 3], frequent pattern mining [37, 36, 11], frequent sub-structure mining [11, 24], regression [21, 20], data cleaning [18, 8, 1], ranking [6], data warehousing [5], recommender systems [23, 16, 10], bio-informatics [19], outlier detection [22, 13, 4], nearest neighbors [17, 9] and social networks [31, 32], have been widely discussed in data management and prediction.", "startOffset": 42, "endOffset": 72}, {"referenceID": 28, "context": "Data mining topics such as classification [1, 2, 14, 15, 19, 24, 25, 29], clustering [38, 28, 27, 26, 24, 8, 7, 5, 3], frequent pattern mining [37, 36, 11], frequent sub-structure mining [11, 24], regression [21, 20], data cleaning [18, 8, 1], ranking [6], data warehousing [5], recommender systems [23, 16, 10], bio-informatics [19], outlier detection [22, 13, 4], nearest neighbors [17, 9] and social networks [31, 32], have been widely discussed in data management and prediction.", "startOffset": 42, "endOffset": 72}, {"referenceID": 37, "context": "Data mining topics such as classification [1, 2, 14, 15, 19, 24, 25, 29], clustering [38, 28, 27, 26, 24, 8, 7, 5, 3], frequent pattern mining [37, 36, 11], frequent sub-structure mining [11, 24], regression [21, 20], data cleaning [18, 8, 1], ranking [6], data warehousing [5], recommender systems [23, 16, 10], bio-informatics [19], outlier detection [22, 13, 4], nearest neighbors [17, 9] and social networks [31, 32], have been widely discussed in data management and prediction.", "startOffset": 85, "endOffset": 117}, {"referenceID": 27, "context": "Data mining topics such as classification [1, 2, 14, 15, 19, 24, 25, 29], clustering [38, 28, 27, 26, 24, 8, 7, 5, 3], frequent pattern mining [37, 36, 11], frequent sub-structure mining [11, 24], regression [21, 20], data cleaning [18, 8, 1], ranking [6], data warehousing [5], recommender systems [23, 16, 10], bio-informatics [19], outlier detection [22, 13, 4], nearest neighbors [17, 9] and social networks [31, 32], have been widely discussed in data management and prediction.", "startOffset": 85, "endOffset": 117}, {"referenceID": 26, "context": "Data mining topics such as classification [1, 2, 14, 15, 19, 24, 25, 29], clustering [38, 28, 27, 26, 24, 8, 7, 5, 3], frequent pattern mining [37, 36, 11], frequent sub-structure mining [11, 24], regression [21, 20], data cleaning [18, 8, 1], ranking [6], data warehousing [5], recommender systems [23, 16, 10], bio-informatics [19], outlier detection [22, 13, 4], nearest neighbors [17, 9] and social networks [31, 32], have been widely discussed in data management and prediction.", "startOffset": 85, "endOffset": 117}, {"referenceID": 25, "context": "Data mining topics such as classification [1, 2, 14, 15, 19, 24, 25, 29], clustering [38, 28, 27, 26, 24, 8, 7, 5, 3], frequent pattern mining [37, 36, 11], frequent sub-structure mining [11, 24], regression [21, 20], data cleaning [18, 8, 1], ranking [6], data warehousing [5], recommender systems [23, 16, 10], bio-informatics [19], outlier detection [22, 13, 4], nearest neighbors [17, 9] and social networks [31, 32], have been widely discussed in data management and prediction.", "startOffset": 85, "endOffset": 117}, {"referenceID": 23, "context": "Data mining topics such as classification [1, 2, 14, 15, 19, 24, 25, 29], clustering [38, 28, 27, 26, 24, 8, 7, 5, 3], frequent pattern mining [37, 36, 11], frequent sub-structure mining [11, 24], regression [21, 20], data cleaning [18, 8, 1], ranking [6], data warehousing [5], recommender systems [23, 16, 10], bio-informatics [19], outlier detection [22, 13, 4], nearest neighbors [17, 9] and social networks [31, 32], have been widely discussed in data management and prediction.", "startOffset": 85, "endOffset": 117}, {"referenceID": 7, "context": "Data mining topics such as classification [1, 2, 14, 15, 19, 24, 25, 29], clustering [38, 28, 27, 26, 24, 8, 7, 5, 3], frequent pattern mining [37, 36, 11], frequent sub-structure mining [11, 24], regression [21, 20], data cleaning [18, 8, 1], ranking [6], data warehousing [5], recommender systems [23, 16, 10], bio-informatics [19], outlier detection [22, 13, 4], nearest neighbors [17, 9] and social networks [31, 32], have been widely discussed in data management and prediction.", "startOffset": 85, "endOffset": 117}, {"referenceID": 6, "context": "Data mining topics such as classification [1, 2, 14, 15, 19, 24, 25, 29], clustering [38, 28, 27, 26, 24, 8, 7, 5, 3], frequent pattern mining [37, 36, 11], frequent sub-structure mining [11, 24], regression [21, 20], data cleaning [18, 8, 1], ranking [6], data warehousing [5], recommender systems [23, 16, 10], bio-informatics [19], outlier detection [22, 13, 4], nearest neighbors [17, 9] and social networks [31, 32], have been widely discussed in data management and prediction.", "startOffset": 85, "endOffset": 117}, {"referenceID": 4, "context": "Data mining topics such as classification [1, 2, 14, 15, 19, 24, 25, 29], clustering [38, 28, 27, 26, 24, 8, 7, 5, 3], frequent pattern mining [37, 36, 11], frequent sub-structure mining [11, 24], regression [21, 20], data cleaning [18, 8, 1], ranking [6], data warehousing [5], recommender systems [23, 16, 10], bio-informatics [19], outlier detection [22, 13, 4], nearest neighbors [17, 9] and social networks [31, 32], have been widely discussed in data management and prediction.", "startOffset": 85, "endOffset": 117}, {"referenceID": 2, "context": "Data mining topics such as classification [1, 2, 14, 15, 19, 24, 25, 29], clustering [38, 28, 27, 26, 24, 8, 7, 5, 3], frequent pattern mining [37, 36, 11], frequent sub-structure mining [11, 24], regression [21, 20], data cleaning [18, 8, 1], ranking [6], data warehousing [5], recommender systems [23, 16, 10], bio-informatics [19], outlier detection [22, 13, 4], nearest neighbors [17, 9] and social networks [31, 32], have been widely discussed in data management and prediction.", "startOffset": 85, "endOffset": 117}, {"referenceID": 36, "context": "Data mining topics such as classification [1, 2, 14, 15, 19, 24, 25, 29], clustering [38, 28, 27, 26, 24, 8, 7, 5, 3], frequent pattern mining [37, 36, 11], frequent sub-structure mining [11, 24], regression [21, 20], data cleaning [18, 8, 1], ranking [6], data warehousing [5], recommender systems [23, 16, 10], bio-informatics [19], outlier detection [22, 13, 4], nearest neighbors [17, 9] and social networks [31, 32], have been widely discussed in data management and prediction.", "startOffset": 143, "endOffset": 155}, {"referenceID": 35, "context": "Data mining topics such as classification [1, 2, 14, 15, 19, 24, 25, 29], clustering [38, 28, 27, 26, 24, 8, 7, 5, 3], frequent pattern mining [37, 36, 11], frequent sub-structure mining [11, 24], regression [21, 20], data cleaning [18, 8, 1], ranking [6], data warehousing [5], recommender systems [23, 16, 10], bio-informatics [19], outlier detection [22, 13, 4], nearest neighbors [17, 9] and social networks [31, 32], have been widely discussed in data management and prediction.", "startOffset": 143, "endOffset": 155}, {"referenceID": 10, "context": "Data mining topics such as classification [1, 2, 14, 15, 19, 24, 25, 29], clustering [38, 28, 27, 26, 24, 8, 7, 5, 3], frequent pattern mining [37, 36, 11], frequent sub-structure mining [11, 24], regression [21, 20], data cleaning [18, 8, 1], ranking [6], data warehousing [5], recommender systems [23, 16, 10], bio-informatics [19], outlier detection [22, 13, 4], nearest neighbors [17, 9] and social networks [31, 32], have been widely discussed in data management and prediction.", "startOffset": 143, "endOffset": 155}, {"referenceID": 10, "context": "Data mining topics such as classification [1, 2, 14, 15, 19, 24, 25, 29], clustering [38, 28, 27, 26, 24, 8, 7, 5, 3], frequent pattern mining [37, 36, 11], frequent sub-structure mining [11, 24], regression [21, 20], data cleaning [18, 8, 1], ranking [6], data warehousing [5], recommender systems [23, 16, 10], bio-informatics [19], outlier detection [22, 13, 4], nearest neighbors [17, 9] and social networks [31, 32], have been widely discussed in data management and prediction.", "startOffset": 187, "endOffset": 195}, {"referenceID": 23, "context": "Data mining topics such as classification [1, 2, 14, 15, 19, 24, 25, 29], clustering [38, 28, 27, 26, 24, 8, 7, 5, 3], frequent pattern mining [37, 36, 11], frequent sub-structure mining [11, 24], regression [21, 20], data cleaning [18, 8, 1], ranking [6], data warehousing [5], recommender systems [23, 16, 10], bio-informatics [19], outlier detection [22, 13, 4], nearest neighbors [17, 9] and social networks [31, 32], have been widely discussed in data management and prediction.", "startOffset": 187, "endOffset": 195}, {"referenceID": 20, "context": "Data mining topics such as classification [1, 2, 14, 15, 19, 24, 25, 29], clustering [38, 28, 27, 26, 24, 8, 7, 5, 3], frequent pattern mining [37, 36, 11], frequent sub-structure mining [11, 24], regression [21, 20], data cleaning [18, 8, 1], ranking [6], data warehousing [5], recommender systems [23, 16, 10], bio-informatics [19], outlier detection [22, 13, 4], nearest neighbors [17, 9] and social networks [31, 32], have been widely discussed in data management and prediction.", "startOffset": 208, "endOffset": 216}, {"referenceID": 19, "context": "Data mining topics such as classification [1, 2, 14, 15, 19, 24, 25, 29], clustering [38, 28, 27, 26, 24, 8, 7, 5, 3], frequent pattern mining [37, 36, 11], frequent sub-structure mining [11, 24], regression [21, 20], data cleaning [18, 8, 1], ranking [6], data warehousing [5], recommender systems [23, 16, 10], bio-informatics [19], outlier detection [22, 13, 4], nearest neighbors [17, 9] and social networks [31, 32], have been widely discussed in data management and prediction.", "startOffset": 208, "endOffset": 216}, {"referenceID": 17, "context": "Data mining topics such as classification [1, 2, 14, 15, 19, 24, 25, 29], clustering [38, 28, 27, 26, 24, 8, 7, 5, 3], frequent pattern mining [37, 36, 11], frequent sub-structure mining [11, 24], regression [21, 20], data cleaning [18, 8, 1], ranking [6], data warehousing [5], recommender systems [23, 16, 10], bio-informatics [19], outlier detection [22, 13, 4], nearest neighbors [17, 9] and social networks [31, 32], have been widely discussed in data management and prediction.", "startOffset": 232, "endOffset": 242}, {"referenceID": 7, "context": "Data mining topics such as classification [1, 2, 14, 15, 19, 24, 25, 29], clustering [38, 28, 27, 26, 24, 8, 7, 5, 3], frequent pattern mining [37, 36, 11], frequent sub-structure mining [11, 24], regression [21, 20], data cleaning [18, 8, 1], ranking [6], data warehousing [5], recommender systems [23, 16, 10], bio-informatics [19], outlier detection [22, 13, 4], nearest neighbors [17, 9] and social networks [31, 32], have been widely discussed in data management and prediction.", "startOffset": 232, "endOffset": 242}, {"referenceID": 0, "context": "Data mining topics such as classification [1, 2, 14, 15, 19, 24, 25, 29], clustering [38, 28, 27, 26, 24, 8, 7, 5, 3], frequent pattern mining [37, 36, 11], frequent sub-structure mining [11, 24], regression [21, 20], data cleaning [18, 8, 1], ranking [6], data warehousing [5], recommender systems [23, 16, 10], bio-informatics [19], outlier detection [22, 13, 4], nearest neighbors [17, 9] and social networks [31, 32], have been widely discussed in data management and prediction.", "startOffset": 232, "endOffset": 242}, {"referenceID": 5, "context": "Data mining topics such as classification [1, 2, 14, 15, 19, 24, 25, 29], clustering [38, 28, 27, 26, 24, 8, 7, 5, 3], frequent pattern mining [37, 36, 11], frequent sub-structure mining [11, 24], regression [21, 20], data cleaning [18, 8, 1], ranking [6], data warehousing [5], recommender systems [23, 16, 10], bio-informatics [19], outlier detection [22, 13, 4], nearest neighbors [17, 9] and social networks [31, 32], have been widely discussed in data management and prediction.", "startOffset": 252, "endOffset": 255}, {"referenceID": 4, "context": "Data mining topics such as classification [1, 2, 14, 15, 19, 24, 25, 29], clustering [38, 28, 27, 26, 24, 8, 7, 5, 3], frequent pattern mining [37, 36, 11], frequent sub-structure mining [11, 24], regression [21, 20], data cleaning [18, 8, 1], ranking [6], data warehousing [5], recommender systems [23, 16, 10], bio-informatics [19], outlier detection [22, 13, 4], nearest neighbors [17, 9] and social networks [31, 32], have been widely discussed in data management and prediction.", "startOffset": 274, "endOffset": 277}, {"referenceID": 22, "context": "Data mining topics such as classification [1, 2, 14, 15, 19, 24, 25, 29], clustering [38, 28, 27, 26, 24, 8, 7, 5, 3], frequent pattern mining [37, 36, 11], frequent sub-structure mining [11, 24], regression [21, 20], data cleaning [18, 8, 1], ranking [6], data warehousing [5], recommender systems [23, 16, 10], bio-informatics [19], outlier detection [22, 13, 4], nearest neighbors [17, 9] and social networks [31, 32], have been widely discussed in data management and prediction.", "startOffset": 299, "endOffset": 311}, {"referenceID": 15, "context": "Data mining topics such as classification [1, 2, 14, 15, 19, 24, 25, 29], clustering [38, 28, 27, 26, 24, 8, 7, 5, 3], frequent pattern mining [37, 36, 11], frequent sub-structure mining [11, 24], regression [21, 20], data cleaning [18, 8, 1], ranking [6], data warehousing [5], recommender systems [23, 16, 10], bio-informatics [19], outlier detection [22, 13, 4], nearest neighbors [17, 9] and social networks [31, 32], have been widely discussed in data management and prediction.", "startOffset": 299, "endOffset": 311}, {"referenceID": 9, "context": "Data mining topics such as classification [1, 2, 14, 15, 19, 24, 25, 29], clustering [38, 28, 27, 26, 24, 8, 7, 5, 3], frequent pattern mining [37, 36, 11], frequent sub-structure mining [11, 24], regression [21, 20], data cleaning [18, 8, 1], ranking [6], data warehousing [5], recommender systems [23, 16, 10], bio-informatics [19], outlier detection [22, 13, 4], nearest neighbors [17, 9] and social networks [31, 32], have been widely discussed in data management and prediction.", "startOffset": 299, "endOffset": 311}, {"referenceID": 18, "context": "Data mining topics such as classification [1, 2, 14, 15, 19, 24, 25, 29], clustering [38, 28, 27, 26, 24, 8, 7, 5, 3], frequent pattern mining [37, 36, 11], frequent sub-structure mining [11, 24], regression [21, 20], data cleaning [18, 8, 1], ranking [6], data warehousing [5], recommender systems [23, 16, 10], bio-informatics [19], outlier detection [22, 13, 4], nearest neighbors [17, 9] and social networks [31, 32], have been widely discussed in data management and prediction.", "startOffset": 329, "endOffset": 333}, {"referenceID": 21, "context": "Data mining topics such as classification [1, 2, 14, 15, 19, 24, 25, 29], clustering [38, 28, 27, 26, 24, 8, 7, 5, 3], frequent pattern mining [37, 36, 11], frequent sub-structure mining [11, 24], regression [21, 20], data cleaning [18, 8, 1], ranking [6], data warehousing [5], recommender systems [23, 16, 10], bio-informatics [19], outlier detection [22, 13, 4], nearest neighbors [17, 9] and social networks [31, 32], have been widely discussed in data management and prediction.", "startOffset": 353, "endOffset": 364}, {"referenceID": 12, "context": "Data mining topics such as classification [1, 2, 14, 15, 19, 24, 25, 29], clustering [38, 28, 27, 26, 24, 8, 7, 5, 3], frequent pattern mining [37, 36, 11], frequent sub-structure mining [11, 24], regression [21, 20], data cleaning [18, 8, 1], ranking [6], data warehousing [5], recommender systems [23, 16, 10], bio-informatics [19], outlier detection [22, 13, 4], nearest neighbors [17, 9] and social networks [31, 32], have been widely discussed in data management and prediction.", "startOffset": 353, "endOffset": 364}, {"referenceID": 3, "context": "Data mining topics such as classification [1, 2, 14, 15, 19, 24, 25, 29], clustering [38, 28, 27, 26, 24, 8, 7, 5, 3], frequent pattern mining [37, 36, 11], frequent sub-structure mining [11, 24], regression [21, 20], data cleaning [18, 8, 1], ranking [6], data warehousing [5], recommender systems [23, 16, 10], bio-informatics [19], outlier detection [22, 13, 4], nearest neighbors [17, 9] and social networks [31, 32], have been widely discussed in data management and prediction.", "startOffset": 353, "endOffset": 364}, {"referenceID": 16, "context": "Data mining topics such as classification [1, 2, 14, 15, 19, 24, 25, 29], clustering [38, 28, 27, 26, 24, 8, 7, 5, 3], frequent pattern mining [37, 36, 11], frequent sub-structure mining [11, 24], regression [21, 20], data cleaning [18, 8, 1], ranking [6], data warehousing [5], recommender systems [23, 16, 10], bio-informatics [19], outlier detection [22, 13, 4], nearest neighbors [17, 9] and social networks [31, 32], have been widely discussed in data management and prediction.", "startOffset": 384, "endOffset": 391}, {"referenceID": 8, "context": "Data mining topics such as classification [1, 2, 14, 15, 19, 24, 25, 29], clustering [38, 28, 27, 26, 24, 8, 7, 5, 3], frequent pattern mining [37, 36, 11], frequent sub-structure mining [11, 24], regression [21, 20], data cleaning [18, 8, 1], ranking [6], data warehousing [5], recommender systems [23, 16, 10], bio-informatics [19], outlier detection [22, 13, 4], nearest neighbors [17, 9] and social networks [31, 32], have been widely discussed in data management and prediction.", "startOffset": 384, "endOffset": 391}, {"referenceID": 30, "context": "Data mining topics such as classification [1, 2, 14, 15, 19, 24, 25, 29], clustering [38, 28, 27, 26, 24, 8, 7, 5, 3], frequent pattern mining [37, 36, 11], frequent sub-structure mining [11, 24], regression [21, 20], data cleaning [18, 8, 1], ranking [6], data warehousing [5], recommender systems [23, 16, 10], bio-informatics [19], outlier detection [22, 13, 4], nearest neighbors [17, 9] and social networks [31, 32], have been widely discussed in data management and prediction.", "startOffset": 412, "endOffset": 420}, {"referenceID": 31, "context": "Data mining topics such as classification [1, 2, 14, 15, 19, 24, 25, 29], clustering [38, 28, 27, 26, 24, 8, 7, 5, 3], frequent pattern mining [37, 36, 11], frequent sub-structure mining [11, 24], regression [21, 20], data cleaning [18, 8, 1], ranking [6], data warehousing [5], recommender systems [23, 16, 10], bio-informatics [19], outlier detection [22, 13, 4], nearest neighbors [17, 9] and social networks [31, 32], have been widely discussed in data management and prediction.", "startOffset": 412, "endOffset": 420}, {"referenceID": 38, "context": "In order to minimize misclassification error, people use methods such as regularization, kernel transformations, feature extraction and feature selection [39].", "startOffset": 154, "endOffset": 158}, {"referenceID": 1, "context": "Examples of discriminant classifiers include Support Vector Machines and Logistic Regression [2, 29].", "startOffset": 93, "endOffset": 100}, {"referenceID": 28, "context": "Examples of discriminant classifiers include Support Vector Machines and Logistic Regression [2, 29].", "startOffset": 93, "endOffset": 100}, {"referenceID": 32, "context": "Other types of classifiers are Decision Trees, Rule-based [33, 34, 35] methods and Nearest Neighbor methods.", "startOffset": 58, "endOffset": 70}, {"referenceID": 33, "context": "Other types of classifiers are Decision Trees, Rule-based [33, 34, 35] methods and Nearest Neighbor methods.", "startOffset": 58, "endOffset": 70}, {"referenceID": 34, "context": "Other types of classifiers are Decision Trees, Rule-based [33, 34, 35] methods and Nearest Neighbor methods.", "startOffset": 58, "endOffset": 70}, {"referenceID": 23, "context": "In web-based scenarios, data changes very frequently [24].", "startOffset": 53, "endOffset": 57}, {"referenceID": 29, "context": "Although time always plays a key role and sometimes sequential update may not be optimal [30].", "startOffset": 89, "endOffset": 93}, {"referenceID": 39, "context": "We explain the Convex Hull problem from Computational Geometry [40].", "startOffset": 63, "endOffset": 67}, {"referenceID": 21, "context": "Database community has shown tremendous interest in solving problems formulated similar to convex hulls such as designing algorithms for finding Skylines [22, 13, 4].", "startOffset": 154, "endOffset": 165}, {"referenceID": 12, "context": "Database community has shown tremendous interest in solving problems formulated similar to convex hulls such as designing algorithms for finding Skylines [22, 13, 4].", "startOffset": 154, "endOffset": 165}, {"referenceID": 3, "context": "Database community has shown tremendous interest in solving problems formulated similar to convex hulls such as designing algorithms for finding Skylines [22, 13, 4].", "startOffset": 154, "endOffset": 165}, {"referenceID": 23, "context": "We can also use properties of these domains such as link structure in order to define entities such as web pages in a multidimensional Euclidean space, and use convex hull for modeling [24].", "startOffset": 185, "endOffset": 189}, {"referenceID": 39, "context": "It is provable that the best possible worse case running time for this problem is O(nlog(n)), since sorting can be reduced to the convex hull problem [40].", "startOffset": 150, "endOffset": 154}, {"referenceID": 40, "context": "Previous work in Computational Geometry also approves the possibility of O(n) expected running time, if the algorithm is designed within the given framework [41].", "startOffset": 157, "endOffset": 161}, {"referenceID": 38, "context": "This, is a natural assumption, used widely in Machine Learning [39].", "startOffset": 63, "endOffset": 67}, {"referenceID": 38, "context": "It is known that we can convert other distributions to Uniform as well before classification, using Normalization [39].", "startOffset": 114, "endOffset": 118}, {"referenceID": 28, "context": "DiscriminantClassifier, finds decision boundaries using L1 and L2 Regularization [29], in order to avoid overfitting to the training data.", "startOffset": 81, "endOffset": 85}, {"referenceID": 1, "context": "In [2], authors propose an \u201dExact Indexing\u201d approach for Support Vector Machines.", "startOffset": 3, "endOffset": 6}, {"referenceID": 6, "context": "ArrayStore [7], is a storage manager for complex array processing.", "startOffset": 11, "endOffset": 14}, {"referenceID": 17, "context": "ERACER [18], provides an iterative statistical framework for filling in missing data, as well as data cleaning and fixing corrupted values using conventional statistical methods.", "startOffset": 7, "endOffset": 11}, {"referenceID": 11, "context": "Spanners [12], is an interesting theoretical contribution, and a formal framework for information extraction.", "startOffset": 9, "endOffset": 13}, {"referenceID": 22, "context": "Several other previous works have also tried to achieve the same goal such as [23].", "startOffset": 78, "endOffset": 82}, {"referenceID": 26, "context": "Another interesting direction to achieve parallel statistical and data mining algorithms is through Sampling [27].", "startOffset": 109, "endOffset": 113}, {"referenceID": 25, "context": "Examples of such methods include [26, 28].", "startOffset": 33, "endOffset": 41}, {"referenceID": 27, "context": "Examples of such methods include [26, 28].", "startOffset": 33, "endOffset": 41}, {"referenceID": 32, "context": "Rule-based classifiers are other examples of discrete classification algorithms, discussed in the data mining literature [33, 34, 35].", "startOffset": 121, "endOffset": 133}, {"referenceID": 33, "context": "Rule-based classifiers are other examples of discrete classification algorithms, discussed in the data mining literature [33, 34, 35].", "startOffset": 121, "endOffset": 133}, {"referenceID": 34, "context": "Rule-based classifiers are other examples of discrete classification algorithms, discussed in the data mining literature [33, 34, 35].", "startOffset": 121, "endOffset": 133}, {"referenceID": 39, "context": "Convex Hull problem has a long history in Computational Geometry [40].", "startOffset": 65, "endOffset": 69}, {"referenceID": 40, "context": "In [41], there is a proposal for expected O(n) algorithms along with theoretical analysis to prove its possibility.", "startOffset": 3, "endOffset": 7}], "year": 2015, "abstractText": "It has been a long time, since data mining technologies have made their ways to the field of data management. Classification is one of the most important data mining tasks for label prediction, categorization of objects into groups, advertisement and data management. In this paper, we focus on the standard classification problem which is predicting unknown labels in Euclidean space. Most efforts in Machine Learning communities are devoted to methods that use probabilistic algorithms which are heavy on Calculus and Linear Algebra. Most of these techniques have scalability issues for big data, and are hardly parallelizable if they are to maintain their high accuracies in their standard form. Sampling is a new direction for improving scalability, using many small parallel classifiers. In this paper, rather than conventional sampling methods, we focus on a discrete classification algorithm with O(n) expected running time. Our approach performs a similar task as sampling methods. However, we use column-wise sampling of data, rather than the row-wise sampling used in the literature. In either case, our algorithm is completely deterministic. Our algorithm, proposes a way of combining 2D convex hulls in order to achieve high classification accuracy as well as scalability in the same time. First, we thoroughly describe and prove our O(n) algorithm for finding the convex hull of a point set in 2D. Then, we show with experiments our classifier model built based on this idea is very competitive compared with existing sophisticated classification algorithms included in commercial statistical applications such as MATLAB.", "creator": "LaTeX with hyperref package"}}}