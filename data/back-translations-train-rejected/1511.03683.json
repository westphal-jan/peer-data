{"id": "1511.03683", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "11-Nov-2015", "title": "Generative Concatenative Nets Jointly Learn to Write and Classify Reviews", "abstract": "We present a character-level recurrent neural network that generates relevant and coherent text given auxiliary information such as a sentiment or topic. Using a simple input replication strategy, we preserve the signal of auxiliary input across wider sequence intervals than can feasibly be trained by back-propagation through time. Our main results center on a large corpus of 1.5 million beer reviews from BeerAdvocate. In generative mode, our network produces reviews on command, tailored to a star rating or item category. The generative model can also run in reverse, performing classification with surprising accuracy. Performance of the reverse model provides a straightforward way to determine what the generative model knows without relying too heavily on subjective analysis. Given a review, the model can accurately determine the corresponding rating and infer the beer's category (IPA, Stout, etc.). We exploit this capability, tracking perceived sentiment and class membership as each character in a review is processed. Quantitative and qualitative empirical evaluations demonstrate that the model captures meaning and learns nonlinear dynamics in text, such as the effect of negation on sentiment, despite possessing no a priori notion of words. Because the model operates at the character level, it handles misspellings, slang, and large vocabularies without any machinery explicitly dedicated to the purpose.", "histories": [["v1", "Wed, 11 Nov 2015 21:16:59 GMT  (1020kb,D)", "http://arxiv.org/abs/1511.03683v1", null], ["v2", "Mon, 16 Nov 2015 10:27:27 GMT  (1080kb,D)", "http://arxiv.org/abs/1511.03683v2", null], ["v3", "Wed, 18 Nov 2015 08:20:05 GMT  (1080kb,D)", "http://arxiv.org/abs/1511.03683v3", null], ["v4", "Fri, 20 Nov 2015 19:17:07 GMT  (1072kb,D)", "http://arxiv.org/abs/1511.03683v4", null], ["v5", "Thu, 7 Apr 2016 07:08:42 GMT  (1211kb,D)", "http://arxiv.org/abs/1511.03683v5", null]], "reviews": [], "SUBJECTS": "cs.CL cs.LG", "authors": ["zachary c lipton", "sharad vikram", "julian mcauley"], "accepted": false, "id": "1511.03683"}, "pdf": {"name": "1511.03683.pdf", "metadata": {"source": "CRF", "title": "CHARACTER-LEVEL GENERATIVE TEXT MODELS", "authors": ["Zachary C. Lipton", "Sharad Vikram", "Julian McAuley"], "emails": ["zlipton@cs.ucsd.edu", "svikram@cs.ucsd.edu", "jmcauley@cs.ucsd.edu"], "sections": [{"heading": "1 INTRODUCTION", "text": "It is a question of whether and to what extent it is a question of a way in which people are able to survive themselves. (...) It is a question of the extent to which they are able to survive themselves. (...) It is a question of the extent to which they are able to survive themselves. (...) It is a question of the extent to which they are able to survive themselves. (...) It is a question of the extent to which they are able to survive themselves. (...) It is a question of the extent to which they are able to survive themselves. (...) It is a question of the extent to which they are able to survive themselves. \""}, {"heading": "2 THE BEER ADVOCATE DATASET", "text": "We focus on data from Beer Advocate, as originally collected and described by McAuley and Leskovec (2013). Beer Advocate is a large online review community with 1,586,614 reviews from 66,051 different articles, compiled by 33,387 users. Each review is accompanied by a number of numerical reviews that correspond to the \"appearance,\" \"aroma,\" \"palate,\" \"taste\" and also the \"overall impression\" of the user. In addition, the reviews are commented on the category of the article. For our experiments on rating-based generation and classification, we select 250,000 ratings for training, focusing on the most active users and popular articles. For our experiments, which focus on generating ratings conditioned on article categories, we select a subset of 150,000 reviews, each of 30,000 reviews from 5 of the top categories, namely \"American PA,\" \"\" Imperial Porter \"and\" Imperial, \"American Adct for both Fruit."}, {"heading": "3 RECURRENT NEURAL NETWORK METHODOLOGY", "text": "Recursive neural networks extend the capabilities of feed networks to process sequential data. Inputs x (1),..., x (T) are passed to the network one at a time. At each step t, the network updates its hidden state depending on both the current input and the hidden state of the previous step, and outputs a prediction y (t). In this paper, we use RNNs containing long-term memory (LSTM) cells introduced by Hochreiter and Schmidhuber (1997) with oblivion gates that are preserved in Gers et al. (2000), due to their empirical successes and proven ability to overcome the exploding / vanishing history problems of other RNNNs (Bengio et al., 1994). In short, each memory cell has an internal state s in which the activation along a self-connected recursive edge is preserved. Each cell also contains three mosignatory units (i), each containing a specific input unit (i)."}, {"heading": "3.1 GENERATIVE RECURRENT NEURAL NETWORKS", "text": "Before presenting our contributions, we review the generative RNN model of Sutskever et al. (2011; 2014) on which we build. A generative RNN is trained to predict the next character in a sequence, i.e. y-t = x (t + 1), when all inputs are given up to this point (x1,..., xt). Thus, input and output strings are equivalent except for a symbolic shift (Figure 2a). The output layer is fully connected to Softmax activation, ensuring that outputs specify a distribution. Cross-entropy is the loss function during training. Once trained, the model is executed in generative mode, scanning tackastically from the distribution output at each sequence step, with some start marks and state predetermined. If we continue to run the sampled output as a subsequent input, we generate another output that is conditioned on the first sequence, and can continue to generate this way for too long."}, {"heading": "3.2 CONCATENATED INPUT RECURRENT NEURAL NETWORKS", "text": "This has been done at the word level with encoder decoder models (Figure 2b), in which the auxiliary input is encoded and passed as the initial state to a decoder, which then has to maintain this input signal over many sequence steps (Sutskever et al., 2014; Karpathy and Fei-Fei, 2014). Such models have successfully produced captions (typically less than 10 characters long), but seem impractical for generating complete reviews at the character level, because the signal from xaux has to survive hundreds of sequence steps. We take inspiration from an analogy to human text generation. Consider that in the face of a topic that is told at length, a human might be suited to meander and wander, but in the face of a subject that needs to be stared at, it is much easier to stay focused."}, {"heading": "3.3 WEIGHT TRANSPLANTATION", "text": "To solve this problem, we first train a character model for convergence. Then, we transplant these weights into a concatenated input model and initialize the additional weights (between the input layer and the first hidden layer) to zero. Zero initialization is not a problem here, as the symmetry in the hidden layers is already broken. In this way, we guarantee that the model achieves a strictly lower loss than a character model and saves (days) of repeated training. In a way, this scheme is similar to the pre-training usual in the computer vision community (Yosinski et al., 2014). Instead of new output weights, we train new input weights here."}, {"heading": "3.4 RUNNING THE MODEL IN REVERSE", "text": "Many common classification models for documents, such as the logistical regression between tf and idf, maximize the probability of the training labels contained in the text. Considering our generative model, we can then make a predictor by reversing the order of conclusions, that is, by maximizing the probability of the text in a classification. The relationship between these two tasks (P (Review | Review) and P (Review | xaux) results from Bayes \"rule. That is, our model predicts the conditional probability of an entire verification in a rating P (Review | xaux). The normalizing term can be disregarded in determining the most likely evaluation, and when the classes are balanced, as in our test cases, the previous one disappears from the decision rule and P (Review | Xaux) is abandoned."}, {"heading": "4 EXPERIMENTS", "text": "All experiments are performed using a custom recursive neural network library written in Python, using Theano for GPU acceleration. Our networks use 2 hidden layers with 1024 nodes per layer. During training, examples are processed in mini-batches and we update weights with RMSprop (Tieleman and Hinton, 2012). To assemble batches, we concatenate all ratings in the training set and separate them with (< STR >) and (< EOS >) tokens. We divide this string into batches of 256 and re-divide the batch into segments with sequence length 200. In addition, during training, the LSTM state is reserved by batches and to combat exploding gradients, we cut the elements of each gradient at \u00b1 5. To train the concatenated inputs, we found that it was faster to first use a non-generative gradient to graft the N to the N to the N to the N to the N to the N to the N."}, {"heading": "4.1 GENERATING TEXT", "text": "When we run the chained input RNN in generative mode and condition it to a 5-star rating, we produce a decidedly positive rating: < STR > Presented from a 12oz bottle into a pint glass. A: Exudes a dark brown color with a thin brown head. The aroma is of coffee, chocolate and coffee. The taste is of roasted malt, coffee, chocolate and coffee. The finish is slightly sweet and supple with slight bitterness and a slight bitterness that lingers on the palate. The finish is slightly bitter and dry. Mouthfeel is medium with a good percentage of carbonation. The alcohol is well hidden. Drinkability is good. I could drink this all day long. I would love to taste this beer again and again. < EOS > Conditioning on the \"fruit / vegetable beer\" category: This model quickly produces a smell of ice."}, {"heading": "4.2 PREDICTING SENTIMENT AND CATEGORY ONE CHARACTER AT A TIME", "text": "In addition to executing the model for generating results, we can also use sample sentences from invisible ratings and present the rating that gives the sentence the maximum likelihood of each character occurring (Figure 3). For more examples of these charts, see Appendix C and Appendix D. To ensure that Argmax is reasonable over many settings of the rating, we also record the log probability of sentences (after editing the final character) using uniform, grainy settings of the rating (1.0, 1.01, etc.) These charts show that the log probability tends to be smooth and monotonous for sentences with a clear sense, and for less obvious sentences they are smooth with a peak value in the middle of the rating scale (Figure 4). We also find that the model understands nonlinear dynamics of consent and can handle simple spelling errors, as in Appendix E and Appendix D."}, {"heading": "4.3 CLASSIFICATION RESULTS", "text": "While our motivation is to create a general model at the drawing level, the reversal proved to be an effective method of objectively assessing the knowledge of the model. To examine this ability in more detail, we compared it to a tf-idf n-gram multinomial logistic regression (LR) model at the word level using the top 10,000 n-grams. Our model achieves a classification accuracy of 83.0%, while LR achieves an accuracy of 93.4% (Figure 5). Both models confuse most of their errors with stouts and carriers, which is not surprising since a stout is considered a subtype of the carrier. If we split these two into one category, the RNN achieves an accuracy of 91.6%, while LR reaches 96.5%. While the reverse model is not competitive with a state of the art, it has been trained at the drawing level and has not been optimized to minimize classification errors or allow for a reference to the generalization results to be sufficiently detailed in this light."}, {"heading": "5 RELATED WORK", "text": "In his groundbreaking work \"Finding Structure in Time\" (1990), Elman speculated that \"one may wonder if the term\" word \"(or something underlying this concept) might arise as a result of learning the sequential structure of strings of letters that form words and sentences (but in which word boundaries are not marked).\" In this work, an \"Elman RNN\" was formed with 5 input nodes, 5 output nodes, and a single hidden layer of 20 nodes, each of which had a corresponding context unit to predict the next letter in a sequence. At each step, the network received a binary (not one-hot) coding of a character and attempted to predict the binary coding of the next character."}, {"heading": "5.1 KEY DIFFERENCES AND CONTRIBUTIONS", "text": "To our knowledge, however, this is the first work to show that a recursive neural network can generate relevant text at the character level. Sutskever et al. (2011) demonstrates the use of a character level RNN as a scoring mechanism, but this is the first work to use such a scoring mechanism to derive labels in an uncontrolled manner while simultaneously learning to generate text and perform tasks such as regression and multiclass classification with high accuracy. Our work is not the first to demonstrate such a scoring mechanism, as Zhang and LeCun (2015) have offered such an approach. Although their model is strictly discriminatory, the main purpose of our model is to generate text, an ability that is not present in their approach."}, {"heading": "6 DISCUSSION", "text": "Our quantitative and qualitative analysis shows that our model is capable of performing emotional analyses and model categories with precision. Although this ability is fascinating, much remains to be explored as to whether such an approach can compete with the state of the art at the word level. It learns nonlinear dynamics of negation and seems to respond intelligently to a broad vocabulary, although there is no a priori notion of words. We believe that this is only the beginning of this line of research. Next steps include expanding our work into the more complex realm of individual items and users. Given that users respond with extensive historical feedback in a review community and a number of frequently reviewed items, we would like to apply a set of hitherto invisible (user, item) and generate a review that plausibly reflects the style of our user architecture, we can imagine the proposed item as well as the network's approach as ending."}, {"heading": "7 ACKNOWLEDGEMENTS", "text": "Zachary C. Lipton's research is funded by the UCSD Division of Biomedical Informatics through a NIH / NLM Training Scholarship (T15LM011271). Sharad Vikram's research was supported in part by the NSF Scholarship CNS-1446912. We would also like to pay tribute to NVIDIA Corporation, whose hardware donation program provided us with a Tesla K40 GPU that made our research possible."}, {"heading": "A REVIEWS GENERATED BY ITEM CATEGORY", "text": "In the following examples, we generate ratings that match the product category. < M > S > S > S > S > S > S > S > S > S > S > S > S > S > S > S > S > S > S > S > S > S > S > S. < M > S > S > S..S > S > S > S > S..S > S > S > S > S > S > S > S > S > S > S > S > S > S > S > S >.S >.S > S > S..S > S > S > S > S > S > S > S > S > S > S > S > S > S > S > S > S > S > S > S > S > S > S > S > S > S > S > S > S > S >.S >.S > S > S > S > S > S > S > S > S > S > S > S > S > S > S > S >.S > S > S > S > S > S > S > S > S >.S > S > S > S > S > S > S > S > S > S > S > S > S > S > S > S > S > S > S > S > S > S > S > S > S > S > S > S > S > S > S > S > S > S > S > S > S > S >.S > S > S > S > S >.S > S > S > S > S > S > S > S > S >.S > S > S > S > S > S > S > S > S > S > S > S > S > S > S > S > S > S > S > S > S > S > S > S > S > S > S > S > S > S > S > S > S > S > S > S > S > S > S > S > S > S > S > S > S > S > S > S > S > S > S > S > S > S > S > S > S > S > S > S > S > S > S > S > S > S > S > S > S > S > S > S > S > S > S > S > S > S >"}, {"heading": "B REVIEWS GENERATED BY STAR RATING", "text": "This year, it is so far that it can only take a few days to reach an agreement."}, {"heading": "C CATEGORY TRACKING OVER SENTENCES", "text": "Here we show some representative examples demonstrating the ability of the chained input network (in reverse order) to recognize the category of beer exactly at the point in the sentence where it becomes clear. First, the probabilities are all close to 0.2, reflecting the uniform state. At the end of the sentences, the distribution due to the input is much less entropy."}, {"heading": "D SENTIMENT TRACKING OVER SENTENCES", "text": "Since each character is encountered in a review, we can present the review (with a graininess of 100 evenly distributed settings between 1 star and 5), which gives the review the highest probability. Thus, we can recognize not only the feeling of the review, but also the exact word and even the sign by which this feeling became evident."}, {"heading": "E NONLINEAR DYNAMICS OF NEGATION", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "F CLASSIFICATION RESULTS", "text": "We trained a category RNN, where the additional input to the network is a one-time encoding of one of five beer categories, from which we can derive the class probability via the conditional probabilities of the review and use the model in the reverse direction to predict the category of beer described in a review. Using a balanced test set of 5000 reviews, we evaluated the classification performance of the category RNN using two multinomial regression classifiers, one of which trains to the top 10,000 n-grams from the training set and the other to tf-idf converted n-grams. The confusion matrices for these experiments can be seen in Table 1, Table 2 and Table 3."}, {"heading": "G A PROSPECTIVE ENCODER CONCATENATION NETWORK", "text": "In this paper, we have presented and demonstrated the effectiveness of a simple technique for embedding auxiliary information xaux in a generative RNN by combining it with the character representation x (t) char in each sequence step. However, sometimes we do not simply want to create a predefined xaux representation, but learn a xaux representation. For example, to create a character-level caption, we would like to jointly learn a Convolutionary Neural Network to encode the image, and a generative RNN to output a caption. To accomplish this task at the character level, we propose the following network architecture and hypothesis, which offers the advantages of learning encoding while maintaining our ability to generate long passages at the character level. At train time, the xaux is passed to an encoder, whose output is then passed to a concatenated input network as auxiliary information."}, {"heading": "H LEARNING CURVES", "text": "For each task, we train two unsupervised donor networks at the sign level, so that we can harvest the weights for transplantation into the concatenated input networks. We train separate donor networks for the two tasks (evaluation and category modeling), as each is trained on a different subset of data (the beer substitute is selected for class balance between the 5 categories and is therefore smaller), these networks are trained to convergence (Figure 10). After the transplant, we train the concatenated input networks with high learning rates to drive the weights for auxiliary information to rapid growth. We suspect that this leads to an initial increase in loss, as seen in Figure 10a and Figure 10b, according to which the loss decreases rapidly."}], "references": [{"title": "Learning long-term dependencies with gradient descent is difficult", "author": ["Yoshua Bengio", "Patrice Simard", "Paolo Frasconi"], "venue": "Neural Networks, IEEE Transactions on,", "citeRegEx": "Bengio et al\\.,? \\Q1994\\E", "shortCiteRegEx": "Bengio et al\\.", "year": 1994}, {"title": "Finding structure in time", "author": ["Jeffrey L. Elman"], "venue": "Cognitive science,", "citeRegEx": "Elman.,? \\Q1990\\E", "shortCiteRegEx": "Elman.", "year": 1990}, {"title": "Learning to forget: Continual prediction with LSTM", "author": ["Felix A. Gers", "J\u00fcrgen Schmidhuber", "Fred Cummins"], "venue": "Neural computation,", "citeRegEx": "Gers et al\\.,? \\Q2000\\E", "shortCiteRegEx": "Gers et al\\.", "year": 2000}, {"title": "Generating sequences with recurrent neural networks", "author": ["Alex Graves"], "venue": "arXiv preprint arXiv:1308.0850,", "citeRegEx": "Graves.,? \\Q2013\\E", "shortCiteRegEx": "Graves.", "year": 2013}, {"title": "Long short-term memory", "author": ["Sepp Hochreiter", "J\u00fcrgen Schmidhuber"], "venue": "Neural Computation,", "citeRegEx": "Hochreiter and Schmidhuber.,? \\Q1997\\E", "shortCiteRegEx": "Hochreiter and Schmidhuber.", "year": 1997}, {"title": "Deep visual-semantic alignments for generating image descriptions", "author": ["Andrej Karpathy", "Li Fei-Fei"], "venue": "arXiv preprint arXiv:1412.2306,", "citeRegEx": "Karpathy and Fei.Fei.,? \\Q2014\\E", "shortCiteRegEx": "Karpathy and Fei.Fei.", "year": 2014}, {"title": "Backpropagation applied to handwritten zip code recognition", "author": ["Yann LeCun", "Bernhard Boser", "John S Denker", "Donnie Henderson", "Richard E Howard", "Wayne Hubbard", "Lawrence D Jackel"], "venue": "Neural computation,", "citeRegEx": "LeCun et al\\.,? \\Q1989\\E", "shortCiteRegEx": "LeCun et al\\.", "year": 1989}, {"title": "Gradient-based learning applied to document recognition", "author": ["Yann LeCun", "L\u00e9on Bottou", "Yoshua Bengio", "Patrick Haffner"], "venue": "Proceedings of the IEEE,", "citeRegEx": "LeCun et al\\.,? \\Q1998\\E", "shortCiteRegEx": "LeCun et al\\.", "year": 1998}, {"title": "A critical review of recurrent neural networks for sequence learning", "author": ["Zachary C. Lipton", "John Berkowitz", "Charles Elkan"], "venue": "arXiv preprint arXiv:1506.00019,", "citeRegEx": "Lipton et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Lipton et al\\.", "year": 2015}, {"title": "Deep captioning with multimodal recurrent neural networks (m-RNN)", "author": ["Junhua Mao", "Wei Xu", "Yi Yang", "Jiang Wang", "Alan Yuille"], "venue": "arXiv preprint arXiv:1412.6632,", "citeRegEx": "Mao et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Mao et al\\.", "year": 2014}, {"title": "From amateurs to connoisseurs: modeling the evolution of user expertise through online reviews", "author": ["Julian John McAuley", "Jure Leskovec"], "venue": "In Proceedings of the 22nd international conference on World Wide Web,", "citeRegEx": "McAuley and Leskovec.,? \\Q2013\\E", "shortCiteRegEx": "McAuley and Leskovec.", "year": 2013}, {"title": "Effect of Prior Domain Knowledge and Headings on Processing of Informative Text", "author": ["John R. Surber", "Mark Schroeder"], "venue": "Contemporary Educational Psychology,", "citeRegEx": "Surber and Schroeder.,? \\Q2007\\E", "shortCiteRegEx": "Surber and Schroeder.", "year": 2007}, {"title": "Generating text with recurrent neural networks", "author": ["Ilya Sutskever", "James Martens", "Geoffrey E. Hinton"], "venue": "In Proceedings of the 28th International Conference on Machine Learning", "citeRegEx": "Sutskever et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Sutskever et al\\.", "year": 2011}, {"title": "Sequence to sequence learning with neural networks", "author": ["Ilya Sutskever", "Oriol Vinyals", "Quoc V. Le"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Sutskever et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Sutskever et al\\.", "year": 2014}, {"title": "Lecture 6.5- RMSprop: Divide the gradient by a running average of its recent magnitude", "author": ["Tijmen Tieleman", "Geoffrey E. Hinton"], "venue": "https://www.youtube.com/watch?v= LGA-gRkLEsI,", "citeRegEx": "Tieleman and Hinton.,? \\Q2012\\E", "shortCiteRegEx": "Tieleman and Hinton.", "year": 2012}, {"title": "Translating videos to natural language using deep recurrent neural networks", "author": ["Subhashini Venugopalan", "Huijuan Xu", "Jeff Donahue", "Marcus Rohrbach", "Raymond Mooney", "Kate Saenko"], "venue": "arXiv preprint arXiv:1412.4729,", "citeRegEx": "Venugopalan et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Venugopalan et al\\.", "year": 2014}, {"title": "Show and tell: A neural image caption generator", "author": ["Oriol Vinyals", "Alexander Toshev", "Samy Bengio", "Dumitru Erhan"], "venue": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,", "citeRegEx": "Vinyals et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Vinyals et al\\.", "year": 2015}, {"title": "How transferable are features in deep neural networks", "author": ["Jason Yosinski", "Jeff Clune", "Yoshua Bengio", "Hod Lipson"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Yosinski et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Yosinski et al\\.", "year": 2014}, {"title": "Text understanding from scratch", "author": ["Xiang Zhang", "Yann LeCun"], "venue": "arXiv preprint arXiv:1502.01710,", "citeRegEx": "Zhang and LeCun.,? \\Q2015\\E", "shortCiteRegEx": "Zhang and LeCun.", "year": 2015}], "referenceMentions": [{"referenceID": 12, "context": "Character-level Recurrent Neural Networks (RNNs) have a remarkable ability to generate coherent text (Sutskever et al., 2011), appearing to hallucinate passages that plausibly resemble a training corpus.", "startOffset": 101, "endOffset": 125}, {"referenceID": 10, "context": "Our work focuses on reviews scraped from Beer Advocate (McAuley and Leskovec, 2013).", "startOffset": 55, "endOffset": 83}, {"referenceID": 13, "context": "Such conditioning of sequential output has been performed successfully with word-level models, for tasks including machine translation (Sutskever et al., 2014), image captioning (Vinyals et al.", "startOffset": 135, "endOffset": 159}, {"referenceID": 16, "context": ", 2014), image captioning (Vinyals et al., 2015; Karpathy and Fei-Fei, 2014; Mao et al., 2014), and even video captioning (Venugopalan et al.", "startOffset": 26, "endOffset": 94}, {"referenceID": 5, "context": ", 2014), image captioning (Vinyals et al., 2015; Karpathy and Fei-Fei, 2014; Mao et al., 2014), and even video captioning (Venugopalan et al.", "startOffset": 26, "endOffset": 94}, {"referenceID": 9, "context": ", 2014), image captioning (Vinyals et al., 2015; Karpathy and Fei-Fei, 2014; Mao et al., 2014), and even video captioning (Venugopalan et al.", "startOffset": 26, "endOffset": 94}, {"referenceID": 15, "context": ", 2014), and even video captioning (Venugopalan et al., 2014).", "startOffset": 35, "endOffset": 61}, {"referenceID": 5, "context": ", 2015; Karpathy and Fei-Fei, 2014; Mao et al., 2014), and even video captioning (Venugopalan et al., 2014). However, despite the aforementioned virtues of character-level models, no prior work, to our knowledge, has successfully trained them in such a supervised fashion. Most supervised approaches to word-level generative text models follow the encoder-decoder approach popularized by Sutskever et al. (2014). Some auxiliary input, which might be a sentence or an image, is encoded by an encoder model as a fixed-length vector.", "startOffset": 8, "endOffset": 412}, {"referenceID": 10, "context": "We focus on data scraped from Beer Advocate as originally collected and described by McAuley and Leskovec (2013). Beer Advocate is a large online review community boasting 1,586,614 reviews of 66,051 distinct items composed by 33,387 users.", "startOffset": 85, "endOffset": 113}, {"referenceID": 0, "context": "(2000), owing to their empirical successes and demonstrated ability to overcome the exploding/vanishing gradient problems suffered by other RNNs (Bengio et al., 1994).", "startOffset": 145, "endOffset": 166}, {"referenceID": 1, "context": "In this paper, we use RNNs containing long short term memory (LSTM) cells introduced by Hochreiter and Schmidhuber (1997) with forget gates introduced in Gers et al.", "startOffset": 88, "endOffset": 122}, {"referenceID": 1, "context": "In this paper, we use RNNs containing long short term memory (LSTM) cells introduced by Hochreiter and Schmidhuber (1997) with forget gates introduced in Gers et al. (2000), owing to their empirical successes and demonstrated ability to overcome the exploding/vanishing gradient problems suffered by other RNNs (Bengio et al.", "startOffset": 154, "endOffset": 173}, {"referenceID": 0, "context": "(2000), owing to their empirical successes and demonstrated ability to overcome the exploding/vanishing gradient problems suffered by other RNNs (Bengio et al., 1994). In short, each memory cell has an internal state s in which activation is preserved along a self-connected recurrent edge. Each cell also contains three sigmoidal gating units for input (i), output (o), and forget (f ) that respectively determine when to let activation into the internal state, when to pass activation to the rest of the network, and when to flush the cell\u2019s hidden state. The output of each LSTM layer is another sequence, allowing us to stack several layers of LSTMs as in Graves (2013). At step t, each LSTM layer h l receives input from the previous layer h (t) l\u22121 at the same sequence step and the same layer at the previous time step h l .", "startOffset": 146, "endOffset": 674}, {"referenceID": 8, "context": "While a thorough treatment of the LSTM is beyond the scope of this paper, we refer to our review of the literature (Lipton et al., 2015) for a gentler unpacking of the material.", "startOffset": 115, "endOffset": 136}, {"referenceID": 13, "context": "This has been done at the word-level with encoder-decoder models (Figure 2b), in which the auxiliary input is encoded and passed as the initial state to a decoder, which then must preserve this input signal across many sequence steps (Sutskever et al., 2014; Karpathy and Fei-Fei, 2014).", "startOffset": 234, "endOffset": 286}, {"referenceID": 5, "context": "This has been done at the word-level with encoder-decoder models (Figure 2b), in which the auxiliary input is encoded and passed as the initial state to a decoder, which then must preserve this input signal across many sequence steps (Sutskever et al., 2014; Karpathy and Fei-Fei, 2014).", "startOffset": 234, "endOffset": 286}, {"referenceID": 5, "context": ", 2014; Karpathy and Fei-Fei, 2014). Such models have successfully produced image captions (typically less than 10 tokens long), but seem impractical for generating full reviews at the character level because signal from xaux must survive for hundreds of sequence steps. We take inspiration from an analogy to human text generation. Consider that given a topic and told to speak at length, a human might be apt to meander and ramble. But given a subject to stare at, it is far easier to remain focused. The value of re-iterating high-level material is borne out in one study, Surber and Schroeder (2007), which showed that repetitive subject headings in textbooks resulted in faster learning, less rereading and more accurate answers to high-level questions.", "startOffset": 8, "endOffset": 604}, {"referenceID": 17, "context": "This scheme bears some resemblance to the pre-training common in the computer vision community (Yosinski et al., 2014).", "startOffset": 95, "endOffset": 118}, {"referenceID": 14, "context": "During training, examples are processed in mini-batches and we update weights with RMSprop (Tieleman and Hinton, 2012).", "startOffset": 91, "endOffset": 118}, {"referenceID": 1, "context": "In the seminal work, \u201cFinding Structure in Time\u201d, Elman (1990) speculated, \u201cone can ask whether the notion \u2018word\u2019 (or something which maps on to this concept) could emerge as a consequence of learning the sequential structure of letter sequences that form words and sentences (but in which word boundaries are not marked).", "startOffset": 50, "endOffset": 63}, {"referenceID": 1, "context": "In the seminal work, \u201cFinding Structure in Time\u201d, Elman (1990) speculated, \u201cone can ask whether the notion \u2018word\u2019 (or something which maps on to this concept) could emerge as a consequence of learning the sequential structure of letter sequences that form words and sentences (but in which word boundaries are not marked).\u201d In this work, an \u2018Elman RNN\u2019 was trained with 5 input nodes, 5 output nodes, and a single hidden layer of 20 nodes, each of which had a corresponding context unit to predict the next character in a sequence. At each step, the network received a binary encoding (not one-hot) of a character and tried to predict the next character\u2019s binary encoding. Elman plots the error of the net character by character, showing that it is typically high at the onset of words, but decreasing as it becomes clear what each word is. While these nets do not possess the size or capabilities of large modern LSTM networks trained on GPUs, this work lays the foundation for much of our research. Subsequently, in 2011, Sutskever et al. (2011) introduced the model of text generation on which we build.", "startOffset": 50, "endOffset": 1048}, {"referenceID": 6, "context": "Also relevant to our work is Zhang and LeCun (2015), which trains a strictly discriminative model of text at the character level using convolutional neural networks (LeCun et al., 1989; 1998).", "startOffset": 165, "endOffset": 191}, {"referenceID": 9, "context": "Several papers followed up on this idea, extending it to image captioning by swapping the encoder RNN for a convolutional neural network (Mao et al., 2014; Vinyals et al., 2015; Karpathy and Fei-Fei, 2014).", "startOffset": 137, "endOffset": 205}, {"referenceID": 16, "context": "Several papers followed up on this idea, extending it to image captioning by swapping the encoder RNN for a convolutional neural network (Mao et al., 2014; Vinyals et al., 2015; Karpathy and Fei-Fei, 2014).", "startOffset": 137, "endOffset": 205}, {"referenceID": 5, "context": "Several papers followed up on this idea, extending it to image captioning by swapping the encoder RNN for a convolutional neural network (Mao et al., 2014; Vinyals et al., 2015; Karpathy and Fei-Fei, 2014).", "startOffset": 137, "endOffset": 205}, {"referenceID": 11, "context": "Also relevant to our work is Zhang and LeCun (2015), which trains a strictly discriminative model of text at the character level using convolutional neural networks (LeCun et al.", "startOffset": 29, "endOffset": 52}, {"referenceID": 5, "context": "Also relevant to our work is Zhang and LeCun (2015), which trains a strictly discriminative model of text at the character level using convolutional neural networks (LeCun et al., 1989; 1998). Demonstrating success on both English and Chinese language datasets, their models achieve high accuracy on a number of classification tasks. Related works generating sequences in a supervised fashion follow the pattern of Sutskever et al. (2014) which uses a word-level encoder-decoder RNN to map sequences onto sequences.", "startOffset": 166, "endOffset": 439}, {"referenceID": 12, "context": "Further while Sutskever et al. (2011) demonstrates the use of a character level RNN as a scoring mechanism, this is the first paper to use such a scoring mechanism to infer labels in an unsupervised fashion, simultaneously learning to generate text and to perform tasks like regression and multiclass classification with high accuracy.", "startOffset": 14, "endOffset": 38}, {"referenceID": 12, "context": "Further while Sutskever et al. (2011) demonstrates the use of a character level RNN as a scoring mechanism, this is the first paper to use such a scoring mechanism to infer labels in an unsupervised fashion, simultaneously learning to generate text and to perform tasks like regression and multiclass classification with high accuracy. Our work is not the first to demonstrate a characterlevel classifier, as Zhang and LeCun (2015) offered such an approach.", "startOffset": 14, "endOffset": 432}], "year": 2017, "abstractText": "We present a character-level recurrent neural network that generates relevant and coherent text given auxiliary information such as a sentiment or topic. Using a simple input replication strategy, we preserve the signal of auxiliary input across wider sequence intervals than can feasibly be trained by back-propagation through time. Our main results center on a large corpus of 1.5 million beer reviews from BeerAdvocate. In generative mode, our network produces reviews on command, tailored to a star rating or item category. The generative model can also run in reverse, performing classification with surprising accuracy. Performance of the reverse model provides a straightforward way to determine what the generative model knows without relying too heavily on subjective analysis. Given a review, the model can accurately determine the corresponding rating and infer the beer\u2019s category (IPA, Stout, etc.). We exploit this capability, tracking perceived sentiment and class membership as each character in a review is processed. Quantitative and qualitative empirical evaluations demonstrate that the model captures meaning and learns nonlinear dynamics in text, such as the effect of negation on sentiment, despite possessing no a priori notion of words. Because the model operates at the character level, it handles misspellings, slang, and large vocabularies without any machinery explicitly dedicated to the purpose.", "creator": "LaTeX with hyperref package"}}}