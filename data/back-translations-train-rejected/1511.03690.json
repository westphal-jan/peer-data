{"id": "1511.03690", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "11-Nov-2015", "title": "Deep Multimodal Semantic Embeddings for Speech and Images", "abstract": "In this paper, we present a model which takes as input a corpus of images with relevant spoken captions and finds a correspondence between the two modalities. We employ a pair of convolutional neural networks to model visual objects and speech signals at the word level, and tie the networks together with an embedding and alignment model which learns a joint semantic space over both modalities. We evaluate our model using image search and annotation tasks on the Flickr8k dataset, which we augmented by collecting a corpus of 40,000 spoken captions using Amazon Mechanical Turk.", "histories": [["v1", "Wed, 11 Nov 2015 21:30:10 GMT  (2105kb,D)", "http://arxiv.org/abs/1511.03690v1", null]], "reviews": [], "SUBJECTS": "cs.CV cs.AI cs.CL", "authors": ["david harwath", "james glass"], "accepted": false, "id": "1511.03690"}, "pdf": {"name": "1511.03690.pdf", "metadata": {"source": "CRF", "title": "DEEP MULTIMODAL SEMANTIC EMBEDDINGS FOR SPEECH AND IMAGES", "authors": ["David Harwath"], "emails": ["glass}@mit.edu"], "sections": [{"heading": null, "text": "Index Terms - Neural Networks, Multimodal Semantic Embedding"}, {"heading": "1. INTRODUCTION AND RELATED WORK", "text": "In this work, we examine what is possible when these text transcripts are replaced by relevant visual images. Faced with a dataset that consists of image scenes with accompanying spoken audio labels segmented at the word level, we propose a model that is capable of associating spoken instances of the word \"dog\" with images of dogs, to name just one example. Our model relies on a pair of Convolutionary Neural Networks (CNNs), one for images and another for language, along with an orientation and embedding of the visual model. The outputs of the networks provide fixed-dimensional representations of variably sized visual objects and spoken words, which are then embedded in a common semantic space, allowing us to align the words in the captions of the objects they refer to in the image scene."}, {"heading": "2. MODEL DESCRIPTION", "text": "In addition to examples of visual objects, our overall goal is to be able to display examples of spoken words such as pointsar Xiv: 151 1.03 690v 1 [cs.C V] 11 Nov 201 5 in a single, high-dimensional vector space. In this vector space, for example, we want different spoken examples of the word \"dog\" to each other and also for neighboring image cultures that contain dogs. To achieve this, we need some means to convert image segments of different sizes as well as variable continuous tone waveforms into fixed-dimensional vector representations. In addition, we also need a way to get these vectors to adopt the property that combines semantically similar images and words. To achieve this, we use two separate neural network architectures, one for images and one for audio, which we then combine with an embedding model."}, {"heading": "2.1. Region Convolutional Neural Network", "text": "To identify a number of candidate regions in an image that is likely to contain significant objects, we use the model of the Region Convolutional Neural Network (RCNN) [10]. The object detector RCNN first works with selective search [11] to create a large list of supply regions, typically numbered in thousands for a given image. Each supply region is then fed into a CNN object classifier, which is used to extract the activations of the penultimate layer of neurons in the network. These activations form a fixed-dimensional (4096 in [10], as well as our work) feature vector representation of each supply region. A series of one-on-all support vector machines are then used to calculate detection values across some classes for each region, and highly overlapping regions with similar classification values are merged. Finally, the remaining regions can be summarized in the order of their maximum order of support vector machines are then used to compute detection values across all classes for each region, and in the order of their maximum follow-up image we rank all classes [6] in our classification values across all classes."}, {"heading": "2.2. Spectrogram Convolutional Neural Network", "text": "Previous efforts [6, 7] to semantically align text with objects in visual scenes have each benefited from the fact that text is naturally segmented into words, and all instances of the same word share the same orthography. On the other hand, the segmentation of continuous language into words is not trivial, and different spoken instances of the same underlying word will inevitably differ in duration, but also in their acoustic representations, influenced by factors such as the microphone and speaker characteristics, and the context in which the word was spoken. While a speech recognition system is a reasonable solution for building a spoken interface for the restoration of natural language, such as the one described in this work, we are more interested in studying the potential of neural networks to learn meaningful semantic representations that function directly at the level of the property we are training."}, {"heading": "2.3. Embedding Alignment Model", "text": "Considering an image-caption pair and their corresponding object-detection fields and word spectrograms, our task is to align each word with one of the detection fields found in the image. (To do this, we adopt the transformation model of [8], but with the objective function represented by [6]. However, we replace the text modeling page of Karpathy's models with our word spectrogram CNN, which allows us to align the image fragments directly with the segments of the voice audio. We provide a brief overview of the alignment model and the objective similarity. Let V = {vi | i = 1. 20} be the set of dI-dimensional vectors representing the activation of the penultimate layer of the RCNN for each detected image region, as described in Section 2.1. Also, let us leave W = {wj | j} the dW - dimensional vectors that represent the similar activations of the spectrogram in the CNN header."}, {"heading": "3. DATA", "text": "We have used a number of datasets that contain images alongside human-made text captions, such as Pascal, Flickr8k [14], Flickr30k [15], and MSCOCO [16], but none of these datasets contain any language data, so we have decided to collect our own audio data for our experiments. Due to their manageable size and ubiquity in the previous literature, we have decided to use Flickr8k as the starting point for our data collection. Flickr8k contains approximately 8,000 images from the Flickr photo-sharing website, each of which includes humans or animals. Each image has been commented on with a text caption by five different people, resulting in a total of 40,000 captions. To collect these descriptions, the authors turned to Amazon's mechanical services, which allow applicants to send post \"Human Intelligence Tasks\" (HITs)."}, {"heading": "4. EXPERIMENTS", "text": "We use stochastic gradient pedigree with a learning rate of 1e6 and impulse of 0.9, through stacks of 40 images to trainour embedding and alignment model, and run our training for 20 epochs. Training is performed using the standard 6,000 image pull from Flickr8k data, using the accompanying 30,000 captions. At each stack we randomly select, only one of the five captions associated with each image is used. We also tried several different settings for h, the dimension of the semantic embedding space, and found that the values between 512 and 1024 seemed to work well, in accordance with [8]. We also found that it was necessary to normalize the captions to prevent exploitation of the gradations. To evaluate the alignment and embedding of the model, we follow the example of [6, 8, 21] and use our model to perform a revaluation and accounting of the image."}, {"heading": "5. CONCLUSION", "text": "In this paper, we have outlined our initial efforts to construct a model that can learn a common semantic representation of spoken words and visual objects. At training times, the model requires only weak labels in the form of paired images and natural language subtitles. Our system aligns distinctive visual objects in the images with associated captions, building semantic representation across both modalities. We evaluate our model using Flickr8k image search and annotation tasks, and compare it to multiple systems that have access to the basic truth text. There are many ways we want to pursue this research. An in-depth examination of the performance gap between CNN voice embedding and basic truth texts is a logical first step, and increasing the amount of training data could shed some light on it. We also want to include word segmentation in the alignment scheme to reduce the need for forced alignment and make our attitudes more realistic."}, {"heading": "6. REFERENCES", "text": "[1] K. Barnard, P. Duygulu, D. Forsyth, N. DeFreitas, D. M. Blei, and M. I. Jordan, \"Matching words and pictures,\" in Journal of Machine Learning Research, 2003. [2] R. Socher and L. Fei-Fei, \"Connecting modalities: Semi-supervised segmentation and annotation of images using unaligned text corpora,\" in Proceedings of the 2009 Conference on Computer Vision and Pattern Recognition (CVPR), 2010. [3] C. Kong, K. Lin, M. Bansal, R. Urtasun, and S. Fidler, \"What are you talking about? text-to-image coreference,\" in Proceedings of the 2014 Conference on Computer Vision and Pattern Recognition (CVPR), 2014. [4] D. Lin, S. Fidler, C. Kong, and R. Urtasun search: Retving videos via complex,. \""}], "references": [{"title": "Connecting modalities: Semi-supervised segmentation and annotation of images using unaligned text corpora,", "author": ["R. Socher", "L. Fei-Fei"], "venue": "Proceedings of the 2009 Conference on Computer Vision and Pattern Recognition (CVPR),", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2010}, {"title": "What are you talking about? text-to-image coreference,", "author": ["C. Kong", "K. Lin", "M. Bansal", "R. Urtasun", "S. Fidler"], "venue": "Proceedings of the 2014 Conference on Computer Vision and Pattern Recognition (CVPR),", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2014}, {"title": "Visual semantic search: Retrieving videos via complex textual queries,", "author": ["D. Lin", "S. Fidler", "C. Kong", "R. Urtasun"], "venue": "Proceedings of the 2014 Conference on Computer Vision and Pattern Recognition (CVPR),", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2014}, {"title": "A joint model of language and perception for grounded attribute learning,", "author": ["C. Matuszek", "N. Fitzgerald", "L. Zettlemoyer", "L. Bo", "D. Fox"], "venue": "Proceedings of the 2012 International Conference on Machine Learning (ICML),", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2012}, {"title": "Deep visual-semantic alignments for generating image descriptions,", "author": ["A. Karpathy", "L. Fei-Fei"], "venue": "Proceedings of the 2015 Conference on Computer Vision and Pattern Recognition,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2015}, {"title": "Show and tell: A neural image caption generator,", "author": ["O. Vinyals", "A. Toshev", "S. Bengio", "D. Erhan"], "venue": "Proceedings of the 2015 Conference on Computer Vision and Pattern Recognition,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2015}, {"title": "Deep fragment embeddings for bidirectional image sentence mapping,", "author": ["A. Karpathy", "A. Joulin", "L. Fei-Fei"], "venue": "Proceedings of NIPS,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2014}, {"title": "Devise: A deep visualsemantic embedding model,", "author": ["A. Frome", "G.S. Corrado", "J. Shlens", "S. Bengio", "J. Dean", "M. Ranzato", "T. Mikolov"], "venue": "Proceedings of the Neural Information Processing Society,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2013}, {"title": "Rich feature hierarchies for accurate object detection and semantic segmentation,", "author": ["R. Girshick", "J. Donahue", "T. Darrell", "J. Malik"], "venue": "Proceedings of the 2013 Conference on Computer Vision and Pattern Recognition,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2013}, {"title": "Selective search for object recognition,", "author": ["J. Uijlings", "K. van de Sande", "T. Gevers", "A. Smeulders"], "venue": "in International Journal of Computer Vision,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2013}, {"title": "Word embeddings for speech recognition,", "author": ["S. Bengio", "G. Heigold"], "venue": "Proceedings of the 15th Conference of the International Speech Communication Association,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2014}, {"title": "Caffe: Convolutional architecture for fast feature embedding,", "author": ["Y. Jia", "E. Shelhamer", "J. Donahue", "S. Karayev", "J. Long", "R. Girshick", "S. Guadarrama", "T. Darrell"], "venue": "in arXiv preprint arXiv:1408.5093,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2014}, {"title": "Collecting image annotations using amazon\u2019s mechanical turk,", "author": ["C. Rashtchian", "P. Young", "M. Hodosh", "J. Hockenmaier"], "venue": "Proceedings of the NAACL HLT 2010 Workshop on Creating Speech and Language Data with Amazon\u2019s Mechanical Turk,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2010}, {"title": "From image descriptions to visual denotations: New similarity metrics for semantic inference over event descriptions,", "author": ["P. Young", "A. Lai", "M. Hodosh", "J. Hockenmaier"], "venue": "in Transactions for the Association of Computational Linguistics,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2014}, {"title": "Microsoft coco: Common objects in context,", "author": ["T.Y. Lin", "M. Marie", "S. Belongie", "J. Hays", "P. Perona", "D. Ramanan", "P. Dollar", "C.L. Zitnick"], "venue": "in arXiv preprint arXiv:1405.0312,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2014}, {"title": "Spoke: A framework for building speechenabled websites,", "author": ["P. Saylor"], "venue": "M.S. thesis, Massachusetts Institute of Technology,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2015}, {"title": "The Kaldi speech recognition toolkit,", "author": ["D. Povey", "A. Ghoshal", "G. Boulianne", "L. Burget", "O. Glembek", "N. Goel", "M. Hannemann", "P. Motlicek", "Y. Qian", "P. Schwarz", "J. Silovsky", "G. Stemmer", "K. Vesely"], "venue": "IEEE 2011 Workshop on Automatic Speech Recognition and Understanding,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2011}, {"title": "Imagenet: A large scale hierarchical image database,", "author": ["J. Deng", "W. Dong", "R. Socher", "L.J. Li", "K. Li", "L. Fei-Fei"], "venue": "Proceedings of the 2009 Conference on Computer Vision and Pattern Recognition (CVPR),", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2009}, {"title": "The design for the wall street journal-based csr corpus,", "author": ["D.B. Paul", "J.M. Baker"], "venue": "Proceedings of the Association for Computational Linguistics Workshop of Speech and Natural Language,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 1992}, {"title": "Grounded compositional semantics for finding and describing images with sentences,", "author": ["R. Socher", "A. Karpathy", "Q.V. Le", "C.D. Manning", "A.Y. Ng"], "venue": "in Transactions of the Association for Computational Linguistics (TACL),", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2014}, {"title": "Improving word representations via global context and  multiple word prototypes,", "author": ["E. Huang", "R. Socher", "C.D. Manning", "A.Y. Ng"], "venue": "Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2012}], "referenceMentions": [{"referenceID": 0, "context": "[1] relied on pre-segmented and labelled images to estimate joint distributions over words and objects, while Socher [2] learned a latent meaning space covering images and words learned on non-parallel data.", "startOffset": 117, "endOffset": 120}, {"referenceID": 1, "context": "[3] took visual scenes with high level captions, parsed the text, detected visual objects, and then aligned the two modalities with a Markov random field.", "startOffset": 0, "endOffset": 3}, {"referenceID": 2, "context": "Lin et al [4] aligned semantic graphs over text queries to relational graphs over objects in videos to perform natural language video search.", "startOffset": 10, "endOffset": 13}, {"referenceID": 3, "context": "[5] employed separate classifiers over text and visual objects that shared the same label sets.", "startOffset": 0, "endOffset": 3}, {"referenceID": 4, "context": "While a large number of papers have been published on this subject, recent efforts using recurrent deep neural networks [6, 7] have made tremendous progress and generated much interest in the field.", "startOffset": 120, "endOffset": 126}, {"referenceID": 5, "context": "While a large number of papers have been published on this subject, recent efforts using recurrent deep neural networks [6, 7] have made tremendous progress and generated much interest in the field.", "startOffset": 120, "endOffset": 126}, {"referenceID": 4, "context": "While our work in this paper does not aim to generate captions for images, it was originally inspired by the text-to-image alignment models presented by Karpathy in [6, 8].", "startOffset": 165, "endOffset": 171}, {"referenceID": 6, "context": "While our work in this paper does not aim to generate captions for images, it was originally inspired by the text-to-image alignment models presented by Karpathy in [6, 8].", "startOffset": 165, "endOffset": 171}, {"referenceID": 4, "context": "In [6], Karpathy uses a refined version of the alignment model presented in [8] to produce training exemplars for a caption-generating RNN language model that can be conditioned on visual features.", "startOffset": 3, "endOffset": 6}, {"referenceID": 6, "context": "In [6], Karpathy uses a refined version of the alignment model presented in [8] to produce training exemplars for a caption-generating RNN language model that can be conditioned on visual features.", "startOffset": 76, "endOffset": 79}, {"referenceID": 7, "context": "[9] who trained separate deep neural networks for language modeling as well as visual object classification.", "startOffset": 0, "endOffset": 3}, {"referenceID": 8, "context": "In order to detect a set of candidate regions in an image which are likely to contain meaningful objects, we use the Region Convolutional Neural Network (RCNN) model [10].", "startOffset": 166, "endOffset": 170}, {"referenceID": 9, "context": "The RCNN object detector works by first using selective search [11] to build a large list of proposal regions, typically numbering in the thousands for a given image.", "startOffset": 63, "endOffset": 67}, {"referenceID": 8, "context": "These activations form a fixed-dimensional (4096 in [10], as well as our work) feature vector representation of each proposal region.", "startOffset": 52, "endOffset": 56}, {"referenceID": 4, "context": "In our work, we follow [6] and take the top 19 detected regions along with the entire image frame, resulting in 20 regions per image.", "startOffset": 23, "endOffset": 26}, {"referenceID": 4, "context": "Previous efforts [6, 7] to perform semantic alignment of text to objects in image scenes have benefited from the fact that text is naturally segmented into words, and all instances of the same word share the same orthography.", "startOffset": 17, "endOffset": 23}, {"referenceID": 5, "context": "Previous efforts [6, 7] to perform semantic alignment of text to objects in image scenes have benefited from the fact that text is naturally segmented into words, and all instances of the same word share the same orthography.", "startOffset": 17, "endOffset": 23}, {"referenceID": 4, "context": "While a speech recognition system is a reasonable solution for building a spoken interface for natural language image retrieval systems such as the one described in [6], in this work we are more interested in investigating the potential of neural networks to learn meaningful semantic representations which operate directly on the feature level.", "startOffset": 165, "endOffset": 168}, {"referenceID": 10, "context": "In [12], the authors trained a CNN isolated word recognizer and utilized it for N-best recognition hypothesis re-ranking; here, we propose to use a similar CNN to model the spectrogram of each isolated word in the image captions.", "startOffset": 3, "endOffset": 7}, {"referenceID": 10, "context": "Standard CNNs expect their inputs to be of a fixed size, so in order to accommodate our variable duration words we follow [12] and choose to embed their spectrograms in a fixed duration window, applying zero-padding and truncation when necessary.", "startOffset": 122, "endOffset": 126}, {"referenceID": 10, "context": "While [12] found that a 2 second window was sufficient to capture the duration of 97% of the words in their corpus, in our case a 1 second long window is long enough to capture 99.", "startOffset": 6, "endOffset": 10}, {"referenceID": 11, "context": "We rely on the Caffe [13] toolkit to train our networks and extract the word spectrogram features.", "startOffset": 21, "endOffset": 25}, {"referenceID": 6, "context": "To do this, we adopt the transform model from [8] but with the objective function presented by [6].", "startOffset": 46, "endOffset": 49}, {"referenceID": 4, "context": "To do this, we adopt the transform model from [8] but with the objective function presented by [6].", "startOffset": 95, "endOffset": 98}, {"referenceID": 4, "context": "In [6], Karpathy uses a max margin objective function which forces matching image-caption pairs to have a higher similarity score than mismatched pairs, by a margin.", "startOffset": 3, "endOffset": 6}, {"referenceID": 4, "context": "Recent works on natural language image caption generation [6, 7] have utilized a number of datasets which contain images alongside human-generated text captions, such as Pascal, Flickr8k [14], Flickr30k [15], and MSCOCO [16].", "startOffset": 58, "endOffset": 64}, {"referenceID": 5, "context": "Recent works on natural language image caption generation [6, 7] have utilized a number of datasets which contain images alongside human-generated text captions, such as Pascal, Flickr8k [14], Flickr30k [15], and MSCOCO [16].", "startOffset": 58, "endOffset": 64}, {"referenceID": 12, "context": "Recent works on natural language image caption generation [6, 7] have utilized a number of datasets which contain images alongside human-generated text captions, such as Pascal, Flickr8k [14], Flickr30k [15], and MSCOCO [16].", "startOffset": 187, "endOffset": 191}, {"referenceID": 13, "context": "Recent works on natural language image caption generation [6, 7] have utilized a number of datasets which contain images alongside human-generated text captions, such as Pascal, Flickr8k [14], Flickr30k [15], and MSCOCO [16].", "startOffset": 203, "endOffset": 207}, {"referenceID": 14, "context": "Recent works on natural language image caption generation [6, 7] have utilized a number of datasets which contain images alongside human-generated text captions, such as Pascal, Flickr8k [14], Flickr30k [15], and MSCOCO [16].", "startOffset": 220, "endOffset": 224}, {"referenceID": 15, "context": "We use the Spoke JavaScript framework [17] as the basis of our audio collection HIT.", "startOffset": 38, "endOffset": 42}, {"referenceID": 16, "context": "7% of the collected utterances were easily aligned to their caption text using our Kaldi [18] forced alignment system.", "startOffset": 89, "endOffset": 93}, {"referenceID": 12, "context": "Our splits correspond with the training, validation, and testing splits given by [14].", "startOffset": 81, "endOffset": 85}, {"referenceID": 17, "context": "Because the Flickr8k corpus contains a small number of images and captions relative to datasets such as ImageNet [19], we follow the example of [6] and use the off-the-shelf RCNN provided by [10] trained on ImageNet to extract the 4096-dimensional visual object embeddings.", "startOffset": 113, "endOffset": 117}, {"referenceID": 4, "context": "Because the Flickr8k corpus contains a small number of images and captions relative to datasets such as ImageNet [19], we follow the example of [6] and use the off-the-shelf RCNN provided by [10] trained on ImageNet to extract the 4096-dimensional visual object embeddings.", "startOffset": 144, "endOffset": 147}, {"referenceID": 8, "context": "Because the Flickr8k corpus contains a small number of images and captions relative to datasets such as ImageNet [19], we follow the example of [6] and use the off-the-shelf RCNN provided by [10] trained on ImageNet to extract the 4096-dimensional visual object embeddings.", "startOffset": 191, "endOffset": 195}, {"referenceID": 18, "context": "Similarly, we employ supervised pretraining for the word spectrogram CNN using the Wall Street Journal SI-284 split [20].", "startOffset": 116, "endOffset": 120}, {"referenceID": 6, "context": "We tried several different settings for h, the dimension of the semantic embedding space, and found that values between 512 and 1024 seemed to work well, in line with [8].", "startOffset": 167, "endOffset": 170}, {"referenceID": 4, "context": "To evaluate the alignment and embedding model, we follow the example of [6, 8, 21] and use our model to perform image retrieval and annotation.", "startOffset": 72, "endOffset": 82}, {"referenceID": 6, "context": "To evaluate the alignment and embedding model, we follow the example of [6, 8, 21] and use our model to perform image retrieval and annotation.", "startOffset": 72, "endOffset": 82}, {"referenceID": 19, "context": "To evaluate the alignment and embedding model, we follow the example of [6, 8, 21] and use our model to perform image retrieval and annotation.", "startOffset": 72, "endOffset": 82}, {"referenceID": 20, "context": "Table 1 details the results of our system (\u201cSpectrogram CNN\u201d), as well as a comparison to replacing the word spectrogram embeddings with 200dimensional word vectors taken from [22].", "startOffset": 176, "endOffset": 180}, {"referenceID": 19, "context": "[21] and Karpathy [8].", "startOffset": 0, "endOffset": 4}, {"referenceID": 6, "context": "[21] and Karpathy [8].", "startOffset": 18, "endOffset": 21}, {"referenceID": 6, "context": "While our text + word vector system outperforms [8], the model is more similar to Karpathy\u2019s refinements made in [6] but with a single layer word embedding network rather than a bidirectional recurrent neural network.", "startOffset": 48, "endOffset": 51}, {"referenceID": 4, "context": "While our text + word vector system outperforms [8], the model is more similar to Karpathy\u2019s refinements made in [6] but with a single layer word embedding network rather than a bidirectional recurrent neural network.", "startOffset": 113, "endOffset": 116}, {"referenceID": 4, "context": "[6] reports high recalls on the Flickr30k data (50.", "startOffset": 0, "endOffset": 3}, {"referenceID": 19, "context": "[21] 28.", "startOffset": 0, "endOffset": 4}, {"referenceID": 6, "context": "Karpathy [8] 42.", "startOffset": 9, "endOffset": 12}], "year": 2015, "abstractText": "In this paper, we present a model which takes as input a corpus of images with relevant spoken captions and finds a correspondence between the two modalities. We employ a pair of convolutional neural networks to model visual objects and speech signals at the word level, and tie the networks together with an embedding and alignment model which learns a joint semantic space over both modalities. We evaluate our model using image search and annotation tasks on the Flickr8k dataset, which we augmented by collecting a corpus of 40,000 spoken captions using Amazon Mechanical Turk.", "creator": "LaTeX with hyperref package"}}}