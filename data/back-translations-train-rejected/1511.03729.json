{"id": "1511.03729", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "11-Nov-2015", "title": "Larger-Context Language Modelling", "abstract": "In this work, we propose a novel method to incorporate corpus-level discourse information into language modelling. We call this larger-context language model. We introduce a late fusion approach to a recurrent language model based on long short-term memory units (LSTM), which helps the LSTM unit keep intra-sentence dependencies and inter-sentence dependencies separate from each other. Through the evaluation on three corpora (IMDB, BBC, and PennTree Bank), we demon- strate that the proposed model improves perplexity significantly. In the experi- ments, we evaluate the proposed approach while varying the number of context sentences and observe that the proposed late fusion is superior to the usual way of incorporating additional inputs to the LSTM. By analyzing the trained larger- context language model, we discover that content words, including nouns, adjec- tives and verbs, benefit most from an increasing number of context sentences. This analysis suggests that larger-context language model improves the unconditional language model by capturing the theme of a document better and more easily.", "histories": [["v1", "Wed, 11 Nov 2015 23:24:29 GMT  (1238kb,D)", "https://arxiv.org/abs/1511.03729v1", null], ["v2", "Fri, 25 Dec 2015 17:51:01 GMT  (1635kb,D)", "http://arxiv.org/abs/1511.03729v2", null]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["tian wang", "kyunghyun cho"], "accepted": false, "id": "1511.03729"}, "pdf": {"name": "1511.03729.pdf", "metadata": {"source": "CRF", "title": "LARGER-CONTEXT LANGUAGE MODELLING WITH RECURRENT NEURAL NETWORK", "authors": ["Tian Wang", "Kyunghyun Cho"], "emails": ["t.wang@nyu.edu", "kyunghyun.cho@nyu.edu"], "sections": [{"heading": "1 INTRODUCTION", "text": "The goal of language modeling is to estimate the probability distribution of different linguistic units, e.g. words, sentences (Rosenfeld, 2000). Early techniques include count-based n-gram language models, which aim to determine the probability distribution of a particular word observed after a certain number of previous words. Later, Bengio et al. (2003) proposed the neural language model, which achieved significant improvements in perplexity over number-based language models. Bengio et al. demonstrated that this neural language model simultaneously achieved the conditional probability of the last word in a sequence and vector representation for each word in a predefined vocabulary."}, {"heading": "2 BACKGROUND: STATISTICAL LANGUAGE MODELLING", "text": "It is often assumed that each sentence in the entire document is independent of each other, but the task of language modeling is often reduced to assigning a probability to a single sentence P (Sl).A sentence Sl = (1).A sentence Sl = (1).A sentence Sl = (2).A sentence Sl = (2).A sentence Sl = (2).A sentence Sl = (2).A sentence Sl = (2).A sentence Sl = (2).A sentence Sl = (2).A sentence Sl = (2).A sentence Sl = (2).A sentence Sl = (2).A sentence Sl = (2).A sentence Sl = (2).A sentence Sl = (2).A sentence (2).A =.A sentence (2).A =.A (2).A sentence (2).A =.A =..A sentence Sl (2).A =.A (2).A sentence (2).A =.A (2).A sentence (.L).A =.A (2).A sentence (2).A =.A (2).A sentence (2).A =.A =.A sentence (2).A =.A (2).A =.A sentence (2).A =.A sentence (2).A =.A (2).A sentence (.A =.A =.A =.A sentence (2).A =.A (2).A sentence (.A =.A =.A =.A =.A sentence (2).A =.A =.A sentence (.A).A =.A sentence (.A =.A sentence (2).A =.A =.A sentence (.A).A sentence (.A =.A =.A sentence (.A).A sentence (.L =.A).A sentence (.L =.L =.L =.A sentence (.A).A sentence (.L =.A).A sentence (2).L =.L (.L =.A).A sentence (.L =.A).A sentence (.A).A"}, {"heading": "2.1 LANGUAGE MODELLING WITH LONG SHORT-TERM MEMORY", "text": "Here we briefly describe a short-term memory unit that is widely used as a recursive activation function \u03c6 (see Equation (4)) for speech modelling (see e.g. Graves, 2013).One layer of long-term memory (LSTM) consists of three gates and a single memory cell. Three gate input, output and forget- are calculated by it = \u03c3 (Wixt + Uiht \u2212 1 + bi) (6) ot = \u03c3 (Woxt + Uoht \u2212 1 + bo) (7) ft = \u03c3 (Wfxt + Ufht \u2212 1 + bf), (8), where \u03c3 is a sigmoid function. xt is the input at the t-th time test. The memory cell is calculated by ct = ft ct \u2212 1 + it tanh (Wcx + Ufht \u2212 1 + bf), (9), which is an element-by-element multiplication. This adaptive, leaky integration of the memory cell Uc, for example, enables this long-term output of STM-1, where the STM-1 does not work without problems."}, {"heading": "3 LARGER-CONTEXT LANGUAGE MODELLING", "text": "In this work, we do not aim to improve the probability estimation of the sentence plane P (S) (see Equation (2)), but to directly improve the probability of the corpus plane P (D) of Equation (1). One thing we noticed at the beginning of this work is that it is not necessary for us to make the assumption of mutual independence of sentences in a corpus. Rather, much like we model a sentence probability, we can loosen this assumption of P (D) \u2248 L (Sl \u2212 1l \u2212 n), (10) where Sl \u2212 1l \u2212 n = (Sl \u2212 n, Sl \u2212 n + 1). n determines how many preceding sentences each conditional sentence probability condition is based on, similar to what happens with an ordinary n-gram language modeling."}, {"heading": "3.1 CONTEXT REPRESENTATION", "text": "A sequence of preceding sentences can be represented in many different ways. Here, let us describe two alternatives that we are testing in connection with the network = 4. The first representation is simply to put all the words in the preceding sentences into a single vector. \u2212 This vector is multiplied from the left by a matrix P, which is matched with all the other parameters: p = P. We call this representation p a bag-of-words (BoW) context. Secondly, we try to represent the preceding context sentences as a sequence of words. \u2212 Each bag-of-words sj is the bag-of-words representation of the j-of-words context set, and they become a sequence (sl \u2212 n,."}, {"heading": "3.2 CONDITIONAL LSTM", "text": "Early Fusion Once the context vector p is calculated from the n preceding sentences, we need to feed this sentence into the sentence-level recurring language model. A simplest way is to simply consider it as input at each step, like thatx = E > wt + Wpp, where E is the word that embeds the matrix that transforms the most unified vector of the t-word into a continuous word vector. This x is used by the LSTM layer as input, as described in Sec. 2.1. We call this approach an early fusion of the context into language modeling. Late Fusion In addition to this approach, we propose a modification of the LSTM so that it better incorporates the context from the preceding sentences (summarized by pt.) The basic idea is to keep the dependencies within the sentence being modeled (intra-sentence dependencies)."}, {"heading": "4 RELATED WORK", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.1 CONTEXT-DEPENDENT RECURRENT LANGUAGE MODEL", "text": "This possibility of extending a neural or recursive language model has been discussed in the past. In particular (Mikolov & Zweig, 2012), an approach called context-dependent recursive neural networks, which is very similar to the proposed approach, was proposed; the basic idea of their approach is to use a thematic distribution that is presented as a vector of a previous sentence. Nevertheless, their main goal was to use this recursive neural network each time, the words used to propagate the topic often went beyond the sentence boundary."}, {"heading": "4.2 DIALOGUE MODELLING WITH RECURRENT NEURAL NETWORKS", "text": "A more similar model to the proposed, more contextually recurring language model is a hierarchically recursive encoder decoder (HRED) recently proposed by Serban et al. (2015). HRED consists of three recursive neural networks to model a dialogue between two people from the perspective of one of them to whom we refer as a speaker. If we look at the last utterance of the speaker modeled by the decoder of HRED, this model can be viewed as a more contextually recurring language model with early fusion. Apart from the fact that the ultimate goals differ (in their case dialogue modeling and in our case document modeling), there are two technical differences. Firstly, they test only with the early fusion approach. We show later in the experiments that the proposed late fusion yields better speech quality than the early fusion. Secondly, we use a sequence of words to represent the sequence of the preceding sentences while the RED word model is a sequence of words."}, {"heading": "4.3 SKIP-THOUGHT VECTORS", "text": "Perhaps the most similar work is the Skip Thought Vector by Kiros et al. (2015), in which a relapsing neural network is trained to read a current sentence as a word sequence and extract a so-called skip thought vector of the sentence. There are two other relapsing neural networks that model preceding and subsequent sentences. If we only look at the prediction of the following sentence, this model becomes a more comprehensive relapsing language model that considers a single preceding sentence as context. As with the other work discussed so far, the main difference lies in the ultimate goal of the model. Kiros et al. (2015) focused entirely on using their model to extract a good, generic sentence vector, while in this essay we focus on obtaining a good language model. There are fewer technical differences: firstly, the skip thought model only causes the immediate preceding sentence, while we expand on several preceding sentences."}, {"heading": "4.4 NEURAL MACHINE TRANSLATION: CONDITIONAL LANGUAGE MODELLING", "text": "Another related approach is neural machine translation (Forcada & N, eco, 1997; Kalchbrenner & Blunsom, 2013; Cho et al., 2014b; Sutskever et al., 2014; Bahdanau et al., 2014). Neural machine translation often uses two recursive neural networks: the first recursive neural network, an encoder, reads a set of sources that is presented as a sequence of words in a source language to form a context vector or set of context vectors; the other recursive neural network, a decoder, then models the target translation conditioned on that source context, similar to the proposed recursive language model when we consider the source sentence as the preceding sentence in a corpus; the main difference is in the final application, machine translation versus language modeling, and the technical differences between the proposed neural translation and the larger context."}, {"heading": "4.5 CONTEXT-DEPENDENT QUESTION-ANSWERING MODELS", "text": "The context-dependent question and answer is a task in which a model is asked to answer a question based on the facts of a paragraph in the natural language. Question and answer are often formulated in such a way that a missing word is inserted into a query sentence (Hermann et al., 2015; Hill et al., 2015). This task is closely related to the broader language model proposed in this essay in the sense that the goal is to create a model that contains facts about that query (qk > k, D), (12) in which qk is the missing k word in a Q query, and q < k and q > k are the context words from the query. D is the paragraph that contains facts about that query. Often, it is explicitly constructed so that the query q does not appear in paragraph q, and q < k > k are the context words from the query."}, {"heading": "5 EXPERIMENTAL SETTINGS", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "5.1 MODELS", "text": "There are six possible combinations of the proposed methods. First, there are two ways to represent the context sentences; (1) Word bags (BoW) and (2) a sequence of Word bags (SeqBoW), from Sec. 3.1. There are two separate ways to include the SeqBoW; (1) with attention mechanism (ATT) and (2) without. Then, there are two ways to insert the context vector into the main recurring language model (RLM); (1) early fusion (EF) and (2) late fusion (LF), from Sec. 3.2. We will use these six possible models by 1. RLM-BoW-EF-n 2. RLM-EF-n 3. RLM-SeqBoW-EF-n 3. RLM-SeqBoW-ATT-EF-n 4. RLF-LF-LF-n 5. RLLM-SeqBoW-n-n-n-4-W-T-T-T-K."}, {"heading": "5.2 DATASETS", "text": "For detailed statistics, see Table 1.IMDB Movie Reviews A series of movie reviews is an ideal dataset to evaluate many different settings of the proposed contextual language models, since each review is likely to be based on a single topic (the movie being reviewed.) A set of words or font style is well determined based on the previous sentences. We use the IMDB Move Review Corpus (IMDB) prepared by Maas et al. (2011). (1) This corpus has 75k training reviews and 25k test reviews. We use the 30k most common words for recurring language models. Similar to movie reviews, each new article tends to convey a single topic. We use the BBC corpus prepared by Greene & Cunningham (2006). (2) Unlike the IMDB corpus, we use the 30k most common words for recurring language models."}, {"heading": "6 RESULTS AND ANALYSIS", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "6.1 CORPUS-LEVEL PERPLEXITY", "text": "We evaluated the models, including all proposed approaches (RLM- {BoW, SeqBoW} - {EF, LF} -n), on the IMDB corpus. In Fig. 2 (a), we see three major trends. First, RLM-BoW, 1 http: / / ai.Bndr / Scripts / tokenizer / tokenizer / ucd.ie / datasets / bbc.htSe / moSmt-smt / mosesdecoh / srteeSe / eScripteS / tokenizer / tokenizer.ie"}, {"heading": "6.2 ANALYSIS: PERPLEXITY PER PART-OF-SPEECH TAG", "text": "Next, we tried to figure out why the context-dependent recurring language model outperforms the unconditional recurring language model. To do this, we calculated the perplexity per part of the speech (POS).4 We then calculated the perplexity of each word and determined it for each day type individually. Of the 36 POS keywords used by the Stanford POS Tagger, we looked at the perplexity of the ten most common keywords (NN, IN, DT, JJ, RB, VBZ, PRP, CC). Among the 36 POS keywords used by the Stanford POS Tagger, we looked at the perplexity of the ten most common keywords (NN, IN, DT, JJ, RB, VBZ, CC)."}, {"heading": "7 CONCLUSION", "text": "In this paper, we proposed a method to improve the language model at the body level by incorporating a larger context. Using this model leads to an improvement in perplexity on the corpora IMDB, BBC and Penn Treebank and confirms the advantage of providing a larger context to a recurring language model.From our experiments, we found that the sequence of word bags with attention is better than word bags for displaying the context sentences (see paragraph 3.1), and late fusion is better than the early fusion to feed the context vector into the main recurring language model (see paragraph 3.2).Our part-of-language analysis found that substantive words, including nouns, adjectives and verbs, benefit most from an increasing number of context sentences (see paragraph 6.2).This analysis suggests that a larger context-related language model improves perplexity because it should be easier to recognize the subject of a document and to grasp the larger aspects of it in order to better understand the larger one of those aspects rather than the larger one."}, {"heading": "ACKNOWLEDGMENTS", "text": "This work is part of the DS-GA 1010-001 Independent Study in Data Science course at the Center for Data Science at New York University."}], "references": [{"title": "Neural machine translation by jointly learning to align and translate", "author": ["Bahdanau", "Dzmitry", "Cho", "Kyunghyun", "Bengio", "Yoshua"], "venue": "arXiv preprint arXiv:1409.0473,", "citeRegEx": "Bahdanau et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2014}, {"title": "Pragmatic neural language modelling in machine translation", "author": ["Baltescu", "Paul", "Blunsom", "Phil"], "venue": "arXiv preprint arXiv:1412.7119,", "citeRegEx": "Baltescu et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Baltescu et al\\.", "year": 2014}, {"title": "Learning long-term dependencies with gradient descent is difficult", "author": ["Bengio", "Yoshua", "Simard", "Patrice", "Frasconi", "Paolo"], "venue": "Neural Networks, IEEE Transactions on,", "citeRegEx": "Bengio et al\\.,? \\Q1994\\E", "shortCiteRegEx": "Bengio et al\\.", "year": 1994}, {"title": "A neural probabilistic language model", "author": ["Bengio", "Yoshua", "Ducharme", "R\u00e9jean", "Vincent", "Pascal", "Janvin", "Christian"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "Bengio et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Bengio et al\\.", "year": 2003}, {"title": "On the properties of neural machine translation: Encoder-decoder approaches", "author": ["Cho", "Kyunghyun", "van Merrienboer", "Bart", "Bahdanau", "Dzmitry", "Bengio", "Yoshua"], "venue": "In Eighth Workshop on Syntax, Semantics and Structure in Statistical Translation", "citeRegEx": "Cho et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Cho et al\\.", "year": 2014}, {"title": "Learning phrase representations using rnn encoderdecoder for statistical machine translation", "author": ["Cho", "Kyunghyun", "Van Merri\u00ebnboer", "Bart", "Gulcehre", "Caglar", "Bahdanau", "Dzmitry", "Bougares", "Fethi", "Schwenk", "Holger", "Bengio", "Yoshua"], "venue": "arXiv preprint arXiv:1406.1078,", "citeRegEx": "Cho et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Cho et al\\.", "year": 2014}, {"title": "The psychological functions of function words", "author": ["Chung", "Cindy", "Pennebaker", "James W"], "venue": "Social communication,", "citeRegEx": "Chung et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Chung et al\\.", "year": 2007}, {"title": "Gated feedback recurrent neural networks", "author": ["Chung", "Junyoung", "Gulcehre", "Caglar", "Cho", "Kyunghyun", "Bengio", "Yoshua"], "venue": "In Proceedings of the 32nd International Conference on Machine Learning", "citeRegEx": "Chung et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Chung et al\\.", "year": 2015}, {"title": "Latent semantic analysis", "author": ["Dumais", "Susan T"], "venue": "Annual review of information science and technology,", "citeRegEx": "Dumais and T.,? \\Q2004\\E", "shortCiteRegEx": "Dumais and T.", "year": 2004}, {"title": "Recursive hetero-associative memories for translation", "author": ["Forcada", "Mikel L", "\u00d1eco", "Ram\u00f3n P"], "venue": "Biological and Artificial Computation: From Neuroscience to Technology,", "citeRegEx": "Forcada et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Forcada et al\\.", "year": 1997}, {"title": "Generating sequences with recurrent neural networks", "author": ["Graves", "Alex"], "venue": "arXiv preprint arXiv:1308.0850,", "citeRegEx": "Graves and Alex.,? \\Q2013\\E", "shortCiteRegEx": "Graves and Alex.", "year": 2013}, {"title": "Practical solutions to the problem of diagonal dominance in kernel document clustering", "author": ["Greene", "Derek", "Cunningham", "P\u00e1draig"], "venue": "In Proc. 23rd International Conference on Machine learning", "citeRegEx": "Greene et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Greene et al\\.", "year": 2006}, {"title": "Lstm: A search space odyssey", "author": ["Greff", "Klaus", "Srivastava", "Rupesh Kumar", "Koutn\u0131\u0301k", "Jan", "Steunebrink", "Bas R", "Schmidhuber", "J\u00fcrgen"], "venue": "arXiv preprint arXiv:1503.04069,", "citeRegEx": "Greff et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Greff et al\\.", "year": 2015}, {"title": "Scalable modified Kneser-Ney language model estimation", "author": ["Heafield", "Kenneth", "Pouzyrevsky", "Ivan", "Clark", "Jonathan H", "Koehn", "Philipp"], "venue": "In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics,", "citeRegEx": "Heafield et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Heafield et al\\.", "year": 2013}, {"title": "Teaching machines to read and comprehend", "author": ["Hermann", "Karl Moritz", "Ko\u010disk\u1ef3", "Tom\u00e1\u0161", "Grefenstette", "Edward", "Espeholt", "Lasse", "Kay", "Will", "Suleyman", "Mustafa", "Blunsom", "Phil"], "venue": "arXiv preprint arXiv:1506.03340,", "citeRegEx": "Hermann et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Hermann et al\\.", "year": 2015}, {"title": "The goldilocks principle: Reading children\u2019s books with explicit memory representations", "author": ["Hill", "Felix", "Bordes", "Antoine", "Chopra", "Sumit", "Weston", "Jason"], "venue": "arXiv preprint arXiv:1511.02301,", "citeRegEx": "Hill et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Hill et al\\.", "year": 2015}, {"title": "Long short-term memory", "author": ["Hochreiter", "Sepp", "Schmidhuber", "J\u00fcrgen"], "venue": "Neural computation,", "citeRegEx": "Hochreiter et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Hochreiter et al\\.", "year": 1997}, {"title": "An empirical exploration of recurrent network architectures", "author": ["Jozefowicz", "Rafal", "Zaremba", "Wojciech", "Sutskever", "Ilya"], "venue": "In Proceedings of the 32nd International Conference on Machine Learning", "citeRegEx": "Jozefowicz et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Jozefowicz et al\\.", "year": 2015}, {"title": "Recurrent continuous translation models", "author": ["Kalchbrenner", "Nal", "Blunsom", "Phil"], "venue": "In EMNLP, pp. 1700\u20131709,", "citeRegEx": "Kalchbrenner et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Kalchbrenner et al\\.", "year": 2013}, {"title": "Improved backing-off for m-gram language modeling", "author": ["Kneser", "Reinhard", "Ney", "Hermann"], "venue": "In Acoustics, Speech, and Signal Processing,", "citeRegEx": "Kneser et al\\.,? \\Q1995\\E", "shortCiteRegEx": "Kneser et al\\.", "year": 1995}, {"title": "Learning word vectors for sentiment analysis", "author": ["Maas", "Andrew L", "Daly", "Raymond E", "Pham", "Peter T", "Huang", "Dan", "Ng", "Andrew Y", "Potts", "Christopher"], "venue": "In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies-Volume", "citeRegEx": "Maas et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Maas et al\\.", "year": 2011}, {"title": "Context dependent recurrent neural network language model", "author": ["Mikolov", "Tomas", "Zweig", "Geoffrey"], "venue": "In SLT,", "citeRegEx": "Mikolov et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2012}, {"title": "Recurrent neural network based language model", "author": ["Mikolov", "Tomas", "Karafi\u00e1t", "Martin", "Burget", "Lukas", "Cernock\u1ef3", "Jan", "Khudanpur", "Sanjeev"], "venue": "In INTERSPEECH 2010, 11th Annual Conference of the International Speech Communication Association,", "citeRegEx": "Mikolov et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2010}, {"title": "Extensions of recurrent neural network language model", "author": ["Mikolov", "Tom\u00e1\u0161", "Kombrink", "Stefan", "Burget", "Luk\u00e1\u0161", "\u010cernock\u1ef3", "Jan Honza", "Khudanpur", "Sanjeev"], "venue": "In Acoustics, Speech and Signal Processing (ICASSP),", "citeRegEx": "Mikolov et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2011}, {"title": "Marc\u2019Aurelio. Learning longer memory in recurrent neural networks", "author": ["Mikolov", "Tomas", "Joulin", "Armand", "Chopra", "Sumit", "Mathieu", "Michael", "Ranzato"], "venue": "arXiv preprint arXiv:1412.7753,", "citeRegEx": "Mikolov et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2014}, {"title": "On knowing a word", "author": ["Miller", "George A"], "venue": "Annual Review of psychology,", "citeRegEx": "Miller and A.,? \\Q1999\\E", "shortCiteRegEx": "Miller and A.", "year": 1999}, {"title": "Two decades of statistical language modeling: where do we go from here", "author": ["Rosenfeld", "Ronald"], "venue": "In Proceedings of the IEEE,", "citeRegEx": "Rosenfeld and Ronald.,? \\Q2000\\E", "shortCiteRegEx": "Rosenfeld and Ronald.", "year": 2000}, {"title": "Learning representations by back-propagating errors", "author": ["Rumelhart", "David E", "Hinton", "Geoffrey E", "Williams", "Ronald J"], "venue": "Cognitive modeling,", "citeRegEx": "Rumelhart et al\\.,? \\Q1988\\E", "shortCiteRegEx": "Rumelhart et al\\.", "year": 1988}, {"title": "Continuous space language models", "author": ["Schwenk", "Holger"], "venue": "Computer Speech & Language,", "citeRegEx": "Schwenk and Holger.,? \\Q2007\\E", "shortCiteRegEx": "Schwenk and Holger.", "year": 2007}, {"title": "Hierarchical neural network generative models for movie dialogues", "author": ["Serban", "Iulian V", "Sordoni", "Alessandro", "Bengio", "Yoshua", "Courville", "Aaron", "Pineau", "Joelle"], "venue": "arXiv preprint arXiv:1507.04808,", "citeRegEx": "Serban et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Serban et al\\.", "year": 2015}, {"title": "End-to-end memory networks", "author": ["Sukhbaatar", "Sainbayar", "Szlam", "Arthur", "Weston", "Jason", "Fergus", "Rob"], "venue": "arXiv preprint arXiv:1503.08895,", "citeRegEx": "Sukhbaatar et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Sukhbaatar et al\\.", "year": 2015}, {"title": "From feedforward to recurrent lstm neural networks for language modeling", "author": ["Sundermeyer", "Martin", "Ney", "Hermann", "Schluter", "Ralf"], "venue": "Audio, Speech, and Language Processing, IEEE/ACM Transactions on,", "citeRegEx": "Sundermeyer et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Sundermeyer et al\\.", "year": 2015}, {"title": "Sequence to sequence learning with neural networks. In Advances in neural information processing", "author": ["Sutskever", "Ilya", "Vinyals", "Oriol", "Le", "Quoc VV"], "venue": null, "citeRegEx": "Sutskever et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Sutskever et al\\.", "year": 2014}, {"title": "Feature-rich part-ofspeech tagging with a cyclic dependency network", "author": ["Toutanova", "Kristina", "Klein", "Dan", "Manning", "Christopher D", "Singer", "Yoram"], "venue": "In Proceedings of the 2003 Conference of the North American Chapter of the Association for Computational Linguistics on Human Language Technology-Volume", "citeRegEx": "Toutanova et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Toutanova et al\\.", "year": 2003}, {"title": "Adadelta: An adaptive learning rate method", "author": ["Zeiler", "Matthew D"], "venue": "arXiv preprint arXiv:1212.5701,", "citeRegEx": "Zeiler and D.,? \\Q2012\\E", "shortCiteRegEx": "Zeiler and D.", "year": 2012}], "referenceMentions": [{"referenceID": 22, "context": "Recently recurrent neural networks have become one of the most widely used models in language modelling (Mikolov et al., 2010).", "startOffset": 104, "endOffset": 126}, {"referenceID": 2, "context": "Later Bengio et al. (2003) proposed feed-forward neural language model, which achieved substantial improvements in perplexity over count-based language models.", "startOffset": 6, "endOffset": 27}, {"referenceID": 2, "context": "Later Bengio et al. (2003) proposed feed-forward neural language model, which achieved substantial improvements in perplexity over count-based language models. Bengio et al. showed that this neural language model could simultaneously learn the conditional probability of the latest word in a sequence as well as a vector representation for each word in a predefined vocabulary. Recently recurrent neural networks have become one of the most widely used models in language modelling (Mikolov et al., 2010). Long short-term memory unit (LSTM, Hochreiter & Schmidhuber, 1997) is one of the most common recurrent activation function. Architecturally speaking, the memory state and output state are explicitly separated by activation gates such that the vanishing gradient and exploding gradient problems described in Bengio et al. (1994) is avoided.", "startOffset": 6, "endOffset": 834}, {"referenceID": 22, "context": "This rewritten probability expression can be either directly modelled by a recurrent neural network (Mikolov et al., 2010) or further approximated as a product of n-gram conditional probabilities such that", "startOffset": 100, "endOffset": 122}, {"referenceID": 17, "context": "For more details on these recurrent activation units, we refer the reader to (Jozefowicz et al., 2015; Greff et al., 2015).", "startOffset": 77, "endOffset": 122}, {"referenceID": 12, "context": "For more details on these recurrent activation units, we refer the reader to (Jozefowicz et al., 2015; Greff et al., 2015).", "startOffset": 77, "endOffset": 122}, {"referenceID": 31, "context": "These different approaches to language modelling have been extensively tested against each other in terms of speech recognition and machine translation in recent years (Sundermeyer et al., 2015; Baltescu & Blunsom, 2014; Schwenk, 2007).", "startOffset": 168, "endOffset": 235}, {"referenceID": 2, "context": "More recently, Bengio et al. (2003) proposed to use a feedforward neural network to model those n-gram conditional probabilities to avoid the issue of data sparsity.", "startOffset": 15, "endOffset": 36}, {"referenceID": 2, "context": "More recently, Bengio et al. (2003) proposed to use a feedforward neural network to model those n-gram conditional probabilities to avoid the issue of data sparsity. This model is often referred to as neural language model. This n-gram language modelling is however limited due to the n-th order Markov assumption made in Eq. (3). Hence, Mikolov et al. (2010) proposed recently to use a recurrent neural network to directly model Eq.", "startOffset": 15, "endOffset": 360}, {"referenceID": 21, "context": "\u201d More recently, Mikolov et al. (2014) proposed a similar approach however without relying on external topic modelling.", "startOffset": 17, "endOffset": 39}, {"referenceID": 21, "context": "\u201d More recently, Mikolov et al. (2014) proposed a similar approach however without relying on external topic modelling. There are three major differences in the proposed approach from the work by Mikolov & Zweig (2012). First, the goal in this work is to explicitly model preceding sentences to better approximate the corpus-level probability (see Eq.", "startOffset": 17, "endOffset": 219}, {"referenceID": 21, "context": "\u201d More recently, Mikolov et al. (2014) proposed a similar approach however without relying on external topic modelling. There are three major differences in the proposed approach from the work by Mikolov & Zweig (2012). First, the goal in this work is to explicitly model preceding sentences to better approximate the corpus-level probability (see Eq. (10)) rather than to get a better context of the current sentence. Second, Mikolov & Zweig (2012) use an external method, such as latent Dirichlet allocation (Blei et al.", "startOffset": 17, "endOffset": 450}, {"referenceID": 21, "context": "\u201d More recently, Mikolov et al. (2014) proposed a similar approach however without relying on external topic modelling. There are three major differences in the proposed approach from the work by Mikolov & Zweig (2012). First, the goal in this work is to explicitly model preceding sentences to better approximate the corpus-level probability (see Eq. (10)) rather than to get a better context of the current sentence. Second, Mikolov & Zweig (2012) use an external method, such as latent Dirichlet allocation (Blei et al., 2003) or latent semantics analysis (Dumais, 2004) to extract a feature vector, where we learn the whole model, including the context vector extraction, end-to-end. Third, we propose a late fusion approach which is well suited for the LSTM units which have recently been widely adopted many works involving language models (see, e.g., Sundermeyer et al., 2015). This late fusion is later shown to be superior to the early fusion approach. Similarly, Sukhbaatar et al. (2015) proposed more recently to use a memory network for language modelling with a very large context of a hundred to two hundreds preceding words.", "startOffset": 17, "endOffset": 998}, {"referenceID": 30, "context": "It is however important to notice that these two previous works by Mikolov & Zweig (2012) and Sukhbaatar et al. (2015) are not in competition with the proposed larger-context recurrent language model.", "startOffset": 94, "endOffset": 119}, {"referenceID": 29, "context": "A more similar model to the proposed larger-context recurrent language model is a hierarchical recurrent encoder decoder (HRED) proposed recently by Serban et al. (2015). The HRED consists of three recurrent neural networks to model a dialogue between two people from the perspective of one of them, to which we refer as a speaker.", "startOffset": 149, "endOffset": 170}, {"referenceID": 29, "context": "Second, similarly to the two other previous works by Mikolov & Zweig (2012) and Serban et al. (2015), the skip-thought vector model only implements early fusion.", "startOffset": 80, "endOffset": 101}, {"referenceID": 32, "context": "Neural machine translation is another related approach (Forcada & \u00d1eco, 1997; Kalchbrenner & Blunsom, 2013; Cho et al., 2014b; Sutskever et al., 2014; Bahdanau et al., 2014).", "startOffset": 55, "endOffset": 173}, {"referenceID": 0, "context": "Neural machine translation is another related approach (Forcada & \u00d1eco, 1997; Kalchbrenner & Blunsom, 2013; Cho et al., 2014b; Sutskever et al., 2014; Bahdanau et al., 2014).", "startOffset": 55, "endOffset": 173}, {"referenceID": 0, "context": "Attention mechanism was introduced by Bahdanau et al. (2014) with intention to build a variable-length context representation in source sentence.", "startOffset": 38, "endOffset": 61}, {"referenceID": 14, "context": "The question and answer are often formulated as filling in a missing word in a query sentence (Hermann et al., 2015; Hill et al., 2015).", "startOffset": 94, "endOffset": 135}, {"referenceID": 15, "context": "The question and answer are often formulated as filling in a missing word in a query sentence (Hermann et al., 2015; Hill et al., 2015).", "startOffset": 94, "endOffset": 135}, {"referenceID": 13, "context": "Furthermore, we also report the result with the conventional, count-based n-gram language model with the modified Kneser-Ney smoothing with KenLM (Heafield et al., 2013).", "startOffset": 146, "endOffset": 169}, {"referenceID": 20, "context": "We use the IMDB Move Review Corpus (IMDB) prepared by Maas et al. (2011).1 This corpus has 75k training reviews and 25k test reviews.", "startOffset": 54, "endOffset": 73}, {"referenceID": 23, "context": "We preprocess the corpus according to (Mikolov et al., 2011) and use a vocabulary of 10k words.", "startOffset": 38, "endOffset": 60}, {"referenceID": 0, "context": "Attention mechanism, which was found to avoid this problem in machine translation (Bahdanau et al., 2014), is found to solve this problem in our task as well.", "startOffset": 82, "endOffset": 105}, {"referenceID": 15, "context": ") This observation is in line with a recent finding by Hill et al. (2015). They also observed significant gain in predicting open-class, or content, words when a question-answering model, including humans, was allowed larger context.", "startOffset": 55, "endOffset": 74}], "year": 2015, "abstractText": "In this work, we propose a novel method to incorporate corpus-level discourse information into language modelling. We call this larger-context language model. We introduce a late fusion approach to a recurrent language model based on long short-term memory units (LSTM), which helps the LSTM unit keep intra-sentence dependencies and inter-sentence dependencies separate from each other. Through the evaluation on three corpora (IMDB, BBC, and Penn TreeBank), we demonstrate that the proposed model improves perplexity significantly. In the experiments, we evaluate the proposed approach while varying the number of context sentences and observe that the proposed late fusion is superior to the usual way of incorporating additional inputs to the LSTM. By analyzing the trained largercontext language model, we discover that content words, including nouns, adjectives and verbs, benefit most from an increasing number of context sentences. This analysis suggests that larger-context language model improves the unconditional language model by capturing the theme of a document better and more easily.", "creator": "LaTeX with hyperref package"}}}