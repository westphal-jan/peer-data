{"id": "1511.03766", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "12-Nov-2015", "title": "Sparse Learning for Large-scale and High-dimensional Data: A Randomized Convex-concave Optimization Approach", "abstract": "In this paper, we develop a randomized algorithm and theory for learning a sparse model from large-scale and high-dimensional data, which is usually formulated as an empirical risk minimization problem with a sparsity-inducing regularizer. Under the assumption that there exists a (approximately) sparse solution with high classification accuracy, we argue that the dual solution is also sparse or approximately sparse. The fact that both primal and dual solutions are sparse motivates us to develop a randomized approach for a general convex-concave optimization problem. Specifically, the proposed approach combines the strength of random projection with that of sparse learning: it utilizes random projection to reduce the dimensionality, and introduces $\\ell_1$-norm regularization to alleviate the approximation error caused by random projection. Theoretical analysis shows that under favored conditions, the randomized algorithm can accurately recover the optimal solutions to the convex-concave optimization problem (i.e., recover both the primal and dual solutions). Furthermore, the solutions returned by our algorithm are guaranteed to be approximately sparse.", "histories": [["v1", "Thu, 12 Nov 2015 03:11:48 GMT  (21kb)", "http://arxiv.org/abs/1511.03766v1", null], ["v2", "Sun, 16 Oct 2016 14:36:01 GMT  (24kb)", "http://arxiv.org/abs/1511.03766v2", "Proceedings of the 27th International Conference on Algorithmic Learning Theory (ALT 2016)"]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["lijun zhang", "tianbao yang", "rong jin", "zhi-hua zhou"], "accepted": false, "id": "1511.03766"}, "pdf": {"name": "1511.03766.pdf", "metadata": {"source": "CRF", "title": "Sparse Learning for Large-scale and High-dimensional Data: A Randomized Convex-concave Optimization Approach", "authors": ["Lijun Zhang", "Tianbao Yang", "Rong Jin", "Zhi-Hua Zhou"], "emails": ["zhanglj@lamda.nju.edu.cn", "tianbao-yang@uiowa.edu", "rongjin@cse.msu.edu", "zhouzh@lamda.nju.edu.cn"], "sections": [{"heading": null, "text": "ar Xiv: 151 1.03 766v 1 [csKeywords: random projection, sparse learning, convex-concave optimization, master solution, dual solution"}, {"heading": "1. Introduction", "text": "In order to tackle this challenge, we must first reduce the dimensionality of the data and finally move the solution back to the original space."}, {"heading": "2. Related Work", "text": "In the case of unsupervised learning, it has been proven that random projection is capable of maintaining distance (Dasgupta and Gupta, 2003), the inner product (Arriaga and Vempala, 2006), volume and distance to affine spaces (Stomach, 2002). In the case of supervised learning, random projection is generally used as a pre-processing step to find a low-dimensional representation of the data, thereby reducing the compressive cost of training. For classification, theoretical studies mainly focus on investigating the generalization error or maintaining the classification span in low-dimensional space (Balcan et al., 2006; Shi et al., 2012; Paul et al., 2013). Theoretical guarantees for regression exist, but they consider only the slightest square problem (Mahoney et al., DSang et al., 2012; Paul et al., Dulse, 2013)."}, {"heading": "3. Algorithm", "text": "To reduce the computational costs of (3), we first create a random matrix R-Rn-m, with m-Rn-M (d, n). If we define A-Rd-Rd-M, we propose to solve the following problem: The construction of the random matrix R, as well as the values of the two control parameters between and between are discussed later. The optimization problem in (4) can be solved by the algorithm designed for compound convex problems (he and Monteiro, 2014). Compared to (3), the main advantage of (4) is that it only needs to load A-R and R solutions into memory, making it convenient to deal with large-scale problems."}, {"heading": "4. Main Results", "text": "We first present the assumptions we make, and then present the theoretical guarantees of (4)."}, {"heading": "4.1 Assumptions", "text": "Let us take the optimization problem in (2) as an example. (2) If this assumption is fulfilled when a strongly convex function (e.g. the constant convex function (e.g. the constant convex function (w) 22) is part of the regularization problem (w). \u2022 h () is strongly convex in relation to the euclidean norm. \u2022 For the problem in (2), if it is a smooth function (e.g. the logistic loss), then its convex conjugation is strongly convex (Rockafellar, 1997; Kakade et al., 2009). \u2022 Either columns or rows of A have a constant function (e.g. the logistic loss)."}, {"heading": "4.2 Theoretical Guarantees", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.2.1 Sparse Solutions", "text": "We first consider the case that both w + 0 and v + 0 are a problem. (8) With a probability of at least 1-3 percent, we have a probability of at least 2 percent. (8) With a probability of at least 1-3 percent, we have a probability of 2 percent. (9) We have a probability of 2 percent. (9) We have a probability of 2 percent. (8) We have a probability of 2 percent. (9) We have a probability of 2 percent. (9) We have a probability of 2 percent. (9) We have a probability of 2 percent."}, {"heading": "4.2.2 Approximately Sparse Solutions", "text": "We now proceed to examine the case that the optimal solutions to (3) are only approximately sufficient. (3) With a slight misuse of the notation, we assume that there are two sparse vectors, with (3) and (3) only minor solutions available, which (3) are approximately solved in the sense that the above conditions can be regarded as sub-optimal conditions (Boyd and Vandenberghe, 2004). (11) The above conditions can be regarded as sub-optimal conditions (Boyd and Vandenberghe, 2004)."}, {"heading": "5. Analysis", "text": "In this section we provide evidence for the most important theorems, and others can be found in the appendix."}, {"heading": "5.1 Proof of Theorem 2", "text": "To facilitate the analysis, we introduce a pseudo-optimization problem. \u2212 Following we will first discuss how the difference between the difference between the difference between the difference between the difference and the difference can be tied in a similar way between the difference between the difference between the difference between the difference between the difference between the difference between the difference between the difference between the difference between the difference between the difference between the difference and the difference in a similar way. \u2212 Following we derive from the difference between the optimality of the difference between the difference between the difference and the difference between the difference between the difference between the difference between the difference and the difference between the difference between the difference and the difference between the difference between the difference between the difference between the difference and the difference in a similar way. \u2212 Following we will derive from the optimality of the difference between the difference between the difference between the difference between the difference and the difference between the difference between the difference between the difference between the difference and the difference \u2212 The difference between the difference between the difference \u2212 We will select the difference between the difference between the difference between the difference between the difference between the difference between the difference between the difference and the difference \u2212 The difference between the difference between the difference between the difference between the difference and the difference \u2212 The difference."}, {"heading": "5.2 Proof of Lemma 1", "text": "Let us include the subset of non-null entries in formula 1 and in formula 1 in formula 1. (DefineL (DefineL) = \u2212 h (DefineL) = \u2212 h (DefineL) = \u2212 h (DefineL) = \u2212 h (DefineL) = \u2212 h (DefineL) = \u2212 h (DefineL) = \u2212 h (DefineL) \u2212 h (DefineL) + min \u2212 h (DefineL) \u2212 p (DefineL) = p (DefineL) = p (DefineL) = p (DefineL) \u2212 p (DefineL) = p (DefineP) \u2212 p (DefineL) = p (DefineL) \u2212 h (DefineL) \u2212 p (DefineL) \u2212 p (DefineL) = p (DefineP). (P.) = p. (P.). (P. (P.) =. (P.). (P.). (P. (P.) =. (P.). (P.). (P. (P.). (P.). (P.). (P.). (Definep. (P.). (P.) =. (DefineP.). (DefineL). (DefineL)."}, {"heading": "5.3 Proof of Lemma 2", "text": "First of all, we present a central problem of our analysis: starting from the property that R preserves the 2-norm, it is easy to check whether it also preserves the inner product (Arriaga and Vempala, 2006). Specifically, we have the following Lemma.Lemma 6 Let us assume that R satisfies theorem 1. For any two solid vectors u Rn and v Rn, with a probability of at least 1 \u2212 \u043c, we have a probability of at least 2 RR v \u2212 v v. v."}, {"heading": "5.4 Proof of Lemma 4", "text": "(W) (W) (W) (W) (W) (W) (W) (W) (W) (W) (W) (W) (W) (W) (W) (W) (W) (W) (W) (W) (W) (W) (W) (W) (W) (W) (W) (W) (W) (W) (W) (W) (W) (W) (W) (W) (W) (W) (W) (W) (W) (W) (W) (W) (W) (W) (W) (W) (W) (W) (W) (W) (W) (W) (W) (W) (W) (W) (W) (W) (W) (W) (W) (W) (W) (W) (W) (W) (W) (W) (W) (W) (W) (W) (W) (W) (W) (W) (W) (W) (W) (W) (W) (W) (W) (W) (W) (W) (W) (W) (W) (W) (W) (W) (W) (W) (W) (W) (W) (W) (W) (W) (W) (W) (W) (W) (W) (W) (W) (W) (W) (W) (W) (W) (W) (W) (W) (W) (W) (W) (W) (W) (W) (W) (W) (W) (W (W) (W) (W) (W) (W) (W) (W) (W (W) (W) (W) (W) (W (W) (W) (W) (W (W) (W) (W) (W) (W (W) (W) (W) (W) (W) (W) (W (W) (W) (W) (W) (W) (W (W) (W) (W) (W) (W) (W) (W) (W) (W) (W) ("}, {"heading": "5.5 Proof of Lemma 5", "text": "We have first an upper limit of 2.0 percent, then an upper limit of 2.0 percent, then a lower limit of 2.0 percent, then a lower limit of 2.0 percent, then a lower limit of 2.0 percent, then a lower limit of 2.0 percent, then a lower limit of 2.0 percent, then a lower limit of 2.0 percent, then a lower limit of 2.0 percent, then a lower limit of 2.0 percent, then a lower limit of 2.0 percent, then a lower limit of 2.0 percent, then a lower limit of 2.0 percent, then a lower limit of 2.0 percent, then a lower limit of 2.0 percent, then a lower limit of 2.0 percent, then a lower limit of 2.0 percent, then a lower limit of 2.0 percent, then a lower limit of 2.0 percent, then a lower limit of 2.0 percent, then a lower limit of 2.0 percent, then a lower limit of 2.0 percent, then a lower limit of 2.0 percent, then a lower limit of 2.0 percent."}, {"heading": "5.6 Proof of Lemma 6", "text": "First, we assume that the two vectors u + v and u \u2212 v. theorem 1 are applied with a probability of at least 1 \u2212 4 Exp (\u2212 m\u03b52 / c) to the vectors u + v and u \u2212 v. Then, with a probability of at least 1 \u2212 4 Exp (\u2212 m\u03b52 / c), (1 \u2212 \u03b5) and u \u2212 v \u00b2 22 \u2264 (1 + v) and (1 + \u03b5) + v \u00b2 22, (29) (1 \u2212 \u03b5) and (22 \u2212 u \u2212 v \u00b2 (u \u2212 v) 22 \u2264 (1 \u2212 v), (30) and (29). From (29) and (30) it can be clearly shown that the probability of at least 1 \u2212 4 \u2012 RR \u00b2 v \u2212 v \u00b2 and (1 \u2212 v) and the probability of at least 1 \u2212 3 \u2012 RR \u00b2 v \u00b2 and (30) that we have a probability of at least 1 \u2212 4 \u2012 V \u00b2."}, {"heading": "5.7 Proof of Lemma 7", "text": "First: We define Sn, 16r\u03bb = [x] Rn: [x] 2 [x] 2 [x] 0 [x] 0 [x] 0 [x] 0 [x) 0 [x] 0 [x] 0 [x) 0 [x) 0 (x) 0 (x) 0 (x) 0 (x) 0 (x) 0 (x) 0 (x) 0 (x) 0 (x) 0 (x) 0 (x) 0 (x) 0 (x) 0 (x) 0 (x) 0 (x) 0 (x) 0 (x) 0 (x) 0 (x) 0 (x) 0 (x) 0 (x) 1 (x) 0 (x) 0 (x) 1 (x) 0 (x) 1 (x) 1) (x) 0 (x) 1) (x) 1) (x) 0 (x) 1) (x) 0 (x) 0 (x) 0 (x) 1) (x) 0 (x) 1) (x) (x) 0 (x) 1) (x) (x) 0 (c) 0 (x) 0 (x) 0 (x) 0 (x) 0 (x) 1) (x) (x) (x) 0 (x) 0 (x) 0 (x) 0 (x) 0 (x) 1) (x) (x) 0 (x) 0 (x) 0 (x) 0 (x) 0 (x) 0 (x) 0 (x) 0 (x) 0 (x) 0 (x) 0 (x) (x) 0 (x) 0 (x) 0 (x) 0 (x) 0 (x) 0 (x) 0 (x) 0 (x) 0 (x) 0 (x) (x) 0 (x) 0 (x) 0 (x) 0 (x) 0 (x) 0 (x) (x) 0 (x) (x) 0 (x) (x) 0 (x) 0 (x) 0 (x) 0 (x) 0 (x) (x) (x) 0 (x) 0 (x) (x) 0 (x) (x) (x) 0 (x) (x) (x) (x) 0 (x) (x) (x) ("}, {"heading": "6. Conclusion and Future Work", "text": "Compared to previous studies, a characteristic feature of the proposed algorithm is that it introduces scant regulation to control the damage caused by random projection. Under mild assumptions about the optimization problem, we show that it is able to accurately restore the optimal solutions according to (3), provided they are sparse or approximately sparse. From the current analysis, we have to solve two different problems if our goal is to accurately restore both w * and p \u00b2. It is unclear whether this is an artifact of the proof method or indeed unavoidable. We will study this problem in the future. As the proposed algorithm is designed for the case that the optimal solutions are (approximately) sparse, it is practically important to develop a precision method that can estimate the scarcity of solutions before applying our algorithm."}, {"heading": "Appendix A. Proof of Theorem 3", "text": "The analysis here is similar to that for Lemma 1. \u2212 Look at the evidence of Theorem 2 showing that the probability is at least 1 \u2212 3. \u2212 Look at the probability that the value of L () is above the domain and that h () is strongly convex. \u2212 Look at the probability that L () is above the domain. \u2212 Look at the probability that L () is above the domain. \u2212 Look at the probability that L () is above the domain. \u2212 Look at the probability that L () is above the domain. \u2212 Look at the probability. \u2212 See (See) + Look at the probability? \u2212 See (See) + See (See) + See (See) what the probability is. \u2212 See (See) what the probability is."}, {"heading": "Appendix B. Proof of Theorem 5", "text": "The evidence is almost identical to that of Theorem 2. We only need to replace Lemmas (1) and (4) with the following Lemma 10 Denote\u03c1\u03bb = Denote\u03c1\u03bb = Denote\u03c1\u03bb = RR \u2212 I w = Denote\u03c1\u03bb = Denote\u03c1\u03bb = Denote\u03c1\u03bb (RR \u2212 I) A w = Denote\u03c1\u03bb _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _"}, {"heading": "By choosing \u03b3\u03bb \u2265 2\u03c1\u03bb, then the conclusion of Lemma (1) still holds.", "text": "Lemma 11 Denote\u03c1w = KI-RR (I-RR), KI-RR (I-RR), KI-RR (I-RR), KI-RR (I-RR), KI-ARR (I-RR), KI-RR (I-RR), KI-RR (I-RR), KI-RR (I-RR). (36)"}, {"heading": "By choosing \u03b3w \u2265 2\u03c1w, then the conclusion of Lemma (4) still holds.", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "Appendix C. Proof of Lemma 10", "text": "Based on the assumption that the aforementioned inequality is replaced by (20) and the rest of the evidence is identical to that of Lemma 1, this results in: (1), (2), (3), (3), (4), (4), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5, (5), (5), (5), (5, (5), (5), (5), (5), (5), (5, (5), (5), (5), (5), (5), (5, (5), (5), (5), (5), (5, (5), (5), (5), (5), (5, (5), (5), (5), (5, (5), (5), (5, (5), (5), (5), (5, (5), (5), (5, (5), (5, (5), (5), (5), (5, (5), (5), (5, (5), (5, (5), (5), (5, (5), (5), (5, (5), (5, (5), (5), (5, (5), (5, (5), (5), (5), (5, (5), (5), (5, (5), (5, (5), (5), (5), (5, (5), (5), (5), ("}, {"heading": "Appendix D. Proof of Lemma 11", "text": "Similarly, we have the following proofs: (1), (2), (3), (4), (4), (5), (5), (5), (5), (5), (5), (5), (5), (8), (8), (8), (8), (8), (8), (8), (8), (8), (8), (8), (8), (9), (9), (9), (9), (9), (9), (9), (9), (9), (9), (9), (9), (9), (9), (9), (9), (9), (9), (9), (9)."}, {"heading": "Appendix E. Proof of Theorem 6", "text": "The proof is almost identical to that of Theorem 2. We just have to replace Lemmas (1) and (4) with the following. (37) By choosing Lemma 12 Denotefic = \"RR\" (RR) \"A\" w \"(1 +)\" A \"(1 +\"). \"(37) By choosing\" N \"(2)\" N \"(2)\" N \"(3)\" A \"(I)\" R \"(1)\" A \"(1)\" 1 \"(1)\" N \"(1)\" 2 \"(4).\" Lemma 13 Denote\u03c1w = \"A\" (I) \"RR\" (1) \"A\" (1) (1) + (1). (38) By choosing \"W\" 2 \"w\" (2) \"R\" (1) and \"W\" W \"(2)."}, {"heading": "Appendix F. Proof of Lemma 12", "text": "Recall the definition of L (\u00b7) in section 5.2. Starting from the fact that the optimal solution to this problem lies within the interior of the country, we have the definition of L (\u03bb) = \u2212 h (\u03bb) \u2212 A w \u00b2 = 0. (39) Then we have the definition of L (and) + RR A w \u00b2 = 0. (39) Then we have the definition of L (\u03bb) \u2212 h (ds) + RR A w \u00b2, A w \u00b2, A p \u00b2, A p \u00b2, A p \u00b2, A p \u00b2, A p \u00b2, A p \u00b2, A p \u00b2, A p \u00b2, A p \u00b2, A p \u00b2, A p \u00b2, A p \u00b2, A p \u00b2, A p \u00b2, A p \u00b2, A p \u00b2, A p \u00b2, A p, A p \u00b2, A p \u00b2, A p \u00b2, A p, A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, A."}, {"heading": "Appendix G. Proof of Lemma 13", "text": "Remember the definition of G (\u00b7) in Section 5.4. Starting from the fact that the optimal solution W (44) is inside A (46), we have G (w) = D (44) \u2212 D (43) Then we have G (w) \u2212 W (43) Then we have G (w) \u2212 W (43) Then we have G (w) \u2212 W (43) Then we have G (w) \u2212 W (43) Then we have G (w) \u2212 W (43) \u2212 G (w) > + W (43) \u2212 G (w) > + G (w) \u2212 W (44)."}], "references": [{"title": "Database-friendly random projections: Johnson-lindenstrauss with binary coins", "author": ["Dimitris Achlioptas"], "venue": "Journal of Computer and System Sciences,", "citeRegEx": "Achlioptas.,? \\Q2003\\E", "shortCiteRegEx": "Achlioptas.", "year": 2003}, {"title": "An algorithmic theory of learning: Robust concepts and random projection", "author": ["Rosa I. Arriaga", "Santosh Vempala"], "venue": "Machine Learning,", "citeRegEx": "Arriaga and Vempala.,? \\Q2006\\E", "shortCiteRegEx": "Arriaga and Vempala.", "year": 2006}, {"title": "Optimization with sparsity-inducing penalties", "author": ["Francis Bach", "Rodolphe Jenatton", "Julien Mairal", "Guillaume Obozinski"], "venue": "Foundations and Trends in Machine Learning,", "citeRegEx": "Bach et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Bach et al\\.", "year": 2012}, {"title": "Kernels as features: On kernels, margins, and low-dimensional mappings", "author": ["Maria-Florina Balcan", "Avrim Blum", "Santosh Vempala"], "venue": "Machine Learning,", "citeRegEx": "Balcan et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Balcan et al\\.", "year": 2006}, {"title": "Random projection in dimensionality reduction: applications to image and text data", "author": ["Ella Bingham", "Heikki Mannila"], "venue": "In Proceedings of the 7th ACM SIGKDD international conference on Knowledge discovery and data mining,", "citeRegEx": "Bingham and Mannila.,? \\Q2001\\E", "shortCiteRegEx": "Bingham and Mannila.", "year": 2001}, {"title": "Convex Optimization", "author": ["Stephen Boyd", "Lieven Vandenberghe"], "venue": null, "citeRegEx": "Boyd and Vandenberghe.,? \\Q2004\\E", "shortCiteRegEx": "Boyd and Vandenberghe.", "year": 2004}, {"title": "Prediction, Learning, and Games", "author": ["Nicol\u00f2 Cesa-Bianchi", "G\u00e1bor Lugosi"], "venue": null, "citeRegEx": "Cesa.Bianchi and Lugosi.,? \\Q2006\\E", "shortCiteRegEx": "Cesa.Bianchi and Lugosi.", "year": 2006}, {"title": "An elementary proof of a theorem of johnson and lindenstrauss", "author": ["Sanjoy Dasgupta", "Anupam Gupta"], "venue": "Random Structures & Algorithms,", "citeRegEx": "Dasgupta and Gupta.,? \\Q2003\\E", "shortCiteRegEx": "Dasgupta and Gupta.", "year": 2003}, {"title": "The Elements of Statistical Learning", "author": ["Trevor Hastie", "Robert Tibshirani", "Jerome Friedman"], "venue": null, "citeRegEx": "Hastie et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Hastie et al\\.", "year": 2009}, {"title": "An accelerated hpe-type algorithm for a class of composite convex-concave saddle-point problems", "author": ["Yunlong He", "Renato D.C. Monteiro"], "venue": "Technical report, Georgia Institute of Technology,,", "citeRegEx": "He and Monteiro.,? \\Q2014\\E", "shortCiteRegEx": "He and Monteiro.", "year": 2014}, {"title": "On the duality of strong convexity and strong smoothness: Learning applications and matrix regularization", "author": ["Sham M. Kakade", "Shai Shalev-Shwartz", "Ambuj Tewari"], "venue": "Technical report, Toyota Technological Institute at Chicago,", "citeRegEx": "Kakade et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Kakade et al\\.", "year": 2009}, {"title": "Dimensionality reduction by random mapping: fast similarity computation for clustering", "author": ["Samuel Kaski"], "venue": "In Proceedings of the 1998 IEEE International Joint Conference on Neural Networks,", "citeRegEx": "Kaski.,? \\Q1998\\E", "shortCiteRegEx": "Kaski.", "year": 1998}, {"title": "Oracle Inequalities in Empirical Risk Minimization and Sparse Recovery Problems", "author": ["Vladimir Koltchinskii"], "venue": null, "citeRegEx": "Koltchinskii.,? \\Q2011\\E", "shortCiteRegEx": "Koltchinskii.", "year": 2011}, {"title": "Dimensionality reductions that preserve volumes and distance to affine spaces, and their algorithmic applications", "author": ["Avner Magen"], "venue": "In Randomization and Approximation Techniques in Computer Science,", "citeRegEx": "Magen.,? \\Q2002\\E", "shortCiteRegEx": "Magen.", "year": 2002}, {"title": "Randomized algorithms for matrices and data", "author": ["Michael W. Mahoney"], "venue": "Foundations and Trends in Machine Learning,", "citeRegEx": "Mahoney.,? \\Q2011\\E", "shortCiteRegEx": "Mahoney.", "year": 2011}, {"title": "Uniform uncertainty principle for bernoulli and subgaussian ensembles", "author": ["Shahar Mendelson", "Alain Pajor", "Nicole Tomczak-Jaegermann"], "venue": "Constructive Approximation,", "citeRegEx": "Mendelson et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Mendelson et al\\.", "year": 2008}, {"title": "Prox-method with rate of convergence O(1/t) for variational inequalities with lipschitz continuous monotone operators and smooth convex-concave saddle point problems", "author": ["Arkadi Nemirovski"], "venue": "SIAM Journal on Optimization,", "citeRegEx": "Nemirovski.,? \\Q2005\\E", "shortCiteRegEx": "Nemirovski.", "year": 2005}, {"title": "Smooth minimization of non-smooth functions", "author": ["Yu. Nesterov"], "venue": "Mathematical Programming,", "citeRegEx": "Nesterov.,? \\Q2005\\E", "shortCiteRegEx": "Nesterov.", "year": 2005}, {"title": "Random projections for support vector machines", "author": ["Saurabh Paul", "Christos Boutsidis", "Malik Magdon-Ismail", "Petros Drineas"], "venue": "In Proceedings of the 16th International Conference on Artificial Intelligence and Statistics,", "citeRegEx": "Paul et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Paul et al\\.", "year": 2013}, {"title": "One-bit compressed sensing by linear programming", "author": ["Yaniv Plan", "Roman Vershynin"], "venue": "Communications on Pure and Applied Mathematics,", "citeRegEx": "Plan and Vershynin.,? \\Q2013\\E", "shortCiteRegEx": "Plan and Vershynin.", "year": 2013}, {"title": "Robust 1-bit compressed sensing and sparse logistic regression: A convex programming approach", "author": ["Yaniv Plan", "Roman Vershynin"], "venue": "IEEE Transactions on Information Theory,", "citeRegEx": "Plan and Vershynin.,? \\Q2013\\E", "shortCiteRegEx": "Plan and Vershynin.", "year": 2013}, {"title": "Convex Analysis", "author": ["Ralph Tyrell Rockafellar"], "venue": null, "citeRegEx": "Rockafellar.,? \\Q1997\\E", "shortCiteRegEx": "Rockafellar.", "year": 1997}, {"title": "Is margin preserved after random projection", "author": ["Qinfeng Shi", "Chunhua Shen", "Rhys Hill", "Anton van den Hengel"], "venue": "In Proceedings of the 29th International Conference on Machine Learning,", "citeRegEx": "Shi et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Shi et al\\.", "year": 2012}, {"title": "Fast rates for regularized objectives", "author": ["Karthik Sridharan", "Shai Shalev-shwartz", "Nathan Srebro"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "Sridharan et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Sridharan et al\\.", "year": 2009}, {"title": "User-friendly tail bounds for sums of random matrices", "author": ["Joel A. Tropp"], "venue": "Foundations of Computational Mathematics,", "citeRegEx": "Tropp.,? \\Q2012\\E", "shortCiteRegEx": "Tropp.", "year": 2012}, {"title": "Svm soft margin classifiers: Linear programming versus quadratic programming", "author": ["Qiang Wu", "Ding-Xuan Zhou"], "venue": "Neural Computation,", "citeRegEx": "Wu and Zhou.,? \\Q2005\\E", "shortCiteRegEx": "Wu and Zhou.", "year": 2005}, {"title": "Theory of dual-sparse regularized randomized reduction", "author": ["Tianbao Yang", "Lijun Zhang", "Rong Jin", "Shenghuo Zhu"], "venue": "In Proceedings of the 32nd International Conference on Machine Learning,", "citeRegEx": "Yang et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Yang et al\\.", "year": 2015}, {"title": "Recovering the optimal solution by dual random projection", "author": ["Lijun Zhang", "Mehrdad Mahdavi", "Rong Jin", "Tianbao Yang", "Shenghuo Zhu"], "venue": "In Proceedings of the 26th Annual Conference on Learning Theory (COLT),", "citeRegEx": "Zhang et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Zhang et al\\.", "year": 2013}, {"title": "Regularization and variable selection via the elastic net", "author": ["Hui Zou", "Trevor Hastie"], "venue": "Journal of the Royal Statistical Society. Series B (Statistical Methodology),", "citeRegEx": "Zou and Hastie.,? \\Q2005\\E", "shortCiteRegEx": "Zou and Hastie.", "year": 2005}], "referenceMentions": [{"referenceID": 2, "context": "Introduction Learning the sparse representation of a predictive model has received considerable attention in recent years (Bach et al., 2012).", "startOffset": 122, "endOffset": 141}, {"referenceID": 28, "context": "where l(\u00b7) is a convex function such as the logistic loss to measure the empirical error, and \u03c8(\u00b7) is a sparsity-inducing regularizer such as the elastic net (Zou and Hastie, 2005) to", "startOffset": 158, "endOffset": 180}, {"referenceID": 8, "context": "avoid overfitting (Hastie et al., 2009).", "startOffset": 18, "endOffset": 39}, {"referenceID": 21, "context": "where l\u2217(\u00b7) is the Fenchel conjugate of l(\u00b7) (Rockafellar, 1997) and \u0393 is the domain of the dual variable, we get the following convex-concave formulation:", "startOffset": 45, "endOffset": 64}, {"referenceID": 17, "context": "The problem in (3) has been widely studied in the optimization community, and when n and d are medium size, it can be solved iteratively by gradient based methods (Nesterov, 2005; Nemirovski, 2005).", "startOffset": 163, "endOffset": 197}, {"referenceID": 16, "context": "The problem in (3) has been widely studied in the optimization community, and when n and d are medium size, it can be solved iteratively by gradient based methods (Nesterov, 2005; Nemirovski, 2005).", "startOffset": 163, "endOffset": 197}, {"referenceID": 11, "context": "Related Work Random projection has been widely used as an efficient algorithm for dimensionality reduction (Kaski, 1998; Bingham and Mannila, 2001).", "startOffset": 107, "endOffset": 147}, {"referenceID": 4, "context": "Related Work Random projection has been widely used as an efficient algorithm for dimensionality reduction (Kaski, 1998; Bingham and Mannila, 2001).", "startOffset": 107, "endOffset": 147}, {"referenceID": 7, "context": "In the case of unsupervised learning, it has been proved that random projection is able to preserve the distance (Dasgupta and Gupta, 2003), inner product (Arriaga and Vempala, 2006), volumes and distance to affine spaces (Magen, 2002).", "startOffset": 113, "endOffset": 139}, {"referenceID": 1, "context": "In the case of unsupervised learning, it has been proved that random projection is able to preserve the distance (Dasgupta and Gupta, 2003), inner product (Arriaga and Vempala, 2006), volumes and distance to affine spaces (Magen, 2002).", "startOffset": 155, "endOffset": 182}, {"referenceID": 13, "context": "In the case of unsupervised learning, it has been proved that random projection is able to preserve the distance (Dasgupta and Gupta, 2003), inner product (Arriaga and Vempala, 2006), volumes and distance to affine spaces (Magen, 2002).", "startOffset": 222, "endOffset": 235}, {"referenceID": 3, "context": "For classification, theoretical studies mainly focus on examining the generalization error or the preservation of classification margin in the low-dimensional space (Balcan et al., 2006; Shi et al., 2012; Paul et al., 2013).", "startOffset": 165, "endOffset": 223}, {"referenceID": 22, "context": "For classification, theoretical studies mainly focus on examining the generalization error or the preservation of classification margin in the low-dimensional space (Balcan et al., 2006; Shi et al., 2012; Paul et al., 2013).", "startOffset": 165, "endOffset": 223}, {"referenceID": 18, "context": "For classification, theoretical studies mainly focus on examining the generalization error or the preservation of classification margin in the low-dimensional space (Balcan et al., 2006; Shi et al., 2012; Paul et al., 2013).", "startOffset": 165, "endOffset": 223}, {"referenceID": 14, "context": "For regression, there do exist theoretical guarantees for the recovery error, but they only hold for the least squares problem (Mahoney, 2011).", "startOffset": 127, "endOffset": 142}, {"referenceID": 27, "context": "Our work is closely related to Dual Random Projection (DRP) (Zhang et al., 2013) and Dual-sparse Regularized Randomized Reduction (DSRR) (Yang et al.", "startOffset": 60, "endOffset": 80}, {"referenceID": 26, "context": ", 2013) and Dual-sparse Regularized Randomized Reduction (DSRR) (Yang et al., 2015), which also investigate random projection from the perspective of optimization.", "startOffset": 64, "endOffset": 83}, {"referenceID": 9, "context": "The optimization problem in (4) can be solved by the algorithm designed for composite convex-concave problems (He and Monteiro, 2014).", "startOffset": 110, "endOffset": 133}, {"referenceID": 3, "context": "Compared to previous randomized algorithms (Balcan et al., 2006; Zhang et al., 2013; Yang et al., 2015), (4) has two new features: i) the optimization is still performed in the original space; and ii) the l1-norm is introduced to regularize both primal and dual solutions.", "startOffset": 43, "endOffset": 103}, {"referenceID": 27, "context": "Compared to previous randomized algorithms (Balcan et al., 2006; Zhang et al., 2013; Yang et al., 2015), (4) has two new features: i) the optimization is still performed in the original space; and ii) the l1-norm is introduced to regularize both primal and dual solutions.", "startOffset": 43, "endOffset": 103}, {"referenceID": 26, "context": "Compared to previous randomized algorithms (Balcan et al., 2006; Zhang et al., 2013; Yang et al., 2015), (4) has two new features: i) the optimization is still performed in the original space; and ii) the l1-norm is introduced to regularize both primal and dual solutions.", "startOffset": 43, "endOffset": 103}, {"referenceID": 21, "context": ", the logistic loss), then its convex conjugate l\u2217(\u00b7) will be strongly convex (Rockafellar, 1997; Kakade et al., 2009).", "startOffset": 78, "endOffset": 118}, {"referenceID": 10, "context": ", the logistic loss), then its convex conjugate l\u2217(\u00b7) will be strongly convex (Rockafellar, 1997; Kakade et al., 2009).", "startOffset": 78, "endOffset": 118}, {"referenceID": 7, "context": "The above theorem is widely used to prove the famous Johnson\u2013Lindenstrauss lemma (Dasgupta and Gupta, 2003).", "startOffset": 81, "endOffset": 107}, {"referenceID": 0, "context": "Previous studies (Achlioptas, 2003; Arriaga and Vempala, 2006) have proved that Theorem 1 is true if {Sij} are independent random variables sampled from the Gaussian distribution N (0, 1), uniform distribution over {\u00b11}, or the following database-friendly distribution", "startOffset": 17, "endOffset": 62}, {"referenceID": 1, "context": "Previous studies (Achlioptas, 2003; Arriaga and Vempala, 2006) have proved that Theorem 1 is true if {Sij} are independent random variables sampled from the Gaussian distribution N (0, 1), uniform distribution over {\u00b11}, or the following database-friendly distribution", "startOffset": 17, "endOffset": 62}, {"referenceID": 15, "context": "More generally, a sufficient condition for Theorem 1 is that columns of R are independent, isotropic, and subgaussian vectors (Mendelson et al., 2008).", "startOffset": 126, "endOffset": 150}, {"referenceID": 25, "context": "According to the theoretical analysis of regularized empirical risk minimization (Wu and Zhou, 2005; Sridharan et al., 2009; Koltchinskii, 2011), the optimal \u03b3, that minimizes the generalization error, can be chosen as \u03b3 = O(1/ \u221a n), and thus \u03b1 = O(\u03b3n) = O( \u221a n).", "startOffset": 81, "endOffset": 144}, {"referenceID": 23, "context": "According to the theoretical analysis of regularized empirical risk minimization (Wu and Zhou, 2005; Sridharan et al., 2009; Koltchinskii, 2011), the optimal \u03b3, that minimizes the generalization error, can be chosen as \u03b3 = O(1/ \u221a n), and thus \u03b1 = O(\u03b3n) = O( \u221a n).", "startOffset": 81, "endOffset": 144}, {"referenceID": 12, "context": "According to the theoretical analysis of regularized empirical risk minimization (Wu and Zhou, 2005; Sridharan et al., 2009; Koltchinskii, 2011), the optimal \u03b3, that minimizes the generalization error, can be chosen as \u03b3 = O(1/ \u221a n), and thus \u03b1 = O(\u03b3n) = O( \u221a n).", "startOffset": 81, "endOffset": 144}, {"referenceID": 24, "context": "The upper bound in the above theorem is quite loose, because \u2016RR\u22a4 \u2212 I\u20162 is roughly on the order of n log n/m (Tropp, 2012).", "startOffset": 109, "endOffset": 122}, {"referenceID": 5, "context": "The above conditions can be considered as sub-optimality conditions (Boyd and Vandenberghe, 2004) of w\u2217 and \u03bb\u2217 measured in the l\u221e-norm.", "startOffset": 68, "endOffset": 97}, {"referenceID": 1, "context": "From the property that R preserves the l2-norm, it is easy to verify that it also preserves the inner product (Arriaga and Vempala, 2006).", "startOffset": 110, "endOffset": 137}, {"referenceID": 1, "context": "Following the proof of Corollary 2 in Arriaga and Vempala (2006), we apply Theorem 1 to vectors u+v and u\u2212v.", "startOffset": 38, "endOffset": 65}, {"referenceID": 19, "context": "1 from Plan and Vershynin (2013a), we have", "startOffset": 7, "endOffset": 34}, {"referenceID": 21, "context": "where the last equality follows from the fact that the maximum of a convex function over a convex set generally occurs at some extreme point of the set (Rockafellar, 1997).", "startOffset": 152, "endOffset": 171}], "year": 2017, "abstractText": "In this paper, we develop a randomized algorithm and theory for learning a sparse model from large-scale and high-dimensional data, which is usually formulated as an empirical risk minimization problem with a sparsity-inducing regularizer. Under the assumption that there exists a (approximately) sparse solution with high classification accuracy, we argue that the dual solution is also sparse or approximately sparse. The fact that both primal and dual solutions are sparse motivates us to develop a randomized approach for a general convex-concave optimization problem. Specifically, the proposed approach combines the strength of random projection with that of sparse learning: it utilizes random projection to reduce the dimensionality, and introduces l1-norm regularization to alleviate the approximation error caused by random projection. Theoretical analysis shows that under favored conditions, the randomized algorithm can accurately recover the optimal solutions to the convex-concave optimization problem (i.e., recover both the primal and dual solutions). Furthermore, the solutions returned by our algorithm are guaranteed to be approximately sparse.", "creator": "dvips(k) 5.991 Copyright 2011 Radical Eye Software"}}}