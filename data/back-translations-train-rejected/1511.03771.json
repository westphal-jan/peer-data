{"id": "1511.03771", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "12-Nov-2015", "title": "Improving performance of recurrent neural network with relu nonlinearity", "abstract": "In recent years significant progress has been made in successfully training recurrent neural networks (RNNs) on sequence learning problems involving long range temporal dependencies. The progress has been made on three fronts: (a) Algorithmic improvements involving sophisticated optimization techniques, (b) network design involving complex hidden layer nodes and specialized recurrent layer connections and (c) weight initialization methods. In this paper, we focus on recently proposed weight initialization with identity matrix for the recurrent weights in a RNN. This initialization is specifically proposed for hidden nodes with Rectified Linear Unit (ReLU) non linearity. We offer a simple dynamical systems perspective on weight initialization process, which allows us to propose a modified weight initialization strategy. We show that this initialization technique leads to successfully training RNNs composed of ReLUs. We demonstrate that our proposal produces comparable or better solution for three toy problems involving long range temporal structure: the addition problem, the multiplication problem and the MNIST classification problem using sequence of pixels. In addition, we present results for a benchmark action recognition problem.", "histories": [["v1", "Thu, 12 Nov 2015 04:35:41 GMT  (414kb,D)", "http://arxiv.org/abs/1511.03771v1", "10 pages 6 figures; under consideration for publication with ICLR 2016"], ["v2", "Mon, 11 Jan 2016 01:14:54 GMT  (471kb,D)", "http://arxiv.org/abs/1511.03771v2", "10 pages 6 figures; under consideration for publication with ICLR 2016"], ["v3", "Thu, 23 Jun 2016 12:52:26 GMT  (472kb,D)", "http://arxiv.org/abs/1511.03771v3", "10 pages 6 figures; under consideration for publication with ICLR 2016"]], "COMMENTS": "10 pages 6 figures; under consideration for publication with ICLR 2016", "reviews": [], "SUBJECTS": "cs.NE cs.LG", "authors": ["sachin s talathi", "aniket vartak"], "accepted": false, "id": "1511.03771"}, "pdf": {"name": "1511.03771.pdf", "metadata": {"source": "CRF", "title": "IMPROVING PERFORMANCE OF RECURRENT NEURAL NETWORK WITH RELU NONLINEARITY", "authors": ["Sachin S. Talathi", "Aniket Vartak"], "emails": ["stalathi@qti.qualcomm.com", "avartak@qti.qualcomm.com"], "sections": [{"heading": "1 INTRODUCTION", "text": "In fact, it is the case that most of us are able to abide by the rules that they have imposed on themselves. (...) In fact, it is the case that they are able to abide by the rules. (...) It is not that they are able to abide by the rules. (...) It is not that they are able to break the rules. \"(...) It is that they are able to abide by the rules.\" (...) \"It is as if they are able to break the rules.\" (...) \"It is not as if they are able to break the rules.\" (...) \"It is not as if they are able to break the rules.\""}, {"heading": "3 EXPERIMENTS", "text": "In the years to come, we will be able to be in a position, to be in a position, to be in a position, to be in a position, to put ourselves in a position, to put ourselves in a position, to put ourselves in a position, to put ourselves in a position, to put ourselves in a position, to put ourselves in a position, to put ourselves in a position, to put ourselves in a position, to put ourselves in a position, to put ourselves in a position where we are."}, {"heading": "4 RESULTS", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.1 ADDITION PROBLEM", "text": "As suggested in (Quoc et al., 2015), a basic solution would be to always predict 1 as a result, regardless of the input, resulting in a Mean Squared Error (MSE) of about 0.166. To solve this problem, the recurring network must remember the two relevant numbers and their sum, ignoring the irrelevant numbers. The task becomes more difficult with the length of the T sequence. To evaluate the RNN networks (iRNN, IRNN, np-RNN, gRNN), we generate 100,000 training examples and 10,000 test examples. For all reported results, we trained the network using the SGDBPTT optimization. The learning rate was set to 0.001 and the gradient clip parameter was set to 10. Batch size was set to 1 and all networks were trained for 10 epochs. Subsequently (Quoc et al., 2015) we were trained for the NN results."}, {"heading": "4.2 MULTIPLICATION PROBLEM", "text": "In this case, a basic solution would be to always forecast the two numbers to 0.5, so that the product is 0.25, resulting in a Mean Squared Error (MSE) of about 0.2025. Again, the goal is to train RNN to produce MSE much lower than 0.2025. We follow (Martens & Sutskever, 2011) and evaluate the performance of RNNs for the sequence of length T = {50, 100, 200}. Since gRNN could not be trained on the additional benchmark for each sequence of length T \u2265 150, we did not follow up on the training of gRNN for the multiplication problem. We again generated 100,000 training examples and 10,000 test examples and trained all networks using the SGD-BPTT optimization. The training went on for 100 eras with the batch size of 16. The learning rate was initially set at 0.0002 and then cooled twice by a factor of 10 at equal intervals. The results are summarized in Figure 4."}, {"heading": "4.3 MNIST CLASSIFICATION WITH SEQUENTIAL PRESENTATION OF PIXELS", "text": "This is another challenging toy problem, where the goal is to classify the MNIST digits (Lecun et al.) when the 784 pixels are passed sequentially to the RNN. The network is asked to predict the digit by all the 784 pixels that are serially presented to the RNN one pixel at a time. We started with the first attempt to reproduce the results from (Quoc et al., 2015), but with the SGD-BPTT optimization, we were unable to train IRNN with the series of parameters reported in (Quoc et al., 2015) using RMSprop (Tieleman & Hinton) successfully in training IRNN with the learning parameters of 10 \u2212 6. The np-RNN was trained with the parameters indicated in the RNIST."}, {"heading": "4.4 ACTION RECOGNITION BENCHMARK", "text": "In fact, it is that we are able to assert ourselves, that we are able, that we are able to play by the rules, and that we are able, that we are able, to put ourselves in a position, to put ourselves in a position, to put ourselves in a position, to put ourselves in a position, to put ourselves in a position, to put ourselves in a position, to put ourselves in a position, to put ourselves in a position, to put ourselves in the position we are in."}, {"heading": "5 CONCLUSION", "text": "We offer a dynamic system perspective on the identity weight initialization for the recursive weight matrix for hidden node RNNs composed of ReLUs. We believe that the sensitivity of hidden nodes to input disturbances resulting from the identity weight matrix initialization can make IRNN sensitive to the choice of hyperparameters for successful training. We offer an alternative weight initialization strategy, the normalized positive-defined weight matrix, which attempts to reduce the sensitivity of hidden nodes to input disturbances by reducing the dynamics to a one-dimensional manifold. We compare the performance of IRNN to np-RNN using several toy examples and also a real scale for detecting actions and show that np-RNN either performs better or is comparable to IRNN on these benchmarks."}, {"heading": "ACKNOWLEDGMENTS", "text": "We appreciate valuable feedback from our colleagues at Qualcomm Research: Daniel Fontijne and Anthony Sarah."}], "references": [{"title": "Neural machine translation by jointly learning to alogn and translate", "author": ["D. Bahdanau", "K. Cho", "Bengio"], "venue": null, "citeRegEx": "Bahdanau et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2014}, {"title": "Learning phrase representations using rnn encoder-decoder for statistical machine translation", "author": ["K. Cho", "B. van Merrienboer", "C. Gulcehre", "D. Bahdanau", "F. Bougares", "H. Schwenk", "Bengio"], "venue": null, "citeRegEx": "Cho et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Cho et al\\.", "year": 2014}, {"title": "Understanding the difficulty of training deep feedforward neural networks", "author": ["X. Glorot", "Y. Bengio"], "venue": "In 13th International Conference on Artificial Intelligence and Statistics,", "citeRegEx": "Glorot and Bengio,? \\Q2019\\E", "shortCiteRegEx": "Glorot and Bengio", "year": 2019}, {"title": "Generating sequences with recurrent neural networks", "author": ["A. Graves"], "venue": null, "citeRegEx": "Graves,? \\Q2013\\E", "shortCiteRegEx": "Graves", "year": 2013}, {"title": "Towards end-to-end speech recognition with recurrent neural networks", "author": ["A. Graves", "N. Jaitly"], "venue": "In Proceedings of 31st Internation Conference on Machine Learning,", "citeRegEx": "Graves and Jaitly,? \\Q2014\\E", "shortCiteRegEx": "Graves and Jaitly", "year": 2014}, {"title": "Long short-term memory", "author": ["S. Hochreiter", "J. Schmidhuber"], "venue": "Neural Computation,", "citeRegEx": "Hochreiter and Schmidhuber,? \\Q1997\\E", "shortCiteRegEx": "Hochreiter and Schmidhuber", "year": 1997}, {"title": "A field guide to dynamical recurrent network, chapter Gradient flow in recurrent nets", "author": ["S. Hochreiter", "Y. Bengio", "P. Frasconi", "J. Schmidhuber"], "venue": null, "citeRegEx": "Hochreiter et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Hochreiter et al\\.", "year": 2001}, {"title": "The schematic diagram of the work flow for training rnn on ucf-101 benchmark is shown in figure 6", "author": ["M. Jain", "J.C. van Gemert", "C. Snoek"], "venue": "IEEE conference on Computer Vision and Pattern Recognition,", "citeRegEx": "Jain et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Jain et al\\.", "year": 2015}, {"title": "Unifying visual-semantic embeddings with multimodal neural language models", "author": ["R. Kiros", "R. Salakhutdinov", "R.S. Zemel"], "venue": null, "citeRegEx": "Kiros et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Kiros et al\\.", "year": 2014}, {"title": "Lstm: A search space odyssey", "author": ["G. Klaus", "R.K. Srivastava", "J. Koutnik", "B.R. Steunebrink", "J. Schmidheuber"], "venue": null, "citeRegEx": "Klaus et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Klaus et al\\.", "year": 2015}, {"title": "Deep learning via hessian-free optimization", "author": ["J. Martens"], "venue": "Proceedings of 27th International Conference on Machine Learning,", "citeRegEx": "Martens,? \\Q2010\\E", "shortCiteRegEx": "Martens", "year": 2010}, {"title": "Learning recurrent neural networks with hessian-free optimization", "author": ["J. Martens", "I. Sutskever"], "venue": "Proceedings of the 28th International Conference on Machine Learning,", "citeRegEx": "Martens and Sutskever,? \\Q2011\\E", "shortCiteRegEx": "Martens and Sutskever", "year": 2011}, {"title": "Beyond short snippets: Deep networks for video classification", "author": ["J. Ng Yue-Hui", "M. Hausknecht", "S. Vijayanarasimhan", "O. Vinyals", "R. Monga", "G. Toderici"], "venue": null, "citeRegEx": "Yue.Hui et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Yue.Hui et al\\.", "year": 2015}, {"title": "On the difficulty of training recurrent neural networks", "author": ["R. Pascanu", "T. Mikolov", "Y. Bengio"], "venue": null, "citeRegEx": "Pascanu et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Pascanu et al\\.", "year": 2013}, {"title": "A simple way to initialize recurrent network of rectified linear units", "author": ["V.L. Quoc", "Navdeep", "G.E", "Hinton"], "venue": null, "citeRegEx": "Quoc et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Quoc et al\\.", "year": 2015}, {"title": "Learning representations by back-propagating", "author": ["D. Rumelhart", "G.E. Hinton", "R.J. Williams"], "venue": "error. Nature,", "citeRegEx": "Rumelhart et al\\.,? \\Q1986\\E", "shortCiteRegEx": "Rumelhart et al\\.", "year": 1986}, {"title": "Ucf101: A dataset of 101 human actions classes from videos in the wild", "author": ["K. Soomro", "A.R. Zamir", "M. Shah"], "venue": null, "citeRegEx": "Soomro et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Soomro et al\\.", "year": 2012}, {"title": "Nonlinear Dynamics and Chaos: With Applications to Physics, Biology, Chemistry, and Engineering", "author": ["S. Strogatz"], "venue": "Westview Press, second edition,", "citeRegEx": "Strogatz,? \\Q2014\\E", "shortCiteRegEx": "Strogatz", "year": 2014}, {"title": "Random walk intialization for training very deep networks", "author": ["D. Sussillo", "A. Abbott"], "venue": null, "citeRegEx": "Sussillo and Abbott,? \\Q2015\\E", "shortCiteRegEx": "Sussillo and Abbott", "year": 2015}], "referenceMentions": [{"referenceID": 15, "context": "Traditionally, training RNNs using stochastic gradient descent methods such as back-propagation through time (BPTT) (Rumelhart et al., 1986) have been riddled with difficulty.", "startOffset": 116, "endOffset": 140}, {"referenceID": 6, "context": "Early attempts suffered from the so-called vanishing gradient or exploding gradient problems resulting in difficulties to learn longrange temporal dependencies (Hochreiter et al., 2001).", "startOffset": 160, "endOffset": 185}, {"referenceID": 10, "context": "The Hessian-Free (HF) optimization method (Martens, 2010; Martens & Sutskever, 2011) for training RNNs falls under this category.", "startOffset": 42, "endOffset": 84}, {"referenceID": 1, "context": "LSTM RNNs and variants thereof (Cho et al., 2014) have produced impressive results on several sequence learning tasks including handwritting recognition (Graves, 2013), speech recognition (Graves & Jaitly, 2014), machine translation (Cho et al.", "startOffset": 31, "endOffset": 49}, {"referenceID": 3, "context": ", 2014) have produced impressive results on several sequence learning tasks including handwritting recognition (Graves, 2013), speech recognition (Graves & Jaitly, 2014), machine translation (Cho et al.", "startOffset": 111, "endOffset": 125}, {"referenceID": 1, "context": ", 2014) have produced impressive results on several sequence learning tasks including handwritting recognition (Graves, 2013), speech recognition (Graves & Jaitly, 2014), machine translation (Cho et al., 2014; Bahdanau et al., 2014), image captioning (Kiros et al.", "startOffset": 191, "endOffset": 232}, {"referenceID": 0, "context": ", 2014) have produced impressive results on several sequence learning tasks including handwritting recognition (Graves, 2013), speech recognition (Graves & Jaitly, 2014), machine translation (Cho et al., 2014; Bahdanau et al., 2014), image captioning (Kiros et al.", "startOffset": 191, "endOffset": 232}, {"referenceID": 8, "context": ", 2014), image captioning (Kiros et al., 2014), predicting output of simple computer programs (Zaremba & Sutskever, 2014) and action recognition from video clips (Ng Yue-Hui et al.", "startOffset": 26, "endOffset": 46}, {"referenceID": 14, "context": "(iii) Weight initialization, more recently Quoc et al, (Quoc et al., 2015) proposed a simpler solution to the long-range dependence problem with RNNs composed of rectified linear units (RELU)s.", "startOffset": 55, "endOffset": 74}, {"referenceID": 14, "context": "Experimental results with IRNN on several benchmark problems with long-range temporal structure are comparable to the those produced by LSTM RNNs (Quoc et al., 2015).", "startOffset": 146, "endOffset": 165}, {"referenceID": 14, "context": "We show that on several of the tasks investigated by (Quoc et al., 2015), np-RNN performs better than IRNN or the corresponding scaled version of the IRNN, the iRNN, where the recurrent weight matrix is initialized to 0.", "startOffset": 53, "endOffset": 72}, {"referenceID": 14, "context": "The IRNN model proposed by (Quoc et al., 2015) is a special case of sRNN with the following modifications: (i) the nonlinearity f is RELU and (ii) the hidden layer weights Whh are initialized with identity matrix and the bias terms are set to zero.", "startOffset": 27, "endOffset": 46}, {"referenceID": 13, "context": "In order to better understand how these modifications may enable RNN to overcome some of the long range problems, let us look at the gradient equation in the BPTT algorithm (Pascanu et al., 2013): \u2202C \u2202\u03b8 = \u2211", "startOffset": 173, "endOffset": 195}, {"referenceID": 17, "context": "In Figure 2a-d, we show the schematic diagram of the phase-space of the hidden node dynamical system in Equation 8, dependent on the eigenvalues, \u03bb, of the recurrent weight matrix (Strogatz, 2014).", "startOffset": 180, "endOffset": 196}, {"referenceID": 14, "context": "We follow the procedure described in (Quoc et al., 2015) and the RNN reads one pixel at a time in a scanline order, starting at the top left corner of the MNIST image and ending at the bottom right corner of the image.", "startOffset": 37, "endOffset": 56}, {"referenceID": 16, "context": "4 Action recognition benchmark We consider one real world example of action recognition task based on the standard UCF101 benchmark (Soomro et al., 2012).", "startOffset": 132, "endOffset": 153}, {"referenceID": 14, "context": "As suggested in (Quoc et al., 2015), a basic baseline solution would be to always predict 1 as the output regardless of the inputs.", "startOffset": 16, "endOffset": 35}, {"referenceID": 14, "context": "Following from (Quoc et al., 2015), we began by evaluating the performance of RNNs on the addition benchmark for sequences of length T = {150, 200, 300, 400}.", "startOffset": 15, "endOffset": 34}, {"referenceID": 14, "context": "For sequences up to T = 300, our results for gRNN and IRNN match those obtained by (Quoc et al., 2015).", "startOffset": 83, "endOffset": 102}, {"referenceID": 14, "context": "However, for sequence of length T = 400, contrary to findings in the (Quoc et al., 2015), we were able to train the IRNN as well as the np-RNN.", "startOffset": 69, "endOffset": 88}, {"referenceID": 14, "context": "We began by first attempting to reproduce the findings from (Quoc et al., 2015).", "startOffset": 60, "endOffset": 79}, {"referenceID": 14, "context": "However using the SGD-BPTT optimization, we were unable to train IRNN on MNIST using the set of parameters reported in (Quoc et al., 2015).", "startOffset": 119, "endOffset": 138}, {"referenceID": 14, "context": "We note that while the IRNN produces accuracy of about 83 %, the np-RNN produces an accuracy of about 92 %, which is inline with the results reported by (Quoc et al., 2015).", "startOffset": 153, "endOffset": 172}, {"referenceID": 14, "context": "4) and our inability to reproduce the results reported in (Quoc et al., 2015), we conclude that IRNN is extremely sensitive to the choice of the training protocol and the hyper parameters of the network.", "startOffset": 58, "endOffset": 77}, {"referenceID": 16, "context": "We also benchmarked IRNN, np-RNN and the LSTMs on the UCF-101 action recognition dataset (Soomro et al., 2012).", "startOffset": 89, "endOffset": 110}, {"referenceID": 7, "context": "Motivated by the recent findings of (Jain et al., 2015), we use a CNN trained on 15000 Imagenet object categories as the feature generator.", "startOffset": 36, "endOffset": 55}, {"referenceID": 7, "context": "data for the UCF101 benchmark (Ng Yue-Hui et al., 2015; Jain et al., 2015).", "startOffset": 30, "endOffset": 74}, {"referenceID": 9, "context": "Note that while the LSTM produces the best scores for this benchmark, LSTM is five times computational complex relative to the sRNN (Klaus et al., 2015).", "startOffset": 132, "endOffset": 152}], "year": 2017, "abstractText": "In recent years significant progress has been made in successfully training recurrent neural networks (RNNs) on sequence learning problems involving long range temporal dependencies. The progress has been made on three fronts: (a) Algorithmic improvements involving sophisticated optimization techniques, (b) network design involving complex hidden layer nodes and specialized recurrent layer connections and (c) weight initialization methods. In this paper, we focus on recently proposed weight initialization with identity matrix for the recurrent weights in a RNN. This initialization is specifically proposed for hidden nodes with Rectified Linear Unit (ReLU) non linearity. We offer a simple dynamical systems perspective on weight initialization process, which allows us to propose a modified weight initialization strategy. We show that this initialization technique leads to successfully training RNNs composed of ReLUs. We demonstrate that our proposal produces comparable or better solution for three toy problems involving long range temporal structure: the addition problem, the multiplication problem and the MNIST classification problem using sequence of pixels. In addition, we present results for a benchmark action recognition problem.", "creator": "LaTeX with hyperref package"}}}