{"id": "1511.03774", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "12-Nov-2015", "title": "On the Optimal Sample Complexity for Best Arm Identification", "abstract": "We study the best arm identification (BEST-1-ARM) problem, which is defined as follows. We are given $n$ stochastic bandit arms. The $i$th arm has a reward distribution $D_i$ with an unknown mean $\\mu_i$. Upon each play of the $i$th arm, we can get a reward, sampled i.i.d. from $D_i$. We would like to identify the arm with largest mean with probability at least $1-\\delta$, using as few samples as possible. We also study an important special case where there are only two arms, which we call the sign problem. We achieve a very detailed understanding of the optimal sample complexity of sign, simplifying and significantly extending a classical result by Farrell in 1964, with a completely new proof. Using the new lower bound for sign, we obtain the first lower bound for BEST-1-ARM that goes beyond the classic Mannor-Tsitsiklis lower bound, by an interesting reduction from sign to BEST-1-ARM.", "histories": [["v1", "Thu, 12 Nov 2015 04:49:46 GMT  (51kb)", "https://arxiv.org/abs/1511.03774v1", "39 pages"], ["v2", "Fri, 13 Nov 2015 05:47:39 GMT  (51kb)", "http://arxiv.org/abs/1511.03774v2", "39 pages"], ["v3", "Tue, 23 Aug 2016 18:05:29 GMT  (63kb)", "http://arxiv.org/abs/1511.03774v3", null]], "COMMENTS": "39 pages", "reviews": [], "SUBJECTS": "cs.LG cs.DS", "authors": ["lijie chen", "jian li"], "accepted": false, "id": "1511.03774"}, "pdf": {"name": "1511.03774.pdf", "metadata": {"source": "CRF", "title": "On the Optimal Sample Complexity for Best Arm Identification", "authors": ["Lijie Chen", "Jian Li"], "emails": [], "sections": [{"heading": null, "text": "ar Xiv: 151 1.03 774v 3 [cs.L G] 23 Aug 201 6"}, {"heading": "1 Introduction", "text": "The stochastic multi-arm bandit is a paradigmatic model for grasping the fundamental problem of multiple armament. We assume that many decision problems can be solved in stochastic environments. While the most studied goal is to maximize the cumulative rewards (or minimize the cumulative regret) that are separated by the forecasters (see e.g., [8, 5]), the pure exploration and exploitation phase, have also attracted significant attention due to their applications in various areas such as medical studies [29, 2, 10], communications network [2], crowdsourcing [30, 7]. In a pure exploration phase, the forecaster phase is by drawing samples from the stochastic weapons to derive the optimal (or near optimal) solution."}, {"heading": "1.1 Our Contributions", "text": "We need some notations to formalize our results. Let's use {\u00b5 [1], \u00b5 [2],..., \u00b5 [n] to be the mean of the n arms, sorted in the non-decreasing order (ties are broken arbitrarily but consistently). We use A [i] to mean the arm \u00b5 [i]. In the best-of-breed arm problem, we define the gap for the arm A [i], which is \u2206 [i] = \u00b5 [1] \u2212 \u00b5 [i], which is an important quantity for measuring sample complexity. We use A to denote an algorithm, and TA (I) to be the expected number of total arm strokes (i.e. samples) of A on instance I."}, {"heading": "1.1.1 Upper Bounds", "text": "First, we look at the upper limits for the problem of the best 1-arm with n-arm. We offer a novel algorithm that strictly improves the KKS limit and implies that it is not optimal. In particular, our algorithm for the best 1-arm limit is not optimal. (2) We can see that the improvement of the KKS limit is mainly due to the third term. (2) The ln-n factor appears to be an artifact of our algorithms or analyses. (2) However, it turns out that we can be a fundamental quantity in the best 1-arm problem. (2) We can see that the improvement over the KKS limit is mainly due to the third level. (2) We can be the artifact of our algorithms or analyses. (2) It turns out that we can be a fundamental quantity in the best 1-arm problem because we can prove a problem (2)."}, {"heading": "1.1.2 Lower Bounds", "text": "First, we emphasize that the lower limit (1) may not be sufficient for our purposes (our reduction for best-1 arm, however, requires a stronger quantitative lower limit). Furthermore, we point out that there is an infinite number of instances that require an infinite number of instances that reflect an infinite number of instances. (2) We want the lower limit to reflect the hardness caused by ignorance. (2) We say that the instances are clustered if the cardinality of the set is [i] n [i] n = 2."}, {"heading": "1.2 Other Related Work", "text": "In fact, if we allow them to approach 0, their lower limit is not tight. 5 Sign-1 and A / B problem: The problem is closely related to the problem. [25] provided that a larger problem is solved. [25] provided that a larger problem is solved. [25] Provided that a larger problem is solved, it is useful to have a lower limit for Best-1 arm, with a better constant factor than in [27]. Garivier and Kaufmann [18] get a complete sample4 We sometimes mention the sample complexity of an algorithm and its runtime interchable, since for all of our algorithms the runtime is at most a constant time of the number of samples. Therefore, sometimes, when we speak informally, we have to say that an algorithm must be \"slow,\" which also means that it needs many samples.Resolution of the asymptotic sample complexity of the Best-1 arm group is treated as the 0 that is required in the regime in which it is."}, {"heading": "4 Concluding Remarks", "text": "The most interesting open problem in this work is to obtain a near-instance optimal algorithm for best-1 arm, especially to prove (or refute) the E.4. assumption. Note that we already have such an algorithm for the cluster cases and the cases where gap entropy is. Our techniques may be helpful in achieving better boundaries for the best-k arm problem, or even the combinatorial problem of pure exploration. In an ongoing work, we already have some partial results in applying some of the ideas in this work to achieve improved upper and lower boundaries for best-k arm. 6That is, when the algorithm redraws, we withdraw A to receive a reward r, and return 2-r as a reward for new."}, {"heading": "Supplementary Material for \u201cOn the Optimal Sample Complexity", "text": "The supplementary material is broken down as follows: 1. (Section A) We provide some preliminary findings for our future developments. 2. (Section B) We provide all the details of our new algorithm DistrBasedElim and the proof for Theorem 2.5. In Section B.4 we define the cluster instances and show that our algorithm is almost excessively optimal for such instances. In Section B.5 we modify the algorithm slightly and obtain an improved (\u03b5, \u03b4) -PAC algorithm for Best-1-Arm.3. (Section C) We provide the detailed proof for Theorem 3.1, our new lower limit for Best-1-Arm.4. (Section D) We present our new lower limit for Sign-\u0439, which is the basis for our lower limit reduction in Section C.5. (Section E) We propose to examine the Best-1 arm from the perspective of instance optimality and propose a guess regarding the basic sample complexity."}, {"heading": "A Preliminaries", "text": "Definition A.1. Let R > 0, we say a distribution D on R has R-sub-Gaussian tail (or D is R-sub-Gaussian tail (or D is R-subGaussian) if for the random variable X and any t-Gaussian distribution, we have that E [exp (tX \u2212 tE [X])] \u2264 exp (R2t2 / 2).It is known that the family of R-sub-Gaussian distribution supports all distributions with support for [0, R] as well as many unlimited distributions such as Gaussian distribution with Variance R2 (Best).It is known that the standard concentration imbalance for R-sub-Gaussian random variables.Lemma A.2. (Hoeffding's inequality) Let X1,. Xn be n i.d. random variables drawn from an R-sub-Gaussian distribution D. Let II = Ex-Gaussian distribution D [x]."}, {"heading": "B.1 Useful Building Blocks", "text": "We do not make assumptions about the behavior of A in this dilemma. Example: A can even output erroneous answers with a high probability. [1] Data: Arm Set S, approximation level, confidence level. Result: For each arm a, output of empirical averages (1) output of empirical averages (1) output of empirical averages (1) output of empirical averages (1) output of empirical averages (1) output of empirical averages (1) output of empirical averages (1) output of empirical averages (1). [2] Example of empirical averages (2). We have the following Lemma for algorithms 2, which is a simple sequence of the Lemma A.2. Lemma B.1. For each group we have an empirical averaging."}, {"heading": "B.2 Our Algorithm", "text": "Now everything is ready to describe our algorithm DistrBasedElim for Best-1-Arm. In addition, we offer a high-level description here. All the detailed parameters can be found in a series of sub-optimal arms, while the algorithm ensures that the best arm is not eliminated. First, it applies the MedianElim method to find an optimal arm, where \u03b5r = 2 \u2212 r. Suppose it is ar. Then we take a number of samples from ar to estimate its mean (denote the empirical mean by \u00b5 [ar]). Unlike previous algorithms in [12, 24] where either a fixed fraction of weapons or these weapons with an average of much less than ar are eliminated, we use a fractiontest to see if there are many arms with much less than ar."}, {"heading": "B.3 Analysis of the running time", "text": "We use A to mark the algorithm. < p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p"}, {"heading": "B.4 Almost Instance Optimal Bound for Clustered Instances", "text": "In this case, we can get an almost instantial optimal algorithm for such instances. To this end, we just need to set a narrower limit for hI.Lemma B.21. hI \u2264 2 \u00b7 | {U i 6 = 0. In this case, we can get an almost instantial optimal algorithm for such instances. Let's design an index so that U s is not empty. Let's design the largest index so that U s \u00b2 is not empty. If such an index does not exist, let's leave s \u00b2 = 0. We show that during rounds s \u00b2 and s \u00b2 is not true."}, {"heading": "B.5 An Improved PAC Algorithm", "text": "Finally, we will discuss how to convert our algorithm into a (\u03b5, \u03b4) -PAC algorithm for Best-1-Arm. < < < / p > p > p > p > p > p > p > p > p > p \"p > p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p. \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p.\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p.\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p."}, {"heading": "D A New Lower Bound for Sign-\u03be", "text": "In this section, we will prove a new lower limit of the sign ID (theorem D.1), from which C.1 is easily followed. We will introduce some notations. We will remember that the distributions of all arms are Gaussian with variance 1. If we fix an algorithm A for sign ID, for a random event E, we will abbreviate PrA, A\u00b5 [E] (remember that A\u00b5 denotes an arm of medium size) the probability that E happens when we run A on arm A\u00b5. For notational simplicity, when A is clear from the context, we will abbreviate it as Pr\u00b5 [E]. Likewise, we will write E\u00b5 [X] as short notation for EA, A\u00b5 [X], which denotes the expectation of random variable X when we run A on arm A\u00b5.We will use 1 {expr} to designate the indicator function, which corresponds to 1 if export is 1, and 0 otherwise, and we will define Z (F)."}, {"heading": "D.1 Proof for Theorem D.1", "text": "The rest of this section is devoted to the proof of theorem D.1. From now on we can see that there is a fixed constant. < < < < < < < < < < < < < 0.01) is a fixed constant. First, we show a simple but convenient problem based on Lemma A.3.Lemma D.2. Let us I1 (with reward distribution D1) and I2 (with reward distribution D2) be two instances of character ID. Let E is a random event and the total number of samples taken by A. Suppose PrD1 [E]. \u00b7 KL (D1, D2)."}, {"heading": "E On Almost Instance Optimality", "text": "To put it casually, an algorithm A is an optimal instance when the runtime of Aon instance I is maximum O (L (I), where L (I) is the lower instance required to solve the instance for each instance. First, let's consider the lower instance for each instance, as we can design an algorithm that uses o (2 ln ln) an infinite number of instances. We offer a detailed discussion in Section G. We can see that it is impossible to get an optimal algorithm even for sign-II."}, {"heading": "F Missing Proofs in Section B", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "F.1 Proof for Lemma B.3", "text": "Lemma B.3 (reordered) Suppose that \u03b5 < 0,1 and t-j. (\u03b5, 1 \u2212 j. j. j. j. j. j. j. j.) < (1 \u2212 j. j. j. j. j. j. j.) < (1 \u2212 t + j. j. j. j. j. j. j.) | S | (or equivalent | S \u2264 cr | (t \u2212 j. j. j. j. j. j. j. j. j. j. j. / j. j. j. j. j. j. j. / j. j. j. j. j. j. j. / j. j. j. j. j. / j. j. j. j. j. j. j. / j. j. j. j. j. / j. j. j. j. j. / j."}, {"heading": "F.2 Proof for Lemma B.4", "text": "Lemma B.4 (redefined) Assume that we perform in all rounds, 2. Fraction tests, 1. Fraction tests, 2. Fraction tests, 2. Fraction tests, 3. Fraction tests, 4. Fraction tests, 4. Fraction tests, 5. Fraction tests, 5. Fraction tests, 6. Fraction tests, 6. Fraction tests, 6. Fraction tests, 7. Fraction tests, 7. Fraction tests, 7. Fraction tests, 7. Fraction tests, 9. Fraction tests, 8. Fraction tests, 8. Fraction tests, 8. Fraction tests, 8. Fraction tests, 8. Fraction tests, 8. Fraction tests, 8. Fraction tests, 9. Fraction tests, 9. Fraction tests, 9. Fraction tests, 11. Fraction tests, 11. Fraction tests, 11."}, {"heading": "F.3 Proofs for Lemma B.16, Lemma B.17 and Lemma B.18", "text": "For convenience, we have letS = rr = U > rr = U > rr = U > rr = U > rr = Sr + 1, Ncur = S = rr, s). To prove these three terms, we need a significant inequality for Pr. If we have more than 1 percent, we must also remember that ls = ln (min (hI, s) + 1 percent (ln) is a significant inequality for Pr. + lmaxs, we have: Pr \u2212 Pr + 1 = c2 \u00b7 (+ 3 percent) s = r (ln) s (1 + ls) \u00b7 2 percent (ln) \u00b7 2 percent (ln). + lmaxs) \u00b7 2r) \u00b7 2r)."}, {"heading": "G More About Sign-\u03be", "text": "In this section, we introduce a class of procedures that require a reference. < < p > p > p > p > p > p > p > p > p > p < p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p, p > p > p, p > p > p > p, p > p > p, p > p > p, p > p > p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p"}, {"heading": "H \u03b4-correct Algorithms and Parallel Simulation", "text": "In Corollary B.19 we show that our algorithm can output the correct response with a probability of at least 1 \u2212 \u03b4 (and the conditional expected runtime is more limited). However, it is preferable to have an algorithm with a limited expected runtime and to be correct with a probability of at least 1 \u2212 \u03b4 time. In this section we provide such a transformation that generates an algorithm for the earlier way, one of the later (increasing the time limit only by a constant factor).Now we formally define the two types of algorithms. Definition H.1. Leave an algorithm for any problem P. An additional input is made as a confidence level. Let me set the set of all valid instances for P. We write the algorithms with a fixed trust level."}], "references": [], "referenceMentions": [], "year": 2016, "abstractText": "<lb>We study the best arm identification (Best-1-Arm) problem, which is defined as follows.<lb>We are given n stochastic bandit arms. The ith arm has a reward distribution<lb>Di with an<lb>unknown mean \u03bci. Upon each play of the ith arm, we can get a reward, sampled i.i.d. from<lb>Di. We would like to identify the arm with the largest mean with probability at least 1 \u2212 \u03b4,<lb>using as few samples as possible. We provide a nontrivial algorithm for Best-1-Arm, which<lb>improves upon several prior upper bounds on the same problem. We also study an important<lb>special case where there are only two arms, which we call the Sign-\u03be problem. We provide a<lb>new lower bound of Sign-\u03be, simplifying and significantly extending a classical result by Farrell<lb>in 1964, with a completely new proof. Using the new lower bound for Sign-\u03be, we obtain the<lb>first lower bound for Best-1-Arm that goes beyond the classic Mannor-Tsitsiklis lower bound,<lb>by an interesting reduction from Sign-\u03be to Best-1-Arm. We propose an interesting conjecture<lb>concerning the optimal sample complexity of Best-1-Arm from the perspective of instance-wise<lb>optimality.", "creator": "LaTeX with hyperref package"}}}