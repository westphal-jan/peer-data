{"id": "1511.03791", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "12-Nov-2015", "title": "Towards Vision-Based Deep Reinforcement Learning for Robotic Motion Control", "abstract": "This paper introduces a machine learning based system for controlling a robotic manipulator with visual perception only. The capability to autonomously learn robot controllers solely from raw-pixel images and without any prior knowledge of configuration is shown for the first time. We build upon the success of recent deep reinforcement learning and develop a system for learning target reaching with a three-joint robot manipulator using external visual observation. A Deep Q Network (DQN) was demonstrated to perform target reaching after training in simulation. Transferring the network to real hardware and real observation in a naive approach failed, but experiments show that the network works when replacing camera images with synthetic images.", "histories": [["v1", "Thu, 12 Nov 2015 06:19:59 GMT  (4489kb,D)", "https://arxiv.org/abs/1511.03791v1", "8 pages, to appear in the proceedings of Australasian Conference on Robotics and Automation (ACRA) 2015"], ["v2", "Fri, 13 Nov 2015 05:41:08 GMT  (4489kb,D)", "http://arxiv.org/abs/1511.03791v2", "8 pages, to appear in the proceedings of Australasian Conference on Robotics and Automation (ACRA) 2015"]], "COMMENTS": "8 pages, to appear in the proceedings of Australasian Conference on Robotics and Automation (ACRA) 2015", "reviews": [], "SUBJECTS": "cs.LG cs.CV cs.RO", "authors": ["fangyi zhang", "j\\\"urgen leitner", "michael milford", "ben upcroft", "peter corke"], "accepted": false, "id": "1511.03791"}, "pdf": {"name": "1511.03791.pdf", "metadata": {"source": "CRF", "title": "Towards Vision-Based Deep Reinforcement Learning for Robotic Motion Control", "authors": ["Fangyi Zhang", "J\u00fcrgen Leitner", "Michael Milford", "Ben Upcroft", "Peter Corke"], "emails": ["fangyi.zhang@hdr.qut.edu.au"], "sections": [{"heading": "1 Introduction", "text": "In fact, we are able to maneuver in such highly dynamic and complex environments by observing how other people perform them. Methods are necessary in order to gain new skills through experimentation. We want the robots to be able to learn in the same way as they do. Methods are such that they are able to solve a number of different tasks. We want the robots to be able to learn in the same way that they do."}, {"heading": "2 Related Work", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "2.1 Vision-based Robotic Manipulation", "text": "Vision-based robot manipulation is the process in which robots use their manipulators (such as robot arms) to rearrange environments [Mason, 2001], based on camera images. Early vision-based robot manipulation was implemented using pose-based (positioning and orientation) closed-loop control, in which vision was typically used to extract the pose of an object as input for a manipulation controller at the beginning of a task [Kragic and Christensen, 2002]. Most current vision-based robot manipulation methods are closed-loop based on visual perception. A vision-based manipulation system was applied on a Johns Hopkins \"Steady Hand Robot\" for cooperative manipulation on a millimeter to micrometer scale, using virtual equipment objects based on visual perception. [Bettini et al., 2004] A vision-based manipulation system was applied on a Johns Hopkins \"Steady Hand Robot\" for cooperative manipulation on a millimeter to micrometer scale, using virtual devices based on visual perception."}, {"heading": "2.2 Reinforcement Learning in Robotics", "text": "Reinforcement Learning (RL) [Sutton and Barto, 1998; Kormushev et al., 2013] was used in robotics because it promises a way to learn complex actions on complex robotic systems by merely informing the robot whether its actions were successful (positive reward) or not (negative reward). [Peters et al., 2003] reviewed some of the RL concepts with regard to their applicability to control complex humanoid robots and highlighted some of the problems with greedy political search and gradient-based methods. How to generate the right reward is an active research topic. Intrinsic motivation and curiosity have been shown to provide means to explore large state spaces, such as those found on complex humanoids, more quickly and efficiently [Frank et al., 2014]."}, {"heading": "2.3 Deep Visuomotor Policies", "text": "In order to enable robots to learn manipulation skills with little prior knowledge, Sergey et al. [Levine et al., 2015a; Levine et al., 2015b] introduced a political representation architecture (Deep Visuomotor Policies) based on Convolutionary Neural Networks and its guided political search methodology. Deep Visuomotor Policies maps joint angles and camera images directly to the shared torque. Robot configurations are the only prior knowledge necessary. The political search methodology consists of two phases, i.e. optimal control phase and supervised learning phase. The training consists of three procedures, i.e. Pose CNN training, pre-training trajectories and end-to-end-to-training. The Deep Visuomotor Policies enabled robots to learn manipulation skills with little prior knowledge through supervised learning, but pre-collected data sets were necessary."}, {"heading": "2.4 Deep Q Network", "text": "The DQN, a current example of DRL, fulfills both the autonomy and flexibility requirements for learning from exploration. It has successfully learned to play 49 different Atari 2600 games, thereby achieving a human control level [Mnih et al., 2015]. The DQN used a deep Convolutionary Neural Network (CNN) [Krizhevsky et al., 2012] to approximate a Q-value function. It maps raw pixel images directly to actions. No pre-entry feature extraction is required. The only thing that allows the algorithm to improve the guidelines through games over and over again. It has learned to play 49 different games using the same network architecture without modifications. The DQN is defined by its input - raw pixels of game video frames and received rewards - and the results, i.e. the number of actions available in a game, no motion control is required in order to use any motion dynamics."}, {"heading": "3 Problem Definition and System Description", "text": "A common problem in robot-assisted manipulation is reaching the object to be interacted with. This goal-accomplishment task is defined as controlling a robot arm so that its end effector achieves a certain target configuration. We are interested in the case where a robot achieves the goal only with visual perception. To learn such a task, we have developed a system consisting of three parts: \u2022 a 2D robot-assisted goal-accomplishment simulator that generates visual inputs for the learner \u2022 a deep amplification learning framework based on the DQN implementation of Google Deepmind [Mnih et al., 2015], and \u2022 a component of ROS-based interfaces to control a Baxter robot according to DQN outputs."}, {"heading": "3.1 DQN-based Learning System", "text": "The DQN used here has the same architecture as the Atari game, which contains three revolutionary layers and two completely connected layers [Mnih et al., 2015]. Its implementation is based on the Google Deepmind DQN code1 with minor modifications. Fig. 2 shows the architecture and exemplary output of each layer. The input of the DQN includes rewards and images. Its output is the index of actions to be taken. The DQN learns goal achievement skills in interaction with the simulator to achieve the goal. An overview of the system framework for learning in the simulation as well as for testing on a real robot is in Fig. 3.1https: / / sites.google.com / a / deepmind.com / dqn / When training or testing in the simulation, the goal achievement simulator delivers the reward value (R) and the simulation (R) is sent directly to the network of the DQN (The Activity Output)."}, {"heading": "3.2 Target Reaching Simulator", "text": "We simulate the accomplishment of the task of controlling a robotic arm with three joints in 2D (Fig. 4). The simulator was implemented from scratch. No simulation platform was used in the implementation. As shown in Fig. 4 (a), the robotic arm consists of four joints and three joints, the configurations of which conform to the specifications of a baxter arm, including the joint limitations. The blue dot is the goal to be achieved. For better visualization, the position of the end effector is marked with a red dot. The simulator can be controlled by sending certain commands to the individual joints \"S1,\" \"E1\" and \"W1.\" The simulation screen resolution is 160 x 320 cm. The corresponding real scenario that the simulator simulates is: with corresponding constant hinged angles on a baxter arm, the arm moves in a vertical plane controlled by the joints."}, {"heading": "3.3 Reward Function", "text": "To stay in line with the DQN setup, the reward function has two return values: one for the reward of each action; the other indicates whether the target reaches the game algorithm 1: Reward function Input: Pt: the target 2D coordinates; Pe: the end effector 2D coordinates. Output: R: the reward for the current state; T: whether the game terminal.1 Dis = ComputeDistance (Pt, Pe); 2 DisChange = Dis \u2212 PreviousDis; 3 if DisChange > 0 then 4 R = \u2212 1; 5 otherwise DisChange < 0 then 6 R = 1; 7 otherwise 8 R = 0; 9 end10 Racc = Rt + Rt \u2212 1 + Rt \u2212 2; 11 if Racc < \u2212 1 then 12 T = True; 13 otherwise 14 T = False; 15 endis Terminal. Its algorithm is displayed in algorithm 1. The reward is determined according to the distance between the effect."}, {"heading": "4 Experiments and Results", "text": "In order to evaluate the feasibility of the DQN-based system in learning to achieve the objectives, we conducted some experiments in both simulation and real-world scenarios. Experiments consist of three phases: training in simulation, testing in simulation and testing in the real world."}, {"heading": "4.1 Training in Simulation Scenarios", "text": "In order to evaluate the DQN's ability to adapt to a particular noise in terms of manipulation of robotics, several agents have been trained with different simulation settings, including noise, image offset, variations in the initial phase and the length of the connection. Settings for training the five agents are shown in Table 1. Settings are shown in Figure 5. To simulate the noise of the camera, noise has been added in Figure B. Random settings have been included in the same sequence (0.0, 0.0, 0.0)."}, {"heading": "4.2 Testing in Simulation Scenarios", "text": "This year it has come to the point that it will only be a matter of time before it will happen, until it does."}, {"heading": "4.3 Real World Experiment Using Camera Images", "text": "In order to verify the feasibility of trained agents in the real world, we conducted a goal achievement experiment in real scenarios using camera images, i.e. the second phase mentioned in Section 3.1. In this experiment, we used Agent B, who was trained with 3 million steps, which has relatively high success rates for both Setting A and B in the simulation test. Experimental settings were arranged for the case where the 2D simulator simulated, i.e. a Baxter arm moved on a vertical plane with a white background. A gray scale camera was placed in front of the arm and observed the arm with a horizontal view of the point (for the DQN, the gray scale camera is the same as with a color camera, since even the images from Ai games and the 2D goal achievement simulator are RGB color images, they are converted into RGB color images that are reacted to the background on the network)."}, {"heading": "4.4 Real World Experiment Using", "text": "Synthetic imaging To verify the analysis of why Agent B was not able to achieve the goal, we performed another real experiment using synthetic images instead of camera images. In the experiment, the synthetic images were generated from the 2D simulator using real-time joint angles (\"S1,\" \"E1\" and \"W1\") on a Baxter robot. In this experiment, the real-time joint angles were provided by the ROS-based interfaces. In this case, there was no difference between real scenario and simulation scenario images. All other settings were identical to those in Section 4.3, as in Fig. 1. In this experiment, we used the same agent used in Section 4.3, i.e. Agent B trained with 3 million steps. It achieved a consistent success rate with the one used in simulation scenario investigations. From the results, we can conclude that the reason why Agent B is using image form to achieve the goal may arise from differences in some of the factors that differed with the existence of the objective images."}, {"heading": "5 Conclusion and Discussion", "text": "The DQN-based system is practicable to learn to achieve objectives based on exploration in the simulation, using only visual observation without prior knowledge. However, the agent (Agent B) trained in simulation scenarios did not achieve objectives in the real world using camera images as input. Instead, an experiment was conducted in the real world using synthetic images as input scenarios. To determine the causes of this more work, a consistent success rate with the simulation is required. These two different results show that the failure in the real experiment with camera images was caused by the input image differences between real and simulation scenarios. To determine the causes of this more work, we must either reduce the image differences or make the agents more robust in the face of these differences."}, {"heading": "Acknowledgements", "text": "This research was carried out by the Australian Research Council Centre of Excellence for Robotic Vision (project number CE140100016) and was partly funded by the HPC and Research Support Group of Queensland University of Technology (QUT)."}], "references": [{"title": "International Journal of Humanoid Robotics", "author": ["Tamim Asfour", "Pedram Azad", "Florian Gyarfas", "R\u00fcdiger Dillmann. Imitation learning of dual-arm manipulation tasks in humanoid robots"], "venue": "5(02):183\u2013202,", "citeRegEx": "Asfour et al.. 2008", "shortCiteRegEx": null, "year": 2008}, {"title": "IEEE Transactions on Robotics", "author": ["Alessandro Bettini", "Panadda Marayong", "Samuel Lang", "Allison M Okamura", "Gregory D Hager. Vision-assisted control for manipulation using virtual fixtures"], "venue": "20(6):953\u2013966,", "citeRegEx": "Bettini et al.. 2004", "shortCiteRegEx": null, "year": 2004}, {"title": "Frontiers in Neurorobotics", "author": ["Mikhail Frank", "J\u00fcrgen Leitner", "Marijn Stollenga", "Alexander F\u00f6rster", "J\u00fcrgen Schmidhuber. Curiosity driven reinforcement learning for motion planning on humanoids"], "venue": "7(25),", "citeRegEx": "Frank et al.. 2014", "shortCiteRegEx": null, "year": 2014}, {"title": "Stockholm", "author": ["Danica Kragic", "Henrik I Christensen. Survey on visual servoing for manipulation. Technical report", "Computational Vision", "Active Perception Laboratory", "Royal Institute of Technology"], "venue": "Sweden,", "citeRegEx": "Kragic and Christensen. 2002", "shortCiteRegEx": null, "year": 2002}, {"title": "Robotics and Autonomous Systems", "author": ["Danica Kragic", "M\u00e5rten Bj\u00f6rkman", "Henrik I Christensen", "Jan-Olof Eklundh. Vision for robotic object manipulation in domestic settings"], "venue": "52(1):85\u2013 100,", "citeRegEx": "Kragic et al.. 2005", "shortCiteRegEx": null, "year": 2005}, {"title": "In Advances in Neural Information Processing Systems (NIPS)", "author": ["Alex Krizhevsky", "Ilya Sutskever", "Geoffrey E Hinton. Imagenet classification with deep convolutional neural networks"], "venue": "pages 1097\u20131105,", "citeRegEx": "Krizhevsky et al.. 2012", "shortCiteRegEx": null, "year": 2012}, {"title": "CA", "author": ["Sergey Levine", "Chelsea Finn", "Trevor Darrell", "Pieter Abbeel. End-to-end training of deep visuomotor policies. Technical report", "University of California", "Berkeley"], "venue": "USA,", "citeRegEx": "Levine et al.. 2015a", "shortCiteRegEx": null, "year": 2015}, {"title": "Proceedings of the IEEE International Conference on Robotics and Automation (ICRA)", "author": ["Sergey Levine", "Nolan Wagener", "Pieter Abbeel. Learning contact-rich manipulation skills with guided policy search"], "venue": "pages 156\u2013163,", "citeRegEx": "Levine et al.. 2015b", "shortCiteRegEx": null, "year": 2015}, {"title": "Mechanics of robotic manipulation", "author": ["Matthew T Mason"], "venue": "MIT Press,", "citeRegEx": "Mason. 2001", "shortCiteRegEx": null, "year": 2001}, {"title": "Playing atari with deep reinforcement learning", "author": ["Mnih et al", "2013] Volodymyr Mnih", "Koray Kavukcuoglu", "David Silver", "Alex Graves", "Ioannis Antonoglou", "Daan Wierstra", "Martin Riedmiller"], "venue": "Technical report,", "citeRegEx": "al. et al\\.,? \\Q2013\\E", "shortCiteRegEx": "al. et al\\.", "year": 2013}, {"title": "Human-level control through deep reinforcement", "author": ["Mnih et al", "2015] Volodymyr Mnih", "Koray Kavukcuoglu", "David Silver", "Andrei A Rusu", "Joel Veness", "Marc G Bellemare", "Alex Graves", "Martin Riedmiller", "Andreas K Fidjeland", "Georg Ostrovski"], "venue": null, "citeRegEx": "al. et al\\.,? \\Q2015\\E", "shortCiteRegEx": "al. et al\\.", "year": 2015}, {"title": "In Proceedings of the IEEE-RAS International Conference on Humanoid Robots", "author": ["Jan Peters", "Sethu Vijayakumar", "Stefan Schaal. Reinforcement learning for humanoid robotics"], "venue": "pages 1\u201320,", "citeRegEx": "Peters et al.. 2003", "shortCiteRegEx": null, "year": 2003}, {"title": "Reinforcement learning: An introduction", "author": ["Richard S Sutton", "Andrew G Barto"], "venue": "MIT Press,", "citeRegEx": "Sutton and Barto. 1998", "shortCiteRegEx": null, "year": 1998}], "referenceMentions": [{"referenceID": 8, "context": "Vision-based robotic manipulation is the process by which robots use their manipulators (such as robotic arms) to rearrange environments [Mason, 2001], based on camera images.", "startOffset": 137, "endOffset": 150}, {"referenceID": 3, "context": "The early vision-based robotic manipulation was implemented using pose-based (position and orientation) closed-loop control, where vision was typically used to extract the pose of an object as an input for a manipulation controller at the beginning of a task [Kragic and Christensen, 2002].", "startOffset": 259, "endOffset": 289}, {"referenceID": 1, "context": "A visionbased manipulation system was implemented on a Johns Hopkins \u201cSteady Hand Robot\u201d for cooperative manipulation at millimeter to micrometer scales, using virtual fixtures [Bettini et al., 2004].", "startOffset": 177, "endOffset": 199}, {"referenceID": 4, "context": "With both monocular and binocular vision cues, various closed-loop visual strategies were applied to enable robots to manipulate both known and unknown objects [Kragic et al., 2005].", "startOffset": 160, "endOffset": 181}, {"referenceID": 0, "context": "With continuous hidden Markov models (HMMs), a humanoid robot was able to learn dual-arm manipulation tasks from human demonstrations through vision [Asfour et al., 2008].", "startOffset": 149, "endOffset": 170}, {"referenceID": 12, "context": "Reinforcement Learning (RL) [Sutton and Barto, 1998; Kormushev et al., 2013] has been applied in robotics, as it promises a way to learn complex actions on complex robotic systems by just providing informing the robot whether its actions were successful (positive reward) or not (negative reward).", "startOffset": 28, "endOffset": 76}, {"referenceID": 11, "context": "[Peters et al., 2003] reviewed some of the RL concepts in terms of applicability to control complex humanoid robots and highlighting some of the issues with greedy policy search and gradient based methods.", "startOffset": 0, "endOffset": 21}, {"referenceID": 2, "context": "Intrinsic motivation and curiosity have been shown to provide means to explore large state spaces, such as the ones found on complex humanoids, faster and more efficient [Frank et al., 2014].", "startOffset": 170, "endOffset": 190}, {"referenceID": 6, "context": "[Levine et al., 2015a; Levine et al., 2015b].", "startOffset": 0, "endOffset": 44}, {"referenceID": 7, "context": "[Levine et al., 2015a; Levine et al., 2015b].", "startOffset": 0, "endOffset": 44}, {"referenceID": 5, "context": "The DQN used a deep convolutional neural network (CNN) [Krizhevsky et al., 2012] to approximate a Q-value function.", "startOffset": 55, "endOffset": 80}], "year": 2015, "abstractText": "This paper introduces a machine learning based system for controlling a robotic manipulator with visual perception only. The capability to autonomously learn robot controllers solely from raw-pixel images and without any prior knowledge of configuration is shown for the first time. We build upon the success of recent deep reinforcement learning and develop a system for learning target reaching with a three-joint robot manipulator using external visual observation. A Deep Q Network (DQN) was demonstrated to perform target reaching after training in simulation. Transferring the network to real hardware and real observation in a naive approach failed, but experiments show that the network works when replacing camera images with synthetic images.", "creator": "LaTeX with hyperref package"}}}