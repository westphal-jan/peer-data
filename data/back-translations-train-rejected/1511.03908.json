{"id": "1511.03908", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "12-Nov-2015", "title": "Learning Human Identity from Motion Patterns", "abstract": "We present a large-scale study, exploring the capability of temporal deep neural networks in interpreting natural human kinematics and introduce the first method for active biometric authentication with mobile inertial sensors. At Google, we have created a first-of-its-kind dataset of human movements, passively collected by 1500 volunteers using their smartphones daily over several months. We (1) compare several neural architectures for efficient learning of temporal multi-modal data representations, (2) propose an optimized shift-invariant dense convolutional mechanism (DCWRNN) and (3) incorporate the discriminatively-trained dynamic features in a probabilistic generative framework taking into account temporal characteristics. Our results demonstrate, that human kinematics convey important information about user identity and can serve as a valuable component of multi-modal authentication systems.", "histories": [["v1", "Thu, 12 Nov 2015 14:48:53 GMT  (1598kb)", "https://arxiv.org/abs/1511.03908v1", "10 pages, 6 figures, 2 tables"], ["v2", "Tue, 8 Dec 2015 15:23:06 GMT  (1427kb)", "http://arxiv.org/abs/1511.03908v2", "10 pages, 6 figures, 2 tables"], ["v3", "Wed, 9 Dec 2015 01:59:58 GMT  (2044kb)", "http://arxiv.org/abs/1511.03908v3", "10 pages, 6 figures, 2 tables"], ["v4", "Thu, 21 Apr 2016 16:04:00 GMT  (3357kb,D)", "http://arxiv.org/abs/1511.03908v4", "10 pages, 6 figures, 2 tables"]], "COMMENTS": "10 pages, 6 figures, 2 tables", "reviews": [], "SUBJECTS": "cs.LG cs.CV cs.NE", "authors": ["natalia neverova", "christian wolf", "griffin lacey", "lex fridman", "deepak chandra", "brandon barbello", "graham taylor"], "accepted": false, "id": "1511.03908"}, "pdf": {"name": "1511.03908.pdf", "metadata": {"source": "CRF", "title": "Learning Human Identity from Motion Patterns", "authors": ["Natalia Neverova", "Christian Wolf", "Griffin Lacey", "Lex Fridman", "Deepak Chandra", "Brandon Barbello", "Graham Taylor"], "emails": ["firstname.surname@liris.cnrs.fr", "laceyg@uoguelph.ca,", "gwtaylor@uoguelph.ca", "fridman@mit.edu", "dchandra@google.com,", "bbarbello@google.com."], "sections": [{"heading": null, "text": "Index Terms - Authentication, Biometrics (access control), Learning, Mobile computing, Recurrent Neural Networks retaks.I. INTRODUCTIONFor the billion of smartphone users worldwide, remembering dozens of passwords for all the services we need to use and spending precious seconds typing pins or drawing sophisticated swipe patterns on touchscreens becomes a source of frustration. In recent years, researchers in various fields have been working to create fast and secure authentication alternatives that would make it possible to remove this burden from the user [29].Historically, research into biometric data has been hampered by the difficulty of collecting data, both from a practical and legal perspective. Previous studies have been limited to narrowly defined laboratory-scale data collection, poorly representing real-world scenarios: not only due to the limited amount and diversity of data, but also due to the essential self-awareness of the participants who perform tasks on it, which we naturally respond."}, {"heading": "II. RELATED WORK", "text": "The use of portable or mobile inertial sensors for authentication, action detection or estimation of parameters of a particular activity has been explored in various contexts. A detailed overview and benchmarking of existing state-of-the-art is provided in [23]. Derawi et al. [9], for example, used a smartphone attached to the human body to extract information about walking cycles, achieving a 20.1% error rate. A number of papers exist that examine the problem of activity and gesture recognition with motion sensors, including methods based on deep learning. In [12] and [7] exhaustive Xiv: 151 1.03 908v 4 [cs.L G] 21 Apr 201 62 Overviews of preprocessing techniques and manual features that promise multiple recognition."}, {"heading": "III. A GENERATIVE BIOMETRIC FRAMEWORK", "text": "Our goal is to separate a user from a fraudster on the basis of a time series of inertial measurements (Fig. 1). Our method is based on two components: a feature extraction pipeline that links each user's motion sequence to a collection of discriminatory features, and a biometric model that accepts and verifies these features as inputs. Although the feature extraction component is the most interesting and novel aspect of our technique, we delay discussing it in Section IV. We begin by discussing the data format and the biometric model."}, {"heading": "A. Movement data", "text": "Each measurement (frame) in a synchronized raw input stream of acceleration and gyroscopic data is in the form {ax, ay, az, \u03c9x, \u03c9y, \u03c9z}, where a stands for linear acceleration, \u03c9 angular velocity and x, y, z for projections on corresponding axes coordinated with the phone. There are two important steps we take before the function extraction. Obfuscation-based regulation - it is important to distinguish between the term \"device\" and \"user.\" In the data set we have collected (Section VI), each device is assigned to a single user, so that all data is considered authentic. In real-world scenarios such as theft, authentic and fraudulent data can come from the same device. In a recent study [8] it was shown that under laboratory conditions a particular device can be identified by a reaction of its motion sensors to a given signal."}, {"heading": "B. Biometric model", "text": "Although this technology is well established for many mobile services, our application is essentially different from others such as voice search, as it is a constant collection of particularly sensitive user data. In addition, adaptation to a new user should be quick, which would result in a limited amount of training data for the \"positive\" class. This data may not be fully representative of typical usage. For these reasons, a purely discriminatory setting involving a separate model per user is required, or even fine-tuning a model for each new user would hardly be feasible. Therefore, we adapt a generative model, namely a general data distribution in dynamic space, and create a universal background."}, {"heading": "IV. LEARNING EFFECTIVE AND EFFICIENT REPRESENTATIONS", "text": "The first two conditions are known to be contradictory, since the performance of a distinct trait typically increases with integration time. [21] Two paradigms that establish a balance between power of representation and speed have dominated the feature learning landscape in recent years. These are multidimensional temporal aggregation via 1-dimensional Convolutionary Networks Fig. 2a and explicit modelling of temporal dependencies via recursive neural networks Fig. 2b. The earlier model, which is popular in speech recognition, involves the revolutionary learning of integrated temporal statistics from short and long sequences of data (known as \"short-term\" and \"long-term\" constellations)."}, {"heading": "A. Classical RNN and Clockwork RNN", "text": "It (t) s (t) s (t) s (t) s (t) s (t) s (t) s (t) s (t) s (t) s (t) s (t) s (t) s (t) s (t) s (t) s (t) s (t) s (t) s (t) s (t) s (t) s (t) s (t) s (t) s (t) s (t) s (t) s (t) s (t) s (t) s (t) s (t) s (t) s (t) s (t) s (t) s (t) s (t) s (t) s (t) s (t) s (t) s (t) s (t) s (t) s (t) s (t) s (t) s (t) s (t) s (t) s (t) s (t) s (t) s (t) s (t) s (t) s (t) s (t) s (t) s (t) s (t) s) s (t) s (t) s (t) s (t) s) s (t) s (t) s (t) s (t) s (t) s (t) s (t) s (t) s (t) s (t) s (t) s (t) s (t) s (t) s (t) s (t) s (t) s (t) s (t) s (t) s (t) s (t) s (t) s (t) s (t) s (t) s (t) s (t) s (t) s (t) s (t) s (t) s (t) s (t) s (t) s (t) s (t) s (t) s (t) s (t) s (t) s (t) s (t (t) s (t) s (t) s (t) s (t) s (t) s (t) s (t) s (t) s (t) s (t (t) s (t) s (t) s (t) s (t) s (t) s ("}, {"heading": "B. Long Short-Term Memory", "text": "Long Short-Term Memory (LSTM) Networks [14], another variant of RNNs, and their recent revolutionary extensions [10], [27] have so far proven to be the most powerful models for learning long-term time dependencies. They handle past information through additional gates that control how a memory cell is affected by the input signal. Specifically, an input gate allows adding new memory to the cell state, a forge gate resets the memory, and an output gate controls how gates are affected by the current state of the cell in the next step. The basic unit consists of input i, output o, forge f and input modulation gates, as well as a memory cell c (see Figure 3b). Each element is parameterized by appropriate feed-forward (W) and recurring (U) weights and bias vectors."}, {"heading": "C. Convolutional learning of RNNs", "text": "Given the low correlation of individual images with the user identity, we found it very advantageous to make the input layer undulating independently of the model type, thus forcing an earlier fusion of temporal information. To simplify the presentation, we did not explicitly describe the wave formation in the description of the above methods, but it can be included in the input-hidden matrix W.5 input, x (t) x (t + 1) x (t + 2) x (t + 2) V V V V V V V V k = 0 k = 1 h (t 1) h (t + 1) h (t + 1) WWW W UU h (t + 2) h (t 2) k = 2 U o (t 1) o (t + 1) o (t + 2) Fig. 5. Proposed dense clockwork RNN with the same parameters as the original clockwork RNN in Figure 3a."}, {"heading": "V. DENSE CONVOLUTIONAL CLOCKWORK RNNS", "text": "In fact, it is the case that most of us are able to abide by the rules that they have imposed on themselves. (...) In fact, it is the case that they are able to abide by the rules. (...) In fact, it is the case that they are able to abide by the rules. (...) In fact, it is the case that they are able to abide by the rules. (...) In fact, it is the case that they are able to abide by the rules. (...) In fact, it is as if they are able to break the rules. \"(...)"}, {"heading": "VI. DATA COLLECTION", "text": "The data set introduced in this work is part of a more general multimodal data collection conducted by Google ATAP, known as Project Abacus. To facilitate research, we worked with a third-party panel to recruit volunteers and obtain their consent, and provided them with a specialized reader (ROM) for data collection. Volunteers had complete control over their data throughout their collection, as well as the ability to verify and delete it before it was used for research, and volunteers were able to choose based on the fact that all their data was deleted, and the third-party acted as a data buffer between Google ATAP and the volunteers. The data body consisted of 27.62 TB of smartphone sensor signals, including images from a front-facing camera, touch screen, Bluetooth, Wifi, the cell antenna, etc."}, {"heading": "VII. EXPERIMENTAL RESULTS", "text": "In this area, we are in a position to look for a solution."}, {"heading": "B. Large-scale study: Google Abacus dataset", "text": "This year, it has reached the point where it will be able to put itself at the top without being able to put itself at the top."}, {"heading": "VIII. MODEL ADAPTATION FOR A VISUAL CONTEXT", "text": "Finally, we would like to emphasize that the proposed DCWRNN framework can also be applied to other sequence modeling tasks, including the visual context. The model described is not specific to the data type, and there is no particular reason why it cannot be applied to the general human kinematics problem (such as action or gesture detection from motion detection).To support this claim, we have conducted additional tests of the proposed method as part of a visual gesture detection task. We are providing results on the Motion Capture (Mocap) modality of the ChaLearn 2014 Looking at People gesture dataset [28]. This dataset contains approximately 14,000 instances of Italian conversation gestures with the aim of recognizing and locating gestures in continuous, loud recordings. Generally, this corpus includes multimodal data captured with the Kinect and therefore RGB video, depth stream and mocap data. However, only the last channel is used in this experiment session."}, {"heading": "IX. CONCLUSION", "text": "From a modeling perspective, this work has shown that temporary architectures are particularly efficient for learning dynamic functions from a large body of noisy time signals, and that the learned representations can be further integrated into a generative setting. In terms of application, we have confirmed that natural human kinematics conveys necessary information about the person's identity and can therefore be useful for authenticating the user on mobile devices. Results are particularly promising as the system is completely non-intrusive and non-cooperative, i.e. requires no effort from the user side. Non-standard weak biometrics are particularly interesting for providing context in, for example, face recognition or speaker verification scenarios. Further augmentation with data extracted from keyboard and touch patterns, user location, connectivity and application statistics (ongoing work), which are key to creating the first secure framework, non-mobile obtrusive authentication."}, {"heading": "ACKNOWLEDGMENT", "text": "The authors thank their colleagues Elie Khoury, Laurent El Shafey, S\u00e9bastien Marcel and Anupam Das for valuable discussions."}], "references": [{"title": "Bob: a free signal processing and machine learning toolbox for researchers", "author": ["A. Anjos", "L. El Shafey", "R. Wallace", "M. G\u00fcnther", "C. McCool", "S. Marcel"], "venue": "ACMMM,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2012}, {"title": "Score normalization for text-independent speaker verification systems", "author": ["R. Auckenthaler", "M. Carey", "H. Lloyd-Thomas"], "venue": "Digital Signal Processing", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2000}, {"title": "Theano: a CPU and GPU math expression compiler", "author": ["J. Bergstra"], "venue": "In Proceedings of the Python for Scientific Computing Conference (SciPy),", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2010}, {"title": "Siamese Neural Network based Similarity Metric for Inertial Gesture Classification and Rejection", "author": ["S. Berlemont", "G. Lefebvre", "S. Duffner", "C. Garcia"], "venue": "In FG,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2015}, {"title": "Using unlabeled data in a sparse-coding framework for human activity recognition", "author": ["S. Bhattacharya", "P. Nurmi", "N. Hammerla", "T. Plotz"], "venue": "In Pervasive and Mobile Computing,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2014}, {"title": "Silentsense: Silent user identification via touch and movement behavioral biometrics", "author": ["C. Bo", "L. Zhang", "X.-Y. Li", "Q. Huang", "Y. Wang"], "venue": "In MobiCom,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2013}, {"title": "A tutorial on human activity recognition using body-worn inertial sensors", "author": ["A. Bulling", "U. Blanke", "B. Schiele"], "venue": "In ACM Computing Surveys,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2014}, {"title": "Exploring ways to mitigate sensorbased smartphone fingerprinting", "author": ["A. Das", "N. Borisov", "M. Caesar"], "venue": null, "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2015}, {"title": "Unobtrusive User- Authentication on Mobile Phones Using Biometric Gait Recognition", "author": ["M.O. Derawi", "C. Nickel", "P. Bours", "C. Busch"], "venue": "IIH-MSP,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2010}, {"title": "Long-term recurrent convolutional networks for visual recognition and description", "author": ["J. Donahue"], "venue": null, "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2015}, {"title": "3D gesture classification with convolutional neural networks", "author": ["S. Duffner", "S. Berlemont", "G. Lefebre", "C. Garcia"], "venue": "In ICASSP,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2014}, {"title": "Preprocessing techniques for context recognition from accelerometer data", "author": ["D. Figo", "P. Diniz", "D. Ferreira", "J. Cardoso"], "venue": "In Pervasive and Ubiquituous Computing,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2010}, {"title": "Deepspeech: Scaling up end-to-end speech recognition", "author": ["A. Hannun", "C. Case", "J. Casper", "B. Catanzaro", "G. Diamos", "E. Elsen", "R. Prenger", "S. Satheesh", "S. Sengupta", "A. Coates"], "venue": "arXiv preprint arXiv:1412.5567,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2014}, {"title": "Long short-term memory", "author": ["S. Hochreiter", "J. Schmidhuber"], "venue": "Neural Computation,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 1997}, {"title": "Combining modality specific deep neural networks for emotion recognition", "author": ["S.E. Kahou", "C. Pal", "X. Bouthillier", "P. Froumenty", "c. G\u00fcl\u00e7ehre", "R. Memisevic", "P. Vincent", "A. Courville", "Y. Bengio"], "venue": "ICMI,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2013}, {"title": "Large-scale video classification with convolutional neural networks", "author": ["A. Karpathy", "G. Toderici", "S. Shetty", "T. Leung", "R. Sukthankar", "L. Fei- Fei"], "venue": "In CVPR,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2014}, {"title": "Bi-Modal Biometric Authentication on Mobile Phones in Challenging Conditions", "author": ["E. Khoury", "L. El Shafey", "C. McCool", "M. Gunther", "S. Marcel"], "venue": "In Image and Vision Computing,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2014}, {"title": "A clockwork rnn", "author": ["J. Koutnik", "K. Greff", "F. Gomez", "J. Schmidhuber"], "venue": null, "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2014}, {"title": "BLSTM-RNN Based 3D Gesture Classification", "author": ["G. Lefebvre", "S. Berlemont", "F. Mamalet", "C. Garcia"], "venue": "In ICANN,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2013}, {"title": "The prehensile movements of the human hand", "author": ["J. Napier"], "venue": "In Journal of Bone and Joint Surgery,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 1956}, {"title": "Moddrop: adaptive multi-modal gesture recognition", "author": ["N. Neverova", "C. Wolf", "G.W. Taylor", "F. Nebout"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2015}, {"title": "Multimodal deep learning", "author": ["J. Ngiam", "A. Khosla", "M. Kin", "J. Nam", "H. Lee", "A.Y. Ng"], "venue": "In ICML,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2011}, {"title": "Benchmarking the performance of svms and hmms for accelerometer-based biometric gait recognition", "author": ["C. Nickel", "H. Brandt", "C. Busch"], "venue": "ISSPIT,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2011}, {"title": "Feature learning for activity  10 recognition in ubiquitous computing", "author": ["T. Plotz", "N.Y. Hammerla", "P. Olivier"], "venue": "In 22nd International Joint Conference on Artificial Intelligence,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2011}, {"title": "Speaker verification using adapted gaussian mixture models", "author": ["D.A. Reynolds", "T.F. Quatieri", "R.B. Dunn"], "venue": "Digital Signal Processing", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2000}, {"title": "Convolutional, long short-term memory, fully connected deep neural networks", "author": ["T.N. Sainath", "O. Vinyals", "A. Senior", "H. Sak"], "venue": null, "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2015}, {"title": "ChaLearn Looking at People Challenge 2014: Dataset and Results", "author": ["S.Escalera", "X.Bar\u00f3", "J.Gonz\u00e0lez", "M.Bautista", "M.Madadi", "M.Reyes", "V. Ponce", "H.Escalante", "J.Shotton", "I.Guyon"], "venue": "ECCVW,", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2014}, {"title": "HMOG: A New Biometric Modality for Continuous Authentication of Smartphone Users", "author": ["Z. Sitova", "J. Sedenka", "Q. Yang", "G. Peng", "G. Zhou", "P. Gasti", "K. Balagani"], "venue": null, "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2015}, {"title": "Multimodal learning with Deep Boltzmann Machines", "author": ["N. Srivastava", "R. Salakhutdinov"], "venue": "In NIPS,", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2013}, {"title": "Two distributed-state models for generating high-dimensional time series", "author": ["G. Taylor", "G. Hinton", "S. Roweis"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2011}, {"title": "Unobtrusive multimodal biometrics for ensuring privacy and information security with personal devices", "author": ["E. Vildjiounaite"], "venue": "In Pervasive Computing,", "citeRegEx": "32", "shortCiteRegEx": "32", "year": 2006}, {"title": "A multimodal data set for evaluating continuous authentication performance in smartphones", "author": ["Q. Yang", "G. Peng", "D.T. Nguyen", "X. Qi", "G. Zhou", "Z. Sitova", "P. Gasti", "K.S. Balagani"], "venue": "In SenSys", "citeRegEx": "33", "shortCiteRegEx": "33", "year": 2014}], "referenceMentions": [{"referenceID": 27, "context": "In recent years, researchers in different fields have been working on creating fast and secure authentication alternatives that would make it possible to remove this burden from the user [29], [6].", "startOffset": 187, "endOffset": 191}, {"referenceID": 5, "context": "In recent years, researchers in different fields have been working on creating fast and secure authentication alternatives that would make it possible to remove this burden from the user [29], [6].", "startOffset": 193, "endOffset": 196}, {"referenceID": 19, "context": "those in which an object is seized and held, partly or wholly, by the hand [20]) collected by 1,500 volunteers over several months of daily use (Fig.", "startOffset": 75, "endOffset": 79}, {"referenceID": 17, "context": "However, apart from the application itself, the main contribution of this work is in developing a new shift-invariant temporal model which fixes a deficiency of the recently proposed Clockwork recurrent neural networks [18] yet retains their ability to explicitly model multiple temporal scales.", "startOffset": 219, "endOffset": 223}, {"referenceID": 22, "context": "A detailed overview and benchmarking of existing state-of-the-art is provided in [23].", "startOffset": 81, "endOffset": 85}, {"referenceID": 8, "context": "[9], for example, used a smartphone attached to the human body to extract information about walking cycles, achieving 20.", "startOffset": 0, "endOffset": 3}, {"referenceID": 11, "context": "In [12] and [7], exhaustive ar X iv :1 51 1.", "startOffset": 3, "endOffset": 7}, {"referenceID": 6, "context": "In [12] and [7], exhaustive ar X iv :1 51 1.", "startOffset": 12, "endOffset": 15}, {"referenceID": 23, "context": "Perhaps most relevant to this study is [25], the first to report the effectiveness of RBM-based feature learning from accelerometer data, and [5], which proposed a data-adaptive sparse coding framework.", "startOffset": 39, "endOffset": 43}, {"referenceID": 4, "context": "Perhaps most relevant to this study is [25], the first to report the effectiveness of RBM-based feature learning from accelerometer data, and [5], which proposed a data-adaptive sparse coding framework.", "startOffset": 142, "endOffset": 145}, {"referenceID": 10, "context": "Convolutional networks have been explored in the context of gesture and activity recognition [11], [34].", "startOffset": 93, "endOffset": 97}, {"referenceID": 18, "context": "[19] applied a bidirectional LSTM network to a problem of 14-class gesture classification, while Berlemont et al.", "startOffset": 0, "endOffset": 4}, {"referenceID": 3, "context": "[4] proposed a fully-connected Siamese network for the same task.", "startOffset": 0, "endOffset": 3}, {"referenceID": 16, "context": "A combination of face recognition and speech [17], and of gait and voice [32] have been proposed in this context.", "startOffset": 45, "endOffset": 49}, {"referenceID": 30, "context": "A combination of face recognition and speech [17], and of gait and voice [32] have been proposed in this context.", "startOffset": 73, "endOffset": 77}, {"referenceID": 29, "context": "Deep learning techniques, which achieved early success modeling sequential data such as motion capture [31] and video [16] have shown promise in multi-modal feature learning [22], [30], [15], [21].", "startOffset": 103, "endOffset": 107}, {"referenceID": 15, "context": "Deep learning techniques, which achieved early success modeling sequential data such as motion capture [31] and video [16] have shown promise in multi-modal feature learning [22], [30], [15], [21].", "startOffset": 118, "endOffset": 122}, {"referenceID": 21, "context": "Deep learning techniques, which achieved early success modeling sequential data such as motion capture [31] and video [16] have shown promise in multi-modal feature learning [22], [30], [15], [21].", "startOffset": 174, "endOffset": 178}, {"referenceID": 28, "context": "Deep learning techniques, which achieved early success modeling sequential data such as motion capture [31] and video [16] have shown promise in multi-modal feature learning [22], [30], [15], [21].", "startOffset": 180, "endOffset": 184}, {"referenceID": 14, "context": "Deep learning techniques, which achieved early success modeling sequential data such as motion capture [31] and video [16] have shown promise in multi-modal feature learning [22], [30], [15], [21].", "startOffset": 186, "endOffset": 190}, {"referenceID": 20, "context": "Deep learning techniques, which achieved early success modeling sequential data such as motion capture [31] and video [16] have shown promise in multi-modal feature learning [22], [30], [15], [21].", "startOffset": 192, "endOffset": 196}, {"referenceID": 7, "context": "In a recent study [8], it was shown that under lab conditions a particular device could be identified by a response of its motion sensors to a given signal.", "startOffset": 18, "endOffset": 21}, {"referenceID": 7, "context": "Formally, the measured output of both the accelerometer and gyroscope can be expressed as follows [8]:", "startOffset": 98, "endOffset": 101}, {"referenceID": 7, "context": "Following [8], the noise vector is obtained by drawing a 12dimensional (3 offset and 3 gain coefficients per sensor) obfuscation vector from a uniform distribution \u03bc \u223c U12[0.", "startOffset": 10, "endOffset": 13}, {"referenceID": 24, "context": "Along the lines of [26], maximum a posteriori (MAP) adaptation of mean vectors for a given user is performed.", "startOffset": 19, "endOffset": 23}, {"referenceID": 1, "context": "As a final step, zt-score normalization [2] is performed to compensate for inter-session and inter-person variations and reduce the overlap between the distribution of scores from authentic users and impostors.", "startOffset": 40, "endOffset": 43}, {"referenceID": 20, "context": "The first two conditions are known to contradict each other as performance of a standalone feature typically grows with integration time [21].", "startOffset": 137, "endOffset": 141}, {"referenceID": 12, "context": "The former model, popular in speech recognition [13], involves convolutional learning of integrated temporal statistics from short and long sequences of data (referred to as \u201cshort-term\u201d and \u201clong-term\u201d convnets).", "startOffset": 48, "endOffset": 52}, {"referenceID": 17, "context": "Then, Section V introduces a new shift-invariant model based on modified Clockwork RNNs [18].", "startOffset": 88, "endOffset": 92}, {"referenceID": 13, "context": "Temporal models: (a) a basic recurrent unit; (b) an LSTM unit [14]; (c) Clockwork RNN [18] with 3 bands and a base of 2; Increasing k indicates lower operating frequency.", "startOffset": 62, "endOffset": 66}, {"referenceID": 17, "context": "Temporal models: (a) a basic recurrent unit; (b) an LSTM unit [14]; (c) Clockwork RNN [18] with 3 bands and a base of 2; Increasing k indicates lower operating frequency.", "startOffset": 86, "endOffset": 90}, {"referenceID": 17, "context": "The recently proposed Clockwork RNN (CWRNN) [18] operates at several temporal scales which are incorporated in a single network and trained jointly.", "startOffset": 44, "endOffset": 48}, {"referenceID": 17, "context": "4, inspired from [18].", "startOffset": 17, "endOffset": 21}, {"referenceID": 17, "context": "Updates made by the Clockwork RNN [18] (top) and our proposed Dense CWRNN (bottom).", "startOffset": 34, "endOffset": 38}, {"referenceID": 13, "context": "Long Short-Term Memory (LSTM) networks [14], another variant of RNNs, and their recent convolutional extensions [10], [27] have proven to be, so far, the best performing models for learning long-term temporal dependencies.", "startOffset": 39, "endOffset": 43}, {"referenceID": 9, "context": "Long Short-Term Memory (LSTM) networks [14], another variant of RNNs, and their recent convolutional extensions [10], [27] have proven to be, so far, the best performing models for learning long-term temporal dependencies.", "startOffset": 112, "endOffset": 116}, {"referenceID": 25, "context": "Long Short-Term Memory (LSTM) networks [14], another variant of RNNs, and their recent convolutional extensions [10], [27] have proven to be, so far, the best performing models for learning long-term temporal dependencies.", "startOffset": 118, "endOffset": 122}, {"referenceID": 17, "context": "To be consistent, we employ the same matrix form as in the original CWRNN paper [18]) and show components, which are inactive at time t, in dark gray.", "startOffset": 80, "endOffset": 84}, {"referenceID": 31, "context": "To explore the nature of inertial sensor signals, we performed a preliminary analysis on the HMOG dataset [33] containing similar data, but collected in constrained settings as a part of a lab study.", "startOffset": 106, "endOffset": 110}, {"referenceID": 2, "context": "All deep nets were implemented with Theano [3] and trained on 8 Nvidia Tesla K80 GPUs.", "startOffset": 43, "endOffset": 46}, {"referenceID": 0, "context": "UBM-GMMs were trained with the Bob toolbox [1] and did not employ GPUs.", "startOffset": 43, "endOffset": 46}, {"referenceID": 20, "context": "Single network [21] 0.", "startOffset": 15, "endOffset": 19}, {"referenceID": 20, "context": "Ensemble [21] 0.", "startOffset": 9, "endOffset": 13}, {"referenceID": 26, "context": "Namely, we provide results on the motion capture (mocap) modality of the ChaLearn 2014 Looking at People gesture dataset [28].", "startOffset": 121, "endOffset": 125}, {"referenceID": 20, "context": "In the spirit of [21] (the method ranked firstst in the ECCV 2014 ChaLearn competition), we use the same skeleton descriptor as input.", "startOffset": 17, "endOffset": 21}, {"referenceID": 20, "context": "The final aggregation and localization step correspond to [21].", "startOffset": 58, "endOffset": 62}], "year": 2016, "abstractText": "We present a large-scale study exploring the capability of temporal deep neural networks to interpret natural human kinematics and introduce the first method for active biometric authentication with mobile inertial sensors. At Google, we have created a first-of-its-kind dataset of human movements, passively collected by 1500 volunteers using their smartphones daily over several months. We (1) compare several neural architectures for efficient learning of temporal multi-modal data representations, (2) propose an optimized shift-invariant dense convolutional mechanism (DCWRNN), and (3) incorporate the discriminatively-trained dynamic features in a probabilistic generative framework taking into account temporal characteristics. Our results demonstrate that human kinematics convey important information about user identity and can serve as a valuable component of multi-modal authentication systems. Finally, we demonstrate that the proposed model can be successfully applied also in a visual context.", "creator": "LaTeX with hyperref package"}}}