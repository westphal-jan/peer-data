{"id": "1511.03958", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "12-Nov-2015", "title": "Software Agents with Concerns of their Own", "abstract": "We claim that it is possible to have artificial software agents for which their actions and the world they inhabit have first-person or intrinsic meanings. The first-person or intrinsic meaning of an entity to a system is defined as its relation with the system's goals and capabilities, given the properties of the environment in which it operates. Therefore, for a system to develop first-person meanings, it must see itself as a goal-directed actor, facing limitations and opportunities dictated by its own capabilities, and by the properties of the environment. The first part of the paper discusses this claim in the context of arguments against and proposals addressing the development of computer programs with first-person meanings. A set of definitions is also presented, most importantly the concepts of cold and phenomenal first-person meanings. The second part of the paper presents preliminary proposals and achievements, resulting of actual software implementations, within a research approach that aims to develop software agents that intrinsically understand their actions and what happens to them. As a result, an agent with no a priori notion of its goals and capabilities, and of the properties of its environment acquires all these notions by observing itself in action. The cold first-person meanings of the agent's actions and of what happens to it are defined using these acquired notions. Although not solving the full problem of first-person meanings, the proposed approach and preliminary results allow us some confidence to address the problems yet to be considered, in particular the phenomenal aspect of first-person meanings.", "histories": [["v1", "Thu, 12 Nov 2015 16:39:21 GMT  (349kb)", "http://arxiv.org/abs/1511.03958v1", null]], "reviews": [], "SUBJECTS": "cs.AI", "authors": ["luis botelho", "luis nunes", "ricardo ribeiro", "rui j lopes"], "accepted": false, "id": "1511.03958"}, "pdf": {"name": "1511.03958.pdf", "metadata": {"source": "META", "title": "Software Agents with Concerns of their Own", "authors": ["Lu\u00eds Botelho", "Lu\u00eds Nunes", "Ricardo Ribeiro", "Rui J. Lopes"], "emails": ["Luis.Botelho@iscte.pt"], "sections": [{"heading": null, "text": "In fact, it is such that most of them will be able to move into a different world, in which they are able to live, in which they are able to live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they, in which they live, in which they, in which they, in which they live, in which they, in which they live, in which they, in which they, in which they live, in which they live, in which they live, in which they live, in which they live."}, {"heading": "Background Concepts and Main Innovative Contribution", "text": "In fact, it is not a matter of self-representation, but rather of self-representation, in which it is a matter of putting the perception of one's own perceptions and perceptions in the foreground. (...) It is not that perceptions and perceptions of perceptions are placed in the centre. (...) It is not that the perceptions of perceptions and perceptions of perceptions are placed in the centre. (...) It is not that the perceptions of perceptions of perceptions and perceptions of perceptions and perceptions of perceptions of perceptions and perceptions of perceptions of perceptions are placed in the centre. (...) It is that the perceptions of perceptions and perceptions of perceptions are placed in the centre. (...) The perceptions of perceptions and perceptions of perceptions are placed in the centre. (...) It is that the perceptions of perceptions and perceptions of perceptions are placed in the centre."}, {"heading": "Brief Example", "text": "We will briefly present an example that illustrates our point of view and concepts. A robot has been programmed to weld the outer skin of the car doors and then assemble the door to the car. It has no objectives because it only follows the instructions of the robot contained in its control program. It also has no idea of its capabilities and the characteristics of the environment. With one of the proposed algorithms, the robot starts to see itself as if it would reach a situation in which it assembles the doors to the car and welds the external panels to the doors. Additionally, it believes that it has these objectives, it becomes aware of its capabilities, for example by assembling a door to the car by using the welding brew and it to weld the door panels. The robot also discovers the characteristics of the environment as it welds before closing the door."}, {"heading": "Research Assumptions", "text": "In fact, most of them are able to survive on their own."}, {"heading": "The sub problem of symbol reference", "text": "Harnad (1990), Cangelosi, Greco, and Harnad (2002) base the symbols of their systems on the sensors of the system. Mugan and Kuipers (2012) base their robots \"plans on the motor commands of their actuators. Vogt (2002), Steels (2003, 2008), and Machado and Botelho (2006) incorporate their systems into specially designed social interactions, through which they build from scratch a common lexicon of symbols, each of which is associated with the object (and sometimes the category) they represent. Anderson and Perlis (2005) claim that computer symbols have or acquire representational powers for the computer program when they play consistent behavioral guiding roles for the program. In this case, the meaning of each symbol is rooted in its behavioral role, that is, in its causal power."}, {"heading": "The necessity of a motivation system", "text": "For Dreyfus (1972), systems can only develop ego meanings if they have bodies with body-related needs that lead to motivated behavior. Steels (1996) and Birk (1997) have developed robots that successfully learn to stay alive in their environment by relying only on mechanisms aimed at maintaining internal variables within certain values that can be considered a form of motivated behavior. Savage (2003) discusses a series of processes by which complex motivated behavior can be developed in artificial agents, either through gradual improvement through the agent's experience or through historical evolution. We fully agree that a motivational system is needed for the first-person meanings. Nevertheless, we argue that the systems presented act as if they know what is important to them, they do not really have an idea of the meaning of objects, events, and situations. In order to develop ego meanings, we must be aware of such environments."}, {"heading": "Understanding what happens", "text": "In fact, most of them will be able to move to another world, where they will be able to move to another world, where they will be able to move to another world, where they will be able to move, where they will be able to move, where they will be able to move."}, {"heading": "Proof of Concept and Discussion", "text": "This year is the highest in the history of the country."}], "references": [{"title": "The roots of self-awareness", "author": ["M.L. Anderson", "D.R. Perlis"], "venue": "Phenomenology and the Cognitive Sciences,", "citeRegEx": "Anderson and Perlis,? \\Q2005\\E", "shortCiteRegEx": "Anderson and Perlis", "year": 2005}, {"title": "Robot Learning and Self-Sufficiency: What the energy-level can tell us about a robot's performance", "author": ["A. Birk"], "venue": "Learning Robots: Proceedings of the Sixth European Workshop on Learning Robots (EWLR-6), Lecture Notes in Computer Science,", "citeRegEx": "Birk,? \\Q1997\\E", "shortCiteRegEx": "Birk", "year": 1997}, {"title": "Intelligence without representation", "author": [], "venue": "Artificial Intelligence,", "citeRegEx": "Brooks,? \\Q1991\\E", "shortCiteRegEx": "Brooks", "year": 1991}, {"title": "Symbol Grounding and the Symbolic Theft Hypothesis", "author": ["A. Cangelosi", "A. Greco", "S. Harnad"], "venue": "Simulating the Evolution of Language", "citeRegEx": "Cangelosi et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Cangelosi et al\\.", "year": 2002}, {"title": "Facing Up to the Problem of Consciousness", "author": ["D.J. Chalmers"], "venue": "Journal of Consciousness", "citeRegEx": "48", "shortCiteRegEx": "48", "year": 1995}, {"title": "What Computers Can't Do", "author": ["New York: Harcourt Brace"], "venue": null, "citeRegEx": "Brace,? \\Q1972\\E", "shortCiteRegEx": "Brace", "year": 1972}, {"title": "The Centrifugal Development of Artificial Agents", "author": ["A.S. Esteves", "L.B. Botelho"], "venue": null, "citeRegEx": "Esteves and Botelho,? \\Q2007\\E", "shortCiteRegEx": "Esteves and Botelho", "year": 2007}, {"title": "Enactive artificial intelligence: Investigating the systemic", "author": ["T. Froese", "T. Ziemke"], "venue": "New Ideas in Psychology", "citeRegEx": "Froese and Ziemke,? \\Q2009\\E", "shortCiteRegEx": "Froese and Ziemke", "year": 2009}, {"title": "\u00dcber formal unentscheidbare S\u00e4tze der Principia Mathematica und verwandter", "author": ["Gallup", "G.G. Jr."], "venue": "organization of life and mind. Artificial Intelligence,", "citeRegEx": "Gallup and Jr.,? \\Q1970\\E", "shortCiteRegEx": "Gallup and Jr.", "year": 1970}, {"title": "Using probabilistic reasoning over time to self-recognize", "author": ["K. Gold", "B. Scassellati"], "venue": "Robotics and Autonomous Systems", "citeRegEx": "Gold and Scassellati,? \\Q2009\\E", "shortCiteRegEx": "Gold and Scassellati", "year": 2009}, {"title": "Orchestrated reduction of quantum coherence in brain microtubules: A model for consciousness", "author": ["S. Hameroff", "R. Penrose"], "venue": "Mathematics and Computers in Simulation,", "citeRegEx": "Hameroff and Penrose,? \\Q1996\\E", "shortCiteRegEx": "Hameroff and Penrose", "year": 1996}, {"title": "Consciousness in the universe. A review of the\u2018Orch OR\u2019theory", "author": ["S. Hameroff", "R. Penrose"], "venue": "Physics of Life", "citeRegEx": "Hameroff and Penrose,? \\Q2014\\E", "shortCiteRegEx": "Hameroff and Penrose", "year": 2014}, {"title": "The Symbol Grounding Problem", "author": ["S. Harnad"], "venue": "Physica D,", "citeRegEx": "Harnad,? \\Q1990\\E", "shortCiteRegEx": "Harnad", "year": 1990}, {"title": "On G\u00f6del\u2019s Theorem and Mechanism: Inconsistency or Unsoundness is Unavoidable in any Attempt to \u2018Out-G\u00f6del\u2019 the Mechanist", "author": ["S. Krajewski"], "venue": "Fundamenta Informaticae", "citeRegEx": "Krajewski,? \\Q2007\\E", "shortCiteRegEx": "Krajewski", "year": 2007}, {"title": "Why Godel's Theorem Cannot Refute Computationalism", "author": ["G. LaForte", "P.J. Hayes", "K.M. Ford"], "venue": "Artificial Intelligence", "citeRegEx": "LaForte et al\\.,? \\Q1998\\E", "shortCiteRegEx": "LaForte et al\\.", "year": 1998}, {"title": "Software agents that learn through observation", "author": ["J. Machado", "L.M. Botelho"], "venue": "In Proceedings of the International Joint Conference on Autonomous Agents and MultiAgent Systems (AAMAS", "citeRegEx": "Machado and Botelho,? \\Q2006\\E", "shortCiteRegEx": "Machado and Botelho", "year": 2006}, {"title": "Autonomous Learning of High-Level States and Actions in Continuous Environments", "author": ["J. Mugan", "B. Kuipers"], "venue": "IEEE Transactions on Autonomous Mental Development", "citeRegEx": "Mugan and Kuipers,? \\Q2012\\E", "shortCiteRegEx": "Mugan and Kuipers", "year": 2012}, {"title": "The Symbol Grounding Problem Has Been Solved. So What's Next", "author": ["L. Steels"], "venue": "In M. de Vega (Ed.) Symbols and Embodiment: Debates on Meaning and Cognition", "citeRegEx": "Steels,? \\Q2008\\E", "shortCiteRegEx": "Steels", "year": 2008}, {"title": "Reply to Commentaries", "author": ["E. Thompson"], "venue": "Journal of Consciousness Studies,", "citeRegEx": "Thompson,? \\Q2011\\E", "shortCiteRegEx": "Thompson", "year": 2011}, {"title": "At the source of time: Valence and the constitutional dynamics of affect", "author": ["F.J. Varela", "N. Depraz"], "venue": "Journal of Consciousness Studies,", "citeRegEx": "Varela and Depraz,? \\Q2005\\E", "shortCiteRegEx": "Varela and Depraz", "year": 2005}, {"title": "The physical symbol grounding problem", "author": ["P. Vogt"], "venue": "Cognitive Systems Research,", "citeRegEx": "Vogt,? \\Q2002\\E", "shortCiteRegEx": "Vogt", "year": 2002}, {"title": "Rethinking Grounding", "author": ["T. Ziemke"], "venue": null, "citeRegEx": "Ziemke,? \\Q1999\\E", "shortCiteRegEx": "Ziemke", "year": 1999}], "referenceMentions": [{"referenceID": 12, "context": "Introduction In a debate that came to be better known as the symbol grounding problem (Harnad, 1990), several scientists have expressed their opinions that artificial intelligence would never lead to actual intelligent systems (e.", "startOffset": 86, "endOffset": 100}, {"referenceID": 21, "context": "Introduction In a debate that came to be better known as the symbol grounding problem (Harnad, 1990), several scientists have expressed their opinions that artificial intelligence would never lead to actual intelligent systems (e.g., Searle, 1980; Penrose, 1989; Ziemke, 1999; and, Froese & Ziemke, 2009).", "startOffset": 227, "endOffset": 304}, {"referenceID": 12, "context": "This is a difference regarding other work addressing first-person meanings, for example the work by Harnad (1990), Cangelosi, Greco, and Harnad (2002), Vogt (2002), Machado and Botelho (2006), and Steels (2003, 2008), which focuses on the symbol grounding problem, namely the problem of directly or indirectly grounding each symbol of an artificial system on the object, situation or event referred to by the symbol.", "startOffset": 100, "endOffset": 114}, {"referenceID": 12, "context": "This is a difference regarding other work addressing first-person meanings, for example the work by Harnad (1990), Cangelosi, Greco, and Harnad (2002), Vogt (2002), Machado and Botelho (2006), and Steels (2003, 2008), which focuses on the symbol grounding problem, namely the problem of directly or indirectly grounding each symbol of an artificial system on the object, situation or event referred to by the symbol.", "startOffset": 100, "endOffset": 151}, {"referenceID": 12, "context": "This is a difference regarding other work addressing first-person meanings, for example the work by Harnad (1990), Cangelosi, Greco, and Harnad (2002), Vogt (2002), Machado and Botelho (2006), and Steels (2003, 2008), which focuses on the symbol grounding problem, namely the problem of directly or indirectly grounding each symbol of an artificial system on the object, situation or event referred to by the symbol.", "startOffset": 100, "endOffset": 164}, {"referenceID": 12, "context": "This is a difference regarding other work addressing first-person meanings, for example the work by Harnad (1990), Cangelosi, Greco, and Harnad (2002), Vogt (2002), Machado and Botelho (2006), and Steels (2003, 2008), which focuses on the symbol grounding problem, namely the problem of directly or indirectly grounding each symbol of an artificial system on the object, situation or event referred to by the symbol.", "startOffset": 100, "endOffset": 192}, {"referenceID": 9, "context": "Gold and Scassellati (2009) developed an unsupervised learning algorithm that enables a robot to reliably distinguish its own moving parts from those of others, just by looking at a mirror.", "startOffset": 0, "endOffset": 28}, {"referenceID": 13, "context": "LaForte, Hayes, and Ford (1998), and Krajewski (2007) show that Penrose\u2019s conclusion that the human mind involves non-computable processes is wrong.", "startOffset": 37, "endOffset": 54}, {"referenceID": 7, "context": "Ziemke (1999), Froese and Ziemke (2009), and Sharkey and Ziemke (2001) claim that any externally developed mechanisms aimed at creating meanings for a computer program will always produce arbitrary results for the computer program itself exactly because they are externally created.", "startOffset": 15, "endOffset": 40}, {"referenceID": 7, "context": "Ziemke (1999), Froese and Ziemke (2009), and Sharkey and Ziemke (2001) claim that any externally developed mechanisms aimed at creating meanings for a computer program will always produce arbitrary results for the computer program itself exactly because they are externally created.", "startOffset": 15, "endOffset": 71}, {"referenceID": 7, "context": "Thompson and Stapleton (2009), and Froese and Ziemke (2009) claim that the life of even the simplest organism can be described as a sense-making activity.", "startOffset": 35, "endOffset": 60}, {"referenceID": 7, "context": "However, Froese and Ziemke (2009) believe that it is impossible to have systems with first-person meanings that are not rooted on the lower-level sense-making mechanisms resulting of their metabolism.", "startOffset": 9, "endOffset": 34}, {"referenceID": 15, "context": "Vogt (2002), Steels (2003, 2008), and Machado and Botelho (2006) involve their systems in especially designed social interactions through which they develop, from scratch, a shared lexicon of symbols, each of which is connected to the object (and sometimes the category) they represent.", "startOffset": 38, "endOffset": 65}, {"referenceID": 2, "context": "For Brooks (1991), symbols and symbolic processing are not required for intelligent behavior therefore its robots avoid the symbol grounding problem.", "startOffset": 4, "endOffset": 18}, {"referenceID": 2, "context": "For Brooks (1991), symbols and symbolic processing are not required for intelligent behavior therefore its robots avoid the symbol grounding problem. In our point of view, if the robot is to determine the importance of objects, events, and situations, it must see all it does and all that happens to it in the light of its goals. Besides, as Vogt (2002) argues, several high-level mental activities, as language, require symbols (but see Cuffari, di Paolo, and de Jaegher, 2014 for a contrasting opinion).", "startOffset": 4, "endOffset": 354}, {"referenceID": 16, "context": "Steels (1996) and Birk (1997) developed robots that successfully learn to stay alive in their environment relying only on mechanisms aimed at preserving internal variables within specified values, which may be seen as a form of motivated behavior.", "startOffset": 0, "endOffset": 14}, {"referenceID": 1, "context": "Steels (1996) and Birk (1997) developed robots that successfully learn to stay alive in their environment relying only on mechanisms aimed at preserving internal variables within specified values, which may be seen as a form of motivated behavior.", "startOffset": 18, "endOffset": 30}, {"referenceID": 1, "context": "Steels (1996) and Birk (1997) developed robots that successfully learn to stay alive in their environment relying only on mechanisms aimed at preserving internal variables within specified values, which may be seen as a form of motivated behavior. Savage (2003) discusses a set of processes by which complex motivated behavior can be developed in artificial agents, either by gradual improvement through the agent\u2019s experience or by historical evolution.", "startOffset": 18, "endOffset": 262}, {"referenceID": 7, "context": "As already stated before, Froese and Ziemke (2009), among others, contend that it is impossible to build computer programs with first-person meanings because the computer program would have to create and maintain itself and the whole network of its parts and processes from within.", "startOffset": 26, "endOffset": 51}, {"referenceID": 7, "context": "As already stated before, Froese and Ziemke (2009), among others, contend that it is impossible to build computer programs with first-person meanings because the computer program would have to create and maintain itself and the whole network of its parts and processes from within. As a way to circumvent this problem, Froese and Ziemke (2009) present the quite radical idea that instead of trying to build programs with first-person meanings, scientists should focus on the definition and creation of environments with such dynamics that enable the emergence of computer programs with first-person meanings.", "startOffset": 26, "endOffset": 344}, {"referenceID": 19, "context": "Varela and Depraz (2005), and Colombetti (2014) also recognize the primordial importance of the feeling body to consciousness.", "startOffset": 0, "endOffset": 25}, {"referenceID": 19, "context": "Varela and Depraz (2005), and Colombetti (2014) also recognize the primordial importance of the feeling body to consciousness.", "startOffset": 0, "endOffset": 48}], "year": 2015, "abstractText": null, "creator": "Word"}}}