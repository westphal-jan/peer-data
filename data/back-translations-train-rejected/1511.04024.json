{"id": "1511.04024", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "12-Nov-2015", "title": "Multimodal Skip-gram Using Convolutional Pseudowords", "abstract": "This work studies the representational mapping across multimodal data such that given a piece of the raw data in one modality the corresponding semantic description in terms of the raw data in another modality is immediately obtained. Such a representational mapping can be found in a wide spectrum of real-world applications including image/video retrieval, object recognition, action/behavior recognition, and event understand and prediction. To that end, we introduce a simplified training objective for learning multimodal embeddings using the skip-gram architecture by introducing convolutional 'pseudowords:' embeddings composed of the additive combination of distributed word representations and image features from convolutional neural networks projected into the multimodal space. We present some preliminary results of the representational properties of these embeddings on various word similarity benchmarks.", "histories": [["v1", "Thu, 12 Nov 2015 19:32:08 GMT  (125kb)", "https://arxiv.org/abs/1511.04024v1", null], ["v2", "Sun, 29 Nov 2015 19:09:38 GMT  (203kb,D)", "http://arxiv.org/abs/1511.04024v2", null]], "reviews": [], "SUBJECTS": "cs.CL cs.CV", "authors": ["zachary seymour", "yingming li", "zhongfei zhang"], "accepted": false, "id": "1511.04024"}, "pdf": {"name": "1511.04024.pdf", "metadata": {"source": "CRF", "title": "MULTIMODAL SKIP-GRAM USING CONVOLUTIONAL PSEUDOWORDS", "authors": ["Zachary Seymour", "Yingming Li"], "emails": ["zseymou1@binghamton.edu", "liymn@zju.edu.cn", "zhongfei@cs.binghamton.edu"], "sections": [{"heading": "1 INTRODUCTION", "text": "Distributed representations of multimodal embedding (Feng & Lapata, 2010) have recently received increasing attention in machine learning literature, and the techniques developed have found a wide range of applications in the real world. These types of vector representations are particularly desirable for the way in which they better model the grounding of perceptual and semantic concepts in the human vocabulary (Lazaridou et al., 2015; Glenberg & Robertson, 2000; Hill & Korhonen, 2014).As such, there is an evolution towards so-called multimodal semantic models (Silberer & Lapata, 2014; Lazaridou et al al al al., 2015; Kiros et al. 2014; Frome et al., 2014), which describe the textual occurrence of semantic models of semantic models semantic models (Silberer & Lapata, 2014; Lazaridou et al., 2014)."}, {"heading": "2 RELATED WORK", "text": "As explained in Lazaridou et al. (2015), the majority of this literature focuses on constructing textual and visual representations independently of each other and then combining them under certain metrics, but the image vectors used here are constructed using the method of \"bag-of-visual-words.\" In Kiela & Bottou (2014), the authors use a more complex approach to concatenating text and image vectors by extracting visual features using state-of-the-art revolutionary neural networks and the \"Skip-gram architecture\" for the text. Similarly, Frome et al. (2013) also use visual mapping methods as visual shaping, combining the two modalities with a natural similarity."}, {"heading": "3 ARCHITECTURE", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "3.1 SKIP-GRAM FOR WORD REPRESENTATIONS", "text": "This model is primarily derived from the Skip-gram model introduced by Mikolov et al. (2013c). Skip-gram learns representations of words predicting the context of a target word. It maximizes maximizes1T \u2211 t = 1 \u2211 \u2212 c \u2264 j \u2264 c, j 6 = 0 log p (wt + j | wt) (1), where w1, w2,.., wT are words in the training set and c are the window size around the target word. The probability p (wt + j | wt) is specified by softmax, that is: p (wt + j | wt) = e u \u2032 wt + j Tuwt \u0445 W w \u2032 = 1 e u \u2032 w \u2032 Tuwt (2), where uw and u \u2032 w are the context vector and target vector representations induced for the word w or W, and W specifies the vocabulary size. In order to maximize this calculation, a universally constructed vocabulary is then replaced by the tree and the hoof the vocabulary."}, {"heading": "3.2 SKIP-GRAM WITH MULTIMODAL PSEUDOWORDS", "text": "To ground the word representations in a visual context, we introduce a means of replacing a word in the corpus. Although we assume that a single pseudo-word is not necessarily used, the pseudo-word vector for a given word w, denoted zw, is given by zw = uw + Mvw (3), where uw is the multimodal word representation of w to be induced, vw is the visual vector for the concept represented by w, and M is the mapping induced between the visual and multimodal space. (The sources of the textual and visual data are explained below.) Where no visual features are available for a given word, vw is set to 0. Thus, the objective function in (1) remains the same, while each word vector in the context window of the current word in (2) is replaced by its corresponding pseudo-word. In this way, each pseudo-word is predicted in its target word, each target word is trained on its accuracy in its target word."}, {"heading": "4 EXPERIMENTS", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.1 EXPERIMENTAL DATA", "text": "For our text corpus, we use, according to the existing literature, a pre-processed dump from Wikipedia3 containing approximately 800,000 characters. For the visual data, we use the image data from ILSVRC 2012 (Russakovsky et al., 2015) and the corresponding hierarchy from Words3 (Miller, 1995) to visualize a word when the word or one of its hyponyms has an entry in Imagenet and occurs more than 500 times in the text corpus. This results in approximately 5,100 \"visual\" words. To construct the vectors for the visual representations, we follow a similar experimental setup as Lazaridou et al. (2015). In each of the cases described above - centric and hyperspherical - we randomly stomp 100 images from the corresponding Imagenet synsets for each visual word and use a pre-trained neural network as described in Krizhevsky et al. (2012) via the toolekit jaffe, each containing a visual vector 2014."}, {"heading": "4.2 APPROXIMATING HUMAN SIMILARITY JUDGMENTS", "text": "Word Similarity Benchmarks To compare our technique with existing literature, we evaluate our embedding using four common benchmarks that capture different aspects of word meaning: MEN (Bruni et al., 2014), Simlex-999 (Hill et al., 2014), SemSim (Silberer & Lapata, 2014), VisSim (Silberer & Lapata, 2014). MEN is designed to capture generic word pairs of \"kinship.\" Simlex999 and SemSim measure terms of semantic similarity, and VisSim ranks the same words as SemSim, but in terms of visual similarity. In each case, the designers of the benchmark provided word pairs to human judges, who in turn provided ratings based on the metric of the benchmark. To evaluate our model, we calculate the cosmic similarity of our embedding for word pairs and then calculate spearmans between our ratings and those of the human judges."}, {"heading": "4.3 RESULTS", "text": "The results of the human judgement experiments are in Table 1. For these experiments, we tried two methods to initialize mapping: First, random initialization: The visual-textual mapping matrix was randomly initialized in the same way as the word embedding, with the goal of allowing mapping to be freely generated from the word context. Second, Neural Weight Initialization: To increase the performance of multimodal embedding, mapping was initialized with the weights of a simple neural network trained to predict known word embedding 4 from our winding image features."}, {"heading": "4.3.1 RANDOM INITIALIZATION", "text": "Interestingly, the correlation deteriorates with the addition of visual features across all benchmarks, which seems to indicate that induced mapping, when it begins with random initialization, is not yet sufficient to properly classify the revolutionary features in multimodal space. Initially, it seems that adding the visual context to the text vectors during the training, while the mapping is still being learned, may degrade the representative quality of the word embedding."}, {"heading": "4.3.2 NEURAL WEIGHT INITIALIZATION", "text": "On the other hand, if the mapping is quickly prepared for existing distributed word representations, the results are greatly improved. However, in cases of capture of general references and pure visual similarity, the multimodal model of Lazaridou et al. (2015) seems to be better. However, in the case of capture of semantic word similarity, our model performs significantly better than MMSKIP-GRAM-A (although it should be noted that these results are roughly on par with the benchmark authors (Silberer & Lapata, 2014) and one point below the non-mapping word similarity MMMSKIP-GRAM-A). Although further work is needed to examine this result, the performance of the model in this case can be visualized by an example. Table 2 provides some insight into the changes that the word embeds as a result of the incorporation of visual information into the learning process."}, {"heading": "5 CONCLUSION", "text": "Our model performs multimodal skip diagrams, using pseudo-word constructions of revolutionary image features, and then demonstrates the spread of visual information to non-visual words, but in seemingly different ways to existing models. It is noteworthy that it is evident that distributed word representations are improved and informed by this information compared to plain text-based skipprograms, but these embeddings seem to work best on a semantic rather than a visual level. Future work will focus on the nature of this embedding. In particular, we will examine the applicability of induced textual-visual mapping to the task of captioning non-recording images (Socher et al., 2013) and image recovery."}], "references": [{"title": "Multimodal distributional semantics", "author": ["Bruni", "Elia", "Tran", "Nam-Khanh", "Baroni", "Marco"], "venue": "J. Artif. Intell. Res.(JAIR),", "citeRegEx": "Bruni et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Bruni et al\\.", "year": 2014}, {"title": "Visual information in semantic representation. In Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics, pp. 91\u201399", "author": ["Feng", "Yansong", "Lapata", "Mirella"], "venue": "Association for Computational Linguistics,", "citeRegEx": "Feng et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Feng et al\\.", "year": 2010}, {"title": "Devise: A deep visual-semantic embedding model", "author": ["Frome", "Andrea", "Corrado", "Greg S", "Shlens", "Jon", "Bengio", "Samy", "Dean", "Jeff", "Mikolov", "Tomas"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Frome et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Frome et al\\.", "year": 2013}, {"title": "Symbol grounding and meaning: A comparison of high-dimensional and embodied theories of meaning", "author": ["Glenberg", "Arthur M", "Robertson", "David A"], "venue": "Journal of memory and language,", "citeRegEx": "Glenberg et al\\.,? \\Q2000\\E", "shortCiteRegEx": "Glenberg et al\\.", "year": 2000}, {"title": "Learning abstract concept embeddings from multi-modal data: Since you probably cant see what i mean", "author": ["Hill", "Felix", "Korhonen", "Anna"], "venue": "In Proceedings of EMNLP, pp. 255\u2013265,", "citeRegEx": "Hill et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Hill et al\\.", "year": 2014}, {"title": "Simlex-999: Evaluating semantic models with (genuine) similarity estimation", "author": ["Hill", "Felix", "Reichart", "Roi", "Korhonen", "Anna"], "venue": "arXiv preprint arXiv:1408.3456,", "citeRegEx": "Hill et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Hill et al\\.", "year": 2014}, {"title": "Caffe: Convolutional architecture for fast feature embedding", "author": ["Jia", "Yangqing", "Shelhamer", "Evan", "Donahue", "Jeff", "Karayev", "Sergey", "Long", "Jonathan", "Girshick", "Ross", "Guadarrama", "Sergio", "Darrell", "Trevor"], "venue": "arXiv preprint arXiv:1408.5093,", "citeRegEx": "Jia et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Jia et al\\.", "year": 2014}, {"title": "Learning image embeddings using convolutional neural networks for improved multi-modal semantics", "author": ["Kiela", "Douwe", "Bottou", "L\u00e9on"], "venue": "In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP),", "citeRegEx": "Kiela et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Kiela et al\\.", "year": 2014}, {"title": "Multimodal neural language models", "author": ["Kiros", "Ryan", "Salakhutdinov", "Ruslan", "Zemel", "Rich"], "venue": "In Proceedings of the 31st International Conference on Machine Learning", "citeRegEx": "Kiros et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Kiros et al\\.", "year": 2014}, {"title": "Imagenet classification with deep convolutional neural networks. In Advances in neural information processing", "author": ["Krizhevsky", "Alex", "Sutskever", "Ilya", "Hinton", "Geoffrey E"], "venue": null, "citeRegEx": "Krizhevsky et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Krizhevsky et al\\.", "year": 2012}, {"title": "Is this a wampimuk? cross-modal mapping between distributional semantics and the visual world", "author": ["Lazaridou", "Angeliki", "Bruni", "Elia", "Baroni", "Marco"], "venue": "In Proceedings of ACL,", "citeRegEx": "Lazaridou et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Lazaridou et al\\.", "year": 2014}, {"title": "Combining language and vision with a multimodal skip-gram model", "author": ["Lazaridou", "Angeliki", "Pham", "Nghia The", "Baroni", "Marco"], "venue": "arXiv preprint arXiv:1501.02598,", "citeRegEx": "Lazaridou et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Lazaridou et al\\.", "year": 2015}, {"title": "Efficient estimation of word representations in vector space", "author": ["Mikolov", "Tomas", "Chen", "Kai", "Corrado", "Greg", "Dean", "Jeffrey"], "venue": "arXiv preprint arXiv:1301.3781,", "citeRegEx": "Mikolov et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Distributed representations of words and phrases and their compositionality", "author": ["Mikolov", "Tomas", "Sutskever", "Ilya", "Chen", "Kai", "Corrado", "Greg S", "Dean", "Jeff"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Mikolov et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Linguistic regularities in continuous space word representations", "author": ["Mikolov", "Tomas", "Yih", "Wen-tau", "Zweig", "Geoffrey"], "venue": "In HLT-NAACL,", "citeRegEx": "Mikolov et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Wordnet: a lexical database for english", "author": ["Miller", "George A"], "venue": "Communications of the ACM,", "citeRegEx": "Miller and A.,? \\Q1995\\E", "shortCiteRegEx": "Miller and A.", "year": 1995}, {"title": "Hierarchical probabilistic neural network language model", "author": ["Morin", "Frederic", "Bengio", "Yoshua"], "venue": "In Proceedings of the international workshop on artificial intelligence and statistics,", "citeRegEx": "Morin et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Morin et al\\.", "year": 2005}, {"title": "ImageNet Large Scale Visual Recognition Challenge", "author": ["Russakovsky", "Olga", "Deng", "Jia", "Su", "Hao", "Krause", "Jonathan", "Satheesh", "Sanjeev", "Ma", "Sean", "Huang", "Zhiheng", "Karpathy", "Andrej", "Khosla", "Aditya", "Bernstein", "Michael", "Berg", "Alexander C", "Fei-Fei", "Li"], "venue": "International Journal of Computer Vision (IJCV),", "citeRegEx": "Russakovsky et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Russakovsky et al\\.", "year": 2015}, {"title": "Learning grounded meaning representations with autoencoders", "author": ["Silberer", "Carina", "Lapata", "Mirella"], "venue": "In Proceedings of ACL, pp", "citeRegEx": "Silberer et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Silberer et al\\.", "year": 2014}, {"title": "Zero-shot learning through cross-modal transfer", "author": ["Socher", "Richard", "Ganjoo", "Milind", "Manning", "Christopher D", "Ng", "Andrew"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Socher et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Socher et al\\.", "year": 2013}, {"title": "Improving word representations via global visual context", "author": ["Xu", "Ran", "Lu", "Jiasen", "Xiong", "Caiming", "Yang", "Zhi", "Corso", "Jason J"], "venue": "NIPS Workshop on Learning Semantics,", "citeRegEx": "Xu et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Xu et al\\.", "year": 2014}], "referenceMentions": [{"referenceID": 11, "context": "These types of vector representations are particularly desirable for the way in which they better model the grounding of perceptual or semantic concepts in human vocabulary (Lazaridou et al., 2015; Glenberg & Robertson, 2000; Hill & Korhonen, 2014).", "startOffset": 173, "endOffset": 248}, {"referenceID": 11, "context": "As such, there has been development towards so-called multimodal distributional semantic models (Silberer & Lapata, 2014; Lazaridou et al., 2015; Kiros et al., 2014; Frome et al., 2013; Bruni et al., 2014), which leverage textual co-occurance and visual features to form multimodal representations of words or concepts.", "startOffset": 96, "endOffset": 205}, {"referenceID": 8, "context": "As such, there has been development towards so-called multimodal distributional semantic models (Silberer & Lapata, 2014; Lazaridou et al., 2015; Kiros et al., 2014; Frome et al., 2013; Bruni et al., 2014), which leverage textual co-occurance and visual features to form multimodal representations of words or concepts.", "startOffset": 96, "endOffset": 205}, {"referenceID": 2, "context": "As such, there has been development towards so-called multimodal distributional semantic models (Silberer & Lapata, 2014; Lazaridou et al., 2015; Kiros et al., 2014; Frome et al., 2013; Bruni et al., 2014), which leverage textual co-occurance and visual features to form multimodal representations of words or concepts.", "startOffset": 96, "endOffset": 205}, {"referenceID": 0, "context": "As such, there has been development towards so-called multimodal distributional semantic models (Silberer & Lapata, 2014; Lazaridou et al., 2015; Kiros et al., 2014; Frome et al., 2013; Bruni et al., 2014), which leverage textual co-occurance and visual features to form multimodal representations of words or concepts.", "startOffset": 96, "endOffset": 205}, {"referenceID": 0, "context": ", 2013; Bruni et al., 2014), which leverage textual co-occurance and visual features to form multimodal representations of words or concepts. The work introduced in Lazaridou et al. (2015) sought to address many of the drawbacks of these models.", "startOffset": 8, "endOffset": 189}, {"referenceID": 0, "context": ", 2013; Bruni et al., 2014), which leverage textual co-occurance and visual features to form multimodal representations of words or concepts. The work introduced in Lazaridou et al. (2015) sought to address many of the drawbacks of these models. In particular, by incorporating visual information into the training objective, they address the biological inaccuracy of the existing models, in that word representations grounded in visual information have been shown to more closely approximate the way humans learn language. Furthermore, incorporating visual information alongside the text corpus allows the training set to consist of both visual and non-visual words. As a result, the induced multimodal representations and multimodal mapping no longer rely on the assumption of full visual coverage of the vocabulary, so the results are able to generalize beyond the initial training set and to be applied to various representation-related tasks, such as image annotation or retrieval. In this work, we introduce a further refinement on the multimodal skip-gram architecture, building upon the approaches of Mikolov et al. (2013a;b), Hill & Korhonen (2014), and Lazaridou et al.", "startOffset": 8, "endOffset": 1158}, {"referenceID": 0, "context": ", 2013; Bruni et al., 2014), which leverage textual co-occurance and visual features to form multimodal representations of words or concepts. The work introduced in Lazaridou et al. (2015) sought to address many of the drawbacks of these models. In particular, by incorporating visual information into the training objective, they address the biological inaccuracy of the existing models, in that word representations grounded in visual information have been shown to more closely approximate the way humans learn language. Furthermore, incorporating visual information alongside the text corpus allows the training set to consist of both visual and non-visual words. As a result, the induced multimodal representations and multimodal mapping no longer rely on the assumption of full visual coverage of the vocabulary, so the results are able to generalize beyond the initial training set and to be applied to various representation-related tasks, such as image annotation or retrieval. In this work, we introduce a further refinement on the multimodal skip-gram architecture, building upon the approaches of Mikolov et al. (2013a;b), Hill & Korhonen (2014), and Lazaridou et al. (2015). Rather than adding a visual term to the linguistic training objective, we directly situate terms in a visual context by replacing relevant words with multimodal pseudowords, derived by composing the textual representations with convolutional features projected into the multimodal space.", "startOffset": 8, "endOffset": 1187}, {"referenceID": 0, "context": ", 2013; Bruni et al., 2014), which leverage textual co-occurance and visual features to form multimodal representations of words or concepts. The work introduced in Lazaridou et al. (2015) sought to address many of the drawbacks of these models. In particular, by incorporating visual information into the training objective, they address the biological inaccuracy of the existing models, in that word representations grounded in visual information have been shown to more closely approximate the way humans learn language. Furthermore, incorporating visual information alongside the text corpus allows the training set to consist of both visual and non-visual words. As a result, the induced multimodal representations and multimodal mapping no longer rely on the assumption of full visual coverage of the vocabulary, so the results are able to generalize beyond the initial training set and to be applied to various representation-related tasks, such as image annotation or retrieval. In this work, we introduce a further refinement on the multimodal skip-gram architecture, building upon the approaches of Mikolov et al. (2013a;b), Hill & Korhonen (2014), and Lazaridou et al. (2015). Rather than adding a visual term to the linguistic training objective, we directly situate terms in a visual context by replacing relevant words with multimodal pseudowords, derived by composing the textual representations with convolutional features projected into the multimodal space. In this way, we further address the grounding problem of Glenberg & Robertson (2000) by incorpo-", "startOffset": 8, "endOffset": 1561}, {"referenceID": 19, "context": "Finally, we would also like the learned embeddings to be applicable to the problem of zero-shot learning (Socher et al., 2013; Lazaridou et al., 2014; Frome et al., 2013).", "startOffset": 105, "endOffset": 170}, {"referenceID": 10, "context": "Finally, we would also like the learned embeddings to be applicable to the problem of zero-shot learning (Socher et al., 2013; Lazaridou et al., 2014; Frome et al., 2013).", "startOffset": 105, "endOffset": 170}, {"referenceID": 2, "context": "Finally, we would also like the learned embeddings to be applicable to the problem of zero-shot learning (Socher et al., 2013; Lazaridou et al., 2014; Frome et al., 2013).", "startOffset": 105, "endOffset": 170}, {"referenceID": 8, "context": "As explained in Lazaridou et al. (2015), the majority of this literature focuses on constructing textual and visual representations independently and then combining them under some metrics.", "startOffset": 16, "endOffset": 40}, {"referenceID": 0, "context": "Bruni et al. (2014) utilize a direct approach to \u201cmixing\u201d the vector representations by concatenating the text and image vectors and applying Singular Value Decomposition.", "startOffset": 0, "endOffset": 20}, {"referenceID": 0, "context": "Bruni et al. (2014) utilize a direct approach to \u201cmixing\u201d the vector representations by concatenating the text and image vectors and applying Singular Value Decomposition. The image vectors used here, though, are constructed using the bag-of-visual-words method. In Kiela & Bottou (2014), the authors utilize a more sophisticated approach to the concatenation method by extracting visual features using state-of-the-art convolutional neural networks and the skip-gram architecture for the text.", "startOffset": 0, "endOffset": 288}, {"referenceID": 0, "context": "Bruni et al. (2014) utilize a direct approach to \u201cmixing\u201d the vector representations by concatenating the text and image vectors and applying Singular Value Decomposition. The image vectors used here, though, are constructed using the bag-of-visual-words method. In Kiela & Bottou (2014), the authors utilize a more sophisticated approach to the concatenation method by extracting visual features using state-of-the-art convolutional neural networks and the skip-gram architecture for the text. Similarly, Frome et al. (2013) also utilizes the skip-gram architecture and convolutional features; however the two modalities are then combined using a natural similarity metric.", "startOffset": 0, "endOffset": 526}, {"referenceID": 0, "context": "Bruni et al. (2014) utilize a direct approach to \u201cmixing\u201d the vector representations by concatenating the text and image vectors and applying Singular Value Decomposition. The image vectors used here, though, are constructed using the bag-of-visual-words method. In Kiela & Bottou (2014), the authors utilize a more sophisticated approach to the concatenation method by extracting visual features using state-of-the-art convolutional neural networks and the skip-gram architecture for the text. Similarly, Frome et al. (2013) also utilizes the skip-gram architecture and convolutional features; however the two modalities are then combined using a natural similarity metric. Other recent work has presented several methods for directly incorporating visual context in neural language models. In Xu et al. (2014), word context is enhanced by global visual context; i.", "startOffset": 0, "endOffset": 812}, {"referenceID": 0, "context": "Bruni et al. (2014) utilize a direct approach to \u201cmixing\u201d the vector representations by concatenating the text and image vectors and applying Singular Value Decomposition. The image vectors used here, though, are constructed using the bag-of-visual-words method. In Kiela & Bottou (2014), the authors utilize a more sophisticated approach to the concatenation method by extracting visual features using state-of-the-art convolutional neural networks and the skip-gram architecture for the text. Similarly, Frome et al. (2013) also utilizes the skip-gram architecture and convolutional features; however the two modalities are then combined using a natural similarity metric. Other recent work has presented several methods for directly incorporating visual context in neural language models. In Xu et al. (2014), word context is enhanced by global visual context; i.e., a single image is used as the context for the whole sentence (conversely, the sentence acts as a caption for the image). The multimodal skip-gram architecture proposed by Lazaridou et al. (2015) takes a more fine-grained approach by incorporating word-level visual context and concurrently training words to predict other text words in the window as well as their visual representation.", "startOffset": 0, "endOffset": 1065}, {"referenceID": 0, "context": "Bruni et al. (2014) utilize a direct approach to \u201cmixing\u201d the vector representations by concatenating the text and image vectors and applying Singular Value Decomposition. The image vectors used here, though, are constructed using the bag-of-visual-words method. In Kiela & Bottou (2014), the authors utilize a more sophisticated approach to the concatenation method by extracting visual features using state-of-the-art convolutional neural networks and the skip-gram architecture for the text. Similarly, Frome et al. (2013) also utilizes the skip-gram architecture and convolutional features; however the two modalities are then combined using a natural similarity metric. Other recent work has presented several methods for directly incorporating visual context in neural language models. In Xu et al. (2014), word context is enhanced by global visual context; i.e., a single image is used as the context for the whole sentence (conversely, the sentence acts as a caption for the image). The multimodal skip-gram architecture proposed by Lazaridou et al. (2015) takes a more fine-grained approach by incorporating word-level visual context and concurrently training words to predict other text words in the window as well as their visual representation. Our model makes this approach even more explicit, by training the word vectors to predict an additive composition of the textual and visual context and thus constructing an implicit mapping between the textual and visual modalities. Finally, the work introduced in Hill & Korhonen (2014) employs a similar \u201cpseudoword\u201d architecture to that proposed here.", "startOffset": 0, "endOffset": 1545}, {"referenceID": 12, "context": "This model is primarily derived from the skip-gram model introduced by Mikolov et al. (2013c). Skip-gram learns representations of words that predict a target word\u2019s context.", "startOffset": 71, "endOffset": 94}, {"referenceID": 17, "context": "For the visual data, we use the image data from ILSVRC 2012 (Russakovsky et al., 2015) and the corresponding Wordnet hierarchy (Miller, 1995) to represent a word visually if the word or any of its hyponyms has an entry in Imagenet and occurs more than 500 times in the text corpus.", "startOffset": 60, "endOffset": 86}, {"referenceID": 6, "context": "(2012) via the Caffe toolkit (Jia et al., 2014) to extract a 4096-dimensional vector representation of each image.", "startOffset": 29, "endOffset": 47}, {"referenceID": 8, "context": "To construct the vectors for the visual representations, we follow a similar experimental set-up as that used by Lazaridou et al. (2015). In each of the cases described above\u2014centroid and hypersphere\u2014, we randomly sample 100 images from the corresponding synsets of Imagenet for each visual word and use a pre-trained convolutional neural network as described in Krizhevsky et al.", "startOffset": 113, "endOffset": 137}, {"referenceID": 8, "context": "In each of the cases described above\u2014centroid and hypersphere\u2014, we randomly sample 100 images from the corresponding synsets of Imagenet for each visual word and use a pre-trained convolutional neural network as described in Krizhevsky et al. (2012) via the Caffe toolkit (Jia et al.", "startOffset": 225, "endOffset": 250}, {"referenceID": 0, "context": "Word Similarity Benchmarks To compare our technique to the existing literature, we evaluate our embeddings on four common benchmarks which capture several diverse aspects of word meaning: MEN (Bruni et al., 2014), Simlex-999 (Hill et al.", "startOffset": 192, "endOffset": 212}, {"referenceID": 4, "context": ", 2014), Simlex-999 (Hill et al., 2014), SemSim (Silberer & Lapata, 2014), VisSim (Silberer & Lapata, 2014).", "startOffset": 20, "endOffset": 39}, {"referenceID": 0, "context": "Word Similarity Benchmarks To compare our technique to the existing literature, we evaluate our embeddings on four common benchmarks which capture several diverse aspects of word meaning: MEN (Bruni et al., 2014), Simlex-999 (Hill et al., 2014), SemSim (Silberer & Lapata, 2014), VisSim (Silberer & Lapata, 2014). MEN was designed to capture general word \u201crelatedness.\u201d Simlex999 and SemSim measure notions of semantic similarity, and VisSim ranks the same words as SemSim but in terms of visual similarity. In each case, the designers of the benchmarks provided pairs of words to human judges, who in turned provide ratings based on the metric of the benchmark. To judge our model, we calculate the cosine similarity of our embeddings for the word pairs and then calculate Spearman\u2019s \u03c1 between our list of ratings and those of the human judges. We evaluate three versions of our model on these benchmarks: pseudowords using the centroid method (PSUEDOWORDS-C), pseudowords using the hypersphere method (PSEUDOWORDS-H), and the centroid method with a randomly initialized mapping (PSEUDOWORDS-RAN), as explained below. Existing Multimodal Models We compare our results on these benchmarks against previously published results for other multimodal word embeddings . Using the results published by Lazaridou et al. (2015) and a target word embedding of 300, we compare our results to their MMSKIP-GRAMA and MMSKIP-GRAM-B, which maximize the similarity of the textual and visual representations See, for example, http://cs.", "startOffset": 193, "endOffset": 1320}, {"referenceID": 0, "context": "Word Similarity Benchmarks To compare our technique to the existing literature, we evaluate our embeddings on four common benchmarks which capture several diverse aspects of word meaning: MEN (Bruni et al., 2014), Simlex-999 (Hill et al., 2014), SemSim (Silberer & Lapata, 2014), VisSim (Silberer & Lapata, 2014). MEN was designed to capture general word \u201crelatedness.\u201d Simlex999 and SemSim measure notions of semantic similarity, and VisSim ranks the same words as SemSim but in terms of visual similarity. In each case, the designers of the benchmarks provided pairs of words to human judges, who in turned provide ratings based on the metric of the benchmark. To judge our model, we calculate the cosine similarity of our embeddings for the word pairs and then calculate Spearman\u2019s \u03c1 between our list of ratings and those of the human judges. We evaluate three versions of our model on these benchmarks: pseudowords using the centroid method (PSUEDOWORDS-C), pseudowords using the hypersphere method (PSEUDOWORDS-H), and the centroid method with a randomly initialized mapping (PSEUDOWORDS-RAN), as explained below. Existing Multimodal Models We compare our results on these benchmarks against previously published results for other multimodal word embeddings . Using the results published by Lazaridou et al. (2015) and a target word embedding of 300, we compare our results to their MMSKIP-GRAMA and MMSKIP-GRAM-B, which maximize the similarity of the textual and visual representations See, for example, http://cs.stanford.edu/people/karpathy/cnnembed/ This is the approach taken by Lazaridou et al. (2015) http://wacky.", "startOffset": 193, "endOffset": 1613}, {"referenceID": 10, "context": "In the cases of capturing general relatedness and pure visual similarity, the multimodal model of Lazaridou et al. (2015) performs better.", "startOffset": 98, "endOffset": 122}], "year": 2015, "abstractText": "This work studies the representational mapping across multimodal data such that given a piece of the raw data in one modality the corresponding semantic description in terms of the raw data in another modality is immediately obtained. Such a representational mapping can be found in a wide spectrum of real-world applications including image/video retrieval, object recognition, action/behavior recognition, and event understanding and prediction. To that end, we introduce a simplified training objective for learning multimodal embeddings using the skip-gram architecture by introducing convolutional \u201cpseudowords:\u201d embeddings composed of the additive combination of distributed word representations and image features from convolutional neural networks projected into the multimodal space. We present extensive results of the representational properties of these embeddings on various word similarity benchmarks to show the promise of this approach.", "creator": "LaTeX with hyperref package"}}}