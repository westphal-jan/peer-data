{"id": "1511.04119", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "12-Nov-2015", "title": "Action Recognition using Visual Attention", "abstract": "We propose a soft attention based model for the task of action recognition in videos. We use multi-layered Recurrent Neural Networks (RNNs) with Long Short-Term Memory (LSTM) units which are deep both spatially and temporally. Our model learns to focus selectively on parts of the video frames and classifies videos after taking a few glimpses. The model essentially learns which parts in the frames are relevant for the task at hand and attaches higher importance to them. We evaluate the model on UCF-11 (YouTube Action), HMDB-51 and Hollywood2 datasets and analyze how the model focuses its attention depending on the scene and the action being performed.", "histories": [["v1", "Thu, 12 Nov 2015 23:06:42 GMT  (5992kb,D)", "http://arxiv.org/abs/1511.04119v1", null], ["v2", "Wed, 6 Jan 2016 20:46:47 GMT  (5994kb,D)", "http://arxiv.org/abs/1511.04119v2", null], ["v3", "Sun, 14 Feb 2016 17:20:19 GMT  (5993kb,D)", "http://arxiv.org/abs/1511.04119v3", null]], "reviews": [], "SUBJECTS": "cs.LG cs.CV", "authors": ["shikhar sharma", "ryan kiros", "ruslan salakhutdinov"], "accepted": false, "id": "1511.04119"}, "pdf": {"name": "1511.04119.pdf", "metadata": {"source": "CRF", "title": "ACTION RECOGNITION USING VISUAL ATTENTION", "authors": ["Shikhar Sharma", "Ryan Kiros"], "emails": ["shikhar@cs.toronto.edu", "rkiros@cs.toronto.edu", "rsalakhu@cs.toronto.edu"], "sections": [{"heading": "1 INTRODUCTION", "text": "It has been noted in visual cognitive literature that people do not focus their attention on an entire scene at the same time (Rensink, 2000); instead, they focus sequentially on different parts of the scene to extract relevant information. Most traditional computer vision algorithms do not address attention mechanisms and are indifferent to different parts of the image / video. Recent interest in deep neural networks has yielded promising results on several challenging tasks, including captioning generation (Xu et al., 2015), machine translation (Bahdanau et al., 2015), game and tracking (Mnih et al., 2014), and image recognition (Ba et al.)."}, {"heading": "2 RELATED WORK", "text": "They are very successful in image classification and object recognition tasks (Ren et al., 2015; Wu et al., 2015). Classifying videos instead of images Xiv: 151 1.04 119v 1 [cs.L G] 12 Nov 201 5adds a temporal dimension to the problem of image classification. Learning temporal dynamics is a difficult problem and earlier approaches have used optical flow, HOG and handmade features to generate descriptors with appearance and dynamics that are encoded. LSTMs have recently shown that they perform well in the field of speech recognition (Graves et al., 2013), machine translation (Sutskever et al., 2014), image description (Xu et al., 2015; Vinyals et al., 2015) and video description (Yao et al., 2015; Venugopalan et al., 2014). They have begun to detect dynamics in action."}, {"heading": "3 THE MODEL AND THE ATTENTION MECHANISM", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "3.1 CONVOLUTIONAL FEATURES", "text": "We extract the last revolutionary layer we obtain by pushing the video images through the GoogLeNet model (Szegedy et al., 2015), which was trained on the ImageNet dataset (Deng et al., 2009). This last revolutionary layer has D-revolutionary maps and is a feature cube of the form K \u00b7 K \u00b7 D (7 x 7 x 1024 in our experiments). Therefore, at each step of time we extract t K2 D-dimensional vectors. We refer to these vectors as feature sections in a feature cube: Xt = [Xt, 1,.., Xt, K2], Xt, i-RD. Each of these K2-vertical feature sections intersects maps into different overlapping regions in the input space and our model chooses to focus its attention on these K2 regions."}, {"heading": "3.2 THE LSTM AND THE ATTENTION MECHANISM", "text": "We use the LSTM implementation used in Zaremba et al. (2014) and Xu et al. (2015): itftot gt = \u03c3\u03c3\u03c3 tanh M (ht \u2212 1, xt), (1) ct = ft ct \u2212 1 + it gt, (2) ht = ot tanh (ct), (3) where it is the entrance gate, ft is the entrance gate, ot is the exit gate, and gt is calculated as it is in Q. 1. ct is the cell state, ht is the hidden state, and xt (see QT. 4, 5) represents the input to the LSTM in time step. M: Ra \u2192 Rb is an affine transformation consisting of traceable parameters with a = d + D and b = 4d, where d is the dimensionality of all of it, ot, gt, ct, and ht.At each time step t, our model predicts lt + 1, a softmax-K locations."}, {"heading": "3.3 LOSS FUNCTION AND THE ATTENTION PENALTY", "text": "We use the cross entropy loss coupled with the double stochastic penalty introduced in Xu et al. (2015). We place an additional restriction on the Softmax location, so \u2211 T = 1 lt, i \u2248 1. This is the attention regulation that forces the model to look at each region of the frame at some point in time. Loss function is defined as follows: L = \u2212 T \u2211 t = 1 C \u2211 i = 1 yt, i log y-t, i + \u03bb K2 \u2211 i = 1 (1 \u2212 T \u0445 t = 1 lt, i) 2 + \u03b3i \u0445 j \u03b82i, j, (7) where yt is the only hot label vector, y-t is the vector of class probabilities in time step t, T the total number of time steps, C the number of output classes, \u03bb the attention penalty coefficient, \u03b3 the weight decay coefficient and 2001 represents all model parameters. Details about the architecture and hyper section 4.2 are given."}, {"heading": "4 EXPERIMENTS", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.1 DATASETS", "text": "We used UCF-11, HMDB-51 and Hollywood2 datasets in our experiments. UCF-11 is the YouTube action dataset consisting of 1600 videos and 11 actions - basketball shooting, cycling / cycling, diving, golf swinging, horseback riding, football juggling, swings, tennis swinging, trampoline jumping, volleyball spiking and walking with a dog. Clips have a frame rate of 29.97 fps and each video has only one related action. We use 975 videos for training and 625 videos for testing. HMDB-51 Human Motion Database Dataset offers three train test splits, each consisting of 5100 videos. These clips are labeled with 51 classes of human actions such as Clap, Drink, Hug, Jump, Somersault, Throw and many others. Each video has only one action associated with it."}, {"heading": "4.2 TRAINING DETAILS AND EVALUATION", "text": "In all of our experiments, the model architecture and various other hyperparameters were cross-validated. Specifically, we trained three-layer LSTM models for all datasets, in which the dimensionality of the LSTM hidden state, cell state, and hidden layer for both UCF-11 and Hollywood2 and 1024 for HMDB-51. We also experimented with models that had one LSTM layer on five LSTM layers, but did not observe significant improvements in model performance. For the attention penalty coefficient, we experimented with the values 0, 1, 10. While reporting the results, we set the weight decay penalty to 10 \u2212 5 and used dropout (Srivastava et al., 2014) of 0.5 for all non-recurrent compounds. All models were trained using the Adam optimization algorithm (Kingma & Ba, 2015) for 15 epochs of the entire datasets."}, {"heading": "4.2.1 BASELINES", "text": "The Softmax regression model uses the full 7 x 7 x 1024 Feature Cube as input to predict the caption at each time step t, while all other models only use a 1024-dimensional feature slice as input. Average pooled and max pooled LSTM models use the same architecture as our model except that they have no attention mechanism and therefore do not generate Location Softmax. Inputs in each time step for these models are achieved by the average or maximum pooling over the 7 x 7 x 1024 Cube to obtain 1024 dimensional slices, while our soft attention model dynamically weights the slices by location Softmax (see eq. 5)."}, {"heading": "4.3 QUANTITATIVE ANALYSIS", "text": "Table 1 reports accuracies on both UCF-11 and HMDB-51 datasets and mean mean mean accuracy (MAP) on Hollywood2. Although the Softmax regression baseline indicates the full 7 x 7 x 1024 cube as input, it performs worse than our model for all three datasets and worse than all models in the case of HMDB-51 and Hollywood2. Results from Table 1 show that our attention model performs better than average and maximum pooled LSTMs. Next, we experimented with double stochastic penalty terms \u03bb (see Equation 7). Figure 3a shows that the model without attention regulation term \u03bb = 0 tends to select a few specific locations and insist on them. Setting \u03bb = 1 encourages the model to continue exploring different viewpoints. the model with \u03bb = 10 looks everywhere (see Figure 3c), with its behavior being better than the second group dynamically weighted between these two pool categories on average."}, {"heading": "4.4 QUALITATIVE ANALYSIS", "text": "In fact, it is so that most of them are able to survive themselves, without seeing themselves able to survive themselves. In fact, it is so that they are able to survive themselves, and that they are able to survive themselves. In fact, it is so that they are able to survive themselves, \"he said in an interview with the\" New York Times. \""}, {"heading": "5 CONCLUSION", "text": "Our proposed model tends to recognize important elements in video images based on the action performed. We have also shown that our model performs better than baseline models that do not use an attention mechanism. Soft attention models are impressive, but still mathematically expensive, as they still require all the features to perform dynamic pooling. In the future, we plan to explore hard attention models as well as hybrid soft and hard attention approaches that can reduce the computing costs of our model so that we can potentially scale to larger datasets such as the Sports-1M dataset. These models can also be extended to the high resolution setting, where the attention mechanism could also focus on the earlier evolutionary layers to focus on the subordinate features in the video images. Acknowledgements: This work was supported by IARPA and Raytheon N BBContract NoD1120071."}, {"heading": "A ADDITIONAL EXAMPLES", "text": "We present some correctly classified examples from UCF-11, HMDB-51 and Hollywood2 in Fig. 10 and incorrectly classified examples in Fig. 11."}], "references": [{"title": "Learning wake-sleep recurrent attention models", "author": ["J. Ba", "R. Grosse", "R. Salakhutdinov", "B. Frey"], "venue": "In NIPS,", "citeRegEx": "Ba et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Ba et al\\.", "year": 2015}, {"title": "Multiple object recognition with visual attention", "author": ["J. Ba", "V. Mnih", "K. Kavukcuoglu"], "venue": "ICLR,", "citeRegEx": "Ba et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Ba et al\\.", "year": 2015}, {"title": "Neural machine translation by jointly learning to align and translate", "author": ["D. Bahdanau", "K. Cho", "Y. Bengio"], "venue": null, "citeRegEx": "Bahdanau et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2015}, {"title": "Imagenet: A large-scale hierarchical image database", "author": ["J. Deng", "W. Dong", "R. Socher", "Li", "L.-J", "K. Li", "F.-F"], "venue": "In CVPR,", "citeRegEx": "Deng et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Deng et al\\.", "year": 2009}, {"title": "Long-term recurrent convolutional networks for visual recognition and description", "author": ["J. Donahue", "L.A. Hendricks", "S. Guadarrama", "M. Rohrbach", "S. Venugopalan", "K. Saenko", "T. Darrell"], "venue": "In CVPR,", "citeRegEx": "Donahue et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Donahue et al\\.", "year": 2015}, {"title": "Modeling video evolution for action recognition", "author": ["B. Fernando", "E. Gavves", "J. Oramas", "A. Ghodrati", "T. Tuytelaars"], "venue": "In CVPR,", "citeRegEx": "Fernando et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Fernando et al\\.", "year": 2015}, {"title": "Hybrid speech recognition with deep bidirectional LSTM", "author": ["A. Graves", "N. Jaitly", "Mohamed", "A.-r"], "venue": "IEEE Workshop on Automatic Speech Recognition and Understanding,", "citeRegEx": "Graves et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Graves et al\\.", "year": 2013}, {"title": "Long short-term memory", "author": ["S. Hochreiter", "J. Schmidhuber"], "venue": "Neural Computation,", "citeRegEx": "Hochreiter and Schmidhuber,? \\Q1997\\E", "shortCiteRegEx": "Hochreiter and Schmidhuber", "year": 1997}, {"title": "What do 15,000 object categories tell us about classifying and localizing actions", "author": ["M. Jain", "J.C. v. Gemert", "C.G.M. Snoek"], "venue": null, "citeRegEx": "Jain et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Jain et al\\.", "year": 2015}, {"title": "Large-scale video classification with convolutional neural networks", "author": ["A. Karpathy", "G. Toderici", "S. Shetty", "T. Leung", "R. Sukthankar", "Li", "F.-F"], "venue": "In CVPR,", "citeRegEx": "Karpathy et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Karpathy et al\\.", "year": 2014}, {"title": "Adam: A method for stochastic optimization", "author": ["D.P. Kingma", "J. Ba"], "venue": "ICLR,", "citeRegEx": "Kingma and Ba,? \\Q2015\\E", "shortCiteRegEx": "Kingma and Ba", "year": 2015}, {"title": "Beyond gaussian pyramid: Multi-skip feature stacking for action recognition", "author": ["Lan", "Z.-Z", "M. Lin", "X. Li", "A.G. Hauptmann", "B. Raj"], "venue": "CoRR, abs/1411.6660,", "citeRegEx": "Lan et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Lan et al\\.", "year": 2014}, {"title": "Recurrent models of visual attention", "author": ["V. Mnih", "N. Heess", "A. Graves", "K. Kavukcuoglu"], "venue": "In NIPS,", "citeRegEx": "Mnih et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Mnih et al\\.", "year": 2014}, {"title": "Reading digits in natural images with unsupervised feature learning", "author": ["Y. Netzer", "T. Wang", "A. Coates", "A. Bissacco", "B. Wu", "A.Y. Ng"], "venue": "In NIPS Workshop on Deep Learning and Unsupervised Feature Learning", "citeRegEx": "Netzer et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Netzer et al\\.", "year": 2011}, {"title": "Beyond short snippets: Deep networks for video classification", "author": ["Ng", "J.Y.-H", "M.J. Hausknecht", "S. Vijayanarasimhan", "O. Vinyals", "R. Monga", "G. Toderici"], "venue": "In CVPR,", "citeRegEx": "Ng et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Ng et al\\.", "year": 2015}, {"title": "Action recognition with stacked fisher vectors", "author": ["X. Peng", "C. Zou", "Y. Qiao", "Q. Peng"], "venue": "In ECCV,", "citeRegEx": "Peng et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Peng et al\\.", "year": 2014}, {"title": "Object detection networks on convolutional feature", "author": ["S. Ren", "K. He", "R.B. Girshick", "X. Zhang", "J. Sun"], "venue": "maps. CoRR,", "citeRegEx": "Ren et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Ren et al\\.", "year": 2015}, {"title": "The dynamic representation of scenes", "author": ["R.A. Rensink"], "venue": "Visual Cognition,", "citeRegEx": "Rensink,? \\Q2000\\E", "shortCiteRegEx": "Rensink", "year": 2000}, {"title": "Two-stream convolutional networks for action recognition in videos", "author": ["K. Simonyan", "A. Zisserman"], "venue": "In NIPS", "citeRegEx": "Simonyan and Zisserman,? \\Q2014\\E", "shortCiteRegEx": "Simonyan and Zisserman", "year": 2014}, {"title": "Dropout: a simple way to prevent neural networks from overfitting", "author": ["N. Srivastava", "G.E. Hinton", "A. Krizhevsky", "I. Sutskever", "R. Salakhutdinov"], "venue": null, "citeRegEx": "Srivastava et al\\.,? \\Q1929\\E", "shortCiteRegEx": "Srivastava et al\\.", "year": 1929}, {"title": "Unsupervised learning of video representations using LSTMs", "author": ["N. Srivastava", "E. Mansimov", "R. Salakhutdinov"], "venue": null, "citeRegEx": "Srivastava et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Srivastava et al\\.", "year": 2015}, {"title": "DL-SFA: deeply-learned slow feature analysis for action recognition", "author": ["L. Sun", "K. Jia", "Chan", "T.-H", "Y. Fang", "G. Wang", "S. Yan"], "venue": "In CVPR,", "citeRegEx": "Sun et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Sun et al\\.", "year": 2014}, {"title": "Sequence to sequence learning with neural networks", "author": ["I. Sutskever", "O. Vinyals", "Q.V.V. Le"], "venue": "In NIPS", "citeRegEx": "Sutskever et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Sutskever et al\\.", "year": 2014}, {"title": "Translating videos to natural language using deep recurrent neural networks", "author": ["S. Venugopalan", "H. Xu", "J. Donahue", "M. Rohrbach", "R.J. Mooney", "K. Saenko"], "venue": "CoRR, abs/1412.4729,", "citeRegEx": "Venugopalan et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Venugopalan et al\\.", "year": 2014}, {"title": "Show and tell: A neural image caption generator", "author": ["O. Vinyals", "A. Toshev", "S. Bengio", "D. Erhan"], "venue": "In CVPR,", "citeRegEx": "Vinyals et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Vinyals et al\\.", "year": 2015}, {"title": "Simple statistical gradient-following algorithms for connectionist reinforcement learning", "author": ["R.J. Williams"], "venue": "Machine Learning,", "citeRegEx": "Williams,? \\Q1992\\E", "shortCiteRegEx": "Williams", "year": 1992}, {"title": "Deep image: Scaling up image recognition", "author": ["R. Wu", "S. Yan", "Y. Shan", "Q. Dang", "G. Sun"], "venue": "CoRR, abs/1501.02876,", "citeRegEx": "Wu et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Wu et al\\.", "year": 2015}, {"title": "Show, attend and tell: Neural image caption generation with visual attention", "author": ["K. Xu", "J. Ba", "R. Kiros", "K. Cho", "A.C. Courville", "R. Salakhutdinov", "R.S. Zemel", "Y. Bengio"], "venue": null, "citeRegEx": "Xu et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Xu et al\\.", "year": 2015}, {"title": "Describing videos by exploiting temporal structure", "author": ["L. Yao", "A. Torabi", "K. Cho", "N. Ballas", "C. Pal", "H. Larochelle", "A. Courville"], "venue": "CoRR, abs/1502.08029,", "citeRegEx": "Yao et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Yao et al\\.", "year": 2015}, {"title": "Recurrent neural network regularization", "author": ["W. Zaremba", "I. Sutskever", "O. Vinyals"], "venue": "CoRR, abs/1409.2329,", "citeRegEx": "Zaremba et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Zaremba et al\\.", "year": 2014}, {"title": "Exploiting image-trained CNN architectures for unconstrained video classification", "author": ["S. Zha", "F. Luisier", "W. Andrews", "N. Srivastava", "R. Salakhutdinov"], "venue": "CoRR, abs/1503.04144,", "citeRegEx": "Zha et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Zha et al\\.", "year": 2015}], "referenceMentions": [{"referenceID": 17, "context": "1 INTRODUCTION It has been noted in visual cognition literature that humans do not focus their attention on an entire scene at once (Rensink, 2000).", "startOffset": 132, "endOffset": 147}, {"referenceID": 27, "context": "With the recent surge of interest in deep neural networks, attention based models have been shown to achieve promising results on several challenging tasks, including caption generation (Xu et al., 2015), machine translation (Bahdanau et al.", "startOffset": 186, "endOffset": 203}, {"referenceID": 2, "context": ", 2015), machine translation (Bahdanau et al., 2015), game-playing and tracking (Mnih et al.", "startOffset": 29, "endOffset": 52}, {"referenceID": 12, "context": ", 2015), game-playing and tracking (Mnih et al., 2014), as well as image recognition (e.", "startOffset": 35, "endOffset": 54}, {"referenceID": 25, "context": "Soft attention models are deterministic and can be trained using backpropagation, whereas hard attention models are stochastic and can be trained by the REINFORCE algorithm (Williams, 1992; Mnih et al., 2014), or by maximizing a variational lower bound or using importance sampling (Ba et al.", "startOffset": 173, "endOffset": 208}, {"referenceID": 12, "context": "Soft attention models are deterministic and can be trained using backpropagation, whereas hard attention models are stochastic and can be trained by the REINFORCE algorithm (Williams, 1992; Mnih et al., 2014), or by maximizing a variational lower bound or using importance sampling (Ba et al.", "startOffset": 173, "endOffset": 208}, {"referenceID": 30, "context": "We describe how our model dynamically pools convolutional features and show that using these features for action recognition gives better results compared to average or max pooling which is used by many of the existing models (Zha et al., 2015).", "startOffset": 226, "endOffset": 244}, {"referenceID": 16, "context": "2 RELATED WORK Convolutional Neural Networks (CNNs) have been highly successful in image classification and object recognition tasks (Ren et al., 2015; Wu et al., 2015).", "startOffset": 133, "endOffset": 168}, {"referenceID": 26, "context": "2 RELATED WORK Convolutional Neural Networks (CNNs) have been highly successful in image classification and object recognition tasks (Ren et al., 2015; Wu et al., 2015).", "startOffset": 133, "endOffset": 168}, {"referenceID": 6, "context": "LSTMs have been recently shown to perform well in the domain of speech recognition (Graves et al., 2013), machine translation (Sutskever et al.", "startOffset": 83, "endOffset": 104}, {"referenceID": 22, "context": ", 2013), machine translation (Sutskever et al., 2014), image description (Xu et al.", "startOffset": 29, "endOffset": 53}, {"referenceID": 27, "context": ", 2014), image description (Xu et al., 2015; Vinyals et al., 2015) and video description (Yao et al.", "startOffset": 27, "endOffset": 66}, {"referenceID": 24, "context": ", 2014), image description (Xu et al., 2015; Vinyals et al., 2015) and video description (Yao et al.", "startOffset": 27, "endOffset": 66}, {"referenceID": 28, "context": ", 2015) and video description (Yao et al., 2015; Venugopalan et al., 2014).", "startOffset": 30, "endOffset": 74}, {"referenceID": 23, "context": ", 2015) and video description (Yao et al., 2015; Venugopalan et al., 2014).", "startOffset": 30, "endOffset": 74}, {"referenceID": 20, "context": "They have also started picking up momentum in action recognition (Srivastava et al., 2015; Ng et al., 2015).", "startOffset": 65, "endOffset": 107}, {"referenceID": 14, "context": "They have also started picking up momentum in action recognition (Srivastava et al., 2015; Ng et al., 2015).", "startOffset": 65, "endOffset": 107}, {"referenceID": 4, "context": "Most of the existing approaches also tend to have CNNs underlying the LSTMs and classify sequences directly or do temporal pooling of features prior to classification (Donahue et al., 2015; Ng et al., 2015).", "startOffset": 167, "endOffset": 206}, {"referenceID": 14, "context": "Most of the existing approaches also tend to have CNNs underlying the LSTMs and classify sequences directly or do temporal pooling of features prior to classification (Donahue et al., 2015; Ng et al., 2015).", "startOffset": 167, "endOffset": 206}, {"referenceID": 20, "context": "LSTMs have also been used to learn an effective representation of videos in unsupervised settings (Srivastava et al., 2015) by using them in an encoder-decoder framework.", "startOffset": 98, "endOffset": 123}, {"referenceID": 13, "context": "Instead of weighting locations using a softmax layer which we do, they apply affine transformations to multiple layers of their CNN to attend to the relevant part and get state-ofthe-art results on the Street View House Numbers dataset (Netzer et al., 2011).", "startOffset": 236, "endOffset": 257}, {"referenceID": 4, "context": "Most of the existing approaches also tend to have CNNs underlying the LSTMs and classify sequences directly or do temporal pooling of features prior to classification (Donahue et al., 2015; Ng et al., 2015). LSTMs have also been used to learn an effective representation of videos in unsupervised settings (Srivastava et al., 2015) by using them in an encoder-decoder framework. More recently, Yao et al. (2015) have proposed to use 3-D CNN features and an LSTM decoder in an encoder-decoder framework to generate video descriptions.", "startOffset": 168, "endOffset": 412}, {"referenceID": 4, "context": "Most of the existing approaches also tend to have CNNs underlying the LSTMs and classify sequences directly or do temporal pooling of features prior to classification (Donahue et al., 2015; Ng et al., 2015). LSTMs have also been used to learn an effective representation of videos in unsupervised settings (Srivastava et al., 2015) by using them in an encoder-decoder framework. More recently, Yao et al. (2015) have proposed to use 3-D CNN features and an LSTM decoder in an encoder-decoder framework to generate video descriptions. Their model incorporates attention on a video level by defining a probability distribution over frames used to generate individual words. They, however, do not employ an attention mechanism on a frame level (i.e. within a single frame). In general, it is rather difficult to interpret internal representations learned by deep neural networks. Attention models add a dimension of interpretability by capturing where the model is focusing its attention when performing a particular task. Karpathy et al. (2014) used a multi-resolution CNN architecture to perform action recognition in videos.", "startOffset": 168, "endOffset": 1043}, {"referenceID": 4, "context": "Most of the existing approaches also tend to have CNNs underlying the LSTMs and classify sequences directly or do temporal pooling of features prior to classification (Donahue et al., 2015; Ng et al., 2015). LSTMs have also been used to learn an effective representation of videos in unsupervised settings (Srivastava et al., 2015) by using them in an encoder-decoder framework. More recently, Yao et al. (2015) have proposed to use 3-D CNN features and an LSTM decoder in an encoder-decoder framework to generate video descriptions. Their model incorporates attention on a video level by defining a probability distribution over frames used to generate individual words. They, however, do not employ an attention mechanism on a frame level (i.e. within a single frame). In general, it is rather difficult to interpret internal representations learned by deep neural networks. Attention models add a dimension of interpretability by capturing where the model is focusing its attention when performing a particular task. Karpathy et al. (2014) used a multi-resolution CNN architecture to perform action recognition in videos. They mention the concept of fovea but they fix attention to the center of the frame. A recent work of Xu et al. (2015) used both soft attention and hard attention mechanisms to generate image descriptions.", "startOffset": 168, "endOffset": 1244}, {"referenceID": 4, "context": "Most of the existing approaches also tend to have CNNs underlying the LSTMs and classify sequences directly or do temporal pooling of features prior to classification (Donahue et al., 2015; Ng et al., 2015). LSTMs have also been used to learn an effective representation of videos in unsupervised settings (Srivastava et al., 2015) by using them in an encoder-decoder framework. More recently, Yao et al. (2015) have proposed to use 3-D CNN features and an LSTM decoder in an encoder-decoder framework to generate video descriptions. Their model incorporates attention on a video level by defining a probability distribution over frames used to generate individual words. They, however, do not employ an attention mechanism on a frame level (i.e. within a single frame). In general, it is rather difficult to interpret internal representations learned by deep neural networks. Attention models add a dimension of interpretability by capturing where the model is focusing its attention when performing a particular task. Karpathy et al. (2014) used a multi-resolution CNN architecture to perform action recognition in videos. They mention the concept of fovea but they fix attention to the center of the frame. A recent work of Xu et al. (2015) used both soft attention and hard attention mechanisms to generate image descriptions. Their model actually looks at the respective objects when generating their description. More recently, Jaderberg et al. (2015) have proposed a soft-attention mechanism called the Spatial Transformer module which they add between the layers of CNNs.", "startOffset": 168, "endOffset": 1458}, {"referenceID": 3, "context": ", 2015) trained on the ImageNet dataset (Deng et al., 2009).", "startOffset": 40, "endOffset": 59}, {"referenceID": 28, "context": "2 THE LSTM AND THE ATTENTION MECHANISM We use the LSTM implementation discussed in Zaremba et al. (2014) and Xu et al.", "startOffset": 83, "endOffset": 105}, {"referenceID": 27, "context": "(2014) and Xu et al. (2015): \uf8ec\uf8ed it ft ot gt \uf8f7\uf8f8 = \uf8ec\uf8ed \u03c3\u03c3\u03c3 tanh \uf8f7\uf8f8M (ht\u22121, xt ) , (1)", "startOffset": 11, "endOffset": 28}, {"referenceID": 2, "context": "After calculating these probabilities, the soft attention mechanism (Bahdanau et al., 2015) computes the expected value of the input at the next time-step xt by taking expectation over the feature slices at different regions (see Fig.", "startOffset": 68, "endOffset": 91}, {"referenceID": 27, "context": "We use cross-entropy loss coupled with the doubly stochastic penalty introduced in Xu et al. (2015). We impose an additional constraint over the location softmax, so that \u2211T t=1 lt,i \u2248 1.", "startOffset": 83, "endOffset": 100}, {"referenceID": 20, "context": "9 Composite LSTM Model (Srivastava et al., 2015) 44.", "startOffset": 23, "endOffset": 48}, {"referenceID": 21, "context": "0 DL-SFA (Sun et al., 2014) 48.", "startOffset": 9, "endOffset": 27}, {"referenceID": 5, "context": "4 VideoDarwin (Fernando et al., 2015) 63.", "startOffset": 14, "endOffset": 37}, {"referenceID": 11, "context": "7 Multi-skIp Feature Stacking (Lan et al., 2014) 65.", "startOffset": 30, "endOffset": 48}, {"referenceID": 15, "context": "0 Traditional+Stacked Fisher Vectors (Peng et al., 2014) 66.", "startOffset": 37, "endOffset": 56}, {"referenceID": 8, "context": "8 Objects+Traditional+Stacked Fisher Vectors (Jain et al., 2015) 71.", "startOffset": 45, "endOffset": 64}], "year": 2015, "abstractText": "We propose a soft attention based model for the task of action recognition in videos. We use multi-layered Recurrent Neural Networks (RNNs) with Long Short-Term Memory (LSTM) units which are deep both spatially and temporally. Our model learns to focus selectively on parts of the video frames and classifies videos after taking a few glimpses. The model essentially learns which parts in the frames are relevant for the task at hand and attaches higher importance to them. We evaluate the model on UCF-11 (YouTube Action), HMDB-51 and Hollywood2 datasets and analyze how the model focuses its attention depending on the scene and the action being performed.", "creator": "LaTeX with hyperref package"}}}