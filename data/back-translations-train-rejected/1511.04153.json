{"id": "1511.04153", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "13-Nov-2015", "title": "Adaptive Affinity Matrix for Unsupervised Metric Learning", "abstract": "Spectral clustering is one of the most popular clustering approaches with the capability to handle some challenging clustering problems. Most spectral clustering methods provide a nonlinear map from the data manifold to a subspace. Only a little work focuses on the explicit linear map which can be viewed as the unsupervised distance metric learning. In practice, the selection of the affinity matrix exhibits a tremendous impact on the unsupervised learning. While much success of affinity learning has been achieved in recent years, some issues such as noise reduction remain to be addressed. In this paper, we propose a novel method, dubbed Adaptive Affinity Matrix (AdaAM), to learn an adaptive affinity matrix and derive a distance metric from the affinity. We assume the affinity matrix to be positive semidefinite with ability to quantify the pairwise dissimilarity. Our method is based on posing the optimization of objective function as a spectral decomposition problem. We yield the affinity from both the original data distribution and the widely-used heat kernel. The provided matrix can be regarded as the optimal representation of pairwise relationship on the manifold. Extensive experiments on a number of real-world data sets show the effectiveness and efficiency of AdaAM.", "histories": [["v1", "Fri, 13 Nov 2015 03:59:14 GMT  (467kb,D)", "https://arxiv.org/abs/1511.04153v1", null], ["v2", "Sun, 11 Sep 2016 13:58:06 GMT  (55kb,D)", "http://arxiv.org/abs/1511.04153v2", null]], "reviews": [], "SUBJECTS": "cs.CV cs.LG", "authors": ["yaoyi li", "junxuan chen", "hongtao lu"], "accepted": false, "id": "1511.04153"}, "pdf": {"name": "1511.04153.pdf", "metadata": {"source": "CRF", "title": "ADAPTIVE AFFINITY MATRIX FOR UNSUPERVISED METRIC LEARNING", "authors": ["Yaoyi Li", "Junxuan Chen", "Yiru Zhao", "Hongtao Lu", "Jiao Tong"], "emails": ["htlu}@sjtu.edu.cn"], "sections": [{"heading": null, "text": "Index Terms - Affinity Learning, Feature Projection, Dimensionality Reduction, Spectral Clustering"}, {"heading": "1. INTRODUCTION", "text": "This year it is more than ever before."}, {"heading": "2. ADAPTIVE AFFINITY MATRIX", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "2.1. Notation", "text": "In this thesis we write all matrices as uppercase letters (English or Greek alphabet) and vectors as lowercase letters. The vector with all elements is denoted by 1. H is the centering matrix denoted by H = I \u2212 1n11T. The original data matrix is denoted by X-Rn \u00b7 d, where n is the number of data points and d is the dimension of the data. X is assumed to be normalized with the mean value zero, i.e. X = HX. The denotation xi means the i-th data point vector. We also denote the linear projection with A and denote the metric matrix with M = ATA. Therefore, the Mahalanobis distance is based on Dism (xi, xj) = (xi \u2212 xj) TM (xi \u2212 xj) TM (xi \u2212 matrix).The k-NN heat kernel matrix is denoted by W-Rn with \u2212 wij = {exp (xi \u2212 rij) or else by the mean value (xi) and Nxi (nxi)."}, {"heading": "2.2. Intermediate Affinity Matrix", "text": "We separate our algorithms into two identical matrices, since the graph is Laplacian. (3) In this section we will introduce the first part of the matrix. (4) In this section we will introduce the first part of the matrix. (4) In this section we will select the first part of the matrix. (5) We will apply the following objective operating principles. (4) In the hope that the small Euclidean distance between two data points will lead to a great similarity, we will try to minimize the following objective functionalities. (5) We will reform the equation with Graphics Laplacian. (3) We assume that the XTL-X-X-X is the ij-th element of the intermediate affinity, subject to appropriate limitations. (4) We can decompose the Laplacian into two identical matrices, since the graph is Laplacian, min tr (XTL-X)."}, {"heading": "2.3. Final Adaptive Affinity Matrix", "text": "In this section we formulate a naive linear spectral clustering and provide the final adaptive affinity matrix (A = A). With the intermediate affinity matrix (A) we can solve the following problem for a linear projection A: a = arg min aT a = 1tr (aTXT (L + L) Xa) (13), where a is the one-dimensional case of A and L + L (14) the combination of the laplacian of the k-NN heat core and the intermediate affinity matrix. The projection vector a is given by the minimal eigenvalue of the proprietary problem: XT (L \u2212 \u2206) Xa = percrix (14), in order to then calculate the L-value of Eq. (13) given A, we rewrite the affinity optimization problem with the linear projection matrix A, as we do the PATx. (6) arg = Ptc + P."}, {"heading": "2.4. Sparsification Strategy", "text": "From the optimization problem (12) and (16), we can conclude that the matrices XXT and XAATXT are both low-graded matrix. Since the solution to the above-mentioned optimization problem is based on the decomposition of the singular value, this low-graded fact will result in the number of columns of solution P being much smaller than the ranks of XXT and XAATXT. This process will produce a low-graded affinity matrix that will lead to a progressive decrease in the rank in our approach. To prevent the rank from decreasing, we implement thriftification in our approach. The thriftification strategy can also mitigate the problem of noise limits. Figure 1 justifies our thriftification method by detecting the histogram of the order of magnitude of the final adaptive affinity matrix obtained from equality matrix. (16) Without thriftification, we can observe that most of the elements with lower size matrix are concentrated in the affrix."}, {"heading": "3. EXPERIMENTS", "text": "In this section, we will conduct several experiments to demonstrate the effectiveness and efficiency of the proposed AdaAM approach."}, {"heading": "3.1. Data Sets", "text": "We evaluate the proposed approach using five image data sets: UMIST The UMIST face database consists of 575 images of 20 individuals with 220 x 220 pixels [16]. We use images reduced to 40 x 40 pixels in our experiments. COIL20 One data set consists of 1,440 images of 20 objects with discarded background [17].USPS The handwritten number database of the USPS comprises 9,298 images of 10 digits with 16 x 16 pixels [18].MNIST The MNIST handwritten number database contains 70,000 images from 10 classes [19]. In our experiments we select the first 10,000 images from this database.ExYaleB The extended Yale face database B consists of 2,414 cropped images with 38 individuals and approximately 64 images under different illumination per individual [20].The statistics of the data sets are summarized in Tab. 2."}, {"heading": "3.2. Compared Algorithms", "text": "We compare our approach with the other affinity learning algorithms described in Section Related Work. We use LPP on the affinity matrices generated by these state-of-the-art approaches to obtain distance metrics.Con-kNN Cons-kNN Consensus k-NNs [12] with the aim of selecting robust neighborhood matrices proposed in [11]. ClustRF-Bi A spatial case of ClustRF-Strct [13], which is also proposed in [21, 22]. Due to the enormous storage requirements of ClustRF-Strct on the dataset with thousands of instances, we implement this particular case in our experiments. PCAN Projected Clustering with Adaptive Neighbors, as proposed in [14]."}, {"heading": "3.3. Parameter Selection and Experiment Details", "text": "Since there is no validation data for unattended learning tasks, we submit the same selection criteria for parameters to all algorithms in our experiments for more general cases. We set the size of the neighborhood to k = round (log2 (n / c)), where n is the number of data instances and c is the number of classes. We also set the projected dimension corresponding to the rank of the metric matrix as the number of classes [5]. All other parameters of our approach are specified in each experiment.We designate ten times the k averages as round and select the cluster result with the minimum sum within the clusters as the result of each round of k averages. We apply to each algorithm 100 round averages for evaluating performance (tab. 1), 10 rounds of k averages for the experiment of sensitivity to neighborhood magnitude k (fig. 2), and a round k averages for conducting the experiment (fig. 3)."}, {"heading": "3.4. Experiment Results", "text": "In the cluster accuracy experiment, we evaluate the projection capability of AdaAM with other five algorithms on five of the above-mentioned benchmark datasets. Tab. 1 indicates the average and maximum accuracy of 100 rounds of k-Means of each model. From Tab. 1, we can observe this superiority of AdaAM in the task of unattended metric learning. In most cases, AdaAM performs much better than the other approaches. Our approach achieves four best results of average accuracy and five best maximum accuracy on five datasets. We can also find that the proposed AdaAM significantly outperforms other five methods on the ExYaleB dataset. Unlike the other datasets, the image data in ExYaleB is properly aligned and illuminated. This difference makes some images more similar to the image in different classes than the matrix metrics that result in a high ranking."}, {"heading": "4. CONCLUSION", "text": "In this paper, we present a novel affinity learning approach for unattended metric learning, the Adaptive Affinity Matrix (AdaAM). In our new affinity learning model, the affinity matrix is learned from the same framework of spectral clustering. Specifically, we show that affinity learning can be reduced to a singular value allocation problem. With the learned affinity matrix, the distance metrix can be derived by some standard approaches based on the affinity graph such as LPP. Extensive experiments on clustering image data demonstrate the superiority of the proposed method AdaAM."}, {"heading": "5. REFERENCES", "text": "[1] Trevor F Cox and Michael AA Cox, Multidimensional scaling, CRC Press, 2000. [2] Sam T Roweis and Lawrence K Saul, \"Nonlinear dimensionality reduction by locally linear embedding,\" Science, vol. 290, no. 5500, pp. 2323-2326, 2000. [3] Joshua B Tenenbaum, Vin De Silva, and John C Langford, \"A global geometric framework for embedding and clustering,\" Science, vol. 290, no. 5500, pp. 2319-2323, 2000. [4] Mikhail Belkin and Partha De Silva, and John C Langford, \"Laplacian eigenmaps and spectral techniques for embedding and clustering,\" in NIPS, 2001. [5] Andrew Y Ng, Michael I Jordan, Yair Weiss, et al., \"On spectral clustering: Analysis and an algorithm, in NIPS, 2002."}], "references": [{"title": "Nonlinear dimensionality reduction by locally linear embedding", "author": ["Sam T Roweis", "Lawrence K Saul"], "venue": "Science, vol. 290, no. 5500, pp. 2323\u20132326, 2000.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2000}, {"title": "A global geometric framework for nonlinear dimensionality reduction", "author": ["Joshua B Tenenbaum", "Vin De Silva", "John C Langford"], "venue": "Science, vol. 290, no. 5500, pp. 2319\u20132323, 2000.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2000}, {"title": "Laplacian eigenmaps and spectral techniques for embedding and clustering", "author": ["Mikhail Belkin", "Partha Niyogi"], "venue": "NIPS, 2001.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2001}, {"title": "On spectral clustering: Analysis and an algorithm", "author": ["Andrew Y Ng", "Michael I Jordan", "Yair Weiss"], "venue": "NIPS, 2002.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2002}, {"title": "Out-of-sample extensions for lle, isomap, mds, eigenmaps, and spectral clustering", "author": ["Yoshua Bengio", "Jean-Fran\u00e7ois Paiement", "Pascal Vincent", "Olivier Delalleau", "Nicolas Le Roux", "Marie Ouimet"], "venue": "NIPS, 2004.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2004}, {"title": "Locality preserving projections", "author": ["Xiaofei He", "Partha Niyogi"], "venue": "NIPS, 2004.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2004}, {"title": "Spectral grouping using the nystrom method", "author": ["Charless Fowlkes", "Serge Belongie", "Fan Chung", "Jitendra Malik"], "venue": "IEEE TPAMI, vol. 26, no. 2, pp. 214\u2013225, 2004.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2004}, {"title": "Fast approximate spectral clustering", "author": ["Donghui Yan", "Ling Huang", "Michael I Jordan"], "venue": "ACM SIGKDD, 2009.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2009}, {"title": "Large scale spectral clustering with landmark-based representation", "author": ["Xinlei Chen", "Deng Cai"], "venue": "AAAI, 2011.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2011}, {"title": "Dominant sets and pairwise clustering", "author": ["Massimiliano Pavan", "Marcello Pelillo"], "venue": "IEEE TPAMI, vol. 29, no. 1, pp. 167\u2013172, 2007.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2007}, {"title": "Consensus of k-nns for robust neighborhood selection on graph-based manifolds", "author": ["Vittal Premachandran", "Ramakrishna Kakarala"], "venue": "CVPR, 2013.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2013}, {"title": "Constructing robust affinity graphs for spectral clustering", "author": ["Xiatian Zhu", "Chen Change Loy", "Shaogang Gong"], "venue": "CVPR, 2014.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2014}, {"title": "Clustering and projected clustering with adaptive neighbors", "author": ["Feiping Nie", "Xiaoqian Wang", "Heng Huang"], "venue": "ACM SIGKDD, 2014.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2014}, {"title": "Normalized cuts and image segmentation", "author": ["Jianbo Shi", "Jitendra Malik"], "venue": "IEEE TPAMI, vol. 22, no. 8, pp. 888\u2013905, 2000.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2000}, {"title": "Characterising virtual eigensignatures for general purpose face recognition", "author": ["Daniel B Graham", "Nigel M Allinson"], "venue": "Face Recognition, pp. 446\u2013456. Springer, 1998.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 1998}, {"title": "Columbia object image library (coil-20)", "author": ["Sameer A Nene", "Shree K Nayar", "Hiroshi Murase"], "venue": "Tech. Rep., Technical Report CUCS-005-96, 1996.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 1996}, {"title": "A database for handwritten text recognition research", "author": ["Jonathan J Hull"], "venue": "IEEE TPAMI, vol. 16, no. 5, pp. 550\u2013 554, 1994.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 1994}, {"title": "Gradient-based learning applied to document recognition", "author": ["Yann LeCun", "L\u00e9on Bottou", "Yoshua Bengio", "Patrick Haffner"], "venue": "Proceedings of the IEEE, vol. 86, no. 11, pp. 2278\u20132324, 1998.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 1998}, {"title": "Acquiring linear subspaces for face recognition under variable lighting", "author": ["K.C. Lee", "J. Ho", "D. Kriegman"], "venue": "IEEE TPAMI, vol. 27, no. 5, pp. 684\u2013698, 2005.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2005}, {"title": "Decision forests: A unified framework for classification, regression, density estimation, manifold learning and semi-supervised learning", "author": ["Antonio Criminisi", "Jamie Shotton", "Ender Konukoglu"], "venue": "Now Publishers Inc.,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2012}, {"title": "Unsupervised random forest manifold alignment for lipreading", "author": ["Yuru Pei", "Tae-Kyun Kim", "Hongbin Zha"], "venue": "ICCV, 2013.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2013}], "referenceMentions": [{"referenceID": 0, "context": "During the past decades, a series of spectral clustering methods have been proposed: Multidimensional Scaling (MDS) [1], Local Linear Embedding (LLE) [2], Isomap [3], Laplacian Eigenmaps [4] and variant of Spectral Clustering [5].", "startOffset": 150, "endOffset": 153}, {"referenceID": 1, "context": "During the past decades, a series of spectral clustering methods have been proposed: Multidimensional Scaling (MDS) [1], Local Linear Embedding (LLE) [2], Isomap [3], Laplacian Eigenmaps [4] and variant of Spectral Clustering [5].", "startOffset": 162, "endOffset": 165}, {"referenceID": 2, "context": "During the past decades, a series of spectral clustering methods have been proposed: Multidimensional Scaling (MDS) [1], Local Linear Embedding (LLE) [2], Isomap [3], Laplacian Eigenmaps [4] and variant of Spectral Clustering [5].", "startOffset": 187, "endOffset": 190}, {"referenceID": 3, "context": "During the past decades, a series of spectral clustering methods have been proposed: Multidimensional Scaling (MDS) [1], Local Linear Embedding (LLE) [2], Isomap [3], Laplacian Eigenmaps [4] and variant of Spectral Clustering [5].", "startOffset": 226, "endOffset": 229}, {"referenceID": 4, "context": "Many important progresses [6, 7, 8, 9, 10, 11, 12, 13, 14] have been made to mitigate the above issues of the spectral \u2217Corresponding author This paper is supported by NSFC (No.", "startOffset": 26, "endOffset": 58}, {"referenceID": 5, "context": "Many important progresses [6, 7, 8, 9, 10, 11, 12, 13, 14] have been made to mitigate the above issues of the spectral \u2217Corresponding author This paper is supported by NSFC (No.", "startOffset": 26, "endOffset": 58}, {"referenceID": 6, "context": "Many important progresses [6, 7, 8, 9, 10, 11, 12, 13, 14] have been made to mitigate the above issues of the spectral \u2217Corresponding author This paper is supported by NSFC (No.", "startOffset": 26, "endOffset": 58}, {"referenceID": 7, "context": "Many important progresses [6, 7, 8, 9, 10, 11, 12, 13, 14] have been made to mitigate the above issues of the spectral \u2217Corresponding author This paper is supported by NSFC (No.", "startOffset": 26, "endOffset": 58}, {"referenceID": 8, "context": "Many important progresses [6, 7, 8, 9, 10, 11, 12, 13, 14] have been made to mitigate the above issues of the spectral \u2217Corresponding author This paper is supported by NSFC (No.", "startOffset": 26, "endOffset": 58}, {"referenceID": 9, "context": "Many important progresses [6, 7, 8, 9, 10, 11, 12, 13, 14] have been made to mitigate the above issues of the spectral \u2217Corresponding author This paper is supported by NSFC (No.", "startOffset": 26, "endOffset": 58}, {"referenceID": 10, "context": "Many important progresses [6, 7, 8, 9, 10, 11, 12, 13, 14] have been made to mitigate the above issues of the spectral \u2217Corresponding author This paper is supported by NSFC (No.", "startOffset": 26, "endOffset": 58}, {"referenceID": 11, "context": "Many important progresses [6, 7, 8, 9, 10, 11, 12, 13, 14] have been made to mitigate the above issues of the spectral \u2217Corresponding author This paper is supported by NSFC (No.", "startOffset": 26, "endOffset": 58}, {"referenceID": 12, "context": "Many important progresses [6, 7, 8, 9, 10, 11, 12, 13, 14] have been made to mitigate the above issues of the spectral \u2217Corresponding author This paper is supported by NSFC (No.", "startOffset": 26, "endOffset": 58}, {"referenceID": 5, "context": "Locality Preserving Projections (LPP) proposed in [7] introduces a linear projection obtained from Laplacian Eigenmaps.", "startOffset": 50, "endOffset": 53}, {"referenceID": 12, "context": "Nie, Wang, and Huang proposed the Projected Clustering with Adaptive Neighbors (PCAN) in [14] where they regard the pairwise similarity as an extra variable to be solved in the optimization problem and they set a penalty of the rank of graph Laplacian to restrict specific connected components in the affinity matrix.", "startOffset": 89, "endOffset": 93}, {"referenceID": 8, "context": "Inspired by the recent progress on scalable spectral clustering [10] and data similarity learning [14], we propose a novel approach dubbed Adaptive Affinity Matrix (AdaAM).", "startOffset": 64, "endOffset": 68}, {"referenceID": 12, "context": "Inspired by the recent progress on scalable spectral clustering [10] and data similarity learning [14], we propose a novel approach dubbed Adaptive Affinity Matrix (AdaAM).", "startOffset": 98, "endOffset": 102}, {"referenceID": 3, "context": "As the ideal case described in [5], if we assume the pairwise affinity in the same class are exceedingly similar, the affinity matrix may turn into a low-rank matrix.", "startOffset": 31, "endOffset": 34}, {"referenceID": 2, "context": "[4] and some other state-of-the-art algorithms in Section 3.", "startOffset": 0, "endOffset": 3}, {"referenceID": 12, "context": "Different from PCAN [14], we reformulate the equation with graph Laplacian,", "startOffset": 20, "endOffset": 24}, {"referenceID": 8, "context": "Inspired by LSC [10] we assume the affinity matrix to be a positive semidefinite matrix and decompose it into the product of a matrix P \u2208 Rn\u00d7t with orthogonal columns and P instead of decomposing the Laplacian matrix, where t is the expected rank of \u2206.", "startOffset": 16, "endOffset": 20}, {"referenceID": 13, "context": "Since the weight of nodes in the graph plays an important role in some algorithms and methods based on Normalized Cuts [15] like LPP has the constraint relying on D\u2206.", "startOffset": 119, "endOffset": 123}, {"referenceID": 14, "context": "We evaluate the proposed approach on five image data sets: UMIST The UMIST Face Database consists of 575 images of 20 individuals with 220\u00d7220 pixels [16].", "startOffset": 150, "endOffset": 154}, {"referenceID": 15, "context": "COIL20 A data set consists of 1,440 images of 20 objects with discarded background [17].", "startOffset": 83, "endOffset": 87}, {"referenceID": 16, "context": "USPS The USPS handwritten digit database has 9,298 images of 10 digits with 16\u00d716 pixels [18].", "startOffset": 89, "endOffset": 93}, {"referenceID": 17, "context": "MNIST The MNIST database of handwritten digits has 70,000 images of 10 classes [19].", "startOffset": 79, "endOffset": 83}, {"referenceID": 18, "context": "ExYaleB The Extended Yale Face Database B consists of 2,414 cropped images with 38 individuals and around 64 images under different illuminations per individual [20].", "startOffset": 161, "endOffset": 165}, {"referenceID": 10, "context": "Con-kNN Cons-kNN Consensus k-NNs [12] with the aim of selecting robust neighborhoods.", "startOffset": 33, "endOffset": 37}, {"referenceID": 9, "context": "DN Dominant Neighborhoods proposed in [11].", "startOffset": 38, "endOffset": 42}, {"referenceID": 11, "context": "ClustRF-Bi A spacial case of ClustRF-Strct [13], which is also proposed in [21, 22].", "startOffset": 43, "endOffset": 47}, {"referenceID": 19, "context": "ClustRF-Bi A spacial case of ClustRF-Strct [13], which is also proposed in [21, 22].", "startOffset": 75, "endOffset": 83}, {"referenceID": 20, "context": "ClustRF-Bi A spacial case of ClustRF-Strct [13], which is also proposed in [21, 22].", "startOffset": 75, "endOffset": 83}, {"referenceID": 12, "context": "PCAN Projected Clustering with Adaptive Neighbors proposed in [14].", "startOffset": 62, "endOffset": 66}, {"referenceID": 3, "context": "We also set the projected dimension, which is equal to the rank of metric matrix, to be the same as the number of classes [5].", "startOffset": 122, "endOffset": 125}], "year": 2016, "abstractText": "Spectral clustering is one of the most popular clustering approaches with the capability to handle some challenging clustering problems. Only a little work of spectral clustering focuses on the explicit linear map which can be viewed as the distance metric learning. In practice, the selection of the affinity matrix exhibits a tremendous impact on the unsupervised learning. In this paper, we propose a novel method, dubbed Adaptive Affinity Matrix (AdaAM), to learn an adaptive affinity matrix and derive a distance metric. We assume the affinity matrix to be positive semidefinite with ability to quantify the pairwise dissimilarity. Our method is based on posing the optimization of objective function as a spectral decomposition problem. The provided matrix can be regarded as the optimal representation of pairwise relationship on the manifold. Extensive experiments on a number of image data sets show the effectiveness and efficiency of AdaAM.", "creator": "LaTeX with hyperref package"}}}