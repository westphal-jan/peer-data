{"id": "1511.04306", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "13-Nov-2015", "title": "Deep Feature Learning for EEG Recordings", "abstract": "We introduce and compare several strategies for learning discriminative features from electroencephalography (EEG) recordings using deep learning techniques. EEG data are generally only available in small quantities, they are high-dimensional with a poor signal-to-noise ratio, and there is considerable variability between individual subjects and recording sessions. Our proposed techniques specifically address these challenges for feature learning. Similarity-constraint encoders learn features that allow to distinguish between classes by demanding that two trials from the same class are more similar to each other than to trials from other classes. This tuple-based training approach is especially suitable for small datasets. Hydra-nets allow for separate processing pathways adapting to subsets of a dataset and thus combine the advantages of individual feature learning (better adaptation of early, low-level processing) with group model training (better generalization of higher-level processing in deeper layers). This way, models can, for instance, adapt to each subject individually to compensate for differences in spatial patterns due to anatomical differences or variance in electrode positions. The different techniques are evaluated using the publicly available OpenMIIR dataset of EEG recordings taken while participants listened to and imagined music.", "histories": [["v1", "Fri, 13 Nov 2015 15:07:17 GMT  (31kb)", "http://arxiv.org/abs/1511.04306v1", "submitted as conference paper for ICLR 2016"], ["v2", "Thu, 19 Nov 2015 22:04:12 GMT  (8322kb,D)", "http://arxiv.org/abs/1511.04306v2", "submitted as conference paper for ICLR 2016"], ["v3", "Fri, 27 Nov 2015 18:24:08 GMT  (8674kb,D)", "http://arxiv.org/abs/1511.04306v3", "submitted as conference paper for ICLR 2016"], ["v4", "Thu, 7 Jan 2016 16:26:42 GMT  (4964kb,D)", "http://arxiv.org/abs/1511.04306v4", "submitted as conference paper for ICLR 2016"]], "COMMENTS": "submitted as conference paper for ICLR 2016", "reviews": [], "SUBJECTS": "cs.NE cs.LG", "authors": ["sebastian stober", "avital sternin", "adrian m owen", "jessica a grahn"], "accepted": false, "id": "1511.04306"}, "pdf": {"name": "1511.04306.pdf", "metadata": {"source": "CRF", "title": null, "authors": [], "emails": ["sstober@uwo.ca", "asternin@uwo.ca", "adrian.owen@uwo.ca", "jgrahn@uwo.ca"], "sections": [{"heading": null, "text": "ar Xiv: 151 1.04 306v 1 [cs.N E] 13 Nov 201 5"}, {"heading": "1 INTRODUCTION", "text": "Over the last decade, deep learning techniques have become very popular in various fields of application, such as computer vision, automatic speech recognition, natural language processing, and bioinformatics, where they have been proven to produce state-of-the-art results in various tasks. At the same time, very little progress has been made in the application of cognitive neuroscience, and in particular in the analysis of signals recorded by electroencephalography (EEG) - a non-invasive imaging technology that relies on electrodes to measure the electrical activity of the brain. EEG is particularly popular in the development of brain-computer interfaces (BCIs) that must distinguish between different brain regions."}, {"heading": "2 RELATED WORK", "text": "In this area of application, the potential of deep learning techniques for neuroimaging was recently demonstrated by Plis et al. (2013) for functional and structural magnetic resonance imaging (MRI) data. (2009) Applied Convolutionary Neural Networks (CNNs) for epileptic seizure prediction in EEG and intercranial EEG research and specifically for processing EEG recordings have been very limited to date. (2011) Applied Neural Networks (CNNs) for epileptic seizure prediction in EEG and intercranial EEG systems. Wulsin et al. (2011) Applied Deep Belief Networks (DBNs) to detect abnormalities associated with EEG recordings that classify individual \"channel seconds,\" i.e., one second chunks from a single EEG channel without further information from other channels or previous values."}, {"heading": "3 DATASET AND PRE-PROCESSING", "text": "The OpenMIIR dataset (Stober et al. (2015)) is a publicly available dataset of EEG recordings taken during music perception and imagination.2 We recently removed this data from known pieces during an ongoing study that so far included 10 subjects listening to and imagining 12 short fragments of music. These stimuli were selected from different genres and systematically include multiple musical dimensions such as meters, tempo and the presence of texts as shown in Table 1. In this way, different retrieval and classification scenarios can be addressed. All stimuli were normalized in volume and kept as long as possible to ensure that they all contain complete musical phrases as shown in Table 1.The pairs of recordings for the same song with and without text became tempo. The stimuli were presented by the participants in several areas."}, {"heading": "4 EXPERIMENTS", "text": "Using the EEG data set described in the previous section, we would like to learn discriminatory traits that can be used by a classifier to differentiate between different music stimuli. Ideally, these traits should also allow for interpretation by cognitive neuroscientists to facilitate insights into the underlying cognitive processes. In our previous experiments with EEG recordings of rhythm perception, CNNs showed promising classification performance, but the traits learned were not easy to interpret (Stober et al. (2014a)). To establish a baseline for the OpenMIIR data set, we first applied a clearly monitored CNN training. Then, we were able to compare the classification accuracy and the traits learned with the results obtained through our proposed feature-learning approaches. To measure classification accuracy, we used the studies of the third block of each subject as a test set. This set included 108 studies (9) (12) 4x The trial sperm were applied to the third block of each study (4)."}, {"heading": "4.1 IMPLEMENTATION AND REPRODUCIBILITY", "text": "For reproducibility and to promote further developments and research in this direction, the entire code required to build the proposed deep network structures and carry out the experiments described below will be used as open source within the deepthought project. 3 The implementation is based on the libraries Pylearn2 (Goodfellow et al. (2013) and Theano (Bergstra et al. (2010) and includes various user-defined levels and dataset classes - such as for on-the-fly generation of experimental stages and the respective classification targets during iteration. Within the framework of this paper, the following general implementation conventions apply: All conventional layers use sigmoid tanh nonlinearity because their output naturally coincides with the value range of the experimental entations ([-1.1]) and thus facilitate the interpretation of the activation values."}, {"heading": "4.2 SUPERVISED CNN TRAINING BASELINE", "text": "As a starting point, we considered CNNs with two convolutionary layers that used raw EEG input, either sampled at 64 Hz or maintained at the original sampling rate of 512 Hz. The higher rate provides better timing precision at the expense of increasing processing time and memory requirements. We wanted to know whether the use of 512 Hz would be justified by a significant increase in classification accuracy. All experiments were aborted at 6.9 s, the length of the shortest stimulus, resulting in an input length of 440 and 4866 samples, respectively. We conducted a hyperparameter grid search, optimizing only structural network parameters and learning rate. Results for selected networks can be found in Table 2.3 https: / / github.com / sstober / deepthought (code updated in paper publications) 4 Beyond the scope of this paper, confusion could be applied in the spatial or frequency range."}, {"heading": "ACKNOWLEDGMENTS", "text": "This work was supported by a fellowship within the postdoctoral program of the German Academic Exchange Service (DAAD), the Canada Excellence Research Chairs (CERC) Program, the National Sciences and Engineering Research Council (NSERC) Discovery Grant, the Ontario Early Researcher Award, and the James S. McDonells Foundation. The authors would also like to thank the study participants: Bergstra, J., Breuleux, O., Bastien, F., Lamblin, P., Pascanu, R., Desjardins, G., Turian, J., WardeFarley, D., and Bengio, Y. Theano: a CPU and GPU math expression compiler. In Proceedings of the Python for Scientific Computing Conference (SciPy), 2010.Cecotti, H. and Graser, A. Convolutional Neural Network with embedded Fourier form."}], "references": [{"title": "Theano: a CPU and GPU math expression compiler", "author": ["J. Bergstra", "O. Breuleux", "F. Bastien", "P. Lamblin", "R. Pascanu", "G. Desjardins", "J. Turian", "D. WardeFarley", "Y. Bengio"], "venue": "In Proceedings of the Python for Scientific Computing Conference (SciPy),", "citeRegEx": "Bergstra et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Bergstra et al\\.", "year": 2010}, {"title": "Convolutional Neural Network with embedded Fourier Transform for EEG classification", "author": ["H. Cecotti", "A. Gr\u00e4ser"], "venue": "In 19th International Conference on Pattern Recognition,", "citeRegEx": "Cecotti and Gr\u00e4ser,? \\Q2008\\E", "shortCiteRegEx": "Cecotti and Gr\u00e4ser", "year": 2008}, {"title": "Convolutional Neural Networks for P300 Detection with Application to Brain-Computer Interfaces", "author": ["H. Cecotti", "A. Gr\u00e4ser"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence,", "citeRegEx": "Cecotti and Gr\u00e4ser,? \\Q2011\\E", "shortCiteRegEx": "Cecotti and Gr\u00e4ser", "year": 2011}, {"title": "Pylearn2: a machine learning research library", "author": ["I.J. Goodfellow", "D. Warde-Farley", "P. Lamblin", "V. Dumoulin", "M. Mirza", "R. Pascanu", "J. Bergstra", "F. Bastien", "Y. Bengio"], "venue": null, "citeRegEx": "Goodfellow et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Goodfellow et al\\.", "year": 2013}, {"title": "Generative Adversarial Networks", "author": ["I.J. Goodfellow", "J. Pouget-Abadie", "M. Mirza", "B. Xu", "D. Warde-Farley", "S. Ozair", "A. Courville", "Y. Bengio"], "venue": null, "citeRegEx": "Goodfellow et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Goodfellow et al\\.", "year": 2014}, {"title": "MEG and EEG data analysis with MNEPython", "author": ["A. Gramfort", "M. Luessi", "E. Larson", "D.A. Engemann", "D. Strohmeier", "C. Brodbeck", "R. Goj", "M. Jas", "T. Brooks", "L. Parkkonen", "M. H\u00e4m\u00e4l\u00e4inen"], "venue": "Frontiers in Neuroscience,", "citeRegEx": "Gramfort et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Gramfort et al\\.", "year": 2013}, {"title": "Improving neural networks by preventing co-adaptation of feature detectors", "author": ["G.E. Hinton", "N. Srivastava", "A. Krizhevsky", "I. Sutskever", "R.R. Salakhutdinov"], "venue": null, "citeRegEx": "Hinton et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Hinton et al\\.", "year": 2012}, {"title": "EEG-Based Emotion Recognition Using Deep Learning Network with Principal Component Based Covariate Shift Adaptation", "author": ["S. Jirayucharoensak", "S. Pan-Ngum", "P. Israsena"], "venue": "The Scientific World Journal,", "citeRegEx": "Jirayucharoensak et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Jirayucharoensak et al\\.", "year": 2014}, {"title": "Sleep stage classification using unsupervised feature learning", "author": ["M. L\u00e4ngkvist", "L. Karlsson", "M. Loutfi"], "venue": "Advances in Artificial Neural Systems,", "citeRegEx": "L\u00e4ngkvist et al\\.,? \\Q2012\\E", "shortCiteRegEx": "L\u00e4ngkvist et al\\.", "year": 2012}, {"title": "Independent Component Analysis Using an Extended Infomax Algorithm for Mixed Subgaussian and Supergaussian Sources", "author": ["T. Lee", "M. Girolami", "T.J. Sejnowski"], "venue": "Neural Computation,", "citeRegEx": "Lee et al\\.,? \\Q1999\\E", "shortCiteRegEx": "Lee et al\\.", "year": 1999}, {"title": "Classification of patterns of EEG synchronization for seizure prediction", "author": ["P. Mirowski", "D. Madhavan", "Y. LeCun", "R. Kuzniecky"], "venue": "Clinical Neurophysiology,", "citeRegEx": "Mirowski et al\\.,? \\Q1927\\E", "shortCiteRegEx": "Mirowski et al\\.", "year": 1927}, {"title": "Deep learning for neuroimaging: a validation study", "author": ["S.M. Plis", "D.R. Hjelm", "R.Salakhutdinov", "V.D. Calhoun"], "venue": null, "citeRegEx": "Plis et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Plis et al\\.", "year": 2013}, {"title": "Practical bayesian optimization of machine learning algorithms", "author": ["J. Snoek", "H. Larochelle", "R.P. Adams"], "venue": "In Neural Information Processing Systems", "citeRegEx": "Snoek et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Snoek et al\\.", "year": 2012}, {"title": "Classifying EEG recordings of rhythm perception", "author": ["S. Stober", "D.J. Cameron", "J.A. Grahn"], "venue": "In 15th International Society for Music Information Retrieval Conference", "citeRegEx": "Stober et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Stober et al\\.", "year": 2014}, {"title": "Towards music imagery information retrieval: Introducing the openmiir dataset of eeg recordings from music perception and imagination", "author": ["S. Stober", "A Sternin", "A.M. Owen", "J.A. Grahn"], "venue": "In 16th Int. Society for Music Information Retrieval Conf", "citeRegEx": "Stober et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Stober et al\\.", "year": 2015}, {"title": "Deep Learning using Linear Support Vector Machines", "author": ["Y. Tang"], "venue": "[cs, stat],", "citeRegEx": "Tang,? \\Q2013\\E", "shortCiteRegEx": "Tang", "year": 2013}, {"title": "Modeling electroencephalography waveforms with semi-supervised deep belief nets: fast classification and anomaly measurement", "author": ["D.F. Wulsin", "J.R. Gupta", "R. Mani", "J.A. Blanco", "B. Litt"], "venue": "Journal of Neural Engineering,", "citeRegEx": "Wulsin et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Wulsin et al\\.", "year": 2011}, {"title": "EEG-based emotion classification using deep belief networks", "author": ["Zheng", "Wei-Long", "Zhu", "Jia-Yi", "Peng", "Yong", "Lu", "Bao-Liang"], "venue": "In 2014 IEEE International Conference on Multimedia and Expo (ICME), pp", "citeRegEx": "Zheng et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Zheng et al\\.", "year": 2014}], "referenceMentions": [{"referenceID": 11, "context": "In this application domain, the potential of deep learning techniques for neuroimaging has been demonstrated very recently by Plis et al. (2013) for functional and structural magnetic resonance imaging (MRI) data.", "startOffset": 126, "endOffset": 145}, {"referenceID": 7, "context": "There has also been early work on emotion recognition from EEG using deep neural networks such as described by Jirayucharoensak et al. (2014) and Zheng et al.", "startOffset": 111, "endOffset": 142}, {"referenceID": 7, "context": "There has also been early work on emotion recognition from EEG using deep neural networks such as described by Jirayucharoensak et al. (2014) and Zheng et al. (2014). In our early work, we used stacked denoising auto-encoders (SDAs) and CNNs to classify EEG recordings of rhythm perception and identify their ethnic origin \u2013 East African or Western \u2013 (Stober et al.", "startOffset": 111, "endOffset": 166}, {"referenceID": 7, "context": "There has also been early work on emotion recognition from EEG using deep neural networks such as described by Jirayucharoensak et al. (2014) and Zheng et al. (2014). In our early work, we used stacked denoising auto-encoders (SDAs) and CNNs to classify EEG recordings of rhythm perception and identify their ethnic origin \u2013 East African or Western \u2013 (Stober et al. (2014b)) as well as distinguish individual rhythms (Stober et al.", "startOffset": 111, "endOffset": 374}, {"referenceID": 7, "context": "There has also been early work on emotion recognition from EEG using deep neural networks such as described by Jirayucharoensak et al. (2014) and Zheng et al. (2014). In our early work, we used stacked denoising auto-encoders (SDAs) and CNNs to classify EEG recordings of rhythm perception and identify their ethnic origin \u2013 East African or Western \u2013 (Stober et al. (2014b)) as well as distinguish individual rhythms (Stober et al. (2014a)).", "startOffset": 111, "endOffset": 440}, {"referenceID": 13, "context": "The OpenMIIR dataset (Stober et al. (2015)) is a public domain dataset of EEG recordings taken during music perception and imagination.", "startOffset": 22, "endOffset": 43}, {"referenceID": 5, "context": "The following common-practice pre-processing steps were applied to the raw EEG and EOG data using the MNE-python toolbox by Gramfort et al. (2013) to remove unwanted artifacts.", "startOffset": 124, "endOffset": 147}, {"referenceID": 5, "context": "The following common-practice pre-processing steps were applied to the raw EEG and EOG data using the MNE-python toolbox by Gramfort et al. (2013) to remove unwanted artifacts. We removed and interpolated bad EEG channels (between 0 and 3 per subject) identified by manual visual inspection. The data was then filtered with a bandpass keeping a frequency range between 0.5 and 30Hz. This also removed any slow signal drift in the EEG. To remove artifacts caused by eye blinks, we computed independent components using extended Infomax independent component analysis (ICA) as described by Lee et al. (1999) and semi-automatically removed components that had a high correlation with the EOG channels.", "startOffset": 124, "endOffset": 606}, {"referenceID": 13, "context": "In our previous experiments with EEG recordings of rhythm perception, CNNs showed promising classification performance but the learned features were not easy to interpret (Stober et al. (2014a)).", "startOffset": 172, "endOffset": 194}, {"referenceID": 2, "context": "3 The implementation is based on the libraries Pylearn2 (Goodfellow et al. (2013)) and Theano (Bergstra et al.", "startOffset": 57, "endOffset": 82}, {"referenceID": 0, "context": "(2013)) and Theano (Bergstra et al. (2010)) and comprises various custom Layer and Dataset classes \u2013 such as for on-the-fly generation of trial tuples and the respective classification targets during iteration.", "startOffset": 20, "endOffset": 43}, {"referenceID": 0, "context": "(2013)) and Theano (Bergstra et al. (2010)) and comprises various custom Layer and Dataset classes \u2013 such as for on-the-fly generation of trial tuples and the respective classification targets during iteration. In the context of this paper, the following general implementation conventions apply: All convolutional layers use the sigmoid tanh nonlinearity because its output naturally matches the value range of the network inputs ([-1,1]) and thus facilitates easier interpretation of the activation values. Furthermore, bias terms are not used. Convolution is always solely applied along the time (samples) axis.4 For the classifiers, we use a DLSVM output layer employing the hinge loss as described by Tang (2013) with an implementation based on the one provided by Kastner.", "startOffset": 20, "endOffset": 718}, {"referenceID": 0, "context": "(2013)) and Theano (Bergstra et al. (2010)) and comprises various custom Layer and Dataset classes \u2013 such as for on-the-fly generation of trial tuples and the respective classification targets during iteration. In the context of this paper, the following general implementation conventions apply: All convolutional layers use the sigmoid tanh nonlinearity because its output naturally matches the value range of the network inputs ([-1,1]) and thus facilitates easier interpretation of the activation values. Furthermore, bias terms are not used. Convolution is always solely applied along the time (samples) axis.4 For the classifiers, we use a DLSVM output layer employing the hinge loss as described by Tang (2013) with an implementation based on the one provided by Kastner.5 This generally resulted in a better classification performance than the commonly used Softmax in all our previous experiments. For the convolutional auto-encoders, our implementation of the de-convolutional layers has been derived from the code for generative adversarial nets by Goodfellow et al. (2014).6 Stochastic gradient descent with batches of 128 trials is used for training.", "startOffset": 20, "endOffset": 1085}, {"referenceID": 0, "context": "(2013)) and Theano (Bergstra et al. (2010)) and comprises various custom Layer and Dataset classes \u2013 such as for on-the-fly generation of trial tuples and the respective classification targets during iteration. In the context of this paper, the following general implementation conventions apply: All convolutional layers use the sigmoid tanh nonlinearity because its output naturally matches the value range of the network inputs ([-1,1]) and thus facilitates easier interpretation of the activation values. Furthermore, bias terms are not used. Convolution is always solely applied along the time (samples) axis.4 For the classifiers, we use a DLSVM output layer employing the hinge loss as described by Tang (2013) with an implementation based on the one provided by Kastner.5 This generally resulted in a better classification performance than the commonly used Softmax in all our previous experiments. For the convolutional auto-encoders, our implementation of the de-convolutional layers has been derived from the code for generative adversarial nets by Goodfellow et al. (2014).6 Stochastic gradient descent with batches of 128 trials is used for training. During supervised training, we apply Dropout regularization Hinton et al. (2012) and a learning rate momentum.", "startOffset": 20, "endOffset": 1245}, {"referenceID": 0, "context": "(2013)) and Theano (Bergstra et al. (2010)) and comprises various custom Layer and Dataset classes \u2013 such as for on-the-fly generation of trial tuples and the respective classification targets during iteration. In the context of this paper, the following general implementation conventions apply: All convolutional layers use the sigmoid tanh nonlinearity because its output naturally matches the value range of the network inputs ([-1,1]) and thus facilitates easier interpretation of the activation values. Furthermore, bias terms are not used. Convolution is always solely applied along the time (samples) axis.4 For the classifiers, we use a DLSVM output layer employing the hinge loss as described by Tang (2013) with an implementation based on the one provided by Kastner.5 This generally resulted in a better classification performance than the commonly used Softmax in all our previous experiments. For the convolutional auto-encoders, our implementation of the de-convolutional layers has been derived from the code for generative adversarial nets by Goodfellow et al. (2014).6 Stochastic gradient descent with batches of 128 trials is used for training. During supervised training, we apply Dropout regularization Hinton et al. (2012) and a learning rate momentum. During unsupervised pre-training, we do not use Dropout as the expected benefit is much lower here and does not justify the increase in processing time. Generally, the learning rate is set to decay by a constant factor per epoch. Furthermore, we use a L1 weight regularization penalty term in the cost function to encourage feature sparsity. For hyper-parameter selection, we employ the Bayesian optimization technique described by Snoek et al. (2012) which has been implemented in the Spearmint library.", "startOffset": 20, "endOffset": 1727}], "year": 2017, "abstractText": "We introduce and compare several strategies for learning discriminative features from electroencephalography (EEG) recordings using deep learning techniques. EEG data are generally only available in small quantities, they are highdimensional with a poor signal-to-noise ratio, and there is considerable variability between individual subjects and recording sessions. Our proposed techniques specifically address these challenges for feature learning. Similarity-constraint encoders learn features that allow to distinguish between classes by demanding that two trials from the same class are more similar to each other than to trials from other classes. This tuple-based training approach is especially suitable for small datasets. Hydra-nets allow for separate processing pathways adapting to subsets of a dataset and thus combine the advantages of individual feature learning (better adaptation of early, low-level processing) with group model training (better generalization of higher-level processing in deeper layers). This way, models can, for instance, adapt to each subject individually to compensate for differences in spatial patterns due to anatomical differences or variance in electrode positions. The different techniques are evaluated using the publicly available OpenMIIR dataset of EEG recordings taken while participants listened to and imagined music.", "creator": "pdfLaTeX"}}}