{"id": "1511.04401", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "13-Nov-2015", "title": "Symbol Grounding Association in Multimodal Sequences with Missing Elements", "abstract": "In this paper, we extend a symbolic association framework to being able to handle missing elements in multimodal sequences. The general scope of the work is the symbolic associations of object-word mappings as it happens in language development on infants. This scenario has been long interested by Artificial Intelligence, Psychology and Neuroscience. In this work, we extend a recent approach for multimodal sequences (visual and audio) to also cope with missing elements in one or both modalities. Our approach uses two parallel Long Short-Term Memory (LSTM) networks with a learning rule based on EM-algorithm. It aligns both LSTM outputs via Dynamic Time Warping (DTW). We propose to include an extra step for the combination with max and mean operations for handling missing elements in the sequences. The intuition behind is that the combination acts as a condition selector for choosing the best representation from both LSTMs. We evaluated the proposed extension in three different scenarios: audio sequences with missing elements, visual sequences with missing elements, and sequences with missing elements in both modalities. The performance of our extension reaches better results than the original model and similar results to a unique LSTM trained in one modality, i.e., where the learning problem is less difficult.", "histories": [["v1", "Fri, 13 Nov 2015 18:59:36 GMT  (2675kb,D)", "https://arxiv.org/abs/1511.04401v1", null], ["v2", "Wed, 18 Nov 2015 15:59:02 GMT  (4601kb,D)", "http://arxiv.org/abs/1511.04401v2", "Under review as a conference paper at ICLR 2016"], ["v3", "Tue, 12 Jan 2016 11:36:59 GMT  (4763kb,D)", "http://arxiv.org/abs/1511.04401v3", "Under review as a conference paper at ICLR 2016"], ["v4", "Fri, 16 Dec 2016 14:17:02 GMT  (6178kb,D)", "http://arxiv.org/abs/1511.04401v4", null]], "reviews": [], "SUBJECTS": "cs.CV cs.CL cs.LG cs.NE", "authors": ["federico raue", "reas dengel", "thomas m breuel", "marcus liwicki"], "accepted": false, "id": "1511.04401"}, "pdf": {"name": "1511.04401.pdf", "metadata": {"source": "CRF", "title": "Symbol Grounding Association in Multimodal Sequences with Missing Elements", "authors": ["Federico Raue", "Andreas Dengel", "Thomas M. Breuel", "Marcus Liwicki"], "emails": ["federico.raue@dfki.de", "andreas.dengel@dfki.de", "tmb@cs.uni-kl.de", "liwicki@cs.uni-kl.de"], "sections": [{"heading": "1. Introduction", "text": "The idea behind it was and is not new, but the idea behind it was and is not new, and so it is not new either that it is a project in which it is a project in which it is a project in which it is a project in which it is an idea, an idea, an idea, an idea, an idea, an idea, an idea, an idea, an idea, an idea, an idea, an idea, an idea, an idea, an idea, an idea, an idea, an idea, an idea, an idea, an idea, an idea, an idea, an idea, an idea, an idea, an idea, an idea, an idea, an idea, an idea, an idea, an idea, an idea, an idea, an idea, an idea, an idea, an idea, an idea, an idea, an idea, an idea, an idea, an idea, an idea, an idea, an idea, an idea, an idea, an idea, an idea, an idea, an idea, an idea, an idea, an idea, an idea, an idea, an idea, an idea, an idea, an idea, an idea, an idea, an idea, an idea, an idea, an idea, an idea, an idea, an idea, an idea, an idea, an idea, an idea, an idea, an idea, an idea, an idea, an idea, an idea, an idea, an idea, an idea, an idea, an idea, an idea, an idea, an idea, an idea, an idea, an idea, an idea, an idea, an idea, an idea, an idea, an idea, an idea, an idea, an idea, an idea, an idea, an idea, an idea, an idea, an idea, an idea, an idea, an idea, an idea, an idea, an idea, an idea, an idea, an idea, an idea, an idea, an idea, an idea, an idea, an idea, an idea, an idea, an idea, an idea, an idea, an idea, an idea, an idea, an idea, an idea, an idea, an idea, an idea, an idea, an idea, an idea, an idea, an idea, an idea, an idea, an idea, an idea, an idea, an idea, an idea, an idea, an idea, an idea, an idea"}, {"heading": "1.1 Multimodal Tasks in Machine Learning", "text": "Multimodal feature fusion: The task is to combine features of different modalities in order to create a better feature, so the generated feature takes advantage of the best qualities of each modality. Recently, Deep Boltzmann Machines has learned how to combine different modalities in an unattended environment (Srivastava & Salakhutdinov, 2012; Sohn, Shang, & Lee, 2014). Captioning: The task is to generate a textual description of given images as input, which can be regarded as machine translation of images into captions. Convolutionary Neural Networks (CNN) in combination with LSTM have already been applied in cross-modality, which images are translated into text descriptions."}, {"heading": "2. Long Short-Term Memory (LSTM)", "text": "Long Short-Term Memory (LSTM) is a recursive neural network capable of learning long sequences without the disappearing gradient problem (Hochreiter & Schmidhuber, 1997; Hochreiter, 1998) This architecture refers to Recurrent Neural Networks \"concept of gates and memory cells created by it = \u03c3 (Wxixt + Whiht \u2212 1 + bi) (1) ft = \u03c3 (Wxfxt + Whfht \u2212 1 + bf) (2) ot = \u03c3 (Wxoxt + Whoht \u2212 1 + bo) (3) gt = tanh (Wxcxt \u2212 1 + bc) (4) ft = ftct \u2212 1 + itgt (5) ht \u2212 ot tanh (ct) (6) zt = Whzht + bz (7) where xt + Rn is the input vector at time, W \u2012 bc."}, {"heading": "3. Multimodal Symbolic Association", "text": "In fact, the fact is that most of them will be able to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight."}, {"heading": "3.1 Statistical Constraint for Semantic Binding", "text": "In this symbolic association scenario, there is an important limitation related to semantic concepts = semantic identities, which are not bound to vector representations prior to the training. (As already mentioned, the vocabulary is a set of semantic concepts s1,..., sC-SeC identity. In this sense, a series of concept vectors \u03b31,..., \u03b3C-RC-RC can be defined to apply the mapping between the semantic concepts and the output vectors. Note that two or more concepts cannot have the same representation. This component is trained in an EM-style algorithm. For explanatory purposes, it is described taking into account only one LSTM value. However, it can be applied independently to two LSTM networks. E-step states the mapping between semantic concepts in sequence and symbolic representation."}, {"heading": "3.2 Dynamic Time Warping (DTW)", "text": "This module takes advantage of the fact that both modalities represent the same semantic concept. Furthermore, within the timeline, this component converts from one modality to another modality because the monotonous behavior of the LSTM networks is one-dimensional. Furthermore, both output sequences of the CTC forward-backward training are aligned against each other on the basis of Dynamic Time Warping (DTW) (Berndt & Clifford, 1994). The DTW matrix is calculated with the following path: dist [i, j] = dist [i, j] + min DTW [i \u2212 1, j \u2212 1] DTW [i \u2212 1, j \u2212 1] DTW [i, j \u2212 1] (15), where dist [i, j] is the euclidean distance between the output vectors to the timepi of LSTMv and the timeppep j of LSTMa."}, {"heading": "4. Handling Missing Elements", "text": "In this paper, we are interested in the multimodal association inspired by the symbol tolerance problem in the event that the multimodal sequences have some of the semantic concepts shared between modalities, but not all of them. Therefore, we have updated the problem definition (cf. section 3). In addition, each sequence input is associated with a semantic sequence Sv = {s1,.., t2, where v, a represent the visual and audiovisual modalities, t1, t2 represent the time step of each sequence. Furthermore, each sequence input is associated with a semantic sequence Sv = {s1,.., sk} and Sa = {s1,., sp} where Sv, s1,., sg} 1 \u2264 g, min (k, p). As a result, the semantic sequence Sv = 2, the semantic sequence Sv and Sa have some semantic concepts shared between the modalities."}, {"heading": "5. Experimental Design", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "5.1 Datasets", "text": "We created several multimodal datasets in which the elements of the sequence are missing in one or both modalities, but the relative sequence between the elements is the same. For example, a visual semantic concept sequence can be represented by a line of text in the digits \"24 7\" and an audiosemantic concept sequence by \"two seven.\" In this case, we started from a simplified scenario of symbol degradation in which the continuity of semantic concepts is different on each modality. The visual component is a horizontal arrangement of isolated objects, and the audio component is a spoken semantic concept of some elements of the visual component, and vice versa. We want to point out that the visual component is similar to a panorama view. The procedure for generalizing the multimodal datasets is explainable. Generative sequences: Two scenarios are considered for the semantic sequences: missing elements in both modalities and modalities in one modality."}, {"heading": "5.2 Input Features and LSTM setup", "text": "The audio representation is a vector of 123 components: a Fourier filter bank with 40 coefficients (plus energy), including the first and second derivatives. Afterwards, all audiovisual and audiovisual components were normalized to have a mean and standard deviation of zero. Furthermore, the proposed extension was compared with the original model in (Raue et al., 2015). Furthermore, we compared the extension against LSTM with the CTC layer and a predefined encoding scheme. The parameters of the visual LSTM were: 40 memory cells, learning rate 1e-4 and dynamics 0.9. On the other hand, the audio LSTM had 100 memory cells, and the learning rate and dynamics are the same as in the visual LSTM. Furthermore, the learning rate in the statistical constraint was set at 0.001."}, {"heading": "6. Results and Discussion", "text": "This year, it has reached the stage where it will be able to take the lead."}, {"heading": "7. Conclusions", "text": "In summary, we have presented a solution to the symbol tolerance problem for the object-word association problem, based on multimodal sequences (image and sound) in which the semantic elements can be represented in one or both modalities. Further work is planned for more realistic scenarios in which the visual component is not clearly segmentable. Furthermore, we are interested in expanding the word-association problem between two-dimensional image and language. In this sense, we will include visual attention mechanisms in synchronization with language. Finally, the development of human language depends on how abstract concepts are linked to the real world through sensory input, and the scenario of the symbol tolerance problem can be regarded as simple. However, many questions remain open (Needham, Santos, Magee, Devin, Hogg & Cohn, 2005; Steels, 2008)."}], "references": [{"title": "The impact of input: language acquisition in the visually impaired", "author": ["E.S. Andersen", "A. Dunlea", "L. Kekelis"], "venue": "First Language,", "citeRegEx": "Andersen et al\\.,? \\Q1993\\E", "shortCiteRegEx": "Andersen et al\\.", "year": 1993}, {"title": "Using Dynamic Time Warping to Find Patterns in Time", "author": ["D.J. Berndt", "J. Clifford"], "venue": null, "citeRegEx": "Berndt and Clifford,? \\Q1994\\E", "shortCiteRegEx": "Berndt and Clifford", "year": 1994}, {"title": "High-performance ocr for printed english and fraktur using lstm networks", "author": ["T. Breuel", "A. Ul-Hasan", "M. Al-Azawi", "F. Shafait"], "venue": "In Document Analysis and Recognition (ICDAR),", "citeRegEx": "Breuel et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Breuel et al\\.", "year": 2013}, {"title": "Scene labeling with lstm recurrent neural networks", "author": ["W. Byeon", "T.M. Breuel", "F. Raue", "M. Liwicki"], "venue": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,", "citeRegEx": "Byeon et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Byeon et al\\.", "year": 2015}, {"title": "Shape and the first hundred nouns", "author": ["L. Gershkoff-Stowe", "L.B. Smith"], "venue": "Child development,", "citeRegEx": "Gershkoff.Stowe and Smith,? \\Q2004\\E", "shortCiteRegEx": "Gershkoff.Stowe and Smith", "year": 2004}, {"title": "Connectionist temporal classification", "author": ["A. Graves", "S. Fern\u00e1ndez", "F. Gomez", "J. Schmidhuber"], "venue": "In Proceedings of the 23rd international conference on Machine learning - ICML", "citeRegEx": "Graves et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Graves et al\\.", "year": 2006}, {"title": "The symbol grounding problem", "author": ["S. Harnad"], "venue": "Physica D: Nonlinear Phenomena,", "citeRegEx": "Harnad,? \\Q1990\\E", "shortCiteRegEx": "Harnad", "year": 1990}, {"title": "The Vanishing Gradient Problem During Learning Recurrent Neural Nets and Problem Solutions", "author": ["S. Hochreiter"], "venue": "International Journal of Uncertainty, Fuzziness and Knowledge-Based Systems,", "citeRegEx": "Hochreiter,? \\Q1998\\E", "shortCiteRegEx": "Hochreiter", "year": 1998}, {"title": "Long Short-Term Memory", "author": ["S. Hochreiter", "J. Schmidhuber"], "venue": "Neural Computation,", "citeRegEx": "Hochreiter and Schmidhuber,? \\Q1997\\E", "shortCiteRegEx": "Hochreiter and Schmidhuber", "year": 1997}, {"title": "Deep fragment embeddings for bidirectional image sentence mapping", "author": ["A. Karpathy", "A. Joulin", "F.F.F. Li"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Karpathy et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Karpathy et al\\.", "year": 2014}, {"title": "Grounding of word meanings in latent dirichlet allocation-based multimodal concepts", "author": ["T. Nakamura", "T. Araki", "T. Nagai", "N. Iwahashi"], "venue": "Advanced Robotics,", "citeRegEx": "Nakamura et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Nakamura et al\\.", "year": 2011}, {"title": "Protocols from perceptual observations", "author": ["C.J. Needham", "P.E. Santos", "D.R. Magee", "V. Devin", "D.C. Hogg", "A.G. Cohn"], "venue": "Artificial Intelligence,", "citeRegEx": "Needham et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Needham et al\\.", "year": 2005}, {"title": "Columbia object image library (coil-100)", "author": ["S. Nene", "S. Nayar", "H. Murase"], "venue": "Tech. rep.", "citeRegEx": "Nene et al\\.,? \\Q1996\\E", "shortCiteRegEx": "Nene et al\\.", "year": 1996}, {"title": "A tutorial on hidden markov models and selected applications in speech recognition", "author": ["L.R. Rabiner"], "venue": "Proceedings of the IEEE,", "citeRegEx": "Rabiner,? \\Q1989\\E", "shortCiteRegEx": "Rabiner", "year": 1989}, {"title": "Symbol Grounding in Multimodal Sequences using Recurrent Neural Network", "author": ["F. Raue", "W. Byeon", "T. Breuel", "M. Liwicki"], "venue": "In Workshop Cognitive Computation: Integrating Neural and Symbolic Approaches at NIPS", "citeRegEx": "Raue et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Raue et al\\.", "year": 2015}, {"title": "Improved multimodal deep learning with variation of information", "author": ["K. Sohn", "W. Shang", "H. Lee"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Sohn et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Sohn et al\\.", "year": 2014}, {"title": "Looking without listening: is audition a prerequisite for normal development of visual attention during infancy", "author": ["P.E. Spencer"], "venue": "Journal of deaf studies and deaf education,", "citeRegEx": "Spencer,? \\Q2000\\E", "shortCiteRegEx": "Spencer", "year": 2000}, {"title": "Multimodal learning with deep boltzmann machines", "author": ["N. Srivastava", "R.R. Salakhutdinov"], "venue": "In Advances in neural information processing systems,", "citeRegEx": "Srivastava and Salakhutdinov,? \\Q2012\\E", "shortCiteRegEx": "Srivastava and Salakhutdinov", "year": 2012}, {"title": "The symbol grounding problem has been solved, so whats next ?. Symbols, Embodiment and Meaning", "author": ["L. Steels"], "venue": null, "citeRegEx": "Steels,? \\Q2008\\E", "shortCiteRegEx": "Steels", "year": 2008}, {"title": "Sequence to sequence learning with neural networks. In Advances in neural information processing", "author": ["I. Sutskever", "O. Vinyals", "Q.V. Le"], "venue": null, "citeRegEx": "Sutskever et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Sutskever et al\\.", "year": 2014}, {"title": "Show and tell: A neural image caption generator", "author": ["O. Vinyals", "A. Toshev", "S. Bengio", "D. Erhan"], "venue": "arXiv preprint arXiv:1411.4555", "citeRegEx": "Vinyals et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Vinyals et al\\.", "year": 2014}, {"title": "Backpropagation through time: what it does and how to do it", "author": ["P.J. Werbos"], "venue": "Proceedings of the IEEE,", "citeRegEx": "Werbos,? \\Q1990\\E", "shortCiteRegEx": "Werbos", "year": 1990}, {"title": "A multimodal learning interface for grounding spoken language in sensory perceptions", "author": ["C. Yu", "D.H. Ballard"], "venue": "ACM Transactions on Applied Perception (TAP),", "citeRegEx": "Yu and Ballard,? \\Q2004\\E", "shortCiteRegEx": "Yu and Ballard", "year": 2004}], "referenceMentions": [{"referenceID": 6, "context": "This scenario is known as Symbol Grounding Problem (Harnad, 1990) and is still an open problem (Steels, 2008).", "startOffset": 51, "endOffset": 65}, {"referenceID": 18, "context": "This scenario is known as Symbol Grounding Problem (Harnad, 1990) and is still an open problem (Steels, 2008).", "startOffset": 95, "endOffset": 109}, {"referenceID": 4, "context": "Gershkoff-Stowe and Smith (2004) found the initial", "startOffset": 0, "endOffset": 33}, {"referenceID": 16, "context": "In contrast, the language development can be limited by the lack of stimulus (Andersen, Dunlea, & Kekelis, 1993; Spencer, 2000), i.", "startOffset": 77, "endOffset": 127}, {"referenceID": 14, "context": "In contrast, the language development can be limited by the lack of stimulus (Andersen, Dunlea, & Kekelis, 1993; Spencer, 2000), i.e., deafness, blindness. Asano et al. found two different patterns in the brain activity of infants depending on the semantic correctness between a visual and an audio stimulus. In simpler terms, the brain activity is pattern \u2018A\u2019 if the visual and audio signals represent the same semantic concept. Otherwise, the pattern is \u2018B\u2019. Related work has been proposed in different multimodal scenarios inspired by the Symbol Grounding Problem. Yu and Ballard (2004) explored a framework that learns the association between objects and their spoken names in day-to-day tasks.", "startOffset": 113, "endOffset": 590}, {"referenceID": 10, "context": "Nakamura et al. (2011) introduced a multimodal categorization applied to robotics.", "startOffset": 0, "endOffset": 23}, {"referenceID": 10, "context": "Nakamura et al. (2011) introduced a multimodal categorization applied to robotics. Their framework exploited the relation of concepts in different modalities (visual, audio and haptic) using a Multimodal latent Dirichlet allocation. Previous approaches has focused on feature engineering, and the segmentation and the classification tasks are considered as independent modules. This paper is focusing on a model for segmentation and classification tasks in the objectword association scenario. Moreover, we are interested in multimodal sequences that represent a semantic concept sequence with the constraint that not all elements can be on both modalities. For instance, one modality sequence (text lines of digits) is represented by \u20182 4 5\u2019, and the other modality (spoken words) is represented by \u2018four six five\u2019. Also, the association problem is different from the traditional setup where the association is fixed via a pre-defined coding scheme of the classes (e.g. 1-of-K scheme) before training. We explain the difference between common approaches for multimodal machine learning and our problem setup in Section 1.1. In this work, we investigate the benefits of exploiting the alignment between elements that are common in the multimodal sequence and still agree in a similar representation via coding scheme. Note that our work is an extension of Raue et al. (2015) where both modalities represent the same semantic sequence (no missing elements).", "startOffset": 0, "endOffset": 1375}, {"referenceID": 10, "context": "Nakamura et al. (2011) introduced a multimodal categorization applied to robotics. Their framework exploited the relation of concepts in different modalities (visual, audio and haptic) using a Multimodal latent Dirichlet allocation. Previous approaches has focused on feature engineering, and the segmentation and the classification tasks are considered as independent modules. This paper is focusing on a model for segmentation and classification tasks in the objectword association scenario. Moreover, we are interested in multimodal sequences that represent a semantic concept sequence with the constraint that not all elements can be on both modalities. For instance, one modality sequence (text lines of digits) is represented by \u20182 4 5\u2019, and the other modality (spoken words) is represented by \u2018four six five\u2019. Also, the association problem is different from the traditional setup where the association is fixed via a pre-defined coding scheme of the classes (e.g. 1-of-K scheme) before training. We explain the difference between common approaches for multimodal machine learning and our problem setup in Section 1.1. In this work, we investigate the benefits of exploiting the alignment between elements that are common in the multimodal sequence and still agree in a similar representation via coding scheme. Note that our work is an extension of Raue et al. (2015) where both modalities represent the same semantic sequence (no missing elements). Similarly to Raue et al. (2015), the model was implemented by two Long Short-Term Memories (LSTMs) that their output vectors were aligned in the time axis using Dynamic Time Warping (DTW) (Berndt & Clifford, 1994).", "startOffset": 0, "endOffset": 1489}, {"referenceID": 14, "context": "In both cases, our model performances better that the model proposed by Raue et al. (2015).", "startOffset": 72, "endOffset": 91}, {"referenceID": 7, "context": "Long Short-Term Memory (LSTM) is a recurrent neural network, which is capable to learn long sequences without the vanishing gradient problem (Hochreiter & Schmidhuber, 1997; Hochreiter, 1998).", "startOffset": 141, "endOffset": 191}, {"referenceID": 9, "context": "LSTM has been succesfully applied to several scenarios, such as, image captioning (Karpathy et al., 2014), texture classification (Byeon, Breuel, Raue, & Liwicki, 2015), and machine translation (Sutskever, Vinyals, & Le, 2014).", "startOffset": 82, "endOffset": 105}, {"referenceID": 13, "context": "One sequence is LSTM output vectors, and the other sequence is obtained by a forward-backward propagation of probabilities similar to Hidden Markov Models (HMM) (Rabiner, 1989).", "startOffset": 161, "endOffset": 176}, {"referenceID": 5, "context": "In more detail, Graves et al. (2006) introduced Connectionist Temporal Classification (CTC).", "startOffset": 16, "endOffset": 37}, {"referenceID": 21, "context": "An LSTM is trained by Backpropagation Trough Time (BPTT) (Werbos, 1990), and the loss function is defined by", "startOffset": 57, "endOffset": 71}, {"referenceID": 5, "context": "Please refer to the original paper for more information (Graves et al., 2006).", "startOffset": 56, "endOffset": 77}, {"referenceID": 14, "context": "In Section 1, we mentioned that this work is an extension of Raue et al. (2015). They have introduced a Symbolic Association scenario where their model learns to associate multimodal sequences and learns the semantic binding between Semantic Concepts (SeC ) and vectorial representation (SyF ).", "startOffset": 61, "endOffset": 80}, {"referenceID": 14, "context": "Figure 2: General overview of the original model proposed by Raue et al. (2015) and the contributions of this paper.", "startOffset": 61, "endOffset": 80}, {"referenceID": 14, "context": "In addition, the proposed extension was compared against the original model in (Raue et al., 2015).", "startOffset": 79, "endOffset": 98}, {"referenceID": 14, "context": "00 Original Model (Raue et al. (2015)) 12.", "startOffset": 19, "endOffset": 38}, {"referenceID": 18, "context": "However, many questions remain still open (Needham, Santos, Magee, Devin, Hogg, & Cohn, 2005; Steels, 2008).", "startOffset": 42, "endOffset": 107}], "year": 2016, "abstractText": "In this paper, we extend a symbolic association framework to being able to handle missing elements in multimodal sequences. The general scope of the work is the symbolic associations of object-word mappings as it happens in language development on infants. In other words, two different representations of the same abstract concepts can be associated in both directions. This scenario has been long interested in Artificial Intelligence, Psychology, and Neuroscience. In this work, we extend a recent approach for multimodal sequences (visual and audio) to also cope with missing elements in one or both modalities. Our approach uses two parallel Long Short-Term Memories (LSTMs) with a learning rule based on EM-algorithm. It aligns both LSTM outputs via Dynamic Time Warping (DTW). We propose to include an extra step for the combination with max operation for exploiting the common elements between both sequences. The intuition behind is that the combination acts as a condition selector for choosing the best representation from both LSTMs. We evaluated the proposed extension in the following scenarios: missing elements in one modality (visual or audio) and missing elements in both modalities (visual and audio). The performance of our extension reaches better results than the original model and similar results to individual LSTM trained in each modality.", "creator": "TeX"}}}