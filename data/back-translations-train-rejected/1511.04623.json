{"id": "1511.04623", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "14-Nov-2015", "title": "Learning to Represent Words in Context with Multilingual Supervision", "abstract": "We present a neural network architecture based on bidirectional LSTMs to compute representations of words in the sentential contexts. These context-sensitive word representations are suitable for, e.g., distinguishing different word senses and other context-modulated variations in meaning. To learn the parameters of our model, we use cross-lingual supervision, hypothesizing that a good representation of a word in context will be one that is sufficient for selecting the correct translation into a second language. We evaluate the quality of our representations as features in three downstream tasks: prediction of semantic supersenses (which assign nouns and verbs into a few dozen semantic classes), low resource machine translation, and a lexical substitution task, and obtain state-of-the-art results on all of these.", "histories": [["v1", "Sat, 14 Nov 2015 21:36:38 GMT  (109kb)", "https://arxiv.org/abs/1511.04623v1", null], ["v2", "Thu, 19 Nov 2015 23:35:42 GMT  (190kb)", "http://arxiv.org/abs/1511.04623v2", null]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["kazuya kawakami", "chris dyer"], "accepted": false, "id": "1511.04623"}, "pdf": {"name": "1511.04623.pdf", "metadata": {"source": "CRF", "title": null, "authors": [], "emails": ["cdyer}@cs.cmu.edu"], "sections": [{"heading": null, "text": "ar Xiv: 151 1.04 623v 2 [cs.C L] 19 Nov 2We present a neural network architecture based on bidirectional LSTMs to calculate word representations in sentence context. These context-sensitive word representations are suitable, for example, for distinguishing between different sense of words and other context-modulated variations of meaning. To learn the parameters of our model, we use linguistic monitoring and assume that a good representation of a word in context is sufficient to select the correct translation into a second language. We evaluate the quality of our representations as features in three downstream tasks: predicting semantic supersense (assigning nouns and verbs to a few dozen semantic classes), machine translation with few resources, and a lexical substitution task, obtaining state-of-the-art results in all of these."}, {"heading": "1 INTRODUCTION", "text": "In fact, the value of such representations is due to their ability to grasp intuitive concepts such as syntactical and semantic similarity as geometric locality. Despite their empirically proven significance as a source of properties in many downstream fields of application (Turian et al., 2010), the \"one word type, one vector\" formed by most word representation models, it is problematic that words have multiple meanings. Two standard solutions to this problem exist to treat each word as a collection of discrete, mutually exclusive senses presented as vectors (Tian et al., 2014; Neelakantan et al., 2015; Huang et al, 2015)."}, {"heading": "2 MODEL", "text": "Our contextual word model is a bidirectional sequence model based on recursive neural networks (Chan et al., 2015; Bahdanau et al., 2014; 2015, et al.). Intuitively, this model allows us to condition dependencies of any length while implicitly referring to local contexts. Let w = (w1, w2,.., wn) use the words in a sentence with the length n. We also project all words into a fixed d-dimensional vector x = (x1, x2,., xn) using a (one-word-per-type) word search table. The model encodes each symbol of the sentence from left to right according to the standard short-term memory recurrences: it = \u03c3 (Wxixt + Whiht \u2212 1 + Wcict) ft = battle-challenged (Wxfxt \u2212 Whfht \u2212 1 + bf) ft = This word can be repeated in this context + Wxx."}, {"heading": "2.1 MODEL INTUITION", "text": "To get a representation of a word in its context, we need to use functions that mask or scale some dimensions of the vector according to its context. Functions that use the same scaling function, even the word and context, are different, average and multilayered perception for example, may not be suitable. The input gate in long-term memory is considered a suitable scaling function that accommodates the target word and its context (xt, ht, ct \u2212 1). Figure 1 shows a simplified version of the operation to modulate a meaning (vegetable plant) from an ambiguous type vector with semantic mask conditioned to word and context."}, {"heading": "3 MEANING AND TRANSLATION", "text": "The question we want to answer is: What is a suitable substitute (or \"grounding\") for averaging words in the context that we can use to construct word representations at the token level (and not at the type level)? To illustrate the problem we want to solve, consider the meaning of the token bank in the following sentences: \u2022 I went to the bank to deposit my paycheck. \u2022 I went to the river bank to eat something. A very productive strategy for learning semantic word embeddings is to rely on the distribution hypothesis (Harris, 1954), according to which semantically similar objects occur in similar contexts. \u2022 The distribution hypothesis is also convenient because it allows to learn semantic words from large, uncommented text bodies. Despite the empirical success of the distribution hypothesis in creating word types, generating a representation of words in a context that is both potentially rich in context and in context is appropriate."}, {"heading": "3.1 OBJECTIVE & PARAMETER LEARNING", "text": "To operationalize our hypothesis that translation provides a good monitoring signal for learning semantic representations, we learn the parameters of source-language word type embedding and the composition function (i.e., the parameters of bidirectional LSTMs) by using the calculated representation to calculate the lexical translation probability of a word in context; that is, we use the calculated token embedding to define a probability estimate that a source-language word et in the context c = (e1,...) et \u2212 1, et + 1., en) will be translated into a second language, as f in the vocabulary F. i.e. p (f | et, c). 1For example, Mikolv et al. (2013) showed that short multiword expressions could be embedded using the \"broader\" context in which they occur."}, {"heading": "3.2 PARAMETER LEARNING", "text": "The model parameters W and b as well as the word projection parameters Ve are first pre-trained with the objective function: L = \u2212 \u2211 (f, e) log p (f | e, c) That is, we want to find the parameters that maximize the probability of the lexical translation protocol over the entire parallel corpus of lexical translations (f) of a source word (e) in context (c). If we want to transfer the model to another monitored task to predict the designation s \u00b2 S for a word e in context c, we can transfer the final values of the W and b parameters and formulate a similar model to predict designations. On the basis of the transformation matrix S \u00b2 R | S | \u00d7 dh and the distortions b \u00b2 R | S |, we can determine the label probability asu \u00b2 = Sht + b \u00b2 p (s | et, c) = exp (u \u00b2 s \u00b2 s \u00b2)."}, {"heading": "4 EXPERIMENTS", "text": "We will now turn to a series of experiments to demonstrate the value of learning word representations in context according to the aforementioned goal. Our paradigm will be to prepare the use of the lens over the parameters of a word-in-context model and then use it (without further fine-tuning) for downstream tasks: predicting semantic supersensuality (assigning nouns and verbs to a few dozen semantic classes), machine translation with limited resources, and a lexical substitution task."}, {"heading": "4.1 MODEL CONFIGURATION AND PRE-TRAINING", "text": "To prepare our model, we extracted words (s) into contexts and translations (f) from the Europerl parallel corpus (Koehn, 2005), which are typologically quite diverse. Table 1 shows the number of parallel sentences and the number of words. For each language pair, we used 2000 sentences for development, the rest was used for training. After normal tokenization, we obtained alignments using a quick alignment tool (Dyer et al., 2013). Because we model individual word translations and want high-quality training instances, we perform the alignment model in both directions and achieve symmetrical alignments by taking intersections between forward and backward alignments. To control the size of the vocabulary, we used 30,000 most common words. For target languages, we removed 10 most common words. The words not included in the vocabularies are replaced by uniform units. < < All introductory units are replaced by 0.010 units."}, {"heading": "4.2 SUPERSENSE TAGGING", "text": "This means that the number of word sensors tends to be too numerous for existing models to generalize properly with the small amount of data available, and the supersenses address this problem by incorporating all the senses into a tractable set of tags. Table 4.2 shows examples of supersensors and their definition, which are commonly used in semantically oriented downstream tasks such as co-reference resolution (O're & Heilman, 2013) and answering questions (Pasca & Harabagiu, 2001)."}, {"heading": "4.3 LEXICAL TRANSLATION IN LOW RESOURCE LANGUAGE", "text": "Since languages with limited resources do not have enough data to adequately estimate the translation probabilities, we hope that we can learn more effective mappings with pre-trained word-in-context embeddings (Chahuneau et al., 2013). We have developed a lexical translation model that predicts the translation of aligned English sentences, for languages with limited resources, malagasy and urdu in addition to the pre-trained word in the context model. Table 1 shows the number of parallel sentences and the number of words. We used a data set (Dou et al., 2014) for malagasy and the Urdu data we used are part of the NIST-MT evaluation in 2008-20122. We used 2000 sentences to develop and perform tests. We have filtered out sentences that have less than 3 words for preschool and words occur less than once, are translated by Intoflunk > We have our system synchronized with IRC (translation system)."}, {"heading": "4.4 LEXICAL SUBSTITUTION", "text": "The task was introduced in SemEval-2007 (McCarthy & Navigli, 2007) and involves both locating the synonyms and disambiguating the context. As such, it is an ideal test case for our presentations. Models are evaluated for their ability to predict the substitutions in the gold standard of the LS-SE test set. We evaluated our model against a task that evaluates the quality of best2https: / / katalog.ldc.upenn.edu / LDC2010T21 predictions. The initial task allows multiple predictions to be made, but we only say one substitution according to (Melamud et al., 2015) This task is challenging as it requires the best substitutions from the entire vocabulary. The way of making predictions is the following. Given a target and its context, we deduce a word in the context of the representation of all possible French words."}, {"heading": "5 RESULT", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "5.1 SUPERSENSE TAGGING", "text": "Our bidirectional LSTM model (Bi-LSTM) exceeded the first sensual heuristic baseline, the Hidden Markov model with perceptron training (Ciaramita & Altun) proposed in Ciaramita & Altun, 2006, and our new word-in-context pre-training model resulted in further improvements in all language pairs. The average of 4 linguistically trained models, as in the Bi-LSTM (average), shows significant improvements over Bi-LSTM. The German-trained model achieved the best result F1 84.1 on Senseval3. Furthermore, the neural network base model exceeds existing baselines even without linguistic monitoring. 3It can lead to an F score that does not lie between precision and memory."}, {"heading": "5.2 LEXICAL TRANSLATION IN LOW RESOURCE LANGUAGE", "text": "Table 4 shows the results of machine translation in a language with few resources. We report on the average BLEU score of 5 passes to avoid optimizer randomness Clark et al. (2011). The result shows a significant improvement in helplessness and a consistent improvement over BLEU in all language pairs. The average score of 4 linguistically trained models improved the helplessness by about 3 points and the BLEU score by 0.3."}, {"heading": "5.3 LEXICAL SUBSTITUTION", "text": "Table 5 shows results on the lexical substitution task. Since our word-in-context representations are based only on Europerl parallel corpora, the base system is the Skipgram word embedding, which is trained on the English side of the EN-FR parallel corpora, which is the largest in the corpus. The Skipgram model, which uses the most similar word as a prediction, is the context-sensitive baseline. In addition, we compared our results with various context-sensitive models, which take arithmetic averages (as in Add and BalAdd) and a geometric mean (as in Mult and BalMult) of embedding, proposed by (Melamud et al., 2015). They trained their baseline embedding (as in Base) on a two billion word web corpus, ukWaC (Ferraresi et al., 2008).The model achieved the best measurements 10.63, the best measurement mode we achieved in comparison with the Czech (18.33) and the second-best result was achieved in the Czech context."}, {"heading": "6 DISCUSSION", "text": "We proposed the model to predict the lexical translation in order to build a word-in-context representation. Table 6 shows an example of disambiguation with the translation model in order of invaluability of the translation. The model correctly distinguishes industrial plants (usine in French) and vegetable plants (plantes in French). Figure 3 shows the effect of a pre-trained word-in-context representation for downstream tasks. Pre-trained models start at the first update with little confusion and converge earlier, in two epochs, for machine translation with low resources. The evaluation was done using a script provided by the task organizer. We investigated the effect of 4 linguistically diverse languages. The results show the benefit of cross-language pre-training in all languages, but overall the German-trained model shows stable results and the Finnish-trained model tends to underperform others, especially in lexical substitution tasks where we do not have a fine tuning process."}, {"heading": "7 RELATED WORK", "text": "Distributed word representations have been successfully applied to several downstream tasks such as chunking, parsing, sentiment analysis, and paraphrase detection. Most tasks require not only word representation, but the representation of phrases or documents. In previous work, many architectures have been proposed to learn and use word representation. In sequence modeling of problems such as BIO chunking, conditional random fields and recursive neural networks are used to represent a sequence of word representations (Turian et al., 2010; Mesnil et al., 2013). For classification tasks such as document classification, sentiment analysis, paraphrase recognition, word embedding summation (Lauly et al., 2014), evolutionary neural word networks (Kalchbrenner et al., 2014), and recursive networks (Socher et al., 2013; Cheng & Kartsaklis, 2015) have been proposed to represent the composition of words."}], "references": [{"title": "Neural machine translation by jointly learning to align and translate", "author": ["Bahdanau", "Dzmitry", "Cho", "Kyunghyun", "Bengio", "Yoshua"], "venue": "arXiv preprint arXiv:1409.0473,", "citeRegEx": "Bahdanau et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2014}, {"title": "End-to-end attention-based large vocabulary speech recognition", "author": ["Bahdanau", "Dzmitry", "Chorowski", "Jan", "Serdyuk", "Dmitriy", "Brakel", "Phil\u00e9mon", "Bengio", "Yoshua"], "venue": "CoRR, abs/1508.04395,", "citeRegEx": "Bahdanau et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2015}, {"title": "Translating into morphologically rich languages with synthetic phrases", "author": ["Chahuneau", "Victor", "Schlinger", "Eva", "Smith", "Noah A", "Dyer", "Chris"], "venue": "In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing,", "citeRegEx": "Chahuneau et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Chahuneau et al\\.", "year": 2013}, {"title": "Listen, attend, and spell", "author": ["Chan", "William", "Jaitly", "Navdeep", "Le", "Quoc V", "Vinyals", "Oriol"], "venue": "CoRR, abs/1508.01211,", "citeRegEx": "Chan et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Chan et al\\.", "year": 2015}, {"title": "Syntax-aware multi-sense word embeddings for deep compositional models of meaning", "author": ["Cheng", "Jianpeng", "Kartsaklis", "Dimitri"], "venue": "arXiv preprint arXiv:1508.02354,", "citeRegEx": "Cheng et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Cheng et al\\.", "year": 2015}, {"title": "Broad-coverage sense disambiguation and information extraction with a supersense sequence tagger", "author": ["Ciaramita", "Massimiliano", "Altun", "Yasemin"], "venue": "In Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing,", "citeRegEx": "Ciaramita et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Ciaramita et al\\.", "year": 2006}, {"title": "Better hypothesis testing for statistical machine translation: Controlling for optimizer instability", "author": ["Clark", "Jonathan H", "Dyer", "Chris", "Lavie", "Alon", "Smith", "Noah A"], "venue": "In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies: short papers-Volume", "citeRegEx": "Clark et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Clark et al\\.", "year": 2011}, {"title": "Natural language processing (almost) from scratch", "author": ["Collobert", "Ronan", "Weston", "Jason", "Bottou", "L\u00e9on", "Karlen", "Michael", "Kavukcuoglu", "Koray", "Kuksa", "Pavel"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "Collobert et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Collobert et al\\.", "year": 2011}, {"title": "Trans-gram, fast cross-lingual word-embeddings", "author": ["Coulmance", "Jocelyn", "Marty", "Jean-Marc", "Wenzek", "Guillaume", "Benhalloum", "Amine"], "venue": "In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing,", "citeRegEx": "Coulmance et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Coulmance et al\\.", "year": 2015}, {"title": "Ultraconservative online algorithms for multiclass problems", "author": ["Crammer", "Koby", "Singer", "Yoram"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "Crammer et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Crammer et al\\.", "year": 2003}, {"title": "Meaning in language. An introduction to Semantics and Pragmatics", "author": ["Cruse", "Alan"], "venue": null, "citeRegEx": "Cruse and Alan.,? \\Q2000\\E", "shortCiteRegEx": "Cruse and Alan.", "year": 2000}, {"title": "Two step CCA: A new spectral method for estimating vector models of words", "author": ["Dhillon", "Paramveer S", "Rodu", "Jordan", "Foster", "Dean P", "Ungar", "Lyle H"], "venue": "In Proc. ICML,", "citeRegEx": "Dhillon et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Dhillon et al\\.", "year": 2012}, {"title": "Word sense disambiguation within a multilingual framework", "author": ["Diab", "Mona Talat"], "venue": "PhD thesis, University of Maryland,", "citeRegEx": "Diab and Talat.,? \\Q2003\\E", "shortCiteRegEx": "Diab and Talat.", "year": 2003}, {"title": "Beyond parallel data: Joint word alignment and decipherment improves machine translation", "author": ["Dou", "Qing", "Vaswani", "Ashish", "Knight", "Kevin"], "venue": "In Proceedings of EMNLP,", "citeRegEx": "Dou et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Dou et al\\.", "year": 2014}, {"title": "cdec: A decoder, alignment, and learning framework for finite-state and context-free translation models", "author": ["Dyer", "Chris", "Weese", "Jonathan", "Setiawan", "Hendra", "Lopez", "Adam", "Ture", "Ferhan", "Eidelman", "Vladimir", "Ganitkevitch", "Juri", "Blunsom", "Phil", "Resnik", "Philip"], "venue": "In Proceedings of the ACL 2010 System Demonstrations,", "citeRegEx": "Dyer et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Dyer et al\\.", "year": 2010}, {"title": "A simple, fast, and effective reparameterization of IBM model 2", "author": ["Dyer", "Chris", "Chahuneau", "Victor", "Smith", "Noah A"], "venue": "In Proc. NAACL,", "citeRegEx": "Dyer et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Dyer et al\\.", "year": 2013}, {"title": "A structured vector space model for word meaning in context", "author": ["Erk", "Katrin", "Pad\u00f3", "Sebastian"], "venue": "In Proc. EMNLP,", "citeRegEx": "Erk et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Erk et al\\.", "year": 2008}, {"title": "Measuring word meaning in context", "author": ["Erk", "Katrin", "McCarthy", "Diana", "Gaylord", "Nicholas"], "venue": "Computational Linguistics,", "citeRegEx": "Erk et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Erk et al\\.", "year": 2013}, {"title": "Improving vector space word representations using multilingual correlation", "author": ["Faruqui", "Manaal", "Dyer", "Chris"], "venue": "In Proceedings of EACL,", "citeRegEx": "Faruqui et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Faruqui et al\\.", "year": 2014}, {"title": "Introducing and evaluating ukwac, a very large web-derived corpus of english", "author": ["Ferraresi", "Adriano", "Zanchetta", "Eros", "Baroni", "Marco", "Bernardini", "Silvia"], "venue": "In Proceedings of the 4th Web as Corpus Workshop (WAC-4) Can we beat Google,", "citeRegEx": "Ferraresi et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Ferraresi et al\\.", "year": 2008}, {"title": "Understanding the difficulty of training deep feedforward neural networks", "author": ["Glorot", "Xavier", "Bengio", "Yoshua"], "venue": "In International conference on artificial intelligence and statistics,", "citeRegEx": "Glorot et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Glorot et al\\.", "year": 2010}, {"title": "Multilingual distributed representations without word alignment", "author": ["Hermann", "Karl Moritz", "Blunsom", "Phil"], "venue": "arXiv preprint arXiv:1312.6173,", "citeRegEx": "Hermann et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Hermann et al\\.", "year": 2013}, {"title": "Multilingual models for compositional distributed semantics", "author": ["Hermann", "Karl Moritz", "Blunsom", "Phil"], "venue": "In Proc. ACL,", "citeRegEx": "Hermann et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Hermann et al\\.", "year": 2014}, {"title": "Embedding word similarity with neural machine translation", "author": ["Hill", "Felix", "Cho", "Kyunghyun", "Jean", "S\u00e9bastien", "Devin", "Coline", "Bengio", "Yoshua"], "venue": "CoRR, abs/1412.6448,", "citeRegEx": "Hill et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Hill et al\\.", "year": 2014}, {"title": "Improving word representations via global context and multiple word prototypes", "author": ["Huang", "Eric H", "Socher", "Richard", "Manning", "Christopher D", "Ng", "Andrew Y"], "venue": "In Proc. ACL,", "citeRegEx": "Huang et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Huang et al\\.", "year": 2012}, {"title": "Ontologically grounded multi-sense representation learning for semantic vector space models", "author": ["Jauhar", "Sujay K", "Dyer", "Chris", "Hovy", "Eduard"], "venue": "In Proc. NAACL,", "citeRegEx": "Jauhar et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Jauhar et al\\.", "year": 2015}, {"title": "A convolutional neural network for modelling sentences", "author": ["Kalchbrenner", "Nal", "Grefenstette", "Edward", "Blunsom", "Phil"], "venue": "In Proc. ACL,", "citeRegEx": "Kalchbrenner et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Kalchbrenner et al\\.", "year": 2014}, {"title": "I don\u2019t believe in word senses", "author": ["Kilgarriff", "Adam"], "venue": "Computers and the Humanities,", "citeRegEx": "Kilgarriff and Adam.,? \\Q1997\\E", "shortCiteRegEx": "Kilgarriff and Adam.", "year": 1997}, {"title": "Adam: A method for stochastic optimization", "author": ["Kingma", "Diederik", "Ba", "Jimmy"], "venue": "arXiv preprint arXiv:1412.6980,", "citeRegEx": "Kingma et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Kingma et al\\.", "year": 2014}, {"title": "Europarl: A parallel corpus for statistical machine translation", "author": ["Koehn", "Philipp"], "venue": "In MT summit,", "citeRegEx": "Koehn and Philipp.,? \\Q2005\\E", "shortCiteRegEx": "Koehn and Philipp.", "year": 2005}, {"title": "A solution to Plato\u2019s problem: the latent semantic analysis theory of acquisition, induction and representation of knowledge", "author": ["Landauer", "Thomas K", "Dumais", "Susan"], "venue": "Psychological Review,", "citeRegEx": "Landauer et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Landauer et al\\.", "year": 1997}, {"title": "Learning multilingual word representations using a bag-of-words autoencoder", "author": ["Lauly", "Stanislas", "Boulanger", "Alex", "Larochelle", "Hugo"], "venue": "arXiv preprint arXiv:1401.1803,", "citeRegEx": "Lauly et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Lauly et al\\.", "year": 2014}, {"title": "Finding function in form: Compositional character models for open vocabulary word representation", "author": ["Ling", "Wang", "Lu\u0131\u0301s", "Tiago", "Marujo", "Astudillo", "Ram\u00f3n Fernandez", "Amir", "Silvio", "Dyer", "Chris", "Black", "Alan W", "Trancoso", "Isabel"], "venue": "arXiv preprint arXiv:1508.02096,", "citeRegEx": "Ling et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Ling et al\\.", "year": 2015}, {"title": "Semeval-2007 task 10: English lexical substitution task", "author": ["McCarthy", "Diana", "Navigli", "Roberto"], "venue": "In Proceedings of the 4th International Workshop on Semantic Evaluations,", "citeRegEx": "McCarthy et al\\.,? \\Q2007\\E", "shortCiteRegEx": "McCarthy et al\\.", "year": 2007}, {"title": "A simple word embedding model for lexical substitution", "author": ["Melamud", "Oren", "Levy", "Omer", "Dagan", "Ido"], "venue": "In Proceedings of the 1st Workshop on Vector Space Modeling for Natural Language Processing,", "citeRegEx": "Melamud et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Melamud et al\\.", "year": 2015}, {"title": "Investigation of recurrent-neuralnetwork architectures and learning methods for spoken language understanding", "author": ["Mesnil", "Gr\u00e9goire", "He", "Xiaodong", "Deng", "Li", "Bengio", "Yoshua"], "venue": "In INTERSPEECH,", "citeRegEx": "Mesnil et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mesnil et al\\.", "year": 2013}, {"title": "The senseval-3 english lexical sample task", "author": ["R. Mihalcea", "T. Chklovski", "A. Kilgarriff"], "venue": "In Proceedings of SENSEVAL-3: Third International Workshop on the Evaluation of Systems for the Semantic Analysis of Text,", "citeRegEx": "Mihalcea et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Mihalcea et al\\.", "year": 2004}, {"title": "Efficient estimation of word representations in vector space", "author": ["Mikolov", "Tomas", "Chen", "Kai", "Corrado", "Greg", "Dean", "Jeffrey"], "venue": "CoRR, abs/1301.3781,", "citeRegEx": "Mikolov et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Distributed representations of words and phrases and their compositionality", "author": ["Mikolv", "Tomas", "Sutskever", "Ilya", "Chen", "Kai", "Corrado", "Greg", "Dean", "Jeffrey"], "venue": "In Proc. NIPS,", "citeRegEx": "Mikolv et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mikolv et al\\.", "year": 2013}, {"title": "Vector-based models of semantic composition", "author": ["Mitchell", "Jeff", "Lapata", "Mirella"], "venue": "In Proc. ACL,", "citeRegEx": "Mitchell et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Mitchell et al\\.", "year": 2008}, {"title": "Efficient nonparametric estimation of multiple embeddings per word in vector space", "author": ["Neelakantan", "Arvind", "Shankar", "Jeevan", "Passos", "Alexandre", "McCallum", "Andrew"], "venue": "In Proc. EMNLP,", "citeRegEx": "Neelakantan et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Neelakantan et al\\.", "year": 2014}, {"title": "Arkref: a rule-based coreference resolution system", "author": ["O\u2019Connor", "Brendan", "Heilman", "Michael"], "venue": "CoRR, abs/1310.1975,", "citeRegEx": "O.Connor et al\\.,? \\Q2013\\E", "shortCiteRegEx": "O.Connor et al\\.", "year": 2013}, {"title": "The informative role of wordnet in open-domain question answering", "author": ["Pasca", "Marius", "Harabagiu", "Sanda"], "venue": "In Proceedings of NAACL-01 Workshop on WordNet and Other Lexical Resources,", "citeRegEx": "Pasca et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Pasca et al\\.", "year": 2001}, {"title": "GloVe: Global vectors for word representation", "author": ["Pennington", "Jeffrey", "Socher", "Richard", "Manning", "Christopher D"], "venue": "In Proc. EMNLP,", "citeRegEx": "Pennington et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Pennington et al\\.", "year": 2014}, {"title": "Estimating supersenses with conditional random fields", "author": ["Reichartz", "Frank", "Paa\u00df", "Gerhard"], "venue": "Proceedings of ECMLPKDD,", "citeRegEx": "Reichartz et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Reichartz et al\\.", "year": 2008}, {"title": "Distinguishing systems and distinguishing senses: new evaluation methods for word sense disambiguation", "author": ["Resnik", "Philip", "Yarowsky", "David"], "venue": "Natural Language Engineering,", "citeRegEx": "Resnik et al\\.,? \\Q1999\\E", "shortCiteRegEx": "Resnik et al\\.", "year": 1999}, {"title": "Exact solutions to the nonlinear dynamics of learning in deep linear neural networks", "author": ["Saxe", "Andrew M", "McClelland", "James L", "Ganguli", "Surya"], "venue": "CoRR, abs/1312.6120,", "citeRegEx": "Saxe et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Saxe et al\\.", "year": 2013}, {"title": "Dynamic pooling and unfolding recursive autoencoders for paraphrase detection", "author": ["Socher", "Richard", "Huang", "Eric H", "Pennington", "Jeffrey", "Ng", "Andrew Y", "Manning", "Chirstopher D"], "venue": "In Proc. NIPS,", "citeRegEx": "Socher et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Socher et al\\.", "year": 2011}, {"title": "Recursive deep models for semantic compositionality over a sentiment treebank", "author": ["Socher", "Richard", "Perelygin", "Alex", "Wu", "Jean Y", "Chuang", "Jason", "Manning", "Christopher D", "Ng", "Andrew Y", "Potts", "Christopher"], "venue": "In Proceedings of the conference on empirical methods in natural language processing (EMNLP),", "citeRegEx": "Socher et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Socher et al\\.", "year": 2013}, {"title": "A probabilistic model for learning multi-prototype word embeddings", "author": ["Tian", "Fei", "Dai", "Hanjun", "Bian", "Jiang", "Gao", "Bin", "Zhang", "Rui", "Chen", "Enhong", "Liu", "Tie-Yan"], "venue": "In Proc. COLING,", "citeRegEx": "Tian et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Tian et al\\.", "year": 2014}, {"title": "Word representations: a simple and general method for semi-supervised learning", "author": ["Turian", "Joseph", "Ratinov", "Lev", "Bengio", "Yoshua"], "venue": "In Proc. ACL,", "citeRegEx": "Turian et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Turian et al\\.", "year": 2010}, {"title": "Sense-aware semantic analysis: A multi-prototype word representation model using Wikipedia", "author": ["Wu", "Zhaohui", "Giles", "C. Lee"], "venue": "In Proc. AAAI,", "citeRegEx": "Wu et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Wu et al\\.", "year": 2015}, {"title": "The noisy channel model for unsupervised word sense disambiguation", "author": ["Yuret", "Deniz", "Yatbaz", "Mehmet Ali"], "venue": "Computational Linguistics,", "citeRegEx": "Yuret et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Yuret et al\\.", "year": 2010}], "referenceMentions": [{"referenceID": 37, "context": "Distributed representations of words, which represent each word as a vector in a low-dimensional space, can be learned from unannotated text corpora using a variety of techniques (Mikolov et al., 2013; Pennington et al., 2014; Landauer & Dumais, 1997).", "startOffset": 179, "endOffset": 251}, {"referenceID": 43, "context": "Distributed representations of words, which represent each word as a vector in a low-dimensional space, can be learned from unannotated text corpora using a variety of techniques (Mikolov et al., 2013; Pennington et al., 2014; Landauer & Dumais, 1997).", "startOffset": 179, "endOffset": 251}, {"referenceID": 50, "context": "Despite their empirically proven value as a source of features in many downstream applications (Turian et al., 2010), the \u201cone word type, one vector\u201d assumption made by most word representation models is problematic because words may have multiple meanings.", "startOffset": 95, "endOffset": 116}, {"referenceID": 49, "context": "The first to treat each word as a collection of discrete, mutually exclusive senses which are individually represented as vectors (Tian et al., 2014; Neelakantan et al., 2014; Wu & Giles, 2015; Huang et al., 2012; Jauhar et al., 2015).", "startOffset": 130, "endOffset": 234}, {"referenceID": 40, "context": "The first to treat each word as a collection of discrete, mutually exclusive senses which are individually represented as vectors (Tian et al., 2014; Neelakantan et al., 2014; Wu & Giles, 2015; Huang et al., 2012; Jauhar et al., 2015).", "startOffset": 130, "endOffset": 234}, {"referenceID": 24, "context": "The first to treat each word as a collection of discrete, mutually exclusive senses which are individually represented as vectors (Tian et al., 2014; Neelakantan et al., 2014; Wu & Giles, 2015; Huang et al., 2012; Jauhar et al., 2015).", "startOffset": 130, "endOffset": 234}, {"referenceID": 25, "context": "The first to treat each word as a collection of discrete, mutually exclusive senses which are individually represented as vectors (Tian et al., 2014; Neelakantan et al., 2014; Wu & Giles, 2015; Huang et al., 2012; Jauhar et al., 2015).", "startOffset": 130, "endOffset": 234}, {"referenceID": 17, "context": "However, identifying the appropriate sense granularity in such models is difficult in practice and in theory (Kilgarriff, 1997; Erk et al., 2013).", "startOffset": 109, "endOffset": 145}, {"referenceID": 47, "context": "This is surprising given the success of learning composition functions for computing phrase and sentence representations (Socher et al., 2011; Kalchbrenner et al., 2014).", "startOffset": 121, "endOffset": 169}, {"referenceID": 26, "context": "This is surprising given the success of learning composition functions for computing phrase and sentence representations (Socher et al., 2011; Kalchbrenner et al., 2014).", "startOffset": 121, "endOffset": 169}, {"referenceID": 0, "context": "Since bidirectional RNN-LSTMs have been shown to be able to learn both compositional (Bahdanau et al., 2014) as well as more arbitrary relationships (Ling et al.", "startOffset": 85, "endOffset": 108}, {"referenceID": 32, "context": ", 2014) as well as more arbitrary relationships (Ling et al., 2015), we use these as our composition function class (\u00a72).", "startOffset": 48, "endOffset": 67}, {"referenceID": 38, "context": "For example, Mikolv et al. (2013) showed that short multiword expressions could be embedding by using the \u201cwider\u201d context that they occur in.", "startOffset": 13, "endOffset": 34}, {"referenceID": 15, "context": "To obtain pairs of words in context and their lexical translations into a second language, we use unsupervised word alignment techniques (Dyer et al., 2013), to obtain high precision word alignments from a parallel corpus.", "startOffset": 137, "endOffset": 156}, {"referenceID": 15, "context": "After normal tokenization, we obtained alignments with fast-align tool (Dyer et al., 2013).", "startOffset": 71, "endOffset": 90}, {"referenceID": 46, "context": "All recurrent materices with orthogonal initialization (Saxe et al., 2013), and non-recurrent weights are initialized from scaled uniform distribution (Glorot & Bengio, 2010).", "startOffset": 55, "endOffset": 74}, {"referenceID": 7, "context": "To avoid the computational overhead of reading extremely wide contexts, we used sliding window to delimit the range of contexts as in (Collobert et al., 2011), that is, each token wt is embedded using a context window of words wt\u2212n/2, .", "startOffset": 134, "endOffset": 158}, {"referenceID": 2, "context": "Since low-resource languages do not have enough data to adequate estimate translation probabilities, we hope that we can learn more effective mappings with pre-trained word-in-context embeddings (Chahuneau et al., 2013).", "startOffset": 195, "endOffset": 219}, {"referenceID": 13, "context": "We used a dataset used in (Dou et al., 2014) for Malagasy and the Urdu data we used is a part of NIST MT evaluation in 2008-20122.", "startOffset": 26, "endOffset": 44}, {"referenceID": 14, "context": "We trained our baseline system with cdec (Dyer et al., 2010) and obtained synchronous context-free grammars rules to translate sentences.", "startOffset": 41, "endOffset": 60}, {"referenceID": 34, "context": "The original task allow to make multiple predictions but we only predict only one substitution following (Melamud et al., 2015).", "startOffset": 105, "endOffset": 127}, {"referenceID": 6, "context": "We report the averaged BLEU score of 5 runs to avoid optimizer randomness Clark et al. (2011). The result show large improvement on perplexity and consistent improvement on BLEU in all language pairs.", "startOffset": 74, "endOffset": 94}, {"referenceID": 34, "context": "Also we compared our results with various context sensitive models, which take arithmetic mean (as in Add and BalAdd) and a geometrical mean (as in Mult and BalMult) of embeddings, proposed by (Melamud et al., 2015).", "startOffset": 193, "endOffset": 215}, {"referenceID": 19, "context": "They trained their baseline embeddings (as in Base) on a two billion word web corpus, ukWaC (Ferraresi et al., 2008).", "startOffset": 92, "endOffset": 116}, {"referenceID": 19, "context": "They trained their baseline embeddings (as in Base) on a two billion word web corpus, ukWaC (Ferraresi et al., 2008). The model achieved best measures4 10.63, best mode measure 18.90 with German supervision. And the second best result was obtained with Czech. As for comparison with Melamud et al. (2015), we cannot compare score directly since we used different corpus and candidate generation.", "startOffset": 93, "endOffset": 305}, {"referenceID": 50, "context": "In the sequence modeling problems such as BIO chunking, conditional random fields and recurrent neural networks are applied to represent a sequence of word representations (Turian et al., 2010; Mesnil et al., 2013).", "startOffset": 172, "endOffset": 214}, {"referenceID": 35, "context": "In the sequence modeling problems such as BIO chunking, conditional random fields and recurrent neural networks are applied to represent a sequence of word representations (Turian et al., 2010; Mesnil et al., 2013).", "startOffset": 172, "endOffset": 214}, {"referenceID": 31, "context": "For classification tasks such as document classification, sentiment analysis, paraphrase detection, summation of word embeddings (Lauly et al., 2014), convolutional neural networks (Kalchbrenner et al.", "startOffset": 129, "endOffset": 149}, {"referenceID": 26, "context": ", 2014), convolutional neural networks (Kalchbrenner et al., 2014) and recursive networks (Socher et al.", "startOffset": 39, "endOffset": 66}, {"referenceID": 48, "context": ", 2014) and recursive networks (Socher et al., 2013; Cheng & Kartsaklis, 2015) were proposed to represent compositionality function of words.", "startOffset": 31, "endOffset": 78}, {"referenceID": 8, "context": "Coulmance et al. (2015) shows that predicting context in target language is an effective way to train word representation shared across languages.", "startOffset": 0, "endOffset": 24}, {"referenceID": 8, "context": "Coulmance et al. (2015) shows that predicting context in target language is an effective way to train word representation shared across languages. Hill et al. (2014) investigated the quality of word embedding learned by neural machine translation model and show its benefit on tasks that require modeling word similarity.", "startOffset": 0, "endOffset": 166}, {"referenceID": 47, "context": "Furthermore, one can learn reasonable generalizations from models that condition on and the generate text using an autoencoding objective (Socher et al., 2011).", "startOffset": 138, "endOffset": 159}, {"referenceID": 11, "context": "Dhillon et al. (2012) make the intriguing proposition that left- and right- contexts can be used to supervise each other.", "startOffset": 0, "endOffset": 22}], "year": 2015, "abstractText": "We present a neural network architecture based on bidirectional LSTMs to compute representations of words in the sentential contexts. These context-sensitive word representations are suitable for, e.g., distinguishing different word senses and other context-modulated variations in meaning. To learn the parameters of our model, we use cross-lingual supervision, hypothesizing that a good representation of a word in context will be one that is sufficient for selecting the correct translation into a second language. We evaluate the quality of our representations as features in three downstream tasks: prediction of semantic supersenses (which assign nouns and verbs into a few dozen semantic classes), low resource machine translation, and a lexical substitution task, and obtain state-of-the-art results on all of these.", "creator": "LaTeX with hyperref package"}}}