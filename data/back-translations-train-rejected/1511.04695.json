{"id": "1511.04695", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "15-Nov-2015", "title": "An Iterative Reweighted Method for Tucker Decomposition of Incomplete Multiway Tensors", "abstract": "We consider the problem of low-rank decomposition of incomplete multiway tensors. Since many real-world data lie on an intrinsically low dimensional subspace, tensor low-rank decomposition with missing entries has applications in many data analysis problems such as recommender systems and image inpainting. In this paper, we focus on Tucker decomposition which represents an Nth-order tensor in terms of N factor matrices and a core tensor via multilinear operations. To exploit the underlying multilinear low-rank structure in high-dimensional datasets, we propose a group-based log-sum penalty functional to place structural sparsity over the core tensor, which leads to a compact representation with smallest core tensor. The method for Tucker decomposition is developed by iteratively minimizing a surrogate function that majorizes the original objective function, which results in an iterative reweighted process. In addition, to reduce the computational complexity, an over-relaxed monotone fast iterative shrinkage-thresholding technique is adapted and embedded in the iterative reweighted process. The proposed method is able to determine the model complexity (i.e. multilinear rank) in an automatic way. Simulation results show that the proposed algorithm offers competitive performance compared with other existing algorithms.", "histories": [["v1", "Sun, 15 Nov 2015 12:56:36 GMT  (4993kb)", "http://arxiv.org/abs/1511.04695v1", null]], "reviews": [], "SUBJECTS": "cs.NA cs.LG", "authors": ["linxiao yang", "jun fang", "hongbin li", "bing zeng"], "accepted": false, "id": "1511.04695"}, "pdf": {"name": "1511.04695.pdf", "metadata": {"source": "CRF", "title": "An Iterative Reweighted Method for Tucker Decomposition of Incomplete Multiway Tensors", "authors": ["Linxiao Yang", "Jun Fang", "Hongbin Li", "Bing Zeng"], "emails": ["Fang@uestc.edu.cn", "bin.Li@stevens.edu", "eezeng@uestc.edu.cn"], "sections": [{"heading": null, "text": "This year, it is only a matter of time before such a process takes place."}, {"heading": "II. NOTATIONS AND BASICS ON TENSORS", "text": "A tensor is the generalization of a matrix to higher dimensions, also referred to as paths or procedures. (1) A tensor is the generalization of a matrix to higher dimensions, also referred to as paths or procedures. (1) A tensor with one and two procedures, each with one and two procedures. (1) In this paper we use symbols (1) that denote the Kronecker, the outer and the Hadamard product. (1) Here is the orderFig. (1) The tucker decay of a three-dimensional tensor. N of a tensor is the number of process dimensions. Fibers are the higher order of the matrix rows and columns."}, {"heading": "III. PROBLEM FORMULATION", "text": "Let us define Y-1 \u00b7 I2 \u00b7 I2 \u00b7 IN as an incomplete N-th order tensor, with its input observing Yi1i2... iN when Oi1i2... iN = 1, where O-N \u00b7 I1 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 IN is clearly a binary tensor of the same size as Y and indicates which entries of Y are missing or being observed. Since the dimension is the smallest attainable core tensor, we must develop a method that can achieve automatic model determination. To this end, we will first introduce a new idea called order (N \u2212 1) sub-sor.Definition: order (N \u2212 1) sub-sor.Definition is a method that can achieve automatic model determination."}, {"heading": "IV. PROPOSED ITERATIVE REWEIGHTED METHOD", "text": "We use a limited optimization approach, also known as the Majorization Minimization (MM) approach (41), to solve the optimization (14). The idea of the MM approach is to minimize a simple substitute function that majorizes the given objective function. However, it can be shown that by iteratively minimizing the substitute function, the iterative process yields a non-increasing objective value and ultimately converges to a stationary point of the original objective function. To obtain an adequate replacement function for (14), we first find a suitable replacement function for the log function."}, {"heading": "V. A COMPUTATIONALLY EFFICIENT ITERATIVE REWEIGHTED ALGORITHM", "text": "A famous method of the first order is the fast iterative shrinkage threshold algorithm (FISTA) [42], which has a convergence rate of O (1 / k2) to minimize the sum of a smooth and a possibly non-smooth convex function, where k denotes the iteration counter. Later in [34], a too relaxed monotonous FISTA (MFISTA) was proposed to overcome some of the inherent limitations of FISTA. Specifically, the overrelaxed MFISTA guarantees the monotonous decrease in functional values, which has proven helpful in many practical applications. Furthermore, the overrelaxed MFISTA allows a variable step size in a wider range than FISTA while maintaining the same convergence rate. Below, we will give a brief review of the problem of the relaxed FISTA, and then discuss how the technique can be expanded."}, {"heading": "A. Review of Over-Relaxed MFISTA", "text": "Consider the general convex optimization problem: min x F (x) = f (x) + g (x), where f is a smooth convex function with the continuous Lipschitz gradient L (f) and g is a convex, but possibly not smooth function. The overrelaxed MFISTA scheme can be summarized as follows: Given x (0) = w (1), \u03b7 (1) = 1, \u03b4 (0, 2) and \u03b2 (0, (2 \u2212 \u03b4) / L (f)], the sequence {x (t)} in byz (t) = prox\u03b2g (w (t) \u2212 \u03b2 f (t))))) (30) x (t) = argmin {F (z) | z \u00b2 {z \u00b2 (t), x (t (t), x (t), the guaranteed (t) and the guaranteed (t) (31) (t) (t) (t), x (z \u2212 1)."}, {"heading": "B. Solving (22) via the Over-Relaxed MFISTA", "text": "(5). (5). (5). (5). (5). (5). (5). (5). (5). (5). (5). (5). (5). (5). (5). (5). (5). (5). (5). (5). (5). (5). (5). (5). (5). (5). (5). (5). (5). (5). (5). (5). (5). (5). (5). (5). (5). (5). (5). (5). (5). (5). (5). (5). (5). (5). (5). (5). (5). (5). (5). (5). (5). (5). (5). (5). (5). (5). (5). (5). (5). (5). (5). (5). (5). (5). (5). (5). (5)."}, {"heading": "VI. SIMULATION RESULTS", "text": "In this section, we conduct experiments to illustrate the performance of our proposed iterative method for reweighting the Tucker decomposition (referred to as RTD). In our simulations, we use \u03b4 = 0.1, \u03b2 = (2 \u2212 \u03b4) / L value (f) and \u03bb2 = 1. In fact, our proposed algorithm is insensitive to the selection of these parameters. The choice of \u03bb1 is more critical than the others, and a suitable choice of \u03bb1 depends on the noise level and the lack of data ratio. Empirical results indicate that a stable recovery performance can be achieved when set in the range [0.5, 2]. Factor matrices and core types are initialized by decomposing the observed tensor (the missing elements are set to zero), with a highly ordered singular value degradation [43]. In our algorithm, the overlocked MFISTA performs only two iterations to update the core tenor, i.e., to recompose our existing method to max = decomposition."}, {"heading": "A. Synthetic and Chemometrics Data", "text": "In this subset, we conduct experiments based on synthetic and chemometric data. Two sets of synthetic data are generated, and both are deemed insufficient. (The first tensors are generated according to the CP model, which is a sum of six equally weighted rank-one tensors, with all factor matrices drawn from a normal distribution.) So the truth is 6 or (6, 6, 6) in a multilinear ranking. The other tensor is generated based on the Tucker decomposition model, with a random core tensor of size (3, 4, 5) multiplied by random factor matrices along each mode that shows the basic truth for the multilinear ranking (3, 4, 5). Two chemometric data sets are also considered in our simulations."}, {"heading": "VII. CONCLUSIONS", "text": "In this paper, we proposed an iterative reweighted algorithm to break down an incomplete tensor into a concise tucker decomposition. To automatically determine model complexity, we introduced a new term called order (N \u2212 1) sub-tensor and introduced a group log sum penalty for each order (N \u2212 1) sub-tensor to achieve a structurally sparse core tensor. By shrinking the zero-order (N \u2212 1) sub-tensor, the core tensor becomes smaller and a compact tucker decomposition can be achieved. An iterative reweighting algorithm was developed using the majority minimization approach. In addition, the overloosened monotonous, fast iterative shrinkage threshold technique is adapted and embedded in the iterative reweighted process to reduce compressed complexity."}], "references": [{"title": "Multiverse recommendation: n-dimensional tensor factorization for context-aware collaborative filtering", "author": ["A. Karatzoglou", "X. Amatriain", "L. Baltrunas", "N. Oliver"], "venue": "Proceedings of the fourth ACM conference on Recommender systems. ACM, 2010, pp. 79\u201386.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2010}, {"title": "Temporal collaborative filtering with Bayesian probabilistic tensor factorization", "author": ["L. Xiong", "X. Chen", "T.-K. Huang", "J. Schneider", "J.G. Carbonell"], "venue": "SDM, vol. 10. SIAM, 2010, pp. 211\u2013222.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2010}, {"title": "Context-aware recommender systems", "author": ["G. Adomavicius", "A. Tuzhilin"], "venue": "Recommender systems handbook. Springer, 2011, pp. 217\u2013 253.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2011}, {"title": "Leveraging features and networks for probabilistic tensor decomposition", "author": ["P. Rai", "Y. Wang", "L. Carin"], "venue": "AAAI Conference on Artificial Intelligence, 2015.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2015}, {"title": "Modelling relational data using bayesian clustered tensor factorization", "author": ["I. Sutskever", "J.B. Tenenbaum", "R.R. Salakhutdinov"], "venue": "Advances in neural information processing systems, 2009, pp. 1821\u20131828.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2009}, {"title": "Tensor decomposition of EEG signals: A brief review", "author": ["F. Cong", "Q.-H. Lin", "L.-D. Kuang", "X.-F. Gong", "P. Astikainen", "T. Ristaniemi"], "venue": "Journal of neuroscience methods, vol. 248, pp. 59\u201369, 2015.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2015}, {"title": "Nonnegative matrix and tensor factorizations", "author": ["A. Cichocki", "R. Zdunek", "A.H. Phan", "S.I. Amari"], "venue": null, "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2009}, {"title": "Higher-order SVD-based subspace estimation to improve the parameter estimation accuracy in multidimensional harmonic retrieval problems", "author": ["M. Haardt", "F. Roemer", "G.D. Galdo"], "venue": "IEEE Transactions on Signal Processing, vol. 56, no. 7, pp. 3198\u20133213, 2008.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2008}, {"title": "Variational bayesian parafac decomposition for multidimensional harmonic retrieval", "author": ["W. Guo", "W. Yu"], "venue": "2011 IEEE CIE International Conference on Radar (Radar), vol. 2. IEEE, 2011, pp. 1864\u20131867.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2011}, {"title": "A tensor-based approach for automatic music genre classification", "author": ["E. Benetos", "C. Kotropoulos"], "venue": "2008 16th European Signal Processing Conference. IEEE, 2008, pp. 1\u20134.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2008}, {"title": "Nonnegative matrix and tensor factorizations: An algorithmic perspective", "author": ["G. Zhou", "A. Cichocki", "Q. Zhao", "S. Xie"], "venue": "IEEE Signal Processing Magazine, vol. 31, no. 3, pp. 54\u201365, 2014.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2014}, {"title": "Multilinear discriminant analysis for higherorder tensor data classification", "author": ["Q. Li", "D. Schonfeld"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 36, no. 12, pp. 2524\u20132537, 2014.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2014}, {"title": "Scalable Bayesian low-rank decomposition of incomplete multiway tensors", "author": ["P. Rai", "Y. Wang", "S. Guo", "G. Chen", "D. Dunson", "L. Carin"], "venue": "Proceedings of the 31st International Conference on Machine Learning (ICML-14), 2014, pp. 1800\u20131808.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2014}, {"title": "Some mathematical notes on three-mode factor analysis", "author": ["L.R. Tucker"], "venue": "Psychometrika, vol. 31, no. 3, pp. 279\u2013311, 1966.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 1966}, {"title": "PARAFAC. Tutorial and applications", "author": ["R. Bro"], "venue": "Chemometrics and intelligent laboratory systems, vol. 38, no. 2, pp. 149\u2013171, 1997.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 1997}, {"title": "Tensor decompositions for signal processing applications: From two-way to multiway component analysis", "author": ["A. Cichocki", "D.P. Mandic", "H.A. Phan", "C.F. Caiafa", "G. Zhou", "Q. Zhao", "L.D. Lathauwer"], "venue": "IEEE Signal Processing Magazine, vol. 32, no. 2, pp. 145\u2013163, 2015.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2015}, {"title": "Tensor completion for estimating missing values in visual data", "author": ["J. Liu", "P. Musialski", "P. Wonka", "J. Ye"], "venue": "IEEE 12th International Conference on Computer Vision. IEEE, 2009, pp. 2114\u20132121.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2009}, {"title": "Rank regularization and bayesian inference for tensor completion and extrapolation", "author": ["J.A.G. Mateos", "G.B. Giannakis"], "venue": "IEEE Transactions on Signal Processing, no. 22, pp. 5689\u20135703, Nov. 2013.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2013}, {"title": "Subspace learning and imputation for streaming big data matrices and tensors", "author": ["M. Mardani", "G. Mateos", "G.B. Giannakis"], "venue": "IEEE Transactions on Signal Processing, vol. 63, no. 10, pp. 2663\u20132677, 2015.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2015}, {"title": "Scalable tensor factorizations for incomplete data", "author": ["E. Acar", "D.M. Dunlavy", "T.G. Kolda", "M. M\u00f8rup"], "venue": "Chemometrics and Intelligent Laboratory Systems, vol. 106, no. 1, pp. 41\u201356, 2011.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2011}, {"title": "Simultaneous tensor decomposition and completion using factor priors", "author": ["Y.-L. Chen", "C.-T. Hsu", "H.-Y.M. Liao"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 36, no. 3, pp. 577\u2013591, 2014.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2014}, {"title": "Tucker factorization with missing data with application to low-n-rank tensor completion", "author": ["M.F.A. Juki\u0107"], "venue": "Multidimensional Systems and Signal Processing, vol. 26, pp. 677\u2013692, 2015.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2015}, {"title": "Tensor completion for estimating missing values in visual data", "author": ["J. Liu", "P. Musialski", "P. Wonka", "J. Ye"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 35, no. 1, pp. 208\u2013220, Jan. 2013.  10", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2013}, {"title": "Generalized higherorder orthogonal iteration for tensor decomposition and completion", "author": ["Y. Liu", "F. Shang", "W. Fan", "J. Cheng", "H. Cheng"], "venue": "Advances in Neural Information Processing Systems, 2014, pp. 1763\u2013 1771.", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2014}, {"title": "Tensor factorization using auxiliary information", "author": ["A. Narita", "K. Hayashi", "R. Tomioka", "H. Kashima"], "venue": "Data Mining and Knowledge Discovery, vol. 25, no. 2, pp. 298\u2013324, 2012.", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2012}, {"title": "Bayesian CP factorization of incomplete tensors with automatic rank determination", "author": ["Q. Zhao", "L. Zhang", "A. Cichocki"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence, no. 1, pp. 1\u20131, 2015.", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2015}, {"title": "Bayesian nonparametric models for multiway data analysis", "author": ["Z. Xu", "F. Yan", "Y. Qi"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 37, no. 2, pp. 475\u2013487, Feb. 2015.", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2015}, {"title": "On the best rank-1 and rank-(r1, r2, . . . ,  rn) approximation of higher-order tensors", "author": ["L.D. Lathauwer", "B.D. Moor", "J. Vandewalle"], "venue": "SIAM Journal on Matrix Analysis and Applications, vol. 21, no. 4, pp. 1324\u2013 1342, 2000.", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2000}, {"title": "Tensor rank is NP-complete", "author": ["J. H\u00e5stad"], "venue": "Journal of Algorithms, vol. 11, no. 4, pp. 644\u2013654, 1990.", "citeRegEx": "29", "shortCiteRegEx": null, "year": 1990}, {"title": "Rank, decomposition, and uniqueness for 3-way and n-way arrays", "author": ["Kruskal", "J. B"], "venue": "Multiway data analysis, vol. 33, pp. 7\u201318, 1989.", "citeRegEx": "30", "shortCiteRegEx": null, "year": 1989}, {"title": "Tensor decompositions and applications", "author": ["T.G. Kolda", "B.W. Bader"], "venue": "SIAM review, vol. 51, no. 3, pp. 455\u2013500, 2009.", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2009}, {"title": "Tensor completion and low-n-rank tensor recovery via convex optimization", "author": ["S. Gandy", "B. Recht", "I. Yamada"], "venue": "Inverse Problems, vol. 27, no. 2, p. 025010, 2011.", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2011}, {"title": "Tensor completion via a multi-linear low-n-rank factorization model", "author": ["H. Tan", "B. Cheng", "W. Wang", "Y.-J. Zhang", "B. Ran"], "venue": "Neurocomputing, vol. 133, pp. 161\u2013169, 2014.", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2014}, {"title": "Over-relaxation of the fast iterative shrinkage-thresholding algorithm with variable stepsize", "author": ["M. Yamagishi", "I. Yamada"], "venue": "Inverse Problems, vol. 27, no. 10, p. 105008, 2011.", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2011}, {"title": "Enhancing sparsity by reweighted l1 minimization", "author": ["E.J. Cand\u00e9s", "M.B. Wakin", "S.P. Boyd"], "venue": "Journal of Fourier analysis and applications, vol. 14, pp. 877\u2013905, 2008.", "citeRegEx": "35", "shortCiteRegEx": null, "year": 2008}, {"title": "Iteratively reweighted algorithms for compressive sensing", "author": ["R. Chartrand", "W. Yin"], "venue": "IEEE international conference on acoustics, speech and signal processing, 2008. ICASSP 2008. IEEE, 2008, pp. 3869\u2013 3872.", "citeRegEx": "36", "shortCiteRegEx": null, "year": 2008}, {"title": "Iterative reweighted l1 and l2 methods for finding sparse solutions", "author": ["D. Wipf", "S. Nagarajan"], "venue": "IEEE Journal of Selected Topics in Signal Processing, vol. 4, no. 2, pp. 317\u2013329, 2010.", "citeRegEx": "37", "shortCiteRegEx": null, "year": 2010}, {"title": "Exact reconstruction analysis of log-sum minimization for compressed sensing", "author": ["Y. Shen", "J. Fang", "H. Li"], "venue": "IEEE Signal Processing Letters, vol. 20, no. 12, pp. 1223\u20131226, 2013.", "citeRegEx": "38", "shortCiteRegEx": null, "year": 2013}, {"title": "Regression shrinkage and selection via the lasso", "author": ["R. Tibshirani"], "venue": "Journal of the Royal Statistical Society: Series B (Methodological), pp. 267\u2013288, 1996.", "citeRegEx": "39", "shortCiteRegEx": null, "year": 1996}, {"title": "Model selection and estimation in regression with grouped variables", "author": ["M. Yuan", "Y. Lin"], "venue": "Journal of the Royal Statistical Society: Series B (Statistical Methodology), vol. 68, no. 1, pp. 49\u201367, 2006.", "citeRegEx": "40", "shortCiteRegEx": null, "year": 2006}, {"title": "A tutorial on MM algorithms", "author": ["D.R. Hunter", "K. Lange"], "venue": "The American Statistician, vol. 58, no. 1, pp. 30\u201337, 2004.", "citeRegEx": "41", "shortCiteRegEx": null, "year": 2004}, {"title": "A fast iterative shrinkage-thresholding algorithm for linear inverse problems", "author": ["A. Beck", "M. Teboulle"], "venue": "SIAM journal on imaging sciences, vol. 2, no. 1, pp. 183\u2013202, 2009.", "citeRegEx": "42", "shortCiteRegEx": null, "year": 2009}, {"title": "A multilinear singular value decomposition", "author": ["L.D. Lathauwer", "B.D. Moor", "J. Vandewalle"], "venue": "SIAM journal on Matrix Analysis and Applications, vol. 21, no. 4, pp. 1253\u20131278, 2000.", "citeRegEx": "43", "shortCiteRegEx": null, "year": 2000}], "referenceMentions": [{"referenceID": 0, "context": "Multi-dimensional data arise in a variety of applications, such as recommender systems [1]\u2013[3], multirelational networks [4], [5], and brain-computer imaging [6], [7].", "startOffset": 87, "endOffset": 90}, {"referenceID": 2, "context": "Multi-dimensional data arise in a variety of applications, such as recommender systems [1]\u2013[3], multirelational networks [4], [5], and brain-computer imaging [6], [7].", "startOffset": 91, "endOffset": 94}, {"referenceID": 3, "context": "Multi-dimensional data arise in a variety of applications, such as recommender systems [1]\u2013[3], multirelational networks [4], [5], and brain-computer imaging [6], [7].", "startOffset": 121, "endOffset": 124}, {"referenceID": 4, "context": "Multi-dimensional data arise in a variety of applications, such as recommender systems [1]\u2013[3], multirelational networks [4], [5], and brain-computer imaging [6], [7].", "startOffset": 126, "endOffset": 129}, {"referenceID": 5, "context": "Multi-dimensional data arise in a variety of applications, such as recommender systems [1]\u2013[3], multirelational networks [4], [5], and brain-computer imaging [6], [7].", "startOffset": 158, "endOffset": 161}, {"referenceID": 6, "context": "Multi-dimensional data arise in a variety of applications, such as recommender systems [1]\u2013[3], multirelational networks [4], [5], and brain-computer imaging [6], [7].", "startOffset": 163, "endOffset": 166}, {"referenceID": 7, "context": "Compared with matrix factorization, tensor decomposition can capture the intrinsic multi-dimensional structure of the multiway data, which has led to a substantial performance improvement for harmonic retrieval [8], [9], regression/classification [10]\u2013[12], and data completion [3], [13], etc.", "startOffset": 211, "endOffset": 214}, {"referenceID": 8, "context": "Compared with matrix factorization, tensor decomposition can capture the intrinsic multi-dimensional structure of the multiway data, which has led to a substantial performance improvement for harmonic retrieval [8], [9], regression/classification [10]\u2013[12], and data completion [3], [13], etc.", "startOffset": 216, "endOffset": 219}, {"referenceID": 9, "context": "Compared with matrix factorization, tensor decomposition can capture the intrinsic multi-dimensional structure of the multiway data, which has led to a substantial performance improvement for harmonic retrieval [8], [9], regression/classification [10]\u2013[12], and data completion [3], [13], etc.", "startOffset": 247, "endOffset": 251}, {"referenceID": 11, "context": "Compared with matrix factorization, tensor decomposition can capture the intrinsic multi-dimensional structure of the multiway data, which has led to a substantial performance improvement for harmonic retrieval [8], [9], regression/classification [10]\u2013[12], and data completion [3], [13], etc.", "startOffset": 252, "endOffset": 256}, {"referenceID": 2, "context": "Compared with matrix factorization, tensor decomposition can capture the intrinsic multi-dimensional structure of the multiway data, which has led to a substantial performance improvement for harmonic retrieval [8], [9], regression/classification [10]\u2013[12], and data completion [3], [13], etc.", "startOffset": 278, "endOffset": 281}, {"referenceID": 12, "context": "Compared with matrix factorization, tensor decomposition can capture the intrinsic multi-dimensional structure of the multiway data, which has led to a substantial performance improvement for harmonic retrieval [8], [9], regression/classification [10]\u2013[12], and data completion [3], [13], etc.", "startOffset": 283, "endOffset": 287}, {"referenceID": 13, "context": "Tucker decomposition [14] and CANDECOMP/PARAFAC (CP) decomposition [15] are two", "startOffset": 21, "endOffset": 25}, {"referenceID": 14, "context": "Tucker decomposition [14] and CANDECOMP/PARAFAC (CP) decomposition [15] are two", "startOffset": 67, "endOffset": 71}, {"referenceID": 15, "context": "It is generally believed that Tucker decomposition has a better generalization ability than CP decomposition for different types of data [16].", "startOffset": 137, "endOffset": 141}, {"referenceID": 16, "context": "Low-rank decomposition of incomplete multiway tensors has attracted a lot of attention over the past few years and a number of algorithms [17]\u2013[28] were proposed via either optimization techniques or probabilistic model learning.", "startOffset": 138, "endOffset": 142}, {"referenceID": 27, "context": "Low-rank decomposition of incomplete multiway tensors has attracted a lot of attention over the past few years and a number of algorithms [17]\u2013[28] were proposed via either optimization techniques or probabilistic model learning.", "startOffset": 143, "endOffset": 147}, {"referenceID": 28, "context": "the minimum number of rank-one terms in CP decomposition, is an NP-hard problem even for a completely observed tensor [29].", "startOffset": 118, "endOffset": 122}, {"referenceID": 12, "context": "To address this issue, a Bayesian method was proposed in [13] for CP decomposition, where a shrinkage prior called as the multiplicative gamma process (MGP) was employed to adaptively learn a concise representation of the tensor.", "startOffset": 57, "endOffset": 61}, {"referenceID": 25, "context": "In [26], a sparsity-inducing Gaussian inverse-Gamma prior was placed over multiple latent factors to achieve automatic rank determination.", "startOffset": 3, "endOffset": 7}, {"referenceID": 17, "context": "Besides the above Bayesian methods, an optimization-based CP decomposition method was proposed in [18], [19], where the Frobenius-norm of the factor matrices is used as the rank regularization to determine an appropriate number of component tensors.", "startOffset": 98, "endOffset": 102}, {"referenceID": 18, "context": "Besides the above Bayesian methods, an optimization-based CP decomposition method was proposed in [18], [19], where the Frobenius-norm of the factor matrices is used as the rank regularization to determine an appropriate number of component tensors.", "startOffset": 104, "endOffset": 108}, {"referenceID": 29, "context": "In addition to the CP rank, another notion of tensor rank is multilinear rank [30], which is defined as the tuple of the ranks of the mode-n unfoldings of the tensor.", "startOffset": 78, "endOffset": 82}, {"referenceID": 30, "context": "Multilinear rank is closely related to the Tucker decomposition since the multilinear rank is equivalent to the dimension of the smallest achievable core tensor in Tucker decomposition [31].", "startOffset": 185, "endOffset": 189}, {"referenceID": 16, "context": "Specifically, an alternating direction method of multipliers (ADMM) was developed in [17], [23] to minimize the tensor", "startOffset": 85, "endOffset": 89}, {"referenceID": 22, "context": "Specifically, an alternating direction method of multipliers (ADMM) was developed in [17], [23] to minimize the tensor", "startOffset": 91, "endOffset": 95}, {"referenceID": 16, "context": "The success of [17] has inspired a number of subsequent works [21], [22], [24], [25], [32], [33] for tensor completion and decomposition based on tensor nuclear norm minimization.", "startOffset": 15, "endOffset": 19}, {"referenceID": 20, "context": "The success of [17] has inspired a number of subsequent works [21], [22], [24], [25], [32], [33] for tensor completion and decomposition based on tensor nuclear norm minimization.", "startOffset": 62, "endOffset": 66}, {"referenceID": 21, "context": "The success of [17] has inspired a number of subsequent works [21], [22], [24], [25], [32], [33] for tensor completion and decomposition based on tensor nuclear norm minimization.", "startOffset": 68, "endOffset": 72}, {"referenceID": 23, "context": "The success of [17] has inspired a number of subsequent works [21], [22], [24], [25], [32], [33] for tensor completion and decomposition based on tensor nuclear norm minimization.", "startOffset": 74, "endOffset": 78}, {"referenceID": 24, "context": "The success of [17] has inspired a number of subsequent works [21], [22], [24], [25], [32], [33] for tensor completion and decomposition based on tensor nuclear norm minimization.", "startOffset": 80, "endOffset": 84}, {"referenceID": 31, "context": "The success of [17] has inspired a number of subsequent works [21], [22], [24], [25], [32], [33] for tensor completion and decomposition based on tensor nuclear norm minimization.", "startOffset": 86, "endOffset": 90}, {"referenceID": 32, "context": "The success of [17] has inspired a number of subsequent works [21], [22], [24], [25], [32], [33] for tensor completion and decomposition based on tensor nuclear norm minimization.", "startOffset": 92, "endOffset": 96}, {"referenceID": 20, "context": "Nevertheless, the tensor nuclear norm, albeit effective, is not necessarily the tightest convex envelope of the multilinear rank [21].", "startOffset": 129, "endOffset": 133}, {"referenceID": 33, "context": "Also, the over-relaxed monotone fast iterative shrinkage-thresholding technique [34] is adapted and embedded in the iterative reweighted process, which achieves a substantial reduction in computational complexity.", "startOffset": 80, "endOffset": 84}, {"referenceID": 34, "context": "Log-sum penalty function has been extensively used for sparse signal recovery and was shown to be more sparsity-encouraging than the l1-norm [35]\u2013[38].", "startOffset": 141, "endOffset": 145}, {"referenceID": 37, "context": "Log-sum penalty function has been extensively used for sparse signal recovery and was shown to be more sparsity-encouraging than the l1-norm [35]\u2013[38].", "startOffset": 146, "endOffset": 150}, {"referenceID": 38, "context": "This is different from the group-LASSO [39], [40] method in which entries are grouped into a number of non-overlapping subsets with sparsity imposed on each subset.", "startOffset": 39, "endOffset": 43}, {"referenceID": 39, "context": "This is different from the group-LASSO [39], [40] method in which entries are grouped into a number of non-overlapping subsets with sparsity imposed on each subset.", "startOffset": 45, "endOffset": 49}, {"referenceID": 30, "context": "It can be shown that n-rank is equivalent to the dimensions of the smallest achievable core tensor in Tucker decomposition [31].", "startOffset": 123, "endOffset": 127}, {"referenceID": 40, "context": "We resort to a bounded optimization approach, also known as the majorization-minimization (MM) approach [41], to solve the optimization (14).", "startOffset": 104, "endOffset": 108}, {"referenceID": 41, "context": "One famous first order method is the fast iterative shrinkagethresholding algorithm (FISTA) [42].", "startOffset": 92, "endOffset": 96}, {"referenceID": 33, "context": "Later on in [34], an over-relaxed monotone FISTA (MFISTA) was proposed to overcome some limitations inherent in the FISTA.", "startOffset": 12, "endOffset": 16}, {"referenceID": 33, "context": "It was proved in [34] that the sequence {x} is guaranteed to monotonically decrease the objective function F (x) and the convergence rate is O(1/k).", "startOffset": 17, "endOffset": 21}, {"referenceID": 42, "context": "The factor matrices and core tensor are initialized by decomposing the observed tensor (the missing elements are set to zero) with high order singular value decomposition [43].", "startOffset": 171, "endOffset": 175}, {"referenceID": 17, "context": "We compare our method with several existing state-of-theart tensor decomposition/completion methods, namely, a CP decomposition-based tensor completion method (also referred to as the low rank tensor imputation (LRTI)) which uses the Frobenius-norm of the factor matrices as the rank regularization [18], a tensor nuclear-norm based tensor completion method [23] which is also referred to as the high accuracy low rank tensor completion (HaLRTC) method, and a Tucker factorization method based on pre-specified multilinear rank [22] which is referred to as the WTucker method.", "startOffset": 299, "endOffset": 303}, {"referenceID": 22, "context": "We compare our method with several existing state-of-theart tensor decomposition/completion methods, namely, a CP decomposition-based tensor completion method (also referred to as the low rank tensor imputation (LRTI)) which uses the Frobenius-norm of the factor matrices as the rank regularization [18], a tensor nuclear-norm based tensor completion method [23] which is also referred to as the high accuracy low rank tensor completion (HaLRTC) method, and a Tucker factorization method based on pre-specified multilinear rank [22] which is referred to as the WTucker method.", "startOffset": 358, "endOffset": 362}, {"referenceID": 21, "context": "We compare our method with several existing state-of-theart tensor decomposition/completion methods, namely, a CP decomposition-based tensor completion method (also referred to as the low rank tensor imputation (LRTI)) which uses the Frobenius-norm of the factor matrices as the rank regularization [18], a tensor nuclear-norm based tensor completion method [23] which is also referred to as the high accuracy low rank tensor completion (HaLRTC) method, and a Tucker factorization method based on pre-specified multilinear rank [22] which is referred to as the WTucker method.", "startOffset": 528, "endOffset": 532}], "year": 2015, "abstractText": "We consider the problem of low-rank decomposition of incomplete multiway tensors. Since many real-world data lie on an intrinsically low dimensional subspace, tensor low-rank decomposition with missing entries has applications in many data analysis problems such as recommender systems and image inpainting. In this paper, we focus on Tucker decomposition which represents an N th-order tensor in terms of N factor matrices and a core tensor via multilinear operations. To exploit the underlying multilinear low-rank structure in high-dimensional datasets, we propose a group-based log-sum penalty functional to place structural sparsity over the core tensor, which leads to a compact representation with smallest core tensor. The method for Tucker decomposition is developed by iteratively minimizing a surrogate function that majorizes the original objective function, which results in an iterative reweighted process. In addition, to reduce the computational complexity, an over-relaxed monotone fast iterative shrinkage-thresholding technique is adapted and embedded in the iterative reweighted process. The proposed method is able to determine the model complexity (i.e. multilinear rank) in an automatic way. Simulation results show that the proposed algorithm offers competitive performance compared with other existing algorithms.", "creator": "LaTeX with hyperref package"}}}