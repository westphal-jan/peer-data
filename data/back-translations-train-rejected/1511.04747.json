{"id": "1511.04747", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "15-Nov-2015", "title": "Learning Representations of Affect from Speech", "abstract": "There has been a lot of prior work on representation learning for speech recognition applications, but not much emphasis has been given to an investigation of effective representations of affect from speech, where the paralinguistic elements of speech are separated out from the verbal content. In this paper, we explore denoising autoencoders for learning paralinguistic attributes i.e. dimensional affective traits from speech. We show that the representations learnt by the bottleneck layer of the autoencoder are highly discriminative at separating out negative sentiments (sadness and anger) from positive sentiments (happiness). We also learn utterance specific representations by a combination of denoising autoencoders and LSTM based recurrent autoencoders, and perform emotion classification with the learnt temporal/dynamic representations. Experiments on a well-established real-life speech dataset (IEMOCAP) show that the utterance representations are comparable to state of the art feature extractors (such as voice quality features and MFCCs) at emotion and affect recognition.", "histories": [["v1", "Sun, 15 Nov 2015 18:16:20 GMT  (596kb,D)", "http://arxiv.org/abs/1511.04747v1", "This is a submission for the ICLR (International Conference on learning Representations). The submission will be updated and revised by 11/19/2016, after the arXiv identifier is obtained"], ["v2", "Fri, 20 Nov 2015 01:37:01 GMT  (571kb)", "http://arxiv.org/abs/1511.04747v2", "This is a submission for the ICLR (International Conference on Learning Representations). The submission will be updated and revised by 11/19/2016, after the arXiv identifier is obtained"], ["v3", "Mon, 11 Jan 2016 20:44:51 GMT  (3088kb,D)", "http://arxiv.org/abs/1511.04747v3", "This is a submission for the ICLR (International Conference on Learning Representations). Final modified draft will be updated shortly"], ["v4", "Mon, 18 Jan 2016 20:36:36 GMT  (3088kb,D)", "http://arxiv.org/abs/1511.04747v4", "This is a submission for the ICLR (International Conference on Learning Representations). Final modified draft will be updated shortly"], ["v5", "Tue, 19 Jan 2016 04:05:50 GMT  (3088kb,D)", "http://arxiv.org/abs/1511.04747v5", "This is a submission for the ICLR (International Conference on Learning Representations). Final modified draft will be updated shortly"], ["v6", "Sun, 14 Feb 2016 18:11:46 GMT  (3088kb,D)", "http://arxiv.org/abs/1511.04747v6", "This is a submission for the ICLR (International Conference on Learning Representations) Workshop 2016"]], "COMMENTS": "This is a submission for the ICLR (International Conference on learning Representations). The submission will be updated and revised by 11/19/2016, after the arXiv identifier is obtained", "reviews": [], "SUBJECTS": "cs.CL cs.LG", "authors": ["sayan ghosh", "eugene laksana", "louis-philippe morency", "stefan scherer"], "accepted": false, "id": "1511.04747"}, "pdf": {"name": "1511.04747.pdf", "metadata": {"source": "CRF", "title": "SPEECH / CONFERENCE SUBMISSIONS", "authors": ["Sayan Ghosh", "Eugene Laksana", "Stefan Scherer"], "emails": ["sghosh@ict.usc.edu", "elaksana@ict.usc.edu", "scherer@ict.usc.edu", "morency@cs.cmu.edu"], "sections": [{"heading": null, "text": "Much has already been done on the learning of representational elements for speech recognition applications, but little emphasis has been placed on the study of effective representations of affects from language, in which the paralinguistic elements of language are separated from verbal content. In this paper, we examine the denocialization of autoencoders to learn paralinguistic attributes, i.e., dimensional affective characteristics from language. We show that the representations learned through the bottleneck layer of the autoencoder are highly differentiated in separating negative feelings (sadness and anger) from positive feelings (happiness). We also learn expression-specific representations through a combination of denoising autoencoders and LFCM-based recurring autoencoders, and perform an emotion classification with the learned temporal / dynamic representations. Experiments with a well-established real-life language dataset (IEMOCAP) show that the representations are comparable to the state of the art of recognition technology (such as language characteristics and)."}, {"heading": "1 INTRODUCTION", "text": "This has been made possible by developments in deep learning, where it has been observed that the stacking of layers in neural networks with large amounts of data has led to classification accuracies that have exceeded the state of the art in large-scale recognition problems, such as the challenge posed by the ILSVRC (Russakovsky et al.) and large-scale speech recognition (Hinton et al., 2012). However, there has been limited progress in the research of representation characteristics for non-phonetic and affective attributes from linguistic usage. Current research mainly focuses on the use of off-the-self feature extractors, such as the cepstral coefficients (MFCCs), and language quality characteristics (Scherer et al., 2013), which relate to normalized amplitude quotients and basic frequencies."}, {"heading": "2 PRIOR WORK IN REPRESENTATION LEARNING FROM SPEECH", "text": "Jaitly & Hinton (2011) modeled speech waveforms using RBMs (Restricted Boltzmann Machines) and used the extracted features for speech emotion recognition, achieving state-of-the-art phoneme recognition performance (using Melcepstrum coefficients). Graves et al. (2013) investigated deep, recurrent neural networks for speech recognition and achieved a 17.7% test rate error on TIMIT. Sainath et al. (2013) worked on speech emotion recognition in a deep neural network framework, finding that filter banks similar to the Mel scale were learned. They improved their work in Sainath et al. (2014) by incorporating delta learning and loudspeaker adaptation. Han et al. (2014) performed speech emotion recognition using a combination of DNN (Deep Neural Network) and extreme learning machines by using 20% relative accuracy improvement compared to taskforce weighting in 2015."}, {"heading": "3 MODEL AND EXPERIMENTAL SETUP", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "3.1 DENOISING AUTOENCODER", "text": "The autoencoder is a neural network in which the entered data set {xi} i = Ni = 1 is transferred into a forward-facing neural network of two layers, the middle layer being a bottleneck with activations {zi} i = Ni = 1, where the activations are obtained as: zi = tanh (W1xi).The reconstructed input x-i is obtained from the activations of the autoencoder as: x-i = tanh (W2zi) Thus, the autoencoder is a forward-facing neural network, and it is trained by means of backpropagation. Vincent et al. (2008) introduce denoising autoencoders in which the data point x-i is corrupted by Gaussian noise in order to generate x-i, from which the original clean data point xi is reconstructed by the autoencoder. This enables the autoencoder not only to learn latent information in the data, but also structural information between the elements of the autocoding layer (the weight of the layer 1)."}, {"heading": "3.2 DATASET USED", "text": "For our experiments, we used the IEMOCAP dataset (Busso et al., 2008), a well-known played dataset for speech recognition consisting of multimodal interactions of dyadic sessions between actors in which conversations are scripted to elicit emotional expressions; the dataset consists of approximately 12 hours of speech from 10 human subjects and is labeled by three commentators for emotions such as Happy, Sad, Angry, Excitement, Neutral and Surprise, along with dimensional labels such as Valence, Activation and Dominance. We use all datapoints for pre-training the auto-coder, but conduct classification experiments on only four emotions - Neutral, Angry, Sad and Happy. Each statement lasts about 2-5 seconds, with short periods of silence occurring in all utterances before and after the speech. Before the classification experiments, we divided the dataset into seven experiments (codials), during the training session (codials) and during the three."}, {"heading": "3.3 FEATURES EXTRACTED", "text": "We extracted spectrograms from the expressions with an image width of 25 ms, image overlaps of 10 ms and 513 FFT (Fast Fourier Transform) containers. We used a log scale in the frequency range because it has been shown that a stronger emphasis on lower frequencies is more important for auditory perception. Unlike other work in the field of speech emotion recognition, we do not extract off-the-shelf features, such as Mel frequency Cepstral Coefficient (MFCCs) or speech quality characteristics (Scherer et al., 2013), as we want to investigate whether the autoencoder is useful for detecting affective characteristics directly from the frequency representation of the speech signal."}, {"heading": "4 CORRELATION OF REPRESENTATIONS WITH AFFECTIVE ATTRIBUTES", "text": "In this section, we describe our experiments with the representation of affective traits using the stacked, denosing autoencoder. First, we present results with a single bottleneck layer of 256, upon which we greedily pre-train by adding additional layers; the bottleneck size was selected based on performance in a subsequent emotion classification task; the activations learned at each layer of the autoencoder are dimensionally reduced using tSNE (Van der Maaten & Hinton, 2008) and visualized at frame level with different emotion categories; and we investigate the variation of bottleneck representations with paralinguistic attributes such as valence, activation, and dominance."}, {"heading": "4.1 EMOTIONS", "text": "In Figure 1 we present scatter diagrams of the visualizations for four primary emotions - (1) Happy (2) Sad (3) Angry and (4) Neutral. From examining the diagrams we conclude that one layer of the autoencoder can separate the angry and sad emotions. Learned characteristics do not distinguish the happy and neutral categories so much. We expect that higher layers of the network will perceive these emotions more differentiated. It is also interesting that a cluster has been formed that corresponds to pauses and silence in language and has a completely different acoustics from language."}, {"heading": "4.2 VALENCE, DOMINANCE AND ACTIVATION", "text": "We also examine whether the autoencoder can learn affective attributes such as activation (intensity of emotions), valence (positive or negative feelings) and dominance (dominant and submissive emotions) from language. Figure 2 shows a scatter plot of learned representations colored for each attribute according to intensity (blue for lowest and red for highest). We find that the autoencoder represents the activation most differentiated, followed by valence and dominance. Sensitivity to activation can also be the reason for the separation of anger and sad emotions."}, {"heading": "5 EMOTION CLASSIFICATION WITH LEARNT REPRESENTATIONS", "text": "We use two layers of MLP (Multi-layer perceptron) with Tanh activations, with the bottom layer initialized on the weights of the pre-trained auto encoder. Let's represent the data set by language frame xij, where i denotes the statement index and j the time frame in the i-th utterance. If yij = f (xij) is the activation of the top layer in the dimensions of the K-emotion category predicted by the MLP, then the predicted emotion category Ci for the i-th utterance is given by: Ci = argmaxi j = Ti \u0445 j = 1 yij (k) We divided the data set into a training set of seven subjects and a test set of three subjects. The validation accuracy on this endured sentence (at statement level) is 47.63%. Emotions Happy and Sad were the most accurately classified, followed by neutral and neutral emotions."}, {"heading": "6 FUTURE WORK AND CONCLUSIONS", "text": "In this paper, we have studied the denoticizing autoencoder to learn the learning of affective and paralinguistic attributes through language. We have found that the representations learned from the autoencoder are sensitive to anger and sadness as emotions and to activation as dimensions. Emotion classification experiments show that a multi-layer perceptron, together with pre-trained layers of the autoencoder, achieves a classification accuracy of 47.63% on a pre-set validation theory.For future work, we would like to explore evolutionary autoencoders for the recognition of emotions through language."}], "references": [{"title": "Iemocap: Interactive emotional dyadic motion capture database", "author": ["Busso", "Carlos", "Bulut", "Murtaza", "Lee", "Chi-Chun", "Kazemzadeh", "Abe", "Mower", "Emily", "Kim", "Samuel", "Chang", "Jeannette N", "Sungbok", "Narayanan", "Shrikanth S"], "venue": "Language resources and evaluation,", "citeRegEx": "Busso et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Busso et al\\.", "year": 2008}, {"title": "Speech recognition with deep recurrent neural networks", "author": ["Graves", "Alan", "Mohamed", "Abdel-rahman", "Hinton", "Geoffrey"], "venue": "In Acoustics, Speech and Signal Processing (ICASSP),", "citeRegEx": "Graves et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Graves et al\\.", "year": 2013}, {"title": "Speech emotion recognition using deep neural network and extreme learning machine", "author": ["Han", "Kun", "Yu", "Dong", "Tashev", "Ivan"], "venue": "Proceedings of INTERSPEECH, ISCA, Singapore,", "citeRegEx": "Han et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Han et al\\.", "year": 2014}, {"title": "Deep neural networks for acoustic modeling in speech recognition: The shared views of four research groups", "author": ["Hinton", "Geoffrey", "Deng", "Li", "Yu", "Dong", "Dahl", "George E", "Mohamed", "Abdel-rahman", "Jaitly", "Navdeep", "Senior", "Andrew", "Vanhoucke", "Vincent", "Nguyen", "Patrick", "Sainath", "Tara N"], "venue": "Signal Processing Magazine, IEEE,", "citeRegEx": "Hinton et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Hinton et al\\.", "year": 2012}, {"title": "A new way to learn acoustic events", "author": ["Jaitly", "Navdeep", "Hinton", "Geoffrey E"], "venue": "Advances in Neural Information Processing Systems,", "citeRegEx": "Jaitly et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Jaitly et al\\.", "year": 2011}, {"title": "High-level feature representation using recurrent neural network for speech emotion recognition", "author": ["Lee", "Jinkyu", "Tashev", "Ivan"], "venue": "In Sixteenth Annual Conference of the International Speech Communication Association,", "citeRegEx": "Lee et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Lee et al\\.", "year": 2015}, {"title": "Learning filter banks within a deep neural network framework", "author": ["Sainath", "Tara N", "Kingsbury", "Brian", "Mohamed", "Abdel-rahman", "Ramabhadran", "Bhuvana"], "venue": "In Automatic Speech Recognition and Understanding (ASRU),", "citeRegEx": "Sainath et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Sainath et al\\.", "year": 2013}, {"title": "Improvements to filterbank and delta learning within a deep neural network framework", "author": ["Sainath", "Tara N", "Kingsbury", "Brian", "Mohamed", "Abdel-rahman", "Saon", "George", "Ramabhadran", "Bhuvana"], "venue": "In Acoustics, Speech and Signal Processing (ICASSP),", "citeRegEx": "Sainath et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Sainath et al\\.", "year": 2014}, {"title": "Investigating fuzzy-input fuzzy-output support vector machines for robust voice quality classification", "author": ["Scherer", "Stefan", "Kane", "John", "Gobl", "Christer", "Schwenker", "Friedhelm"], "venue": "Computer Speech & Language,", "citeRegEx": "Scherer et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Scherer et al\\.", "year": 2013}, {"title": "Visualizing data using t-sne", "author": ["Van der Maaten", "Laurens", "Hinton", "Geoffrey"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Maaten et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Maaten et al\\.", "year": 2008}, {"title": "Extracting and composing robust features with denoising autoencoders", "author": ["Vincent", "Pascal", "Larochelle", "Hugo", "Bengio", "Yoshua", "Manzagol", "Pierre-Antoine"], "venue": "In Proceedings of the 25th international conference on Machine learning,", "citeRegEx": "Vincent et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Vincent et al\\.", "year": 2008}], "referenceMentions": [{"referenceID": 3, "context": "), and large scale speech recognition (Hinton et al., 2012).", "startOffset": 38, "endOffset": 59}, {"referenceID": 8, "context": "Current research is mainly on the use of off-the-self feature extractors, such as Mel-Frequency Cepstral Coefficients (MFCCs), and voice quality features (Scherer et al., 2013) such as normalized amplitude quotient, and fundamental frequency).", "startOffset": 154, "endOffset": 176}, {"referenceID": 1, "context": "Graves et al. (2013) investigated deep recurrent neural networks for speech recognition, achieving a test set error of 17.", "startOffset": 0, "endOffset": 21}, {"referenceID": 1, "context": "Graves et al. (2013) investigated deep recurrent neural networks for speech recognition, achieving a test set error of 17.7% on TIMIT. Sainath et al. (2013) worked on filterbank learning in a deep neural network framework, where it was found that filterbanks similar to the Mel scale were learnt.", "startOffset": 0, "endOffset": 157}, {"referenceID": 1, "context": "Graves et al. (2013) investigated deep recurrent neural networks for speech recognition, achieving a test set error of 17.7% on TIMIT. Sainath et al. (2013) worked on filterbank learning in a deep neural network framework, where it was found that filterbanks similar to the Mel scale were learnt. They improved on their work in Sainath et al. (2014) by incorporating delta learning and speaker adaptation.", "startOffset": 0, "endOffset": 350}, {"referenceID": 1, "context": "Graves et al. (2013) investigated deep recurrent neural networks for speech recognition, achieving a test set error of 17.7% on TIMIT. Sainath et al. (2013) worked on filterbank learning in a deep neural network framework, where it was found that filterbanks similar to the Mel scale were learnt. They improved on their work in Sainath et al. (2014) by incorporating delta learning and speaker adaptation. Han et al. (2014) performed speech emotion recognition using a combination of DNN (Deep Neural Network) and Extreme Learning Machines.", "startOffset": 0, "endOffset": 424}, {"referenceID": 1, "context": "Graves et al. (2013) investigated deep recurrent neural networks for speech recognition, achieving a test set error of 17.7% on TIMIT. Sainath et al. (2013) worked on filterbank learning in a deep neural network framework, where it was found that filterbanks similar to the Mel scale were learnt. They improved on their work in Sainath et al. (2014) by incorporating delta learning and speaker adaptation. Han et al. (2014) performed speech emotion recognition using a combination of DNN (Deep Neural Network) and Extreme Learning Machines. They obtained 20% relative accuracy improvement compared to state-of-the-art approaches. Lee & Tashev (2015) use recurrent neural networks for speech emotion recognition, where the label of each frame is modeled as a sequence of random variables.", "startOffset": 0, "endOffset": 650}, {"referenceID": 10, "context": "Vincent et al. (2008) introduce denoising autoencoders, where the data point x\u0302i is corrupted with Gaussian noise to produce x\u0303i, from which the original clean data point xi is reconstructed by the autoencoder.", "startOffset": 0, "endOffset": 22}, {"referenceID": 0, "context": "For our experiments, we used the IEMOCAP dataset (Busso et al., 2008), which is a well-known acted dataset for speech emotion recognition, consists of multimodal interactions of dyadic sessions between actors, where conversations are scripted to elicit emotional expressions.", "startOffset": 49, "endOffset": 69}, {"referenceID": 8, "context": "any off-the-shelf features, such as Mel-Frequency Cepstral Coefficients (MFCCs), or voice quality features (Scherer et al., 2013), since we wish to investigate if the autoencoder is useful at discovering affective features directly from the frequency representation of the speech signal.", "startOffset": 107, "endOffset": 129}], "year": 2017, "abstractText": "There has been a lot of prior work on representation learning for speech recognition applications, but not much emphasis has been given to an investigation of effective representations of affect from speech, where the paralinguistic elements of speech are separated out from the verbal content. In this paper, we explore denoising autoencoders for learning paralinguistic attributes i.e. dimensional affective traits from speech. We show that the representations learnt by the bottleneck layer of the autoencoder are highly discriminative at separating out negative sentiments (sadness and anger) from positive sentiments (happiness). We also learn utterance specific representations by a combination of denoising autoencoders and LSTM based recurrent autoencoders, and perform emotion classification with the learnt temporal/dynamic representations. Experiments on a well-established reallife speech dataset (IEMOCAP) show that the utterance representations are comparable to state of the art feature extractors (such as voice quality features and MFCCs) at emotion and affect recognition.", "creator": "LaTeX with hyperref package"}}}