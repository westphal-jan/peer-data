{"id": "1511.04798", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "16-Nov-2015", "title": "Heterogeneous Knowledge Transfer in Video Emotion Recognition, Attribution and Summarization", "abstract": "Emotional content is a key element in user-generated videos. However, it is difficult to understand emotions conveyed in such videos due to the complex and unstructured nature of user-generated content and the sparsity of video frames that express emotion. In this paper, for the first time, we study the problem of transferring knowledge from heterogeneous external sources, including image and textual data, to facilitate three related tasks in video emotion understanding: emotion recognition, emotion attribution and emotion-oriented summarization. Specifically, our framework (1) learns a video encoding from an auxiliary emotional image dataset in order to improve supervised video emotion recognition, and (2) transfers knowledge from an auxiliary textual corpus for zero-shot \\pl{recognition} of emotion classes unseen during training. The proposed technique for knowledge transfer facilitates novel applications of emotion attribution and emotion-oriented summarization. A comprehensive set of experiments on multiple datasets demonstrate the effectiveness of our framework.", "histories": [["v1", "Mon, 16 Nov 2015 01:40:15 GMT  (5200kb,D)", "http://arxiv.org/abs/1511.04798v1", "13 pages, 11 figures"]], "COMMENTS": "13 pages, 11 figures", "reviews": [], "SUBJECTS": "cs.CV cs.AI cs.MM", "authors": ["baohan xu", "yanwei fu", "yu-gang jiang", "boyang li", "leonid sigal"], "accepted": false, "id": "1511.04798"}, "pdf": {"name": "1511.04798.pdf", "metadata": {"source": "CRF", "title": "Heterogeneous Knowledge Transfer in Video Emotion Recognition, Attribution and Summarization", "authors": ["Baohan Xu", "Yanwei Fu", "Yu-Gang Jiang", "Boyang Li", "Leonid Sigal"], "emails": ["bhxu14@fudan.edu.cn.", "ygj@fudan.edu.cn.", "y.fu@qmul.ac.uk,", "bert.li}@disneyresearch.com."], "sections": [{"heading": null, "text": "Index Terms - Video, Emotion Recognition, Transfer Learning, Zero-Shot Learning, Summary.F"}, {"heading": "1 INTRODUCTION", "text": "In addition to the recognition of objective content, such as objects and scenes, an important dimension of video understanding is emotional or affective content. Such content can have a strong impact on viewers and play a critical role in the video experience. Some success has been achieved with the use of deep learning architectures that have been trained for text on both sides and at the document level. [39] The ability to understand emotions from within the video remains largely an unsolved problem. Understanding emotional information in videos has many real-world applications, such as those used by YouTube and Netflix."}, {"heading": "2 RELATED WORK", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "2.1 Psychological Theories of Emotion", "text": "It is a widely held view in psychology that emotion contains a number of static categories, each of which is associated with stereotypical facial expressions, physiological measurements, behaviors, and external causes [15], [17]. Ekman's six basic pan-cultural feelings, including happiness, sadness, disgust, anger, fear, and surprise [16], [17] are probably the best known model. However, the exact categories may vary from one model to another. Plutschik [59] added expectation and trust to the list. Ortony, Clore, and Collins \"[56] model of emotion defined up to 22 emotions, including categories such as hope, shame, and severity. Nevertheless, recent empirical findings and theories from the psychological construction approach [3], [44] suggest that emotional experiences are much more diverse than previously thought. It is argued that the categories of modal or stereotypical emotions and large blurred areas of emotion exist."}, {"heading": "2.2 Automatic Emotion Analysis", "text": "In this section, we briefly consider three relevant areas of research: recognizing facial expressions from images and videos, recognizing the emotional impact of images on the viewer, and recognizing the emotional impact of video. Recognizing facial expressions from images and videos. In light of the results that people rely on contextual emotions to detect emotions [4], we aim to detect the overall emotional impact of a video from all of its information, in contrast to the identification of facial expressions and associated emotions in static images and videos, which are an object that has been extensively studied. Two recent reviews of the topic can be found in [58] and [65]. Several competitions, such as the Facial Expression Recognition and Analysis Challenge, the Audio / Visual Emotion Challenge [60] and the Emotion Recognition Recognition In The Wild Challenge [12], have been conducted."}, {"heading": "2.3 Multi-Instance Learning", "text": "The knowledge transfer approach used in this paper is related to multi-instance learning (MIL), which has been extensively studied in the machine learning community and applied in other areas. Therefore, we will review briefly related techniques below. MIL refers to recognition problems where each label is associated with a bag of instances, such as a bag of video frames. It has been used in many problems, such as predicting drug activity [14], speech recognition [61], image query, and classification [78]. The problem investigated in this paper is inherently a multi-instance learning case, as each video consists of many frames with potentially different emotions. There are essentially two branches of MIL algorithms. In the first industry, many studies have attempted to enable individually monitored learning algorithms that are directly applicable to multiple-instance feature bags [are applicable to multiple-instance feature bags]. This branch includes most of MIL data collection, such as MIL-75-1."}, {"heading": "2.4 Video Summarization", "text": "There are two types of video summaries that are used for different types of content, including videos."}, {"heading": "3 APPROACH", "text": "The overview of the proposed framework is illustrated in Figure 2. First, we present the problem formulation and common notation, and then discuss the additional image transmission encoding for supervised detection, the additional text-based transmission encoding for zero-shot detection, and the mapping and summary of video sensations."}, {"heading": "3.1 Problem Setup", "text": "Suppose we had a training video datasetTr = {Vi, Xi, si, zi} i = 1, \u00b7 \u00b7, nTr, where the video Vi = {fi, 1, \u00b7 \u00b7, fi, ni} ni frames, with the characteristics Xi = {xi, 1, \u00b7 \u00b7, xi, ni}, where the subscriptions i, j the jth frame of the video Vi. xi, j by the state-of-the-art, deep Convolutionary Neural Network (CNN) architecture, which was recently shown to significantly exceed more traditional handmade low-level features, such as HOG and SIFT, on several benchmark datasets, including MNIST and ImageNet [40] in machine learning and computer vision communities. Specifically, we retrain AlexNet [40] with 2, 600 ImageNet classes and use the seventh layer (\"fc7\") of displaying videos as xi, j, Zi composed."}, {"heading": "3.2 Auxiliary Image Transfer Encoding (ITE) for Supervised Emotion Recognition", "text": "In the multi-instance, we learn the emotional information from a large, multi-instance dataset in order to explain the contents using video clips, which means we group the instances of all the pockets into multiple groups. Examples are CCE [78] and Mi-FV [73]. Such clustering allows the individual pockets to be reproduced as the new Bag-of-Words (BoW) characteristics and their possible long-term progression and interaction. First, the feature is learned for the tasks of image classification rather than for video emotion recognition. Second, the large and complex space of visual elements (such as objects, events and people) and their possible time progression and interaction is the domain intrinsically more complex than the previous image sensation and MIL work. Third, emotion is often expressed in certain limited (sparse) keyframes or video clips, and many video clips that are not directly related to the emotions expressed."}, {"heading": "3.3 Zero-Shot Emotion Recognition", "text": "\"We must ask ourselves whether we are in a position to learn a new language.\" (\"We must learn,\" \"We must learn,\" \"We must learn,\" \"We must learn,\" \"We must learn,\" \"We must learn,\" \"We must learn,\" \"We must learn,\" \"We must learn,\" \"We must learn,\" \"We must learn,\" \"We must learn,\" \"We must learn,\" \"\" We must learn, \"(\" We must learn, \"\" We must learn, \"\" We, \"\" We, \"\" \"We,\" \"\" We, \"\" \"We,\" \"\" We, \"\" \"We,\" \"\" We, \"\" \"We,\" \"\" We, \"\" \"We,\" \"\" We, \"\" \"We,\" (\"We,\"), \"We,\" (\"We,\"), \"We,\" \"we,\" we, \"we,\" we, \"we,\" we, \"we,\" we, \"we,\" we, \"we,\" we. (\"We,\" we, \"we,\" we, \"we,\" we, \"we,\" we, \"we,\" we, \"we,\" we, \"we,\" we, \"we,\" we, \"we,\" we, \"we,\" we, \"we,\" we, \"we,\" we, \"we,\" we, \"we,\" we, \"we,\" we, \"we,\" we, \"we,\" we, \"we,\" we, \"we,\" we, \"we,\" we, \"we,\" we, \"we,\" we, \"we,\" we, \"we,\" we, \"we,\" we, \"we,\" we, \"we,\" we, \"we,\" we, \"we,\" we, \"we,\" we, \"we,\" we, \"we,\" we, \"we,\" we, \"we,\" we, \"we,\" we, \"we,\" we, \"we,\" we, \"we,\" we, \"we,\" we, \"we,\" we, \"we,\" we, \"we,\" we, \"we,\" we, \"we,\" we, \"we,\" we, \""}, {"heading": "3.4 Attribution and Summarization", "text": "We define the video mapping problem as one of attributing the emotion of a video to its frames / clips. The video-emotion mapping problem is inspired by, and yet different from, the text-based mapping of sentiment [39]. The difference is that the mapping of sentiment only takes into account positive or negative settings, while we take into account different emotions for the mapping of emotion. Emotion mapping can help us find video highlights that are the interesting or important events that happened in the video. Generally, the concepts of \"interesting\" and \"important\" video mappings can be variable for different target-video domains and applications, such as scoring a goal in football videos, applause and cheering in talk show videos, and exciting speech in presentation videos. Nevertheless, most of these \"interesting\" or \"important\" video events can contribute / convey very strong video emotions, thus highlighting the core parts of the entire video."}, {"heading": "4 EXPERIMENTS", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.1 Datasets and Settings", "text": "Most of them are able to outdo themselves by focusing on themselves, most of them are able to outdo themselves, most of them are able to outdo themselves, most of them are able to outdo themselves, most of them are able to outdo themselves, most of them are able to outdo themselves, most of them are able to outdo themselves, most of them are able to outdo themselves, most of them are able to outdo themselves, most of them are able to outdo themselves, most of them are able to outdo themselves, most of them are able to outdo themselves, most of them are able to outdo themselves, most of them are able to outdo themselves."}, {"heading": "4.2 Video Emotion Recognition", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.2.1 Supervised Recognition", "text": "We first report on the task of assisted emotion detection and compare our ITE encoding method with the following alternative baselines: MaxP [46]. The instance-level classifiers are trained with the labels inherited from their respective pockets. However, these classifiers can be used to predict instance names of test videos. The final labels of the pockets are produced by majority coordination of instance names. This method is a variant of Key Instance Detection (KID) [46] in multi-level multi-level labeling methods of video-level feature descriptions of instance descriptions. For the ith video, its average pooling function is calculated as 1 ni, j = 1 xi, j. Average pooling is the standard approach of aggregating frame-level descriptions to image-level descriptions, video descriptions are presented as a video-level description."}, {"heading": "4.2.2 Zero-Shot Recognition", "text": "We use anger, joy, surprise and terror as test classes in the Plutchik Dataset (300 test cases in total). We validate our methods on both versions of YouTube Dataset: YouTube8 and YouTube-24. For YouTube-8, we use fear and sadness as test classes. For YouTube-24, we randomly divide the 24 classes into 18 training classes and 6 test classes with 5-round repeat experiments. In this zero-shot setting, no test classes are seen during the training. First and second, we must emphasize that without the heterogeneous knowledge transmitted from the text, there is no way to facilitate zero-shot emotion learning.There is no previous work that has ever reported successful experiments on zero-shot emotion learning, mainly due to the difficulties in clearly defining the semantic word / attribute of emotion classes."}, {"heading": "4.2.3 Key Implementation Choices", "text": "In this subsection, we discuss the settings of two important experimental options within our framework. Number of clusters for emotional auxiliary images. We vary the number of clusters (i.e., D) of auxiliary images in Eq (2). These experiments are performed on the Ekman dataset. The results are presented in Figure 6, which further confirms the effectiveness of our ITE method: If we vary the number of auxiliary images from 100 to 5000, our ITE results will be progressively improved. This is because the increased number of clusters helps capture additional discriminatory information. Fine-tune the AlexNet CNN model. We also confirm the experiments by using the auxiliary images to refine the weights in the AlexNet CNN model. After the fine-tuning step, the classification on all three datasets has only slight changes (\u00b1 0.5%) and no significant provocations. We postulate that the videos in our datasets have more differences between words than the differences between images are relatively small."}, {"heading": "4.3 Video Emotion Attribution", "text": "As already mentioned, another advantage of our encryption scheme is that we can identify the video clips3 that have a high impact on the overall video emotion. As the first work on the assignment of video emotions, we define the evaluation protocol of the user study to evaluate the performance of the various algorithms for this task: Ten subjects who are unaware of the project objectives were invited to the user study. Given all the emotion keywords of the corresponding data set and clips calculated from the video, participants are asked to guess the name of the emotion expressed in the clip. These clips are selected by various methods discussed below. As the emotion labels of the video are known, we are able to calculate the fraction of the participants that contain the correct emotion label for each clip.We randomly select 20 videos from each of the three data sets. For each video, we calculate the 2-second video clip that contains the highest emotion label of the video we assign to the one that contains the highest emotion."}, {"heading": "4.4 Emotion-Oriented Video Summarization", "text": "Finally, we evaluate our frame on emotion-oriented video summary. We compare it with four baselines: (1) Uniform sampling. Several clips from the video are consistently sampled. (2) K-Means sampling. We simply group the clips and select a clip that comes closest to each cluster. (3) Story-driven summarization [49]. This approach is designed to summarize very long egocentric videos. We easily modify the implementation and make the length of the summary controllable for our task. (4) Real-time summarization [72]. Wang et al. [72] aim to efficiently summarize the videos based on semantic content recognition. For all methods, the length of the summary is set at 6 seconds if the original video is longer than 1 minute. The length is fixed at 10% of the original video duration."}, {"heading": "5 CONCLUSIONS", "text": "This paper is the first study of knowledge transfer from heterogeneous sources to the task of video emotional understanding, which includes supervised and zero-hot emotion recognition, emotion attribution and emotion-oriented summary. To effectively transfer knowledge, we learn encoding schemes from a large emotional image set and a large, 7 billion-word text corpus. This transfer facilitates the creation of a representation that is conducive to the tasks of video emotional comprehension. Zero-shot emotion recognition connects an unknown emotional word to known emotion classes by using a distributed representation to identify emotions that are not perceived during training. Our experiments with three challenging data sets clearly demonstrate the benefits of using external knowledge. Our framework also allows novel applications such as emotion mapping and emotion-oriented video summary. A user study shows that our summaries accurately capture and connect emotional content that is consistent with the motion of the original video as a whole."}, {"heading": "ACKNOWLEDGMENTS", "text": "The authors thank Chong-Wah Ngo for his constructive advice."}], "references": [{"title": "Multiple instance classification: Review, taxonomy and comparative study", "author": ["J. Amores"], "venue": "Artif. Intell.,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2013}, {"title": "Support vector machines for multiple-instance learning", "author": ["S. Andrews", "I. Tsochantaridis", "T. Hofmann"], "venue": "In NIPS,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2003}, {"title": "Are emotions natural kinds", "author": ["L.F. Barrett"], "venue": "Perspectives on Psychological Science,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2006}, {"title": "Language as context for the perception of emotion", "author": ["L.F. Barrett", "K.A. Lindquist", "M. Gendron"], "venue": "Trends in cognitive sciences,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2007}, {"title": "Largescale visual sentiment ontology and detectors using adjective noun pairs", "author": ["D. Borth", "R. Ji", "T. Chen", "T.M. Breuel", "S.-F. Chang"], "venue": "In ACM MM,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2013}, {"title": "Automatic detection of learningcentered affective states in the wild", "author": ["N. Bosch", "S. D\u2019Mello", "R. Baker", "J. Ocumpaugh", "V. Shute", "M. Ventura", "L. Wang", "W. Zhao"], "venue": "In the 2015 International Conference on Intelligent User Interfaces,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2015}, {"title": "Do facial expressions signal specific emotions? Judging emotion from the face in context", "author": ["J.M. Carroll", "J.A. Russell"], "venue": "Journal of Personality and Social Psychology,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 1996}, {"title": "Deepsentibank: Visual sentiment concept classification with deep convolutional neural networks", "author": ["T. Chen", "D. Borth", "Darrell", "S.-F. Chang"], "venue": null, "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2014}, {"title": "Miles: Multiple-instance learning via embedded instance selection", "author": ["Y. Chen", "J. Bi", "J.Z. Wang"], "venue": "IEEE TPAMI,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2006}, {"title": "Vision and attention theory based sampling for continuous facial emotion recognition", "author": ["A.C. Cruz", "B. Bhanu", "N. S"], "venue": "IEEE TAC,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2014}, {"title": "Video summarization by curve simplification", "author": ["D. DeMenthon", "V. Kobla", "D. Doermann"], "venue": "In ACM MM,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 1998}, {"title": "Emotion recognition in the wild challenge", "author": ["A. Dhall", "R. Goecke", "J. Joshi", "M. Wagner", "T. Gedeon"], "venue": "In ACM ICMI,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2013}, {"title": "Group expression intensity estimation in videos via gaussian processes", "author": ["A. Dhall", "G. Roland"], "venue": "In ICPR,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2012}, {"title": "Solving the multiple instance problem with axis-parallel rectangles", "author": ["T.G. Dietterich", "R.H. Lathrop", "T. Lozano-P\u00e9rez"], "venue": "Artif. Intell.,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 1997}, {"title": "Emotion, cognition, and behavior", "author": ["R.J. Dolan"], "venue": "Science, 298:1191\u2013", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2002}, {"title": "Universals and cultural differences in facial expressions of emotion", "author": ["P. Ekman"], "venue": "In Nebraska Symposium on Motivation,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 1972}, {"title": "An argument for basic emotions", "author": ["P. Ekman"], "venue": "Cognition & emotion,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 1992}, {"title": "Real-time inference of complex mental states from facial expressions and head gestures", "author": ["R. el Kaliouby", "P. Robinson"], "venue": "In CVPR,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2004}, {"title": "Multi-view metric learning for multi-view video summarization", "author": ["Y. Fu"], "venue": null, "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2014}, {"title": "Multi-view video summarization", "author": ["Y. Fu", "Y. Guo", "Y. Zhu", "F. Liu", "C. Song", "Z.-H. Zhou"], "venue": "IEEE TMM,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2010}, {"title": "Learning multimodal latent attributes", "author": ["Y. Fu", "T.M. Hospedales", "T. Xiang", "S. Gong"], "venue": "IEEE TPAMI,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2013}, {"title": "Interestingness prediction by robust learning to rank", "author": ["Y. Fu", "T.M. Hospedales", "T. Xiang", "S. Gong", "Y. Yao"], "venue": "In ECCV,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2014}, {"title": "Multiinstance kernels", "author": ["T. G\u00e4rtner", "P.A. Flach", "A. Kowalczyk", "A.J. Smola"], "venue": "Proc. 19th International Conf. on Machine Learning,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2002}, {"title": "Domain adaptation for largescale sentiment classification: A deep learning approach", "author": ["X. Glorot", "A. Bordes", "Y. Bengio"], "venue": "In ICML,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2011}, {"title": "Emotion regulation: Affective", "author": ["J.J. Gross"], "venue": "cognitive, and social consequences. Psychophysiology,", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2002}, {"title": "Videostory: A new multimedia embedding for few-example recognition and translation of events", "author": ["A. Habibian", "T. Mensink", "C.G.M. Snoek"], "venue": "In ACM MM,", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2014}, {"title": "An integrated scheme for automated video abstraction based on unsupervised cluster-validity analysis", "author": ["A. Hanjalic", "H. Zhang"], "venue": "IEEE TCSVT,", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 1999}, {"title": "Affective audio-visual words and latent topic driving model for realizing movie affective scene classification", "author": ["G. Irie", "T. Satou", "A. Kojima", "T. Yamasaki", "K. Aizawa"], "venue": "IEEE TMM,", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2010}, {"title": "A probabilistic framework for modeling and real-time monitoring human fatigue. Systems, Man and Cybernetics, Part A: Systems and Humans", "author": ["Q. Ji", "P. Lan", "C. Looney"], "venue": "IEEE Transactions on,", "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2006}, {"title": "Caffe: Convolutional architecture for fast feature embedding", "author": ["Y. Jia", "E. Shelhamer", "J. Donahue", "S. Karayev", "J. Long", "R. Girshick", "S. Guadarrama", "T. Darrell"], "venue": null, "citeRegEx": "32", "shortCiteRegEx": "32", "year": 2014}, {"title": "Predicting emotions in usergenerated videos", "author": ["Y.-G. Jiang", "B. Xu", "X. Xue"], "venue": "In AAAI,", "citeRegEx": "33", "shortCiteRegEx": "33", "year": 2014}, {"title": "Representations of keypoint-based semantic concept detection: A comprehensive study", "author": ["Y.-G. Jiang", "J. Yang", "C.-W. Ngo", "A.G. Hauptmann"], "venue": "IEEE TMM,", "citeRegEx": "34", "shortCiteRegEx": "34", "year": 2010}, {"title": "Understanding and predicting interestingness of videos", "author": ["Y.-G. Jiang", "YanranWang", "R. Feng", "X. Xue", "Y. Zheng", "H. Yang"], "venue": "In AAAI,", "citeRegEx": "35", "shortCiteRegEx": "35", "year": 2013}, {"title": "Predicting viewer perceived emotions in animated gifs", "author": ["B. Jou", "S. Bhattacharya", "S.-F. Chang"], "venue": "In ACM MM,", "citeRegEx": "36", "shortCiteRegEx": "36", "year": 2014}, {"title": "Affective content detection using HMMs", "author": ["H.-B. Kang"], "venue": "In ACM MM,", "citeRegEx": "37", "shortCiteRegEx": "37", "year": 2003}, {"title": "Automatic prediction of frustration", "author": ["A. Kapoor", "W. Burleson", "R.W. Picard"], "venue": "Int. J. Hum.-Comput. Stud.,", "citeRegEx": "38", "shortCiteRegEx": "38", "year": 2007}, {"title": "Deep multiinstance transfer learning", "author": ["D. Kotzias", "M. Denil", "P. Blunsom", "N. de Freitas"], "venue": null, "citeRegEx": "39", "shortCiteRegEx": "39", "year": 2014}, {"title": "Imagenet classification with deep convolutional neural networks", "author": ["A. Krizhevsky", "I. Sutskever", "G.E. Hinton"], "venue": "In NIPS,", "citeRegEx": "40", "shortCiteRegEx": "40", "year": 2012}, {"title": "Learning to detect unseen object classes by between-class attribute transfer", "author": ["C.H. Lampert", "H. Nickisch", "S. Harmeling"], "venue": "In CVPR,", "citeRegEx": "41", "shortCiteRegEx": "41", "year": 2009}, {"title": "Attribute-based classification for zero-shot visual object categorization", "author": ["C.H. Lampert", "H. Nickisch", "S. Harmeling"], "venue": "IEEE TPAMI,", "citeRegEx": "42", "shortCiteRegEx": "42", "year": 2013}, {"title": "A dynamic and dual-process theory of humor", "author": ["B. Li"], "venue": "In The 3rd Annual Conference on Advances in Cognitive Systems,", "citeRegEx": "43", "shortCiteRegEx": "43", "year": 2015}, {"title": "The brain basis of emotion: a meta-analytic review", "author": ["K.A. Lindquist", "T.D. Wager", "H. Kober", "E. Bliss-Moreau", "L.F. Barrett"], "venue": "Trends in cognitive sciences,", "citeRegEx": "44", "shortCiteRegEx": "44", "year": 2012}, {"title": "A hierarchical visual model for video object summarization", "author": ["D. Liu", "G. Hua", "T. Chen"], "venue": "IEEE TPAMI,", "citeRegEx": "45", "shortCiteRegEx": "45", "year": 2009}, {"title": "Key instance detection in multiinstance learning", "author": ["G. Liu", "J. Wu", "Z. Zhou"], "venue": null, "citeRegEx": "46", "shortCiteRegEx": "46", "year": 2012}, {"title": "Learning expressionlets on spatio-temporal manifold for dynamic facial expression recognition", "author": ["M. Liu", "S. Shan", "R. Wang", "X. Chen"], "venue": "In IEEE CVPR,", "citeRegEx": "47", "shortCiteRegEx": "47", "year": 2014}, {"title": "On shape and the computability of emotions", "author": ["X. Lu", "P. Suryanarayan", "R.B. Adams", "J. Li", "M.G. Newman", "J.Z. Wang"], "venue": "In ACM MM,", "citeRegEx": "48", "shortCiteRegEx": "48", "year": 2012}, {"title": "Story-driven summarization for egocentric video", "author": ["Z. Lu", "K. Grauman"], "venue": "In CVPR,", "citeRegEx": "49", "shortCiteRegEx": "49", "year": 2013}, {"title": "A user attention model for video summarization", "author": ["Y.-F. Ma", "L. Lu", "H.-J. Zhang", "M. Li"], "venue": "In ACM MM,", "citeRegEx": "50", "shortCiteRegEx": "50", "year": 2002}, {"title": "Affective image classication using features inspired by psychology and art theory", "author": ["J. Machajdik", "A. Hanbury"], "venue": "In ACM MM,", "citeRegEx": "51", "shortCiteRegEx": "51", "year": 2010}, {"title": "EMA: A process model of appraisal dynamics", "author": ["S. Marsella", "J. Gratch"], "venue": "Journal of Cognitive Systems Research,", "citeRegEx": "52", "shortCiteRegEx": "52", "year": 2009}, {"title": "Predicting ad liking and purchase intent: Large-scale analysis of facial responses to ads", "author": ["D. McDuff", "R.E. Kaliouby", "J.F. Cohn", "R. Picard"], "venue": "IEEE TAC,", "citeRegEx": "53", "shortCiteRegEx": "53", "year": 2015}, {"title": "Distributed representations of words and phrases and their compositionality", "author": ["T. Mikolov", "I. Sutskever", "K. Chen", "G. Corrado", "J. Dean"], "venue": "In NIPS,", "citeRegEx": "54", "shortCiteRegEx": "54", "year": 2013}, {"title": "Video summarization and scene detection by graph modeling", "author": ["C.-W. Ngo", "Y.-F. Ma", "H.-J. Zhang"], "venue": "IEEE TCSVT,", "citeRegEx": "55", "shortCiteRegEx": "55", "year": 2005}, {"title": "The Cognitive Structure of Emotions", "author": ["A. Ortony", "G. Clore", "A. Collins"], "venue": null, "citeRegEx": "56", "shortCiteRegEx": "56", "year": 1988}, {"title": "What\u2019s basic about basic emotions", "author": ["A. Ortony", "T.J. Turner"], "venue": "Psychological Review,", "citeRegEx": "57", "shortCiteRegEx": "57", "year": 1990}, {"title": "Machine analysis of facial behaviour: Naturalistic and dynamic behaviour", "author": ["M. Pantic"], "venue": "Philosophical Transactions of the Royal Society B: Biological Sciences,", "citeRegEx": "58", "shortCiteRegEx": "58", "year": 2009}, {"title": "Emotion: Theory, research, and experience", "author": ["R. Plutchik"], "venue": "In Theories of Emotion,", "citeRegEx": "59", "shortCiteRegEx": "59", "year": 1980}, {"title": "Avec 2011\u2014The first international audio/visual emotion challenge", "author": ["B. Schuller", "M.F. Valstar", "F. Eyben", "G. McKeown", "R. Cowie", "M. Pantic"], "venue": "ICACII,", "citeRegEx": "60", "shortCiteRegEx": "60", "year": 2011}, {"title": "Recognising interest in conversational speechcomparing bag of frames and supra-segmental features", "author": ["B.R.G. Schuller"], "venue": "In IN- TERSPEECH,", "citeRegEx": "61", "shortCiteRegEx": "61", "year": 2009}, {"title": "Smile or smirk? automatic detection of spontaneous asymmetric smiles to understand viewer experience", "author": ["T. Senechal", "J. Turcot", "R.E. Kaliouby"], "venue": "In IEEE FG,", "citeRegEx": "62", "shortCiteRegEx": "62", "year": 2013}, {"title": "Weakly supervised pain localization using multiple instance learning", "author": ["K B.M. Sikka", "A. Dhall"], "venue": "In IEEE FG,", "citeRegEx": "63", "shortCiteRegEx": "63", "year": 2013}, {"title": "Video google: a text retrieval approach to object matching in videos", "author": ["J. Sivic", "A. Zisserman"], "venue": "In ICCV,", "citeRegEx": "64", "shortCiteRegEx": "64", "year": 2003}, {"title": "Facial expression recognition", "author": ["Y. Tian", "T. Kanade", "J. Cohn"], "venue": "Handbook of Face Recognition,", "citeRegEx": "65", "shortCiteRegEx": "65", "year": 2011}, {"title": "Video abstraction: A systematic review and classification", "author": ["B.T. Truong", "S. Venkatesh"], "venue": "ACM TOMM,", "citeRegEx": "66", "shortCiteRegEx": "66", "year": 2007}, {"title": "The first facial expression recognition and analysis challenge", "author": ["M. Valstar", "B. Jiang", "M. Mehu", "M. Pantic", "S. Klaus"], "venue": "In IEEE FG,", "citeRegEx": "67", "shortCiteRegEx": "67", "year": 2011}, {"title": "Affective understanding in film", "author": ["H.-L. Wang", "L.-F. Cheong"], "venue": "IEEE TCSVT,", "citeRegEx": "68", "shortCiteRegEx": "68", "year": 2006}, {"title": "Solving the multiple-instance problem: A lazy learning approach", "author": ["J. Wang", "J.-D. Zucker"], "venue": "In ICML,", "citeRegEx": "69", "shortCiteRegEx": "69", "year": 2000}, {"title": "Event driven web video summarization by tag localization and key-shot identification", "author": ["M. Wang", "R. Hong", "G. Li", "Z.-J. Zha", "S. Yan", "T.-S. Chua"], "venue": "IEEE TMM,", "citeRegEx": "70", "shortCiteRegEx": "70", "year": 2012}, {"title": "Video affective content analysis: a survey of state of the art methods", "author": ["S. Wang", "Q. Ji"], "venue": "IEEE TAC,", "citeRegEx": "71", "shortCiteRegEx": "71", "year": 2015}, {"title": "Realtime summarization of user-generated videos based on semantic recognition", "author": ["X. Wang", "Y. Jiang", "Z. Chai", "Z. Gu", "X. Du", "D. Wang"], "venue": "In ACM MM,", "citeRegEx": "72", "shortCiteRegEx": "72", "year": 2014}, {"title": "Scalable multi-instance learning", "author": ["X.-S. Wei", "J. Wu", "Z.-H. Zhou"], "venue": "In ICDM,", "citeRegEx": "73", "shortCiteRegEx": "73", "year": 2014}, {"title": "Visual sentiment prediction with deep convolutional neural networks", "author": ["C. Xu", "S. Cetintas", "K.-C. Lee", "L.-J. Li"], "venue": null, "citeRegEx": "74", "shortCiteRegEx": "74", "year": 2014}, {"title": "Logistic regression and boosting for labeled bags of instances", "author": ["X. Xu", "E. Frank"], "venue": "In 8th Pacific-Asia Conference on Knowledge Discovery and Data Mining,", "citeRegEx": "75", "shortCiteRegEx": "75", "year": 2004}, {"title": "A discriminative CNN video representation for event detection", "author": ["Z. Xu", "Y. Yang", "A.G. Hauptmann"], "venue": null, "citeRegEx": "76", "shortCiteRegEx": "76", "year": 2014}, {"title": "Robust image sentiment analysis using progressively trained and domain transferred deep networks", "author": ["Q. You", "J. Luo", "H. Jin", "J. Yang"], "venue": "In AAAI,", "citeRegEx": "77", "shortCiteRegEx": "77", "year": 2015}, {"title": "Solving multi-instance problems with classifier ensemble based on constructive clustering", "author": ["Z.-H. Zhou", "M.-L. Zhang"], "venue": "Knowledge and Information Systems,", "citeRegEx": "78", "shortCiteRegEx": "78", "year": 2007}, {"title": "Face detection, pose estimation, and landmark localization in the wild", "author": ["X.R.D. Zhu"], "venue": "In CVPR,", "citeRegEx": "79", "shortCiteRegEx": "79", "year": 2012}], "referenceMentions": [{"referenceID": 36, "context": "Some successes have been achieved with the use of deeplearning architectures trained for text at both sentenceand document-level [39] or image sentiment analysis [8].", "startOffset": 129, "endOffset": 133}, {"referenceID": 7, "context": "Some successes have been achieved with the use of deeplearning architectures trained for text at both sentenceand document-level [39] or image sentiment analysis [8].", "startOffset": 162, "endOffset": 165}, {"referenceID": 21, "context": "Video recommendation services, such as those employed by YouTube and Netflix, can benefit from matching user interests with the emotions of video content and prediction of interestingness [23], [24], [35], leading to improved user satisfaction.", "startOffset": 188, "endOffset": 192}, {"referenceID": 32, "context": "Video recommendation services, such as those employed by YouTube and Netflix, can benefit from matching user interests with the emotions of video content and prediction of interestingness [23], [24], [35], leading to improved user satisfaction.", "startOffset": 200, "endOffset": 204}, {"referenceID": 2, "context": "This task is motivated by recent cognitive theories [3], [4], [7], [44] that suggest human emotional experiences extend beyond the traditional \u201cbasic emotion\u201d categories (e.", "startOffset": 52, "endOffset": 55}, {"referenceID": 3, "context": "This task is motivated by recent cognitive theories [3], [4], [7], [44] that suggest human emotional experiences extend beyond the traditional \u201cbasic emotion\u201d categories (e.", "startOffset": 57, "endOffset": 60}, {"referenceID": 6, "context": "This task is motivated by recent cognitive theories [3], [4], [7], [44] that suggest human emotional experiences extend beyond the traditional \u201cbasic emotion\u201d categories (e.", "startOffset": 62, "endOffset": 65}, {"referenceID": 41, "context": "This task is motivated by recent cognitive theories [3], [4], [7], [44] that suggest human emotional experiences extend beyond the traditional \u201cbasic emotion\u201d categories (e.", "startOffset": 67, "endOffset": 71}, {"referenceID": 15, "context": "Ekman\u2019s six emotions [16]).", "startOffset": 21, "endOffset": 25}, {"referenceID": 24, "context": "Emotion processes and other cognitive processes in our brain cooperate closely to create rich and diverse emotional and affective experiences [27], [43], [52] , such as ecstasy, nostalgia, or suspense.", "startOffset": 142, "endOffset": 146}, {"referenceID": 40, "context": "Emotion processes and other cognitive processes in our brain cooperate closely to create rich and diverse emotional and affective experiences [27], [43], [52] , such as ecstasy, nostalgia, or suspense.", "startOffset": 148, "endOffset": 152}, {"referenceID": 49, "context": "Emotion processes and other cognitive processes in our brain cooperate closely to create rich and diverse emotional and affective experiences [27], [43], [52] , such as ecstasy, nostalgia, or suspense.", "startOffset": 154, "endOffset": 158}, {"referenceID": 54, "context": "the boundary between which can be blurry (see, for example, the argument around surprise [57].", "startOffset": 89, "endOffset": 93}, {"referenceID": 3, "context": "Reproduced from [4].", "startOffset": 16, "endOffset": 19}, {"referenceID": 14, "context": "It is a widely held view in psychology that emotion contains a number of static categories, each of which is associated with stereotypical facial expressions, physiological measurements, behaviors, and external causes [15], [17].", "startOffset": 218, "endOffset": 222}, {"referenceID": 16, "context": "It is a widely held view in psychology that emotion contains a number of static categories, each of which is associated with stereotypical facial expressions, physiological measurements, behaviors, and external causes [15], [17].", "startOffset": 224, "endOffset": 228}, {"referenceID": 15, "context": "The most well known model is probably Ekman\u2019s six pan-cultural basic emotions, including happiness, sadness, disgust, anger, fear, and surprise [16], [17].", "startOffset": 144, "endOffset": 148}, {"referenceID": 16, "context": "The most well known model is probably Ekman\u2019s six pan-cultural basic emotions, including happiness, sadness, disgust, anger, fear, and surprise [16], [17].", "startOffset": 150, "endOffset": 154}, {"referenceID": 56, "context": "Plutchik [59] added anticipation and trust to the list.", "startOffset": 9, "endOffset": 13}, {"referenceID": 53, "context": "Ortony, Clore and Collins\u2019s [56] model of emotion defined up to 22 emotions, including categories like hope, shame, and gratitude.", "startOffset": 28, "endOffset": 32}, {"referenceID": 2, "context": "Nevertheless, more recent empirical findings and theories from the psychological construction approach [3], [44] suggest emotional experiences are much more varied than previously assumed.", "startOffset": 103, "endOffset": 106}, {"referenceID": 41, "context": "Nevertheless, more recent empirical findings and theories from the psychological construction approach [3], [44] suggest emotional experiences are much more varied than previously assumed.", "startOffset": 108, "endOffset": 112}, {"referenceID": 3, "context": "Instead of associating a fixed set of facial expressions with each emotion, emotion recognition is influenced by visual and language context [4], [7].", "startOffset": 141, "endOffset": 144}, {"referenceID": 6, "context": "Instead of associating a fixed set of facial expressions with each emotion, emotion recognition is influenced by visual and language context [4], [7].", "startOffset": 146, "endOffset": 149}, {"referenceID": 2, "context": "The facial expression of smile, for example, can indicate happiness, embarrassment, or being subordinate in different contexts [3].", "startOffset": 127, "endOffset": 130}, {"referenceID": 24, "context": "Other theories [27], [43], [52] highlight the dynamics of emotion and the interactions between emotional processes and other cognitive processes.", "startOffset": 15, "endOffset": 19}, {"referenceID": 40, "context": "Other theories [27], [43], [52] highlight the dynamics of emotion and the interactions between emotional processes and other cognitive processes.", "startOffset": 21, "endOffset": 25}, {"referenceID": 49, "context": "Other theories [27], [43], [52] highlight the dynamics of emotion and the interactions between emotional processes and other cognitive processes.", "startOffset": 27, "endOffset": 31}, {"referenceID": 3, "context": "In the light of findings that humans rely on contextual information to recognize emotion [4], [7], in this paper, we aim to recognize the overall emotional impact of a video from all of its information.", "startOffset": 89, "endOffset": 92}, {"referenceID": 6, "context": "In the light of findings that humans rely on contextual information to recognize emotion [4], [7], in this paper, we aim to recognize the overall emotional impact of a video from all of its information.", "startOffset": 94, "endOffset": 97}, {"referenceID": 55, "context": "Two recent reviews of the topic can be found in [58] and [65].", "startOffset": 48, "endOffset": 52}, {"referenceID": 62, "context": "Two recent reviews of the topic can be found in [58] and [65].", "startOffset": 57, "endOffset": 61}, {"referenceID": 64, "context": "Several competitions, such as the Facial Expression Recognition and Analysis Challenge [67], the Audio/Visual Emotion Challenge [60], and the Emotion Recognition In The Wild Challenge [12], have been held.", "startOffset": 87, "endOffset": 91}, {"referenceID": 57, "context": "Several competitions, such as the Facial Expression Recognition and Analysis Challenge [67], the Audio/Visual Emotion Challenge [60], and the Emotion Recognition In The Wild Challenge [12], have been held.", "startOffset": 128, "endOffset": 132}, {"referenceID": 11, "context": "Several competitions, such as the Facial Expression Recognition and Analysis Challenge [67], the Audio/Visual Emotion Challenge [60], and the Emotion Recognition In The Wild Challenge [12], have been held.", "startOffset": 184, "endOffset": 188}, {"referenceID": 44, "context": "[47] construct a mid-level representation called expressionlet from spatio-temporal manifold.", "startOffset": 0, "endOffset": 4}, {"referenceID": 9, "context": "[10] proposed a dynamic downsampling of facial expressions in video", "startOffset": 0, "endOffset": 4}, {"referenceID": 17, "context": "Kaliouby and Robinson [18] recognized emotion-related mental states, such as agreeing or feeling unsure.", "startOffset": 22, "endOffset": 26}, {"referenceID": 5, "context": "[6] detect learning-related affects including boredom, confusion, delight, engagement, and frustration; other work recognized smirk [62] / fatigue [31].", "startOffset": 0, "endOffset": 3}, {"referenceID": 59, "context": "[6] detect learning-related affects including boredom, confusion, delight, engagement, and frustration; other work recognized smirk [62] / fatigue [31].", "startOffset": 132, "endOffset": 136}, {"referenceID": 28, "context": "[6] detect learning-related affects including boredom, confusion, delight, engagement, and frustration; other work recognized smirk [62] / fatigue [31].", "startOffset": 147, "endOffset": 151}, {"referenceID": 48, "context": "Machajdik and Hanbury [51] classified images into 8 affective categories: amusement, awe, contentment, excitement, anger, disgust, fear, and sadness.", "startOffset": 22, "endOffset": 26}, {"referenceID": 45, "context": "[48] studied shape features along the dimensions of rounded-angular and simple-complex, and their effects in arousing viewers\u2019 emotions.", "startOffset": 0, "endOffset": 4}, {"referenceID": 74, "context": "[77] designed a deep convolutional neural network (CNN) for visual sentiment analysis.", "startOffset": 0, "endOffset": 4}, {"referenceID": 7, "context": "A few work [8], [74] also employed off-the-shelf CNN features.", "startOffset": 11, "endOffset": 14}, {"referenceID": 71, "context": "A few work [8], [74] also employed off-the-shelf CNN features.", "startOffset": 16, "endOffset": 20}, {"referenceID": 68, "context": "For a more comprehensive review, we refer reader to the latest survey [71].", "startOffset": 70, "endOffset": 74}, {"referenceID": 27, "context": "[30], [37], [68]).", "startOffset": 0, "endOffset": 4}, {"referenceID": 34, "context": "[30], [37], [68]).", "startOffset": 6, "endOffset": 10}, {"referenceID": 65, "context": "[30], [37], [68]).", "startOffset": 12, "endOffset": 16}, {"referenceID": 65, "context": "Wang and Cheong [68] used an SVM with diverse audio-visual features to classify 2040 scenes in 36 Hollywood movies into 7 emotions.", "startOffset": 16, "endOffset": 20}, {"referenceID": 33, "context": "[36] worked on animated GIF files.", "startOffset": 0, "endOffset": 4}, {"referenceID": 27, "context": "[30] use Latent Dirichlet Allocation to extract audio-visual topics as midlevel features, which are combined with a Hidden-Markovlike dynamic model.", "startOffset": 0, "endOffset": 4}, {"referenceID": 4, "context": "SentiBank [5] contains a set of 1, 553 adjective-noun pairs, such as \u201cbeautiful flowers\u201d and \u201csad eyes\u201d, and images exemplifying each pair.", "startOffset": 10, "endOffset": 13}, {"referenceID": 7, "context": "[8] replaced the SVM detectors with deep convolutional neural networks.", "startOffset": 0, "endOffset": 3}, {"referenceID": 30, "context": "[33] explored a large set of features and confirmed the effectiveness of mid-level representations like SentiBank.", "startOffset": 0, "endOffset": 4}, {"referenceID": 50, "context": "[53] analyzed facial expressions exhibited by viewers of video advertisements recorded with webcams.", "startOffset": 0, "endOffset": 4}, {"referenceID": 35, "context": "[38] used video, skin conductance, and pressure sensors on the chair and the mouse to predict frustration when a user interacted with an intelligent tutoring system.", "startOffset": 0, "endOffset": 4}, {"referenceID": 13, "context": "It has been used in many problems, such as drug activity prediction [14] , speech recognition [61], image retrieval and classification [78].", "startOffset": 68, "endOffset": 72}, {"referenceID": 58, "context": "It has been used in many problems, such as drug activity prediction [14] , speech recognition [61], image retrieval and classification [78].", "startOffset": 94, "endOffset": 98}, {"referenceID": 75, "context": "It has been used in many problems, such as drug activity prediction [14] , speech recognition [61], image retrieval and classification [78].", "startOffset": 135, "endOffset": 139}, {"referenceID": 0, "context": "This branch includes most of the early works on MIL [1], [63] such as miSVM [2], MIBoosting [75], Citation-kNN [69], MI-Kernel [25], among others.", "startOffset": 52, "endOffset": 55}, {"referenceID": 60, "context": "This branch includes most of the early works on MIL [1], [63] such as miSVM [2], MIBoosting [75], Citation-kNN [69], MI-Kernel [25], among others.", "startOffset": 57, "endOffset": 61}, {"referenceID": 1, "context": "This branch includes most of the early works on MIL [1], [63] such as miSVM [2], MIBoosting [75], Citation-kNN [69], MI-Kernel [25], among others.", "startOffset": 76, "endOffset": 79}, {"referenceID": 72, "context": "This branch includes most of the early works on MIL [1], [63] such as miSVM [2], MIBoosting [75], Citation-kNN [69], MI-Kernel [25], among others.", "startOffset": 92, "endOffset": 96}, {"referenceID": 66, "context": "This branch includes most of the early works on MIL [1], [63] such as miSVM [2], MIBoosting [75], Citation-kNN [69], MI-Kernel [25], among others.", "startOffset": 111, "endOffset": 115}, {"referenceID": 22, "context": "This branch includes most of the early works on MIL [1], [63] such as miSVM [2], MIBoosting [75], Citation-kNN [69], MI-Kernel [25], among others.", "startOffset": 127, "endOffset": 131}, {"referenceID": 75, "context": "Popular algorithms include constructive clustering based ensemble (CCE) [78], multi-instance learning based on the Fisher Vector representation (Mi-FV) [73] and multiinstance learning via embedded instance selection [9].", "startOffset": 72, "endOffset": 76}, {"referenceID": 70, "context": "Popular algorithms include constructive clustering based ensemble (CCE) [78], multi-instance learning based on the Fisher Vector representation (Mi-FV) [73] and multiinstance learning via embedded instance selection [9].", "startOffset": 152, "endOffset": 156}, {"referenceID": 8, "context": "Popular algorithms include constructive clustering based ensemble (CCE) [78], multi-instance learning based on the Fisher Vector representation (Mi-FV) [73] and multiinstance learning via embedded instance selection [9].", "startOffset": 216, "endOffset": 219}, {"referenceID": 63, "context": "A complete review is beyond the scope of this paper and we refer readers to [66].", "startOffset": 76, "endOffset": 80}, {"referenceID": 10, "context": "There are two main types of video summaries: keyframes [11], [19], [29], [45] and video skims [20], [55], [70], [72].", "startOffset": 55, "endOffset": 59}, {"referenceID": 18, "context": "There are two main types of video summaries: keyframes [11], [19], [29], [45] and video skims [20], [55], [70], [72].", "startOffset": 61, "endOffset": 65}, {"referenceID": 26, "context": "There are two main types of video summaries: keyframes [11], [19], [29], [45] and video skims [20], [55], [70], [72].", "startOffset": 67, "endOffset": 71}, {"referenceID": 42, "context": "There are two main types of video summaries: keyframes [11], [19], [29], [45] and video skims [20], [55], [70], [72].", "startOffset": 73, "endOffset": 77}, {"referenceID": 19, "context": "There are two main types of video summaries: keyframes [11], [19], [29], [45] and video skims [20], [55], [70], [72].", "startOffset": 94, "endOffset": 98}, {"referenceID": 52, "context": "There are two main types of video summaries: keyframes [11], [19], [29], [45] and video skims [20], [55], [70], [72].", "startOffset": 100, "endOffset": 104}, {"referenceID": 67, "context": "There are two main types of video summaries: keyframes [11], [19], [29], [45] and video skims [20], [55], [70], [72].", "startOffset": 106, "endOffset": 110}, {"referenceID": 69, "context": "There are two main types of video summaries: keyframes [11], [19], [29], [45] and video skims [20], [55], [70], [72].", "startOffset": 112, "endOffset": 116}, {"referenceID": 52, "context": ", movies or news reports) [55], [70], [72], surveillance videos [19],", "startOffset": 26, "endOffset": 30}, {"referenceID": 67, "context": ", movies or news reports) [55], [70], [72], surveillance videos [19],", "startOffset": 32, "endOffset": 36}, {"referenceID": 69, "context": ", movies or news reports) [55], [70], [72], surveillance videos [19],", "startOffset": 38, "endOffset": 42}, {"referenceID": 18, "context": ", movies or news reports) [55], [70], [72], surveillance videos [19],", "startOffset": 64, "endOffset": 68}, {"referenceID": 19, "context": "[20], and, to a lesser extent, user-generated videos [72].", "startOffset": 0, "endOffset": 4}, {"referenceID": 69, "context": "[20], and, to a lesser extent, user-generated videos [72].", "startOffset": 53, "endOffset": 57}, {"referenceID": 47, "context": "To extract the video summary, most approaches have to rely either on the low-level information, such as visual saliency [50] and motion cues [55]; or on the mid-level information e.", "startOffset": 120, "endOffset": 124}, {"referenceID": 52, "context": "To extract the video summary, most approaches have to rely either on the low-level information, such as visual saliency [50] and motion cues [55]; or on the mid-level information e.", "startOffset": 141, "endOffset": 145}, {"referenceID": 42, "context": "object trajectories [45], tag localization [70] and semantic recognition [72].", "startOffset": 20, "endOffset": 24}, {"referenceID": 67, "context": "object trajectories [45], tag localization [70] and semantic recognition [72].", "startOffset": 43, "endOffset": 47}, {"referenceID": 69, "context": "object trajectories [45], tag localization [70] and semantic recognition [72].", "startOffset": 73, "endOffset": 77}, {"referenceID": 12, "context": "Facial expressions has been considered by Dhall and Roland [13] who extracted video summaries by considering smile/happy face expression.", "startOffset": 59, "endOffset": 63}, {"referenceID": 37, "context": "xi,j is extracted by the state-of-the-art deep Convolutional Neural Network (CNN) architecture which was recently shown to greatly outperform more traditional hand-crafted low-level features, such as HOG and SIFT, on several benchmark datasets including MNIST and ImageNet [40] in machine learning and computer vision communities.", "startOffset": 273, "endOffset": 277}, {"referenceID": 37, "context": "Specifically, we retrain AlexNet [40] with 2, 600 ImageNet classes and use the seventh layer (\u201cfc7\u201d) representation as xi,j , computed on the input frame fi,j .", "startOffset": 33, "endOffset": 37}, {"referenceID": 75, "context": "Examples of this include CCE [78] and Mi-FV [73].", "startOffset": 29, "endOffset": 33}, {"referenceID": 70, "context": "Examples of this include CCE [78] and Mi-FV [73].", "startOffset": 44, "endOffset": 48}, {"referenceID": 61, "context": "Our encoding scheme in Eq (5) is different from the standard BoW [64] and soft-weighting BoW encoding [34].", "startOffset": 65, "endOffset": 69}, {"referenceID": 31, "context": "Our encoding scheme in Eq (5) is different from the standard BoW [64] and soft-weighting BoW encoding [34].", "startOffset": 102, "endOffset": 106}, {"referenceID": 61, "context": "Thus directly using standard BoW [64] to our problem will make the generated video-level features too sparse to be discriminative.", "startOffset": 33, "endOffset": 37}, {"referenceID": 16, "context": "Canonical emotion theories such as Ekman [17] often provide detailed textual definitions for a fixed number of", "startOffset": 41, "endOffset": 45}, {"referenceID": 2, "context": "However, recent research [3], [44] questioned the validity of basic emotional categories and highlighted differences within each category.", "startOffset": 25, "endOffset": 28}, {"referenceID": 41, "context": "However, recent research [3], [44] questioned the validity of basic emotional categories and highlighted differences within each category.", "startOffset": 30, "endOffset": 34}, {"referenceID": 51, "context": "Optimizing J is an effective way to learn continuous word representations, but the total computational cost has been intractable until recent deep learning developments such as word2vec [54], and GloVe [26].", "startOffset": 186, "endOffset": 190}, {"referenceID": 23, "context": "Optimizing J is an effective way to learn continuous word representations, but the total computational cost has been intractable until recent deep learning developments such as word2vec [54], and GloVe [26].", "startOffset": 202, "endOffset": 206}, {"referenceID": 39, "context": "Similar support vector classifiers have also been used in the attribute learning works [22], [42].", "startOffset": 93, "endOffset": 97}, {"referenceID": 20, "context": "To ameliorate such generalization problems, we take inspiration from [21] and apply Transductive 1-Step Self-Training (T1S) to adjust the word vector of new emotion classes.", "startOffset": 69, "endOffset": 73}, {"referenceID": 20, "context": "Compared with the zero-shot learning algorithm in [21], we skip the intermediate level of latent attributes and directly apply the 1-step self-training in the semantic word vector space.", "startOffset": 50, "endOffset": 54}, {"referenceID": 36, "context": "The video emotion attribution problem is inspired by, and yet different from, text-based sentiment attribution [39].", "startOffset": 111, "endOffset": 115}, {"referenceID": 26, "context": "Emotion attribution can help us find video highlights [29], which are the interesting or important events happened in the video.", "startOffset": 54, "endOffset": 58}, {"referenceID": 18, "context": "Comparing with previous work [19], [20], [29], [66], [72], Eq (14) considers the summary of both video highlights (by the first term for emotion attribution) and information coverage (by the second term for eliminating redundancy and selecting information-centric frames/clips).", "startOffset": 29, "endOffset": 33}, {"referenceID": 19, "context": "Comparing with previous work [19], [20], [29], [66], [72], Eq (14) considers the summary of both video highlights (by the first term for emotion attribution) and information coverage (by the second term for eliminating redundancy and selecting information-centric frames/clips).", "startOffset": 35, "endOffset": 39}, {"referenceID": 26, "context": "Comparing with previous work [19], [20], [29], [66], [72], Eq (14) considers the summary of both video highlights (by the first term for emotion attribution) and information coverage (by the second term for eliminating redundancy and selecting information-centric frames/clips).", "startOffset": 41, "endOffset": 45}, {"referenceID": 63, "context": "Comparing with previous work [19], [20], [29], [66], [72], Eq (14) considers the summary of both video highlights (by the first term for emotion attribution) and information coverage (by the second term for eliminating redundancy and selecting information-centric frames/clips).", "startOffset": 47, "endOffset": 51}, {"referenceID": 69, "context": "Comparing with previous work [19], [20], [29], [66], [72], Eq (14) considers the summary of both video highlights (by the first term for emotion attribution) and information coverage (by the second term for eliminating redundancy and selecting information-centric frames/clips).", "startOffset": 53, "endOffset": 57}, {"referenceID": 30, "context": "The YouTube emotion dataset [33].", "startOffset": 28, "endOffset": 32}, {"referenceID": 25, "context": "This dataset is derived from the recently proposed VideoStory dataset [28].", "startOffset": 70, "endOffset": 74}, {"referenceID": 56, "context": "We use the keywords of the Plutchik\u2019s Wheel of Emotions [59] to query the textual descriptions of the VideoStory dataset.", "startOffset": 56, "endOffset": 60}, {"referenceID": 16, "context": "As discussed in the related work, the studies of Ekman [17] found a high agreement of emotions across cultures can be labelled as 6 basic emotion types.", "startOffset": 55, "endOffset": 59}, {"referenceID": 4, "context": "From the Flickr image dataset [5], we select as the auxiliary image data a subset of 110K images of Adjective-Noun Pairs (ANPs) that have top ranks with respect to the emotions (see Table 2 in [5]).", "startOffset": 30, "endOffset": 33}, {"referenceID": 4, "context": "From the Flickr image dataset [5], we select as the auxiliary image data a subset of 110K images of Adjective-Noun Pairs (ANPs) that have top ranks with respect to the emotions (see Table 2 in [5]).", "startOffset": 193, "endOffset": 196}, {"referenceID": 51, "context": "As shown in [54], the large-scale text data can greatly benefit the trained language model.", "startOffset": 12, "endOffset": 16}, {"referenceID": 29, "context": "Our AlexNet CNN model is trained by ourselves using 2, 600 ImageNet classes with the Caffe toolkit [32], and we use the 4, 096-dimensional activations of the 7th fully-connected layer after the Rectified", "startOffset": 99, "endOffset": 103}, {"referenceID": 43, "context": "MaxP [46].", "startOffset": 5, "endOffset": 9}, {"referenceID": 43, "context": "This method is a variant of the Key Instance Detection (KID) [46] in multi-class multi-instance setting.", "startOffset": 61, "endOffset": 65}, {"referenceID": 73, "context": "AvgP [76].", "startOffset": 5, "endOffset": 9}, {"referenceID": 73, "context": "The average pooling is the standard approach of aggregating frame-level features into video-level descriptions as mentioned in [76].", "startOffset": 127, "endOffset": 131}, {"referenceID": 70, "context": "Mi-FV [73].", "startOffset": 6, "endOffset": 10}, {"referenceID": 75, "context": "CCE [78].", "startOffset": 4, "endOffset": 8}, {"referenceID": 75, "context": "Such representations may have better performance than the hand-crafted features used in [78], but they cannot beat the recently proposed deep", "startOffset": 88, "endOffset": 92}, {"referenceID": 37, "context": "features, which have been shown to be able to extract higher level information [40].", "startOffset": 79, "endOffset": 83}, {"referenceID": 38, "context": "We compare our T1S algorithm with Direct Attribution Prediction (DAP) [41], [42].", "startOffset": 70, "endOffset": 74}, {"referenceID": 39, "context": "We compare our T1S algorithm with Direct Attribution Prediction (DAP) [41], [42].", "startOffset": 76, "endOffset": 80}, {"referenceID": 76, "context": "\u201cface_present\u201d features [79] is used to rank all the videos frames.", "startOffset": 24, "endOffset": 28}, {"referenceID": 46, "context": "(3) Story-driven summarization [49].", "startOffset": 31, "endOffset": 35}, {"referenceID": 69, "context": "(4) Real-time summarization [72].", "startOffset": 28, "endOffset": 32}, {"referenceID": 69, "context": "[72] aim at efficient summarization of videos based on semantic content recognition results.", "startOffset": 0, "endOffset": 4}, {"referenceID": 63, "context": "Following [66], we conduct a user study to evaluate different summarization methods.", "startOffset": 10, "endOffset": 14}], "year": 2015, "abstractText": "Emotional content is a key element in user-generated videos. However, it is difficult to understand emotions conveyed in such videos due to the complex and unstructured nature of user-generated content and the sparsity of video frames that express emotion. In this paper, for the first time, we study the problem of transferring knowledge from heterogeneous external sources, including image and textual data, to facilitate three related tasks in video emotion understanding: emotion recognition, emotion attribution and emotion-oriented summarization. Specifically, our framework (1) learns a video encoding from an auxiliary emotional image dataset in order to improve supervised video emotion recognition, and (2) transfers knowledge from an auxiliary textual corpus for zero-shot recognition of emotion classes unseen during training. The proposed technique for knowledge transfer facilitates novel applications of emotion attribution and emotion-oriented summarization. A comprehensive set of experiments on multiple datasets demonstrate the effectiveness of our framework.", "creator": "LaTeX with hyperref package"}}}