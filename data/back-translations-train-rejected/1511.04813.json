{"id": "1511.04813", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "16-Nov-2015", "title": "Budget Online Multiple Kernel Learning", "abstract": "Online learning with multiple kernels has gained increasing interests in recent years and found many applications. For classification tasks, Online Multiple Kernel Classification (OMKC), which learns a kernel based classifier by seeking the optimal linear combination of a pool of single kernel classifiers in an online fashion, achieves superior accuracy and enjoys great flexibility compared with traditional single-kernel classifiers. Despite being studied extensively, existing OMKC algorithms suffer from high computational cost due to their unbounded numbers of support vectors. To overcome this drawback, we present a novel framework of Budget Online Multiple Kernel Learning (BOMKL) and propose a new Sparse Passive Aggressive learning to perform effective budget online learning. Specifically, we adopt a simple yet effective Bernoulli sampling to decide if an incoming instance should be added to the current set of support vectors. By limiting the number of support vectors, our method can significantly accelerate OMKC while maintaining satisfactory accuracy that is comparable to that of the existing OMKC algorithms. We theoretically prove that our new method achieves an optimal regret bound in expectation, and empirically found that the proposed algorithm outperforms various OMKC algorithms and can easily scale up to large-scale datasets.", "histories": [["v1", "Mon, 16 Nov 2015 03:40:50 GMT  (46kb)", "http://arxiv.org/abs/1511.04813v1", null], ["v2", "Wed, 18 Nov 2015 08:08:43 GMT  (35kb)", "http://arxiv.org/abs/1511.04813v2", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["jing lu", "steven c h hoi", "doyen sahoo", "peilin zhao"], "accepted": false, "id": "1511.04813"}, "pdf": {"name": "1511.04813.pdf", "metadata": {"source": "CRF", "title": null, "authors": ["Peilin Zhao"], "emails": ["chhoi@smu.edu.sg", "jing.lu.2014@phdis.smu.edu.sg", "doyensahoo.2014@phdis.smu.edu.sg", "zhaop@i2r.a-star.edu.sg"], "sections": [{"heading": null, "text": "ar Xiv: 151 1,04 813v 1 [cs.L G] 16 Nov 2"}, {"heading": "1 INTRODUCTION", "text": "In fact, it is so that most of them are able to survive themselves, and that they are able to survive themselves. (...) Most of them are able to survive themselves. (...) Most of them are able to survive themselves. (...) Most of them are able to survive themselves. (...) Most of them are able to survive themselves. (...) Most of them are able to survive themselves. (...) Most of them are able to survive themselves. (...) Most of them are able to survive themselves. (...) Most of them are able to survive themselves. (...) Most of them are able to survive themselves. (...) Most of them are able to survive themselves. (...)"}, {"heading": "2 BUDGET ONLINE MULTIPLE KERNEL LEARNING", "text": "In this section we will first formulate the problem for the Online Multiple Kernel Classification (OMKC) and then present the proposed Budget Online Multiple Kernel Classification (BOMKC) with Sparse Passive Aggressive (SPA) Learning for classification tasks."}, {"heading": "2.1 PROBLEM SETTING AND PRELIMINARIES", "text": "In a typical binary classification task, our goal is to learn a function f: Rd \u2192 R from a sequence of learning examples (x1, y1),.., (xT, yT) where the feature vector xt-X-Rd and the class label yt-Y = {+ 1, \u2212 1}. We use y = characters (f (x)) to predict the class label, and | f (x) | to safely measure the classification. Consider a collection of core m functions K = {+ 1, \u2212 R, i = 1,.,., m}. Each can be a predefined parametric or non-parametric function. MKC aims to learn a kernel-based prediction model by identifying the best linear combination of m-kernel functions whose weights are determined by the respective way."}, {"heading": "2.2 SPARSE PASSIVE AGGRESSIVE LEARNING FOR ONLINE MULTIPLE KERNEL LEARNING", "text": "In order to overcome the critical limitations of existing OMKC algorithms, we seek to explore new techniques to build scalable and efficient OMKC algorithms. (ii) Similar to the existing OMKC algorithms, we will update the details of our proposed SPA algorithm in two steps: (i) we will update the individual kernel classifiers individually; (ii) we will update the weights for the combination of classifiers. (ii) Subsequently, we will update the details of our proposed SPA algorithms. (i) We will generalize the individual kernel classifiers (Crammer et al., 2006), the online hypothesis will be updated: ft + 1 = min f HB \u2212 ft \u2212 ft \u2212 ft The strategy of each kernel classifier will be generalized. (f), where we will take over the hinge losses. (f) = [1 tf \u2212 +] This includes two objectives."}, {"heading": "3 THEORETICAL ANALYSIS", "text": "In this section, we will provide detailed theoretical analysis of the losses of our proposed SPA algorithms."}, {"heading": "4 EXPERIMENTS", "text": "In this section, we conduct extensive experiments to evaluate the empirical performance of the proposed SPA multi-core algorithm for online binary classification tasks."}, {"heading": "4.1 EXPERIMENTAL TESTBED", "text": "Table 1 summarizes details of the binary classification datasets used in our experiments. All datasets are commonly used benchmark datasets and are available on the LIBSVM1 and KDDCUP competition pages. These datasets are selected relatively randomly to cover a wide variety of sizes.1http: / / www.csie.ntu.edu.tw / cjlin / libsvmtools / datasets /"}, {"heading": "4.2 KERNELS", "text": "In our experiments we examine BOMKC by examining a set of 16 predefined nuclei, including 3 polynomial nuclei \u0443 (xi, xj) = (x i xj) p with the degree parameter p = 1, 2, 3, 13 Gaussian nuclei \u0445 (xi, xj) = exp (\u2212 | xi \u2212 xj | | 2 2\u03c32) with the parameter of the nuclear width \u03c3 = [2 \u2212 6, 2 \u2212 5,..., 26]."}, {"heading": "4.3 COMPARED ALGORITHMS", "text": "First, we include an important basic algorithm that shows the best performance that could be achieved by a single kernel classifier, assuming an optimal choice of kernel function before the training instances arrive. Specifically, we search for the best single-kernel classifier from the set of our predefined 16 cores, using a random permutation of all training examples, and then apply the \"Perceptron\" algorithm (Rosenblatt, 1958) with the best kernel function. Our second group of comparative algorithms are the online multiple-kernel classification algorithms (Hoi et al., 2013), which achieve state-of-the-art performance on many benchmark datasets."}, {"heading": "4.4 PARAMETER SETTINGS", "text": "In order to make a fair comparison, we use the same experimental setup for all algorithms; the weight discount parameter \u03b3 is set at 0.99 for all multiple kernel algorithms; the smoothing parameter \u03b4 for all stochastic update algorithms is set at 0.001; the parameters of the learning rate in all algorithms (SPA, BOGD and BPAS) are all set at 0.1; for the proposed SPA algorithm, we simply set the random sampling parameter \u03b1 = 1 and \u03b2 = 3 for the first 8 data sets and discuss the parameter sensitivity later; for a fair comparison between the budget algorithms, we set a uniform limit B for the SV size for all component classifiers, so that the total number of SV used by all component classifiers 16B is almost equal to the SV size of the SPA algorithm. In order to determine the scalability of our proposed algorithm, we also have set the 16 million data sets for one experiment."}, {"heading": "4.5 EVALUATION OF ONLINE LEARNING PERFORMANCE WITH COMPARISON TO NON-BUDGET OMKL ALGORITHMS", "text": "The first experiment is to evaluate the performance of SPA for binary classification tasks by comparing it to non-budget OMKL algorithms. We did not report the results of the three largest data sets because it is almost impossible for non-budget algorithms to process large-scale data in limited time and memory. We can generally estimate the error rate of OMKC (DD) much lower than the accuracy of the three algorithms with deterministic updates and full SVKs (U)."}, {"heading": "4.6 EVALUATION OF ONLINE LEARNING PERFORMANCE WITH COMPARISON TO DIFFERENT BUDGET OMKL ALGORITHMS", "text": "The next experiment is to test the accuracy and efficiency of our proposed SPA algorithm compared to other budget maintenance strategies \u03b2 \u03b2 \u03b2. Table 3 summarizes the experimental results \u03b2 \u03b2. The results show that our SPA algorithm achieves the best accuracy of all budget algorithms in most cases. Furthermore, the time taken by the proposed SPA algorithm is always the lowest. This is due to the advantage of our algorithm design: SPA can find the optimal kernel and concentrate the effort in that kernel. Therefore, poor cores will receive only a few SVs and thus the prediction time will be significantly shortened. If the same number of SVs are used as other budget algorithms instead of paying the same attention to all components, SPA focuses on the best kernel and thus achieves the highest accuracy. 4.7 PARAMETER SENSITIVITY SENIVITY SENITY SENITY OF-AND The proposed SPA algorithm has two critical parameters."}, {"heading": "5 CONCLUSIONS", "text": "In contrast to the existing Online Multiple Kernel Classification (OMKC) algorithms, which normally suffer from extremely high time costs due to their unlimited number of support vectors in the online learning process, our proposed algorithm uses a simple but effective SV sampling strategy, making it applicable to large-scale applications. We have theoretically shown that the SPA algorithm enjoyed optimal expectation. Furthermore, the experimental results showed that the proposed method achieved a significant acceleration and delivered more precise classifiers than existing methods, confirming the effectiveness, efficiency and scalability of the proposed technology compared to both the existing non-budget OMKL algorithms and the budget OMKL algorithms."}, {"heading": "APPENDIX A: PROOF FOR LEMMA 1", "text": "The proof: The Pt (f) defined in the equation is not (f), but (f), (f), (f), (f), (f), (f), (f), (f), (f), (f), (f), (f), (f), (f), (f), (f), (f), (f), (f), (f), (f), (f), (f), (f), (f), (f), (f), (f), (f), (f), (f), (f), (f), (f), (f), (f), (f), (f), (f), (f), (f), (f), (f), (f), (f), (f), (f), (f), (f), (f), (f), (f), (f)."}, {"heading": "APPENDIX B: PROOF FOR THEOREM 1", "text": "Proof: In the following proof, we first generalize the limit of loss of the hedge algorithm (Freund & Schapire, 1995) to another situation where 1) stochastic actualization and stochastic combination are adopted and 2) the hinge loss function that we adopt is limited to L (f), L (0, L) and L (1). On the basis of the convexity, we then have the proposed convexity i = 1 \u2212 1 \u2212 p (it), where \u00b5 > 1 fulfills the equality if it = L and this constant depends only on L. We then get the convexity i = 1wit + 1 = m (it) that we have proposed, i = 1wit (it), T = 1wit (it), T = 1wit (it), (t), (t), (t), (t)."}, {"heading": "APPENDIX C: PROOF FOR THEOREM 2", "text": "Proof: Since Et [Zt] = \u03c1t, where Et is the conditional expectation, we have E [T-T = 1Zt] = E [T-T-T = 1EtZt] = E [T-T = 1\u03c1t] = E [T-T = 1min (\u03b1-\u03b2, T-T (ft) \u03b2)] \u2264 min (\u03b1-T, 1 \u03b2 ET-T = 1\u0445 t (ft))) \u2264 min {\u03b1\u03b2 T, 1 \u03b2 [T-T = 1\u0445 t (f-T) + 12\u03b7-F-N-N-N (\u03b1-N) T], which concludes the first part of the theorem."}], "references": [{"title": "Tracking the best hyperplane with a simple budget perceptron", "author": ["Cavallanti", "Giovanni", "Cesa-Bianchi", "Nicol\u00f2", "Gentile", "Claudio"], "venue": "Machine Learning,", "citeRegEx": "Cavallanti et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Cavallanti et al\\.", "year": 2007}, {"title": "Online passiveaggressive algorithms", "author": ["Crammer", "Koby", "Dekel", "Ofer", "Keshet", "Joseph", "Shalev-Shwartz", "Shai", "Singer", "Yoram"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "Crammer et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Crammer et al\\.", "year": 2006}, {"title": "The forgetron: A kernel-based perceptron on a fixed budget", "author": ["Dekel", "Ofer", "Shalev-Shwartz", "Shai", "Singer", "Yoram"], "venue": "In NIPS,", "citeRegEx": "Dekel et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Dekel et al\\.", "year": 2005}, {"title": "A desicion-theoretic generalization of on-line learning and an application to boosting", "author": ["Freund", "Yoav", "Schapire", "Robert E"], "venue": "In Computational learning theory,", "citeRegEx": "Freund et al\\.,? \\Q1995\\E", "shortCiteRegEx": "Freund et al\\.", "year": 1995}, {"title": "Multiple kernel learning algorithms", "author": ["G\u00f6nen", "Mehmet", "Alpayd\u0131n", "Ethem"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "G\u00f6nen et al\\.,? \\Q2011\\E", "shortCiteRegEx": "G\u00f6nen et al\\.", "year": 2011}, {"title": "Online multiple kernel classification", "author": ["Hoi", "Steven CH", "Jin", "Rong", "Zhao", "Peilin", "Yang", "Tianbao"], "venue": "Machine Learning,", "citeRegEx": "Hoi et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Hoi et al\\.", "year": 2013}, {"title": "Online multiple kernel learning: Algorithms and mistake bounds", "author": ["Jin", "Rong", "Hoi", "Steven CH", "Yang", "Tianbao"], "venue": "In Algorithmic Learning Theory,", "citeRegEx": "Jin et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Jin et al\\.", "year": 2010}, {"title": "Online learning with kernels", "author": ["Kivinen", "Jyrki", "Smola", "Alex J", "Williamson", "Robert C"], "venue": "In NIPS, pp", "citeRegEx": "Kivinen et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Kivinen et al\\.", "year": 2001}, {"title": "Online multiple kernel learning for structured prediction", "author": ["Martins", "Andr\u00e9 FT", "Figueiredo", "Mario AT", "Aguiar", "Pedro MQ", "Smith", "Noah A", "Xing", "Eric P"], "venue": "arXiv preprint arXiv:1010.2770,", "citeRegEx": "Martins et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Martins et al\\.", "year": 2010}, {"title": "The projectron: a bounded kernel-based perceptron", "author": ["Orabona", "Francesco", "Keshet", "Joseph", "Caputo", "Barbara"], "venue": "In ICML, pp", "citeRegEx": "Orabona et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Orabona et al\\.", "year": 2008}, {"title": "Bounded kernel-based online learning", "author": ["Orabona", "Francesco", "Keshet", "Joseph", "Caputo", "Barbara"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "Orabona et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Orabona et al\\.", "year": 2009}, {"title": "The perceptron: A probabilistic model for information storage and organization in the brain", "author": ["F. Rosenblatt"], "venue": "Psychological Review,", "citeRegEx": "Rosenblatt,? \\Q1958\\E", "shortCiteRegEx": "Rosenblatt", "year": 1958}, {"title": "Online multiple kernel regression", "author": ["Sahoo", "Doyen", "Hoi", "Steven CH", "Li", "Bin"], "venue": "In Proceedings of the 20th ACM SIGKDD international conference on Knowledge discovery and data mining,", "citeRegEx": "Sahoo et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Sahoo et al\\.", "year": 2014}, {"title": "Large scale multiple kernel learning", "author": ["Sonnenburg", "S\u00f6ren", "R\u00e4tsch", "Gunnar", "Sch\u00e4fer", "Christin", "Sch\u00f6lkopf", "Bernhard"], "venue": "JMLR, 7:1531\u20131565,", "citeRegEx": "Sonnenburg et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Sonnenburg et al\\.", "year": 2006}, {"title": "Online passive-aggressive algorithms on a budget", "author": ["Wang", "Zhuang", "Vucetic", "Slobodan"], "venue": "In International Conference on Artificial Intelligence and Statistics,", "citeRegEx": "Wang et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2010}, {"title": "Breaking the curse of kernelization: Budgeted stochastic gradient descent for large-scale svm training", "author": ["Wang", "Zhuang", "Crammer", "Koby", "Vucetic", "Slobodan"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Wang et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2012}, {"title": "Online multiple kernel similarity learning for visual search", "author": ["Xia", "Hao", "Hoi", "Steven CH", "Jin", "Rong", "Zhao", "Peilin"], "venue": "Pattern Analysis and Machine Intelligence, IEEE Transactions on,", "citeRegEx": "Xia et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Xia et al\\.", "year": 2014}, {"title": "An extended level method for efficient multiple kernel learning", "author": ["Xu", "Zenglin", "Jin", "Rong", "King", "Irwin", "Lyu", "Michael R"], "venue": "In NIPS,", "citeRegEx": "Xu et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Xu et al\\.", "year": 2008}, {"title": "Fast bounded online gradient descent algorithms for scalable kernel-based online learning", "author": ["Zhao", "Peilin", "Wang", "Jialei", "Wu", "Pengcheng", "Jin", "Rong", "Hoi", "Steven CH"], "venue": "In ICML,", "citeRegEx": "Zhao et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Zhao et al\\.", "year": 2012}], "referenceMentions": [{"referenceID": 5, "context": "1 INTRODUCTION Online Multiple Kernel Learning has been successfully used in many real-world applications including classification (Hoi et al., 2013; Jin et al., 2010), regression (Sahoo et al.", "startOffset": 131, "endOffset": 167}, {"referenceID": 6, "context": "1 INTRODUCTION Online Multiple Kernel Learning has been successfully used in many real-world applications including classification (Hoi et al., 2013; Jin et al., 2010), regression (Sahoo et al.", "startOffset": 131, "endOffset": 167}, {"referenceID": 12, "context": ", 2010), regression (Sahoo et al., 2014), similarity learning for multimedia search(Xia et al.", "startOffset": 20, "endOffset": 40}, {"referenceID": 16, "context": ", 2014), similarity learning for multimedia search(Xia et al., 2014), and structured prediction (Martins et al.", "startOffset": 50, "endOffset": 68}, {"referenceID": 8, "context": ", 2014), and structured prediction (Martins et al., 2010).", "startOffset": 35, "endOffset": 57}, {"referenceID": 7, "context": "In contrast to traditional online kernel methods (Kivinen et al., 2001) where a single kernel function is often chosen either manually or via some intensive cross-validation process, online multiple kernel learning algorithms learn multiple kernel classifiers and their combination simultaneously.", "startOffset": 49, "endOffset": 71}, {"referenceID": 0, "context": "Examples include Randomized Budget Perceptron (RBP) (Cavallanti et al., 2007), Forgetron (Dekel et al.", "startOffset": 52, "endOffset": 77}, {"referenceID": 2, "context": ", 2007), Forgetron (Dekel et al., 2005), Projectron (Orabona et al.", "startOffset": 19, "endOffset": 39}, {"referenceID": 9, "context": ", 2005), Projectron (Orabona et al., 2008; 2009), Budget Passive Aggressive (BPA) learning (Wang & Vucetic, 2010), Bounded Online Gradient Descent (BOGD) (Zhao et al.", "startOffset": 20, "endOffset": 48}, {"referenceID": 18, "context": ", 2008; 2009), Budget Passive Aggressive (BPA) learning (Wang & Vucetic, 2010), Bounded Online Gradient Descent (BOGD) (Zhao et al., 2012; Wang et al., 2012), among others.", "startOffset": 119, "endOffset": 157}, {"referenceID": 15, "context": ", 2008; 2009), Budget Passive Aggressive (BPA) learning (Wang & Vucetic, 2010), Bounded Online Gradient Descent (BOGD) (Zhao et al., 2012; Wang et al., 2012), among others.", "startOffset": 119, "endOffset": 157}, {"referenceID": 0, "context": ", RBP(Cavallanti et al., 2007)), while some other algorithms, despite being more effective, are often computationally very intensive for multiple kernel learning (e.", "startOffset": 5, "endOffset": 30}, {"referenceID": 9, "context": ", Projectron (Orabona et al., 2008)).", "startOffset": 13, "endOffset": 35}, {"referenceID": 1, "context": "In this paper, we investigate a novel framework of Budget Online Multiple Kernel Classification (BOMKC) and propose a new algorithm termed online Sparse Passive Aggressive learning (SPA) by extending the popular online Passive Aggressive (PA) technique (Crammer et al., 2006).", "startOffset": 253, "endOffset": 275}, {"referenceID": 13, "context": "The above convex optimization problem of regular batch MKL have been solved by different optimization schemes (Sonnenburg et al., 2006; Xu et al., 2008; G\u00f6nen & Alpayd\u0131n, 2011).", "startOffset": 110, "endOffset": 176}, {"referenceID": 17, "context": "The above convex optimization problem of regular batch MKL have been solved by different optimization schemes (Sonnenburg et al., 2006; Xu et al., 2008; G\u00f6nen & Alpayd\u0131n, 2011).", "startOffset": 110, "endOffset": 176}, {"referenceID": 5, "context": "To address the challenges faced by batch MKC methods, several algorithms attempt to solve the MKC problem in an online manner (Hoi et al., 2013; Sahoo et al., 2014) whose updating scheme usually consists of two steps.", "startOffset": 126, "endOffset": 164}, {"referenceID": 12, "context": "To address the challenges faced by batch MKC methods, several algorithms attempt to solve the MKC problem in an online manner (Hoi et al., 2013; Sahoo et al., 2014) whose updating scheme usually consists of two steps.", "startOffset": 126, "endOffset": 164}, {"referenceID": 1, "context": "Our update strategy of each single kernel classifier is generalized from the PA algorithm (Crammer et al., 2006), At the t-step, the online hypothesis will be updated: ft+1 = min f\u2208H\u03ba 1 2 \u2016f \u2212 ft\u20162H\u03ba + \u03b7lt(f) where \u03b7 > 0 and the hinge loss is adopted lt(f) = [1\u2212 ytf(xt)]+.", "startOffset": 90, "endOffset": 112}, {"referenceID": 11, "context": "Specially, we search for the best single kernel classifier from the set of our predefined 16 kernels using one random permutation of all the training examples and then apply the \u201cPerceptron\u201d algorithm (Rosenblatt, 1958) with the best kernel function.", "startOffset": 201, "endOffset": 219}, {"referenceID": 5, "context": "Our second group of compared algorithms are the Online Multiple Kernel Classification algorithms (Hoi et al., 2013) which achieved state-of-the-art performance on many benchmark datasets.", "startOffset": 97, "endOffset": 115}, {"referenceID": 0, "context": "Three variants of this algorithm are included: \u2022 \u201cOMKC(U)\u201d: the OMKC algorithm with a naive uniform combination; \u2022 \u201cOMKC(DD)\u201d: the OMKC algorithm with deterministic combination and update; \u2022 \u201cOMKC(SD)\u201d: OMKC with stochastic update and deterministic combination; Finally, to test the efficiency and effectiveness of our budget strategy, we also compare with multiple kernel classification algorithms whose component classifiers are updated by budget kernel learning algorithms including: \u2022 \u201cRBP\u201d: the Random Budget Perceptron algorithm (Cavallanti et al., 2007); \u2022 \u201cForgetron\u201d: the Forgetron algorithm that discards the oldest SV (Dekel et al.", "startOffset": 535, "endOffset": 560}, {"referenceID": 2, "context": ", 2007); \u2022 \u201cForgetron\u201d: the Forgetron algorithm that discards the oldest SV (Dekel et al., 2005); \u2022 \u201cBOGD\u201d: the Budget Online Gradient Descent algorithm (Zhao et al.", "startOffset": 76, "endOffset": 96}, {"referenceID": 18, "context": ", 2005); \u2022 \u201cBOGD\u201d: the Budget Online Gradient Descent algorithm (Zhao et al., 2012); \u2022 \u201cBPAS\u201d: the Budget Passive-aggressive algorithm (simple)(Wang & Vucetic, 2010).", "startOffset": 64, "endOffset": 83}, {"referenceID": 5, "context": "Second, we find that although using fewer SV\u2019s, the stochastic update algorithms OMKC(SD) can even achieve lower mistake rate compared with deterministic update OMKC(DD), which consists with the previous observations (Hoi et al., 2013).", "startOffset": 217, "endOffset": 235}], "year": 2017, "abstractText": "Online learning with multiple kernels has gained increasing interests in recent years and found many applications. For classification tasks, Online Multiple Kernel Classification (OMKC), which learns a kernel based classifier by seeking the optimal linear combination of a pool of single kernel classifiers in an online fashion, achieves superior accuracy and enjoys great flexibility compared with traditional single-kernel classifiers. Despite being studied extensively, existing OMKC algorithms suffer from high computational cost due to their unbounded numbers of support vectors. To overcome this drawback, we present a novel framework of Budget Online Multiple Kernel Learning (BOMKL) and propose a new Sparse Passive Aggressive learning to perform effective budget online learning. Specifically, we adopt a simple yet effective Bernoulli sampling to decide if an incoming instance should be added to the current set of support vectors. By limiting the number of support vectors, our method can significantly accelerate OMKC while maintaining satisfactory accuracy that is comparable to that of the existing OMKC algorithms. We theoretically prove that our new method achieves an optimal regret bound in expectation, and empirically found that the proposed algorithm outperforms various OMKC algorithms and can easily scale up to large-scale datasets.", "creator": "LaTeX with hyperref package"}}}