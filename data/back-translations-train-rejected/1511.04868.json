{"id": "1511.04868", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "16-Nov-2015", "title": "A Neural Transducer", "abstract": "Sequence-to-sequence models have achieved impressive results on various tasks. However, they are unsuitable for tasks that require incremental predictions to be made as more data arrives. This is because they generate an output sequence conditioned on an entire input sequence. In this paper, we present a new model that can make incremental predictions as more input arrives, without redoing the entire computation. Unlike sequence-to-sequence models, our method computes the next-step distribution conditioned on the partial input sequence observed and the partial sequence generated. It accomplishes this goal using an encoder recurrent neural network (RNN) that computes features at the same frame rate as the input, and a transducer RNN that operates over blocks of input steps. The transducer RNN extends the sequence produced so far using a local sequence-to-sequence model. During training, our method uses alignment information to generate supervised targets for each block. Approximate alignment is easily available for tasks such as speech recognition, action recognition in videos, etc. During inference (decoding), beam search is used to find the most likely output sequence for an input sequence. This decoding is performed online - at the end of each block, the best candidates from the previous block are extended through the local sequence-to-sequence model. On TIMIT, our online method achieves 19.8% phone error rate (PER). For comparison with published sequence-to-sequence methods, we used a bidirectional encoder and achieved 18.7% PER. This compares favorably to the best reported sequence-to-sequence model which achieves 17.6%. Importantly, unlike sequence-to-sequence models our model is minimally impacted by the length of the input. On 10-times replicated utterances, it achieves 20.9% with a unidirectional model, compared to 20% from the best bidirectional sequence-to-sequence models.", "histories": [["v1", "Mon, 16 Nov 2015 08:53:44 GMT  (762kb,D)", "http://arxiv.org/abs/1511.04868v1", null], ["v2", "Wed, 18 Nov 2015 19:56:58 GMT  (762kb,D)", "http://arxiv.org/abs/1511.04868v2", null], ["v3", "Thu, 19 Nov 2015 19:27:14 GMT  (762kb,D)", "http://arxiv.org/abs/1511.04868v3", null], ["v4", "Thu, 4 Aug 2016 23:31:46 GMT  (434kb,D)", "http://arxiv.org/abs/1511.04868v4", null]], "reviews": [], "SUBJECTS": "cs.LG cs.CL cs.NE", "authors": ["navdeep jaitly", "david sussillo", "quoc v le", "oriol vinyals", "ilya sutskever", "samy bengio"], "accepted": false, "id": "1511.04868"}, "pdf": {"name": "1511.04868.pdf", "metadata": {"source": "CRF", "title": "AN ONLINE SEQUENCE-TO-SEQUENCE MODEL USING PARTIAL CONDITIONING", "authors": ["Navdeep Jaitly", "Quoc V. Le", "Oriol Vinyals", "Ilya Sutskeyver", "Samy Bengio"], "emails": ["ndjaitly@google.com", "qvl@google.com", "vinyals@google.com", "ilyasu@google.com", "bengio@google.com"], "sections": [{"heading": "1 INTRODUCTION", "text": "The recently presented sequence-to-sequence model has shown tremendous success in various tasks that make sequences into sequences (Sutskever et al., 2014; Cho et al., 2014; Bahdanau et al., 2015; Chan et al., 2015; Bahdanau et al., 2015b; Nacional et al., 2015b; a; Vinyals & Le, 2015). However, this method is unsuitable for tasks where it is important to produce outputs such as the input of sequences. Speech recognition is an example of such an online task - users prefer a continuous transcription of the speech about receiving an expression. This limitation of the sequence-to-sequence model is due to the fact that predictions about the entire input of sequences are conditioned."}, {"heading": "2 RELATED WORK", "text": "In fact, the fact is that you are able to be in a position without being able to play by the rules."}, {"heading": "3 METHODS", "text": "In this section we describe the model in detail. An overview can be found in Figure 2."}, {"heading": "3.1 MODEL", "text": "Let x1 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 L be the input data that are L-time steps long, with xi representing the features in the input time step \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 b is the number of blocks from a dictionary of K targets."}, {"heading": "3.2 NEXT STEP PREDICTION", "text": "We refer again to Figure 2 for this discussion. The example shows a converter with two hidden layers, with units sm and h'm at the output step M. In the figure, the next step is shown prediction for block b. For this block, the index of the first output symbol is m = eb \u2212 1 + 1, and the index of the last output symbol is m + 2 (i.e. eb = m + 2).The converter calculates the next step prediction using parameters of the neural network by the following sequence of steps: sm = fRNN (sm \u2212 1), [cm \u2212 1], the prediction (3) cm = fcontext (sm \u2212 1)."}, {"heading": "3.4 ADDRESSING END OF BLOCKS", "text": "Since the model generates only a small sequence of output markers in each block, we need to look at the mechanism for shifting the converter from one block to the next. We experimented with three different ways to do this. In the first approach, we did not introduce an explicit block-end mechanism, hoping that the neural network of the converter would implicitly learn a model from the training data. In the second approach, we added block symbols < e > to the label sequence to delimit the end of the blocks, and we added this symbol to the target dictionary. Thus, the Softmax function in Equation 6 implicitly learns to output either a symbol or move the converter forward to the next block. In the third approach, we model the forward motion of the converter using a separate logistic function of the attention vector. The goal of the logistic function is 0 or 1, depending on whether the current step is the last step in the block or not."}, {"heading": "3.5 TRAINING", "text": "The parameters, \u03b8, of the model are learned by stochastic gradient descent (SGD) with momentum over the loss function l (\u03b8; x1 \u00b7 \u00b7 L, y1 \u00b7 \u00b7 \u00b7 \u00b7 S) = B \u2211 b = 1 S \u2211 m = 1 log p (y (eb \u2212 1 + 1) \u00b7 \u00b7 eb | x1 \u00b7 \u00b7 \u00b7 bW, y1 \u00b7 \u00b7 \u00b7 (eb \u2212 1 + 1)) (10)."}, {"heading": "3.6 INFERENCE", "text": "Considering the input acoustics x1 \u00b7 \u00b7 L and > model parameters, we find for each step the sequence of labels y1.. M, which maximizes the probability of labels due to the data, i.e. y-1 \u00b7 \u00b7 S = arg max y1 \u00b7 \u00b7 \u00b7 S, \"e1 \u00b7 \u00b7 N-B = 1 log (ye (b \u2212 1) + 1 \u00b7 eb | x1 \u00b7 \u00b7 \u00b7 bW, y1 \u00b7 \u00b7 \u00b7 e (b \u2212 1) (11) Exact inference in this scheme is mathematically expensive, because the term for the protocol probability does not allow for division into smaller terms that can be calculated independently. Instead, each candidate, y1 \u00b7 S, would have to be independently verified, and the best sequence in an exponentially large number of conceptual sequences would be the last sequence."}, {"heading": "4 EXPERIMENTS AND RESULTS", "text": "This year, it is closer than ever before in the history of the country."}, {"heading": "5 DISCUSSION", "text": "Our model simplifies the earlier approach of Chorowski et al. (2014) by using alignment information in the data. However, note that, unlike DNN-HMM models, the alignment used here is coarse in nature and does not use precise box labels for the input. Instead, coarse labels are used that indicate which tokens were sent up to the time a particular input block was processed. As the alignment corresponds to the information at block level and is not aligned at box level, it may contain labels, such as which words were verbalized by which block of input data. Using an RNN converter that processes input at a slower frame rate reduces the importance of precise alignment. One of the important side effects of our model, which uses partial conditioning with a blocked converter, is that it naturally alleviates the problem of \"attention loss\" suffered by sequence-to-sequence models."}, {"heading": "6 CONCLUSION", "text": "With a new attention mechanism, we applied the model to a phone recognition task, showing that it can deliver results comparable to the state-of-the-art in sequence-to-sequence models. It tackles the two main barriers to adopting sequence-to-sequence models as speech recognition systems. First, the model can produce output transcripts when the data arrives without having to repeat the entire decryption. Second, due to its blocked architecture, problems with attention in one part of the input are unlikely to affect the entire transcript - reducing the impact of the length of the input sequence on the accuracy of the transcript."}, {"heading": "ACKNOWLEDGMENTS", "text": "We thank George Dahl, Andrew Senior, Geoffrey Hinton, Erik McDermott, Tara Sainath, Vincent Vanhoucke and the Google Brain team for their help with the project. We thank William Chan for fruitful discussions on sequence-to-sequence models."}], "references": [{"title": "Applying convolutional neural networks concepts to hybrid nn-hmm model for speech recognition", "author": ["Abdel-Hamid", "Ossama", "Mohamed", "Abdel-rahman", "Jiang", "Hui", "Penn", "Gerald"], "venue": "In Acoustics, Speech and Signal Processing (ICASSP),", "citeRegEx": "Abdel.Hamid et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Abdel.Hamid et al\\.", "year": 2012}, {"title": "Neural Machine Translation by Jointly Learning to Align and Translate", "author": ["Bahdanau", "Dzmitry", "Cho", "Kyunghyun", "Bengio", "Yoshua"], "venue": "In International Conference on Learning Representations,", "citeRegEx": "Bahdanau et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2015}, {"title": "End-to-end attention-based large vocabulary speech recognition", "author": ["Bahdanau", "Dzmitry", "Chorowski", "Jan", "Serdyuk", "Dmitriy", "Brakel", "Philemon", "Bengio", "Yoshua"], "venue": "In http://arxiv.org/abs/1508.04395,", "citeRegEx": "Bahdanau et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2015}, {"title": "Scheduled Sampling for Sequence Prediction with Recurrent Neural Networks", "author": ["Bengio", "Samy", "Vinyals", "Oriol", "Jaitly", "Navdeep", "Shazeer", "Noam"], "venue": "In Neural Information Processing Systems,", "citeRegEx": "Bengio et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Bengio et al\\.", "year": 2015}, {"title": "Listen, attend and spell", "author": ["Chan", "William", "Jaitly", "Navdeep", "Le", "Quoc V", "Vinyals", "Oriol"], "venue": "arXiv preprint arXiv:1508.01211,", "citeRegEx": "Chan et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Chan et al\\.", "year": 2015}, {"title": "Learning Phrase Representations using RNN EncoderDecoder for Statistical Machine Translation", "author": ["Cho", "Kyunghyun", "van Merrienboer", "Bart", "Gulcehre", "Caglar", "Bahdanau", "Dzmitry", "Bougares", "Fethi", "Schwen", "Holger", "Bengio", "Yoshua"], "venue": "In Conference on Empirical Methods in Natural Language Processing,", "citeRegEx": "Cho et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Cho et al\\.", "year": 2014}, {"title": "End-to-end Continuous Speech Recognition using Attention-based Recurrent NN: First Results", "author": ["Chorowski", "Jan", "Bahdanau", "Dzmitry", "Cho", "Kyunghyun", "Bengio", "Yoshua"], "venue": "In Neural Information Processing Systems: Workshop Deep Learning and Representation Learning Workshop,", "citeRegEx": "Chorowski et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Chorowski et al\\.", "year": 2014}, {"title": "Attention-Based Models for Speech Recognition", "author": ["Chorowski", "Jan", "Bahdanau", "Dzmitry", "Serdyuk", "Dmitriy", "Cho", "Kyunghyun", "Bengio", "Yoshua"], "venue": "In Neural Information Processing Systems,", "citeRegEx": "Chorowski et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Chorowski et al\\.", "year": 2015}, {"title": "The darpa speech recognition research database: specifications and status", "author": ["Fisher", "William M", "Doddington", "George R", "Goudie-Marshall", "Kathleen M"], "venue": "In Proc. DARPA Workshop on speech recognition,", "citeRegEx": "Fisher et al\\.,? \\Q1986\\E", "shortCiteRegEx": "Fisher et al\\.", "year": 1986}, {"title": "Speech recognition with deep recurrent neural networks", "author": ["Graves", "Alan", "Mohamed", "Abdel-rahman", "Hinton", "Geoffrey"], "venue": "In Acoustics, Speech and Signal Processing (ICASSP),", "citeRegEx": "Graves et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Graves et al\\.", "year": 2013}, {"title": "Hybrid Speech Recognition with Bidirectional LSTM", "author": ["Graves", "Alex", "Jaitly", "Navdeep", "Mohamed", "Abdel-rahman"], "venue": "In Automatic Speech Recognition and Understanding Workshop,", "citeRegEx": "Graves et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Graves et al\\.", "year": 2013}, {"title": "Deepspeech: Scaling up endto-end speech recognition", "author": ["Hannun", "Awni", "Case", "Carl", "Casper", "Jared", "Catanzaro", "Bryan", "Diamos", "Greg", "Elsen", "Erich", "Prenger", "Ryan", "Satheesh", "Sanjeev", "Sengupta", "Shubho", "Coates", "Adam"], "venue": "In http://arxiv.org/abs/1412.5567,", "citeRegEx": "Hannun et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Hannun et al\\.", "year": 2014}, {"title": "Deep neural networks for acoustic modeling in speech recognition: The shared views of four research groups", "author": ["Hinton", "Geoffrey", "Deng", "Li", "Yu", "Dong", "Dahl", "George E", "Mohamed", "Abdel-rahman", "Jaitly", "Navdeep", "Senior", "Andrew", "Vanhoucke", "Vincent", "Nguyen", "Patrick", "Sainath", "Tara N", "Kingsbury", "Brian"], "venue": "Signal Processing Magazine, IEEE,", "citeRegEx": "Hinton et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Hinton et al\\.", "year": 2012}, {"title": "Long Short-Term Memory", "author": ["Hochreiter", "Sepp", "Schmidhuber", "Jurgen"], "venue": "Neural Computation,", "citeRegEx": "Hochreiter et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Hochreiter et al\\.", "year": 1997}, {"title": "Fast and accurate recurrent neural network acoustic models for speech recognition", "author": ["Sak", "Hasim", "Senior", "Andrew W", "Rao", "Kanishka", "Beaufays", "Fran\u00e7oise"], "venue": "CoRR, abs/1507.06947,", "citeRegEx": "Sak et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Sak et al\\.", "year": 2015}, {"title": "Bidirectional recurrent neural networks", "author": ["Schuster", "Mike", "Paliwal", "Kuldip K"], "venue": "Signal Processing, IEEE Transactions on,", "citeRegEx": "Schuster et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Schuster et al\\.", "year": 1997}, {"title": "Sequence to Sequence Learning with Neural Networks", "author": ["Sutskever", "Ilya", "Vinyals", "Oriol", "Le", "Quoc V"], "venue": "In Neural Information Processing Systems,", "citeRegEx": "Sutskever et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Sutskever et al\\.", "year": 2014}, {"title": "A neural conversational model", "author": ["Vinyals", "Oriol", "Le", "Quoc V"], "venue": "In ICML Deep Learning Workshop,", "citeRegEx": "Vinyals et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Vinyals et al\\.", "year": 2015}, {"title": "Grammar as a foreign language", "author": ["Vinyals", "Oriol", "Kaiser", "Lukasz", "Koo", "Terry", "Petrov", "Slav", "Sutskever", "Ilya", "Hinton", "Geoffrey"], "venue": "In NIPS,", "citeRegEx": "Vinyals et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Vinyals et al\\.", "year": 2015}, {"title": "Show and Tell: A Neural Image Caption Generator", "author": ["Vinyals", "Oriol", "Toshev", "Alexander", "Bengio", "Samy", "Erhan", "Dumitru"], "venue": "In IEEE Conference on Computer Vision and Pattern Recognition,", "citeRegEx": "Vinyals et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Vinyals et al\\.", "year": 2015}, {"title": "Recurrent neural network regularization", "author": ["Zaremba", "Wojciech", "Sutskever", "Ilya", "Vinyals", "Oriol"], "venue": "CoRR, abs/1409.2329,", "citeRegEx": "Zaremba et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Zaremba et al\\.", "year": 2014}], "referenceMentions": [{"referenceID": 16, "context": "The recently introduced sequence-to-sequence model has shown tremendous success in various tasks that map sequences to sequences (Sutskever et al., 2014; Cho et al., 2014; Bahdanau et al., 2015a; Chorowski et al., 2014; 2015; Chan et al., 2015; Bahdanau et al., 2015b; Vinyals et al., 2015b;a; Vinyals & Le, 2015).", "startOffset": 129, "endOffset": 313}, {"referenceID": 5, "context": "The recently introduced sequence-to-sequence model has shown tremendous success in various tasks that map sequences to sequences (Sutskever et al., 2014; Cho et al., 2014; Bahdanau et al., 2015a; Chorowski et al., 2014; 2015; Chan et al., 2015; Bahdanau et al., 2015b; Vinyals et al., 2015b;a; Vinyals & Le, 2015).", "startOffset": 129, "endOffset": 313}, {"referenceID": 6, "context": "The recently introduced sequence-to-sequence model has shown tremendous success in various tasks that map sequences to sequences (Sutskever et al., 2014; Cho et al., 2014; Bahdanau et al., 2015a; Chorowski et al., 2014; 2015; Chan et al., 2015; Bahdanau et al., 2015b; Vinyals et al., 2015b;a; Vinyals & Le, 2015).", "startOffset": 129, "endOffset": 313}, {"referenceID": 4, "context": "The recently introduced sequence-to-sequence model has shown tremendous success in various tasks that map sequences to sequences (Sutskever et al., 2014; Cho et al., 2014; Bahdanau et al., 2015a; Chorowski et al., 2014; 2015; Chan et al., 2015; Bahdanau et al., 2015b; Vinyals et al., 2015b;a; Vinyals & Le, 2015).", "startOffset": 129, "endOffset": 313}, {"referenceID": 4, "context": "(b) Sequence-to-sequence models (Chan et al., 2015; Bahdanau et al., 2015b).", "startOffset": 32, "endOffset": 75}, {"referenceID": 6, "context": "This attention mechanism is different from that of Chorowski et al. (2015) in that, instead of taking a softmax over attention scalars, we input these values into an LSTM-RNN which produces the attention vector as its output.", "startOffset": 51, "endOffset": 75}, {"referenceID": 1, "context": "However, CTC makes independent predictions at each step (see figure 1(a)), and as such can only produce high accuracy on large vocabulary continuous speech recognition (LVCSR) tasks when it is coupled to a strong language model and pronounciation dictionary (Bahdanau et al., 2015b). Our model is inherently an end-to-end model that can incorporate a language model within its parameters, and should produce accurate results on LVCSR even without strong language models. For a comparison with previously reported sequence-to-sequence results on TIMIT, we used a bidirectional encoder and achieved 18.7% PER compared to the best reported number of 17.6% achieved by Chorowski et al. (2015). 1 Even with the bidirectional encoder, our model is different from the sequence-to-sequence model in that it uses a slightly different training objective.", "startOffset": 259, "endOffset": 689}, {"referenceID": 12, "context": "The predominant methodology for speech recognition until recently was to use a Hidden Markov Model coupled with a Deep Neural Network, a Convolutional Network or a Recurrent Network (Hinton et al., 2012; Abdel-Hamid et al., 2012; Graves et al., 2013b).", "startOffset": 182, "endOffset": 251}, {"referenceID": 0, "context": "The predominant methodology for speech recognition until recently was to use a Hidden Markov Model coupled with a Deep Neural Network, a Convolutional Network or a Recurrent Network (Hinton et al., 2012; Abdel-Hamid et al., 2012; Graves et al., 2013b).", "startOffset": 182, "endOffset": 251}, {"referenceID": 11, "context": "More recently, a CTC approach was also proposed for this task (Graves et al., 2013a; Hannun et al., 2014; Sak et al., 2015).", "startOffset": 62, "endOffset": 123}, {"referenceID": 14, "context": "More recently, a CTC approach was also proposed for this task (Graves et al., 2013a; Hannun et al., 2014; Sak et al., 2015).", "startOffset": 62, "endOffset": 123}, {"referenceID": 6, "context": "Sequence-to-sequence models make no such assumptions \u2013 the output sequence is generated by next step prediction, conditioning on the entire input sequence and the partial output sequence generated so far (Chorowski et al., 2014; 2015; Bahdanau et al., 2015b; Chan et al., 2015).", "startOffset": 204, "endOffset": 277}, {"referenceID": 4, "context": "Sequence-to-sequence models make no such assumptions \u2013 the output sequence is generated by next step prediction, conditioning on the entire input sequence and the partial output sequence generated so far (Chorowski et al., 2014; 2015; Bahdanau et al., 2015b; Chan et al., 2015).", "startOffset": 204, "endOffset": 277}, {"referenceID": 4, "context": "Moreover, it has been empirically observed that these models perform significantly worse on longer inputs \u2013 presumably because inacuracies in attention at one step can negatively impact the entire transcripts produced subsequently (Chan et al., 2015).", "startOffset": 231, "endOffset": 250}, {"referenceID": 0, "context": ", 2012; Abdel-Hamid et al., 2012; Graves et al., 2013b). More recently, a CTC approach was also proposed for this task (Graves et al., 2013a; Hannun et al., 2014; Sak et al., 2015). An important aspect of these approaches is that the model makes predictions at every input time step. A high-level picture of this architecture is shown in Figure 1(a). A weakness of these models is that they typically assume conditional independence between the predictions at each output step. Sequence-to-sequence models make no such assumptions \u2013 the output sequence is generated by next step prediction, conditioning on the entire input sequence and the partial output sequence generated so far (Chorowski et al., 2014; 2015; Bahdanau et al., 2015b; Chan et al., 2015). Figure 1(b) shows the high-level picture of this architecture. As can be seen from the figure, these models have a significant limitation in that they have to wait until the end of the speech utterance to start decoding. This property makes them unattractive for real time speech recognition. Conceptually, speech is ordered left-to-right, and localization of relevant frames using attention over the entire input seems to be overkill. Moreover, it has been empirically observed that these models perform significantly worse on longer inputs \u2013 presumably because inacuracies in attention at one step can negatively impact the entire transcripts produced subsequently (Chan et al., 2015). Chorowski et al. (2015) ameliorate (but not eliminate) this problem by introducing a heuristic that requires the attention to move from one step to the next.", "startOffset": 8, "endOffset": 1469}, {"referenceID": 1, "context": "Moreover, although we only performed phone recognition experiments for this paper, like Bahdanau et al. (2015b); Chan et al.", "startOffset": 88, "endOffset": 112}, {"referenceID": 1, "context": "Moreover, although we only performed phone recognition experiments for this paper, like Bahdanau et al. (2015b); Chan et al. (2015) the model can use words or phrases as tokens, and can be trained as an end-to-end model.", "startOffset": 88, "endOffset": 132}, {"referenceID": 6, "context": "We first describe how the context vector is computed by an attention model similar to earlier work (Chorowski et al., 2014; Bahdanau et al., 2015a; Chan et al., 2015).", "startOffset": 99, "endOffset": 166}, {"referenceID": 4, "context": "We first describe how the context vector is computed by an attention model similar to earlier work (Chorowski et al., 2014; Bahdanau et al., 2015a; Chan et al., 2015).", "startOffset": 99, "endOffset": 166}, {"referenceID": 8, "context": "We used TIMIT, a standard bechmark for speech recognition (Fisher et al., 1986) for our experiments.", "startOffset": 58, "endOffset": 79}, {"referenceID": 3, "context": "We combined our model with scheduled sampling (Bengio et al., 2015) but found no gains on this task.", "startOffset": 46, "endOffset": 67}, {"referenceID": 20, "context": "These results could probably be improved with dropout training (Zaremba et al., 2014) or other regularization techniques (Chorowski et al.", "startOffset": 63, "endOffset": 85}, {"referenceID": 7, "context": ", 2014) or other regularization techniques (Chorowski et al., 2015) but we did not pursue those avenues in this paper.", "startOffset": 43, "endOffset": 67}, {"referenceID": 7, "context": "6% (Chorowski et al., 2015).", "startOffset": 3, "endOffset": 27}, {"referenceID": 4, "context": "reported by sequence-to-sequence methods (Chan et al., 2015) that report strong decrease in accuracy of results as a function of length of utterances.", "startOffset": 41, "endOffset": 60}, {"referenceID": 4, "context": "reported by sequence-to-sequence methods (Chan et al., 2015) that report strong decrease in accuracy of results as a function of length of utterances. Chorowski et al. (2014) reported a means of measuring this impact, by computing the PER of a dataset made by concatenating repetitions of the same utterance.", "startOffset": 42, "endOffset": 175}, {"referenceID": 7, "context": "Because of this, sequence-to-sequence models perform worse on longer utterances (Chorowski et al., 2015; Chan et al., 2015).", "startOffset": 80, "endOffset": 123}, {"referenceID": 4, "context": "Because of this, sequence-to-sequence models perform worse on longer utterances (Chorowski et al., 2015; Chan et al., 2015).", "startOffset": 80, "endOffset": 123}, {"referenceID": 6, "context": "Finally, we note that increasing the block size, W , so that it is as large as the input utterance makes the model similar to other end-to-end models (Chorowski et al., 2014; Chan et al., 2015).", "startOffset": 150, "endOffset": 193}, {"referenceID": 4, "context": "Finally, we note that increasing the block size, W , so that it is as large as the input utterance makes the model similar to other end-to-end models (Chorowski et al., 2014; Chan et al., 2015).", "startOffset": 150, "endOffset": 193}, {"referenceID": 5, "context": "Our model simplifies the earlier approach of Chorowski et al. (2014) by using alignment information in the data.", "startOffset": 45, "endOffset": 69}], "year": 2017, "abstractText": "Sequence-to-sequence models have achieved impressive results on various tasks. However, they are unsuitable for tasks that require incremental predictions to be made as more data arrives. This is because they generate an output sequence conditioned on an entire input sequence. In this paper, we present a new model that can make incremental predictions as more input arrives, without redoing the entire computation. Unlike sequence-to-sequence models, our method computes the next-step distribution conditioned on the partial input sequence observed and the partial sequence generated. It accomplishes this goal using an encoder recurrent neural network (RNN) that computes features at the same frame rate as the input, and a transducer RNN that operates over blocks of input steps. The transducer RNN extends the sequence produced so far using a local sequence-to-sequence model. During training, our method uses alignment information to generate supervised targets for each block. Approximate alignment is easily available for tasks such as speech recognition, action recognition in videos, etc. During inference (decoding), beam search is used to find the most likely output sequence for an input sequence. This decoding is performed online at the end of each block, the best candidates from the previous block are extended through the local sequenceto-sequence model. On TIMIT, our online method achieves 19.8% phone error rate (PER). For comparison with published sequence-to-sequence methods, we used a bidirectional encoder and achieved 18.7% PER. This compares favorably to the best reported sequence-to-sequence model which achieves 17.6%. Importantly, unlike sequence-to-sequence models our model is minimally impacted by the length of the input. On 10-times replicated utterances, it achieves 20.9% with a unidirectional model, compared to 20% from the best bidirectional sequence-tosequence models.", "creator": "LaTeX with hyperref package"}}}