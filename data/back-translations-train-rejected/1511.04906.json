{"id": "1511.04906", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "16-Nov-2015", "title": "Performing Highly Accurate Predictions Through Convolutional Networks for Actual Telecommunication Challenges", "abstract": "We investigated how the application of deep learning, specifically the use of convolutional networks trained with GPUs, can help to build better predictive models in telecommunication business environments, and fill this gap. In particular, we focus on the non-trivial problem of predicting customer churn in telecommunication operators. Our model, called WiseNet, consists of a convolutional network and a novel encoding method that transforms customer activity data and Call Detail Records (CDRs) into images. Experimental evaluation with several machine learning classifiers supports the ability of WiseNet for learning features when using structured input data. For this type of telecommunication business problems, we found that WiseNet outperforms machine learning models with hand-crafted features, and does not require the labor-intensive step of feature engineering. Furthermore, the same model has been applied without retraining to a different market, achieving consistent results. This confirms the generalization property of WiseNet and the ability to extract useful representations.", "histories": [["v1", "Mon, 16 Nov 2015 10:42:08 GMT  (1873kb,D)", "http://arxiv.org/abs/1511.04906v1", "11 pages, 6 figures, under review as a conference paper at ICLR 2016"], ["v2", "Mon, 4 Jan 2016 18:36:24 GMT  (1874kb,D)", "http://arxiv.org/abs/1511.04906v2", "11 pages, 6 figures, under review as a conference paper at ICLR 2016"], ["v3", "Thu, 14 Jul 2016 10:21:47 GMT  (1815kb,D)", "http://arxiv.org/abs/1511.04906v3", "11 pages, 6 figures, accepted by IJCAI-16 Workshop on Deep Learning for Artificial Intelligence (DLAI)"]], "COMMENTS": "11 pages, 6 figures, under review as a conference paper at ICLR 2016", "reviews": [], "SUBJECTS": "cs.LG cs.CV", "authors": ["jaime zaratiegui", "ana montoro", "federico castanedo"], "accepted": false, "id": "1511.04906"}, "pdf": {"name": "1511.04906.pdf", "metadata": {"source": "CRF", "title": "PERFORMING HIGHLY ACCURATE PREDICTIONS THROUGH CONVOLUTIONAL NETWORKS FOR ACTUAL TELECOMMUNICATION CHALLENGES", "authors": ["Jaime Zaratiegui", "Ana Montoro", "Federico Castanedo"], "emails": ["jaime.zaratiegui@wiseathena.com", "ana.montoro@wiseathena.com", "fcastanedo@wiseathena.com"], "sections": [{"heading": "1 INTRODUCTION", "text": "In fact, it is the case that most of them are in a position to go into another world, in which they are able to move, in which they are able to move, in which they move, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they, in which they, in which they live, in which they, in which they, in which they live, in which they, in which they live, in which they, in which they are able to move, in which they are able to move, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live in which they live, in which they live, in which they live, in which they live in which they live, in which they live, in which they, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they, in which they live, in which they, in which they, in which they, in which they live, in which they live, in which they, in which they live, in which they, in which they live, in which they, in which they, in which they live, in which they, in which they live, in which they, in which they, in which they live, in which they, in which they, in which they live, in which they live, in which they, in which they, in which they, in which they live, in which they, in which they, in which they live, in which they, in which they live, in which they, in which they, in which they are able to move, in which they, in which they are able"}, {"heading": "2 DATA REPRESENTATION", "text": "In a telecommunications environment, multiple data sources are available from the perspective of customer behavior, such as call detail records (CDRs), balance refill events, geographic activity, social networking activity, data usage behavior, promotional / marketing campaigns, to name a few. To develop a general model, our goal is to avoid specific details and therefore to use as little and harsh data as possible. In this work, we only use the following data from customer behavior: \u2022 Call Detail Records (CDRs): Provide log information with details of each call made by the customer. This information includes the cell tower in which the call was made, the number that triggered the call (time stamp), the duration of the call, the destination number, and the service identification (incoming / outgoing calls or SMS). \u2022 Topups: the description of a balance of fill events for each customer we were motivated to view in the amount of time ID, the amount of each customer received in 2010."}, {"heading": "2.1 IMAGE ENCODING", "text": "In our work, the customer data is converted into a linear time representation, which can then be 20 / 120 = 120 minutes. Therefore, the time is arranged linearly in the x-axis of the image = 1 minute. The y-axis is reserved for each of the data sources, i.e. outgoing call activity (MOC), incoming calls (MTC) and topups. The image width extends over the entire period that we have considered in the training phase, which is exactly four weeks or 28 consecutive days of user activity. In addition, we have divided each day into 12 equally large time periods and, as a result, each of these sections, i.e. pixels, extends over a period of two hours. Therefore, the total image width and height in pixels extends over a period of 336 x 3. The value or intensity of each pixel in the image is proportional to the activity of the customer in each of the three possible categories. For example, we assume that each possible category of activity is proportional to the three."}, {"heading": "3 NETWORK ARCHITECTURE", "text": "As we mentioned earlier to develop the WiseNet architecture, we chose ConvNets because of their ability to generalize new patterns and perform feature learning. ConvNets have a special connectivity structure that takes advantage of the strong spatial correlation shown in customer behavior. Another reason for using Convolutions is that our patterns can be moved into a specific class and it would be desirable to recognize them by their position in the image. Once features are learned through ConvNets, they are passed to a fully connected neural network that combines them to classify the input image into a specific class. In our case, images represent customer behavior, and the target class indicates whether they are active or inactive."}, {"heading": "4 EXPERIMENTAL RESULTS", "text": "In this section, we present the performance metrics of image classification, which are applied to the validation data set described in Section 3. For the evaluation and comparison of models, we have used the following four different metrics with decreasing importance: 1. The area below the curve (AUC); 2. The logarithmic probability of classification (logloss); 3. The true positive share in the top 5% quantity (TP5); 4. The Brier score.While the TP5 metric can be considered one of the best metrics when we use the result of the model to optimize the detection and prevention of customer returns, we prioritize AUC as our main indicator, which is considered as a more general metric, because it takes into account not only the highest probabilities, but all possible metrics used for performance evaluation."}, {"heading": "4.1 COMPARISON WITH OTHER MODELS", "text": "To compare our best WiseNet model with other machine learning methods, we used the same artificial images as comma-separated values (CSV files), and the information contained in the RGB channels was smoothed into a single value per pixel and stored in a CSV file. To maintain the information at the beginning of the week, we added a new variable in the CSV that fills the gap. This variable shows the offset in pixels where the random 4-week sample was truncated, and as a result, each customer was described by a feature vector of 1009 dimensions plus an additional column with the class name. We looked at the performance of four known machine learning algorithms compared to the most powerful WiseNet model: randomForests, Generalized Linear Models (GLM), Generalized Increased Machines (GBM), and Extreme Gradient Boost."}, {"heading": "4.2 COMPARISON WITH FEATURE-ENGINEERING MODELS", "text": "Due to the lack of published literature on machine learning techniques used in prepaid telecommunications networks to distort predictions, in this section we present a performance comparison between one of our internal models, which uses feature engineering and a WiseNet on a large scale. These features, which have been developed and refined over months of work, consolidate our knowledge of this particular market and its situation. The in-house model has been trained with the same clientele as previous models. It even includes new data sources not used in the WiseNet model, such as the geographical location of customers, the use of other mobile services, categorical variables and social relationships with other customers. Another significant difference from the WiseNet model is that the size of the training datasets of the feature engineering model is larger, but the same validation dataset has not been used ethically. As we can see in Table 4, our WiseNet model outperforms the model, the feature of which is particularly noteworthy among all the metrics and metrics considered in this feature 5."}, {"heading": "4.3 APPLICATION IN OTHER MARKETS", "text": "Inspired by this hypothesis, we evaluated the bestWiseNet using data from other countries and without retraining the model, i.e. using the same network architecture, weights and hyperparameters, with the aim of testing the transferable property or generalization error of the model. We were surprised by the extremely good results achieved with this model (see Table 5). This outstanding feature can be justified for two different reasons: (i) the behavior of customers is similar in both countries and / or (ii) the model is able to extract general patterns that can be applied to different markets. The obtained probability distributions and calibration curves are shown in Figure 5."}, {"heading": "4.4 LEARNED REPRESENTATIONS ANALYSIS WITH T-SNE EMBEDDINGS", "text": "To analyze the possible configurations associated with churn, we used a t-Distributed Stochastic Neighbor Embedding (t-SNE) to perform a dimensionality reduction on a subsample of 25,000 users from the validation dataset. The input variables passed to the t-SNE are the states of the neurons in the last FC-1024 hidden layer. Results of the two-dimensional embedding can be seen in Figure 6.On the left side of Figure 6, we can see the embedding of the customers in which we have colored each point according to its probability of belonging to the Churner class. It can be determined that despite the non-inclusion of this probability among the dimensionality reduction inputs, there is a smooth distribution along the map of this variable, especially in the largest collection of points that indicate the churn probability of change from one extreme to the other."}, {"heading": "5 SUMMARY AND CONCLUSIONS", "text": "In this paper, we examined the application of ConvNets, which were trained with GPUs to create better predictive models in telecommunications business environments, and developed the WiseNet model. For the specific problem we looked at, it was shown that WiseNet outperforms commonly used machine learning models by using the same input data. We also demonstrated that it is possible to automatically learn the representations required for a specific business problem and provide more accurate predictions than traditional machine learning models, which are created using handmade functions. This finding is translated into an exponential reduction in the human effort required for feature engineering compared to traditional machine learning models. Furthermore, similar results were obtained with the pre-trained WiseNet model in other markets, which can be influenced by various factors. Thus, we confirmed the generalization characteristics of the WiseNet model and its ability to discover useful representations."}, {"heading": "ACKNOWLEDGEMENTS", "text": "We would like to thank Alfonso Vazquez for his valuable comments and insights in the development of this work."}], "references": [{"title": "Representation learning: A review and new perspectives", "author": ["Bengio", "Yoshua", "Courville", "Aaron", "Vincent", "Pierre"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence,", "citeRegEx": "Bengio et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Bengio et al\\.", "year": 2013}, {"title": "Handling class imbalance in customer churn prediction", "author": ["Burez", "Jonathan", "Van den Poel", "Dirk"], "venue": "Expert Systems with Applications,", "citeRegEx": "Burez et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Burez et al\\.", "year": 2009}, {"title": "Understanding the difficulty of training deep feedforward neural networks", "author": ["Glorot", "Xavier", "Bengio", "Yoshua"], "venue": "In International conference on artificial intelligence and statistics,", "citeRegEx": "Glorot et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Glorot et al\\.", "year": 2010}, {"title": "Delving deep into rectifiers: Surpassing human-level performance on imagenet classification", "author": ["He", "Kaiming", "Zhang", "Xiangyu", "Ren", "Shaoqing", "Sun", "Jian"], "venue": "arXiv preprint arXiv:1502.01852,", "citeRegEx": "He et al\\.,? \\Q2015\\E", "shortCiteRegEx": "He et al\\.", "year": 2015}, {"title": "A fast learning algorithm for deep belief nets", "author": ["Hinton", "Geoffrey E", "Osindero", "Simon", "Teh", "Yee Whye"], "venue": "Neural Computation,", "citeRegEx": "Hinton et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Hinton et al\\.", "year": 2006}, {"title": "Customer churn prediction in telecommunications", "author": ["Huang", "Bingquan", "Kechadi", "Mohand Tahar", "Buckley", "Brian"], "venue": "Expert Systems with Applications,", "citeRegEx": "Huang et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Huang et al\\.", "year": 2012}, {"title": "Large-scale video classification with convolutional neural networks", "author": ["Karpathy", "Andrej", "Toderici", "George", "Shetty", "Sachin", "Leung", "Tommy", "Sukthankar", "Rahul", "FeiFei", "Li"], "venue": "In IEEE Conference on Computer Vision and Pattern Recognition (CVPR),", "citeRegEx": "Karpathy et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Karpathy et al\\.", "year": 2014}, {"title": "Convolutional networks and applications in vision", "author": ["LeCun", "Yann", "Kavukcuoglu", "Koray", "Farabet", "Cl\u00e9ment"], "venue": "In Proceedings of 2010 IEEE International Symposium on Circuits and Systems (ISCAS),", "citeRegEx": "LeCun et al\\.,? \\Q2010\\E", "shortCiteRegEx": "LeCun et al\\.", "year": 2010}, {"title": "Increasing loyalty using predictive modeling in business-to-business telecommunication", "author": ["Luciano", "Patrick", "Rebai", "Ismail", "Lemaire", "Vincent"], "venue": "CoRR, abs/1506.03214,", "citeRegEx": "Luciano et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Luciano et al\\.", "year": 2015}, {"title": "Efficient estimation of word representations in vector space", "author": ["Mikolov", "Tomas", "Chen", "Kai", "Corrado", "Greg", "Dean", "Jeffrey"], "venue": "arXiv preprint arXiv:1301.3781,", "citeRegEx": "Mikolov et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Distributed representations of words and phrases and their compositionality", "author": ["Mikolov", "Tomas", "Sutskever", "Ilya", "Chen", "Kai", "Corrado", "Greg S", "Dean", "Jeff"], "venue": "In Advances in neural information processing systems,", "citeRegEx": "Mikolov et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Predicting customer churn in mobile networks through analysis of social groups", "author": ["Richter", "Yossi", "Yom-Tov", "Elad", "Slonim", "Noam"], "venue": "In SDM,", "citeRegEx": "Richter et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Richter et al\\.", "year": 2010}, {"title": "An empirical study of learning rates in deep neural networks for speech recognition", "author": ["Senior", "Alan", "Heigold", "Georg", "Ranzato", "Marc\u2019Aurelio", "Yang", "Ke"], "venue": "In Acoustics, Speech and Signal Processing (ICASSP),", "citeRegEx": "Senior et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Senior et al\\.", "year": 2013}, {"title": "Two-stream convolutional networks for action recognition in videos", "author": ["Simonyan", "Karen", "Zisserman", "Andrew"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Simonyan et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Simonyan et al\\.", "year": 2014}, {"title": "Very deep convolutional networks for large-scale image recognition", "author": ["Simonyan", "Karen", "Zisserman", "Andrew"], "venue": "arXiv preprint arXiv:1409.1556,", "citeRegEx": "Simonyan et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Simonyan et al\\.", "year": 2015}, {"title": "Dropout: A simple way to prevent neural networks from overfitting", "author": ["Srivastava", "Nitish", "Hinton", "Geoffrey", "Krizhevsky", "Alex", "Sutskever", "Ilya", "Salakhutdinov", "Ruslan"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "Srivastava et al\\.,? \\Q1929\\E", "shortCiteRegEx": "Srivastava et al\\.", "year": 1929}, {"title": "Going deeper with convolutions", "author": ["Szegedy", "Christian", "Liu", "Wei", "Jia", "Yangqing", "Sermanet", "Pierre", "Reed", "Scott", "Anguelov", "Dragomir", "Erhan", "Dumitru", "Vanhoucke", "Vincent", "Rabinovich", "Andrew"], "venue": "arXiv preprint arXiv:1409.4842,", "citeRegEx": "Szegedy et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Szegedy et al\\.", "year": 2014}, {"title": "Real world applications of machine learning techniques over large mobile subscriber datasets", "author": ["Wilson", "Jobin", "Kachappilly", "Chitharanj", "Mohan", "Rakesh", "Kapadia", "Prateek", "Soman", "Arun", "Chaudhury", "Santanu"], "venue": "CoRR, abs/1502.02215,", "citeRegEx": "Wilson et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Wilson et al\\.", "year": 2015}, {"title": "Prediction of advertiser churn for google adwords", "author": ["Yoon", "Sangho", "Koehler", "Jim", "Ghobarah", "Adam"], "venue": "In JSM proceedings,", "citeRegEx": "Yoon et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Yoon et al\\.", "year": 2010}], "referenceMentions": [{"referenceID": 4, "context": "Advances in the amount of available data, hardware performance and methods for training deep learning networks (Hinton et al., 2006) (Srivastava et al.", "startOffset": 111, "endOffset": 132}, {"referenceID": 12, "context": ", 2014) have shown an extraordinary predictive performance improvement in areas such as audio processing (Senior et al., 2013), image recognition (Simonyan & Zisserman, 2015) (Szegedy et al.", "startOffset": 105, "endOffset": 126}, {"referenceID": 16, "context": ", 2013), image recognition (Simonyan & Zisserman, 2015) (Szegedy et al., 2014), natural language processing (Mikolov et al.", "startOffset": 56, "endOffset": 78}, {"referenceID": 6, "context": ", 2013b) and video analysis (Simonyan & Zisserman, 2014) (Karpathy et al., 2014).", "startOffset": 57, "endOffset": 80}, {"referenceID": 0, "context": "feature engineering) which is difficult, time consuming and requires domain knowledge, can be avoided by using deep learning (Bengio et al., 2013) and convolutional networks (ConvNets) in the specific case of images (LeCun et al.", "startOffset": 125, "endOffset": 146}, {"referenceID": 7, "context": ", 2013) and convolutional networks (ConvNets) in the specific case of images (LeCun et al., 2010).", "startOffset": 77, "endOffset": 97}, {"referenceID": 5, "context": "Previous state-of-the-art works tackled the problem by training machine learning models with hand-crafted features (Huang et al., 2012) (Luciano et al.", "startOffset": 115, "endOffset": 135}, {"referenceID": 8, "context": ", 2012) (Luciano et al., 2015) (Wilson et al.", "startOffset": 8, "endOffset": 30}, {"referenceID": 17, "context": ", 2015) (Wilson et al., 2015) (Yoon et al.", "startOffset": 8, "endOffset": 29}, {"referenceID": 18, "context": ", 2015) (Yoon et al., 2010) (Burez & Van den Poel, 2009) or by applying social network analysis (Richter et al.", "startOffset": 8, "endOffset": 27}, {"referenceID": 11, "context": ", 2010) (Burez & Van den Poel, 2009) or by applying social network analysis (Richter et al., 2010).", "startOffset": 76, "endOffset": 98}, {"referenceID": 3, "context": "Regarding activation functions, we used parametric rectifier linear units (PReLU) (He et al., 2015).", "startOffset": 82, "endOffset": 99}], "year": 2017, "abstractText": "We investigated how the application of deep learning, specifically the use of convolutional networks trained with GPUs, can help to build better predictive models in telecommunication business environments, and fill this gap. In particular, we focus on the non-trivial problem of predicting customer churn in telecommunication operators. Our model, called WiseNet, consists of a convolutional network and a novel encoding method that transforms customer activity data and Call Detail Records (CDRs) into images. Experimental evaluation with several machine learning classifiers supports the ability of WiseNet for learning features when using structured input data. For this type of telecommunication business problems, we found that WiseNet outperforms machine learning models with hand-crafted features, and does not require the labor-intensive step of feature engineering. Furthermore, the same model has been applied without retraining to a different market, achieving consistent results. This confirms the generalization property of WiseNet and the ability to extract useful representations.", "creator": "LaTeX with hyperref package"}}}