{"id": "1511.05076", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "16-Nov-2015", "title": "Latent Dirichlet Allocation Based Organisation of Broadcast Media Archives for Deep Neural Network Adaptation", "abstract": "This paper presents a new method for the discovery of latent domains in diverse speech data, for the use of adaptation of Deep Neural Networks (DNNs) for Automatic Speech Recognition. Our work focuses on transcription of multi-genre broadcast media, which is often only categorised broadly in terms of high level genres such as sports, news, documentary, etc. However, in terms of acoustic modelling these categories are coarse. Instead, it is expected that a mixture of latent domains can better represent the complex and diverse behaviours within a TV show, and therefore lead to better and more robust performance. We propose a new method, whereby these latent domains are discovered with Latent Dirichlet Allocation, in an unsupervised manner. These are used to adapt DNNs using the Unique Binary Code (UBIC) representation for the LDA domains. Experiments conducted on a set of BBC TV broadcasts, with more than 2,000 shows for training and 47 shows for testing, show that the use of LDA-UBIC DNNs reduces the error up to 13% relative compared to the baseline hybrid DNN models.", "histories": [["v1", "Mon, 16 Nov 2015 18:25:33 GMT  (454kb,D)", "http://arxiv.org/abs/1511.05076v1", "IEEE Automatic Speech Recognition and Understanding Workshop (ASRU 2015), 13-17 Dec 2015, Scottsdale, Arizona, USA"]], "COMMENTS": "IEEE Automatic Speech Recognition and Understanding Workshop (ASRU 2015), 13-17 Dec 2015, Scottsdale, Arizona, USA", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["mortaza doulaty", "oscar saz", "raymond w m ng", "thomas hain"], "accepted": false, "id": "1511.05076"}, "pdf": {"name": "1511.05076.pdf", "metadata": {"source": "CRF", "title": "LATENT DIRICHLET ALLOCATION BASED ORGANISATION OF BROADCAST MEDIA ARCHIVES FOR DEEP NEURAL NETWORK ADAPTATION", "authors": ["Mortaza Doulaty", "Oscar Saz", "Raymond W. M. Ng", "Thomas Hain"], "emails": [], "sections": [{"heading": null, "text": "In fact, most of them will be able to abide by the rules that they have imposed on themselves, and they will be able to understand the rules that they have imposed on themselves."}, {"heading": "5.1. Data", "text": "For the experiments, BBC television programmes were selected whose data are identical to the data defined and provided for the Multi-Genre Broadcast (MGB) Challenge 2015 [23, 24, 25]. Programmes were selected to cover the full range of current TV types and categorised into 8 genres: Counselling, Children, Comedy, Competition, Documentary, Drama, Events and News. Training dates for the acoustic model were specified and limited to more than 2,000 programmes broadcast by the BBC for 6 weeks in April and May 2008. Development dates for the task were 47 programmes broadcast by the BBC during a week in mid-May 2008. The number of programmes and airtime for training and development dates were shown in Table 1. High-quality broadcasting was not available for the training dates. Instead, only the subtitle text broadcast with each programme plus a coordinated version of the subtitles for which the timestamps were available."}, {"heading": "5.2. Baseline", "text": "Initial models were GMM / HMM systems with 13-dimensional PLP [27] functions, in which four adjacent frames on each side were merged to form a 117-dimensional feature vector. Linear Discriminant Analysis [28] was used to project this feature vector down to 40-dimensional vector and a global Constrained Maximum Likelihood Linear Regression [29] transformation was applied to de-correlate the features. Speaker Adaptive Training (SAT) [30] was performed, and then the models were discriminated against using the Boosted Maximum Mutual Information Criterion [31] and used to obtain the state-level alignments for DNN training. Input to the DNN was 440-dimensional PLP features that were left / right of the current frame. The network had 6 hidden layers of size 2048 and an output layer of 6478."}, {"heading": "5.3. LDA\u2013DNN Experiments", "text": "This year it is more than ever before."}], "references": [{"title": "Broadcast news transcription using HTK", "author": ["P.C. Woodland", "M.J.F. Gales", "D. Pye", "S.J. Young"], "venue": "Proc. of ICASSP, Munich, Germany, 1997.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 1997}, {"title": "The LIMSI broadcast news transcription system", "author": ["J.L. Gauvain", "L. Lamel", "G. Adda"], "venue": "Speech Communication, vol. 37, no. 1\u20132, pp. 89\u2013108, 2002.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2002}, {"title": "Progress in the CU- HTK broadcast news transcription system", "author": ["M.J.F. Gales", "D.Y. Kim", "P.C. Woodland", "H.Y. Chan", "D. Mrva", "R. Sinha", "S.E. Tranter"], "venue": "IEEE Trans. on Audio, Speech and Language Processing, vol. 14, no. 5, pp. 1513\u20131525, 2006.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2006}, {"title": "Automatic transcription of multi\u2013genre media archives", "author": ["P. Lanchantin", "P. Bell", "M. Gales", "T. Hain", "X. Liu", "Y. Long", "J. Quinnell", "S. Renals", "O. Saz", "M. Seigel"], "venue": "Proc. of SLAM, Marseille, France, 2013.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2013}, {"title": "Data-selective transfer learning for multi-domain speech recognition", "author": ["M. Doulaty", "O. Saz", "T. Hain"], "venue": "Proc. of Interspeech, Dresden, Germany, 2015.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2015}, {"title": "Automatic Speech Recognition: A Deep Learning Approach, Springer-Verlag", "author": ["D. Yu", "L. Deng"], "venue": null, "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2015}, {"title": "Connectionist speaker normalization and adaptation", "author": ["V. Abrash", "H. Franco", "A. Sankar", "M. Cohen"], "venue": "Proc. of EuroSpeech, Madrid, Spain, 1995.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 1995}, {"title": "Comparison of discriminative input and output transformations for speaker adaptation in the hybrid nn/hmm systems", "author": ["B. Li", "K.C. Sim"], "venue": "Proc. of Interspeech, Makuhari, Japan, 2010.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2010}, {"title": "Linear hidden transformations for adaptation of hybrid ANN/HMM models", "author": ["R. Gemello", "F. Mana", "S. Scanzio", "P. Laface", "R. De Mori"], "venue": "Speech Communication, vol. 49, no. 10, pp. 827\u2013835, 2007.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2007}, {"title": "Two-stage speaker adaptation of hybrid tied-posterior acoustic models", "author": ["J. Stadermann", "G. Rigoll"], "venue": "Proc. of ICASSP, Philadelphia, USA, 2005.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2005}, {"title": "Speaker dependent bottleneck layer training for speaker adaptation in automatic speech recognition", "author": ["R. Doddipatla", "M. Hasan", "T. Hain"], "venue": "Proc. of Interspeech, Singapore, 2014.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2014}, {"title": "Fast speaker adaptation of artificial neural networks for automatic speech recognition", "author": ["S. Dupont", "L. Cheboub"], "venue": "Proc. of ICASSP, Istanbul, Turkey, 2000.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2000}, {"title": "Speaker adaptation of neural network acoustic models using i-Vectors", "author": ["G. Saon", "H. Soltau", "D. Nahamoo", "M. Picheny"], "venue": "Proc. of ASRU, Olomouc, Czech Republic, 2013.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2013}, {"title": "An investigation into speaker informed DNN front-end for LVCSR", "author": ["Y. Liu", "P. Karanasou", "T. Hain"], "venue": "Proc. of ICASSP, Brisbane, Australia, 2015.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2015}, {"title": "Latent Dirichlet Allocation", "author": ["D.M. Blei", "A.Y. Ng", "M.I. Jordan"], "venue": "Journal of Machine Learning Research, vol. 3, pp. 993\u20131022, 2003.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2003}, {"title": "Acoustic topic model for audio information retrieval", "author": ["S. Kim", "S. Narayanan", "S. Sundaram"], "venue": "Proc. of WASPAA, New Paltz NY, USA, 2009, pp. 37\u201340.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2009}, {"title": "Unsupervised domain discovery using latent dirichlet allocation for acoustic modelling in speech recognition", "author": ["M. Doulaty", "O. Saz", "T. Hain"], "venue": "Proc. of Interspeech, Dresden, Germany, 2015.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2015}, {"title": "Discovering objects and their location in images", "author": ["J. Sivic", "B.C. Russell", "A.A. Efros", "A. Zisserman", "W.T. Freeman"], "venue": "Proc. of ICCV, Beijing, China, 2005.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2005}, {"title": "A probabilistic topic model for unsupervised learning of musical key-profiles", "author": ["D. Hu", "L.K. Saul"], "venue": "Proc. of ISMIR, Kobe, Japan, 2009.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2009}, {"title": "Finding scientific topics", "author": ["T.L. Griffiths", "M. Steyvers"], "venue": "Proc. of National Academy of Sciences of the United States of America, vol. 101, pp. 5228\u20135235, 2004.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2004}, {"title": "Vector quantization and signal compression", "author": ["A. Gersho", "R.M. Gray"], "venue": null, "citeRegEx": "21", "shortCiteRegEx": "21", "year": 1992}, {"title": "Unsupervised data selection and word\u2013morph mixed language model for tamil low-resource keyword search", "author": ["C. Ni", "C.C. Leung", "L. Wang", "N.F. Chen", "B. Ma"], "venue": "Proc. of ICASSP, Brisbane, Australia, 2015.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2015}, {"title": "The MGB Challenge: Evaluating multi-genre broadcast media recognition", "author": ["P. Bell", "M.J.F. Gales", "T. Hain", "J. Kilgour", "P. Lanchantin", "X. Liu", "A. McParland", "S. Renals", "O. Saz", "M. Webster", "P. Woodland"], "venue": "Proc. of ASRU, Arizona, USA, 2015.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2015}, {"title": "The 2015 Sheffield system for transcription of multi\u2013genre broadcast media", "author": ["O. Saz", "M. Doulaty", "S. Deena", "R. Milner", "R.W.M. Ng", "M. Hasan", "Y. Liu", "T. Hain"], "venue": "Proc. of ASRU, Arizona, USA, 2015.", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2015}, {"title": "The USFD spoken language translation system for IWSLT 2014", "author": ["R.W.M. Ng", "M. Doulaty", "R. Doddipatla", "O. Saz", "M. Hasan", "T. Hain", "W. Aziz", "K. Shaf", "L. Specia"], "venue": "Proc. of IWSLT, Lake Tahoe NV, USA, 2014.", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2014}, {"title": "Improving lightly supervised training for broadcast transcriptions", "author": ["Y. Long", "M.J.F. Gales", "P. Lanchantin", "X. Liu", "M.S. Seigel", "P.C. Woodland"], "venue": "Proc. of Interspeech, Lyon, France, 2013.", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2013}, {"title": "Perceptual linear predictive (PLP) analysis of speech", "author": ["H. Hermansky"], "venue": "the Journal of the Acoustical Society of America, vol. 87, no. 4, pp. 1738\u20131752, 1990.", "citeRegEx": "27", "shortCiteRegEx": null, "year": 1990}, {"title": "Linear discriminant analysis for improved large vocabulary continuous speech recognition", "author": ["R. Haeb-Umbach", "H. Ney"], "venue": "Proc. of ICASSP, San Francisco, USA, 1992.", "citeRegEx": "28", "shortCiteRegEx": null, "year": 1992}, {"title": "Maximum likelihood linear transformations for HMM-based speech recognition", "author": ["M. Gales"], "venue": "Computer Speech & Language, vol. 12, no. 2, pp. 75 \u2013 98, 1998.", "citeRegEx": "29", "shortCiteRegEx": null, "year": 1998}, {"title": "A compact model for speaker-adaptive training", "author": ["T. Anastasakos", "J. McDonough", "R. Schwartz", "J. Makhoul"], "venue": "Proc. of ICSLP, Philadelphia, USA, 1996.", "citeRegEx": "30", "shortCiteRegEx": null, "year": 1996}, {"title": "Boosted MMI for model and feature-space discriminative training", "author": ["D. Povey", "D. Kanevsky", "B. Kingsbury", "B. Ramabhadran", "G. Saon", "K. Visweswariah"], "venue": "Proc. of ICASSP, Las Vegas, USA, 2008.", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2008}, {"title": "A fast learning algorithm for deep belief nets", "author": ["G.E. Hinton", "S. Osindero", "Y.-W. Teh"], "venue": "Neural Computation, vol. 18, no. 7, pp. 1527\u20131554, 2006.", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2006}, {"title": "The Kaldi speech recognition toolkit", "author": ["D. Povey", "A. Ghoshal", "G. Boulianne", "L. Burget", "O. Glembek", "N. Goel", "M. Hannemann", "P. Motlicek", "Y. Qian", "P. Schwarz", "J. Silovsky", "G. Stemmer", "K. Vesely"], "venue": "Proc. of ASRU, Hawaii, USA, 2011.", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2011}, {"title": "Srilm-an extensible language modeling toolkit", "author": ["A. Stolcke"], "venue": "Proc. of Interspeech, Denver, US, 2002.", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2002}, {"title": "Software framework for topic modelling with large corpora", "author": ["R. Rehurek", "P. Sojka"], "venue": "Proc. of LREC, Valletta, Malta, 2010.", "citeRegEx": "35", "shortCiteRegEx": null, "year": 2010}, {"title": "Background\u2013tracking acoustic features for genre identification of broadcast shows", "author": ["O. Saz", "M. Doulaty", "T. Hain"], "venue": "Proc. of SLT, Lake Tahoe NV, USA, 2014.", "citeRegEx": "36", "shortCiteRegEx": null, "year": 2014}], "referenceMentions": [{"referenceID": 0, "context": "Error rates for the traditional broadcast news programmes could reach below 10% even in 1990s [1, 2, 3].", "startOffset": 94, "endOffset": 103}, {"referenceID": 1, "context": "Error rates for the traditional broadcast news programmes could reach below 10% even in 1990s [1, 2, 3].", "startOffset": 94, "endOffset": 103}, {"referenceID": 2, "context": "Error rates for the traditional broadcast news programmes could reach below 10% even in 1990s [1, 2, 3].", "startOffset": 94, "endOffset": 103}, {"referenceID": 3, "context": "However broadcast media is not just limited to clean and read studio speech but also includes other types of multi\u2013genre data with diverse speakers, variety of acoustic and recording conditions and diversity of the topics covered resulting in complex acoustic, lexical and linguistic conditions which is not yet well studied [4].", "startOffset": 325, "endOffset": 328}, {"referenceID": 4, "context": "The wide variety of conditions in complex broadcast media causes mismatch between training and testing data, and therefore degrades the performance of the speech recognition systems [5].", "startOffset": 182, "endOffset": 185}, {"referenceID": 5, "context": "DNN adaptation methods can be divided into these three main categories [6]:", "startOffset": 71, "endOffset": 74}, {"referenceID": 6, "context": "Linear input transformations: this is the most common adaptation method where a linear transformation is applied to either input feature [7], input to the softmax layer [8] or activation of the hidden layers [9]", "startOffset": 137, "endOffset": 140}, {"referenceID": 7, "context": "Linear input transformations: this is the most common adaptation method where a linear transformation is applied to either input feature [7], input to the softmax layer [8] or activation of the hidden layers [9]", "startOffset": 169, "endOffset": 172}, {"referenceID": 8, "context": "Linear input transformations: this is the most common adaptation method where a linear transformation is applied to either input feature [7], input to the softmax layer [8] or activation of the hidden layers [9]", "startOffset": 208, "endOffset": 211}, {"referenceID": 9, "context": "Retraining: all or some of the model parameters are adapted or trained using the adaptation data [10, 11].", "startOffset": 97, "endOffset": 105}, {"referenceID": 10, "context": "Retraining: all or some of the model parameters are adapted or trained using the adaptation data [10, 11].", "startOffset": 97, "endOffset": 105}, {"referenceID": 11, "context": "Principle Component Analysis (PCA) based adaptation approach [12], i-Vector based speaker\u2013aware training [13] or speaker\u2013aware DNNs [14] can be considered as subspace methods.", "startOffset": 61, "endOffset": 65}, {"referenceID": 12, "context": "Principle Component Analysis (PCA) based adaptation approach [12], i-Vector based speaker\u2013aware training [13] or speaker\u2013aware DNNs [14] can be considered as subspace methods.", "startOffset": 105, "endOffset": 109}, {"referenceID": 13, "context": "Principle Component Analysis (PCA) based adaptation approach [12], i-Vector based speaker\u2013aware training [13] or speaker\u2013aware DNNs [14] can be considered as subspace methods.", "startOffset": 132, "endOffset": 136}, {"referenceID": 14, "context": "Latent Dirichlet Allocation (LDA) is a statistical approach to discover latent variables in a collection of data that is describable with first\u2013 order statistic, in an unsupervised manner [15].", "startOffset": 188, "endOffset": 192}, {"referenceID": 15, "context": "In audio tasks, LDA has been used for classifying unstructured audio files into onomatopoeic and semantic descriptions with successful results [16].", "startOffset": 143, "endOffset": 147}, {"referenceID": 16, "context": "We have previously used LDA for domain adaptation of GMM/HMM systems [17].", "startOffset": 69, "endOffset": 73}, {"referenceID": 14, "context": "Latent Dirichlet Allocation (LDA) [15] is an unsupervised probabilistic generative model for collections of discrete data.", "startOffset": 34, "endOffset": 38}, {"referenceID": 17, "context": "LDA was originally used for topic modelling of text corpora; however, it is a generic model and can be applied to other tasks, such as object categorisation and localisation in image processing [18], automatic harmonic analysis in music processing [19], acoustic information retrieval in unstructured audio analysis [16] and our previous work for domain adaptation of GMM/HMM systems [17].", "startOffset": 194, "endOffset": 198}, {"referenceID": 18, "context": "LDA was originally used for topic modelling of text corpora; however, it is a generic model and can be applied to other tasks, such as object categorisation and localisation in image processing [18], automatic harmonic analysis in music processing [19], acoustic information retrieval in unstructured audio analysis [16] and our previous work for domain adaptation of GMM/HMM systems [17].", "startOffset": 248, "endOffset": 252}, {"referenceID": 15, "context": "LDA was originally used for topic modelling of text corpora; however, it is a generic model and can be applied to other tasks, such as object categorisation and localisation in image processing [18], automatic harmonic analysis in music processing [19], acoustic information retrieval in unstructured audio analysis [16] and our previous work for domain adaptation of GMM/HMM systems [17].", "startOffset": 316, "endOffset": 320}, {"referenceID": 16, "context": "LDA was originally used for topic modelling of text corpora; however, it is a generic model and can be applied to other tasks, such as object categorisation and localisation in image processing [18], automatic harmonic analysis in music processing [19], acoustic information retrieval in unstructured audio analysis [16] and our previous work for domain adaptation of GMM/HMM systems [17].", "startOffset": 384, "endOffset": 388}, {"referenceID": 14, "context": "A reasonable approximate can be acquired using variational approximation, which is shown to work reasonably well in various applications [15].", "startOffset": 137, "endOffset": 141}, {"referenceID": 14, "context": "Training tries to minimise the Kullback\u2013Leiber Divergence (KLD) between the real and the approximated joint probabilities (equations 2 and 3) [15]:", "startOffset": 142, "endOffset": 146}, {"referenceID": 19, "context": "Other training methods based on Markov\u2013Chain MonteCarlo are also proposed, like Gibbs sampling method [20].", "startOffset": 102, "endOffset": 106}, {"referenceID": 14, "context": "As outlined above, LDA is a model to describe latent factors in sets of discrete symbols [15] which are here interpreted as \u201cdomains\u201d.", "startOffset": 89, "endOffset": 93}, {"referenceID": 16, "context": "work [17] we used Linde\u2013Buzo\u2013Gray vector quantization algorithm [21] to represent each speech frame with a discrete symbol, equivalent to an acoustic word or phone label.", "startOffset": 5, "endOffset": 9}, {"referenceID": 20, "context": "work [17] we used Linde\u2013Buzo\u2013Gray vector quantization algorithm [21] to represent each speech frame with a discrete symbol, equivalent to an acoustic word or phone label.", "startOffset": 64, "endOffset": 68}, {"referenceID": 21, "context": "In this paper an approach similar to that used in [22] was implemented.", "startOffset": 50, "endOffset": 54}, {"referenceID": 14, "context": "After acoustic symbols are established and speech segments are represented as bag\u2013of\u2013sounds, LDA models with designated latent domain sizes are trained using the variational EM algorithm [15].", "startOffset": 187, "endOffset": 191}, {"referenceID": 13, "context": "Finally, domain information derived from the LDA model with K domains is encoded with a K-dimensional one\u2013hot vector called Unique Binary Index Code (UBIC) [14].", "startOffset": 156, "endOffset": 160}, {"referenceID": 5, "context": "This type of adaptation is efficient, since it is implicit in the training process and does not require further adaptation steps [6].", "startOffset": 129, "endOffset": 132}, {"referenceID": 22, "context": "[23, 24, 25].", "startOffset": 0, "endOffset": 12}, {"referenceID": 23, "context": "[23, 24, 25].", "startOffset": 0, "endOffset": 12}, {"referenceID": 24, "context": "[23, 24, 25].", "startOffset": 0, "endOffset": 12}, {"referenceID": 25, "context": "Instead only the subtitle text broadcast with each show plus an aligned version of the subtitles were available where the time stamps of the subtitles had been corrected in a lightly supervised manner [26].", "startOffset": 201, "endOffset": 205}, {"referenceID": 25, "context": "The WMER was a by\u2013product of the semi\u2013supervised alignment process that measures how similar the text in the subtitle matched the output of a lightly supervised ASR system for that segment [26].", "startOffset": 189, "endOffset": 193}, {"referenceID": 26, "context": "Initial models were GMM/HMM systems with 13 dimensional PLP [27] features where four neighbouring frames on each side were spliced together to form a 117\u2013dimensional feature vector.", "startOffset": 60, "endOffset": 64}, {"referenceID": 27, "context": "Using Linear Discriminant Analysis [28] this feature vector was projected down to 40\u2013dimensional vector and a global Constrained Maximum Likelihood Linear Regression [29] transformation was applied to de\u2013correlate the features.", "startOffset": 35, "endOffset": 39}, {"referenceID": 28, "context": "Using Linear Discriminant Analysis [28] this feature vector was projected down to 40\u2013dimensional vector and a global Constrained Maximum Likelihood Linear Regression [29] transformation was applied to de\u2013correlate the features.", "startOffset": 166, "endOffset": 170}, {"referenceID": 29, "context": "Speaker Adaptive Training (SAT) [30] was performed and then models were discriminatively trained using Table 2.", "startOffset": 32, "endOffset": 36}, {"referenceID": 30, "context": "the Boosted Maximum Mutual Information criterion [31] and used to get the state level alignments for the DNN training.", "startOffset": 49, "endOffset": 53}, {"referenceID": 31, "context": "The network was initialised using Deep Belief Network [32] pre\u2013training and then trained to optimise per frame Cross Entropy objective function with Stochastic Gradient Descent.", "startOffset": 54, "endOffset": 58}, {"referenceID": 32, "context": "The Kaldi open\u2013source speech recognition toolkit [33] was used to train the acoustic models.", "startOffset": 49, "endOffset": 53}, {"referenceID": 33, "context": "Both of the language models were trained on the 650M words of the subtitles data using the SRILM toolkit [34] Table 2 presents the Word Error Rate (WER) of the development set with baseline models.", "startOffset": 105, "endOffset": 109}, {"referenceID": 34, "context": "Using this GMM, the audio frames are mapped to discrete symbols to train the LDA models [35].", "startOffset": 88, "endOffset": 92}, {"referenceID": 35, "context": "Newer sets of features, better targeted to describe background acoustic characteristics [36], could also provide an improvement.", "startOffset": 88, "endOffset": 92}, {"referenceID": 22, "context": "org) [23] through a licence with the BBC.", "startOffset": 5, "endOffset": 9}], "year": 2015, "abstractText": "This paper presents a new method for the discovery of latent domains in diverse speech data, for the use of adaptation of Deep Neural Networks (DNNs) for Automatic Speech Recognition. Our work focuses on transcription of multi-genre broadcast media, which is often only categorised broadly in terms of high level genres such as sports, news, documentary, etc. However, in terms of acoustic modelling these categories are coarse. Instead, it is expected that a mixture of latent domains can better represent the complex and diverse behaviours within a TV show, and therefore lead to better and more robust performance. We propose a new method, whereby these latent domains are discovered with Latent Dirichlet Allocation, in an unsupervised manner. These are used to adapt DNNs using the Unique Binary Code (UBIC) representation for the LDA domains. Experiments conducted on a set of BBC TV broadcasts, with more than 2,000 shows for training and 47 shows for testing, show that the use of LDA-UBIC DNNs reduces the error up to 13% relative compared to the baseline hybrid DNN models.", "creator": "LaTeX with hyperref package"}}}