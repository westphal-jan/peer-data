{"id": "1511.05077", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "16-Nov-2015", "title": "Diversity Networks: Neural Network Compression Using Determinantal Point Processes", "abstract": "We introduce Divnet, a flexible technique for learning networks with diverse neurons. Divnet models neuronal diversity by placing a Determinantal Point Process (DPP) over neurons in a given layer. It uses this DPP to select a subset of diverse neurons and subsequently fuses the redundant neurons into the selected ones. Compared with previous approaches, Divnet offers a more principled, flexible technique for capturing neuronal diversity. This enables effective auto-tuning of network architecture and leads to smaller network sizes without hurting performance. Moreover, through its focus on diversity and neuron fusing, Divnet remains compatible with other procedures that seek to reduce memory footprints of networks. We present experimental results to corroborate our claims: for pruning neural networks, Divnet is seen to be notably superior to competing approaches.", "histories": [["v1", "Mon, 16 Nov 2015 18:28:10 GMT  (585kb,D)", "http://arxiv.org/abs/1511.05077v1", null], ["v2", "Wed, 18 Nov 2015 02:22:30 GMT  (669kb,D)", "http://arxiv.org/abs/1511.05077v2", null], ["v3", "Wed, 6 Jan 2016 17:55:06 GMT  (716kb,D)", "http://arxiv.org/abs/1511.05077v3", null], ["v4", "Tue, 19 Jan 2016 18:14:16 GMT  (717kb,D)", "http://arxiv.org/abs/1511.05077v4", null], ["v5", "Wed, 3 Feb 2016 16:37:39 GMT  (716kb,D)", "http://arxiv.org/abs/1511.05077v5", null], ["v6", "Tue, 18 Apr 2017 20:33:53 GMT  (716kb,D)", "http://arxiv.org/abs/1511.05077v6", "This paper appeared under the shorter title Diversity Networks at ICLR 2016 (this http URL)"]], "reviews": [], "SUBJECTS": "cs.LG cs.NE", "authors": ["zelda mariet", "suvrit sra"], "accepted": false, "id": "1511.05077"}, "pdf": {"name": "1511.05077.pdf", "metadata": {"source": "CRF", "title": null, "authors": [], "emails": [], "sections": [{"heading": null, "text": "We introduce Divnet, a flexible technique for learning networks with different neurons. Divnet models neural diversity by placing a determinantal point process (DPP) over neurons in a specific layer. It uses this DPP to select a subset of different neurons and then fuse the redundant neurons with the selected ones. Compared to previous approaches, Divnet offers a more basic, flexible technique for capturing neural diversity, allowing effective auto-tuning of the network architecture and resulting in smaller network sizes without compromising performance. Furthermore, Divnet remains compatible with other methods aimed at reducing the memory footprint of networks by focusing on diversity and neuron merging."}, {"heading": "1 Introduction", "text": "It takes several hyperparameters to appropriate values, for example, the number of hidden layers, the number of neurons per hidden layer, the learning rate, the dynamics, the dropout rate. Although tuning such hyperparameters via the parameter search recently by Maclaurin et al. (2015), this remains extremely expensive, requiring the development of new preferred techniques. Of the many hyperparameters, those that determine the architecture of the network are the most difficult to agree with, mainly because the change during training is much more difficult than adjusting dynamic parameters such as learning rate or dynamics. Typically, the architecture parameters are set once for all before training."}, {"heading": "1.1 Contributions", "text": "The main contributions in this paper are as follows: \u2022 We show how to use Determinantal Point Processes (DPPs) as a flexible, powerful tool to model layer-by-layer neural diversity (Section 2.1). In particular, we present a practical method for generating DPPs via neurons that promotes diversity and leads to smaller network sizes. \u2022 We show that the potential negative effects of cutting back on classification errors can be minimized by \"merging\" redundant neurons. To this end, we introduce a reweighting method for the connections of a neuron that transfers the contributions of the cropped neurons to the retained ones (Section 2.2). We call the combination of these two ideas Divnet, and through several experiments we compare Divnet with previous approaches to neuron pruning (Section 3.1,3.2)."}, {"heading": "1.2 Related work", "text": "Due to their large number of parameters, deep neural networks usually have a high storage capacity. Furthermore, these parameters show a significant degree of redundancy (Denil et al., 2013) and promote different approaches to reducing their size without damaging their performance. A common approach to reducing the number of parameters in a network is to remove connections between the layers. In (Cun et al., 1990) and (Hassibi et al., 1993) connections are deleted with information drawn from the Hessian error function of the network. Sainath et al. (2013) reduce the number of parameters in the network by analyzing the weight matrices by applying low matrix layers to the final weight layer. Han et al. (2015) remove connections with weights lower than a certain threshold before retraining."}, {"heading": "2 Eliminating redundancy in networks", "text": "As shown in (Denil et al., 2013), many models of deep neural networks show significant redundancy in their parameters. By recognizing and removing redundant elements, we can reduce the size of the network without compromising its performance. From now on, we call T the training set, \"a given layer in the network, aij the activation of a neuron ni at the input tj, and vi = (ai1,..., ai | ') > the activation vector of neurons ni, which is achieved by feeding the training data through the network."}, {"heading": "2.1 Pruning", "text": "In order to force diversity in a layer, \"we need to determine which neurons compute and remove redundant information. Theoretically, this can be done by finding a larger subset of linearly independent activation vectors in a layer and retaining only the corresponding neurons. In practice, however, the number of items in the training set (or the number of batches) can be significantly greater than the number of neurons in a layer: the activation vectors v1,..., v | '| will probably be linearly independent."}, {"heading": "2.1.1 Determinantal Point Processes", "text": "DPPs model probability distributions over subsets of items to assign a higher probability to different subsets of items that are of good quality. They were first introduced in model fermions (Macchi, 1975) and have since been used in many different machine learning applications (Kulesza and Taskar, 2012). More recently, they have also been used to model interneuron inhibition in behavior in neuronal spiking in the hippocampus of rats (Snoek et al., 2013). Formally, the probability of a subset Y'Y indexed by the elements of Y is given by P (Y) = det (LY) (L + I) (1), where L is a N-for-N positive definitive matrix referred to as the DPP kernel. LY indicates the probability of a subset Y'Y indexed by the elements of Y. When sampling from a subset of items, the size of the subset can be determined either on a specific integer of the nucleus of which is assigned to the DY-matrion the elements of DPP (LPP)."}, {"heading": "2.1.2 Setting the DPP kernel", "text": "Experimentally, we found that the setting of a Gaussian kernel yielded the best results, both in comparison to simpler approaches such as the outer product of the activation vectors and to more complex Gaussian nuclei with additional parameters. (2) To ensure the positive definition of the nuclear matrix L, we add \"I\" to \"L\" (\u03b5 = 0.01). Experimentally, we chose \u03b2 = 10 / | T |. In connection with determining the size of a neural network, we typically want to control the size k of the sampled subset."}, {"heading": "2.2 Merging redundant neurons", "text": "By simply deleting neurons that have not been sampled by the DPP, the input values of the neurons on the next layer are drastically altered. We present here a rebalancing procedure that rebalances the network after removing the redundant neurons by merging with the remaining ones. We note that the weight that connects the i-th neuron in the current layer with the l-th neuron in the next layer and the weight that connects the neurons is after removing the redundant neurons. To minimize the effects of removing the i-th neurons in the current layer with the l-th neuron in the next layer, we want to minimize the solution weight w-ij + the weight that connects the neurons after removing the redundant neurons by removing the i-th neurons in the next layer."}, {"heading": "3 Experimental results", "text": "To quantify the performance of our algorithm, we present below the results of experiments performed on common neural network evaluation datasets: the MNIST dataset (LeCun and Cortes, 2010), the MNIST-RED dataset (Larochelle et al., 2007), and the CIFAR-10 dataset (Krizhevsky, 2009). All networks were trained up to a certain training error threshold, using the Softmax activation function on the output layer and sigmoid on other layers; see Table 1. Error bars are standard deviations in all of the following diagrams. 2experiments were performed in MATLAB, based on the code of DeepLearnToolBox (https: / / github.com / rasmusbergpalm / DeepLearnToolbox) and Alex Kulesza's code for DPPs (http: / / webecles.clesulescu ~ CRAQ on a size of 70Gb / 4GB)."}, {"heading": "3.1 Pruning and reweighting analysis", "text": "Figure 1 shows the activation of neurons in the first hidden layer of a network trained on the MNIST dataset. Each column in the thermal maps represents the activation of a neuron on instances of the digits 0 to 9. Figure 1a shows the activation of the 50 neurons scanned with a k-DPP (k = 50) above the first hidden layer, while Figure 1b shows only the activation of the first 50 neurons of the same layer. Figure 1b contains several similar columns: For example, there are three completely green columns corresponding to three neurons saturated to 1 on each of the 10 instances. Since the DPP samples were executed with different activations, Figure 1a shows the effects of circumcision on the network error, with three neurons capable of 1 saturated neurons."}, {"heading": "3.2 Performance analysis", "text": "When using neural networks locally on devices with limited memory, it is crucial that their memory requirements are as small as possible. Node-importance-based circumcision (henceforth, \"importance-circumcision\") is one of the most intuitive methods for reducing network size. Introduced by He et al. (2014) into deep networks, this method removes the neurons whose calculations have the least impact on the network. Of the three solutions for estimating the importance of a neuron discussed in He et al. (2014), the sum of the output values of each neuron (the norm function) yielded the best results: Onorm (ni) = 1 | '+ 1 |' j = 1 | w '+ 1ij | Figure 5 shows the relative error of networks after circumcision using the importance-circumcision of the norm function and the reweighting of operations."}, {"heading": "4 Discussion", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "5 Future work and conclusion", "text": "Divnet leverages the similarities between the behaviors of neurons in a layer to detect and merge redundant parameters, thereby enforcing neural diversity within each hidden layer. Divnet allows large, redundant networks to shrink to much smaller structures without compromising performance or requiring further training. Many parameters in this process can be customized to the needs of the user: the number of remaining neurons per layer can be set manually; the precision of the reweighting and scanning process can be determined by choosing how many training instances to use to generate the DPP kernel and the reweighting of coefficients, creating a trade-off between accuracy, memory management and computing time. In addition, Divnet is agnostic for most network parameters, requiring only knowledge of the activation vectors; as a consequence, Divnet can easily be used with other showcase / network methods to reduce the size of the network."}, {"heading": "A Pruning the second layer", "text": ""}], "references": [{"title": "Compressing neural networks with the hashing", "author": ["W. Chen", "J.T. Wilson", "S. Tyree", "K.Q. Weinberger", "Y. Chen"], "venue": "trick. CoRR,", "citeRegEx": "Chen et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Chen et al\\.", "year": 2015}, {"title": "Low precision arithmetic for deep learning", "author": ["M. Courbariaux", "Y. Bengio", "J. David"], "venue": "CoRR, abs/1412.7024,", "citeRegEx": "Courbariaux et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Courbariaux et al\\.", "year": 2014}, {"title": "Optimal brain damage", "author": ["Y.L. Cun", "J.S. Denker", "S.A. Solla"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Cun et al\\.,? \\Q1990\\E", "shortCiteRegEx": "Cun et al\\.", "year": 1990}, {"title": "Predicting parameters in deep learning", "author": ["M. Denil", "B. Shakibi", "L. Dinh", "M. Ranzato", "N. de Freitas"], "venue": "CoRR, abs/1306.0543,", "citeRegEx": "Denil et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Denil et al\\.", "year": 2013}, {"title": "Deep learning with limited numerical precision", "author": ["S. Gupta", "A. Agrawal", "K. Gopalakrishnan", "P. Narayanan"], "venue": "CoRR, abs/1502.02551,", "citeRegEx": "Gupta et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Gupta et al\\.", "year": 2015}, {"title": "Learning both weights and connections for efficient neural networks", "author": ["S. Han", "J. Pool", "J. Tran", "W.J. Dally"], "venue": "CoRR, abs/1506.02626,", "citeRegEx": "Han et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Han et al\\.", "year": 2015}, {"title": "Second order derivatives for network pruning: Optimal brain surgeon", "author": ["B. Hassibi", "D.G. Stork", "S.C.R. Com"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "Hassibi et al\\.,? \\Q1993\\E", "shortCiteRegEx": "Hassibi et al\\.", "year": 1993}, {"title": "Reshaping deep neural network for fast decoding by node-pruning", "author": ["T. He", "Y. Fan", "Y. Qian", "T. Tan", "K. Yu"], "venue": "In Acoustics, Speech and Signal Processing (ICASSP),", "citeRegEx": "He et al\\.,? \\Q2014\\E", "shortCiteRegEx": "He et al\\.", "year": 2014}, {"title": "Distilling the knowledge in a neural network", "author": ["G.E. Hinton", "O. Vinyals", "J. Dean"], "venue": "CoRR, abs/1503.02531,", "citeRegEx": "Hinton et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Hinton et al\\.", "year": 2015}, {"title": "Learning multiple layers of features from tiny images", "author": ["A. Krizhevsky"], "venue": "Technical report,", "citeRegEx": "Krizhevsky.,? \\Q2009\\E", "shortCiteRegEx": "Krizhevsky.", "year": 2009}, {"title": "Determinantal point processes for machine learning", "author": ["A. Kulesza", "B. Taskar"], "venue": "Foundations and Trends in Machine Learning,", "citeRegEx": "Kulesza and Taskar.,? \\Q2012\\E", "shortCiteRegEx": "Kulesza and Taskar.", "year": 2012}, {"title": "An empirical evaluation of deep architectures on problems with many factors of variation", "author": ["H. Larochelle", "D. Erhan", "A. Courville", "J. Bergstra", "Y. Bengio"], "venue": "In Proceedings of the 24th International Conference on Machine Learning,", "citeRegEx": "Larochelle et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Larochelle et al\\.", "year": 2007}, {"title": "Gradient-based learning applied to document recognition", "author": ["Y. Lecun", "L. Bottou", "Y. Bengio", "P. Haffner"], "venue": "In Proceedings of the IEEE,", "citeRegEx": "Lecun et al\\.,? \\Q1998\\E", "shortCiteRegEx": "Lecun et al\\.", "year": 1998}, {"title": "The coincidence approach to stochastic point processes", "author": ["O. Macchi"], "venue": "Advances in Applied Probability,", "citeRegEx": "Macchi.,? \\Q1975\\E", "shortCiteRegEx": "Macchi.", "year": 1975}, {"title": "Gradient-based hyperparameter optimization through reversible learning", "author": ["D. Maclaurin", "D. Duvenaud", "R.P. Adams"], "venue": "In Proceedings of the 32nd International Conference on Machine Learning,", "citeRegEx": "Maclaurin et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Maclaurin et al\\.", "year": 2015}, {"title": "Low-rank matrix factorization for deep neural network training with high-dimensional output targets", "author": ["T.N. Sainath", "B. Kingsbury", "V. Sindhwani", "E. Arisoy", "B. Ramabhadran"], "venue": "In IEEE International Conference on Acoustics, Speech and Signal Processing,", "citeRegEx": "Sainath et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Sainath et al\\.", "year": 2013}, {"title": "A determinantal point process latent variable model for inhibition in neural spiking data", "author": ["J. Snoek", "R. Zemel", "R.P. Adams"], "venue": "Advances in Neural Information Processing Systems", "citeRegEx": "Snoek et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Snoek et al\\.", "year": 2013}], "referenceMentions": [{"referenceID": 14, "context": "Although tuning such hyper-parameters via parameter search has been recently investigated by Maclaurin et al. (2015), doing so remains extremely costly, which makes it imperative to develop new more preferable techniques.", "startOffset": 93, "endOffset": 117}, {"referenceID": 3, "context": "Moreover, in many deep neural network models, these parameters show a significant amount of redundancy (Denil et al., 2013), encouraging different approaches to reducing their size without damaging their performance.", "startOffset": 103, "endOffset": 123}, {"referenceID": 2, "context": "In (Cun et al., 1990) and (Hassibi et al.", "startOffset": 3, "endOffset": 21}, {"referenceID": 6, "context": ", 1990) and (Hassibi et al., 1993), connections are deleted using information drawn from the Hessian of the network\u2019s error function.", "startOffset": 12, "endOffset": 34}, {"referenceID": 12, "context": "Convolutional Neural Networks (Lecun et al., 1998) replace fully-connected layers with convolutional and subsampling layers, significantly decreasing the number of parameters in the model.", "startOffset": 30, "endOffset": 50}, {"referenceID": 8, "context": "Recent approaches have investigated network compression without pruning: in (Hinton et al., 2015), a new, smaller network is trained on the outputs of the large network; Chen et al.", "startOffset": 76, "endOffset": 97}, {"referenceID": 0, "context": "In (Cun et al., 1990) and (Hassibi et al., 1993), connections are deleted using information drawn from the Hessian of the network\u2019s error function. Sainath et al. (2013) reduce the number of parameters in the network by analyzing the weight matrices, this time by applying low-rank matrix factorization to the final weight layer.", "startOffset": 4, "endOffset": 170}, {"referenceID": 0, "context": "In (Cun et al., 1990) and (Hassibi et al., 1993), connections are deleted using information drawn from the Hessian of the network\u2019s error function. Sainath et al. (2013) reduce the number of parameters in the network by analyzing the weight matrices, this time by applying low-rank matrix factorization to the final weight layer. Han et al. (2015) remove connections with weights smaller than a given threshold before retraining.", "startOffset": 4, "endOffset": 348}, {"referenceID": 0, "context": "In (Cun et al., 1990) and (Hassibi et al., 1993), connections are deleted using information drawn from the Hessian of the network\u2019s error function. Sainath et al. (2013) reduce the number of parameters in the network by analyzing the weight matrices, this time by applying low-rank matrix factorization to the final weight layer. Han et al. (2015) remove connections with weights smaller than a given threshold before retraining. Whereas these methods focus on deleting parameters whose removal will influence the network the least, Divnet enforces diversity by merging similar parameters; these methods can thus be used in conjunction with ours. Convolutional Neural Networks (Lecun et al., 1998) replace fully-connected layers with convolutional and subsampling layers, significantly decreasing the number of parameters in the model. However, CNNs still maintain fully-connected layers, and can therefore also benefit from Divnet. Closer to our approach of shrinking the network by reducing the number of hidden neurons, He et al. (2014) evaluate each hidden neuron\u2019s importance and delete neurons with the smaller importance measures.", "startOffset": 4, "endOffset": 1040}, {"referenceID": 0, "context": ", 2015), a new, smaller network is trained on the outputs of the large network; Chen et al. (2015) use hashing to reduce the size of the weight matrices\u2019 representations by forcing all connections within a same hash bucket to have the same weight.", "startOffset": 80, "endOffset": 99}, {"referenceID": 0, "context": ", 2015), a new, smaller network is trained on the outputs of the large network; Chen et al. (2015) use hashing to reduce the size of the weight matrices\u2019 representations by forcing all connections within a same hash bucket to have the same weight. Courbariaux et al. (2014) and Gupta et al.", "startOffset": 80, "endOffset": 274}, {"referenceID": 0, "context": ", 2015), a new, smaller network is trained on the outputs of the large network; Chen et al. (2015) use hashing to reduce the size of the weight matrices\u2019 representations by forcing all connections within a same hash bucket to have the same weight. Courbariaux et al. (2014) and Gupta et al. (2015) show that networks can be trained and run using limited precision to store the network\u2019s parameters, thus reducing the network\u2019s overall memory footprint.", "startOffset": 80, "endOffset": 298}, {"referenceID": 3, "context": "2 Eliminating redundancy in networks As shown in (Denil et al., 2013), many deep neural network models show significant redundancy in their parameters.", "startOffset": 49, "endOffset": 69}, {"referenceID": 13, "context": "First introduced to model fermions (Macchi, 1975), they have since then been used in many different machine-learning applications (Kulesza and Taskar, 2012).", "startOffset": 35, "endOffset": 49}, {"referenceID": 10, "context": "First introduced to model fermions (Macchi, 1975), they have since then been used in many different machine-learning applications (Kulesza and Taskar, 2012).", "startOffset": 130, "endOffset": 156}, {"referenceID": 16, "context": "More recently, they have also been applied to modeling inter-neuron inhibitions in neural spiking behavior in the rat hippocampus (Snoek et al., 2013).", "startOffset": 130, "endOffset": 150}, {"referenceID": 11, "context": "In order to quantify the performance of our algorithm, we present below the results of experiments run on common datasets for neural network evaluation: the MNIST dataset (LeCun and Cortes, 2010), the MNIST ROT dataset (Larochelle et al., 2007) and the CIFAR-10 dataset (Krizhevsky, 2009).", "startOffset": 219, "endOffset": 244}, {"referenceID": 9, "context": ", 2007) and the CIFAR-10 dataset (Krizhevsky, 2009).", "startOffset": 33, "endOffset": 51}, {"referenceID": 7, "context": "Introduced to deep networks by He et al. (2014), this method removes the neurons whose calculations impact least the network.", "startOffset": 31, "endOffset": 48}, {"referenceID": 7, "context": "Introduced to deep networks by He et al. (2014), this method removes the neurons whose calculations impact least the network. Among the three solutions to estimating a neuron\u2019s importance discussed in He et al. (2014), the sum the output weights of each neuron (the onorm function) provided the best results:", "startOffset": 31, "endOffset": 218}], "year": 2017, "abstractText": "We introduce Divnet, a flexible technique for learning networks with diverse neurons. Divnet models neuronal diversity by placing a Determinantal Point Process (DPP) over neurons in a given layer. It uses this DPP to select a subset of diverse neurons and subsequently fuses the redundant neurons into the selected ones. Compared with previous approaches, Divnet offers a more principled, flexible technique for capturing neuronal diversity. This enables effective auto-tuning of network architecture and leads to smaller network sizes without hurting performance. Moreover, through its focus on diversity and neuron fusing, Divnet remains compatible with other procedures that seek to reduce memory footprints of networks. We present experimental results to corroborate our claims: for pruning neural networks, Divnet is seen to be notably superior to competing approaches.", "creator": "LaTeX with hyperref package"}}}