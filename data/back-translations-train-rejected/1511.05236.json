{"id": "1511.05236", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "17-Nov-2015", "title": "Reduced-Precision Strategies for Bounded Memory in Deep Neural Nets", "abstract": "This work shows how using reduced precision data in Convolutional Neural Networks (CNNs) affects network accuracy during classification. Unlike similar studies, this study considers networks where each layer may use different precision data. The key result of this study is that the tolerance of CNNs to reduced precision data not only varies across networks, a well established observation, but also within networks. Tuning precision per layer is appealing as it could enable energy and performance improvements. The trends of error tolerance across layers are studied and a method for finding a low precision configuration for a network while maintaining high accuracy is presented. A diverse set of CNNs is analyzed showing that compared to a conventional implementation using a 32-bit floating-point representation for all layers, and with less than 1% loss in relative accuracy, the data footprint required by these networks can be reduced by an average of 74% and up to 92%.", "histories": [["v1", "Tue, 17 Nov 2015 01:03:03 GMT  (3577kb,D)", "https://arxiv.org/abs/1511.05236v1", "Submitted to ICLR 2016, 12 pages, 5 figures"], ["v2", "Thu, 19 Nov 2015 20:38:17 GMT  (3653kb,D)", "http://arxiv.org/abs/1511.05236v2", "Submitted to ICLR 2016, 12 pages, 5 figures"], ["v3", "Wed, 2 Dec 2015 00:20:48 GMT  (3660kb,D)", "http://arxiv.org/abs/1511.05236v3", "Submitted to ICLR 2016, 12 pages, 5 figures"], ["v4", "Fri, 8 Jan 2016 07:22:41 GMT  (3666kb,D)", "http://arxiv.org/abs/1511.05236v4", "Submitted to ICLR 2016, 12 pages, 5 figures"]], "COMMENTS": "Submitted to ICLR 2016, 12 pages, 5 figures", "reviews": [], "SUBJECTS": "cs.LG cs.NE", "authors": ["patrick judd", "jorge albericio", "tayler hetherington", "tor aamodt", "natalie enright jerger", "raquel urtasun", "reas moshovos"], "accepted": false, "id": "1511.05236"}, "pdf": {"name": "1511.05236.pdf", "metadata": {"source": "CRF", "title": "REDUCED-PRECISION STRATEGIES FOR BOUNDED MEMORY IN DEEP NEURAL NETS", "authors": ["Patrick Judd", "Jorge Albericio", "Tayler Hetherington", "Tor Aamodt", "Natalie Enright Jerger", "Raquel Urtasun", "Andreas Moshovos"], "emails": ["juddpatr@ece.utoronto.ca", "jorge@ece.utoronto.ca", "enright@ece.utoronto.ca", "moshovos@ece.utoronto.ca", "taylerh@ece.ubc.ca", "aamodt@ece.ubc.ca", "urtasun@cs.utoronto.edu"], "sections": [{"heading": "1 INTRODUCTION", "text": "In fact, most of them are able to outdo themselves by outperforming themselves, by outperforming themselves, by outperforming each other, by outperforming each other, by outperforming each other, by outdoing each other, by outdoing each other, by outdoing each other, by outdoing each other, by outdoing each other, by outdoing each other and outdoing each other, by outdoing each other, by outdoing each other, by outdoing each other and outdoing each other, by outdoing each other and outdoing each other, by outdoing each other, by outdoing each other, by outdoing each other and outdoing each other."}, {"heading": "2 CNN ACCURACY VS. REPRESENTATION LENGTH", "text": "Section 2.1 describes the experimental setup and measurement methodology, defines the terminology used for the rest of this study, and lists the CNNs studied. Section 2.2 examines how accuracy varies across the network, with all layers within each network forced to use the same representation, and this analysis confirms that accuracy requirements vary across the network. Section 2.3 Studies per layer precision in isolation for each layer show that the precision requirements within each network vary, the most important finding of this study. Here, we examine each layer in isolation and with varying accuracy at once. Finally, Sections 2.4 and 2.5 consider the impact of precision selection per layer on overall network accuracy, where we can assign a different precision to each layer."}, {"heading": "2.1 MEASUREMENT METHODOLOGY", "text": "This year is the highest in the history of the country."}, {"heading": "2.2 UNIFORM REPRESENTATION ACROSS ALL LAYERS", "text": "The results of this section confirm that the requirements for accuracy vary across the network. Specifically, this section examines the percentage per network, the minimum uniform representation length. For this analysis, we require that the same representation be used by all levels of the network. As existing implementations choose a representation that is sufficient for each network, they use a worst-case analysis approach. The results of this section show that this current worst-case analysis is suboptimal. Weights: Figure 2 (a) shows a fraction difference with the number of bits used for the weights. Weights are real numbers, typically between -1 and 1, and therefore we fix the integer part to 1 bit and report results only when the fraction of the fixed-point representation varies. The use of 10-bit weights is sufficient to maintain accuracy for all networks examined. Note that LeNet can use 8-bit fixed-point weights that also vary without loss of accuracy."}, {"heading": "2.3 PER LAYER REPRESENTATION REQUIREMENTS", "text": "This section shows that accuracy requirements vary even within each network, while we should allow each layer to use a different representation, further reducing memory and communication requirements. To perform these experiments, we maintain the numerical representation of the baseline for all layers and vary the representation for one layer at a time. Section 2.5 looks at the combined effect of selecting different representations per layer for all layers at the same time.Weights: The first column of Fig. 3 shows how CNN accuracy varies when we change the fixed point representation used for the weights of a layer at a time. Since weights are typically between -1 and 1, we use a single integer bit (sign bit) and vary the number of fraction bits used by the fixed point representation. Results show that the minimum number of bits required per layer and per network varies. In LeNet, for example, three bits are sufficient for layer 2, while seven bits are required for layer 2 and three for layer N and three for layer 3 are required."}, {"heading": "2.4 DATA TRAFFIC MEASUREMENTS", "text": "This section reports on the number of data accesses performed by the networks, including the input data, the intermediate data read and written by the layers, and the weights. We will use these measurements in the next section, where we will present a method for selecting different precisions per layer with the goal of minimizing overall traffic. The reported measurements underestimate the amount of traffic generated by the CNNs, and hence the benefits of reducing off-chip traffic. Specifically, these experiments assume that once a layer touches a piece of data, that data will only be transmitted once for the duration of the execution of the layer. In practice, the layers read the values multiple times. These measurements assume that there are enough buffers on the chip to capture any data reuse by a layer, regardless of the reuse distance. In practice, this may not be possible because the amount of buffering required for the duration of the execution of the layer is required."}, {"heading": "2.5 CHOOSING THE PER LAYER DATA REPRESENTATION", "text": "It is in such a way that the majority of them are able to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to fight, to move, to move, to move, to move, to move, to move, to fight, to move, to move, to fight, to move, to move, to move, to move, to fight, to move, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight"}, {"heading": "3 RELATED WORK", "text": "Reduced accuracy of neural networks has been an active topic of research for many years (Xie & Jabri, 1991; Presley & Haggard, 1994; Holt & Baker, 1991; Strey & Avellana, 1996; Larkin & Kinane; Asanovic & Morgan, 1993; Holt & neng Hwang, 1993). Gupta et al. (2015) form neural networks with 16-bit fixed-point numbers and stochastic rounding, and suggest how to add hardware support for stochastic rounding. Courbariaux et al. (2014) used three different data formats for intermediate data: floating point, fixed point, and dynamic fixed point. They also suggest how to train networks with a low data format."}, {"heading": "4 CONCLUSION", "text": "Classification applications and quality in deep neural networks are currently limited by computing power and the ability to communicate and store numerical data. An effective technique to improve performance and traffic consists in the use of numerical representations with reduced longitude. This work provides a detailed characterization of the reduced precision tolerance per layer of a wide range of neural networks. We highlight a trend of higher precision requirements for newer, more complex networks.We proposed a method for determining the mapping of representations to layers that provides a good balance between accuracy and traffic. We estimate that we can reduce the storage requirement for the intermediate data in our set of revolutionary neural networks by an average of 74%, while not taking into account the effects of decreased precision during training. The results of this work serve as motivation for further study of this data. Promisingly, it has been shown that training with decreased precision may increase the tolerance of the network compared to error due to lower precision, but may increase the precision of the network."}, {"heading": "A SUPPLEMENTARY MATERIAL", "text": "Table 3 shows the caffe models used for each network and the caffe layers (computational levels) that are assigned to each layer in our analysis."}], "references": [], "referenceMentions": [], "year": 2016, "abstractText": "This work investigates how using reduced precision data in Convolutional Neural Networks (CNNs) affects network accuracy during classification. More specifically, this study considers networks where each layer may use different precision data. Our key result is the observation that the tolerance of CNNs to reduced precision data not only varies across networks, a well established observation, but also within networks. Tuning precision per layer is appealing as it could enable energy and performance improvements. In this paper we study how error tolerance across layers varies and propose a method for finding a low precision configuration for a network while maintaining high accuracy. A diverse set of CNNs is analyzed showing that compared to a conventional implementation using a 32-bit floatingpoint representation for all layers, and with less than 1% loss in relative accuracy, the data footprint required by these networks can be reduced by an average of 74% and up to 92%.", "creator": "LaTeX with hyperref package"}}}