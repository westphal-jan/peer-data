{"id": "1511.05266", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "17-Nov-2015", "title": "Semi-supervised Collaborative Ranking with Push at Top", "abstract": "Existing collaborative ranking based recommender systems tend to perform best when there is enough observed ratings for each user and the observation is made completely at random. Under this setting recommender systems can properly suggest a list of recommendations according to the user interests. However, when the observed ratings are extremely sparse (e.g. in the case of cold-start users where no rating data is available), and are not sampled uniformly at random, existing ranking methods fail to effectively leverage side information to transduct the knowledge from existing ratings to unobserved ones. We propose a semi-supervised collaborative ranking model, dubbed \\texttt{S$^2$COR}, to improve the quality of cold-start recommendation. \\texttt{S$^2$COR} mitigates the sparsity issue by leveraging side information about both observed and missing ratings by collaboratively learning the ranking model. This enables it to deal with the case of missing data not at random, but to also effectively incorporate the available side information in transduction. We experimentally evaluated our proposed algorithm on a number of challenging real-world datasets and compared against state-of-the-art models for cold-start recommendation. We report significantly higher quality recommendations with our algorithm compared to the state-of-the-art.", "histories": [["v1", "Tue, 17 Nov 2015 04:02:26 GMT  (25kb)", "http://arxiv.org/abs/1511.05266v1", null]], "reviews": [], "SUBJECTS": "cs.LG cs.IR", "authors": ["iman barjasteh", "rana forsati", "abdol-hossein esfahanian", "hayder radha"], "accepted": false, "id": "1511.05266"}, "pdf": {"name": "1511.05266.pdf", "metadata": {"source": "CRF", "title": "Semi-supervised Collaborative Ranking with Push at Top", "authors": ["Iman Barjasteh", "Rana Forsati", "Abdol-Hossein Esfahanian", "Hayder Radha"], "emails": ["forsati@cse.msu.edu,", "esfahanian@cse.msu.edu,", "barjaste@msu.edu", "radha@msu.edu"], "sections": [{"heading": null, "text": "ar Xiv: 151 1,05 266v 1 [cs"}, {"heading": "1 Introduction", "text": "In fact, most of them will be able to follow the rules without being able to do so. (...) Most of them will not be able to follow the rules. (...) Most of them will not be able to follow the rules. (...) Most of them will not be able to follow the rules. (...) Most of them will not be able to follow the rules. (...) Most of them will not be able to follow the rules. (...) Most of them will not be able to follow the rules. (...) Most of them will not be able to follow the rules. (...) Most of them will not be able to follow the rules. (...) Most of them will be able to decide for themselves. (...)"}, {"heading": "Is it possible to effectively learn a collaborative ranking model in the presence of cold-start items/users that is robust to the sampling of observed ratings?", "text": "This year it has come to the point that it will only be a matter of time before it will happen, until it does."}, {"heading": "2 Related Work", "text": "This year it has come to the point that it will only be a matter of time before it will happen, until it does."}, {"heading": "3 Preliminaries", "text": "In this section we note the notation used throughout the paper and formally describe our problem. Scalars are indicated by lowercase letters and vectors by bold lowercase letters such as u. We use bold uppercase letters such as M to identify matrices. Frobenius norms of a matrix M, Rn, m are indicated by bold lowercase letters such as u."}, {"heading": "4 Transductive Collaborating Ranking", "text": "We now turn our attention to the main purpose of the paper, in which we present our transductive collaborative ranking algorithm with precision at the top, exploiting the characteristics of unevaluated data. We start with the basic formulation and then extend it to include the unevaluated items. The pseudo-code of the resulting learning algorithm is contained in Algorithm 1."}, {"heading": "4.1 A basic formulation", "text": "We look at a ranking problem where the unrated items are rated in order of relevance to the user."}, {"heading": "4.2 Semi-supervised collaborative ranking", "text": "In this section, we expand the proposed ranking idea to learn from both rated and unrated items. The motivation to integrate unrated items stems from the following key observations. < < < < < < < < < > Integrating items (unrated items) stems from the following key observations. (2) The unrated items do not play a role in learning the model for each user, as learning is based only on the pair of rated items. If the feature information is available for items, it would be very helpful if such unrated items could be used in the Learning Torank process to effectively use the available page information. By using both types of rated and unrated items, we can compensate for the lack of rating data. Second, non-randomness in observing the observed evaluations creates a bias in learning that may affect the accuracy of the model."}, {"heading": "5 The Optimization", "text": "Now we turn to the solution of the optimization problem in (5). We start by discussing a method of gradient descent with shrink operator, followed by its accelerated version, and then propose a non-convex formulation with alternative minimization for more effective optimization of the target in S2COR."}, {"heading": "5.1 Gradient descent with shrinkage operator", "text": "In this section, we propose an easy way to solve the above optimization problem. First, we write the target as: min W + Rn \u00b7 dF (W), which must evaluate the lens gradient at each iteration. To deal with the non-smooth threshold, we must first note that the optimization problem can be reformed in Eq (6). (6) Using the compositional structure, it is possible that the optimization problem can be reformed in Eq. (6) the optimization problem can be reformed in Eq."}, {"heading": "5.2 Efficient optimization by dropping convexity", "text": "The most important cost factors in each iteration of the S2COR algorithm are the calculation of the SVD decomposition of Wk. An alternative would be to solve the optimization problem in Eq. (6) For a fixed order of the target parameter matrix W, let's say, onecan decompose it as W = UV. Of the equivalence relationship between the orm and the Frobenius of its components in the decomposition, we can write the target K-Rn-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K"}, {"heading": "6 Regularization by Exploiting Similarity of Users", "text": "In many recommended systems, in addition to handling cold-start items, it would be advantageous to provide high-quality recommendations to new users. (The new user cold-start problem is a serious problem in recommended systems, as it can lead to the loss of new users who decide to stop using the system because the recommendations received in this first stage do not yet contain a significant number of ratings with which the recommenders can feed the system's collaborative filtering correction. (The above formulation estimates a parameter vector separately for each user regardless of potential similarities between users.) In this section, we generalize the proposed ranking list of users. (In this section, we generalize the proposed ranking modeling algorithm 2 S2COR + 1: Input: Input R +: the suggested ranking parameters, {Procedure} the Imp: 1: Scale S: Scale 2S)."}, {"heading": "7 Experiments", "text": "In this section, we will conduct in-depth experiments to demonstrate the merits and benefits of the proposed algorithm. We will conduct our experiments on three known data sets, MovieLens, Amazon and CiteULike. We will examine how the proposed S2COR compares to the most advanced methods. In the following, we will first present the data sets we use in our experiments, and then the metrics we use to evaluate the results, followed by our detailed experimental results on the real data sets. In the following subsections, we will answer these key questions: \u2022 Ranking versus Rating: How does learning optimization for a rank-based loss function affect the performance of the recommendation versus the square-loss function? \u2022 Use of missing ratings: How can the use of the missing ratings contribute to making more precise recommendations? \u2022 Handling cold-start items: How does the proposed algorithm work with ancillary information of items compared to the state of the art?"}, {"heading": "7.1 Datasets", "text": "We use the following known datasets to evaluate the performance of S2COR: \u2022 ML-IMDB. We used ML-IMDB, a dataset extracted from the IMDB and MovieLens 1M datasets by mapping the MovieLens and IMDB and collecting the movies that have charts and keywords. The rating values are 10 single numbers ranging from 1 to 10, and the rating became binary by having all ratings greater than 5 than + 1 and below 5 than \u2212 1. \u2022 Amazon. We also used the data set of the best-selling books and their ratings in Amazon. Each book has one or two paragraphs of the text description that was used to have a number of characteristics of the books. Ratings can be integer numbers from 1 to 5. The ratings were also binary by having all ratings greater than or equal to 3 as + 1 and below 3 as \u2212 1. For all the above datasets, the descriptions of the stops, the DF and the rest of the statistics were presented as the words, the rest of the words, and the DF."}, {"heading": "7.2 Metrics", "text": "We use the widely used metrics Discounted Cumulative Gain at n and Recall at n to assess the performance of our and the base algorithms. For each user u who receives an item i, let sk be the relevance value of the item ranked at position k, where sk = 1 is if the item is relevant to user u and sk = 0 otherwise. Well, given the list of top n item recommendations for user u, Discounted Cumulative Gain at n is defined as: DCGu @ n = s1 + n \u2211 k = 2sk log2 (k) If we divide the DCGu @ n by its maximum value, we get the NDCGu @ n value. Given the list of top n item recommendations for each user u, Recall at n counts the number of relevant item recommendations that appear in the recommendation list divided by the size of the list. Recall at n is defined as {RECU-IteG: relevant to each user {| In for each user."}, {"heading": "7.3 Methodology", "text": "Given the partially observed evaluation matrix, we transformed the observed scores of all data sets from a multi-level relevance scale to a two-level scale (+ 1, \u2212 1), while 0 is considered for non-observed scores. We randomly selected 60% of observed scores for training and 20% for validation scores, and considered the remaining 20% of scores as our test set. To better evaluate the results, we performed triple cross-validation and averaged our results."}, {"heading": "7.4 Baseline Algorithms", "text": "The proposed S2COR algorithms are compared with the following algorithms: \u2022 Matrix Factorization (MF): Is a matrix processing method that maps the incomplete matrix and completes the matrix factor process. \u2022 Matrix Factorization with Side Information (KPMF): Is a matrix processing based on algorithms that includes external page information from users."}, {"heading": "7.6 Robustness to not missing at random ratings", "text": "Most methods in the literature ignore the unobserved evaluations and train their model only on the basis of the observed evaluations. By including the unevaluated items in the ranking, our method can limit the distortion caused by learning solely on the basis of the observed evaluations, and therefore addresses the non-random question of evaluations. Table 3 shows the results of comparing these two scenarios for S2COR on MLIMDB. To see the difference between these two scenarios, we looked at 70% of the evaluations for training and 30% for the test in order to have more basic truthfulness in our tests. Table 3 shows the NDCG @ 5, 10.15 and 20 for both scenarios and shows that the inclusion of the unobserved evaluations leads to an improvement in the accuracy of the recommendation list. Therefore, the NDCG values for the top 5, 10, 15 and 20 items improved when unevaluated items were included as part of the training process."}, {"heading": "7.7 Dealing with cold-start items", "text": "We now turn to evaluating the effectiveness of S2COR for cold start recommendations. To do this, we randomly selected 60% of items as training elements and 20% for validation sets, and considered the remaining 20% of items as a test set. In this scenario, the baseline algorithms used for comparison are CSR, FBS, CUFSM, and RLF. For the experiments, we used ML-IMDB, Amazon, and CiteULike datasets. Table 4 shows the measurement results of applying the aforementioned algorithms to these datasets. For each test, the values of the parameters that yielded the best placement on the validation set were selected, used, and reported. As can be seen from the results in Table 4, the proposed S2COR algorithm outperformed all the other baseline algorithms and provided a recommendation with higher quality compared to other methods. We can also see from Table 4 that the improvement of these datasets compared to other calculation algorithms is essential."}, {"heading": "8 Conclusions", "text": "In this paper, we have introduced a semi-supervised collaborative ranking model by using lateral information about observed and missing scores when learning the ranking model together. In the learned model, unevaluated items are conservatively pushed behind the relevant and before the relevant items in the item ranking for each individual user. This crucial difference increases performance and limits the bias caused by learning only from sparse, not randomly observed scores. The proposed algorithm is compared with seven baseline algorithms on three real-world datasets that have shown the effectiveness of the proposed algorithm in solving cold-start problems and mitigating the problem of data sparseness, while being robust to the sample of missing scores. Second, we would like to examine the performance of the proposed S2COR algorithm when lateral information about users is also available, with the graph regulation idea discussed in Section 6 below. Secondly, we would like to evaluate performance in an optimistic way."}], "references": [{"title": "Toward the next generation of recommender systems: A survey of the state-of-the-art and possible extensions", "author": ["G. Adomavicius", "A. Tuzhilin"], "venue": "IEEE Transactions on Knowledge and Data Engineering, 17(6):734\u2013749", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2005}, {"title": "Regression-based latent factor models", "author": ["D. Agarwal", "B.-C. Chen"], "venue": "SIGKDD, pages 19\u201328. ACM", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2009}, {"title": "The infinite push: A new support vector ranking algorithm that directly optimizes accuracy at the absolute top of the list", "author": ["S. Agarwal"], "venue": "SDM, pages 839\u2013850. SIAM", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2011}, {"title": "Collaborative ranking", "author": ["S. Balakrishnan", "S. Chopra"], "venue": "ACM WSDM, pages 143\u2013152. ACM", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2012}, {"title": "Cold-start item and user recommendation with decoupled completion and transduction", "author": ["I. Barjasteh", "R. Forsati", "F. Masrour", "A.-H. Esfahanian", "H. Radha"], "venue": "Proceedings of the 9th ACM Conference on Recommender Systems, pages 91\u201398. ACM", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2015}, {"title": "Manifold regularization: A geometric framework for learning from labeled and unlabeled examples", "author": ["M. Belkin", "P. Niyogi", "V. Sindhwani"], "venue": "The Journal of Machine Learning Research, 7:2399\u20132434", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2006}, {"title": "A tutorial on support vector machines for pattern recognition", "author": ["C.J. Burges"], "venue": "Data mining and knowledge discovery, 2(2):121\u2013167", "citeRegEx": "7", "shortCiteRegEx": null, "year": 1998}, {"title": "A singular value thresholding algorithm for matrix completion", "author": ["J.-F. Cai", "E.J. Cand\u00e8s", "Z. Shen"], "venue": "SIAM Journal on Optimization, 20(4):1956\u20131982", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2010}, {"title": "The power of convex relaxation: Near-optimal matrix completion", "author": ["E.J. Cand\u00e8s", "T. Tao"], "venue": "Information Theory, IEEE Transactions on, 56(5):2053\u20132080", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2010}, {"title": "Collaborative ranking with a push at the top", "author": ["K. Christakopoulou", "A. Banerjee"], "venue": "WWW, pages 205\u2013215. International WorldWide Web Conferences Steering Committee", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2015}, {"title": "Performance of recommender algorithms on top-n recommendation tasks", "author": ["P. Cremonesi", "Y. Koren", "R. Turrin"], "venue": "ACM RecSys, pages 39\u201346. ACM", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2010}, {"title": "Dynamic matrix factorization with priors on unknown values", "author": ["R. Devooght", "N. Kourtellis", "A. Mantrach"], "venue": "ACM SIGKDD, pages 189\u2013198. ACM", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2015}, {"title": "User-specific feature-based similarity models for top-n recommendation of new items", "author": ["A. Elbadrawy", "G. Karypis"], "venue": "ACM Transactions on Intelligent Systems and Technology (TIST), 6(3):33", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2015}, {"title": "Ir evaluation methods for retrieving highly relevant documents", "author": ["K. J\u00e4rvelin", "J. Kek\u00e4l\u00e4inen"], "venue": "ACM SIGIR, pages 41\u201348. ACM", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2000}, {"title": "Matrix factorization techniques for recommender systems", "author": ["Y. Koren", "R. Bell", "C. Volinsky"], "venue": "Computer, (8):30\u201337", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2009}, {"title": "Local collaborative ranking", "author": ["J. Lee", "S. Bengio", "S. Kim", "G. Lebanon", "Y. Singer"], "venue": "Proceedings of the 23rd international conference on World wide web, pages 85\u201396. ACM", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2014}, {"title": "Top rank optimization in linear time", "author": ["N. Li", "R. Jin", "Z.-H. Zhou"], "venue": "Advances in Neural Information Processing Systems, pages 1502\u20131510", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2014}, {"title": "Learning to rank for information retrieval", "author": ["T.-Y. Liu"], "venue": "Foundations and Trends in Information Retrieval, 3(3):225\u2013331", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2009}, {"title": "Collaborative prediction and ranking with non-random missing data", "author": ["B.M. Marlin", "R.S. Zemel"], "venue": "RecSys, pages 5\u201312. ACM", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2009}, {"title": "Collaborative filtering and the missing at random assumption", "author": ["B.M. Marlin", "R.S. Zemel", "S.T. Roweis", "M. Slaney"], "venue": "UAI, pages 267\u2013275", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2007}, {"title": "Robust stochastic approximation approach to stochastic programming", "author": ["A. Nemirovski", "A. Juditsky", "G. Lan", "A. Shapiro"], "venue": "SIAM Journal on Optimization, 19(4):1574\u20131609", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2009}, {"title": "Introductory lectures on convex optimization", "author": ["Y. Nesterov"], "venue": "volume 87. Springer Science & Business Media", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2004}, {"title": "Gradient methods for minimizing composite functions", "author": ["Y. Nesterov"], "venue": "Mathematical Programming, 140(1):125\u2013161", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2013}, {"title": "Pairwise preference regression for cold-start recommendation", "author": ["S.-T. Park", "W. Chu"], "venue": "RecSys, pages 21\u201328. ACM", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2009}, {"title": "Factorization machines with libfm", "author": ["S. Rendle"], "venue": "ACM Transactions on Intelligent Systems and Technology (TIST), 3(3):57", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2012}, {"title": "The p-norm push: A simple convex ranking algorithm that concentrates at the top of the list", "author": ["C. Rudin"], "venue": "The Journal of Machine Learning Research, 10:2233\u20132271", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2009}, {"title": "Item cold-start recommendations: learning local collective embeddings", "author": ["M. Saveski", "A. Mantrach"], "venue": "RecSys, pages 89\u201396. ACM", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2014}, {"title": "Methods and metrics for cold-start recommendations", "author": ["A.I. Schein", "A. Popescul", "L.H. Ungar", "D.M. Pennock"], "venue": "SIGIR, pages 253\u2013260. ACM", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2002}, {"title": "Feature-based factorized bilinear similarity model for cold-start top-n item recommendation", "author": ["M. Sharma", "J. Zhou", "J. Hu", "G. Karypis"], "venue": "SDM", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2015}, {"title": "List-wise learning to rank with matrix factorization for collaborative filtering", "author": ["Y. Shi", "M. Larson", "A. Hanjalic"], "venue": "ACM RecSys, pages 269\u2013272. ACM", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2010}, {"title": "Collaborative filtering beyond the user-item matrix: A survey of the state of the art and future challenges", "author": ["Y. Shi", "M. Larson", "A. Hanjalic"], "venue": "ACM Computing Surveys (CSUR), 47(1):3", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2014}, {"title": "One-class matrix completion with low-density factorizations", "author": ["V. Sindhwani", "S.S. Bucak", "J. Hu", "A. Mojsilovic"], "venue": "ICDM, pages 1055\u20131060. IEEE", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2010}, {"title": "Maximum-margin matrix factorization", "author": ["N. Srebro", "J. Rennie", "T.S. Jaakkola"], "venue": "Advances in neural information processing systems, pages 1329\u20131336", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2004}, {"title": "Training and testing of recommender systems on data missing not at random", "author": ["H. Steck"], "venue": "KDD, pages 713\u2013722. ACM", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2010}, {"title": "Gaussian ranking by matrix factorization", "author": ["H. Steck"], "venue": "Proceedings of the 9th ACM Conference on Recommender Systems, pages 115\u2013122. ACM", "citeRegEx": "35", "shortCiteRegEx": null, "year": 2015}, {"title": "Collaborative ranking with 17 parameters", "author": ["M. Volkovs", "R.S. Zemel"], "venue": "Advances in Neural Information Processing Systems, pages 2294\u20132302", "citeRegEx": "36", "shortCiteRegEx": null, "year": 2012}, {"title": "Maximum margin matrix factorization for collaborative ranking", "author": ["M. Weimer", "A. Karatzoglou", "Q.V. Le", "A. Smola"], "venue": "NIPS", "citeRegEx": "37", "shortCiteRegEx": null, "year": 2007}, {"title": "Kernelized probabilistic matrix factorization: Exploiting graphs and side information", "author": ["T. Zhou", "H. Shan", "A. Banerjee", "G. Sapiro"], "venue": "SDM, volume 12, pages 403\u2013414. SIAM", "citeRegEx": "38", "shortCiteRegEx": null, "year": 2012}], "referenceMentions": [{"referenceID": 0, "context": "products, books, movies) [1].", "startOffset": 25, "endOffset": 28}, {"referenceID": 14, "context": "In collaborative filtering (CF) methods such as matrix factorization [15], where the aim is to accurately predict the ratings, the latent features are extracted in a way to minimize the prediction error measured in terms of popular performance measures such as root mean square error (RMSE).", "startOffset": 69, "endOffset": 73}, {"referenceID": 14, "context": "In spark contrast to CF, in collaborating ranking (CR) models [15, 10, 36, 11], where the goal is to rank the unrated items in the order of relevance to the user, the popular ranking measures such as as discounted cumulative gain (DCG), normalized discounted cumulative gain (NDCG), and average precision (AP) [14] are often employed to collaboratively learn a ranking model for the latent features.", "startOffset": 62, "endOffset": 78}, {"referenceID": 9, "context": "In spark contrast to CF, in collaborating ranking (CR) models [15, 10, 36, 11], where the goal is to rank the unrated items in the order of relevance to the user, the popular ranking measures such as as discounted cumulative gain (DCG), normalized discounted cumulative gain (NDCG), and average precision (AP) [14] are often employed to collaboratively learn a ranking model for the latent features.", "startOffset": 62, "endOffset": 78}, {"referenceID": 35, "context": "In spark contrast to CF, in collaborating ranking (CR) models [15, 10, 36, 11], where the goal is to rank the unrated items in the order of relevance to the user, the popular ranking measures such as as discounted cumulative gain (DCG), normalized discounted cumulative gain (NDCG), and average precision (AP) [14] are often employed to collaboratively learn a ranking model for the latent features.", "startOffset": 62, "endOffset": 78}, {"referenceID": 10, "context": "In spark contrast to CF, in collaborating ranking (CR) models [15, 10, 36, 11], where the goal is to rank the unrated items in the order of relevance to the user, the popular ranking measures such as as discounted cumulative gain (DCG), normalized discounted cumulative gain (NDCG), and average precision (AP) [14] are often employed to collaboratively learn a ranking model for the latent features.", "startOffset": 62, "endOffset": 78}, {"referenceID": 13, "context": "In spark contrast to CF, in collaborating ranking (CR) models [15, 10, 36, 11], where the goal is to rank the unrated items in the order of relevance to the user, the popular ranking measures such as as discounted cumulative gain (DCG), normalized discounted cumulative gain (NDCG), and average precision (AP) [14] are often employed to collaboratively learn a ranking model for the latent features.", "startOffset": 310, "endOffset": 314}, {"referenceID": 25, "context": "Therefore, the introduction of ranking metrics such as push norm or infinite norm [26, 3, 10, 16], sparked a widespread interest in CR models and has been proven to be more effective in practice [35, 10].", "startOffset": 82, "endOffset": 97}, {"referenceID": 2, "context": "Therefore, the introduction of ranking metrics such as push norm or infinite norm [26, 3, 10, 16], sparked a widespread interest in CR models and has been proven to be more effective in practice [35, 10].", "startOffset": 82, "endOffset": 97}, {"referenceID": 9, "context": "Therefore, the introduction of ranking metrics such as push norm or infinite norm [26, 3, 10, 16], sparked a widespread interest in CR models and has been proven to be more effective in practice [35, 10].", "startOffset": 82, "endOffset": 97}, {"referenceID": 15, "context": "Therefore, the introduction of ranking metrics such as push norm or infinite norm [26, 3, 10, 16], sparked a widespread interest in CR models and has been proven to be more effective in practice [35, 10].", "startOffset": 82, "endOffset": 97}, {"referenceID": 34, "context": "Therefore, the introduction of ranking metrics such as push norm or infinite norm [26, 3, 10, 16], sparked a widespread interest in CR models and has been proven to be more effective in practice [35, 10].", "startOffset": 195, "endOffset": 203}, {"referenceID": 9, "context": "Therefore, the introduction of ranking metrics such as push norm or infinite norm [26, 3, 10, 16], sparked a widespread interest in CR models and has been proven to be more effective in practice [35, 10].", "startOffset": 195, "endOffset": 203}, {"referenceID": 27, "context": ", the density of the data is around 1% for many publicly available datasets) or for a subset of users or items the rating data is entirely missing (knows as cold-start user and cold-start item problem, respectively) [28].", "startOffset": 216, "endOffset": 220}, {"referenceID": 33, "context": "However, in many real applications of recommender systems, this assumption is not believed to hold, as invariably some users are more active than others and some items are rated by many people while others are rarely rated [34].", "startOffset": 223, "endOffset": 227}, {"referenceID": 9, "context": "These issues have been investigated in factorization based methods, nonetheless, it is not straightforward to adapt them to CR models and are left open [10].", "startOffset": 152, "endOffset": 156}, {"referenceID": 25, "context": "In summary, the key features of SCOR are: \u2022 Inspired by recent developments in ranking at top [26, 3, 17], the proposed model is a collaborative ranking model that primarily focuses on the top of the recommendation list for each user.", "startOffset": 94, "endOffset": 105}, {"referenceID": 2, "context": "In summary, the key features of SCOR are: \u2022 Inspired by recent developments in ranking at top [26, 3, 17], the proposed model is a collaborative ranking model that primarily focuses on the top of the recommendation list for each user.", "startOffset": 94, "endOffset": 105}, {"referenceID": 16, "context": "In summary, the key features of SCOR are: \u2022 Inspired by recent developments in ranking at top [26, 3, 17], the proposed model is a collaborative ranking model that primarily focuses on the top of the recommendation list for each user.", "startOffset": 94, "endOffset": 105}, {"referenceID": 11, "context": "This is equivalent to assuming a prior about the unknown ratings which is believed to perform well as investigated in [12].", "startOffset": 118, "endOffset": 122}, {"referenceID": 11, "context": "However, unlike [12], the proposed ranking idea is free of deciding an explicit value for missing ratings", "startOffset": 16, "endOffset": 20}, {"referenceID": 3, "context": "The PMF-based approach [4] uses the latent representations produced by matrix factorization as user-item features and learns a ranking model on these features.", "startOffset": 23, "endOffset": 26}, {"referenceID": 36, "context": "CofiRank [37] learns latent representations that minimize a ranking-based loss instead of the squared error.", "startOffset": 9, "endOffset": 13}, {"referenceID": 29, "context": "ListRankMF [30] aims at minimizing the cross entropy between the predict item permutation probability and true item permutation probability.", "startOffset": 11, "endOffset": 15}, {"referenceID": 15, "context": "In [16] a method for Local Collaborative Ranking (LCR) where ideas of local low-rank matrix approximation were applied to the pairwise ranking loss minimization framework is introduced.", "startOffset": 3, "endOffset": 7}, {"referenceID": 34, "context": "In [35] a framework that allows for pointwise as well as listwise training with respect to various ranking metrics is proposed.", "startOffset": 3, "endOffset": 7}, {"referenceID": 9, "context": "Finally, [10] proposed a CR model build on the recent developments in ranking methods [3, 26] that focus on accuracy at top and proposed CR methods with p-push and infinite push norms.", "startOffset": 9, "endOffset": 13}, {"referenceID": 2, "context": "Finally, [10] proposed a CR model build on the recent developments in ranking methods [3, 26] that focus on accuracy at top and proposed CR methods with p-push and infinite push norms.", "startOffset": 86, "endOffset": 93}, {"referenceID": 25, "context": "Finally, [10] proposed a CR model build on the recent developments in ranking methods [3, 26] that focus on accuracy at top and proposed CR methods with p-push and infinite push norms.", "startOffset": 86, "endOffset": 93}, {"referenceID": 30, "context": "users or items besides the rating data that are usually available [31].", "startOffset": 66, "endOffset": 70}, {"referenceID": 23, "context": "A feature based regression ranking model for predicting the values (rates) of user-item matrix in cold-start scenarios by leveraging all information available for users and items is proposed in [24].", "startOffset": 194, "endOffset": 198}, {"referenceID": 37, "context": "The kernelized matrix factorization approach studied in [38], which incorporates the auxiliary information into the MF.", "startOffset": 56, "endOffset": 60}, {"referenceID": 26, "context": "In [27] joint factorization of the user-item and item-feature matrices by using the same item latent feature matrix in both decompositions is utilized.", "startOffset": 3, "endOffset": 7}, {"referenceID": 28, "context": "The FBSM model is introduced in [29], which learns factorized bilinear similarity model for new items, given the rating information as well as the features of these items.", "startOffset": 32, "endOffset": 36}, {"referenceID": 4, "context": "Recently, [5] proposed a decoupling approach to transduct knowledge from side information to rating prediction which is able to handle both cold-start items and users problems in factorization based models.", "startOffset": 10, "endOffset": 13}, {"referenceID": 19, "context": "Substantial evidence for violations of the missing at random condition in recommender systems is reported in [20] and it has been showed that incorporating an explicit model of the missing data mechanism can lead to significant improvements in prediction performance.", "startOffset": 109, "endOffset": 113}, {"referenceID": 18, "context": "The first study of the effect of non-random missing data on collaborative ranking is presented in [19].", "startOffset": 98, "endOffset": 102}, {"referenceID": 31, "context": "In [32] an EM algorithm to optimize in turn the factorization and the estimation of missing values.", "startOffset": 3, "endOffset": 7}, {"referenceID": 11, "context": "Recently, in [12] a novel dynamic matrix factorization framework that allows to set an explicit prior on unknown values is introduced.", "startOffset": 13, "endOffset": 17}, {"referenceID": 17, "context": "These algorithms reduce the ranking problem into a binary classification problem by treating each relevant/irrelevant instance pair as a single object to be classified [18].", "startOffset": 168, "endOffset": 172}, {"referenceID": 25, "context": "Specifically, we propose an algorithm that maximizes the number of relevant items which are pushed to the absolute top of the list by utilizing the P-Norm Push ranking measure which is specially designed for this purpose [26] .", "startOffset": 221, "endOffset": 225}, {"referenceID": 32, "context": "The trace-norm regularization is well-known to be a convex surrogate to the matrix rank, and has repeatedly shown good performance in practice [33, 9].", "startOffset": 143, "endOffset": 150}, {"referenceID": 8, "context": "The trace-norm regularization is well-known to be a convex surrogate to the matrix rank, and has repeatedly shown good performance in practice [33, 9].", "startOffset": 143, "endOffset": 150}, {"referenceID": 6, "context": ", [7]) .", "startOffset": 2, "endOffset": 5}, {"referenceID": 22, "context": "A simple way to solving the above optimization problem is gradient descent algorithm [23], which needs to evaluate the gradient of objective at each iteration.", "startOffset": 85, "endOffset": 89}, {"referenceID": 22, "context": "(6) can be reformulated under the framework of proximal regularization or composite gradient mapping [23].", "startOffset": 101, "endOffset": 105}, {"referenceID": 7, "context": "We use the singular value shrinkage operator introduced in [8] to find the optimal solution to Eq.", "startOffset": 59, "endOffset": 62}, {"referenceID": 7, "context": "1, [8]) For each \u03c4 \u2265 0 and W \u2208 R, the singular value shrinkage operator (9) obeys", "startOffset": 3, "endOffset": 6}, {"referenceID": 21, "context": "Since the subdifferential of the maximum of functions is the convex hull of the union of subdifferentials of the active functions at point w [22], we have:", "startOffset": 141, "endOffset": 145}, {"referenceID": 21, "context": "However, by using other smooth convex surrogate losses such as smoothed hinge loss or exponential loss, one can apply the accelerated gradient descent methods [22] to solve the optimization problem which results in significantly faster convergence rate compared to the naive gradient descent (i.", "startOffset": 159, "endOffset": 163}, {"referenceID": 20, "context": "To alleviate this problem one can utilize stochastic gradient method [21] to solve the optimization problem.", "startOffset": 69, "endOffset": 73}, {"referenceID": 5, "context": "It is natural to require the similar users in the matrix S have similarity evaluation on rating the items [6].", "startOffset": 106, "endOffset": 109}, {"referenceID": 28, "context": "Then those words that have been appeared in less than 20 items and more that 20% of the items were also removed [29].", "startOffset": 112, "endOffset": 116}, {"referenceID": 32, "context": "4 Baseline Algorithms The proposed SCOR algorithm is compared to the following algorithms: \u2022 Matrix Factorization (MF) [33]: Is a matrix completion method that factorizes the incomplete observed matrix and completes the matrix using the unveiled latent features.", "startOffset": 119, "endOffset": 123}, {"referenceID": 37, "context": "\u2022 Matrix Factorization with Side Information (KPMF) [38]: Is a matrix completion based algorithm, which incorporates external side information of the users or items into the matrix factorization process.", "startOffset": 52, "endOffset": 56}, {"referenceID": 4, "context": "\u2022 Decoupled Completion and Transduction (DCT) [5]: Is a matrix factorization based algorithm that decouples the completion and transduction stages and exploits the similarity information among users and items to complete the (rating) matrix.", "startOffset": 46, "endOffset": 49}, {"referenceID": 28, "context": "\u2022 Feature Based Factorized Bilinear Similarity Model (FBS) [29]: This algorithm uses bilinear model to capture pairwise dependencies between the features.", "startOffset": 59, "endOffset": 63}, {"referenceID": 12, "context": "This method is one of the best performing collaborative latent factor based model [13].", "startOffset": 82, "endOffset": 86}, {"referenceID": 1, "context": "\u2022 Regression based Latent Factor Model (RLF): This method incorporates the features of items in factorization process by transforming the features to the latent space using linear regression [2].", "startOffset": 191, "endOffset": 194}, {"referenceID": 24, "context": "The implementation of this method is available in LibFM library [25].", "startOffset": 64, "endOffset": 68}, {"referenceID": 9, "context": "Then, due to the fact that users tend to only care about the top of their recommendation list, predicting a ranking of items of interest instead of ratings became the main focus of recent works [10].", "startOffset": 194, "endOffset": 198}], "year": 2015, "abstractText": "Existing collaborative ranking based recommender systems tend to perform best when there is enough observed ratings for each user and the observation is made completely at random. Under this setting recommender systems can properly suggest a list of recommendations according to the user interests. However, when the observed ratings are extremely sparse (e.g. in the case of cold-start users where no rating data is available), and are not sampled uniformly at random, existing ranking methods fail to effectively leverage side information to transduct the knowledge from existing ratings to unobserved ones. We propose a semi-supervised collaborative ranking model, dubbed SCOR, to improve the quality of cold-start recommendation. SCOR mitigates the sparsity issue by leveraging side information about both observed and missing ratings by collaboratively learning the ranking model. This enables it to deal with the case of missing data not at random, but to also effectively incorporate the available side information in transduction. We experimentally evaluated our proposed algorithm on a number of challenging real-world datasets and compared against state-of-the-art models for cold-start recommendation. We report significantly higher quality recommendations with our algorithm compared to the state-of-the-art.", "creator": "LaTeX with hyperref package"}}}