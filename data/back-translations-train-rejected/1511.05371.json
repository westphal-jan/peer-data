{"id": "1511.05371", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "17-Nov-2015", "title": "Constant Time EXPected Similarity Estimation using Stochastic Optimization", "abstract": "A new algorithm named EXPected Similarity Estimation (EXPoSE) was recently proposed to solve the problem of large-scale anomaly detection. It is a non-parametric and distribution free kernel method based on the Hilbert space embedding of probability measures. Given a dataset of $n$ samples, EXPoSE needs only $\\mathcal{O}(n)$ (linear time) to build a model and $\\mathcal{O}(1)$ (constant time) to make a prediction. In this work we improve the linear computational complexity and show that an $\\epsilon$-accurate model can be estimated in constant time, which has significant implications for large-scale learning problems. To achieve this goal, we cast the original EXPoSE formulation into a stochastic optimization problem. It is crucial that this approach allows us to determine the number of iteration based on a desired accuracy $\\epsilon$, independent of the dataset size $n$. We will show that the proposed stochastic gradient descent algorithm works in general (possible infinite-dimensional) Hilbert spaces, is easy to implement and requires no additional step-size parameters.", "histories": [["v1", "Tue, 17 Nov 2015 12:10:03 GMT  (362kb,D)", "http://arxiv.org/abs/1511.05371v1", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["markus schneider", "wolfgang ertel", "g\\\"unther palm"], "accepted": false, "id": "1511.05371"}, "pdf": {"name": "1511.05371.pdf", "metadata": {"source": "CRF", "title": "CONSTANT TIME EXPECTED SIMILARITY ESTIMATION USING STOCHASTIC OPTIMIZATION", "authors": ["markus schneider"], "emails": [], "sections": [{"heading": null, "text": "A new algorithm called EXPected Similarity Estimation (EXPoSE) was recently proposed to solve the problem of detecting large-scale anomalies. It is a non-parametric and non-distributive core method based on the Hilbert Space embedding of probability variables. In the face of a dataset of n samples, EXPoSE only needs O (n) (linear time) to build a model, and O (1) (constant time) to make a prediction. In this work, we improve linear computational complexity and show that an accurate model can be estimated in constant time, which has significant implications for large-scale learning problems. To achieve this goal, we pour the original formulation of EXPoSE into a stochastic optimization problem. Crucially, this approach allows us to determine the number of iterations based on a desired accuracy, regardless of the data set size proposed by the auxiliary linear space (we may not show any additional auxiliary linear space)."}, {"heading": "1 Introduction", "text": "EXPected Similarity Estimation (EXPoSE) was recently proposed to solve the problem of large-scale detection of anomalies where the number of training samples n and the dimension of data d are too high for most other algorithms [SEP15]. Here, \"detection of anomalies refers to the problem of finding patterns in data that do not match the expected behavior, patterns that are often referred to as anomalies.\" [CBK09] As explained later, the EXPoSE anomaly has detection classification (y) = < \u03c6 (y), \u00b5 [P] > calculates a score (the probability that y belongs to the class of normal data) using the internal product between a function card and the core mean card \u00b5 [P] of the distribution P (fig. 1)."}, {"heading": "1.1 CONTRIBUTIONS & RELATED WORK", "text": "In this thesis, we derive a methodology to create an - accurate model of \u00b5 [P], using only a random subset of training data by means of stochastic optimization.Definition 1: We say an algorithm finds a -precise solution w of an objective function f iff (w) 6 inf + for a given > 0. \"We will show that for the proposed objective function E [f (wt) -f (P]]]] 6 O (1 / t), where wt only needs access to random data elements, t \u00b2. The most important observation is that for a proposed objective function we can achieve E [f (wt) -f (p]]] 6 O (1 / t), where wt only needs access to random data elements, t \u00b2. Furthermore, it can be shown that (without further assumptions) the O (1 / t) rate is optimal for stochastic optimization."}, {"heading": "2 Problem Description", "text": "EXPOSE is a probable approach that assumes that the normal, non-abnormal data are distributed according to any measurement variable. Formally, X is a random variable that records values in a measurement room (X, X) with distribution P. We refer to the reproducing Hilbert space (RKHS), which is associated with the kernel k: X \u00b7 X \u2192 R with (H, < \u00b7, \u00b7 >). A RKHS is a Hilbert space of functions g: X \u2192 R, in which the evaluation of functional \u03b4x: g (x) is continuous. The function \u03c6: X \u2192 Hwithk (x, y) = < B (x), \u03c6 (y) > is a feature map denoted by \u03c6 (x) = k (x, \u00b7)."}, {"heading": "3 Stochastic Optimization", "text": "These sections derive the stochastic optimization problem along with some general conditions that will be required at a later stage. (Obviously \u00b5 [P], w > \u2212 2 < \u00b5 [P], w > + < \u00b5 [P], p [P] > = min w H1 2 < w, w > < \u00b5 [P], w > + < \u00b5 [P] > = min w H1 2 < w, w > < \u00b5 [P], w > < \u00b5 [P], w > < \u00b5 [P]. This corresponds to the stochastic optimization problem where we anticipate an objective function mode w H [f (w) = min w H X (w) dP (x), w)."}, {"heading": "3.1 STOCHASTIC APPROXIMATION", "text": "Let H be a Hilbert space, H \u2212 H a subset and f: H > R some objective function. (In addition, letH (w) = arg min v \u2212 H \u2212 v \u00b2 Hbe is the metric projection operator. (H) is generally not expanding, so H (w) \u2212 H (w) to solve the stochastic optimization problem in w \u2212 H. (H) is a sequence of positive step variables and the optimal solution of the problem is considered by w?.Nemirovski et al. (Nem + 09] as H = Rd. (H) is a sequence of positive step variables and the optimal solution of the problem is considered by w?. (H).Nemirovski et al. (F) as H = Rd and has shown that stochastic approximation can achieve an O (1 / t) if the optimal solution of the problem is achieved by w?"}, {"heading": "4 Stochastic Optimization of EXPoSE", "text": "In this section we show the existence and uniqueness of a solution to the previously defined problems. (< p) < p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p p? p p? p? p p? p p? p p? p? p p p p? p? p p p p p? p p p p p? p p p? p p? p p p p p p p p? p p p p p p p? p p p p p p p p p p? p p p p p p p p p p? p p p p p p p p p p p p p p p? p p p p p p p p p p p p p p p? p p p p p p p p p p p p p p p p p p? p p p p p p p p p p p p p p p p p p p p p p? p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p"}, {"heading": "4.1 CONVERGENCE OF EXPOSE", "text": "Since w \u2192 w? converges, this also implies the weak convergence [Pey15] of w'w? namelylim t \u2192 \u221e < u, wt > = < u, w? >, and especially the hand convergence t \u2192 \u221e < \u03c6 (y), wt > = < \u03c6 (y), \u00b5 [P] >, \u0394y-X. This justifies the use of wt as a substitute for \u00b5 [P]."}, {"heading": "4.2 REGULARIZATION", "text": "We would like to mention that the reformulation of EXPoSE as an optimization problem also introduces the possibility of adding constraints or similar properties to the objective function. An approach is the definition of a general regularization mechanism to H that replaces 12 < w, w > in equivalent (3) giving w = H E [f (w)] = min w = H-B (w) \u2212 < p > (X), w > dP with a certain regularization parameter \u03bb > 0. An example would be to add a roughness penalty to the space of functions that set (w) = \u03bb < D2w, D2w >, where D denotes the differential operator. Another possibility is to impose a saving constraint on w. If H admits this, we can use surface constraints (w) = surface constraints (w), where surface variation is the l1 standard. The disadvantage of other functions is that these are less constrained in general and may not require an objective constraint."}, {"heading": "5 Experimental Evaluation", "text": "??????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????"}, {"heading": "5.1 APPROXIMATE FEATURE MAPS", "text": "While it is theoretically possible to calculate quantities such as \u0432 wt \u2212 \u00b5 [Pn] \u0445 H for any kernel k, this is extremely slow and unruly for most large-format datasets. For datasets with a small sample size n, we cannot expect \u00b5 [Pn] to be a good substitute for \u00b5 [P]. To solve this problem, EXPoSE uses the idea of approximate function boards for their computing efficiency. The goal is to find approximate values: X \u2192 Rr such Thatk (x, y) (small n). To solve this problem, EXPOSE uses the idea of approximate function boards for their computing efficiency. We will use the Random Kitchen Sinks (RKS) approach as approximate quantity."}, {"heading": "5.2 DATASETS", "text": "The following datasets, all of which have intentionally very different characteristics, are used to detect anomalies. We refer to [SEP15] for a detailed description of the datasets and features. \u2022 The MNIST database contains 70,000 images of handwritten digits. As in [SEP15], we scale the 34 sequential characteristics to [0, 1] and apply binary encoding to the 7 symbolic characteristics. \u2022 The third dataset contains 600,000 instances of Google Street View house numbers (SVHN) [Net + 11], in which we use the histogram of oriented gradients (HOG) with a cell size of 3 to obtain a 2592-dimensional data model."}, {"heading": "5.3 DISCUSSION", "text": "The experimental results with approximate characteristic maps are shown in Fig. 2. The first row contains traces of the objective function f (wt) \u2212 f (w?), with w? \u2248 \u00b5 [Pn] for all three datasets. The stochastic optimization algorithm achieves a reasonably low target after just a few hundred iterations. Further improvement is only visible on a logarithmic scale (dashed blue) on the second y-axis to the right. More importantly, we observe a similar effect in the second row when comparing \"wt \u2212 w?.\" We approach the result relatively quickly, but it requires much more samples to estimate the result with high accuracy. However, we will see that a high accuracy of the prediction is necessary to achieve good anomaly detection performance. To measure the anomaly detection rate, we must first measure the anomaly detection rate, which we classify in the EXPOSE estimators."}, {"heading": "6 Conclusion", "text": "In this thesis, we throw the EXPoSE anomaly detection algorithm into a stochastic optimization problem. This allows us to make a -precise approximation of the kernel mean value map \u00b5 [P] in constant time, regardless of the training dataset size n. Specifically, this approximation reduces the computational complexity of EXPoSE and the empirical kernel mean value map from the previous O (n) to O (1) when a -accurate estimate is sufficient. Specifically, we are able to determine the number of necessary stochastic optimization siterations T for a custom error threshold so that a very high accuracy estimate w? does not necessarily lead to better anomaly detection performance, and therefore there is no benefit in issuing additional computing resources. This intuition is also experimentally confirmed on three large-scale datasets where we achieve the same anomaly recognition performance."}], "references": [{"title": "Information-Theoretic Lower Bounds on the Oracle Complexity of Convex Optimization Convex optimization", "author": ["A. Agarwal"], "venue": "Advances in Neural Information Processing Systems", "citeRegEx": "Agarwal,? \\Q2011\\E", "shortCiteRegEx": "Agarwal", "year": 2011}, {"title": "Adaptivity of averaged stochastic gradient descent to local strong convexity for logistic regression", "author": ["F. Bach"], "venue": "The Journal of Machine Learning Research", "citeRegEx": "Bach.,? \\Q2014\\E", "shortCiteRegEx": "Bach.", "year": 2014}, {"title": "The tradeoffs of large scale learning", "author": ["O. Bousquet", "L. Bottou"], "venue": "Advances in neural information processing systems", "citeRegEx": "Bousquet and Bottou.,? \\Q2008\\E", "shortCiteRegEx": "Bousquet and Bottou.", "year": 2008}, {"title": "Convex optimization", "author": ["S. Boyd", "L. Vandenberghe"], "venue": "Cambridge university press", "citeRegEx": "BV04", "shortCiteRegEx": null, "year": 2004}, {"title": "Anomaly detection: A survey", "author": ["V. Chandola", "A. Banerjee", "V. Kumar"], "venue": "ACM Computing Surveys (CSUR)", "citeRegEx": "Chandola et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Chandola et al\\.", "year": 2009}, {"title": "Measuring statistical dependence with Hilbert-Schmidt norms", "author": ["A. Gretton"], "venue": "Algorithmic learning theory. Springer", "citeRegEx": "Gretton,? \\Q2005\\E", "shortCiteRegEx": "Gretton", "year": 2005}, {"title": "A kernel two-sample test", "author": ["A. Gretton"], "venue": "The Journal of Machine Learning Research", "citeRegEx": "Gretton,? \\Q2012\\E", "shortCiteRegEx": "Gretton", "year": 2012}, {"title": "Primal-dual subgradient methods for minimizing uniformly convex functions", "author": ["A. Juditsky", "Y. Nesterov"], "venue": null, "citeRegEx": "Juditsky and Nesterov.,? \\Q2010\\E", "shortCiteRegEx": "Juditsky and Nesterov.", "year": 2010}, {"title": "Random feature maps for dot product kernels", "author": ["P. Kar", "H. Karnick"], "venue": "In: International Conference on Artificial Intelligence and Statistics", "citeRegEx": "Kar and Karnick.,? \\Q2012\\E", "shortCiteRegEx": "Kar and Karnick.", "year": 2012}, {"title": "Random Fourier approximations for skewed multiplicative histogram kernels", "author": ["F. Li", "C. Ionescu", "C. Sminchisescu"], "venue": "Pattern Recognition. Springer,", "citeRegEx": "Li et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Li et al\\.", "year": 2010}, {"title": "Robust stochastic approximation approach to stochastic programming", "author": ["A. Nemirovski"], "venue": "SIAM Journal on Optimization", "citeRegEx": "Nemirovski,? \\Q2009\\E", "shortCiteRegEx": "Nemirovski", "year": 2009}, {"title": "Introductory lectures on convex optimization", "author": ["Y. Nesterov"], "venue": "Vol. 87. Springer Science & Business Media", "citeRegEx": "Nes04", "shortCiteRegEx": null, "year": 2004}, {"title": "A method of solving a convex programming problem with convergence rate O (1/k2)", "author": ["Y. Nesterov"], "venue": "Soviet Mathematics Doklady", "citeRegEx": "Nesterov.,? \\Q1983\\E", "shortCiteRegEx": "Nesterov.", "year": 1983}, {"title": "Reading digits in natural images with unsupervised feature learning", "author": ["Y. Netzer"], "venue": "NIPS workshop on deep learning and unsupervised feature learning", "citeRegEx": "Netzer,? \\Q2011\\E", "shortCiteRegEx": "Netzer", "year": 2011}, {"title": "A stochastic approximation method", "author": ["H. Robbins", "S. Monro"], "venue": null, "citeRegEx": "Robbins and Monro.,? \\Q1951\\E", "shortCiteRegEx": "Robbins and Monro.", "year": 1951}, {"title": "Weighted sums of random kitchen sinks: Replacing minimization with randomization in learning", "author": ["A Rahimi", "B Recht"], "venue": "Advances in neural information processing systems", "citeRegEx": "Rahimi and Recht.,? \\Q2008\\E", "shortCiteRegEx": "Rahimi and Recht.", "year": 2008}, {"title": "A stochastic gradient method with an exponential convergence _rate for finite training sets", "author": ["N.L. Roux", "M. Schmidt", "F.R. Bach"], "venue": "Advances in Neural Information Processing Systems", "citeRegEx": "Roux et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Roux et al\\.", "year": 2012}, {"title": "Expected Similarity Estimation for Large Scale Anomaly Detection", "author": ["M. Schneider", "W. Ertel", "G. Palm"], "venue": "In: International Joint Conference on Neural Networks. IEEE,", "citeRegEx": "Schneider et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Schneider et al\\.", "year": 2015}, {"title": "Universality, characteristic kernels and RKHS embedding of measures", "author": ["B.K. Sriperumbudur", "K. Fukumizu", "G.R.G. Lanckriet"], "venue": "The Journal of Machine Learning Research", "citeRegEx": "Sriperumbudur et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Sriperumbudur et al\\.", "year": 2011}, {"title": "A Hilbert space embedding for distributions", "author": ["A.J. Smola"], "venue": "Algorithmic Learning Theory. Springer", "citeRegEx": "Smola,? \\Q2007\\E", "shortCiteRegEx": "Smola", "year": 2007}, {"title": "Pegasos: Primal estimated sub-gradient solver for SVM", "author": ["S. Shalev-Shwartz"], "venue": "Mathematical Programming. Vol. 127", "citeRegEx": "Shalev.Shwartz,? \\Q2011\\E", "shortCiteRegEx": "Shalev.Shwartz", "year": 2011}, {"title": "Stochastic methods for l 1-regularized loss minimization", "author": ["S. Shalev-Shwartz", "A. Tewari"], "venue": "The Journal of Machine Learning Research", "citeRegEx": "Shalev.Shwartz and Tewari.,? \\Q2011\\E", "shortCiteRegEx": "Shalev.Shwartz and Tewari.", "year": 2011}, {"title": "Efficient Additive Kernels via Explicit Feature Maps", "author": ["A. Vedaldi", "A. Zisserman"], "venue": "Pattern Analysis and Machine Intelligence, IEEE Transactions on 34.3", "citeRegEx": "Vedaldi and Zisserman.,? \\Q2012\\E", "shortCiteRegEx": "Vedaldi and Zisserman.", "year": 2012}], "referenceMentions": [{"referenceID": 3, "context": "Other optimization techniques such as projected gradient decent [BV04] or Nesterov\u2019s accelerated gradient descent [Nes83; Nes04] are also applicable in principle, however a single gradient evaluation takes already O(n) time and hence would be slower than the originally proposed EXPoSE approach.", "startOffset": 64, "endOffset": 70}, {"referenceID": 11, "context": "Other optimization techniques such as projected gradient decent [BV04] or Nesterov\u2019s accelerated gradient descent [Nes83; Nes04] are also applicable in principle, however a single gradient evaluation takes already O(n) time and hence would be slower than the originally proposed EXPoSE approach.", "startOffset": 114, "endOffset": 128}], "year": 2015, "abstractText": "A new algorithm named EXPected Similarity Estimation (EXPoSE) was recently proposed to solve the problem of large-scale anomaly detection. It is a non-parametric and distribution free kernel method based on the Hilbert space embedding of probability measures. Given a dataset of n samples, EXPoSE needs only O(n) (linear time) to build a model and O(1) (constant time) to make a prediction. In this work we improve the linear computational complexity and show that an -accurate model can be estimated in constant time, which has significant implications for large-scale learning problems. To achieve this goal, we cast the original EXPoSE formulation into a stochastic optimization problem. It is crucial that this approach allows us to determine the number of iteration based on a desired accuracy , independent of the dataset size n. We will show that the proposed stochastic gradient descent algorithm works in general (possible infinite-dimensional) Hilbert spaces, is easy to implement and requires no additional step-size parameters.", "creator": "LaTeX with hyperref package"}}}