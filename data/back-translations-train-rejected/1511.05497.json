{"id": "1511.05497", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "17-Nov-2015", "title": "Learning Neural Network Architectures using Backpropagation", "abstract": "Deep neural networks with millions of parameters are at the heart of many state of the art machine learning models today. However, recent works have shown that models with much smaller number of parameters can also perform just as well. In this work, we introduce the problem of architecture-learning, i.e; learning the architecture of a neural network along with weights. We introduce a new trainable parameter called tri-state ReLU, which helps in eliminating unnecessary neurons. We also propose a smooth regularizer which encourages the total number of neurons after elimination to be small. The resulting objective is differentiable and simple to optimize. We experimentally validate our method on both small and large networks, and show that it can learn models with a considerably small number of parameters without affecting prediction accuracy.", "histories": [["v1", "Tue, 17 Nov 2015 18:26:11 GMT  (104kb,D)", "http://arxiv.org/abs/1511.05497v1", "ICLR 2016 submission"], ["v2", "Tue, 2 Aug 2016 11:46:48 GMT  (101kb,D)", "http://arxiv.org/abs/1511.05497v2", "BMVC 2016 ; Title modified from 'Learning the Architecture of Deep Neural Networks'"]], "COMMENTS": "ICLR 2016 submission", "reviews": [], "SUBJECTS": "cs.LG cs.CV cs.NE", "authors": ["suraj srinivas", "r venkatesh babu"], "accepted": false, "id": "1511.05497"}, "pdf": {"name": "1511.05497.pdf", "metadata": {"source": "CRF", "title": null, "authors": ["Suraj Srinivas"], "emails": ["{surajsrinivas@ssl.,", "venky@}serc.iisc.in"], "sections": [{"heading": null, "text": "Deep neural networks with millions of parameters are now at the heart of many state-of-the-art machine learning models. However, recent work has shown that models with a much smaller number of parameters can also work just as well. In this work, we present the problem of architecture learning, i.e. learning the architecture of a neural network along with weights. We introduce a new traceable parameter called Tri-State ReLU, which helps eliminate unnecessary neurons. We also propose a smooth regulator that encourages the total number of neurons after elimination to be small, the resulting goal being differentiable and easy to optimize. We validate our method experimentally on both small and large networks and show that it can learn models with a considerably small number of parameters without compromising predictive accuracy."}, {"heading": "1 INTRODUCTION", "text": "Everything should be made as simple as possible, but not easier - Albert EinsteinFor large tasks such as image classification, the general practice has been to train large networks with many millions of parameters (see Krizhevsky et al. (2012); Simonyan & Zisserman (2015); Szegedy et al. (2015). It is natural to ask - are all these parameters really necessary for good performance? In other words, are these models as simple as possible? A smaller model has the advantage of being faster to evaluate and easier to store - both are crucial for real-time and embedded applications. In this paper, we look at the construction of smaller networks that achieve similar performance. Regulators are often used to promote the learning of simpler models, which usually limit the size ('2) or the sparseness (' 1) of individual weights."}, {"heading": "2 COMPLEXITY AS A REGULARIZER", "text": "In general, the term \"architecture\" of a neural network can refer to other aspects of a network than width and depth (such as filter size, increment, etc.), but here we use this word simply to mean width and depth. Given that we want to reduce the complexity of the model, let's formally define our concept of complexity. Notation. Let's use this word to be an endless dimensional vector whose first m components are positive numbers, while the rest are zeros. This represents a m-layered neural network architecture with ni-neurons for the layer. For these vectors, we define an associated standard that corresponds to our notion of the architectural complexity of the neural network. Given that we want to limit the width and depth of this neural network, our notion of complexity is simply the total number of neurons."}, {"heading": "2.1 A STRATEGY FOR A TRAINABLE REGULARIZER", "text": "We need a strategy for automatically selecting the architecture of a neural network, i.e. the width of each layer and the depth of the mesh. One way to select the width of a layer is to introduce additional learnable parameters multiplying by the output of each neuron, as shown on the left in Figure 1. If these new parameters are binary, then these neurons can easily be removed with a zero parameter. In the illustration, we also try to reduce the parameters corresponding to the neurons with the values b and d, thereby negating their contribution. Thus, the sum of these binary detectable parameters corresponds to the effective width of the mesh. In order to further reduce the complexity of the mesh, we also try to define the mesh. It is well known that two neural network layers without any non-linearity between them are equivalent to a single layer, the parameters of which are given by the matrix product of the weight matrices of the original two layers."}, {"heading": "2.1.1 DEFINITION: TRI-STATE RELU", "text": "We define a new tractable nonlinearity, which we call Tri-State ReLU (tsReLU), as follows: tsReLU (x) = {wx, x \u2265 0 wdx, otherwise (2) this will be reduced to the usual ReLU for w = 1 and d = 0. For a fixed w = 1 and a tractable d, this results in a parametric ReLU introduced by He et al. (2015). For us, both w and d are tractable. However, we limit these two parameters to the assumption of binary values. As a result, there are three possible states for this function. For w = 0, this function always returns zero. For w = 1 and d = 0, it behaves similarly to ReLU, while for d = 1 it is reduced to the identity function. Here, parameter w selects the width of the layer, while d determines the depth. While the w parameter across channels is different, the d parameter is bound to the same value."}, {"heading": "2.1.2 LEARNING BINARY PARAMETERS", "text": "Given the above definition of Tri-State RelU (tsReLU), we need a method to use binary parameters for w and d. for this purpose, we use the regularizer specified by w (1 \u2212 w) (or d (1 \u2212 d)), as used by Murray & Ng (2010). This is a smooth regularizer - which is helpful for descent procedures. This regularizer recommends binary values for parameters when forced to lie in [0, 1]. \u2212 0.5 0 0.5 0 0.5 1 00.050.10.150.20.25xyy = x2 y = x (1 \u2212 x) With this intuition, we now specify our tsReLU optimization depth when in [0, 1] in [0, wij, di: i, j] (y: i, w, d), y: 1 m \u00b2, i = 1, wij = 1 wij (1 \u2212 wij)."}, {"heading": "2.2 ADDING MODEL COMPLEXITY", "text": "In the sections above, we considered the problem of forming binary tsReLU parameters without limiting the complexity of the model. Consequently, the objective function described above is not necessarily selected for smaller models. Let's formulate the complexity of a layer in such a way that for di = 0 the complexity in a layer is simply hi, while for di = 1 (non-linearity absent) the complexity is 0. Overall, it counts the total number of non-zero neurons in the model in convergence. We will now add a regulator analogous to the complexity of the model (defined above) in our optimization goal in Equation 4. Let's call the regulator corresponding to the complexity of the model as Rm (h, d), which differentiates Rm (h, d) = him \u00b2."}, {"heading": "2.3 DIFFERENT LEARNING SCHEMES", "text": "The regulators described above have roughly minimized the complexity of the neural network. However, we used a fairly simple definition of complexity that corresponded to the sum of all the neurons in a neural network. What happens when we use different notions of complexity in the regulator? Here, we look at two such variations with interesting properties. In both cases, we first let the neurons learn with a larger architecture and then slowly discard them in order to obtain a smaller model. To encourage this behavior, we can use n \u2032 i: = ni \u2212 nxi, where x < 1. We can analyze the gradient of the defined complexity to identify their behavior, which makes the complexity sublinear and promotes the behavior mentioned above. \u2022 Fixed Final Width: We sometimes want tighter control over the final architecture to recognize the behavior."}, {"heading": "3 PROPERTIES OF THE METHOD", "text": "Now that we have a method of determining the neural network architecture along with the weights, we expect certain properties to be kept natural, and while this list is certainly not exhaustive, it presents some desirable traits. 1. Non-redundancy of architecture: The final architecture learned must not have redundant neurons.3. The removal of neurons should necessarily lead to a degradation of performance.2. Local optimization of weights: The performance of the final architecture learned must be at least equal to an learned neural network initialized with that final architecture.3. Mirroring data complexity: A \"harder\" dataset should inevitably lead to a larger model than a \"simpler\" dataset. We intuitively observe that all these properties are automatically held when a \"masterful\" property that keeps both the architecture and the weights optimal. In such a case, any smaller architecture with a set of possible weights is necessary."}, {"heading": "4 RELATED WORK", "text": "There are a number of reasons why people who work for equality between men and women are unable to decide for themselves."}, {"heading": "5 EXPERIMENTS", "text": "In this section, we conduct experiments to analyze the behavior of our method. In the first group, we evaluate the performance of the MNIST dataset. Later, we consider a case study on the ILSVRC 2012 dataset. All of our experiments are conducted using the Caffe Deep Learning Framework (see Jia et al. (2014))."}, {"heading": "5.1 COMPRESSION PERFORMANCE", "text": "We evaluate our method based on the MNIST dataset using a LeNet-like (see LeCun et al. (1998)) architecture. The network consists of two 5 x 5 convolutionary layers with 20 and 50 filters and two fully connected layers with 500 and 10 (output layer) neurons. Although there is nothing special about this architecture, we simply use it as a starting point to learn smaller architectures. Further details of the experimental setup can be found in the appendix. We perform our Architecture Learning Method (AL) with linear complexity and sublinear complexity measures. We also compare it with baseline pooling, which is applied immediately after each of the revolutionary layers, which excludes depth selection for these two layers."}, {"heading": "5.2 ANALYSIS", "text": "We now conduct a few more experiments to further analyze the behavior of our method. In all cases, we train \"AL-line2\" -like models and consider the third level for evaluation. We start with the basic architecture mentioned above. First, we look at the effects of using different complexity measures. Specifically, we look at the learning dynamics of the AL-linear and AL-sublinear methods. From Figure 4a, we find that the sublinear method explores intermediate architectures over a longer period of time and therefore looks much smoother. In addition, we observe that the convergence of architectures occurs quite early in both methods (\u0445 10k iterations) - leaving the usual weight learning for the remaining iterations. Using a smaller xi 1 (blue curve) is another way to slow down architectural learning by delaying convergence. Second, we look at the learned architectures for different amounts of data complexity, such as in each property we get the number 3."}, {"heading": "5.3 CASE STUDY: ALEXNET", "text": "For the following experiments, we use an AlexNet-like (see Krizhevsky et al. (2012) model called CaffeNet, which is equipped with the Caffe Deep Learning Framework. It is very similar to AlexNet, except that the sequence of max pooling and normalization has been reversed. We use the ILSVRC 2012 (see Russakovsky et al. (2015) validation set to calculate accuracy in Table 3. Unlike the previous experiments, we start with a pre-trained model and then perform architectural learning (AL) on it. We see that our method does not work as well as the state-of-the-art compression methods. This indicates that fully connected layers can actually be over-parameterized and that clever approaches to weight repair metrization (see Yang et al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al. (2014))) are better when it comes to compression performance."}, {"heading": "6 CONCLUSIONS", "text": "We proposed a method to learn the architecture of a neural network along with weights. Instead of directly selecting width and depth, we introduced real hyperparameters that selected width and depth for us. We also saw that we get smaller architectures for MNIST and ImageNet datasets that work on an equal footing with the large architectures. Our method is very simple and straightforward and can be applied to any neural network. It can also be used as a tool to further explore the architecture's dependence on the optimization and convergence of neural networks."}, {"heading": "APPENDIX A", "text": "PROOFS OF PROPOSITIONSLetE = '+ \u03bbbRb + \u03bbmRm is the total objective function, where Rb is the binary regularizer, Rm = \u0445 \u03c6 is the model term of complexity. In the case of convergence, we assume that Rb = 0 as the corresponding weights are all binary or close to the binary. Proof of of Proposal 1. In the case of convergence, we assume that it is a sufficiently small proof of Proposition 2. Let us leave q = \"2 \u0432.\" Also, let us deny b = ED (q) + k\u03c3 where such damage exists. < \u2212 \u03bbm + = \u21d2 has been anticipated."}, {"heading": "APPENDIX B", "text": "EXPERIENCE DETAILSMNIST EXPERIENCES For the training of the network, we used the ADAM Optimizer (see Kingma & Ba (2014) with a learning rate of 0.001, a pulse of 0.9 and a pulse of 0.99 for a total of 30,000 iterations. Dropouts were not used during the training of these networks. For the base model, each layer is followed by a ReLU nonlinearity. For the AL sublinear experiments, we used a fixed learning rate of 10 \u2212 4 for weights and 10 \u2212 5 for architecture with a pulse of 0.9, over 50,000 iterations. The resulting weights were fine-tuned for 10,000 iterations while the architecture was specified."}], "references": [{"title": "Do deep nets really need to be deep", "author": ["Ba", "Jimmy", "Caruana", "Rich"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Ba et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Ba et al\\.", "year": 2014}, {"title": "Memory bounded deep convolutional networks", "author": ["Collins", "Maxwell D", "Kohli", "Pushmeet"], "venue": "CoRR, abs/1412.1442,", "citeRegEx": "Collins et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Collins et al\\.", "year": 2014}, {"title": "Predicting parameters in deep learning", "author": ["Denil", "Misha", "Shakibi", "Babak", "Dinh", "Laurent", "de Freitas", "Nando"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Denil et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Denil et al\\.", "year": 2013}, {"title": "Exploiting linear structure within convolutional networks for efficient evaluation", "author": ["Denton", "Emily L", "Zaremba", "Wojciech", "Bruna", "Joan", "LeCun", "Yann", "Fergus", "Rob"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Denton et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Denton et al\\.", "year": 2014}, {"title": "The cascade-correlation learning architecture", "author": ["Fahlman", "Scott E", "Lebiere", "Christian"], "venue": null, "citeRegEx": "Fahlman et al\\.,? \\Q1989\\E", "shortCiteRegEx": "Fahlman et al\\.", "year": 1989}, {"title": "Compressing deep convolutional networks using vector quantization", "author": ["Gong", "Yunchao", "Liu", "Yang", "Ming", "Bourdev", "Lubomir"], "venue": "arXiv preprint arXiv:1412.6115,", "citeRegEx": "Gong et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Gong et al\\.", "year": 2014}, {"title": "Learning both weights and connections for efficient neural networks", "author": ["Han", "Song", "Pool", "Jeff", "Tran", "John", "Dally", "William J"], "venue": "arXiv preprint arXiv:1506.02626,", "citeRegEx": "Han et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Han et al\\.", "year": 2015}, {"title": "Second order derivatives for network pruning: Optimal brain surgeon", "author": ["Hassibi", "Babak", "Stork", "David G"], "venue": "Advances in Neural Information Processing Systems,", "citeRegEx": "Hassibi et al\\.,? \\Q1993\\E", "shortCiteRegEx": "Hassibi et al\\.", "year": 1993}, {"title": "Delving deep into rectifiers: Surpassing human-level performance on imagenet classification", "author": ["He", "Kaiming", "Zhang", "Xiangyu", "Ren", "Shaoqing", "Sun", "Jian"], "venue": "arXiv preprint arXiv:1502.01852,", "citeRegEx": "He et al\\.,? \\Q2015\\E", "shortCiteRegEx": "He et al\\.", "year": 2015}, {"title": "Distilling the knowledge in a neural network", "author": ["Hinton", "Geoffrey E", "Vinyals", "Oriol", "Dean", "Jeff"], "venue": "In NIPS 2014 Deep Learning Workshop,", "citeRegEx": "Hinton et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Hinton et al\\.", "year": 2014}, {"title": "Complexity measures of supervised classification problems", "author": ["Ho", "Tin Kamo", "Basu", "Mitra"], "venue": "Pattern Analysis and Machine Intelligence, IEEE Transactions on,", "citeRegEx": "Ho et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Ho et al\\.", "year": 2002}, {"title": "Speeding up convolutional neural networks with low rank expansions", "author": ["Jaderberg", "Max", "Vedaldi", "Andrea", "Zisserman", "Andrew"], "venue": "In Proceedings of the British Machine Vision Conference. BMVA Press,", "citeRegEx": "Jaderberg et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Jaderberg et al\\.", "year": 2014}, {"title": "Caffe: Convolutional architecture for fast feature embedding", "author": ["Jia", "Yangqing", "Shelhamer", "Evan", "Donahue", "Jeff", "Karayev", "Sergey", "Long", "Jonathan", "Girshick", "Ross", "Guadarrama", "Sergio", "Darrell", "Trevor"], "venue": "arXiv preprint arXiv:1408.5093,", "citeRegEx": "Jia et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Jia et al\\.", "year": 2014}, {"title": "Adam: A method for stochastic optimization", "author": ["Kingma", "Diederik", "Ba", "Jimmy"], "venue": "arXiv preprint arXiv:1412.6980,", "citeRegEx": "Kingma et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Kingma et al\\.", "year": 2014}, {"title": "Imagenet classification with deep convolutional neural networks", "author": ["Krizhevsky", "Alex", "Sutskever", "Ilya", "Hinton", "Geoffrey E"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Krizhevsky et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Krizhevsky et al\\.", "year": 2012}, {"title": "Optimal brain damage", "author": ["LeCun", "Yann", "Denker", "John S", "Solla", "Sara A", "Howard", "Richard E", "Jackel", "Lawrence D"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "LeCun et al\\.,? \\Q1989\\E", "shortCiteRegEx": "LeCun et al\\.", "year": 1989}, {"title": "Gradient-based learning applied to document recognition", "author": ["LeCun", "Yann", "Bottou", "L\u00e9on", "Bengio", "Yoshua", "Haffner", "Patrick"], "venue": "Proceedings of the IEEE,", "citeRegEx": "LeCun et al\\.,? \\Q1998\\E", "shortCiteRegEx": "LeCun et al\\.", "year": 1998}, {"title": "Fully convolutional networks for semantic segmentation", "author": ["Long", "Jonathan", "Shelhamer", "Evan", "Darrell", "Trevor"], "venue": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,", "citeRegEx": "Long et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Long et al\\.", "year": 2015}, {"title": "An algorithm for nonlinear optimization problems with binary variables", "author": ["Murray", "Walter", "Ng", "Kien-Ming"], "venue": "Computational Optimization and Applications,", "citeRegEx": "Murray et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Murray et al\\.", "year": 2010}, {"title": "Fitnets: Hints for thin deep nets", "author": ["Romero", "Adriana", "Ballas", "Nicolas", "Kahou", "Samira Ebrahimi", "Chassang", "Antoine", "Gatta", "Carlo", "Bengio", "Yoshua"], "venue": "arXiv preprint arXiv:1412.6550,", "citeRegEx": "Romero et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Romero et al\\.", "year": 2014}, {"title": "ImageNet Large Scale Visual Recognition Challenge", "author": ["Russakovsky", "Olga", "Deng", "Jia", "Su", "Hao", "Krause", "Jonathan", "Satheesh", "Sanjeev", "Ma", "Sean", "Huang", "Zhiheng", "Karpathy", "Andrej", "Khosla", "Aditya", "Bernstein", "Michael", "Berg", "Alexander C", "Fei-Fei", "Li"], "venue": "International Journal of Computer Vision (IJCV),", "citeRegEx": "Russakovsky et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Russakovsky et al\\.", "year": 2015}, {"title": "Very deep convolutional networks for large-scale image recognition", "author": ["K. Simonyan", "A. Zisserman"], "venue": "In International Conference on Learning Representations,", "citeRegEx": "Simonyan and Zisserman,? \\Q2015\\E", "shortCiteRegEx": "Simonyan and Zisserman", "year": 2015}, {"title": "Practical bayesian optimization of machine learning algorithms", "author": ["Snoek", "Jasper", "Larochelle", "Hugo", "Adams", "Ryan P"], "venue": "In Advances in neural information processing systems,", "citeRegEx": "Snoek et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Snoek et al\\.", "year": 2012}, {"title": "Data-free parameter pruning for deep neural networks", "author": ["Srinivas", "Suraj", "Babu", "R. Venkatesh"], "venue": "Proceedings of the British Machine Vision Conference (BMVC),", "citeRegEx": "Srinivas et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Srinivas et al\\.", "year": 2015}, {"title": "Going deeper with convolutions", "author": ["Szegedy", "Christian", "Liu", "Wei", "Jia", "Yangqing", "Sermanet", "Pierre", "Reed", "Scott", "Anguelov", "Dragomir", "Erhan", "Dumitru", "Vanhoucke", "Vincent", "Rabinovich", "Andrew"], "venue": "In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR),", "citeRegEx": "Szegedy et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Szegedy et al\\.", "year": 2015}, {"title": "Deep fried convnets", "author": ["Yang", "Zichao", "Moczulski", "Marcin", "Denil", "Misha", "de Freitas", "Nando", "Smola", "Alex", "Song", "Le", "Wang", "Ziyu"], "venue": "arXiv preprint arXiv:1412.7149,", "citeRegEx": "Yang et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Yang et al\\.", "year": 2014}], "referenceMentions": [{"referenceID": 14, "context": "For large-scale tasks like image classification, the general practice has been to train large networks with many millions of parameters (see Krizhevsky et al. (2012); Simonyan & Zisserman (2015); Szegedy et al.", "startOffset": 141, "endOffset": 166}, {"referenceID": 14, "context": "For large-scale tasks like image classification, the general practice has been to train large networks with many millions of parameters (see Krizhevsky et al. (2012); Simonyan & Zisserman (2015); Szegedy et al.", "startOffset": 141, "endOffset": 195}, {"referenceID": 14, "context": "For large-scale tasks like image classification, the general practice has been to train large networks with many millions of parameters (see Krizhevsky et al. (2012); Simonyan & Zisserman (2015); Szegedy et al. (2015)).", "startOffset": 141, "endOffset": 218}, {"referenceID": 8, "context": "For a fixed w = 1 and a trainable d, this turns into parametric ReLU introduced by He et al. (2015). For us, both w and d are trainable.", "startOffset": 83, "endOffset": 100}, {"referenceID": 13, "context": "Weightpruning techniques were popularized by LeCun et al. (1989) and Hassibi et al.", "startOffset": 45, "endOffset": 65}, {"referenceID": 6, "context": "(1989) and Hassibi et al. (1993), who introduced Optimal Brain Damage and Optimal Brain Surgery respectively.", "startOffset": 11, "endOffset": 33}, {"referenceID": 6, "context": "(1989) and Hassibi et al. (1993), who introduced Optimal Brain Damage and Optimal Brain Surgery respectively. Recently, Srinivas & Babu (2015) proposed a neuron pruning technique which scaled better than these weight pruning techniques.", "startOffset": 11, "endOffset": 143}, {"referenceID": 6, "context": "The learning objective can thus be seen as performing pruning and learning together, unlike the work of Han et al. (2015), who perform both operations alternately.", "startOffset": 104, "endOffset": 122}, {"referenceID": 6, "context": "The learning objective can thus be seen as performing pruning and learning together, unlike the work of Han et al. (2015), who perform both operations alternately. Learning neural network architecture has also been explored to some extent. The Cascadecorrelation algorithm by Fahlman & Lebiere (1989) proposed a novel learning rule to \u2018grow\u2019 the neural network.", "startOffset": 104, "endOffset": 301}, {"referenceID": 6, "context": "The learning objective can thus be seen as performing pruning and learning together, unlike the work of Han et al. (2015), who perform both operations alternately. Learning neural network architecture has also been explored to some extent. The Cascadecorrelation algorithm by Fahlman & Lebiere (1989) proposed a novel learning rule to \u2018grow\u2019 the neural network. However, it was shown for only a single layer network and is hence not clear how to scale to large deep networks. Our work is inspired from the recent work of Kulkarni et al. (2015) who proposed to learn the width of neural networks in a way similar to ours.", "startOffset": 104, "endOffset": 544}, {"referenceID": 6, "context": "The learning objective can thus be seen as performing pruning and learning together, unlike the work of Han et al. (2015), who perform both operations alternately. Learning neural network architecture has also been explored to some extent. The Cascadecorrelation algorithm by Fahlman & Lebiere (1989) proposed a novel learning rule to \u2018grow\u2019 the neural network. However, it was shown for only a single layer network and is hence not clear how to scale to large deep networks. Our work is inspired from the recent work of Kulkarni et al. (2015) who proposed to learn the width of neural networks in a way similar to ours. Specifically, they proposed to learn a diagonal matrix D along with neurons Wx, such that DWx represents that layer\u2019s neurons. However, instead of imposing a binary constraint (like ours), they learn realweights and impose an `1-based sparsity-inducing regularizer on D to encourage zeros. By imposing a binary constraint, we are able to directly regularize for the model complexity. Recently, Bayesian Optimization-based algorithms (see Snoek et al. (2012)) have also been proposed for automatically learning hyper-parameters of neural networks.", "startOffset": 104, "endOffset": 1079}, {"referenceID": 6, "context": "Knowledge Distillation (KD) by Hinton et al. (2014) is a more general approach, of which Bucila et al.", "startOffset": 31, "endOffset": 52}, {"referenceID": 6, "context": "Knowledge Distillation (KD) by Hinton et al. (2014) is a more general approach, of which Bucila et al.\u2019s is a special case. Our method of learning with a fixed final architecture is reminiscent of KD. However, the formulations seem to be very different. FitNets by Romero et al. (2014) use a KD-like method at several layers to learn networks which are deeper but thinner (in contrast to Ba and Caruna\u2019s shallow and wide), and achieve high levels of compression on trained models.", "startOffset": 31, "endOffset": 286}, {"referenceID": 6, "context": "Knowledge Distillation (KD) by Hinton et al. (2014) is a more general approach, of which Bucila et al.\u2019s is a special case. Our method of learning with a fixed final architecture is reminiscent of KD. However, the formulations seem to be very different. FitNets by Romero et al. (2014) use a KD-like method at several layers to learn networks which are deeper but thinner (in contrast to Ba and Caruna\u2019s shallow and wide), and achieve high levels of compression on trained models. In contrast, our method can only make networks shallower, not deeper. However, we note that it is possible to modify our method to enable such learning. Many methods have been proposed to train models that are deep, yet have a lower parameterisation than conventional networks. Collins & Kohli (2014) propose a sparsity inducing regulariser for backpropogation which promotes many weights to have zero magnitude.", "startOffset": 31, "endOffset": 782}, {"referenceID": 2, "context": "Denil et al. (2013) demonstrate that most of the parameters of a model can be predicted given only a few parameters.", "startOffset": 0, "endOffset": 20}, {"referenceID": 2, "context": "Denil et al. (2013) demonstrate that most of the parameters of a model can be predicted given only a few parameters. At training time, they learn only a few parameters and predict the rest. Yang et al. (2014) propose an Adaptive Fastfood transform, which is an efficient re-parametrization of fully-connected layer weights.", "startOffset": 0, "endOffset": 209}, {"referenceID": 2, "context": "Denil et al. (2013) demonstrate that most of the parameters of a model can be predicted given only a few parameters. At training time, they learn only a few parameters and predict the rest. Yang et al. (2014) propose an Adaptive Fastfood transform, which is an efficient re-parametrization of fully-connected layer weights. This results in a reduction of complexity for weight storage and computation. Some recent works have also focussed on using approximations of weight matrices to perform compression. Jaderberg et al. (2014) and Denton et al.", "startOffset": 0, "endOffset": 530}, {"referenceID": 2, "context": "Denil et al. (2013) demonstrate that most of the parameters of a model can be predicted given only a few parameters. At training time, they learn only a few parameters and predict the rest. Yang et al. (2014) propose an Adaptive Fastfood transform, which is an efficient re-parametrization of fully-connected layer weights. This results in a reduction of complexity for weight storage and computation. Some recent works have also focussed on using approximations of weight matrices to perform compression. Jaderberg et al. (2014) and Denton et al. (2014) use SVD-based low rank approximations of the weight matrix.", "startOffset": 0, "endOffset": 555}, {"referenceID": 2, "context": "Denil et al. (2013) demonstrate that most of the parameters of a model can be predicted given only a few parameters. At training time, they learn only a few parameters and predict the rest. Yang et al. (2014) propose an Adaptive Fastfood transform, which is an efficient re-parametrization of fully-connected layer weights. This results in a reduction of complexity for weight storage and computation. Some recent works have also focussed on using approximations of weight matrices to perform compression. Jaderberg et al. (2014) and Denton et al. (2014) use SVD-based low rank approximations of the weight matrix. Gong et al. (2014) use a clustering-based product quantization approach to build an indexing scheme that reduces the space occupied by the matrix on disk.", "startOffset": 0, "endOffset": 634}, {"referenceID": 12, "context": "All our experiments are performed using the Caffe Deep learning framework (see Jia et al. (2014)).", "startOffset": 79, "endOffset": 97}, {"referenceID": 15, "context": "We evaluate our method on the MNIST dataset, using a LeNet-like (see LeCun et al. (1998)) architecture.", "startOffset": 69, "endOffset": 89}, {"referenceID": 14, "context": "3 CASE STUDY: ALEXNET For the experiments that follow, we use an AlexNet-like (see Krizhevsky et al. (2012)) model, called CaffeNet, provided with the Caffe Deep Learning framework.", "startOffset": 83, "endOffset": 108}, {"referenceID": 14, "context": "3 CASE STUDY: ALEXNET For the experiments that follow, we use an AlexNet-like (see Krizhevsky et al. (2012)) model, called CaffeNet, provided with the Caffe Deep Learning framework. It is very similar to AlexNet, except that the order of max-pooling and normalization have been interchanged. We use the ILSVRC 2012 (see Russakovsky et al. (2015)) validation set to compute accuracies in the Table 3.", "startOffset": 83, "endOffset": 346}, {"referenceID": 14, "context": "3 CASE STUDY: ALEXNET For the experiments that follow, we use an AlexNet-like (see Krizhevsky et al. (2012)) model, called CaffeNet, provided with the Caffe Deep Learning framework. It is very similar to AlexNet, except that the order of max-pooling and normalization have been interchanged. We use the ILSVRC 2012 (see Russakovsky et al. (2015)) validation set to compute accuracies in the Table 3. Unlike the experiments done previously, we start with a pre-trained model and then perform architecture learning (AL) on it. We see that our method does not perform as well as the state of the art compression methods. This points to the fact that fully-connected layers may indeed be over-parametrized and that clever approaches of weight re-parametrization (see Yang et al. (2014)) do better when it comes to compression performance.", "startOffset": 83, "endOffset": 782}, {"referenceID": 24, "context": "Further, state-of-the-art image classification networks (see Szegedy et al. (2015)) are increasingly deep with customized layer definitions (Eg: Inception module).", "startOffset": 61, "endOffset": 83}, {"referenceID": 25, "context": "60 35 SVD-quarter-F (Yang et al. (2014)) 25.", "startOffset": 21, "endOffset": 40}, {"referenceID": 25, "context": "60 35 SVD-quarter-F (Yang et al. (2014)) 25.6M 56.19 58 Adaptive FastFood 32 (Yang et al. (2014)) 22.", "startOffset": 21, "endOffset": 97}], "year": 2015, "abstractText": "Deep neural networks with millions of parameters are at the heart of many state of the art machine learning models today. However, recent works have shown that models with much smaller number of parameters can also perform just as well. In this work, we introduce the problem of architecture-learning, i.e; learning the architecture of a neural network along with weights. We introduce a new trainable parameter called tri-state ReLU, which helps in eliminating unnecessary neurons. We also propose a smooth regularizer which encourages the total number of neurons after elimination to be small. The resulting objective is differentiable and simple to optimize. We experimentally validate our method on both small and large networks, and show that it can learn models with a considerably small number of parameters without affecting prediction accuracy.", "creator": "LaTeX with hyperref package"}}}