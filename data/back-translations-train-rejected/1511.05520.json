{"id": "1511.05520", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "17-Nov-2015", "title": "Automatic Instrument Recognition in Polyphonic Music Using Convolutional Neural Networks", "abstract": "Traditional methods to tackle many music information retrieval tasks typically follow a two-step architecture: feature engineering followed by a simple learning algorithm. In these \"shallow\" architectures, feature engineering and learning are typically disjoint and unrelated. Additionally, feature engineering is difficult, and typically depends on extensive domain expertise.", "histories": [["v1", "Tue, 17 Nov 2015 19:43:53 GMT  (2068kb,D)", "http://arxiv.org/abs/1511.05520v1", null]], "reviews": [], "SUBJECTS": "cs.SD cs.IR cs.LG cs.NE", "authors": ["peter li", "jiyuan qian", "tian wang"], "accepted": false, "id": "1511.05520"}, "pdf": {"name": "1511.05520.pdf", "metadata": {"source": "CRF", "title": "AUTOMATIC INSTRUMENT RECOGNITION IN POLYPHONIC MUSIC USING CONVOLUTIONAL NEURAL NETWORKS", "authors": ["Peter Li", "Jiyuan Qian", "Tian Wang"], "emails": [], "sections": [{"heading": null, "text": "In this article, we present an application of Convolutionary Neural Networks for the automatic identification of musical instruments. In this model, feature extraction and learning algorithms are trained together in an end-to-end manner. We show that a Convolutionary Neural Network trained on Raw Audio can achieve a performance that surpasses traditional methods based on handcrafted characteristics."}, {"heading": "1. INTRODUCTION", "text": "Computer Audition is the general study of the systems and methods necessary for the understanding of audio by a machine. In a way, Computer Audition is concerned with the development of computers that can \"hear\" like humans. The goal is a machine that can \"organize what they hear; learn names for recognizable objects, actions, events, places, musical styles, instruments and speakers, and retrieve sounds from those names.\" [1] In this essay, we focus on the first two tasks. How can we train a system to identify existing instruments in the face of a recording of music? We present an application of deep learning for the task of automatic musical instrument identification in polyphonic music. We show that an end system that uses raw audio-trained conventional neural networks can surpass traditional MIR models trained on technical characteristics."}, {"heading": "1.1. Relation to Previous Work", "text": "In an essay calling for the introduction of deep architectures into MIR, [2] we describe traditional MIR methods as follows: The traditional approaches to these problems are fairly homogeneous and use a two-tier architecture of feature extraction and semantic interpretation, e.g. classification, regression, clustering, similarity ranking, etc. Feature representations are predominantly handmade and rely on significant expertise from music theory or psychoacoustics. Since good features are difficult to create, much of the recent research in the MIR community has focused on the semantic interpretation part of the problem, i.e. the formation of better models using a range of standard audio features (e.g. MelFrequency Cepstral Coefficient or chroma) [2]. In this essay, we deviate from the traditional MIR approach. With the help of Convolutionary Neural Networks, we train a model that uses raw audio as input. We use a deep architecture that can both be interpreted as well as reductive."}, {"heading": "2. PROBLEM DEFINITION", "text": "Before going into the details of our work, we give a formal definition of our task: Given a section of audio x, we would like to predict a vector y = 0.1 l, where l is the total number of instruments andyi = {1 if instrument i is in x 0, otherwise we treat this as a problem with multiple labels, where Xiv: 151 1.05 520v 1 [cs.S D] 17 Nov 201 5each label corresponds to an instrument. A model should output 1 if an instrument is present and 0 otherwise."}, {"heading": "3. MODEL", "text": "In general, they are known to be good at extracting high-level properties that represent abstract concepts from the original data. A typical model contains several layers of modules in which each module performs a simple data transformation. In Figure 1, we show a few common operations on a general matrix input. In a folding layer, a filter whose weights are learned is connected to its input by multiplying points and then adding up the results. Pooling is a down-sampling operation that combines nearby points. Nonlinearity is applied point by point. In Figure 2, we show a sample model architecture. Each of the first two layers contains a folding, pooling and nonlinearity. The last two layers are a fully connected neural network."}, {"heading": "4. EXPERIMENTS", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.1. Data and Setup", "text": "We trained and evaluated our model with data from MedleDB [7]. MedleyDB is a multitrack dataset of 122 annotated music recordings. For each song, we have three types of audio content: mix, strains and raw audio. Mixed audio consists of a set of strains and strains that consist of a mixture of raw materials. Since MedleyDB was created primarily to support research into melody extractions, we had to create our own labels for model training. In the following sections, we outline our approach. For each strain, we have attributes that represent confidence in the instrument whether the instrument is active or not."}, {"heading": "4.2. CNN Training", "text": "To accelerate training time, we transformed input using global contrast normalization. This is a standard pre-processing step that has been shown to accelerate training time. [9]"}, {"heading": "4.3. Benchmarks", "text": "For comparison, we also performed tests using more traditional MIR techniques. In the benchmarks, we use domain knowledge to construct music-related characteristics. For each audio clip, we first calculated the Mel frequency Cepstral coefficients (MFCC) together with the differences of first and second order, MFCC \u0445 and \u0445 2. These three matrices were stacked and modeled using a Gaussian distribution [10]. MFCCMFCC \u0445 MFCC \u0445 2 = | | m1.... mT |, with mi-N (\u00b5, \u03a3) (\u00b5, \u03a3) used as characteristics to train a random forest and logistical regression."}, {"heading": "4.4. Experiment Results", "text": "The results of our experiments are shown in Table 2. In addition to random forest and logistical regression, we also offer a na\u00efve benchmark that always predicts the three most common instruments in the training set. As you can see, the Audio + CNN model generally performs better than the basic models."}, {"heading": "5. DISCUSSION", "text": "In this section, we will attempt to examine the weights learned in the first layer. This section follows the procedures of previous work in training CNN's for audio waveforms and confirms previous results. Looking at the filter weights learned in the first layer, we see that the model appears to learn a series of frequency-selective filters. In Figure 4, we will record the size spectra of each filter sorted by dominant frequency. Similar to the results in [4] and [3], the first layer appears to learn an auditory filter.In Figure 5, we will show a sample of filters learned in the first layer. As in [4], many of the filters learned are translated versions of each other. This is not surprising, as phase invariance is most likely difficult to learn given our architecture."}, {"heading": "6. CONCLUSION AND FUTURE WORK", "text": "We present a revolutionary neural network for instrument identification. We show that an end-to-end deep learning system can be trained to achieve performance in line with (and sometimes over) traditional methods based on extensive domain knowledge. In future work, we will explore methods to further understand the transformations of CNN. In addition, we will explore various architectures that may be able to learn phase and translation invariance.CodeCode for this project can be found at https: / / github.com / glennq / instrument-recognition."}, {"heading": "7. REFERENCES", "text": "[1] Richard F Lyon, \"Machine hearing: An emerging field [exploratory dsp],\" Signal Processing Magazine, IEEE, vol. 27, no. 5, pp. 131-139, 2010. [2] Eric J Humphrey, Juan Pablo Bello, and Yann LeCun \"Moving beyond feature design: Deep architectures and automatic feature learning in music informatics.,\" in: ISMIR. Citeseer, 2012, pp. 403-408. [3] Y. Hoshen, R. J. Weiss, and K. W. Wilson, \"Speech Acoustic Modeling from Raw Multichannel Waveforms,\" in: Proc. IEEE International Conference on Acoustics, Speech, and Signal Processing (ICASSP), Brisbane, Australia, Apr. 2015. [4] Sander Dieleman and Benjamin Schrauwen, \"End-toend learning for music audio,\" in: Acoustics, Speech and Signal Processing (ICSP)."}], "references": [{"title": "Machine hearing: An emerging field [exploratory dsp", "author": ["Richard F Lyon"], "venue": "Signal Processing Magazine, IEEE, vol. 27, no. 5, pp. 131\u2013139, 2010.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2010}, {"title": "Moving beyond feature design: Deep architectures and automatic feature learning in music informatics", "author": ["Eric J Humphrey", "Juan Pablo Bello", "Yann LeCun"], "venue": "IS- MIR. Citeseer, 2012, pp. 403\u2013408.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2012}, {"title": "Speech Acoustic Modeling from Raw Multichannel Waveforms", "author": ["Y. Hoshen", "R.J. Weiss", "K.W. Wilson"], "venue": "Proc. IEEE International Conference on Acoustics, Speech, and Signal Processing (ICASSP), Brisbane, Australia, Apr. 2015.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2015}, {"title": "End-toend learning for music audio", "author": ["Sander Dieleman", "Benjamin Schrauwen"], "venue": "Acoustics, Speech and Signal Processing (ICASSP), 2014 IEEE International Conference on. IEEE, 2014, pp. 6964\u20136968.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2014}, {"title": "Gradient-based learning applied to document recognition", "author": ["Yann LeCun", "L\u00e9on Bottou", "Yoshua Bengio", "Patrick Haffner"], "venue": "Proceedings of the IEEE, vol. 86, no. 11, pp. 2278\u20132324, 1998.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 1998}, {"title": "Improving neural networks by preventing co-adaptation of feature detectors", "author": ["Geoffrey E Hinton", "Nitish Srivastava", "Alex Krizhevsky", "Ilya Sutskever", "Ruslan R Salakhutdinov"], "venue": "arXiv preprint arXiv:1207.0580, 2012.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2012}, {"title": "Medleydb: a multitrack dataset for annotation-intensive mir research", "author": ["Rachel Bittner", "Justin Salamon", "Mike Tierney", "Matthias Mauch", "Chris Cannam", "Juan Bello"], "venue": "15th International Society for Music Information Retrieval Conference (ISMIR 2014), 2014, pp. 155\u2013160.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2014}, {"title": "On the stratification of multi-label data", "author": ["Konstantinos Sechidis", "Grigorios Tsoumakas", "Ioannis Vlahavas"], "venue": "Machine Learning and Knowledge Discovery in Databases, pp. 145\u2013158. Springer, 2011.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2011}, {"title": "Efficient backprop", "author": ["Yann A LeCun", "L\u00e9on Bottou", "Genevieve B Orr", "Klaus-Robert M\u00fcller"], "venue": "Neural networks: Tricks of the trade, pp. 9\u201348. Springer, 2012.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2012}, {"title": "Mel frequency cepstral coefficients for music modeling", "author": ["Beth Logan"], "venue": "ISMIR, 2000.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2000}], "referenceMentions": [{"referenceID": 0, "context": "\u201d [1] In this paper, we focus on the first two tasks.", "startOffset": 2, "endOffset": 5}, {"referenceID": 1, "context": "In a paper calling for the adoption of deep architectures in MIR,[2] describe traditional MIR methods as follows:", "startOffset": 65, "endOffset": 68}, {"referenceID": 1, "context": "MelFrequency Cepstral Coefficients or chroma)[2].", "startOffset": 45, "endOffset": 48}, {"referenceID": 2, "context": ", [3] and musical audio tagging [4].", "startOffset": 2, "endOffset": 5}, {"referenceID": 3, "context": ", [3] and musical audio tagging [4].", "startOffset": 32, "endOffset": 35}, {"referenceID": 4, "context": "Convolutional Neural Networks (CNN) [5] can be seen as a trainable feature extractor coupled with a learning model.", "startOffset": 36, "endOffset": 39}, {"referenceID": 5, "context": "The choice of activation functions usually include the sigmoid function, hyperbolic tangent function and more recently Rectified Linear Unit (ReLU)[6].", "startOffset": 147, "endOffset": 150}, {"referenceID": 0, "context": "This gives us a 11\u00d7 1 vector \u0177 where each \u0177i \u2208 [0, 1].", "startOffset": 47, "endOffset": 53}, {"referenceID": 6, "context": "We trained and evaluated our model using data from MedleDB [7].", "startOffset": 59, "endOffset": 62}, {"referenceID": 6, "context": "For additional details see [7].", "startOffset": 27, "endOffset": 30}, {"referenceID": 7, "context": "To address both issues at the same time, we use the algorithm in [8] to split the 122 mixed tracks into a training and test set based on the instruments that appear in each track.", "startOffset": 65, "endOffset": 68}, {"referenceID": 8, "context": "[9].", "startOffset": 0, "endOffset": 3}, {"referenceID": 9, "context": "These three matricies where stacked and modelled using a Gaussian distribution [10].", "startOffset": 79, "endOffset": 83}, {"referenceID": 3, "context": "Similar to results in [4] and [3], the first convolutional layer seems to learn an auditory scale filter bank.", "startOffset": 22, "endOffset": 25}, {"referenceID": 2, "context": "Similar to results in [4] and [3], the first convolutional layer seems to learn an auditory scale filter bank.", "startOffset": 30, "endOffset": 33}, {"referenceID": 0, "context": "The spectra of each filter was rescaled to [0,1] by subtracting the minimum and dividing by the range.", "startOffset": 43, "endOffset": 48}, {"referenceID": 3, "context": "As in [4] many of the learned filters are translated versions of each other.", "startOffset": 6, "endOffset": 9}], "year": 2015, "abstractText": "Traditional methods to tackle many music information retrieval tasks typically follow a two-step architecture: feature engineering followed by a simple learning algorithm. In these \u201dshallow\u201d architectures, feature engineering and learning are typically disjoint and unrelated. Additionally, feature engineering is difficult, and typically depends on extensive domain expertise. In this paper, we present an application of convolutional neural networks for the task of automatic musical instrument identification. In this model, feature extraction and learning algorithms are trained together in an end-to-end fashion. We show that a convolutional neural network trained on raw audio can achieve performance surpassing traditional methods that rely on hand-crafted features.", "creator": "LaTeX with hyperref package"}}}