{"id": "1511.05526", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "17-Nov-2015", "title": "Learning Articulated Motion Models from Visual and Lingual Signals", "abstract": "In order for robots to operate effectively in homes and workplaces, they must be able to manipulate the articulated objects common to environments built for and by humans. Previous work learns kinematic models that prescribe this manipulation from visual demonstrations. Lingual signals, such as natural language descriptions and instructions, offer a complementary means of conveying knowledge of such manipulation models and are suitable to a wide range of interactions (e.g., remote manipulation). In this paper, we present a multimodal learning framework that incorporates both visual and lingual information to estimate the structure and parameters that define kinematic models of articulated objects. The visual signal takes the form of an RGB-D image stream that opportunistically captures object motion in an unprepared scene. Accompanying natural language descriptions of the motion constitute the lingual signal. We present a probabilistic language model that uses word embeddings to associate lingual verbs with their corresponding kinematic structures. By exploiting the complementary nature of the visual and lingual input, our method infers correct kinematic structures for various multiple-part objects on which the previous state-of-the-art, visual-only system fails. We evaluate our multimodal learning framework on a dataset comprised of a variety of household objects, and demonstrate a 36% improvement in model accuracy over the vision-only baseline.", "histories": [["v1", "Tue, 17 Nov 2015 19:55:34 GMT  (1573kb,D)", "https://arxiv.org/abs/1511.05526v1", "Submitted to ICRA 2016"], ["v2", "Fri, 1 Jul 2016 14:53:28 GMT  (4911kb,D)", "http://arxiv.org/abs/1511.05526v2", null]], "COMMENTS": "Submitted to ICRA 2016", "reviews": [], "SUBJECTS": "cs.RO cs.CL cs.CV", "authors": ["zhengyang wu", "mohit bansal", "matthew r walter"], "accepted": false, "id": "1511.05526"}, "pdf": {"name": "1511.05526.pdf", "metadata": {"source": "CRF", "title": "Learning Articulated Motion Models from Visual and Lingual Signals", "authors": ["Zhengyang Wu", "Mohit Bansal", "Matthew R. Walter"], "emails": ["zwu66@cc.gatech.edu", "mbansal@ttic.edu", "mwalter@ttic.edu"], "sections": [{"heading": null, "text": "In fact, it is such that most of them are able to survive themselves without there being a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is in which there is a process in which there is a process in which there is a process in which there is a process in which there is in which there is a process in which there is a process in which there is a process in which there is a process in which"}, {"heading": "II. RELATED WORK", "text": "In fact, most people who are able to move, to move, to move, to move, to move, to move, to move, to move, to move, to dance, to move, to move, to move, to dance, to dance, to dance, to dance, to move, to move, to dance, to move, to move, to move, to move, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to"}, {"heading": "III. MULTIMODAL LEARNING FRAMEWORK", "text": "We assume that we represent the structure and parameters of the respective object and the edges of the respective motion sequences (e.g. a link) between two parts (fig. 1). Formally, we estimate a kinematic diagram G = (VG, EG), which consists of different parts of the object, which consist of different visual backgrounds and edges, which indicate the presence of limited motion (e.g. a link) between two parts (fig. 1). Formally, we estimate a kinematic diagram G = (VG, EG), which consists of inverted VG for each part of the object and edges EG-VG between parts, whose relative movement is restricted. Associated with each edge (ij), it is its kinematic type Mij (rotational, prismatic, rigid) and the corresponding parameters."}, {"heading": "B. Language-guided Model Selection", "text": "In fact, the fact is that most of us are able to survive ourselves, and that they are able to survive themselves, \"he told the German Press Agency.\" But it's not that we are able to change the world, \"he said.\" But it's not that we are able to change the world. \"He added,\" It's not that we are able to change the world, but that we are able to change the world. \""}, {"heading": "C. Combining Visual and Lingual Observations", "text": "The last step in our frame chooses the kinematic graphene structure M-2-2-3-3-3-3-3-3-3-3-3-3-3-3-3-3-3-3-3-3-3-3-3-3-3-3-3-3-3-3-3-3-3-3-3-3-3-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-3-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-3-4-4-4-4-4-4-4-4-4-4-4-4-3-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-3-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-3-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-3-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-in"}, {"heading": "IV. RESULTS", "text": "We evaluate our framework based on 28 RGB-D videos in which a user manipulates a variety of common household and office objects (e.g. microwave oven, refrigerator, and drawer).AprilTags [39] were placed on each part of an object and used as an observation of movement on the floor.We mask AprilTags when operating the visual pipeline so as not to interfere with the extraction of features.Of the 28 videos, 13 relate to one-piece objects and 15 to multi-part objects.The one-piece object videos are used to demonstrate that adding linguistic observations can only improve the accuracy of the learned kinematic models.The extent of these improvements to one-piece objects is limited by the relative ease of inferencing individual freedom motions.In the case of multi-part objects, the larger space of the candidates makes kinematic graphs visually only an inference challenge, since feature tracking errors can lead to incorrect assessments of the graphical structure. These modal experiments are designed to evaluate the magnitude of learning."}, {"heading": "A. Evaluation Metrics and Baselines", "text": "We estimate the ground-truth kinematics models by performing MAP conclusions based on the motion paths observed with the help of April tags. We refer to the resulting kinematic graph as G \u043a. The kinematic type and parameters for each object part pair are called M * ij and \u03b8 * ij, respectively. The first metric we consider is whether the vision component estimates the correct number of parts. We determine the ground-truth number of parts as the number of April tags observed in each video, which we refer to as N *. We point to the number of parts (motion clusters) that we have identified as Nv through the visual pipeline. We report the average success rate if we only use visual observations such as Sv = 1K \u00b2 K \u00b2 K \u00b2 K \u00b2 K = 1."}, {"heading": "B. Results and Analysis", "text": "This year, it has come to the point where it only takes one year to get to the next round."}, {"heading": "V. CONCLUSION", "text": "We have described a method that uses a common combination of visual and linguistic signals to learn accurate probability models that define the structure and parameters of articulated objects. Our framework treats linguistic descriptions of detected motion as complementary observation of the structure of kinematic links. We evaluate our framework using a series of RGB-D videos paired with captions of common household and office objects, and show that the use of linguistic cues leads to improved model accuracy. Future work will include the inclusion of visual object splitter recognition using captions to mitigate noise in visual recognition. We will also examine a word that is more suitable for this specific area to more efficiently utilize visual and lingual signals for complex objects. In addition, we will examine an expansion 4We consider a scenario to be successful if our method identifies the correct kinematic graph consisting of parts that are known by visual cluster kinematic models based on the problem of novel language classes."}], "references": [{"title": "A probabilistic framework for learning kinematic models of articulated objects", "author": ["J. Sturm", "C. Stachniss", "W. Burgard"], "venue": "J. of Artificial Intelligence Research, vol. 41, no. 2, pp. 477\u2013526, 2011.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2011}, {"title": "Occlusionaware reconstruction and manipulation of 3D articulated objects", "author": ["X. Huang", "I. Walker", "S. Birchfield"], "venue": "Proc. IEEE Int\u2019l Conf. on Robotics and Automation (ICRA), 2012, pp. 1365\u20131371.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2012}, {"title": "Interactive segmentation, tracking, and kinematic modeling of unknown 3D articulated objects", "author": ["D. Katz", "M. Kazemi", "J. Andrew Bagnell", "A. Stentz"], "venue": "Proc. IEEE Int\u2019l Conf. on Robotics and Automation (ICRA), 2013, pp. 5003\u20135010.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2013}, {"title": "Learning articulated motions from visual demonstration", "author": ["S. Pillai", "M.R. Walter", "S. Teller"], "venue": "Proc. Robotics: Science and Systems (RSS), Berkeley, CA, July 2014.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2014}, {"title": "Im2Text: Describing images using 1 million captioned photographs", "author": ["V. Ordonez", "G. Kulkarni", "T.L. Berg"], "venue": "Adv. Neural Information Processing Systems (NIPS), 2011.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2011}, {"title": "Midge: Generating image descriptions from computer vision detections", "author": ["M. Mitchell", "X. Han", "J. Dodge", "A. Mensch", "A. Goyal", "A. Berg", "K. Yamaguchi", "T. Berg", "K. Stratos", "H. Daum\u00e9 III"], "venue": "Proc. Assoc. for Computational Linguistics (ACL), 2012, pp. 747\u2013756.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2012}, {"title": "Generalized multiview analysis: A discriminative latent space", "author": ["A. Sharma", "A. Kumar", "H. Daume III", "D.W. Jacobs"], "venue": "Proc. IEEE Conf. on Computer Vision and Pattern Recognition (CVPR), 2012, pp. 2160\u20132167.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2012}, {"title": "What are you talking about? text-to-image coreference", "author": ["C. Kong", "D. Lin", "M. Bansal", "R. Urtasun", "S. Fidler"], "venue": "Proc. IEEE Conf. on Computer Vision and Pattern Recognition (CVPR), 2014.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2014}, {"title": "Linking people in videos with their names using coreference resolution", "author": ["V. Ramanathan", "A. Joulin", "P. Liang", "L. Fei-Fei"], "venue": "Proc. European Conf. on Computer Vision (ECCV), 2014, pp. 95\u2013110.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2014}, {"title": "Openvocabulary object retrieval", "author": ["S. Guadarrama", "E. Rodner", "K. Saenko", "N. Zhang", "R. Farrell", "J. Donahue", "T. Darrell"], "venue": "Proc. Robotics: Science and Systems (RSS), Berkeley, CA, July 2014.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2014}, {"title": "Deep visual-semantic alignments for generating image descriptions", "author": ["A. Karpathy", "L. Fei-Fei"], "venue": "Proc. IEEE Conf. on Computer Vision and Pattern Recognition (CVPR), June 2015.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2015}, {"title": "Show and tell: A neural image caption generator", "author": ["O. Vinyals", "A. Toshev", "S. Bengio", "D. Erhan"], "venue": "Proc. IEEE Conf. on Computer Vision and Pattern Recognition (CVPR), June 2015.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2015}, {"title": "Show,  attend and tell: Neural image caption generation with visual attention", "author": ["K. Xu", "J. Ba", "R. Kiros", "K. Cho", "A. Courville", "R. Salakhutdinov", "R. Zemel", "Y. Bengio"], "venue": "Proc. Int\u2019l Conf. on Machine Learning (ICML), 2015.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2015}, {"title": "Unifying visual-semantic embeddings with multimodal neural language models", "author": ["R. Kiros", "R. Salakhutdinov", "R.S. Zemel"], "venue": "arXiv:1411.2539, 2014.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2014}, {"title": "Deep captioning with multimodal recurrent neural networks (m-RNN)", "author": ["J. Mao", "W. Xu", "Y. Yang", "J. Wang", "A. Yuille"], "venue": "arXiv:1412.6632, 2014.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2014}, {"title": "Long-term recurrent convolutional networks for visual recognition and description", "author": ["J. Donahue", "L.A. Hendricks", "S. Guadarrama", "M. Rohrbach", "S. Venugopalan", "K. Saenko", "T. Darrell"], "venue": "arXiv:1411.4389, 2014.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2014}, {"title": "Show and tell: A neural image caption generator", "author": ["O. Vinyals", "A. Toshev", "S. Bengio", "D. Erhan"], "venue": "Proc. IEEE Conf. on Computer Vision and Pattern Recognition (CVPR), June 2015.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2015}, {"title": "Mind\u2019s eye: A recurrent visual representation for image caption generation", "author": ["X. Chen", "C.L. Zitnick"], "venue": "Proc. IEEE Conf. on Computer Vision and Pattern Recognition (CVPR), 2015.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2015}, {"title": "Describing videos by exploiting temporal structure", "author": ["L. Yao", "A. Torabi", "K. Cho", "N. Ballas", "C. Pal", "H. Larochelle", "A. Courville"], "venue": "arXiv preprint arXiv:1502.08029, 2015.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2015}, {"title": "Unsupervised learning of video representations using LSTMs", "author": ["N. Srivastava", "E. Mansimov", "R. Salakhutdinov"], "venue": "arXiv preprint arXiv:1502.04681, 2015.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2015}, {"title": "Sequence to sequence\u2013 video to text", "author": ["S. Venugopalan", "M. Rohrbach", "J. Donahue", "R. Mooney", "T. Darrell", "K. Saenko"], "venue": "arXiv preprint arXiv:1505.00487, 2015.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2015}, {"title": "Listen, attend, and walk: Neural mapping of navigational instructions to action sequences", "author": ["H. Mei", "M. Bansal", "M.R. Walter"], "venue": "Proc. Nat\u2019l Conf. on Artificial Intelligence (AAAI), Phoenix, AZ, February 2016.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2016}, {"title": "A general framework for motion segmentation: Independent, articulated, rigid, non-rigid, degenerate and non-degenerate", "author": ["J. Yan", "M. Pollefeys"], "venue": "Proc. European Conf. on Computer Vision (ECCV), 2006, pp. 94\u2013106.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2006}, {"title": "Interactive perception of articulated objects", "author": ["D. Katz", "A. Orthey", "O. Brock"], "venue": "Proc. Int\u2019l. Symp. on Experimental Robotics (ISER), 2010.", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2010}, {"title": "Active articulation model estimation through interactive perception", "author": ["K. Hausman", "S. Niekum", "S. Ostenoski", "G.S. Sukhatme"], "venue": "Proc. IEEE Int\u2019l Conf. on Robotics and Automation (ICRA), Seattle, WA, May 2015.", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2015}, {"title": "Toward understanding natural language directions", "author": ["T. Kollar", "S. Tellex", "D. Roy", "N. Roy"], "venue": "Proc. ACM/IEEE Int\u2019l. Conf. on Human-Robot Interaction (HRI), 2010.", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2010}, {"title": "Following directions using statistical machine translation", "author": ["C. Matuszek", "D. Fox", "K. Koscher"], "venue": "Proc.  ACM/IEEE Int\u2019l. Conf. on Human-Robot Interaction (HRI), 2010.", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2010}, {"title": "Understanding natural language commands for robotic navigation and mobile manipulation", "author": ["S. Tellex", "T. Kollar", "S. Dickerson", "M.R. Walter", "A.G. Banerjee", "S. Teller", "N. Roy"], "venue": "Proc. Nat\u2019l Conf. on Artificial Intelligence (AAAI), 2011.", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2011}, {"title": "Learning to interpret natural language navigation instructions from observations", "author": ["D.L. Chen", "R.J. Mooney"], "venue": "Proc. Nat\u2019l Conf. on Artificial Intelligence (AAAI), 2011.", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2011}, {"title": "Weakly supervised learning of semantic parsers for mapping instructions to actions", "author": ["Y. Artzi", "L. Zettlemoyer"], "venue": "Trans. Assoc. for Computational Linguistics, vol. 1, pp. 49\u201362, 2013.", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2013}, {"title": "Learning semantic maps from natural language descriptions", "author": ["M.R. Walter", "S. Hemachandra", "B. Homberg", "S. Tellex", "S. Teller"], "venue": "Proc. Robotics: Science and Systems (RSS), Berlin, Germany, June 2013.", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2013}, {"title": "Inferring maps and behaviors from natural language instructions", "author": ["F. Duvallet", "M.R. Walter", "T. Howard", "S. Hemachandra", "J. Oh", "S. Teller", "N. Roy", "A. Stentz"], "venue": "Proc. Int\u2019l. Symp. on Experimental Robotics (ISER), Marrakech/Essaouira, Morocco, June 2014.", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2014}, {"title": "Learning models for following natural language directions in unknown environments", "author": ["S. Hemachandra", "F. Duvallet", "T.M. Howard", "N. Roy", "A. Stentz", "M.R. Walter"], "venue": "Proc. IEEE Int\u2019l Conf. on Robotics and Automation (ICRA), May 2015.", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2015}, {"title": "VQA: Visual question answering", "author": ["S. Antol", "A. Agrawal", "J. Lu", "M. Mitchell", "D. Batra", "C.L. Zitnick", "D. Parikh"], "venue": "arXiv preprint arXiv:1505.00468, 2015.", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2015}, {"title": "What\u2019s cookin\u2019? Interpreting cooking videos using text, speech and vision", "author": ["J. Malmaud", "J. Huang", "V. Rathod", "N. Johnston", "A. Rabinovich", "K. Murphy"], "venue": "arXiv preprint arXiv:1503.01558, 2015.", "citeRegEx": "35", "shortCiteRegEx": null, "year": 2015}, {"title": "The stanford corenlp natural language processing toolkit", "author": ["C.D. Manning", "M. Surdeanu", "J. Bauer", "J. Finkel", "S.J. Bethard", "D. McClosky"], "venue": "Proceedings of 52nd Annual Meeting of the Association for Computational Linguistics: System Demonstrations, 2014, pp. 55\u201360.", "citeRegEx": "36", "shortCiteRegEx": null, "year": 2014}, {"title": "Rich feature hierarchies for accurate object detection and semantic segmentation", "author": ["R. Girschick", "J. Donahue", "T. Darrell", "J. Malik"], "venue": "Proc. IEEE Conf. on Computer Vision and Pattern Recognition (CVPR), June 2014.", "citeRegEx": "37", "shortCiteRegEx": null, "year": 2014}, {"title": "Distributed representations of words and phrases and their compositionality", "author": ["T. Mikolov", "I. Sutskever", "K. Chen", "G.S. Corrado", "J. Dean"], "venue": "Adv. Neural Information Processing Systems (NIPS), 2013, pp. 3111\u2013 3119.", "citeRegEx": "38", "shortCiteRegEx": null, "year": 2013}, {"title": "AprilTag: A robust and flexible visual fiducial system", "author": ["E. Olson"], "venue": "Proc. IEEE Int\u2019l Conf. on Robotics and Automation (ICRA), May 2011.", "citeRegEx": "39", "shortCiteRegEx": null, "year": 2011}], "referenceMentions": [{"referenceID": 0, "context": "In an effort to improve efficiency and generalizability, recent work employs visual demonstrations to learn representations that describe the motion of these parts in the form of kinematic models that express the rotational, prismatic, and rigid relationships between object parts [1\u20134].", "startOffset": 281, "endOffset": 286}, {"referenceID": 1, "context": "In an effort to improve efficiency and generalizability, recent work employs visual demonstrations to learn representations that describe the motion of these parts in the form of kinematic models that express the rotational, prismatic, and rigid relationships between object parts [1\u20134].", "startOffset": 281, "endOffset": 286}, {"referenceID": 2, "context": "In an effort to improve efficiency and generalizability, recent work employs visual demonstrations to learn representations that describe the motion of these parts in the form of kinematic models that express the rotational, prismatic, and rigid relationships between object parts [1\u20134].", "startOffset": 281, "endOffset": 286}, {"referenceID": 3, "context": "In an effort to improve efficiency and generalizability, recent work employs visual demonstrations to learn representations that describe the motion of these parts in the form of kinematic models that express the rotational, prismatic, and rigid relationships between object parts [1\u20134].", "startOffset": 281, "endOffset": 286}, {"referenceID": 4, "context": "Our effort is inspired by the recent attention that has been paid to the joint use of vision and language as complementary signals for machine perception [5\u201322].", "startOffset": 154, "endOffset": 160}, {"referenceID": 5, "context": "Our effort is inspired by the recent attention that has been paid to the joint use of vision and language as complementary signals for machine perception [5\u201322].", "startOffset": 154, "endOffset": 160}, {"referenceID": 6, "context": "Our effort is inspired by the recent attention that has been paid to the joint use of vision and language as complementary signals for machine perception [5\u201322].", "startOffset": 154, "endOffset": 160}, {"referenceID": 7, "context": "Our effort is inspired by the recent attention that has been paid to the joint use of vision and language as complementary signals for machine perception [5\u201322].", "startOffset": 154, "endOffset": 160}, {"referenceID": 8, "context": "Our effort is inspired by the recent attention that has been paid to the joint use of vision and language as complementary signals for machine perception [5\u201322].", "startOffset": 154, "endOffset": 160}, {"referenceID": 9, "context": "Our effort is inspired by the recent attention that has been paid to the joint use of vision and language as complementary signals for machine perception [5\u201322].", "startOffset": 154, "endOffset": 160}, {"referenceID": 10, "context": "Our effort is inspired by the recent attention that has been paid to the joint use of vision and language as complementary signals for machine perception [5\u201322].", "startOffset": 154, "endOffset": 160}, {"referenceID": 11, "context": "Our effort is inspired by the recent attention that has been paid to the joint use of vision and language as complementary signals for machine perception [5\u201322].", "startOffset": 154, "endOffset": 160}, {"referenceID": 12, "context": "Our effort is inspired by the recent attention that has been paid to the joint use of vision and language as complementary signals for machine perception [5\u201322].", "startOffset": 154, "endOffset": 160}, {"referenceID": 13, "context": "Our effort is inspired by the recent attention that has been paid to the joint use of vision and language as complementary signals for machine perception [5\u201322].", "startOffset": 154, "endOffset": 160}, {"referenceID": 14, "context": "Our effort is inspired by the recent attention that has been paid to the joint use of vision and language as complementary signals for machine perception [5\u201322].", "startOffset": 154, "endOffset": 160}, {"referenceID": 15, "context": "Our effort is inspired by the recent attention that has been paid to the joint use of vision and language as complementary signals for machine perception [5\u201322].", "startOffset": 154, "endOffset": 160}, {"referenceID": 16, "context": "Our effort is inspired by the recent attention that has been paid to the joint use of vision and language as complementary signals for machine perception [5\u201322].", "startOffset": 154, "endOffset": 160}, {"referenceID": 17, "context": "Our effort is inspired by the recent attention that has been paid to the joint use of vision and language as complementary signals for machine perception [5\u201322].", "startOffset": 154, "endOffset": 160}, {"referenceID": 18, "context": "Our effort is inspired by the recent attention that has been paid to the joint use of vision and language as complementary signals for machine perception [5\u201322].", "startOffset": 154, "endOffset": 160}, {"referenceID": 19, "context": "Our effort is inspired by the recent attention that has been paid to the joint use of vision and language as complementary signals for machine perception [5\u201322].", "startOffset": 154, "endOffset": 160}, {"referenceID": 20, "context": "Our effort is inspired by the recent attention that has been paid to the joint use of vision and language as complementary signals for machine perception [5\u201322].", "startOffset": 154, "endOffset": 160}, {"referenceID": 21, "context": "Our effort is inspired by the recent attention that has been paid to the joint use of vision and language as complementary signals for machine perception [5\u201322].", "startOffset": 154, "endOffset": 160}, {"referenceID": 22, "context": "Several methods formulate this problem as bundle adjustment, using structure-from-motion methods to first segment an articulated object into its compositional parts and to then estimate the parameters of the rotational and prismatic degrees-of-freedom that describe inter-part motion [23, 2].", "startOffset": 284, "endOffset": 291}, {"referenceID": 1, "context": "Several methods formulate this problem as bundle adjustment, using structure-from-motion methods to first segment an articulated object into its compositional parts and to then estimate the parameters of the rotational and prismatic degrees-of-freedom that describe inter-part motion [23, 2].", "startOffset": 284, "endOffset": 291}, {"referenceID": 23, "context": "[24] propose an active learning framework that allows a robot to interact with articulated objects to induce motion.", "startOffset": 0, "endOffset": 4}, {"referenceID": 2, "context": "[3] improve upon the complexity of this method while preserving the accuracy of the inferred models.", "startOffset": 0, "endOffset": 3}, {"referenceID": 24, "context": "[25] similarly enable a robot to interact with the object and describe a probabilistic model that integrates observations of fiducials with manipulator feedback.", "startOffset": 0, "endOffset": 4}, {"referenceID": 0, "context": "[1] propose a probabilistic approach that simultaneously reasons over the likelihood of observations while accounting for the learned model complexity.", "startOffset": 0, "endOffset": 3}, {"referenceID": 3, "context": "[4] propose an extension to this work that uses novel vision-based motion segmentation and tracking that enables model learning without prior knowledge of the number of parts or the placement of fiducial markers.", "startOffset": 0, "endOffset": 3}, {"referenceID": 25, "context": "Meanwhile, recent work in the natural language processing community has focused on the role of language as a means of commanding [26\u201330, 22] and sharing spatial information [31\u201333] with robots.", "startOffset": 129, "endOffset": 140}, {"referenceID": 26, "context": "Meanwhile, recent work in the natural language processing community has focused on the role of language as a means of commanding [26\u201330, 22] and sharing spatial information [31\u201333] with robots.", "startOffset": 129, "endOffset": 140}, {"referenceID": 27, "context": "Meanwhile, recent work in the natural language processing community has focused on the role of language as a means of commanding [26\u201330, 22] and sharing spatial information [31\u201333] with robots.", "startOffset": 129, "endOffset": 140}, {"referenceID": 28, "context": "Meanwhile, recent work in the natural language processing community has focused on the role of language as a means of commanding [26\u201330, 22] and sharing spatial information [31\u201333] with robots.", "startOffset": 129, "endOffset": 140}, {"referenceID": 29, "context": "Meanwhile, recent work in the natural language processing community has focused on the role of language as a means of commanding [26\u201330, 22] and sharing spatial information [31\u201333] with robots.", "startOffset": 129, "endOffset": 140}, {"referenceID": 21, "context": "Meanwhile, recent work in the natural language processing community has focused on the role of language as a means of commanding [26\u201330, 22] and sharing spatial information [31\u201333] with robots.", "startOffset": 129, "endOffset": 140}, {"referenceID": 30, "context": "Meanwhile, recent work in the natural language processing community has focused on the role of language as a means of commanding [26\u201330, 22] and sharing spatial information [31\u201333] with robots.", "startOffset": 173, "endOffset": 180}, {"referenceID": 31, "context": "Meanwhile, recent work in the natural language processing community has focused on the role of language as a means of commanding [26\u201330, 22] and sharing spatial information [31\u201333] with robots.", "startOffset": 173, "endOffset": 180}, {"referenceID": 32, "context": "Meanwhile, recent work in the natural language processing community has focused on the role of language as a means of commanding [26\u201330, 22] and sharing spatial information [31\u201333] with robots.", "startOffset": 173, "endOffset": 180}, {"referenceID": 4, "context": "Meanwhile, other methods have similarly used visual and lingual cues in a multimodal learning framework for such tasks as image and video caption synthesis [5\u20137, 10\u201321], visual coreference resolution [8, 9], visual question-answering [34], and understanding cooking videos paired with recipes [35].", "startOffset": 156, "endOffset": 168}, {"referenceID": 5, "context": "Meanwhile, other methods have similarly used visual and lingual cues in a multimodal learning framework for such tasks as image and video caption synthesis [5\u20137, 10\u201321], visual coreference resolution [8, 9], visual question-answering [34], and understanding cooking videos paired with recipes [35].", "startOffset": 156, "endOffset": 168}, {"referenceID": 6, "context": "Meanwhile, other methods have similarly used visual and lingual cues in a multimodal learning framework for such tasks as image and video caption synthesis [5\u20137, 10\u201321], visual coreference resolution [8, 9], visual question-answering [34], and understanding cooking videos paired with recipes [35].", "startOffset": 156, "endOffset": 168}, {"referenceID": 9, "context": "Meanwhile, other methods have similarly used visual and lingual cues in a multimodal learning framework for such tasks as image and video caption synthesis [5\u20137, 10\u201321], visual coreference resolution [8, 9], visual question-answering [34], and understanding cooking videos paired with recipes [35].", "startOffset": 156, "endOffset": 168}, {"referenceID": 10, "context": "Meanwhile, other methods have similarly used visual and lingual cues in a multimodal learning framework for such tasks as image and video caption synthesis [5\u20137, 10\u201321], visual coreference resolution [8, 9], visual question-answering [34], and understanding cooking videos paired with recipes [35].", "startOffset": 156, "endOffset": 168}, {"referenceID": 11, "context": "Meanwhile, other methods have similarly used visual and lingual cues in a multimodal learning framework for such tasks as image and video caption synthesis [5\u20137, 10\u201321], visual coreference resolution [8, 9], visual question-answering [34], and understanding cooking videos paired with recipes [35].", "startOffset": 156, "endOffset": 168}, {"referenceID": 12, "context": "Meanwhile, other methods have similarly used visual and lingual cues in a multimodal learning framework for such tasks as image and video caption synthesis [5\u20137, 10\u201321], visual coreference resolution [8, 9], visual question-answering [34], and understanding cooking videos paired with recipes [35].", "startOffset": 156, "endOffset": 168}, {"referenceID": 13, "context": "Meanwhile, other methods have similarly used visual and lingual cues in a multimodal learning framework for such tasks as image and video caption synthesis [5\u20137, 10\u201321], visual coreference resolution [8, 9], visual question-answering [34], and understanding cooking videos paired with recipes [35].", "startOffset": 156, "endOffset": 168}, {"referenceID": 14, "context": "Meanwhile, other methods have similarly used visual and lingual cues in a multimodal learning framework for such tasks as image and video caption synthesis [5\u20137, 10\u201321], visual coreference resolution [8, 9], visual question-answering [34], and understanding cooking videos paired with recipes [35].", "startOffset": 156, "endOffset": 168}, {"referenceID": 15, "context": "Meanwhile, other methods have similarly used visual and lingual cues in a multimodal learning framework for such tasks as image and video caption synthesis [5\u20137, 10\u201321], visual coreference resolution [8, 9], visual question-answering [34], and understanding cooking videos paired with recipes [35].", "startOffset": 156, "endOffset": 168}, {"referenceID": 16, "context": "Meanwhile, other methods have similarly used visual and lingual cues in a multimodal learning framework for such tasks as image and video caption synthesis [5\u20137, 10\u201321], visual coreference resolution [8, 9], visual question-answering [34], and understanding cooking videos paired with recipes [35].", "startOffset": 156, "endOffset": 168}, {"referenceID": 17, "context": "Meanwhile, other methods have similarly used visual and lingual cues in a multimodal learning framework for such tasks as image and video caption synthesis [5\u20137, 10\u201321], visual coreference resolution [8, 9], visual question-answering [34], and understanding cooking videos paired with recipes [35].", "startOffset": 156, "endOffset": 168}, {"referenceID": 18, "context": "Meanwhile, other methods have similarly used visual and lingual cues in a multimodal learning framework for such tasks as image and video caption synthesis [5\u20137, 10\u201321], visual coreference resolution [8, 9], visual question-answering [34], and understanding cooking videos paired with recipes [35].", "startOffset": 156, "endOffset": 168}, {"referenceID": 19, "context": "Meanwhile, other methods have similarly used visual and lingual cues in a multimodal learning framework for such tasks as image and video caption synthesis [5\u20137, 10\u201321], visual coreference resolution [8, 9], visual question-answering [34], and understanding cooking videos paired with recipes [35].", "startOffset": 156, "endOffset": 168}, {"referenceID": 20, "context": "Meanwhile, other methods have similarly used visual and lingual cues in a multimodal learning framework for such tasks as image and video caption synthesis [5\u20137, 10\u201321], visual coreference resolution [8, 9], visual question-answering [34], and understanding cooking videos paired with recipes [35].", "startOffset": 156, "endOffset": 168}, {"referenceID": 7, "context": "Meanwhile, other methods have similarly used visual and lingual cues in a multimodal learning framework for such tasks as image and video caption synthesis [5\u20137, 10\u201321], visual coreference resolution [8, 9], visual question-answering [34], and understanding cooking videos paired with recipes [35].", "startOffset": 200, "endOffset": 206}, {"referenceID": 8, "context": "Meanwhile, other methods have similarly used visual and lingual cues in a multimodal learning framework for such tasks as image and video caption synthesis [5\u20137, 10\u201321], visual coreference resolution [8, 9], visual question-answering [34], and understanding cooking videos paired with recipes [35].", "startOffset": 200, "endOffset": 206}, {"referenceID": 33, "context": "Meanwhile, other methods have similarly used visual and lingual cues in a multimodal learning framework for such tasks as image and video caption synthesis [5\u20137, 10\u201321], visual coreference resolution [8, 9], visual question-answering [34], and understanding cooking videos paired with recipes [35].", "startOffset": 234, "endOffset": 238}, {"referenceID": 34, "context": "Meanwhile, other methods have similarly used visual and lingual cues in a multimodal learning framework for such tasks as image and video caption synthesis [5\u20137, 10\u201321], visual coreference resolution [8, 9], visual question-answering [34], and understanding cooking videos paired with recipes [35].", "startOffset": 293, "endOffset": 297}, {"referenceID": 0, "context": "[1], we represent this model as a graph, where each vertex denotes a different part of the object (or the stationary background) and edges denote the existence of constrained motion (e.", "startOffset": 0, "endOffset": 3}, {"referenceID": 0, "context": "[1] and use a two-step inference procedure that alternates between model parameter fitting and model structure selection steps (Fig.", "startOffset": 0, "endOffset": 3}, {"referenceID": 3, "context": "While our previous work [4] provides visual observations of motion without the need for fiducials, it relies upon feature tracking and segmentation that can fail when the object parts lack texture (e.", "startOffset": 24, "endOffset": 27}, {"referenceID": 3, "context": "[4] to arrive at a visual observation of the trajectory of each object part (Fig.", "startOffset": 0, "endOffset": 3}, {"referenceID": 3, "context": "[4] for specific details regarding the visual pipeline.", "startOffset": 0, "endOffset": 3}, {"referenceID": 0, "context": "We perform this optimization over the joint kinematic structure defined by the edges in the graph [1].", "startOffset": 98, "endOffset": 101}, {"referenceID": 3, "context": "Methods that solely use visual input are sensitive to the effects of scene clutter and the lack of texture, which can result in erroneous estimates for the structure and parameters of the kinematic model [4].", "startOffset": 204, "endOffset": 207}, {"referenceID": 35, "context": "We use the Stanford CoreNLP pipeline, tokenizer, and POS-tagger to identify the nouns and verbs in the caption [36].", "startOffset": 111, "endOffset": 115}, {"referenceID": 36, "context": "investigated the use of vision-based object recognition to reduce this search space [37], but found the recognition accuracy to be insufficient for such tasks (detectors were prone to false negatives and tend to predict holistic object classes like \u201cbicycle\u201d instead of their parts like \u201cbicycle", "startOffset": 84, "endOffset": 88}, {"referenceID": 37, "context": "Next, we convert each word to a d-dimensional word embedding space using word2vec [38], a popular neural language model.", "startOffset": 82, "endOffset": 86}, {"referenceID": 3, "context": "While our previous method [4] only considers visual observations, our new framework performs this optimization over the joint space of visual and lingual observations.", "startOffset": 26, "endOffset": 29}, {"referenceID": 38, "context": "AprilTags [39] were placed on each of the objects parts and used as an observation of ground-truth motion.", "startOffset": 10, "endOffset": 14}, {"referenceID": 33, "context": "This is similar to discussions for image and video captioning and questionanswering research, where it is well-known that a more detailed, database-like caption is more useful for capturing multiple salient events in the image/video, and for answering questions made of them [34].", "startOffset": 275, "endOffset": 279}, {"referenceID": 3, "context": "Table I summarizes the performance of our multimodal learning method using our embedding-based language model with hard alignment, comparing against the performance of the vision-only baseline [4].", "startOffset": 193, "endOffset": 196}], "year": 2016, "abstractText": "In order for robots to operate effectively in homes and workplaces, they must be able to manipulate the articulated objects common within environments built for and by humans. Previous work learns kinematic models that prescribe this manipulation from visual demonstrations. Lingual signals, such as natural language descriptions and instructions, offer a complementary means of conveying knowledge of such manipulation models and are suitable to a wide range of interactions (e.g., remote manipulation). In this paper, we present a multimodal learning framework that incorporates both visual and lingual information to estimate the structure and parameters that define kinematic models of articulated objects. The visual signal takes the form of an RGB-D image stream that opportunistically captures object motion in an unprepared scene. Accompanying natural language descriptions of the motion constitute the lingual signal. We present a probabilistic language model that uses word embeddings to associate lingual verbs with their corresponding kinematic structures. By exploiting the complementary nature of the visual and lingual input, our method infers correct kinematic structures for various multiplepart objects on which the previous state-of-the-art, visual-only system fails. We evaluate our multimodal learning framework on a dataset comprised of a variety of household objects, and demonstrate a 36% improvement in model accuracy over the vision-only baseline.", "creator": "LaTeX with hyperref package"}}}