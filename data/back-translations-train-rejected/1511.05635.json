{"id": "1511.05635", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "18-Nov-2015", "title": "Competitive Multi-scale Convolution", "abstract": "In this paper, we introduce a new deep convolutional neural network (ConvNet) module that promotes competition among a set of multi-scale convolutional filters. This new module is inspired by the inception module, where we replace the original collaborative pooling stage (consisting of a concatenation of the multi-scale filter outputs) by a competitive pooling represented by a maxout activation unit. This extension has the following two objectives: 1) the selection of the maximum response among the multi-scale filters prevents filter co-adaptation and allows the formation of multiple sub-networks within the same model, which has been shown to facilitate the training of complex learning problems; and 2) the maxout unit reduces the dimensionality of the outputs from the multi-scale filters. We show that the use of our proposed module in typical deep ConvNets produces classification results that are either better than or comparable to the state of the art on the following benchmark datasets: MNIST, CIFAR-10, CIFAR-100 and SVHN.", "histories": [["v1", "Wed, 18 Nov 2015 01:19:00 GMT  (1701kb,D)", "http://arxiv.org/abs/1511.05635v1", null]], "reviews": [], "SUBJECTS": "cs.CV cs.LG cs.NE", "authors": ["zhibin liao", "gustavo carneiro"], "accepted": false, "id": "1511.05635"}, "pdf": {"name": "1511.05635.pdf", "metadata": {"source": "CRF", "title": "Competitive Multi-scale Convolution", "authors": ["Zhibin Liao", "Gustavo Carneiro"], "emails": ["zhibin.liao@adelaide.edu.au", "gustavo.carneiro@adelaide.edu.au"], "sections": [{"heading": "1. Introduction", "text": "The use of competitive units in deep conventional networks (ConvNets) is generally understood as a type of network by combining several subnets, each of which is capable of solving an easier task taking into account the complexity of the original problem [23, 22]. Similar ideas have been explored in the past by using multi-layer models."}, {"heading": "2. Literature Review", "text": "One of the main reasons for the high level of unemployment in the US is that over the past decade, unemployment in the US has risen many times faster than in other countries around the world."}, {"heading": "3. Methodology", "text": "Suppose an image is represented by x. (...) Suppose an image is represented by x. (...) Suppose an image is represented by x. (...) Suppose an image is represented by x. (...) Suppose an image is represented by x. (...) Suppose an image is represented by x. (...) Suppose an image is represented by x. (...) Suppose an image is represented by x. (...) Suppose an image is represented by x. (...) Suppose an image is represented by x. (...) Suppose an image is represented by x. (...) Suppose an image is represented by. (...) Suppose an image is represented by x. (...) Suppose an image is represented by x. (...) Suppose an image is represented by. (...) Suppose an image is represented by. (...) Suppose an image is represented by. (...) Suppose an image is represented by. (...) Suppose an image is represented by. (...) Suppose an image is represented by x."}, {"heading": "3.1. Competitive Multi-scale Convolution Prevent", "text": "In this context, it should be noted that this project is a project which is, first and foremost, a project which aims to put the interests of the people first."}, {"heading": "4. Experiments", "text": "This year is the highest in the history of the country."}, {"heading": "4.1. Model Design Choices", "text": "In this context, it should be noted that the people mentioned in the study are people who are able to play by the rules and who are able to play by the rules."}, {"heading": "4.2. Comparison with the State of the Art", "text": "We show the performance of the proposed Competitive Multi-Scale and Competitive Inception Convolution models on CIFAR-10, CIFAR-100, MNIST and SVHN, and compare them with the current state of the art in the field, which can be listed as follows. Stochastic Pooling [28] proposes a regulation based on replacing the deterministic pooling unit (e.g., max or average pooling unit) with a stochastic procedure that randomly selects the activation within each pooling region, estimating the activation of the pooling unit. Maxout Networks [4] introduces a step-by-step linear activation unit that is used in conjunction with dropout training."}, {"heading": "5. Discussion and Conclusions", "text": "In this context, it should be noted that this case is an accident."}], "references": [{"title": "Deep sparse rectifier neural networks", "author": ["X. Glorot", "A. Bordes", "Y. Bengio"], "venue": "International Conference on Artificial Intelligence and Statistics, pages 315\u2013323", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2011}, {"title": "Multi-scale orderless pooling of deep convolutional activation features", "author": ["Y. Gong", "L. Wang", "R. Guo", "S. Lazebnik"], "venue": "Computer Vision\u2013ECCV 2014, pages 392\u2013407. Springer", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2014}, {"title": "Multi-digit number recognition from street view imagery using deep convolutional neural networks", "author": ["I.J. Goodfellow", "Y. Bulatov", "J. Ibarz", "S. Arnoud", "V. Shet"], "venue": "International Conference on Learning Representations (ICLR)", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2014}, {"title": "Maxout networks", "author": ["I.J. Goodfellow", "D. Warde-Farley", "M. Mirza", "A. Courville", "Y. Bengio"], "venue": "The 30th International Conference on Machine Learning (ICML)", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2013}, {"title": "Batch normalization: Accelerating deep network training by reducing internal covariate shift", "author": ["S. Ioffe", "C. Szegedy"], "venue": "International Conference on Machine Learning (ICML)", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2015}, {"title": "Adaptive mixtures of local experts", "author": ["R.A. Jacobs", "M.I. Jordan", "S.J. Nowlan", "G.E. Hinton"], "venue": "Neural computation, 3(1):79\u201387", "citeRegEx": "6", "shortCiteRegEx": null, "year": 1991}, {"title": "Learning multiple layers of features from tiny images", "author": ["A. Krizhevsky", "G. Hinton"], "venue": "Computer Science Department, University of Toronto, Tech. Rep, 1(4):7", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2009}, {"title": "Gradientbased learning applied to document recognition", "author": ["Y. LeCun", "L. Bottou", "Y. Bengio", "P. Haffner"], "venue": "Proceedings of the IEEE, 86(11):2278\u20132324", "citeRegEx": "8", "shortCiteRegEx": null, "year": 1998}, {"title": "Deeplysupervised nets", "author": ["C.-Y. Lee", "S. Xie", "P. Gallagher", "Z. Zhang", "Z. Tu"], "venue": "Proceedings of AISTATS", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2015}, {"title": "Recurrent convolutional neural network for object recognition", "author": ["M. Liang", "X. Hu"], "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 3367\u20133375", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2015}, {"title": "On the importance of normalisation layers in deep learning with piecewise linear activation units", "author": ["Z. Liao", "G. Carneiro"], "venue": "CoRR, abs/1508.00330", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2015}, {"title": "Network in network", "author": ["M. Lin", "Q. Chen", "S. Yan"], "venue": "International Conference on Learning Representations (ICLR)", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2013}, {"title": "Catastrophic interference in connectionist networks: The sequential learning problem", "author": ["M. McCloskey", "N.J. Cohen"], "venue": "The psychology of learning and motivation, 24(109-165):92", "citeRegEx": "13", "shortCiteRegEx": null, "year": 1989}, {"title": "On the number of linear regions of deep neural networks", "author": ["G.F. Montufar", "R. Pascanu", "K. Cho", "Y. Bengio"], "venue": "Advances in Neural Information Processing Systems (NIPS), pages 2924\u20132932", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2014}, {"title": "Rectified linear units improve restricted boltzmann machines", "author": ["V. Nair", "G.E. Hinton"], "venue": "Proceedings of the 27th International Conference on Machine Learning (ICML), pages 807\u2013814", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2010}, {"title": "Reading digits in natural images with unsupervised feature learning", "author": ["Y. Netzer", "T. Wang", "A. Coates", "A. Bissacco", "B. Wu", "A.Y. Ng"], "venue": "NIPS workshop on deep learning and unsupervised feature learning, volume 2011, page 5. Granada, Spain", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2011}, {"title": "et al", "author": ["O. Russakovsky", "J. Deng", "H. Su", "J. Krause", "S. Satheesh", "S. Ma", "Z. Huang", "A. Karpathy", "A. Khosla", "M. Bernstein"], "venue": "Imagenet large scale visual recognition challenge. International Journal of Computer Vision, pages 1\u201342", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2014}, {"title": "Robust object recognition with cortex-like mechanisms", "author": ["T. Serre", "L. Wolf", "S. Bileschi", "M. Riesenhuber", "T. Poggio"], "venue": "Pattern Analysis and Machine Intelligence, IEEE Transactions on, 29(3):411\u2013426", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2007}, {"title": "Very deep convolutional networks for large-scale image recognition", "author": ["K. Simonyan", "A. Zisserman"], "venue": "International Conference on Learning Representations (ICLR)", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2015}, {"title": "Dropout: A simple way to prevent neural networks from overfitting", "author": ["N. Srivastava", "G. Hinton", "A. Krizhevsky", "I. Sutskever", "R. Salakhutdinov"], "venue": "The Journal of Machine Learning Research, 15(1):1929\u20131958", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2014}, {"title": "Discriminative transfer learning with tree-based priors", "author": ["N. Srivastava", "R.R. Salakhutdinov"], "venue": "Advances in Neural Information Processing Systems (NIPS), pages 2094\u20132102", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2013}, {"title": "Understanding locally competitive networks", "author": ["R.K. Srivastava", "J. Masci", "F. Gomez", "J. Schmidhuber"], "venue": "International Conference on Learning Representations (ICLR)", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2015}, {"title": "Compete to compute", "author": ["R.K. Srivastava", "J. Masci", "S. Kazerounian", "F. Gomez", "J. Schmidhuber"], "venue": "Advances in Neural Information Processing Systems (NIPS), pages 2310\u20132318", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2013}, {"title": "Going deeper with convolutions", "author": ["C. Szegedy", "W. Liu", "Y. Jia", "P. Sermanet", "S. Reed", "D. Anguelov", "D. Erhan", "V. Vanhoucke", "A. Rabinovich"], "venue": "In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR),", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2015}, {"title": "Regularization of neural networks using dropconnect", "author": ["L. Wan", "M. Zeiler", "S. Zhang", "Y.L. Cun", "R. Fergus"], "venue": "Proceedings of the 30th International Conference on Machine Learning (ICML), pages 1058\u20131066", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2013}, {"title": "Learning to compare image patches via convolutional neural networks", "author": ["S. Zagoruyko", "N. Komodakis"], "venue": "The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2015}, {"title": "Stochastic pooling for regularization of deep convolutional neural networks", "author": ["M.D. Zeiler", "R. Fergus"], "venue": "International Conference on Learning Representations (ICLR)", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2013}], "referenceMentions": [{"referenceID": 21, "context": "The use of competitive activation units in deep convolutional neural networks (ConvNets) is generally understood as a way of building one network by the combination of multiple sub-networks, where each one is capable of solving a simpler task when compared to the complexity of the original problem involving the whole dataset [22].", "startOffset": 327, "endOffset": 331}, {"referenceID": 5, "context": "Similar ideas have been explored in the past using multi-layer perceptron models [6], but there is a resurgence in the use of competitive activation units in deep ConvNets [23, 22].", "startOffset": 81, "endOffset": 84}, {"referenceID": 22, "context": "Similar ideas have been explored in the past using multi-layer perceptron models [6], but there is a resurgence in the use of competitive activation units in deep ConvNets [23, 22].", "startOffset": 172, "endOffset": 180}, {"referenceID": 21, "context": "Similar ideas have been explored in the past using multi-layer perceptron models [6], but there is a resurgence in the use of competitive activation units in deep ConvNets [23, 22].", "startOffset": 172, "endOffset": 180}, {"referenceID": 0, "context": "For instance, rectified linear unit (ReLU) [1] promotes a competition between the input sum (usually computed from the output of convolutional layers) and a fixed value of 0, while maxout [4] and local winner-take-all (LWTA) [23] explore an explicit competition among the input units.", "startOffset": 43, "endOffset": 46}, {"referenceID": 3, "context": "For instance, rectified linear unit (ReLU) [1] promotes a competition between the input sum (usually computed from the output of convolutional layers) and a fixed value of 0, while maxout [4] and local winner-take-all (LWTA) [23] explore an explicit competition among the input units.", "startOffset": 188, "endOffset": 191}, {"referenceID": 22, "context": "For instance, rectified linear unit (ReLU) [1] promotes a competition between the input sum (usually computed from the output of convolutional layers) and a fixed value of 0, while maxout [4] and local winner-take-all (LWTA) [23] explore an explicit competition among the input units.", "startOffset": 225, "endOffset": 229}, {"referenceID": 21, "context": "[22], these competitive activation units allow the formation of sub-networks that respond similarly to similar input patterns, which facilitates training [1, 4, 23]", "startOffset": 0, "endOffset": 4}, {"referenceID": 0, "context": "[22], these competitive activation units allow the formation of sub-networks that respond similarly to similar input patterns, which facilitates training [1, 4, 23]", "startOffset": 154, "endOffset": 164}, {"referenceID": 3, "context": "[22], these competitive activation units allow the formation of sub-networks that respond similarly to similar input patterns, which facilitates training [1, 4, 23]", "startOffset": 154, "endOffset": 164}, {"referenceID": 22, "context": "[22], these competitive activation units allow the formation of sub-networks that respond similarly to similar input patterns, which facilitates training [1, 4, 23]", "startOffset": 154, "endOffset": 164}, {"referenceID": 23, "context": "(c) Original inception module [24]", "startOffset": 30, "endOffset": 34}, {"referenceID": 21, "context": "and generally produces superior classification results [22].", "startOffset": 55, "endOffset": 59}, {"referenceID": 23, "context": "Our idea has been inspired by the recently proposed inception module [24], which currently produces state-of-the-art results on the ILSVRC 2014", "startOffset": 69, "endOffset": 73}, {"referenceID": 16, "context": "classification and detection challenges [17].", "startOffset": 40, "endOffset": 44}, {"referenceID": 1, "context": "1, where we have the data in the input layer filtered in parallel by a set of multi-scale convolutional filters [2, 24, 27].", "startOffset": 112, "endOffset": 123}, {"referenceID": 23, "context": "1, where we have the data in the input layer filtered in parallel by a set of multi-scale convolutional filters [2, 24, 27].", "startOffset": 112, "endOffset": 123}, {"referenceID": 25, "context": "1, where we have the data in the input layer filtered in parallel by a set of multi-scale convolutional filters [2, 24, 27].", "startOffset": 112, "endOffset": 123}, {"referenceID": 4, "context": "Then the output of each scale of the convolutional layer passes through a batch normalisation unit (BNU) [5] that weights the importance of each scale and also pre-conditions the model (note that the pre-conditioning ability of BNUs in ConvNets containing piece-wise linear activation units has recently been empirically shown in [11]).", "startOffset": 105, "endOffset": 108}, {"referenceID": 10, "context": "Then the output of each scale of the convolutional layer passes through a batch normalisation unit (BNU) [5] that weights the importance of each scale and also pre-conditions the model (note that the pre-conditioning ability of BNUs in ConvNets containing piece-wise linear activation units has recently been empirically shown in [11]).", "startOffset": 330, "endOffset": 334}, {"referenceID": 3, "context": "Finally, the multi-scale filter outputs, weighted by BNU, are joined with a maxout unit [4] that reduces the dimensionality of the joint filter outputs and promotes competition among the multi-scale filters, which prevents filter co-adaptation and allows the formation of multiple sub-networks.", "startOffset": 88, "endOffset": 91}, {"referenceID": 6, "context": "We show that the introduction of our proposal module in a typical deep ConvNet produces the best results in the field for the benchmark datasets CIFAR-10 [7], CIFAR-100 [7], and street view house number (SVHN) [16], while producing competitive results for MNIST [8].", "startOffset": 154, "endOffset": 157}, {"referenceID": 6, "context": "We show that the introduction of our proposal module in a typical deep ConvNet produces the best results in the field for the benchmark datasets CIFAR-10 [7], CIFAR-100 [7], and street view house number (SVHN) [16], while producing competitive results for MNIST [8].", "startOffset": 169, "endOffset": 172}, {"referenceID": 15, "context": "We show that the introduction of our proposal module in a typical deep ConvNet produces the best results in the field for the benchmark datasets CIFAR-10 [7], CIFAR-100 [7], and street view house number (SVHN) [16], while producing competitive results for MNIST [8].", "startOffset": 210, "endOffset": 214}, {"referenceID": 7, "context": "We show that the introduction of our proposal module in a typical deep ConvNet produces the best results in the field for the benchmark datasets CIFAR-10 [7], CIFAR-100 [7], and street view house number (SVHN) [16], while producing competitive results for MNIST [8].", "startOffset": 262, "endOffset": 265}, {"referenceID": 13, "context": "One of the main reasons behind the outstanding performance of deep ConvNets is attributed to the use of competitive activation units in the form of piece-wise linear functions [14, 22], such as ReLU [1], maxout [4] and LWTA [23] (see Fig.", "startOffset": 176, "endOffset": 184}, {"referenceID": 21, "context": "One of the main reasons behind the outstanding performance of deep ConvNets is attributed to the use of competitive activation units in the form of piece-wise linear functions [14, 22], such as ReLU [1], maxout [4] and LWTA [23] (see Fig.", "startOffset": 176, "endOffset": 184}, {"referenceID": 0, "context": "One of the main reasons behind the outstanding performance of deep ConvNets is attributed to the use of competitive activation units in the form of piece-wise linear functions [14, 22], such as ReLU [1], maxout [4] and LWTA [23] (see Fig.", "startOffset": 199, "endOffset": 202}, {"referenceID": 3, "context": "One of the main reasons behind the outstanding performance of deep ConvNets is attributed to the use of competitive activation units in the form of piece-wise linear functions [14, 22], such as ReLU [1], maxout [4] and LWTA [23] (see Fig.", "startOffset": 211, "endOffset": 214}, {"referenceID": 22, "context": "One of the main reasons behind the outstanding performance of deep ConvNets is attributed to the use of competitive activation units in the form of piece-wise linear functions [14, 22], such as ReLU [1], maxout [4] and LWTA [23] (see Fig.", "startOffset": 224, "endOffset": 228}, {"referenceID": 21, "context": "In general, these activation functions enable the formation of sub-networks that respond consistently to similar input patterns [22], dividing the input data points (and more generally the training space) into regions [14], where classifiers and regressors can be learned more effectively given that the sub-problems in each of these regions is simpler than the one involving the whole training set.", "startOffset": 128, "endOffset": 132}, {"referenceID": 13, "context": "In general, these activation functions enable the formation of sub-networks that respond consistently to similar input patterns [22], dividing the input data points (and more generally the training space) into regions [14], where classifiers and regressors can be learned more effectively given that the sub-problems in each of these regions is simpler than the one involving the whole training set.", "startOffset": 218, "endOffset": 222}, {"referenceID": 0, "context": "In addition, the joint training of the subnetworks present in such deep ConvNets represents a useful regularization method [1, 4, 23].", "startOffset": 123, "endOffset": 133}, {"referenceID": 3, "context": "In addition, the joint training of the subnetworks present in such deep ConvNets represents a useful regularization method [1, 4, 23].", "startOffset": 123, "endOffset": 133}, {"referenceID": 22, "context": "In addition, the joint training of the subnetworks present in such deep ConvNets represents a useful regularization method [1, 4, 23].", "startOffset": 123, "endOffset": 133}, {"referenceID": 4, "context": "An important aspect about deep ConvNets with competitive activation units is the fact that the use of batch normalization units (BNU) helps not only with respect to the convergence rate [5], but also with the preconditioning of the model by promoting an even distribution of the input data points, which results in the maximization of the number of the regions (and respective sub-networks) produced by the piece-wise linear activation functions [11].", "startOffset": 186, "endOffset": 189}, {"referenceID": 10, "context": "An important aspect about deep ConvNets with competitive activation units is the fact that the use of batch normalization units (BNU) helps not only with respect to the convergence rate [5], but also with the preconditioning of the model by promoting an even distribution of the input data points, which results in the maximization of the number of the regions (and respective sub-networks) produced by the piece-wise linear activation functions [11].", "startOffset": 446, "endOffset": 450}, {"referenceID": 10, "context": "Furthermore, training ConvNets with competitive activation units [11, 22] usually involves the use of dropout [20] that consists of a regularization method that prevents filter coadaptation [20], which is a particularly important issue in such models, because filter co-adaptation can lead to a severe reduction in the number of the sub-networks that can be formed during training.", "startOffset": 65, "endOffset": 73}, {"referenceID": 21, "context": "Furthermore, training ConvNets with competitive activation units [11, 22] usually involves the use of dropout [20] that consists of a regularization method that prevents filter coadaptation [20], which is a particularly important issue in such models, because filter co-adaptation can lead to a severe reduction in the number of the sub-networks that can be formed during training.", "startOffset": 65, "endOffset": 73}, {"referenceID": 19, "context": "Furthermore, training ConvNets with competitive activation units [11, 22] usually involves the use of dropout [20] that consists of a regularization method that prevents filter coadaptation [20], which is a particularly important issue in such models, because filter co-adaptation can lead to a severe reduction in the number of the sub-networks that can be formed during training.", "startOffset": 110, "endOffset": 114}, {"referenceID": 19, "context": "Furthermore, training ConvNets with competitive activation units [11, 22] usually involves the use of dropout [20] that consists of a regularization method that prevents filter coadaptation [20], which is a particularly important issue in such models, because filter co-adaptation can lead to a severe reduction in the number of the sub-networks that can be formed during training.", "startOffset": 190, "endOffset": 194}, {"referenceID": 0, "context": "ReLU [1] (a) is active when the input is bigger than 0, LWTA [23]", "startOffset": 5, "endOffset": 8}, {"referenceID": 22, "context": "ReLU [1] (a) is active when the input is bigger than 0, LWTA [23]", "startOffset": 61, "endOffset": 65}, {"referenceID": 3, "context": "(b) activates only the node that has the maximum value (setting to zero the other ones), and maxout [4] (c) has only one output containing the maximum value from the input.", "startOffset": 100, "endOffset": 103}, {"referenceID": 21, "context": "1 of [22].", "startOffset": 5, "endOffset": 9}, {"referenceID": 2, "context": "Another aspect of the current research on deep ConvNets is the idea of making the network deeper, which has been shown to improve classification results [3].", "startOffset": 153, "endOffset": 156}, {"referenceID": 18, "context": "However, one of the main ideas being studied in the field is how to increase the depth of a ConvNet without necessarily increasing the complexity of the model parameter space [19, 24].", "startOffset": 175, "endOffset": 183}, {"referenceID": 23, "context": "However, one of the main ideas being studied in the field is how to increase the depth of a ConvNet without necessarily increasing the complexity of the model parameter space [19, 24].", "startOffset": 175, "endOffset": 183}, {"referenceID": 23, "context": "\u2019s model [24], this is achieved with the use of 1 \u00d7 1 convolutional filters [12] that are placed before each local filter present in the inception module in order to reduce the input dimensionality of the filter.", "startOffset": 9, "endOffset": 13}, {"referenceID": 11, "context": "\u2019s model [24], this is achieved with the use of 1 \u00d7 1 convolutional filters [12] that are placed before each local filter present in the inception module in order to reduce the input dimensionality of the filter.", "startOffset": 76, "endOffset": 80}, {"referenceID": 18, "context": "\u2019s approach [19], the idea is to use a large number of layers with convolutional filters of very small size (e.", "startOffset": 12, "endOffset": 16}, {"referenceID": 1, "context": "Finally, multi-scale filters in deep ConvNets is another important implementation that is increasingly being explored by several researchers [2, 24, 27].", "startOffset": 141, "endOffset": 152}, {"referenceID": 23, "context": "Finally, multi-scale filters in deep ConvNets is another important implementation that is increasingly being explored by several researchers [2, 24, 27].", "startOffset": 141, "endOffset": 152}, {"referenceID": 25, "context": "Finally, multi-scale filters in deep ConvNets is another important implementation that is increasingly being explored by several researchers [2, 24, 27].", "startOffset": 141, "endOffset": 152}, {"referenceID": 17, "context": "Essentially, multiscale filtering follows a neuroscience model [18] that suggests that the input image data should be processed at several scales and then pooled together, so that the deeper processing stages can become robust to scale changes [24].", "startOffset": 63, "endOffset": 67}, {"referenceID": 23, "context": "Essentially, multiscale filtering follows a neuroscience model [18] that suggests that the input image data should be processed at several scales and then pooled together, so that the deeper processing stages can become robust to scale changes [24].", "startOffset": 244, "endOffset": 248}, {"referenceID": 11, "context": "The models being proposed in this paper follow the structure of the the NIN model [12], and is in general defined as follows:", "startOffset": 82, "endOffset": 86}, {"referenceID": 11, "context": ") denotes an averaging pooling unit followed by a softmax activation function [12], and the network has blocks represented by l \u2208 {1, .", "startOffset": 78, "endOffset": 82}, {"referenceID": 3, "context": ") represents the maxout activation function [4], the convolutional filters of the module are represented by the weight matrices W2k\u22121 for k \u2208 {1, .", "startOffset": 44, "endOffset": 47}, {"referenceID": 4, "context": ", filters of size 2k \u2212 1 \u00d7 2k \u2212 1 \u00d7 #filters, with #filters denoting the number of 2-D filters present in W), which means that each module n in block l has Kl different filter sizes and #filters different filters, \u03b3 and \u03b2 represent the batch normalization scaling and shifting parameters [5], and p3\u00d73(xi\u00b11) represents a max pooling operator on the 3 \u00d7 3 subset of the input data for layer l centred at i \u2208 \u03a9, i.", "startOffset": 288, "endOffset": 291}, {"referenceID": 23, "context": "3-(b)) because of its similarity to the original inception module [24].", "startOffset": 66, "endOffset": 70}, {"referenceID": 0, "context": ") in (2) denotes the concatenation of the input parameters; 2) a 1\u00d7 1 convolution is applied to the input x before a second round of convolutions with filter sizes larger than or equal to 3\u00d73; and 3) a ReLU activation function [1] is present after each convolutional layer.", "startOffset": 227, "endOffset": 230}, {"referenceID": 11, "context": "Note that all models are inspired by NIN [12], GoogLeNet [24], and MIM [11].", "startOffset": 41, "endOffset": 45}, {"referenceID": 23, "context": "Note that all models are inspired by NIN [12], GoogLeNet [24], and MIM [11].", "startOffset": 57, "endOffset": 61}, {"referenceID": 10, "context": "Note that all models are inspired by NIN [12], GoogLeNet [24], and MIM [11].", "startOffset": 71, "endOffset": 75}, {"referenceID": 23, "context": "For the inception style model, we ensure that the number of output units in each module is the same as for the competitive inception and competitive multi-scale convolution, and we also use a 3 \u00d7 3 max-pooling path in each module, as used in the original inception module [24].", "startOffset": 272, "endOffset": 276}, {"referenceID": 23, "context": "[24] and include a relatively larger number of 3\u00d73 and 5\u00d75 filters in each module, compared to filters of other sizes (e.", "startOffset": 0, "endOffset": 4}, {"referenceID": 23, "context": "An important distinction between the original GoogLeNet [24] and the inception style network in Fig.", "startOffset": 56, "endOffset": 60}, {"referenceID": 11, "context": "3-(c) is the fact that we replace the fully connected layer in the last layer by a single 3 \u00d7 3 convolution node in the last module, followed by an average pooling and a softmax unit, similarly to the NIN model [12].", "startOffset": 211, "endOffset": 215}, {"referenceID": 0, "context": "The main reason being explored in the field to justify the use of competitive activation units [1, 4, 23] is the fact that they build a network formed by multiple underlying sub-networks [22].", "startOffset": 95, "endOffset": 105}, {"referenceID": 3, "context": "The main reason being explored in the field to justify the use of competitive activation units [1, 4, 23] is the fact that they build a network formed by multiple underlying sub-networks [22].", "startOffset": 95, "endOffset": 105}, {"referenceID": 22, "context": "The main reason being explored in the field to justify the use of competitive activation units [1, 4, 23] is the fact that they build a network formed by multiple underlying sub-networks [22].", "startOffset": 95, "endOffset": 105}, {"referenceID": 21, "context": "The main reason being explored in the field to justify the use of competitive activation units [1, 4, 23] is the fact that they build a network formed by multiple underlying sub-networks [22].", "startOffset": 187, "endOffset": 191}, {"referenceID": 13, "context": "More clearly, given that these activation units consist of piece-wise linear functions, it has been shown that the composition of several layers containing such units, divide the input space in a number of regions that is exponentially proportional to the number of network layers [14], where sub-networks will be trained with the samples that fall into one of these regions, and as a result become specialised to the problem in that particular region [22], where overfitting can be avoided because these sub-networks must share their parameters with one another [22].", "startOffset": 281, "endOffset": 285}, {"referenceID": 21, "context": "More clearly, given that these activation units consist of piece-wise linear functions, it has been shown that the composition of several layers containing such units, divide the input space in a number of regions that is exponentially proportional to the number of network layers [14], where sub-networks will be trained with the samples that fall into one of these regions, and as a result become specialised to the problem in that particular region [22], where overfitting can be avoided because these sub-networks must share their parameters with one another [22].", "startOffset": 452, "endOffset": 456}, {"referenceID": 21, "context": "More clearly, given that these activation units consist of piece-wise linear functions, it has been shown that the composition of several layers containing such units, divide the input space in a number of regions that is exponentially proportional to the number of network layers [14], where sub-networks will be trained with the samples that fall into one of these regions, and as a result become specialised to the problem in that particular region [22], where overfitting can be avoided because these sub-networks must share their parameters with one another [22].", "startOffset": 563, "endOffset": 567}, {"referenceID": 10, "context": "Recently, Liao and Carneiro [11] propose a solution to this problem based on the use of BNU [5] that distributes the training samples evenly over the regions formed by the competitive unit, allowing the training to use different sets of training points for each region of the competitive unit, resulting in the formation of an exponential number of sub-networks.", "startOffset": 28, "endOffset": 32}, {"referenceID": 4, "context": "Recently, Liao and Carneiro [11] propose a solution to this problem based on the use of BNU [5] that distributes the training samples evenly over the regions formed by the competitive unit, allowing the training to use different sets of training points for each region of the competitive unit, resulting in the formation of an exponential number of sub-networks.", "startOffset": 92, "endOffset": 95}, {"referenceID": 10, "context": "However, there is still a potential problem with that approach [11], which is that the underlying convolutional filters are trained using feature spaces of the same size (i.", "startOffset": 63, "endOffset": 67}, {"referenceID": 14, "context": "the inception style model uses ReLU [15] after all convolutional layers, the number of filters per convolutional node is represented by the number in brackets, and these models assume a 10-class classification problem.", "startOffset": 36, "endOffset": 40}, {"referenceID": 10, "context": "The competitive multi-scale convolution module proposed in this paper represents a way to fix the issue introduced above [11].", "startOffset": 121, "endOffset": 125}, {"referenceID": 24, "context": "In some sense, this idea is similar to DropConnect [26], which, during training, drops to zero the weights of randomly picked network connections with the goal of training regularization.", "startOffset": 51, "endOffset": 55}, {"referenceID": 6, "context": "We quantitatively measure the performance of our proposed models Competitive Multi-scale Convolution and Competitive Inception on four computer vision/machine learning benchmark datasets: CIFAR-10[7], CIFAR100[7], MNIST [8] and SVHN [16].", "startOffset": 196, "endOffset": 199}, {"referenceID": 6, "context": "We quantitatively measure the performance of our proposed models Competitive Multi-scale Convolution and Competitive Inception on four computer vision/machine learning benchmark datasets: CIFAR-10[7], CIFAR100[7], MNIST [8] and SVHN [16].", "startOffset": 209, "endOffset": 212}, {"referenceID": 7, "context": "We quantitatively measure the performance of our proposed models Competitive Multi-scale Convolution and Competitive Inception on four computer vision/machine learning benchmark datasets: CIFAR-10[7], CIFAR100[7], MNIST [8] and SVHN [16].", "startOffset": 220, "endOffset": 223}, {"referenceID": 15, "context": "We quantitatively measure the performance of our proposed models Competitive Multi-scale Convolution and Competitive Inception on four computer vision/machine learning benchmark datasets: CIFAR-10[7], CIFAR100[7], MNIST [8] and SVHN [16].", "startOffset": 233, "endOffset": 237}, {"referenceID": 6, "context": "The CIFAR-10 [7] dataset contains 60000 images of 10 commonly seen object categories (e.", "startOffset": 13, "endOffset": 16}, {"referenceID": 6, "context": "The CIFAR100 [7] dataset extends CIFAR-10 by increasing the number of categories to 100, whereas the total number of images remains the same, so the CIFAR-100 dataset is considered as a harder classification problem than CIFAR-10 since it contains 10 times less images per class and 10 times more categories.", "startOffset": 13, "endOffset": 16}, {"referenceID": 7, "context": "The well-known MNIST [8] dataset contains 28 \u00d7 28 grayscale images comprising 10 handwritten digits (from 0 to 9), where the dataset is divided into 60000 images for training and 10000 for testing, but note that the number of images per digit is not uniformly distributed.", "startOffset": 21, "endOffset": 24}, {"referenceID": 15, "context": "Finally, the Street View House Number (SVHN) [16] is also a digit classification benchmark dataset that contains 600000 32\u00d732 RGB images of printed digits (from 0 to 9) cropped from pictures of house number plates.", "startOffset": 45, "endOffset": 49}, {"referenceID": 11, "context": "Note that all models in Tables 1 and 2 are constrained to have the same numbers of input channels and output channels in each module, and all networks contain three blocks [12], each with three modules (so there is a total of nine modules in each network), as shown in Fig.", "startOffset": 172, "endOffset": 176}, {"referenceID": 10, "context": "Notice that this configuration implies that we replace the multi-scale filters by the filter of the largest size of the module in each node, which is a configuration similar to the recently proposed MIM model [11].", "startOffset": 209, "endOffset": 213}, {"referenceID": 24, "context": "1 is if the deterministic masking provided by our proposed Competitive Multi-scale Convolution module is more effective at avoiding filter co-adaptation than the stochastic masking provided by DropConnect [26].", "startOffset": 205, "endOffset": 209}, {"referenceID": 26, "context": "Stochastic Pooling [28] proposes a regularization based on a replacement of the deterministic pooling (e.", "startOffset": 19, "endOffset": 23}, {"referenceID": 3, "context": "Maxout Networks [4] introduces a piece-wise linear activation unit that is used together with dropout training [20] and is introduced in Fig.", "startOffset": 16, "endOffset": 19}, {"referenceID": 19, "context": "Maxout Networks [4] introduces a piece-wise linear activation unit that is used together with dropout training [20] and is introduced in Fig.", "startOffset": 111, "endOffset": 115}, {"referenceID": 11, "context": "The Network in Network (NIN) [12] model consists of the introduction of multilayer perceptrons as activation functions to be placed between convolution layers, and the replacement of a final fully connected layer by average pooling, where the number of output channels represent the final number of classes in the classification problem.", "startOffset": 29, "endOffset": 33}, {"referenceID": 8, "context": "Deeply-supervised nets [9] introduce explicit training objectives to all hidden layers, in addition to the back-propagated errors from the last softmax layer.", "startOffset": 23, "endOffset": 26}, {"referenceID": 9, "context": "The use of a recurrent structure that replaces the purely feed-forward structure in ConvNets is explored by the model RCNN [10].", "startOffset": 123, "endOffset": 127}, {"referenceID": 10, "context": "An extension of the NIN model based on the use of maxout activation function instead of the multilayer perceptron is introduced in the MIM model [11], which also shows that the use of batch normalization units are crucial for allowing an effective training of several single-scale filters that are joined by maxout units.", "startOffset": 145, "endOffset": 149}, {"referenceID": 20, "context": "Finally, the Tree based Priors [21] model proposes a training method for classes with few samples, using a generative prior that is learned from the data and shared between related classes during the model learning.", "startOffset": 31, "endOffset": 35}, {"referenceID": 6, "context": "The comparison on CIFAR-10 [7] dataset is shown in Tab.", "startOffset": 27, "endOffset": 30}, {"referenceID": 6, "context": "The results on CIFAR-100[7] dataset are displayed in Tab.", "startOffset": 24, "endOffset": 27}, {"referenceID": 7, "context": "Table 5 shows the results on MNIST [8], where it is worth reporting that the best result (over the five trained models) produced by our Competitive Multi-scale Convolution model is a test error of 0.", "startOffset": 35, "endOffset": 38}, {"referenceID": 10, "context": "31% MIM [11] 8.", "startOffset": 8, "endOffset": 12}, {"referenceID": 9, "context": "20% RCNN-160 [10] 8.", "startOffset": 13, "endOffset": 17}, {"referenceID": 8, "context": "Deeply-supervised nets [9] 9.", "startOffset": 23, "endOffset": 26}, {"referenceID": 11, "context": "Network in Network [12] 10.", "startOffset": 19, "endOffset": 23}, {"referenceID": 3, "context": "Maxout Networks [4] 11.", "startOffset": 16, "endOffset": 19}, {"referenceID": 26, "context": "Stochastic Pooling [28] 15.", "startOffset": 19, "endOffset": 23}, {"referenceID": 6, "context": "CIFAR-10 [7].", "startOffset": 9, "endOffset": 12}, {"referenceID": 10, "context": "25% MIM [11] 29.", "startOffset": 8, "endOffset": 12}, {"referenceID": 9, "context": "20% RCNN-160 [10] 31.", "startOffset": 13, "endOffset": 17}, {"referenceID": 8, "context": "Deeply-supervised nets [9] 34.", "startOffset": 23, "endOffset": 26}, {"referenceID": 11, "context": "Network in Network [12] 35.", "startOffset": 19, "endOffset": 23}, {"referenceID": 20, "context": "Tree based Priors [21] 36.", "startOffset": 18, "endOffset": 22}, {"referenceID": 3, "context": "Maxout Networks [4] 38.", "startOffset": 16, "endOffset": 19}, {"referenceID": 26, "context": "Stochastic Pooling [28] 42.", "startOffset": 19, "endOffset": 23}, {"referenceID": 6, "context": "proposed models (highlighted) and the state-of-the-art methods on CIFAR-100 [7].", "startOffset": 76, "endOffset": 79}, {"referenceID": 9, "context": "RCNN-96 [10] 0.", "startOffset": 8, "endOffset": 12}, {"referenceID": 10, "context": "04% MIM [11] 0.", "startOffset": 8, "endOffset": 12}, {"referenceID": 8, "context": "03% Deeply-supervised nets [9] 0.", "startOffset": 27, "endOffset": 30}, {"referenceID": 11, "context": "02% Network in Network [12] 0.", "startOffset": 23, "endOffset": 27}, {"referenceID": 3, "context": "Maxout+Dropout [4] 0.", "startOffset": 15, "endOffset": 18}, {"referenceID": 26, "context": "Stochastic Pooling [28] 0.", "startOffset": 19, "endOffset": 23}, {"referenceID": 7, "context": "MNIST [8].", "startOffset": 6, "endOffset": 9}, {"referenceID": 9, "context": "07% RCNN-192 [10] 1.", "startOffset": 13, "endOffset": 17}, {"referenceID": 8, "context": "05% Deeply-supervised nets [9] 1.", "startOffset": 27, "endOffset": 30}, {"referenceID": 24, "context": "Drop-connect [26] 1.", "startOffset": 13, "endOffset": 17}, {"referenceID": 10, "context": "MIM [11] 1.", "startOffset": 4, "endOffset": 8}, {"referenceID": 11, "context": "08% Network in Network [12] 2.", "startOffset": 23, "endOffset": 27}, {"referenceID": 3, "context": "Maxout+Dropout [4] 2.", "startOffset": 15, "endOffset": 18}, {"referenceID": 26, "context": "Stochastic Pooling [28] 2.", "startOffset": 19, "endOffset": 23}, {"referenceID": 15, "context": "proposed models (highlighted) and the state-of-the-art methods on SVHN [16].", "startOffset": 71, "endOffset": 75}, {"referenceID": 9, "context": "ter than the single result from Liang and Hu [10].", "startOffset": 45, "endOffset": 49}, {"referenceID": 15, "context": "Finally, the comparison on SVHN[16] dataset is shown in Table 6, where two out of the five models show test error results of 1.", "startOffset": 31, "endOffset": 35}, {"referenceID": 13, "context": "1, we assume that classification accuracy is a proxy for measuring the co-adaptation between filters within a single module, where the intuition is that if the filters joined by a maxout activation unit co-adapt and become similar to each other, a relatively small number of large regions in the input space will be formed, which results in few sub-networks to train, with each sub-network becoming less specialized to its region [14, 22].", "startOffset": 430, "endOffset": 438}, {"referenceID": 21, "context": "1, we assume that classification accuracy is a proxy for measuring the co-adaptation between filters within a single module, where the intuition is that if the filters joined by a maxout activation unit co-adapt and become similar to each other, a relatively small number of large regions in the input space will be formed, which results in few sub-networks to train, with each sub-network becoming less specialized to its region [14, 22].", "startOffset": 430, "endOffset": 438}, {"referenceID": 24, "context": "Nevertheless, the reason behind the worse performance of the stochastic mapping may be due to the fact that DropConnect has been designed for the fully connected layers only [26], while our test bed for the comparison is set in the convolutional filters.", "startOffset": 174, "endOffset": 178}, {"referenceID": 12, "context": "However, the convolution filters are of small dimensions, and each of our maxout unit controls 4 to 5 filters at most, so such masking scheme over small weights matrix could result in \u201ccatastrophic forgetting\u201d [13] which explains why the Competitive DropConnect Single-scale Convolution performs even worse than Competitive Singlescale Convolution on CIFAR-10.", "startOffset": 210, "endOffset": 214}, {"referenceID": 10, "context": "An analysis of these results also allows us to conclude that the main competitors of our approach are the MIM [11] and RCNN [10] models, where the MIM method is quite related to our approach, but the RCNN method follows a quite different strategy.", "startOffset": 110, "endOffset": 114}, {"referenceID": 9, "context": "An analysis of these results also allows us to conclude that the main competitors of our approach are the MIM [11] and RCNN [10] models, where the MIM method is quite related to our approach, but the RCNN method follows a quite different strategy.", "startOffset": 124, "endOffset": 128}, {"referenceID": 9, "context": "It is important to note that such modules can be applied in several types of deep learning networks, and we plan to apply it to other types of models, such as the recurrent neural network [10].", "startOffset": 188, "endOffset": 192}], "year": 2015, "abstractText": "In this paper, we introduce a new deep convolutional neural network (ConvNet) module that promotes competition among a set of multi-scale convolutional filters. This new module is inspired by the inception module, where we replace the original collaborative pooling stage (consisting of a concatenation of the multi-scale filter outputs) by a competitive pooling represented by a maxout activation unit. This extension has the following two objectives: 1) the selection of the maximum response among the multi-scale filters prevents filter co-adaptation and allows the formation of multiple sub-networks within the same model, which has been shown to facilitate the training of complex learning problems; and 2) the maxout unit reduces the dimensionality of the outputs from the multi-scale filters. We show that the use of our proposed module in typical deep ConvNets produces classification results that are either better than or comparable to the state of the art on the following benchmark datasets: MNIST, CIFAR-10, CIFAR-100 and SVHN.", "creator": "LaTeX with hyperref package"}}}