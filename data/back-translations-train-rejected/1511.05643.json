{"id": "1511.05643", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "18-Nov-2015", "title": "A New Smooth Approximation to the Zero One Loss with a Probabilistic Interpretation", "abstract": "We examine a new form of smooth approximation to the zero one loss in which learning is performed using a reformulation of the widely used logistic function. Our approach is based on using the posterior mean of a novel generalized Beta-Bernoulli formulation. This leads to a generalized logistic function that approximates the zero one loss, but retains a probabilistic formulation conferring a number of useful properties. The approach is easily generalized to kernel logistic regression and easily integrated into methods for structured prediction. We present experiments in which we learn such models using an optimization method consisting of a combination of gradient descent and coordinate descent using localized grid search so as to escape from local minima. Our experiments indicate that optimization quality is improved when learning meta-parameters are themselves optimized using a validation set. Our experiments show improved performance relative to widely used logistic and hinge loss methods on a wide variety of problems ranging from standard UC Irvine and libSVM evaluation datasets to product review predictions and a visual information extraction task. We observe that the approach: 1) is more robust to outliers compared to the logistic and hinge losses; 2) outperforms comparable logistic and max margin models on larger scale benchmark problems; 3) when combined with Gaussian- Laplacian mixture prior on parameters the kernelized version of our formulation yields sparser solutions than Support Vector Machine classifiers; and 4) when integrated into a probabilistic structured prediction technique our approach provides more accurate probabilities yielding improved inference and increasing information extraction performance.", "histories": [["v1", "Wed, 18 Nov 2015 02:31:16 GMT  (546kb,D)", "http://arxiv.org/abs/1511.05643v1", "32 pages, 7 figures, 15 tables"]], "COMMENTS": "32 pages, 7 figures, 15 tables", "reviews": [], "SUBJECTS": "cs.CV cs.AI cs.IR cs.LG", "authors": ["md kamrul hasan", "christopher j pal"], "accepted": false, "id": "1511.05643"}, "pdf": {"name": "1511.05643.pdf", "metadata": {"source": "CRF", "title": "A New Smooth Approximation to the Zero One Loss with a Probabilistic Interpretation", "authors": ["Md Kamrul Hasan", "Christopher J. Pal"], "emails": ["md-kamrul.hasan@polymtl.ca,", "christopher.pal@polymtl.ca"], "sections": [{"heading": "1 Introduction", "text": "Loss of function is a standard method for solving many important learning problems. In classical statistical literature, this is known as Empirical Risk Minimization (ERM), where learning is lost by minimizing the average risk or loss of more than 151 1.05 643v 1 [cs.C V] 1 of the training data. Formally, this is a significantly reduced loss, and it is the loss of the model f (xi), ti). For now, we are focusing on the standard binary classification task, in which we encode the target class name as ti (1) and the model parameters as w (xi).Letting zi = tiwTxi, we can define the logistical, linear classification task in which we encode the target class name as ti (1)."}, {"heading": "2 Relevant Recent Work", "text": "It has been shown in [24] that it is possible to define a generalized logistic loss and produce a smooth approximation of the logistic loss, which includes the following formulationLglog (ti, xi; w, \u03b3) = 1\u03b3 log (1 \u2212 tiwTxi)], (7) Lglog (zi, \u03b3) = 1 log [1 + exp (1 \u2212 zi))], (8) such as these algorithms have a loss (1 \u2212 tiwTxi) and a displaced version of the usual logistic loss. We illustrate the way in which this construction can be used to approximate the loss of the hinge in Figure 3 (left).The maximum margin Bayesian network formulation in [16] also employs a smooth differential loss inspired by Huber loss, with a similar form tomin [1, zi]."}, {"heading": "3 Our Approach: Generalized Beta-Bernoulli Logistic Classification", "text": "We derive a novel form of logistic regression based on the formulation of a generalized sigmoid function (BB = BB = BB = 11), which results from an underlying Bernoulli model with a beta prediction. We also use a \u03b3 scaling factor to increase the sharpness of our approximation. First, let's consider the traditional and widespread formulation of logistic regression, which can often be derived from a probability model based on the Bernoulli distribution. Bernoulli's probability model has the form: P (1 \u2212 y), (11), in which y-prediction of logistic regression can be derived from a probability model, and which is the parameter of the model. Bernoulli distribution can be given in the standard exponential form family asP (y | V) = exp {log (1 \u2212 V) y + log {(1)}, which is natural in the parameter V (\u2212 1)."}, {"heading": "3.1 Parameter Estimation and Gradients", "text": "We now turn to the problem of estimating the parameters w, given data in the form of D = {yi, xi}, i = 1,.., n using our model. As we have defined a probabilistic model, we simply write the probability defined by our model as usual and then optimize the parameters by maximizing the log probability or minimizing the negative log probability. As we will go into more detail in Section 4, we use a modified form of the SLA optimization algorithm in which we slowly increase \u03b3 and implement gradient descend steps with coordinate descend as a grid search. For the descending part of the optimization, we need the gradients of our loss function and therefore give them following.Let us first consider the usual formulation of the conditional probability used in logistic regressionP ({yi}, w) ({xi}, w) = n = 1 \u00b5yii (1 \u2212 \u00b5i) (1 \u2212 yi), (23 \u00b5i)."}, {"heading": "3.2 Some Asymptotic Analysis", "text": "As we explained at the beginning of our discussion on parameter estimation, at the end of our optimization we will have a model with a large \u03b3. At a sufficiently large \u03b3, all predictions get their maximum or minimum probabilities that are possible under the \u03b2B\u03b3 model. If we set the maximum probability under the model equal to the True Positive Rate (TPR) (e.g. on training and / or validation data) and the maximum probability for the negative class equal to the True Negative Rate (TNR), we have the probability \u03b2 + (1 \u2212 w) = TPR, (27) 1 \u2212 w\u03b8B = TNR, (28) which allows us to conclude that this would be equivalent to the setting w = 2 \u2212 (TNR + TPR), (29) bsB = 1 \u2212 TNR2 \u2212 (TNR + TPR)."}, {"heading": "3.3 Learning hyper-parameters", "text": "In the previous section, we provided an asymptotic analysis of the expected values for w and \u03b8B. In the experiment section, we provide BBLR results for the use of asymptotic values of these two parameters together with cross-validated values for other hyperparameters, \u03bb being the regularization parameter described in section 4. However, it is also possible to learn these hyperparameters using the training set, the validation set, or both. In the following, we provide partial derivatives of the probability function (24) for these hyperparameters. dL dw = n \u2211 i = 1 (yi \u00b5i \u2212 1 \u2212 yi) d\u00b5i dw (31) with d\u00b5i dw = investB \u2212 1 \u2212 1 + exp (\u2212 \u03b3wTx) (32) The partial derivatives relating to investB and \u03b3 are d\u03b7B = w \u2212 \u2212 \u2212 i \u2212 1 (yi \u2212 yi \u2212 1) (\u2212 yi) (\u2212 yi) (\u2212 yi)."}, {"heading": "3.4 Kernel Beta-Bernoulli Classification", "text": "It is possible to convert the traditional logistic regression technique discussed above into a kernel logistic regression (KLR) by replacing the linear discriminant function, \u03b7 = wT x, with\u03b7 = f (a, x) = N \u2211 j = 1 ajK (x, xj), (36) where K (x, xj) is a kernel function and j is used as an index in the sum of all N training examples. To create our generalized beta-Bernoulli KLR model, we take a similar approach; in this case, however, we allow \u03b7 = \u03b3f (a, x). Thus, our kernel Bernoulli model can be written as follows: \u00b5K\u03b2B (a, x) = w (\u03b1 \u03b1 + \u03b2) + (1 \u2212 w) 1 + exp (\u2212 \u03b3f (a, x). (37) If we write f (a, x) = aTk (x), where \u2212 k (x) is a vector of the corresponding value (K), then (nb) is (K)."}, {"heading": "4 Optimization and Algorithms", "text": "As we have discussed in the relevant recent work section above, the work of [14] \u03b2 = has fixed that their SLA algorithm is applied to Lsig (zi, \u03b3) using a number of other techniques in terms of both true 0-1 loss minimizing performance and runtime. Since our generalized beta-Bernoulli loss, LBB\u03b3 (zi, \u03b3) is a different kind of gentle approach to the 0-1 loss, we therefore use a variation of their SLA algorithm to optimize BB\u03b3 loss. Remember that when considering our generalized beta-Bernoulli logistical loss with the directly defined sigmoidal loss used in the SLA work of [14], it becomes clear that the BBLR formulation has three additional hyper parameters, {\u03b1, w}. These additional parameters we control the locations of the plateaus of our function and these plateaus have well-defined interpretations in terms of probabilities."}, {"heading": "4.1 Our SLA Algorithm Meta-optimization (SLAM)", "text": "Here we present our meta-optimization parameters and various other modifications of the SLA approach of the SLA technique. The SLA algorithm proposed in [14] can be broken down into two different parts; an outer loop that initializes a model then enters a loop in which one slowly increases the \u03b3 factor of its sigmoidal loss and repeatedly calls an algorithm that they call the Range 2 factor. The Range optimization part consists of two stages. Stage 1 is a standard gradient optimization with decreasing learning rate (using the new \u03b3 factor). Stage 2 Probes each parameter wi in a radius R with a one-dimensional grid search to determine whether the loss can be further reduced, i.e. a coordinate descent implemented on a number of grid points. We offer a slightly modified form of the outer loop of its algorithms in which we have explained the initial parameters of the model to them."}, {"heading": "5 Experimental Setup and Results", "text": "Below we present the results for three different groups of benchmark problems: (1) a selection from the repository of the University of California Irvine (UCI), (2) some larger and higher dimensionality word processing tasks from the LibSVM evaluation archive 1, and (3) the prediction data used in [5] for product testing. We then present results on a structured prediction problem formulated for the task of visual information retrieval from Wikipedia biographies pages. Finally, we examine the kernel-based version of our classification. In all experiments, unless otherwise specified, we use a Gaussian prediction problem that results in an L2 penalty term. We examine four experimental configurations for our BBLR approach: (1) BBLR1, where we use our modified SLA algorithm with the following BBLR parameters for BBLR optimization and BBR optimization 100: BBLR = L\u03b2 = 1 and are fixed."}, {"heading": "5.1 Binary Classification Tasks", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "5.1.1 Experiments with UCI Benchmarks", "text": "We evaluate our technique on the following datasets from the University of California Irvine (UCI), which we have associated with the site [1]: Breast, Heart, Liver and Pima. We use these datasets in their entirety to understand the results of our novel logistics analysis, and to explore the behavior of our learning parameters, we have to deal with the way in which we use them. In the way in which we apply them, we can make comparisons with the results of our previous work in Section 2, in which we perform a small number of initial experiments that follow their experimental protocols. In our experiments, we compare our BBLRs with the following models: our own L2 Logistic Regression (LR) implementation, a linear SVM implementation (liblinear SVM) that we use."}, {"heading": "5.1.2 Pooled McNemar Tests :", "text": "We performed McNemar tests for the four UCI benchmarks and compared BBLR3 with LR and linear SVMs. Since we do not have a significant number of test cases for any of these benchmarks, it became difficult to statistically justify and compare the results. Therefore, we conducted pooled McNemar tests by treating each division of our 5x experiments as independent tests and performing the significance tests as a whole. Interestingly, the results of this pooled McNemar test are shown in Table 8. Interestingly, our noisy dataset experiments found that our BBLR3 is statistically significant in both the LR and SVM models with p \u2264 0.01."}, {"heading": "5.1.3 Experiments with LibSVM Benchmarks", "text": "In this section, we present classification results using two much larger datasets: web8 and webspam-unigrams. These datasets have predefined training and test splits, which can be found on the related web page [25] 2. These benchmarks are also distributed via the binary data collection LibSVM. 3 The web spam unigrams data originally came from the study in [21] 4. Table 9 compiles some details of these databases. For these experiments, we do not add additional noise to the feature vectors. In Table 10, we present classification results, and you can see that in both cases our BBLR3 approach has improved performance over the LR and the linear SVM baselines. As in our earlier small-scale experiments, we have used our own LR implementation and the non-linear SVM implementation for these large-scale experiments."}, {"heading": "5.1.4 Experiments with Product Reviews", "text": "For this series of experiments, we used the numbered unigram characteristics for four databases of the web site associated with [5]. For each database, there are 1,000 positive and 1,000 negative product ratings. Table 11 compiles the characteristic dimension size of these sparse databases. We present the results in Table 12 using a tenfold cross-validation setup as performed by [5]. Again, we do not add the data. For all four databases, our models BBLR3 and BBLR4 exceeded both the LR and the linear SVM. To further analyze these results, we also performed a McNemer test. For the books and the DVDs database, the results of our models BBLR3 and BBLR4 were found to be statistically significant over the LR and linear SVM with a p-value of \u2264 0.05. BBLR4 tended to predict BBLR3, but not in a statistically significant way."}, {"heading": "5.2 Structured Prediction Experiments", "text": "One of the advantages of our Beta-Bernoulli logistics loss is that it enables a model to produce more accurate probability estimates. Intuitively, therefore, the controllable nature of the plateaus in the log probability analysis of our formulation allows probability predictions to be based on values that are more representative of an appropriate level of confidence for a classification. In simple terms, predictions for feature vectors are used far from a decision limit that are not based on values that are close to zero probability or probability when the Beta-Bernoulli logistics model is used. If such models are used as components for larger systems that use probability conclusions for more complex thought tasks, the added flexibility over traditional logistic function formulation could be a significant advantage. The following experiments examine this hypothesis. In [9] we conducted a series of facemining experiments from Wikipedia biography pages that rely on probability conclusions in a common probability model."}, {"heading": "5.3 Kernel Logistic Regression with the Generalized Beta-Bernoulli Loss", "text": "In Table 14 we compare the logistic regression of Beta-Bernoulli with a logistic regression of SVM and Kernel Beta-Bernoulli (KBBLR). We see that our proposed approach is favorable compared to the SVM result, which is widely regarded as a modern, strong starting point."}, {"heading": "5.4 Sparse Kernel BBLR", "text": "As shown in [2], one of the advantages of using ramp losses for kernel-based classification is that there may be models that are even more economical than traditional SVMs based on horizontal loss. It is well known that L2-based regulation does not generally provide economical solutions when used with traditional logistic regression. Our analysis of the previous experiments shows that the L2-based approach to regulated zero-loss approximation does not generally lead to economical models here. The well-known L1 or lasso-regulation method can provide economical solutions, but often at the expense of predictive performance. Recently, the so-called elastic approach to regulation based on a weighted combination of L1 and L2 regularization has been presented as more effective in promoting frugality with less adverse effects on performance."}, {"heading": "6 Discussion and Conclusions", "text": "Through our generalized Beta-Bernoulli formulation, we have provided both a new smooth 0-1 loss approximation method and a new class of probable classifiers. Our experimental results suggest that our generalized Beta-Bernoulli formulation is capable of delivering superior performance over traditional logistic regression and maximum linear SVMs for binary classification. Like other ramp functions, one of the main advantages of our approach is that it is more robust with outliers than with traditional conveexotic loss functions. Our modified SLA algorithm, which adds a learning hyperparameter optimization level, shows improved performance over the original SLA optimization algorithm in [14].We have also presented a kerelized version of our approach that delivers competing performance with nonlinear SVMs for binary classification."}, {"heading": "Acknowledgements", "text": "We thank the NSERC Discovery Grants program and Google for a faculty research award that has helped support this work."}, {"heading": "A Experimental Details", "text": "In the interest of reproducibility, we list below the parameters of the algorithm and the recommended settings as indicated in [14]: rR = 2 \u2212 1, a factor for reducing the search radius; R0 = 8, the initial search radius; r = 2 \u2212 1, a factor for reducing the grid spacing; S0 = 0.2, the initial grid spacing for the 1-D search; r\u03b3 = 10, the factor for reducing the gamma parameters; \u03b3MIN = 2, the starting point for the search over \u03b3; \u03b3MAX = 200, the end point for the search over \u03b3.As part of the procedure for area optimization, there is also a standard method for reducing the gradient with a slowly reduced learning rate. The procedure has the following specified and unspecified default values for the constants defined below: rG = 0.1, a factor for reducing the learning rate; rGMAX, the initial learning rate; rIN, the minimum learning rate; the modification of the L, based on the size of the capability, is used."}, {"heading": "B Gradients for a Gaussian-Laplacian Mixture Prior", "text": "The gradient of the KBBLR probability is given in section 3.4. In the following, we give the gradient of the log-gauss-laplace mixture before or after the regularization term R = \u2211 j ln (\u03c0gN (aj; 0, \u03c3g) + \u03c0lL (aj; 0, bl))) dR dai = \u2211 i \u03c0g d dai N (ai; 0, \u03c3g) + \u03c0l ddaiL (ai; 0, bl) \u03c0gN (ai | 0, \u03c3g) + \u03c0lL (ai | 0, bl) (42) ddai N (ai; 0, \u03c3g) = \u2212 ai\u03c33g \u221a 2\u03c0exp (\u2212 a 2 i2\u03c32g) (43) ddai L (ai; 0, bl) = \u2212 1 2b2l exp (\u2212 | ai | bl) d dai (44) d dai (| ai |) = 1 if ai > 00 if = = 0 \u2212 1 if ai < 0"}, {"heading": "C Algorithm Modifications for Sparse Gauss-Laplace KBBLR", "text": "Algorithm 3 Range Optimization for Sparse KBBLR Input: w, X, t, \u03b3, Radius R, Step Size S Output: Updated estimate for w \u0432, minimization of 0-1 loss.1: Function GRAD-DESC-IN-RANGE-KBBLR (w, X, t, \u03b3, R, S) 2: Repeat B Stage 1: Find local minimum 3: w losses. VANILLA-GRAD-DESC (w) B Stage 2: Sparsify w 4: Assign initial near-zero weights {wi} to Laplassian cluster, Cl, or Gaus cluster, Cg using rear. 5: Estimation of Priors, {p (Cl (w)), p (Cg (w))} 6: Repeat 7: {p (w)."}], "references": [{"title": "Trading convexity for scalability", "author": ["R. Collobert", "F. Sinz", "J. Weston", "L. Bottou"], "venue": "Proceedings of the 23rd international conference on Machine learning, pages 201\u2013208. ACM", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2006}, {"title": "Learning optimally sparse support vector machines", "author": ["A. Cotter", "S. Shalev-Shwartz", "N. Srebro"], "venue": "Proceedings of the 30th International Conference on Machine Learning (ICML-13), pages 266\u2013274", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2013}, {"title": "Tighter bounds for structured estimation", "author": ["C.B. Do", "Q. Le", "C.H. Teo", "O. Chapelle", "A. Smola"], "venue": "Proc. of NIPS", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2008}, {"title": "Confidence-weighted linear classification", "author": ["M. Dredze", "K. Crammer", "F. Pereira"], "venue": "Proceedings of the 25th international conference on Machine learning, pages 264\u2013271. ACM New York, NY, USA", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2008}, {"title": "Nonconvex online support vector machines", "author": ["S. Ertekin", "L. Bottou", "C.L. Giles"], "venue": "Pattern Analysis and Machine Intelligence, IEEE Transactions on, 33(2):368\u2013 381", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2011}, {"title": "Agnostic learning of monomials by halfspaces is hard", "author": ["V. Feldman", "V. Guruswami", "P. Raghavendra", "Y. Wu"], "venue": "SIAM Journal on Computing, 41(6):1558\u2013 1590", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2012}, {"title": "Structured ramp loss minimization for machine translation", "author": ["K. Gimpel", "N.A. Smith"], "venue": "Proceedings of the 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 221\u2013231. Association for Computational Linguistics", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2012}, {"title": "Experiments on visual information extraction with the faces of wikipedia", "author": ["M.K. Hasan", "C. Pal"], "venue": "AAAI Conference on Artificial Intelligence (AI)", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2014}, {"title": "Sparse probabilistic classifiers", "author": ["R. H\u00e9rault", "Y. Grandvalet"], "venue": "Proceedings of the 24th international conference on Machine learning, pages 337\u2013344. ACM", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2007}, {"title": "An automatic method of solving discrete programming problems", "author": ["A.H. Land", "A.G. Doig"], "venue": "Econometrica, 28(3):497\u2013520", "citeRegEx": "11", "shortCiteRegEx": null, "year": 1960}, {"title": "Breast cancer diagnosis and prognosis via linear programming", "author": ["O.L. Mangasarian", "W.N. Street", "W.H. Wolberg"], "venue": "Operations Research, 43(4):570\u2013577", "citeRegEx": "12", "shortCiteRegEx": null, "year": 1995}, {"title": "Boosting algorithms as gradient descent in function space", "author": ["L. Mason", "J. Baxter", "P. Bartlett", "M. Frean"], "venue": "NIPS", "citeRegEx": "13", "shortCiteRegEx": null, "year": 1999}, {"title": "Algorithms for direct 0\u20131 loss optimization in binary classification", "author": ["T. Nguyen", "S. Sanner"], "venue": "Proceedings of the 30th International Conference on Machine Learning (ICML-13), pages 1085\u20131093", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2013}, {"title": "Empirical risk minimization for support vector classifiers", "author": ["F. P\u00e9rez-Cruz", "A. Navia-V\u00e1zquez", "A.R. Figueiras-Vidal", "A. Artes-Rodriguez"], "venue": "Neural Networks, IEEE Transactions on, 14(2):296\u2013303", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2003}, {"title": "Maximum margin bayesian network classifiers", "author": ["F. Pernkopf", "M. Wohlmayr", "S. Tschiatschek"], "venue": "Pattern Analysis and Machine Intelligence, IEEE Transactions on, 34(3):521\u2013532", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2012}, {"title": "Sparseness of support vector machines", "author": ["I. Steinwart"], "venue": "The Journal of Machine Learning Research, 4:1071\u20131105", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2003}, {"title": "The nature of statistical learning theory", "author": ["V. Vapnik"], "venue": "springer", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2000}, {"title": "Mod\u00e8les \u00e0 noyaux \u00e0 structure locale", "author": ["P. Vincent"], "venue": "Citeseer", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2004}, {"title": "Kernel matching pursuit", "author": ["P. Vincent", "Y. Bengio"], "venue": "Machine Learning, 48(1- 3):165\u2013187", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2002}, {"title": "Evolutionary study of web spam: Webb spam corpus 2011 versus webb spam corpus 2006", "author": ["D. Wang", "D. Irani", "C. Pu"], "venue": "Collaborative Computing: Networking, Applications and Worksharing (CollaborateCom), 2012 8th International Conference on, pages 40\u201349. IEEE", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2012}, {"title": "Robust truncated hinge loss support vector machines", "author": ["Y. Wu", "Y. Liu"], "venue": "Journal of the American Statistical Association, 102(479)", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2007}, {"title": "The concave-convex procedure", "author": ["A.L. Yuille", "A. Rangarajan"], "venue": "Neural Computation, 15(4):915\u2013936", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2003}, {"title": "Text categorization based on regularized linear classification methods", "author": ["T. Zhang", "F.J. Oles"], "venue": "Information retrieval, 4(1):5\u201331", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2001}, {"title": "Smoothing multivariate performance measures", "author": ["X. Zhang", "A. Saha", "S. Vishwanathan"], "venue": "Journal of Machine Learning Research, 10:1\u201355", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2011}, {"title": "Regularization and variable selection via the elastic net", "author": ["H. Zou", "T. Hastie"], "venue": "Journal of the Royal Statistical Society: Series B (Statistical Methodology), 67", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2005}], "referenceMentions": [{"referenceID": 16, "context": "In the classical statistical literature, this is known as Empirical Risk Minimization (ERM) [18], where learning is performed by minimizing the average risk or loss over", "startOffset": 92, "endOffset": 96}, {"referenceID": 18, "context": "Of course, loss functions can be more complex, for example defined and learned through a linear combination of simpler basis loss functions [20], but we focus on the widely used losses above for now.", "startOffset": 140, "endOffset": 144}, {"referenceID": 5, "context": "While seemingly a sensible objective for a classification problem, empirical risk minimization with the 0-1 loss function is known to be an NP-hard problem [7].", "startOffset": 156, "endOffset": 159}, {"referenceID": 22, "context": "The zero-one loss captures the intuitive goal of simply minimizing classification errors and recent research has been directed to learning models using a smoothed zero-one loss approximation [24, 14].", "startOffset": 191, "endOffset": 199}, {"referenceID": 12, "context": "The zero-one loss captures the intuitive goal of simply minimizing classification errors and recent research has been directed to learning models using a smoothed zero-one loss approximation [24, 14].", "startOffset": 191, "endOffset": 199}, {"referenceID": 22, "context": "Previous work has shown that both the hinge loss [24] and more recently the 0-1 loss [14] can be efficiently and effectively optimized directly using smooth approximations.", "startOffset": 49, "endOffset": 53}, {"referenceID": 12, "context": "Previous work has shown that both the hinge loss [24] and more recently the 0-1 loss [14] can be efficiently and effectively optimized directly using smooth approximations.", "startOffset": 85, "endOffset": 89}, {"referenceID": 12, "context": "The work in [14] also underscored the robustness advantages of the 0-1 loss to outliers.", "startOffset": 12, "endOffset": 16}, {"referenceID": 0, "context": "While the 0-1 loss is not convex, the current flurry of activity in the area of deep neural networks as well as the award winning work on 0-1 loss approximations in [2] have highlighted numerous other advantages to the use of non-convex loss functions.", "startOffset": 165, "endOffset": 168}, {"referenceID": 22, "context": "The answer to our second question is indeed yes; and more specifically, to control the sharpness of our approximation, we use a \u03b3 factor reminiscent of a technique used in previous work which has created smooth approximations to the hinge loss [24] as well as smooth approximations of the 0-1 loss [14].", "startOffset": 244, "endOffset": 248}, {"referenceID": 12, "context": "The answer to our second question is indeed yes; and more specifically, to control the sharpness of our approximation, we use a \u03b3 factor reminiscent of a technique used in previous work which has created smooth approximations to the hinge loss [24] as well as smooth approximations of the 0-1 loss [14].", "startOffset": 298, "endOffset": 302}, {"referenceID": 8, "context": "In Figure 2, we show the probability given by the model as a function of wx at the right and the negative log probability or the loss on the left as \u03b3 is varied over the integer powers in the interval [0, 10].", "startOffset": 201, "endOffset": 208}, {"referenceID": 12, "context": "(2) A second key contribution of our work is that we present and explore an adapted version of the optimization algorithm proposed in [14] in which we optimize the meta parameters of learning using validation sets.", "startOffset": 134, "endOffset": 138}, {"referenceID": 12, "context": "We present a series of experiments in which we optimize the BB\u03b3 loss using the basic algorithm from [14] and our modified version.", "startOffset": 100, "endOffset": 104}, {"referenceID": 22, "context": "It has been shown in [24] that it is possible to define a generalized logistic loss and produce a smooth approximation to the hinge loss using the following formulation", "startOffset": 21, "endOffset": 25}, {"referenceID": 14, "context": "The maximum margin Bayesian network formulation in [16] also employs a smooth differentiable hinge loss inspired by the Huber loss, having a similar shape tomin[1, zi].", "startOffset": 51, "endOffset": 55}, {"referenceID": 8, "context": "The sparse probabilistic classifier approach in [10] truncates the logistic loss leading to a sparse kernel logistic regression models.", "startOffset": 48, "endOffset": 52}, {"referenceID": 13, "context": "[15] proposed a technique for learning support vector classifiers based on arbitrary loss functions composed of using the combination of a hyperbolic tangent loss function and a polynomial loss function.", "startOffset": 0, "endOffset": 4}, {"referenceID": 12, "context": "Other recent work [14] has created a smooth approximation to the 0-1 loss by directly defining the loss as a modified sigmoid.", "startOffset": 18, "endOffset": 22}, {"referenceID": 22, "context": "Figure 3: (left) The way in which the generalized log logistic loss, Lglog proposed in [24] can approximate the hinge loss, Lhinge through translating the log logistic loss, Llog then increasing the \u03b3 factor.", "startOffset": 87, "endOffset": 91}, {"referenceID": 12, "context": "(right) The way in a sigmoid function is used in [14] to directly approximate the 0-1 loss, L01.", "startOffset": 49, "endOffset": 53}, {"referenceID": 22, "context": "The approach also uses a similar \u03b3 factor to [24] and we show \u03b3 = 1, 2 and 32.", "startOffset": 45, "endOffset": 49}, {"referenceID": 12, "context": "Another important aspect of [14] is that they compared a variety of algorithms for directly optimizing the 0-1 loss with a novel algorithm for optimizing the sigmoid loss, Lsig(zi, \u03b3).", "startOffset": 28, "endOffset": 32}, {"referenceID": 9, "context": "The compared direct 0-1 loss optimization algorithms are: (1) a Branch and Bound (BnB) [11] technique, (2) a Prioritized Combinatorial Search (PCS) technique and (3) an algorithm referred to as a Combinatorial Search Approximation (CSA), both of which are presented in more detail in [14].", "startOffset": 87, "endOffset": 91}, {"referenceID": 12, "context": "The compared direct 0-1 loss optimization algorithms are: (1) a Branch and Bound (BnB) [11] technique, (2) a Prioritized Combinatorial Search (PCS) technique and (3) an algorithm referred to as a Combinatorial Search Approximation (CSA), both of which are presented in more detail in [14].", "startOffset": 284, "endOffset": 288}, {"referenceID": 12, "context": "To evaluate and compare the quality of the non-convex optimization results produced by the BnB, PCS and CSA, with their SLA algorithm for the sigmoid loss, [14] also presents training set errors for a number of standard evaluation datasets.", "startOffset": 156, "endOffset": 160}, {"referenceID": 12, "context": "Furthermore, in [14], they also provide an analysis of the run-time performance for each of the algorithms.", "startOffset": 16, "endOffset": 20}, {"referenceID": 0, "context": "The award winning work of [2] produced an approximation to the 0-1 loss by creating a ramp loss, Lramp, obtained by combining the traditional hinge loss with a shifted and inverted hinge loss as illustrated in Figure 4.", "startOffset": 26, "endOffset": 29}, {"referenceID": 21, "context": "They showed how to optimize the ramp loss using the Concave-Convex Procedure (CCCP) of [23] and that this yields faster training times compared to traditional SVMs.", "startOffset": 87, "endOffset": 91}, {"referenceID": 4, "context": "Other more recent work has proposed an alternative online SVM learning algorithm for the ramp loss [6].", "startOffset": 99, "endOffset": 102}, {"referenceID": 20, "context": "[22] explored", "startOffset": 0, "endOffset": 4}, {"referenceID": 12, "context": "Table 1: An excerpt from [14] of the total 0-1 loss for a variety of algorithms on some standard datasets.", "startOffset": 25, "endOffset": 29}, {"referenceID": 1, "context": "More recent work [3] has explored a similar ramp like construction which they refer to as the slant loss.", "startOffset": 17, "endOffset": 20}, {"referenceID": 2, "context": "Interestingly, the ramp loss formulation has also been generalized to structured predictions [4, 8].", "startOffset": 93, "endOffset": 99}, {"referenceID": 6, "context": "Interestingly, the ramp loss formulation has also been generalized to structured predictions [4, 8].", "startOffset": 93, "endOffset": 99}, {"referenceID": 0, "context": "Figure 4: The way in which shifted hinge losses are combined in [2] to produce the ramp loss, Lramp.", "startOffset": 64, "endOffset": 67}, {"referenceID": 11, "context": "There has been the activity of using zero-one loss like functional losses in machine learning, specially by the boosting [13] and neural network [19] communities.", "startOffset": 121, "endOffset": 125}, {"referenceID": 17, "context": "There has been the activity of using zero-one loss like functional losses in machine learning, specially by the boosting [13] and neural network [19] communities.", "startOffset": 145, "endOffset": 149}, {"referenceID": 17, "context": "Vincent [19] analyzes that the loss defined through a functional of the hyperbolic tangent, 1\u2212 tanh(zi), is more robust as it doesn\u2019t penalize the outliers too excessively compared to other { log logistic loss, hinge loss, and squared loss } loss functions.", "startOffset": 8, "endOffset": 12}, {"referenceID": 11, "context": "A variant of this loss has been used in boosting algorithms [13].", "startOffset": 60, "endOffset": 64}, {"referenceID": 17, "context": "Other work [19] has also shown that a hyperbolic tangent parametrized squared error loss, (0.", "startOffset": 11, "endOffset": 15}, {"referenceID": 6, "context": "In this way our work is similar to that of [8] which explored the use of the ramp loss of [2] in the context of structured prediction for machine translation.", "startOffset": 43, "endOffset": 46}, {"referenceID": 0, "context": "In this way our work is similar to that of [8] which explored the use of the ramp loss of [2] in the context of structured prediction for machine translation.", "startOffset": 90, "endOffset": 93}, {"referenceID": 12, "context": "As we have discussed in the relevant recent work section above, the work of [14] has shown that their SLA algorithm applied to Lsig(zi, \u03b3) outperformed a number of other techniques in terms of both true 0-1 loss minimization performance and run time.", "startOffset": 76, "endOffset": 80}, {"referenceID": 12, "context": "Recall that if one compares our generalized Beta-Bernoulli logistic loss with the directly defined sigmoidal loss used in the SLA work of [14], it becomes apparent that the BBLR formulation has three additional hyper-parameters, {\u03b1, \u03b2, w}.", "startOffset": 138, "endOffset": 142}, {"referenceID": 12, "context": "In contrast, the plateaus of the sigmoidal loss in [14] are located at zero and one.", "startOffset": 51, "endOffset": 55}, {"referenceID": 12, "context": "Here we present our meta-optimization extension and various other modifications to the SLA approach of [14].", "startOffset": 103, "endOffset": 107}, {"referenceID": 12, "context": "The SLA algorithm proposed in [14] can be decomposed into two different parts; an outer loop that initializes a model then enters a loop in which one slowly increases the \u03b3 factor of their sigmoidal loss, repeatedly calling an algorithm they refer to as Range Optimization for SLA or Gradient Descent in Range.", "startOffset": 30, "endOffset": 34}, {"referenceID": 12, "context": "The first minor difference between the SLA optimization algorithm of [14] and our extension to it are the selection of the initial w0 that the SLA algorithm starts optimizing.", "startOffset": 69, "endOffset": 73}, {"referenceID": 12, "context": "We use the suggested values in the original SLA algorithm [14] for the parameters rR, R0, r , and S0 .", "startOffset": 58, "endOffset": 62}, {"referenceID": 3, "context": "Below, we present results for three different groups of benchmark problems: (1) a selection from the University of California Irvine (UCI) repository, (2) some larger and higher dimensionality text processing tasks from the LibSVM evaluation archive 1, and (3) the product review sentiment prediction datasets used in [5].", "startOffset": 318, "endOffset": 321}, {"referenceID": 12, "context": "We use these datasets in part so as to compare directly with results in [14], to understand the behaviour of our novel logistic function formulation and to explore the behavior of our learning parameter optimization procedure.", "startOffset": 72, "endOffset": 76}, {"referenceID": 10, "context": "Dataset # Examples # Dimensions Description Breast 683 10 Breast Cancer Diagnosis [12] Heart 270 13 Statlog Liver 345 6 Liver Disorders Pima 768 8 Pima Indians Diabetes", "startOffset": 82, "endOffset": 86}, {"referenceID": 12, "context": "To facilitate comparisons with previous results presented [14] such as those summarized in Table 3 of our literature review in Section 2, we provide a small set of initial experiments here following their experimental protocols.", "startOffset": 58, "endOffset": 62}, {"referenceID": 12, "context": "In our experiments here we compare our BBLRs with the following models: our own L2 Logistic Regression (LR) implementation, a linear SVM - using the same implementation (liblinear) that was used in [14], and the optimization of the sigmoid loss, Lsig(zi, \u03b3) of [14] using the SLA algorithm and the code distributed on the web site associated with [14] (indicated by SLA in our tables).", "startOffset": 198, "endOffset": 202}, {"referenceID": 12, "context": "In our experiments here we compare our BBLRs with the following models: our own L2 Logistic Regression (LR) implementation, a linear SVM - using the same implementation (liblinear) that was used in [14], and the optimization of the sigmoid loss, Lsig(zi, \u03b3) of [14] using the SLA algorithm and the code distributed on the web site associated with [14] (indicated by SLA in our tables).", "startOffset": 261, "endOffset": 265}, {"referenceID": 12, "context": "In our experiments here we compare our BBLRs with the following models: our own L2 Logistic Regression (LR) implementation, a linear SVM - using the same implementation (liblinear) that was used in [14], and the optimization of the sigmoid loss, Lsig(zi, \u03b3) of [14] using the SLA algorithm and the code distributed on the web site associated with [14] (indicated by SLA in our tables).", "startOffset": 347, "endOffset": 351}, {"referenceID": 12, "context": "Despite the fact that we used the code distributed on the website associated with [14] we found that the SLA algorithm applied to their sigmoid loss, Lsig(zi, \u03b3) gave errors that are slightly higher than those given in [14].", "startOffset": 82, "endOffset": 86}, {"referenceID": 12, "context": "Despite the fact that we used the code distributed on the website associated with [14] we found that the SLA algorithm applied to their sigmoid loss, Lsig(zi, \u03b3) gave errors that are slightly higher than those given in [14].", "startOffset": 219, "endOffset": 223}, {"referenceID": 12, "context": "We use the term SLA in Table 3 and subsequent tables to denote experiments performed using both the sigmoidal loss explored in [14] and their algorithm for minimizing it.", "startOffset": 127, "endOffset": 131}, {"referenceID": 12, "context": "We therefore provide the next set of experiments using traditional training, validation and testing splits, again following the protocols used in [14]; however, as we shall soon see, these experiments underscored the importance of extending the original SLA algorithm to automate the adjustment of learning parameters.", "startOffset": 146, "endOffset": 150}, {"referenceID": 12, "context": "The same observation was made in [14] and it motivated their own exploration of learning with noisy feature vectors.", "startOffset": 33, "endOffset": 37}, {"referenceID": 12, "context": "In Table 6, we present the sum of the mean 0-1 loss over 10 repetitions of a 5 fold leave one out experiment where 10% noise has been added to the data following the protocol given in [14].", "startOffset": 184, "endOffset": 188}, {"referenceID": 12, "context": "However, the fact that the SLA approach failed to outperform the LR and SVM baselines in our experiments here; whereas in a similar experiment in [14] the SLA algorithm and sigmoidal loss did outperform these methods leads us to believe", "startOffset": 146, "endOffset": 150}, {"referenceID": 12, "context": "Table 6: The sum of the mean 0-1 loss over 10 repetitions of a 5 fold leave one out experiment where 10% noise has been added to the data following the protocol given in [14].", "startOffset": 170, "endOffset": 174}, {"referenceID": 23, "context": "These datasets have predefined training and testing splits, which are distributed on the web site accompanying [25]2.", "startOffset": 111, "endOffset": 115}, {"referenceID": 19, "context": "The webspam unigrams data originally came from the study in [21]4.", "startOffset": 60, "endOffset": 64}, {"referenceID": 3, "context": "For this set of experiments, we used the count based unigram features for four databases from the website associated with [5].", "startOffset": 122, "endOffset": 125}, {"referenceID": 3, "context": "We present results in Table 12 using a ten fold cross validation setup as performed by [5].", "startOffset": 87, "endOffset": 90}, {"referenceID": 7, "context": "In [9], we performed a set of face mining experiments from Wikipedia biography pages using a technique that relies on probabilistic inference in a joint probability model.", "startOffset": 3, "endOffset": 6}, {"referenceID": 7, "context": "The bottom layer or set of random variables {X} in Figure 6 are used to encode these features, and we discuss the precise nature and definition of these features in more detail in [9].", "startOffset": 180, "endOffset": 183}, {"referenceID": 7, "context": "As mentioned above and discussed in more detail in [9], we used Maximum Entropy Models (MEMs) or Logistic Regression models for these local binary predictions working on multimedia features in our previous work.", "startOffset": 51, "endOffset": 54}, {"referenceID": 0, "context": "As shown in [2], one of the advantages of using the ramp loss for kernel based classification is that it can yield models that are even sparser than traditional SVMs based on the hinge loss.", "startOffset": 12, "endOffset": 15}, {"referenceID": 24, "context": "Recently the so called elastic net regularization approach [26] based on a weighted combination of L1 and L2 regularization has been shown more effective at encouraging sparsity with a less negative impact on performance.", "startOffset": 59, "endOffset": 63}, {"referenceID": 15, "context": "Support vectors for SVMs increase almost linearly for an increase in the database size, an effect that has been confirmed in a number of other studies [17, 2].", "startOffset": 151, "endOffset": 158}, {"referenceID": 0, "context": "Support vectors for SVMs increase almost linearly for an increase in the database size, an effect that has been confirmed in a number of other studies [17, 2].", "startOffset": 151, "endOffset": 158}, {"referenceID": 12, "context": "Our modified SLA algorithm, which adds a learning hyper-parameter optimization step shows improved performance over the original SLA optimization algorithm in [14].", "startOffset": 159, "endOffset": 163}], "year": 2015, "abstractText": "We examine a new form of smooth approximation to the zero one loss in which learning is performed using a reformulation of the widely used logistic function. Our approach is based on using the posterior mean of a novel generalized BetaBernoulli formulation. This leads to a generalized logistic function that approximates the zero one loss, but retains a probabilistic formulation conferring a number of useful properties. The approach is easily generalized to kernel logistic regression and easily integrated into methods for structured prediction. We present experiments in which we learn such models using an optimization method consisting of a combination of gradient descent and coordinate descent using localized grid search so as to escape from local minima. Our experiments indicate that optimization quality is improved when learning meta-parameters are themselves optimized using a validation set. Our experiments show improved performance relative to widely used logistic and hinge loss methods on a wide variety of problems ranging from standard UC Irvine and libSVM evaluation datasets to product review predictions and a visual information extraction task. We observe that the approach: 1) is more robust to outliers compared to the logistic and hinge losses; 2) outperforms comparable logistic and max margin models on larger scale benchmark problems; 3) when combined with GaussianLaplacian mixture prior on parameters the kernelized version of our formulation yields sparser solutions than Support Vector Machine classifiers; and 4) when integrated into a probabilistic structured prediction technique our approach provides more accurate probabilities yielding improved inference and increasing information extraction performance.", "creator": "LaTeX with hyperref package"}}}