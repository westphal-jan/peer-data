{"id": "1511.05644", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "18-Nov-2015", "title": "Adversarial Autoencoders", "abstract": "In this paper we propose a new method for regularizing autoencoders by imposing an arbitrary prior on the latent representation of the autoencoder. Our method, named \"adversarial autoencoder\", uses the recently proposed generative adversarial networks (GAN) in order to match the aggregated posterior of the hidden code vector of the autoencoder with an arbitrary prior. Matching the aggregated posterior to the prior ensures that there are no \"holes\" in the prior, and generating from any part of prior space results in meaningful samples. As a result, the decoder of the adversarial autoencoder learns a deep generative model that maps the imposed prior to the data distribution. We show how adversarial autoencoders can be used to disentangle style and content of images and achieve competitive generative performance on MNIST, Street View House Numbers and Toronto Face datasets.", "histories": [["v1", "Wed, 18 Nov 2015 02:32:39 GMT  (4071kb,D)", "http://arxiv.org/abs/1511.05644v1", null], ["v2", "Wed, 25 May 2016 00:17:45 GMT  (7189kb,D)", "http://arxiv.org/abs/1511.05644v2", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["alireza makhzani", "jonathon shlens", "navdeep jaitly", "ian goodfellow", "brendan frey"], "accepted": false, "id": "1511.05644"}, "pdf": {"name": "1511.05644.pdf", "metadata": {"source": "CRF", "title": null, "authors": ["ADVERSARIAL AUTOENCODERS", "Alireza Makhzani", "Ian Goodfellow"], "emails": ["makhzani@psi.utoronto.ca", "shlens@google.com", "ndjaitly@google.com", "goodfellow@google.com"], "sections": [{"heading": "1 INTRODUCTION", "text": "Until recently, deep generative models such as Restricted Boltzmann Machines (RBM), Deep Belief Networks (DBNs), and Deep Boltzmann Machines (DBMs) were trained primarily through MCMC-based algorithms (Hinton et al., 2006; Salakhutdinov & Hinton, 2009).In these approaches, the MCMC methods calculate the gradient of loglikelihood, which becomes less accurate with increasing training, because samples from the Markov chains cannot mix between modes fast enough. Generative models have been developed that can be trained via direct retransmission and avoid the difficulties associated with MCMC training."}, {"heading": "1.1 GENERATIVE ADVERSARIAL NETWORKS", "text": "The Generative Adversarial Networks (GAN) (Goodfellow et al., 2014) framework establishes a min-max adversarial game between two neural networks - a generative model, G, and a discrimi-ar Xiv: 151 1.05 644v 1 [cs.L G] 18 Nov 201 5native model, D. The discrimination model, D (x), is a neural network that calculates the probability that a point x in the data space is a sample from the data distribution (positive samples) that we are trying to model, and not a sample from our generative model (negative samples). At the same time, the generator uses a function G (z) that maps samples from the previous p (z) to the data space. G (z) is trained to maximize the confusion of the discriminator so that samples it generates come from the data distribution. The generator is trained by using the gradient of D (x) w.r.t. x to modify its parameters."}, {"heading": "2 ADVERSARIAL AUTOENCODERS", "text": "We assume that the distribution of the data (the encryption function of the car distribution) is an aggregated posterior distribution of q (z) on the hidden code vector of the car distribution, as follows: q (z) is the encryption function of the car distribution. The encryption function of the car distribution q (z) defines an aggregated posterior distribution of q (z) on the hidden code vector of the car distribution as follows: q (z) is the distribution of the car distribution. The encryption function of the car distribution q (z) is an auto encoder that aggregates the posterior, q (z), into an arbitrary previous one, p (z)."}, {"heading": "2.1 RELATIONSHIP TO VARIATIONAL AUTOENCODERS", "text": "This year, we have reached the stage where a process of this kind is taking place, in which the question is to what extent it is a country in which it is a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a city, a country, a country, a country, a country, a country, a country, a country, a country, a country, a city, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country"}, {"heading": "2.2 RELATIONSHIP TO GANS AND GMMNS", "text": "In the original publication \"Generative Adversarial Networks (GAN)\" (Goodfellow et al., 2014), GANs were used to force data distribution at the pixel level at the output level of a neural network. However, conflicting autoencoders rely on autoencoder training to capture the data distribution. In the adversarial training method of our method, a much simpler distribution (e.g. Gaussian as opposed to data distribution) is imposed in a much smaller dimensional space (e.g. 20 as opposed to 1000), resulting in a better test probability, as discussed in Section 4. Generative Moment Matching Networks (GMMN) (Li et al., 2015), with the maximum mean discrepancy (MMD) aimed at shaping the distribution of the output layer of a neural network. The MMD target can be interpreted to minimize the distance between all moments of model distribution and data distribution."}, {"heading": "3 SEMI-SUPERVISED ADVERSARIAL AUTOENCODERS", "text": "In scenarios where data is fully or partially labeled, the label can be integrated into the opposing training to improve the distribution of the hidden codes. We show two different ways to incorporate the label information into opposing autoencoders: The labels can be integrated either in the opposing training process (regulation phase) or as additional latent variables in the reconstruction phase."}, {"heading": "3.1 INCORPORATING LABEL INFORMATION IN THE ADVERSARIAL TRAINING", "text": "We first describe how to use partial or complete label information to regulate the latent representation of the auto encoder more closely. To demonstrate this architecture, we return to Figure 2bin, which the opposing auto encoder fits into a mixture of 10 2-D gaussers. We add a compulsion to the input of the discriminatory network to associate the label with a single label from MNIST. Figure 3a demonstrates the training process for this semi-supervised approach. We add a hot vector to the label by associating the label with a mode of distribution. The one-sided hot vector acts like a switch that selects the corresponding decision limit of the discriminatory label. This one-sided hot vector has an extra class for undescribed examples."}, {"heading": "3.2 INCORPORATING LABEL INFORMATION AS ADDITIONAL LATENT VARIABLES", "text": "Many latent variation factors interact to generate an image. Recently, semi-monitored variation autoencoders have shown that the style and content of the images can be detangled with additional monitored costs (Kingma et al., 2014; Cheung et al., 2014). We now show that opposing autoencoders can also be used to decode the image style of the class label information. We modify the network architecture to provide the decoder with a uniform vector encoding of the label (Figure 3b). The decoder uses both the one hot vector that identifies the label and the hidden code z to reconstruct the image. This architecture forces the network to reconstruct all information regardless of the label in the hidden code.Figure 5a shows the results of such a network trained on MNIST digits in which the hidden code is forced into a 15-GD reconstructed line of each line of the 5a image."}, {"heading": "4 LIKELIHOOD ANALYSIS", "text": "There are several methods to quantitatively analyze the quality of a generative model (Buades et al., 2005; Lyu & Simoncelli, 2009).In this section, we measure the generative model's ability to capture the distribution of data by comparing the probability of this model to generate images in which the model imposes a high-dimensional distribution of the hidden code. (Goodfellow et al., 2014) We trained an adaptive autocode on MNIST and TFD, in which the model imposes a high-dimensional distribution of the hidden code. Goodfellow et al al al al al al al al al, 2014) We trained the learned TFD units on MNIST and TFD, in which the model imposes a high-dimensional distribution of the hidden code."}, {"heading": "5 CONCLUSION", "text": "In this paper, we proposed a general framework for transforming each autoencoder into a generative model by imposing arbitrary distribution on the latent representation of the autoencoder. We discussed how this method can be extended to semi-monitored settings by including label information to better shape the hidden code distribution. Importantly, we showed how it can be used to untangle the style and label information of a data set (Kingma et al., 2014; Cheung et al., 2014). Finally, we showed that opposing autoencoders on realized MNIST and Toronto Face data sets can achieve state-of-the-art probabilities."}, {"heading": "ACKNOWLEDGMENTS", "text": "We would like to thank Ilya Sutskever, Oriol Vinyals, Jon Gauthier, Sam Bowman and other members of the Google Brain team for their valuable comments. We would like to thank the developers of TensorFlow (Abadi et al., 2015), a machine learning framework that enabled us to easily develop fast and optimized code for the GPU."}], "references": [{"title": "Better mixing via deep representations", "author": ["Bengio", "Yoshua", "Mesnil", "Gr\u00e9goire", "Dauphin", "Yann", "Rifai", "Salah"], "venue": "International Conference on Machine Learning (ICML),", "citeRegEx": "Bengio et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Bengio et al\\.", "year": 2013}, {"title": "Deep generative stochastic networks trainable by backprop", "author": ["Bengio", "Yoshua", "Thibodeau-Laufer", "Eric", "Alain", "Guillaume", "Yosinski", "Jason"], "venue": "International Conference on Machine Learning (ICML),", "citeRegEx": "Bengio et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Bengio et al\\.", "year": 2014}, {"title": "A review of image denoising algorithms, with a new", "author": ["A. Buades", "B. Coll", "J.M. Morel"], "venue": "one. Simul,", "citeRegEx": "Buades et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Buades et al\\.", "year": 2005}, {"title": "Importance weighted autoencoders", "author": ["Burda", "Yuri", "Grosse", "Roger", "Salakhutdinov", "Ruslan"], "venue": "arXiv preprint arXiv:1509.00519,", "citeRegEx": "Burda et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Burda et al\\.", "year": 2015}, {"title": "Discovering hidden factors of variation in deep networks", "author": ["Cheung", "Brian", "Livezey", "Jesse A", "Bansal", "Arjun K", "Olshausen", "Bruno A"], "venue": "arXiv preprint arXiv:1412.6583,", "citeRegEx": "Cheung et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Cheung et al\\.", "year": 2014}, {"title": "Generative adversarial nets", "author": ["Goodfellow", "Ian", "Pouget-Abadie", "Jean", "Mirza", "Mehdi", "Xu", "Bing", "Warde-Farley", "David", "Ozair", "Sherjil", "Courville", "Aaron", "Bengio", "Yoshua"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Goodfellow et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Goodfellow et al\\.", "year": 2014}, {"title": "A fast learning algorithm for deep belief nets", "author": ["Hinton", "Geoffrey E", "Osindero", "Simon", "Teh", "Yee Whye"], "venue": "Neural Computation,", "citeRegEx": "Hinton et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Hinton et al\\.", "year": 2006}, {"title": "Auto-encoding variational bayes", "author": ["Kingma", "Diederik P", "Welling", "Max"], "venue": "International Conference on Learning Representations (ICLR),", "citeRegEx": "Kingma et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Kingma et al\\.", "year": 2014}, {"title": "Semisupervised learning with deep generative models", "author": ["Kingma", "Diederik P", "Mohamed", "Shakir", "Rezende", "Danilo Jimenez", "Welling", "Max"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Kingma et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Kingma et al\\.", "year": 2014}, {"title": "Generative moment matching networks", "author": ["Li", "Yujia", "Swersky", "Kevin", "Zemel", "Richard"], "venue": "International Conference on Machine Learning (ICML),", "citeRegEx": "Li et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Li et al\\.", "year": 2015}, {"title": "Modeling multiscale subbands of photographic images with fields of Gaussian scale mixtures", "author": ["S Lyu", "Simoncelli", "E P"], "venue": "IEEE Trans. Patt. Analysis and Machine Intelligence,", "citeRegEx": "Lyu et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Lyu et al\\.", "year": 2009}, {"title": "Reading digits in natural images with unsupervised feature learning", "author": ["Netzer", "Yuval", "Wang", "Tao", "Coates", "Adam", "Bissacco", "Alessandro", "Wu", "Bo", "Ng", "Andrew Y"], "venue": "In NIPS workshop on deep learning and unsupervised feature learning,", "citeRegEx": "Netzer et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Netzer et al\\.", "year": 2011}, {"title": "Stochastic backpropagation and approximate inference in deep generative models", "author": ["Rezende", "Danilo Jimenez", "Mohamed", "Shakir", "Wierstra", "Daan"], "venue": "International Conference on Machine Learning (ICML),", "citeRegEx": "Rezende et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Rezende et al\\.", "year": 2014}, {"title": "Nonlinear dimensionality reduction by locally linear embedding", "author": ["Roweis", "Sam T", "Saul", "Lawrence K"], "venue": "SCIENCE, 290:2323\u20132326,", "citeRegEx": "Roweis et al\\.,? \\Q2000\\E", "shortCiteRegEx": "Roweis et al\\.", "year": 2000}, {"title": "Deep boltzmann machines", "author": ["Salakhutdinov", "Ruslan", "Hinton", "Geoffrey E"], "venue": "In International Conference on Artificial Intelligence and Statistics, pp", "citeRegEx": "Salakhutdinov et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Salakhutdinov et al\\.", "year": 2009}], "referenceMentions": [{"referenceID": 6, "context": "Until recently, deep generative models, such as Restricted Boltzmann Machines (RBM), Deep Belief Networks (DBNs) and Deep Boltzmann Machines (DBMs) were trained primarily by MCMC-based algorithms (Hinton et al., 2006; Salakhutdinov & Hinton, 2009).", "startOffset": 196, "endOffset": 247}, {"referenceID": 12, "context": "For example, variational autoencoders (VAE) (Kingma & Welling, 2014; Rezende et al., 2014) or importance weighted autoencoders (Burda et al.", "startOffset": 44, "endOffset": 90}, {"referenceID": 3, "context": ", 2014) or importance weighted autoencoders (Burda et al., 2015) use a recognition network to predict the posterior distribution over the latent variables, generative adversarial networks (GAN) (Goodfellow et al.", "startOffset": 44, "endOffset": 64}, {"referenceID": 5, "context": ", 2015) use a recognition network to predict the posterior distribution over the latent variables, generative adversarial networks (GAN) (Goodfellow et al., 2014) use an adversarial training procedure to directly shape the output distribution of the network via back-propagation and generative moment matching networks (GMMN) (Li et al.", "startOffset": 137, "endOffset": 162}, {"referenceID": 9, "context": ", 2014) use an adversarial training procedure to directly shape the output distribution of the network via back-propagation and generative moment matching networks (GMMN) (Li et al., 2015) use a moment matching cost function to learn the data distribution.", "startOffset": 171, "endOffset": 188}, {"referenceID": 5, "context": "In our model, an autoencoder is trained with dual objectives \u2013 a traditional reconstruction error criterion, and an adversarial training criterion (Goodfellow et al., 2014) that matches the aggregated posterior distribution of the latent representation of the autoencoder to an arbitrary prior distribution.", "startOffset": 147, "endOffset": 172}, {"referenceID": 5, "context": "The Generative Adversarial Networks (GAN) (Goodfellow et al., 2014) framework establishes a min-max adversarial game between two neural networks \u2013 a generative model, G, and a discrimi-", "startOffset": 42, "endOffset": 67}, {"referenceID": 5, "context": "The solution to this game can be expressed as following (Goodfellow et al., 2014): min G max D Ex\u223cpdata [logD(x)] + Ez\u223cp(z)[log(1\u2212D(G(z))]", "startOffset": 56, "endOffset": 81}, {"referenceID": 5, "context": "In the original generative adversarial networks (GAN) paper (Goodfellow et al., 2014), GANs were used to impose the data distribution at the pixel level on the output layer of a neural network.", "startOffset": 60, "endOffset": 85}, {"referenceID": 9, "context": "Generative moment matching networks (GMMN) (Li et al., 2015) use the maximum mean discrepancy (MMD) objective to shape the distribution of the output layer of a neural network.", "startOffset": 43, "endOffset": 60}, {"referenceID": 7, "context": "Recently, semi-supervised variational autoencoders have shown that the style and content of the images can be disentangled using an additional supervised cost (Kingma et al., 2014; Cheung et al., 2014).", "startOffset": 159, "endOffset": 201}, {"referenceID": 4, "context": "Recently, semi-supervised variational autoencoders have shown that the style and content of the images can be disentangled using an additional supervised cost (Kingma et al., 2014; Cheung et al., 2014).", "startOffset": 159, "endOffset": 201}, {"referenceID": 11, "context": "Figure 5b demonstrates the same experiment applied to Street View House Numbers dataset (Netzer et al., 2011).", "startOffset": 88, "endOffset": 109}, {"referenceID": 2, "context": "Several methods exist for quantitatively analyzing the quality of a generative model (Buades et al., 2005; Lyu & Simoncelli, 2009).", "startOffset": 85, "endOffset": 130}, {"referenceID": 5, "context": "In this section we measure the ability of the generative model to capture the data distribution by comparing the likelihood of this model to generate hold-out images on the MNIST and Toronto face dataset (TFD) using the evaluation procedure described in (Goodfellow et al., 2014).", "startOffset": 254, "endOffset": 279}, {"referenceID": 0, "context": "Thus, we calculate a lower bound of the true log-likelihood using the methods described in prior work (Bengio et al., 2013; 2014; Goodfellow et al., 2014).", "startOffset": 102, "endOffset": 154}, {"referenceID": 5, "context": "Thus, we calculate a lower bound of the true log-likelihood using the methods described in prior work (Bengio et al., 2013; 2014; Goodfellow et al., 2014).", "startOffset": 102, "endOffset": 154}, {"referenceID": 6, "context": "Table 1 compares the log-likelihood of the adversarial autoencoder for real-valued MNIST and TFD to many state-of-the-art methods including DBN (Hinton et al., 2006), Stacked CAE (Bengio et al.", "startOffset": 144, "endOffset": 165}, {"referenceID": 0, "context": ", 2006), Stacked CAE (Bengio et al., 2013), Deep GSN (Bengio et al.", "startOffset": 21, "endOffset": 42}, {"referenceID": 1, "context": ", 2013), Deep GSN (Bengio et al., 2014), Generative Adversarial Networks (Goodfellow et al.", "startOffset": 18, "endOffset": 39}, {"referenceID": 5, "context": ", 2014), Generative Adversarial Networks (Goodfellow et al., 2014) and GMMN + AE (Li et al.", "startOffset": 41, "endOffset": 66}, {"referenceID": 9, "context": ", 2014) and GMMN + AE (Li et al., 2015).", "startOffset": 22, "endOffset": 39}, {"referenceID": 6, "context": "MNIST (10K) MNIST (10M) TFD (10K) TFD (10M) DBN (Hinton et al., 2006) 138\u00b1 2 1909\u00b1 66 Stacked CAE (Bengio et al.", "startOffset": 48, "endOffset": 69}, {"referenceID": 0, "context": ", 2006) 138\u00b1 2 1909\u00b1 66 Stacked CAE (Bengio et al., 2013) 121\u00b1 1.", "startOffset": 36, "endOffset": 57}, {"referenceID": 1, "context": "6 2110\u00b1 50 Deep GSN (Bengio et al., 2014) 214\u00b1 1.", "startOffset": 20, "endOffset": 41}, {"referenceID": 5, "context": "1 1890\u00b1 29 GAN (Goodfellow et al., 2014) 225\u00b1 2 386 2057\u00b1 26 GMMN + AE (Li et al.", "startOffset": 15, "endOffset": 40}, {"referenceID": 9, "context": ", 2014) 225\u00b1 2 386 2057\u00b1 26 GMMN + AE (Li et al., 2015) 282\u00b1 2 2204\u00b1 20 Adversarial Autoencoder 340\u00b1 2 427 2252\u00b1 16 2522 Table 1: Log-likelihood of test data on MNIST and Toronto Face dataset.", "startOffset": 38, "endOffset": 55}, {"referenceID": 5, "context": "To obtain a comparison with a tighter lower bound, we additionally report Parzen window estimates evaluated with 10 million samples for both the adversarial autoencoders and the generative adversarial network (Goodfellow et al., 2014).", "startOffset": 209, "endOffset": 234}, {"referenceID": 7, "context": "Importantly, we demonstrated how it can be used to disentangle the style and label information of a dataset (Kingma et al., 2014; Cheung et al., 2014).", "startOffset": 108, "endOffset": 150}, {"referenceID": 4, "context": "Importantly, we demonstrated how it can be used to disentangle the style and label information of a dataset (Kingma et al., 2014; Cheung et al., 2014).", "startOffset": 108, "endOffset": 150}], "year": 2015, "abstractText": "In this paper we propose a new method for regularizing autoencoders by imposing an arbitrary prior on the latent representation of the autoencoder. Our method, named \u201cadversarial autoencoder\u201d, uses the recently proposed generative adversarial networks (GAN) in order to match the aggregated posterior of the hidden code vector of the autoencoder with an arbitrary prior. Matching the aggregated posterior to the prior ensures that there are no \u201choles\u201d in the prior, and generating from any part of prior space results in meaningful samples. As a result, the decoder of the adversarial autoencoder learns a deep generative model that maps the imposed prior to the data distribution. We show how adversarial autoencoders can be used to disentangle style and content of images and achieve competitive generative performance on MNIST, Street View House Numbers and Toronto Face datasets.", "creator": "LaTeX with hyperref package"}}}