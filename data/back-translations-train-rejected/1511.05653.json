{"id": "1511.05653", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "18-Nov-2015", "title": "Why are deep nets reversible: A simple theory, with implications for training", "abstract": "Generative model approaches to deep learning are of interest in the quest for both better understanding as well as training methods requiring fewer labeled samples.", "histories": [["v1", "Wed, 18 Nov 2015 04:33:09 GMT  (31kb)", "http://arxiv.org/abs/1511.05653v1", null], ["v2", "Thu, 19 Nov 2015 23:48:36 GMT  (862kb,D)", "http://arxiv.org/abs/1511.05653v2", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["sanjeev arora", "yingyu liang", "tengyu ma"], "accepted": false, "id": "1511.05653"}, "pdf": {"name": "1511.05653.pdf", "metadata": {"source": "CRF", "title": null, "authors": [], "emails": ["arora@cs.princeton.edu", "yingyul@cs.princeton.edu", "tengyu@cs.princeton.edu"], "sections": [{"heading": null, "text": "ar Xiv: 151 1.05 653v 1 [cs.L G] 18 Nov 2Generative approaches to deep learning are of interest both for the pursuit of a better understanding and for training methods that require less labeled samples. Recent work uses generative model approaches to generate the input of the deep mesh, as the value of a hidden layer is several levels above it. However, there is no accompanying \"proof of correctness\" for the generative model that shows that the forward-facing deep mesh is the correct sequence method for recovering the hidden layer. Furthermore, these models are complicated. The current work takes a more theoretical course. It presents a very simple generative model for RELU deep meshes with the following properties: (a) The generative model is only the inversion of the forward-facing mesh: If the forward transformation is at a layer A, then the backward transformation is AT. (This can be considered an explanation of the weighting method of the old one)."}, {"heading": "1 INTRODUCTION", "text": "In fact, most people are able to decide whether they will be able to play by the rules, or whether they will be able to break the rules."}, {"heading": "1.1 RELATED WORK", "text": "This year it has come to the point that it will only be a matter of time before it will happen, until it does."}, {"heading": "807\u2013814, 2010.", "text": "Andrew Y. Ng and Michael I. Jordan. On discriminative vs. generative classifiers: A comparison of logistic regression and naive bayes. In Advances in Neural Information Processing Systems 14 [Neural Information Processing Systems: Natural and Synthetic, NIPS 2001, December 3-8, 2001, Vancouver, British Columbia, Canada], pp. 841-848, 2001.A. B. Patel, T. Nguyen, and R. G. Baraniuk. A Probabilistic Theory of Deep Learning. ArXiv e-prints, 2015.Rajesh Ranganath, Linpeng Tang, Laurent Charlin, and David M. Blei. Deep exponential families. In AISTATS, 2015.Pascal Vincent, Hugo Larochelle, Yoshua Bengio, and Pierre-Antoine Manzagol. Extracting and composing robust features with denoising autoencoders. In BenICML, pp. 1096-1103, 2008.Pascal Vincent, Hugo Larozelle Anagine, Hugo Managine and Pierre-Pierre-Pierre Managagagol."}], "references": [{"title": "Provable bounds for learning some deep representations", "author": ["Sanjeev Arora", "Aditya Bhaskara", "Rong Ge", "Tengyu Ma"], "venue": "In Proceedings of the 31th International Conference on Machine Learning,", "citeRegEx": "Arora et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Arora et al\\.", "year": 2014}, {"title": "Learning deep architectures for AI", "author": ["Yoshua Bengio"], "venue": "Foundations and Trends in Machine Learning,", "citeRegEx": "Bengio.,? \\Q2009\\E", "shortCiteRegEx": "Bengio.", "year": 2009}, {"title": "Greedy layer-wise training of deep networks", "author": ["Yoshua Bengio", "Pascal Lamblin", "Dan Popovici", "Hugo Larochelle"], "venue": "In NIPS,", "citeRegEx": "Bengio et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Bengio et al\\.", "year": 2006}, {"title": "Deep generative stochastic networks trainable by backprop", "author": ["Yoshua Bengio", "Eric Thibodeau-Laufer", "Guillaume Alain", "Jason Yosinski"], "venue": "arXiv preprint arXiv:1306.1091,", "citeRegEx": "Bengio et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Bengio et al\\.", "year": 2013}, {"title": "Generalized denoising auto-encoders as generative models", "author": ["Yoshua Bengio", "Li Yao", "Guillaume Alain", "Pascal Vincent"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Bengio et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Bengio et al\\.", "year": 2013}, {"title": "Unsupervised learning of distributions on binary vectors using two layer networks", "author": ["Yoav Freund", "David Haussler"], "venue": "Technical report,", "citeRegEx": "Freund and Haussler.,? \\Q1994\\E", "shortCiteRegEx": "Freund and Haussler.", "year": 1994}, {"title": "Reducing the dimensionality of data with neural networks", "author": ["Geoffrey E Hinton", "Ruslan R Salakhutdinov"], "venue": null, "citeRegEx": "Hinton and Salakhutdinov.,? \\Q2006\\E", "shortCiteRegEx": "Hinton and Salakhutdinov.", "year": 2006}, {"title": "A fast learning algorithm for deep belief nets", "author": ["Geoffrey E. Hinton", "Simon Osindero", "Yee Whye Teh"], "venue": "Neural Computation,", "citeRegEx": "Hinton et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Hinton et al\\.", "year": 2006}, {"title": "Semisupervised learning with deep generative models", "author": ["Diederik P. Kingma", "Shakir Mohamed", "Danilo Jimenez Rezende", "Max Welling"], "venue": "In NIPS,", "citeRegEx": "Kingma et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Kingma et al\\.", "year": 2014}, {"title": "Convolutional deep belief networks for scalable unsupervised learning of hierarchical representations", "author": ["Honglak Lee", "Roger Grosse", "Rajesh Ranganath", "Andrew Y. Ng"], "venue": "In ICML,", "citeRegEx": "Lee et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Lee et al\\.", "year": 2009}, {"title": "Convolutional deep belief networks for scalable unsupervised learning of hierarchical representations", "author": ["Honglak Lee", "Roger Grosse", "Rajesh Ranganath", "Andrew Y Ng"], "venue": "In Proceedings of the 26th Annual International Conference on Machine Learning,", "citeRegEx": "Lee et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Lee et al\\.", "year": 2009}, {"title": "Understanding deep image representations by inverting them", "author": ["A. Mahendran", "A. Vedaldi"], "venue": "In IEEE Conference on Computer Vision and Pattern Recognition,", "citeRegEx": "Mahendran and Vedaldi.,? \\Q2015\\E", "shortCiteRegEx": "Mahendran and Vedaldi.", "year": 2015}, {"title": "Rectified linear units improve restricted boltzmann machines", "author": ["Vinod Nair", "Geoffrey E Hinton"], "venue": "In Proceedings of the 27th International Conference on Machine Learning", "citeRegEx": "Nair and Hinton.,? \\Q2010\\E", "shortCiteRegEx": "Nair and Hinton.", "year": 2010}, {"title": "On discriminative vs. generative classifiers: A comparison of logistic regression and naive bayes", "author": ["Andrew Y. Ng", "Michael I. Jordan"], "venue": "NIPS 2001, December", "citeRegEx": "Ng and Jordan.,? \\Q2001\\E", "shortCiteRegEx": "Ng and Jordan.", "year": 2001}, {"title": "A Probabilistic Theory of Deep Learning", "author": ["A.B. Patel", "T. Nguyen", "R.G. Baraniuk"], "venue": "ArXiv e-prints,", "citeRegEx": "Patel et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Patel et al\\.", "year": 2015}, {"title": "Deep exponential families", "author": ["Rajesh Ranganath", "Linpeng Tang", "Laurent Charlin", "David M. Blei"], "venue": "In AISTATS,", "citeRegEx": "Ranganath et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Ranganath et al\\.", "year": 2015}, {"title": "Extracting and composing robust features with denoising autoencoders", "author": ["Pascal Vincent", "Hugo Larochelle", "Yoshua Bengio", "Pierre-Antoine Manzagol"], "venue": "In ICML,", "citeRegEx": "Vincent et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Vincent et al\\.", "year": 2008}, {"title": "Stacked denoising autoencoders: Learning useful representations in a deep network with a local denoising criterion", "author": ["Pascal Vincent", "Hugo Larochelle", "Isabelle Lajoie", "Yoshua Bengio", "Pierre-Antoine Manzagol"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "Vincent et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Vincent et al\\.", "year": 2010}], "referenceMentions": [{"referenceID": 6, "context": "Discriminative/generative pairs of models for classification tasks are an old theme in machine learning (Ng and Jordan (2001)) A good generative model analog for for deep learning may not only cast new light on the discriminative backpropagation algorithm, but also allow learning with fewer labeled samples.", "startOffset": 105, "endOffset": 126}, {"referenceID": 1, "context": ", not tied to specific domain) approaches to defining such models include Restricted Boltzmann Machines (Freund and Haussler (1994); Hinton and Salakhutdinov (2006)) and Denoising Autoencoders Bengio et al.", "startOffset": 105, "endOffset": 132}, {"referenceID": 1, "context": ", not tied to specific domain) approaches to defining such models include Restricted Boltzmann Machines (Freund and Haussler (1994); Hinton and Salakhutdinov (2006)) and Denoising Autoencoders Bengio et al.", "startOffset": 105, "endOffset": 165}, {"referenceID": 1, "context": ", not tied to specific domain) approaches to defining such models include Restricted Boltzmann Machines (Freund and Haussler (1994); Hinton and Salakhutdinov (2006)) and Denoising Autoencoders Bengio et al. (2006); Vincent et al.", "startOffset": 193, "endOffset": 214}, {"referenceID": 1, "context": ", not tied to specific domain) approaches to defining such models include Restricted Boltzmann Machines (Freund and Haussler (1994); Hinton and Salakhutdinov (2006)) and Denoising Autoencoders Bengio et al. (2006); Vincent et al. (2008). Surprisingly, these suggest that deep nets are reversible: the generative model is a essentially the feedforward net run in reverse.", "startOffset": 193, "endOffset": 237}, {"referenceID": 1, "context": ", not tied to specific domain) approaches to defining such models include Restricted Boltzmann Machines (Freund and Haussler (1994); Hinton and Salakhutdinov (2006)) and Denoising Autoencoders Bengio et al. (2006); Vincent et al. (2008). Surprisingly, these suggest that deep nets are reversible: the generative model is a essentially the feedforward net run in reverse. There does not seem to be an explanation for why such reversible nets fit problems in varied domains. The above models have since led to refinements such as Stacked Denoising Autoencoders Vincent et al. (2010), Generalized Denoising Auto-Encoders Bengio et al.", "startOffset": 193, "endOffset": 581}, {"referenceID": 1, "context": ", not tied to specific domain) approaches to defining such models include Restricted Boltzmann Machines (Freund and Haussler (1994); Hinton and Salakhutdinov (2006)) and Denoising Autoencoders Bengio et al. (2006); Vincent et al. (2008). Surprisingly, these suggest that deep nets are reversible: the generative model is a essentially the feedforward net run in reverse. There does not seem to be an explanation for why such reversible nets fit problems in varied domains. The above models have since led to refinements such as Stacked Denoising Autoencoders Vincent et al. (2010), Generalized Denoising Auto-Encoders Bengio et al. (2013b) and Deep Generative Stochastic Networks Bengio et al.", "startOffset": 193, "endOffset": 640}, {"referenceID": 1, "context": ", not tied to specific domain) approaches to defining such models include Restricted Boltzmann Machines (Freund and Haussler (1994); Hinton and Salakhutdinov (2006)) and Denoising Autoencoders Bengio et al. (2006); Vincent et al. (2008). Surprisingly, these suggest that deep nets are reversible: the generative model is a essentially the feedforward net run in reverse. There does not seem to be an explanation for why such reversible nets fit problems in varied domains. The above models have since led to refinements such as Stacked Denoising Autoencoders Vincent et al. (2010), Generalized Denoising Auto-Encoders Bengio et al. (2013b) and Deep Generative Stochastic Networks Bengio et al. (2013a). In case of image recognition it is possible to working harder \u2014using a custom deep net to invert the feedforward net \u2014-and reproduce the input very well from the values of hidden layers much higher up (e.", "startOffset": 193, "endOffset": 702}, {"referenceID": 1, "context": ", not tied to specific domain) approaches to defining such models include Restricted Boltzmann Machines (Freund and Haussler (1994); Hinton and Salakhutdinov (2006)) and Denoising Autoencoders Bengio et al. (2006); Vincent et al. (2008). Surprisingly, these suggest that deep nets are reversible: the generative model is a essentially the feedforward net run in reverse. There does not seem to be an explanation for why such reversible nets fit problems in varied domains. The above models have since led to refinements such as Stacked Denoising Autoencoders Vincent et al. (2010), Generalized Denoising Auto-Encoders Bengio et al. (2013b) and Deep Generative Stochastic Networks Bengio et al. (2013a). In case of image recognition it is possible to working harder \u2014using a custom deep net to invert the feedforward net \u2014-and reproduce the input very well from the values of hidden layers much higher up (e.g., (Mahendran and Vedaldi (2015))), and in fact to generate new images very different from any that were used to train the net.", "startOffset": 193, "endOffset": 941}, {"referenceID": 11, "context": "The shadow distribution defined by the final net generates somewhat reasonable images, albeit cruder compared to say Mahendran and Vedaldi (2015).", "startOffset": 117, "endOffset": 146}, {"referenceID": 5, "context": "Deep Boltzmann Machine( Hinton et al. (2006)) is an attempt to define a generative model in the above sense, and is related to the older notion of autoencoder.", "startOffset": 24, "endOffset": 45}, {"referenceID": 0, "context": "But as pointed out in Bengio (2009) this does not yield a generative model per se since one cannot ensure that the marginal distributions of two adjacent layers match.", "startOffset": 22, "endOffset": 36}, {"referenceID": 0, "context": "But as pointed out in Bengio (2009) this does not yield a generative model per se since one cannot ensure that the marginal distributions of two adjacent layers match. On the other hand, if one defines the generative model by stacking the conditional probability of RBM, then the reversibility of RBM is lost since the joint probability is no longer nice. Thus the model violates one of (a) or (b). We know of no prior solution to this issue. (In the current paper it is resolved under the random-like nets hypothesis.) In (Lee et al. (2009b)) the RBM notion is extended to convolutional RBMs.", "startOffset": 22, "endOffset": 543}, {"referenceID": 0, "context": "But as pointed out in Bengio (2009) this does not yield a generative model per se since one cannot ensure that the marginal distributions of two adjacent layers match. On the other hand, if one defines the generative model by stacking the conditional probability of RBM, then the reversibility of RBM is lost since the joint probability is no longer nice. Thus the model violates one of (a) or (b). We know of no prior solution to this issue. (In the current paper it is resolved under the random-like nets hypothesis.) In (Lee et al. (2009b)) the RBM notion is extended to convolutional RBMs. In (Nair and Hinton (2010)), the theory of RBMs is extended to allow rectifier linear units, but this involves approximating RELU\u2019s with multiple binary units, which seems inefficient.", "startOffset": 22, "endOffset": 621}, {"referenceID": 0, "context": "But as pointed out in Bengio (2009) this does not yield a generative model per se since one cannot ensure that the marginal distributions of two adjacent layers match. On the other hand, if one defines the generative model by stacking the conditional probability of RBM, then the reversibility of RBM is lost since the joint probability is no longer nice. Thus the model violates one of (a) or (b). We know of no prior solution to this issue. (In the current paper it is resolved under the random-like nets hypothesis.) In (Lee et al. (2009b)) the RBM notion is extended to convolutional RBMs. In (Nair and Hinton (2010)), the theory of RBMs is extended to allow rectifier linear units, but this involves approximating RELU\u2019s with multiple binary units, which seems inefficient. (Our generative model below will sidestep this inefficient conversion to binary and work directly with RELUs in forward and backward direction.) Recently, a sequence of papers define hierarchichal probabilistic models (Ranganath et al. (2015); Kingma et al.", "startOffset": 22, "endOffset": 1022}, {"referenceID": 0, "context": "But as pointed out in Bengio (2009) this does not yield a generative model per se since one cannot ensure that the marginal distributions of two adjacent layers match. On the other hand, if one defines the generative model by stacking the conditional probability of RBM, then the reversibility of RBM is lost since the joint probability is no longer nice. Thus the model violates one of (a) or (b). We know of no prior solution to this issue. (In the current paper it is resolved under the random-like nets hypothesis.) In (Lee et al. (2009b)) the RBM notion is extended to convolutional RBMs. In (Nair and Hinton (2010)), the theory of RBMs is extended to allow rectifier linear units, but this involves approximating RELU\u2019s with multiple binary units, which seems inefficient. (Our generative model below will sidestep this inefficient conversion to binary and work directly with RELUs in forward and backward direction.) Recently, a sequence of papers define hierarchichal probabilistic models (Ranganath et al. (2015); Kingma et al. (2014); Patel et al.", "startOffset": 22, "endOffset": 1044}, {"referenceID": 0, "context": "But as pointed out in Bengio (2009) this does not yield a generative model per se since one cannot ensure that the marginal distributions of two adjacent layers match. On the other hand, if one defines the generative model by stacking the conditional probability of RBM, then the reversibility of RBM is lost since the joint probability is no longer nice. Thus the model violates one of (a) or (b). We know of no prior solution to this issue. (In the current paper it is resolved under the random-like nets hypothesis.) In (Lee et al. (2009b)) the RBM notion is extended to convolutional RBMs. In (Nair and Hinton (2010)), the theory of RBMs is extended to allow rectifier linear units, but this involves approximating RELU\u2019s with multiple binary units, which seems inefficient. (Our generative model below will sidestep this inefficient conversion to binary and work directly with RELUs in forward and backward direction.) Recently, a sequence of papers define hierarchichal probabilistic models (Ranganath et al. (2015); Kingma et al. (2014); Patel et al. (2015)) that are plausibly reminiscent of standard deep nets.", "startOffset": 22, "endOffset": 1065}, {"referenceID": 0, "context": "The paper of (Arora et al. (2014)) defines a consistent generative model satisfying (a) and (b) under some restrictive conditions: the neural net edge weights are random numbers, and the connections satisfy some conditions on degrees, sparsity of each layer etc.", "startOffset": 14, "endOffset": 34}, {"referenceID": 9, "context": "Finally, several works have tried to show that the deep nets can be inverted, such that the observable layer can be recovered from its representation at some very high hidden layer of the net Lee et al. (2009a). The recovery problem is solved in the recent pape Mahendran and Vedaldi (2015) also", "startOffset": 192, "endOffset": 211}, {"referenceID": 9, "context": "Finally, several works have tried to show that the deep nets can be inverted, such that the observable layer can be recovered from its representation at some very high hidden layer of the net Lee et al. (2009a). The recovery problem is solved in the recent pape Mahendran and Vedaldi (2015) also", "startOffset": 192, "endOffset": 291}], "year": 2017, "abstractText": "Generative model approaches to deep learning are of interest in the quest for both better understanding as well as training methods requiring fewer labeled samples. Recent works use generative model approaches to produce the deep net\u2019s input given the value of a hidden layer several levels above. However, there is no accompanying \u201cproof of correctness,\u201d for the generative model, showing that the feedforward deep net is the correct inference method for recovering the hidden layer given the input. Furthermore, these models are complicated. The current paper takes a more theoretical tack. It presents a very simple generative model for RELU deep nets, with the following characteristics: (a) The generative model is just the reverse of the feedforward net: if the forward transformation at a layer is A then the reverse transformation is A . (This can be seen as an explanation of the old weight tying method for denoising autoencoders.) (b) Its correctness can be proven under a clean theoretical assumption: the edge weights in real-life deep nets behave like random numbers. Under this assumption \u2014which is experimentally tested on real-life nets like AlexNet\u2014 it is formally proved that feed forward net is a correct inference method for recovering the hidden layer. (c) The generative model suggests a simple modification for training\u2014use an input to produce several synthetic inputs with the same label, and include them in the backprop training. This appears to yield benefits similar to dropout, and can also be seen as a generative explanation for the efficacy of dropout.", "creator": "LaTeX with hyperref package"}}}