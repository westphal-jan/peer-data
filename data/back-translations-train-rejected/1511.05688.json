{"id": "1511.05688", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "18-Nov-2015", "title": "A Distribution Adaptive Framework for Prediction Interval Estimation Using Nominal Variables", "abstract": "Proposed methods for prediction interval estimation focus on cases where input variables are numerical. Since categorical input variables can also be represented using numerical variables, such methods are applicable to most datasets. We propose a new prediction interval estimation method tailored for datasets with nominal input variables. This method has two main phases: I) modeling the output variable distribution separately for groups of samples with equal input vectors II) training the parameters of the predictor such that, it can best represent the relationship between the input variables and modeled distributions of output variables. The trained predictor is then used to provide prediction intervals for a new input. For Gaussian distribution, our experiment on synthetic dataset show higher performance when compared to the popular bootstrap method.", "histories": [["v1", "Wed, 18 Nov 2015 08:13:35 GMT  (449kb,D)", "https://arxiv.org/abs/1511.05688v1", null], ["v2", "Fri, 20 Nov 2015 08:12:23 GMT  (449kb,D)", "http://arxiv.org/abs/1511.05688v2", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["ameen eetemadi", "ilias tagkopoulos"], "accepted": false, "id": "1511.05688"}, "pdf": {"name": "1511.05688.pdf", "metadata": {"source": "CRF", "title": "DICTION INTERVAL ESTIMATION USING NOMINAL VARIABLES", "authors": ["Ameen Eetemadi", "Ilias Tagkopoulos"], "emails": ["eetemadi@ucdavis.edu", "itagkopoulos@ucdavis.edu"], "sections": [{"heading": "1 INTRODUCTION", "text": "Although most of the known regression methods are designed to provide a score prediction for a real, estimated variable, this is not always sufficient; there are many cases where stakeholders are interested in knowing the accuracy of individual predictions; in machine learning literature, this is sometimes referred to as Conformal Prediction (CP) Papadopoulos & Haralambous (2011), and others simply point out that they provide prediction intervals (PI) Khosravi et al. (2011); in the general section on Single Task Learning (STL), a regressive CP method forms a predictive model; the predictive model takes an input feature vector x-Rd, a desired confidence level \u03b1 [0, 100] and provides a PI [yl, yu] for the output of variable y-R. It is conceivable to have a model that would predict multiple intervals, but only one case we examine."}, {"heading": "2 RELATED WORK", "text": "Khosravi et al. (2011) provides a comprehensive overview of neural network-based prediction intervals, followed by a brief summary of PI benchmarks and a discussion of the latest developments in PI prediction methods, some of which have not yet been reviewed jointly."}, {"heading": "2.1 PI ASSESSMENT MEASURES", "text": "When it comes to evaluating the quality of PIs, it is customary to focus exclusively on the overall coverage of the calculated intervals (ToDo: Add ref from Khosravi et al. (2011). Such an approach can lead to prediction methods that offer large intervals and therefore provide minimal specificity for the predicted result. Among the most meaningful PI quality metrics are the following. \u2022 Prediction intervals Coverage Probability (PICP) is calculated by measuring the proportion of target values covered by the PIsPICP method. \u2022 Prediction interval Coverage Probability (PICP) is calculated by quantifying the total breadth of the PIsPICP method."}, {"heading": "2.2 NEURAL NETWORK-BASED METHODS FOR PROVIDING PREDICTION INTERVALS", "text": "Before introducing the new DAPIEN method, it is important to understand existing methods to see why a method tailored to categorical datasets is necessary."}, {"heading": "2.2.1 BOOTSTRAP", "text": "The most common method of constructing PIs is Heske's Bootstrap Method (1997), which consists of two phases: First, it estimates the portion of the error caused by inaccurate modeling, and then estimates the error caused by noise in the observed output itself. In the first phase, bootstrap resampling is used to generate B data sets, which are then used to train the prediction method. Although what we describe is applicable in many regressors and classifiers, for the rest of the work, we focus on artificial neural networks (ANN). When a new sample is provided for prediction, all trained ANNs are used to provide B predictions. The variance of the predictions represents the modeling error. In the second phase, a separate ANN is trained to predict the output error. This is done by generating a new sample for predicting the residual error, and each sample for predicting the residual error is generated."}, {"heading": "2.2.2 BAYESIAN AND DELTA", "text": "Although the Bavarian Bishops et al. (1995) and Delta Hwang & Ding (1997) differ greatly from each other, they exhibit similar characteristics. Similar to the bootstrap, they distinguish between modeling errors and data errors, but unlike the bootstrap, they assume a normal distribution of error regardless of the initial values (where Delta has stronger assumptions) and also take into account the costly calculation of the Hessian matrix. Both provide high-quality PIs when the assumptions for error distribution are valid. Bavarian method for the formation of ANN has a particularly strong mathematical basis and good generalization errors Khosravi et al. (2011)."}, {"heading": "2.2.3 MVE", "text": "The Nix & Weigend Method for Mean and Variance Estimation (1994) also assumes that the errors are normally distributed around the mean of the target. Although the Bayese and Delta Method assumes a fixed variance, MVE assumes that the variance is also a function of the input and therefore trains two different neural networks to estimate the mean and the variance of the output. A disadvantage of this method is the ignorance of the modelling error Khosravi et al. (2011), which affects the quality of the predicted intervals."}, {"heading": "2.2.4 BACK-PROPAGATION OF PSEUDO-ERRORS", "text": "In Ding & He (2003), the authors consider a case where the error distribution is not necessarily normal and can be distorted; the idea is to integrate the Box-Cox transformation Sakia (1992) during training to model non-Gaussian noise distributions and then apply the bootstrap method to provide PIs."}, {"heading": "2.2.5 COMBINED PIS", "text": "Khosravi et al. (2011) propose an ensemble method using Bootstrap, Bayesian, Delta and MVE methods. First, the dataset is divided into two parts D1, D2. D1 is used to train all methods, then an ensemble approach is chosen by providing new PI that represent a linear combination of the intervals provided by all four approaches, and the parameters of the linear combination are estimated in an optimization process to minimize the overall measurement of CWC over D2. Due to the shape of the CWC function, a gradient descent cannot be used, but an approach of the Genetic Algorithm is chosen. Authors show that the combined PI approach consistently outperforms the individual methods in comparison based on the CWC measure."}, {"heading": "3 METHOD", "text": "All PI prediction methods described in 2.2 deal with the case in which the input vector has real value, and therefore lack the ability to measure and model the variance of the output within a group of samples with identical nominal inputs. In other words, although all nominal input vectors can be represented with real value vectors, the quality of the prediction intervals can be undermined. Here, we discuss the problem of estimating the confidence interval for an output variable y-R given the input binary vector x = (x1, x2,..., xd) in which xi variable {0, 1} represents nominal characteristics. Suppose that the output variable y-R is a single scale value. However, the same method can be generalized to multitask learning for predicting multiple output variables. Let's also assume that the distribution of the input function is variable, where the input function follows the input function independently of the variable."}, {"heading": "3.1 GENERAL PROCEDURE", "text": "1. In view of a distribution function f\u03b8 (y), which identifies the individual input preference vectors xu, u = 1... p. For each xu, there is a corresponding yu, which consists of all yi data sets that share the same xi (b) data sets. Construct p data sets, each containing samples with the same input vectors, represented by Du = {xu, yu}, u = 1. p, so that xu-Rd, yu-RNu contain the number of samples with input vectors x = xu (c). Using each data set Du, you estimate two types that represent the parameters of the distributions f\u03b81 (y | x1)."}, {"heading": "3.2 GAUSSIAN DISTRIBUTION PROCEDURE", "text": "Consider the case where a single output variable follows a Gaussian distribution, while the parameters of the distribution may vary relative to the input vector x. To provide prediction intervals, the general procedure is followed as in Section 3.2. In step 1c, for each unique xu, the Gaussian distribution parameters \u03b8u = (\u00b5u, \u03c32u) from each dataset Du would be calculated. After constructing Ddist, a predictor would be trained to make predictions based on a new input. Finally, (as in step 4), the t distribution of the student is used to provide the prediction interval. Considering a confidence level \u03b1 and degree of freedom (nDF), the confidence is calculated from the t table. The nDF is the average number of samples in Dus. Hence, the following PI using the estimated values of \u00b5 and \u03c3 and the T-cent of the confidentier confidentiality condensity: \u2264-nomenciency] is missing."}, {"heading": "3.3 GAMMA DISTRIBUTION PROCEDURE", "text": "Although Gaussian distributions are widely used, they are not applicable to many datasets. For example, the stationary probability distribution of the protein number per cell is known to follow a gamma distribution Cai et al. (2006). For a gamma probability distribution, the general procedure is followed in Section 3.2. in Step 1c, the gamma distribution parameters \u03b8u = (\u03b1u, \u03b2u, \u00b5u) are estimated for each xu (\u03b1 is the form parameter, \u03b2 is the scale parameter and \u00b5 is the position parameter). Then, similar to Section 3.2, the dist is constructed using the xu parameters and estimated values of \u03b8. Subsequently, a model is trained using dist as described in 3.2. To provide the prediction interval (as in Step 4), the inverse gamma distribution function F \u2212 1 (see Equation (8) is used to construct the confidence rate in 4.0, the confidence rate in point 4.0, the confidence rate in point 4.0, the confidence rate in point in point 4.0, the confidence rate in point 4.0, the confidence rate in point in point 3.2, step 4, the confidence function in point \u2212."}, {"heading": "4 RESULTS", "text": "At an early stage of the methodological evaluation of performance, synthetic data was used, enabling us to (i) introduce controlled errors into the dataset, (ii) ensure that there is an underlying pattern in the dataset that must be detected by training ANN, and (iii) examine and debug a given method on a smaller scale prior to scaling. Therefore, two datasets with different characteristics are created to evaluate the performance of the proposed method against the common bootstrap method in the case where the input variable consists solely of categorical values. Table 1 shows the overall results in which the proposed DAPIEN method provides higher PICP levels while maintaining an appropriate MPIW level.The following sections provide more details about the data generation process as well as the visualization of performance for each method. 4.1 DatasetA: SUM OF INPUT BITS ONDITIONAL data is generated in this NOMATIC experiment."}, {"heading": "4.1.1 RESULTS", "text": "In accordance with the procedure mentioned in 3.2, 3.1, two separate feed networks are trained without a hidden layer. An exponential activation function is added for the FNN predicting \u03c3 to ensure that predictions are always positive. Hecht-Nielsen (1989) reverse propagation is used to optimize the neural network. To find the optimal weights, both stochastic gradient descent (SGD) and conjugate gradient (CG) were first tried. However, for our data set, CG delivered a much faster convergence rate with a full batch, which is in line with the optimization guidelines of LeCun et al. (2012) for small networks with small datasets. To avoid overmatching, a layered 5-fold cross validation was used to select the model with the lowest generalization error."}, {"heading": "4.2.1 RESULTS", "text": "As shown in Figure 2, DAPIEN adjusts the PI width when noise increases at higher target values, while the bootstrap method provides a similar PI width regardless of the target value.4.3 Dataset C: SUM OF INPUT BITS, WITH ADDITIONAL GAMMA NOISE BY TARGET VALUEDataset C generates datasets in a similar way to dataset B, except for the added error, a gamma distribution follows as in Equation (15).err (x) = f (x) \u00b7 gamma (\u03b1 = 1, \u03b2 = 1, \u00b5 = 0) (15)"}, {"heading": "4.3.1 RESULTS", "text": "Using the 3.3 method, three single-layer FNN predictors were trained to predict the gamma distribution parameters \u03b1, \u03b2 and \u00b5. The same optimization techniques described in 4.1.1 were used. Figure 3 shows the comparison of this technique with the bootstrap method using the 95% confidence level. DAPIEN method is able to restore the original function and provide appropriate prediction intervals. Although the prediction intervals provided by the bootstrap method cover 99% of the data set, they are too wide."}, {"heading": "5 CONCLUSION", "text": "The results presented on synthetic data suggest that DAPIEN can provide accurate prediction intervals for datasets with categorical input variables. In particular, the performance of this method depends on the appropriate selection of the distribution function. It can model a system in which the parameters of the target distribution can change as input is made, while the distribution function remains the same. This method can play a key role in the development of more accurate predictions when input variables are exclusively categorical in nature. In future work, we intend to use this technique to provide prediction intervals for the level of gene expression under genetic and environmental conditions."}], "references": [{"title": "Neural networks for pattern recognition", "author": ["Bishop", "Christopher M"], "venue": null, "citeRegEx": "Bishop and M,? \\Q1995\\E", "shortCiteRegEx": "Bishop and M", "year": 1995}, {"title": "Stochastic protein expression in individual cells at the single molecule", "author": ["Cai", "Long", "Friedman", "Nir", "Xie", "X Sunney"], "venue": "level. Nature,", "citeRegEx": "Cai et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Cai et al\\.", "year": 2006}, {"title": "Backpropagation of pseudo-errors: neural networks that are adaptive to heterogeneous noise", "author": ["Ding", "Aidong Adam", "He", "Xiali"], "venue": "Neural Networks, IEEE Transactions on,", "citeRegEx": "Ding et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Ding et al\\.", "year": 2003}, {"title": "Theory of the backpropagation neural network", "author": ["Hecht-Nielsen", "Robert"], "venue": "In Neural Networks,", "citeRegEx": "Hecht.Nielsen and Robert.,? \\Q1989\\E", "shortCiteRegEx": "Hecht.Nielsen and Robert.", "year": 1989}, {"title": "Practical confidence and prediction intervals", "author": ["Heskes", "Tom"], "venue": "Advances in neural information processing systems,", "citeRegEx": "Heskes and Tom.,? \\Q1997\\E", "shortCiteRegEx": "Heskes and Tom.", "year": 1997}, {"title": "A fast learning algorithm for deep belief nets", "author": ["Hinton", "Geoffrey", "Osindero", "Simon", "Teh", "Yee-Whye"], "venue": "Neural computation,", "citeRegEx": "Hinton et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Hinton et al\\.", "year": 2006}, {"title": "Prediction intervals for artificial neural networks", "author": ["Hwang", "JT Gene", "Ding", "A Adam"], "venue": "Journal of the American Statistical Association,", "citeRegEx": "Hwang et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Hwang et al\\.", "year": 1997}, {"title": "Comprehensive review of neural network-based prediction intervals and new advances", "author": ["Khosravi", "Abbas", "Nahavandi", "Saeid", "Creighton", "Doug", "Atiya", "Amir F"], "venue": "Neural Networks, IEEE Transactions on,", "citeRegEx": "Khosravi et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Khosravi et al\\.", "year": 2011}, {"title": "Efficient backprop", "author": ["LeCun", "Yann A", "Bottou", "L\u00e9on", "Orr", "Genevieve B", "M\u00fcller", "Klaus-Robert"], "venue": "In Neural networks: Tricks of the trade,", "citeRegEx": "LeCun et al\\.,? \\Q2012\\E", "shortCiteRegEx": "LeCun et al\\.", "year": 2012}, {"title": "Convolutional deep belief networks for scalable unsupervised learning of hierarchical representations", "author": ["Lee", "Honglak", "Grosse", "Roger", "Ranganath", "Rajesh", "Ng", "Andrew Y"], "venue": "In Proceedings of the 26th Annual International Conference on Machine Learning,", "citeRegEx": "Lee et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Lee et al\\.", "year": 2009}, {"title": "Estimating the mean and variance of the target probability distribution", "author": ["Nix", "David A", "Weigend", "Andreas S"], "venue": "In Neural Networks,", "citeRegEx": "Nix et al\\.,? \\Q1994\\E", "shortCiteRegEx": "Nix et al\\.", "year": 1994}, {"title": "Reliable prediction intervals with regression neural networks", "author": ["Papadopoulos", "Harris", "Haralambous", "Haris"], "venue": "Neural Networks,", "citeRegEx": "Papadopoulos et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Papadopoulos et al\\.", "year": 2011}, {"title": "The box-cox transformation technique: a review", "author": ["Sakia", "RM"], "venue": "The statistician, pp", "citeRegEx": "Sakia and RM.,? \\Q1992\\E", "shortCiteRegEx": "Sakia and RM.", "year": 1992}, {"title": "Conversational speech transcription using context-dependent deep neural networks", "author": ["Seide", "Frank", "Li", "Gang", "Yu", "Dong"], "venue": "In Interspeech, pp", "citeRegEx": "Seide et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Seide et al\\.", "year": 2011}], "referenceMentions": [{"referenceID": 7, "context": "In the machine learning literature this is sometimes referred to as Conformal Prediction (CP) Papadopoulos & Haralambous (2011) and others simply refer to it as providing Prediction Interval (PI) Khosravi et al. (2011). In the general single-task learning (STL) problem, a regression CP method trains a predictive model.", "startOffset": 196, "endOffset": 219}, {"referenceID": 5, "context": "record breaking achievements through the application of ANNs particularly in speech recognition, computer vision, machine translation and predictive genomics in the context of deep learning Hinton et al. (2006); Seide et al.", "startOffset": 190, "endOffset": 211}, {"referenceID": 5, "context": "record breaking achievements through the application of ANNs particularly in speech recognition, computer vision, machine translation and predictive genomics in the context of deep learning Hinton et al. (2006); Seide et al. (2011); Lee et al.", "startOffset": 190, "endOffset": 232}, {"referenceID": 5, "context": "record breaking achievements through the application of ANNs particularly in speech recognition, computer vision, machine translation and predictive genomics in the context of deep learning Hinton et al. (2006); Seide et al. (2011); Lee et al. (2009); ?.", "startOffset": 190, "endOffset": 251}, {"referenceID": 7, "context": "An extensive review for neural network-based prediction intervals is provided in Khosravi et al. (2011). Next we provide a concise summary of PI assessment measures and also discuss the latest developments in PI prediction methods, some of which have not been reviewed together before.", "startOffset": 81, "endOffset": 104}, {"referenceID": 7, "context": "1 PI ASSESSMENT MEASURES When it comes to assessing the quality of PIs, it is common to solely focus on the overall coverage of the calculated intervals [ToDo: Add ref from Khosravi et al. (2011)].", "startOffset": 173, "endOffset": 196}, {"referenceID": 7, "context": "The Bayesian method for training ANN, has particularly strong mathematical foundation and good generalization error Khosravi et al. (2011).", "startOffset": 116, "endOffset": 139}, {"referenceID": 7, "context": "One drawback for this method, is ignoring the modeling error Khosravi et al. (2011) which affects the quality of the predicted intervals.", "startOffset": 61, "endOffset": 84}, {"referenceID": 7, "context": "5 COMBINED PIS Khosravi et al. (2011) proposes an ensemble method using Bootstrap, Bayesian, Delta and MVE methods.", "startOffset": 15, "endOffset": 38}, {"referenceID": 1, "context": "For example the steady-state probability distribution of protein number per cell is known to follow a gamma distribution Cai et al. (2006). For a Gamma probability distribution, the general procedure in section 3.", "startOffset": 121, "endOffset": 139}, {"referenceID": 8, "context": "This is in accordance with optimization guidelines provided in LeCun et al. (2012) for small networks with small datasets.", "startOffset": 63, "endOffset": 83}], "year": 2015, "abstractText": "Proposed methods for prediction interval estimation so far focus on cases where input variables are numerical. In datasets with solely nominal input variables, we observe records with the exact same input x, but different real valued outputs due to the inherent noise in the system. Existing prediction interval estimation methods do not use representations that can accurately model such inherent noise in the case of nominal inputs. We propose a new prediction interval estimation method tailored for this type of data, which is prevalent in biology and medicine. We call this method Distribution Adaptive Prediction Interval Estimation given Nominal inputs (DAPIEN) and has four main phases. First, we select a distribution function that can best represent the inherent noise of the system for all unique inputs. Then we infer the parameters \u03b8i (e.g. \u03b8i = [meani, variancei]) of the selected distribution function for all unique input vectors xi and generate a new corresponding training set using pairs of xi , \u03b8i. III). Then, we train a model to predict \u03b8 given a new xu. Finally, we calculate the prediction interval for a new sample using the inverse of the cumulative distribution function once the parameters \u03b8 is predicted by the trained model. We compared DAPIEN to the commonly used Bootstrap method on three synthetic datasets. Our results show that DAPIEN provides tighter prediction intervals while preserving the requested coverage when compared to Bootstrap. This work can facilitate broader usage of regression methods in medicine and biology where it is necessary to provide tight prediction intervals while preserving coverage when input variables are nominal.", "creator": "LaTeX with hyperref package"}}}