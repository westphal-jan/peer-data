{"id": "1511.05743", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "18-Nov-2015", "title": "Sparse learning of maximum likelihood model for optimization of complex loss function", "abstract": "Traditional machine learning methods usually minimize a simple loss function to learn a predictive model, and then use a complex performance measure to measure the prediction performance. However, minimizing a simple loss function cannot guarantee that an optimal performance. In this paper, we study the problem of optimizing the complex performance measure directly to obtain a predictive model. We proposed to construct a maximum likelihood model for this problem, and to learn the model parameter, we minimize a com- plex loss function corresponding to the desired complex performance measure. To optimize the loss function, we approximate the upper bound of the complex loss. We also propose impose the sparsity to the model parameter to obtain a sparse model. An objective is constructed by combining the upper bound of the loss function and the sparsity of the model parameter, and we develop an iterative algorithm to minimize it by using the fast iterative shrinkage- thresholding algorithm framework. The experiments on optimization on three different complex performance measures, including F-score, receiver operating characteristic curve, and recall precision curve break even point, over three real-world applications, aircraft event recognition of civil aviation safety, in- trusion detection in wireless mesh networks, and image classification, show the advantages of the proposed method over state-of-the-art methods.", "histories": [["v1", "Wed, 18 Nov 2015 11:40:02 GMT  (50kb)", "http://arxiv.org/abs/1511.05743v1", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["ning zhang", "prathamesh chandrasekar"], "accepted": false, "id": "1511.05743"}, "pdf": {"name": "1511.05743.pdf", "metadata": {"source": "CRF", "title": null, "authors": ["Prathamesh Chandrasekar", "Ning Zhang"], "emails": ["zhangning115@yahoo.com", "prathameshchandrasekar@yahoo.com"], "sections": [{"heading": null, "text": "ar Xiv: 151 1.05 743v 1 [cs.L G] 18 Nov 2Keywords Machine learning \u00b7 Complex multivariate performance \u00b7 Economical learning \u00b7 Maximum probability \u00b7 Civil aviation safety N. Zhang Guangzhou Civil Aviation College, Guangzhou 510403, China Email: zhangning115 @ yahoo.com P. Chandrasekar Uttar Pradesh Technical University, Lucknow, Uttar Pradesh 226021, India Email: prathameshchandrasekar @ yahoo.com"}, {"heading": "1 Introduction", "text": "In fact, it is in such a way that the greater part of them are able to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to fight, to fight, to move, to fight, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to fight, to move, to fight, to fight, to fight, to move, to fight, to fight, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to fight, to move, to move, to move, to move, to move, to fight, to move, to move, to move, to move, to fight, to move, to move, to fight, to move, to move, to move, to move, to fight, to move, to move, to move, to fight, to move, to move, to move, to move, to move, to fight, to move, to move, to move, to move, to move, to fight, to move, to move, to move, to move, to move, to move, to fight, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to"}, {"heading": "2.2.1 Search point step", "text": "In this step, when looking for an objective function f (w) with respect to a variable vector w (w) with an increment L and a previous solution w (w), we search for a search point u (w) as follows. \u2212 Due to the complexity of the function f (w), the close form of the gradient function f (w) is difficult to obtain (14), where the gradient function f (w) is the gradient function of f (w). \u2212 In each iteration, we first define w as wpre, and then calculate the gradient function f (w) (w) (w)."}, {"heading": "2.2.2 Weighting factor step", "text": "We assume that the weighting factor of the previous iteration is \u03c4pre, we can get the weighting factor of the current iteration, \u03c4cur, as follows: \u03c4cur = 1 + \u221a 1 + 4\u03c4pre22. (18)"}, {"heading": "2.2.3 Solution update step", "text": "After we have set the search point of this current iteration 1: FISTA constant size to constant optimization (13), we assume that we have the problem in (13).The iterative algorithm is converted to algorithm 1.algorithm 1: FISTA constant size (13).We have changed the search point in algorithm 1.algorithm constant size to optimization (13).We have changed the search point in algorithm 1.algorithm constant size to optimization (13).We have the problem in (13).The iterative algorithm is converted to algorithm 1.algorithm 1: FISTA constant size to optimization (13).We have changed the search point in algorithm 1.algorithm constant size to optimization (13).We have optimized the search point of the iterative algorithms to optimize the problem in (13)."}, {"heading": "3 Experiment", "text": "In this section, we evaluate the proposed SMLM to optimize complex loss functions. Three different applications are considered, namely the detection of events in aircraft, the detection of intrusions into wireless networks and the classification of image recognition. 3.1 The detection of events during landing of aircraft is an important problem in the field of safety research in civil aviation. This method provides important information for the fault diagnosis and structural maintenance of aircraft [36]. In view of a landing state, we want to predict whether it is normal and abnormal. To this end, we extract some features and use them to predict the aircraft event of normal or abnormal. In this experiment, we evaluate the proposed algorithm in this application and use it as a model for predicting events in aircraft."}, {"heading": "3.1.1 Data set", "text": "In this experiment, we collect a dataset of 160 data points. Each data point is a landing condition, and we describe the landing condition based on five characteristics, including vertical acceleration, vertical velocity, lateral acceleration, rolling angle, and climb rate. Data points are divided into two classes, normal class and abnormal class. Normal class is treated as a positive class, while abnormal class is treated as a negative class. The number of positive data points is 108 and the number of negative data points 52."}, {"heading": "3.1.2 Experiment setup", "text": "In this experiment, we use 10-fold cross validation. The data set is divided into 10 folds, randomly, and each fold contains 16 data points. Each fold is used as a test set, and the remaining 9 folds are combined and used as a training set. The proposed model is training on the training set, and then used to predict the class names of the test data points in the test set. Prediction results are evaluated by a performance measurement, which is used to compare the true class names of the test data points against the predicted class names. In the training process, a complex loss function corresponding to the performance measurement is minimized. In our experiments, we look at three performance measurements that are F-score, range below Receiver Operating Curve (AUROC), and negative curve description curve limitation (RPBEP). To define these performance measurements, we must first predict the following points:"}, {"heading": "3.1.3 Experiment result", "text": "We compare the proposed algorithm, SMLM, with several state-of-the-art complex methods for loss optimization, including support for multivariate performance optimization (SVMmulti) vector computers [9], classifier adjustment for multivariate performance optimization (CAPO) [12], and selection functions for multivariate performance optimization (FSmulti) [17]. However, the box plots of optimized F-scores of 10-fold cross-validation of different algorithms on the problem of aircraft event detection are given in Fig. 1, these optimized AUROC attacks are given in Fig. 2, and those of optimized PRBEP are shown in Fig. 3. From these numbers, we can see that the proposed method, SMLM, exceeds the compared algorithms on three different optimized performances. For example, in Fig. 3, we can see that the boxplot of PRBEP of SMLM trusion is significantly higher than other methods overall, while the mean SML6 is almost 0.6."}, {"heading": "3.2.1 Data set", "text": "In this experiment we use the dataset KDD CPU1999. This dataset contains 4,000 attack datasets, and for each class there are 1,0000 datasets. For each dataset we first edit the dataset and then convert the attributes as new attributes into digital signatures."}, {"heading": "3.2.2 Experiment setup", "text": "In this experiment we also use the 10-fold cross-validation and the performance variables F-Score, AUROC and RPBEP."}, {"heading": "3.2.3 Experiment result", "text": "The box plots of the optimised F-scores of the ten-fold cross-validation are shown in Fig. 4, the box plots of AUROC in Fig. 5 and the box plots of PRBEP in Fig. 6. Similar to the results on the problem of aircraft event detection, the performance of the proposed algorithm, SMLM, is also important compared to other methods. This is strong evidence of the advantages of sparse learning and maximum probability. 3.3 ImageNet image classification In the third experiment, we use a large image set to test the performance of the proposed algorithm with big data."}, {"heading": "3.3.1 Data set", "text": "In this experiment, we use a large dataset, ImageNet [11]. This dataset contains over 15 million images, and the images belong to 22,000 classes. These images come from websites and are manually labeled by humans. The entire dataset is divided into three sub-sets, i.e. a training set, a validation set and a test set. The training set contains 1.2 million images, the validation set contains 50,000 images, and the test set contains 150,000 images. To display each image, we use the Bag-of-Features method. Local SIFT features are extracted from each image and quantified into a histogram. Features can be downloaded directly from http: / / image-net.org / download-features."}, {"heading": "3.3.2 Experiment setup", "text": "In this experiment, we do not use 10-fold cross-validation, but the predefined division of the training / validation / test set. First, we run the proposed algorithm on the training set to learn the classifier, then we use the validation set to justify the optimal target parameters, and finally test the classifier via the test set. In this experiment, the performance of F-Score, AUROC, and RPBEP is taken into account. To solve the problem of multiple-classification, we have a binary classification problem for each class, and in this problem, the class under consideration is a positive class, while the combination of all other classes is a negative class."}, {"heading": "3.3.3 Experiment results", "text": "The box plots of the optimized F score, AUROC and RPBEP of different classes are shown in Fig. 7, Fig. 8 and Fig. 9. These figures clearly show that the proposed algorithm outperforms the competing methods.3.4 Runtime of the proposed algorithm on the three data sets used is shown in Fig. 10. This shows that the first two experiments do not consume much time, while the third large data set based on experiments.3.4 Runtime The runtime of the proposed algorithm on the three data sets used is shown in Fig. 10. This indicates that the first two experiments do not consume much time, while the third large data set based on experiments.This is natural, because in each iteration of the algorithm we have a function for each data point and a summary of the reactions of this function."}, {"heading": "4 Conclusion", "text": "In this thesis, we examine the problem of optimizing a complex, multivariate measure of power. We propose a novel predictive model to solve this problem. This model is based on the maximum probability of a class label with an input data tuple. In order to solve the model parameter, we propose an optimization problem based on the approximation of the upper limit of the loss function and the rarity of the model. Furthermore, an iterative algorithm is developed to solve it. Experiments on two applications in the real world show its advantages over the state of the art. In the future, we will extend the proposed algorithm to various applications, such as computer biology [35.42,15,20] and multimedia information processing [34.32.31.33]. Appendix of theorem 1: According to (6), we will extend the proposed algorithm to various applications."}], "references": [{"title": "A simple and practical control of the authenticity of organic sugarcane samples based on the use of machine-learning algorithms and trace elements determination by inductively coupled plasma mass spectrometry", "author": ["R. Barbosa", "B. Batista", "C. Bario", "R. Varrique", "V. Coelho", "A. Campiglia", "F. Barbosa"], "venue": "Food Chemistry 184, 154\u2013159", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2015}, {"title": "On shape properties of the receiver operating characteristic curve", "author": ["B. Bhattacharya", "G. Hughes"], "venue": "Statistics and Probability Letters 103, 73\u201379", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2015}, {"title": "Using statistics for computing joins with map reduce", "author": ["T. Csar", "R. Pichler", "E. Sallinger", "V. Savenkov"], "venue": "CEUR Workshop Proceedings, vol. 1378, pp. 69\u201374", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2015}, {"title": "Performance and energy efficiency of big data applications in cloud environments: A hadoop case study", "author": ["E. Feller", "L. Ramakrishnan", "C. Morin"], "venue": "Journal of Parallel and Distributed Computing 79-80, 80\u201389", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2015}, {"title": "Multi-ring local binary patterns for rotation invariant texture classification", "author": ["Y. He", "N. Sang"], "venue": "Neural Computing and Applications 22(3-4), 793\u2013802", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2013}, {"title": "Scalable multidimensional anonymization algorithm over big data using map reduce on public cloud", "author": ["A. Irudayasamy", "L. Arockiam"], "venue": "Journal of Theoretical and Applied Information Technology 74(2), 221\u2013231", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2015}, {"title": "Evaluation of hemoglobin performance in the assessment of iron stores  20  Ning Zhang, Prathamesh Chandrasekar in feto-maternal pairs in a high-risk population: Receiver operating characteristic curve analysis", "author": ["J. Jaime-Prez", "G. Garca-Arellano", "N. Mndez-Ramrez", "T. Gonzlez-Llano", "D. GmezAlmaguer"], "venue": "Revista Brasileira de Hematologia e Hemoterapia 37(3), 178\u2013183", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2015}, {"title": "A support vector method for multivariate performance measures", "author": ["T. Joachims"], "venue": "ICML 2005 - Proceedings of the 22nd International Conference on Machine Learning, pp. 377\u2013384", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2005}, {"title": "A support vector method for multivariate performance measures", "author": ["T. Joachims"], "venue": "Proceedings of the 22nd international conference on Machine learning, pp. 377\u2013384. ACM", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2005}, {"title": "Computational fluid dynamics simulation based on hadoop ecosystem and heterogeneous computing", "author": ["M. Kim", "Y. Lee", "H.H. Park", "S. Hahn", "C.G. Lee"], "venue": "Computers and Fluids 115, 1\u201310", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2015}, {"title": "Imagenet classification with deep convolutional neural networks", "author": ["A. Krizhevsky", "I. Sutskever", "G. Hinton"], "venue": "Advances in Neural Information Processing Systems, vol. 2, pp. 1097\u20131105", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2012}, {"title": "Efficient optimization of performance measures by classifier adaptation", "author": ["N. Li", "I.W. Tsang", "Z.H. Zhou"], "venue": "Pattern Analysis and Machine Intelligence, IEEE Transactions on 35(6), 1370\u20131382", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2013}, {"title": "Determination of internal qualities of newhall navel oranges based on nir spectroscopy using machine learning", "author": ["C. Liu", "S. Yang", "L. Deng"], "venue": "Journal of Food Engineering 161, 16\u201323", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2015}, {"title": "Supervised learning of sparse context reconstruction coefficients for data representation and classification", "author": ["X. Liu", "J. Wang", "M. Yin", "B. Edwards", "P. Xu"], "venue": "Neural Computing and Applications", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2015}, {"title": "Structure design of vascular stents", "author": ["Y. Liu", "J. Yang", "Y. Zhou", "J. Hu"], "venue": "Multiscale simulations and mechanics of biological materials pp. 301\u2013317", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2013}, {"title": "Handling big data efficiently by using map reduce technique", "author": ["S. Maitrey", "C. Jha", "C. Jha"], "venue": "Proceedings - 2015 IEEE International Conference on Computational Intelligence and Communication Technology, CICT 2015, pp. 703\u2013708", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2015}, {"title": "A feature selection method for multivariate performance measures", "author": ["Q. Mao", "I.W.H. Tsang"], "venue": "Pattern Analysis and Machine Intelligence, IEEE Transactions on 35(9), 2051\u2013 2063", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2013}, {"title": "Intelligent facial emotion recognition using a layered encoding cascade optimization model", "author": ["S. Neoh", "L. Zhang", "K. Mistry", "M. Hossain", "C. Lim", "N. Aslam", "P. Kinghorn"], "venue": "Applied Soft Computing Journal 34, 72\u201393", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2015}, {"title": "The precision-recall curve overcame the optimism of the receiver operating characteristic curve in rare diseases", "author": ["B. Ozenne", "F. Subtil", "D. Maucort-Boulch"], "venue": "Journal of Clinical Epidemiology 68(8), 855\u2013859", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2015}, {"title": "Modeling nanoparticle targeting to a vascular surface in shear flow through diffusive particle dynamics", "author": ["B. Peng", "Y. Liu", "Y. Zhou", "L. Yang", "G. Zhang", "Y. Liu"], "venue": "Nanoscale Research Letters 10(1), 235", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2015}, {"title": "Sorted consecutive local binary pattern for texture classification", "author": ["J. Ryu", "S. Hong", "H. Yang"], "venue": "IEEE Transactions on Image Processing 24(7), 2254\u20132265", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2015}, {"title": "The precision-recall plot is more informative than the roc plot when evaluating binary classifiers on imbalanced datasets", "author": ["T. Saito", "M. Rehmsmeier"], "venue": "PLoS ONE 10(3)", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2015}, {"title": "Walking pattern classification using a granular linguistic analysis", "author": ["D. Sanchez-Valdes", "A. Alvarez-Alvarez", "G. Trivino"], "venue": "Applied Soft Computing Journal 33, 100\u2013113", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2015}, {"title": "Mixture models in diagnostic meta-analyses - clustering summary receiver operating characteristic curves accounted for heterogeneity and correlation", "author": ["P. Schlattmann", "M. Verba", "M. Dewey", "M. Walther"], "venue": "Journal of Clinical Epidemiology 68(1), 61\u201372", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2015}, {"title": "How to read a receiver operating characteristic curve", "author": ["P. Sedgwick"], "venue": "BMJ (Online) 350", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2015}, {"title": "Jomr: Multi-join optimizer technique to enhance map-reduce job", "author": ["M. Shanoda", "S. Senbel", "M. Khafagy"], "venue": "2014 9th International Conference on Informatics and Systems, INFOS 2014, pp. PDC80\u2013PDC87", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2015}, {"title": "Mammoth: Gearing hadoop towards memory-intensive mapreduce applications", "author": ["X. Shi", "M. Chen", "L. He", "X. Xie", "L. Lu", "H. Jin", "Y. Chen", "S. Wu"], "venue": "IEEE Transactions on Parallel and Distributed Systems 26(8), 2300\u20132315", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2015}, {"title": "\u03c5-nonparallel support vector machine for pattern classification", "author": ["Y. Tian", "Q. Zhang", "D. Liu"], "venue": "Neural Computing and Applications 25(5), 1007\u20131020", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2014}, {"title": "Precision-recalloptimization in learning vector quantization classifiers for improved medical classification systems", "author": ["T. Villmann", "M. Kaden", "M. Lange", "P. Sturmer", "W. Hermann"], "venue": "pp. 71\u201377", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2015}, {"title": "An effective image representation method using kernel classification", "author": ["H. Wang", "J. Wang"], "venue": "2014 IEEE 26th International Conference on Tools with Artificial Intelligence (ICTAI 2014), pp. 853\u2013858", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2014}, {"title": "Multiple kernel multivariate performance learning using cutting plane algorithm", "author": ["J. Wang", "H. Wang", "Y. Zhou", "N. McDonald"], "venue": "Systems, Man and Cybernetics (SMC), 2015 IEEE International Conference on. IEEE", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2015}, {"title": "Supervised cross-modal factor analysis for multiple modal data classification", "author": ["J. Wang", "Y. Zhou", "K. Duan", "J. Wang", "H. Bensmail"], "venue": "SMC2015", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2015}, {"title": "Image tag completion by local learning", "author": ["J. Wang", "Y. Zhou", "H. Wang", "X. Yang", "F. Yang", "A. Peterson"], "venue": "Advances in Neural Networks\u2013ISNN 2015, pp. 232\u2013239. Springer", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2015}, {"title": "Representing data by sparse combination of contextual data points for classification", "author": ["J. Wang", "Y. Zhou", "M. Yin", "S. Chen", "B. Edwards"], "venue": "Advances in Neural Networks\u2013 ISNN 2015, pp. 373\u2013381. Springer", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2015}, {"title": "Computational modeling of magnetic nanoparticle targeting to stent surface under high gradient field", "author": ["S. Wang", "Y. Zhou", "J. Tan", "J. Xu", "J. Yang", "Y. Liu"], "venue": "Computational mechanics 53(3), 403\u2013412", "citeRegEx": "35", "shortCiteRegEx": null, "year": 2014}, {"title": "Incremental support vector machine learning method for aircraft event recognition", "author": ["X. Wang", "P. Shu"], "venue": "Proceedings - 2nd International Conference on Enterprise Systems, ES 2014, pp. 201\u2013204", "citeRegEx": "36", "shortCiteRegEx": null, "year": 2014}, {"title": "Enabling precision/recall preferences for semisupervised svm training", "author": ["Z. Wen", "R. Zhang", "K. Ramamohanarao"], "venue": "pp. 421\u2013430", "citeRegEx": "37", "shortCiteRegEx": null, "year": 2014}, {"title": "Learning with positive and unlabeled examples using biased twin support vector machine", "author": ["Z. Xu", "Z. Qi", "J. Zhang"], "venue": "Neural Computing and Applications 25(6), 1303\u20131311", "citeRegEx": "38", "shortCiteRegEx": null, "year": 2014}, {"title": "Gom-hadoop: A distributed framework for efficient analytics on ordered datasets", "author": ["J. Yin", "Y. Liao", "M. Baldi", "L. Gao", "A. Nucci"], "venue": "Journal of Parallel and Distributed Computing 83, 58\u201369", "citeRegEx": "39", "shortCiteRegEx": null, "year": 2015}, {"title": "Smoothing multivariate performance measures", "author": ["X. Zhang", "A. Saha", "S. Vishwanathan"], "venue": "The Journal of Machine Learning Research 13(1), 3623\u20133680", "citeRegEx": "40", "shortCiteRegEx": null, "year": 2012}, {"title": "Human face recognition based on ensemble of polyharmonic extreme learning machine", "author": ["J. Zhao", "Z. Zhou", "F. Cao"], "venue": "Neural Computing and Applications 24(6), 1317\u20131326", "citeRegEx": "41", "shortCiteRegEx": null, "year": 2014}, {"title": "Biomarker binding on an antibody-functionalized biosensor surface: the influence of surface properties, electric field, and coating density", "author": ["Y. Zhou", "W. Hu", "B. Peng", "Y. Liu"], "venue": "The Journal of Physical Chemistry C 118(26), 14,586\u201314,594", "citeRegEx": "42", "shortCiteRegEx": null, "year": 2014}], "referenceMentions": [{"referenceID": 13, "context": "2 Ning Zhang, Prathamesh Chandrasekar 1 Introduction Machine learning aims to train a predictive model from a training set of inputout pairs, and then use the model to predict an unknown output from a given test input [14,41,38,13,34,1,18].", "startOffset": 218, "endOffset": 239}, {"referenceID": 40, "context": "2 Ning Zhang, Prathamesh Chandrasekar 1 Introduction Machine learning aims to train a predictive model from a training set of inputout pairs, and then use the model to predict an unknown output from a given test input [14,41,38,13,34,1,18].", "startOffset": 218, "endOffset": 239}, {"referenceID": 37, "context": "2 Ning Zhang, Prathamesh Chandrasekar 1 Introduction Machine learning aims to train a predictive model from a training set of inputout pairs, and then use the model to predict an unknown output from a given test input [14,41,38,13,34,1,18].", "startOffset": 218, "endOffset": 239}, {"referenceID": 12, "context": "2 Ning Zhang, Prathamesh Chandrasekar 1 Introduction Machine learning aims to train a predictive model from a training set of inputout pairs, and then use the model to predict an unknown output from a given test input [14,41,38,13,34,1,18].", "startOffset": 218, "endOffset": 239}, {"referenceID": 33, "context": "2 Ning Zhang, Prathamesh Chandrasekar 1 Introduction Machine learning aims to train a predictive model from a training set of inputout pairs, and then use the model to predict an unknown output from a given test input [14,41,38,13,34,1,18].", "startOffset": 218, "endOffset": 239}, {"referenceID": 0, "context": "2 Ning Zhang, Prathamesh Chandrasekar 1 Introduction Machine learning aims to train a predictive model from a training set of inputout pairs, and then use the model to predict an unknown output from a given test input [14,41,38,13,34,1,18].", "startOffset": 218, "endOffset": 239}, {"referenceID": 17, "context": "2 Ning Zhang, Prathamesh Chandrasekar 1 Introduction Machine learning aims to train a predictive model from a training set of inputout pairs, and then use the model to predict an unknown output from a given test input [14,41,38,13,34,1,18].", "startOffset": 218, "endOffset": 239}, {"referenceID": 4, "context": "In this problem, each input is a feature vector of a data point, and each output is a binary class label of a data point, either positive or negative [5,28,21,23,33,31,30].", "startOffset": 150, "endOffset": 171}, {"referenceID": 27, "context": "In this problem, each input is a feature vector of a data point, and each output is a binary class label of a data point, either positive or negative [5,28,21,23,33,31,30].", "startOffset": 150, "endOffset": 171}, {"referenceID": 20, "context": "In this problem, each input is a feature vector of a data point, and each output is a binary class label of a data point, either positive or negative [5,28,21,23,33,31,30].", "startOffset": 150, "endOffset": 171}, {"referenceID": 22, "context": "In this problem, each input is a feature vector of a data point, and each output is a binary class label of a data point, either positive or negative [5,28,21,23,33,31,30].", "startOffset": 150, "endOffset": 171}, {"referenceID": 32, "context": "In this problem, each input is a feature vector of a data point, and each output is a binary class label of a data point, either positive or negative [5,28,21,23,33,31,30].", "startOffset": 150, "endOffset": 171}, {"referenceID": 30, "context": "In this problem, each input is a feature vector of a data point, and each output is a binary class label of a data point, either positive or negative [5,28,21,23,33,31,30].", "startOffset": 150, "endOffset": 171}, {"referenceID": 29, "context": "In this problem, each input is a feature vector of a data point, and each output is a binary class label of a data point, either positive or negative [5,28,21,23,33,31,30].", "startOffset": 150, "endOffset": 171}, {"referenceID": 6, "context": "For example, prediction accuracy, F-score, area under receiver operating characteristic curve (AUROC) [7,24,25,2], and precision-recall curve break-even point (RPBEP) [37,19,29,22].", "startOffset": 102, "endOffset": 113}, {"referenceID": 23, "context": "For example, prediction accuracy, F-score, area under receiver operating characteristic curve (AUROC) [7,24,25,2], and precision-recall curve break-even point (RPBEP) [37,19,29,22].", "startOffset": 102, "endOffset": 113}, {"referenceID": 24, "context": "For example, prediction accuracy, F-score, area under receiver operating characteristic curve (AUROC) [7,24,25,2], and precision-recall curve break-even point (RPBEP) [37,19,29,22].", "startOffset": 102, "endOffset": 113}, {"referenceID": 1, "context": "For example, prediction accuracy, F-score, area under receiver operating characteristic curve (AUROC) [7,24,25,2], and precision-recall curve break-even point (RPBEP) [37,19,29,22].", "startOffset": 102, "endOffset": 113}, {"referenceID": 36, "context": "For example, prediction accuracy, F-score, area under receiver operating characteristic curve (AUROC) [7,24,25,2], and precision-recall curve break-even point (RPBEP) [37,19,29,22].", "startOffset": 167, "endOffset": 180}, {"referenceID": 18, "context": "For example, prediction accuracy, F-score, area under receiver operating characteristic curve (AUROC) [7,24,25,2], and precision-recall curve break-even point (RPBEP) [37,19,29,22].", "startOffset": 167, "endOffset": 180}, {"referenceID": 28, "context": "For example, prediction accuracy, F-score, area under receiver operating characteristic curve (AUROC) [7,24,25,2], and precision-recall curve break-even point (RPBEP) [37,19,29,22].", "startOffset": 167, "endOffset": 180}, {"referenceID": 21, "context": "For example, prediction accuracy, F-score, area under receiver operating characteristic curve (AUROC) [7,24,25,2], and precision-recall curve break-even point (RPBEP) [37,19,29,22].", "startOffset": 167, "endOffset": 180}, {"referenceID": 7, "context": "\u2013 Joachims proposed to learn a support vector machine to optimize a complex loss function [8].", "startOffset": 90, "endOffset": 93}, {"referenceID": 16, "context": "Title Suppressed Due to Excessive Length 3 \u2013 Mao and Tsang improved the Joachims\u2019s work by integrating feature selection to support vector machine for complex loss optimization [17].", "startOffset": 177, "endOffset": 181}, {"referenceID": 11, "context": "proposed a classifier adaptation method to extend Joachims\u2019s work [12].", "startOffset": 66, "endOffset": 70}, {"referenceID": 39, "context": "[40] proposed a novel smoothing strategy by using Nesterov\u2019s accelerated gradient method to improve the convergence rate of the method proposed by Joachims [8].", "startOffset": 0, "endOffset": 4}, {"referenceID": 7, "context": "[40] proposed a novel smoothing strategy by using Nesterov\u2019s accelerated gradient method to improve the convergence rate of the method proposed by Joachims [8].", "startOffset": 156, "endOffset": 159}, {"referenceID": 39, "context": "This method, according to the results reported in [40], can achieve converges significantly faster than Joachims\u2019s method [8], but it does not scarify generalization ability.", "startOffset": 50, "endOffset": 54}, {"referenceID": 7, "context": "This method, according to the results reported in [40], can achieve converges significantly faster than Joachims\u2019s method [8], but it does not scarify generalization ability.", "startOffset": 122, "endOffset": 125}, {"referenceID": 0, "context": "And then we apply a Sigmoid function to the response of this function to impose it to a range of [0, 1], g(xi, y \u2032 i) = 1 1 + exp (\u2212f(xi, y\u2032 i)) = 1 1 + exp (\u2212y\u2032 iw xi) .", "startOffset": 97, "endOffset": 103}, {"referenceID": 3, "context": "The clusters are managed by a big data platform, Hadoop [4,39,10,27].", "startOffset": 56, "endOffset": 68}, {"referenceID": 38, "context": "The clusters are managed by a big data platform, Hadoop [4,39,10,27].", "startOffset": 56, "endOffset": 68}, {"referenceID": 9, "context": "The clusters are managed by a big data platform, Hadoop [4,39,10,27].", "startOffset": 56, "endOffset": 68}, {"referenceID": 26, "context": "The clusters are managed by a big data platform, Hadoop [4,39,10,27].", "startOffset": 56, "endOffset": 68}, {"referenceID": 5, "context": "To process the data and obtain a final output, it uses a Map-Reduce framework [6,26,16,3].", "startOffset": 78, "endOffset": 89}, {"referenceID": 25, "context": "To process the data and obtain a final output, it uses a Map-Reduce framework [6,26,16,3].", "startOffset": 78, "endOffset": 89}, {"referenceID": 15, "context": "To process the data and obtain a final output, it uses a Map-Reduce framework [6,26,16,3].", "startOffset": 78, "endOffset": 89}, {"referenceID": 2, "context": "To process the data and obtain a final output, it uses a Map-Reduce framework [6,26,16,3].", "startOffset": 78, "endOffset": 89}, {"referenceID": 35, "context": "This procedure provides important information for fault diagnosis and structure maintenance of aircraft [36].", "startOffset": 104, "endOffset": 108}, {"referenceID": 8, "context": "3 Experiment result We compare the proposed algorithm, SMLM, against several state-of-the-art complex loss optimization methods, including support vector machine for multivariate performance optimization (SVMmulti) [9], classifier adaptation for multivariate performance optimization (CAPO) [12], and features selection for multivariate performance optimization (FSmulti) [17].", "startOffset": 215, "endOffset": 218}, {"referenceID": 11, "context": "3 Experiment result We compare the proposed algorithm, SMLM, against several state-of-the-art complex loss optimization methods, including support vector machine for multivariate performance optimization (SVMmulti) [9], classifier adaptation for multivariate performance optimization (CAPO) [12], and features selection for multivariate performance optimization (FSmulti) [17].", "startOffset": 291, "endOffset": 295}, {"referenceID": 16, "context": "3 Experiment result We compare the proposed algorithm, SMLM, against several state-of-the-art complex loss optimization methods, including support vector machine for multivariate performance optimization (SVMmulti) [9], classifier adaptation for multivariate performance optimization (CAPO) [12], and features selection for multivariate performance optimization (FSmulti) [17].", "startOffset": 372, "endOffset": 376}, {"referenceID": 10, "context": "1 Data set In this experiment, we use a large data set, ImageNet [11].", "startOffset": 65, "endOffset": 69}, {"referenceID": 34, "context": "In the future, we will extend the proposed algorithm to different applications, such as computational biology [35,42,15,20] and multimedia information processing [34,32,31,33].", "startOffset": 110, "endOffset": 123}, {"referenceID": 41, "context": "In the future, we will extend the proposed algorithm to different applications, such as computational biology [35,42,15,20] and multimedia information processing [34,32,31,33].", "startOffset": 110, "endOffset": 123}, {"referenceID": 14, "context": "In the future, we will extend the proposed algorithm to different applications, such as computational biology [35,42,15,20] and multimedia information processing [34,32,31,33].", "startOffset": 110, "endOffset": 123}, {"referenceID": 19, "context": "In the future, we will extend the proposed algorithm to different applications, such as computational biology [35,42,15,20] and multimedia information processing [34,32,31,33].", "startOffset": 110, "endOffset": 123}, {"referenceID": 33, "context": "In the future, we will extend the proposed algorithm to different applications, such as computational biology [35,42,15,20] and multimedia information processing [34,32,31,33].", "startOffset": 162, "endOffset": 175}, {"referenceID": 31, "context": "In the future, we will extend the proposed algorithm to different applications, such as computational biology [35,42,15,20] and multimedia information processing [34,32,31,33].", "startOffset": 162, "endOffset": 175}, {"referenceID": 30, "context": "In the future, we will extend the proposed algorithm to different applications, such as computational biology [35,42,15,20] and multimedia information processing [34,32,31,33].", "startOffset": 162, "endOffset": 175}, {"referenceID": 32, "context": "In the future, we will extend the proposed algorithm to different applications, such as computational biology [35,42,15,20] and multimedia information processing [34,32,31,33].", "startOffset": 162, "endOffset": 175}], "year": 2015, "abstractText": "Traditional machine learning methods usually minimize a simple loss function to learn a predictive model, and then use a complex performance measure to measure the prediction performance. However, minimizing a simple loss function cannot guarantee that an optimal performance. In this paper, we study the problem of optimizing the complex performance measure directly to obtain a predictive model. We proposed to construct a maximum likelihood model for this problem, and to learn the model parameter, we minimize a complex loss function corresponding to the desired complex performance measure. To optimize the loss function, we approximate the upper bound of the complex loss. We also propose impose the sparsity to the model parameter to obtain a sparse model. An objective is constructed by combining the upper bound of the loss function and the sparsity of the model parameter, and we develop an iterative algorithm to minimize it by using the fast iterative shrinkagethresholding algorithm framework. The experiments on optimization on three different complex performance measures, including F-score, receiver operating characteristic curve, and recall precision curve break even point, over three real-world applications, aircraft event recognition of civil aviation safety, intrusion detection in wireless mesh networks, and image classification, show the advantages of the proposed method over state-of-the-art methods.", "creator": "LaTeX with hyperref package"}}}