{"id": "1511.05789", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "18-Nov-2015", "title": "Metric learning approach for graph-based label propagation", "abstract": "The efficiency of graph-based semi-supervised algorithms depends on the graph of instances on which they are applied. The instances are often in a vectorial form before a graph linking them is built. The construction of the graph relies on a metric over the vectorial space that help define the weight of the connection between entities. The classic choice for this metric is usually a distance measure or a similarity measure based on the euclidean norm. We claim that in some cases the euclidean norm on the initial vectorial space might not be the more appropriate to solve the task efficiently. We propose an algorithm that aims at learning the most appropriate vectorial representation for building a graph on which the task at hand is solved efficiently.", "histories": [["v1", "Wed, 18 Nov 2015 14:04:55 GMT  (268kb,D)", "https://arxiv.org/abs/1511.05789v1", "Pre-submission ICLR 2016"], ["v2", "Thu, 19 Nov 2015 21:58:32 GMT  (446kb,D)", "http://arxiv.org/abs/1511.05789v2", "Pre-submission ICLR 2016"], ["v3", "Fri, 27 Nov 2015 14:03:22 GMT  (465kb,D)", "http://arxiv.org/abs/1511.05789v3", "Pre-submission ICLR 2016"], ["v4", "Thu, 7 Jan 2016 21:41:36 GMT  (445kb,D)", "http://arxiv.org/abs/1511.05789v4", "Pre-submission ICLR 2016"], ["v5", "Tue, 19 Jan 2016 20:59:03 GMT  (1015kb)", "http://arxiv.org/abs/1511.05789v5", "Pre-submission ICLR 2016"], ["v6", "Thu, 18 Feb 2016 15:53:01 GMT  (62kb)", "http://arxiv.org/abs/1511.05789v6", "Workshop track submission ICLR 2016"]], "COMMENTS": "Pre-submission ICLR 2016", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["pauline wauquier", "mikaela keller"], "accepted": false, "id": "1511.05789"}, "pdf": {"name": "1511.05789.pdf", "metadata": {"source": "CRF", "title": null, "authors": [], "emails": ["pauline.wauquier@inria.fr", "mikaela.keller@univ-lille3.fr"], "sections": [{"heading": null, "text": "ar Xiv: 151 1,05 789v 6 [cs.L G] 1"}, {"heading": "1 INTRODUCTION", "text": "In fact, most of them are able to determine for themselves what they want and what they want. (...) Most of them are able to understand themselves. (...) Most of them are not. (...) Most of them are able to understand themselves. (...) Most of them are able to understand themselves. (...) Most of them are able to understand themselves. (...) Most of them are not. (...) Most of them are not. (...) Most of them are not. (...) Most of them are. (...) Most of them are able to understand themselves. (...)"}, {"heading": "2 ALGORITHM\u2019S DESCRIPTION", "text": "We define D = (xi, yi) i = 1.. n a dataset of examples, as we (x) x x (x) x (x) x (x) x (x) x (x) x (x) x (x) x (x) x (x) x (x) x (x) x (x) x (x) x (x) x (x) x (x) x (x) x (x) x (x) x (x) x (x) x (x) x (x) x (x) x (x) x (x) x (x) x (x) x (x) x (x) x (x) x x (x) x x x (x) x x x (x) x x x x x x (x) x) x x x x x (x) x x (x) x (x) x (x) x (x) x (x) x (x) x x) x x x x x x x x x x x x x x (x) x) x x x x x x x x x (x) x x x x) x x (x) x x x (x) x x x x x x x) x x x x x x x x x x (x) x x x x x x x x x) x x x x x (x) x x (x) x x x x x x x x x x) x x x x x (x) x x x x x x x x x (x) x x x x x x x x x x x x x x x x x (x) x x x x x) x x x x x x x x x x x x x x x x x x x (x) x x x x x x x x x x x x x x x x x x x x x x (x) x x x x x x x x x x x x x x x x x x x x x x x (x x x x x) x x x x x x (x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x) x x x x x x x x x x x x x x x x x x x x x x x x x x x x"}, {"heading": "3 THEORETICAL GUARANTEES", "text": "Below, we substantiate our claim by showing that we can assure the existence of such an approach if we assume that each test triplet takes place in a close neighborhood of at least one similarly designated training triplet. We prove our claim by three main steps. We first establish that we can test the distance between the projection of points as a function of their initial distance if we assume that a triplet is in the immediate vicinity of at least one similarly designated training triplet. We prove our claim by three main steps. We first establish that we can test the distance between the projection of points as a function of their initial distance. In a second step, we show that if a triplet is correctly projected into the new space, then triplets projected nearby will also be correctly projected. By matching the first two steps that we are able to show that triplets projected in the initial diplet are close to the order."}, {"heading": "4 EXPERIMENTS", "text": "After testing the interest of our approach for an ideal learning method, we seem to evaluate experimentally the performance of our algorithm. We evaluate our framework and compare it with various algorithms on several artificial datasets, consisting of 500 instances, selected for their increasing degree of complexity. The first artificial datasets we use are the classical circle data. They are achieved by sampling points on two concentric circles in 2-D, the circle defines class affiliation. The other three artificial datasets are developed variants of the circle data set; therefore, their first features are generated by circular datasets of generative modeling."}, {"heading": "5 CONCLUSION", "text": "In this paper, we presented an algorithm to learn a representation space for data sets adapted to a specific task. We defined and proved a first theoretical prerequisite that our algorithm is optimal; what has been proven can easily be generalized to multi-layered neural networks. Experiments with artificial and real data sets confirmed the relevance of our approach to solving classification tasks."}, {"heading": "6 ACKNOWLEDGEMENT", "text": "This work was partially supported by a scholarship from the CPER Nord-Pas de Calais / FEDER DATA Advanced data science and technologies 2015-2020 and ANRT under the scholarship number CIFRE No 2013 / 0961."}], "references": [{"title": "Learning a similarity metric discriminatively, with application to face verification", "author": ["Sumit Chopra", "Raia Hadsell", "Yann LeCun"], "venue": "In Proceedings of the 2005 IEEE Computer Society Conference on Computer Vision and Pattern Recognition", "citeRegEx": "Chopra et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Chopra et al\\.", "year": 2005}, {"title": "Torch7: A Matlab-like Environment for Machine Learning", "author": ["Ronan Collobert", "Koray Kavukcuoglu", "Clement Farabet"], "venue": "In BigLearn NIPS Workshop,", "citeRegEx": "Collobert et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Collobert et al\\.", "year": 2011}, {"title": "Influence of graph construction on semi-supervised learning", "author": ["Celso Andr R. de Sousa", "Solange O. Rezende", "Gustavo E.A.P.A. Batista"], "venue": null, "citeRegEx": "Sousa et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Sousa et al\\.", "year": 2013}, {"title": "Extremely randomized trees", "author": ["Pierre Geurts", "Damien Ernst", "Louis Wehenkel"], "venue": "Machine Learning,", "citeRegEx": "Geurts et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Geurts et al\\.", "year": 2006}, {"title": "Kernel density metric learning", "author": ["Yujie He", "Wenlin Chen", "Yixin Chen", "Yi Mao"], "venue": "In Data Mining (ICDM),", "citeRegEx": "He et al\\.,? \\Q2013\\E", "shortCiteRegEx": "He et al\\.", "year": 2013}, {"title": "Deep metric learning using triplet network", "author": ["Elad Hoffer", "Nir Ailon"], "venue": "CoRR, abs/1412.6622,", "citeRegEx": "Hoffer and Ailon.,? \\Q2014\\E", "shortCiteRegEx": "Hoffer and Ailon.", "year": 2014}, {"title": "Deep learning via semi-supervised embedding", "author": ["R. Collobert. J. Weston", "F. Ratle"], "venue": "In International Conference on Machine Learning,", "citeRegEx": "Weston and Ratle.,? \\Q2008\\E", "shortCiteRegEx": "Weston and Ratle.", "year": 2008}, {"title": "Graph construction and b-matching for semi-supervised learning", "author": ["T. Jebara", "J. Wang", "S.F. Chang"], "venue": "In International Conference on Machine Learning,", "citeRegEx": "Jebara et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Jebara et al\\.", "year": 2009}, {"title": "Non-linear metric learning", "author": ["Dor Kedem", "Stephen Tyree", "Kilian Weinberger", "Fei Sha", "Gert Lanckriet"], "venue": "Advances in Neural Information Processing Systems", "citeRegEx": "Kedem et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Kedem et al\\.", "year": 2012}, {"title": "Robust multi-class transductive learning with graphs", "author": ["Wei Liu", "Shih-Fu Chang"], "venue": "In Computer Vision and Pattern Recognition,", "citeRegEx": "Liu and Chang.,? \\Q2009\\E", "shortCiteRegEx": "Liu and Chang.", "year": 2009}, {"title": "Influence of graph construction on graphbased clustering measures", "author": ["Markus Maier", "Ulrike von Luxburg", "Matthias Hein"], "venue": "In Advances in Neural Information Processing Systems 21, Proceedings of the Twenty-Second Annual Conference on Neural Information Processing Systems,", "citeRegEx": "Maier et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Maier et al\\.", "year": 2008}, {"title": "How the result of graph clustering methods depends on the construction of the graph", "author": ["Maier", "Markus", "von Luxburg", "Ulrike", "Hein", "Matthias"], "venue": "ESAIM: PS,", "citeRegEx": "Maier et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Maier et al\\.", "year": 2013}, {"title": "The manifold tangent classifier. In Advances in Neural Information Processing Systems", "author": ["Salah Rifai", "Yann Dauphin", "Pascal Vincent", "Yoshua Bengio", "Xavier Muller"], "venue": "Annual Conference on Neural Information Processing Systems", "citeRegEx": "Rifai et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Rifai et al\\.", "year": 2011}, {"title": "Beyond the point cloud: from transductive to semisupervised learning", "author": ["Partha Niyogi Vikas Sindhwani", "M. Belkin"], "venue": "In International Conference on Machine Learning (ICML),", "citeRegEx": "Sindhwani and Belkin.,? \\Q2005\\E", "shortCiteRegEx": "Sindhwani and Belkin.", "year": 2005}, {"title": "Distance metric learning for large margin nearest neighbor classification", "author": ["Kilian Q. Weinberger", "Lawrence K. Saul"], "venue": "Journal of Machine Learning Research (JMLR),", "citeRegEx": "Weinberger and Saul.,? \\Q2009\\E", "shortCiteRegEx": "Weinberger and Saul.", "year": 2009}, {"title": "Learning with local and global consistency", "author": ["Dengyong Zhou", "Olivier Bousquet", "Thomas N. Lal", "Jason Weston", "Bernhard Sch\u00f6lkopf"], "venue": "Advances in Neural Information Processing Systems", "citeRegEx": "Zhou et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Zhou et al\\.", "year": 2004}, {"title": "Semi-supervised learning literature survey", "author": ["Xiaojin Zhu"], "venue": "Technical Report 1530,", "citeRegEx": "Zhu.,? \\Q2005\\E", "shortCiteRegEx": "Zhu.", "year": 2005}, {"title": "Learning from labeled and unlabeled data with label propagation", "author": ["Xiaojin Zhu", "Zoubin Ghahramani"], "venue": "Technical report,", "citeRegEx": "Zhu and Ghahramani.,? \\Q2002\\E", "shortCiteRegEx": "Zhu and Ghahramani.", "year": 2002}, {"title": "Semi-supervised learning using gaussian fields and harmonic functions", "author": ["Xiaojin Zhu", "Zoubin Ghahramani", "John D. Lafferty"], "venue": "In Machine Learning, Proceedings of the Twentieth International Conference (ICML", "citeRegEx": "Zhu et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Zhu et al\\.", "year": 2003}], "referenceMentions": [{"referenceID": 11, "context": "Transductive or semi-supervised graph-based learning algorithms, such as Zhu et al. (2003); Zhou et al.", "startOffset": 73, "endOffset": 91}, {"referenceID": 11, "context": "(2003); Zhou et al. (2004); Zhu (2005); Liu & Chang (2009), take advantage of both annotated and unannotate data to solve automated labeling tasks.", "startOffset": 8, "endOffset": 27}, {"referenceID": 11, "context": "(2003); Zhou et al. (2004); Zhu (2005); Liu & Chang (2009), take advantage of both annotated and unannotate data to solve automated labeling tasks.", "startOffset": 8, "endOffset": 39}, {"referenceID": 11, "context": "(2003); Zhou et al. (2004); Zhu (2005); Liu & Chang (2009), take advantage of both annotated and unannotate data to solve automated labeling tasks.", "startOffset": 8, "endOffset": 59}, {"referenceID": 8, "context": "Indeed the effectiveness of graph-based learning algorithms relies on the relevance of the data representation for the targeted task (Maier et al. (2008); Jebara et al.", "startOffset": 134, "endOffset": 154}, {"referenceID": 6, "context": "(2008); Jebara et al. (2009); de Sousa et al.", "startOffset": 8, "endOffset": 29}, {"referenceID": 2, "context": "(2009); de Sousa et al. (2013)).", "startOffset": 11, "endOffset": 31}, {"referenceID": 2, "context": "(2009); de Sousa et al. (2013)). In general the graph representation of the data is built in two steps Liu & Chang (2009); Maier, Markus et al.", "startOffset": 11, "endOffset": 122}, {"referenceID": 2, "context": "(2009); de Sousa et al. (2013)). In general the graph representation of the data is built in two steps Liu & Chang (2009); Maier, Markus et al. (2013); de Sousa et al.", "startOffset": 11, "endOffset": 151}, {"referenceID": 2, "context": "(2009); de Sousa et al. (2013)). In general the graph representation of the data is built in two steps Liu & Chang (2009); Maier, Markus et al. (2013); de Sousa et al. (2013). In this representation, every data point is seen as a vertex of the graph and the first building step is to compute the edge weight between every pair of vertices.", "startOffset": 11, "endOffset": 175}, {"referenceID": 0, "context": "In particular, the work presented in Chopra et al. (2005); Rifai et al.", "startOffset": 37, "endOffset": 58}, {"referenceID": 0, "context": "In particular, the work presented in Chopra et al. (2005); Rifai et al. (2011); J.", "startOffset": 37, "endOffset": 79}, {"referenceID": 0, "context": "In particular, the work presented in Chopra et al. (2005); Rifai et al. (2011); J. Weston (2008); Hoffer & Ailon (2014) is very close to our own work.", "startOffset": 37, "endOffset": 97}, {"referenceID": 0, "context": "In particular, the work presented in Chopra et al. (2005); Rifai et al. (2011); J. Weston (2008); Hoffer & Ailon (2014) is very close to our own work.", "startOffset": 37, "endOffset": 120}, {"referenceID": 0, "context": "In particular, the work presented in Chopra et al. (2005); Rifai et al. (2011); J. Weston (2008); Hoffer & Ailon (2014) is very close to our own work. The main difference of Rifai et al. (2011); J.", "startOffset": 37, "endOffset": 194}, {"referenceID": 0, "context": "In particular, the work presented in Chopra et al. (2005); Rifai et al. (2011); J. Weston (2008); Hoffer & Ailon (2014) is very close to our own work. The main difference of Rifai et al. (2011); J. Weston (2008); Hoffer & Ailon (2014) with our approach is that in those models the classifier is parametric while we rely on a non-parametric classifier on which we give guarantees.", "startOffset": 37, "endOffset": 212}, {"referenceID": 0, "context": "In particular, the work presented in Chopra et al. (2005); Rifai et al. (2011); J. Weston (2008); Hoffer & Ailon (2014) is very close to our own work. The main difference of Rifai et al. (2011); J. Weston (2008); Hoffer & Ailon (2014) with our approach is that in those models the classifier is parametric while we rely on a non-parametric classifier on which we give guarantees.", "startOffset": 37, "endOffset": 235}, {"referenceID": 0, "context": "In particular, the work presented in Chopra et al. (2005); Rifai et al. (2011); J. Weston (2008); Hoffer & Ailon (2014) is very close to our own work. The main difference of Rifai et al. (2011); J. Weston (2008); Hoffer & Ailon (2014) with our approach is that in those models the classifier is parametric while we rely on a non-parametric classifier on which we give guarantees. Our main difference Chopra et al. (2005) approach, is the shape of their representation function (convolutionnal network vs multi-layer perceptron) and their exact learning criterion (pairwise comparison vs relative comparison).", "startOffset": 37, "endOffset": 421}, {"referenceID": 0, "context": "In particular, the work presented in Chopra et al. (2005); Rifai et al. (2011); J. Weston (2008); Hoffer & Ailon (2014) is very close to our own work. The main difference of Rifai et al. (2011); J. Weston (2008); Hoffer & Ailon (2014) with our approach is that in those models the classifier is parametric while we rely on a non-parametric classifier on which we give guarantees. Our main difference Chopra et al. (2005) approach, is the shape of their representation function (convolutionnal network vs multi-layer perceptron) and their exact learning criterion (pairwise comparison vs relative comparison). Among the metric learning approaches, the more popular ones have as objective the learning of a linear re-weighting of the euclidean distance, or Mahalanobis distance (for example Weinberger & Saul (2009); Dhillon et al.", "startOffset": 37, "endOffset": 814}, {"referenceID": 0, "context": "In particular, the work presented in Chopra et al. (2005); Rifai et al. (2011); J. Weston (2008); Hoffer & Ailon (2014) is very close to our own work. The main difference of Rifai et al. (2011); J. Weston (2008); Hoffer & Ailon (2014) with our approach is that in those models the classifier is parametric while we rely on a non-parametric classifier on which we give guarantees. Our main difference Chopra et al. (2005) approach, is the shape of their representation function (convolutionnal network vs multi-layer perceptron) and their exact learning criterion (pairwise comparison vs relative comparison). Among the metric learning approaches, the more popular ones have as objective the learning of a linear re-weighting of the euclidean distance, or Mahalanobis distance (for example Weinberger & Saul (2009); Dhillon et al.). Although linear metrics are convenient to optimize, they are not able to capture the non-linear structure of the data; some non-linear metric learning algorithms have been developed and compose a second group of metric learning approaches. Most non-linear metric learning are kernelized version of linear metrics learning approaches (Kedem et al. (2012); He et al.", "startOffset": 37, "endOffset": 1186}, {"referenceID": 0, "context": "In particular, the work presented in Chopra et al. (2005); Rifai et al. (2011); J. Weston (2008); Hoffer & Ailon (2014) is very close to our own work. The main difference of Rifai et al. (2011); J. Weston (2008); Hoffer & Ailon (2014) with our approach is that in those models the classifier is parametric while we rely on a non-parametric classifier on which we give guarantees. Our main difference Chopra et al. (2005) approach, is the shape of their representation function (convolutionnal network vs multi-layer perceptron) and their exact learning criterion (pairwise comparison vs relative comparison). Among the metric learning approaches, the more popular ones have as objective the learning of a linear re-weighting of the euclidean distance, or Mahalanobis distance (for example Weinberger & Saul (2009); Dhillon et al.). Although linear metrics are convenient to optimize, they are not able to capture the non-linear structure of the data; some non-linear metric learning algorithms have been developed and compose a second group of metric learning approaches. Most non-linear metric learning are kernelized version of linear metrics learning approaches (Kedem et al. (2012); He et al. (2013)) and present the drawback of the choice of the kernel.", "startOffset": 37, "endOffset": 1204}, {"referenceID": 0, "context": "In particular, the work presented in Chopra et al. (2005); Rifai et al. (2011); J. Weston (2008); Hoffer & Ailon (2014) is very close to our own work. The main difference of Rifai et al. (2011); J. Weston (2008); Hoffer & Ailon (2014) with our approach is that in those models the classifier is parametric while we rely on a non-parametric classifier on which we give guarantees. Our main difference Chopra et al. (2005) approach, is the shape of their representation function (convolutionnal network vs multi-layer perceptron) and their exact learning criterion (pairwise comparison vs relative comparison). Among the metric learning approaches, the more popular ones have as objective the learning of a linear re-weighting of the euclidean distance, or Mahalanobis distance (for example Weinberger & Saul (2009); Dhillon et al.). Although linear metrics are convenient to optimize, they are not able to capture the non-linear structure of the data; some non-linear metric learning algorithms have been developed and compose a second group of metric learning approaches. Most non-linear metric learning are kernelized version of linear metrics learning approaches (Kedem et al. (2012); He et al. (2013)) and present the drawback of the choice of the kernel. In Vikas Sindhwani & Belkin (2005), a deformed kernel is learn depending on the data geometry which will be used to the classification task resolve.", "startOffset": 37, "endOffset": 1294}, {"referenceID": 15, "context": "We can use the computed graph in order to predict the hidden label, through the well known label propagation algorithm (Zhu & Ghahramani (2002); Zhou et al.", "startOffset": 120, "endOffset": 144}, {"referenceID": 15, "context": "We can use the computed graph in order to predict the hidden label, through the well known label propagation algorithm (Zhu & Ghahramani (2002); Zhou et al. (2004)); In order to remove the non relevant edges of the graph, a pruning phase is usually performed on the graph.", "startOffset": 145, "endOffset": 164}, {"referenceID": 3, "context": "For each data set, we can compute a graph where the weights are computed by the the euclidean distance in, respectively, the initial space and the learned space for LMNN algorithm (Weinberger & Saul (2009)), the ExtraTrees feature selection algorithm (Geurts et al. (2006)) and our approach2.", "startOffset": 252, "endOffset": 273}, {"referenceID": 1, "context": "Implemented with torch7 (Collobert et al. (2011))", "startOffset": 25, "endOffset": 49}], "year": 2016, "abstractText": "The efficiency of graph-based semi-supervised algorithms depends on the graph of instances on which they are applied. The instances are often in a vectorial form before a graph linking them is built. The construction of the graph relies on a metric over the vectorial space that help define the weight of the connection between entities. The classic choice for this metric is usually a distance measure or a similarity measure based on the euclidean norm. We claim that in some cases the euclidean norm on the initial vectorial space might not be the more appropriate to solve the task efficiently. We propose an algorithm that aims at learning the most appropriate vectorial representation for building a graph on which the task at hand is solved efficiently.", "creator": "LaTeX with hyperref package"}}}