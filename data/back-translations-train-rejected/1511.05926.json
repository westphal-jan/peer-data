{"id": "1511.05926", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "18-Nov-2015", "title": "Combining Neural Networks and Log-linear Models to Improve Relation Extraction", "abstract": "The last decade has witnessed the success of the traditional feature-based method on exploiting the discrete structures such as words or lexical patterns to extract relations from text. Recently, convolutional and recurrent neural networks has provided very effective mechanisms to capture the hidden structures within sentences via continuous representations, thereby significantly advancing the performance of relation extraction. The advantage of convolutional neural networks is their capacity to generalize the consecutive k-grams in the sentences while recurrent neural networks are effective to encode long ranges of sentence context. This paper proposes to combine the traditional feature-based method, the convolutional and recurrent neural networks to simultaneously benefit from their advantages. Our systematic evaluation of different network architectures and combination methods demonstrates the effectiveness of this approach and results in the state-of-the-art performance on the ACE 2005 and SemEval dataset.", "histories": [["v1", "Wed, 18 Nov 2015 20:17:39 GMT  (34kb)", "http://arxiv.org/abs/1511.05926v1", null]], "reviews": [], "SUBJECTS": "cs.CL cs.LG", "authors": ["thien huu nguyen", "ralph grishman"], "accepted": false, "id": "1511.05926"}, "pdf": {"name": "1511.05926.pdf", "metadata": {"source": "CRF", "title": "Combining Neural Networks and Log-linear Models to Improve Relation Extraction", "authors": ["Thien Huu Nguyen"], "emails": ["thien@cs.nyu.edu", "grishman@cs.nyu.edu"], "sections": [{"heading": null, "text": "ar Xiv: 151 1.05 926v 1 [cs.C L] 18 NThe last decade has witnessed the success of the traditional feature-based method of exploiting discrete structures such as words or lexical patterns to extract relationships from texts. Lately, revolutionary and relapsing neural networks have provided very effective mechanisms for capturing the hidden structures within sentences through continuous representations, thereby greatly improving the performance of relation extraction. The advantage of Convolutionary Neural Networks is their ability to generalize successive kg in sentences, while recidividing neural networks are effective at encoding large areas of the sentence context. This paper suggests combining the traditional feature-based method of combining the revolutionary and reciprocating neural networks in order to benefit from their advantages at the same time. Our systematic evaluation of various network architectures and combination methods demonstrates the effectiveness of this approach in 2005 and leads to a state-of-the-art EDATE and Semasval Performance."}, {"heading": "1 Introduction", "text": "We are investigating the relation extraction (RE) problem, one of the major problems of information extraction and natural language processing (NLP). Given two entities in a sentence (relation mentions), we need to identify the semantic relationship (if any) between the two entities. An example is the recognition of the localized relationship between \"He\" and \"Texas\" in the sentence \"He lives in Texas.\" The two methods that have dominated RE research over the past decade are the feature-based method (Kambhatla, 2004; Boschee et al., 2005; CNCNS, 2005; Grishman et al, 2005; Jiang and Zhai, 2007; Chan and Roth, 2010; Sun et al., 2011) and the kernel-based method (Zelenko et al., 2003; Culotta and Sorensen, 2004; Bunescu and Mooney, 2005a; Bunescu and Mooney, 2005b; Zhang et al."}, {"heading": "2 Models", "text": "In this thesis, we examine two different representations for the sentences in RE: (i) the standard representation, called SEQ, which takes into account all the words in the sentences, and (ii) the dependency representation, called DEP, which only takes into account the words along the dependency paths between the two units. In the following, unless expressly stated, all statements about the sentences point to both representations SEQ and DEP. Throughout this paper, we assume that the input sentences of the relation mentions have the same fixed length n. This can be achieved by placing n on the length of the input sentences and filling the shorter sentences with a special character. Let W = w1w2. Let W = w1w2 mention the input sentence of some relation mentions, where wi is the i-th word in the sentence. Let also wi1 and wi2 the two heads of the two entities of interest."}, {"heading": "2.1 The Separate Models", "text": "We describe two typical NN architectures for RE, which form the basis of the combined models in this work."}, {"heading": "2.1.1 The Convolutional Neural Networks", "text": "In CNNs (Kalchbrenner et al., 2014; Kim, 2014), given a window size of k, we have a series of ck characteristic cards (filters). Each characteristic card f is a weight matrix f = [f1, f2,.., fk], where fi is a vector to be learned during the training as a model parameter. The core of the CNNs is the application of the Convolutionary Operator to the input matrix X: sfi = g (k \u2212 1 \u2211 j = 0fj + 1xj + i + b) in order to generate a score sequence sf = [sf1, s f \u2212 f 2,.., s fn \u2212 k + 1], which is interpreted as a more abstract representation of the input matrix X: sfi = g (k \u2212 1 \u0445 j = 0fj + 1xj + b), where b is a bias term and g is the tanh function. In the next step, we obtain the maximum abstract function to generate the aggregation of these values by means of an aggregation in the positive."}, {"heading": "2.1.2 The Recurrent Neural Networks", "text": "In RNNs, we consider the input matrix X = [x1, x2,.., xn] as a sequence of column vectors indexed from 1 to n. At each step i, we calculate the hidden vector hi from the current input vector xi and the previous hidden vector hi \u2212 1 using the nonlinear transformation function: hi = \u03a6 (xi, hi \u2212 1). This recurring calculation can be done using three different directional mechanisms: (i) the forward mechanism that returns from 1 to n and generates the forward hidden vector sequences: R (x1, x2,., xn) = h1, h2,., hn, the reverse mechanism, the RNNs from n to 1 and the results in the backward hidden vector sequence R (xn, xn \u2212 1,."}, {"heading": "2.2 The Combined Models", "text": "We will first present three different methods for the composition of CNNs and RNNNs: Ensembling, Stacking and Voting, which are investigated in this thesis. Combining the neural networks with the log-linear model will be discussed in the next section."}, {"heading": "2.2.1 Ensembling", "text": "In this method, we first execute CNN and RNN in Section 2.1 using the input matrix X to collect the corresponding distributions pC (y | X) and pR (y | X), then combine CNN and RNN by multiplying their distributions (element by element): pensemble (y | X) = 1 Z pC (y | X) pR (y | X) (Z is a normalization constant)."}, {"heading": "2.2.2 Stacking", "text": "The overall architecture of the stack method is to use one of the two network architectures (i.e. CNNs and RNNs) to generalize the hidden vectors of the other architecture, and the expectation is that we can learn more effective properties for RE over such a deeper architecture by switching between the local and global representations provided by CNNs and RNNs. We are examining two variants of this method. On the other hand, the first variant, called RNN-CNN, applies the CNN model in Section 2.1.1 to the hidden vector sequence generated by some RNN in Section 2.1.2 to perform RE. On the other hand, the second variant, called CNN-RNN, uses the CNN model to capture the hidden vector sequences, i.e. the input is fed into any RNN for RE. For the second variant, since the length of the hidden vector s f = [sf1, s, f, 2."}, {"heading": "2.2.3 Voting", "text": "Instead of integrating CNNs and RNNs at the model level like the two previous methods, the voting method opts for a relation mention X by aligning the individual decisions of the different models. Although there are several voting schemes in the literature, we use the simplest scheme of majority voting for this work. If there is more than one relation class that receives the highest number of votes, the relation class returned by a model would be chosen with the highest probability."}, {"heading": "2.3 The Hybrid Models", "text": "In order to further improve the RE performance of the above models, we are investigating the integration of these neural network models with the traditional loglinear choice model, which is based on various linguistic features from previous RE research (Zhou et al., 2005; Sun et al., 2011; Gormley et al., 2015). Let's take the ensembling model in Section 2.2.1 as an example. The corresponding hybrid model in this case would be: phybrid (y | X) = 1Z pC (y | X) plogin (y | X) plogin (y | X), as the summing plogin model in Section 2.2.1, the distribution of the loglinear model and Z could be the normalization constant. The parameters of the log-linear model are set together with the predicted plogin networks (y | X)."}, {"heading": "2.4 Training", "text": "We train the models by minimizing the negative log likelihood function using the stochastic gradient descent algorithm with mixed mini-batches and the AdaDelta update rule (Zeiler, 2012; Kim, 2014). Gradients are calculated by backpropagation, while regularization is performed by a dropout on the hidden vectors in front of the multilayered neural networks (Hinton et al., 2012). During the training, in addition to the weight matrices, we optimized the embedding tables E, D, T, Q to achieve the optimal state. Finally, we scale weights whose l2 standards exceed a hyperparameter (Kim, 2014; Nguyen and Grishman, 2015a)."}, {"heading": "3 Experiments", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "3.1 Resources and Parameters", "text": "For all the experiments below, we use the prefabricated Word embedded word2vec with 300 dimensions by Mikolov et al. (2013) to initialize the Word embedded table E. The parameters for CNNs and the transformation of networks have been adopted from the previous studies, i.e. the window size for feature maps = {2, 3, 4, 5}, 150 feature maps for each window size, 50 dimensions for all embedded tables (except the word embedded table E), the failure rate = 0.5, the minibatch size = 50, the hyperparameter for the l2 standards = 3 (Kim, 2014; Nguyen and Grishman, 2015a). With regard to RNNNNs, we employ 300 units in the hidden layers."}, {"heading": "3.2 Dataset", "text": "We evaluate our models on two datasets: the ACE 2005 dataset for relation extraction and the SemEval-2010 Task 8 dataset (Hendrickx et al., 2010) for relation categorization. The ACE 2005 dataset comprises 6 different domains: broadcast conversation (bc), broadcast news (bn), telephone conversations (cts), newswire (nw), usenet (un) and web blogs (wl). According to the common practice of domain adaptation research to this dataset (Plank and Moschitti, 2013; Nguyen and Grishman, 2014; Nguyen et al., 2015c; Gormley et al., 2015) we use news (the merger of bn and nw) as training data, half of bc as development set and the rest (cts, wl and the other half of bc) as test data."}, {"heading": "3.3 RNN Architectures", "text": "This section evaluates the performance of different RNN architectures for RNE on the development set. In particular, we compare different design combinations of the four following factors: (i) Sentence representations (i.e., SEQ or DEP), (ii) transformation functions \u03a6 (i.e., FF or GRU), (iii) strategies for applying the hidden vector sequence for RE (i.e., HEAD or MAX), and (iv) instructions for operating RNNs (i.e., forward (\u2192), backward (\u2190) or bidirectional (). Table 1 presents the results. Key conclusions include: 2There was an error in Gormley et al. (2015), which reported 43,518 total relationships in the training set. Authors acknowledged this error. (i) Assuming the same decisions for the other three corresponding factors, the RNU is more effective than FF, SEQ is better than DEP."}, {"heading": "3.4 Evaluating the Combined Models", "text": "We evaluate the combination methods for CNNs and RNNs presented in Section 2.2. In particular, for each method, we examine three models that are combined by one of the three RNN models FORWARD, BACKWARD, BIDIRECT and the CNN model. For example, in the batch method, the three combination models that correspond to the RNN-CNN variant are FORWARD-CNN, BACKWARDCNN, BIDIRECT-CNN, while the three combination models that correspond to the CNN-RNN variant are CNN-FORWARD, CNN-BACKWARD, CNBIDIRECT. Notations for the other methods are self-explanatory. Model performance on the development set is given in Table 3.4, which also includes the performance of the individual models (i.e, CNN, FORWARD, BACKWARD, BIDIRECT) in the WARD models."}, {"heading": "3.5 Evaluating the Hybrid Models", "text": "This section examines the hybrid and hybrid voting models (Section 2.3) to see if they can further improve the performance of the respective neural network models. In particular, we evaluate the individual models: CNN, BIDIRECT, FORWARD, BACKWARD and the combined models: STACK-FORWARD, VOTE-BIDIRECT and VOTE-BACKWARD when complemented with the traditional log-linear model (the hybrid models). In addition, to confirm the hybrid voting models in Section 2.3, we also test the corresponding hybrid voting models. Experimental results are shown in Table 3. There are three main conclusions: (i) For all models in the columns \"Neural Networks,\" \"Hybrid models\" and \"Hybrid-Voting Mod-els,\" which significantly exceed the hybrid voting model iels. \""}, {"heading": "3.6 Comparing to the State-of-the-art", "text": "The state-of-the-art ACE 2005 system for the invisible areas was the feature-rich compositional embedding model (FCM) and the hybrid FCM model by Gormley et al. (2015). In this section, we compare the proposed hybrid voting systems with these state-of-the-art systems on the test domains bc, cts, wl. Table 4 shows the results. For the sake of completeness, we also include the performance of the log-linear model and the separate models CNN, BIDIRECT, FORWARD, BACKWARD, which serve as other baselines for this work. The table shows that the separate neural networks exceed the FCM model across domains, but are still inferior to the hybrid FCM model due to the introduction of the log-linear model in FCM. However, when the networks are combined and integrated with the log-linear model, they (the hybrid choice system) become significantly better than the average performance of the M models (the overall improvement of the M models)."}, {"heading": "3.7 Relation Classification Experiments", "text": "The most important observation is that the hybrid voting systems VOTEBIDIRECT and VOTE-BACKWARD reach the state of the art for this dataset, which further underlines their usefulness for classifying relationships. In this case, the hybrid voting STACK-FORWARD system is less effective, possibly due to the small size of the SemEval dataset, which is not sufficient to train such a deep model."}, {"heading": "3.8 Analysis", "text": "To better understand why the combination of CNNs and RNNs outperforms the individual networks, we evaluate the performance breakdown per relationship for the CNN and BIDIRECT models. The results of the ACE 2005 data set are in Tabel 6.One of the most important findings is that while CNN and BIDIRECT have comparable overall performance, their callbacks to each relationship vary greatly. In particular, BIDIRECT remembers the PHYS relationship much better, while CNN callbacks are significantly better for the ART, ORG-AFF and GEN-AFF relationships. A closer examination reveals two facts: (i) the mentions of the PHYS relationship, which are correctly predicted only by BIDIRECT, include the long distances between two units, such as the PHYS relationship between \"some\" (one person-unit) and \"desert\" (one location unit)."}, {"heading": "4 Related Work", "text": "Based on the invention of distributed representations for words (Bengio et al., 2003; Mnih and Hinton, 2008; Collobert and Weston, 2008; Turian et al., 2010; Mikolov et al., 2013), CNNs and RNNNNs have achieved significant successes in various NLP tasks, including sequential labeling (Collobert et al., 2011), sentence modeling and classification (Kalchbrenner et al., 2014; Kim, 2014), paraphrase identification (Yin and Shots, 2015), event extraction (Nguyen and Grishman, 2015b; Chen et al., 2015) for CNNs and machine translation (Cho et al., 2014; Bahdanau et al., 2015) for RNNNNNNs and Neng (Neng), to name a few."}, {"heading": "5 Conclusion", "text": "The experimental results show that a simple majority decision between CNNs, RNNs and the associated hybrid models is the best combination method. We achieve state-of-the-art in relation extraction as well as relation categorization. In the future, we plan to further evaluate the proposed methods for other tasks such as event extraction and slot filling in the KBP evaluation."}, {"heading": "Acknowledgment", "text": "We thank Matthew Gormley and Mo Yu for providing the dataset and Kyunghyun Cho and Yifan He for valuable suggestions."}], "references": [{"title": "Neural machine translation by jointly learning to align and translate", "author": ["Kyunghyun Cho", "Yoshua Bengio"], "venue": "In ICLR", "citeRegEx": "Bahdanau et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2015}, {"title": "Learning long-term dependencies with gradient descent is difficult", "author": ["Bengio et al.1994] Yoshua Bengio", "Patrice Simard", "Paolo Frasconi"], "venue": "In Journal of Machine Learning Research", "citeRegEx": "Bengio et al\\.,? \\Q1994\\E", "shortCiteRegEx": "Bengio et al\\.", "year": 1994}, {"title": "A neural probabilistic language model", "author": ["Bengio et al.2003] Yoshua Bengio", "R\u00e9jean Ducharme", "Pascal Vincent", "Christian Jauvin"], "venue": "In Journal of Machine Learning Research", "citeRegEx": "Bengio et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Bengio et al\\.", "year": 2003}, {"title": "Automatic information extraction", "author": ["Ralph Weischedel", "Alex Zamanian"], "venue": "In Proceedings of the International Conference on Intelligence Analysis", "citeRegEx": "Boschee et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Boschee et al\\.", "year": 2005}, {"title": "A shortest path dependency kernel for relation extraction", "author": ["Bunescu", "Mooney2005a] Razvan Bunescu", "Raymond Mooney"], "venue": "HLT-EMNLP", "citeRegEx": "Bunescu et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Bunescu et al\\.", "year": 2005}, {"title": "Subsequence kernels for relation extraction", "author": ["Bunescu", "Mooney2005b] Razvan Bunescu", "Raymond J. Mooney"], "venue": null, "citeRegEx": "Bunescu et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Bunescu et al\\.", "year": 2005}, {"title": "Exploiting background knowledge for relation extraction", "author": ["Chan", "Roth2010] Yee S. Chan", "Dan Roth"], "venue": "In COLING", "citeRegEx": "Chan et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Chan et al\\.", "year": 2010}, {"title": "Event extraction via dynamic multi-pooling convolutional neural networks. In ACL-IJCNLP", "author": ["Chen et al.2015] Yubo Chen", "Liheng Xu", "Kang Liu", "Daojian Zeng", "Jun Zhao"], "venue": null, "citeRegEx": "Chen et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Chen et al\\.", "year": 2015}, {"title": "Learning phrase representations using rnn encoder\u2013decoder for statistical machine translation", "author": ["Cho et al.2014] Kyunghyun Cho", "Bart van Merrienboer", "Caglar Gulcehre", "Dzmitry Bahdanau", "Fethi Bougares", "Holger Schwenk", "Yoshua Bengio"], "venue": null, "citeRegEx": "Cho et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Cho et al\\.", "year": 2014}, {"title": "A unified architecture for natural language processing: deep neural networks with multitask learning", "author": ["Collobert", "Weston2008] Ronan Collobert", "Jason Weston"], "venue": null, "citeRegEx": "Collobert et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Collobert et al\\.", "year": 2008}, {"title": "Natural language processing (almost) from scratch", "author": ["Jason Weston", "Lon Bottou", "Michael Karlen", "Koray Kavukcuoglu", "Pavel P. Kuksa"], "venue": "In CoRR", "citeRegEx": "Collobert et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Collobert et al\\.", "year": 2011}, {"title": "Dependency tree kernels for relation extraction", "author": ["Culotta", "Sorensen2004] Aron Culotta", "Jeffrey Sorensen"], "venue": null, "citeRegEx": "Culotta et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Culotta et al\\.", "year": 2004}, {"title": "Classifying relations by ranking with convolutional neural networks. In ACLIJCNLP", "author": ["Bing Xiang", "Bowen Zhou"], "venue": null, "citeRegEx": "Santos et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Santos et al\\.", "year": 2015}, {"title": "Chain based rnn for relation classification", "author": ["Ebrahimi", "Dou2015] Javid Ebrahimi", "Dejing Dou"], "venue": null, "citeRegEx": "Ebrahimi et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Ebrahimi et al\\.", "year": 2015}, {"title": "Improved relation extraction with feature-rich compositional embedding models", "author": ["Mo Yu", "Mark Dredze"], "venue": null, "citeRegEx": "Gormley et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Gormley et al\\.", "year": 2015}, {"title": "A novel connectionist system for unconstrained handwriting recognition", "author": ["Graves et al.2009] A. Graves", "Marcus EichenbergerLiwicki", "S. Fernandez", "R. Bertolami", "H. Bunke", "J. Schmidhuber"], "venue": "In IEEE Transactions on Pattern Analysis and Machine Intelli-", "citeRegEx": "Graves et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Graves et al\\.", "year": 2009}, {"title": "Semeval-2010 task 8: Multi-way classification of semantic relations", "author": ["Su Nam Kim", "Zornitsa Kozareva", "Preslav Nakov", "Diarmuid \u00d3 S\u00e9aghdha", "Sebastian Pad\u00f3", "Marco Pennacchiotti", "Lorenza Romano", "Stan Szpakowicz"], "venue": null, "citeRegEx": "Hendrickx et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Hendrickx et al\\.", "year": 2010}, {"title": "Improving neural networks by preventing co-adaptation of feature detectors", "author": ["Nitish Srivastava", "Alex Krizhevsky", "Ilya Sutskever", "Ruslan Salakhutdinov"], "venue": "In CoRR,", "citeRegEx": "Hinton et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Hinton et al\\.", "year": 2012}, {"title": "Long short-term memory", "author": ["Hochreiter", "Schmidhuber1997] Sepp Hochreiter", "Jurgen Schmidhuber"], "venue": "In Neural Computation", "citeRegEx": "Hochreiter et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Hochreiter et al\\.", "year": 1997}, {"title": "A systematic exploration of the feature space for relation extraction", "author": ["Jiang", "Zhai2007] Jing Jiang", "ChengXiang Zhai"], "venue": null, "citeRegEx": "Jiang et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Jiang et al\\.", "year": 2007}, {"title": "A convolutional neural network for modelling sentences", "author": ["Edward Grefenstette", "Phil Blunsom"], "venue": null, "citeRegEx": "Kalchbrenner et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Kalchbrenner et al\\.", "year": 2014}, {"title": "Combining lexical, syntactic, and semantic features with maximum entropy models for information extraction", "author": ["Nanda Kambhatla"], "venue": null, "citeRegEx": "Kambhatla.,? \\Q2004\\E", "shortCiteRegEx": "Kambhatla.", "year": 2004}, {"title": "Convolutional neural networks for sentence classification", "author": ["Yoon Kim"], "venue": "In EMNLP", "citeRegEx": "Kim.,? \\Q2014\\E", "shortCiteRegEx": "Kim.", "year": 2014}, {"title": "A dependency-based neural network for relation classification", "author": ["Liu et al.2015] Yang Liu", "Furu Wei", "Sujian Li", "Heng Ji", "Ming Zhou", "Houfeng WANG"], "venue": "ACL-IJCNLP", "citeRegEx": "Liu et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Liu et al\\.", "year": 2015}, {"title": "Efficient estimation of word representations in vector space. In ICLR", "author": ["Kai Chen", "Greg Corrado", "Jeffrey Dean"], "venue": null, "citeRegEx": "Mikolov et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "A scalable hierarchical distributed language model", "author": ["Mnih", "Hinton2008] Andriy Mnih", "Geoffrey Hinton"], "venue": null, "citeRegEx": "Mnih et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Mnih et al\\.", "year": 2008}, {"title": "Employing word representations and regularization for domain adaptation of relation extraction", "author": ["Nguyen", "Grishman2014] Thien Huu Nguyen", "Ralph Grishman"], "venue": null, "citeRegEx": "Nguyen et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Nguyen et al\\.", "year": 2014}, {"title": "2015a. Relation extraction: Perspective from convolutional neural networks", "author": ["Nguyen", "Grishman2015a] Thien Huu Nguyen", "Ralph Grishman"], "venue": null, "citeRegEx": "Nguyen et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Nguyen et al\\.", "year": 2015}, {"title": "2015b. Event detection and domain adaptation with convolutional neural networks. In ACL-IJCNLP", "author": ["Nguyen", "Grishman2015b] Thien Huu Nguyen", "Ralph Grishman"], "venue": null, "citeRegEx": "Nguyen et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Nguyen et al\\.", "year": 2015}, {"title": "Convolution kernels on constituent, dependency and sequential structures for relation extraction", "author": ["Alessandro Moschitti", "Giuseppe Riccardi"], "venue": null, "citeRegEx": "Nguyen et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Nguyen et al\\.", "year": 2009}, {"title": "Semantic representations for domain adaptation: A case study on the tree kernel-based method for relation extraction", "author": ["Barbara Plank", "Ralph Grishman"], "venue": "ACLIJCNLP", "citeRegEx": "Nguyen et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Nguyen et al\\.", "year": 2015}, {"title": "On the difficulty of training recurrent neural networks", "author": ["Tomas Mikolov", "Yoshua Bengio"], "venue": "In arXiv preprint arXiv:1211.5063", "citeRegEx": "Pascanu et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Pascanu et al\\.", "year": 2012}, {"title": "Embedding semantic similarity in tree kernels for domain adaptation of relation extraction", "author": ["Plank", "Moschitti2013] Barbara Plank", "Alessandro Moschitti"], "venue": null, "citeRegEx": "Plank et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Plank et al\\.", "year": 2013}, {"title": "Exploiting constituent dependencies for tree kernel-based semantic relation extraction", "author": ["Qian et al.2008] Longhua Qian", "Guodong Zhou", "Fang Kong", "Qiaoming Zhu", "Peide Qian"], "venue": "In COLING", "citeRegEx": "Qian et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Qian et al\\.", "year": 2008}, {"title": "Semantic compositionality through recursive matrixvector spaces", "author": ["Brody Huval", "Christopher D. Manning", "Andrew Y. Ng"], "venue": null, "citeRegEx": "Socher et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Socher et al\\.", "year": 2012}, {"title": "Semi-supervised relation extraction with large-scale word clustering", "author": ["Sun et al.2011] Ang Sun", "Ralph Grishman", "Satoshi Sekine"], "venue": null, "citeRegEx": "Sun et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Sun et al\\.", "year": 2011}, {"title": "Word representations: A simple and general method for semi-supervised learning", "author": ["Turian et al.2010] Joseph Turian", "Lev-Arie Ratinov", "Yoshua Bengio"], "venue": null, "citeRegEx": "Turian et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Turian et al\\.", "year": 2010}, {"title": "Classifying relations via long short term memory networks along shortest dependency paths", "author": ["Xu et al.2015] Yan Xu", "Lili Mou", "Ge Li", "Yunchuan Chen", "Hao Peng", "Zhi Jin"], "venue": null, "citeRegEx": "Xu et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Xu et al\\.", "year": 2015}, {"title": "Convolutional neural network for paraphrase identification", "author": ["Yin", "Sch\u00fctze2015] Wenpeng Yin", "Hinrich Sch\u00fctze"], "venue": null, "citeRegEx": "Yin et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Yin et al\\.", "year": 2015}, {"title": "Combining word embeddings and feature embeddings for fine-grained relation extraction", "author": ["Yu et al.2015] Mo Yu", "Matthew R. Gormley", "Mark Dredze"], "venue": null, "citeRegEx": "Yu et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Yu et al\\.", "year": 2015}, {"title": "Adadelta: An adaptive learning rate method", "author": ["Matthew D. Zeiler"], "venue": "In CoRR,", "citeRegEx": "Zeiler.,? \\Q2012\\E", "shortCiteRegEx": "Zeiler.", "year": 2012}, {"title": "Exploring various knowledge in relation extraction", "author": ["Chinatsu Aone", "Anthony Richardella"], "venue": "In Journal of Machine Learning Research", "citeRegEx": "Zelenko et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Zelenko et al\\.", "year": 2003}, {"title": "Relation classification via convolutional deep neural network. In COLING", "author": ["Zeng et al.2014] Daojian Zeng", "Kang Liu", "Siwei Lai", "Guangyou Zhou", "Jun Zhao"], "venue": null, "citeRegEx": "Zeng et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Zeng et al\\.", "year": 2014}, {"title": "Distant supervision for relation extraction via piecewise convolutional neural networks", "author": ["Zeng et al.2015] Daojian Zeng", "Kang Liu", "Yubo Chen", "Jun Zhao"], "venue": null, "citeRegEx": "Zeng et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Zeng et al\\.", "year": 2015}, {"title": "Relation classification via recurrent neural network", "author": ["Zhang", "Wang2015] Dongxu Zhang", "Dong Wang"], "venue": null, "citeRegEx": "Zhang et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Zhang et al\\.", "year": 2015}, {"title": "A composite kernel to extract relations between entities with both flat and structured features", "author": ["Zhang et al.2006] Min Zhang", "Jie Zhang", "Jian Su", "GuoDong Zhou"], "venue": "COLING-ACL", "citeRegEx": "Zhang et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Zhang et al\\.", "year": 2006}, {"title": "Exploring various knowledge in relation extraction", "author": ["Zhou et al.2005] GuoDong Zhou", "Jian Su", "Jie Zhang", "Min Zhang"], "venue": null, "citeRegEx": "Zhou et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Zhou et al\\.", "year": 2005}, {"title": "Tree kernel-based relation extraction with context-sensitive structured parse tree information", "author": ["Zhou et al.2007] GuoDong Zhou", "Min Zhang", "DongHong Ji", "QiaoMing Zhu"], "venue": null, "citeRegEx": "Zhou et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Zhou et al\\.", "year": 2007}], "referenceMentions": [{"referenceID": 21, "context": "The two methods dominating RE research in the last decade are the feature-based method (Kambhatla, 2004; Boschee et al., 2005; Zhou et al., 2005; Grishman et al., 2005; Jiang and Zhai, 2007; Chan and Roth, 2010; Sun et al., 2011) and the kernel-based method (Zelenko et al.", "startOffset": 87, "endOffset": 229}, {"referenceID": 3, "context": "The two methods dominating RE research in the last decade are the feature-based method (Kambhatla, 2004; Boschee et al., 2005; Zhou et al., 2005; Grishman et al., 2005; Jiang and Zhai, 2007; Chan and Roth, 2010; Sun et al., 2011) and the kernel-based method (Zelenko et al.", "startOffset": 87, "endOffset": 229}, {"referenceID": 46, "context": "The two methods dominating RE research in the last decade are the feature-based method (Kambhatla, 2004; Boschee et al., 2005; Zhou et al., 2005; Grishman et al., 2005; Jiang and Zhai, 2007; Chan and Roth, 2010; Sun et al., 2011) and the kernel-based method (Zelenko et al.", "startOffset": 87, "endOffset": 229}, {"referenceID": 35, "context": "The two methods dominating RE research in the last decade are the feature-based method (Kambhatla, 2004; Boschee et al., 2005; Zhou et al., 2005; Grishman et al., 2005; Jiang and Zhai, 2007; Chan and Roth, 2010; Sun et al., 2011) and the kernel-based method (Zelenko et al.", "startOffset": 87, "endOffset": 229}, {"referenceID": 41, "context": ", 2011) and the kernel-based method (Zelenko et al., 2003; Culotta and Sorensen, 2004; Bunescu and Mooney, 2005a; Bunescu and Mooney, 2005b; Zhang et al., 2006; Zhou et al., 2007; Qian et al., 2008; Nguyen et al., 2009; Plank and Moschitti, 2013).", "startOffset": 36, "endOffset": 246}, {"referenceID": 45, "context": ", 2011) and the kernel-based method (Zelenko et al., 2003; Culotta and Sorensen, 2004; Bunescu and Mooney, 2005a; Bunescu and Mooney, 2005b; Zhang et al., 2006; Zhou et al., 2007; Qian et al., 2008; Nguyen et al., 2009; Plank and Moschitti, 2013).", "startOffset": 36, "endOffset": 246}, {"referenceID": 47, "context": ", 2011) and the kernel-based method (Zelenko et al., 2003; Culotta and Sorensen, 2004; Bunescu and Mooney, 2005a; Bunescu and Mooney, 2005b; Zhang et al., 2006; Zhou et al., 2007; Qian et al., 2008; Nguyen et al., 2009; Plank and Moschitti, 2013).", "startOffset": 36, "endOffset": 246}, {"referenceID": 33, "context": ", 2011) and the kernel-based method (Zelenko et al., 2003; Culotta and Sorensen, 2004; Bunescu and Mooney, 2005a; Bunescu and Mooney, 2005b; Zhang et al., 2006; Zhou et al., 2007; Qian et al., 2008; Nguyen et al., 2009; Plank and Moschitti, 2013).", "startOffset": 36, "endOffset": 246}, {"referenceID": 29, "context": ", 2011) and the kernel-based method (Zelenko et al., 2003; Culotta and Sorensen, 2004; Bunescu and Mooney, 2005a; Bunescu and Mooney, 2005b; Zhang et al., 2006; Zhou et al., 2007; Qian et al., 2008; Nguyen et al., 2009; Plank and Moschitti, 2013).", "startOffset": 36, "endOffset": 246}, {"referenceID": 14, "context": "Although these approaches are able to exploit the symbolic (discrete) structures within relation mentions, they also suffer from the difficulty to generalize over the unseen words (Gormley et al., 2015), motivating some very recent work on employing the continuous representations of words (word embeddings) to do RE.", "startOffset": 180, "endOffset": 202}, {"referenceID": 43, "context": "The NN research for relation extraction and classification has centered around two main network architectures: convolutional neural networks (CNNs) (dos Santos et al., 2015; Zeng et al., 2015) and recursive/recurrent neural", "startOffset": 148, "endOffset": 192}, {"referenceID": 34, "context": "networks (Socher et al., 2012; Xu et al., 2015).", "startOffset": 9, "endOffset": 47}, {"referenceID": 37, "context": "networks (Socher et al., 2012; Xu et al., 2015).", "startOffset": 9, "endOffset": 47}, {"referenceID": 8, "context": "e, the k-grams) of the relation mentions (Nguyen and Grishman, 2015a) while the latter adaptively accumulate the context information in the whole sentence via the memory units, thereby encoding the global and possibly unconsecutive patterns for RE (Hochreiter and Schmidhuber, 1997; Cho et al., 2014).", "startOffset": 248, "endOffset": 300}, {"referenceID": 46, "context": "In order to prepare the relation mention for neural networks, we first transform each word wi into a real-valued vector xi using the concatenation of the following seven vectors, motivated by the previous research on neural networks and feature analysis for RE (Zhou et al., 2005; Sun et al., 2011; Gormley et al., 2015).", "startOffset": 261, "endOffset": 320}, {"referenceID": 35, "context": "In order to prepare the relation mention for neural networks, we first transform each word wi into a real-valued vector xi using the concatenation of the following seven vectors, motivated by the previous research on neural networks and feature analysis for RE (Zhou et al., 2005; Sun et al., 2011; Gormley et al., 2015).", "startOffset": 261, "endOffset": 320}, {"referenceID": 14, "context": "In order to prepare the relation mention for neural networks, we first transform each word wi into a real-valued vector xi using the concatenation of the following seven vectors, motivated by the previous research on neural networks and feature analysis for RE (Zhou et al., 2005; Sun et al., 2011; Gormley et al., 2015).", "startOffset": 261, "endOffset": 320}, {"referenceID": 20, "context": "In CNNs (Kalchbrenner et al., 2014; Kim, 2014), given a window size of k, we have a set of ck feature maps (filters).", "startOffset": 8, "endOffset": 46}, {"referenceID": 22, "context": "In CNNs (Kalchbrenner et al., 2014; Kim, 2014), given a window size of k, we have a set of ck feature maps (filters).", "startOffset": 8, "endOffset": 46}, {"referenceID": 35, "context": "This is motivated by the importance of the two mention heads in RE (Sun et al., 2011; Nguyen and Grishman, 2014).", "startOffset": 67, "endOffset": 112}, {"referenceID": 1, "context": "Unfortunately, the application of FF causes the socalled \u201cvanishing/exploding gradient\u201d problems (Bengio et al., 1994), making it challenging to train RNNs properly (Pascanu et al.", "startOffset": 97, "endOffset": 118}, {"referenceID": 31, "context": ", 1994), making it challenging to train RNNs properly (Pascanu et al., 2012).", "startOffset": 54, "endOffset": 76}, {"referenceID": 8, "context": "GRU is shown to be much simpler than LSTM in terms of computation but still achieves the comparable performance (Cho et al., 2014).", "startOffset": 112, "endOffset": 130}, {"referenceID": 8, "context": "In this work, we apply a variant of the memory units: the Gated Recurrent Units from Cho et al. (2014), called GRU .", "startOffset": 85, "endOffset": 103}, {"referenceID": 46, "context": "In order to further improve the RE performance of models above, we investigate the integration of these neural network models with the traditional loglinear model that relies on various linguistic features from the past research on RE (Zhou et al., 2005; Sun et al., 2011; Gormley et al., 2015).", "startOffset": 235, "endOffset": 294}, {"referenceID": 35, "context": "In order to further improve the RE performance of models above, we investigate the integration of these neural network models with the traditional loglinear model that relies on various linguistic features from the past research on RE (Zhou et al., 2005; Sun et al., 2011; Gormley et al., 2015).", "startOffset": 235, "endOffset": 294}, {"referenceID": 14, "context": "In order to further improve the RE performance of models above, we investigate the integration of these neural network models with the traditional loglinear model that relies on various linguistic features from the past research on RE (Zhou et al., 2005; Sun et al., 2011; Gormley et al., 2015).", "startOffset": 235, "endOffset": 294}, {"referenceID": 40, "context": "We train the models by minimizing the negative log-likelihood function using the stochastic gradient descent algorithm with shuffled mini-batches and the AdaDelta update rule (Zeiler, 2012; Kim, 2014).", "startOffset": 175, "endOffset": 200}, {"referenceID": 22, "context": "We train the models by minimizing the negative log-likelihood function using the stochastic gradient descent algorithm with shuffled mini-batches and the AdaDelta update rule (Zeiler, 2012; Kim, 2014).", "startOffset": 175, "endOffset": 200}, {"referenceID": 17, "context": "The gradients are computed via back-propagation while regularization is executed by a dropout on the hidden vectors before the the multilayer neural networks (Hinton et al., 2012).", "startOffset": 158, "endOffset": 179}, {"referenceID": 22, "context": "Finally, we rescale the weights whose l2-norms exceed a hyperparameter (Kim, 2014; Nguyen and Grishman, 2015a).", "startOffset": 71, "endOffset": 110}, {"referenceID": 22, "context": "5, the mini-batch size = 50, the hyperparameter for the l2 norms = 3 (Kim, 2014; Nguyen and Grishman, 2015a).", "startOffset": 69, "endOffset": 108}, {"referenceID": 23, "context": "For all the experiments below, we utilize the pretrained word embeddings word2vec with 300 dimensions from Mikolov et al. (2013) to initialize the word embedding table E.", "startOffset": 107, "endOffset": 129}, {"referenceID": 16, "context": "We evaluate our models on two datasets: the ACE 2005 dataset for relation extraction and the SemEval-2010 Task 8 dataset (Hendrickx et al., 2010) for relation classifica-", "startOffset": 121, "endOffset": 145}, {"referenceID": 14, "context": ", 2015c; Gormley et al., 2015), we use news (the union of bn and nw) as the training data, a half of bc as the development set and the remainder (cts, wl and the other half of bc) as the test data. Note that we are using the data prepared by Gormley et. al (2015), thus utilizing the same data split on bc as well as", "startOffset": 9, "endOffset": 264}, {"referenceID": 34, "context": "In order to make it compatible with the previous research (Socher et al., 2012; Gormley et al., 2015), for this dataset, besides the word embeddings and the distance embeddings, we apply the name tagging, part of speech tagging and WordNet features (inherited from Socher et al.", "startOffset": 58, "endOffset": 101}, {"referenceID": 14, "context": "In order to make it compatible with the previous research (Socher et al., 2012; Gormley et al., 2015), for this dataset, besides the word embeddings and the distance embeddings, we apply the name tagging, part of speech tagging and WordNet features (inherited from Socher et al.", "startOffset": 58, "endOffset": 101}, {"referenceID": 34, "context": "The other settings are also adopted from the past studies (Socher et al., 2012; Xu et al., 2015).", "startOffset": 58, "endOffset": 96}, {"referenceID": 37, "context": "The other settings are also adopted from the past studies (Socher et al., 2012; Xu et al., 2015).", "startOffset": 58, "endOffset": 96}, {"referenceID": 14, "context": ", 2012; Gormley et al., 2015), for this dataset, besides the word embeddings and the distance embeddings, we apply the name tagging, part of speech tagging and WordNet features (inherited from Socher et al. (2012) and encoded by the real-valued vectors for each word).", "startOffset": 8, "endOffset": 214}, {"referenceID": 14, "context": "It was an error in Gormley et al. (2015) that reported 43,518 total relations in the training set.", "startOffset": 19, "endOffset": 41}, {"referenceID": 14, "context": "The state-of-the-art system on the ACE 2005 for the unseen domains has been the feature-rich compositional embedding model (FCM) and the hybrid FCM model from Gormley et al. (2015). In this section, we compare the proposed hybrid-voting systems with these state-of-the-art systems on the test domains bc, cts, wl.", "startOffset": 159, "endOffset": 181}, {"referenceID": 16, "context": "SVM (Hendrickx et al., 2010) 82.", "startOffset": 4, "endOffset": 28}, {"referenceID": 34, "context": "2 RNN (Socher et al., 2012) 77.", "startOffset": 6, "endOffset": 27}, {"referenceID": 34, "context": "6 MVRNN (Socher et al., 2012) 82.", "startOffset": 8, "endOffset": 29}, {"referenceID": 42, "context": "4 CNN (Zeng et al., 2014) 82.", "startOffset": 6, "endOffset": 25}, {"referenceID": 14, "context": "1\u2020 FCM (Gormley et al., 2015) 83.", "startOffset": 7, "endOffset": 29}, {"referenceID": 14, "context": "0 Hybrid FCM (Gormley et al., 2015) 83.", "startOffset": 13, "endOffset": 35}, {"referenceID": 23, "context": "4 DepNN (Liu et al., 2015) 83.", "startOffset": 8, "endOffset": 26}, {"referenceID": 37, "context": "6 SDP-LSTM (Xu et al., 2015) 83.", "startOffset": 11, "endOffset": 28}, {"referenceID": 2, "context": "Starting from the invention of the distributed representations for words (Bengio et al., 2003; Mnih and Hinton, 2008; Collobert and Weston, 2008; Turian et al., 2010; Mikolov et al., 2013), CNNs and RNNs have gained significant successes on various NLP tasks, including sequential labeling (Collobert et al.", "startOffset": 73, "endOffset": 188}, {"referenceID": 36, "context": "Starting from the invention of the distributed representations for words (Bengio et al., 2003; Mnih and Hinton, 2008; Collobert and Weston, 2008; Turian et al., 2010; Mikolov et al., 2013), CNNs and RNNs have gained significant successes on various NLP tasks, including sequential labeling (Collobert et al.", "startOffset": 73, "endOffset": 188}, {"referenceID": 24, "context": "Starting from the invention of the distributed representations for words (Bengio et al., 2003; Mnih and Hinton, 2008; Collobert and Weston, 2008; Turian et al., 2010; Mikolov et al., 2013), CNNs and RNNs have gained significant successes on various NLP tasks, including sequential labeling (Collobert et al.", "startOffset": 73, "endOffset": 188}, {"referenceID": 10, "context": ", 2013), CNNs and RNNs have gained significant successes on various NLP tasks, including sequential labeling (Collobert et al., 2011), sentence modeling and clas-", "startOffset": 109, "endOffset": 133}, {"referenceID": 20, "context": "sification (Kalchbrenner et al., 2014; Kim, 2014), paraphrase identification (Yin and Sch\u00fctze, 2015),", "startOffset": 11, "endOffset": 49}, {"referenceID": 22, "context": "sification (Kalchbrenner et al., 2014; Kim, 2014), paraphrase identification (Yin and Sch\u00fctze, 2015),", "startOffset": 11, "endOffset": 49}, {"referenceID": 7, "context": "event extraction (Nguyen and Grishman, 2015b; Chen et al., 2015) for CNNs and machine translation (Cho et al.", "startOffset": 17, "endOffset": 64}, {"referenceID": 8, "context": ", 2015) for CNNs and machine translation (Cho et al., 2014; Bahdanau et al., 2015) for RNNs, to name a few.", "startOffset": 41, "endOffset": 82}, {"referenceID": 0, "context": ", 2015) for CNNs and machine translation (Cho et al., 2014; Bahdanau et al., 2015) for RNNs, to name a few.", "startOffset": 41, "endOffset": 82}, {"referenceID": 31, "context": "In particular, Socher et al. (2012) and Ebrahimi and Dou (2015) study the recursive NNs that recur over the tree structures while Xu et al.", "startOffset": 15, "endOffset": 36}, {"referenceID": 31, "context": "In particular, Socher et al. (2012) and Ebrahimi and Dou (2015) study the recursive NNs that recur over the tree structures while Xu et al.", "startOffset": 15, "endOffset": 64}, {"referenceID": 31, "context": "In particular, Socher et al. (2012) and Ebrahimi and Dou (2015) study the recursive NNs that recur over the tree structures while Xu et al. (2015) and Zhang and Wang (2015) investigate recurrent NNs.", "startOffset": 15, "endOffset": 147}, {"referenceID": 31, "context": "In particular, Socher et al. (2012) and Ebrahimi and Dou (2015) study the recursive NNs that recur over the tree structures while Xu et al. (2015) and Zhang and Wang (2015) investigate recurrent NNs.", "startOffset": 15, "endOffset": 173}, {"referenceID": 31, "context": "In particular, Socher et al. (2012) and Ebrahimi and Dou (2015) study the recursive NNs that recur over the tree structures while Xu et al. (2015) and Zhang and Wang (2015) investigate recurrent NNs. Regarding CNNs, Zeng et al. (2014) examine CNNs via the sequential representation of sentences, dos Santos et al.", "startOffset": 15, "endOffset": 235}, {"referenceID": 12, "context": "(2014) examine CNNs via the sequential representation of sentences, dos Santos et al. (2015) explore a ranking loss function with data cleaning while Zeng et al.", "startOffset": 72, "endOffset": 93}, {"referenceID": 12, "context": "(2014) examine CNNs via the sequential representation of sentences, dos Santos et al. (2015) explore a ranking loss function with data cleaning while Zeng et al. (2015) propose dynamic pooling and multi-instance learning.", "startOffset": 72, "endOffset": 169}, {"referenceID": 12, "context": "(2014) examine CNNs via the sequential representation of sentences, dos Santos et al. (2015) explore a ranking loss function with data cleaning while Zeng et al. (2015) propose dynamic pooling and multi-instance learning. For RE, Yu et al. (2015) and Gormley et al.", "startOffset": 72, "endOffset": 247}, {"referenceID": 12, "context": "(2014) examine CNNs via the sequential representation of sentences, dos Santos et al. (2015) explore a ranking loss function with data cleaning while Zeng et al. (2015) propose dynamic pooling and multi-instance learning. For RE, Yu et al. (2015) and Gormley et al. (2015) work on the feature-rich compositional embedding models.", "startOffset": 72, "endOffset": 273}, {"referenceID": 12, "context": "(2014) examine CNNs via the sequential representation of sentences, dos Santos et al. (2015) explore a ranking loss function with data cleaning while Zeng et al. (2015) propose dynamic pooling and multi-instance learning. For RE, Yu et al. (2015) and Gormley et al. (2015) work on the feature-rich compositional embedding models. Finally, the only work that combines NN architectures is due to Liu et al. (2015) but it only focuses on the stacking of the recursive NNs and CNNs for relation classification.", "startOffset": 72, "endOffset": 412}], "year": 2015, "abstractText": "The last decade has witnessed the success of the traditional feature-based method on exploiting the discrete structures such as words or lexical patterns to extract relations from text. Recently, convolutional and recurrent neural networks has provided very effective mechanisms to capture the hidden structures within sentences via continuous representations, thereby significantly advancing the performance of relation extraction. The advantage of convolutional neural networks is their capacity to generalize the consecutive kgrams in the sentences while recurrent neural networks are effective to encode long ranges of sentence context. This paper proposes to combine the traditional feature-based method, the convolutional and recurrent neural networks to simultaneously benefit from their advantages. Our systematic evaluation of different network architectures and combination methods demonstrates the effectiveness of this approach and results in the state-of-the-art performance on the ACE 2005 and SemEval dataset.", "creator": "LaTeX with hyperref package"}}}