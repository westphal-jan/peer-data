{"id": "1511.05933", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "18-Nov-2015", "title": "Seeding K-Means using Method of Moments", "abstract": "K-means is one of the most widely used algorithms for clustering in Data Mining applications, which attempts to minimize the sum of square of Euclidean distance of the points in the clusters from the respective means of the clusters. The simplicity and scalability of K-means makes it very appealing. However, K-means suffers from local minima problem, and comes with no guarantee to converge to the optimal cost. K-means++ tries to address the problem by seeding the means using a distance based sampling scheme. However, seeding the means in K-means++ needs O(K) passes through the entire dataset, which could be very costly in large amount of dataset. Here we propose a method of seeding initial means based on higher order moments of the data, which takes O(1) passes through the entire dataset to extract the initial set of means. Our method yields competitive performance with respect to all the existing K-means algorithms, whilst avoiding the expensive mean selection steps of K-means++ and other heuristics. We demonstrate the performance of our algorithm in comparison with the existing algorithms on various benchmark datasets.", "histories": [["v1", "Wed, 18 Nov 2015 20:26:42 GMT  (312kb,D)", "http://arxiv.org/abs/1511.05933v1", "arXiv admin note: text overlap witharXiv:1511.00792"], ["v2", "Sat, 21 Nov 2015 21:54:01 GMT  (575kb,D)", "http://arxiv.org/abs/1511.05933v2", "arXiv admin note: text overlap witharXiv:1511.00792"], ["v3", "Thu, 4 Feb 2016 10:21:55 GMT  (422kb,D)", "http://arxiv.org/abs/1511.05933v3", null], ["v4", "Thu, 3 Mar 2016 17:40:02 GMT  (422kb,D)", "http://arxiv.org/abs/1511.05933v4", null], ["v5", "Thu, 21 Apr 2016 21:50:39 GMT  (459kb,D)", "http://arxiv.org/abs/1511.05933v5", null], ["v6", "Fri, 3 Jun 2016 17:50:10 GMT  (532kb,D)", "http://arxiv.org/abs/1511.05933v6", null], ["v7", "Mon, 12 Sep 2016 22:33:06 GMT  (532kb,D)", "http://arxiv.org/abs/1511.05933v7", null], ["v8", "Mon, 31 Oct 2016 15:59:13 GMT  (0kb,I)", "http://arxiv.org/abs/1511.05933v8", "Paper contained an error in Equation 5 and 7"]], "COMMENTS": "arXiv admin note: text overlap witharXiv:1511.00792", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["sayantan dasgupta"], "accepted": false, "id": "1511.05933"}, "pdf": {"name": "1511.05933.pdf", "metadata": {"source": "CRF", "title": null, "authors": ["Sayantan Dasgupta"], "emails": ["sayantad@uci.edu"], "sections": [{"heading": "1 INTRODUCTION", "text": "It is the most common method of extracting clusters with real attributes. More recently, it has also been used to extract features that are further used for the classification of neural networks in Coates et al. (2011). K means is a hard problem even for K = 2 (2013), and only heuristic solutions exist. Perhaps the most common of these heuristics is the Lloyd's algorithm, which uses an EM approach; it first assigns the points to different clusters, according to their distance from the means of the clusters, and then updates the cluster means in each cluster. The algorithm uses a random initialization and comes without guarantee to achieve the global minima. K means + + in Arthur & Vassilvitskii (2007) addresses the problem by using the means of the means of the means of the means of the means of the means of the means of the means of the means, and uses convergences within each cluster."}, {"heading": "2 PROBLEM FORMULATION", "text": "Faced with a data set X'RD, the K-mean algorithm tries to find a set C'RD of K-centers (K > 0) that optimizes the following cost function: \u03c6 (C) = \u2211 x-X min c'C | | x \u2212 c | | 2 (1) The algorithm is usually started with a random initialization, but misses convergence with global minima of the target. K-mean + + tries to alleviate the problem by oversampling the seed points during each run. In our algorithm, we propose a probabilistic method to extract the means of the cluster based on the covariance and the central moment of the third order of the data we outline in this section."}, {"heading": "2.1 METHOD OF MOMENTS", "text": "Here we formulate a generative model for K-averages in which we create a cluster h = = > points based on the probability P [h = k] = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p."}, {"heading": "2.2 UNIQUENESS OF THE MEANS", "text": "The extracted values are the eigenvalues and eigenvectors of the tensor M \u00b2 3 = M3 (W, W, W, W, W, W and hence also the dependence on the eigenvalues of the own decomposition of a third order symmetrical tensor Tensor T \u00b2 3 = M3 (W, W, W, W, W, K, number of iterations L, N and tolerance.) Output: The set of eigenvalues {m, W, W, W, W, W, W, W, W, W, W, W, W, W, K, number of iterations L, N and tolerance.) Output: The set of eigenvalues {m, W, W, W, W, W, W, W, W, W, W, W, W, W, W, W, W, W, W, W, W, W, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K"}, {"heading": "3 EXPERIMENTAL RESULTS", "text": "We show the best results of our algorithms in Bahi et al. (2012) For each method of initialization that we use, we perform the best results in the world. (5) We use r = 5 and l = 2K, as these settings usually show the best results in Bahi et al. (5) We show that we have the best results in the world of the Internet. (5) We use r = 5 and l = 2K, as these settings normally show the best results in Bahi et al. (5) We show the best results in the world of the Internet. (5) We use r = 5 and l = 2K, as these settings provide the best results in Bahi et al. (2012) We show the best results in the world of the Internet. (4) We show that we have the best results in the world of the Internet. (5) We use r = 5 and l = 2K = 2K, as these settings usually show the best results in Bahi et al."}, {"heading": "4 CONCLUSION AND FURTHER EXTENSION", "text": "In fact, most people are able to decide for themselves what they want and what they want."}], "references": [{"title": "Streaming k-means approximation", "author": ["Ailon", "Nir", "Jaiswal", "Ragesh", "Monteleoni", "Claire"], "venue": "In Advances in Neural Information Processing Systems, pp", "citeRegEx": "Ailon et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Ailon et al\\.", "year": 2009}, {"title": "Np-hardness of euclidean sum-of-squares clustering", "author": ["Aloise", "Daniel", "Deshpande", "Amit", "Hansen", "Pierre", "Popat", "Preyas"], "venue": "Machine Learning,", "citeRegEx": "Aloise et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Aloise et al\\.", "year": 2009}, {"title": "A spectral algorithm for latent dirichlet allocation", "author": ["Anandkumar", "Anima", "Liu", "Yi-kai", "Hsu", "Daniel J", "Foster", "Dean P", "Kakade", "Sham M"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Anandkumar et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Anandkumar et al\\.", "year": 2012}, {"title": "Tensor decompositions for learning latent variable models", "author": ["Anandkumar", "Animashree", "Ge", "Rong", "Hsu", "Daniel", "Kakade", "Sham M", "Telgarsky", "Matus"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "Anandkumar et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Anandkumar et al\\.", "year": 2014}, {"title": "k-means++: The advantages of careful seeding", "author": ["Arthur", "David", "Vassilvitskii", "Sergei"], "venue": "In Proceedings of the eighteenth annual ACM-SIAM symposium on Discrete algorithms,", "citeRegEx": "Arthur et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Arthur et al\\.", "year": 2007}, {"title": "Scalable k-means++", "author": ["Bahmani", "Bahman", "Moseley", "Benjamin", "Vattani", "Andrea", "Kumar", "Ravi", "Vassilvitskii", "Sergei"], "venue": "Proceedings of the VLDB Endowment,", "citeRegEx": "Bahmani et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Bahmani et al\\.", "year": 2012}, {"title": "An analysis of single-layer networks in unsupervised feature learning", "author": ["Coates", "Adam", "Ng", "Andrew Y", "Lee", "Honglak"], "venue": "In International conference on artificial intelligence and statistics,", "citeRegEx": "Coates et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Coates et al\\.", "year": 2011}, {"title": "Spectral learning of latentvariable pcfgs: Algorithms and sample complexity", "author": ["Cohen", "Shay B", "Stratos", "Karl", "Collins", "Michael", "Foster", "Dean P", "Ungar", "Lyle"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "Cohen et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Cohen et al\\.", "year": 2014}, {"title": "Spectral dependency parsing with latent variables. In Proceedings of the 2012 joint conference on empirical methods in natural language processing and computational natural language learning, pp. 205\u2013213", "author": ["Dhillon", "Paramveer S", "Rodu", "Jordan", "Collins", "Michael", "Foster", "Dean P", "Ungar", "Lyle H"], "venue": "Association for Computational Linguistics,", "citeRegEx": "Dhillon et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Dhillon et al\\.", "year": 2012}, {"title": "Learning mixtures of spherical gaussians: Moment methods and spectral decompositions", "author": ["Hsu", "Daniel", "Kakade", "Sham M"], "venue": "In Proceedings of the 4th Conference on Innovations in Theoretical Computer Science,", "citeRegEx": "Hsu et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Hsu et al\\.", "year": 2013}, {"title": "A spectral algorithm for learning hidden markov models", "author": ["Hsu", "Daniel", "Kakade", "Sham M", "Zhang", "Tong"], "venue": "Journal of Computer and System Sciences,", "citeRegEx": "Hsu et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Hsu et al\\.", "year": 2012}, {"title": "Shifted power method for computing tensor eigenpairs", "author": ["Kolda", "Tamara G", "Mayo", "Jackson R"], "venue": "SIAM Journal on Matrix Analysis and Applications,", "citeRegEx": "Kolda et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Kolda et al\\.", "year": 2011}, {"title": "Revisiting k-means: New algorithms via bayesian nonparametrics", "author": ["Kulis", "Brian", "Jordan", "Michael I"], "venue": "arXiv preprint arXiv:1111.0352,", "citeRegEx": "Kulis et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Kulis et al\\.", "year": 2011}, {"title": "Transition-aware human activity recognition using smartphones", "author": ["Reyes-Ortiz", "Jorge-L", "Oneto", "Luca", "Sam\u00e0", "Albert", "Parra", "Xavier", "Anguita", "Davide"], "venue": null, "citeRegEx": "Reyes.Ortiz et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Reyes.Ortiz et al\\.", "year": 2015}, {"title": "Fast and accurate k-means for large datasets. In Advances in neural information processing", "author": ["Shindler", "Michael", "Wong", "Alex", "Meyerson", "Adam W"], "venue": null, "citeRegEx": "Shindler et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Shindler et al\\.", "year": 2011}, {"title": "Hilbert space embeddings of hidden markov models", "author": ["Song", "Le", "Boots", "Byron", "Siddiqi", "Sajid M", "Gordon", "Geoffrey J", "Smola", "Alex J"], "venue": "In Proceedings of the 27th international conference on machine learning", "citeRegEx": "Song et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Song et al\\.", "year": 2010}, {"title": "Spectral experts for estimating mixtures of linear regressions", "author": ["Tejasvi Chaganty", "Arun", "Liang", "Percy"], "venue": "In Proceedings of The 30th International Conference on Machine Learning,", "citeRegEx": "Chaganty et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Chaganty et al\\.", "year": 2013}], "referenceMentions": [{"referenceID": 1, "context": "In recent times it has also been used for feature extraction purpose, which is further used for classification using Neural Networks in Coates et al. (2011). K-means is an NP hard problem even for K = 2 (Aloise et al.", "startOffset": 136, "endOffset": 157}, {"referenceID": 0, "context": "K-means is an NP hard problem even for K = 2 (Aloise et al. (2009)), and only heuristic solutions exist.", "startOffset": 46, "endOffset": 67}, {"referenceID": 0, "context": "K-means is an NP hard problem even for K = 2 (Aloise et al. (2009)), and only heuristic solutions exist. Perhaps the most common of such heuristics is the Lloyd\u2019s algorithm, which uses an EM style approach; it first assigns the points to different clusters according to their distance from the means of the clusters, and then updates the cluster means by averaging out the points in each cluster. The algorithm uses a random initialization, and comes with no guarantee to reach the global minima. The K-means++ in Arthur & Vassilvitskii (2007) addresses the problem by using an initial seeding of the means, and converges within O (logK) of the global optima.", "startOffset": 46, "endOffset": 544}, {"referenceID": 0, "context": "K-means is an NP hard problem even for K = 2 (Aloise et al. (2009)), and only heuristic solutions exist. Perhaps the most common of such heuristics is the Lloyd\u2019s algorithm, which uses an EM style approach; it first assigns the points to different clusters according to their distance from the means of the clusters, and then updates the cluster means by averaging out the points in each cluster. The algorithm uses a random initialization, and comes with no guarantee to reach the global minima. The K-means++ in Arthur & Vassilvitskii (2007) addresses the problem by using an initial seeding of the means, and converges within O (logK) of the global optima. However, for most practical purposes, a bound of O (logK) might be too high. Moreover, the algorithm uses K passes through the entire dataset to seed the means; this makes it very expensive for large amount of data. K-means|| in Bahmani et al. (2012) suggests a method to oversample the means during every pass to reduce the number of passes in the seeding phase.", "startOffset": 46, "endOffset": 911}, {"referenceID": 0, "context": "There are other variants of K-means algorithm, such as Ailon et al. (2009) and Shindler et al.", "startOffset": 55, "endOffset": 75}, {"referenceID": 0, "context": "There are other variants of K-means algorithm, such as Ailon et al. (2009) and Shindler et al. (2011). Most of these algorithms build on K-means++ and aim to reduce the computational burden, rather than improving accuracy.", "startOffset": 55, "endOffset": 102}, {"referenceID": 0, "context": "There are other variants of K-means algorithm, such as Ailon et al. (2009) and Shindler et al. (2011). Most of these algorithms build on K-means++ and aim to reduce the computational burden, rather than improving accuracy. Hence, we limit our discussion on K-means++ and K-means||. There have been recent developments on clustering algorithms based on Method of Moments (MoM), also referred to as Spectral Methods in the literature. Unlike traditional clustering algorithms which rely on EM or similar algorithms to maximize the likelihood of the data, MoM tries to learn the parameters from the higher order moments of the data. The method can be proven to be globally convergent using PAC style proof (see Anandkumar et al. (2014)), and has been successfully applied for Hidden Markov Model in Hsu et al.", "startOffset": 55, "endOffset": 733}, {"referenceID": 0, "context": "There are other variants of K-means algorithm, such as Ailon et al. (2009) and Shindler et al. (2011). Most of these algorithms build on K-means++ and aim to reduce the computational burden, rather than improving accuracy. Hence, we limit our discussion on K-means++ and K-means||. There have been recent developments on clustering algorithms based on Method of Moments (MoM), also referred to as Spectral Methods in the literature. Unlike traditional clustering algorithms which rely on EM or similar algorithms to maximize the likelihood of the data, MoM tries to learn the parameters from the higher order moments of the data. The method can be proven to be globally convergent using PAC style proof (see Anandkumar et al. (2014)), and has been successfully applied for Hidden Markov Model in Hsu et al. (2012) and Song et al.", "startOffset": 55, "endOffset": 814}, {"referenceID": 0, "context": "There are other variants of K-means algorithm, such as Ailon et al. (2009) and Shindler et al. (2011). Most of these algorithms build on K-means++ and aim to reduce the computational burden, rather than improving accuracy. Hence, we limit our discussion on K-means++ and K-means||. There have been recent developments on clustering algorithms based on Method of Moments (MoM), also referred to as Spectral Methods in the literature. Unlike traditional clustering algorithms which rely on EM or similar algorithms to maximize the likelihood of the data, MoM tries to learn the parameters from the higher order moments of the data. The method can be proven to be globally convergent using PAC style proof (see Anandkumar et al. (2014)), and has been successfully applied for Hidden Markov Model in Hsu et al. (2012) and Song et al. (2010), for Topic", "startOffset": 55, "endOffset": 837}, {"referenceID": 2, "context": "Modeling in Anandkumar et al. (2012), for various Natural Language Processing applications in Cohen et al.", "startOffset": 12, "endOffset": 37}, {"referenceID": 2, "context": "Modeling in Anandkumar et al. (2012), for various Natural Language Processing applications in Cohen et al. (2014), Dhillon et al.", "startOffset": 12, "endOffset": 114}, {"referenceID": 2, "context": "Modeling in Anandkumar et al. (2012), for various Natural Language Processing applications in Cohen et al. (2014), Dhillon et al. (2012), for Mixtures of Gaussian in Hsu & Kakade (2013) and for Spectral Experts of Linear Regression in Tejasvi Chaganty & Liang (2013).", "startOffset": 12, "endOffset": 137}, {"referenceID": 2, "context": "Modeling in Anandkumar et al. (2012), for various Natural Language Processing applications in Cohen et al. (2014), Dhillon et al. (2012), for Mixtures of Gaussian in Hsu & Kakade (2013) and for Spectral Experts of Linear Regression in Tejasvi Chaganty & Liang (2013).", "startOffset": 12, "endOffset": 186}, {"referenceID": 2, "context": "Modeling in Anandkumar et al. (2012), for various Natural Language Processing applications in Cohen et al. (2014), Dhillon et al. (2012), for Mixtures of Gaussian in Hsu & Kakade (2013) and for Spectral Experts of Linear Regression in Tejasvi Chaganty & Liang (2013). Here we propose a probabilistic K-means algorithm based on Method of Moments.", "startOffset": 12, "endOffset": 267}, {"referenceID": 3, "context": "If the estimated value of M\u03033 from training sample, denoted as \u02c6\u0303 M3, varies from the actual M\u03033 of the population in a way such that \u2225\u2225\u2225 \u02c6\u0303 M3 \u2212 M\u03033\u2225\u2225\u2225 < for some > 0, then the complexity of the tensor factorization (Algorithm 4) is O(k(log(k) + loglog(1/ )), where \u03b4 is a small positive number, with 0 < \u03b4 1 (Please refer to (Anandkumar et al., 2014) for a detailed derivation).", "startOffset": 327, "endOffset": 352}, {"referenceID": 2, "context": "We adopt the tensor factorization from Anandkumar et al. (2014), which computes the eigen-vectors of the tensor M\u03033 through power iteration followed by deflation, as described as Algorithm 4.", "startOffset": 39, "endOffset": 64}, {"referenceID": 2, "context": "We adopt the tensor factorization from Anandkumar et al. (2014), which computes the eigen-vectors of the tensor M\u03033 through power iteration followed by deflation, as described as Algorithm 4. The term T (\u00b7, \u03b8, \u03b8) \u2208 R is the product of the tensor T \u2208 RK\u00d7K\u00d7K with the vector \u03b8 \u2208 R along any two axes. The space complexity of the overall algorithm is O(K + DK), whereas time complexity is O((N + D)K + NDK) excluding the tensor factorization step. Since the space complexity is independent of the number of instances in X , the algorithm has the capability to scale to a large amount of data. Method of Moments is proven to be globally convergent ( please see Anandkumar et al. (2014) for proof).", "startOffset": 39, "endOffset": 682}, {"referenceID": 5, "context": "For K-means||, we use r = 5 and l = 2K, since these settings usually give the best results as described in Bahmani et al. (2012). For each method of initialization, we run the K-means iterations until \u03c6t\u22121(C)\u2212 \u03c6(C) < 10\u22123\u03c6t\u22121(C), where \u03c6(C) is the cost defined in Equation 1 at iteration t.", "startOffset": 107, "endOffset": 129}, {"referenceID": 11, "context": "3 Human Activities and Postural Transitions (HAPT) Dataset: We perform our experiments on the training set of Smartphone-Based Recognition of Human Activities and Postural Transitions Dataset from UCI Repository used in Reyes-Ortiz et al. (2015). The training set consists of 7767 instances with each instance containing 561 attributes corresponding to 12 distinct activities and postures.", "startOffset": 220, "endOffset": 246}, {"referenceID": 2, "context": "Our model is very similar to the Latent Dirichlet Allocation using Method of Moments presented in Anandkumar et al. (2012), where the pair-wise and triple-wise probabilities replace the covariance and third order moment.", "startOffset": 98, "endOffset": 123}], "year": 2017, "abstractText": "K-means is one of the most widely used algorithms for clustering in Data Mining applications, which attempts to minimize the sum of square of Euclidean distance of the points in the clusters from the respective means of the clusters. The simplicity and scalability of K-means makes it very appealing. However, K-means suffers from local minima problem, and comes with no guarantee to converge to the optimal cost. K-means++ tries to address the problem by seeding the means using a distance based sampling scheme. However, seeding the means in K-means++ needsO (K) passes through the entire dataset, which could be very costly in large amount of dataset. Here we propose a method of seeding initial means based on higher order moments of the data, which takes O (1) passes through the entire dataset to extract the initial set of means. Our method yields competitive performance with respect to all the existing K-means algorithms, whilst avoiding the expensive mean selection steps of K-means++ and other heuristics. We demonstrate the performance of our algorithm in comparison with the existing algorithms on various benchmark datasets.", "creator": "LaTeX with hyperref package"}}}