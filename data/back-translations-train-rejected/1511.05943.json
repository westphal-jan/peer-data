{"id": "1511.05943", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "18-Nov-2015", "title": "Unitary-Group Invariant Kernels and Features from Transformed Unlabeled Data", "abstract": "The study of representations invariant to common transformations of the data is important to learning. Most techniques have focused on local approximate invariance implemented within expensive optimization frameworks lacking explicit theoretical guarantees. In this paper, we study kernels that are invariant to the unitary group while having theoretical guarantees in addressing practical issues such as (1) unavailability of transformed versions of labelled data and (2) not observing all transformations. We present a theoretically motivated alternate approach to the invariant kernel SVM. Unlike previous approaches to the invariant SVM, the proposed formulation solves both issues mentioned. We also present a kernel extension of a recent technique to extract linear unitary-group invariant features addressing both issues and extend some guarantees regarding invariance and stability. We present experiments on the UCI ML datasets to illustrate and validate our methods.", "histories": [["v1", "Wed, 18 Nov 2015 20:48:18 GMT  (51kb)", "http://arxiv.org/abs/1511.05943v1", "11 page main paper (including references), 2 page supplementary, for a total of 13 pages. Submitted for review at ICLR 2016"]], "COMMENTS": "11 page main paper (including references), 2 page supplementary, for a total of 13 pages. Submitted for review at ICLR 2016", "reviews": [], "SUBJECTS": "cs.LG cs.AI", "authors": ["dipan k pal", "marios savvides"], "accepted": false, "id": "1511.05943"}, "pdf": {"name": "1511.05943.pdf", "metadata": {"source": "CRF", "title": null, "authors": [], "emails": ["dipanp@andrew.cmu.edu", "msavvid@andrew.cmu.edu"], "sections": [{"heading": null, "text": "ar Xiv: 151 1,05 943v 1 [cs.L G] 18 Nov 201 5"}, {"heading": "1 INTRODUCTION", "text": "This year is the highest in the history of the country."}, {"heading": "2 GLOBALLY GROUP INVARIANT KERNELS: WHEN THE GROUP G IS EXPLICITLY KNOWN", "text": "The question that arises is whether it is a \"new\" or \"new\" or \"new\" system that creates a \"new\" or \"new\" world. \"The question about the\" new \"world arises\" new. \"The question about the\" new \"world arises\" new. \"The question about the\" new \"world arises\" new. \"The question about the\" new \"world arises\" new. \"The question about the\" new \"world arises\" new. \"The question about the\" new world \"arises\" new. \"The question about the\" new world \"arises\" new. \"The question about the\" new world \"is\" new. \"The question about the\" new world \"is\" new. \"The question about the\" new world \"is\" new. \"The question about the\" new world \"is\" new. \""}, {"heading": "2.1 GROUP ACTIONS RECIPROCATE IN A REPRODUCING KERNEL HILBERT SPACE", "text": "??????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????"}, {"heading": "2.2 INVARIANT NON-LINEAR SVM: AN ALTERNATE APPROACH THROUGH GROUP INTEGRATION", "text": "The decision-making function of SVMs can be described in the general form as follows: (x) = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = ="}, {"heading": "3 GLOBALLY GROUP INVARIANT KERNELS: WHEN ACTION OF GROUP G IS", "text": "Although the formulation addresses problem 2, it cannot address problem 1, i.e. the nuclear phenomenon (x, y) = < - G \u03c6 (gx) dgH, - G \u03c6 (gy) dgH, - G \u03c6 (gy) dgH > = < - HB (x), - HB (x), - HB (x), - HB (g), - G (g), - G (g), - G (g), - G (g), - G (g), - G (g), - G (g), - G (g), - G (g), - G (g), - (g), - G (g), - (g), - (g), - (g), - (G), - (G)."}, {"heading": "4 GLOBALLY GROUP INVARIANT KERNEL FEATURES FROM A SINGLE", "text": "So far, we have studied the properties of the proposed unified group invariant nuclei. We are now turning our attention to group invariant traits. Invariant nuclei are a form of invariant similarity and can be used to construct invariant traits. Anselmi et al. (2013) proposed linear invariant traits that enjoy traits such as global invariance and stability. We extend their method to the RKHS with unified nuclei and extend the invariance and stability properties. We will now briefly present their theory of invariance."}, {"heading": "4.1 THEORY OF LINEAR INVARIANT FEATURES", "text": "In fact, the measurements of distribution, through a limited number of one-dimensional projections {P < x, tk >} K = 1, we can be considered as a kind of distribution effect in the same class. (...) The measurements of distribution, through a limited number of one-dimensional projections {P < x, tk >} K = 1, can be used as similarity between two orbits. (...) Furthermore, the measurements are invariant. (...) The measurements of distribution, through a one-dimensional projection {P < x, tk >} K = 1, we can be used as comparative measurements between two orbits 3. (...) The measurements are invariant to the action of the one-dimensional group G. (...)"}, {"heading": "5 TOWARDS PARTIALLY GROUP INVARIANT KERNELS: WHEN THE GROUP G", "text": "In practice, this is the most likely case. Partial variance, however, can be achieved via the observed subset G0 by a local core characteristic, which can also be generalized to locally compact groups. Partial variance is most likely in practice if a subset G0 is observed by a local core characteristic, which can also be generalized to locally compact groups. Partial variant core characteristics are not recognizable in practice."}, {"heading": "6 EXPERIMENTAL VALIDATION", "text": "In fact it is the way in which the real world of the real world of the real world of the real world of the real world of the real world of the real world of the real world of the real world of the real world of the real world of the real world of the real world of the real world of the real world of the real world of the real world of the real world of the real world of the real world of the real world of the real world of the real world of the real world of the real world of the real world of the real world of the real world of the real world of the real world of the real world of the real world of the real world of the real world of the real world of the real world of the real world of the real world of the real world of the real world of the real world of the real world of the real world of the real world of the real world of the real world of the real world of the real world of the real world of the real world of the real world of the real world of the real world of the real world of the real world of the real world of the real world of the real world of the real world of the real world of the real world of the real world of the real world of the real world of the real world of the real world of the real world of the real world of the real world of the real world of the real world of the real world of the real world of the real world of the real world of the real world of the real world of the real world of the real world of the real world of the real world of the real world of the real world of the real world of the real world of the real world of the real world of the real world of the real world of the real world of the real world of the real world of the real world of the real world of the real world of the real world of the real world of the real world of the real world of the real world of the real world of the real world of the real world of the real world of the real world of the real world of the real world of the real world of the real world of the real world of the real world of the real world of the real world of the real world of the real world of the real world of the real world of the real world of the real world of the real world of the real world of the real world of the real world of the real world of the real world of the real"}, {"heading": "7 CONCLUSION", "text": "One of the main obstacles to the application of invariant kernel methods has been the computational effort involved in generating and processing additional transformed data. Furthermore, in many cases it is difficult to generate such samples because the transformation is unknown. However, in many cases it is easier to obtain transformed unlabeled samples (such as video sequences in vision).The invariant nuclei described in this paper can be used to solve such problems while theoretically guaranteeing invariability."}, {"heading": "8 SUPPLEMENTARY MATERIAL", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "8.1 PROOF OF LEMMA 2.2", "text": "Proof. We have g'ig g'ig g'g'g'g'g'g'g \"g'g\" g'g \"g'g\" g'dgDa the normalized hair measure is invariant, i.e. dg = dg. \"Intuitively g'simply rearranges the group integral based on elementary group properties."}, {"heading": "8.2 PROOF OF LEMMA 2.3", "text": "Proof: As I said, we have T = (HG g dg) T = HG gT dg = HG g \u2212 1 dg \u2212 1 = HG using the fact g HG \u21d2 g \u2212 1 HG and dg = HG \u2212 1."}, {"heading": "8.3 PROOF LEMMA 2.4", "text": "Evidence: Since the hair measure is normalized (Hg G dg = 1) and invariant, we also have for each \"G,\" \"G\" Rd, \"\" G, \"G\" < > \"G <\" G \">\" G < g < g \u2212 1\u043e \"> Dg \u2212 1 = <\" E, \"\" E \">\" G < > \"G < G\" > \"G < g < g \u2212 1\u043e\" > Dg \u2212 1 = < \"E\" > \">\" G < G \">\" G < G \">\" G < g \">"}, {"heading": "8.4 PROOF OF LEMMA 2.7", "text": "Proof: We have < p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p g g, p g g g, p g g g, p g g g, p g g g, p g g g, p, p g g g, p, p g g, p, p g g, p, p, p, p g, p g, p g, p g, p g, p, p g, p g, p g, p, p g, p g, p g, p g, p g, p g, p g, p g, p, p g, p g, p, p g, p g, p g, p g, p, p g, p, p g, p g, p, p g, p, p g, p g, p g, p g, p g, p g, p g, p g, p g, p g, p g, p g, p g g, p, p g g, p, p g, p g, p g, p g, p g, p g, p g, p g, p g, p g, p g, p, p g, p g, p, p g, p g, p g, p g, p g, p, p, p g, p g, p g, p g, p g, p g, p g, p g, p, p g, p g, p g, p, p, p g, p, p, p g, p g, p, p, p g, p"}, {"heading": "8.5 PROOF OF THEOREM 2.5", "text": "Proof: We have < \u03c6 (gx), \u03c6 (gy) > = < \u03c6 (x), \u03c6 (y) > = < gH\u03c6 (x), gH\u03c6 (y) > because the kernel k is uniform. At this point, we define gH\u03c6 (x) as the action of gH to \u03c6 (x). We find that the linearity of gH can be derived from the linearity of the internal product and its conservation under gH in H. Especially for an arbitrary vector p and a scalar vector H."}, {"heading": "8.6 PROOF OF THEOREM 2.6", "text": "Proof. Since \"HN\" is a perfect delimiter for \"HN (X),\" \"HN\" > 0, \"\" HN (xi) \"T (HN)\" xi \"and\" X. \"With Lemma 2.4 and Theorem 2.5 we have for each fixed g\" H \"GH, (HN (xi))) T (HN (xi)))) T (HN (HN))) T (HN)) T (HN) Therefore,\" HN \"is a perfect delimiter for\" HN (XG) \"with a margin of at least.\" It also implies that a delimiter with a maximum margin of \"HN (X) is also a delimiter with a maximum margin of\" XG. \""}, {"heading": "8.7 PROOF OF THEOREM 3.1", "text": "Proof. For each specified g \"H we find < H\u03c6 (x), H\u03c6 (y) > = < g\" H\u03c6 (x), H\u03c6 (y) > using Lemma 2.4. If we select g \"H as identity and replace the extension of\" H, \"\u03c6\" (x) = \"\u03c6\" (T) ux and \"\u03c6\" (y) = \"\u03c6\" (T) uy, we have the desired result."}, {"heading": "8.8 PROOF OF THEOREM 4.1", "text": "Proof: Since \u03b7n Lipschitz are continuous \u2012 n, we have for each k component of the signature \u2012 kn (x) \u2012 k (x) \u2012 k (x) \u2012 k (x) \u2012 k (x) \u2012 k (x) \u2012 2 RN \u2264 1 | G | 2 \u0445 n \u2211 g \u0445 GH L\u043e n | < p (x), gHorizon g > \u2212 < p (x), gH\u03c9 k > | 2 (7) \u2264 L2\u043c | H (9), where we use Cauchy-Schwartz, theorem 2.5 and the fact that for some tk \u2012 Rd, we | gH\u03c9 k | 2 (8) \u2264 N2L2\u03b7 | | | p (x) \u2212 p (x) \u2012 p (x) \u2012 p (x) \u2012 p (9), theorem 2.5 and the fact that for some tk \u2012 Rd, we | gHorizon k \u2012 k \u2012 Rd (2) \u2012 g (2k), x \u00b2 g (H), x \u00b2 g (H), x \u00b2 g (H)."}, {"heading": "8.9 PROOF OF THEOREM 5.1", "text": "Evidence. The proof is very similar to that of Theorem 6 in Anselmi et al. (2013), since Theorem 2.5 allows the group structure of GH in H. to be preserved."}, {"heading": "8.10 PROOF OF THEOREM 5.2", "text": "Proof: The first condition follows the exact analysis as in theorem 4.1. In order for the second condition to hold, we apply theorem 5.1 and follow an argument that is very similar to that of theorem 4.1."}], "references": [{"title": "Unsupervised learning of invariant representations in hierarchical architectures", "author": ["Anselmi", "Fabio", "Leibo", "Joel Z", "Rosasco", "Lorenzo", "Mutch", "Jim", "Tacchetti", "Andrea", "Poggio", "Tomaso"], "venue": "CoRR, abs/1311.4158,", "citeRegEx": "Anselmi et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Anselmi et al\\.", "year": 2013}, {"title": "Training invariant support vector machines", "author": ["Decoste", "Dennis", "Sch\u00f6lkopf", "Bernhard"], "venue": "Mach. Learn.,", "citeRegEx": "Decoste et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Decoste et al\\.", "year": 2002}, {"title": "Tangent distance kernels for support vector machines", "author": ["B. Haasdonk", "D. Keysers"], "venue": "In Pattern Recognition,", "citeRegEx": "Haasdonk and Keysers,? \\Q2002\\E", "shortCiteRegEx": "Haasdonk and Keysers", "year": 2002}, {"title": "Invariant kernel functions for pattern analysis and machine learning", "author": ["Haasdonk", "Bernard", "Burkhardt", "Hans"], "venue": "In Machine Learning,", "citeRegEx": "Haasdonk et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Haasdonk et al\\.", "year": 2007}, {"title": "Learning translation invariant recognition in a massively parallel networks", "author": ["Hinton", "Geoffrey E"], "venue": "In PARLE Parallel Architectures and Languages Europe,", "citeRegEx": "Hinton and E.,? \\Q1987\\E", "shortCiteRegEx": "Hinton and E.", "year": 1987}, {"title": "Incorporating prior knowledge in support vector machines for classification: A review", "author": ["Lauer", "Fabien", "Bloch", "G\u00e9rard"], "venue": null, "citeRegEx": "Lauer et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Lauer et al\\.", "year": 2008}, {"title": "Subtasks of unconstrained face recognition", "author": ["Leibo", "Joel Z", "Liao", "Qianli", "Poggio", "Tomaso"], "venue": "In International Joint Conference on Computer Vision, Imaging and Computer Graphics, VISIGRAPP,", "citeRegEx": "Leibo et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Leibo et al\\.", "year": 2014}, {"title": "Learning invariant representations and applications to face verification", "author": ["Q. Liao", "J.Z. Leibo", "T. Poggio"], "venue": "Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "Liao et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Liao et al\\.", "year": 2013}, {"title": "Training Invariant Support Vector Machines using Selective Sampling", "author": ["Loosli", "Ga\u00eblle", "Canu", "St\u00e9phane", "Bottou", "L\u00e9on"], "venue": "In Large Scale Kernel Machines,", "citeRegEx": "Loosli et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Loosli et al\\.", "year": 2007}, {"title": "Incorporating prior information in machine learning by creating virtual examples", "author": ["P. Niyogi", "F. Girosi", "T. Poggio"], "venue": "In Proceedings of the IEEE,", "citeRegEx": "Niyogi et al\\.,? \\Q1998\\E", "shortCiteRegEx": "Niyogi et al\\.", "year": 1998}, {"title": "Recognition and structure from one 2d model view: Observations on prototypes, object classes and symmetries", "author": ["T. Poggio", "T. Vetter"], "venue": "Laboratory, Massachusetts Institute of Technology,", "citeRegEx": "Poggio and Vetter,? \\Q1992\\E", "shortCiteRegEx": "Poggio and Vetter", "year": 1992}, {"title": "Group integration techniques in pattern analysis a kernel view", "author": ["Reisert", "Marco"], "venue": "PhD Thesis,", "citeRegEx": "Reisert and Marco.,? \\Q2008\\E", "shortCiteRegEx": "Reisert and Marco.", "year": 2008}, {"title": "Prior knowledge in support vector kernels", "author": ["B. Schlkopf", "P. Simard", "A. Smola", "V. Vapnik"], "venue": "Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "Schlkopf et al\\.,? \\Q1998\\E", "shortCiteRegEx": "Schlkopf et al\\.", "year": 1998}, {"title": "Incorporating invariances in support vector learning machines", "author": ["Schlkopf", "Bernhard", "Burges", "Chris", "Vapnik", "Vladimir"], "venue": null, "citeRegEx": "Schlkopf et al\\.,? \\Q1996\\E", "shortCiteRegEx": "Schlkopf et al\\.", "year": 1996}, {"title": "Learning with kernels: Support vector machines, regularization, optimization, and beyond", "author": ["Sch\u00f6lkopf", "Bernhard", "Smola", "Alexander J"], "venue": "MIT press,", "citeRegEx": "Sch\u00f6lkopf et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Sch\u00f6lkopf et al\\.", "year": 2002}, {"title": "Learning with transformation invariant kernels", "author": ["Walder", "Christian", "Chapelle", "Olivier"], "venue": "In Advances in Neural Information Processing Systems, pp", "citeRegEx": "Walder et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Walder et al\\.", "year": 2007}, {"title": "Learning with invariance via linear functionals on reproducing kernel hilbert space", "author": ["Zhang", "Xinhua", "Lee", "Wee Sun", "Teh", "Yee Whye"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Zhang et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Zhang et al\\.", "year": 2013}], "referenceMentions": [{"referenceID": 5, "context": "In fact, explicitly factoring them out leads to improvements in recognition performance as found in Leibo et al. (2014); Hinton (1987).", "startOffset": 100, "endOffset": 120}, {"referenceID": 5, "context": "In fact, explicitly factoring them out leads to improvements in recognition performance as found in Leibo et al. (2014); Hinton (1987). To this end, the study of invariant features is important.", "startOffset": 100, "endOffset": 135}, {"referenceID": 0, "context": "Anselmi et al. (2013) showed that features that are explicitly invariant to intra-class transformations allow the sample complexity of the recognition problem to be reduced.", "startOffset": 0, "endOffset": 22}, {"referenceID": 0, "context": "Anselmi et al. (2013) showed that features that are explicitly invariant to intra-class transformations allow the sample complexity of the recognition problem to be reduced. Prior Art: Invariant Kernels. Kernel methods in machine learning have long been studied to considerable depth. Nonetheless, the study of invariant kernels and techniques to extract invariant features has received less attention. An invariant kernel allows the kernel product to remain invariant under transformations of the inputs. There has been some work on incorporating invariances into popular machinery such as the SVM in Lauer & Bloch (2008). Most instances of incorporating invariances focused on local invariances through regularization and optimization such as Schlkopf et al.", "startOffset": 0, "endOffset": 623}, {"referenceID": 0, "context": "Anselmi et al. (2013) showed that features that are explicitly invariant to intra-class transformations allow the sample complexity of the recognition problem to be reduced. Prior Art: Invariant Kernels. Kernel methods in machine learning have long been studied to considerable depth. Nonetheless, the study of invariant kernels and techniques to extract invariant features has received less attention. An invariant kernel allows the kernel product to remain invariant under transformations of the inputs. There has been some work on incorporating invariances into popular machinery such as the SVM in Lauer & Bloch (2008). Most instances of incorporating invariances focused on local invariances through regularization and optimization such as Schlkopf et al. (1996; 1998); Decoste & Sch\u00f6lkopf (2002); Zhang et al.", "startOffset": 0, "endOffset": 802}, {"referenceID": 0, "context": "Anselmi et al. (2013) showed that features that are explicitly invariant to intra-class transformations allow the sample complexity of the recognition problem to be reduced. Prior Art: Invariant Kernels. Kernel methods in machine learning have long been studied to considerable depth. Nonetheless, the study of invariant kernels and techniques to extract invariant features has received less attention. An invariant kernel allows the kernel product to remain invariant under transformations of the inputs. There has been some work on incorporating invariances into popular machinery such as the SVM in Lauer & Bloch (2008). Most instances of incorporating invariances focused on local invariances through regularization and optimization such as Schlkopf et al. (1996; 1998); Decoste & Sch\u00f6lkopf (2002); Zhang et al. (2013). Some other techniques were jittering kernels (Sch\u00f6lkopf & Smola (2002); Decoste & Sch\u00f6lkopf (2002)) and tangent-distance kernels (Haasdonk & Keysers (2002)), both of which sacrificed the positive semidefinite property of its kernels and were computationally expensive.", "startOffset": 0, "endOffset": 823}, {"referenceID": 0, "context": "Anselmi et al. (2013) showed that features that are explicitly invariant to intra-class transformations allow the sample complexity of the recognition problem to be reduced. Prior Art: Invariant Kernels. Kernel methods in machine learning have long been studied to considerable depth. Nonetheless, the study of invariant kernels and techniques to extract invariant features has received less attention. An invariant kernel allows the kernel product to remain invariant under transformations of the inputs. There has been some work on incorporating invariances into popular machinery such as the SVM in Lauer & Bloch (2008). Most instances of incorporating invariances focused on local invariances through regularization and optimization such as Schlkopf et al. (1996; 1998); Decoste & Sch\u00f6lkopf (2002); Zhang et al. (2013). Some other techniques were jittering kernels (Sch\u00f6lkopf & Smola (2002); Decoste & Sch\u00f6lkopf (2002)) and tangent-distance kernels (Haasdonk & Keysers (2002)), both of which sacrificed the positive semidefinite property of its kernels and were computationally expensive.", "startOffset": 0, "endOffset": 895}, {"referenceID": 0, "context": "Anselmi et al. (2013) showed that features that are explicitly invariant to intra-class transformations allow the sample complexity of the recognition problem to be reduced. Prior Art: Invariant Kernels. Kernel methods in machine learning have long been studied to considerable depth. Nonetheless, the study of invariant kernels and techniques to extract invariant features has received less attention. An invariant kernel allows the kernel product to remain invariant under transformations of the inputs. There has been some work on incorporating invariances into popular machinery such as the SVM in Lauer & Bloch (2008). Most instances of incorporating invariances focused on local invariances through regularization and optimization such as Schlkopf et al. (1996; 1998); Decoste & Sch\u00f6lkopf (2002); Zhang et al. (2013). Some other techniques were jittering kernels (Sch\u00f6lkopf & Smola (2002); Decoste & Sch\u00f6lkopf (2002)) and tangent-distance kernels (Haasdonk & Keysers (2002)), both of which sacrificed the positive semidefinite property of its kernels and were computationally expensive.", "startOffset": 0, "endOffset": 923}, {"referenceID": 0, "context": "Anselmi et al. (2013) showed that features that are explicitly invariant to intra-class transformations allow the sample complexity of the recognition problem to be reduced. Prior Art: Invariant Kernels. Kernel methods in machine learning have long been studied to considerable depth. Nonetheless, the study of invariant kernels and techniques to extract invariant features has received less attention. An invariant kernel allows the kernel product to remain invariant under transformations of the inputs. There has been some work on incorporating invariances into popular machinery such as the SVM in Lauer & Bloch (2008). Most instances of incorporating invariances focused on local invariances through regularization and optimization such as Schlkopf et al. (1996; 1998); Decoste & Sch\u00f6lkopf (2002); Zhang et al. (2013). Some other techniques were jittering kernels (Sch\u00f6lkopf & Smola (2002); Decoste & Sch\u00f6lkopf (2002)) and tangent-distance kernels (Haasdonk & Keysers (2002)), both of which sacrificed the positive semidefinite property of its kernels and were computationally expensive.", "startOffset": 0, "endOffset": 980}, {"referenceID": 0, "context": "Anselmi et al. (2013) showed that features that are explicitly invariant to intra-class transformations allow the sample complexity of the recognition problem to be reduced. Prior Art: Invariant Kernels. Kernel methods in machine learning have long been studied to considerable depth. Nonetheless, the study of invariant kernels and techniques to extract invariant features has received less attention. An invariant kernel allows the kernel product to remain invariant under transformations of the inputs. There has been some work on incorporating invariances into popular machinery such as the SVM in Lauer & Bloch (2008). Most instances of incorporating invariances focused on local invariances through regularization and optimization such as Schlkopf et al. (1996; 1998); Decoste & Sch\u00f6lkopf (2002); Zhang et al. (2013). Some other techniques were jittering kernels (Sch\u00f6lkopf & Smola (2002); Decoste & Sch\u00f6lkopf (2002)) and tangent-distance kernels (Haasdonk & Keysers (2002)), both of which sacrificed the positive semidefinite property of its kernels and were computationally expensive. Haasdonk & Burkhardt (2007) had first used group integration to arrive at invariant kernels, however, their approach does not address two important problems that arise in practice (group observation through unlabelled samples and partially observed groups).", "startOffset": 0, "endOffset": 1121}, {"referenceID": 0, "context": "Anselmi et al. (2013) showed that features that are explicitly invariant to intra-class transformations allow the sample complexity of the recognition problem to be reduced. Prior Art: Invariant Kernels. Kernel methods in machine learning have long been studied to considerable depth. Nonetheless, the study of invariant kernels and techniques to extract invariant features has received less attention. An invariant kernel allows the kernel product to remain invariant under transformations of the inputs. There has been some work on incorporating invariances into popular machinery such as the SVM in Lauer & Bloch (2008). Most instances of incorporating invariances focused on local invariances through regularization and optimization such as Schlkopf et al. (1996; 1998); Decoste & Sch\u00f6lkopf (2002); Zhang et al. (2013). Some other techniques were jittering kernels (Sch\u00f6lkopf & Smola (2002); Decoste & Sch\u00f6lkopf (2002)) and tangent-distance kernels (Haasdonk & Keysers (2002)), both of which sacrificed the positive semidefinite property of its kernels and were computationally expensive. Haasdonk & Burkhardt (2007) had first used group integration to arrive at invariant kernels, however, their approach does not address two important problems that arise in practice (group observation through unlabelled samples and partially observed groups). We will shortly state these problems more concretely and will show that the invariant kernels proposed do in fact solve both problems. Prior Art: Invariance through dataset augmentation. Many approaches in the past have enforced invariance through generating transformed training samples in some form such as Poggio & Vetter (1992); Sch\u00f6lkopf & Smola (2002); Schlkopf et al.", "startOffset": 0, "endOffset": 1683}, {"referenceID": 0, "context": "Anselmi et al. (2013) showed that features that are explicitly invariant to intra-class transformations allow the sample complexity of the recognition problem to be reduced. Prior Art: Invariant Kernels. Kernel methods in machine learning have long been studied to considerable depth. Nonetheless, the study of invariant kernels and techniques to extract invariant features has received less attention. An invariant kernel allows the kernel product to remain invariant under transformations of the inputs. There has been some work on incorporating invariances into popular machinery such as the SVM in Lauer & Bloch (2008). Most instances of incorporating invariances focused on local invariances through regularization and optimization such as Schlkopf et al. (1996; 1998); Decoste & Sch\u00f6lkopf (2002); Zhang et al. (2013). Some other techniques were jittering kernels (Sch\u00f6lkopf & Smola (2002); Decoste & Sch\u00f6lkopf (2002)) and tangent-distance kernels (Haasdonk & Keysers (2002)), both of which sacrificed the positive semidefinite property of its kernels and were computationally expensive. Haasdonk & Burkhardt (2007) had first used group integration to arrive at invariant kernels, however, their approach does not address two important problems that arise in practice (group observation through unlabelled samples and partially observed groups). We will shortly state these problems more concretely and will show that the invariant kernels proposed do in fact solve both problems. Prior Art: Invariance through dataset augmentation. Many approaches in the past have enforced invariance through generating transformed training samples in some form such as Poggio & Vetter (1992); Sch\u00f6lkopf & Smola (2002); Schlkopf et al.", "startOffset": 0, "endOffset": 1709}, {"referenceID": 0, "context": "Anselmi et al. (2013) showed that features that are explicitly invariant to intra-class transformations allow the sample complexity of the recognition problem to be reduced. Prior Art: Invariant Kernels. Kernel methods in machine learning have long been studied to considerable depth. Nonetheless, the study of invariant kernels and techniques to extract invariant features has received less attention. An invariant kernel allows the kernel product to remain invariant under transformations of the inputs. There has been some work on incorporating invariances into popular machinery such as the SVM in Lauer & Bloch (2008). Most instances of incorporating invariances focused on local invariances through regularization and optimization such as Schlkopf et al. (1996; 1998); Decoste & Sch\u00f6lkopf (2002); Zhang et al. (2013). Some other techniques were jittering kernels (Sch\u00f6lkopf & Smola (2002); Decoste & Sch\u00f6lkopf (2002)) and tangent-distance kernels (Haasdonk & Keysers (2002)), both of which sacrificed the positive semidefinite property of its kernels and were computationally expensive. Haasdonk & Burkhardt (2007) had first used group integration to arrive at invariant kernels, however, their approach does not address two important problems that arise in practice (group observation through unlabelled samples and partially observed groups). We will shortly state these problems more concretely and will show that the invariant kernels proposed do in fact solve both problems. Prior Art: Invariance through dataset augmentation. Many approaches in the past have enforced invariance through generating transformed training samples in some form such as Poggio & Vetter (1992); Sch\u00f6lkopf & Smola (2002); Schlkopf et al. (1998); Niyogi et al.", "startOffset": 0, "endOffset": 1733}, {"referenceID": 0, "context": "Anselmi et al. (2013) showed that features that are explicitly invariant to intra-class transformations allow the sample complexity of the recognition problem to be reduced. Prior Art: Invariant Kernels. Kernel methods in machine learning have long been studied to considerable depth. Nonetheless, the study of invariant kernels and techniques to extract invariant features has received less attention. An invariant kernel allows the kernel product to remain invariant under transformations of the inputs. There has been some work on incorporating invariances into popular machinery such as the SVM in Lauer & Bloch (2008). Most instances of incorporating invariances focused on local invariances through regularization and optimization such as Schlkopf et al. (1996; 1998); Decoste & Sch\u00f6lkopf (2002); Zhang et al. (2013). Some other techniques were jittering kernels (Sch\u00f6lkopf & Smola (2002); Decoste & Sch\u00f6lkopf (2002)) and tangent-distance kernels (Haasdonk & Keysers (2002)), both of which sacrificed the positive semidefinite property of its kernels and were computationally expensive. Haasdonk & Burkhardt (2007) had first used group integration to arrive at invariant kernels, however, their approach does not address two important problems that arise in practice (group observation through unlabelled samples and partially observed groups). We will shortly state these problems more concretely and will show that the invariant kernels proposed do in fact solve both problems. Prior Art: Invariance through dataset augmentation. Many approaches in the past have enforced invariance through generating transformed training samples in some form such as Poggio & Vetter (1992); Sch\u00f6lkopf & Smola (2002); Schlkopf et al. (1998); Niyogi et al. (1998); Reisert (2008); Haasdonk & Burkhardt (2007).", "startOffset": 0, "endOffset": 1755}, {"referenceID": 0, "context": "Anselmi et al. (2013) showed that features that are explicitly invariant to intra-class transformations allow the sample complexity of the recognition problem to be reduced. Prior Art: Invariant Kernels. Kernel methods in machine learning have long been studied to considerable depth. Nonetheless, the study of invariant kernels and techniques to extract invariant features has received less attention. An invariant kernel allows the kernel product to remain invariant under transformations of the inputs. There has been some work on incorporating invariances into popular machinery such as the SVM in Lauer & Bloch (2008). Most instances of incorporating invariances focused on local invariances through regularization and optimization such as Schlkopf et al. (1996; 1998); Decoste & Sch\u00f6lkopf (2002); Zhang et al. (2013). Some other techniques were jittering kernels (Sch\u00f6lkopf & Smola (2002); Decoste & Sch\u00f6lkopf (2002)) and tangent-distance kernels (Haasdonk & Keysers (2002)), both of which sacrificed the positive semidefinite property of its kernels and were computationally expensive. Haasdonk & Burkhardt (2007) had first used group integration to arrive at invariant kernels, however, their approach does not address two important problems that arise in practice (group observation through unlabelled samples and partially observed groups). We will shortly state these problems more concretely and will show that the invariant kernels proposed do in fact solve both problems. Prior Art: Invariance through dataset augmentation. Many approaches in the past have enforced invariance through generating transformed training samples in some form such as Poggio & Vetter (1992); Sch\u00f6lkopf & Smola (2002); Schlkopf et al. (1998); Niyogi et al. (1998); Reisert (2008); Haasdonk & Burkhardt (2007).", "startOffset": 0, "endOffset": 1771}, {"referenceID": 0, "context": "Anselmi et al. (2013) showed that features that are explicitly invariant to intra-class transformations allow the sample complexity of the recognition problem to be reduced. Prior Art: Invariant Kernels. Kernel methods in machine learning have long been studied to considerable depth. Nonetheless, the study of invariant kernels and techniques to extract invariant features has received less attention. An invariant kernel allows the kernel product to remain invariant under transformations of the inputs. There has been some work on incorporating invariances into popular machinery such as the SVM in Lauer & Bloch (2008). Most instances of incorporating invariances focused on local invariances through regularization and optimization such as Schlkopf et al. (1996; 1998); Decoste & Sch\u00f6lkopf (2002); Zhang et al. (2013). Some other techniques were jittering kernels (Sch\u00f6lkopf & Smola (2002); Decoste & Sch\u00f6lkopf (2002)) and tangent-distance kernels (Haasdonk & Keysers (2002)), both of which sacrificed the positive semidefinite property of its kernels and were computationally expensive. Haasdonk & Burkhardt (2007) had first used group integration to arrive at invariant kernels, however, their approach does not address two important problems that arise in practice (group observation through unlabelled samples and partially observed groups). We will shortly state these problems more concretely and will show that the invariant kernels proposed do in fact solve both problems. Prior Art: Invariance through dataset augmentation. Many approaches in the past have enforced invariance through generating transformed training samples in some form such as Poggio & Vetter (1992); Sch\u00f6lkopf & Smola (2002); Schlkopf et al. (1998); Niyogi et al. (1998); Reisert (2008); Haasdonk & Burkhardt (2007). This assumes that one has knowledge about the transformation.", "startOffset": 0, "endOffset": 1800}, {"referenceID": 9, "context": "most popular method for incorporating invariances in SVMs is the virtual support method (VSV) in Schlkopf et al. (1996), which used sequential runs of SVMs in order to find and augment the support vectors with transformed versions of themselves.", "startOffset": 97, "endOffset": 120}, {"referenceID": 6, "context": "Loosli et al. (2007) proposed a similar algorithm to generate and prune out examples.", "startOffset": 0, "endOffset": 21}, {"referenceID": 0, "context": "Recently, Anselmi et al. (2013) proposed linear groupinvariant features as an explanation for multiple characteristics of the visual cortex.", "startOffset": 10, "endOffset": 32}, {"referenceID": 0, "context": "Recently, Anselmi et al. (2013) proposed linear groupinvariant features as an explanation for multiple characteristics of the visual cortex. They achieve invariance in a slightly more general way than group integration, utilizing measures of the distribution characterizing an orbit of a sample under the action group. We extend the method to the RKHS using unitary kernels and extend some properties regarding invariance and stability. We also show that the extension can solve both motivating problems (Problem 1 and Problem 2). This leads to a practical way of extracting non-linear invariant features with theoretical guarantees. Motivating Problems. We now state the two central problems that this paper tries to address through invariant kernels and features. A common practical problem that one faces utilizing previous methods involving generating transformed samples is the computational expense of generating and processing them (including virtual support vectors). Further, in many cases transformed labelled samples are unavailable.Two important problems that arise when practically applying invariant kernels and features are: Problem 1: (Group observed through unlabelled samples) The transformed versions of the training labelled data are not available i.e. one might only have access to transformed versions of unlabelled data outside of the training set (theoretically equivalent to having transformed versions of arbitrary vectors), e.g. only unlabelled transformed images are observed. Problem 2: (Partially observed group) Not all members of the group (symmetric set) of transformations G are observed i.e. the group is only partially observed through its actions, e.g. not all transformations of an image are observed. In many practical cases, partial invariance is in fact necessary, when a transformation from one class to another exists. Group Theory and Invariance. Towards this goal, the study of incorporating invariance through group integration seems useful. Group theory is an elegant way to model symmetry. Classical invariant theory provides group integration techniques to enforce invariance. Group integration can also be used to model mean pooling (and max pooling albeit in a different framework as proposed in Anselmi et al. (2013)), which is in implicit use in several areas of machine learning and computer vision.", "startOffset": 10, "endOffset": 2269}, {"referenceID": 0, "context": "Recently, Anselmi et al. (2013) proposed linear groupinvariant features as an explanation for multiple characteristics of the visual cortex. They achieve invariance in a slightly more general way than group integration, utilizing measures of the distribution characterizing an orbit of a sample under the action group. We extend the method to the RKHS using unitary kernels and extend some properties regarding invariance and stability. We also show that the extension can solve both motivating problems (Problem 1 and Problem 2). This leads to a practical way of extracting non-linear invariant features with theoretical guarantees. Motivating Problems. We now state the two central problems that this paper tries to address through invariant kernels and features. A common practical problem that one faces utilizing previous methods involving generating transformed samples is the computational expense of generating and processing them (including virtual support vectors). Further, in many cases transformed labelled samples are unavailable.Two important problems that arise when practically applying invariant kernels and features are: Problem 1: (Group observed through unlabelled samples) The transformed versions of the training labelled data are not available i.e. one might only have access to transformed versions of unlabelled data outside of the training set (theoretically equivalent to having transformed versions of arbitrary vectors), e.g. only unlabelled transformed images are observed. Problem 2: (Partially observed group) Not all members of the group (symmetric set) of transformations G are observed i.e. the group is only partially observed through its actions, e.g. not all transformations of an image are observed. In many practical cases, partial invariance is in fact necessary, when a transformation from one class to another exists. Group Theory and Invariance. Towards this goal, the study of incorporating invariance through group integration seems useful. Group theory is an elegant way to model symmetry. Classical invariant theory provides group integration techniques to enforce invariance. Group integration can also be used to model mean pooling (and max pooling albeit in a different framework as proposed in Anselmi et al. (2013)), which is in implicit use in several areas of machine learning and computer vision. The transformations, in this paper, are modelled as unitary and collectively form the unitarygroup G. Classes of learning problems, such as vision, often have transformations belonging to the unitary-group, that one would like to be invariant towards (such as translation and rotation). The results can also be extended to discrete groups. In practice however, Liao et al. (2013) found that invariance to much more general transformations not captured by this model can been achieved.", "startOffset": 10, "endOffset": 2734}, {"referenceID": 0, "context": "We propose kernel unitary-group invariant feature extraction techniques by extending a theory of linear group invariant features presented in Anselmi et al. (2013). We show that the kernel extension addresses both Problem 1 and Problem 2 and preserves properties such as global (and local) invariance and stability.", "startOffset": 142, "endOffset": 164}, {"referenceID": 0, "context": "In Section 4, we propose kernel unitary-group invariant feature extraction techniques by extending a linear invariant feature extraction method (Anselmi et al. (2013)) to the kernel domain.", "startOffset": 145, "endOffset": 167}, {"referenceID": 12, "context": "Relating the Virtual Support Vector Method (VSV): Consider the popular Virtual Support Vector Method (VSV) (Schlkopf et al. (1996)).", "startOffset": 108, "endOffset": 131}, {"referenceID": 0, "context": "Anselmi et al. (2013) proposed linear invariant features that enjoys properties such as global invariance and stability.", "startOffset": 0, "endOffset": 22}, {"referenceID": 0, "context": "In fact, Anselmi et al. (2013) show Px to be both invariant and unique, i.", "startOffset": 9, "endOffset": 31}, {"referenceID": 0, "context": "In fact, Anselmi et al. (2013) show Px to be both invariant and unique, i.e. x \u223c x\u2032 \u21d4 Ox \u223c Ox\u2032 \u21d4 Px \u223c Px\u2032 , where \u223c denotes membership in the same class. Thus, measures of the distribution, through a finite number of one-dimensional projections {P\u3008x,tk\u3009} K k=1, can be used as a similarity measure between two orbits 3. Further, the measures are invariant to the action of unitary group G. For unitary group G, normalized dot-products and an arbitrary template t, an empirical estimate of the 1-dimensional distribution of the projection onto template t can be expressed as \u03bcn(x) = \u222b G \u03b7n(\u3008x, gt \u3009)dg, where \u03b7n \u2200n \u2208 {1...N}, a non-linearity, can either estimate the n-th bin of the CDF or the n-th moment, the set of which together define P\u3008x,tk\u3009. In practice, Liao et al. (2013) found that a few or even one of these moments has been shown to be sufficiently invariant.", "startOffset": 9, "endOffset": 780}, {"referenceID": 0, "context": "Invariance can then be achieved using a form of Equation 7 in Anselmi et al. (2013).", "startOffset": 62, "endOffset": 84}, {"referenceID": 0, "context": "a form of a stability result in Anselmi et al. (2013) can be proved using a similar analysis.", "startOffset": 32, "endOffset": 54}, {"referenceID": 0, "context": "We extend the notion of partial invariance to the kernel features extracted similar to Equation 2 following the analysis of Anselmi et al. (2013). Partial invariance arises from partially observing the group G, i.", "startOffset": 124, "endOffset": 146}, {"referenceID": 0, "context": "Uniqueness: The analysis for uniqueness in Anselmi et al. (2013) can be applied to \u03a5\u0302(x) with no significant changes, since the group structure is preserved in H through Theorem 2.", "startOffset": 43, "endOffset": 65}, {"referenceID": 0, "context": "Uniqueness: The analysis for uniqueness in Anselmi et al. (2013) can be applied to \u03a5\u0302(x) with no significant changes, since the group structure is preserved in H through Theorem 2.5. In summary, any two partial orbits with a common point are identical. Invariance: Theorem 6 from Anselmi et al. (2013) can be applied in H with some modification.", "startOffset": 43, "endOffset": 302}], "year": 2015, "abstractText": "The study of representations invariant to common transformations of the data is important to learning. Most techniques have focused on local approximate invariance implemented within expensive optimization frameworks lacking explicit theoretical guarantees. In this paper, we study kernels that are invariant to the unitary group while having theoretical guarantees in addressing practical issues such as (1) unavailability of transformed versions of labelled data and (2) not observing all transformations. We present a theoretically motivated alternate approach to the invariant kernel SVM. Unlike previous approaches to the invariant SVM, the proposed formulation solves both issues mentioned. We also present a kernel extension of a recent technique to extract linear unitary-group invariant features addressing both issues and extend some guarantees regarding invariance and stability. We present experiments on the UCI ML datasets to illustrate and validate our methods.", "creator": "LaTeX with hyperref package"}}}