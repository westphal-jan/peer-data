{"id": "1511.06066", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "19-Nov-2015", "title": "Transfer Learning for Speech and Language Processing", "abstract": "Transfer learning is a vital technique that generalizes models trained for one setting or task to other settings or tasks. For example in speech recognition, an acoustic model trained for one language can be used to recognize speech in another language, with little or no re-training data. Transfer learning is closely related to multi-task learning (cross-lingual vs. multilingual), and is traditionally studied in the name of `model adaptation'. Recent advance in deep learning shows that transfer learning becomes much easier and more effective with high-level abstract features learned by deep models, and the `transfer' can be conducted not only between data distributions and data types, but also between model structures (e.g., shallow nets and deep nets) or even model types (e.g., Bayesian models and neural models). This review paper summarizes some recent prominent research towards this direction, particularly for speech and language processing. We also report some results from our group and highlight the potential of this very interesting research field.", "histories": [["v1", "Thu, 19 Nov 2015 05:54:45 GMT  (205kb)", "http://arxiv.org/abs/1511.06066v1", "13 pages, APSIPA 2015"]], "COMMENTS": "13 pages, APSIPA 2015", "reviews": [], "SUBJECTS": "cs.CL cs.LG", "authors": ["dong wang", "thomas fang zheng"], "accepted": false, "id": "1511.06066"}, "pdf": {"name": "1511.06066.pdf", "metadata": {"source": "CRF", "title": "Transfer Learning for Speech and Language Processing", "authors": ["Dong Wang", "Thomas Fang Zheng"], "emails": [], "sections": [{"heading": null, "text": "In fact, the fact is that most of them will be able to move to a different world in which they are able to live than in a world in which they are able, in which they are able to change the world."}, {"heading": "II. TRANSFER LEARNING: A QUICK REVIEW", "text": "This year, it has reached the point where it will be able to leave the country in which it is located, to leave it, and it is able to leave the country in order to leave it."}, {"heading": "III. TRANSFER LEARNING IN SPEECH PROCESSING", "text": "In fact, it is such that most of them will be able to move into another world, in which they are able to move, in which they are able to live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they, in which they live, in which they live, in which they, in which they live, in which they live, in which they live, in which they live, in which they live."}, {"heading": "IV. TRANSFER LEARNING IN LANGUAGE PROCESSING", "text": "In fact, it is such that most of them will be able to move into a different world, in which they are able to move, in which they are able to move, in which they move, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they, in which they live, in which they live, in which they, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they, in which they live, in which they, in which they live, in which they live, in which they, in which they live, in which they, in which they, in which they, in which they live, in which they, in which they, in which they live, in which they, in which they, in which they, in which they, in which they live, in which they, in which they, in which they, in which they, in which they, in which they, in which they are able to move, in which they are able to move, in which they are able to move."}, {"heading": "V. PERSPECTIVE AND CONCLUSIONS", "text": "This year it is more than ever before."}, {"heading": "ACKNOWLEDGEMENT", "text": "This research was supported by the National Science Foundation of China (NSFC) under Projects No. 61271389 and No. 61371136 and the National Basic Research Program (973 Program) of China under Funding Program No. 2013CB329302. It was also supported by the MESTDC PhD Foundation Project No. 20130002120011 as well as Sinovoice and Huilan Ltd. Thanks to Zhiyuan Tang for the careful reference collection."}], "references": [{"title": "Speech and language processing", "author": ["J.H. Martin", "D. Jurafsky"], "venue": "International Edition, 2000.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2000}, {"title": "Machine learning paradigms for speech recognition: An overview", "author": ["L. Deng", "X. Li"], "venue": "Audio, Speech, and Language Processing, IEEE Transactions on, vol. 21, no. 5, pp. 1060\u20131089, 2013.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2013}, {"title": "A survey on transfer learning", "author": ["S.J. Pan", "Q. Yang"], "venue": "Knowledge and Data Engineering, IEEE Transactions on, vol. 22, no. 10, pp. 1345\u2013 1359, 2010.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2010}, {"title": "Transfer learning for reinforcement learning domains: A survey", "author": ["M.E. Taylor", "P. Stone"], "venue": "The Journal of Machine Learning Research, vol. 10, pp. 1633\u20131685, 2009.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2009}, {"title": "Deep learning of representations for unsupervised and transfer learning", "author": ["Y. Bengio"], "venue": "ICML Unsupervised and Transfer Learning, 2012.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2012}, {"title": "Transfer learning using computational intelligence: A survey", "author": ["J. Lu", "V. Behbood", "P. Hao", "H. Zuo", "S. Xue", "G. Zhang"], "venue": "Knowledge-Based Systems, vol. 80, pp. 14\u201323, 2015.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2015}, {"title": "Learning to learn", "author": ["S. Thrun", "L. Pratt"], "venue": "Springer Science & Business Media,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2012}, {"title": "Distilling the knowledge in a neural network", "author": ["G.E. Hinton", "O. Vinyals", "J. Dean"], "venue": "NIPS 2014 Deep Learning Workshop, 2014.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2014}, {"title": "Multitask learning", "author": ["R. Caruana"], "venue": "Machine learning, vol. 28, no. 1, pp. 41\u201375, 1997.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 1997}, {"title": "Maximum a posteriori estimation for multivariate Gaussian mixture observations of Markov chains", "author": ["J.-L. Gauvain", "C.-H. Lee"], "venue": "IEEE Transactions on Speech and audio processing, vol. 2, no. 2, pp. 291\u2013 298, 1994.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 1994}, {"title": "Maximum likelihood linear regression for speaker adaptation of continuous density hidden Markov models", "author": ["C.J. Leggetter", "P. Woodland"], "venue": "Computer Speech & Language, vol. 9, no. 2, pp. 171\u2013185, 1995.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 1995}, {"title": "Incremental induction of decision trees", "author": ["P.E. Utgoff"], "venue": "Machine learning, vol. 4, no. 2, pp. 161\u2013186, 1989.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 1989}, {"title": "Combining labeled and unlabeled data with co-training", "author": ["A. Blum", "T. Mitchell"], "venue": "Proceedings of the eleventh annual conference on Computational learning theory. ACM, 1998, pp. 92\u2013100.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 1998}, {"title": "Heterogeneous domain adaptation using manifold alignment", "author": ["C. Wang", "S. Mahadevan"], "venue": "IJCAI Proceedings-International Joint Conference on Artificial Intelligence, vol. 22, no. 1, 2011, p. 1541.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2011}, {"title": "Heterogeneous transfer learning for image classification.", "author": ["Y. Zhu", "Y. Chen", "Z. Lu", "S.J. Pan", "G.-R. Xue", "Y. Yu", "Q. Yang"], "venue": null, "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2011}, {"title": "Transfer learning by structural analogy", "author": ["H.-Y. Wang", "Q. Yang"], "venue": "AAAI. Citeseer, 2011.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2011}, {"title": "Incremental learning of temporallycoherent Gaussian mixture models", "author": ["O. Arandjelovic", "R. Cipolla"], "venue": "Society of Manufacturing Engineers (SME) Technical Papers, pp. 1\u20131, 2006.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2006}, {"title": "Online learning of Gaussian mixture models-a two-level approach.", "author": ["A. Declercq", "J.H. Piater"], "venue": null, "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2008}, {"title": "Semi-supervised learning literature survey", "author": ["X. Zhu"], "venue": "Computer Sciences TRP 1530, University of Wisconsin C Madison, 2005.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2005}, {"title": "Domain adaptation via transfer component analysis", "author": ["S.J. Pan", "I.W. Tsang", "J.T. Kwok", "Q. Yang"], "venue": "Neural Networks, IEEE Transactions on, vol. 22, no. 2, pp. 199\u2013210, 2011.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2011}, {"title": "Self-taught learning: transfer learning from unlabeled data", "author": ["R. Raina", "A. Battle", "H. Lee", "B. Packer", "A.Y. Ng"], "venue": "Proceedings of the 24th international conference on Machine learning. ACM, 2007, pp. 759\u2013766.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2007}, {"title": "Cross-lingual adaptation using structural correspondence learning", "author": ["P. Prettenhofer", "B. Stein"], "venue": "ACM Transactions on Intelligent Systems and Technology (TIST), vol. 3, no. 1, p. 13, 2011.", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2011}, {"title": "Translated learning: Transfer learning across different feature spaces", "author": ["W. Dai", "Y. Chen", "G.-R. Xue", "Q. Yang", "Y. Yu"], "venue": "Advances in neural information processing systems, 2008, pp. 353\u2013360.", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2008}, {"title": "What you saw is not what you get: Domain adaptation using asymmetric kernel transforms", "author": ["B. Kulis", "K. Saenko", "T. Darrell"], "venue": "Computer Vision and Pattern Recognition (CVPR), 2011 IEEE Conference on. IEEE, 2011, pp. 1785\u20131792.", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2011}, {"title": "Heterogeneous transfer learning with RBMs.", "author": ["B. Wei", "C.J. Pal"], "venue": "in AAAI,", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2011}, {"title": "Transfer learning on heterogenous feature spaces via spectral transformation", "author": ["X. Shi", "Q. Liu", "W. Fan", "P.S. Yu", "R. Zhu"], "venue": "Data Mining (ICDM), 2010 IEEE 10th International Conference on. IEEE, 2010, pp. 1049\u20131054.", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2010}, {"title": "Learning with augmented features for heterogeneous domain adaptation", "author": ["L. Duan", "D. Xu", "I. Tsang"], "venue": "arXiv preprint arXiv:1206.4660, 2012.", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2012}, {"title": "Hybrid heterogeneous transfer learning through deep learning", "author": ["J.T. Zhou", "S.J. Pan", "I.W. Tsang", "Y. Yan"], "venue": "Twenty-Eighth AAAI Conference on Artificial Intelligence, 2014.", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2014}, {"title": "Structure-mapping: A theoretical framework for analogy", "author": ["D. Gentner"], "venue": "Cognitive science, vol. 7, no. 2, pp. 155\u2013170, 1983.", "citeRegEx": "31", "shortCiteRegEx": null, "year": 1983}, {"title": "Reasoning and learning by analogy: Introduction.", "author": ["D. Gentner", "K.J. Holyoak"], "venue": "American Psychologist,", "citeRegEx": "32", "shortCiteRegEx": "32", "year": 1997}, {"title": "Domain adaptation with structural correspondence learning", "author": ["J. Blitzer", "R. McDonald", "F. Pereira"], "venue": "Proceedings of the 2006 conference on empirical methods in natural language processing. Association for Computational Linguistics, 2006, pp. 120\u2013128.", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2006}, {"title": "Learning by analogy: Formulating and generalizing plans from past experience", "author": ["J.G. Carbonell"], "venue": null, "citeRegEx": "34", "shortCiteRegEx": "34", "year": 1983}, {"title": "Recurrent neural network training with dark knowledge transfer", "author": ["D. Wang", "C. Liu", "Z. Tang", "Z. Zhang", "M. Zhao"], "venue": "arXiv preprint arXiv:1505.04630, 2015.", "citeRegEx": "35", "shortCiteRegEx": null, "year": 2015}, {"title": "Knowledge transfer pretraining", "author": ["Z. Tang", "D. Wang", "Y. Pan", "Z. Zhang"], "venue": "arXiv preprint arXiv:1506.02256, 2015.", "citeRegEx": "36", "shortCiteRegEx": null, "year": 2015}, {"title": "A model of inductive bias learning", "author": ["J. Baxter"], "venue": "J. Artif. Intell. Res.(JAIR), vol. 12, pp. 149\u2013198, 2000.", "citeRegEx": "37", "shortCiteRegEx": null, "year": 2000}, {"title": "Estimating variable structure and dependence in multitask learning via gradients", "author": ["J. Guinney", "Q. Wu", "S. Mukherjee"], "venue": "Machine Learning, vol. 83, no. 3, pp. 265\u2013287, 2011.", "citeRegEx": "38", "shortCiteRegEx": null, "year": 2011}, {"title": "Exploiting unrelated tasks in multi-task learning", "author": ["B. Romera-Paredes", "A. Argyriou", "N. Berthouze", "M. Pontil"], "venue": "International Conference on Artificial Intelligence and Statistics, 2012, pp. 951\u2013959.", "citeRegEx": "39", "shortCiteRegEx": null, "year": 2012}, {"title": "Deep neural networks for acoustic modeling in speech recognition: The shared views of four research groups", "author": ["G. Hinton", "L. Deng", "D. Yu", "G.E. Dahl", "A.-r. Mohamed", "N. Jaitly", "A. Senior", "V. Vanhoucke", "P. Nguyen", "T.N. Sainath"], "venue": "IEEE Signal Processing Magazine, vol. 29, no. 6, pp. 82\u201397, 2012.", "citeRegEx": "40", "shortCiteRegEx": null, "year": 2012}, {"title": "Deep learning: Methods and applications", "author": ["L. Deng", "D. Yu"], "venue": "Foundations and Trends in Signal Processing, vol. 7, no. 3-4, pp. 197\u2013 387, 2013. [Online]. Available: http://dx.doi.org/10.1561/2000000039", "citeRegEx": "41", "shortCiteRegEx": null, "year": 2013}, {"title": "Deep learning for natural language processing and related applications (Tutorial at ICASSP)", "author": ["X. He", "J. Gao", "L. Deng"], "venue": "IEEE International Conference on Acoustics, Speech, and Signal Processing (ICASSP), 2014.", "citeRegEx": "42", "shortCiteRegEx": null, "year": 2014}, {"title": "Advances in natural language processing", "author": ["J. Hirschberg", "C.D. Manning"], "venue": "Science, vol. 349, no. 6245, pp. 261\u2013266, 2015.", "citeRegEx": "43", "shortCiteRegEx": null, "year": 2015}, {"title": "A fast learning algorithm for deep belief nets", "author": ["G.E. Hinton", "S. Osindero", "Y.-W. Teh"], "venue": "Neural computation, vol. 18, no. 7, pp. 1527\u2013 1554, 2006.", "citeRegEx": "44", "shortCiteRegEx": null, "year": 2006}, {"title": "Deep boltzmann machines", "author": ["R. Salakhutdinov", "G.E. Hinton"], "venue": "International Conference on Artificial Intelligence and Statistics, 2009, pp. 448\u2013455.", "citeRegEx": "45", "shortCiteRegEx": null, "year": 2009}, {"title": "Greedy layer-wise training of deep networks", "author": ["Y. Bengio", "P. Lamblin", "D. Popovici", "H. Larochelle"], "venue": "Advances in neural information processing systems, vol. 19, p. 153, 2007.", "citeRegEx": "46", "shortCiteRegEx": null, "year": 2007}, {"title": "Stacked denoising autoencoders: Learning useful representations in a deep network with a local denoising criterion", "author": ["P. Vincent", "H. Larochelle", "I. Lajoie", "Y. Bengio", "P.-A. Manzagol"], "venue": "The Journal of Machine Learning Research, vol. 11, pp. 3371\u20133408, 2010.", "citeRegEx": "47", "shortCiteRegEx": null, "year": 2010}, {"title": "Context-dependent pretrained deep neural networks for large-vocabulary speech recognition", "author": ["G.E. Dahl", "D. Yu", "L. Deng", "A. Acero"], "venue": "Audio, Speech, and Language Processing, IEEE Transactions on, vol. 20, no. 1, pp. 30\u201342, 2012.", "citeRegEx": "48", "shortCiteRegEx": null, "year": 2012}, {"title": "Speech recognition with deep recurrent neural networks", "author": ["A. Graves", "A.-R. Mohamed", "G. Hinton"], "venue": "Proceedings of IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2013, pp. 6645\u20136649.", "citeRegEx": "49", "shortCiteRegEx": null, "year": 2013}, {"title": "On the expressive power of deep architectures", "author": ["Y. Bengio", "O. Delalleau"], "venue": "Algorithmic Learning Theory. Springer, 2011, pp. 18\u201336.", "citeRegEx": "50", "shortCiteRegEx": null, "year": 2011}, {"title": "A unified architecture for natural language processing: Deep neural networks with multitask learning", "author": ["R. Collobert", "J. Weston"], "venue": "Proceedings of the 25th international conference on Machine learning. ACM, 2008, pp. 160\u2013167.", "citeRegEx": "51", "shortCiteRegEx": null, "year": 2008}, {"title": "Recent advances in deep learning for speech research at Microsoft", "author": ["L. Deng", "J. Li", "J.-T. Huang", "K. Yao", "D. Yu", "F. Seide", "M. Seltzer", "G. Zweig", "X. He", "J. Williams"], "venue": "Acoustics, Speech and Signal Processing (ICASSP), 2013 IEEE International Conference on. IEEE, 2013, pp. 8604\u20138608.", "citeRegEx": "52", "shortCiteRegEx": null, "year": 2013}, {"title": "Transfer learning techniques for deep neural nets", "author": ["S.M. Gutstein"], "venue": "The University of Texas at El Paso,", "citeRegEx": "53", "shortCiteRegEx": "53", "year": 2010}, {"title": "Multimodal deep learning", "author": ["J. Ngiam", "A. Khosla", "M. Kim", "J. Nam", "H. Lee", "A.Y. Ng"], "venue": "Proceedings of the 28th international conference on machine learning (ICML-11), 2011, pp. 689\u2013696.", "citeRegEx": "54", "shortCiteRegEx": null, "year": 2011}, {"title": "Deep Learning, 2015, book in preparation for MIT Press", "author": ["Y. Bengio", "I.J. Goodfellow", "A. Courville"], "venue": null, "citeRegEx": "55", "shortCiteRegEx": "55", "year": 2015}, {"title": "Reducing the dimensionality of data with neural networks", "author": ["G.E. Hinton", "R.R. Salakhutdinov"], "venue": "Science, vol. 313, no. 5786, pp. 504\u2013507, 2006.", "citeRegEx": "56", "shortCiteRegEx": null, "year": 2006}, {"title": "Domain adaptation for largescale sentiment classification: A deep learning approach", "author": ["X. Glorot", "A. Bordes", "Y. Bengio"], "venue": "Proceedings of the 28th International Conference on Machine Learning (ICML- 11), 2011, pp. 513\u2013520.", "citeRegEx": "57", "shortCiteRegEx": null, "year": 2011}, {"title": "Learning and transferring mid-level image representations using convolutional neural networks", "author": ["M. Oquab", "L. Bottou", "I. Laptev", "J. Sivic"], "venue": "Computer Vision and Pattern Recognition (CVPR), 2014 IEEE Conference on. IEEE, 2014, pp. 1717\u20131724.", "citeRegEx": "58", "shortCiteRegEx": null, "year": 2014}, {"title": "Deep model based transfer and multi-task learning for biological image analysis", "author": ["W. Zhang", "R. Li", "T. Zeng", "Q. Sun", "S. Kumar", "J. Ye", "S. Ji"], "venue": "Proceedings of the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining. ACM, 2015, pp. 1475\u20131484.", "citeRegEx": "59", "shortCiteRegEx": null, "year": 2015}, {"title": "One-shot learning of object categories", "author": ["L. Fei-Fei", "R. Fergus", "P. Perona"], "venue": "Pattern Analysis and Machine Intelligence, IEEE Transactions on, vol. 28, no. 4, pp. 594\u2013611, 2006.", "citeRegEx": "60", "shortCiteRegEx": null, "year": 2006}, {"title": "Zero-data learning of new tasks.", "author": ["H. Larochelle", "D. Erhan", "Y. Bengio"], "venue": "in AAAI,", "citeRegEx": "61", "shortCiteRegEx": "61", "year": 2008}, {"title": "Zero-shot learning through cross-modal transfer", "author": ["R. Socher", "M. Ganjoo", "C.D. Manning", "A. Ng"], "venue": "Advances in neural information processing systems, 2013, pp. 935\u2013943.", "citeRegEx": "62", "shortCiteRegEx": null, "year": 2013}, {"title": "Language-independent and languageadaptive acoustic modeling for speech recognition", "author": ["T. Schultz", "A. Waibel"], "venue": "Speech Communication, vol. 35, no. 1, pp. 31\u201351, 2001.", "citeRegEx": "63", "shortCiteRegEx": null, "year": 2001}, {"title": "Cross-language bootstrapping based on completely unsupervised training using multilingual A-stabil", "author": ["N.T. Vu", "F. Kraus", "T. Schultz"], "venue": "Acoustics, Speech and Signal Processing (ICASSP), 2011 IEEE International Conference on. IEEE, 2011, pp. 5000\u20135003.", "citeRegEx": "64", "shortCiteRegEx": null, "year": 2011}, {"title": "Unsupervised crosslingual knowledge transfer in DNN-based LVCSR", "author": ["P. Swietojanski", "A. Ghoshal", "S. Renals"], "venue": "Spoken Language Technology Workshop (SLT), 2012 IEEE. IEEE, 2012, pp. 246\u2013251.", "citeRegEx": "65", "shortCiteRegEx": null, "year": 2012}, {"title": "Cross-language knowledge transfer using multilingual deep neural network with shared hidden layers", "author": ["J.-T. Huang", "J. Li", "D. Yu", "L. Deng", "Y. Gong"], "venue": "Acoustics, Speech and Signal Processing (ICASSP), 2013 IEEE International Conference on. IEEE, 2013, pp. 7304\u20137308.", "citeRegEx": "66", "shortCiteRegEx": null, "year": 2013}, {"title": "Multilingual acoustic models using distributed deep neural networks", "author": ["G. Heigold", "V. Vanhoucke", "A. Senior", "P. Nguyen", "M. Ranzato", "M. Devin", "J. Dean"], "venue": "Acoustics, Speech and Signal Processing (ICASSP), 2013 IEEE International Conference on. IEEE, 2013, pp. 8619\u20138623.", "citeRegEx": "67", "shortCiteRegEx": null, "year": 2013}, {"title": "Multilingual training of deep neural networks", "author": ["A. Ghoshal", "P. Swietojanski", "S. Renals"], "venue": "Acoustics, Speech and Signal Processing (ICASSP), 2013 IEEE International Conference on. IEEE, 2013, pp. 7319\u20137323.", "citeRegEx": "68", "shortCiteRegEx": null, "year": 2013}, {"title": "The language-independent bottleneck features", "author": ["K. Vesely", "M. Karafi\u00e1t", "F. Grezl", "M. Janda", "E. Egorova"], "venue": "Spoken Language Technology Workshop (SLT), 2012 IEEE. IEEE, 2012, pp. 336\u2013341.", "citeRegEx": "69", "shortCiteRegEx": null, "year": 2012}, {"title": "Deep neural network features and semi-supervised training for low resource speech recognition", "author": ["S. Thomas", "M.L. Seltzer", "K. Church", "H. Hermansky"], "venue": "Acoustics, Speech and Signal Processing (ICASSP), 2013 IEEE International Conference on. IEEE, 2013, pp. 6704\u20136708.", "citeRegEx": "70", "shortCiteRegEx": null, "year": 2013}, {"title": "Investigation on cross-and multilingual mlp features under matched and mismatched acoustical conditions", "author": ["Z. Tuske", "J. Pinto", "D. Willett", "R. Schluter"], "venue": "Acoustics, Speech and Signal Processing (ICASSP), 2013 IEEE International Conference on. IEEE, 2013, pp. 7349\u20137353.", "citeRegEx": "71", "shortCiteRegEx": null, "year": 2013}, {"title": "Language independent and unsupervised acoustic models for speech recognition and keyword spotting", "author": ["K.M. Knill", "M.J. Gales", "A. Ragni", "S.P. Rath"], "venue": "Proc. Interspeech14, 2014.", "citeRegEx": "72", "shortCiteRegEx": null, "year": 2014}, {"title": "Multi-level adaptive networks in tandem and hybrid ASR systems", "author": ["P. Bell", "P. Swietojanski", "S. Renals"], "venue": "Acoustics, Speech and Signal Processing (ICASSP), 2013 IEEE International Conference on. IEEE, 2013, pp. 6975\u20136979.", "citeRegEx": "73", "shortCiteRegEx": null, "year": 2013}, {"title": "DNN acoustic modeling with modular multi-lingual feature extraction networks", "author": ["J. Gehring", "Q.B. Nguyen", "F. Metze", "A. Waibel"], "venue": "Automatic Speech Recognition and Understanding (ASRU), 2013 IEEE Workshop on. IEEE, 2013, pp. 344\u2013349.", "citeRegEx": "74", "shortCiteRegEx": null, "year": 2013}, {"title": "Joint acoustic modeling of triphones and trigraphemes by multi-task learning deep neural networks for low-resource speech recognition", "author": ["D. Chen", "B. Mak", "C.-C. Leung", "S. Sivadas"], "venue": "Acoustics, Speech and Signal Processing (ICASSP), 2014 IEEE International Conference on. IEEE, 2014, pp. 5592\u20135596.", "citeRegEx": "75", "shortCiteRegEx": null, "year": 2014}, {"title": "Speech recognition with pronunciation vecotrs", "author": ["X.Z. Zhiyuan Tang"], "venue": "CSLT, Tsinghua University, 2015. [Online]. Available: http://cslt.riit.tsinghua.edu.cn/publications.php?Publication-trp", "citeRegEx": "76", "shortCiteRegEx": null, "year": 2015}, {"title": "Frontend factor analysis for speaker verification", "author": ["N. Dehak", "P. Kenny", "R. Dehak", "P. Dumouchel", "P. Ouellet"], "venue": "Audio, Speech, and Language Processing, IEEE Transactions on, vol. 19, no. 4, pp. 788\u2013798, 2011.", "citeRegEx": "77", "shortCiteRegEx": null, "year": 2011}, {"title": "Deep neural networks for small footprint text-dependent speaker verification", "author": ["V. Ehsan", "L. Xin", "M. Erik", "L.M. Ignacio", "G.-D. Javier"], "venue": "IEEE International Conference on Acoustic, Speech and Signal Processing (ICASSP), vol. 28, no. 4, pp. 357\u2013366, 2014.", "citeRegEx": "78", "shortCiteRegEx": null, "year": 2014}, {"title": "Cross-language transfer learning for deep neural network based speech enhancement", "author": ["Y. Xu", "J. Du", "L.-R. Dai", "C.-H. Lee"], "venue": "Chinese Spoken Language Processing (ISCSLP), 2014 9th International Symposium on. IEEE, 2014, pp. 336\u2013340.", "citeRegEx": "79", "shortCiteRegEx": null, "year": 2014}, {"title": "Music removal by convolutional denoising autoencoder in speech recognition", "author": ["M. Zhao", "D. Wang", "Z. Zhang", "X. Zhang"], "venue": "Interspeech 2015, 2015.", "citeRegEx": "80", "shortCiteRegEx": null, "year": 2015}, {"title": "Separating speaker and environmental variability using factored transforms.", "author": ["M.L. Seltzer", "A. Acero"], "venue": "in INTERSPEECH,", "citeRegEx": "81", "shortCiteRegEx": "81", "year": 2011}, {"title": "A basis representation of constrained MLLR transforms for robust adaptation", "author": ["D. Povey", "K. Yao"], "venue": "Computer Speech & Language, vol. 26, no. 1, pp. 35\u201351, 2012.", "citeRegEx": "82", "shortCiteRegEx": null, "year": 2012}, {"title": "Learning discriminative basis coefficients for eigenspace MLLR unsupervised adaptation", "author": ["Y. Miao", "F. Metze", "A. Waibel"], "venue": "Acoustics, Speech and Signal Processing (ICASSP), 2013 IEEE International Conference on. IEEE, 2013, pp. 7927\u20137931.", "citeRegEx": "83", "shortCiteRegEx": null, "year": 2013}, {"title": "Speaker adaptation techniques for automatic speech recognition", "author": ["K. Shinoda"], "venue": "Proc. APSIPA ASC 2011, 2011.", "citeRegEx": "84", "shortCiteRegEx": null, "year": 2011}, {"title": "Fast speaker adaptation of hybrid NN/HMM model for speech recognition based on discriminative learning of speaker code", "author": ["O. Abdel-Hamid", "H. Jiang"], "venue": "Acoustics, Speech and Signal Processing (ICASSP), 2013 IEEE International Conference on. IEEE, 2013, pp. 7942\u20137946.", "citeRegEx": "85", "shortCiteRegEx": null, "year": 2013}, {"title": "Direct adaptation of hybrid DNN/HMM model for fast speaker adaptation in LVCSR based on speaker code", "author": ["S. Xue", "O. Abdel-Hamid", "H. Jiang", "L. Dai"], "venue": "Acoustics, Speech and Signal Processing (ICASSP), 2014 IEEE International Conference on. IEEE, 2014, pp. 6339\u20136343.", "citeRegEx": "87", "shortCiteRegEx": null, "year": 2014}, {"title": "Speaker adaptation of neural network acoustic models using i-vectors", "author": ["G. Saon", "H. Soltau", "D. Nahamoo", "M. Picheny"], "venue": "Automatic Speech Recognition and Understanding (ASRU), 2013 IEEE Workshop on. IEEE, 2013, pp. 55\u201359.", "citeRegEx": "88", "shortCiteRegEx": null, "year": 2013}, {"title": "Adaptation of deep neural network acoustic models using factorised i-vectors", "author": ["P. Karanasou", "Y. Wang", "M.J. Gales", "P.C. Woodland"], "venue": "Proc Interspeech, 2014.", "citeRegEx": "89", "shortCiteRegEx": null, "year": 2014}, {"title": "Improving DNN speaker independence with i-vector inputs", "author": ["A. Senior", "I. Lopez-Moreno"], "venue": "Proc. ICASSP, 2014.", "citeRegEx": "90", "shortCiteRegEx": null, "year": 2014}, {"title": "I-vector-based speaker adaptation of deep neural networks for french broadcast audio  transcription", "author": ["V. Gupta", "P. Kenny", "P. Ouellet", "T. Stafylakis"], "venue": "Acoustics, Speech and Signal Processing (ICASSP), 2014 IEEE International Conference on. IEEE, 2014, pp. 6334\u20136338.", "citeRegEx": "91", "shortCiteRegEx": null, "year": 2014}, {"title": "Speaker adaptation of DNN-based ASR with i-vectors: Does it actually adapt models to speakers?", "author": ["M. Rouvier", "B. Favre"], "venue": "Fifteenth Annual Conference of the International Speech Communication Association,", "citeRegEx": "92", "shortCiteRegEx": "92", "year": 2014}, {"title": "Speaker-adaptation for hybrid HMM-ANN continuous speech recognition system", "author": ["J. Neto", "L. Almeida", "M. Hochberg", "C. Martins", "L. Nunes", "S. Renals", "T. Robinson"], "venue": "Proc. EUROSPEECH\u201995. International Speech Communication Association, 1995.", "citeRegEx": "93", "shortCiteRegEx": null, "year": 1995}, {"title": "Adaptation of context-dependent deep neural networks for automatic speech recognition", "author": ["K. Yao", "D. Yu", "F. Seide", "H. Su", "L. Deng", "Y. Gong"], "venue": "Spoken Language Technology Workshop (SLT), 2012 IEEE. IEEE, 2012, pp. 366\u2013369.", "citeRegEx": "94", "shortCiteRegEx": null, "year": 2012}, {"title": "Linear hidden transformations for adaptation of hybrid ANN/HMM models", "author": ["R. Gemello", "F. Mana", "S. Scanzio", "P. Laface", "R. De Mori"], "venue": "Speech Communication, vol. 49, no. 10, pp. 827\u2013835, 2007.", "citeRegEx": "95", "shortCiteRegEx": null, "year": 2007}, {"title": "Hermitian polynomial for speaker adaptation of connectionist speech recognition systems", "author": ["S.M. Siniscalchi", "J. Li", "C.-H. Lee"], "venue": "Audio, Speech, and Language Processing, IEEE Transactions on, vol. 21, no. 10, pp. 2152\u20132161, 2013.", "citeRegEx": "96", "shortCiteRegEx": null, "year": 2013}, {"title": "Learning hidden unit contributions for unsupervised speaker adaptation of neural network acoustic models", "author": ["P. Swietojanski", "S. Renals"], "venue": "Spoken Language Technology Workshop (SLT), 2014 IEEE. IEEE, 2014, pp. 171\u2013176.", "citeRegEx": "97", "shortCiteRegEx": null, "year": 2014}, {"title": "Comparison of discriminative input and output transformations for speaker adaptation in the hybrid NN/HMM systems", "author": ["B. Li", "K.C. Sim"], "venue": "Interspeech\u201910, 2010.", "citeRegEx": "98", "shortCiteRegEx": null, "year": 2010}, {"title": "Speaker adaptation of context dependent deep neural networks", "author": ["H. Liao"], "venue": "Acoustics, Speech and Signal Processing (ICASSP), 2013 IEEE International Conference on. IEEE, 2013, pp. 7947\u20137951.", "citeRegEx": "99", "shortCiteRegEx": null, "year": 2013}, {"title": "Singular value decomposition based low-footprint speaker adaptation and personalization for deep neural network", "author": ["J. Xue", "J. Li", "D. Yu", "M. Seltzer", "Y. Gong"], "venue": "Acoustics, Speech and Signal Processing (ICASSP), 2014 IEEE International Conference on. IEEE, 2014, pp. 6359\u20136363.", "citeRegEx": "100", "shortCiteRegEx": null, "year": 2014}, {"title": "Speaker adaptation of hybrid NN/HMM model for speech recognition based on singular value decomposition", "author": ["S. Xue", "H. Jiang", "L. Dai"], "venue": "Chinese Spoken Language Processing (ISCSLP), 2014 9th International Symposium on. IEEE, 2014, pp. 1\u20135.", "citeRegEx": "101", "shortCiteRegEx": null, "year": 2014}, {"title": "Kl-divergence regularized deep neural network adaptation for improved large vocabulary speech recognition", "author": ["D. Yu", "K. Yao", "H. Su", "G. Li", "F. Seide"], "venue": "Acoustics, Speech and Signal Processing (ICASSP), 2013 IEEE International Conference on. IEEE, 2013, pp. 7893\u20137897.", "citeRegEx": "102", "shortCiteRegEx": null, "year": 2013}, {"title": "Deep neural network trained with speaker representation for speaker normalization", "author": ["Y. Tang", "A. Mohan", "R.C. Rose", "C. Ma"], "venue": "Acoustics, Speech and Signal Processing (ICASSP), 2014 IEEE International Conference on. IEEE, 2014, pp. 6329\u20136333.", "citeRegEx": "103", "shortCiteRegEx": null, "year": 2014}, {"title": "Towards speaker adaptive training of deep neural network acoustic models", "author": ["Y. Miao", "H. Zhang", "F. Metze"], "venue": "Interspeech\u201914, 2014.", "citeRegEx": "104", "shortCiteRegEx": null, "year": 2014}, {"title": "On speaker adaptation of long short-term memory recurrent neural networks", "author": ["Y. Miao", "F. Metze"], "venue": "Sixteenth Annual Conference of the International Speech Communication Association (INTERSPEECH)(To Appear). ISCA, 2015.", "citeRegEx": "105", "shortCiteRegEx": null, "year": 2015}, {"title": "Adaptation of pitch and spectrum for HMM-based speech synthesis using MLLR", "author": ["M. Tamura", "T. Masuko", "K. Tokuda", "T. Kobayashi"], "venue": "Acoustics, Speech, and Signal Processing, 2001. Proceedings.(ICASSP\u201901). 2001 IEEE International Conference on, vol. 2. IEEE, 2001, pp. 805\u2013808.", "citeRegEx": "106", "shortCiteRegEx": null, "year": 2001}, {"title": "State mapping based method for cross-lingual speaker adaptation in HMM-based speech synthesis.", "author": ["Y.-J. Wu", "Y. Nankaku", "K. Tokuda"], "venue": "in Interspeech,", "citeRegEx": "107", "shortCiteRegEx": "107", "year": 2009}, {"title": "Average-voice-based speech synthesis using HSMM-based speaker adaptation and adaptive training", "author": ["J. Yamagishi", "T. Kobayashi"], "venue": "IEICE TRANSACTIONS on Information and Systems, vol. 90, no. 2, pp. 533\u2013 543, 2007.", "citeRegEx": "108", "shortCiteRegEx": null, "year": 2007}, {"title": "Analysis of speaker adaptation algorithms for HMM-based speech synthesis and a constrained SMAPLR adaptation algorithm", "author": ["J. Yamagishi", "T. Kobayashi", "Y. Nakano", "K. Ogata", "J. Isogai"], "venue": "Audio, Speech, and Language Processing, IEEE Transactions on, vol. 17, no. 1, pp. 66\u201383, 2009.", "citeRegEx": "109", "shortCiteRegEx": null, "year": 2009}, {"title": "A comparison of supervised and unsupervised cross-lingual speaker adaptation approaches for HMMbased speech synthesis", "author": ["H. Liang", "J. Dines", "L. Saheer"], "venue": "Acoustics Speech and Signal Processing (ICASSP), 2010 IEEE International Conference on. IEEE, 2010, pp. 4598\u20134601.", "citeRegEx": "110", "shortCiteRegEx": null, "year": 2010}, {"title": "Unsupervised intralingual and cross-lingual speaker adaptation for HMM-based speech synthesis using two-pass decision tree construction", "author": ["M. Gibson", "W. Byrne"], "venue": "Audio, Speech, and Language Processing, IEEE Transactions on, vol. 19, no. 4, pp. 895\u2013904, May 2011.", "citeRegEx": "111", "shortCiteRegEx": null, "year": 2011}, {"title": "Modeling spectral envelopes using restricted boltzmann machines and deep belief networks for statistical parametric speech synthesis", "author": ["Z.-H. Ling", "L. Deng", "D. Yu"], "venue": "Audio, Speech, and Language Processing, IEEE Transactions on, vol. 21, no. 10, pp. 2129\u20132139, 2013.", "citeRegEx": "112", "shortCiteRegEx": null, "year": 2013}, {"title": "Deep mixture density networks for acoustic modeling in statistical parametric speech synthesis", "author": ["H. Zen", "A. Senior"], "venue": "Acoustics, Speech and Signal Processing (ICASSP), 2014 IEEE International Conference on. IEEE, 2014, pp. 3844\u20133848.", "citeRegEx": "113", "shortCiteRegEx": null, "year": 2014}, {"title": "The effect of neural networks in statistical parametric speech synthesis", "author": ["K. Hashimoto", "K. Oura", "Y. Nankaku", "K. Tokuda"], "venue": "Acoustics, Speech and Signal Processing (ICASSP), 2015 IEEE International Conference on. IEEE, 2015, pp. 4455\u20134459.", "citeRegEx": "114", "shortCiteRegEx": null, "year": 2015}, {"title": "Preliminary work on speaker adaptation for DNN-based speech synthesis", "author": ["B. Potard", "P. Motlicek", "D. Imseng"], "venue": "Idiap, Tech. Rep., 2015.", "citeRegEx": "115", "shortCiteRegEx": null, "year": 2015}, {"title": "A study of speaker adaptation for DNN-based speech synthesis", "author": ["Z. Wu", "P. Swietojanski", "C. Veaux", "S. Renals", "S. King"], "venue": "Interspeech 2015, 2015.", "citeRegEx": "116", "shortCiteRegEx": null, "year": 2015}, {"title": "Do deep nets really need to be deep?", "author": ["J. Ba", "R. Caruana"], "venue": "Advances in Neural Information Processing Systems,", "citeRegEx": "117", "shortCiteRegEx": "117", "year": 2014}, {"title": "Learning small-size DNN with output-distribution-based criteria", "author": ["J. Li", "R. Zhao", "J.-T. Huang", "Y. Gong"], "venue": "Proceedings of the Annual Conference of International Speech Communication Association (INTERSPEECH), September 2014. [Online]. Available: http://research.microsoft.com/apps/pubs/default.aspx?id=230080", "citeRegEx": "118", "shortCiteRegEx": null, "year": 2014}, {"title": "Transferring knowledge from a RNN to a DNN", "author": ["W. Chan", "N.R. Ke", "I. Lane"], "venue": "arXiv preprint arXiv:1504.01483, 2015.", "citeRegEx": "119", "shortCiteRegEx": null, "year": 2015}, {"title": "Fitnets: Hints for thin deep nets", "author": ["A. Romero", "N. Ballas", "S.E. Kahou", "A. Chassang", "C. Gatta", "Y. Bengio"], "venue": "arXiv preprint arXiv:1412.6550, 2014.", "citeRegEx": "120", "shortCiteRegEx": null, "year": 2014}, {"title": "Learning transferable features with deep adaptation networks", "author": ["M. Long", "J. Wang"], "venue": "arXiv preprint arXiv:1502.02791, 2015.", "citeRegEx": "121", "shortCiteRegEx": null, "year": 2015}, {"title": "Unsupervised learning of neural network outputs", "author": ["Y. Lu"], "venue": "arXiv preprint arXiv:1506.00990, 2015.", "citeRegEx": "122", "shortCiteRegEx": null, "year": 2015}, {"title": "Multi-task learning for text-dependent speaker verification", "author": ["N. Chen", "Y. Qian", "K. Yu"], "venue": "Sixteenth Annual Conference of the International Speech Communication Association, 2015.", "citeRegEx": "123", "shortCiteRegEx": null, "year": 2015}, {"title": "Multilingual bottleneck features for language recognition", "author": ["R. F\u00e9r", "P. Mat\u011bjka", "F. Gr\u00e9zl", "O. Plchot", "J. \u010cernock\u1ef3"], "venue": "Sixteenth Annual Conference of the International Speech Communication Association, 2015.", "citeRegEx": "124", "shortCiteRegEx": null, "year": 2015}, {"title": "Syntactic transfer using a bilingual lexicon", "author": ["G. Durrett", "A. Pauls", "D. Klein"], "venue": "Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning. Association for Computational Linguistics, 2012, pp. 1\u201311.", "citeRegEx": "125", "shortCiteRegEx": null, "year": 2012}, {"title": "Cross language text classification by model translation and semi-supervised learning", "author": ["L. Shi", "R. Mihalcea", "M. Tian"], "venue": "Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics, 2010, pp. 1057\u20131067.", "citeRegEx": "126", "shortCiteRegEx": null, "year": 2010}, {"title": "Recognize foreign low-frequency words with similar pairs", "author": ["X. Ma", "X. Wang", "D. Wang"], "venue": "Interspeech 2015, 2015.", "citeRegEx": "127", "shortCiteRegEx": null, "year": 2015}, {"title": "Statistical machine translation", "author": ["P. Koehn"], "venue": null, "citeRegEx": "128", "shortCiteRegEx": "128", "year": 2009}, {"title": "Knowledge transfer across multilingual corpora via latent topics", "author": ["W. De Smet", "J. Tang", "M.-F. Moens"], "venue": "Advances in Knowledge Discovery and Data Mining. Springer, 2011, pp. 549\u2013560.", "citeRegEx": "129", "shortCiteRegEx": null, "year": 2011}, {"title": "Cross-lingual word clusters for direct transfer of linguistic structure", "author": ["O. T\u00e4ckstr\u00f6m", "R. McDonald", "J. Uszkoreit"], "venue": "Proceedings of the 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. Association for Computational Linguistics, 2012, pp. 477\u2013487.", "citeRegEx": "130", "shortCiteRegEx": null, "year": 2012}, {"title": "Nudging the envelope of direct transfer methods for multilingual named entity recognition", "author": ["O. T\u00e4ckstr\u00f6m"], "venue": "Proceedings of the NAACL- HLT Workshop on the Induction of Linguistic Structure. Association for Computational Linguistics, 2012, pp. 55\u201363.", "citeRegEx": "131", "shortCiteRegEx": null, "year": 2012}, {"title": "Neural probabilistic language models", "author": ["Y. Bengio", "H. Schwenk", "J.-S. Sen\u00e9cal", "F. Morin", "J.-L. Gauvain"], "venue": "Innovations in Machine Learning. Springer, 2006, pp. 137\u2013186.", "citeRegEx": "132", "shortCiteRegEx": null, "year": 2006}, {"title": "A scalable hierarchical distributed language model", "author": ["A. Mnih", "G.E. Hinton"], "venue": "NIPS, 2008, pp. 1081\u20131088.", "citeRegEx": "133", "shortCiteRegEx": null, "year": 2008}, {"title": "Word representations: a simple and general method for semi-supervised learning", "author": ["J. Turian", "L. Ratinov", "Y. Bengio"], "venue": "Proceedings of the 48th annual meeting of the association for computational linguistics. Association for Computational Linguistics, 2010, pp. 384\u2013 394.", "citeRegEx": "134", "shortCiteRegEx": null, "year": 2010}, {"title": "Natural language processing (almost) from scratch", "author": ["R. Collobert", "J. Weston", "L. Bottou", "M. Karlen", "K. Kavukcuoglu", "P. Kuksa"], "venue": "The Journal of Machine Learning Research, vol. 12, pp. 2493\u20132537, 2011.", "citeRegEx": "135", "shortCiteRegEx": null, "year": 2011}, {"title": "Efficient estimation of word representations in vector space", "author": ["T. Mikolov", "K. Chen", "G. Corrado", "J. Dean"], "venue": "arXiv preprint arXiv:1301.3781, 2013.", "citeRegEx": "136", "shortCiteRegEx": null, "year": 2013}, {"title": "Exploiting similarities among languages for machine translation", "author": ["T. Mikolov", "Q.V. Le", "I. Sutskever"], "venue": "arXiv preprint arXiv:1309.4168, 2013.", "citeRegEx": "137", "shortCiteRegEx": null, "year": 2013}, {"title": "Normalized word embedding and orthogonal transform for bilingual word translation", "author": ["C. Xing", "D. Wang", "C. Liu", "Y. Lin"], "venue": "NAACL\u201915, 2015.", "citeRegEx": "138", "shortCiteRegEx": null, "year": 2015}, {"title": "Improving vector space word representations using multilingual correlation", "author": ["M. Faruqui", "C. Dyer"], "venue": "EACL\u201914. Association for Computational Linguistics, 2014.", "citeRegEx": "139", "shortCiteRegEx": null, "year": 2014}, {"title": "Inducing crosslingual distributed representations of words", "author": ["A. Klementiev", "I. Titov", "B. Bhattarai"], "venue": "COLING\u201912. Citeseer, 2012.", "citeRegEx": "140", "shortCiteRegEx": null, "year": 2012}, {"title": "Multilingual models for compositional distributed semantics", "author": ["K.M. Hermann", "P. Blunsom"], "venue": "arXiv preprint arXiv:1404.4641, 2014.", "citeRegEx": "141", "shortCiteRegEx": null, "year": 2014}, {"title": "Bilbowa: Fast bilingual distributed representations without word alignments", "author": ["S. Gouws", "Y. Bengio", "G. Corrado"], "venue": "arXiv preprint arXiv:1410.2455, 2014.", "citeRegEx": "142", "shortCiteRegEx": null, "year": 2014}, {"title": "Distributional semantics in technicolor", "author": ["E. Bruni", "G. Boleda", "M. Baroni", "N.-K. Tran"], "venue": "Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics: Long Papers-Volume 1. Association for Computational Linguistics, 2012, pp. 136\u2013145.", "citeRegEx": "143", "shortCiteRegEx": null, "year": 2012}, {"title": "Going beyond text: A hybrid imagetext approach for measuring word relatedness.", "author": ["C.W. Leong", "R. Mihalcea"], "venue": "IJCNLP,", "citeRegEx": "144", "shortCiteRegEx": "144", "year": 2011}, {"title": "Connecting modalities: Semi-supervised segmentation and annotation of images using unaligned text corpora", "author": ["R. Socher", "L. Fei-Fei"], "venue": "Computer Vision and Pattern Recognition (CVPR), 2010 IEEE Conference on. IEEE, 2010, pp. 966\u2013973.", "citeRegEx": "145", "shortCiteRegEx": null, "year": 2010}, {"title": "Devise: A deep visual-semantic embedding model", "author": ["A. Frome", "G.S. Corrado", "J. Shlens", "S. Bengio", "J. Dean", "T. Mikolov"], "venue": "Advances in Neural Information Processing Systems, 2013, pp. 2121\u20132129.", "citeRegEx": "146", "shortCiteRegEx": null, "year": 2013}, {"title": "Multimodal neural language models", "author": ["R. Kiros", "R. Salakhutdinov", "R. Zemel"], "venue": "Proceedings of the 31st International Conference on Machine Learning (ICML-14), 2014, pp. 595\u2013603.", "citeRegEx": "147", "shortCiteRegEx": null, "year": 2014}, {"title": "Grounded compositional semantics for finding and describing images with sentences", "author": ["R. Socher", "A. Karpathy", "Q.V. Le", "C.D. Manning", "A.Y. Ng"], "venue": "Transactions of the Association for Computational Linguistics, vol. 2, pp. 207\u2013218, 2014.", "citeRegEx": "148", "shortCiteRegEx": null, "year": 2014}, {"title": "Multimodal learning with deep boltzmann machines", "author": ["N. Srivastava", "R.R. Salakhutdinov"], "venue": "Advances in neural information processing systems, 2012, pp. 2222\u20132230.", "citeRegEx": "149", "shortCiteRegEx": null, "year": 2012}, {"title": "Distilling word embeddings: An encoding approach", "author": ["L. Mou", "G. Li", "Y. Xu", "L. Zhang", "Z. Jin"], "venue": "arXiv preprint arXiv:1506.04488, 2015.", "citeRegEx": "150", "shortCiteRegEx": null, "year": 2015}, {"title": "Learning from LDA using deep neural networks", "author": ["D. Zhang", "T. Luo", "D. Wang", "R. Liu"], "venue": "arXiv preprint arXiv:1508.01011, 2015.", "citeRegEx": "151", "shortCiteRegEx": null, "year": 2015}, {"title": "Transfer learning with graph co-regularization", "author": ["M. Long", "J. Wang", "G. Ding", "D. Shen", "Q. Yang"], "venue": "Knowledge and Data Engineering, IEEE Transactions on, vol. 26, no. 7, pp. 1805\u20131818, 2014.", "citeRegEx": "152", "shortCiteRegEx": null, "year": 1805}], "referenceMentions": [{"referenceID": 0, "context": "exploited in modern speech and language processing research [1], [2], [3].", "startOffset": 60, "endOffset": 63}, {"referenceID": 1, "context": "exploited in modern speech and language processing research [1], [2], [3].", "startOffset": 70, "endOffset": 73}, {"referenceID": 2, "context": ") to enhance model learning for the target task [4], [5], [6], [7].", "startOffset": 48, "endOffset": 51}, {"referenceID": 3, "context": ") to enhance model learning for the target task [4], [5], [6], [7].", "startOffset": 53, "endOffset": 56}, {"referenceID": 4, "context": ") to enhance model learning for the target task [4], [5], [6], [7].", "startOffset": 58, "endOffset": 61}, {"referenceID": 5, "context": ") to enhance model learning for the target task [4], [5], [6], [7].", "startOffset": 63, "endOffset": 66}, {"referenceID": 2, "context": "more detailed surveys on transfer learning in broad research fields, readers are referred to the nice review articles from Pan, Taylor, Bengio and Lu [4], [5], [6], [7] and the references therein.", "startOffset": 150, "endOffset": 153}, {"referenceID": 3, "context": "more detailed surveys on transfer learning in broad research fields, readers are referred to the nice review articles from Pan, Taylor, Bengio and Lu [4], [5], [6], [7] and the references therein.", "startOffset": 155, "endOffset": 158}, {"referenceID": 4, "context": "more detailed surveys on transfer learning in broad research fields, readers are referred to the nice review articles from Pan, Taylor, Bengio and Lu [4], [5], [6], [7] and the references therein.", "startOffset": 160, "endOffset": 163}, {"referenceID": 5, "context": "more detailed surveys on transfer learning in broad research fields, readers are referred to the nice review articles from Pan, Taylor, Bengio and Lu [4], [5], [6], [7] and the references therein.", "startOffset": 165, "endOffset": 168}, {"referenceID": 6, "context": "The motivation of transfer learning can be found in the idea of \u201dLearning to Learn\u201d, which stats that learning from scratch (tabula rasa learning) is often limited, and so past experience should be used as much as possible [8].", "startOffset": 223, "endOffset": 226}, {"referenceID": 2, "context": "For example, Pan and Yang [4] believed transfer learning should really", "startOffset": 26, "endOffset": 29}, {"referenceID": 4, "context": "\u2018transfer\u2019 something so multitask learning should be regarded as a different approach, while Bengio [6] treated transfer", "startOffset": 100, "endOffset": 103}, {"referenceID": 2, "context": "com/site/icdmwptl2015/home taxonomy in [4], we use data and task as two conditional factors of transfer learning.", "startOffset": 39, "endOffset": 42}, {"referenceID": 7, "context": "X+ P(X)+ Conventional ML Model transfer[10] Multitask learning[11] P(X)- Model Adaptation[12], [13], incremental learning[14]", "startOffset": 39, "endOffset": 43}, {"referenceID": 8, "context": "X+ P(X)+ Conventional ML Model transfer[10] Multitask learning[11] P(X)- Model Adaptation[12], [13], incremental learning[14]", "startOffset": 62, "endOffset": 66}, {"referenceID": 9, "context": "X+ P(X)+ Conventional ML Model transfer[10] Multitask learning[11] P(X)- Model Adaptation[12], [13], incremental learning[14]", "startOffset": 89, "endOffset": 93}, {"referenceID": 10, "context": "X+ P(X)+ Conventional ML Model transfer[10] Multitask learning[11] P(X)- Model Adaptation[12], [13], incremental learning[14]", "startOffset": 95, "endOffset": 99}, {"referenceID": 11, "context": "X+ P(X)+ Conventional ML Model transfer[10] Multitask learning[11] P(X)- Model Adaptation[12], [13], incremental learning[14]", "startOffset": 121, "endOffset": 125}, {"referenceID": 12, "context": "X\u2212 Co-training[15] Heterogeneous transfer learning[16], [17] Analogy learning [18]", "startOffset": 14, "endOffset": 18}, {"referenceID": 13, "context": "X\u2212 Co-training[15] Heterogeneous transfer learning[16], [17] Analogy learning [18]", "startOffset": 50, "endOffset": 54}, {"referenceID": 14, "context": "X\u2212 Co-training[15] Heterogeneous transfer learning[16], [17] Analogy learning [18]", "startOffset": 56, "endOffset": 60}, {"referenceID": 15, "context": "X\u2212 Co-training[15] Heterogeneous transfer learning[16], [17] Analogy learning [18]", "startOffset": 78, "endOffset": 82}, {"referenceID": 9, "context": "(MAP) [12] estimation and the maximum likelihood linear regression (MLLR) algorithm [13].", "startOffset": 6, "endOffset": 10}, {"referenceID": 10, "context": "(MAP) [12] estimation and the maximum likelihood linear regression (MLLR) algorithm [13].", "startOffset": 84, "endOffset": 88}, {"referenceID": 11, "context": "[14], [19], [20].", "startOffset": 0, "endOffset": 4}, {"referenceID": 16, "context": "[14], [19], [20].", "startOffset": 6, "endOffset": 10}, {"referenceID": 17, "context": "[14], [19], [20].", "startOffset": 12, "endOffset": 16}, {"referenceID": 18, "context": "The latter case is often referred to as semi-supervised learning [21].", "startOffset": 65, "endOffset": 69}, {"referenceID": 19, "context": "analysis (TCA) [22].", "startOffset": 15, "endOffset": 19}, {"referenceID": 20, "context": "This approach is often referred to as self-taught learning [23], and it essentially holds the same idea as the more recent deep representation learning that will be discussed in Section II-C.", "startOffset": 59, "endOffset": 63}, {"referenceID": 21, "context": "For example, [24] employed an oracle word translator to define some pivot words that were used to establish the crossdomain correspondence by learning multiple linear classifiers that predict the \u2018joint existence\u2019 of these words in the multi-", "startOffset": 13, "endOffset": 17}, {"referenceID": 22, "context": "In [25] some instance-level co-occurrence data were used to estimate the correspondence in the form of joint or conditional probabilities; this correspondence was then used to improve the model in the target domain by riskminimization inference.", "startOffset": 3, "endOffset": 7}, {"referenceID": 23, "context": "Asymmetric regularized cross-domain transformation was proposed in [26], which tries to learn a non-linear transform between the source and target domains by class-labeled instances from both source and target domains.", "startOffset": 67, "endOffset": 71}, {"referenceID": 14, "context": "factorization [17], RBM-based latent factor learning [27], or joint transfer optimization [28], [16], [29].", "startOffset": 14, "endOffset": 18}, {"referenceID": 24, "context": "factorization [17], RBM-based latent factor learning [27], or joint transfer optimization [28], [16], [29].", "startOffset": 53, "endOffset": 57}, {"referenceID": 25, "context": "factorization [17], RBM-based latent factor learning [27], or joint transfer optimization [28], [16], [29].", "startOffset": 90, "endOffset": 94}, {"referenceID": 13, "context": "factorization [17], RBM-based latent factor learning [27], or joint transfer optimization [28], [16], [29].", "startOffset": 96, "endOffset": 100}, {"referenceID": 26, "context": "factorization [17], RBM-based latent factor learning [27], or joint transfer optimization [28], [16], [29].", "startOffset": 102, "endOffset": 106}, {"referenceID": 27, "context": "More recently, deep learning and heterogeneous transfer learning are combined where high-level features are derived by deep learning and inter-domain transforms are learned by transfer learning [30].", "startOffset": 194, "endOffset": 198}, {"referenceID": 28, "context": "respondence between two independent but analogous domains is easy for humans [31], [32], [33], however it is very difficult for machines.", "startOffset": 77, "endOffset": 81}, {"referenceID": 29, "context": "respondence between two independent but analogous domains is easy for humans [31], [32], [33], however it is very difficult for machines.", "startOffset": 83, "endOffset": 87}, {"referenceID": 30, "context": "respondence between two independent but analogous domains is easy for humans [31], [32], [33], however it is very difficult for machines.", "startOffset": 89, "endOffset": 93}, {"referenceID": 31, "context": ", [34], [18], though not too much achievement yet.", "startOffset": 2, "endOffset": 6}, {"referenceID": 15, "context": ", [34], [18], though not too much achievement yet.", "startOffset": 8, "endOffset": 12}, {"referenceID": 12, "context": "called co-training [15].", "startOffset": 19, "endOffset": 23}, {"referenceID": 7, "context": "For example, it is possible to learn simple neural nets from a complex DNN model, or vice versa [10], [35], [36].", "startOffset": 96, "endOffset": 100}, {"referenceID": 32, "context": "For example, it is possible to learn simple neural nets from a complex DNN model, or vice versa [10], [35], [36].", "startOffset": 102, "endOffset": 106}, {"referenceID": 33, "context": "For example, it is possible to learn simple neural nets from a complex DNN model, or vice versa [10], [35], [36].", "startOffset": 108, "endOffset": 112}, {"referenceID": 8, "context": "applicable [11], [37], [38].", "startOffset": 11, "endOffset": 15}, {"referenceID": 34, "context": "applicable [11], [37], [38].", "startOffset": 17, "endOffset": 21}, {"referenceID": 35, "context": "applicable [11], [37], [38].", "startOffset": 23, "endOffset": 27}, {"referenceID": 36, "context": ", text content classification and emotion detection [39].", "startOffset": 52, "endOffset": 56}, {"referenceID": 35, "context": "Although there is not a simple solution yet, [38] indeed provided an interesting approach", "startOffset": 45, "endOffset": 49}, {"referenceID": 37, "context": "Because deep learning gains so much success in speech and language processing [40], [41], [42], [43], we put more emphasis on transfer learning methods based on deep models in this paper.", "startOffset": 78, "endOffset": 82}, {"referenceID": 38, "context": "Because deep learning gains so much success in speech and language processing [40], [41], [42], [43], we put more emphasis on transfer learning methods based on deep models in this paper.", "startOffset": 84, "endOffset": 88}, {"referenceID": 39, "context": "Because deep learning gains so much success in speech and language processing [40], [41], [42], [43], we put more emphasis on transfer learning methods based on deep models in this paper.", "startOffset": 90, "endOffset": 94}, {"referenceID": 40, "context": "Because deep learning gains so much success in speech and language processing [40], [41], [42], [43], we put more emphasis on transfer learning methods based on deep models in this paper.", "startOffset": 96, "endOffset": 100}, {"referenceID": 41, "context": "Typical deep models include deep belief networks (DBNs) [44], deep Boltzmann machines (DBMs) [45], deep auto encoders", "startOffset": 56, "endOffset": 60}, {"referenceID": 42, "context": "Typical deep models include deep belief networks (DBNs) [44], deep Boltzmann machines (DBMs) [45], deep auto encoders", "startOffset": 93, "endOffset": 97}, {"referenceID": 43, "context": "(DAEs) [46], [47], deep neural networks (DNNs) [48], [41] and deep recurrent neural networks (RNNs) [49].", "startOffset": 7, "endOffset": 11}, {"referenceID": 44, "context": "(DAEs) [46], [47], deep neural networks (DNNs) [48], [41] and deep recurrent neural networks (RNNs) [49].", "startOffset": 13, "endOffset": 17}, {"referenceID": 45, "context": "(DAEs) [46], [47], deep neural networks (DNNs) [48], [41] and deep recurrent neural networks (RNNs) [49].", "startOffset": 47, "endOffset": 51}, {"referenceID": 38, "context": "(DAEs) [46], [47], deep neural networks (DNNs) [48], [41] and deep recurrent neural networks (RNNs) [49].", "startOffset": 53, "endOffset": 57}, {"referenceID": 46, "context": "(DAEs) [46], [47], deep neural networks (DNNs) [48], [41] and deep recurrent neural networks (RNNs) [49].", "startOffset": 100, "endOffset": 104}, {"referenceID": 47, "context": "First, it can learn high-level features which are more robust against data variation than features at low-levels; second, it offers a hierarchal parameter sharing that holds great expressive power [50]; third, the feature learning can be easily conducted without any labelled data and so is cheap; fourth, with a little", "startOffset": 197, "endOffset": 201}, {"referenceID": 8, "context": "supervised training (fine-tuning), the learned models can be well adapted to specific tasks [11], [51], [52].", "startOffset": 92, "endOffset": 96}, {"referenceID": 48, "context": "supervised training (fine-tuning), the learned models can be well adapted to specific tasks [11], [51], [52].", "startOffset": 98, "endOffset": 102}, {"referenceID": 49, "context": "supervised training (fine-tuning), the learned models can be well adapted to specific tasks [11], [51], [52].", "startOffset": 104, "endOffset": 108}, {"referenceID": 50, "context": "fields [53], [6], [54], [55].", "startOffset": 7, "endOffset": 11}, {"referenceID": 4, "context": "fields [53], [6], [54], [55].", "startOffset": 13, "endOffset": 16}, {"referenceID": 51, "context": "fields [53], [6], [54], [55].", "startOffset": 18, "endOffset": 22}, {"referenceID": 52, "context": "fields [53], [6], [54], [55].", "startOffset": 24, "endOffset": 28}, {"referenceID": 53, "context": ", by restricted Boltzmann machines (RBMs) [56] or auto-associators [46], therefore little or no labelled data are required.", "startOffset": 42, "endOffset": 46}, {"referenceID": 43, "context": ", by restricted Boltzmann machines (RBMs) [56] or auto-associators [46], therefore little or no labelled data are required.", "startOffset": 67, "endOffset": 71}, {"referenceID": 4, "context": "According to [6], as long as the distribution P (X) is relevant to the class-conditional distribution P (Y |X), the unsupervised learning can improve the target supervised learning, in terms of convergence speed, amount of labelled data required and model quality.", "startOffset": 13, "endOffset": 16}, {"referenceID": 54, "context": "An early work based on deep representation learning is [57], where the authors used unsupervised learning (denoising auto-", "startOffset": 55, "endOffset": 59}, {"referenceID": 55, "context": "As another example, [58] showed that CNN-based representations learned from a large image database imageNet were successfully applied to represent images in another database PASCAL VOC.", "startOffset": 20, "endOffset": 24}, {"referenceID": 56, "context": "A similar study was proposed recently in [59] where CNN features trained on multiple tasks were successfully", "startOffset": 41, "endOffset": 45}, {"referenceID": 57, "context": "In another example called \u2018one-short learning\u2019 [60], highlevel features trained on a large image database were found", "startOffset": 47, "endOffset": 51}, {"referenceID": 58, "context": "leads to the zero-data learning [61] and zero-shot learning [62].", "startOffset": 32, "endOffset": 36}, {"referenceID": 59, "context": "leads to the zero-data learning [61] and zero-shot learning [62].", "startOffset": 60, "endOffset": 64}, {"referenceID": 60, "context": ", by a universal phone set or a pair-wised phone mapping [63], [64].", "startOffset": 57, "endOffset": 61}, {"referenceID": 61, "context": ", by a universal phone set or a pair-wised phone mapping [63], [64].", "startOffset": 63, "endOffset": 67}, {"referenceID": 62, "context": "The initial investigation was proposed in [65], where mul-", "startOffset": 42, "endOffset": 46}, {"referenceID": 63, "context": "In the multilingual scenario, the hidden layers of the DNN structure are shared across languages and each language holds its own output layer [66], [67], [68].", "startOffset": 142, "endOffset": 146}, {"referenceID": 64, "context": "In the multilingual scenario, the hidden layers of the DNN structure are shared across languages and each language holds its own output layer [66], [67], [68].", "startOffset": 148, "endOffset": 152}, {"referenceID": 65, "context": "In the multilingual scenario, the hidden layers of the DNN structure are shared across languages and each language holds its own output layer [66], [67], [68].", "startOffset": 154, "endOffset": 158}, {"referenceID": 63, "context": "dently by three research groups in 2013, and tested on three different databases: English and Mandarin data [66], eleven", "startOffset": 108, "endOffset": 112}, {"referenceID": 64, "context": "Romance languages [67] and the global phone dataset with 19 languages [68].", "startOffset": 18, "endOffset": 22}, {"referenceID": 65, "context": "Romance languages [67] and the global phone dataset with 19 languages [68].", "startOffset": 70, "endOffset": 74}, {"referenceID": 66, "context": "In [69], [70], the same DNN structure as in the hybrid setting was used to train a multilingual DNN, however the", "startOffset": 3, "endOffset": 7}, {"referenceID": 67, "context": "In [69], [70], the same DNN structure as in the hybrid setting was used to train a multilingual DNN, however the", "startOffset": 9, "endOffset": 13}, {"referenceID": 68, "context": "The same approach was proposed in [71], though the features were read from a hidden layer in the middle layer (the bottle net layer with less neurons than other layers) instead of the last hidden layer.", "startOffset": 34, "endOffset": 38}, {"referenceID": 69, "context": "independent BN features can be used to train models for languages even without any labelled data [72].", "startOffset": 97, "endOffset": 101}, {"referenceID": 70, "context": "For example, in [73], the BN feature was first derived from a multilingual DNN, and then it was combined with the original feature to train a hybrid system.", "startOffset": 16, "endOffset": 20}, {"referenceID": 71, "context": "A similar approach was proposed in [74], where the BN feature extractor for each language was regarded as a module, and another DNN combined the BN features from the modules of multiple languages to construct the hybrid system.", "startOffset": 35, "endOffset": 39}, {"referenceID": 72, "context": "For example, in [75] phone recognition and grapheme", "startOffset": 16, "endOffset": 20}, {"referenceID": 73, "context": "together [76].", "startOffset": 9, "endOffset": 13}, {"referenceID": 74, "context": "vector system [77] or a DNN system [78].", "startOffset": 14, "endOffset": 18}, {"referenceID": 75, "context": "vector system [77] or a DNN system [78].", "startOffset": 35, "endOffset": 39}, {"referenceID": 8, "context": "a heterogeneous multitask learning that has been proposed for a long time [11] but has not been studied much in speech", "startOffset": 74, "endOffset": 78}, {"referenceID": 76, "context": "For example, in [79], an DNN architecture trained in English data was demonstrated", "startOffset": 16, "endOffset": 20}, {"referenceID": 77, "context": "our group demonstrated that a DNN structure can be used to remove music from speech in multilingual conditions [80].", "startOffset": 111, "endOffset": 115}, {"referenceID": 9, "context": ", Gaussian models or Gaussian mixture models), maximum a posterior (MAP) estimation [12] and maximum likelihood linear regression (MLLR) [13] are", "startOffset": 84, "endOffset": 88}, {"referenceID": 10, "context": ", Gaussian models or Gaussian mixture models), maximum a posterior (MAP) estimation [12] and maximum likelihood linear regression (MLLR) [13] are", "startOffset": 137, "endOffset": 141}, {"referenceID": 78, "context": "[81], [82], [83].", "startOffset": 0, "endOffset": 4}, {"referenceID": 79, "context": "[81], [82], [83].", "startOffset": 6, "endOffset": 10}, {"referenceID": 80, "context": "[81], [82], [83].", "startOffset": 12, "endOffset": 16}, {"referenceID": 81, "context": "A short survey for these early-stage techniques can be found in [84].", "startOffset": 64, "endOffset": 68}, {"referenceID": 82, "context": "An early study reported in [85] introduced a user vector (user code) to represent a speaker, and the vector was", "startOffset": 27, "endOffset": 31}, {"referenceID": 83, "context": "This approach was extended in [87] by involving a transform matrix before the speaker vector was augmented to the input and hidden layers, possibly in the form of low-rank matrices.", "startOffset": 30, "endOffset": 34}, {"referenceID": 74, "context": ", the famous i-vector [77].", "startOffset": 22, "endOffset": 26}, {"referenceID": 84, "context": "(although it is possible) [88], [89], [90], [91].", "startOffset": 26, "endOffset": 30}, {"referenceID": 85, "context": "(although it is possible) [88], [89], [90], [91].", "startOffset": 32, "endOffset": 36}, {"referenceID": 86, "context": "(although it is possible) [88], [89], [90], [91].", "startOffset": 38, "endOffset": 42}, {"referenceID": 87, "context": "(although it is possible) [88], [89], [90], [91].", "startOffset": 44, "endOffset": 48}, {"referenceID": 88, "context": "i-vector augmentation was conducted in [92], which showed that i-vectors not only compensate for speaker variance but", "startOffset": 39, "endOffset": 43}, {"referenceID": 89, "context": "the adaptation can be conducted on the input layer [93], [94], the activations of hidden layers [95], [96], [97], or", "startOffset": 51, "endOffset": 55}, {"referenceID": 90, "context": "the adaptation can be conducted on the input layer [93], [94], the activations of hidden layers [95], [96], [97], or", "startOffset": 57, "endOffset": 61}, {"referenceID": 91, "context": "the adaptation can be conducted on the input layer [93], [94], the activations of hidden layers [95], [96], [97], or", "startOffset": 96, "endOffset": 100}, {"referenceID": 92, "context": "the adaptation can be conducted on the input layer [93], [94], the activations of hidden layers [95], [96], [97], or", "startOffset": 102, "endOffset": 106}, {"referenceID": 93, "context": "the adaptation can be conducted on the input layer [93], [94], the activations of hidden layers [95], [96], [97], or", "startOffset": 108, "endOffset": 112}, {"referenceID": 90, "context": "the output layer [94].", "startOffset": 17, "endOffset": 21}, {"referenceID": 94, "context": "Some comparison for adaptation on different components can be found in [98], [99].", "startOffset": 71, "endOffset": 75}, {"referenceID": 95, "context": "Some comparison for adaptation on different components can be found in [98], [99].", "startOffset": 77, "endOffset": 81}, {"referenceID": 96, "context": "to constrain the adaptation more aggressively, [100], [101] studied a singular value decomposition (SVD) approach which decomposes a weight matrix as production of low rank matri-", "startOffset": 47, "endOffset": 52}, {"referenceID": 97, "context": "to constrain the adaptation more aggressively, [100], [101] studied a singular value decomposition (SVD) approach which decomposes a weight matrix as production of low rank matri-", "startOffset": 54, "endOffset": 59}, {"referenceID": 98, "context": "the form of KL-divergence [102].", "startOffset": 26, "endOffset": 31}, {"referenceID": 10, "context": "(CMLLR) in the HMM-GMM architecture [13].", "startOffset": 36, "endOffset": 40}, {"referenceID": 99, "context": "Recently, an auto-encoder trained with speaker vectors (obtained from a regression-based speaker transform) was used to produce speaker-independent BN features [103].", "startOffset": 160, "endOffset": 165}, {"referenceID": 100, "context": "was studied in [104], though an i-vector was used as the speaker representation.", "startOffset": 15, "endOffset": 20}, {"referenceID": 101, "context": "For example, [105] reported an extensive study on speaker adaptation for LSTMs.", "startOffset": 13, "endOffset": 18}, {"referenceID": 100, "context": ", a speaker-aware DNN projection [104]), or by inserting speaker-dependent layers.", "startOffset": 33, "endOffset": 38}, {"referenceID": 95, "context": "was demonstrated by [99], which showed that the adaptation methods provide some improvement if the network is small", "startOffset": 20, "endOffset": 24}, {"referenceID": 102, "context": ", [106], [107], [108], [109].", "startOffset": 2, "endOffset": 7}, {"referenceID": 103, "context": ", [106], [107], [108], [109].", "startOffset": 9, "endOffset": 14}, {"referenceID": 104, "context": ", [106], [107], [108], [109].", "startOffset": 16, "endOffset": 21}, {"referenceID": 105, "context": ", [106], [107], [108], [109].", "startOffset": 23, "endOffset": 28}, {"referenceID": 103, "context": ", by state mapping [107], [110], [111].", "startOffset": 19, "endOffset": 24}, {"referenceID": 106, "context": ", by state mapping [107], [110], [111].", "startOffset": 26, "endOffset": 31}, {"referenceID": 107, "context": ", by state mapping [107], [110], [111].", "startOffset": 33, "endOffset": 38}, {"referenceID": 108, "context": "synthesis [112], [113], [114], it is relatively new and the adaptation methods have not been extensively studied, except", "startOffset": 10, "endOffset": 15}, {"referenceID": 109, "context": "synthesis [112], [113], [114], it is relatively new and the adaptation methods have not been extensively studied, except", "startOffset": 17, "endOffset": 22}, {"referenceID": 110, "context": "synthesis [112], [113], [114], it is relatively new and the adaptation methods have not been extensively studied, except", "startOffset": 24, "endOffset": 29}, {"referenceID": 111, "context": "a few exceptions [115], [116].", "startOffset": 17, "endOffset": 22}, {"referenceID": 112, "context": "a few exceptions [115], [116].", "startOffset": 24, "endOffset": 29}, {"referenceID": 8, "context": "This was mentioned in the seminal paper of multitask learning [11] and was recently", "startOffset": 62, "endOffset": 66}, {"referenceID": 113, "context": "rediscovered by several researchers in the context of deep learning [117], [10], [118].", "startOffset": 68, "endOffset": 73}, {"referenceID": 7, "context": "rediscovered by several researchers in the context of deep learning [117], [10], [118].", "startOffset": 75, "endOffset": 79}, {"referenceID": 114, "context": "rediscovered by several researchers in the context of deep learning [117], [10], [118].", "startOffset": 81, "endOffset": 86}, {"referenceID": 113, "context": "Ba [117] teaches the child model by encouraging its logits (activations before softmax) close to those generated by the", "startOffset": 3, "endOffset": 8}, {"referenceID": 7, "context": "teacher model in terms of square error, and the dark knowledge distiller model proposed by Hinton [10] encourages the output", "startOffset": 98, "endOffset": 102}, {"referenceID": 114, "context": "For example, [118] utilized the output of a complex DNN as regularization to learn a small DNN that is suitable for speech recognition on mobile devices.", "startOffset": 13, "endOffset": 18}, {"referenceID": 115, "context": "[119] used a complex RNN to train a DNN.", "startOffset": 0, "endOffset": 5}, {"referenceID": 116, "context": "Recently, a new architecture called FitNet was proposed [120].", "startOffset": 56, "endOffset": 61}, {"referenceID": 117, "context": "This work was further extended in [121], where multiple hidden layers were regularized by the teacher", "startOffset": 34, "endOffset": 39}, {"referenceID": 118, "context": "For instance, in [122], unsupervised learning models (PCA and", "startOffset": 17, "endOffset": 22}, {"referenceID": 32, "context": "Our recent work [35] showed that this model transfer can not only learn simple models from complex models, but also", "startOffset": 16, "endOffset": 20}, {"referenceID": 32, "context": "In our work [35], a DNN model was used to train a powerful complex RNN.", "startOffset": 12, "endOffset": 16}, {"referenceID": 33, "context": "In a related work [36], we", "startOffset": 18, "endOffset": 22}, {"referenceID": 121, "context": "the second language [125].", "startOffset": 20, "endOffset": 25}, {"referenceID": 122, "context": "in [126] to translate a maximum entropy (ME) classifier trained in English data to a classifier used for classifying Chinese documents.", "startOffset": 3, "endOffset": 8}, {"referenceID": 123, "context": "In another work from our group, we have applied this approach successfully to train multilingual language models, where some foreign words need to be addressed [127].", "startOffset": 160, "endOffset": 165}, {"referenceID": 124, "context": "A more complicated approach is to translate the whole sentence by machine translation [128], so that any labelling or classification tasks in one language can be conducted with models trained in another language.", "startOffset": 86, "endOffset": 91}, {"referenceID": 125, "context": "For example, the multilingual LDA model proposed in [129] assumes a common latent topic space, so that words from multiple languages can share the same topics.", "startOffset": 52, "endOffset": 57}, {"referenceID": 24, "context": "This is similar to the RMB-based heterogeneous factor learning [27]: both are based on unsupervised learning with weak supervision, i.", "startOffset": 63, "endOffset": 67}, {"referenceID": 126, "context": "A similar approach proposed in [130] learns multilingual word clusters, where a cluster may involve words from different languages.", "startOffset": 31, "endOffset": 36}, {"referenceID": 127, "context": "This work was extend in [131] where cross-lingual clusters were used to \u2018directly\u2019 transfer an NER model trained in the source language to the target language.", "startOffset": 24, "endOffset": 29}, {"referenceID": 26, "context": "For example, in the heterogeneous feature augmentation (HFA) approach proposed in [29], two linear projections are learned to project features in different languages to a common space.", "startOffset": 82, "endOffset": 86}, {"referenceID": 21, "context": "In another work [24], a linear projection was", "startOffset": 16, "endOffset": 20}, {"referenceID": 128, "context": "Recently, word embedding becomes a hot topic [132], [133],", "startOffset": 45, "endOffset": 50}, {"referenceID": 129, "context": "Recently, word embedding becomes a hot topic [132], [133],", "startOffset": 52, "endOffset": 57}, {"referenceID": 130, "context": "[134], [135], [136].", "startOffset": 0, "endOffset": 5}, {"referenceID": 131, "context": "[134], [135], [136].", "startOffset": 7, "endOffset": 12}, {"referenceID": 132, "context": "[134], [135], [136].", "startOffset": 14, "endOffset": 19}, {"referenceID": 133, "context": "For example, in [137], it was found that a linear transform can project word vectors trained in one languages to word vectors in another language so that relevant words are put closely, in spite of their languages.", "startOffset": 16, "endOffset": 21}, {"referenceID": 134, "context": "We extended this work in [138] by modeling the transfer as an orthogonal transform.", "startOffset": 25, "endOffset": 30}, {"referenceID": 135, "context": "A more systematic approach was proposed by [139], where different languages were projected to the same space by", "startOffset": 43, "endOffset": 48}, {"referenceID": 136, "context": "The approach proposed in [140] does not learn any projection, instead the bi-lingual correspondence was taken into account in the embedding process.", "startOffset": 25, "endOffset": 30}, {"referenceID": 128, "context": "This work was based on the neural LM model [132] and changed the objective function by involving an extra term that encourages relevant words in different languages located together.", "startOffset": 43, "endOffset": 48}, {"referenceID": 137, "context": "In another work [141], the relevance constraint was employed at the sentence level.", "startOffset": 16, "endOffset": 21}, {"referenceID": 138, "context": "A similar work was proposed by [142].", "startOffset": 31, "endOffset": 36}, {"referenceID": 137, "context": "As in [141], only", "startOffset": 6, "endOffset": 11}, {"referenceID": 27, "context": "An interesting research that involves much ingredient of deep learning was proposed by [30].", "startOffset": 87, "endOffset": 91}, {"referenceID": 139, "context": "For example on the semantic relatedness task, [143] concatenated visual", "startOffset": 46, "endOffset": 51}, {"referenceID": 140, "context": "and textual features to train multi-stream systems; in [144], the scores predicted by multiple models based on different features are combined.", "startOffset": 55, "endOffset": 60}, {"referenceID": 27, "context": ", [30].", "startOffset": 2, "endOffset": 6}, {"referenceID": 22, "context": "An example is the work proposed in [25], where the authors used co-occurrence data to estimate the correspondence", "startOffset": 35, "endOffset": 39}, {"referenceID": 141, "context": "The common latent space approach was studied in [145], with the task of image segmentation and labelling.", "startOffset": 48, "endOffset": 53}, {"referenceID": 51, "context": "Deep learning provides an elegant way for cross-domain transfer learning, with its great power in learning high-level representations shared by multiple modalities [54].", "startOffset": 164, "endOffset": 168}, {"referenceID": 59, "context": "For example, in [62], [146], images and words are embedded in the same low-dimensional space via neural networks, by which image classification can be improved by the word embedding, even for classes without any image training data.", "startOffset": 16, "endOffset": 20}, {"referenceID": 142, "context": "For example, in [62], [146], images and words are embedded in the same low-dimensional space via neural networks, by which image classification can be improved by the word embedding, even for classes without any image training data.", "startOffset": 22, "endOffset": 27}, {"referenceID": 143, "context": "[147] proposed a multi-modal neural language modeling approach with which the history and prediction can be both text and images, so that the prediction between multiple modalities becomes possible.", "startOffset": 0, "endOffset": 5}, {"referenceID": 144, "context": "In [148], an RNN structure based on dependency-tree was proposed to embed textual sentences into compositional vectors, which were then projected together with image representations to a common space.", "startOffset": 3, "endOffset": 8}, {"referenceID": 145, "context": "The same idea was proposed by [149], though deep Boltzmann machines were used instead of DNNs to infer the common latent space.", "startOffset": 30, "endOffset": 35}, {"referenceID": 146, "context": "recent work [150] studied a knowledge distilling approach on the sentiment classification task.", "startOffset": 12, "endOffset": 17}, {"referenceID": 147, "context": "In a recent study [151], we show that it is possible to learn a neural model using supervision from a Bayesian model.", "startOffset": 18, "endOffset": 23}, {"referenceID": 4, "context": ", the relatedness between marginal and conditional distributions [6] in unsupervised feature learning, or rep-", "startOffset": 65, "endOffset": 68}, {"referenceID": 35, "context": "resentation overlap in model adaptation [38].", "startOffset": 40, "endOffset": 44}, {"referenceID": 36, "context": ", speech recognition and speaker recognition), transfer learning still works by utilizing the fact that the tasks are unrelated [39].", "startOffset": 128, "endOffset": 132}, {"referenceID": 148, "context": ", [152], [38].", "startOffset": 2, "endOffset": 7}, {"referenceID": 35, "context": ", [152], [38].", "startOffset": 9, "endOffset": 13}, {"referenceID": 20, "context": "For example, any data in related domains can be used to assist learning abstract features, even they are sampled from a distribution different from the target domain [23].", "startOffset": 166, "endOffset": 170}], "year": 2015, "abstractText": "Transfer learning is a vital technique that generalizes models trained for one setting or task to other settings or tasks. For example in speech recognition, an acoustic model trained for one language can be used to recognize speech in another language, with little or no re-training data. Transfer learning is closely related to multi-task learning (cross-lingual vs. multilingual), and is traditionally studied in the name of \u2018model adaptation\u2019. Recent advance in deep learning shows that transfer learning becomes much easier and more effective with high-level abstract features learned by deep models, and the \u2018transfer\u2019 can be conducted not only between data distributions and data types, but also between model structures (e.g., shallow nets and deep nets) or even model types (e.g., Bayesian models and neural models). This review paper summarizes some recent prominent research towards this direction, particularly for speech and language processing. We also report some results from our group and highlight the potential of this very interesting research field.", "creator": "LaTeX with hyperref package"}}}