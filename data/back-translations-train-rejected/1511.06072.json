{"id": "1511.06072", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "19-Nov-2015", "title": "Mediated Experts for Deep Convolutional Networks", "abstract": "We present a new supervised architecture termed Mediated Mixture-of-Experts (MMoE) that allows us to improve classification accuracy of Deep Convolutional Networks (DCN). Our architecture achieves this with the help of expert networks: A network is trained on a disjoint subset of a given dataset and then run in parallel to other experts during deployment. A mediator is employed if experts contradict each other. This allows our framework to naturally support incremental learning, as adding new classes requires (re-)training of the new expert only. We also propose two measures to control computational complexity: An early-stopping mechanism halts experts that have low confidence in their prediction. The system allows to trade-off accuracy and complexity without further retraining. We also suggest to share low-level convolutional layers between experts in an effort to avoid computation of a near-duplicate feature set. We evaluate our system on a popular dataset and report improved accuracy compared to a single model of same configuration.", "histories": [["v1", "Thu, 19 Nov 2015 07:01:36 GMT  (124kb,D)", "http://arxiv.org/abs/1511.06072v1", null]], "reviews": [], "SUBJECTS": "cs.LG cs.NE", "authors": ["sebastian agethen", "winston h hsu"], "accepted": false, "id": "1511.06072"}, "pdf": {"name": "1511.06072.pdf", "metadata": {"source": "CRF", "title": null, "authors": ["Sebastian Agethen", "Winston H. Hsu"], "emails": [], "sections": [{"heading": null, "text": "We present a new supervised architecture called Mediated Mixture-of-Experts (MMoE), which enables us to improve the classification accuracy of Deep Convolutional Networks (DCN). Our architecture achieves this through expert networks: A network is trained on a fragmented subset of a given data set and then runs in parallel with other experts during the deployment. A mediator is used when experts disagree, which allows our framework to naturally support gradual learning, since adding new classes requires only one (re) training of the new expert. We also propose two measures to control the complexity of calculations: An early stop mechanism stops experts who have little confidence in their predictions; the system allows us to compare accuracy and complexity with each other without further retraining; we also propose sharing low configuration layers between experts in order to avoid creating a near-duplicate feature set. We evaluate our system by comparing a popular data set to an improved configuration model."}, {"heading": "1 INTRODUCTION", "text": "Over the past decade, we have seen an increase in popularity among researchers. While the application of early DCN was limited to simple tasks such as handwritten number recognition, modern methods can fulfill or exceed human performance in much more complex ways. (2015); Krizhevsky et al. (2012); Szegedy et al. (2014) Two factors that have contributed to this development are the availability of large-format image corporations such as ImageNet Russakovsky et al. (2014); and the widespread availability of GPU-based computer systems based on the availability of image corporations."}, {"heading": "2 RELATED WORK", "text": "In this paper, our choice is the popular ImageNet et al. (2012) There has been a significant increase in classifications in terms of the classes obtained, i.e. there has been a top-1 accuracy in terms of the data obtained. (2012) There has been a significant increase in terms of the data obtained, i.e. there has been a top-1 accuracy of 62.5% showing the potential of the data obtained. (2012) There has been a significant increase in terms of the data acquired. (2012) There has been a significant increase in terms of the data acquired. (2012)"}, {"heading": "3 PROPOSED METHOD", "text": "In tackling a problem, a \"divide-and-rule\" approach can often be helpful: in deep learning, we can train experts on small problems to improve accuracy; this was applied in MoE Jacobs et al. (1991): experts are trained competitively, in the hope that they will automatically learn discriminatory traits; a gating network combines the predictive results; each expert's field of expertise can also be designed with the help of prior knowledge; and in order to gain this knowledge, we can apply two methods: spectral clustering (as in Xiao et al. (2014)) and explicit hierarchies (as in the ImageNet dataset), see Section 4.1. In both cases, the i-th expert is then trained exclusively on the subset of data in that superclass that we call Ci."}, {"heading": "3.1 SIMPLE BRANCHING MODEL", "text": "We briefly discuss a branching network like in Xiao et al. (2014), which predicts the superclass of an image. The branching network is a DCN like AlexNet Krizhevsky et al. (2012), in which the number of outputs in the classifier is reduced to N, the number of superclasses. In the branching decision, the expert corresponding to the highest activation is selected, and then a fine-grained prediction is obtained. We trained such a system with two Slim1 experts, as in Table 1. Although we used only two experts, the classification accuracy suffered from major branch errors: If the wrong expert is selected, the error cannot be restored. Nevertheless, we also see that individual experts demonstrate superior performance and confirm that we are on the right track. The model we discuss below is an attempt to reduce branch errors."}, {"heading": "3.2 BRANCHED EXPERTS WITH EARLY STOPPING", "text": "We propose the following framework, as shown in Fig. 1: The input is guided through a number of revolutionary layers to generate general characteristics as defined in Yosinski et al. (2014). These layers are shared by all experts and could, in the case of CaffeNet, comprise the first two or three layers. Characteristics of higher layers have a higher degree of specialization and therefore need to be finely tuned for each expert. To this end, we propose a confidence module that determines whether the expert is able to solve the problem. In particular, an expert i is considered to be of low confidence (in layer j) in the hope of keeping the parameter complexity low. To this end, we propose a confidence module that determines whether the expert is able to solve the problem."}, {"heading": "3.3 MEDIATOR", "text": "Given the fact that the inequality in (1) applies, an expert is considered untrustworthy and is stopped, and its corresponding activations in the last layer are set to zero. So far, the system described above performs slightly worse than a single model with the same configuration. This is due to the cases in which more than one expert is active: an expert trained in another superclass may \"confuse\" the input for a particular fine-grained class in his own area, leading to conflicting opinions. In these cases, we found it helpful to add an intermediary, that is, a slim model trained in all 1000 classes to evaluate opinions, see Figure 2. The resulting architecture then exceeds the individual model baseline. Finally, the softmax probabilities of each expert are weighted and averaged: The slim 1000-class intermediary is added only if more than one expert is executed and weighted with wMed."}, {"heading": "4 EVALUATION", "text": "The evaluation is based on the ImageNet 1K dataset. We begin with the description of the superclasses assigned to each expert."}, {"heading": "4.1 SUPERCLASS CONSTRUCTION", "text": "Superclasses can be defined automatically or manually: in the first case, spectral clustering, as practiced in Xiao et al. (2014), is a suitable choice. Alternatively, we can traverse the hierarchy available with ImageNet and combine several conceptually related leaf classes with each other, offering a practical advantage in an incremental learning scenario: we simply train one or more new experts for new data. In contrast, using spectral clusters may require retraining some of the existing experts.For space reasons, we limit ourselves to manually defined superclasses and N = 2. We select C1 as an \"artifact\" (with synthesis: n00021939) and C2 as all remaining classes, resulting in a division of 517 compared to 483 leaf classes, each containing approximately 660K and 620K images. In other words, this simulates the addition of \"artifact data\" to an existing model."}, {"heading": "4.2 RESULTS ON HIERARCHICAL SET", "text": "We evaluate our system in three configurations: firstly, a lean configuration of CaffeNet, where both layers are reduced to 512 neurons; secondly, we focus on the FC7 layer as a whole, while we use the CaffeNet (2014) layer. System accuracy is enhanced by the top-1 classification in all three configurations; the result is highly dependent on the value of the T layer, see also the CaffeNet model, see CaffeNet (2014); and system accuracy is improved by the top-1 classification in all three configurations."}, {"heading": "5 CONCLUSION AND FUTURE WORK", "text": "We propose a mediated expert system for deep Convolutionary Networks that allows experts to learn on small partitions of a training set, a case that occurs in an incremental learning scenario. Specifically, experts can be stopped early if the breakpoint is controlled by a single hyperparameter, allowing them to adapt to different circumstances in terms of resource availability. In addition, we avoid the branch error that occurs in the previous work in training on distributed data sets. To better underline the usefulness of our proposal, two points remain for our future work: firstly, a more thorough evaluation is needed, which was not possible in this paper. Secondly, we believe that the mediator concept can be further developed to reduce the computational complexity to a higher level."}], "references": [{"title": "Handwritten digit recognition with a back-propagation network", "author": ["Cun", "Le", "B. Boser", "J.S. Denker", "D. Henderson", "R.E. Howard", "W. Hubbard", "L.D. Jackel"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Cun et al\\.,? \\Q1990\\E", "shortCiteRegEx": "Cun et al\\.", "year": 1990}, {"title": "Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification", "author": ["K. He", "X. Zhang", "S. Ren", "J. Sun"], "venue": null, "citeRegEx": "He et al\\.,? \\Q2015\\E", "shortCiteRegEx": "He et al\\.", "year": 2015}, {"title": "Adaptive mixtures of local experts", "author": ["Jacobs", "Robert A", "Jordan", "Michael I", "Nowlan", "Steven J", "Hinton", "Geoffrey E"], "venue": "Neural Comput.,", "citeRegEx": "Jacobs et al\\.,? \\Q1991\\E", "shortCiteRegEx": "Jacobs et al\\.", "year": 1991}, {"title": "Imagenet classification with deep convolutional neural networks", "author": ["Krizhevsky", "Alex", "Sutskever", "Ilya", "Hinton", "Geoffrey E"], "venue": "Advances in Neural Information Processing Systems", "citeRegEx": "Krizhevsky et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Krizhevsky et al\\.", "year": 2012}, {"title": "Error-driven incremental learning in deep convolutional neural network for large-scale image classification", "author": ["Xiao", "Tianjun", "Zhang", "Jiaxing", "Yang", "Kuiyuan", "Peng", "Yuxin", "Zheng"], "venue": "In Proceedings of the ACM International Conference on Multimedia, MM", "citeRegEx": "Xiao et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Xiao et al\\.", "year": 2014}, {"title": "How transferable are features in deep neural networks", "author": ["Yosinski", "Jason", "Clune", "Jeff", "Bengio", "Yoshua", "Lipson", "Hod"], "venue": "CoRR, abs/1411.1792,", "citeRegEx": "Yosinski et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Yosinski et al\\.", "year": 2014}, {"title": "Visualizing and understanding convolutional networks", "author": ["Matthew D", "Fergus", "Rob"], "venue": null, "citeRegEx": "Zeiler et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Zeiler et al\\.", "year": 2002}], "referenceMentions": [{"referenceID": 0, "context": "While the application of early DCN was limited to simple tasks such as hand-written digit recognition Cun et al. (1990), modern state-of-the-art methods can meet or exceed human-level performance on far more complex tasks, including generic image classification He et al.", "startOffset": 102, "endOffset": 120}, {"referenceID": 0, "context": "While the application of early DCN was limited to simple tasks such as hand-written digit recognition Cun et al. (1990), modern state-of-the-art methods can meet or exceed human-level performance on far more complex tasks, including generic image classification He et al. (2015); Krizhevsky et al.", "startOffset": 102, "endOffset": 279}, {"referenceID": 0, "context": "While the application of early DCN was limited to simple tasks such as hand-written digit recognition Cun et al. (1990), modern state-of-the-art methods can meet or exceed human-level performance on far more complex tasks, including generic image classification He et al. (2015); Krizhevsky et al. (2012); Szegedy et al.", "startOffset": 102, "endOffset": 305}, {"referenceID": 0, "context": "While the application of early DCN was limited to simple tasks such as hand-written digit recognition Cun et al. (1990), modern state-of-the-art methods can meet or exceed human-level performance on far more complex tasks, including generic image classification He et al. (2015); Krizhevsky et al. (2012); Szegedy et al. (2014). Two factors that have contributed to this development are the availability of large-scale image corpora like ImageNet Russakovsky et al.", "startOffset": 102, "endOffset": 328}, {"referenceID": 0, "context": "While the application of early DCN was limited to simple tasks such as hand-written digit recognition Cun et al. (1990), modern state-of-the-art methods can meet or exceed human-level performance on far more complex tasks, including generic image classification He et al. (2015); Krizhevsky et al. (2012); Szegedy et al. (2014). Two factors that have contributed to this development are the availability of large-scale image corpora like ImageNet Russakovsky et al. (2014), and the wide-spread availability of GPU-based computing hardware which renders computationally expensive model training procedures feasible.", "startOffset": 102, "endOffset": 473}, {"referenceID": 0, "context": "While the application of early DCN was limited to simple tasks such as hand-written digit recognition Cun et al. (1990), modern state-of-the-art methods can meet or exceed human-level performance on far more complex tasks, including generic image classification He et al. (2015); Krizhevsky et al. (2012); Szegedy et al. (2014). Two factors that have contributed to this development are the availability of large-scale image corpora like ImageNet Russakovsky et al. (2014), and the wide-spread availability of GPU-based computing hardware which renders computationally expensive model training procedures feasible. Experts systems are known to improve the classification accuracy of neural networks even further and have been studied extensively. Previous work on Mixture of Experts (MoE) Jacobs et al. (1991) is however flawed in two aspects: First, computational complexity in MoE is a multiple of that of a traditional model.", "startOffset": 102, "endOffset": 810}, {"referenceID": 0, "context": "While the application of early DCN was limited to simple tasks such as hand-written digit recognition Cun et al. (1990), modern state-of-the-art methods can meet or exceed human-level performance on far more complex tasks, including generic image classification He et al. (2015); Krizhevsky et al. (2012); Szegedy et al. (2014). Two factors that have contributed to this development are the availability of large-scale image corpora like ImageNet Russakovsky et al. (2014), and the wide-spread availability of GPU-based computing hardware which renders computationally expensive model training procedures feasible. Experts systems are known to improve the classification accuracy of neural networks even further and have been studied extensively. Previous work on Mixture of Experts (MoE) Jacobs et al. (1991) is however flawed in two aspects: First, computational complexity in MoE is a multiple of that of a traditional model. To this end, we propose early-stopping of experts. Our mechanism evaluates each expert\u2019s confidence and is influenced by a hyper-parameter, allowing a trade-off between complexity and classification accuracy without the need for retraining. Second, as recently pointed out in Xiao et al. (2014), training becomes more difficult as datasets are getting larger.", "startOffset": 102, "endOffset": 1224}, {"referenceID": 2, "context": "AlexNet by Krizhevsky et al. Krizhevsky et al. (2012) achieved a significant increase in classification performance over traditional methods on ImageNet, i.", "startOffset": 11, "endOffset": 54}, {"referenceID": 2, "context": "AlexNet by Krizhevsky et al. Krizhevsky et al. (2012) achieved a significant increase in classification performance over traditional methods on ImageNet, i.e., an top-1 accuracy of 62.5%, showing the potential of deep learning for large datasets such as ImageNet. Their success is arguably founded on three factors: Depth, Rectified Linear Units (ReLU) as non-linearity and data augmentations. We use CaffeNet, a variation of this model, as a baseline to evaluate our system. A hierarchical architecture for incremental learning was presented in Xiao et al. (2014). The authors suggested to use a Branching layer to determine the superclass a specific problem belongs to.", "startOffset": 11, "endOffset": 565}, {"referenceID": 2, "context": "Mixture of experts (MoE), as first proposed in Jacobs et al. (1991), have been well-known for a while.", "startOffset": 47, "endOffset": 68}, {"referenceID": 2, "context": "Mixture of experts (MoE), as first proposed in Jacobs et al. (1991), have been well-known for a while. A large deal of work has been done in this area and an overview can be found in Yuksel et al. (2012). Generally, a number N of expert networks are trained together on a dataset.", "startOffset": 47, "endOffset": 204}, {"referenceID": 3, "context": "151% \u2013 \u2013 Baseline Krizhevsky et al. (2012) 49.", "startOffset": 18, "endOffset": 43}, {"referenceID": 5, "context": "The work of Yosinski et al. (2014) discusses generality and specialization of convolutional features, giving insight into how deep learning works.", "startOffset": 12, "endOffset": 35}, {"referenceID": 5, "context": "The work of Yosinski et al. (2014) discusses generality and specialization of convolutional features, giving insight into how deep learning works. The same phenomenon has also been discussed in Zeiler & Fergus (2013) in the context of network visualization.", "startOffset": 12, "endOffset": 217}, {"referenceID": 2, "context": "This has been used in MoE Jacobs et al. (1991): Experts are trained competitively in the hope that they automatically learn discriminant features.", "startOffset": 26, "endOffset": 47}, {"referenceID": 2, "context": "This has been used in MoE Jacobs et al. (1991): Experts are trained competitively in the hope that they automatically learn discriminant features. A gating network combines the prediction results. Each expert\u2019s area of expertise can also be designed with help of prior knowledge. We can utilize two methods to gain this knowledge: Spectral Clustering (as done in Xiao et al. (2014)) and explicit hierarchies (such as the ImageNet dataset provides), see also Sec.", "startOffset": 26, "endOffset": 382}, {"referenceID": 3, "context": "We briefly discuss a branching network as in Xiao et al. (2014), which predicts the superclass of an image.", "startOffset": 45, "endOffset": 64}, {"referenceID": 3, "context": "The branching network is a DCN such as AlexNet Krizhevsky et al. (2012) with the number of outputs in the classifier reduced to N , the number of superclasses.", "startOffset": 47, "endOffset": 72}, {"referenceID": 5, "context": "1: The input is passed through a number of convolutional layers to generate general features, as defined in Yosinski et al. (2014). These layers are shared between all experts and could encompass the first two or three layers in the case of CaffeNet.", "startOffset": 108, "endOffset": 131}, {"referenceID": 4, "context": "In order to update the mediator network, we can simply add extra neurons to the output and finetune the network, see the Flat Increment technique in Xiao et al. (2014).", "startOffset": 149, "endOffset": 168}, {"referenceID": 4, "context": "Superclasses can be defined in an automated or manual fashion: In the former case Spectral Clustering, as was done in Xiao et al. (2014), is a suitable choice.", "startOffset": 118, "endOffset": 137}], "year": 2015, "abstractText": "We present a new supervised architecture termed Mediated Mixture-of-Experts (MMoE) that allows us to improve classification accuracy of Deep Convolutional Networks (DCN). Our architecture achieves this with the help of expert networks: A network is trained on a disjoint subset of a given dataset and then run in parallel to other experts during deployment. A mediator is employed if experts contradict each other. This allows our framework to naturally support incremental learning, as adding new classes requires (re-)training of the new expert only. We also propose two measures to control computational complexity: An early-stopping mechanism halts experts that have low confidence in their prediction. The system allows to trade-off accuracy and complexity without further retraining. We also suggest to share low-level convolutional layers between experts in an effort to avoid computation of a near-duplicate feature set. We evaluate our system on a popular dataset and report improved accuracy compared to a single model of same configuration.", "creator": "LaTeX with hyperref package"}}}