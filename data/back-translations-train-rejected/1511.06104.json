{"id": "1511.06104", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "19-Nov-2015", "title": "Semi-supervised Learning for Convolutional Neural Networks via Online Graph Construction", "abstract": "The recent promising achievements of deep learning rely on the large amount of labeled data. Considering the abundance of data on the web, most of them do not have labels at all. Therefore, it is important to improve generalization performance using unlabeled data on supervised tasks with few labeled instances. In this work, we revisit graph-based semi-supervised learning algorithms and propose an online graph construction technique which suits deep convolutional neural network better. We consider an EM-like algorithm for semi-supervised learning on deep neural networks: In forward pass, the graph is constructed based on the network output, and the graph is then used for loss calculation to help update the network by back propagation in the backward pass. We demonstrate the strength of our online approach compared to the conventional ones whose graph is constructed on static but not robust enough feature representations beforehand.", "histories": [["v1", "Thu, 19 Nov 2015 09:44:57 GMT  (737kb,D)", "http://arxiv.org/abs/1511.06104v1", null], ["v2", "Tue, 19 Jan 2016 00:56:08 GMT  (0kb,I)", "http://arxiv.org/abs/1511.06104v2", "As the original submission of iclr is withdrawn, the arxiv submission should be withdrawn as well"]], "reviews": [], "SUBJECTS": "cs.NE cs.CV cs.LG", "authors": ["sheng-yi bai", "sebastian agethen", "ting-hsuan chao", "winston hsu"], "accepted": false, "id": "1511.06104"}, "pdf": {"name": "1511.06104.pdf", "metadata": {"source": "CRF", "title": "ONLINE GRAPH CONSTRUCTION", "authors": ["Sheng-Yi Bai", "Sebastian Agethen", "Ting-Hsuan Chao"], "emails": ["r02922161@ntu.edu.tw", "d01944015@ntu.edu.tw", "r02922047@ntu.edu.tw", "whsu@ntu.edu.tw"], "sections": [{"heading": null, "text": "The promising achievements of deep learning are based on the large amount of marked data. Considering the abundance of data on the web, most of them do not have labels at all. Therefore, it is important to improve generalization performance by using unmarked data on monitored tasks with few marked instances. In this work, we return to graph-based semi-monitored learning algorithms and propose an online graph construction technique that is better suited to deep, revolutionary neural networks. We are looking at an EM-like algorithm for semi-monitored learning in deep neural networks: in the run-up, the graph is constructed on the basis of network output, and the graph is then used for loss calculation to update the network by backward propagation in reverse. We demonstrate the strength of our online approach compared to traditional ones, whose graph is built on static but not robust enough feature representations in the preview."}, {"heading": "1 INTRODUCTION", "text": "Recently, profound learning has been shown to be a major success in the areas of image (Szegedy et al., 2015), video (Karpathy et al., 2014), text (Severyn & Moschitti, 2015), language (Sak et al., 2014), etc. Specifically, it is about folding neural networks (CNNs), which combine the learning of features and classifications, and that is the most common practice for image and video recognition problems in recent times. For example, it is impracticable to collect large-scale data sets with high-quality images. (Szegedy et al., 2015) showed promising results on ImageNet, which is a large amount of manually submitted data."}, {"heading": "2 RELATED WORK", "text": "Since the literature on semi-supervised learning is abundant, we will focus on the key work that is more specific to deep learning or inspirational for this work in this section. Recently, several papers have addressed the problems of training on intoxicatingly labeled image data using weakly supervised approaches. Mnih & Hinton (2012) trained deep neural networks for learning to mark aerial images and proposed two robust loss functions to deal with incomplete labeling and error registration. Sukhbaatar et al. (2015) developed a bottom-up approach to solve the potential problems of training on intoxicatingly labeled data by adding a linear layer to CNN to model the sound distribution. Grape et al. (2015) proposed a bootstrapping loss function with special emphasis on predictive consistency, so that similar points lead to the same preliminary results of automated learning on the other side."}, {"heading": "3 GRAPH-BASED SEMI-SUPERVISED LEARNING", "text": "Our goal is to use blank data for supervised classification tasks that are confronted with both labeled data pairs (XL, YL) = {(x1, y1), (x2, y2),..., (xl, yl)} and unlabeled data XU = {xl + 1, xl + 2,.., xn}, where vertices V represent all available data points x1,..., xn and edges E by a n \u00d7 n neighborhood matrix W. Input field Wij represents the edge weight between node i and node j and thus the similarity or dissimilarity between them. Detailed discussion will be held in the following paragraphs. In this section, we will first talk about the advantages of this graph combination, then introduce the pseudo function."}, {"heading": "3.1 ONLINE VS. OFFLINE GRAPH CONSTRUCTION", "text": "There are many heuristics for constructing the adjacence matrix, for example, (1) share a higher value of weight between adjacent frames than other frames in video (Weston et al., 2008), and (2) set image pairs of different views from the same object / identity Wij = 1, and otherwise Wij = 0 (Bell & Bala, 2015; Khalil-Hani & Sung, 2014; Lin et al., 2015). However, these types of graph construction are done before the learning process begins, that is, serves as input for pre-processing or training, and therefore we consider them as offline graph construction in contrast to the technique we propose below. Our proposal is to construct the graph online on a local scale, i.e., on a few examples of X that are virtually equivalent to the concept of mini-batch in the formation of modern neural networks."}, {"heading": "3.2 APPROACHES TO CONSTRUCT ADJACENCY MATRICES", "text": "(2008). (2008). (2008). (2008). (2008). (2008). (2008). (2012). (2012). (2012). (2012). (2012). (2012). (2012). (2012). (2012). (2012). (2012). (2012). (2012). (2012). (2012). (2012). (2012). (2012). (2012). (2012). (2012). (2012). (2012). (2012). (2012). (2012). (2012). (2012). (2012). (2012). (2012). (2012). \"(2012). (2012). (2012). (2012). (2012). (2012). (2012). (2012). (2012). (2012). (2012). (2012). (2012). (2012). (2012). (2012). (2012). (2012). (2012). (2012). (2012). (2012). (2012). (2012). (2012). (2012). (2012). (2012). (2012). (2012). (2012). (2012). (2012). (2012). (2012). (2012). (2012). (2012). (2012). (2012). (2012). (2012). (2012).). (2012). (2012).). (2012). (2012). (2012). (2012). (2012).). (2012). (2012). (2012).). (2012). (2012). (2012). (2012). (2012). (2012). (2012). (2012). (2012).). (2012). (2012). (2012). (2012).). (2012). (2012). (2012). (2012). (2012). (2012). (2012).). (2012). (2012). (2012). (2012). (2012). (2012). (2012). (2012). (2012). (2012). (2012). (2012)"}, {"heading": "3.3 SEMI-SUPERVISED LOSS FUNCTION", "text": "Now that we have all the ingredients we need, the loss function of the network is defined as follows: L = l \u2211 i = 1 Lc (fi, yi) + \u03bb n \u2211 i, j = 1 Lcont (fi, fj, Wij) (6) in general, where Lc is classification loss (softmax loss is often used), fi is the network performance of xi, and \u03bb is the balancing term. But we hope that the number of similar and unequal pairs will be balanced, and describe our approach as follows: In each batch, for each example i, we randomly have 3Probably room for further optimization, but the bottleneck is in self-value degradation. Sample two examples j, k with the constraints, the Wij = 1 and Wik = 0. Then we rewrite the loss function from the perspective of the mini batch: L = l \u00b2 label i = 1 Lc label (fi, yi) and the ability of Lc (yi)."}, {"heading": "4 EXPERIMENTAL RESULTS", "text": "In our experiments, we train two CNNs with labeled data only in a supervised manner. For MNIST, there are 60,000 training images. We use 600 labeled samples for supervised training and consider the remaining data as unlabeled for later use. Similarly, 50,000 training images are available in CIFAR-10, and 10,000 of them are used for supervised training. Both data sets have 10,000 test images for our evaluation. Our implementations are based on Caffe (Jia et al., 2014), and the reference network architectures for MNIST and CIFAR-10 are LeNet (LeCun et al., 1998) and Caffe's CIFAR10-quick. In the following experiments, the effects are set to 1 and 0.1, and the mini batch size to 100. The a and b in spectral label refinement KIFAR10-quick is empirically set to 1 and 0.001, respectively."}, {"heading": "4.1 SPECTRAL CLUSTERING VS. SPECTRAL LABEL REFINEMENT", "text": "For comparison of cluster results, Normalized Mutual Information (NMI) is chosen as the measurement value. NMI is 1 if the result is perfect, and 0 if it is completely random. We vary the missing label rate to check the strength of the spectral label refinement that takes labels into account. To meet our CNN training protocols, we extract deep features from the test set of both the MNIST and the CIFAR-10 based on our pre-trained networks. We then randomly select 100 samples, mark a fixed fraction of these samples as unlabeled, and repeat 50 times to achieve the average of the NMI values. The number of clusters is set to the exact number of classes, i.e. 10 for both datasets. Figure 1 shows that the refinement of the labels yields better cluster results even in extreme cases (e.g. few samples with a high missing labeling rate than 0.8). The main difference between the FIST and the FIST experiments is sufficient (the net difference between the MISAR and the FIST is sufficient)."}, {"heading": "4.2 EVALUATION OF OUR ONLINE GRAPH CONSTRUCTION METHODS", "text": "This year, it is more than ever before in the history of the country in which it is a country, in which it is a country, in which it is a country, in which it is a country."}, {"heading": "5 CONCLUSION AND FUTURE WORK", "text": "In this work, we developed a simple but effective technique to construct the graph online at the same time as CNN training, then studied the kNN graph and the refinement of spectral labeling, and demonstrated how they work and function with our online graph design technique. We also demonstrated the effectiveness of our approach to MNIST and CIFAR-10 for semi-supervised deep learning with CNNs. Our approach can easily be applied to other state-of-the-art CNN architectures for semi-supervised learning, and can serve as a framework for other graphs that are effective or specific to some applications. Since some graph approaches, such as refining spectral labels, have the ability to use labels, we could study more about the use of labels. Moreover, since W is binary due to the contractive loss function, it may be promising to consider a more effective loss function that is better suited to modern graphs."}], "references": [], "referenceMentions": [], "year": 2015, "abstractText": "The recent promising achievements of deep learning rely on the large amount of labeled data. Considering the abundance of data on the web, most of them do not have labels at all. Therefore, it is important to improve generalization performance using unlabeled data on supervised tasks with few labeled instances. In this work, we revisit graph-based semi-supervised learning algorithms and propose an online graph construction technique which suits deep convolutional neural network better. We consider an EM-like algorithm for semi-supervised learning on deep neural networks: In forward pass, the graph is constructed based on the network output, and the graph is then used for loss calculation to help update the network by back propagation in the backward pass. We demonstrate the strength of our online approach compared to the conventional ones whose graph is constructed on static but not robust enough feature representations beforehand.", "creator": "LaTeX with hyperref package"}}}