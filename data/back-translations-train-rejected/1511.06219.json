{"id": "1511.06219", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "19-Nov-2015", "title": "Knowledge Base Population using Semantic Label Propagation", "abstract": "A crucial aspect of a knowledge base population system that extracts new facts from text corpora, is the generation of training data for its relation extractors. In this paper, we present a method that maximizes the effectiveness of newly trained relation extractors at a minimal annotation cost. Manual labeling can be significantly reduced by Distant Supervision, which is a method to construct training data automatically by aligning a large text corpus with an existing knowledge base of known facts. For example, all sentences mentioning both 'Barack Obama' and 'US' may serve as positive training instances for the relation born_in(subject,object). However, distant supervision typically results in a highly noisy training set: many training sentences do not really express the intended relation. We propose to combine distant supervision with minimal manual supervision in a technique called feature labeling, to eliminate noise from the large and noisy initial training set, resulting in a significant increase of precision. We further improve on this approach by introducing the Semantic Label Propagation method, which uses the similarity between low-dimensional representations of candidate training instances, to extend the training set in order to increase recall while maintaining high precision. Our proposed strategy for generating training data is studied and evaluated on an established test collection designed for knowledge base population tasks. The experimental results show that the Semantic Label Propagation strategy leads to substantial performance gains when compared to existing approaches, while requiring an almost negligible manual annotation effort.", "histories": [["v1", "Thu, 19 Nov 2015 15:51:31 GMT  (938kb,D)", "http://arxiv.org/abs/1511.06219v1", "Submitted to Knowledge Based Systems, special issue on Knowledge Bases for Natural Language Processing"], ["v2", "Thu, 3 Mar 2016 11:52:14 GMT  (613kb,D)", "http://arxiv.org/abs/1511.06219v2", "Submitted to Knowledge Based Systems, special issue on Knowledge Bases for Natural Language Processing"]], "COMMENTS": "Submitted to Knowledge Based Systems, special issue on Knowledge Bases for Natural Language Processing", "reviews": [], "SUBJECTS": "cs.CL cs.LG", "authors": ["lucas sterckx", "thomas demeester", "johannes deleu", "chris develder"], "accepted": false, "id": "1511.06219"}, "pdf": {"name": "1511.06219.pdf", "metadata": {"source": "CRF", "title": "Knowledge Base Population using Semantic Label Propagation", "authors": ["Lucas Sterckx", "Thomas Demeester", "Johannes Deleu", "Chris Develder"], "emails": ["lucas.sterckx@intec.ugent.be"], "sections": [{"heading": null, "text": "A crucial aspect of a knowledge base population system that extracts new facts from text corpora 06 is the generation of training data for its relation extractors. In this paper, we present a method that maximizes the effectiveness of newly trained relation extractors with minimal annotation costs. Manual labeling can be significantly reduced by using Distant Supervision, a method for automatically constructing training data, by matching a large body of text with an existing knowledge base of known facts. For example, we propose to use all sentences that mention both \"Barack Obama\" and \"USA\" as positive training instances for the relationship born in (subject, object). However, remote supervision typically results in a very loud training set: Many training sets do not really express the intended relationship. We suggest combining remote supervision with minimal manual supervision in a technique called feature labeling to eliminate noise from a large initial training, resulting in a significant increase in the prefix."}, {"heading": "1. Introduction", "text": "This year it is more than ever before."}, {"heading": "2. Related Work", "text": "The basic idea of our proposed approach is to combine remote monitoring with a minimum of monitoring, i.e. to require as few (feature) comments as possible. Therefore, our work should be designed in the context of supervised and semi-supervised relation extraction (RE) and linked to approaches aimed at minimizing annotation costs, e.g. active learning. In addition, we use compact vector representations that carry semantics, i.e. word embeddings. In the following, therefore, we summarize the related work in the areas of (i) supervised RE, (iii) semi-supervised RE, (iii) active learning and (iv) word embeddings."}, {"heading": "2.1. Supervised Relation Extraction", "text": "The monitored RE methods are based on training data in the form of sentences marked with a label indicating the presence or absence of the relationship under consideration. There are three major classes of monitored RE: (i) function-based methods, (ii) nuclear-based methods and (iii) convolution-based neural networks. Afterwards, a classifier is trained using positive and negative examples. In contrast, nuclear-based methods [27, 28, 16] represent each relationship as an object, such as an extended token sequence or a parse tree, and use a carefully designed core function, such as a sequence kernel or a context tree core, to calculate its similarity to test patterns. These objects are usually augmented by additional features such as setic structures."}, {"heading": "2.1.1. Bootstrapping models for Relation Extraction", "text": "When a limited number of highlighted examples are available, bootstrapping methods have proven to be effective methods for generating high-precision relationship patterns [17, 18, 31, 32]. The goal of bootstrapping is to add new relationship instances to an initial seed group of instances. Documents are scanned by units from seed instances and linguistic patterns that bind them together are extracted. Patterns are then sorted by coverage (retrieval) and low error rate (precision). Top scoring patterns are used to extract new seed instances and repeat the cycle. An important step in bootstrapping methods is to calculate the similarity between new patterns and those in the seed set. This measure determines whether a new pattern is relationship-oriented or not, based on the existing set. Systems use measures based on exact matches [31], cosmic similarity [17] or cores."}, {"heading": "2.1.2. Distant Supervision", "text": "Distant Supervision (DS) was first proposed in [36], where marked data was generated by merging instances from the Yeast Protein Database into research articles to train an extractor, and this approach was later applied to the formation of relation extractors between entities in [10]. Automatic acquisition of training data with DS is determined by the assumption that all sentences containing both entities in a reference instance of a particular relationship represent this relationship. Many methods have been proposed to reduce noise in training sets from DS. In a number of papers, the labels of DS data are considered to be latent variables. Riedel et al. [21] loosened the strong adoption of all sentences and loosened them to a minimum one-sentence assumption, creating a multi-instance learner. Hoffman et al. [37] modified this model by allowing entity pairs to express multiple relationships, resulting in a multi-instance-ME-setting (IML)."}, {"heading": "2.2. Semi-supervised Relation Extraction", "text": "The training data includes labeled instances Xl = (x1.. xl), for which labels Yl = (y1.. yl) are provided, and typically a large number of unlabeled instances Xu = (x1.. xu).Semi-monitored techniques have been applied multiple times to RE. Chen et al. [41] apply label propagation by presenting labeled and unlabeled examples as nodes and their similarities, like the weights of the edges in a diagram. In the classification process, the labels of unlabeled examples are then propagated from labeled to unlabeled instances. Experimental results show that this graph-based algorithm can surpass SVM in relation to F1."}, {"heading": "2.3. Active Learning and Feature Labeling", "text": "The most popular form of active learning is based on the iterative need for manual labeling for the most informative cases, an approach known as the uncertainty sample. Typical approaches to extraction are query for discards [24, 45] and cluster-based sampling [46]. While the emphasis in rare cases has been on relation labeling, alternative methods have been proposed in other tasks where characteristics (e.g. patterns or occurrence of terms) are labeled as opposed to instances [47, 48] which results in higher performance for less monitoring. Obtaining positive examples of specific relationships can be difficult, especially when training data is poorly monitored. Standard uncertainty samples are ineffective in this case: it is likely that a feature or instance has a low safety value because it does not contain much discriminatory information about the classes. Assigning labels to the most specific characteristics has a much greater impact on the classifier and the more noise-sided principle is effective in this approach in many areas."}, {"heading": "2.4. Distributional Semantics", "text": "The distribution hypothesis [49] states that words that tend to occur in similar contexts are likely to have similar meanings. Word representations as dense vectors (as opposed to standard one-hot vectors), so-called word embeddings, take advantage of this hypothesis and are trained to predict their context based on large amounts of unlabeled data. Word representations will resemble those of related words, allowing the model to better generalize to invisible events. The resulting vector space is also called a vector meaning model [50]. Common methods for generating very dense, short vectors use dimensionality reduction techniques (e.g. singular value decomposition) or neural networks to generate so-called word embeddings. Word embeddings have been shown to be beneficial for many tasks of processing natural language, including POStagging, machine translation, and sectional role marker-based word models that are based on larger word combinations [52, not on supervised word embeddings] such as Valumatic networks."}, {"heading": "3. Labeling Strategy for Noise Reduction", "text": "In this section, we present our strategy for linking remote control training data with minimal amounts of monitoring. In short, we have designed our labeling strategy to minimize the amount of false-positive instances or noise while maintaining the variety of relationship expressions generated by DSD. Various questions arise that overwhelm the initial training generated by remote monitoring. What is the trade-off between the use of remote monitoring with highly variously designated instances and the restrictive approach of labeling features in the feature space? This is discussed in detail in Sections 3.2 and 3.3."}, {"heading": "3.1. Distantly Supervised Training Data", "text": "In recent years, it has become clear that the problem is not only a problem, but also a problem that has worsened in recent years."}, {"heading": "3.2. Labeling High Confidence Shortest Dependency Paths", "text": "In fact, most of them are able to move to another world, in which they are able, in which they are able to integrate, and in which they are able to change the world."}, {"heading": "3.3. Noise Reduction using Semantic Label Propagation", "text": "If we adhere strictly to the approach proposed in Section 3.2 and maintain only DS training instances that satisfy an accepted SDP, an important advantage of DS will be lost, namely its potential to achieve a high recall. If we limit the annotation effort, we risk losing highly valuable SDPs. To counteract this effect, we will introduce a second (re) labeling level by applying a semi-supervised learning strategy to expand the training set by adding again some instances from the set of previously discarded DS instances that do not match any of the manually labeled patterns. We rely on the basic SSL approach of self-labeling by placing labels from known instances on the next unlabeled instances. This algorithm requires a method to determine the distance between the labeled and unlabeled instances."}, {"heading": "4. Experimental Results", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.1. Testing Methodology", "text": "We evaluate the relation extractors in the context of an existing Knowledge Base Population System [58, 59] using the NIST TAC KBP English Slot Filling (ESF) evaluation from 2012 to 2014. We choose this evaluation because of the variety and difficulty of the units participating in the queries. Under the ESF, the input into the system is a specific unit (the \"query\"), a series of relationships and a series of articles. The output is a series of slot fills in which each slot fill consists of three units (including the query unit) and a relationship (the given relationships) that is likely to exist between these units."}, {"heading": "4.2. Knowledge Base Population System", "text": "Systems participating in the TAC KBP ESF must complete each task of filling missing spaces in a KB. Participants will receive only one interface text occurrence of each query unit in a large collection of text provided by the organizers, which means that a component to retrieve information is required to provide the relation extractor with sentences of candidate responses. Our system performs query enhancements using freebase aliases and Wikipedia pages. Each document containing one of the aliases is analyzed and named units are automatically detected. Persons, organizations and locations are detected, and locations are further categorized as cities, states, or countries. Fill devices for non-entities such as titles or fees are marked with lists and table searches. For more details on the KBP system, see [58, 59]."}, {"heading": "4.3. Pattern-based Restriction vs. Similarity-based Extension", "text": "This year it is as far as never before in the history of the Federal Republic of Germany."}, {"heading": "4.4. End-to-End Knowledge Base Population Results", "text": "This year, it will be able to realize the projects that are mentioned, that are worth mentioning, that are worth mentioning, that are worth mentioning and that are worth mentioning."}, {"heading": "5. Conclusions", "text": "The overall goal of our proposed strategy for building relation extractors in a closed IE setting (i.e. extracting a priori specified relationships) is to maximize their performance with minimal (human) annotation effort. The key ideas that help us do this are: (i) remote monitoring (DS): using known relationship criteria from a knowledge base to automatically generate training data; (ii) using compact semantic vector spaces to detect additional, semantically related patterns (e.g. text patterns expressing a relationship) that are selected using an active learning criterion; and (ii) semantic feature representation: using compact semantic vector spaces to detect additional, semantically related patterns that do not appear in the previously selected training data, thus missing useful patterns."}], "references": [{"title": "Yago: a core of semantic knowledge", "author": ["F.M. Suchanek", "G. Kasneci", "G. Weikum"], "venue": "in: Proceedings of the 16th international conference on World Wide Web, ACM", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2007}, {"title": "Dbpedia-a crystallization point for the web of data", "author": ["C. Bizer", "J. Lehmann", "G. Kobilarov", "S. Auer", "C. Becker", "R. Cyganiak", "S. Hellmann"], "venue": "Web Semantics: science, services and agents on the world wide web 7 (3) ", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2009}, {"title": "Freebase: a collaboratively created graph database for structuring human knowledge", "author": ["K. Bollacker", "C. Evans", "P. Paritosh", "T. Sturge", "J. Taylor"], "venue": "in: Proceedings of the 2008 ACM SIGMOD international conference on Management of data, ACM", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2008}, {"title": "J", "author": ["T. Mitchell", "W. Cohen", "E. Hruschka", "P.P. Talukdar", "J. Betteridge", "A. Carlson", "B. Dalvi Mishra", "M. Gardner", "B. Kisiel"], "venue": "Krishnamurthy, et al., Never-ending learning, in: AAAI, AAAI Press", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2015}, {"title": "Scalable knowledge harvesting with high precision and high recall", "author": ["N. Nakashole", "M. Theobald", "G. Weikum"], "venue": "in: Proceedings of the fourth ACM international conference on Web search and data mining, ACM", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2011}, {"title": "Textrunner: open information extraction on the web", "author": ["A. Yates", "M. Cafarella", "M. Banko", "O. Etzioni", "M. Broadhead", "S. Soderland"], "venue": "in: Proceedings of Human Language Technologies: The Annual Conference of the North American Chapter of the Association for Computational Linguistics: Demonstrations, Association for Computational Linguistics", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2007}, {"title": "Prismatic: Inducing knowledge from a large scale lexicalized relation resource", "author": ["J. Fan", "D. Ferrucci", "D. Gondek", "A. Kalyanpur"], "venue": "in: Proceedings of the NAACL HLT 2010 first international workshop on formalisms and methodology for learning by reading, Association for Computational Linguistics", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2010}, {"title": "Knowledge vault: A web-scale approach to probabilistic knowledge fusion", "author": ["X. Dong", "E. Gabrilovich", "G. Heitz", "W. Horn", "N. Lao", "K. Murphy", "T. Strohmann", "S. Sun", "W. Zhang"], "venue": "in: Proceedings of the 20th ACM SIGKDD international conference on Knowledge discovery and data mining, ACM", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2014}, {"title": "D", "author": ["B. Min", "R. Grishman", "L. Wan", "C. Wang"], "venue": "Gondek, Distant supervision for relation extraction with an incomplete knowledge base., in: HLT-NAACL", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2013}, {"title": "A novel use of statistical parsing to extract information from text", "author": ["S. Miller", "H. Fox", "L. Ramshaw", "R. Weischedel"], "venue": "in: Proceedings of the 1st North American chapter of the Association for Computational Linguistics conference, Association for Computational Linguistics", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2000}, {"title": "Combining lexical", "author": ["N. Kambhatla"], "venue": "syntactic, and semantic features with maximum entropy models for extracting relations, in: Proceedings of the ACL 2004 on Interactive poster and demonstration sessions, Association for Computational Linguistics", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2004}, {"title": "Automatic information extraction", "author": ["E. Boschee", "R. Weischedel", "A. Zamanian"], "venue": "in: Proceedings of the 2005 International Conference on Intelligence Analysis, McLean, VA, Citeseer", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2005}, {"title": "C", "author": ["J. Jiang"], "venue": "Zhai, A systematic exploration of the feature space for relation extraction., in: HLT- NAACL", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2007}, {"title": "Semi-supervised relation extraction with large-scale word clustering", "author": ["A. Sun", "R. Grishman", "S. Sekine"], "venue": "in: Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies-Volume 1, Association for Computational Linguistics", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2011}, {"title": "A shortest path dependency kernel for relation extraction", "author": ["R.C. Bunescu", "R.J. Mooney"], "venue": "in: Proceedings of the conference on Human Language Technology and Empirical Methods in Natural Language Processing, Association for Computational Linguistics", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2005}, {"title": "Snowball: Extracting relations from large plain-text collections", "author": ["E. Agichtein", "L. Gravano"], "venue": "in: Proceedings of the fifth ACM conference on Digital libraries, ACM", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2000}, {"title": "Spied: Stanford pattern-based information extraction and diagnostics, Proceedings of the ACL 2014 Workshop on Interactive Language Learning, Visualization, and Interfaces (ACL-ILLVI)", "author": ["S. Gupta", "C.D. Manning"], "venue": null, "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2014}, {"title": "Overview of the english slot filling track at the tac2014 knowledge base population evaluation", "author": ["M. Surdeanu", "H. Ji"], "venue": "Proc. Text Analysis Conference ", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2014}, {"title": "Incremental knowledge base construction using deepdive", "author": ["J. Shin", "S. Wu", "F. Wang", "C. De Sa", "C. Zhang", "C. R\u00e9"], "venue": "Proceedings of the VLDB Endowment 8 (11) ", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2015}, {"title": "Modeling relations and their mentions without labeled text", "author": ["S. Riedel", "L. Yao", "A. McCallum"], "venue": "in: Machine Learning and Knowledge Discovery in Databases, Springer Berlin Heidelberg", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2010}, {"title": "Knowledge-based weak supervision for information extraction of overlapping relations", "author": ["R. Hoffmann", "C. Zhang", "X. Ling", "L. Zettlemoyer", "D.S. Weld"], "venue": "in: Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies-Volume 1, Association for Computational Linguistics", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2011}, {"title": "Multi-instance multi-label learning for relation extraction", "author": ["M. Surdeanu", "J. Tibshirani", "R. Nallapati", "C.D. Manning"], "venue": "in: Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, Association for Computational Linguistics", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2012}, {"title": "Combining distant and partial supervision for relation extraction", "author": ["G. Angeli", "J. Tibshirani", "J.Y. Wu", "C.D. Manning"], "venue": "in: Proceedings of the 2014 conference on empirical methods in natural language processing (EMNLP)", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2014}, {"title": "R", "author": ["M. Pershina", "B. Min", "W. Xu"], "venue": "Grishman, Infusion of labeled data into distant supervision for relation extraction", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2014}, {"title": "Distant supervision for relation extraction without labeled data", "author": ["M. Mintz", "S. Bills", "R. Snow", "D. Jurafsky"], "venue": "in: Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP: Volume 2-Volume 2, Association for Computational Linguistics", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2009}, {"title": "Kernel methods for relation extraction", "author": ["D. Zelenko", "C. Aone", "A. Richardella"], "venue": "in: Proceedings of the ACL-02 Conference on Empirical Methods in Natural Language Processing - Volume 10, EMNLP \u201902, Association for Computational Linguistics, Stroudsburg, PA, USA", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2002}, {"title": "Dependency tree kernels for relation extraction", "author": ["A. Culotta", "J. Sorensen"], "venue": "in: Proceedings of the 42nd Annual Meeting on Association for Computational Linguistics, Association for Computational Linguistics", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2004}, {"title": "Relation classification via convolutional deep neural network", "author": ["D. Zeng", "K. Liu", "S. Lai", "G. Zhou", "J. Zhao"], "venue": "in: Proceedings of COLING", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2014}, {"title": "Extracting patterns and relations from the world wide web., Technical Report 1999-65, Stanford InfoLab, previous number = SIDL-WP-1999-0119", "author": ["S. Brin"], "venue": null, "citeRegEx": "31", "shortCiteRegEx": "31", "year": 1999}, {"title": "Construction of semantic bootstrapping models for relation extraction", "author": ["C. Zhang", "W. Xu", "Z. Ma", "S. Gao", "Q. Li", "J. Guo"], "venue": "Knowledge-Based Systems 83 ", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2015}, {"title": "Graph-based analysis of semantic drift in espressolike bootstrapping algorithms", "author": ["M. Komachi", "T. Kudo", "M. Shimbo", "Y. Matsumoto"], "venue": "in: Proceedings of the Conference on Empirical Methods in Natural Language Processing, EMNLP \u201908, Association for Computational Linguistics, Stroudsburg, PA, USA", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2008}, {"title": "Minimising semantic drift with mutual exclusion bootstrapping", "author": ["J.R. Curran", "T. Murphy", "B. Scholz"], "venue": "Proceedings of the Conference of the Pacific Association for Computational Linguistics ", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2007}, {"title": "Semi-supervised bootstrapping of relationship extractors with distributional semantics", "author": ["D.S. Batista", "B. Martins", "M.J. Silva"], "venue": "in: Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, Association for Computational Linguistics, Lisbon, Portugal", "citeRegEx": "35", "shortCiteRegEx": null, "year": 2015}, {"title": "J", "author": ["M. Craven"], "venue": "Kumlien, et al., Constructing biological knowledge bases by extracting information from text sources., in: ISMB, Vol. 1999", "citeRegEx": "36", "shortCiteRegEx": null, "year": 1999}, {"title": "Knowledge-based weak supervision for information extraction of overlapping relations, Proc", "author": ["R. Hoffmann", "C. Zhang", "X. Ling"], "venue": null, "citeRegEx": "37", "shortCiteRegEx": "37", "year": 2002}, {"title": "O", "author": ["A. Intxaurrondo", "M. Surdeanu"], "venue": "L. de Lacalle, E. Agirre, Removing noisy mentions for distant supervision, Procesamiento del lenguaje natural 51 ", "citeRegEx": "38", "shortCiteRegEx": null, "year": 2013}, {"title": "Pattern learning for relation extraction with a hierarchical topic model", "author": ["E. Alfonseca", "K. Filippova", "J.-Y. Delort", "G. Garrido"], "venue": "in: Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics: Short Papers-Volume 2, Association for Computational Linguistics", "citeRegEx": "39", "shortCiteRegEx": null, "year": 2012}, {"title": "Reducing wrong labels in distant supervision for relation extraction", "author": ["S. Takamatsu", "I. Sato", "H. Nakagawa"], "venue": "in: Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics: Long Papers-Volume 1, Association for Computational Linguistics", "citeRegEx": "40", "shortCiteRegEx": null, "year": 2012}, {"title": "Relation extraction using label propagation based semi-supervised learning", "author": ["J. Chen", "D. Ji", "C.L. Tan", "Z. Niu"], "venue": "in: Proceedings of the 21st International Conference on Computational Linguistics and the 44th annual meeting of the Association for Computational Linguistics, Association for Computational Linguistics", "citeRegEx": "41", "shortCiteRegEx": null, "year": 2006}, {"title": "Big data versus the crowd: Looking for relationships in all the right places", "author": ["C. Zhang", "F. Niu", "C. R\u00e9", "J. Shavlik"], "venue": "in: Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics: Long Papers-Volume 1, Association for Computational Linguistics", "citeRegEx": "42", "shortCiteRegEx": null, "year": 2012}, {"title": "Deepdive: A data management system for automatic knowledge base construction", "author": ["C. Zhang"], "venue": "Ph.D. thesis, UW-Madison ", "citeRegEx": "44", "shortCiteRegEx": null, "year": 2015}, {"title": "Query by committee", "author": ["H.S. Seung", "M. Opper", "H. Sompolinsky"], "venue": "in: Proceedings of the fifth annual workshop on Computational learning theory, ACM", "citeRegEx": "45", "shortCiteRegEx": null, "year": 1992}, {"title": "Using active learning and semantic clustering for noise reduction in distant supervision", "author": ["L. Sterckx", "T. Demeester", "J. Deleu", "C. Develder"], "venue": "in: 4e Workshop on Automated Base Construction at NIPS2014 ", "citeRegEx": "46", "shortCiteRegEx": null, "year": 2014}, {"title": "Active learning by labeling features", "author": ["G. Druck", "B. Settles", "A. McCallum"], "venue": "in: Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing: Volume 1-Volume 1, Association for Computational Linguistics", "citeRegEx": "47", "shortCiteRegEx": null, "year": 2009}, {"title": "A unified approach to active dual supervision for labeling features and examples", "author": ["J. Attenberg", "P. Melville", "F. Provost"], "venue": "in: In European conference on Machine learning and knowledge discovery in databases", "citeRegEx": "48", "shortCiteRegEx": null, "year": 2010}, {"title": "Distributional structure", "author": ["Z. Harris"], "venue": "Word 10 (23) ", "citeRegEx": "49", "shortCiteRegEx": null, "year": 1954}, {"title": "Glove: Global vectors for word representation", "author": ["J. Pennington", "R. Socher", "C.D. Manning"], "venue": "in: Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing, EMNLP 2014, October 25-29, 2014, Doha, Qatar, A meeting of SIGDAT, a Special Interest Group of the ACL", "citeRegEx": "52", "shortCiteRegEx": null, "year": 2014}, {"title": "A comparison of vector-based representations for semantic composition", "author": ["W. Blacoe", "M. Lapata"], "venue": "in: Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, Association for Computational Linguistics, Jeju Island, Korea", "citeRegEx": "53", "shortCiteRegEx": null, "year": 2012}, {"title": "The stanford corenlp natural language processing toolkit", "author": ["C.D. Manning", "M. Surdeanu", "J. Bauer", "J. Finkel", "S.J. Bethard", "D. McClosky"], "venue": "in: Proceedings of 52nd Annual Meeting of the Association for Computational Linguistics: System Demonstrations", "citeRegEx": "55", "shortCiteRegEx": null, "year": 2014}, {"title": "D", "author": ["M. Mintz", "S. Bills", "R. Snow"], "venue": "Jurafsky, Distant supervision for relation extraction without labeled data (August) ", "citeRegEx": "56", "shortCiteRegEx": null, "year": 2009}, {"title": "Comparing information extraction pattern models", "author": ["M. Stevenson", "M.A. Greenwood"], "venue": "in: Proceedings of the Workshop on Information Extraction Beyond The Document, Association for Computational Linguistics", "citeRegEx": "57", "shortCiteRegEx": null, "year": 2006}, {"title": "Ghent University-IBCN participation in TAC-KBP 2014 slot filling and cold start tasks", "author": ["M. Feys", "L. Sterckx", "L. Mertens", "J. Deleu", "T. Demeester", "C. Develder"], "venue": "in: 7th Text Analysis Conference, Proceedings", "citeRegEx": "58", "shortCiteRegEx": null, "year": 2014}, {"title": "Ghent University-IBCN participation in TAC-KBP 2015 cold start task", "author": ["L. Sterckx", "J. Deleu", "T. Demeester", "C. Develder"], "venue": "in: 8th Text Analysis Conference, Proceedings (To Appear)", "citeRegEx": "59", "shortCiteRegEx": null, "year": 2015}, {"title": "Indexing by latent semantic analysis", "author": ["S.C. Deerwester", "S.T. Dumais", "T.K. Landauer", "G.W. Furnas", "R.A. Harshman"], "venue": "JAsIs 41 (6) ", "citeRegEx": "60", "shortCiteRegEx": null, "year": 1990}], "referenceMentions": [{"referenceID": 0, "context": ", YAGO [1], DBpedia [2], and Freebase [3]), KBs generated from Web documents (e.", "startOffset": 7, "endOffset": 10}, {"referenceID": 1, "context": ", YAGO [1], DBpedia [2], and Freebase [3]), KBs generated from Web documents (e.", "startOffset": 20, "endOffset": 23}, {"referenceID": 2, "context": ", YAGO [1], DBpedia [2], and Freebase [3]), KBs generated from Web documents (e.", "startOffset": 38, "endOffset": 41}, {"referenceID": 3, "context": ", NELL [4], PROSPERA[5]), or open information extraction approaches (e.", "startOffset": 7, "endOffset": 10}, {"referenceID": 4, "context": ", NELL [4], PROSPERA[5]), or open information extraction approaches (e.", "startOffset": 20, "endOffset": 23}, {"referenceID": 5, "context": ", TextRunner [6], PRISMATIC [7]).", "startOffset": 13, "endOffset": 16}, {"referenceID": 6, "context": ", TextRunner [6], PRISMATIC [7]).", "startOffset": 28, "endOffset": 31}, {"referenceID": 7, "context": "Besides academic projects, several commercial projects were initiated by major corporations like Microsoft (Satori1), Google (Knowledge Graph [8]), Facebook2, Walmart [9] and others.", "startOffset": 142, "endOffset": 145}, {"referenceID": 8, "context": "5% of persons in Freebase have no known nationality [10].", "startOffset": 52, "endOffset": 56}, {"referenceID": 7, "context": "To complete a KB, we need a Knowledge Base Population (KBP) system that extracts information from various sources, of which a large fraction comprises unstructured written text items [8].", "startOffset": 183, "endOffset": 186}, {"referenceID": 9, "context": "Effective approaches for closed schema RE apply some form of supervised or semi-supervised learning [11, 12, 13, 14, 15, 16] and generally follow three steps: (i) sentences expressing relations are transformed to a data representation, e.", "startOffset": 100, "endOffset": 124}, {"referenceID": 10, "context": "Effective approaches for closed schema RE apply some form of supervised or semi-supervised learning [11, 12, 13, 14, 15, 16] and generally follow three steps: (i) sentences expressing relations are transformed to a data representation, e.", "startOffset": 100, "endOffset": 124}, {"referenceID": 11, "context": "Effective approaches for closed schema RE apply some form of supervised or semi-supervised learning [11, 12, 13, 14, 15, 16] and generally follow three steps: (i) sentences expressing relations are transformed to a data representation, e.", "startOffset": 100, "endOffset": 124}, {"referenceID": 12, "context": "Effective approaches for closed schema RE apply some form of supervised or semi-supervised learning [11, 12, 13, 14, 15, 16] and generally follow three steps: (i) sentences expressing relations are transformed to a data representation, e.", "startOffset": 100, "endOffset": 124}, {"referenceID": 13, "context": "Effective approaches for closed schema RE apply some form of supervised or semi-supervised learning [11, 12, 13, 14, 15, 16] and generally follow three steps: (i) sentences expressing relations are transformed to a data representation, e.", "startOffset": 100, "endOffset": 124}, {"referenceID": 14, "context": "Effective approaches for closed schema RE apply some form of supervised or semi-supervised learning [11, 12, 13, 14, 15, 16] and generally follow three steps: (i) sentences expressing relations are transformed to a data representation, e.", "startOffset": 100, "endOffset": 124}, {"referenceID": 15, "context": "To counter this problem, the technique of iterative bootstrapping has been proposed [17, 18], in which an initial seed set of known facts is used to learn patterns, which in turn are used to learn new facts and incrementally extend the training set.", "startOffset": 84, "endOffset": 92}, {"referenceID": 16, "context": "To counter this problem, the technique of iterative bootstrapping has been proposed [17, 18], in which an initial seed set of known facts is used to learn patterns, which in turn are used to learn new facts and incrementally extend the training set.", "startOffset": 84, "endOffset": 92}, {"referenceID": 17, "context": "DS has been successfully applied in many relation extraction tasks [19, 20] as it allows for the creation of large training sets with little or no human effort.", "startOffset": 67, "endOffset": 75}, {"referenceID": 18, "context": "DS has been successfully applied in many relation extraction tasks [19, 20] as it allows for the creation of large training sets with little or no human effort.", "startOffset": 67, "endOffset": 75}, {"referenceID": 19, "context": "The heuristic of accepting each co-occurrence of the entity pair < e1, e2 > as a positive training item because of the KB entry r(e1, e2), is known to generate noisy training data or false positives [21], i.", "startOffset": 199, "endOffset": 203}, {"referenceID": 19, "context": "The most prominent is that of latent variable models of the distantly supervised data that make the assumption that a known fact is expressed at least once in the corpus [21, 22, 23].", "startOffset": 170, "endOffset": 182}, {"referenceID": 20, "context": "The most prominent is that of latent variable models of the distantly supervised data that make the assumption that a known fact is expressed at least once in the corpus [21, 22, 23].", "startOffset": 170, "endOffset": 182}, {"referenceID": 21, "context": "The most prominent is that of latent variable models of the distantly supervised data that make the assumption that a known fact is expressed at least once in the corpus [21, 22, 23].", "startOffset": 170, "endOffset": 182}, {"referenceID": 22, "context": "Some focus on active learning, selecting training instances to be labeled according to an uncertainty criterion [24, 19], while others focus on annotations of surface patterns and define rules or guidelines in a semi-supervised learning setting [25].", "startOffset": 112, "endOffset": 120}, {"referenceID": 17, "context": "Some focus on active learning, selecting training instances to be labeled according to an uncertainty criterion [24, 19], while others focus on annotations of surface patterns and define rules or guidelines in a semi-supervised learning setting [25].", "startOffset": 112, "endOffset": 120}, {"referenceID": 23, "context": "Some focus on active learning, selecting training instances to be labeled according to an uncertainty criterion [24, 19], while others focus on annotations of surface patterns and define rules or guidelines in a semi-supervised learning setting [25].", "startOffset": 245, "endOffset": 249}, {"referenceID": 12, "context": "Feature based methods [14, 26] extract a rich list of structural, lexical, syntactic and semantic features to represent the given relation mentions.", "startOffset": 22, "endOffset": 30}, {"referenceID": 24, "context": "Feature based methods [14, 26] extract a rich list of structural, lexical, syntactic and semantic features to represent the given relation mentions.", "startOffset": 22, "endOffset": 30}, {"referenceID": 25, "context": "In contrast, kernel based methods [27, 28, 16] represent each relation mention as an object such as an augmented token sequences or a parse tree, and use a carefully designed kernel function, e.", "startOffset": 34, "endOffset": 46}, {"referenceID": 26, "context": "In contrast, kernel based methods [27, 28, 16] represent each relation mention as an object such as an augmented token sequences or a parse tree, and use a carefully designed kernel function, e.", "startOffset": 34, "endOffset": 46}, {"referenceID": 14, "context": "In contrast, kernel based methods [27, 28, 16] represent each relation mention as an object such as an augmented token sequences or a parse tree, and use a carefully designed kernel function, e.", "startOffset": 34, "endOffset": 46}, {"referenceID": 27, "context": "With the recent success of deep neural networks in NLP, Convolutional Neural Networks (CNNs) have emerged as effective relation extractors [29, 30].", "startOffset": 139, "endOffset": 147}, {"referenceID": 15, "context": "Bootstrapping models for Relation Extraction When a limited set of labeled instances is available, bootstrapping methods have proven to be effective methods to generate high-precision relation patterns [17, 18, 31, 32].", "startOffset": 202, "endOffset": 218}, {"referenceID": 16, "context": "Bootstrapping models for Relation Extraction When a limited set of labeled instances is available, bootstrapping methods have proven to be effective methods to generate high-precision relation patterns [17, 18, 31, 32].", "startOffset": 202, "endOffset": 218}, {"referenceID": 28, "context": "Bootstrapping models for Relation Extraction When a limited set of labeled instances is available, bootstrapping methods have proven to be effective methods to generate high-precision relation patterns [17, 18, 31, 32].", "startOffset": 202, "endOffset": 218}, {"referenceID": 29, "context": "Bootstrapping models for Relation Extraction When a limited set of labeled instances is available, bootstrapping methods have proven to be effective methods to generate high-precision relation patterns [17, 18, 31, 32].", "startOffset": 202, "endOffset": 218}, {"referenceID": 28, "context": "Systems use measures based on exact matches [31], cosine-similarity [17] or kernels [32].", "startOffset": 44, "endOffset": 48}, {"referenceID": 15, "context": "Systems use measures based on exact matches [31], cosine-similarity [17] or kernels [32].", "startOffset": 68, "endOffset": 72}, {"referenceID": 29, "context": "Systems use measures based on exact matches [31], cosine-similarity [17] or kernels [32].", "startOffset": 84, "endOffset": 88}, {"referenceID": 30, "context": "A fundamental problem of these methods is semantic drift [33, 34]: bootstrapping, after several iterations, deviates from the semantics of the seed relationship and extracts unrelated instances which in turn generate faulty patterns.", "startOffset": 57, "endOffset": 65}, {"referenceID": 31, "context": "A fundamental problem of these methods is semantic drift [33, 34]: bootstrapping, after several iterations, deviates from the semantics of the seed relationship and extracts unrelated instances which in turn generate faulty patterns.", "startOffset": 57, "endOffset": 65}, {"referenceID": 32, "context": "[35] proposed the use of word embeddings for capturing semantic similarity between patterns.", "startOffset": 0, "endOffset": 4}, {"referenceID": 33, "context": "Distant Supervision Distant Supervision (DS) was first proposed in [36], where labeled data was generated by aligning instances from the Yeast Protein Database into research articles to train an extractor.", "startOffset": 67, "endOffset": 71}, {"referenceID": 8, "context": "This approach was later applied for training of relation extractors between entities in [10].", "startOffset": 88, "endOffset": 92}, {"referenceID": 19, "context": "[21] relaxed the strong all sentences-assumption and relaxed it to an at-least-one-sentence-assumption, creating a Multi-Instance learner.", "startOffset": 0, "endOffset": 4}, {"referenceID": 34, "context": "[37] modified this model by allowing entity pairs to express multiple relations, resulting in a Multi-Instance Multi-Label", "startOffset": 0, "endOffset": 4}, {"referenceID": 21, "context": "[23] further extended this approach and included a secondary classifier, which jointly modeled all the sentences in texts and all labels in knowledge bases for a given entity pair.", "startOffset": 0, "endOffset": 4}, {"referenceID": 35, "context": "Other methods apply heuristics [38], model the training data as a generative process [39, 40] or use a low-rank representation of the feature-label matrix to exploit the underlying semantic correlated information.", "startOffset": 31, "endOffset": 35}, {"referenceID": 36, "context": "Other methods apply heuristics [38], model the training data as a generative process [39, 40] or use a low-rank representation of the feature-label matrix to exploit the underlying semantic correlated information.", "startOffset": 85, "endOffset": 93}, {"referenceID": 37, "context": "Other methods apply heuristics [38], model the training data as a generative process [39, 40] or use a low-rank representation of the feature-label matrix to exploit the underlying semantic correlated information.", "startOffset": 85, "endOffset": 93}, {"referenceID": 38, "context": "[41] apply Label Propagation by representing labeled and unlabeled examples as nodes and their similarities as the weights of edges in a graph.", "startOffset": 0, "endOffset": 4}, {"referenceID": 13, "context": "[15] show that several different cluster-based features trained on large corpora can improve the RE effectiveness.", "startOffset": 0, "endOffset": 4}, {"referenceID": 39, "context": "[42] compare DS and complete supervision as training resources but do not attempt to fuse them.", "startOffset": 0, "endOffset": 4}, {"referenceID": 22, "context": "[24] show that providing a relatively small number of mention-level annotations can improve the accuracy of MIML-RE.", "startOffset": 0, "endOffset": 4}, {"referenceID": 21, "context": "[23] marginally outperforms the Mintz++ baseline using solely distant supervision, initialization of the latent variables using labeled data is needed for larger improvements.", "startOffset": 0, "endOffset": 4}, {"referenceID": 23, "context": "[25], incorporates labeled patterns and trigger words to guide MIML-RE during training.", "startOffset": 0, "endOffset": 4}, {"referenceID": 40, "context": "The top performing KBP ESF system [43, 44] used DS and the manual labeling of 100, 000 features.", "startOffset": 34, "endOffset": 42}, {"referenceID": 22, "context": "In relation extraction, typical approaches include queryby-committee [24, 45] and cluster-based sampling [46].", "startOffset": 69, "endOffset": 77}, {"referenceID": 41, "context": "In relation extraction, typical approaches include queryby-committee [24, 45] and cluster-based sampling [46].", "startOffset": 69, "endOffset": 77}, {"referenceID": 42, "context": "In relation extraction, typical approaches include queryby-committee [24, 45] and cluster-based sampling [46].", "startOffset": 105, "endOffset": 109}, {"referenceID": 43, "context": ", patterns, or the occurrence of terms) are labeled as opposed to instances [47, 48], resulting in a higher performance for less supervision.", "startOffset": 76, "endOffset": 84}, {"referenceID": 44, "context": ", patterns, or the occurrence of terms) are labeled as opposed to instances [47, 48], resulting in a higher performance for less supervision.", "startOffset": 76, "endOffset": 84}, {"referenceID": 44, "context": "This approach has been coined as Feature Certainty[48], and we show that this approach is especially effective in DS for features which generalize across many training instances.", "startOffset": 50, "endOffset": 54}, {"referenceID": 45, "context": "Distributional Semantics The Distributional Hypothesis [49] states that words that tend to occur in similar contexts are likely to have similar meanings.", "startOffset": 55, "endOffset": 59}, {"referenceID": 46, "context": "include Word2Vec [51] and GloVe [52].", "startOffset": 32, "endOffset": 36}, {"referenceID": 47, "context": "[53] show that, for short phrases, a simple composition like addition or multiplication of the distributional word representations is competitive with more complex supervised models such as recursive neural networks (RNNs).", "startOffset": 0, "endOffset": 4}, {"referenceID": 48, "context": "Articles are first preprocessed using different components of the Stanford CoreNLP toolkit [55], including sentence segmentation, tokenizing, POS-tagging, Named Entity Recognition, and clustering noun phrases which refer to the same entity.", "startOffset": 91, "endOffset": 95}, {"referenceID": 49, "context": "As in previous work [56, 37], we impose the constraint that both entity mentions (m1,m2) \u2208 R i are contained in the same sentence.", "startOffset": 20, "endOffset": 28}, {"referenceID": 34, "context": "As in previous work [56, 37], we impose the constraint that both entity mentions (m1,m2) \u2208 R i are contained in the same sentence.", "startOffset": 20, "endOffset": 28}, {"referenceID": 50, "context": "Dependency paths have empirically been proven to be very informative for relation extraction, their capability in capturing a lot of information is evidenced by a systematic comparison in effectiveness of different kernel methods [57] or as features in feature-based systems [14].", "startOffset": 230, "endOffset": 234}, {"referenceID": 12, "context": "Dependency paths have empirically been proven to be very informative for relation extraction, their capability in capturing a lot of information is evidenced by a systematic comparison in effectiveness of different kernel methods [57] or as features in feature-based systems [14].", "startOffset": 275, "endOffset": 279}, {"referenceID": 14, "context": "[16] who claimed that the relation expressed by a sentence is often captured in the shortest path connecting the entities in the dependency graph.", "startOffset": 0, "endOffset": 4}, {"referenceID": 44, "context": "This approach is called Feature Certainty Sampling [48].", "startOffset": 51, "endOffset": 55}, {"referenceID": 23, "context": "[25] (without the MIML model) in labeling features, but it differs in some essential aspects.", "startOffset": 0, "endOffset": 4}, {"referenceID": 42, "context": "[46], representing small phrases by summing", "startOffset": 0, "endOffset": 4}, {"referenceID": 51, "context": "Testing Methodology We evaluate the relation extractors in the context of an existing Knowledge Base Population system [58, 59] using the NIST TAC KBP English Slot Filling (ESF) Evaluation from 2012 to 2014.", "startOffset": 119, "endOffset": 127}, {"referenceID": 52, "context": "Testing Methodology We evaluate the relation extractors in the context of an existing Knowledge Base Population system [58, 59] using the NIST TAC KBP English Slot Filling (ESF) Evaluation from 2012 to 2014.", "startOffset": 119, "endOffset": 127}, {"referenceID": 51, "context": "For further details of the KBP system we refer to [58, 59].", "startOffset": 50, "endOffset": 58}, {"referenceID": 52, "context": "For further details of the KBP system we refer to [58, 59].", "startOffset": 50, "endOffset": 58}, {"referenceID": 53, "context": "We also transform the set of one-hot representations using Singular Value Decomposition (SVD) [60] fitted on the complete trainingset.", "startOffset": 94, "endOffset": 98}, {"referenceID": 24, "context": "Next to traditional distant supervision (also known as Mintz++[26], indicated as \u2018Distant Supervision\u2019 in Table 4), we compare our new semi-supervised approach (\u2018Semantic Label Propagation\u2019) to a fully supervised classifier trained by manually labeling 50, 000 instances chosen uniformly among the different relations and according to uncertainty sampling criteria (\u2018Fully Supervised\u2019), and to the classifiers obtained by purely filtering on manually labeled patterns (\u2018Pattern Filtered\u2019).", "startOffset": 62, "endOffset": 66}, {"referenceID": 17, "context": "4% attained using Semantic Label Propagation is equivalent with the second best entry out of eighteen submissions to the 2014 ESF track [19].", "startOffset": 136, "endOffset": 140}, {"referenceID": 42, "context": "Thus, we address the problem of noisy training data obtained through DS by expanding the key idea of automatically filtering of the training data to increase precision (see [46]).", "startOffset": 173, "endOffset": 177}], "year": 2017, "abstractText": "A crucial aspect of a knowledge base population system that extracts new facts from text corpora, is the generation of training data for its relation extractors. In this paper, we present a method that maximizes the effectiveness of newly trained relation extractors at a minimal annotation cost. Manual labeling can be significantly reduced by Distant Supervision, which is a method to construct training data automatically by aligning a large text corpus with an existing knowledge base of known facts. For example, all sentences mentioning both \u2018Barack Obama\u2019 and \u2018US\u2019 may serve as positive training instances for the relation born in(subject,object). However, distant supervision typically results in a highly noisy training set: many training sentences do not really express the intended relation. We propose to combine distant supervision with minimal manual supervision in a technique called feature labeling, to eliminate noise from the large and noisy initial training set, resulting in a significant increase of precision. We further improve on this approach by introducing the Semantic Label Propagation method, which uses the similarity between low-dimensional representations of candidate training instances, to extend the training set in order to increase recall while maintaining high precision. Our proposed strategy for generating training data is studied and evaluated on an established test collection designed for knowledge base population tasks. The experimental results show that the Semantic Label Propagation strategy leads to substantial performance gains when compared to existing approaches, while requiring an almost negligible manual annotation effort.", "creator": "LaTeX with hyperref package"}}}