{"id": "1511.06246", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "19-Nov-2015", "title": "Gaussian Mixture Embeddings for Multiple Word Prototypes", "abstract": "Recently, word representation has been increasingly focused on for its excellent properties in representing the word semantics. Previous works mainly suffer from the problem of polysemy phenomenon. To address this problem, most of previous models represent words as multiple distributed vectors. However, it cannot reflect the rich relations between words by representing words as points in the embedded space. In this paper, we propose the Gaussian mixture skip-gram (GMSG) model to learn the Gaussian mixture embeddings for words based on skip-gram framework. Each word can be regarded as a gaussian mixture distribution in the embedded space, and each gaussian component represents a word sense. Since the number of senses varies from word to word, we further propose the Dynamic GMSG (D-GMSG) model by adaptively increasing the sense number of words during training. Experiments on four benchmarks show the effectiveness of our proposed model.", "histories": [["v1", "Thu, 19 Nov 2015 16:46:49 GMT  (702kb,D)", "http://arxiv.org/abs/1511.06246v1", null]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["xinchi chen", "xipeng qiu", "jingxiang jiang", "xuanjing huang"], "accepted": false, "id": "1511.06246"}, "pdf": {"name": "1511.06246.pdf", "metadata": {"source": "CRF", "title": "Gaussian Mixture Embeddings for Multiple Word Prototypes", "authors": ["Xinchi Chen", "Xipeng Qiu", "Jingxiang Jiang", "Xuanjing Huang"], "emails": ["xinchichen13@fudan.edu.cn", "xpqiu@fudan.edu.cn", "jxjiang14@fudan.edu.cn", "xjhuang@fudan.edu.cn"], "sections": [{"heading": "Introduction", "text": "Most of them can address the problem of the curse of dimensionality and the capture of syntactic and semantic characteristics. In embedded space, words that have similar syntactic and semantic roles are also close together. Thus, distributed word representation can be applied to a rich natural language processing (Collobert and Weston 2008; Collobert et al. 2011; Socher et al. 2013). Most of the previous models show a word as a single point vector in embedded space overlaid by polysemy problems."}, {"heading": "Skip-Gram", "text": "In the Skip-gram model (Figure 2a) (Mikolov et al. 2013c), each word w is represented as a distributed vector ew-W, where W-R | V-D is the word that is the matrix for all words in the vocabulary V-D. d is the dimensionality of the word embedding. Accordingly, each contextual word cw also has a distributed representation vector e-cw-C, where C-R | V-D is another distinguishable space.Skip-gram aims to maximize the probability of the simultaneous occurrence of a word w and its context word cw, which is to be maximized as: v (w | cw) = o-W-NEG (w) p (u-cw) p (u-cw) p (u-W) p (u-W) -W (1)."}, {"heading": "Gaussian Skip-Gram Model", "text": "Although the Skip-gram model is highly efficient and the learned word embeddings have greet properties on syntactic and semantic scrolls, there can be no asymmetric spacing between words. Skip-gram defines the representation of a word as a vector and defines the similarity between two words w1 and w2 using the cosinal spacing of the vectors ew1 and ew2. Contrary to the definition of the Skip-gram model, we argue in this paper that a word can be considered as a function. The similarity between two function words f and g can be formalized as: sim (f, g) = x x, x, x, McCnis f (x) g (x) dx (5) Specifically, if we choose the Gaussian distribution as a function, f (x) = N (x) = \u00b5\u03b1 (\u00b5\u03b1), \u00b5g (x) g (b) = formal, p (\u03b2) = \u00df \u2212 \u03b2 as the similiarity between two g\u03b2 and \u03b2 (7)."}, {"heading": "Gaussian Mixture Skip-Gram Model", "text": "Although the GSG model seems to work well, it cannot solve the problem of polysemy phenomena of words, since they are the same at the level of reality pairs. In this work, we propose the Gaussian mixing model (Figure 2c). GMSG considers each word as a Gaussian mixing distribution. Every component of the Gaussian mixing distribution of a word (Figure 1) can be considered as the meaning of the word. The senses are learned automatically during training based on information about the occurrence of words and their context. Besides working on the polysemy problem, GMSG also detects the richer relationships between words. As shown in Figure 1, it is difficult to tell whose distance is smaller between pairs of words (apple, fruit) and (apple, pear). On the one hand, the word \"apple\" seems much closer to the word \"fruit,\" as apple is a kind of fruit, while \"pear\" and \"apple\" are only syntagmatic relationships."}, {"heading": "Dynamic Gaussian Mixture Skip-Gram Model", "text": "Although we could represent the polysemy of words by using the Gaussian mixture distribution (fw), there is still a short Gaussian mixture model (w). In fact, the number of word senses varies from word to word. To dynamically increase the number of Gaussian word components, we propose a random Gaussian distribution. During the training, a new Gaussian component is created if the similarity of the word and its context is less than greater and is learned automatically during the training. Specifically, we consider the wordw and its context c (w). The similarity of these components is defined as: s (w, c (w)) = 1 | c (w)."}, {"heading": "Relation with Skip-Gram", "text": "In this section, we would like to introduce the relationship between the GMSG model and the prevailing Skip-gram model. Skip-gram model (Mikolov et al. 2013c) is a well-known model for word embedding due to its efficiency and effectiveness. Skip-gram defines the similarity of the word w and its context c as: sim (w, c) = 11 + exp (\u2212 e > w e, c) (22) If the covariances of all words and context words [w] = [f] c = 1 2I are fixed, [fw] = ew, [f] fc = e, [f] c and senumer K = 1, the similarity of the GMSG model can be formalized as: sim (fw, fc) = N (0; \u00b5fw \u2212 fc) = fw \u2212 fc, [f] ew [f] e [f] e."}, {"heading": "Training", "text": "The similarity function of the \"Skip-gram model\" is a point product of vectors that could perform a binary classifier to determine the positive and negative (word, context) loss pairs. In this paper, we use a more complex similarity function that defines an absolute value for each positive and negative pair rather than a relative relationship. Therefore, we minimize a different maximum margin loss function L (\u03b8) to (Joachims 2002) and (Weston, Bengio and Usunier 2011): L (\u03b8) = 1Z (w, cw)."}, {"heading": "Experiments", "text": "To evaluate our proposed methods, we learn the word representation using the Wikipedia corpus1. We experiment with four different benchmarks: WordSim-353, Rel-122, MC and SCWS. Only SCWS provides the contextual information.WordSim-353 WordSim-3532 (Finkelstein et al. 2001) consists of 353 word pairs and their similarity corpuss.1http: / / mattmahoney.net / dc / enwik9.zip 2http: / / www.cs.technion.il /.gabr / resources / data / wordsim353 / wordsim353.htmlSampling rate sr = 3 / 4 Context window size N = 5 Dimensionality of the mean d = 50 initial learning rate \u03b1 = 0.025 Margin-reconstitutional setup = 10 \u2212 8 Min count mc = 5 Number of learning iteration."}, {"heading": "Hyper-parameters", "text": "Hyperparameter settings are listed in Table 1. In this thesis we evaluate our models based on the dimensionality of the mean d = 50 and use diagonal covariance matrices for experiments. We remove all words with occurrences below 5 (minimum number)."}, {"heading": "Model Selection", "text": "Figure 3 shows the performance using different sensory numbers for words. According to the results, the sensory number sn = 3 represents a good trade-off between efficiency and model performance for the GMSG model."}, {"heading": "Word Similarity and Polysemy Phenomenon", "text": "To evaluate our proposed models, we are experimenting with four benchmarks that can be divided into two types. Data sets (Sim-353, Rel-122, and MC) provide only the pairs of words and their similarity values, while SCWS data sets additionally provide the contexts of word pairs. It is a natural way to address the polysemitic problem using context information, which means that a word could have different meanings in another contextual environment. Table 2 shows the results of Sim-353, Rel-122, and MC data sets, which shows that our models perform excellently in word similarity tasks. Table 3 shows the results at 3http: / / www.cs.ucf.edu / seansz / rel-122 / 4http: / / www.cs.cmu.edu / \u02dc mfaruqui / word-sim / EN-MC-30.txt 5http: / / www.socherulx.WorphtWordsWordsWordswordwordwordwordwordwordwordwordwordwordwordwordwordwordwordwordwordwordwordwordwordwordwordwordwordwordwordwordwordwordwordwordwordwordwordwordwordwordwordwordwordwordwordwordwordwordwordwordwordwordwordwordwordwordwordwordwordwordwordwordwordwordwordwordwordwordwordwordwordwordwordwordwordwordwordwordwordwordwordwordwordwordwordwordwordwordwordwordwordwordwordwordwordwordwordwordwordwordwordwordwordwordwordwordwordwordwordwordwordwordwordwordwordwordwordwordwordwordwordwordwordwordwordwordwordwordwordwordwordwordwordwordwordwordwordwordwordwordwordwordwordword"}, {"heading": "Methods WordSim-353 Rel-122 MC", "text": "SCWS dataset showing that our models perform well based on polysemitic phenomena. In this paper, we define the similarity of two words w and w as follows: AvgSim (fw, fw) = sim (fw, fw) = [i] jsim (fwi, fw \u2032 j), (26) where fw and fw \u2032 are the distribution representations of the corresponding words. Table 2 shows the results on the three benchmarks. The size of the word representation of all previous models in this paper is chosen to 50. To address the polysemy problem, we include the contextual information. In this paper, we define the similarity of two words (w, w \u2032) with their contexts (c (w), c (w \u2032), c (w \u2032), c (w \u2032), c \u00b2 (), c \u00b2 (), c \u00b2 (), c \u00b2 ()."}, {"heading": "Related Work", "text": "Many previous work has focused on learning the word as a multiple-bed sense of word in embedded space. However, since the Bengio et al. (2003) model, we have required expensive computing resources. Many work attempts to apply the neural network to sentence modeling. Mikolov et al. (2013a) have proposed the Skip-gram model, which is extremely efficient in removing the hidden layers of neural networks so that a larger corpus could be used to train word representation. By representing a word as a distributed vector, we gain a lot of interesting properties, such as similar words in syntactic or semantic roles that are close to each other in embedded space at cosmic distances or Euclidean distances. In addition, word bedding can work excellently in analogy tasks."}, {"heading": "Conclusions and Further Works", "text": "In this paper, we propose the Gaussian Mixture Skip Program (GMSG) model to represent a word as density in embedded space. A word is represented as a Gaussian mixture distribution, the components of which can be considered the senses of the word. GMSG could reflect the rich relationships of words when different distance measurements are used. As the number of word senses varies from word to word, we also propose the Dynamic GMSG Model (D-GMSG) to adaptively increase the number of words during the training.In fact, a word can be considered as any function, including the Gaussian mixture distribution. Further, we would like to investigate the properties of other functions for word representation and try to find out the nature of the word semantically."}], "references": [{"title": "A neural probabilistic language model", "author": ["Y. Bengio", "R. Ducharme", "P. Vincent", "C. Janvin"], "venue": "The Journal of Machine Learning Research 3:1137\u20131155. Bengio, Y.; Schwenk, H.; Sen\u00e9cal, J.-S.; Morin, F.; and Gauvain, J.-L. 2006. Neural probabilistic language models. In", "citeRegEx": "Bengio et al\\.,? 2003", "shortCiteRegEx": "Bengio et al\\.", "year": 2003}, {"title": "A unified architecture for natural language processing: Deep neural networks with multitask learning", "author": ["R. Collobert", "J. Weston"], "venue": "Proceedings of ICML.", "citeRegEx": "Collobert and Weston,? 2008", "shortCiteRegEx": "Collobert and Weston", "year": 2008}, {"title": "Natural language processing (almost) from scratch", "author": ["R. Collobert", "J. Weston", "L. Bottou", "M. Karlen", "K. Kavukcuoglu", "P. Kuksa"], "venue": "The Journal of Machine Learning Research 12:2493\u20132537.", "citeRegEx": "Collobert et al\\.,? 2011", "shortCiteRegEx": "Collobert et al\\.", "year": 2011}, {"title": "Adaptive subgradient methods for online learning and stochastic optimization", "author": ["J. Duchi", "E. Hazan", "Y. Singer"], "venue": "The Journal of Machine Learning Research 12:2121\u2013 2159.", "citeRegEx": "Duchi et al\\.,? 2011", "shortCiteRegEx": "Duchi et al\\.", "year": 2011}, {"title": "Placing search in context: The concept revisited", "author": ["L. Finkelstein", "E. Gabrilovich", "Y. Matias", "E. Rivlin", "Z. Solan", "G. Wolfman", "E. Ruppin"], "venue": "Proceedings of the 10th international conference on World Wide Web, 406\u2013 414. ACM.", "citeRegEx": "Finkelstein et al\\.,? 2001", "shortCiteRegEx": "Finkelstein et al\\.", "year": 2001}, {"title": "Improving word representations via global context and multiple word prototypes", "author": ["E. Huang", "R. Socher", "C. Manning", "A. Ng"], "venue": "Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), 873\u2013882. Jeju Island, Korea: As-", "citeRegEx": "Huang et al\\.,? 2012", "shortCiteRegEx": "Huang et al\\.", "year": 2012}, {"title": "Optimizing search engines using clickthrough data", "author": ["T. Joachims"], "venue": "Proc. of SIGKDD.", "citeRegEx": "Joachims,? 2002", "shortCiteRegEx": "Joachims", "year": 2002}, {"title": "Topical word embeddings", "author": ["Y. Liu", "Z. Liu", "T.-S. Chua", "M. Sun"], "venue": "AAAI.", "citeRegEx": "Liu et al\\.,? 2015", "shortCiteRegEx": "Liu et al\\.", "year": 2015}, {"title": "Learning contextsensitive word embeddings with neural tensor skip-gram model", "author": ["P. Liu", "X. Qiu", "X. Huang"], "venue": "Proceedings of IJCAI.", "citeRegEx": "Liu et al\\.,? 2015", "shortCiteRegEx": "Liu et al\\.", "year": 2015}, {"title": "Recurrent neural network based language model", "author": ["T. Mikolov", "M. Karafi\u00e1t", "L. Burget", "J. Cernock\u1ef3", "S. Khudanpur"], "venue": "INTERSPEECH.", "citeRegEx": "Mikolov et al\\.,? 2010", "shortCiteRegEx": "Mikolov et al\\.", "year": 2010}, {"title": "Efficient estimation of word representations in vector space", "author": ["T. Mikolov", "K. Chen", "G. Corrado", "J. Dean"], "venue": "arXiv preprint arXiv:1301.3781.", "citeRegEx": "Mikolov et al\\.,? 2013a", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Distributed representations of words and phrases and their compositionality", "author": ["T. Mikolov", "I. Sutskever", "K. Chen", "G.S. Corrado", "J. Dean"], "venue": "NIPS, 3111\u20133119.", "citeRegEx": "Mikolov et al\\.,? 2013b", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Distributed representations of words and phrases and their compositionality", "author": ["T. Mikolov", "I. Sutskever", "K. Chen", "G.S. Corrado", "J. Dean"], "venue": "Advances in Neural Information Processing Systems.", "citeRegEx": "Mikolov et al\\.,? 2013c", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Contextual correlates of semantic similarity", "author": ["G.A. Miller", "W.G. Charles"], "venue": "Language and cognitive processes 6(1):1\u201328.", "citeRegEx": "Miller and Charles,? 1991", "shortCiteRegEx": "Miller and Charles", "year": 1991}, {"title": "Three new graphical models for statistical language modelling", "author": ["A. Mnih", "G. Hinton"], "venue": "Proceedings of ICML.", "citeRegEx": "Mnih and Hinton,? 2007", "shortCiteRegEx": "Mnih and Hinton", "year": 2007}, {"title": "Hierarchical probabilistic neural network language model", "author": ["F. Morin", "Y. Bengio"], "venue": "Proceedings of the international workshop on artificial intelligence and statistics, 246\u2013252. Citeseer.", "citeRegEx": "Morin and Bengio,? 2005", "shortCiteRegEx": "Morin and Bengio", "year": 2005}, {"title": "Efficient non-parametric estimation of multiple embeddings per word in vector space", "author": ["A. Neelakantan", "J. Shankar", "A. Passos", "A. McCallum"], "venue": "Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP).", "citeRegEx": "Neelakantan et al\\.,? 2014", "shortCiteRegEx": "Neelakantan et al\\.", "year": 2014}, {"title": "Multi-prototype vector-space models of word meaning", "author": ["J. Reisinger", "R.J. Mooney"], "venue": "Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics, 109\u2013117. Association for Computational", "citeRegEx": "Reisinger and Mooney,? 2010", "shortCiteRegEx": "Reisinger and Mooney", "year": 2010}, {"title": "Parsing natural scenes and natural language with recursive neural networks", "author": ["R. Socher", "C.C. Lin", "C. Manning", "A.Y. Ng"], "venue": "Proceedings of the 28th International Conference on Machine Learning (ICML-11), 129\u2013136.", "citeRegEx": "Socher et al\\.,? 2011", "shortCiteRegEx": "Socher et al\\.", "year": 2011}, {"title": "Parsing with compositional vector grammars", "author": ["R. Socher", "J. Bauer", "C.D. Manning", "A.Y. Ng"], "venue": "In Proceedings of the ACL conference. Citeseer.", "citeRegEx": "Socher et al\\.,? 2013", "shortCiteRegEx": "Socher et al\\.", "year": 2013}, {"title": "A new set of norms for semantic relatedness measures", "author": ["S.R. Szumlanski", "F. Gomez", "V.K. Sims"], "venue": "ACL (2), 890\u2013895.", "citeRegEx": "Szumlanski et al\\.,? 2013", "shortCiteRegEx": "Szumlanski et al\\.", "year": 2013}, {"title": "A probabilistic model for learning multiprototype word embeddings", "author": ["F. Tian", "H. Dai", "J. Bian", "B. Gao", "R. Zhang", "E. Chen", "T.-Y. Liu"], "venue": "Proceedings of COLING, 151\u2013160.", "citeRegEx": "Tian et al\\.,? 2014", "shortCiteRegEx": "Tian et al\\.", "year": 2014}, {"title": "Word representations: a simple and general method for semi-supervised learning", "author": ["J. Turian", "L. Ratinov", "Y. Bengio"], "venue": "Proceedings of the 48th annual meeting of the association for computational linguistics, 384\u2013394. Association for Computational Linguistics.", "citeRegEx": "Turian et al\\.,? 2010", "shortCiteRegEx": "Turian et al\\.", "year": 2010}, {"title": "Word representations via gaussian embedding", "author": ["L. Vilnis", "A. McCallum"], "venue": "arXiv preprint arXiv:1412.6623.", "citeRegEx": "Vilnis and McCallum,? 2014", "shortCiteRegEx": "Vilnis and McCallum", "year": 2014}, {"title": "Wsabie: Scaling up to large vocabulary image annotation", "author": ["J. Weston", "S. Bengio", "N. Usunier"], "venue": "IJCAI, volume 11, 2764\u20132770.", "citeRegEx": "Weston et al\\.,? 2011", "shortCiteRegEx": "Weston et al\\.", "year": 2011}], "referenceMentions": [{"referenceID": 0, "context": "Distributed word representation has been studied for a considerable efforts (Bengio et al. 2003; Morin and Bengio 2005; Bengio et al. 2006; Mnih and Hinton 2007; Mikolov et al. 2010; Turian, Ratinov, and Bengio 2010; Reisinger and Mooney 2010; Huang et al. 2012; Mikolov et al. 2013a; 2013b).", "startOffset": 76, "endOffset": 291}, {"referenceID": 15, "context": "Distributed word representation has been studied for a considerable efforts (Bengio et al. 2003; Morin and Bengio 2005; Bengio et al. 2006; Mnih and Hinton 2007; Mikolov et al. 2010; Turian, Ratinov, and Bengio 2010; Reisinger and Mooney 2010; Huang et al. 2012; Mikolov et al. 2013a; 2013b).", "startOffset": 76, "endOffset": 291}, {"referenceID": 14, "context": "Distributed word representation has been studied for a considerable efforts (Bengio et al. 2003; Morin and Bengio 2005; Bengio et al. 2006; Mnih and Hinton 2007; Mikolov et al. 2010; Turian, Ratinov, and Bengio 2010; Reisinger and Mooney 2010; Huang et al. 2012; Mikolov et al. 2013a; 2013b).", "startOffset": 76, "endOffset": 291}, {"referenceID": 9, "context": "Distributed word representation has been studied for a considerable efforts (Bengio et al. 2003; Morin and Bengio 2005; Bengio et al. 2006; Mnih and Hinton 2007; Mikolov et al. 2010; Turian, Ratinov, and Bengio 2010; Reisinger and Mooney 2010; Huang et al. 2012; Mikolov et al. 2013a; 2013b).", "startOffset": 76, "endOffset": 291}, {"referenceID": 17, "context": "Distributed word representation has been studied for a considerable efforts (Bengio et al. 2003; Morin and Bengio 2005; Bengio et al. 2006; Mnih and Hinton 2007; Mikolov et al. 2010; Turian, Ratinov, and Bengio 2010; Reisinger and Mooney 2010; Huang et al. 2012; Mikolov et al. 2013a; 2013b).", "startOffset": 76, "endOffset": 291}, {"referenceID": 5, "context": "Distributed word representation has been studied for a considerable efforts (Bengio et al. 2003; Morin and Bengio 2005; Bengio et al. 2006; Mnih and Hinton 2007; Mikolov et al. 2010; Turian, Ratinov, and Bengio 2010; Reisinger and Mooney 2010; Huang et al. 2012; Mikolov et al. 2013a; 2013b).", "startOffset": 76, "endOffset": 291}, {"referenceID": 10, "context": "Distributed word representation has been studied for a considerable efforts (Bengio et al. 2003; Morin and Bengio 2005; Bengio et al. 2006; Mnih and Hinton 2007; Mikolov et al. 2010; Turian, Ratinov, and Bengio 2010; Reisinger and Mooney 2010; Huang et al. 2012; Mikolov et al. 2013a; 2013b).", "startOffset": 76, "endOffset": 291}, {"referenceID": 1, "context": "Thus, distributed word representation is applied to a abundant natural language processing (NLP) tasks (Collobert and Weston 2008; Collobert et al. 2011; Socher et al. 2011; 2013).", "startOffset": 103, "endOffset": 179}, {"referenceID": 2, "context": "Thus, distributed word representation is applied to a abundant natural language processing (NLP) tasks (Collobert and Weston 2008; Collobert et al. 2011; Socher et al. 2011; 2013).", "startOffset": 103, "endOffset": 179}, {"referenceID": 18, "context": "Thus, distributed word representation is applied to a abundant natural language processing (NLP) tasks (Collobert and Weston 2008; Collobert et al. 2011; Socher et al. 2011; 2013).", "startOffset": 103, "endOffset": 179}, {"referenceID": 15, "context": "Reisinger and Mooney (2010) constructs multiple high-dimensional vectors for each word.", "startOffset": 0, "endOffset": 28}, {"referenceID": 5, "context": "Huang et al. (2012) learns multiple dense embeddings for each word using global document context.", "startOffset": 0, "endOffset": 20}, {"referenceID": 5, "context": "Huang et al. (2012) learns multiple dense embeddings for each word using global document context. Neelakantan et al. (2014) proposed multi-sense skip-gram (MSSG) to learn word sense embeddings with online word sense discrimination.", "startOffset": 0, "endOffset": 124}, {"referenceID": 23, "context": "In this paper, we propose the Gaussian mixture skip-gram (GMSG) model inspired by Vilnis and McCallum (2014), who maps a word as a gaussian distribution instead of a point vector in the embedded space.", "startOffset": 82, "endOffset": 109}, {"referenceID": 12, "context": "In the skip-gram model (Figure 2a) (Mikolov et al. 2013c), each word w is represented as a distributed vector ew \u2208W, where W \u2208 R|V |\u2217d is the word embedding matrix for all words in word vocabulary V .", "startOffset": 35, "endOffset": 57}, {"referenceID": 23, "context": "Since KL divergence is not symmetric metric, Gaussian skip-gram (GSG) model (Figure 2b) (Vilnis and McCallum 2014) could measure the distance between words asymmetrically.", "startOffset": 88, "endOffset": 114}, {"referenceID": 12, "context": "Skip-gram model (Mikolov et al. 2013c) is a well known model for learning word embeddings for its efficiency and effectiveness.", "startOffset": 16, "endOffset": 38}, {"referenceID": 6, "context": "Thus, we minimise a different max-margin based loss function L(\u03b8) following (Joachims 2002) and (Weston, Bengio, and Usunier 2011):", "startOffset": 76, "endOffset": 91}, {"referenceID": 23, "context": "Following Vilnis and McCallum (2014), we use diagonal covariance matrices with a hard constraint that the eigenvalues % of the covariance matrices lie within the interval [m,M ].", "startOffset": 10, "endOffset": 37}, {"referenceID": 4, "context": "WordSim-353 WordSim-3532 (Finkelstein et al. 2001) consists of 353 pairs of words and their similarity scores.", "startOffset": 25, "endOffset": 50}, {"referenceID": 13, "context": "MC MC4 (Miller and Charles 1991) contains 30 pairs of nouns that vary from high to low semantic similarity.", "startOffset": 7, "endOffset": 32}, {"referenceID": 5, "context": "SCWS SCWS5 (Huang et al. 2012) consists of 2003 word pairs and their contextual information.", "startOffset": 11, "endOffset": 30}, {"referenceID": 16, "context": "MSSG (Neelakantan et al. 2014) model sets the same sense number for each word.", "startOffset": 5, "endOffset": 30}, {"referenceID": 23, "context": "GSG (Vilnis and McCallum 2014) has several variations.", "startOffset": 4, "endOffset": 30}, {"referenceID": 17, "context": "Pruned TFIDF (Reisinger and Mooney 2010) uses spare, high-dimensional word representations.", "startOffset": 13, "endOffset": 40}, {"referenceID": 1, "context": "C&W (Collobert and Weston 2008) is a language model.", "startOffset": 4, "endOffset": 31}, {"referenceID": 5, "context": "Huang (Huang et al. 2012) is a neural network model for learning multi-representations per word.", "startOffset": 6, "endOffset": 25}, {"referenceID": 0, "context": "Bengio et al. (2003) applies the neural network to sentence modelling instead of using n-gram language models.", "startOffset": 0, "endOffset": 21}, {"referenceID": 0, "context": "Bengio et al. (2003) applies the neural network to sentence modelling instead of using n-gram language models. Since the Bengio et al. (2003) model need expensive computational resource.", "startOffset": 0, "endOffset": 142}, {"referenceID": 0, "context": "Bengio et al. (2003) applies the neural network to sentence modelling instead of using n-gram language models. Since the Bengio et al. (2003) model need expensive computational resource. Lots of works are trying to optimize it. Mikolov et al. (2013a) proposed the skip-gram model which is extremely efficient by removing the hidden layers of the neural networks, so that larger corpus could be used to train the word representation.", "startOffset": 0, "endOffset": 251}, {"referenceID": 0, "context": "Bengio et al. (2003) applies the neural network to sentence modelling instead of using n-gram language models. Since the Bengio et al. (2003) model need expensive computational resource. Lots of works are trying to optimize it. Mikolov et al. (2013a) proposed the skip-gram model which is extremely efficient by removing the hidden layers of the neural networks, so that larger corpus could be used to train the word representation. By representing a word as a distributed vector, we gain a lot of interesting properties, such as similar words in syntactic or semantic roles are also close with each other in the embedded space in cosine distance or Euclidian distance. In addition, word embeddings also perform excellently in analogy tasks. For instance, eking \u2212 equeen \u2248 eman \u2212 ewoman. However, previous models mainly suffer from the polysemy problem. To address this problem, Reisinger and Mooney (2010) represents words by constructing multiple sparse, high-dimensional vectors.", "startOffset": 0, "endOffset": 907}, {"referenceID": 0, "context": "Bengio et al. (2003) applies the neural network to sentence modelling instead of using n-gram language models. Since the Bengio et al. (2003) model need expensive computational resource. Lots of works are trying to optimize it. Mikolov et al. (2013a) proposed the skip-gram model which is extremely efficient by removing the hidden layers of the neural networks, so that larger corpus could be used to train the word representation. By representing a word as a distributed vector, we gain a lot of interesting properties, such as similar words in syntactic or semantic roles are also close with each other in the embedded space in cosine distance or Euclidian distance. In addition, word embeddings also perform excellently in analogy tasks. For instance, eking \u2212 equeen \u2248 eman \u2212 ewoman. However, previous models mainly suffer from the polysemy problem. To address this problem, Reisinger and Mooney (2010) represents words by constructing multiple sparse, high-dimensional vectors. Huang et al. (2012) is an neural network based approach, which learns multiple dense, low-dimensional embeddings using global document context.", "startOffset": 0, "endOffset": 1003}, {"referenceID": 0, "context": "Bengio et al. (2003) applies the neural network to sentence modelling instead of using n-gram language models. Since the Bengio et al. (2003) model need expensive computational resource. Lots of works are trying to optimize it. Mikolov et al. (2013a) proposed the skip-gram model which is extremely efficient by removing the hidden layers of the neural networks, so that larger corpus could be used to train the word representation. By representing a word as a distributed vector, we gain a lot of interesting properties, such as similar words in syntactic or semantic roles are also close with each other in the embedded space in cosine distance or Euclidian distance. In addition, word embeddings also perform excellently in analogy tasks. For instance, eking \u2212 equeen \u2248 eman \u2212 ewoman. However, previous models mainly suffer from the polysemy problem. To address this problem, Reisinger and Mooney (2010) represents words by constructing multiple sparse, high-dimensional vectors. Huang et al. (2012) is an neural network based approach, which learns multiple dense, low-dimensional embeddings using global document context. Tian et al. (2014) modeled the word polysemy from a probabilistic perspective and integrate it with the highly efficient continuous Skip-Gram model.", "startOffset": 0, "endOffset": 1146}, {"referenceID": 0, "context": "Bengio et al. (2003) applies the neural network to sentence modelling instead of using n-gram language models. Since the Bengio et al. (2003) model need expensive computational resource. Lots of works are trying to optimize it. Mikolov et al. (2013a) proposed the skip-gram model which is extremely efficient by removing the hidden layers of the neural networks, so that larger corpus could be used to train the word representation. By representing a word as a distributed vector, we gain a lot of interesting properties, such as similar words in syntactic or semantic roles are also close with each other in the embedded space in cosine distance or Euclidian distance. In addition, word embeddings also perform excellently in analogy tasks. For instance, eking \u2212 equeen \u2248 eman \u2212 ewoman. However, previous models mainly suffer from the polysemy problem. To address this problem, Reisinger and Mooney (2010) represents words by constructing multiple sparse, high-dimensional vectors. Huang et al. (2012) is an neural network based approach, which learns multiple dense, low-dimensional embeddings using global document context. Tian et al. (2014) modeled the word polysemy from a probabilistic perspective and integrate it with the highly efficient continuous Skip-Gram model. Neelakantan et al. (2014) proposed Multi-Sense Skip-Gram (MSSG) to learn word sense embeddings with online word sense discrimination.", "startOffset": 0, "endOffset": 1302}, {"referenceID": 0, "context": "Bengio et al. (2003) applies the neural network to sentence modelling instead of using n-gram language models. Since the Bengio et al. (2003) model need expensive computational resource. Lots of works are trying to optimize it. Mikolov et al. (2013a) proposed the skip-gram model which is extremely efficient by removing the hidden layers of the neural networks, so that larger corpus could be used to train the word representation. By representing a word as a distributed vector, we gain a lot of interesting properties, such as similar words in syntactic or semantic roles are also close with each other in the embedded space in cosine distance or Euclidian distance. In addition, word embeddings also perform excellently in analogy tasks. For instance, eking \u2212 equeen \u2248 eman \u2212 ewoman. However, previous models mainly suffer from the polysemy problem. To address this problem, Reisinger and Mooney (2010) represents words by constructing multiple sparse, high-dimensional vectors. Huang et al. (2012) is an neural network based approach, which learns multiple dense, low-dimensional embeddings using global document context. Tian et al. (2014) modeled the word polysemy from a probabilistic perspective and integrate it with the highly efficient continuous Skip-Gram model. Neelakantan et al. (2014) proposed Multi-Sense Skip-Gram (MSSG) to learn word sense embeddings with online word sense discrimination. These models perform word sense discrimination by clustering context of words. Liu et al. (2015) discriminates word sense by introducing latent topic model to globally cluster the words into different topics.", "startOffset": 0, "endOffset": 1507}, {"referenceID": 0, "context": "Bengio et al. (2003) applies the neural network to sentence modelling instead of using n-gram language models. Since the Bengio et al. (2003) model need expensive computational resource. Lots of works are trying to optimize it. Mikolov et al. (2013a) proposed the skip-gram model which is extremely efficient by removing the hidden layers of the neural networks, so that larger corpus could be used to train the word representation. By representing a word as a distributed vector, we gain a lot of interesting properties, such as similar words in syntactic or semantic roles are also close with each other in the embedded space in cosine distance or Euclidian distance. In addition, word embeddings also perform excellently in analogy tasks. For instance, eking \u2212 equeen \u2248 eman \u2212 ewoman. However, previous models mainly suffer from the polysemy problem. To address this problem, Reisinger and Mooney (2010) represents words by constructing multiple sparse, high-dimensional vectors. Huang et al. (2012) is an neural network based approach, which learns multiple dense, low-dimensional embeddings using global document context. Tian et al. (2014) modeled the word polysemy from a probabilistic perspective and integrate it with the highly efficient continuous Skip-Gram model. Neelakantan et al. (2014) proposed Multi-Sense Skip-Gram (MSSG) to learn word sense embeddings with online word sense discrimination. These models perform word sense discrimination by clustering context of words. Liu et al. (2015) discriminates word sense by introducing latent topic model to globally cluster the words into different topics. Liu, Qiu, and Huang (2015) further extended this work to model the complicated interactions of word embedding and its corresponding topic embedding by incorporating the tensor method.", "startOffset": 0, "endOffset": 1646}, {"referenceID": 0, "context": "Bengio et al. (2003) applies the neural network to sentence modelling instead of using n-gram language models. Since the Bengio et al. (2003) model need expensive computational resource. Lots of works are trying to optimize it. Mikolov et al. (2013a) proposed the skip-gram model which is extremely efficient by removing the hidden layers of the neural networks, so that larger corpus could be used to train the word representation. By representing a word as a distributed vector, we gain a lot of interesting properties, such as similar words in syntactic or semantic roles are also close with each other in the embedded space in cosine distance or Euclidian distance. In addition, word embeddings also perform excellently in analogy tasks. For instance, eking \u2212 equeen \u2248 eman \u2212 ewoman. However, previous models mainly suffer from the polysemy problem. To address this problem, Reisinger and Mooney (2010) represents words by constructing multiple sparse, high-dimensional vectors. Huang et al. (2012) is an neural network based approach, which learns multiple dense, low-dimensional embeddings using global document context. Tian et al. (2014) modeled the word polysemy from a probabilistic perspective and integrate it with the highly efficient continuous Skip-Gram model. Neelakantan et al. (2014) proposed Multi-Sense Skip-Gram (MSSG) to learn word sense embeddings with online word sense discrimination. These models perform word sense discrimination by clustering context of words. Liu et al. (2015) discriminates word sense by introducing latent topic model to globally cluster the words into different topics. Liu, Qiu, and Huang (2015) further extended this work to model the complicated interactions of word embedding and its corresponding topic embedding by incorporating the tensor method. Almost previous works are trying to use multiple points to represent the multiple senses of words. However, it cannot reflect the rich relations between words by simply representing words as points in the embedded space. Vilnis and McCallum (2014) represented a word as a gaussian distribution.", "startOffset": 0, "endOffset": 2051}], "year": 2015, "abstractText": "Recently, word representation has been increasingly focused on for its excellent properties in representing the word semantics. Previous works mainly suffer from the problem of polysemy phenomenon. To address this problem, most of previous models represent words as multiple distributed vectors. However, it cannot reflect the rich relations between words by representing words as points in the embedded space. In this paper, we propose the Gaussian mixture skip-gram (GMSG) model to learn the Gaussian mixture embeddings for words based on skip-gram framework. Each word can be regarded as a gaussian mixture distribution in the embedded space, and each gaussian component represents a word sense. Since the number of senses varies from word to word, we further propose the Dynamic GMSG (D-GMSG) model by adaptively increasing the sense number of words during training. Experiments on four benchmarks show the effectiveness of our proposed model.", "creator": "TeX"}}}