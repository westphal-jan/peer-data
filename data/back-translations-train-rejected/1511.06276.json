{"id": "1511.06276", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "19-Nov-2015", "title": "Faster method for Deep Belief Network based Object classification using DWT", "abstract": "A Deep Belief Network (DBN) requires large, multiple hidden layers with high number of hidden units to learn good features from the raw pixels of large images. This implies more training time as well as computational complexity. By integrating DBN with Discrete Wavelet Transform (DWT), both training time and computational complexity can be reduced. The low resolution images obtained after application of DWT are used to train multiple DBNs. The results obtained from these DBNs are combined using a weighted voting algorithm. The performance of this method is found to be competent and faster in comparison with that of traditional DBNs.", "histories": [["v1", "Thu, 19 Nov 2015 17:41:08 GMT  (407kb)", "http://arxiv.org/abs/1511.06276v1", null]], "reviews": [], "SUBJECTS": "cs.CV cs.LG", "authors": ["saurabh sihag", "pranab kumar dutta"], "accepted": false, "id": "1511.06276"}, "pdf": {"name": "1511.06276.pdf", "metadata": {"source": "CRF", "title": "Faster method for Deep Belief Network based Object Classification using DWT", "authors": ["Saurabh Sihag", "Pranab Kr. Dutta"], "emails": [], "sections": [{"heading": null, "text": "This year it is more than ever before."}], "references": [{"title": "High speed deep networks based on Discrete Cosine Transformation,", "author": ["Xiaoyi Zou", "Xiangmin Xu", "Chunmei Qing", "Xiaofen Xing"], "venue": "IEEE International Conference on Image Processing (ICIP),", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2014}, {"title": "Rough-neural image classification using wavelet transform,", "author": ["Jun-Hai Zhai", "Xi-Zhao Wang", "Su-Fang Zhang"], "venue": "IEEE International Conference on Machine Learning and Cybernetics,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2007}, {"title": "An introduction to restricted Boltzmann machines,", "author": ["Asja Fischer", "Christian Igel"], "venue": "Progress in Pattern Recognition,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2012}, {"title": "Columbia object image library (COIL-20),", "author": ["SA Nene", "SK Nayar", "H Murase"], "venue": "Technical Report CUCS-005-96,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 1996}, {"title": "A supervised non-linear dimensionality reduction approach for manifold learning,", "author": ["B Raducanu", "F Dornaika"], "venue": "Pattern Recognition,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2012}, {"title": "Optimal Feature Extraction and Classification of Tensors via Matrix Product State Decomposition,", "author": ["J.A. Bengua", "H. N Phien", "H.D. Tuan"], "venue": "arXiv preprint arXiv:1503.00516,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2015}, {"title": "Sparse penalty in deep belief networks: using the mixed norm constraint,", "author": ["X Halkias", "S Paris", "H Glotin"], "venue": "arXiv preprint arXiv:1301.3533,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2013}], "referenceMentions": [{"referenceID": 0, "context": "One such approach is presented in [1] which illustrates the use of Discrete Cosine Transform (DCT) for image classification.", "startOffset": 34, "endOffset": 37}, {"referenceID": 1, "context": "Wavelet transform, rough set theory, and artificial neural networks are combined together to form a hybrid image classification method in [2].", "startOffset": 138, "endOffset": 141}, {"referenceID": 2, "context": "Theoretical Description: Restricted Boltzmann machines (RBMs) [4] are probabilistic models that are used as nonlinear unsupervised feature learners, consisting of a set of binary hidden units h, a set of (binary or real-valued) visible units v, and a weight matrix W associated with the connections between the two layers.", "startOffset": 62, "endOffset": 65}, {"referenceID": 3, "context": "COIL-20 Dataset Columbia Object Image Library (COIL-20) [6] consists of gray-scale images of 20 objects, as shown in Figure 3.", "startOffset": 56, "endOffset": 59}, {"referenceID": 1, "context": "The results for this dataset are compared with that of the wavelet transform based method adopted in [2], in which 5 objects are chosen at random.", "startOffset": 101, "endOffset": 104}, {"referenceID": 1, "context": "These 5 categories are evaluated at 75/25 hold out ratio (training images/test images) to compare the results obtained using the approach proposed in this letter and those specified in [2] as shown in Table 1.", "startOffset": 185, "endOffset": 188}, {"referenceID": 1, "context": "Architecture of Each DBN: 10 hidden units and 5 output units Architecture of ANN used in [2]: 82 hidden units and 5 output units Table 1 (Comparison of the results mentioned in [2] and results obtained using the approach in this letter) Object ID Approach from [2] Our Approach", "startOffset": 89, "endOffset": 92}, {"referenceID": 1, "context": "Architecture of Each DBN: 10 hidden units and 5 output units Architecture of ANN used in [2]: 82 hidden units and 5 output units Table 1 (Comparison of the results mentioned in [2] and results obtained using the approach in this letter) Object ID Approach from [2] Our Approach", "startOffset": 177, "endOffset": 180}, {"referenceID": 1, "context": "Architecture of Each DBN: 10 hidden units and 5 output units Architecture of ANN used in [2]: 82 hidden units and 5 output units Table 1 (Comparison of the results mentioned in [2] and results obtained using the approach in this letter) Object ID Approach from [2] Our Approach", "startOffset": 261, "endOffset": 264}, {"referenceID": 4, "context": "Results on the whole dataset are compared with that of methods adopted in [7] and [8] for 70/30 hold out ratio (training images/test images) in Table-2.", "startOffset": 74, "endOffset": 77}, {"referenceID": 5, "context": "Results on the whole dataset are compared with that of methods adopted in [7] and [8] for 70/30 hold out ratio (training images/test images) in Table-2.", "startOffset": 82, "endOffset": 85}, {"referenceID": 6, "context": "Results for other DBN based methods [10] are given in Table 4", "startOffset": 36, "endOffset": 40}, {"referenceID": 6, "context": "Table 4 (Results on USPS dataset mentioned in [10]; CPU: AMD Opteron processor 8435, 2.", "startOffset": 46, "endOffset": 50}, {"referenceID": 6, "context": "Therefore, from the results in Table 3 and Table 4, it can be concluded that the approach in this letter can give faster results compared to other DBN based methods listed in [10], albeit with a little trade off in accuracy.", "startOffset": 175, "endOffset": 179}], "year": 2015, "abstractText": "A Deep Belief Network (DBN) requires large, multiple hidden layers with high number of hidden units to learn good features from the raw pixels of large images. This implies more training time as well as computational complexity. By integrating DBN with Discrete Wavelet Transform (DWT), both training time and computational complexity can be reduced. The low resolution images obtained after application of DWT are used to train multiple DBNs. The results obtained from these DBNs are combined using a weighted voting algorithm. The performance of this method is found to be competent and faster in comparison with that of traditional DBNs.", "creator": "SPDF"}}}