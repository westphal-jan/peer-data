{"id": "1511.06297", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "19-Nov-2015", "title": "Conditional Computation in Neural Networks for faster models", "abstract": "Deep learning has become the state-of-art tool in many applications, but the evaluation and training of deep models can be time-consuming and computationally expensive. Dropout has been shown to be an effective strategy to sparsify computations (by not involving all units), as well as to regularize models. In typical dropout, nodes are dropped uniformly at random. Our goal is to use reinforcement learning in order to design better, more informed dropout policies, which are data-dependent. We cast the problem of learning activation-dependent dropout policies for blocks of units as a reinforcement learning problem. We propose a learning scheme motivated by computation speed, capturing the idea of wanting to have parsimonious activations while maintaining prediction accuracy. We apply a policy gradient algorithm for learning policies that optimize this loss function and propose a regularization mechanism that encourages diversification of the dropout policy. We present encouraging empirical results showing that this approach improves the speed of computation without impacting the quality of the approximation.", "histories": [["v1", "Thu, 19 Nov 2015 18:40:22 GMT  (748kb,D)", "http://arxiv.org/abs/1511.06297v1", "ICLR 2016 submission"], ["v2", "Thu, 7 Jan 2016 22:41:10 GMT  (752kb,D)", "http://arxiv.org/abs/1511.06297v2", "ICLR 2016 submission, revised"]], "COMMENTS": "ICLR 2016 submission", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["emmanuel bengio", "pierre-luc bacon", "joelle pineau", "doina precup"], "accepted": false, "id": "1511.06297"}, "pdf": {"name": "1511.06297.pdf", "metadata": {"source": "CRF", "title": "CONDITIONAL COMPUTATION IN NEURAL NETWORKS FOR FASTER MODELS", "authors": ["Emmanuel Bengio", "Pierre-Luc Bacon", "Joelle Pineau", "Doina Precup"], "emails": ["ebengi@cs.mcgill.ca", "pbacon@cs.mcgill.ca", "jpineau@cs.mcgill.ca", "dprecup@cs.mcgill.ca"], "sections": [{"heading": null, "text": "Deep learning has become a state-of-the-art tool in many applications, but evaluating and training deep models can be time-consuming and computationally expensive. Dropout has been proven to be an effective strategy for saving calculations (by not including all units) and regulating models. In typical dropouts, nodes are randomly dropped as a reinforcing learning problem. We propose a computing speed-driven learning scheme that captures the idea of economical activations while maintaining predictive accuracy. We apply a policy-gradient algorithm to learn strategies that optimize this loss function and propose a regulatory mechanism that improves the diversification of the dropout policy that we present without encouraging the diversification of the policy."}, {"heading": "1 INTRODUCTION", "text": "Large-scale neural networks, and in particular deep-learning architectures, have seen a surge in popularity in recent years due to their impressive empirical performance in complex, monitored learning tasks, including state-of-the-art image and speech recognition capabilities (He et al., 2015), but the task of training such networks remains a challenging optimization problem, with several associated problems: very long training times (several weeks on modern computers, for some problems), potential for over-adaptation (where the learned function is too specific to the training data and poorly generalizes to invisible data), and, technically, the disappearing progression problem (Hochreiter, 1991; Bengio et al., 1994), with progression information becoming increasingly diffuse as it spreads from layer to layer. A technique called dropouts was introduced by Hinton et al (2012) as a way to reduce over-adaptation by breaking the co-node."}, {"heading": "2 PROBLEM FORMULATION", "text": "We raise the problem of learning the input-dependent dropout probabilities at each level in the context of the Markov decision-making processes (Puterman, 1994). We define a discrete time, a continuous state and a discrete action MDP < S, U, P (\u00b7 | s, u), C > with C the cost function and P (\u00b7 | s, u) the distribution over the next state given the fact that the action u is performed in state s. As in the original dropout paper (Hinton et al., 2012), each node in a given layer has a failover mask over the units of a given layer. We define the state space of the MDP by the vector-weighted activations of all nodes in the previous layer. As in the original dropout paper (Hinton et al., 2012), each node in a given layer has an associated Bernoulli distribution that determines its probability of being dropped."}, {"heading": "3 LEARNING SIGMOID-BERNOULLI POLICIES", "text": "We use the probability ratio method (Williams, 1992) to learn the parameters of the Sigmoid Bernoulli strategies. As the nature of the observation space changes with each decision step, we learn the L disjunction strategies (one for each level l of the deep network). As a result, the sum disappears in the policy gradient and becomes: E {C (x), E (l), L disjunction strategies (u (l) | s (l)))) (2), since Vicl only appears in the l decision phase and the gradient is otherwise nil. To estimate (2) from the samples, we have to propagate through many instances simultaneously, which we achieve through mini lots of size mb. This approach has the advantage of making optimal use of the fast matrix matrix capabilities of modern hardware. Under the mini-lot setting, s (l) becomes a vector of dimension mb."}, {"heading": "3.1 FAST VECTOR-JACOBIAN MULTIPLICATION", "text": "While Eqn (3) suggests that the Jacobins must be explicitly formed, Pearlmutter (1994) showed that the calculation of a differential derivative is sufficient to calculate the left- or right-vector Jacobin (or Hessian) multiplication, and the same trick has recently been revived with the class of so-called \"Hessian-free\" (Martens, 2010) methods for artificial neural networks. Using Pearlmutter's notation (1994), we write writeR\u03b8l {\u00b7} = c > \u03b8l for the differential operator."}, {"heading": "3.2 SPARSITY AND VARIANCE REGULARIZATIONS", "text": "In order to favour the \"opt-out\" policy with sparse measures, we add two penalty terms Lb and Le, which depend on a certain target spareness rate. Therefore, the first term pushes the policy distribution \u03c0 to activate each unit with a high probability of activating the data. The second term pushes the policy distribution to have the desired scarcity of activations for each example. Therefore, a valid configuration with a low probability would consist of learning a few activations for a portion of the data and activations with a low probability of the rest of the data, which leads to the probability of activation being expected. Lb = n \u00b2 J \u00b2 E {p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p (1 n \u00b2 j \u00b2 p) \u2212 p \u00b2 p \u00b2 p \u00b2 p p (5) Since we are in a minibatch setting p p p p p \u00b2 p p p p p \u00b2 p p p p p p \u00b2 p p p p \u00b2 p p p \u00b2 p p \u00b2 p \u00b2 p \u00b2 p p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p = minibatch expectations can be approximated: Lb \u00b2 p p p \u00b2 p p \u00b2 p \u00b2 p p \u00b2 p \u00b2 p p \u00b2 p \u00b2 p p \u00b2 p \u00b2 p p \u00b2 p \u00b2 p p \u00b2 p \u00b2 p \u00b2 p p p \u00b2 p p \u00b2 p \u00b2 p p = p p \u00b2 p \u00b2 p p p = p p p \u00b2 p p p p p \u00b2 p p p p p \u00b2 p p p p p p \u00b2 p p p p p p p p p p p p p p p p p p p \u00b2 p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p) p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p) p p p p p p p p p p p p p p p p p p p p p"}, {"heading": "3.3 ALGORITHM", "text": "First, we update the network and policy parameters to minimize the following regulated loss function by back propagation (Rumelhart et al., 1988): L = \u2212 logP (Y | X, \u03b8) + \u03bbs (Lb + Le) + \u03bbv (Lv) + \u03bbL2 \u0418\u043a 2, the results being understood as a target parameter between predictive accuracy and computational parity (determined by sparse node activation) and as a target parameter between a stochastic policy and a more input-dependent saturated policy. Next, we minimize the cost function C using a REINFORCE-style approach to update policy parameters (Williams, 1992): C = \u2212 logP (Y | X, 2001) As mentioned above, we use minibatch stochastic gradient deviation and minibatch policy gradient updates."}, {"heading": "3.4 BLOCK POLICY DROPOUT", "text": "In order to achieve computational gains, instead of throwing individual units into hidden layers, we drop contiguous (equally large) groups of units (regardless of each example in the minibatch), which reduces the scope for manoeuvre and the number of probabilities for calculation and sample. Therefore, there are two potential accelerations: First, the policy is much smaller and faster to calculate; second, it offers a computational advantage in calculating the hidden layer itself, since we are now doing a matrix multiplication of the following form: C = (A Ma) B) Mc, where Ma and Mc are binary mask matrices similar to this (here there are 3 blocks of size 2): 0 0 1 0 0... 0 1 1 1 1 1 1 1 This allows us to multiply the matrix quickly by taking into account only the unequal output elements in C and the unequal elements in A Ma."}, {"heading": "4 EXPERIMENTS", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.1 MODEL IMPLEMENTATION", "text": "The proposed model was implemented within Theano (Bergstra et al., 2010), a standard library for deep learning and neural networks. In addition to the optimizations offered by Theano, we also implemented a specialized matrix multiplication code for the operation revealed in Section 3.4. A simple and rather na\u00efve implementation of this operation resulted in accelerations of up to 5-10x, while an equally na\u00efve GPU implementation yielded accelerations of up to 2-4x, both at savings rates below 20% as well as acceptable matrix and block sizes. 1We otherwise use fairly standard methods for our neural network. Weight matrices are initialized with the heuristics of Glorot & Bengio (2010)."}, {"heading": "4.2 MODEL EVALUATION", "text": "We first evaluate the performance of our model on the MNIST digit dataset. We use a single hidden layer of 16 blocks of 16 units (256 units in total), with a target spareness rate of 5% = 1 / 16, learning rates of 10 \u2212 3 for the neural network and 5 \u00b7 10 \u2212 5 for the policy, \u03bbv = \u03bbs = 200 and \u03bbL2 = 0.005. Under these conditions, a test spareness rate of about 2.3% is achieved. A normal neural network with the same number of hidden units achieves a test error of about 1.9%, while a normal neural network with a similar amount of calculation (multiplied-added) (32 hidden units) achieves a test error of about 2.8%.1Implementations used in this paper can be found at http: / / github.com / bengioe / Looking at the activation of the policy (1c), we see that it is hypothesized towards what was hypothesized in Section 3.2, with most examples of e.i.i."}, {"heading": "4.3 EFFECTS OF REGULARIZATION", "text": "The additional regulation proposed in Section 3.2 seems to play an important role in our ability to train the conditional model. If we only use the predictive value, we observed that the algorithm tried to compensate for this by recruiting more units and saturating their participation probability, or even failed by rejecting measures that were likely to be considered bad units very early on. In practice, the variance regulation term Lv only slightly influences the predictive accuracy and the learned strategies of the models, but we observed that it significantly accelerates the training process, probably by encouraging the strategies to become less uniform earlier in the learning process. This can be seen in Figure 5b, where we train a model with different values of p-v. If p-v is increased, the initial periods have a much lower error rate. It is possible to adjust some hyper parameters so that the point at which the trade-off between computing speed and computing time could be pushed to a higher cost, i.e. to a lower error rate."}, {"heading": "5 RELATED WORK", "text": "In recent years, it has been shown that people are able to determine themselves what they want and what they want to do. (...) In recent years, it has been shown that people are able to determine themselves. (...) In the last ten years, the number of people who are able to determine whether they are able to determine what they want or not has increased. (...) In the last ten years, the number of people who are able to determine themselves has increased. (...) In the last ten years, the number of people who are able to determine themselves has increased. (...) In the last ten years, the number of people who are able to determine how high they have to be. (...) In the last ten years, the number of people who are able to determine themselves has increased. (...) In the last ten years, the number of people who are able to determine themselves has decreased. (...) In the last ten years, the number of people who are able to determine themselves has decreased. (...) In the last ten years, the number of people has decreased."}, {"heading": "6 CONCLUSION", "text": "We propose a kind of parameterized block-dropout policy that maps the activation of a layer onto a Bernoulli mask, the amplification signal taking into account the loss function of the network in its predictive task, while the political network itself is regulated to accommodate the desire for sparse calculations, and the REINFORCE algorithm is used to train strategies for optimizing these costs. Our experiments show that it is possible to train such models at the same level of accuracy as their standard counterparts, and it seems possible to execute these similarly precise models more quickly due to their scarcity, and in addition, the model has a few simple parameters that make it possible to control the trade-off between accuracy and runtime, the use of REINFORCE could be replaced by a more efficient policy-search algorithm, and, perhaps, one in which rewards (or costs), as described above, can be replaced by great dynamics."}, {"heading": "ACKNOWLEDGEMENTS", "text": "The authors thank the Samsung Advanced Institute of Technology (SAIT), the Natural Sciences and Engineering Research Council of Canada (NSERC) and the Fonds de recherche du Que \ufffd bec - Nature et Technologies (FQRNT) for their financial support."}], "references": [{"title": "Adaptive dropout for training deep neural networks", "author": ["Ba", "Jimmy", "Frey", "Brendan"], "venue": "K.Q. (eds.), Advances in Neural Information Processing Systems", "citeRegEx": "Ba et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Ba et al\\.", "year": 2013}, {"title": "Learning long-term dependencies with gradient descent is difficult", "author": ["Y. Bengio", "P. Simard", "P. Frasconi"], "venue": "IEEE Transactions on Neural Nets, pp", "citeRegEx": "Bengio et al\\.,? \\Q1994\\E", "shortCiteRegEx": "Bengio et al\\.", "year": 1994}, {"title": "Estimating or propagating gradients through stochastic neurons for conditional computation", "author": ["Bengio", "Yoshua", "L\u00e9onard", "Nicholas", "Courville", "Aaron"], "venue": "arXiv preprint arXiv:1308.3432,", "citeRegEx": "Bengio et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Bengio et al\\.", "year": 2013}, {"title": "Theano: a CPU and GPU math expression compiler", "author": ["Bergstra", "James", "Breuleux", "Olivier", "Bastien", "Fr\u00e9d\u00e9ric", "Lamblin", "Pascal", "Pascanu", "Razvan", "Desjardins", "Guillaume", "Turian", "Joseph", "Warde-Farley", "David", "Bengio", "Yoshua"], "venue": "In Proceedings of the Python for Scientific Computing Conference (SciPy),", "citeRegEx": "Bergstra et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Bergstra et al\\.", "year": 2010}, {"title": "Pattern Recognition and Machine Learning (Information Science and Statistics)", "author": ["Bishop", "Christopher M"], "venue": null, "citeRegEx": "Bishop and M.,? \\Q2006\\E", "shortCiteRegEx": "Bishop and M.", "year": 2006}, {"title": "A survey on policy search for robotics", "author": ["Deisenroth", "Marc Peter", "Neumann", "Gerhard", "Peters", "Jan"], "venue": "Foundations and Trends in Robotics,", "citeRegEx": "Deisenroth et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Deisenroth et al\\.", "year": 2013}, {"title": "Deep sequential neural network", "author": ["Denoyer", "Ludovic", "Gallinari", "Patrick"], "venue": "CoRR, abs/1410.0510,", "citeRegEx": "Denoyer et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Denoyer et al\\.", "year": 2014}, {"title": "Understanding the difficulty of training deep feedforward neural networks", "author": ["Glorot", "Xavier", "Bengio", "Yoshua"], "venue": "In Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics,", "citeRegEx": "Glorot et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Glorot et al\\.", "year": 2010}, {"title": "Draw: A recurrent neural network for image generation", "author": ["Gregor", "Karol", "Danihelka", "Ivo", "Graves", "Alex", "Wierstra", "Daan"], "venue": "arXiv preprint arXiv:1502.04623,", "citeRegEx": "Gregor et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Gregor et al\\.", "year": 2015}, {"title": "Delving deep into rectifiers: Surpassing human-level performance on imagenet classification", "author": ["He", "Kaiming", "Zhang", "Xiangyu", "Ren", "Shaoqing", "Sun", "Jian"], "venue": "arXiv preprint arXiv:1502.01852,", "citeRegEx": "He et al\\.,? \\Q2015\\E", "shortCiteRegEx": "He et al\\.", "year": 2015}, {"title": "Improving neural networks by preventing co-adaptation of feature detectors", "author": ["Hinton", "Geoffrey E", "Srivastava", "Nitish", "Krizhevsky", "Alex", "Sutskever", "Ilya", "Salakhutdinov", "Ruslan"], "venue": "CoRR, abs/1207.0580,", "citeRegEx": "Hinton et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Hinton et al\\.", "year": 2012}, {"title": "Untersuchungen zu dynamischen neuronalen Netzen", "author": ["S. Hochreiter"], "venue": "Diploma thesis, T.U. Mu\u0308nich,", "citeRegEx": "Hochreiter,? \\Q1991\\E", "shortCiteRegEx": "Hochreiter", "year": 1991}, {"title": "Learning multiple layers of features from tiny", "author": ["Krizhevsky", "Alex", "Hinton", "Geoffrey"], "venue": null, "citeRegEx": "Krizhevsky et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Krizhevsky et al\\.", "year": 2009}, {"title": "Deep learning via hessian-free optimization", "author": ["Martens", "James"], "venue": "In Proceedings of the 27th International Conference on Machine Learning", "citeRegEx": "Martens and James.,? \\Q2010\\E", "shortCiteRegEx": "Martens and James.", "year": 2010}, {"title": "Recurrent models of visual attention", "author": ["Mnih", "Volodymyr", "Heess", "Nicolas", "Graves", "Alex", "kavukcuoglu", "koray"], "venue": "Advances in Neural Information Processing Systems", "citeRegEx": "Mnih et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Mnih et al\\.", "year": 2014}, {"title": "Reading digits in natural images with unsupervised feature learning", "author": ["Netzer", "Yuval", "Wang", "Tao", "Coates", "Adam", "Bissacco", "Alessandro", "Wu", "Bo", "Ng", "Andrew Y"], "venue": "In NIPS workshop on deep learning and unsupervised feature learning,", "citeRegEx": "Netzer et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Netzer et al\\.", "year": 2011}, {"title": "Fast exact multiplication by the hessian", "author": ["Pearlmutter", "Barak A"], "venue": "Neural Comput.,", "citeRegEx": "Pearlmutter and A.,? \\Q1994\\E", "shortCiteRegEx": "Pearlmutter and A.", "year": 1994}, {"title": "Learning representations by back-propagating errors", "author": ["Rumelhart", "David E", "Hinton", "Geoffrey E", "Williams", "Ronald J"], "venue": "Cognitive modeling,", "citeRegEx": "Rumelhart et al\\.,? \\Q1988\\E", "shortCiteRegEx": "Rumelhart et al\\.", "year": 1988}, {"title": "Deep networks with internal selective attention through feedback connections", "author": ["Stollenga", "Marijn F", "Masci", "Jonathan", "Gomez", "Faustino", "Schmidhuber", "J\u00fcrgen"], "venue": "Advances in Neural Information Processing Systems", "citeRegEx": "Stollenga et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Stollenga et al\\.", "year": 2014}, {"title": "Dropout training as adaptive regularization", "author": ["Wager", "Stefan", "Wang", "Sida", "Liang", "Percy S"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Wager et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Wager et al\\.", "year": 2013}, {"title": "Simple statistical gradient-following algorithms for connectionist reinforcement learning", "author": ["Williams", "Ronald J"], "venue": "Machine Learning,", "citeRegEx": "Williams and J.,? \\Q1992\\E", "shortCiteRegEx": "Williams and J.", "year": 1992}, {"title": "Show, attend and tell: Neural image caption generation with visual attention", "author": ["Xu", "Kelvin", "Ba", "Jimmy", "Kiros", "Ryan", "Courville", "Aaron", "Salakhutdinov", "Ruslan", "Zemel", "Richard", "Bengio", "Yoshua"], "venue": "arXiv preprint arXiv:1502.03044,", "citeRegEx": "Xu et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Xu et al\\.", "year": 2015}], "referenceMentions": [{"referenceID": 9, "context": "Large-scale neural networks, and in particular deep learning architectures, have seen a surge in popularity in recent years, due to their impressive empirical performance in complex supervised learning tasks, including state-of-the-art performance in image and speech recognition (He et al., 2015).", "startOffset": 280, "endOffset": 297}, {"referenceID": 11, "context": "Several related problems arise: very long training time (several weeks on modern computers, for some problems), potential for over-fitting (whereby the learned function is too specific to the training data and generalizes poorly to unseen data), and more technically, the vanishing gradient problem (Hochreiter, 1991; Bengio et al., 1994), whereby the gradient information gets increasingly diffuse as it propagates from layer to layer.", "startOffset": 299, "endOffset": 338}, {"referenceID": 1, "context": "Several related problems arise: very long training time (several weeks on modern computers, for some problems), potential for over-fitting (whereby the learned function is too specific to the training data and generalizes poorly to unseen data), and more technically, the vanishing gradient problem (Hochreiter, 1991; Bengio et al., 1994), whereby the gradient information gets increasingly diffuse as it propagates from layer to layer.", "startOffset": 299, "endOffset": 338}, {"referenceID": 19, "context": "Dropout can be interpreted as a form of regularization to prevent overfitting (Wager et al., 2013).", "startOffset": 78, "endOffset": 98}, {"referenceID": 1, "context": "Several related problems arise: very long training time (several weeks on modern computers, for some problems), potential for over-fitting (whereby the learned function is too specific to the training data and generalizes poorly to unseen data), and more technically, the vanishing gradient problem (Hochreiter, 1991; Bengio et al., 1994), whereby the gradient information gets increasingly diffuse as it propagates from layer to layer. A technique called dropout was introduced by Hinton et al. (2012) as a way to reduce overfitting by breaking the tendency for \u201cco-adaptations\u201d between nodes.", "startOffset": 318, "endOffset": 503}, {"referenceID": 5, "context": "formulation, and our solution to the proposed optimization problem, using policy search methods (Deisenroth et al., 2013).", "startOffset": 96, "endOffset": 121}, {"referenceID": 10, "context": "As in the original dropout paper (Hinton et al., 2012), each node in a given layer has an associated Bernoulli distribution, which determines its probability of being dropped.", "startOffset": 33, "endOffset": 54}, {"referenceID": 17, "context": "We first update the network and policy parameters to minimize the following regularized loss function via backpropagation (Rumelhart et al., 1988): L = \u2212 logP (Y |X, \u03b8) + \u03bbs(Lb + Le) + \u03bbv(Lv) + \u03bbL2\u2016\u03b8\u2016 where \u03bbs can be understood as a trade-off parameter between prediction accuracy and parsimony of computation (obtained through sparse node activation), and \u03bbv as a trade-off parameter between a stochastic policy and a more input dependent saturated policy.", "startOffset": 122, "endOffset": 146}, {"referenceID": 3, "context": "The proposed model was implemented within Theano (Bergstra et al., 2010), a standard library for deep learning and neural networks.", "startOffset": 49, "endOffset": 72}, {"referenceID": 3, "context": "The proposed model was implemented within Theano (Bergstra et al., 2010), a standard library for deep learning and neural networks. In addition to using optimizations offered by Theano, we also implemented specialized matrix multiplication code for the operation exposed in section 3.4. A straightforward and fairly naive CPU implementation of this operation yielded speedups of up to 5-10x, while an equally naive GPU implementation yielded speedups of up to 2-4x, both for sparsity rates of under 20% and acceptable matrix and block sizes.1 We otherwise use fairly standard methods for our neural network. The weight matrices are initialized using the heuristic of Glorot & Bengio (2010). We use a constant learning rate throughout minibatch SGD.", "startOffset": 50, "endOffset": 690}, {"referenceID": 10, "context": "The neural networks without conditional dropout are trained with L2 regularization as well as regular dropout (Hinton et al., 2012).", "startOffset": 110, "endOffset": 131}, {"referenceID": 15, "context": "Finally we tested our model on the Street View House Numbers (SVHN) (Netzer et al., 2011) dataset, which also yielded encouraging results (figure 3).", "startOffset": 68, "endOffset": 89}, {"referenceID": 1, "context": "Bengio et al. (2013) introduced Stochastic Times Smooth neurons as gaters for conditional computation within a deep neural network.", "startOffset": 0, "endOffset": 21}, {"referenceID": 14, "context": "Another point of comparison for our work are attention models (Mnih et al., 2014; Gregor et al., 2015; Xu et al., 2015).", "startOffset": 62, "endOffset": 119}, {"referenceID": 8, "context": "Another point of comparison for our work are attention models (Mnih et al., 2014; Gregor et al., 2015; Xu et al., 2015).", "startOffset": 62, "endOffset": 119}, {"referenceID": 21, "context": "Another point of comparison for our work are attention models (Mnih et al., 2014; Gregor et al., 2015; Xu et al., 2015).", "startOffset": 62, "endOffset": 119}, {"referenceID": 16, "context": "Stollenga et al. (2014) recently proposed to learn a sequential decision process over the filters of a convolutional neural network (CNN).", "startOffset": 0, "endOffset": 24}, {"referenceID": 16, "context": "Stollenga et al. (2014) recently proposed to learn a sequential decision process over the filters of a convolutional neural network (CNN). As in our work, a direct policy search method was chosen to find the parameters of a control policy. Their problem formulation differs from ours mainly in the notion of decision \u201cstage\u201d. In their model, an input is first fed through a network, the activations are computed during forward propagation then they are served to the next decision stage. The goal of the policy is to select relevant filters from the previous stage so as to improve the decision accuracy on the current example. They also use a gradient-free evolutionary algorithm, in contrast to our policy search method. The Deep Sequential Neural Network (DSNN) model of Denoyer & Gallinari (2014) is possibly closest to our approach.", "startOffset": 0, "endOffset": 801}], "year": 2017, "abstractText": "Deep learning has become the state-of-art tool in many applications, but the evaluation and training of deep models can be time-consuming and computationally expensive. Dropout has been shown to be an effective strategy to sparsify computations (by not involving all units), as well as to regularize models. In typical dropout, nodes are dropped uniformly at random. Our goal is to use reinforcement learning in order to design better, more informed dropout policies, which are datadependent. We cast the problem of learning activation-dependent dropout policies for blocks of units as a reinforcement learning problem. We propose a learning scheme motivated by computation speed, capturing the idea of wanting to have parsimonious activations while maintaining prediction accuracy. We apply a policy gradient algorithm for learning policies that optimize this loss function and propose a regularization mechanism that encourages diversification of the dropout policy. We present encouraging empirical results showing that this approach improves the speed of computation without impacting the quality of the approximation.", "creator": "LaTeX with hyperref package"}}}