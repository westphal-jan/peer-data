{"id": "1511.06303", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "19-Nov-2015", "title": "Alternative structures for character-level RNNs", "abstract": "Recurrent neural networks are convenient and efficient models for language modeling. However, when applied on the level of characters instead of words, they suffer from several problems. In order to successfully model long-term dependencies, the hidden representation needs to be large. This in turn implies higher computational costs, which can become prohibitive in practice. We propose two alternative structural modifications to the classical RNN model. The first one consists on conditioning the character level representation on the previous word representation. The other one uses the character history to condition the output probability. We evaluate the performance of the two proposed modifications on challenging, multi-lingual real world data.", "histories": [["v1", "Thu, 19 Nov 2015 18:46:21 GMT  (52kb)", "http://arxiv.org/abs/1511.06303v1", null], ["v2", "Tue, 24 Nov 2015 17:35:35 GMT  (52kb)", "http://arxiv.org/abs/1511.06303v2", "First revision. Updated Table 3, extended Sec. 5.3 and added a paragraph to the conclusion,"]], "reviews": [], "SUBJECTS": "cs.LG cs.CL", "authors": ["piotr bojanowski", "armand joulin", "tomas mikolov"], "accepted": false, "id": "1511.06303"}, "pdf": {"name": "1511.06303.pdf", "metadata": {"source": "CRF", "title": null, "authors": ["Piotr Bojanowski"], "emails": ["piotr.bojanowski@inria.fr", "tmikolov@fb.com", "ajoulin@fb.com", "leonb@fb.com"], "sections": [{"heading": null, "text": "ar Xiv: 151 1,06 303v 1 [cs.L G] 1"}, {"heading": "1 INTRODUCTION", "text": "In fact, most people are able to move to another world in which they are able to live, in which they want to live."}, {"heading": "1.1 RELATED WORK", "text": "Agglutinative languages such as Finnish or Turkish have a very large vocabulary, making word-based models impractical (Kurimo et al., 2006). Subword units such as morphemes have been used in statistical models for speech recognition (Vergyri et al., 2004; Hirsima \u00bfki et al., 2006; Arisoy et al., 2009; Creutz et al., 2007). Specifically, Creutz et al. (2007) show that morph-based N-gram models outperform words based on most agglutinative languages. A mixture of word and character input for neural network language models has been studied by Kang et al. (2011) in the context of Chinese. More recently, Kim et al. (2015) provide a model for predicting words entered at character level, while we predict characters based on a mixture of word and character levels."}, {"heading": "2 SIMPLE RECURRENT NETWORK", "text": "In this section we describe the simple RNN model popularized by Elman (1990), in the context of language modeling. We formulate language modeling as a discrete sequence prediction problem. That is, we want to predict the next symbol in a sequence given its past. However, we assume that a dictionary of fixed size k words consists of d different characters. We denote by the a hot encoding of the t-th character in the sequence, and wp which is a hot encoding of the p-th word. Our basic unit is the character.An RNN consists of an input layer, a hidden layer and a starting layer. Its hidden layer has a recursive connection that allows the propagation through the time of information. More specifically, the state of the m hidden units, ht is updated as a function of its previous state, ht \u2212 1 and the current character is a hot representation x."}, {"heading": "3 CONDITIONING ON WORDS", "text": "In this section we will consider an extension of the character level RNN by conditioning it to a word level > information. This allows a more direct flow of information from the previous words to predict the character level. We propose to condition the character level on a context vector as follows: ht = \u03c3 (Act + Rht \u2212 1 + Qzt), (2) where Q is a matrix. This context vector is built by collecting information at word level using a word at word level RNN. The word level RNN is similar to the one described in the previous section. Its input for the p-th word is its most uniform representation wp. The context vector is then simply the state of the hidden layer gp. If the t-th character belongs to the p-th word, then zt = gp \u2212 1. Figure 1 provides an illustration of this hybrid word and character level RNN. To train this model, we combine characters with a loss of words."}, {"heading": "4 CONDITIONING PREDICTION ON RECENT HISTORY", "text": "In a character-based RNN, the classifier has a very small number of parameters. We suggest conditioning this classifier on the most recent contextual information to increase its capacity while keeping the computational cost constant. This contextual information refers to simple short-term statistics, such as cooking current in a language. The explicit modeling of this information in the classifier removes some of the load from the hidden layer and allows the use of smaller, recurring matrices while maintaining performance. There is a lot of relevant contextual information on which we can condition the classifier. In particular, simple short-term dependencies are easily captured by n-grams, which are expensive but very efficient to store. Using such cheap information to condition the classifier of the RNN gives an easy way to increase the capacity of the model while the rest of the RNN is written as a two-dimensional pattern that can be manipulated to the NN."}, {"heading": "5 EXPERIMENTAL EVALUATION", "text": "We evaluate the proposed models using the Penn-Treebank corpus and a subset of the Europarl dataset. For simplicity, we compare our models with a simple RNN, but any modifications we propose can be applied to more complex units (LSTM, etc.) We train our model with stochastic gradient descent and select hyperparameters using a validation set. We set a constant learning rate \u03b3 and when the validation entropy begins to rise, we divide it by a factor \u03b1 according to each epoch (values around \u03b3 = 0.1 and \u03b1 = 1.5 work well in practice). Our implementation is a one-thread CPU code that could easily be parallelized. Code for training both models is publicly available. We evaluate our method using entropy in bits per character. It is defined as an empirical estimate of cross-entropy between the target distribution and the model output using a 2. This corresponds to a negative probability that we use NTB."}, {"heading": "5.1 EXPERIMENTS ON PENN TREEBANK", "text": "First, we conduct experiments on the Penn Treebank Corpus (Marcus et al., 1993), which is a dataset with a training set of 930k normalized words, giving a total of 5017k characters. All characters are in ASCII format, which results in a limited size of the character vocabulary C. The text has been normalized and the dictionary is limited to 10000 most common words of the training set. The other words have been fixed to 200 by a < UNK > token in training, validation and testing sets.We evaluate both models described in this essay on the Penn Treebank dataset. For the mixed model from Sec. 3 (mixed), we fix the size of the hidden representations at word level to 200. For the conditional model presented in Sec. 4 (Cond.), we select the optimal N on the validation dataset. We compare these models with our own implementation of a time level relative to NRda, allowing us to run times."}, {"heading": "5.2 BINARY PENN TREEBANK", "text": "We are conducting some experiments on binary representation of Penn Treebank. As mentioned in the introduction, we want to develop models that would be independent of the representation used. Working with binary representation would allow us to have models of sequential data that would be agnostic for the nature of the sequence, which could easily be applied to speech modeling, but also speech recognition directly from wave files, etc. We perform the conditional model and a bit-level base model RNN, both with a hidden representation of 100. For the conditional model, we select the optimal N by selecting it on the validation set (N = 2000). We evaluate both models by calculating entropy per bit and character. The results for this experiment are presented in Table 2. This setting corresponds to the extreme case where the dictionary is as small as it could be. The input and output model has only 2 \u00d7 m, which may represent a serious limitation for NRs."}, {"heading": "5.3 THE MULTILINGUAL EUROPARL DATASET", "text": "We perform another series of experiments with the Europarl dataset (Koehn, 2005), which is a corpus for machine translation with sentences from 20 different languages geared to their English correspondence. For almost every language, there are more than 500k sentences, consisting of more than 10M words. Due to its size, we limit our experiments to a subset of sentences for each language. We randomly permutate lines of transcriptions, select 60k sentences for training, 10k for validation, and 10k for testing. The permutation we use is made publicly available upon publication. In this experiment, as in Sec. 5.1, we compare our models with a character level RNN. We train our mixed model with a hidden word of 200 and a hidden character of 300. For the conditional variant, we fix the hidden representation and select the optimal N on the validation set."}], "references": [{"title": "Turkish broadcast news transcription and retrieval. Audio, Speech, and Language Processing", "author": ["Arisoy", "Ebru", "Can", "Do\u011fan", "Parlak", "Siddika", "Sak", "Ha\u015fim", "Sara\u00e7lar", "Murat"], "venue": "IEEE Transactions on,", "citeRegEx": "Arisoy et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Arisoy et al\\.", "year": 2009}, {"title": "Factored language models and generalized parallel backoff", "author": ["Bilmes", "Jeff A", "Kirchhoff", "Katrin"], "venue": "In Proceedings of the 2003 Conference of the North American Chapter of the Association for Computational Linguistics on Human Language Technology: companion volume of the Proceedings of HLT-NAACL 2003\u2013short papers-Volume", "citeRegEx": "Bilmes et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Bilmes et al\\.", "year": 2003}, {"title": "Morph-based speech recognition and modeling of out-of-vocabulary words across languages", "author": ["Creutz", "Mathias", "Hirsim\u00e4ki", "Teemu", "Kurimo", "Mikko", "Puurula", "Antti", "Pylkk\u00f6nen", "Janne", "Siivola", "Vesa", "Varjokallio", "Matti", "Arisoy", "Ebru", "Sara\u00e7lar", "Murat", "Stolcke", "Andreas"], "venue": "ACM Transactions on Speech and Language Processing (TSLP),", "citeRegEx": "Creutz et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Creutz et al\\.", "year": 2007}, {"title": "Finding structure in time", "author": ["Elman", "Jeffrey L"], "venue": "Cognitive science,", "citeRegEx": "Elman and L.,? \\Q1990\\E", "shortCiteRegEx": "Elman and L.", "year": 1990}, {"title": "A bit of progress in language modeling", "author": ["Goodman", "Joshua T"], "venue": "Computer Speech & Language,", "citeRegEx": "Goodman and T.,? \\Q2001\\E", "shortCiteRegEx": "Goodman and T.", "year": 2001}, {"title": "Generating sequences with recurrent neural networks", "author": ["Graves", "Alex"], "venue": "arXiv preprint arXiv:1308.0850,", "citeRegEx": "Graves and Alex.,? \\Q2013\\E", "shortCiteRegEx": "Graves and Alex.", "year": 2013}, {"title": "Unlimited vocabulary speech recognition with morph language models applied to finnish", "author": ["Hirsim\u00e4ki", "Teemu", "Creutz", "Mathias", "Siivola", "Vesa", "Kurimo", "Mikko", "Virpioja", "Sami", "Pylkk\u00f6nen", "Janne"], "venue": "Computer Speech & Language,", "citeRegEx": "Hirsim\u00e4ki et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Hirsim\u00e4ki et al\\.", "year": 2006}, {"title": "Mandarin word-character hybrid-input neural network language model", "author": ["Kang", "Moonyoung", "Ng", "Tim", "Nguyen", "Long"], "venue": "In INTERSPEECH, pp", "citeRegEx": "Kang et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Kang et al\\.", "year": 2011}, {"title": "Character-aware neural language models", "author": ["Kim", "Yoon", "Jernite", "Yacine", "Sontag", "David", "Rush", "Alexander M"], "venue": "arXiv preprint arXiv:1508.06615,", "citeRegEx": "Kim et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Kim et al\\.", "year": 2015}, {"title": "Europarl: A parallel corpus for statistical machine translation", "author": ["Koehn", "Philipp"], "venue": "In MT summit,", "citeRegEx": "Koehn and Philipp.,? \\Q2005\\E", "shortCiteRegEx": "Koehn and Philipp.", "year": 2005}, {"title": "Moses: Open source toolkit for statistical machine translation", "author": ["Koehn", "Philipp", "Hoang", "Hieu", "Birch", "Alexandra", "Callison-Burch", "Chris", "Federico", "Marcello", "Bertoldi", "Nicola", "Cowan", "Brooke", "Shen", "Wade", "Moran", "Christine", "Zens", "Richard"], "venue": "In Proceedings of the 45th annual meeting of the ACL on interactive poster and demonstration sessions,", "citeRegEx": "Koehn et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Koehn et al\\.", "year": 2007}, {"title": "Unlimited vocabulary speech recognition for agglutinative languages", "author": ["Kurimo", "Mikko", "Puurula", "Antti", "Arisoy", "Ebru", "Siivola", "Vesa", "Hirsim\u00e4ki", "Teemu", "Pylkk\u00f6nen", "Janne", "Alum\u00e4e", "Tanel", "Saraclar", "Murat"], "venue": "In Proceedings of the main conference on Human Language Technology Conference of the North American Chapter of the Association of Computational Linguistics,", "citeRegEx": "Kurimo et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Kurimo et al\\.", "year": 2006}, {"title": "Better word representations with recursive neural networks for morphology", "author": ["Luong", "Minh-Thang", "Socher", "Richard", "Manning", "Christopher D"], "venue": null, "citeRegEx": "Luong et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Luong et al\\.", "year": 2013}, {"title": "Building a large annotated corpus of english: The penn treebank", "author": ["Marcus", "Mitchell P", "Marcinkiewicz", "Mary Ann", "Santorini", "Beatrice"], "venue": "Comput. Linguist.,", "citeRegEx": "Marcus et al\\.,? \\Q1993\\E", "shortCiteRegEx": "Marcus et al\\.", "year": 1993}, {"title": "Statistical Language Models based on Neural Networks", "author": ["Mikolov", "Tom\u00e1\u0161"], "venue": "PhD thesis, PhD thesis, Brno University of Technology.,", "citeRegEx": "Mikolov and Tom\u00e1\u0161.,? \\Q2012\\E", "shortCiteRegEx": "Mikolov and Tom\u00e1\u0161.", "year": 2012}, {"title": "Recurrent neural network based language model", "author": ["Mikolov", "Tomas", "Karafi\u00e1t", "Martin", "Burget", "Lukas", "Cernock\u1ef3", "Jan", "Khudanpur", "Sanjeev"], "venue": "In INTERSPEECH 2010, 11th Annual Conference of the International Speech Communication Association,", "citeRegEx": "Mikolov et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2010}, {"title": "Rnnlm-recurrent neural network language modeling toolkit", "author": ["Mikolov", "Tomas", "Kombrink", "Stefan", "Deoras", "Anoop", "Burget", "Lukar", "Cernocky", "Jan"], "venue": "In Proc. of the 2011 ASRU Workshop,", "citeRegEx": "Mikolov et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2011}, {"title": "Subword language modeling with neural networks. preprint (http://www", "author": ["Mikolov", "Tom\u00e1\u0161", "Sutskever", "Ilya", "Deoras", "Anoop", "Le", "Hai-Son", "Kombrink", "Stefan", "J. Cernocky"], "venue": "fit. vutbr. cz/imikolov/rnnlm/char. pdf),", "citeRegEx": "Mikolov et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2012}, {"title": "Learning internal representations by error propagation", "author": ["Rumelhart", "David E", "Hinton", "Geoffrey E", "Williams", "Ronald J"], "venue": "Technical report, DTIC Document,", "citeRegEx": "Rumelhart et al\\.,? \\Q1985\\E", "shortCiteRegEx": "Rumelhart et al\\.", "year": 1985}, {"title": "Generating text with recurrent neural networks", "author": ["Sutskever", "Ilya", "Martens", "James", "Hinton", "Geoffrey E"], "venue": "In Proceedings of the 28th International Conference on Machine Learning", "citeRegEx": "Sutskever et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Sutskever et al\\.", "year": 2011}, {"title": "Morphology-based language modeling for arabic speech recognition", "author": ["Vergyri", "Dimitra", "Kirchhoff", "Katrin", "Duh", "Kevin", "Stolcke", "Andreas"], "venue": "In INTERSPEECH,", "citeRegEx": "Vergyri et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Vergyri et al\\.", "year": 2004}, {"title": "Generalization of backpropagation with application to a recurrent gas market model", "author": ["Werbos", "Paul J"], "venue": "Neural Networks,", "citeRegEx": "Werbos and J.,? \\Q1988\\E", "shortCiteRegEx": "Werbos and J.", "year": 1988}], "referenceMentions": [{"referenceID": 10, "context": ", 1997) and machine translation Koehn et al. (2007). In particular, recurrent neural network based language models (RNNs) are now widely used and have demonstrated state-of-the-art performance on many tasks, such as automatic speech recognition and statistical machine translation (Mikolov, 2012).", "startOffset": 32, "endOffset": 52}, {"referenceID": 16, "context": "While this type of models has been widely studied in the past (Mikolov et al., 2011; Sutskever et al., 2011; Graves, 2013), Char-RNNs lead to both lower accuracy and higher computational cost than word-based models (Mikolov et al.", "startOffset": 62, "endOffset": 122}, {"referenceID": 19, "context": "While this type of models has been widely studied in the past (Mikolov et al., 2011; Sutskever et al., 2011; Graves, 2013), Char-RNNs lead to both lower accuracy and higher computational cost than word-based models (Mikolov et al.", "startOffset": 62, "endOffset": 122}, {"referenceID": 17, "context": ", 2011; Graves, 2013), Char-RNNs lead to both lower accuracy and higher computational cost than word-based models (Mikolov et al., 2012).", "startOffset": 114, "endOffset": 136}, {"referenceID": 17, "context": "Ad-hoc solutions based on larger sub-word units seem to be able to both deal with new words and offer reasonable accuracy and training speed (Mikolov et al., 2012).", "startOffset": 141, "endOffset": 163}, {"referenceID": 11, "context": "Agglutinative languages such as Finnish or Turkish have very large vocabularies, making word based models impractical (Kurimo et al., 2006).", "startOffset": 118, "endOffset": 139}, {"referenceID": 20, "context": "Subword units such as morphemes have been used in statstical models for speech recognition (Vergyri et al., 2004; Hirsim\u00e4ki et al., 2006; Arisoy et al., 2009; Creutz et al., 2007).", "startOffset": 91, "endOffset": 179}, {"referenceID": 6, "context": "Subword units such as morphemes have been used in statstical models for speech recognition (Vergyri et al., 2004; Hirsim\u00e4ki et al., 2006; Arisoy et al., 2009; Creutz et al., 2007).", "startOffset": 91, "endOffset": 179}, {"referenceID": 0, "context": "Subword units such as morphemes have been used in statstical models for speech recognition (Vergyri et al., 2004; Hirsim\u00e4ki et al., 2006; Arisoy et al., 2009; Creutz et al., 2007).", "startOffset": 91, "endOffset": 179}, {"referenceID": 2, "context": "Subword units such as morphemes have been used in statstical models for speech recognition (Vergyri et al., 2004; Hirsim\u00e4ki et al., 2006; Arisoy et al., 2009; Creutz et al., 2007).", "startOffset": 91, "endOffset": 179}, {"referenceID": 16, "context": "Typical choice of subword units are either characters (Mikolov et al., 2011; Sutskever et al., 2011; Graves, 2013) or syllables (Mikolov et al.", "startOffset": 54, "endOffset": 114}, {"referenceID": 19, "context": "Typical choice of subword units are either characters (Mikolov et al., 2011; Sutskever et al., 2011; Graves, 2013) or syllables (Mikolov et al.", "startOffset": 54, "endOffset": 114}, {"referenceID": 17, "context": ", 2011; Graves, 2013) or syllables (Mikolov et al., 2012).", "startOffset": 35, "endOffset": 57}, {"referenceID": 12, "context": "Others have used embedding of words to deal with OOV words (Bilmes & Kirchhoff, 2003; Alexandrescu & Kirchhoff, 2006; Luong et al., 2013).", "startOffset": 59, "endOffset": 137}, {"referenceID": 0, "context": ", 2006; Arisoy et al., 2009; Creutz et al., 2007). In particular, Creutz et al. (2007) show that morph-based N-gram models outperform word based ones on most of the agglutinative languages.", "startOffset": 8, "endOffset": 87}, {"referenceID": 0, "context": ", 2006; Arisoy et al., 2009; Creutz et al., 2007). In particular, Creutz et al. (2007) show that morph-based N-gram models outperform word based ones on most of the agglutinative languages. A mix of word and character level input for neural network language models has been investigated by Kang et al. (2011) in the context of Chinese.", "startOffset": 8, "endOffset": 309}, {"referenceID": 0, "context": ", 2006; Arisoy et al., 2009; Creutz et al., 2007). In particular, Creutz et al. (2007) show that morph-based N-gram models outperform word based ones on most of the agglutinative languages. A mix of word and character level input for neural network language models has been investigated by Kang et al. (2011) in the context of Chinese. More recently, Kim et al. (2015) propose a model to predict words given character level inputs, while we predict characters based on a mix of word and character level inputs.", "startOffset": 8, "endOffset": 369}, {"referenceID": 0, "context": ", 2006; Arisoy et al., 2009; Creutz et al., 2007). In particular, Creutz et al. (2007) show that morph-based N-gram models outperform word based ones on most of the agglutinative languages. A mix of word and character level input for neural network language models has been investigated by Kang et al. (2011) in the context of Chinese. More recently, Kim et al. (2015) propose a model to predict words given character level inputs, while we predict characters based on a mix of word and character level inputs. Recurrent networks have been popularized for statistical language modeling by Mikolov et al. (2010). Since then, many authors have investigated the use of subword units in order to deal with Out-Of-Vocabulary (OOV) words in the context of recurrent networks.", "startOffset": 8, "endOffset": 611}, {"referenceID": 0, "context": ", 2006; Arisoy et al., 2009; Creutz et al., 2007). In particular, Creutz et al. (2007) show that morph-based N-gram models outperform word based ones on most of the agglutinative languages. A mix of word and character level input for neural network language models has been investigated by Kang et al. (2011) in the context of Chinese. More recently, Kim et al. (2015) propose a model to predict words given character level inputs, while we predict characters based on a mix of word and character level inputs. Recurrent networks have been popularized for statistical language modeling by Mikolov et al. (2010). Since then, many authors have investigated the use of subword units in order to deal with Out-Of-Vocabulary (OOV) words in the context of recurrent networks. Typical choice of subword units are either characters (Mikolov et al., 2011; Sutskever et al., 2011; Graves, 2013) or syllables (Mikolov et al., 2012). Others have used embedding of words to deal with OOV words (Bilmes & Kirchhoff, 2003; Alexandrescu & Kirchhoff, 2006; Luong et al., 2013). Luong et al. (2013) build word embeddings by applying a recursive neural network over morpheme embeddings, while Bilmes & Kirchhoff (2003) build their embedding by concatenating features built on previously seen words.", "startOffset": 8, "endOffset": 1081}, {"referenceID": 0, "context": ", 2006; Arisoy et al., 2009; Creutz et al., 2007). In particular, Creutz et al. (2007) show that morph-based N-gram models outperform word based ones on most of the agglutinative languages. A mix of word and character level input for neural network language models has been investigated by Kang et al. (2011) in the context of Chinese. More recently, Kim et al. (2015) propose a model to predict words given character level inputs, while we predict characters based on a mix of word and character level inputs. Recurrent networks have been popularized for statistical language modeling by Mikolov et al. (2010). Since then, many authors have investigated the use of subword units in order to deal with Out-Of-Vocabulary (OOV) words in the context of recurrent networks. Typical choice of subword units are either characters (Mikolov et al., 2011; Sutskever et al., 2011; Graves, 2013) or syllables (Mikolov et al., 2012). Others have used embedding of words to deal with OOV words (Bilmes & Kirchhoff, 2003; Alexandrescu & Kirchhoff, 2006; Luong et al., 2013). Luong et al. (2013) build word embeddings by applying a recursive neural network over morpheme embeddings, while Bilmes & Kirchhoff (2003) build their embedding by concatenating features built on previously seen words.", "startOffset": 8, "endOffset": 1200}, {"referenceID": 18, "context": "with a stochastic gradient descent method and backpropagation through time (Rumelhart et al., 1985; Werbos, 1988).", "startOffset": 75, "endOffset": 113}, {"referenceID": 15, "context": "Character level RNNs have been shown to perform poorly compared to word level ones Mikolov et al. (2012). In particular, they require massive hidden layer in order to obtain results which are on par with word level models, this makes them very expensive to compute.", "startOffset": 83, "endOffset": 105}, {"referenceID": 13, "context": "We first carry out experiments on the Penn Treebank corpus (Marcus et al., 1993).", "startOffset": 59, "endOffset": 80}, {"referenceID": 17, "context": "The character-level performance we obtain for the \u201cvanilla\u201d RNN is coherent with numbers published in the past on this dataset (Mikolov et al., 2012).", "startOffset": 127, "endOffset": 149}], "year": 2017, "abstractText": "Recurrent neural networks are convenient and efficient models for language modeling. However, when applied on the level of characters instead of words, they suffer from several problems. In order to successfully model long-term dependencies, the hidden representation needs to be large. This in turn implies higher computational costs, which can become prohibitive in practice. We propose two alternative structural modifications to the classical RNN model. The first one consists on conditioning the character level representation on the previous word representation. The other one uses the character history to condition the output probability. We evaluate the performance of the two proposed modifications on challenging, multi-lingual real world data.", "creator": "LaTeX with hyperref package"}}}