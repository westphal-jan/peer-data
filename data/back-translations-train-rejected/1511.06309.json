{"id": "1511.06309", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "19-Nov-2015", "title": "Spatio-temporal video autoencoder with differentiable memory", "abstract": "We describe a new spatio-temporal video autoencoder, based on a classic spatial image autoencoder and a novel nested temporal autoencoder. The temporal encoder is represented by a differentiable visual memory composed of convolutional long short-term memory (LSTM) cells that integrate changes over time. Here we target motion changes and use as temporal decoder a robust optical flow prediction module together with an image sampler serving as built-in feedback loop. The architecture is end-to-end differentiable. At each time step, the system receives as input a video frame, predicts the optical flow based on the current observation and the LSTM memory state as a dense transformation map, and applies it to the current frame to generate the next frame. By minimising the reconstruction error between the predicted next frame and the corresponding ground truth next frame, we train the whole system to extract features useful for motion estimation without any supervision effort. We believe these features can in turn facilitate learning high-level tasks such as path planning, semantic segmentation, or action recognition, reducing the overall supervision effort.", "histories": [["v1", "Thu, 19 Nov 2015 19:06:28 GMT  (4655kb,D)", "http://arxiv.org/abs/1511.06309v1", null], ["v2", "Mon, 30 Nov 2015 21:07:11 GMT  (5783kb,D)", "http://arxiv.org/abs/1511.06309v2", null], ["v3", "Sat, 6 Aug 2016 16:24:58 GMT  (3811kb,D)", "http://arxiv.org/abs/1511.06309v3", null], ["v4", "Wed, 10 Aug 2016 14:46:49 GMT  (3811kb,D)", "http://arxiv.org/abs/1511.06309v4", "The experiments section has been extended and a direct application to weakly-supervised video segmentation through label propagation has been included"], ["v5", "Thu, 1 Sep 2016 11:36:40 GMT  (3811kb,D)", "http://arxiv.org/abs/1511.06309v5", "The experiments section has been extended and a direct application to weakly-supervised video segmentation through label propagation has been included"]], "reviews": [], "SUBJECTS": "cs.LG cs.CV", "authors": ["viorica patraucean", "ankur handa", "roberto cipolla"], "accepted": false, "id": "1511.06309"}, "pdf": {"name": "1511.06309.pdf", "metadata": {"source": "CRF", "title": null, "authors": ["DIFFERENTIABLE MEMORY", "Viorica P\u0103tr\u0103ucean", "Ankur Handa", "Roberto Cipolla"], "emails": ["vp344@cam.ac.uk", "ah781@cam.ac.uk", "rc10001@cam.ac.uk"], "sections": [{"heading": "1 INTRODUCTION", "text": "As a matter of fact, most of them are able to reform themselves by setting out in search of new paths that they are following. (...) Most of them are able to reform themselves. (...) Most of them are able to reform themselves. (...) Most of them are able to reform themselves. (...) Most of them are able to reform themselves. (...) Most of them are able to reform themselves. (...) Most of them are able to reform themselves. (...) Most of them are able to reform themselves. (...) Most of them are able to reform themselves. (...) Most of them are able to reform themselves. (...) Most of them are able to reform themselves. (...) Most of them are able to reform themselves. (...)"}, {"heading": "2 RELATED WORK", "text": "In fact, most people who are able to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move and to move."}, {"heading": "3 ARCHITECTURE", "text": "Our architecture consists of a temporal autoencoder embedded in a spatial autoencoder (see Figure 1). At each step, the network takes a video image Yt of size H x W as input and produces output of the same size that represents the predicted next image Y x t + 1. In the following, we describe each of the modules in detail."}, {"heading": "3.1 SPATIAL AUTOENCODER E AND D", "text": "The spatial autoencoder is a classical folding encoder - decoder architecture. The encoder E contains a folding layer, followed by tanh nonlinearity and a spatial max pooling with subsampling level. The decoder D mirrors the encoder, except for the nonlinearity layer, and uses the closest spatial upsampling to bring the output back to the size of the original input. After passing forward through the spatial encoder YtE \u2212 \u2192 xt, the size of the feature maps is xt d \u00b7 h \u00b7 w, d is the number of features and h or w is the height and width after downsampling."}, {"heading": "3.2 TEMPORAL AUTOENCODER", "text": "The goal of contemporary automobile traffic is to capture significant changes in the movement and movement of objects in the scene that make it possible to foresee the visual future, to recognize the past and the present. In a classical, spatial autoautoautobiographical system (Masci et al., 2011), the encoders and decoders are decided freely according to a composition based on their current characteristic in their own memory spaces, which allow an optimal composition of the input to prevent the learning of a trivial mapping, and the decoder limits the learning of its own characteristics in order to satisfy this composition and to reconstruct the input operations, which are usually very similar to the encoder and have the same number of degrees of freedom. In contrast, the proposed temporary autocoder has a small number of tractable parameters whose role is to enable an immediate feedback to the coder, but without the capacity of errors such as they."}, {"heading": "3.2.2 OPTICAL FLOW PREDICTION \u0398 WITH HUBER PENALTY H", "text": "The optical flow prediction module generates a dense transformation map T, which has the same height and width as the memory output, with one 2D flow vector per pixel representing the shift in x and y direction due to movement between successive images. T allows predicting the next image by distorting the current image. We use two revolutionary layers with relatively large cores (15 x 15) to correct the shift from the memory characteristic to the space of the flow vectors. Large cores are needed, as the size of the predicted optical flow is limited by the size of the filters. To ensure local smoothness, we must punish the local course of the flow chart OT. We add a penalty module H, whose role is to advance its input unchanged during the forward run and punish the non-smooth error course during the return to the modules preceding the architecture."}, {"heading": "3.2.3 GRID GENERATOR GG AND IMAGE SAMPLER S", "text": "The grid generator GG and the image sampler S output the predicted feature maps x-t + 1 of the next image after distorting the current feature maps with the flow map generated by the VP module. We use similar differentiable grid maps and image samplers as STN (Jaderberg et al., 2015). The output of S, the size d-h-w, is considered a fixed h-w grid, which at each (xo, yo) position calculates a feature map entry of the size 1-1-d. We modified the grid generator to accept a transformation per pixel, rather than a single transformation for the entire image as in STN. Given the flow map T, GG calculates the source position (xs, ys) for each element in the grid in the input feature map, from where S must fill the position (xo, yo), xo (xo) (xo), xo (yo) (T = yo)."}, {"heading": "3.2.4 LOSS FUNCTION", "text": "The training of the network boils down to minimizing the reconstruction error between the predicted next frame and the ground truth next frame under certain constraints. We use the mean square error to quantify the reconstruction error. Additionally, we inject penalty gradients during the backspread at different stages of architecture. As mentioned above, we use Huber's smoothing penalty on the optical flow map. The same restriction applies to the spatial characteristic cards xt, along with an L1 thrift penalty on the memory characteristic cards. From our experiments, it seems important not to use thrift restrictions on the spatial characteristic cards as classical image auto-encoders do. As the introduction of thrift on the spatial characteristic cards would also require the pre-processing of the images (subtraction of the mean or local contrast normalization) to discard low frequencies and preserve only the contours, we argue that this is not optimal for estimating the frequencies where movement is also low."}, {"heading": "3.3 NETWORK PARAMETERS", "text": "The entire network has 703,651 trainable parameters, distributed as described in Table 1. Rotary encoders and decoders each have 32 filters of size 7 x 7. The memory module (LSTM) has 45 filters of size 7 x 7, and the optical flow regressor has two shaft layers, each with 2 filters of size 15 x 15 and a shaft layer of size 1. The other modules: GG, H and S have no trainable parameters."}, {"heading": "4 TRAINING", "text": "To train the proposed architecture, we follow a similar setup as (Stollenga et al., 2015); we use rmsprop with parameters = 10 \u2212 5, \u03c1 = 0.9 and a decreasing learning rate according to the rule \u03b7 = 10 \u2212 4 (100 \u221a 1 2) epoch. The distortions of the forge gates are initialized to 1; for the other gates, we set the distortions to 0. Training data is represented by multiple grayscale video sequences. After processing the subsequence, we update the parameters and reset the memory."}, {"heading": "5 EXPERIMENTS", "text": "We discussed the ability of the grid generator GG and the sampler S to generate the next frame with 50 frames depending on the type of video sequence mentioned. Figure 3 shows an example of the vulnerability of the water. Figure 3 shows that the vulnerability of the water is significant during this period. Figure 3 shows that the vulnerability of the water is able to reduce the vulnerability of the water."}, {"heading": "6 CONCLUSION AND FUTURE WORK", "text": "The core of the architecture is a module that implements a revolutionary version of long-term short-term memory (LSTM) cells that can be used as a form of artificial visual short-term memory. However, the proposed architecture is a proof-of-concept memory, and its performance would improve by deepering the entire architecture. On a more general note, we believe that our work can open the way to a number of exciting directions, such as semantic segmentation. The proposed architecture is proof of the concept, and possibly its performance would improve by deeper.On a more general note, we believe that our work can open the way to a number of exciting directions. Due to the built-in feedback loop, various experiments can be effortlessly performed to further develop the basic memory module that we have proposed."}, {"heading": "ACKNOWLEDGMENTS", "text": "We are very grateful to the Torch community for their efforts to maintain this great library, and CSIC Cambridge for funding this work."}], "references": [{"title": "Learning to see by moving", "author": ["Agrawal", "Pulkit", "Carreira", "Jo\u00e3o", "Malik", "Jitendra"], "venue": "CoRR, abs/1505.01596,", "citeRegEx": "Agrawal et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Agrawal et al\\.", "year": 2015}, {"title": "Measuring the objectness of image windows", "author": ["B. Alexe", "T. Deselaers", "V. Ferrari"], "venue": "IEEE Trans. Pattern Anal. Mach. Intell.,", "citeRegEx": "Alexe et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Alexe et al\\.", "year": 2012}, {"title": "Infants\u2019 physical world", "author": ["Baillargeon", "Rene\u00e9"], "venue": "American Psychological Society,", "citeRegEx": "Baillargeon and Rene\u00e9.,? \\Q2004\\E", "shortCiteRegEx": "Baillargeon and Rene\u00e9.", "year": 2004}, {"title": "Neuromorphic microchips", "author": ["K. Boahen"], "venue": "Scientific American,", "citeRegEx": "Boahen,? \\Q2005\\E", "shortCiteRegEx": "Boahen", "year": 2005}, {"title": "A naturalistic open source movie for optical flow evaluation", "author": ["D.J. Butler", "J. Wulff", "G.B. Stanley", "M.J. Black"], "venue": "In European Conf. on Computer Vision (ECCV),", "citeRegEx": "Butler et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Butler et al\\.", "year": 2012}, {"title": "Neural basis for a powerful static motion illusion", "author": ["Conway", "Bevil R", "Kitaoka", "Akiyoshi", "Yazdanbakhsh", "Arash", "Pack", "Christopher C", "Livingstone", "Margaret S"], "venue": "The Journal of Neuroscience,", "citeRegEx": "Conway et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Conway et al\\.", "year": 2005}, {"title": "Flownet: Learning optical flow with convolutional networks", "author": ["Fischer", "Philipp", "Dosovitskiy", "Alexey", "Ilg", "Eddy", "H\u00e4usser", "Philip", "Hazirbas", "Caner", "Golkov", "Vladimir", "van der Smagt", "Patrick", "Cremers", "Daniel", "Brox", "Thomas"], "venue": "CoRR, abs/1504.06852,", "citeRegEx": "Fischer et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Fischer et al\\.", "year": 2015}, {"title": "Are we ready for autonomous driving? the kitti vision benchmark suite", "author": ["Geiger", "Andreas", "Lenz", "Philip", "Urtasun", "Raquel"], "venue": "In Conference on Computer Vision and Pattern Recognition (CVPR),", "citeRegEx": "Geiger et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Geiger et al\\.", "year": 2012}, {"title": "Understanding the difficulty of training deep feedforward neural networks", "author": ["Glorot", "Xavier", "Bengio", "Yoshua"], "venue": "In AISTATS,", "citeRegEx": "Glorot et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Glorot et al\\.", "year": 2010}, {"title": "Generating sequences with recurrent neural networks", "author": ["Graves", "Alex"], "venue": "CoRR, abs/1308.0850,", "citeRegEx": "Graves and Alex.,? \\Q2013\\E", "shortCiteRegEx": "Graves and Alex.", "year": 2013}, {"title": "Multi-dimensional recurrent neural networks", "author": ["Graves", "Alex", "Fern\u00e1ndez", "Santiago", "Schmidhuber", "J\u00fcrgen"], "venue": "In Artificial Neural Networks,", "citeRegEx": "Graves et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Graves et al\\.", "year": 2007}, {"title": "Long short-term memory", "author": ["Hochreiter", "Sepp", "Schmidhuber", "J\u00fcrgen"], "venue": "Neural Comput.,", "citeRegEx": "Hochreiter et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Hochreiter et al\\.", "year": 1997}, {"title": "Gradient Flow in Recurrent Nets: the Difficulty of Learning Long-Term Dependencies", "author": ["Hochreiter", "Sepp", "Informatik", "Fakultat F", "Bengio", "Yoshua", "Frasconi", "Paolo", "Schmidhuber", "Jurgen"], "venue": "In Field Guide to Dynamical Recurrent Networks", "citeRegEx": "Hochreiter et al\\.,? \\Q2000\\E", "shortCiteRegEx": "Hochreiter et al\\.", "year": 2000}, {"title": "Constructing visual representations of natural scenes: the roles of short- and long-term visual memory", "author": ["A. Hollingworth"], "venue": "J Exp Psychol Hum Percept Perform.,", "citeRegEx": "Hollingworth,? \\Q2004\\E", "shortCiteRegEx": "Hollingworth", "year": 2004}, {"title": "Artificial intelligence in perspective. chapter Determining Optical Flow: A Retrospective", "author": ["Horn", "Berthold K. P", "B.G. Schunck"], "venue": null, "citeRegEx": "Horn et al\\.,? \\Q1994\\E", "shortCiteRegEx": "Horn et al\\.", "year": 1994}, {"title": "Spatial transformer networks", "author": ["Jaderberg", "Max", "Simonyan", "Karen", "Zisserman", "Andrew", "Kavukcuoglu", "Koray"], "venue": "CoRR, abs/1506.02025,", "citeRegEx": "Jaderberg et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Jaderberg et al\\.", "year": 2015}, {"title": "Clustering appearance and shape by learning jigsaws", "author": ["Kannan", "Anitha", "Winn", "John", "Rother", "Carsten"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Kannan et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Kannan et al\\.", "year": 2007}, {"title": "Simultaneous mosaicing and tracking with an event camera", "author": ["Kim", "Hanme", "Handa", "Ankur", "Benosman", "Ryad", "Ieng", "Sio-Ho\u0131", "Davison", "Andrew J"], "venue": "In British Machine Vision Conference (BMVC)", "citeRegEx": "Kim et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Kim et al\\.", "year": 2014}, {"title": "Low-level memory processes in vision", "author": ["Magnussen", "Svein"], "venue": "Trends in Neurosciences,", "citeRegEx": "Magnussen and Svein.,? \\Q2000\\E", "shortCiteRegEx": "Magnussen and Svein.", "year": 2000}, {"title": "Stacked convolutional autoencoders for hierarchical feature extraction", "author": ["Masci", "Jonathan", "Meier", "Ueli", "Cirean", "Dan", "Schmidhuber", "Jrgen"], "venue": "In Artificial Neural Networks and Machine Learning,", "citeRegEx": "Masci et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Masci et al\\.", "year": 2011}, {"title": "Object scene flow for autonomous vehicles", "author": ["Menze", "Moritz", "Geiger", "Andreas"], "venue": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,", "citeRegEx": "Menze et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Menze et al\\.", "year": 2015}, {"title": "On the distinction between sensory storage and short-term visual memory", "author": ["W.A. Phillips"], "venue": "Perception & Psychophysics,", "citeRegEx": "Phillips,? \\Q1974\\E", "shortCiteRegEx": "Phillips", "year": 1974}, {"title": "Long short-term memory based recurrent neural network architectures for large vocabulary speech recognition", "author": ["Sak", "Hasim", "Senior", "Andrew W", "Beaufays", "Fran\u00e7oise"], "venue": "CoRR, abs/1402.1128,", "citeRegEx": "Sak et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Sak et al\\.", "year": 2014}, {"title": "PROST Parallel Robust Online Simple Tracking", "author": ["Santner", "Jakob", "Leistner", "Christian", "Saffari", "Amir", "Pock", "Thomas", "Bischof", "Horst"], "venue": "In IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR),", "citeRegEx": "Santner et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Santner et al\\.", "year": 2010}, {"title": "Communication in the presence of noise", "author": ["C.E. Shannon"], "venue": "Proceedings of the IRE,", "citeRegEx": "Shannon,? \\Q1949\\E", "shortCiteRegEx": "Shannon", "year": 1949}, {"title": "Convolutional LSTM network: A machine learning approach for precipitation nowcasting", "author": ["Shi", "Xingjian", "Chen", "Zhourong", "Wang", "Hao", "Yeung", "Dit-Yan", "Wong", "Wai-Kin", "Woo", "Wangchun"], "venue": "CoRR, abs/1506.04214,", "citeRegEx": "Shi et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Shi et al\\.", "year": 2015}, {"title": "Unsupervised learning of video representations using lstms", "author": ["Srivastava", "Nitish", "Mansimov", "Elman", "Salakhutdinov", "Ruslan"], "venue": null, "citeRegEx": "Srivastava et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Srivastava et al\\.", "year": 2015}, {"title": "Parallel multidimensional lstm, with application to fast biomedical volumetric image segmentation", "author": ["Stollenga", "Marijn", "Byeon", "Wonmin", "Liwicki", "Marcus", "Schmidhuber", "J\u00fcrgen"], "venue": "CoRR, abs/1506.07452,", "citeRegEx": "Stollenga et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Stollenga et al\\.", "year": 2015}, {"title": "Sequence to sequence learning with neural networks", "author": ["Sutskever", "Ilya", "Vinyals", "Oriol", "Le", "Quoc V"], "venue": null, "citeRegEx": "Sutskever et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Sutskever et al\\.", "year": 2014}, {"title": "Video surveillance online repository (visor): an integrated framework", "author": ["Vezzani", "Roberto", "Cucchiara", "Rita"], "venue": "Multimedia Tools and Applications,", "citeRegEx": "Vezzani et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Vezzani et al\\.", "year": 2010}, {"title": "Deepflow: Large displacement optical flow with deep matching", "author": ["P. Weinzaepfel", "J. Revaud", "Z. Harchaoui", "C. Schmid"], "venue": "In Computer Vision (ICCV),", "citeRegEx": "Weinzaepfel et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Weinzaepfel et al\\.", "year": 2013}, {"title": "Anisotropic Huber-L1 optical flow", "author": ["Werlberger", "Manuel", "Trobin", "Werner", "Pock", "Thomas", "Wedel", "Andreas", "Cremers", "Daniel", "Bischof", "Horst"], "venue": "In Proceedings of the British Machine Vision Conference (BMVC),", "citeRegEx": "Werlberger et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Werlberger et al\\.", "year": 2009}, {"title": "Backpropagation. chapter Gradient-based Learning Algorithms for Recurrent Networks and Their Computational Complexity", "author": ["Williams", "Ronald J", "Zipser", "David"], "venue": null, "citeRegEx": "Williams et al\\.,? \\Q1995\\E", "shortCiteRegEx": "Williams et al\\.", "year": 1995}], "referenceMentions": [{"referenceID": 12, "context": "Motivated by these two shortcomings, we focus on reducing the supervision effort required to train recurrent neural networks, which are known for their ability to handle sequential input data (Williams & Zipser, 1995; Hochreiter et al., 2000).", "startOffset": 192, "endOffset": 242}, {"referenceID": 13, "context": "The human brain has a complex system of visual memory modules, including visual short-term memory (VSTM), iconic memory, and long-term memory (Hollingworth, 2004).", "startOffset": 142, "endOffset": 162}, {"referenceID": 21, "context": "Among them, VSTM is responsible mainly for understanding visual changes (movement, light changes) in dynamic environments, by integrating visual stimuli over periods of time (Phillips, 1974; Magnussen, 2000).", "startOffset": 174, "endOffset": 207}, {"referenceID": 1, "context": "Understanding objectness is crucial for high-level tasks such as semantic segmentation or action recognition (Alexe et al., 2012).", "startOffset": 109, "endOffset": 129}, {"referenceID": 0, "context": "In this spirit, our approach is similar to the recent work of Agrawal et al. (2015), who show that the features learnt by exploiting (freely-available) ego-motion information as supervision data are as good as features extracted with human-labelled supervision data.", "startOffset": 62, "endOffset": 84}, {"referenceID": 3, "context": "Note that at the hardware level, this variations-centred reasoning is similar to event-based cameras (Boahen, 2005), which have started to make an impact in robotic applications (Kim et al.", "startOffset": 101, "endOffset": 115}, {"referenceID": 17, "context": "Note that at the hardware level, this variations-centred reasoning is similar to event-based cameras (Boahen, 2005), which have started to make an impact in robotic applications (Kim et al., 2014).", "startOffset": 178, "endOffset": 196}, {"referenceID": 22, "context": "Architectures based on LSTM cells (Hochreiter & Schmidhuber, 1997) have been very successful in various tasks involving one-dimensional temporal sequences: speech recognition (Sak et al., 2014), machine translation (Sutskever et al.", "startOffset": 175, "endOffset": 193}, {"referenceID": 28, "context": ", 2014), machine translation (Sutskever et al., 2014), music composition (Eck & Schmidhuber), due to their ability to preserve information over long periods of time.", "startOffset": 29, "endOffset": 53}, {"referenceID": 10, "context": "Multi-dimensional LSTM networks have been proposed to deal with (2D) images (Graves et al., 2007) or (3D) volumetric data (Stollenga et al.", "startOffset": 76, "endOffset": 97}, {"referenceID": 27, "context": ", 2007) or (3D) volumetric data (Stollenga et al., 2015), treating the data as spatial sequences.", "startOffset": 32, "endOffset": 56}, {"referenceID": 30, "context": "Our approach is partially related to optical flow estimation works like DeepFlow (Weinzaepfel et al., 2013) and FlowNet (Fischer et al.", "startOffset": 81, "endOffset": 107}, {"referenceID": 6, "context": ", 2013) and FlowNet (Fischer et al., 2015).", "startOffset": 20, "endOffset": 42}, {"referenceID": 0, "context": "As mentioned in the previous section, our work is similar in spirit to (Agrawal et al., 2015), by establishing a direct link between vision and motion, in an attempt to reduce supervision effort for high-level scene understanding tasks.", "startOffset": 71, "endOffset": 93}, {"referenceID": 8, "context": "Multi-dimensional LSTM networks have been proposed to deal with (2D) images (Graves et al., 2007) or (3D) volumetric data (Stollenga et al., 2015), treating the data as spatial sequences. Since in our work we aim at building a visual short-term memory, customised LSTM cells that deal with temporal sequences of spatial data represent a natural choice. Recently, Srivastava et al. (2015) proposed an LSTM-based video autoencoder, which aims at generating past and future frames in a sequence, in an unsupervised manner.", "startOffset": 77, "endOffset": 388}, {"referenceID": 19, "context": "In a classic spatial autoencoder (Masci et al., 2011), the encoder and decoder learn proprietary feature spaces that allow an optimal decomposition of the input using some form of regularisation to prevent learning a trivial mapping.", "startOffset": 33, "endOffset": 53}, {"referenceID": 25, "context": "Note that a similar convolutional LSTM implementation was recently used by Shi et al. (2015) for precipitation nowcasting.", "startOffset": 75, "endOffset": 93}, {"referenceID": 31, "context": "We use Huber loss as penalty (2) with its corresponding derivative (3), due to its edge-preserving capability (Werlberger et al., 2009).", "startOffset": 110, "endOffset": 135}, {"referenceID": 15, "context": "We use similar differentiable grid generator and image sampler as STN (Jaderberg et al., 2015).", "startOffset": 70, "endOffset": 94}, {"referenceID": 27, "context": "To train the proposed architecture, we follow a setup similar to (Stollenga et al., 2015); we use rmsprop, with parameters = 10\u22125, \u03c1 = 0.", "startOffset": 65, "endOffset": 89}, {"referenceID": 4, "context": "We used Sintel dataset (Butler et al., 2012), which contains 23 synthetic video sequences, with 50 frames each, and ground truth optical flow maps.", "startOffset": 23, "endOffset": 44}, {"referenceID": 23, "context": "The training and validation of the overall architecture was done using a set of video sequences extracted from PROST dataset (Santner et al., 2010) and ViSOR dataset (Vezzani & Cucchiara, 2010).", "startOffset": 125, "endOffset": 147}, {"referenceID": 26, "context": "As a sanity check, we also ran experiments on moving MNIST dataset (Srivastava et al., 2015), which consists of 10k sequences of 20 frames each, obtained by moving (translating) MNIST digit images inside a square of size 64\u00d764, using uniform random sampling to obtain direction and velocity; the sequences can contain several overlapping digits in one frame.", "startOffset": 67, "endOffset": 92}, {"referenceID": 7, "context": "It is worth mentioning that we tried to run experiments on KITTI odometry dataset (Geiger et al., 2012), which contains video sequences captured by a driving car, and Sintel dataset mentioned above, which are classic datasets used in the optical flow community.", "startOffset": 82, "endOffset": 103}, {"referenceID": 24, "context": "with Shannon-Nyquist sampling theorem (Shannon, 1949).", "startOffset": 38, "endOffset": 53}, {"referenceID": 5, "context": "static repeated patterns that produce a false perception of movement (Conway et al., 2005).", "startOffset": 69, "endOffset": 90}, {"referenceID": 16, "context": "Last but not least, the proposed architecture composed of memory module and built-in feedback loop could be applied for static images as a compression mechanism, in an attempt to train an iconic memory, similar to the inspirational work on jigsaw video compression (Kannan et al., 2007).", "startOffset": 265, "endOffset": 286}], "year": 2017, "abstractText": "We describe a new spatio-temporal video autoencoder, based on a classic spatial image autoencoder and a novel nested temporal autoencoder. The temporal encoder is represented by a differentiable visual memory composed of convolutional long short-term memory (LSTM) cells that integrate changes over time. Here we target motion changes and use as temporal decoder a robust optical flow prediction module together with an image sampler serving as built-in feedback loop. The architecture is end-to-end differentiable. At each time step, the system receives as input a video frame, predicts the optical flow based on the current observation and the LSTM memory state as a dense transformation map, and applies it to the current frame to generate the next frame. By minimising the reconstruction error between the predicted next frame and the corresponding ground truth next frame, we train the whole system to extract features useful for motion estimation without any supervision effort. We believe these features can in turn facilitate learning high-level tasks such as path planning, semantic segmentation, or action recognition, reducing the overall supervision effort.", "creator": "LaTeX with hyperref package"}}}