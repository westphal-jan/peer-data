{"id": "1511.06312", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "19-Nov-2015", "title": "Good, Better, Best: Choosing Word Embedding Context", "abstract": "We propose two methods of learning vector representations of words and phrases that each combine sentence context with structural features extracted from dependency trees. Using several variations of neural network classifier, we show that these combined methods lead to improved performance when used as input features for supervised term-matching.", "histories": [["v1", "Thu, 19 Nov 2015 19:13:58 GMT  (12kb)", "http://arxiv.org/abs/1511.06312v1", null]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["james cross", "bing xiang", "bowen zhou"], "accepted": false, "id": "1511.06312"}, "pdf": {"name": "1511.06312.pdf", "metadata": {"source": "CRF", "title": null, "authors": ["Bing Xiang", "Bowen Zhou"], "emails": ["crossj@oregonstate.edu", "bingxia@us.ibm.com", "zhou@us.ibm.com"], "sections": [{"heading": null, "text": "ar Xiv: 151 1,06 312v 1 [cs.C L] 19 N"}, {"heading": "1 Introduction", "text": "When used as attributes to represent words, they are much more compact than \"uniform\" vectors. Moreover, their geometry can encode a wealth of information about relationships between words, and hence their meanings, which can be exploited by downstream applications. The crucial question is how best to achieve this geometry for a given task. There are two types of training data that are often used to learn word vectors. Approaches based on the \"distribution hypothesis\" assume that the semantics of a word can be determined by the contexts in which it appears. They model vectors by some form of dimension compression of word cooperation statistics taken from an example text. Other methods learn vectors from pre-existing information about specific relationships between words, such as those found in a structured knowledge base. We suggest two vector learning methods that combine sentence context entries with the same characteristics that we automatically combine to construct for them."}, {"heading": "2 Related Work", "text": "Many different approaches to learning vector representations of words have been taken. Methods that have been trained on large text examples, relying directly or indirectly on word count coincidence, are based on latent semantic analysis (Deerwester, 1990). Recent examples include the window-based approach of Mikolov et al. (2013c) and the GloVe method (Pennington et al., 2014). There is also work that incorporates previous knowledge with a dictionary context (Yu and Dredze, 2014). It is also possible to perform unattended learning using structured information about the entities or concepts depicted, such as the relationships encoded in an online knowledge database. Approaches to using this type of training data include the use of regional networks and / or core density estimates (Bordes et al., 2011) and coding relationships as translations on hyper-level Wang et (our 2014 word direct estimation)."}, {"heading": "3 Combined Objective", "text": "In this thesis, we combine the set-window context with the structural characteristics described in Section 4. The common goal is to use both types of training data in parallel, so that the training goal is to maximize the combined log probability: 1TT \u2211 t = 1 \u2211 \u2212 c \u2264 j \u2264 c, j 6 = 0log p (wt + j | wt) + \u03b1 | P | \u2211 (w, f) \u0445 Plog p (f | w) (1), where T is the number of words in the training corpus, c is the context window size, P is the set of all word pairs and structural characteristics (w, f), and \u03b1 is a weighting parameter as between the two types of training data. Conditional probabilities are modeled with vectors, as in Word2Vec, and two sets of output vectors, internally to the algorithm, are learned to represent words and structural characteristics as contexts."}, {"heading": "4 Structural Features", "text": "As a representative means of building word vectors on functional relationships between words, rather than just word proximity, we started with first-order dependency arcs, as described in Levy and Goldberg (2013). For each dependence arc, each word is provided with a attribute consisting of the arc name, whether the word is the head or tail of the arc, and the attached word.1 We also extracted two types of higher-value traits from the dependence trees: subject-object relationships and subject-relationships. Noun-relationship characteristics capture certain taxonomic relationships between nouns, as reflected in the structure of a sentence, such as when a word names an instance of a category defined by another word, or when they are different names for the same word. This methodology is based on the syntactic patterns used to recognize the relationships is a type of relationship in (Fan et al., 2011) examples of these dependency patterns that are to be seen in and the patterns of dependence they are."}, {"heading": "5 Qualitative Comparison", "text": "To assess the relative qualities captured by the different embedding methods, we looked at the words that came closest to a number of different target words, just as in Levy and Goldberg (2014). Unsurprisingly, the closest words across the different vector sets were very similar, since all were trained with the same base corpus. In particular, jointly trained vectors lead to similar words that are subjectively very similar to those of vectors that were trained only with structural characteristics. However, there is reason to believe that the common objective vectors capture some additional semantic nuances stemming from the inter-2As maintained by the KOPI project (Pataki et al., 2012)."}, {"heading": "6 Experiments", "text": "To quantitatively compare their predictive properties, we tested word embeddings generated with each method as inputs to binary term matching classifiers, using two different sets of data. We used flat neural network classifiers with three alternative architectures, with a uniform hidden layer size of 200. Results are given in Table 3. For the standard multilayer perceptron (\"MLP\") algorithm, the vector representations of each word were paired to make input to a single hidden layer with a leaky rectifier activation function (Glorot et al., 2011; Maas et al., 2013). We also used a version in which the two vectors were each used as input to the same hidden layer, i.e. the weights were divided, and the hidden layer output for each layer along with its elementary absolute difference became a logically similar layer to a shared (P) layer."}, {"heading": "6.1 WordNet Synonyms", "text": "The first source of term matching pairs we looked at were synonyms from the lexical WordNet database (Miller, 1995). WordNet groups words into synsets that are groups of words (and multi-word phrases) with the same meaning.3This is proving to be a relatively difficult task alone with unattended vectors. One possible reason for this is that the vectors do not distinguish between the meaning of the word and essentially combine all use cases into a single representation.The WordNet data is characterized by relatively frequent words where the synonymy may be based on a secondary or figurative definition. Examples of this in our data are the pairs (fastball, bullet) and (arrest, collar). Especially since the raw text data tend to group words by discourse domain rather than function, it makes sense that visual use as \"bullet\" patterns, which are much more useful in the area of hyperformation spareness. \""}, {"heading": "6.2 Paraphrase Database Lexical Pairs", "text": "As another source of term matching data, we used pairs from the small (most trusted) lexical paraphrase dataset of the Paraphrase Database (Ganitkevitch et al., 2013), which consist of cases where two different words in bi-texts mean the same thing. Unlike the WordNet pairs, this dataset tends to use less common words. It contains many examples of things like alternative spellings and alternative names (or transliterations) for correct terms. Here, the difference in performance between raw text and structural feature vectors was much smaller. The combined objective methods were better than both. This suggests that when the two types of training data have a comparable benefit but work in different ways, a combined training method can create embedding that captures the benefits of both approaches."}, {"heading": "7 Conclusion and Future Work", "text": "It is clear that the best way to train continuous word embedding depends on the scope. If comparably useful information is encoded both in the sentence context and in structured representations, the combination of the two can lead to superior vector representations as training data. In future work, we plan to use richer feature sets, such as relationships between knowledge base, as training inputs and explore the mathematics behind the sequential training mechanism. 4The PDB data consists of 79,696 training pairs, 9,962 validation pairs and 9,962 test pairs, splitting 20% / 80% between matches and non-matches."}], "references": [{"title": "Theano: A CPU and GPU Math Expression Compiler", "author": ["O. Breuleux", "F. Bastien", "P. Lamblin", "R. Pascanu", "G. Desjardins", "J. Turian", "D. Warde-Farley", "Y. Bengio"], "venue": "Proceedings of the Python for Scientific Computing", "citeRegEx": "Bergstra et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Bergstra et al\\.", "year": 2010}, {"title": "Structured Embeddings of Knowledge Bases", "author": ["Jason Weston", "Ronan Collobert", "Yoshua Bengio"], "venue": null, "citeRegEx": "Bordes et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Bordes et al\\.", "year": 2011}, {"title": "A fast and accurate dependency parser using neural networks. In Empirical Methods in Natural Language Processing (EMNLP)", "author": ["Chen", "Manning2014] Danqi Chen", "Christopher D. Manning"], "venue": null, "citeRegEx": "Chen et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Chen et al\\.", "year": 2014}, {"title": "Indexing by latent semantic analysis", "author": ["Scott C. Deerwester", "Susan T. Dumais", "Thomas K. Landauer", "George W. Furnas", "Richard A. Harshman"], "venue": "Journal of the American Society for Information Science,", "citeRegEx": "Deerwester et al\\.,? \\Q1990\\E", "shortCiteRegEx": "Deerwester et al\\.", "year": 1990}, {"title": "Mining Knowledge from Large Corpora for Type Coercion in Question Answering. Web Scale Knowledge Extraction (WEKEX", "author": ["Fan et al.2011] James Fan", "Aditya Kalyanpur", "J. William Murdock", "Branimir K. Boguraev"], "venue": null, "citeRegEx": "Fan et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Fan et al\\.", "year": 2011}, {"title": "Automatic knowledge extraction from documents", "author": ["Fan et al.2012] James Fan", "Aditya Kalyanpur", "D.C. Gondek", "David A. Ferrucci"], "venue": "IBM Journal of Research and Development", "citeRegEx": "Fan et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Fan et al\\.", "year": 2012}, {"title": "PPDB: The Paraphrase Database", "author": ["Benjamin Van Durme", "Chris Callison-Burch"], "venue": "In North American Chapter of the Association for Computational Linguistics", "citeRegEx": "Ganitkevitch et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Ganitkevitch et al\\.", "year": 2013}, {"title": "Deep Sparse Rectifier Neural Networks", "author": ["Glorot et al.2011] Xavier Glorot", "Antoine Bordes", "Yoshua Bengio"], "venue": "In Proceedings of the 14th International Conference on Artificial Intelligence and Statistics. JMLR W&CP Volume", "citeRegEx": "Glorot et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Glorot et al\\.", "year": 2011}, {"title": "Dependency-Based Word Embeddings. In Association for Computational Linguistics (ACL)", "author": ["Levy", "Goldberg2014] Omer Levy", "Yoav Goldberg"], "venue": null, "citeRegEx": "Levy et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Levy et al\\.", "year": 2014}, {"title": "Rectifier nonlinearities improve neural network acoustic models. In Workshop on Deep Learning for Audio, Speech, and Language Processing", "author": ["Maas et al.2013] Andrew L. Maas", "Awni Y. Hannun", "Andrew Y. Ng"], "venue": null, "citeRegEx": "Maas et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Maas et al\\.", "year": 2013}, {"title": "Efficient estimation of word representations in vector space. arXiv preprint arXiv:1301.3781", "author": ["Kai Chen", "Greg Corrado", "Jeffrey Dean"], "venue": null, "citeRegEx": "Mikolov et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Continuous Space Word Representations", "author": ["lya Sutskever", "Kai Chen", "Greg S. Corrado", "Jeff Dean"], "venue": null, "citeRegEx": "Mikolov et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Distributed representations of words and phrases and their compositionality", "author": ["Wen-tau Yih", "Geoffrey Zweig"], "venue": null, "citeRegEx": "Mikolov et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "WordNet: a lexical database for English. Communications of the ACM", "author": ["George A. Miller"], "venue": null, "citeRegEx": "Miller.,? \\Q1995\\E", "shortCiteRegEx": "Miller.", "year": 1995}, {"title": "GloVe: Global vectors for word representation", "author": ["Richard Socher", "Christopher D. Manning"], "venue": "In Empirical Methods in Natural Language Processing (EMNLP)", "citeRegEx": "Pennington et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Pennington et al\\.", "year": 2014}, {"title": "Knowledge graph embedding by translating on hyperplanes", "author": ["Wang et al.2014] Zhen Wang", "Jianwen Zhang", "Jianlin Feng", "Zheng Chen"], "venue": "In Proceedings of the Twenty-Eighth AAAI Conference on Artificial Intelligence", "citeRegEx": "Wang et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2014}, {"title": "Improving Lexical Embeddings with Semantic Knowledge. In Association for Computational Linguistics (ACL)", "author": ["Yu", "Dredze2014] Mo Yu", "Mark Dredze"], "venue": null, "citeRegEx": "Yu et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Yu et al\\.", "year": 2014}], "referenceMentions": [{"referenceID": 14, "context": "(2013c) and the GloVe method (Pennington et al., 2014).", "startOffset": 29, "endOffset": 54}, {"referenceID": 10, "context": "Recent examples include the window-based approach of Mikolov et al. (2013c) and the GloVe method (Pennington et al.", "startOffset": 53, "endOffset": 76}, {"referenceID": 1, "context": "Approaches for utilizing this type of training data include the use of neural networks and/or kernel density estimation (Bordes et al., 2011) and encoding relations as translations on hyperplanes (Wang et al.", "startOffset": 120, "endOffset": 141}, {"referenceID": 15, "context": ", 2011) and encoding relations as translations on hyperplanes (Wang et al., 2014).", "startOffset": 62, "endOffset": 81}, {"referenceID": 10, "context": "The most direct precursor to our work is the wellknown Word2Vec algorithm (Mikolov et al., 2013a; Mikolov et al., 2013b; Mikolov et al., 2013c). It consists of a simple single-layer neural model where the goal is to maximize the ability of the word representations to predict neighbors of the word in a large raw-text corpus. It was observed by Levy and Goldberg (2014) that this algorithm could be generalized to use any type of discrete features with which words", "startOffset": 75, "endOffset": 370}, {"referenceID": 4, "context": "This methodology is based on the syntactic patterns used to recognize is-a relationships in (Fan et al., 2011).", "startOffset": 92, "endOffset": 110}, {"referenceID": 7, "context": "For the standard multilayer perceptron (\u201cMLP\u201d) algorithm, the vector representations of each word in a pair were concatenated to make the input to a single hidden layer with a leaky rectifier activation function (Glorot et al., 2011; Maas et al., 2013).", "startOffset": 212, "endOffset": 252}, {"referenceID": 9, "context": "For the standard multilayer perceptron (\u201cMLP\u201d) algorithm, the vector representations of each word in a pair were concatenated to make the input to a single hidden layer with a leaky rectifier activation function (Glorot et al., 2011; Maas et al., 2013).", "startOffset": 212, "endOffset": 252}, {"referenceID": 13, "context": "The first source of term-matching pairs we considered were synonyms from the WordNet lexical database (Miller, 1995).", "startOffset": 102, "endOffset": 116}, {"referenceID": 6, "context": "As another source of term-matching data, we used pairs from the small (highest-confidence) lexical paraphrase dataset of the Paraphrase Database (Ganitkevitch et al., 2013).", "startOffset": 145, "endOffset": 172}], "year": 2015, "abstractText": "We propose two methods of learning vector representations of words and phrases that each combine sentence context with structural features extracted from dependency trees. Using several variations of neural network classifier, we show that these combined methods lead to improved performance when used as input features for supervised term-matching.", "creator": "LaTeX with hyperref package"}}}