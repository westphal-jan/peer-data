{"id": "1511.06314", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "19-Nov-2015", "title": "Why M Heads are Better than One: Training a Diverse Ensemble of Deep Networks", "abstract": "Convolutional Neural Networks have achieved state-of-the-art performance on a wide range of tasks. Most benchmarks are led by ensembles of these powerful learners, but ensembling is typically treated as a post-hoc procedure implemented by averaging independently trained models with model variation induced by bagging or random initialization. In this paper, we rigorously treat ensembling as a first-class problem to explicitly address the question: what are the best strategies to create an ensemble? We first compare a large number of ensembling strategies, and then propose and evaluate novel strategies, such as parameter sharing (through a new family of models we call TreeNets) as well as training under ensemble-aware and diversity-encouraging losses. We demonstrate that TreeNets can improve ensemble performance and that diverse ensembles can be trained end-to-end under a unified loss, achieving significantly higher \"oracle\" accuracies than classical ensembles.", "histories": [["v1", "Thu, 19 Nov 2015 19:19:58 GMT  (7788kb,D)", "http://arxiv.org/abs/1511.06314v1", null]], "reviews": [], "SUBJECTS": "cs.CV cs.LG cs.NE", "authors": ["stefan lee", "senthil purushwalkam", "michael cogswell", "david crandall", "dhruv batra"], "accepted": false, "id": "1511.06314"}, "pdf": {"name": "1511.06314.pdf", "metadata": {"source": "CRF", "title": "Why M Heads are Better than One: Training a Diverse Ensemble of Deep Networks", "authors": ["Stefan Lee", "Senthil Purushwalkam", "Michael Cogswell", "David Crandall", "Dhruv Batra"], "emails": ["djcran}@indiana.edu", "spurushw@andrew.cmu.edu", "dbatra}@vt.edu"], "sections": [{"heading": null, "text": "Most benchmarks are led by ensembles of these high-performing learners, but ensembles are typically treated as a post-hoc process that is implemented by comparing independently trained models with model variations induced by bagging or random initialization. In this paper, we rigorously treat ensembling as a first-class problem to explicitly answer the question: What are the best strategies for creating an ensemble? We first compare a large number of ensemble strategies and then propose and evaluate new strategies, such as parameter release (through a new model family we call TreeNets) and training among ensemble-conscious and diversity-enhancing losses. We show that TreeNets can improve ensemble performance and that different ensembles can be trained to the end at a uniform loss, achieving significantly higher \"oracle accuracy\" than classical ensembles."}, {"heading": "1. Introduction", "text": "This year, it has come to the point that it has never come as far as it has this year."}, {"heading": "2. Related Work", "text": "Other work to improve the robustness and diversity of grouped learners has decades of work in machine learning research, but more recently ensembles of CNNs have been studied. Related work can generally be divided into two categories: general-network ensemble learning and its more recent application to CNNs.Ensemble Learning Theory. Neural networks have been used in a variety of settings with many different modifications. Many of the theoretical foundations for ensemble learning with neural networks were laid in the 1990s. Krogh et al. [22] and Hansen and Salamon [16] provided theoretical and empirical evidence that diversity in error distributions across member models can increase ensemble performance, leading to ensemble methods, predictions of models trained with different initializations, and models trained at different bootstrapped trainings. These methods take an indirect approach to introducing diversity into ensembles."}, {"heading": "3. Experimental Design", "text": "First, we describe the data sets, architectures, and evaluation metrics that we use in our experiments to better understand the similarity in deep networks."}, {"heading": "3.1. Datasets and Architectures", "text": "We evaluate three popular image classification benchmarks: CIFAR10 [20], CIFAR100 [20], and the ImageNet Large Scale Visual Recognition Challenge (ILSVRC) of 2012 [30]. Since our goal is not to present new designs and architectures for these tasks, but rather to investigate the effects of different similarity techniques, we use standard models and training routines. All models are trained using stochastic gradient decrease with dynamics and weight loss. CIFAR10. For this dataset, we use Caffe \"CIFAR10 Quick\" [19] network as our base model. The reference model is based on a batch size of 350 for 5,000 iterations with a dynamics of 0.9, weight drop of 0.004, and an initial learning rate of 0.0001, which falls to 0.0001, after 4000 iterations. We refer to this network and training method as the CIFAR1- Network Dynamics."}, {"heading": "3.2. Evaluation Metrics", "text": "Ensemble average accuracy is the accuracy of the \"standard\" test time for ensembles - averaging the beliefs of all members and predicting the most confident class. Strong performance on this measure indicates that ensemble members generally agree on the correct response, reducing errors by smoothing out between members. Oracle accuracy, on the other hand, is the accuracy of the ensemble when an \"oracle\" chooses for each example the prediction of the most accurate ensemble member. Oracle accuracy shows what the ensemble as a collection of specialists knows and has been used in previous work to measure ensemble performance [4, 6, 13-15]."}, {"heading": "4. Random Initialization and Bagging", "text": "We now present our analysis of the various approaches to training CNN ensembles. This section focuses on standard approaches, while Sections 5 and 6 present new ideas for parameter distribution and ensemble-conscious loss.Random initializing network weights and random resampling of dataset subsets (bagging) are perhaps the most commonly used methods to create model variations among members of CNN ensembles. Table 1 presents results using three different ensemble techniques: (1) Random initialization, in which all member models see the same training data but are initialized with different random seeds; (2) Bagging, in which each member uses the same initial weights, but trains on a subset of data sampled (with substitutes) from the original; and (3) Combined, which uses both techniques. Figures in the table are accuracies and standard deviations in three studies. The CIFAR ensembles were built with four members, while the SVILC ensembles had five."}, {"heading": "5. Parameter Sharing with TreeNets", "text": "In fact, it is the case that most of them are in a position to move into another world, in which they are able to move, in which they move, in which they move, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they, in which they live, in which they, in which they live, in which they, in which they live, in which they, in which they are able to move, in which they are able to move, in which they are able to move, in which they are able to move, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they, in which they are"}, {"heading": "6. Training Under Ensemble-Aware Losses", "text": "In the two preceding sections, each member of the ensemble was trained with the same goal - the independent transverse entropy of the individual members of the ensemble. What happens if the goal is aware of the ensemble? We begin with a surprising result: the first \"natural\" idea, simply to optimize the performance of the average beliefs of the ensemble, does not work, and we give an intuition as to why this is the case (lack of diversity). This negative result shows that a more cautious conception of loss-conscious functions of the ensemble is crucial. Afterwards, we propose a diversity-enhancing loss function that shows a significantly improved oracle performance."}, {"heading": "6.1. Directly Optimizing for Model Averaging", "text": "For a standard ensemble, the test-time classification is typically done by averaging the results of the member networks = Now it is natural to explicitly optimize the performance of the corresponding ensemble-mean loss during the training. We have run all four ensemble architectures under two settings: (1) Score averaged, in which we determine the results of the last shift (i.e. the results representing inputs to the Softmax function), and (2) Probability averaged, in which we evaluate the Softmax probabilities of the ensemble members on average. Intuitively, the difference between the two settings is that the former are \"calibrated\" to produce scores of similar relative magnitude, while the latter is not. Table 3 shows the results of these experiments, which are averaged over three attempts. In all cases, the average performance of the ensemble is reduced, with rehearsal averaged leading to a greater degradation. This is counterintuitive to the optimization of the ensemble members."}, {"heading": "6.2. Adding Diversity via Multiple Choice Learning", "text": "We have previously discussed the role of ensemble diversity in the context of modeling averaging; however, in many settings, the generation of multiple plausible hypotheses can be preferred to produce a single answer. Ensembles fit naturally into this space, since they provide multiple responses through design.We build on Multiple Choice Learning (MCL) [13], which we usually converge to similar solutions, prompting us to directly optimize the need for diversity. In this section, we develop and experiment with diversity encouraging losses and demonstrate their effectiveness in specializing in ensembles.We build on Multiple Choice Learning (MCL) [13], which we briefly recapitulate here. Let's consider a number of predictors {1, \u03b8M} on how these phenomena: x \u2192 P is a probability distribution across some labels, and a dataset D = {x1, y1), (xN, yN)} where each function has a bottomless truth label."}, {"heading": "7. Distributed Ensemble Training", "text": "Training an ensemble on a single GPU is prohibitively expensive, so standard practice for large ensembles is to train the multiple networks either sequentially or in parallel. However, any form of model pairing requires communication between students. To enable our scale experiments, we have developed and published a modification of Caffe, which we call MPI-Caffe, which uses the Message Passing Interface (MPI) [1] to enable crossGPU / Machine Communication. These communication operations are provided as Caffe model layers, which allow network designers to quickly experiment with distributed networks in which different parts of the model lie on different GPUs and machines. Figure 3 shows how an ensemble of CIFAR10 quick networks with parameter sharing and model averaging is defined as a single specification and distributed over multiple processes. In MPI-Caffe, each process is assigned a single layer of MPU warners (one rank)."}, {"heading": "8. Discussion and Conclusion", "text": "Our bagging experiments show that the diversity induced in ensemble members by random parameter initializations is more useful than that induced by pockets of duplicated examples. Our experiment to explicitly train ensemble members on average shows that the average beliefs of ensemble members before calculating losses have the unintended effect of eliminating diversity in gradients. Our novel diversity-inducing MCL loss shows that promoting diversity among ensemble members can significantly improve performance. Finally, our novel TreeNet architecture shows that diversity is important in high-level representations, while lower-level filters could do better without it."}, {"heading": "Appendix A. TreeNet Object Detection Results on PASCAL VOC 2007", "text": "As briefly described in section 5 of the main paper, the ILSVRC-Alex TreeNet architecture for object detection was evaluated using the PASCAL VOC 2007 dataset, which includes notes on Bounding Truth Bounding Boxes for 20 object classes. For this task, we used Fast R CNNs [11]. During the training, Fast RCNNs refined the class forecast as well as the Bounding Box coordinates from ensemble member models. To evaluate TreeNets and Standard Ensembles, we coordinate four different instances for each within the Fast R-CNN framework and compress the mean and standard deviation of the class average deviation of the average precision (APs), as well as the average ensembles for each of the boxes presented with the different results of these classes."}, {"heading": "Appendix B. Instability of Averaged Softmax Outputs", "text": "As discussed in Section 6.1 of the main work, this forecast reliability requires a multiplication of probabilities that may be quite small, which may result in a underflow of the interesting results of the forecast results of the forecast results of the forecast results of the forecast results of the forecast results of the forecast results of the forecast results of the forecast results of the forecast results of the forecast results of the forecast results of the forecast results of the forecast results of the forecast results of the forecast results of the forecast results of the forecast results of the forecast results of the forecast results of the forecast results of the forecast results of the forecast results of the forecast results of the forecast results of the forecast results of the forecast results of the forecast results of the forecast results of the forecast results of the forecast results of the forecast results of the forecast results of the forecast results of the forecast results of the forecast results of the forecast results of the forecast results of the forecast results of the forecast results of the forecast results of the forecast results of the forecast results of the forecast results of the forecast results of the forecast results of the forecast results of the forecast results of the forecast results of the forecast results of the forecast results of the forecast results of the forecast results of the forecast results of the forecast results of the forecast results of the forecast results of the forecast results of the forecast results of the forecast results of the forecast results of the forecast results of the forecast results of the forecast results of the forecast results of the forecast results of the forecast results of the forecast results of the forecast results of the forecast results of the forecast results of the forecast results of the forecast results of the forecast results of the forecast results of the forecast results of the forecast results of the forecast results of the forecast results of the forecast results of the forecast results of the forecast results of the forecasts"}, {"heading": "Appendix C. Pseudo-code for Stochastic Gradient Descent with MCL", "text": "We describe the classic MCL algorithm and our approach to integrating the MCL coordinate descent with stochastic gradient descent in Section 6.2 of the main paper. Here, we provide psuedocode for both algorithms to highlight the differences and provide additional clarity. Data: Data set (xi, yi) and loss L Result: predictor parameters \u03b81 \u00b7 \u00b7 \u00b7 \u03b8M Initialization: D \u00b2 0 = {D1 \u00b7 \u00b7 DM} k mean (D, k = M) t \u2190 0, while D \u00b2 t 6 = D \u00b2 t \u2212 1 do Step 1: Train each predictor until completion using only its corresponding subset of data. Step 2: Insert each example on its least loss predictor Dm = {d \u00b2 D | p \u00b2 m = target sequence: L (d; m) \u2264 m \u00b2 t \u00b2 t \u00b2 s: c \u00b7 m \u00b7 m \u00b2 s: c \u00b7 m \u00b7 m \u00b7 m \u00b2 s: c \u00b7 b \u00b7 c \u00b7 loss \u00b7 c \u00b7 SD: &ltD: 1; Sollz = 1; Sollz = 1)"}, {"heading": "Appendix D. Visualizations for MCL Trained Ensembles", "text": "To show how the distribution of class examples changes compared to training for MCL, we have created a video showing the proportion of each CIFAR10 class assigned to each predictor at test time, and how it changes over the course of the training processes.The intensity of each class symbol is proportional to the proportion of class examples assigned to each predictor. Figure 4 shows an early and later section of the video. These images can be interpreted as gradients of neuron output in relation to the input image. [34] Visualizations described in Section 6.2 of the main paper for different layers among members of traditional and MCL ensembles have the greatest impact on the output of the network. Figure 5 shows these visualizations for an input image, where they can be interpreted as gradients of a neuron output in relation to the input image. Features that are clear in these images have the greatest impact on the output of the network for an input image."}, {"heading": "Appendix E. MPI-Caffe", "text": "MPI-Caffe is a modification of the popular Caffe Deep Learning Framework that enables cross-GPU / cross-machine communication on MPI-enabled systems as a model level. Providing these MPI operations as layers gives network designers the flexibility to quickly experiment with distributed networks while abstracting much of the communication logic, allowing them to experiment with extremely large (i.e., larger than can be held in a single GPU) networks as well as ensemble-conscious model parallelism schemes. This document explains the function of these layers as well as the use of examples. The core functionality in MPI-Caffe lies in \u2022 the MPI Broadcast layer discussed in Section E.1.1 \u2022 and the MPIGather layer discussed in Section E.1.2.The primary file defining the interface of the MPI layers is MPILayers.h. There are also many supporting modifications to the Caffe source that should be considered or tried."}, {"heading": "Appendix E.1. A Toy Example", "text": "Let's start with a toy example for creating a context for the MPI layer descriptions. Suppose we want to train a TreeNet ensemble of CIFAR10-Quick and train it at an average score loss. Figure 10 shows how we could modify the LeNet structure with MPI-Caffe to implement this model over three processes / GPUs in an MPI-enabled cluster. We'll go through this example to explain the function and parameterization of the MPIBroadcast and MPIGather layers."}, {"heading": "Appendix E.1.1 MPIBroadcast", "text": "The first level we will discuss is MPIBroadcast (highlighted in red in Figure 10). The MPIBroadcast layer sends a copy of its input blob to each process in its communication group during its forward pass. During the backward pass, the gradients of each copy are summed up and returned to the input blob. However, the communication group consists of all processes that perform a copy of a particular broadcast layer. By default, a communication group contains all processes; adding mpi _ rank: n rules in the include or exclude layer parameters can change this layer behavior. 1 layer {2 name: broad 3 type: MPIBroadcast 4 bottom: pool2 5 top: pool2 _ b 6 mpi _ param {7 root: 0 8} 9 include {10 mpi _ rank: 0 11 mpi _ rank: 1 12 mpi _ rank: 2 13} 14} Process 1 lobol2 _ poolb _ poolb _ 2 _ poolb _ 2."}, {"heading": "Appendix E.1.2 MPIGather", "text": "If the purpose of a broadcast layer is to collect some data and move copies into multiple process spaces, the MPIGather layer can be considered the opposite. In a forward pass, multiple copies of a blob are taken from multiple process spaces and collected in the root process. During a backward pass, the gradients for each top blob are returned to the corresponding input blob and process. Similar to the previous section, Figure 12 shows the layer definition from our example and a diagram of forward pass behavior. The mpi _ param {root} parameter in the Collect layer defines which process will pick up the collected blobs and produce the top blobs. In analogy to parsing the broadcast layer, collecting layers in non-root processes are truncated during network parsing (see Figure 10). There are some limitations to the use of the collect layer. Firstly, the bottom blob (ip2 in our example) must be verified during all communication processes."}, {"heading": "Appendix E.2. Notes and Other Examples", "text": "Worth mentioning are a few other MPI-Caffe application points: \u2022 the MPIBroadcast layer can be used to build a very large catchy network across multiple GPUs \u2022 the MPIGather layer can be used to enable more complex ensemble losses \u2022 there is no limit to the number or sequence of MPI layers, allowing for complex distributed networks \u2022 in situations where the latency of the network is lower than reading from the hard disk, the MPIBroadcast layer can be used to train multiple independent networks faster"}, {"heading": "Appendix E.3. Communication Cost Analysis", "text": "We tested our MPI Caffe framework on a large-scale cluster with a Tesla K20 GPU per node and a maximum MPI node bandwidth of 5.8 GB / sec. To characterize the communication effort of an ensemble, we measured the time we spent sharing different layers of the ILSVRC Alex \u00d7 5 architecture. Each network was operated on a separate node (with one node also holding the common layers). Figure 13 shows the communication time for a particular layer as a fraction of the forward retreat. The x-axis indicates the number of floats emitted per stack for each layer."}], "references": [], "referenceMentions": [], "year": 2015, "abstractText": "Convolutional Neural Networks have achieved state-of-<lb>the-art performance on a wide range of tasks. Most bench-<lb>marks are led by ensembles of these powerful learners, but<lb>ensembling is typically treated as a post-hoc procedure im-<lb>plemented by averaging independently trained models with<lb>model variation induced by bagging or random initializa-<lb>tion. In this paper, we rigorously treat ensembling as a first-<lb>class problem to explicitly address the question: what are<lb>the best strategies to create an ensemble? We first compare<lb>a large number of ensembling strategies, and then propose<lb>and evaluate novel strategies, such as parameter sharing<lb>(through a new family of models we call TreeNets) as well as<lb>training under ensemble-aware and diversity-encouraging<lb>losses. We demonstrate that TreeNets can improve ensemble<lb>performance and that diverse ensembles can be trained end-<lb>to-end under a unified loss, achieving significantly higher<lb>\u201coracle\u201d accuracies than classical ensembles.", "creator": "LaTeX with hyperref package"}}}