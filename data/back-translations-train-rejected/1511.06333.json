{"id": "1511.06333", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "19-Nov-2015", "title": "Efficient Sum of Outer Products Dictionary Learning (SOUP-DIL) and Its Application to Inverse Problems", "abstract": "The sparsity of natural signals in a transform domain or dictionary has been extensively exploited in several applications. More recently, the data-driven adaptation of synthesis dictionaries has shown promise in many applications compared to fixed or analytical dictionaries. However, dictionary learning problems are typically non-convex and NP-hard, and the alternating minimization approaches usually adopted to solve these problems are often computationally expensive, with the computations dominated by the NP-hard synthesis sparse coding step. In this work, we investigate an efficient method for dictionary learning by first decomposing the training data set into a sum of sparse rank-one matrices and then using a block coordinate descent approach to estimate the rank-one terms. The proposed algorithm involves efficient closed-form solutions. In particular, the sparse coding step involves a simple form of thresholding. We provide a convergence analysis for the proposed block coordinate descent method that solves a highly non-convex problem. Our experiments show the promising performance and significant speed-ups provided by our method over the classical K-SVD scheme in sparse signal representation and image denoising.", "histories": [["v1", "Thu, 19 Nov 2015 20:01:24 GMT  (1794kb)", "http://arxiv.org/abs/1511.06333v1", "This paper was submitted to ICLR 2016"], ["v2", "Fri, 4 Dec 2015 03:56:34 GMT  (1794kb)", "http://arxiv.org/abs/1511.06333v2", "This paper was submitted to ICLR 2016"], ["v3", "Thu, 7 Jan 2016 23:47:05 GMT  (1798kb)", "http://arxiv.org/abs/1511.06333v3", "This paper was submitted to ICLR 2016"], ["v4", "Fri, 21 Apr 2017 01:39:38 GMT  (768kb,D)", "http://arxiv.org/abs/1511.06333v4", "Accepted to IEEE Transactions on Computational Imaging. This paper also cites experimental results reported inarXiv:1511.08842"]], "COMMENTS": "This paper was submitted to ICLR 2016", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["saiprasad ravishankar", "raj rao nadakuditi", "jeffrey a fessler"], "accepted": false, "id": "1511.06333"}, "pdf": {"name": "1511.06333.pdf", "metadata": {"source": "CRF", "title": null, "authors": [], "emails": ["fessler}@umich.edu"], "sections": [{"heading": null, "text": "ar Xiv: 151 1,06 333v 1 [cs.L G] 19 Nov 2"}, {"heading": "1 INTRODUCTION", "text": "This year, it is closer than ever before in the history of the country."}, {"heading": "2.1 FORMULATION", "text": "In this paper, we consider a variant of problem (P0) (Bao et al., 2014). Specifically, we replace the sparsity constraints in (P0) with a penalty of (P0) N = 1 x x x x x x x x. Next, we dissect the matrix DX = DCT as the sum of (sparse) rank-one matrices or outer products Kk = 1 dkc T k, where ck is the smallest column of C. This is a natural decomposition of training data Y, because it separates the contributions of the different atoms in the representation of the data. It could also provide a natural way to determine the number of atoms (degrees of freedom) in the dictionary, especially atoms of a dictionary whose contributions to the data (Y) representation errors or modeling errors are small, could be dropped. Such a sum of OUter products (SOUP) decomposition was exploited before the problem."}, {"heading": "2.2 ALGORITHM", "text": "We propose a method for solving the variables in problem (P1). For each 1 \u2264 j \u2264 K we solve first (P1) in relation to cj by holding all other variables tight (sparse encoding step). As soon as cj is updated, we solve (P1) in relation to dj by holding all other variables tight (referred to as dictionary atom update step or dictionary update step)."}, {"heading": "2.2.1 SPARSE CODING STEP", "text": "Minimizing (P1) with respect to cj leads to the following non-convex problem, where Ej, Y \u2212 \u2211 k 6 = j dkc T k is a fixed matrix based on the most recent values of all other atoms and coefficients: min cj, year, Ej \u2212 djc T j, 2 F + 0, 0, 0 s.t. The following sentence provides the solution to problem (1) where the hard-swelling operator H\u03bb (\u00b7) is defined as (H\u03bb (b)) i = {0, | bi | < \u03bb bi, | 2) with b-Rn, and the subscript i indexes vector entries. We assume that the bound L > \u03bb and let 1N denote a vector of ones of length N. The operation \"i\" stands for elementary multiplication, the character (\u00b7) denotes the characters of the elements of a vector."}, {"heading": "2.2.2 DICTIONARY ATOM UPDATE STEP", "text": "(P3). (P3). (P3). (P3). (P3). (P3). (P3). (P3). (P3). (P3). (P3). (P3). (P3). (P3). (P3). (P3). (P3). (P3). (P3). (P3). (P3). (P3). (P3). (P3). (P.). (P3). (P3). (P.). (P3). (P3). (P3). (P3). (P3). (P3). (P3). (P3). (P3). (P3). (P3). (P3). (P3). (P3). (P3). (P3). (P3). (P3). (P3). (P3). (P3). (P3). (P. (P3). (P3). (P3). (P3). (P3). (P3). (P. (P3). (P3). (P3). (P3). (P. (P3). (P3). (P3). (P3). (P. (P3). (P. (P3).). (P3). (). (P3). (P. (P3). (). (). (P3). (P3). (). (P3). (). (). ().). (P3).). (P3). (P. (). (). (P. (). (P.). (). (P3). (). (). (). (). (P3). (). ().). (P3). (). (P3). (). ().)."}, {"heading": "3 CONVERGENCE ANALYSIS", "text": "This section presents a convergence analysis for the algorithm 1 that solves the problem (P1).The proposed algorithm is an exact method of derivation of block coordinates that must be solved for the unknowns in (P1).Due to the high degree of non-convergence that goes with it, standard results for convergence of block origin methods apply (e.g. (Tseng, 2001) but not here. We present some definitions and notations below, i.e. before specifying our convergence results for algorithm 1.First, a necessary condition is that x-Rp must be a minimizer of a (proper) function g: Rp 7 \u2192 (\u2212 \u221e, + \u221e] is that x is a critical point of g, i.e., 0-X-Z g (x), where X-Z is the sub-difference of g (x) the sub-difference of g (Rockafelstationary points & Whovich, 1997, 1997, 1997), that \"morality\" is generalized."}, {"heading": "3.1 MAIN RESULTS", "text": "The idea behind it is that it is about a way in which it is about a way in which it is about a way in which it is about a way in which it is about the question, to what extent it is about the question, to what extent it is about the question, to what extent it is about the question, to what extent it is about the question, to what extent it is about the question, to what extent it is about the question, to what extent it is about the question, to what extent it is about the question, to what extent it is about the question, to what extent it is about the question, to what extent it is about the question, to what extent it is about the question, to what extent it is about the question, to what extent it is about the question, to what extent it is about the question, to what extent it is about the question, to what extent it is about the question, to what extent it is about the question, to what extent it is about the question, to what is about the question, to what is about the question, to what is about the question, to what is about the question, to what is about the question, to what is about the question, to what is about the way it is about the question, to what is about the question, to what is about the question is about the question, to what is about the question, to what is about the question, to what is about the question, to what is about the question, to what is about the question, to what is about the question, to what is about the question, to what is about the question, to what is about the question, what is about the question, to what is about the question, to what is about the question, to what is about the question, to what is to what is about the question, to what is about the question, to what is about the question, to what is about the question, to what is about the question, to what is about the question, what is about the question, what is about the question, what is to what is about the question, what is to what is about the question, what is about the question, to what is to what is about the question, to what is to what is about the question, what is about the question, to what is to what is about the question, what is about the question, to what is to what is about the question, what is what is"}, {"heading": "4 NUMERICAL EXPERIMENTS", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.1 CONVERGENCE EXPERIMENT", "text": "To investigate the convergence behavior of the proposed SOUP-DIL algorithm 1, we extracted 3 x 104 size 8 x 8 fields from randomly selected locations in the Barbara, Boat and Hill images shown in Figure 2. Problem (P1) was solved to learn a 64 x 256 complete dictionary for this data, with \u03bb = 69. Algorithm 1 is initialized as described in Section 2.2.Fig.3 initializes the convergence behavior of SOUP-DIL. The goal in our method (Fig. 3 (a)) converged monotonously and quickly across iterations. We define the normalized sparse representation error (NSRE \u2212 DCT) as a sequence used to measure the sparse representation performance of learned dictators."}, {"heading": "4.2 SPARSE REPRESENTATION OF DATA", "text": "The second experiment used the same data as in Section 4.1 and learned dictionaries of size 64 \u00d7 256 for different decisions of the parameter \u03bb in (P1). We compare the performance of our algorithm with the K-SVD dictionary learning scheme (Aharon et al., 2006).The K-SVD method was used for several decisions of spareness (number of non-zeros) of the columns of the CT. \u03bb values for (P1) were chosen to achieve similar average column sparity in the CT as K-SVD. Both algorithms were initialized with the same supercomplete DCT and ran for 10 iterations each. Figure 4 shows the behavior of the SOUP-DIL and K-SVD algorithms for average column sparity in the CT as K-SVD. Both algorithms were initialized with the same supercomplete DCT and ran for 10 iterations each. Figure 4 shows the behavior of the UP-DIL algorithms for average column spareness in the CT SVD-SVD algorithms."}, {"heading": "4.3 IMAGE DENOISING", "text": "It is a very good method, which we then use to learn a dictatorial and economical codex for Y, with which we get the denoized image estimation, in which we then solve the following image estimates, in which D and D get the learned knowledge and patch codes from the noisest patches and patch codes from the noisest patches and patch codes from the noisest patches and patch codes from the noisest patches, and Pj is an operator building a patch as a vector: min xN = 1. Pjx \u2212 D: \"We are a vector building a patch as a vector, a vector, a vector, a vector, a vector, a vector, a vector, a vector, a vector, a vector, a vector, a vector, a vector, a, a vector, a, a vector, a, a vector, a, a vector, a, a vector, a, a vector, a vector, a, a vector, a, a vector, a, a vector, a, a vector, a vector, a, a, a, a, a, a, a, a, a, a, a, a, a, a, a, a, a, a, a, a, a, a, a, a, a, a, a"}, {"heading": "5 CONCLUSIONS", "text": "This paper proposed a method for the rapid descent of block coordinates for synthesis dictionary learning with a penalty of 0. The basic idea is to break down the training data as the sum of sparse rank-one matrices and then efficiently estimate the factors of rank-one matrices. A convergence analysis for a highly non-convex problem was presented for the proposed block-coordinate descent algorithm, which showed comparable or superior performance and significant acceleration compared to the classical KSVD method with sparse signal representation and densification. Extensions to the online learning method (Mairal et al., 2010) merit further investigation."}, {"heading": "ACKNOWLEDGMENTS", "text": "This work was partially supported by the following grants: ONR grants N00014-15-1-2141, DARPA Young Faculty Award D14AP00086, ARO MURI grants W911NF-11-1-0391 and 2015-05174-05, NIH grants U01 EB01875301 and a UM-SJTU seed capital."}], "references": [{"title": "Learning sparsely used overcomplete dictionaries", "author": ["A. Agarwal", "A. Anandkumar", "P. Jain", "P. Netrapalli", "R. Tandon"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Agarwal et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Agarwal et al\\.", "year": 2014}, {"title": "K-SVD: An algorithm for designing overcomplete dictionaries for sparse representation", "author": ["M. Aharon", "M. Elad", "A. Bruckstein"], "venue": "IEEE Transactions on signal processing,", "citeRegEx": "Aharon et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Aharon et al\\.", "year": 2006}, {"title": "New algorithms for learning incoherent and overcomplete dictionaries", "author": ["S. Arora", "R. Ge", "A. Moitra"], "venue": null, "citeRegEx": "Arora et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Arora et al\\.", "year": 2013}, {"title": "L0 norm based dictionary learning by proximal methods with global convergence", "author": ["C. Bao", "H. Ji", "Y. Quan", "Z. Shen"], "venue": "In IEEE Conference on Computer Vision and Pattern Recognition (CVPR),", "citeRegEx": "Bao et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Bao et al\\.", "year": 2014}, {"title": "Dictionary learning for sparse coding: Algorithms and analysis", "author": ["C. Bao", "H. Ji", "Y. Quan", "Z. Shen"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence,", "citeRegEx": "Bao et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Bao et al\\.", "year": 2015}, {"title": "Image denoising by sparse 3D transformdomain collaborative filtering", "author": ["K. Dabov", "A. Foi", "V. Katkovnik", "K. Egiazarian"], "venue": "IEEE Trans. on Image Processing,", "citeRegEx": "Dabov et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Dabov et al\\.", "year": 2007}, {"title": "Image denoising via sparse and redundant representations over learned dictionaries", "author": ["M. Elad", "M. Aharon"], "venue": "IEEE Trans. Image Process.,", "citeRegEx": "Elad and Aharon,? \\Q2006\\E", "shortCiteRegEx": "Elad and Aharon", "year": 2006}, {"title": "Method of optimal directions for frame design", "author": ["K. Engan", "S.O. Aase", "J.H. Hakon-Husoy"], "venue": "In Proc. IEEE International Conference on Acoustics, Speech, and Signal Processing,", "citeRegEx": "Engan et al\\.,? \\Q1999\\E", "shortCiteRegEx": "Engan et al\\.", "year": 1999}, {"title": "Dictionary identification\u2013sparse matrix-factorization via l1 minimization", "author": ["R. Gribonval", "K. Schnass"], "venue": "IEEE Trans. Inform. Theory,", "citeRegEx": "Gribonval and Schnass,? \\Q2010\\E", "shortCiteRegEx": "Gribonval and Schnass", "year": 2010}, {"title": "Non-local sparse models for image restoration", "author": ["J. Mairal", "F. Bach", "J. Ponce", "G. Sapiro", "A. Zisserman"], "venue": "In IEEE International Conference on Computer Vision,", "citeRegEx": "Mairal et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Mairal et al\\.", "year": 2009}, {"title": "Online learning for matrix factorization and sparse coding", "author": ["J. Mairal", "F. Bach", "J. Ponce", "G. Sapiro"], "venue": "J. Mach. Learn. Res.,", "citeRegEx": "Mairal et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Mairal et al\\.", "year": 2010}, {"title": "Variational Analysis and Generalized Differentiation", "author": ["B.S. Mordukhovich"], "venue": "Vol. I: Basic theory. Springer-Verlag,", "citeRegEx": "Mordukhovich,? \\Q2006\\E", "shortCiteRegEx": "Mordukhovich", "year": 2006}, {"title": "Direct optimization of the dictionary learning problem", "author": ["A. Rakotomamonjy"], "venue": "IEEE Transactions on Signal Processing,", "citeRegEx": "Rakotomamonjy,? \\Q2013\\E", "shortCiteRegEx": "Rakotomamonjy", "year": 2013}, {"title": "MR image reconstruction from highly undersampled k-space data by dictionary learning", "author": ["S. Ravishankar", "Y. Bresler"], "venue": "IEEE Trans. Med. Imag.,", "citeRegEx": "Ravishankar and Bresler,? \\Q2011\\E", "shortCiteRegEx": "Ravishankar and Bresler", "year": 2011}, {"title": "Dictionary learning for sparse representation: A novel approach", "author": ["M. Sadeghi", "M. Babaie-Zadeh", "C. Jutten"], "venue": "IEEE Signal Processing Letters,", "citeRegEx": "Sadeghi et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Sadeghi et al\\.", "year": 2013}, {"title": "Learning overcomplete dictionaries based on atomby-atom updating", "author": ["M. Sadeghi", "M. Babaie-Zadeh", "C. Jutten"], "venue": "IEEE Transactions on Signal Processing,", "citeRegEx": "Sadeghi et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Sadeghi et al\\.", "year": 2014}, {"title": "Recursive least squares dictionary learning algorithm", "author": ["K. Skretting", "K. Engan"], "venue": "IEEE Transactions on Signal Processing,", "citeRegEx": "Skretting and Engan,? \\Q2010\\E", "shortCiteRegEx": "Skretting and Engan", "year": 2010}, {"title": "Improving dictionary learning: Multiple dictionary updates and coefficient reuse", "author": ["L.N. Smith", "M. Elad"], "venue": "IEEE Signal Processing Letters,", "citeRegEx": "Smith and Elad,? \\Q2013\\E", "shortCiteRegEx": "Smith and Elad", "year": 2013}, {"title": "Exact recovery of sparsely-used dictionaries", "author": ["D.A. Spielman", "H. Wang", "J. Wright"], "venue": "In Proceedings of the 25th Annual Conference on Learning Theory, pp", "citeRegEx": "Spielman et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Spielman et al\\.", "year": 2012}, {"title": "Convergence of a block coordinate descent method for nondifferentiable minimization", "author": ["P. Tseng"], "venue": "J. Optim. Theory Appl.,", "citeRegEx": "Tseng,? \\Q2001\\E", "shortCiteRegEx": "Tseng", "year": 2001}, {"title": "A fast patch-dictionary method for whole-image recovery", "author": ["Y. Xu", "W. Yin"], "venue": "URL ftp://ftp.math.ucla.edu/pub/camreport/cam13-38.pdf. UCLA CAM report", "citeRegEx": "Xu and Yin,? \\Q2013\\E", "shortCiteRegEx": "Xu and Yin", "year": 2013}, {"title": "Dictionary learning for sparse approximations with the majorization method", "author": ["M. Yaghoobi", "T. Blumensath", "M. Davies"], "venue": "IEEE Transaction on Signal Processing,", "citeRegEx": "Yaghoobi et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Yaghoobi et al\\.", "year": 2009}], "referenceMentions": [{"referenceID": 9, "context": "More recently, the data-driven adaptation of synthesis dictionaries called dictionary learning, has shown promise in applications (Elad & Aharon, 2006; Mairal et al., 2009; Ravishankar & Bresler, 2011) compared to analytical dictionaries such as wavelets or DCT.", "startOffset": 130, "endOffset": 201}, {"referenceID": 1, "context": "Given a collection of training signals {yi} N i=1 that are represented as columns of the matrix Y \u2208 R n\u00d7N , the dictionary learning problem is often formulated as follows (Aharon et al., 2006) (P0) min D,X \u2016Y \u2212DX\u2016 2 F s.", "startOffset": 171, "endOffset": 192}, {"referenceID": 10, "context": ", incoherence) for the dictionary D, or solving an online version (where the dictionary is updated sequentially as new training signals arrive) of the problem (Mairal et al., 2010).", "startOffset": 159, "endOffset": 180}, {"referenceID": 7, "context": "Algorithms for (P0) or its variants (Engan et al., 1999; Aharon et al., 2006; Yaghoobi et al., 2009; Skretting & Engan, 2010; Mairal et al., 2010; Smith & Elad, 2013; Sadeghi et al., 2013) typically alternate in some form between a sparse coding step (updating X), and a dictionary update step (solving for D).", "startOffset": 36, "endOffset": 188}, {"referenceID": 1, "context": "Algorithms for (P0) or its variants (Engan et al., 1999; Aharon et al., 2006; Yaghoobi et al., 2009; Skretting & Engan, 2010; Mairal et al., 2010; Smith & Elad, 2013; Sadeghi et al., 2013) typically alternate in some form between a sparse coding step (updating X), and a dictionary update step (solving for D).", "startOffset": 36, "endOffset": 188}, {"referenceID": 21, "context": "Algorithms for (P0) or its variants (Engan et al., 1999; Aharon et al., 2006; Yaghoobi et al., 2009; Skretting & Engan, 2010; Mairal et al., 2010; Smith & Elad, 2013; Sadeghi et al., 2013) typically alternate in some form between a sparse coding step (updating X), and a dictionary update step (solving for D).", "startOffset": 36, "endOffset": 188}, {"referenceID": 10, "context": "Algorithms for (P0) or its variants (Engan et al., 1999; Aharon et al., 2006; Yaghoobi et al., 2009; Skretting & Engan, 2010; Mairal et al., 2010; Smith & Elad, 2013; Sadeghi et al., 2013) typically alternate in some form between a sparse coding step (updating X), and a dictionary update step (solving for D).", "startOffset": 36, "endOffset": 188}, {"referenceID": 14, "context": "Algorithms for (P0) or its variants (Engan et al., 1999; Aharon et al., 2006; Yaghoobi et al., 2009; Skretting & Engan, 2010; Mairal et al., 2010; Smith & Elad, 2013; Sadeghi et al., 2013) typically alternate in some form between a sparse coding step (updating X), and a dictionary update step (solving for D).", "startOffset": 36, "endOffset": 188}, {"referenceID": 1, "context": "The K-SVD method (Aharon et al., 2006) has been particularly popular and demonstrated to be useful in numerous applications (Elad & Aharon, 2006; Ravishankar & Bresler, 2011).", "startOffset": 17, "endOffset": 38}, {"referenceID": 18, "context": "Some recent works (Spielman et al., 2012; Arora et al., 2013; Xu & Yin, 2013; Bao et al., 2014; Agarwal et al., 2014) have studied the convergence of (specific) synthesis dictionary learning algorithms.", "startOffset": 18, "endOffset": 117}, {"referenceID": 2, "context": "Some recent works (Spielman et al., 2012; Arora et al., 2013; Xu & Yin, 2013; Bao et al., 2014; Agarwal et al., 2014) have studied the convergence of (specific) synthesis dictionary learning algorithms.", "startOffset": 18, "endOffset": 117}, {"referenceID": 3, "context": "Some recent works (Spielman et al., 2012; Arora et al., 2013; Xu & Yin, 2013; Bao et al., 2014; Agarwal et al., 2014) have studied the convergence of (specific) synthesis dictionary learning algorithms.", "startOffset": 18, "endOffset": 117}, {"referenceID": 0, "context": "Some recent works (Spielman et al., 2012; Arora et al., 2013; Xu & Yin, 2013; Bao et al., 2014; Agarwal et al., 2014) have studied the convergence of (specific) synthesis dictionary learning algorithms.", "startOffset": 18, "endOffset": 117}, {"referenceID": 15, "context": "Our work shares similarities with a recent dictionary learning approach (Sadeghi et al., 2014) that exploits a sum of outer products model for the training data.", "startOffset": 72, "endOffset": 94}, {"referenceID": 0, "context": ", 2014; Agarwal et al., 2014) have studied the convergence of (specific) synthesis dictionary learning algorithms. However, these dictionary learning methods have not been demonstrated to be useful in applications such as image denoising. Bao et al. (2014) in fact show that their method, although a fast proximal scheme, denoises less effectively (typically 0.", "startOffset": 8, "endOffset": 257}, {"referenceID": 3, "context": "1 FORMULATION We consider a sparsity penalized variant of Problem (P0) (Bao et al., 2014) in this work.", "startOffset": 71, "endOffset": 89}, {"referenceID": 1, "context": "Such a Sum of OUter Products (SOUP) decomposition has been exploited before (Aharon et al., 2006; Smith & Elad, 2013).", "startOffset": 76, "endOffset": 117}, {"referenceID": 1, "context": "This is lower than the per-iteration cost of learning an n\u00d7K synthesis dictionary D using K-SVD (Aharon et al., 2006), which scales (assuming that the synthesis sparsity level s \u221d n and K \u221d n in K-SVD)1 as O(Nn).", "startOffset": 96, "endOffset": 117}, {"referenceID": 19, "context": ", (Tseng, 2001)) do not apply here.", "startOffset": 2, "endOffset": 15}, {"referenceID": 11, "context": ", 0 \u2208 \u2202g(x), where \u2202g(x) is the sub-differential of g at x (Rockafellar & Wets, 1997; Mordukhovich, 2006).", "startOffset": 59, "endOffset": 105}, {"referenceID": 3, "context": "In contrast, Bao et al. (2014) showed that the distance between successive iterates may not converge to 0 for popular algorithms such as K-SVD.", "startOffset": 13, "endOffset": 31}, {"referenceID": 1, "context": "We compare the performance of our algorithm to the K-SVD dictionary learning scheme (Aharon et al., 2006).", "startOffset": 84, "endOffset": 105}, {"referenceID": 5, "context": "(2009) proposed a non-local method for image denoising that also exploits learned dictionaries and achieves denoising performance comparable to the well-known BM3D (Dabov et al., 2007) denoising method.", "startOffset": 164, "endOffset": 184}, {"referenceID": 8, "context": "Mairal et al. (2009) proposed a non-local method for image denoising that also exploits learned dictionaries and achieves denoising performance comparable to the well-known BM3D (Dabov et al.", "startOffset": 0, "endOffset": 21}, {"referenceID": 10, "context": "Extensions of the method for online learning (Mairal et al., 2010) merit further study.", "startOffset": 45, "endOffset": 66}], "year": 2017, "abstractText": "The sparsity of natural signals in a transform domain or dictionary has been extensively exploited in several applications. More recently, the data-driven adaptation of synthesis dictionaries has shown promise in many applications compared to fixed or analytical dictionaries. However, dictionary learning problems are typically non-convex and NP-hard, and the alternating minimization approaches usually adopted to solve these problems are often computationally expensive, with the computations dominated by the NP-hard synthesis sparse coding step. In this work, we investigate an efficient method for dictionary learning by first decomposing the training data set into a sum of sparse rank-one matrices and then using a block coordinate descent approach to estimate the rank-one terms. The proposed algorithm involves efficient closed-form solutions. In particular, the sparse coding step involves a simple form of thresholding. We provide a convergence analysis for the proposed block coordinate descent method that solves a highly non-convex problem. Our experiments show the promising performance and significant speed-ups provided by our method over the classical K-SVD scheme in sparse signal representation and image denoising.", "creator": "LaTeX with hyperref package"}}}