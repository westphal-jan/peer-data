{"id": "1511.06341", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "19-Nov-2015", "title": "Communicating Semantics: Reference by Description", "abstract": "Messages often refer to entities such as people, places and events. Correct identification of the intended reference is an essential part of communication. Lack of shared unique names often complicates entity reference. Shared knowledge can be used to construct uniquely identifying descriptive references for entities with ambiguous names. We introduce a mathematical model for \"Reference by Description\" and provide results on the conditions under which, with high probability, programs can construct unambiguous references to most entities in the domain of discourse.", "histories": [["v1", "Thu, 19 Nov 2015 20:14:43 GMT  (153kb,D)", "https://arxiv.org/abs/1511.06341v1", null], ["v2", "Fri, 20 Nov 2015 19:33:36 GMT  (153kb,D)", "http://arxiv.org/abs/1511.06341v2", null], ["v3", "Thu, 21 Jan 2016 00:42:06 GMT  (440kb,D)", "http://arxiv.org/abs/1511.06341v3", null], ["v4", "Mon, 7 Mar 2016 16:41:38 GMT  (442kb,D)", "http://arxiv.org/abs/1511.06341v4", null]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["ramanathan v guha", "vineet gupta"], "accepted": false, "id": "1511.06341"}, "pdf": {"name": "1511.06341.pdf", "metadata": {"source": "CRF", "title": null, "authors": ["Vineet Gupta"], "emails": [], "sections": [{"heading": "1 Introduction", "text": "In such cases, ambiguity can be resolved by equipping the symbol with an unambiguous description - \"Lincoln, the President.\" We call this reference, for example, by a combination of language (the possible references of \"Lincoln\") and knowledge / context (that there is only one president named Lincoln) who clearly communicates a reference in a different way. \"Lincoln, the President.\" We use a combination of language (the possible references of \"Lincoln\") and knowledge / context (that there is only one president named Lincoln) who clearly communicates a reference in a different way. This method of ambiguity is common in human communication. For example, in the headline of the New York Times [17] \"John McCarthy, Pioneer in Artificial Intelligence.\" The term \"John McCarthy\" alone is ambiguous. It could refer to a computer scientist, a politician or politician, or a novel description."}, {"heading": "2 Background", "text": "The formal investigation of descriptions began with Frege [10] and Russell's [22] descriptivist theory of names, in which name / identity is synonymous with descriptions. Kripke [16] argued against this position on the basis of examples in which differences in information theory produce vastly different descriptions of the same entity. We focus not on the philosophical foundations of name / identity, but on enabling unambiguous communication between software programs. \"In [23], the founding paper of information theory, Shannon referred to this problem by saying:\" Frequently the messages have meaning; that is, they relate to a particular system with certain physical or conceptual units. \"But he went further by saying that these semantic aspects of communication are irrelevant to the technical problem.\" Computer treatment of descriptions related to the linking of duplicates. \""}, {"heading": "3 Communication model", "text": "In our communication model (fig. 1): 1. Messages are about an underlying domain. A number of fields ranging from databases and artificial intelligence to number theory have modeled their domains as a series of units and a series of N-tuples on these units. We use this model to represent the domain. Since arbitrary N-tuples can be constructed from 3-tuples [21], we limit ourselves to 3-tuples, i.e., directed labeled graphs. We refer to the domain as a graph, and the entities as nodes, which can be people, places, etc., or literal values such as strings and numbers. The graph has N-nodes and the receiver each have a copy of a portion of that graph. The slurs, the copies may be different."}, {"heading": "4 Quantifying Sharing", "text": "When the sender describes a NodeX by specifying an arc between X and a node named N1, he expects the receiver to know both which node N1 refers to (common language) and which nodes have arcs leading to that node (common domain knowledge). We distinguish between the two."}, {"heading": "4.1 Shared Language / Linguistic Ambiguity", "text": "The ambiguity of Ni isAi = N \u2211 j = 0 \u2212 pij log (pij) Ai is conditional entropy - the entropy of the probability distribution given over the entirety of the units that the name was Ni. If there is no ambiguity in Ni, Ai = Studies at Works for Co-Authors with Alumnus of Student VG RGK S B1 2 4Sally and Dave's shared view of the domain: Sally wants to share information about Student 3 (the numbers corresponding to the students are not shared. We have just added them here for our use.) RGIf she sends the following message: Dave could interpret it as: RG3RG4So Sally adds a description to the message: Dave interprets this correctly as:? RGSRGS3orFlat message descriptions0."}, {"heading": "4.2 Shared Domain Knowledge", "text": "The number of divided information is not just a function of how much the graph is divided, but the amount of information / knowledge in the graph. We want to characterize the information / knowledge contents of the graph from the perspective of its ability to support descriptions. However, let the graph's adjectivity matrix be generated by a process with entropy Hg. Then the information content of a sufficiently large differentiation between the nodes in the graph becomes possible. However, the classical measure of information (rate) is entropy. Let's let the graph's adjectivity matrix be generated by a process with entropy Hg. Then the information content of a sufficiently large set of randomly selected L-arcs will be. However, since the goal of our descriptions is to distinguish a given node (X) from other nodes, we do not want to randomly select arcs that include the node."}, {"heading": "4.3 Salience of Random Graphs", "text": "Given our interest in probabilistic estimates of the necessary division processes, we focus on graphs generated by stochastic processes, i.e. random graphs. The most frequently studied random graph model is that proposed by Erdos and Renyi [9]. The Erdos-Renyi random graph G (N, p) has N nodes where the probability of a randomly chosen pair of nodes being joined is p. The selection rate of a sufficiently large G (N, p) graph is \u2212 log (or \u2212 log (1 \u2212 p), whichever is larger). Recent work on stochastic graph models has attempted to capture some of the phenomena found in real graphs."}, {"heading": "4.4 Description Shapes", "text": "The number of knots (D) in a description of length (L) depends on their shape. The decoding complexity of a description is a function of both their size and shape. Flat descriptions that are easiest to decipher only use arcs between the described node and the nodes in the description. For example, Jim, who lives in Palo Alto, CA, age 56, works for Stanford, studying at UC Berkeley, married to Jane. The description consists of arcs from the described node, Jim, to the descriptor nodes, Palo Alto CA, 56 years, Stanford, UC Berkeley and Jane. The length (L) of flat descriptions, i.e. the number of arcs included, is the same as the number of knots (D), i.e. L = D. You have O (aD) the deciphering complexity, where an average degree of each node is."}, {"heading": "5 Description size for unambiguous reference", "text": "Problem definition: The sender tries to communicate a message that mentions a large number of randomly selected units whose average ambiguity is Ax. The general graph has N nodes. Each unit in the message has a description that includes an average of D descriptor nodes whose ambiguity rate is Ad. The description includes bD (b \u2264 D / 2) slurs between the descriptor nodes, which can be used to reduce the ambiguity in the descriptor nodes themselves, if at all. We are interested in the average number of slurs and nodes required in the description. In the most general sense, the ambiguity resolved by a description is smaller or equal to its information content. Specifically, if F is the graph's highlighting rate, we have assumed a uniform highlighting rate: D = AxF \u2212 max (0, Ad \u2212 bF) (3) Equation 33 covers a number of communications scenarios we are now exploring."}, {"heading": "5.1 Flat Descriptions", "text": "Flat descriptions (fig. 2), which are easiest to decipher, only use slurs between the described node and the descriptor nodes. They can be deciphered in O (aD), where a is the average degree of a node. For flat descriptions b = 0, Gib, D = AxF \u2212 Ad Flat descriptions (4) Flat descriptions are very easy to decipher, but require relatively unique descriptor nodes, i.e. F Ad. Most descriptions in human communication fall into this category."}, {"heading": "5.2 Deep Descriptions", "text": "If the descriptor nodes themselves are very ambiguous (F \u2212 Ad is small), the ambiguity of the descriptor nodes can be reduced by adding bD slurs between them. If the descriptor nodes are significantly less ambiguous than the described node (Ad < Ax / 2), any ambiguity in the descriptor nodes can be eliminated by including Ad / F connections between them, which gives us the following:"}, {"heading": "5.3 Purely Structural Descriptions", "text": "If sender and receiver do not share language skills, all nodes are at most ambiguous (Ax = Ad = logN). We have to rely solely on the structure of the graph. We have: D = 2 log (N) / F Purely structural descriptions (6) By using detailed descriptions containing multiple attributes of each of the descriptor nodes, we can start communication even if there is almost no common language."}, {"heading": "5.4 Limiting Sender Computation", "text": "Assuming unique descriptor nodes, we have: D = log (N) + AxF Flat Landmark Descriptions (7) If the descriptions are generated by random selection of facts about the company, the entropy salinity rate corresponds to the adjacence matrix of the graph, Hg. In this case, the nodes can use the same set of descriptor nodes, hence the name \"Landmark Descriptions.\""}, {"heading": "5.5 Language vs Knowledge + Computation trade off", "text": "Consider a node without a name (Ax = log (N)). Given a series of candidate descriptions with an emphasis rate F, we could consider two types of descriptions that are at opposite ends of the spectrum when using language against knowledge. We could use purely structural descriptions (eq 6) that do not use a common language. We could also use a flat boundary description (eq. 34) that uses a much more common language and ignores most of the common graph structure / knowledge. Although the number of nodes D = 2 log (N) / F is the same in both, flat descriptions of length O (D) do not require any calculation to generate and can be interpreted in time O (aD), whereas the former are of length O (D2)."}, {"heading": "5.6 Minimum Sharing Required", "text": "If there are relatively unique descriptor nodes, Ax / F, the minimum size of the description for X, is a measure of the difficulty of communicating a reference to X. It can be high because either X is very ambiguous (Ax \u2192 log (N)) and / or because very little is known about it (F \u2192 0). If Ax / F \u2265 N cannot provide a reference to X. As the ambiguity of the descriptor nodes increases, domain knowledge must play a greater role in discambiguity. At the boundary, if there are no names, we must use purely structural descriptions. In this case, 2 log (N) / F must be smaller than N."}, {"heading": "5.7 Non-identifying descriptions", "text": "In order for this comparison to be meaningful, we use statements with the same emphasis rate (F) in both cases. As the purpose is to hide the identity of the company, its name is not included in the description, i.e., Ax = log (N). For flat descriptions, we have: D \u2264 log (N) \u2212 log (K) F \u2212 Ad (8). Comparing this to Equation 4 (with Ax = log (N), we find that there is only a small size difference (log (K) / (F \u2212 Ad) between K anonymous descriptions and the shortest unambiguous description. This is due to the phase change (discussed in the appendix), whereby approximately D = Ax / (F \u2212 Ad) the probability that such a description is of similar size is not equal to that of a unique description."}, {"heading": "6 Conclusion", "text": "As Shannon [23] has suggested, communication is not only the correct transmission of a symbol sequence, but also the understanding of what those symbols signify. Even if the symbols are ambiguous, the sender can clearly communicate through descriptions of which entities the symbols refer to. We have introduced a model for \"Reference by Description\" and show how the size of the description increases with the amount of shared knowledge, both linguistically and in the domain. We have shown how unique references can be constructed from purely ambiguous terms, at the expense of additional calculations. The framework in this paper opens up many directions for the next steps: - Our model makes a series of simplistic assumptions. It assumes that the sender knows what the recipient knows. An example of this assumption that collapses is when two strangers who speak different languages have to communicate with each other."}, {"heading": "Acknowledgments", "text": "The first author thanks Phokion Kolaitis and Andrew Tompkins for providing a home in IBM research to begin this work, and Bill Coughran at Google to complete it. Carolyn Au made the numbers. Carolyn Au, Vint Cerf, Madeleine Clark, Evgeniy Gabrilovich, Neel Guha, Sreya Guha, Asha Guha, Joe Halpern, Maggie Johnson, Brendan Juba, Arun Majumdar, Peter Norvig, Mukund Sundarajan and Alfred Spector gave feedback on the drafts of this work."}, {"heading": "Appendices", "text": "We have the following annexes: Appendix A: Derivation of Results and Various Special Cases Appendix B: Empirical Validation of Results on Random Graphs Appendix C: Studies on the Use of Descriptive References on a Series of Newspapers / Magazine Articles Appendix D: Alternative Problem Formulations"}, {"heading": "A Derivation of Results", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "A.1 Problem Statement", "text": "We get a large diagram G with N nodes and a message that is a subgraph of G. The message contains a number of randomly selected nodes. We construct descriptions for each of the nodes (X) in this subgraph so that it can be uniquely identified. We are interested in a stochastic characterization of the relationship between the amount of common domain knowledge, common language, the number of nodes in the description (D) and the number of slurs (L) in the description. For the sake of exposure, we go through the derivative for the simpler case where there is no ambiguity in the descriptor nodes. We then expand this evidence to the case where the descriptor nodes themselves can be ambiguous. We model the diagram that corresponds to the domain of the discourse and the message that is transmitted as generated by a stochastic process."}, {"heading": "A.2 Unambiguous Descriptor Nodes", "text": "Consider a node that has the CXDi configuration with X. It provides an ambiguous description if we are not one of the two nodes we know of. (The description for X involvesD, CXD1, CXD2, CXD3, CXD3,...) There are a number of possible configurations of these nodes between a randomly selected element of this configuration that has the CXDi configuration. There are 2Ax \u2212 1 nodes that could be confused. (The possible combination of D descriptor nodes in search of a unique description for X. (The probability is a randomly selected element of this configuration CXDi is qi. There are 2Ax \u2212 1 nodes that could be confused.) We are looking at a specific element in the CXDi configuration with X."}, {"heading": "A.3 Ambiguous Descriptor Nodes", "text": "We consider the case where the descriptor nodes themselves are ambiguous."}, {"heading": "A.5 Flat Descriptions", "text": "The shape of the description affects the decoding of complexity. Flat descriptions, which are easiest to decipher, use only slurs between the described node and the nodes in the description - no slurs between other nodes are taken into account in the description. Therefore, for these b = 0, D = AxF \u2212 Ad Flat descriptions (38) are used. In the case where we do not have a name for the described node, we get: D = logNF \u2212 Ad Flat descriptions (39) As expected, flat descriptions are longer if the sender cannot search through multiple candidates. D = log (N) + Ax F \u2212 Ad Flat descriptions of boundary stones (40) If F < Ad, we cannot use flat descriptions."}, {"heading": "A.6 Deep Descriptions", "text": "If Ad > 0, the number of nodes in the description can be reduced by using deep descriptions. In deep descriptions, in addition to the slurs between the descriptor nodes and X, slurs between the D nodes can also be included in the description. If b \u2264 AdF, we have: D = log (N) + Ax (b + 1) F \u2212 Ad Deep Landmark Descriptions (41) Note: If (b + 1) F < Ad, then communication is not possible. If the descriptor nodes are unique, adding depth is of no use. If we have sufficiently large S, D = Ax (b + 1) F \u2212 Ad Deep Descriptions (42) < Ad < Ad / F & ud / D & uD > D, the depth (Ad > Ax) increases without increasing the depth."}, {"heading": "A.7 Purely Structural Descriptions", "text": "For purely structural descriptions (i.e. no names), Ax = Ad = logN applies. If b = D / 2 is allowed in Equation 42: D = log (N) (D / 2 + 1) F \u2212 log (N) \u2248 log (N) DF / 2 \u2212 log (N) (44) D2F = 2D (1 + log (N)) \u2248 2D log (N) (45), we obtain: D = 2 log (N) F (46)"}, {"heading": "A.8 Message Composition / Self Describing Messages", "text": "We have allowed the entities in a message to be randomly selected, and the entities in the message may or may not have relationships between them. For example, if it is a set of census records or entries from a phone book, the different entities in the message probably will not be part of each other's short descriptions. In contrast, the entities that appear in a message like a news article are \"self-descriptive\" messages because of the relationships between them and may appear in the descriptions of the other. We call messages where all descriptors for the entities in the message are other entities in the message. We are interested in the size of such messages. Let's include the relationship of each node to all other nodes in the message. Let's put Ax = Ad in Equation 41 and assume that Ad / F < D / 2 and Ad log (N), we get: D = Log (N) (47) All messages with at least 34 nodes as equal."}, {"heading": "A.9 Communication Overhead", "text": "Descriptions can be used to overcome linguistic ambiguities, but at the expense of the additional arithmetic complexity involved in encoding and decoding descriptions. We will now consider the impact of the descriptions on the channel capacity required to send the message. We will only consider the case where the sender and recipient share the same view of the graph. Let's consider the case where each node in the graph is assigned a unique name. If we assume that the number of unique arc labels is much lower than N, the bulk of the communication costs related to the W nodes in the message. Now, consider the case in which each node in the graph is assigned a unique name. Any name requires Log (N) bits to encode. If the message is a random selection of arcs from the graph, we will need W Log (N) bits to encode references to the nodes."}, {"heading": "A.10 Non-identifying descriptions", "text": "We are interested in the question of how much can be revealed about an entity without uniquely identifying that entity. However, this is useful in applications that include data for research purposes, to provide personalized content, personalized ads, etc. We are interested in comparing the number of statements that can be made about an entity, while still keeping it indistinguishable from K other entities [7], with the number of statements required to uniquely identify it. To make this comparison meaningful, the descriptions must be drawn from the same entity complex of descriptions. For a given validity rate F, we are interested in how large D can be, so that at least K other nodes fulfill this description. Since the purpose is to hide the entity's identity, the entity name is not included in the description, i.e., Ax = log (N).Given N nodes and R descriptions, we assume that each node is randomly assigned to a description."}, {"heading": "B Empirical Validation", "text": "In fact, it is as if most people are able to determine for themselves what they want and what they want. (...) It is not as if they were able to determine for themselves. (...) It is as if they were able to determine for themselves. (...) It is not as if they were able to determine for themselves. (...) It is as if they were able to determine for themselves. (...) It is as if they were able to do for themselves. (...) It is not as if they were able to do for themselves. (...) It is as if they were able to do for themselves. (...) It is as if they were able to do for themselves. (...) It is as if they were able to do for themselves. (...) It is as if they were able to do for themselves. (...). (...) () () (). () () () (). () () (). () ((). () () (). () (). () (). (). () (). (). (). (). (). (). (). (). (). (). (). (). (). (). (). (). (). (). (). (). (). (). (). (). (). (). (). ()). (). (). (). (). (). (). ().). (). (). (). (). (). (). (). ().). (). (). (). (). (). ().). (). (). (). (). (). (). ().). (). (). (). ().). (). (). (). (). ().). (). (). (). ().). ().). (). ().). (). ().). ().). (). ().). ().). (). ().).). (). ()"}, {"heading": "C Ubiquity of Reference by Description", "text": "Below are the results of an analysis of 50 articles from 7 different news sources covering 3 different types of articles - analysis / opinion pieces, breaking news and wedding announcements / obituaries. From these articles, we extracted references to persons, places and organizations. In each article entity pair, we examined the first reference in the article to this entity and analyzed it. The only exceptions are very well-known numbers (e.g. Obama, Ronald Reagan). 2. References to many places, especially countries and large cities, are in Tables 1 and 2. We found the following: References to smaller places, such as smaller cities and neighborhoods have descriptions. The only exceptions are very well-known numbers (e.g. Obama, Ronald Reagan). 2. References to many places, especially to countries and large cities, have no associated descriptions. References to smaller places, such as smaller cities and neighborhoods, follow a stylized convention, that of the city, the state and, if necessary, the country. 3. Messages do not contain assigned descriptions."}, {"heading": "D Alternate Problem Formulations", "text": "Descriptions based on random chart models are only one way to look at the problem of reference by description. In this section, we briefly consider three alternative formula article type \u2192 analysis / opinion that triggers obituaries on average \u2193 Piece News Weddings NY Times 2.9 3 3.5 3.18 BBC 3 2.54 3.5 2.78 Atlantic 3.7 0 0 3.7 CNN 2.5 2.63 4 2.88 Telegraph 2.6 2.95 3.2 2.9 LA Times 2.43 2.52 3.55 2.69 Washing. Post 3.14 3.65 4 3.45 Average 3.29 2.92 3.57 3.23 Average description size in each source, broken down by article type. In all of these formulations, we continue to use the model described in Section 3. We modify our analysis of the descriptions and / or the richness of the descriptions."}, {"heading": "D.1 Combinatorial analysis", "text": "Here we look at descriptions from a combinatorial perspective. Variations exist for the following combinatorial decision problem: Does each node have a unique description with less than D nodes on a graph with N nodes, B names, and description size D, where each node is assigned one or zero of these names? In the worst case, B = 0, where checking for a possible solution involves solving a subgraph isomorphism problem. Since the subgraph isomorphism is known to be NP-complete, this problem is NP-complete. Since there are (N-D) sets of D nodes, we may have to solve so many subgraph isomorphism problems."}, {"heading": "D.2 Descriptions and Logical Formulae", "text": "Let's continue to model the range of discourse as a guided graph, but instead of restricting descriptions to subgraphs, let's allow a richer description language (Ax = log (N)) in which some of the nodes in the description similarly have no name and others no ambiguity. This class of descriptions can be represented as a first-order formula with a single free variable (corresponding to the described node) and some constant symbols (nodes with common names). The formula is a unique descriptor for a node if the node is the only logical variable satisfying the formula. The simplest class of descriptions, \"flat descriptions,\" corresponding to the zero snoy form, \"corresponds to the logical formula: Lx1 (X, S1)."}, {"heading": "D.3 Algorithmic descriptions", "text": "We can allow descriptions to be arbitrary programs that take an entity (using a suitable identifier that cannot be used in communication) from the sender and output an entity (using another identifier that is understood by the recipient) to the recipient. This approach allows us to handle graphs with structures / regularities that cannot be modeled using stochastic methods. An interesting question is the size of the smallest program required for a particular domain, sender, and recipient. If knowledge of the common domain is very limited, the program simply needs to store an assignment of sender identifier to receiver identifier identifier. As knowledge of the common domain increases, the program can construct and interpret descriptions."}], "references": [{"title": "On k-anonymity and the curse of dimensionality", "author": ["C.C. Aggarwal"], "venue": "In Proceedings of the 31st international conference on Very Large Data Bases, pages 901\u2013909,", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2005}, {"title": "Dbpedia-a crystallization point for the web of data", "author": ["C. Bizer", "J. Lehmann", "G. Kobilarov", "S. Auer", "C. Becker", "R. Cyganiak", "S. Hellmann"], "venue": "Web Semantics: science, services and agents on the world wide web, 7(3):154\u2013165,", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2009}, {"title": "Freebase: a collaboratively created graph database for structuring human knowledge", "author": ["K. Bollacker", "C. Evans", "P. Paritosh", "T. Sturge", "J. Taylor"], "venue": "In 2008 ACM SIGMOD, pages 1247\u20131250. ACM,", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2008}, {"title": "Hardening soft information sources", "author": ["W.W. Cohen", "H. Kautz", "D. McAllester"], "venue": "In Proceedings of the sixth ACM international conference on Knowledge Discovery and Data mining, pages 255\u2013259,", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2000}, {"title": "Elements of Information Theory", "author": ["T. Cover", "J. Thomas"], "venue": "Wiley-Interscience,", "citeRegEx": "5", "shortCiteRegEx": null, "year": 1991}, {"title": "An introduction to database systems", "author": ["C. Data"], "venue": "Addison-Wesley publ.,", "citeRegEx": "6", "shortCiteRegEx": null, "year": 1975}, {"title": "Secure databases: Protection against user influence", "author": ["D. Dobkin", "A.K. Jones", "R.J. Lipton"], "venue": "ACM Transactions on Database systems (TODS), 4(1):97\u2013106,", "citeRegEx": "7", "shortCiteRegEx": null, "year": 1979}, {"title": "Duplicate record detection: A survey", "author": ["A.K. Elmagarmid", "P.G. Ipeirotis", "V.S. Verykios"], "venue": "IEEE Transactions on Knowledge and Data Engineering, 19:1\u201316,", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2007}, {"title": "On random graphs", "author": ["P. Erd\u0151s", "A. R\u00e9nyi"], "venue": "Publicationes Mathematicae Debrecen, 6:290\u2013 297,", "citeRegEx": "9", "shortCiteRegEx": null, "year": 1959}, {"title": "Sense and reference", "author": ["G. Frege"], "venue": "The philosophical review, pages 209\u2013230,", "citeRegEx": "10", "shortCiteRegEx": null, "year": 1948}, {"title": "Learning probabilistic relational models", "author": ["L. Getoor", "N. Friedman", "D. Koller", "A. Pfeffer"], "venue": "In Relational data mining, pages 307\u2013335. Springer,", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2001}, {"title": "Entity resolution: theory, practice & open challenges", "author": ["L. Getoor", "A. Machanavajjhala"], "venue": "Proceedings of the VLDB Endowment, 5(12):2018\u20132019,", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2012}, {"title": "Communicating and resolving entity references", "author": ["R.V. Guha"], "venue": "CoRR, abs/1406.6973,", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2014}, {"title": "Communicating semantics: Reference by description", "author": ["R.V. Guha", "V. Gupta"], "venue": "CoRR, abs/1511.06341,", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2015}, {"title": "Releasing search queries and clicks privately", "author": ["A. Korolova", "K. Kenthapadi", "N. Mishra", "A. Ntoulas"], "venue": "In Proceedings of the 18th international conference on World wide web, pages 171\u2013180. ACM,", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2009}, {"title": "Naming and necessity", "author": ["S.A. Kripke"], "venue": "Springer,", "citeRegEx": "16", "shortCiteRegEx": null, "year": 1972}, {"title": "John mccarthy, pioneer in artificial intelligence, dies at 84", "author": ["J. Markoff"], "venue": "New York Times,", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2011}, {"title": "Myths and fallacies of personally identifiable information", "author": ["A. Narayanan", "V. Shmatikov"], "venue": "Communications of the ACM, 53(6):24\u201326,", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2010}, {"title": "Scaling and percolation in the small-world network model", "author": ["M.E. Newman", "D.J. Watts"], "venue": "Physical Review E, 60(6):7332,", "citeRegEx": "19", "shortCiteRegEx": null, "year": 1999}, {"title": "Identity uncertainty and citation matching", "author": ["H. Pasula", "B. Marthi", "B. Milch", "S. Russell", "I. Shpitser"], "venue": "In NIPS. MIT Press,", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2003}, {"title": "Mathematical logic", "author": ["W. Quine"], "venue": "Harvard University Press,", "citeRegEx": "21", "shortCiteRegEx": null, "year": 1940}, {"title": "On denoting", "author": ["B. Russell"], "venue": "Mind, pages 479\u2013493,", "citeRegEx": "22", "shortCiteRegEx": null, "year": 1905}, {"title": "The mathematical theory of communication", "author": ["C. Shannon"], "venue": "Bell System Technical Journal, 27:379\u2013423,", "citeRegEx": "23", "shortCiteRegEx": null, "year": 1948}, {"title": "Collective dynamics of small-worldnetworks", "author": ["D.J. Watts", "S.H. Strogatz"], "venue": "nature, 393(6684):440\u2013442,", "citeRegEx": "24", "shortCiteRegEx": null, "year": 1998}, {"title": "The state of record linkage and current research problems", "author": ["W.E. Winkler"], "venue": "In Statistical Research Division, US Census Bureau. Citeseer,", "citeRegEx": "25", "shortCiteRegEx": null, "year": 1999}], "referenceMentions": [{"referenceID": 16, "context": "For example in the New York Times headline [17] \u2018John McCarthy, Pioneer in Artificial Intelligence .", "startOffset": 43, "endOffset": 47}, {"referenceID": 9, "context": "Formal study of descriptions started with Frege [10] and Russell\u2019s [22] descriptivist theory of names, in which names/identity are equivalent to descriptions.", "startOffset": 48, "endOffset": 52}, {"referenceID": 21, "context": "Formal study of descriptions started with Frege [10] and Russell\u2019s [22] descriptivist theory of names, in which names/identity are equivalent to descriptions.", "startOffset": 67, "endOffset": 71}, {"referenceID": 15, "context": "Kripke [16] argued against this position using examples where differences in domain knowledge could yield vastly different descriptions of the same entity.", "startOffset": 7, "endOffset": 11}, {"referenceID": 22, "context": "In [23], the founding paper of information theory, Shannon referred to this problem, saying \u2018Frequently the messages have meaning; that is they refer to or are correlated according to some system with certain physical or conceptual entities\u2019, but he passed over it, saying \u2018These semantic aspects of communication are irrelevant to the engineering problem.", "startOffset": 3, "endOffset": 7}, {"referenceID": 24, "context": "\u2019 Computational treatments of descriptions started with linking duplicates in census records [25].", "startOffset": 93, "endOffset": 97}, {"referenceID": 7, "context": "This work ([8], [4], [12] and [20]) has focused on identifying duplicates introduced by typos, alternate punctuation, different naming conventions, transcription errors, etc.", "startOffset": 11, "endOffset": 14}, {"referenceID": 3, "context": "This work ([8], [4], [12] and [20]) has focused on identifying duplicates introduced by typos, alternate punctuation, different naming conventions, transcription errors, etc.", "startOffset": 16, "endOffset": 19}, {"referenceID": 11, "context": "This work ([8], [4], [12] and [20]) has focused on identifying duplicates introduced by typos, alternate punctuation, different naming conventions, transcription errors, etc.", "startOffset": 21, "endOffset": 25}, {"referenceID": 19, "context": "This work ([8], [4], [12] and [20]) has focused on identifying duplicates introduced by typos, alternate punctuation, different naming conventions, transcription errors, etc.", "startOffset": 30, "endOffset": 34}, {"referenceID": 5, "context": "Keys in relational databases [6] are the best example of this.", "startOffset": 29, "endOffset": 32}, {"referenceID": 6, "context": "The goal of privacy preserving information sharing [7] is the complement of unambiguous communication of references, ensuring that the information shared does not reveal the identity of the entities referred to in the message.", "startOffset": 51, "endOffset": 54}, {"referenceID": 0, "context": "[1], [18] discuss the difficulty of doing this while [15] shows how this can be done for search logs.", "startOffset": 0, "endOffset": 3}, {"referenceID": 17, "context": "[1], [18] discuss the difficulty of doing this while [15] shows how this can be done for search logs.", "startOffset": 5, "endOffset": 9}, {"referenceID": 14, "context": "[1], [18] discuss the difficulty of doing this while [15] shows how this can be done for search logs.", "startOffset": 53, "endOffset": 57}, {"referenceID": 20, "context": "Since arbitrary N-tuples can be constructed out of 3-tuples [21], we restrict ourselves to 3-tuples, i.", "startOffset": 60, "endOffset": 64}, {"referenceID": 12, "context": "In an earlier iterations of this paper [13] we presented this model and the solution for simpler version of this problem.", "startOffset": 39, "endOffset": 43}, {"referenceID": 4, "context": "We use the Asymptotic Equipartition Property [5] to estimate the expected number of candidate referents of a set of names from from their ambiguity.", "startOffset": 45, "endOffset": 48}, {"referenceID": 8, "context": "The most commonly studied Random Graph model is that proposed by Erdos and Renyi [9].", "startOffset": 81, "endOffset": 84}, {"referenceID": 23, "context": "[24], [19] study small world graphs, where there are a large number of localized clusters and yet, most nodes can be reach from every other node in a small number of hops.", "startOffset": 0, "endOffset": 4}, {"referenceID": 18, "context": "[24], [19] study small world graphs, where there are a large number of localized clusters and yet, most nodes can be reach from every other node in a small number of hops.", "startOffset": 6, "endOffset": 10}, {"referenceID": 1, "context": "\u2018Knowledge graphs\u2019, such as DBPedia [2] and Freebase [3], that represent relations between people, places, events, etc.", "startOffset": 36, "endOffset": 39}, {"referenceID": 2, "context": "\u2018Knowledge graphs\u2019, such as DBPedia [2] and Freebase [3], that represent relations between people, places, events, etc.", "startOffset": 53, "endOffset": 56}, {"referenceID": 10, "context": "Learning probabilistic models [11] of such dependencies is an active area of research.", "startOffset": 30, "endOffset": 34}, {"referenceID": 6, "context": "We are interested in comparing the number of statements that can be made about an entity, while still keeping it indistinguishable fromK other entities [7], with the number of statements required to uniquely identify it.", "startOffset": 152, "endOffset": 155}, {"referenceID": 22, "context": "As Shannon [23] alluded to, communication is not just correctly transmitting a symbol sequence, but also understanding what these symbols denote.", "startOffset": 11, "endOffset": 15}, {"referenceID": 22, "context": "\u2013 Though our communication model makes no assumptions about the graph, the simple form of the results presented here arise out of assumptions about ergodicity and uniformity of salience rate (which are analogous to those made in [23]).", "startOffset": 229, "endOffset": 233}], "year": 2016, "abstractText": "Messages often refer to entities such as people, places and events. Correct identification of the intended reference is an essential part of communication. Lack of shared unique names often complicates entity reference. Shared knowledge can be used to construct uniquely identifying descriptive references for entities with ambiguous names. We introduce a mathematical model for \u2018Reference by Description\u2019, derive results on the conditions under which, with high probability, programs can construct unambiguous references to most entities in the domain of discourse and provide empirical validation of these results.", "creator": "LaTeX with hyperref package"}}}