{"id": "1511.06349", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "19-Nov-2015", "title": "Generating Sentences from a Continuous Space", "abstract": "The standard unsupervised recurrent neural network language model (RNNLM) generates sentences one word at a time and does not work from an explicit global distributed sentence representation. In this work, we present an RNN-based variational autoencoder language model that incorporates distributed latent representations of entire sentences. This factorization allows it to explicitly model holistic properties of sentences such as style, topic, and high-level syntactic features. Samples from the prior over these sentence representations remarkably produce diverse and well-formed sentences through simple deterministic decoding. By examining paths through this latent space, we are able to generate coherent novel sentences that interpolate between known sentences. We present techniques for solving the difficult learning problem presented by this model, demonstrate strong performance in the imputation of missing tokens, and explore many interesting properties of the latent sentence space.", "histories": [["v1", "Thu, 19 Nov 2015 20:38:45 GMT  (1891kb,D)", "http://arxiv.org/abs/1511.06349v1", "First two authors contributed equally. Work was done when all authors were at Google, Inc. Under review for ICLR 2016"], ["v2", "Fri, 20 Nov 2015 02:59:34 GMT  (188kb,D)", "http://arxiv.org/abs/1511.06349v2", "First two authors contributed equally. Work was done when all authors were at Google, Inc. Under review for ICLR 2016"], ["v3", "Mon, 25 Jan 2016 17:38:42 GMT  (156kb,D)", "http://arxiv.org/abs/1511.06349v3", "First two authors contributed equally. Work was done when all authors were at Google, Inc. Under review for ICLR 2016"], ["v4", "Thu, 12 May 2016 20:51:23 GMT  (596kb,D)", "http://arxiv.org/abs/1511.06349v4", "First two authors contributed equally. Work was done when all authors were at Google, Inc"]], "COMMENTS": "First two authors contributed equally. Work was done when all authors were at Google, Inc. Under review for ICLR 2016", "reviews": [], "SUBJECTS": "cs.LG cs.CL", "authors": ["samuel r bowman", "luke vilnis", "oriol vinyals", "rew m dai", "rafal jozefowicz", "samy bengio"], "accepted": false, "id": "1511.06349"}, "pdf": {"name": "1511.06349.pdf", "metadata": {"source": "CRF", "title": null, "authors": ["Samuel R. Bowman", "Luke Vilnis"], "emails": ["sbowman@stanford.edu", "luke@cs.umass.edu", "bengio}@google.com"], "sections": [{"heading": "1 INTRODUCTION", "text": "In fact, it is that we are able to assert ourselves, that we are able, that we are able to hide ourselves, and that we are able, that we are able, that we are able, that we are able, that we are able, that we are able."}, {"heading": "2 PRIOR WORK", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "2.1 UNSUPERVISED MODELS FOR WHOLE-SENTENCE ENCODING", "text": "A standard RNG model predicts each word of a sentence based on the previous word and a developing hidden state. It is effective but does not learn vector representation of the entire sentence. To enable continuous global latent sentence representation, we first need a method for mapping sentences and distributed representations that can be tested in an unmonitored environment. While no strong generative model is available for this problem, three non-generative techniques have shown promise: sequence autoencoders, skipped thoughts and paragraphs. Sequence autoencoders have some success in the upstream sequence sequence models for higher-level tasks (Dai & Le) and in generating complete documents (Li et al)."}, {"heading": "2.2 THE VARIATIONAL AUTOENCODER", "text": "The variational autocoder (UAE, ~ ~ ~ Kingma & Welling, 2015; Rezende et al., 2014) is a generative model based on a regulated version of the standard autocoder, which imposes a prior distribution on the hidden codes, forcing regular geometry across codes and allowing suitable samples to be extracted from the model by replacing the deterministic function enc with a learned posterior recognition model q (~ z | x). This model parameterizes an approximate posterior distribution across ~ z (usually a diagonal Gaussian) with a neural network conditioned on x. Intuitively, the VAE learns coding not as individual points, but as soft ellipsoidal regions in latent space, forcing the codes model to fill the space instead of storing the training data as an isolated encoder."}, {"heading": "3 THE VARIATIONAL AUTOENCODER LANGUAGE MODEL", "text": "We adapt the variable autoencoder to the text by using single-layer LSTM RNNNs (Hochreiter & Schmidhuber, 1997), both for the encoder and for the decoder, and essentially form a sequence autoencoder that acts as a regulator of the hidden code. The decoder serves as a special RNN language model conditioned on this hidden code, and in the degenerated setting in which the hidden code contains no useful information, this model is effectively equivalent to an RNLM. The model is shown in Figure 1 and will be used in all experiments discussed below."}, {"heading": "3.1 OPTIMIZATION CHALLENGES", "text": "We cannot quantify the degree in which our model exhibits the global characteristics by limiting ourselves to the essential (1). The concentrated attention that we pay to people is primarily directed to the essential: to the essential, namely, to the essential, to the essential, to the essential, to the essential, to the essential, to the essential, to the essential, to the essential, to the essential, to the essential, to the essential, to the essential, to the essential, to the essential, to the essential, to the essential, to the essential, to the essential, to the essential, to the essential, to the essential, the essential, the essential, the essential, the essential, the essential, the essential, the essential, the essential, the essential, the essential, the essential, the essential, the essential, the essential, the essential, the essential, the essential, the essential, the essential, the essential, the essential, the essential, the essential, the essential, the essential, the essential, the essential, the essential, the essential, the essential, the essential, the essential, the essential, the essential, the essential, the essential, the essential, the essential, the essential, the essential, the essential, the, the essential, the essential, the, the essential, the essential, the essential, the essential, the essential, the essential, the, the essential, the essential, the, the essential, the essential, the essential, the essential, the essential, the essential, the essential, the, the essential, the essential, the, the essential, the essential, the essential, the, the essential, the essential, the, the essential, the essential, the essential, the essential, the essential, the essential, the essential, the essential, the, the essential, the essential, the essential, the essential, the essential, the essential, the essential, the essential, the essential, the essential, the essential, the, the, the essential, the essential, the essential, the essential, the, the essential, the essential, the essential, the essential, the essential, the essential, the essential, the essential, the essential, the essential, the essential, the essential, the essential, the essential, the essential, the essential, the essential, the essential, the essential, the, the essential, the essential, the essential, the essential, the essential, the essential, the essential, the essential, the essential, the essential"}, {"heading": "4 RESULTS: LANGUAGE MODELING", "text": "In this section, we report on Penn Treebank's linguistic modeling in an effort to find out whether the inclusion of a global variable is helpful for this standard task. That's why we limit our UAE hyperparameter search to models that perform non-trivial performance in the latent variable that best reaches the development group."}, {"heading": "5 RESULTS: IMPUTING MISSING WORDS", "text": "In fact, it is the case that most of them are in a position to enter another world, in which they move, in which they move, in which they move, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they, in which they live, in which they, in which they live, in which they, in which they live, in which they live, in which they, in which they, in which they live, in which they live."}, {"heading": "6 ANALYZING VARIATIONAL MODELS", "text": "We now turn to a more qualitative analysis of the model. Since our decoder model p (x | ~ z) is a sophisticated RNNLM, a simple sample from the directed graphical model (first p (~ z) then p (x | ~ z)) would not tell us much about how much of the data is explained by the learned vector compared to the speech model. Instead, for this part of the evaluation, we take samples from the Gaussian predecessor model, but use a greedy deterministic decoder for p (x | ~ z), the RNNLM conditioned to ~ z. This allows us to get a sense of how much of the variance in the data distribution is captured by the distributed vector ~ z as opposed to the language model. Interestingly, these results qualitatively show that large amounts of variation in the generated language can be achieved by following this method."}, {"heading": "6.1 ANALYZING THE IMPACT OF WORD DROPOUT", "text": "For this experiment, we are able to go in search of a solution that enables us, puts us in a position, puts us in a position, puts us in a position, puts us in a position, puts us in a position, puts us in a position, puts us in a position, puts us in a position, puts us in a position, puts us in a position, puts us in a position, puts us in a position, puts us in the position we are in."}, {"heading": "6.2 SAMPLING FROM THE POSTERIOR", "text": "In addition to generating unconditional samples, we can also examine the sentences decoded from the posterior vectors p (z | x) for different sentences x. Because the model is regulated to generate distributions and non-deterministic encodings, it does not accurately remember the input and reverse it. Instead, we can see from the posterior samples what the model considers to be similar sentences. In Table 7, it should be noted that the model stores information about the number of characters and language parts for each character, as well as obvious topic information."}, {"heading": "6.3 HOMOTOPIES", "text": "The use of a variational autoencoder allows us to generate sentences with greedy decoding using continuous examples from the space of the codes. Furthermore, the volume-filling and smooth nature of the code space allows us to investigate for the first time a concept of homotopy (linear interpolation) between sentences. In this context, a homotopy between two codes ~ z1 and ~ z2 is the sentence on the line between them, including, ~ z (t) = ~ z1 (1 \u2212 t) + ~ z2 \"t for t.\" Similarly, homotopy between two sentences is decoded (greedy) from the codes ~ z1 and ~ z2 the sentence decoded from the codes on the line. Examining these homotopias allows us to get a sense of what neighborhoods in the code space look like - how the auto encoder organizes information and what it sees as a continuous decoding between two encodes."}, {"heading": "7 CONCLUSION", "text": "We present novel techniques that allow us to successfully train our model, and find that it can significantly exceed an RNNLM baseline in the addition of missing words. We analyze the latent space that our model has learned, and find that it is able to generate coherent and diverse sentences through purely continuous sampling, providing interpretable homotopes that interpolate smoothly between sentences. We hope to examine the factoring of latent variables into separate styles and content components in future work, generate sentences based on extrinsic characteristics, learn sentence embeddings in a semi-supervised manner for classification tasks such as textual deprivation, and move beyond hostile evaluation to a completely contrary training goal."}, {"heading": "ACKNOWLEDGMENTS", "text": "We thank the Google Brain Team, Alireza Makhzani, Laurent Dinh, Jon Gauthier and Stanford NLP Group for their helpful contributions and feedback."}, {"heading": "APPENDIX: HYPERPARAMETER TUNING", "text": "We fully align the hyperparameters of each model with development data using an automatic Bayesian hyperparameter tuning algorithm (based on Snoek et al., 2012). We run the model with each set of hyperparameters for 10 hours, performing 12 experiments in parallel, and select the best set of hyperparameters after 200 runs. Results of our language modeling experiments are shown in Table 9."}, {"heading": "APPENDIX: ADDITIONAL HOMOTOPIES", "text": "Table 10 shows additional homotopias of the kind discussed in Section 6.3. We note that intermediate sentences are almost always grammatical in nature and often contain uniform thematic, vocabulary and syntactic information in local neighborhoods as they interpolate between the endpoint sentences. As the model is trained on fiction, including romance novels, the themes are often quite dramatic."}], "references": [], "referenceMentions": [], "year": 2017, "abstractText": "The standard unsupervised recurrent neural network language model (RNNLM)<lb>generates sentences one word at a time and does not work from an explicit global<lb>distributed sentence representation. In this work, we present an RNN-based vari-<lb>ational autoencoder language model that incorporates distributed latent represen-<lb>tations of entire sentences. This factorization allows it to explicitly model holis-<lb>tic properties of sentences such as style, topic, and high-level syntactic features.<lb>Samples from the prior over these sentence representations remarkably produce<lb>diverse and well-formed sentences through simple deterministic decoding. By ex-<lb>amining paths through this latent space, we are able to generate coherent novel<lb>sentences that interpolate between known sentences. We present techniques for<lb>solving the difficult learning problem presented by this model, demonstrate strong<lb>performance in the imputation of missing tokens, and explore many interesting<lb>properties of the latent sentence space.", "creator": "LaTeX with hyperref package"}}}