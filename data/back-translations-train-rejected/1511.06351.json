{"id": "1511.06351", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "19-Nov-2015", "title": "Learning Representations Using Complex-Valued Nets", "abstract": "Complex-valued neural networks (CVNNs) are an emerging field of research in neural networks due to their potential representational properties for audio, image, and physiological signals. It is common in signal processing to transform sequences of real values to the complex domain via a set of complex basis functions, such as the Fourier transform. We show how CVNNs can be used to learn complex representations of real valued time-series data. We present methods and results using a framework that can compose holomorphic and non-holomorphic functions in a multi-layer network using a theoretical result called the Wirtinger derivative. We test our methods on a representation learning task for real-valued signals, recurrent complex-valued networks and their real-valued counterparts. Our results show that recurrent complex-valued networks can perform as well as their real-valued counterparts while learning filters that are representative of the domain of the data.", "histories": [["v1", "Thu, 19 Nov 2015 20:44:10 GMT  (1716kb,D)", "http://arxiv.org/abs/1511.06351v1", null]], "reviews": [], "SUBJECTS": "cs.LG cs.NE", "authors": ["andy m sarroff", "victor shepardson", "michael a casey"], "accepted": false, "id": "1511.06351"}, "pdf": {"name": "1511.06351.pdf", "metadata": {"source": "CRF", "title": "LEARNING REPRESENTATIONS USING COMPLEX-VALUED NETS", "authors": ["Andy M. Sarroff", "Victor Shepardson", "Michael A. Casey"], "emails": ["sarroff@cs.dartmouth.edu"], "sections": [{"heading": "1 INTRODUCTION", "text": "This year, it is so far that it is only a matter of time before it is ready, until it is ready."}, {"heading": "2 BACKGROUND", "text": "Complex numbers expand the concept of one-dimensional real numbers to two dimensions by expressing an ordered pair (x, y) at the complex level, where z = x + iy and i = \u221a \u2212 1. numbers in the complex domain are a natural means of expressing orders of magnitude, phases or directions together. Suppose that we want a function f: Cm \u2192 Cn by optimizing the square errorL (z) = 1, where (\u00b7) the complex conjugated operator (x + iy) learns, (x + y2) and hence the objective function \"errorL (z) = 2,\" (z), (z), (c), c (c), c (c), c (c), c (c), (c) (c), (c) (c), (c), (c), (c) (c), (c), (c) (c), (c), (c), (c (c), c (c), c (c), c (c (c), c (c), c (), (x), (c), (x, y), (x), (x), (x), (x), (x) (x), (x) (x), (x) (x), (x) (x), (x), (x) (x), (x) (x), (x), (x) (x), (x) (x), (x) in the complex plane, x (x (x), (x), (x), (x), (x), x (x (x) in the complex plane, x (x (x), x (x), x), x (x (x), x (x), x (x (x), x (x), x (x), x (x (x), x (x), x (1), x (1), x (1), x (c (c), x (c (c), x (1), x (c), x (c (c), x), x (c (c (c), x (c),"}, {"heading": "2.1 COMPLEX-VALUED AND REAL-VALUED NETS", "text": "A multiplication of values in the real range leads to scaling. A multiplication of complex values leads to scaling and rotation. So, if we want to model size and phase together, it may be more natural to do so by using a complex representation. There are cases where we want to model real processes in the complex range. For example, one cannot determine the current frequency or amplitude of a real periodical waveform from a single sample. Applying the Hilbert transformation leads to a complex weighted waveform with the same positive frequency components. However, we suggest that complex weighted networks can also learn important relationships to immediate frequency and amplitude."}, {"heading": "2.2 ACTIVATION FUNCTIONS", "text": "Activation functions that are limited and differentiable are generally desirable for training neural networks. (The linear equilibrium unit is a notable exception for boundlessness.) Due to Liouville's theorem, the only complete (holomorphic) function that is limited across the entire complex range is a constant. Therefore, we must opt for complex networks between limitedness and differentiability. Split-complex activation functions act independently on real and imaginary or phase and size components and merge the results. Such functions are not holomorphic. However, it is easy to define a limited split-complex activation function, e.g. Geourgiou and Koutsougeras's size squeezing activation function (Georgiou & Koutsougeras, 1992)."}, {"heading": "3 WIRTINGER FRAMEWORK FOR GRADIENT DESCENT", "text": "This section outlines the routines for optimizing an arbitrarily complex evaluated neural network by means of a Wirtinger derivative and gradient descent. The network has a real evaluated objective function of complex variables. It can have any combination of holomorphic and non-holomorphic activation functions. Wirtinger calculations (also known as CR calculations in some texts) facilitate the definition of an arithmetic diagram that can be modulated as in many popular deep learning libraries, enabling the construction of deep or time networks with many layers. In the following subsection, the core concepts of Wirtinger calculation are reviewed."}, {"heading": "3.1 WIRTINGER DERIVATIVES", "text": "Define z-Z and x, y-R with f (z) = g (x, y) = u (x, y) + iv (x, y). We expand the definition of f to include the complex conjugation of its input variables, so that the R derivative and R derivative of f (z, z) = g (x, y) = u (x, y) + iv (x, y) z = x \u2212 iy (2) Using this definition, the R derivative and R derivative of f (z, z) = g (x, y) = u (x, y) + iv (x, y) z = x \u2212 iyy (2) defines the R derivative and R derivative of f (z, z) as: f derivative of f."}, {"heading": "3.2 THE COMPUTATIONAL GRAPH", "text": "We want to perform a gradient descent based on an arithmetic diagram that has a real cost function and an arbitrary composition of holomorphic and non-holomorphic functions. Performing a reverse propagation of such a diagram can be cumbersome if we decide to repeatedly switch between complex and real representations of the diagram. If we stay in the complex domain for all calculations and use Wirtinger calculations, it is easier to build a modular framework that is useful for deep networks.Consider a complex rated function, F (z, z) = [f1 (z), f2 (z, z)., fM (z, z)] T with (z1, z2,., zN] T and (9) z = [z1, z2, zN] T."}, {"heading": "4 EXPERIMENTS", "text": "The discrete Fourier transformation of N regularly sampled points on a waveform provides complex coefficients of orthogonal complex sinusoids. However, the Fourier representation may not be the best transformation for a given task. Deep networks are regularly trained to learn data representations that are more appropriate than hand-picked characteristics. In this experiment, we generate real and complex evaluated waveforms with broadband spectral components in multiple phases and orders of magnitude. We train recursive complex and real models to predict the N-th frame of a waveform taking into account the first N-1 frames. In the following subsections, we detail the data, models and results."}, {"heading": "4.1 DATA", "text": "We created four synthetic datasets with broad band frequency spectra with random phases: Sawtooth-Like, Sawtooth-Like (Analytic), Inharmonic and Inharmonic (Analytic). Each dataset had unique training, validation and test partitions. The training sets consisted of 10,000 observations, divided into 10 lots. The validation and test sets each contained 1 batch of 1,000 observations. The first three batches were generated as described below. Each observation (waveform) has 1024 samples with a Nyquist frequency called B. The waveform was divided into four non-overlapping rectangular glazed frames with 256 samples."}, {"heading": "4.1.1 SAWTOOTH-LIKE", "text": "Each waveform has a fundamental frequency that is uniformly pulled out of the range [0]. There are harmonics n = (2,.., N) above the fundamental frequency, with all harmonic frequencies being smaller than \u0430 and each harmonic having an amplitude of 1 / n. All sinusoidal components have a random phase drawn from an even distribution [0, 1). Each real waveform becomes complex by adding a zero-value imaginary component. We call these waveforms \"saw-tooth-like\" because they have the same spectral components of a band-delimited saw-tooth waveform. However, since the phases of the spectral components are confused, the waveforms do not look like saw-tooth waveforms with the amplitude of time. Each image of an observation has a number of waveforms in the range [0, 128]."}, {"heading": "4.1.2 SAWTOOTH-LIKE (ANALYTIC)", "text": "For each frequency component with frequency \u03c9 and phase \u03c6, a sinusoidal component is added to the imaginary axes with frequency \u03c9 and phase \u03c6. An analytical signal encodes instantaneous size and phase. In cases where a real network was trained with this data set, the real and imaginary parts of the data were split, and therefore there were twice as many inputs and outputs as other experiments."}, {"heading": "4.2 INHARMONIC", "text": "Inharmonious waveforms were produced with five spectral components, each of which has a frequency derived from an even distribution in the range [0, \u0430), a phase derived from an even distribution in the range [0, 1), and an amplitude of 1 / 5. Therefore, the phases of the individual components are random, but not from the entire available range of [0, 2\u03c0). These waveforms are unlikely to exhibit periodicity."}, {"heading": "4.3 INHARMONIC (ANALYTIC)", "text": "Analytical waveforms were created using the same methodology as for the Sawtooth-Like (Analytic) dataset as above."}, {"heading": "4.4 MODELS", "text": "We trained neural networks of real and complex value with a hidden recursive level of size 256. The input and output levels each had 256 units, except for real networks trained on the analytical data sets, which had 512 inputs and outputs. All models had weights and distortions. Therefore, the models had either 197,376 or 328,704 traceable parameters. The models were trained with a tanh activation function at the hidden level and a linear activation at the output level."}, {"heading": "4.5 TRAINING", "text": "Together with other authors, we found that complex evaluated networks are extremely sensitive to initial conditions and learning rates (Zimmermann et al., 2011). To facilitate a good setting of hyperparameters, we performed hyperparameter optimizations using Spearmint (Snoek et al., 2012) for the following parameters: initial weight scaling, learning rate, and learning rate half-life. For each dataset, 100 real and complex evaluated models with unique hyperparameter settings and baseline weights were trained, and the final model was selected using the best performance on the validation set."}, {"heading": "4.6 RESULTS", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.6.1 OVERALL COMPARISON", "text": "In most cases, the final error between complex and real networks was comparable. However, in all experiments, the real networks had a lower final test error. Table 1 shows that both real and complex networks performed best on the Sawtooth-like dataset, and we were not surprised by this result. As this dataset consists only of harmonically related spectral components, we assume that this dataset is easier to learn than the Inharmonic datasets. We were surprised that both real and complex rated networks had difficulty learning the analytical datasets, which encode immediate frequency and phase, and we therefore expected that they would work well with the complex rated network. It is possible that the fully complex tanh activation function is unsuitable for this dataset because the immediate frequency does not cause any change between input and initial data."}, {"heading": "4.6.2 OPTIMIZATION", "text": "The left pane of Figure 3 shows the sorted error about hyperparameter settings used with the Sawtooth-Like dataset. We find it noteworthy that most of the settings are relatively bad. There were few settings for both types of networks that achieved optimal performance. This figure highlights how sensitive both types of networks are to hyperparameter settings. The right pane shows the cross-era validation error for the Sawtooth-Like dataset. Note the discrepancy in the error curve for the complex evaluated mesh. The complex networks are quite difficult to train and can easily approach regions of instability. We believe that this is due to the singularities of the Tanh function."}, {"heading": "4.6.3 FILTERS", "text": "We examined the input-to-hidden weights of the models and found that, despite the poorer performance of complex-rated networks, they learned filters that can easily be applied to the datasets. Figure 4 shows the size response of the first three input-to-hidden weights for the Sawtooth Like datasets (left) and Inharmonic Like datasets (right). Note that the frequency response of the complex model for the Sawtooth Like dataset has harmonically arranged peaks in the spectrum, while the filters of the real-rated network are much louder and it is difficult to detect harmonic distances. Also, the filters of the complex model trained on the Inharmonic dataset exhibit high selectivity for some spectral peaks, while the filters learned by the real-rated model show selectivity, but to a lesser extent."}, {"heading": "5 CONCLUSIONS", "text": "We argue that the mathematical conveniences of the Wirtinger calculation provide a means of building a modular library for the formation of complex-rated networks. To this end, we compiled several synthetic datasets and compared the performance of complex-rated networks. We found that complex-rated networks work about as well as, but not better than, real networks. We highlighted the fact that the formation of complex-rated networks presents different challenges, including difficulties in releasing and singularities in activation functions. Finally, we showed that despite poorer performance, complex-rated networks learn filter representations that are adapted to the domain of the data. It is obvious that there are many challenges to successfully train complex-rated networks. We need to find good methods to avoid the singularities in holomorphic cost functions. There is no complex equivalent to the reflected linear units. The models are extremely sensitive to the starting conditions of the 1st weight."}], "references": [{"title": "Learning algorithms in complex-valued neural networks using wirtinger calculus. In Complex-Valued Neural Networks: Advances and Applications, pp. 75\u2013102", "author": ["Amin", "Md. Faijul", "Murase", "Kazuyuki"], "venue": null, "citeRegEx": "Amin et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Amin et al\\.", "year": 2013}, {"title": "Wirtinger calculus based gradient descent and levenberg-marquardt learning algorithms in complex-valued neural networks", "author": ["Amin", "Md.Faijul", "MuhammadIlias", "A.Y.H. Al-Nuaimi", "Murase", "Kazuyuki"], "venue": "Neural Information Processing,", "citeRegEx": "Amin et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Amin et al\\.", "year": 2011}, {"title": "A complex gradient operator and its application in adaptive array theory", "author": ["D.H. Brandwood"], "venue": "Communications, Radar and Signal Processing, IEE Proceedings F,", "citeRegEx": "Brandwood,? \\Q1983\\E", "shortCiteRegEx": "Brandwood", "year": 1983}, {"title": "Complex domain backpropagation", "author": ["G.M. Georgiou", "C. Koutsougeras"], "venue": "Circuits and Systems II: Analog and Digital Signal Processing, IEEE Transactions on,", "citeRegEx": "Georgiou and Koutsougeras,? \\Q1992\\E", "shortCiteRegEx": "Georgiou and Koutsougeras", "year": 1992}, {"title": "Complex-valued forecasting of wind profile", "author": ["S.L. Goh", "M. Chen", "D.H. Popovi\u0107", "K. Aihara", "D. Obradovic", "D.P. Mandic"], "venue": "Renewable Energy, 31(11):1733\u20131750,", "citeRegEx": "Goh et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Goh et al\\.", "year": 2006}, {"title": "Complex-Valued Neural Networks", "author": ["A. Hirose"], "venue": "Studies in computational intelligence. SpringerVerlag,", "citeRegEx": "Hirose,? \\Q2006\\E", "shortCiteRegEx": "Hirose", "year": 2006}, {"title": "Complex-Valued Neural Networks: Advances and Applications", "author": ["A. Hirose"], "venue": null, "citeRegEx": "Hirose,? \\Q2013\\E", "shortCiteRegEx": "Hirose", "year": 2013}, {"title": "Generalization characteristics of complex-valued feedforward neural networks in relation to signal coherence", "author": ["A. Hirose", "S. Yoshida"], "venue": "Neural Networks and Learning Systems, IEEE Transactions on,", "citeRegEx": "Hirose and Yoshida,? \\Q2012\\E", "shortCiteRegEx": "Hirose and Yoshida", "year": 2012}, {"title": "Approximation by fully complex multilayer perceptrons", "author": ["T Kim", "T. Adal\u0131"], "venue": "Neural Computation,", "citeRegEx": "Kim and Adal\u0131,? \\Q2003\\E", "shortCiteRegEx": "Kim and Adal\u0131", "year": 2003}, {"title": "The Complex Gradient Operator and the CR-Calculus", "author": ["K. Kreutz-Delgado"], "venue": "ArXiv e-prints,", "citeRegEx": "Kreutz.Delgado,? \\Q2009\\E", "shortCiteRegEx": "Kreutz.Delgado", "year": 2009}, {"title": "Complex-valued adaptive signal processing using nonlinear functions", "author": ["Li", "Hualiang", "Adali", "T\u00fclay"], "venue": "EURASIP J. Adv. Signal Process,", "citeRegEx": "Li et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Li et al\\.", "year": 2008}, {"title": "Complex valued nonlinear adaptive filters: noncircularity, widely linear and neural models", "author": ["Mandic", "Danilo P", "Goh", "Vanessa Su Lee"], "venue": null, "citeRegEx": "Mandic et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Mandic et al\\.", "year": 2009}, {"title": "An empirical study of learning rates in deep neural networks for speech recognition", "author": ["A. Senior", "G. Heigold", "M. Ranzato", "Yang", "Ke"], "venue": "In Acoustics, Speech and Signal Processing (ICASSP),", "citeRegEx": "Senior et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Senior et al\\.", "year": 2013}, {"title": "Practical bayesian optimization of machine learning algorithms", "author": ["Snoek", "Jasper", "Larochelle", "Hugo", "Adams", "Ryan P"], "venue": "In Advances in neural information processing systems,", "citeRegEx": "Snoek et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Snoek et al\\.", "year": 2012}, {"title": "Unconstrained optimization of real functions in complex variables", "author": ["Sorber", "Laurent", "Barel", "Marc Van", "Lathauwer", "Lieven De"], "venue": "SIAM Journal on Optimization,", "citeRegEx": "Sorber et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Sorber et al\\.", "year": 2012}, {"title": "Ultrawideband direction-of-arrival estimation using complex-valued spatiotemporal neural networks", "author": ["K. Terabayashi", "R. Natsuaki", "A. Hirose"], "venue": "Neural Networks and Learning Systems, IEEE Transactions on,", "citeRegEx": "Terabayashi et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Terabayashi et al\\.", "year": 2014}, {"title": "Complex gradient and hessian", "author": ["A. van den Bos"], "venue": "IEE Proceedings - Vision, Image and Signal Processing,", "citeRegEx": "Bos,? \\Q1994\\E", "shortCiteRegEx": "Bos", "year": 1994}, {"title": "Zur formalen Theorie der Funktionen von mehr komplexen Ver anderlichen", "author": ["W. Wirtinger"], "venue": "Mathematische Annalen,", "citeRegEx": "Wirtinger,? \\Q1927\\E", "shortCiteRegEx": "Wirtinger", "year": 1927}, {"title": "Comparison of the complex valued and real valued neural networks trained with gradient descent and random search algorithms", "author": ["Zimmermann", "Hans-Georg", "Minin", "Alexey", "Kusherbaeva", "Victoria"], "venue": "In European Symposium on Artificial Neural Networks, Computational Intelligence and Machine Learning (ESANN),", "citeRegEx": "Zimmermann et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Zimmermann et al\\.", "year": 2011}], "referenceMentions": [{"referenceID": 4, "context": "For example wind measurements may use complex-valued data to represent joint measurements of magnitude and direction (Goh et al., 2006).", "startOffset": 117, "endOffset": 135}, {"referenceID": 15, "context": "Direction of arrival is naturally modeled in ultrawideband communications using complex values (Terabayashi et al., 2014).", "startOffset": 95, "endOffset": 121}, {"referenceID": 18, "context": "Despite such obstacles, research on CVNNs is growing steadily, with new theoretical results (Zimmermann et al., 2011; Sorber et al., 2012; Hirose & Yoshida, 2012) appearing on the heels of comprehensive treatments in recent texts (Hirose, 2006; Mandic & Goh, 2009; Hirose, 2013).", "startOffset": 92, "endOffset": 162}, {"referenceID": 14, "context": "Despite such obstacles, research on CVNNs is growing steadily, with new theoretical results (Zimmermann et al., 2011; Sorber et al., 2012; Hirose & Yoshida, 2012) appearing on the heels of comprehensive treatments in recent texts (Hirose, 2006; Mandic & Goh, 2009; Hirose, 2013).", "startOffset": 92, "endOffset": 162}, {"referenceID": 5, "context": ", 2012; Hirose & Yoshida, 2012) appearing on the heels of comprehensive treatments in recent texts (Hirose, 2006; Mandic & Goh, 2009; Hirose, 2013).", "startOffset": 99, "endOffset": 147}, {"referenceID": 6, "context": ", 2012; Hirose & Yoshida, 2012) appearing on the heels of comprehensive treatments in recent texts (Hirose, 2006; Mandic & Goh, 2009; Hirose, 2013).", "startOffset": 99, "endOffset": 147}, {"referenceID": 1, "context": "In this paper, we follow a more general framework (Amin et al., 2011; Amin & Murase, 2013) for building CVNNs, both deep and temporal, that allows for activation functions that are composed from combinations of both holomorphic and non-holomorphic functions.", "startOffset": 50, "endOffset": 90}, {"referenceID": 17, "context": "In this paper we apply the Wirtinger derivative (Wirtinger, 1927) to compute the gradient (Brandwood, 1983).", "startOffset": 48, "endOffset": 65}, {"referenceID": 2, "context": "In this paper we apply the Wirtinger derivative (Wirtinger, 1927) to compute the gradient (Brandwood, 1983).", "startOffset": 90, "endOffset": 107}, {"referenceID": 0, "context": "In this paper we apply the Wirtinger derivative (Wirtinger, 1927) to compute the gradient (Brandwood, 1983). Doing so allows us to perform differentiation on functions that are not complexanalytic but are real-analytic. It also provides a means for easily composing a combination of holomorphic and non-holomorphic functions within the computational graph of a neural network. Finally, by taking advantage of basic properties of the Wirtinger derivative, we perform gradient descent using two Jacobians per computational node. Due to space limitations the following summary is necessarily brief. A great overview of the core mechanics of complex-valued nets and the Wirtinger derivative is found in Mandic & Goh (2009). This and other literature are built on the theory developed in Brandwood (1983) and van den Bos (1994) for optimization of complex-valued nets using respectively first- and second-order derivatives with Wirtinger calculus.", "startOffset": 91, "endOffset": 719}, {"referenceID": 0, "context": "In this paper we apply the Wirtinger derivative (Wirtinger, 1927) to compute the gradient (Brandwood, 1983). Doing so allows us to perform differentiation on functions that are not complexanalytic but are real-analytic. It also provides a means for easily composing a combination of holomorphic and non-holomorphic functions within the computational graph of a neural network. Finally, by taking advantage of basic properties of the Wirtinger derivative, we perform gradient descent using two Jacobians per computational node. Due to space limitations the following summary is necessarily brief. A great overview of the core mechanics of complex-valued nets and the Wirtinger derivative is found in Mandic & Goh (2009). This and other literature are built on the theory developed in Brandwood (1983) and van den Bos (1994) for optimization of complex-valued nets using respectively first- and second-order derivatives with Wirtinger calculus.", "startOffset": 91, "endOffset": 800}, {"referenceID": 0, "context": "In this paper we apply the Wirtinger derivative (Wirtinger, 1927) to compute the gradient (Brandwood, 1983). Doing so allows us to perform differentiation on functions that are not complexanalytic but are real-analytic. It also provides a means for easily composing a combination of holomorphic and non-holomorphic functions within the computational graph of a neural network. Finally, by taking advantage of basic properties of the Wirtinger derivative, we perform gradient descent using two Jacobians per computational node. Due to space limitations the following summary is necessarily brief. A great overview of the core mechanics of complex-valued nets and the Wirtinger derivative is found in Mandic & Goh (2009). This and other literature are built on the theory developed in Brandwood (1983) and van den Bos (1994) for optimization of complex-valued nets using respectively first- and second-order derivatives with Wirtinger calculus.", "startOffset": 91, "endOffset": 823}, {"referenceID": 0, "context": "In this paper we apply the Wirtinger derivative (Wirtinger, 1927) to compute the gradient (Brandwood, 1983). Doing so allows us to perform differentiation on functions that are not complexanalytic but are real-analytic. It also provides a means for easily composing a combination of holomorphic and non-holomorphic functions within the computational graph of a neural network. Finally, by taking advantage of basic properties of the Wirtinger derivative, we perform gradient descent using two Jacobians per computational node. Due to space limitations the following summary is necessarily brief. A great overview of the core mechanics of complex-valued nets and the Wirtinger derivative is found in Mandic & Goh (2009). This and other literature are built on the theory developed in Brandwood (1983) and van den Bos (1994) for optimization of complex-valued nets using respectively first- and second-order derivatives with Wirtinger calculus. For a deeper discussion of Wirtinger calculus and optimization techniques we refer the reader to Kreutz-Delgado (2009); Li & Adali (2008).", "startOffset": 91, "endOffset": 1062}, {"referenceID": 0, "context": "In this paper we apply the Wirtinger derivative (Wirtinger, 1927) to compute the gradient (Brandwood, 1983). Doing so allows us to perform differentiation on functions that are not complexanalytic but are real-analytic. It also provides a means for easily composing a combination of holomorphic and non-holomorphic functions within the computational graph of a neural network. Finally, by taking advantage of basic properties of the Wirtinger derivative, we perform gradient descent using two Jacobians per computational node. Due to space limitations the following summary is necessarily brief. A great overview of the core mechanics of complex-valued nets and the Wirtinger derivative is found in Mandic & Goh (2009). This and other literature are built on the theory developed in Brandwood (1983) and van den Bos (1994) for optimization of complex-valued nets using respectively first- and second-order derivatives with Wirtinger calculus. For a deeper discussion of Wirtinger calculus and optimization techniques we refer the reader to Kreutz-Delgado (2009); Li & Adali (2008). Finally Amin et al.", "startOffset": 91, "endOffset": 1081}, {"referenceID": 0, "context": "Finally Amin et al. (2011); Amin & Murase (2013) advocate a framework for composing holomorphic and non-holomorphic functions in complex-valued nets.", "startOffset": 8, "endOffset": 27}, {"referenceID": 0, "context": "Finally Amin et al. (2011); Amin & Murase (2013) advocate a framework for composing holomorphic and non-holomorphic functions in complex-valued nets.", "startOffset": 8, "endOffset": 49}, {"referenceID": 2, "context": "Using these definitions, Brandwood (1983) shows that", "startOffset": 25, "endOffset": 42}, {"referenceID": 1, "context": "We need keep track of only two partial derivatives for each function, as shown in the bottom part of Figure 2 (Amin et al., 2011; Li & Adali, 2008).", "startOffset": 110, "endOffset": 147}, {"referenceID": 9, "context": "More generally, given arbitrary functions F and G in the computational graph, we compose their Jacobians in the following way (Kreutz-Delgado, 2009): JF \u25e6G = JFJG + J c FJ c G JF \u25e6G = JFJ c G + J c FJG", "startOffset": 126, "endOffset": 148}, {"referenceID": 12, "context": "We employed a learning rate with power scheduling decay (Senior et al., 2013).", "startOffset": 56, "endOffset": 77}, {"referenceID": 18, "context": "We and other authors have found that complex-valued networks are extremely sensitive to initial conditions and learning rates (Zimmermann et al., 2011).", "startOffset": 126, "endOffset": 151}, {"referenceID": 13, "context": "In order to facilitate finding a good setting of hyperparameters, we performed hyperparameter optimization using Spearmint (Snoek et al., 2012) for the following parameters: initial weight scaling, learning rate, and learning rate decay half life.", "startOffset": 123, "endOffset": 143}], "year": 2015, "abstractText": "Complex-valued neural networks (CVNNs) are an emerging field of research in neural networks due to their potential representational properties for audio, image, and physiological signals. It is common in signal processing to transform sequences of real values to the complex domain via a set of complex basis functions, such as the Fourier transform. We show how CVNNs can be used to learn complex representations of real valued time-series data. We present methods and results using a framework that can compose holomorphic and non-holomorphic functions in a multi-layer network using a theoretical result called the Wirtinger derivative. We test our methods on a representation learning task for real-valued signals, recurrent complex-valued networks and their real-valued counterparts. Our results show that recurrent complex-valued networks can perform as well as their realvalued counterparts while learning filters that are representative of the domain of the data.", "creator": "LaTeX with hyperref package"}}}