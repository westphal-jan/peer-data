{"id": "1511.06359", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "19-Nov-2015", "title": "FRIST - Flipping and Rotation Invariant Sparsifying Transform Learning and Applications to Inverse Problems", "abstract": "Features based on sparse representation, especially using synthesis dictionary model, have been heavily exploited in signal processing and computer vision. However, synthesis dictionary learning involves NP-hard sparse coding and expensive learning steps. Recently, sparsifying transform learning received interest for its cheap computation and closed-form solution. In this work, we develop a methodology for learning of Flipping and Rotation Invariant Sparsifying Transform, dubbed FRIST, to better represent natural images that contain textures with various geometrical directions. The proposed alternating learning algorithm involves simple closed-form solutions. Preliminary experiments show the usefulness of adaptive sparse representation by FRIST for image compression, denoising and inpainting with promising performances.", "histories": [["v1", "Thu, 19 Nov 2015 20:55:49 GMT  (2990kb)", "https://arxiv.org/abs/1511.06359v1", "11 pages (including the references and supplementary material), under review as conference paper at ICLR 2016"], ["v2", "Fri, 8 Jan 2016 06:43:38 GMT  (3720kb,D)", "http://arxiv.org/abs/1511.06359v2", "11 pages (including the references and supplementary material), under review as conference paper at ICLR 2016"], ["v3", "Tue, 17 May 2016 03:54:47 GMT  (2220kb,D)", "http://arxiv.org/abs/1511.06359v3", "25 pages (including the references and supplementary material)"], ["v4", "Mon, 16 Oct 2017 02:42:20 GMT  (1806kb,D)", "http://arxiv.org/abs/1511.06359v4", "Published in Inverse Problems"]], "COMMENTS": "11 pages (including the references and supplementary material), under review as conference paper at ICLR 2016", "reviews": [], "SUBJECTS": "cs.LG cs.CV", "authors": ["bihan wen", "saiprasad ravishankar", "yoram bresler"], "accepted": false, "id": "1511.06359"}, "pdf": {"name": "1511.06359.pdf", "metadata": {"source": "CRF", "title": "FRIST - Flipping and Rotation Invariant Sparsifying Transform Learning and Applications", "authors": ["Bihan Wen", "Saiprasad Ravishankar", "Yoram Bresler"], "emails": ["bwen3@illinois.edu,", "ravisha@umich.edu,", "ybresler@illinois.edu"], "sections": [{"heading": null, "text": "Recently, economical learning has become interesting for its cheap calculation and its optimal updates in the alternating algorithms. In this thesis, we develop a method for learning Flipping and Rotation Invariant Sparsifying Transforms, called FRIST, to better represent natural images that contain textures with different geometrical directions. The proposed alternating FRIST learning algorithm includes efficient optimal updates. We offer a convergence guarantee and demonstrate the empirical convergence behavior of the proposed FRIST learning approach. Pre-experiments show the promising performance of FRIST learning for sparse image representation, segmentation, denosis, robust painting and compressed magnetic ression images. Keywords: Sparsifying transform learning, Dictionary learning, Machine learning, Image denoising, Inpainting, Magnetic ressing."}, {"heading": "1. Introduction", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "1.1. Sparse Modeling", "text": "It is not the first time that the two countries have come to terms with each other in a way that they do. (...) It is not the first time that they have come to terms with each other in a way that they do. (...) It is not the first time that they do it. (...) It is not the second time that they do it. (...) \"It is the second time that they do it.\" \"\" It is the second time that they do it. \"\" \"It is not the first time that they do it.\" (...) \"It is the second time that they do it.\" (...) \"It is the second time that they do it.\" (...) \"It is the second time that they do it.\" \"(...)\" \"It is the third time that they do it.\" \"(...)\" It is the first time. \"(...)\" It is the second time that they do it. \"(...)\" It is the third time that they do it. \"\" \"It is the third time that they do it.\" \"\" It is the third time that they do it. \"\" \"It is the third time that they do it.\" \"(...)\" It is the first time. \"(...\") \"It is the second time that they do it.\""}, {"heading": "1.2. Highlights and Organization", "text": "We summarize some important features and contributions of this work as follows: \u2022 We propose an FRIST model that exploits the property of mirroring and rotation of nature images, i.e., image patches typically contain edges and features at different orientations, and therefore a (single) common transformation could be learned for different applications and inverse problems. \u2022 We propose a novel problem model or an efficient algorithm for learning FRIST (with the limitations that often reflect observed image properties) that proves useful and includes limited or highly corrupt measurements."}, {"heading": "2. FRIST Model and Its Learning Formulation", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "2.1. FRIST Model", "text": "Learning the economical W-domain model [34] has been proposed recently. (We have reached the minimum) We propose an FRIST model that first applies a rotation and rotation (FR). (F) We propose an FRIST model that first applies a rotation and rotation (FR). (F) We have a limited number of rotation and rotation operators. (F) We have a limited number of rotation and rotation operators. (F) We consider the economical coding problem in the FRIST model as follows: (P1) min. (K) min. (K) min. (F) min."}, {"heading": "2.2. FRIST Learning Formulation", "text": "In this work, we limit ourselves to learning FRIST with a quadratic parent (i.e., m = n), which results in a highly efficient learning algorithm with optimal adaptations. (P2) min W, (Ck) K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K"}, {"heading": "3. FRIST Learning Algorithm and Convergence Analysis", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "3.1. FRIST Learning Algorithm", "text": "We propose an efficient problem solving algorithm (P2) alternating between a sparse coding and clustering, and a transformational update step by step. Sparse coding and clustering. (P3) Considering the training matrix Y, and a fixed partransformation W, we solve the following problem (P3) for the sparse codes and clusters: (P3) min {Ck}, {Xi} K \u00b2 K \u00b2, where the best sparse codes are assigned to FR permutation. (P3) The modeling errors (P3) are used as cluster dimensions corresponding to the signal Yi. (P3) The FR operator is an optimal permutation. (GqYF), where both GqYF and F are permutation matrices. (P3) Problem is clearly synonymous with the search for the optimal \"Yi.\""}, {"heading": "3.2. Convergence Analysis", "text": "We analyze the convergence behavior of the proposed FRIST learning algorithm for (P2), assuming that each step in the algorithms (such as SVD) is calculated accurately. Problem (P2) is formulated with the least constraints (for each Xi), which equate to an unrestricted formulation with the least barriers (Xi) if the condition is violated, and is otherwise zero. Thus, the objective function of the problem (P2) can be rewritten asf (W, X,) = K (K) = K (K)."}, {"heading": "4. Applications", "text": "Natural or biomedical images typically contain a variety of directional characteristics and edges. Therefore, the FRIST model is particularly attractive for applications in image processing and inverse problems. In this section we will consider three such applications, namely image sharpening, image coloring and magnetic resonance imaging (MRI) based on blind compressed sensors (BCS)."}, {"heading": "4.1. Image Denoising", "text": "The goal is to reconstruct a 2D image represented as a vector y-RP, from its measurement z = y + h, corrupted by a noise vector h. Various denoising algorithms have been proposed lately, with the status-of-the-art result [49, 9]. Similar to the previous image information and transformation learning methods [11, 44], we propose the following patch-based image information that uses FRIST learning: (P5) min W, {yi, xi, Ck} K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K"}, {"heading": "4.2. Image Inpainting", "text": "The object of the image installation is to restore missing pixels in an image. The default image measurement, with missing pixel intensities set to zero, is called z = i + zi, where additive noise on the available pixels (P6) is a problem, and where there is a diagonal binary matrix with zeros in places that correspond to missing pixels. We propose to define the following patch-based image information using FRIST indicators: (P6) min W, {yi, xi, Ck, K, K, K, K, K, 2, P6, P6, P6, P6, P7, P7, P7, P7, P7, P7, P7, P7, P7, P7, P7, P7, P7, P7, P7, P7, P7, P7, P7, P7, P7, P7, P7, P7, P7, P7, P7, P7, P7, P7, P7, P7, P7, P7, P7, P7, P7, P7, P7, P7, P7, P7, P7, P7, P7, P7, 7, P7, P7, P7, P7, P7, P7, 7, P7, P7, P7, 7, P7, P7, 7, P7, 7, P7, P7, P7, 7, 7, P7, P7, 7, P7, 7, P7, 7, P7, P7, 7, P7, 7, P7, P7, 7, P7, 7, P7, 7, 7, P7, 7, P7, 7, 7, 7, 7, P7, 7, 7, P7, P7, 7, 7, P7, 7, P7, 7, 7, 7, P7, P7, P7, 7, 7, 7, 7, P7, 7, P7, P7, 7, 7, P7, P7, 7, P7, 7, P7, 7, P7, 7, 7, 7, P7, 7, 7, 7,"}, {"heading": "4.3. BCS-based MRI", "text": "That is, we have to deal with the question of what we can do when we deal with the question of what we are doing. (...) We have to deal with the question of what we are doing. (...) We have to deal with the question of what we are doing. (...) We have to deal with the question of what we are doing. (...) We have to deal with the question of what we are doing. (...) We have to deal with the question of what we are doing. (...) We have to deal with the question of what we are doing. (...) We have to deal with the question of what we are doing. (...) We have to do it. (...) We have to do it. (...) We have to do it. (...) We have to do it. (...) We have to do it. (...) We have to do it. (...) We have to do it. (...) We have to do it. (...) We have to do it."}, {"heading": "5. Experiments", "text": "We present numerical convergence results for the FRIST learning algorithm along with examples of image segmentation and some preliminary results showing that FRIST learning is promising in applications including image-saving imaging, denociation, robust inpainting, and MRI reconstruction. We work with 8 x 8 overlapping spots to study convergence and sparse imaging, 8 x 8 overlapping spots for image segmentation, denoising, and robust inpainting, and 6 x 6 overlapping spots (including spots at image boundaries that \"wrap\" on the opposite side of the image) for the MRI experiments. Figure 1 lists the test images used in the image denoization and inpainting experiments."}, {"heading": "5.1. Empirical convergence results", "text": "First, we illustrate the convergence behavior of FRIST learning. From the 44 images in the USC-SIPI database [2] (the color images are converted to grayscale images), we randomly extract an FRIST model with a 64 \u00d7 64 parent transformation W from the randomly selected image fields with a defined sparsity level s = 10 per image field. For simplicity of visualization, we use K = 2 and \u03bb0 = 3.1 \u00d7 10 \u2212 3. In the experiment, we initialize the learning algorithm with various 64 \u00d7 64 parent square transformations W's, including (i) Karhunen-Loe-ve Transform (KLT), (ii) 2D-DCT, (iii) random matrix with i.i.d. Gaussian entries (zero mean and standard deviation 0.2) and (iv) identity matrix."}, {"heading": "1 200 400 600 800", "text": "Figures 2 (a) and 2 (d) illustrate that the objective function and sparsification error (P2) converge to similar values from different W initializations, indicating that the algorithm is relatively insensitive or robust to initializations in practice. Figures 2 (b) and 2 (c) show the changes in cluster sizes via iterations for the 2D-DCT and CLT initializations. The final values of the corresponding cluster sizes are also similar (though not necessarily identical) for different initializations. Figure 2 (e) and 2 (f) show the learned FRIST parents W's with DCT and random matrix initializations. They are not identical and capture features that make the image fields equally sparing. Therefore, we consider such learned transformations to be essentially equivalent, as they achieve similar objective values and sparsification errors for parents are similar to the DCT and sparsification data for parents."}, {"heading": "5.2. Image Segmentation and Clustering Behavior", "text": "The FRIST learning algorithm is able to group image fields according to their orientations. In this subsection, we illustrate cluster behavior through image segmentation experiments. We look at the Wave (512 \u00d7 512) and Field (512 \u00d7 512) images, which are shown as inputs in Fig. 3 (a) and Fig. 4 (a). Both images contain directional textures, and we aim to group the pixels of the images into one of four classes representing different orientations or flips. For each input image, we convert it to grayscale, extract the overlapping averages and subtract patches, and learn the patches using the algorithm in Section 3.1, which represent different orientations or flips."}, {"heading": "5.3. Sparse Image Representation", "text": "This year, it has come to the point that it will only be a matter of time before it is ready, until it is ready."}, {"heading": "5.4. Image Denoising", "text": "We present the results of our FRIST-based questionnaire in section 4.1. We simulate i.i.d. Gaussian noise in 4 different noise levels (\u03c3 = 5, 10, 15, 20) for seven standard images in Figures 1, 2, 3, 4, 5, 5, 6, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7,"}, {"heading": "5.5. Image Inpainting", "text": "We present preliminary results for our adaptive FRIST-based inpainting frames (based on (P6). We remove at will 80% and 90% of the pixels of the entire images in Fig. 5 and i.i.d. simulate additive noise of the sampled pixels at 0, 5, and 15. We use K = 64, n = 64, and apply the proposed adaptive FRIST inpainting algorithms to reconstruct the images from the corrupt and smoky measurements. For comparison, we replace the adaptive FRIST algorithms with the fixed 2D-DCT, and the adaptive OCTOBOS algorithms."}, {"heading": "5.6. MRI Reconstruction", "text": "We present preliminary MRI results using the proposed FRIST-MRI algorithm. The three complex images and associated sampling masks in this section are used in Fig. 8, Fig. 9 (a), and the other parameters were similarly used for the k-space of the reference images."}, {"heading": "6. Conclusion", "text": "In this paper, we presented a novel framework for learning flipping and rotating invariant sparsifying transformations, which correspond to a structured union of transformations and are called FRIST. Collecting transformations in FRIST is associated with an underlying (or generating) parent transformation through flipping and rotation operations. Our FRIST learning algorithm is highly efficient and includes optimal updates with convergence guarantees. We demonstrated the ability of FRIST learning to extract directional characteristics in images. In practice, FRIST learning is insensitive to initialization and works better than several previous adaptive sparse modeling methods in various applications, including sparse imaging, image compression, imaging, and blind compressed MRI reconstruction."}], "references": [{"title": "Sparse and redundant modeling of image content using an imagesignature-dictionary", "author": ["M. Aharon", "M. Elad"], "venue": "SIAM Journal on Imaging Sciences,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2008}, {"title": "K-SVD: An algorithm for designing overcomplete dictionaries for sparse representation", "author": ["M. Aharon", "M. Elad", "A. Bruckstein"], "venue": "IEEE Trans. on Signal Processing,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2006}, {"title": "From sparse solutions of systems of equations to sparse modeling of signals and images", "author": ["A.M. Bruckstein", "D.L. Donoho", "M. Elad"], "venue": "SIAM Review,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2009}, {"title": "Compression of facial images using the k-svd algorithm", "author": ["O. Bryt", "M. Elad"], "venue": "Journal of Visual Communication and Image Representation,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2008}, {"title": "Atomic decomposition by basis pursuit", "author": ["S.S. Chen", "D.L. Donoho", "M.A. Saunders"], "venue": "SIAM J. Sci. Comput.,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 1998}, {"title": "Image denoising by sparse 3D transformdomain collaborative filtering", "author": ["K. Dabov", "A. Foi", "V. Katkovnik", "K. Egiazarian"], "venue": "IEEE Trans. on Image Processing,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2007}, {"title": "Adaptive greedy approximations", "author": ["G. Davis", "S. Mallat", "M. Avellaneda"], "venue": "Journal of Constructive Approximation,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 1997}, {"title": "Image denoising via sparse and redundant representations over learned dictionaries", "author": ["M. Elad", "M. Aharon"], "venue": "IEEE Trans. Image Process.,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2006}, {"title": "Analysis versus synthesis in signal priors", "author": ["M. Elad", "P. Milanfar", "R. Rubinstein"], "venue": "Inverse Problems,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2007}, {"title": "Hakon-Husoy. Method of optimal directions for frame design", "author": ["K. Engan", "S. Aase", "J.H"], "venue": "In Proc. IEEE International Conference on Acoustics, Speech, and Signal Processing,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 1999}, {"title": "Blind compressed sensing", "author": ["S. Gleichman", "Y.C. Eldar"], "venue": "IEEE Transactions on Information Theory,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2011}, {"title": "PCA-SIFT: A more distinctive representation for local image descriptors", "author": ["Y. Ke", "R. Sukthankar"], "venue": "In IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR),", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2004}, {"title": "Object recognition from local scale-invariant features", "author": ["D.G. Lowe"], "venue": "In IEEE International Conference on Computer vision (ICCV),", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 1999}, {"title": "Sparse MRI: The application of compressed sensing for rapid MR imaging", "author": ["M. Lustig", "D. Donoho", "J. Pauly"], "venue": "Magnetic resonance in medicine,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2007}, {"title": "Online learning for matrix factorization and sparse coding", "author": ["J. Mairal", "F. Bach", "J. Ponce", "G. Sapiro"], "venue": "J. Mach. Learn. Res.,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2010}, {"title": "Sparse representation for color image restoration", "author": ["J. Mairal", "M. Elad", "G. Sapiro"], "venue": "IEEE Trans. on Image Processing,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2008}, {"title": "A Wavelet Tour of Signal Processing", "author": ["S. Mallat"], "venue": null, "citeRegEx": "20", "shortCiteRegEx": "20", "year": 1999}, {"title": "Matching pursuits with time-frequency dictionaries", "author": ["S.G. Mallat", "Zhifeng Zhang"], "venue": "IEEE Transactions on Signal Processing,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 1993}, {"title": "Magnetic resonance image reconstruction using trained geometric directions in 2d redundant wavelets domain and non-convex optimization", "author": ["B. Ning", "X. Qu", "D. Guo", "C. Hu", "Z. Chen"], "venue": "Magnetic resonance imaging,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2013}, {"title": "Orthogonal matching pursuit : recursive function approximation with applications to wavelet decomposition", "author": ["Y. Pati", "R. Rezaiifar", "P. Krishnaprasad"], "venue": "In Asilomar Conf. on Signals, Systems and Comput.,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 1993}, {"title": "Bandelet image approximation and compression", "author": ["E.L. Pennec", "S. Mallat"], "venue": "Multiscale Modeling & Simulation,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2005}, {"title": "Adaptive sparsifying transforms for iterative tomographic reconstruction", "author": ["L. Pfister", "Y. Bresler"], "venue": "In International Conference on Image Formation in X-Ray Computed Tomography,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2014}, {"title": "Learning sparsifying filter banks", "author": ["L. Pfister", "Y. Bresler"], "venue": "In Proc. SPIE Wavelets & Sparsity XVI,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2015}, {"title": "Hadamard transform image coding", "author": ["W.K. Pratt", "J. Kane", "H.C. Andrews"], "venue": "Proc. IEEE,", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 1969}, {"title": "Undersampled MRI reconstruction with patch-based directional wavelets", "author": ["X. Qu", "D. Guo", "B. Ning", "Y. Hou", "Y. Lin", "S. Cai", "Z. Chen"], "venue": "Magnetic resonance imaging,", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2012}, {"title": "Magnetic resonance image reconstruction from undersampled measurements using a patch-based nonlocal operator", "author": ["X. Qu", "Y. Hou", "F. Lam", "D. Guo", "J. Zhong", "Z. Chen"], "venue": "Medical image analysis,", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2014}, {"title": "Image processing using smooth ordering of its patches", "author": ["I. Ram", "M. Elad", "I. Cohen"], "venue": "IEEE Transactions on Image Processing,", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2013}, {"title": "MR image reconstruction from highly undersampled k-space data by dictionary learning", "author": ["S. Ravishankar", "Y. Bresler"], "venue": "IEEE Transactions on Medical Imaging,", "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2011}, {"title": "Learning doubly sparse transforms for images", "author": ["S. Ravishankar", "Y. Bresler"], "venue": "IEEE Trans. Image Process.,", "citeRegEx": "32", "shortCiteRegEx": "32", "year": 2013}, {"title": "Learning overcomplete sparsifying transforms for signal processing.  FRIST Learning and Applications", "author": ["S. Ravishankar", "Y. Bresler"], "venue": "IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP),", "citeRegEx": "33", "shortCiteRegEx": "33", "year": 2013}, {"title": "Learning sparsifying transforms", "author": ["S. Ravishankar", "Y. Bresler"], "venue": "IEEE Trans. Signal Process.,", "citeRegEx": "34", "shortCiteRegEx": "34", "year": 2013}, {"title": "`0 sparsifying transform learning with efficient optimal updates and convergence guarantees", "author": ["S. Ravishankar", "Y. Bresler"], "venue": "IEEE Transactions on Signal Processing,", "citeRegEx": "35", "shortCiteRegEx": "35", "year": 2014}, {"title": "Efficient blind compressed sensing using sparsifying transforms with convergence guarantees and application to magnetic resonance imaging", "author": ["S. Ravishankar", "Y. Bresler"], "venue": "SIAM Journal on Imaging Sciences,", "citeRegEx": "36", "shortCiteRegEx": "36", "year": 2015}, {"title": "Online sparsifying transform learning - part i: Algorithms", "author": ["S. Ravishankar", "B. Wen", "Y. Bresler"], "venue": "IEEE Journal of Selected Topics in Signal Process.,", "citeRegEx": "37", "shortCiteRegEx": "37", "year": 2015}, {"title": "Dictionaries for sparse representation modeling", "author": ["R. Rubinstein", "A.M. Bruckstein", "M. Elad"], "venue": "Proceedings of the IEEE,", "citeRegEx": "38", "shortCiteRegEx": "38", "year": 2010}, {"title": "Recursive least squares dictionary learning algorithm", "author": ["K. Skretting", "K. Engan"], "venue": "IEEE Transactions on Signal Processing,", "citeRegEx": "39", "shortCiteRegEx": "39", "year": 2010}, {"title": "Highly undersampled magnetic resonance image reconstruction via homotopic-minimization", "author": ["J. Trzasko", "A. Manduca"], "venue": "IEEE Transactions on Medical imaging,", "citeRegEx": "40", "shortCiteRegEx": "40", "year": 2009}, {"title": "Contouring: a guide to the analysis and display of spatial data", "author": ["D. Watson"], "venue": null, "citeRegEx": "41", "shortCiteRegEx": "41", "year": 2013}, {"title": "When sparsity meets low-rankness: Transform learning with non-local low-rank constraint for image restoration", "author": ["B. Wen", "Y. Li", "Y. Bresler"], "venue": "In IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP),", "citeRegEx": "42", "shortCiteRegEx": "42", "year": 2017}, {"title": "Learning overcomplete sparsifying transforms with block cosparsity", "author": ["B. Wen", "S. Ravishankar", "Y. Bresler"], "venue": "In IEEE International Conference on Image Processing (ICIP),", "citeRegEx": "43", "shortCiteRegEx": "43", "year": 2014}, {"title": "Structured overcomplete sparsifying transform learning with convergence guarantees and applications", "author": ["B. Wen", "S. Ravishankar", "Y. Bresler"], "venue": "Int. J. Computer Vision,", "citeRegEx": "44", "shortCiteRegEx": "44", "year": 2015}, {"title": "Sparse coding with invariance constraints", "author": ["H. Wersing", "J. Eggert", "E. K\u00f6rner"], "venue": "In Artificial Neural Networks and Neural Information Processing,", "citeRegEx": "45", "shortCiteRegEx": "45", "year": 2003}, {"title": "Inverting modified matrices", "author": ["M. Woodbury"], "venue": "Memorandum report,", "citeRegEx": "46", "shortCiteRegEx": "46", "year": 1950}, {"title": "Dictionary learning for sparse approximations with the majorization method", "author": ["M. Yaghoobi", "T. Blumensath", "M. Davies"], "venue": "IEEE Transaction on Signal Processing,", "citeRegEx": "47", "shortCiteRegEx": "47", "year": 2009}, {"title": "Finite element structural analysis, volume 2", "author": ["T. Yang"], "venue": null, "citeRegEx": "48", "shortCiteRegEx": "48", "year": 1986}, {"title": "DCT image denoising: a simple and effective image denoising algorithm", "author": ["G. Yu", "G. Sapiro"], "venue": "Image Processing On Line,", "citeRegEx": "49", "shortCiteRegEx": "49", "year": 2011}, {"title": "Image modeling and enhancement via structured sparse model selection", "author": ["G. Yu", "G. Sapiro", "S. Mallat"], "venue": "In 2010 IEEE International Conference on Image Processing,", "citeRegEx": "50", "shortCiteRegEx": "50", "year": 2010}, {"title": "Solving inverse problems with piecewise linear estimators: From gaussian mixture models to structured sparsity", "author": ["G. Yu", "G. Sapiro", "S. Mallat"], "venue": "IEEE Transactions on Image Processing,", "citeRegEx": "51", "shortCiteRegEx": "51", "year": 2012}, {"title": "Dictionary optimization for block-sparse representations", "author": ["L. Zelnik-Manor", "K. Rosenblum", "Y. Eldar"], "venue": "IEEE Transactions on Signal Processing,", "citeRegEx": "52", "shortCiteRegEx": "52", "year": 2012}, {"title": "Fast multi-class dictionaries learning with geometrical directions in MRI reconstruction", "author": ["Z. Zhan", "J. Cai", "D. Guo", "Y. Liu", "Z. Chen", "X. Qu"], "venue": "arXiv preprint arXiv:1503.02945,", "citeRegEx": "53", "shortCiteRegEx": "53", "year": 2015}], "referenceMentions": [{"referenceID": 2, "context": "Various sparse signal models, such as the synthesis model [6, 12] and the transform model [27] have been studied.", "startOffset": 58, "endOffset": 65}, {"referenceID": 8, "context": "Various sparse signal models, such as the synthesis model [6, 12] and the transform model [27] have been studied.", "startOffset": 58, "endOffset": 65}, {"referenceID": 23, "context": "Various sparse signal models, such as the synthesis model [6, 12] and the transform model [27] have been studied.", "startOffset": 90, "endOffset": 94}, {"referenceID": 9, "context": "Synthesis dictionary learning methods [13, 5] that adapt the dictionary ar X iv :1 51 1.", "startOffset": 38, "endOffset": 45}, {"referenceID": 1, "context": "Synthesis dictionary learning methods [13, 5] that adapt the dictionary ar X iv :1 51 1.", "startOffset": 38, "endOffset": 45}, {"referenceID": 6, "context": "based on training data typically involve a synthesis sparse coding step which is often NPhard [10], so that approximate solutions [23, 21, 8] are widely used.", "startOffset": 94, "endOffset": 98}, {"referenceID": 19, "context": "based on training data typically involve a synthesis sparse coding step which is often NPhard [10], so that approximate solutions [23, 21, 8] are widely used.", "startOffset": 130, "endOffset": 141}, {"referenceID": 17, "context": "based on training data typically involve a synthesis sparse coding step which is often NPhard [10], so that approximate solutions [23, 21, 8] are widely used.", "startOffset": 130, "endOffset": 141}, {"referenceID": 4, "context": "based on training data typically involve a synthesis sparse coding step which is often NPhard [10], so that approximate solutions [23, 21, 8] are widely used.", "startOffset": 130, "endOffset": 141}, {"referenceID": 9, "context": "Various dictionary learning algorithms [13, 47, 39, 18] have been proposed and are popular in numerous applications such as denoising, inpainting, deblurring, and demosaicing [11, 19, 4].", "startOffset": 39, "endOffset": 55}, {"referenceID": 43, "context": "Various dictionary learning algorithms [13, 47, 39, 18] have been proposed and are popular in numerous applications such as denoising, inpainting, deblurring, and demosaicing [11, 19, 4].", "startOffset": 39, "endOffset": 55}, {"referenceID": 35, "context": "Various dictionary learning algorithms [13, 47, 39, 18] have been proposed and are popular in numerous applications such as denoising, inpainting, deblurring, and demosaicing [11, 19, 4].", "startOffset": 39, "endOffset": 55}, {"referenceID": 14, "context": "Various dictionary learning algorithms [13, 47, 39, 18] have been proposed and are popular in numerous applications such as denoising, inpainting, deblurring, and demosaicing [11, 19, 4].", "startOffset": 39, "endOffset": 55}, {"referenceID": 7, "context": "Various dictionary learning algorithms [13, 47, 39, 18] have been proposed and are popular in numerous applications such as denoising, inpainting, deblurring, and demosaicing [11, 19, 4].", "startOffset": 175, "endOffset": 186}, {"referenceID": 15, "context": "Various dictionary learning algorithms [13, 47, 39, 18] have been proposed and are popular in numerous applications such as denoising, inpainting, deblurring, and demosaicing [11, 19, 4].", "startOffset": 175, "endOffset": 186}, {"referenceID": 0, "context": "Various dictionary learning algorithms [13, 47, 39, 18] have been proposed and are popular in numerous applications such as denoising, inpainting, deblurring, and demosaicing [11, 19, 4].", "startOffset": 175, "endOffset": 186}, {"referenceID": 1, "context": "For example, the well-known K-SVD method [5] generalizes the K-means clustering process to a dictionary learning algorithm, and alternates between updating the sparse codes of training signals (sparse coding step) and the dictionary (dictionary or codebook update step).", "startOffset": 41, "endOffset": 44}, {"referenceID": 34, "context": "Moreover, methods such as KSVD lack convergence guarantees, and can get easily caught in local minima, or saddle points [38].", "startOffset": 120, "endOffset": 124}, {"referenceID": 16, "context": "It is well-known that natural images are sparsifiable by analytical transforms such as the discrete cosine transform (DCT), or wavelet transform [20].", "startOffset": 145, "endOffset": 149}, {"referenceID": 30, "context": "Furthermore, recent works proposed learning square sparsifying transforms (SST) [34], which turn out to be advantageous in various applications such as image denoising, magnetic resonance imaging (MRI), and computed tomography (CT) [34, 36, 25, 26].", "startOffset": 80, "endOffset": 84}, {"referenceID": 30, "context": "Furthermore, recent works proposed learning square sparsifying transforms (SST) [34], which turn out to be advantageous in various applications such as image denoising, magnetic resonance imaging (MRI), and computed tomography (CT) [34, 36, 25, 26].", "startOffset": 232, "endOffset": 248}, {"referenceID": 32, "context": "Furthermore, recent works proposed learning square sparsifying transforms (SST) [34], which turn out to be advantageous in various applications such as image denoising, magnetic resonance imaging (MRI), and computed tomography (CT) [34, 36, 25, 26].", "startOffset": 232, "endOffset": 248}, {"referenceID": 21, "context": "Furthermore, recent works proposed learning square sparsifying transforms (SST) [34], which turn out to be advantageous in various applications such as image denoising, magnetic resonance imaging (MRI), and computed tomography (CT) [34, 36, 25, 26].", "startOffset": 232, "endOffset": 248}, {"referenceID": 22, "context": "Furthermore, recent works proposed learning square sparsifying transforms (SST) [34], which turn out to be advantageous in various applications such as image denoising, magnetic resonance imaging (MRI), and computed tomography (CT) [34, 36, 25, 26].", "startOffset": 232, "endOffset": 248}, {"referenceID": 31, "context": "Alternating minimization algorithms for learning SST have been proposed with cheap and optimal updates [35].", "startOffset": 103, "endOffset": 107}, {"referenceID": 40, "context": "Recent work focused on learning a union of unstructured sparsifying transforms [44, 43], dubbed OCTOBOS (for OverComplete TransfOrm with BlOck coSparsity constraint \u2013 cf.", "startOffset": 79, "endOffset": 87}, {"referenceID": 39, "context": "Recent work focused on learning a union of unstructured sparsifying transforms [44, 43], dubbed OCTOBOS (for OverComplete TransfOrm with BlOck coSparsity constraint \u2013 cf.", "startOffset": 79, "endOffset": 87}, {"referenceID": 40, "context": "[44]), to sparsify images with diverse contents, features and textures.", "startOffset": 0, "endOffset": 4}, {"referenceID": 40, "context": "However, learning such an unstructured OCTOBOS model [44] (that has many free parameters) especially from noisy or limited data could suffer from overfitting to noise/artifacts, thereby degrading performance in various applications and inverse problem settings.", "startOffset": 53, "endOffset": 57}, {"referenceID": 41, "context": "synthesis model sparse coding [45], and applied rotational operators with analytical transforms [53], the usefulness of the rotational invariance property in learning adaptive sparse signal models has not been explored.", "startOffset": 30, "endOffset": 34}, {"referenceID": 49, "context": "synthesis model sparse coding [45], and applied rotational operators with analytical transforms [53], the usefulness of the rotational invariance property in learning adaptive sparse signal models has not been explored.", "startOffset": 96, "endOffset": 100}, {"referenceID": 46, "context": "There are other prior works such as the Structured Sparse Model Selection (SSMS) [50, 51] approach that also involve clustering, but the sub-dictionary in each cluster in SSMS is obtained by conventional Principal Component Analysis (PCA).", "startOffset": 81, "endOffset": 89}, {"referenceID": 47, "context": "There are other prior works such as the Structured Sparse Model Selection (SSMS) [50, 51] approach that also involve clustering, but the sub-dictionary in each cluster in SSMS is obtained by conventional Principal Component Analysis (PCA).", "startOffset": 81, "endOffset": 89}, {"referenceID": 30, "context": "The learning of the sparsifying transform model [34] has been proposed recently.", "startOffset": 48, "endOffset": 52}, {"referenceID": 12, "context": "Though there are various methods of formulating the rotation operator G with arbitrary angles [16, 15], rotating image patches by an angle \u03b8 that is not a multiple of 90\u25e6 requires interpolation, and may result in misalignment with the pixel grid.", "startOffset": 94, "endOffset": 102}, {"referenceID": 11, "context": "Though there are various methods of formulating the rotation operator G with arbitrary angles [16, 15], rotating image patches by an angle \u03b8 that is not a multiple of 90\u25e6 requires interpolation, and may result in misalignment with the pixel grid.", "startOffset": 94, "endOffset": 102}, {"referenceID": 20, "context": "Constructions of such {Gq} have been proposed before [24, 28, 53].", "startOffset": 53, "endOffset": 65}, {"referenceID": 24, "context": "Constructions of such {Gq} have been proposed before [24, 28, 53].", "startOffset": 53, "endOffset": 65}, {"referenceID": 49, "context": "Constructions of such {Gq} have been proposed before [24, 28, 53].", "startOffset": 53, "endOffset": 65}, {"referenceID": 28, "context": "For each \u03a6k, the optimal sparse code x\u0302 in Problem (P1) can be solved exactly as x\u0302 = Hs(W\u03a6ky), where Hs(\u00b7) is the projector onto the s-`0 ball [32], i.", "startOffset": 144, "endOffset": 148}, {"referenceID": 40, "context": "The FRIST model can be interpreted as a structured union-of-transforms model, or a structured OCTOBOS model [44], i.", "startOffset": 108, "endOffset": 112}, {"referenceID": 40, "context": "Problem (P1) is similar to the OCTOBOS sparse coding problem [44], where each Wk = W\u03a6k corresponds to a block of OCTOBOS.", "startOffset": 61, "endOffset": 65}, {"referenceID": 48, "context": "When the parent transform W is unitary, FRIST is also equivalent to an overcomplete synthesis dictionary with block sparsity [52], with W T k denoting the kth block of the equivalent overcomplete dictionary.", "startOffset": 125, "endOffset": 129}, {"referenceID": 29, "context": "Generally, the parent transform W can be overcomplete [33, 44, 26].", "startOffset": 54, "endOffset": 66}, {"referenceID": 40, "context": "Generally, the parent transform W can be overcomplete [33, 44, 26].", "startOffset": 54, "endOffset": 66}, {"referenceID": 22, "context": "Generally, the parent transform W can be overcomplete [33, 44, 26].", "startOffset": 54, "endOffset": 66}, {"referenceID": 40, "context": ", N}, which enforces all of the Ck\u2019s to be disjoint [44].", "startOffset": 52, "endOffset": 56}, {"referenceID": 30, "context": "Problem (P2) is to minimize the FRIST learning objective that includes the modeling or sparsification error \u2211K k=1 \u2211 i\u2208Ck \u2016W\u03a6kYi \u2212Xi\u2016 2 2 for Y as well as the regularizer Q(W ) = \u2212 log |detW | + \u2016W\u20162F to prevent trivial solutions [34].", "startOffset": 230, "endOffset": 234}, {"referenceID": 30, "context": "The regularizer Q(W ) fully controls the condition number and scaling of the learned parent transform [34].", "startOffset": 102, "endOffset": 106}, {"referenceID": 30, "context": "Previous works [34] showed that the condition number and spectral norm of the optimal parent transform \u0174 approach 1 and 1/ \u221a 2 respectively, as \u03bb0 \u2192\u221e in (P2).", "startOffset": 15, "endOffset": 19}, {"referenceID": 31, "context": "Problem (P4) has a simple solution involving a singular value decomposition (SVD), which is similar to the transform update step in SST [35].", "startOffset": 136, "endOffset": 140}, {"referenceID": 40, "context": "Unlike the previously proposed OCTOBOS learning algorithm [44], which requires initialization of the clusters using heuristic methods such as K-means, the FRIST learning algorithm only needs initialization of the parent transform W .", "startOffset": 58, "endOffset": 62}, {"referenceID": 20, "context": "In the first iteration, all possible FR operators \u03a6k\u2019s [24, 53] (i.", "startOffset": 55, "endOffset": 63}, {"referenceID": 49, "context": "In the first iteration, all possible FR operators \u03a6k\u2019s [24, 53] (i.", "startOffset": 55, "endOffset": 63}, {"referenceID": 1, "context": "Thus, the overall computational cost per iteration of FRIST learning using the proposed alternating algorithm scales as O(KnN), which is typically lower than the cost per iteration of the overcomplete K-SVD learning algorithm [5], with the number of dictionary atoms m = Kn, and synthesis sparsity s \u221d n.", "startOffset": 226, "endOffset": 229}, {"referenceID": 40, "context": "Since FRIST can be interpreted as a structured OCTOBOS, the convergence results for the FRIST learning algorithm take a form similar to those obtained for the OCTOBOS learning algorithm [44] in our recent work.", "startOffset": 186, "endOffset": 190}, {"referenceID": 40, "context": "The proof of Conclusion (ii) follows the same arguments as in the proofs in Lemma 3 and Lemma 5 in [44].", "startOffset": 99, "endOffset": 103}, {"referenceID": 40, "context": "In Conclusion (iii), Condition (4) can be proved using the arguments for Lemma 7 from [44], while Condition (5) can be proved with the arguments for Lemma 6 from [35].", "startOffset": 86, "endOffset": 90}, {"referenceID": 31, "context": "In Conclusion (iii), Condition (4) can be proved using the arguments for Lemma 7 from [44], while Condition (5) can be proved with the arguments for Lemma 6 from [35].", "startOffset": 162, "endOffset": 166}, {"referenceID": 31, "context": "The last conclusion in Theorem 1 can be shown using similar arguments as from the proof of Lemma 9 in [35].", "startOffset": 102, "endOffset": 106}, {"referenceID": 45, "context": "Various denoising algorithms have been proposed recently, with state-of-the-art performance [49, 9].", "startOffset": 92, "endOffset": 99}, {"referenceID": 5, "context": "Various denoising algorithms have been proposed recently, with state-of-the-art performance [49, 9].", "startOffset": 92, "endOffset": 99}, {"referenceID": 7, "context": "Similar to previous dictionary and transform learning based image denoising methods [11, 44], we propose the following patch-based image denoising formulation using FRIST learning:", "startOffset": 84, "endOffset": 92}, {"referenceID": 40, "context": "Similar to previous dictionary and transform learning based image denoising methods [11, 44], we propose the following patch-based image denoising formulation using FRIST learning:", "startOffset": 84, "endOffset": 92}, {"referenceID": 7, "context": "The data fidelity term \u03c4 \u2016Ri z \u2212 yi\u201622 measures the discrepancy between the observed patch Riz and the (unknown) noiseless patch yi, and uses a weight \u03c4 = \u03c40/\u03c3 that is inversely proportional to the given noise standard deviation \u03c3 [11, 35], and \u03c40 > 0.", "startOffset": 231, "endOffset": 239}, {"referenceID": 31, "context": "The data fidelity term \u03c4 \u2016Ri z \u2212 yi\u201622 measures the discrepancy between the observed patch Riz and the (unknown) noiseless patch yi, and uses a weight \u03c4 = \u03c40/\u03c3 that is inversely proportional to the given noise standard deviation \u03c3 [11, 35], and \u03c40 > 0.", "startOffset": 231, "endOffset": 239}, {"referenceID": 28, "context": "We follow the previous SST-based and OCTOBOSbased denoising methods [32, 44], and impose a sparsity constraint on each yi.", "startOffset": 68, "endOffset": 76}, {"referenceID": 40, "context": "We follow the previous SST-based and OCTOBOSbased denoising methods [32, 44], and impose a sparsity constraint on each yi.", "startOffset": 68, "endOffset": 76}, {"referenceID": 28, "context": "We then update the sparsity levels si for all i, similar to the SST learning-based denoising algorithm [32].", "startOffset": 103, "endOffset": 107}, {"referenceID": 28, "context": "We choose the optimal si to be the smallest integer that makes the reconstructed yi in (7) satisfy the error condition \u2016Riz \u2212 yi\u201622 \u2264 nC\u03c3, where C is a constant parameter [32].", "startOffset": 171, "endOffset": 175}, {"referenceID": 40, "context": "The noise standard deviation \u03c3 decreases gradually in each such pass, and is found (tuned) empirically [44].", "startOffset": 103, "endOffset": 107}, {"referenceID": 33, "context": "The sparse coding problem with a sparsity penalty has a closed-form solution [37], and thus Step (i) is equivalent to solving the following clustering problem:", "startOffset": 77, "endOffset": 81}, {"referenceID": 15, "context": "This is useful because real image measurements are inevitably corrupted with noise [19].", "startOffset": 83, "endOffset": 87}, {"referenceID": 36, "context": "Compressed Sensing (CS) exploits sparsity and enables accurate MRI reconstruction from limited k-space or Fourier measurements [40, 36, 17].", "startOffset": 127, "endOffset": 139}, {"referenceID": 32, "context": "Compressed Sensing (CS) exploits sparsity and enables accurate MRI reconstruction from limited k-space or Fourier measurements [40, 36, 17].", "startOffset": 127, "endOffset": 139}, {"referenceID": 13, "context": "Compressed Sensing (CS) exploits sparsity and enables accurate MRI reconstruction from limited k-space or Fourier measurements [40, 36, 17].", "startOffset": 127, "endOffset": 139}, {"referenceID": 27, "context": "However, CS-based MRI may suffer from artifacts at high undersampling factors, when using non-adaptive or analytical sparsifying transforms [31].", "startOffset": 140, "endOffset": 144}, {"referenceID": 32, "context": "Recent works [36] proposed Blind Compressed Sensing (BCS)-based MR image reconstruction methods using learned signal models,", "startOffset": 13, "endOffset": 17}, {"referenceID": 49, "context": "MR image patches typically contain various oriented features [53], which have recently been shown to be well sparsifiable by directional wavelets [28].", "startOffset": 61, "endOffset": 65}, {"referenceID": 24, "context": "MR image patches typically contain various oriented features [53], which have recently been shown to be well sparsifiable by directional wavelets [28].", "startOffset": 146, "endOffset": 150}, {"referenceID": 32, "context": "Similar to the previous TL-MRI work [36], we propose a BCS-based MR image reconstruction scheme using the (adaptive) FRIST model, dubbed FRIST-MRI.", "startOffset": 36, "endOffset": 40}, {"referenceID": 32, "context": "This sparsity constraint enables variable sparsity levels for individual patches [36].", "startOffset": 81, "endOffset": 85}, {"referenceID": 32, "context": "We use a block coordinate descent approach [36] to solve Problem (P7).", "startOffset": 43, "endOffset": 47}, {"referenceID": 31, "context": "Instead, we present an approximate \u2016 The number of patches is P when we use a patch overlap stride of 1 and include patches at image boundaries by allowing them to \u2018wrap-around\u2019 on the opposite side of the image [35].", "startOffset": 212, "endOffset": 216}, {"referenceID": 32, "context": "| W\u03a6k\u0302PRPy ] and retaining the s largest magnitude elements [36].", "startOffset": 60, "endOffset": 64}, {"referenceID": 31, "context": "The optimal solution, which is similar to previous work [35], is computed as follows.", "startOffset": 56, "endOffset": 60}, {"referenceID": 10, "context": "As Problem (18) is a least squares problem with an `2 constraint, it can be solved exactly using the Lagrange multiplier method [14].", "startOffset": 128, "endOffset": 132}, {"referenceID": 32, "context": "Similar to the previous TL-MRI work [36], the normal equation for Problem (19) (for known multiplier \u03c1) can be simplified as follows, where F denotes the full Fourier encoding matrix assumed normalized (FF = I):", "startOffset": 36, "endOffset": 40}, {"referenceID": 32, "context": "FEF , \u03bcFF u FuF H [36], and \u03c1I are all diagonal matrices, the matrix pre-multiplying Fy in (20) is diagonal and invertible.", "startOffset": 18, "endOffset": 22}, {"referenceID": 32, "context": "17) in [36]) that can be solved using Newton\u2019s method.", "startOffset": 7, "endOffset": 11}, {"referenceID": 3, "context": "Data-driven adaptation of dictionaries using the K-SVD scheme has also been shown to be beneficial for image compression, compared to fixed analytical transforms [7].", "startOffset": 162, "endOffset": 165}, {"referenceID": 31, "context": "In particular, we train a 64\u00d764 SST [35], a 128\u00d764 OCTOBOS [44], as well as a 64\u00d7 64 square (synthesis) dictionary and a 64\u00d7 128 overcomplete dictionary using KSVD [11], using the same training patches and sparsity level as for FRIST.", "startOffset": 36, "endOffset": 40}, {"referenceID": 40, "context": "In particular, we train a 64\u00d764 SST [35], a 128\u00d764 OCTOBOS [44], as well as a 64\u00d7 64 square (synthesis) dictionary and a 64\u00d7 128 overcomplete dictionary using KSVD [11], using the same training patches and sparsity level as for FRIST.", "startOffset": 59, "endOffset": 63}, {"referenceID": 7, "context": "In particular, we train a 64\u00d764 SST [35], a 128\u00d764 OCTOBOS [44], as well as a 64\u00d7 64 square (synthesis) dictionary and a 64\u00d7 128 overcomplete dictionary using KSVD [11], using the same training patches and sparsity level as for FRIST.", "startOffset": 164, "endOffset": 168}, {"referenceID": 30, "context": "Additionally, dictionary learning based representation requires synthesis sparse coding, which is more expensive compared to the cheap and exact sparse coding in the transform model-based methods [34].", "startOffset": 196, "endOffset": 200}, {"referenceID": 7, "context": "1 are compared with those obtained by the adaptive overcomplete K-SVD denoising scheme [11], adaptive SST denoising scheme [35] and the adaptive OCTOBOS denoising scheme [44].", "startOffset": 87, "endOffset": 91}, {"referenceID": 31, "context": "1 are compared with those obtained by the adaptive overcomplete K-SVD denoising scheme [11], adaptive SST denoising scheme [35] and the adaptive OCTOBOS denoising scheme [44].", "startOffset": 123, "endOffset": 127}, {"referenceID": 40, "context": "1 are compared with those obtained by the adaptive overcomplete K-SVD denoising scheme [11], adaptive SST denoising scheme [35] and the adaptive OCTOBOS denoising scheme [44].", "startOffset": 170, "endOffset": 174}, {"referenceID": 31, "context": "Image \u03c3 Noisy DCT SST [35] K-SVD [11] OCTOBOS [44] FRIST PSNR", "startOffset": 22, "endOffset": 26}, {"referenceID": 7, "context": "Image \u03c3 Noisy DCT SST [35] K-SVD [11] OCTOBOS [44] FRIST PSNR", "startOffset": 33, "endOffset": 37}, {"referenceID": 40, "context": "Image \u03c3 Noisy DCT SST [35] K-SVD [11] OCTOBOS [44] FRIST PSNR", "startOffset": 46, "endOffset": 50}, {"referenceID": 31, "context": "\u03c3 Noisy DCT SST [35] K-SVD [11] OCTOBOS [44] PSNR", "startOffset": 16, "endOffset": 20}, {"referenceID": 7, "context": "\u03c3 Noisy DCT SST [35] K-SVD [11] OCTOBOS [44] PSNR", "startOffset": 27, "endOffset": 31}, {"referenceID": 40, "context": "\u03c3 Noisy DCT SST [35] K-SVD [11] OCTOBOS [44] PSNR", "startOffset": 40, "endOffset": 44}, {"referenceID": 31, "context": "For the adaptive SST and OCTOBOS denoising methods, we follow the same parameter settings as used in the previous works [35, 44].", "startOffset": 120, "endOffset": 128}, {"referenceID": 40, "context": "For the adaptive SST and OCTOBOS denoising methods, we follow the same parameter settings as used in the previous works [35, 44].", "startOffset": 120, "endOffset": 128}, {"referenceID": 40, "context": "6 quickly degrades as the number of transforms (in the collection/union) or clusters to be learned from a set of noisy image patches is increased [44].", "startOffset": 146, "endOffset": 150}, {"referenceID": 38, "context": "Although we focused our comparisons here on related adaptive sparse modeling methods, a very recent work [42] shows that combining transform learning based denoising with non-local similarity models leads to better denoising performance, and outperforms the state-of-the-art BM3D denoising method [9].", "startOffset": 105, "endOffset": 109}, {"referenceID": 5, "context": "Although we focused our comparisons here on related adaptive sparse modeling methods, a very recent work [42] shows that combining transform learning based denoising with non-local similarity models leads to better denoising performance, and outperforms the state-of-the-art BM3D denoising method [9].", "startOffset": 297, "endOffset": 300}, {"referenceID": 38, "context": "A further extension of the work in [42] to include FRIST learning is of interest and could potentially provide even greater advantages, but we leave this detailed investigation to future work.", "startOffset": 35, "endOffset": 39}, {"referenceID": 31, "context": "the fixed 2D DCT, adaptive SST [35], and adaptive OCTOBOS [44] respectively, and evaluate the inpainting performance these alternatives.", "startOffset": 31, "endOffset": 35}, {"referenceID": 40, "context": "the fixed 2D DCT, adaptive SST [35], and adaptive OCTOBOS [44] respectively, and evaluate the inpainting performance these alternatives.", "startOffset": 58, "endOffset": 62}, {"referenceID": 44, "context": "The image inpainting results obtained by the FRIST based methods are also compared with those obtained by the cubic interpolation [48, 41] and patch smoothing [30] methods.", "startOffset": 130, "endOffset": 138}, {"referenceID": 37, "context": "The image inpainting results obtained by the FRIST based methods are also compared with those obtained by the cubic interpolation [48, 41] and patch smoothing [30] methods.", "startOffset": 130, "endOffset": 138}, {"referenceID": 26, "context": "The image inpainting results obtained by the FRIST based methods are also compared with those obtained by the cubic interpolation [48, 41] and patch smoothing [30] methods.", "startOffset": 159, "endOffset": 163}, {"referenceID": 26, "context": "The FRIST result is much improved, and also shows fewer artifacts compared to the patch smoothing [30] and adaptive SST results.", "startOffset": 98, "endOffset": 102}, {"referenceID": 32, "context": "05 \u00d7 nP , and the other parameters were set similarly as for TL-MRI in [36].", "startOffset": 71, "endOffset": 75}, {"referenceID": 32, "context": "To speed up convergence, lower sparsity levels are used in the initial iterations [36].", "startOffset": 82, "endOffset": 86}, {"referenceID": 13, "context": "We compare our FRISTMRI reconstruction results to those obtained using conventional or popular methods, including naive Zero-filling, Sparse MRI [17], DL-MRI [31], PBDWS [22], PANO [29], and TL-MRI [36].", "startOffset": 145, "endOffset": 149}, {"referenceID": 27, "context": "We compare our FRISTMRI reconstruction results to those obtained using conventional or popular methods, including naive Zero-filling, Sparse MRI [17], DL-MRI [31], PBDWS [22], PANO [29], and TL-MRI [36].", "startOffset": 158, "endOffset": 162}, {"referenceID": 18, "context": "We compare our FRISTMRI reconstruction results to those obtained using conventional or popular methods, including naive Zero-filling, Sparse MRI [17], DL-MRI [31], PBDWS [22], PANO [29], and TL-MRI [36].", "startOffset": 170, "endOffset": 174}, {"referenceID": 25, "context": "We compare our FRISTMRI reconstruction results to those obtained using conventional or popular methods, including naive Zero-filling, Sparse MRI [17], DL-MRI [31], PBDWS [22], PANO [29], and TL-MRI [36].", "startOffset": 181, "endOffset": 185}, {"referenceID": 32, "context": "We compare our FRISTMRI reconstruction results to those obtained using conventional or popular methods, including naive Zero-filling, Sparse MRI [17], DL-MRI [31], PBDWS [22], PANO [29], and TL-MRI [36].", "startOffset": 198, "endOffset": 202}, {"referenceID": 32, "context": "The parameter settings for these methods are as mentioned in [36].", "startOffset": 61, "endOffset": 65}, {"referenceID": 32, "context": "We separately tuned the sparsity parameter for TL-MRI [36] for reconstructing Image 3 \u2217.", "startOffset": 54, "endOffset": 58}, {"referenceID": 32, "context": "+ The testing image data in this section were used and included in previous works [36, 53] with the data sources.", "startOffset": 82, "endOffset": 90}, {"referenceID": 49, "context": "+ The testing image data in this section were used and included in previous works [36, 53] with the data sources.", "startOffset": 82, "endOffset": 90}, {"referenceID": 32, "context": "\u2217 We observed improved reconstruction PSNR compared to the result obtained using the sparsity settings in [36].", "startOffset": 106, "endOffset": 110}, {"referenceID": 13, "context": "Table 6: Comparison of the PSNRs, corresponding to the Zero-filling, Sparse MRI [17], DL-MRI [31], PBDWS [22], PANO [29], TL-MRI [36], and the proposed FRIST-MRI reconstructions for various images, sampling schemes, and undersampling factors.", "startOffset": 80, "endOffset": 84}, {"referenceID": 27, "context": "Table 6: Comparison of the PSNRs, corresponding to the Zero-filling, Sparse MRI [17], DL-MRI [31], PBDWS [22], PANO [29], TL-MRI [36], and the proposed FRIST-MRI reconstructions for various images, sampling schemes, and undersampling factors.", "startOffset": 93, "endOffset": 97}, {"referenceID": 18, "context": "Table 6: Comparison of the PSNRs, corresponding to the Zero-filling, Sparse MRI [17], DL-MRI [31], PBDWS [22], PANO [29], TL-MRI [36], and the proposed FRIST-MRI reconstructions for various images, sampling schemes, and undersampling factors.", "startOffset": 105, "endOffset": 109}, {"referenceID": 25, "context": "Table 6: Comparison of the PSNRs, corresponding to the Zero-filling, Sparse MRI [17], DL-MRI [31], PBDWS [22], PANO [29], TL-MRI [36], and the proposed FRIST-MRI reconstructions for various images, sampling schemes, and undersampling factors.", "startOffset": 116, "endOffset": 120}, {"referenceID": 32, "context": "Table 6: Comparison of the PSNRs, corresponding to the Zero-filling, Sparse MRI [17], DL-MRI [31], PBDWS [22], PANO [29], TL-MRI [36], and the proposed FRIST-MRI reconstructions for various images, sampling schemes, and undersampling factors.", "startOffset": 129, "endOffset": 133}, {"referenceID": 32, "context": "As we followed a reconstruction framework and parameters similar to those used by TL-MRI [36], the quality improvement obtained with FRIST-MRI is solely because the learned FRIST can serve as a better regularizer for MR image reconstruction compared to the single adaptive square transform in TL-MRI.", "startOffset": 89, "endOffset": 93}], "year": 2017, "abstractText": "Features based on sparse representation, especially using the synthesis dictionary model, have been heavily exploited in signal processing and computer vision. However, synthesis dictionary learning typically involves NP-hard sparse coding and expensive learning steps. Recently, sparsifying transform learning received interest for its cheap computation and its optimal updates in the alternating algorithms. In this work, we develop a methodology for learning Flipping and Rotation Invariant Sparsifying Transforms, dubbed FRIST, to better represent natural images that contain textures with various geometrical directions. The proposed alternating FRIST learning algorithm involves efficient optimal updates. We provide a convergence guarantee, and demonstrate the empirical convergence behavior of the proposed FRIST learning approach. Preliminary experiments show the promising performance of FRIST learning for sparse image representation, segmentation, denoising, robust inpainting, and compressed sensing-based magnetic resonance image reconstruction.", "creator": "LaTeX with hyperref package"}}}