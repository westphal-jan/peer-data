{"id": "1511.06362", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "19-Nov-2015", "title": "Efficient inference in occlusion-aware generative models of images", "abstract": "We present a generative model of images based on layering, in which image layers are individually generated, then composited from front to back. We are thus able to factor the appearance of an image into the appearance of individual objects within the image --- and additionally for each individual object, we can factor content from pose. Unlike prior work on layered models, we learn a shape prior for each object/layer, allowing the model to tease out which object is in front by looking for a consistent shape, without needing access to motion cues or any labeled data. We show that ordinary stochastic gradient variational bayes (SGVB), which optimizes our fully differentiable lower-bound on the log-likelihood, is sufficient to learn an interpretable representation of images. Finally we present experiments demonstrating the effectiveness of the model for inferring foreground and background objects in images.", "histories": [["v1", "Thu, 19 Nov 2015 20:56:27 GMT  (4193kb,D)", "https://arxiv.org/abs/1511.06362v1", "10 pages, Under review at ICLR 2016"], ["v2", "Tue, 16 Feb 2016 07:22:02 GMT  (4193kb,D)", "http://arxiv.org/abs/1511.06362v2", "10 pages"]], "COMMENTS": "10 pages, Under review at ICLR 2016", "reviews": [], "SUBJECTS": "cs.LG cs.CV", "authors": ["jonathan huang", "kevin murphy"], "accepted": false, "id": "1511.06362"}, "pdf": {"name": "1511.06362.pdf", "metadata": {"source": "CRF", "title": "ATIVE MODELS OF IMAGES", "authors": ["Jonathan Huang", "Kevin Murphy"], "emails": ["kpmurphy}@google.com"], "sections": [{"heading": "1 INTRODUCTION", "text": "It is. (...) It is. (...) It is. (...) It is. (...) It is. (...) It is. (...) It is. (...) It is. (...) It is. (...) It is. (...) It is. (...) It is. (...) It is. (...) It is. (...) It is. (...) It is. (...) It is. (...) It is. (...) It is. (...) It is. (...) It is. (...) It is. (...) It is. (...) It is. (...) It is. (...) It is. (...) It is. (...) It is. (...). It is. (...) It is. (...) It is. (...). It is. (...) It is. (...) It is. (...). It is. (...) It is. It is. (...). It is. (...) It is. (...). It is. (...). It is. It is. (...). It is. It is. (...). It is. It is. (...). It is. It is. (...). It is. It is. It is. (...). It is. (...). It is. (...). It is. It is. (...). It is. (...). It is. It is. It is. It is. It is. (). It is. It is. (...). It is. It is. (...). It is. (...). It is. It is. It is. It is. It is. (). It is. (). It is. (). It is. (). It is. (). It is. It is. (). It is."}, {"heading": "2 RELATED WORK", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "2.1 DEEP PROBABILISTIC GENERATIVE MODELS", "text": "Our approach is inspired by the recent introduction of generative deep-learning models that can be trained end-to-end through back propagation, including generative adversarial networks (Denton et al., 2015; Goodfellow et al., 2014) and variational auto-encoder (UAE) models (Kingma & Welling, 2014; Kingma et al., 2014; Rezende et al., 2014; Burda et al., 2015) that are most relevant to our environment.Among variant auto-encoder models, our work is most closely related to Gregor et al. (2015) DRAW network. Like our proposed model, the DRAW network is a generative model of images in the variable auto-encoder framework that decomposes image formation into several stages of complementation into a screen matrix. DRAW paper proceeds from a generative model of these sequential character actions that appears to be more general in nature than our practice is a step-by-step model."}, {"heading": "2.2 MODELING TRANSFORMATION IN NEURAL NETWORKS", "text": "One of our most important contributions is a model that is able to separate the pose of an object from its appearance, which is, of course, a classic problem in computer vision. At this point, we highlight some of the most related works from the deep learning community. Many of these related works were influenced by the Transforming Auto-Encoder models by Hinton et al. (2011), in which pose is explicitly separated from the content in an auto-encoder trained to predict (well-known) small transformations of an image. More recently, Dosovitskiy et al. (2015) introduced a revolutionary network to generate images of stools in which pose was explicitly separated, and Cheung et al. (2014) introduced an auto-encoder in which a subset of variables such as pose can be explicitly observed and remaining variables are encouraged to explain orthogonal factors of variation. Most relevant in this value line is the one that stands on top of a cult, such as a map, like this one that is automated (2015)."}, {"heading": "2.3 LAYERED MODELS OF IMAGES", "text": "Most work uses motion cues to break down video data into layers (Darrell & Pentland, 1991; Wang & Adelson, 1994; Ayer & Sawhney, 1995; Kannan et al., 2005; 2008), but there are some works that work from frames. Yang et al. (2012), for example, suggest a layer model for segmentation, but rely heavily on bounding boxes and categorical annotations. Isola & Liu (2013) deconstruct a single image in layers, but require a series of manually segmented regions. Our generative model is similar to Williams & Titsias (2004), but we can capture more complex appearance models by using deep neural networks compared to their pro-pixel mixtures of Gaussian models. In addition, our training process is simpler as it is only an end-to-end minibatch SGD."}, {"heading": "3 CST-VAE: A PROBABILISTIC LAYERED MODEL OF IMAGE GENERATION", "text": "We assume that these images are generated by (1) generating a sequence of image layers. (2) We assume that these images are generated by (1) generating a sequence of image layers. (2) We assume that these images are generated by (1) generating image layers. (3) We assume that images are generated by (2) generating image layers. (3) We assume that images are generated by (3) generating image layers. (4) We assume that images are generated by (4) generating image layers. (4) We assume that images are generated by (4) generating image layers. (4) We assume that images are generated by (4) generating image layers. (4) We assume that images are generated by (4) generating image layers. (4) We assume that images are generated by (4) generating image layers."}, {"heading": "3.1 INFERENCE AND PARAMETER LEARNING WITH VARIATIONAL AUTO-ENCODERS", "text": "In the context of the CST-UAE model, we are interested in two related problems: Conclusion, with which we derive all latent variables zCi and z _ T in view of model parameters \u03b8 and the image; and Learning, with which we derive the model parameters \u03b8 = {\u03b8C, \u03b8T} in view of a series of images {x (i)} mi = 1. Traditionally for latent variable models such as CST-UAE, these problems could be solved using EM (Dempster et al., 1977) by using approximate conclusions (e.g. loopic propagation of belief, MCMC, or center field) in the E step (see e.g. Wainwright & Jordan (2008)). However, if we want to allow rich expressive parameterization of generative models fC and fT, these approaches become intractable."}, {"heading": "3.2 INFERENCE IN THE ST-VAE MODEL", "text": "We will first focus on how to parameterise the detection network (encoder) for the simpler case of a single-layer Q model (i.e. the ST-VAE model shown in Figure 3 (a), in which we only need to predict a single set of latent variables zC and zT. Naively, one could simply use an ordinary MLP to parameterise a distribution Q (zC, zT | L), but ideally we would use the same insight that we have used for the generative model, namely that it is easier to detect content when we consider it separately. To this end, we propose the ST-VAE detection model shown in Figure 3 (b). Conceptually, the ST-VAE detection model breaks down the prediction of zC and zT in two stages. Given the observed image L, let us first say the latent representation of the pose, zT."}, {"heading": "3.3 INFERENCE IN THE CST-VAE MODEL", "text": "We now turn to the multi-layer CST-UAE model, where the task once again consists of parameterizing the detection model Q. In particular, we want to avoid learning a model that must perform a \"rectilinear\" common prediction of all objects and their poses in an image. Instead, our approach assumes that the latent parameters zCi and zTi are each excluded from consideration for the explanation of a part of the residual image until the last layer has been explained. We proceed recursively: To draw conclusions about the layer Li, we assume that the latent parameters zCi and zTi are responsible for the explanation of a part of the residual image \u2206 i - i.e., the image that was not explained by the layer L1. Li \u2212 1 (note: \u04451 = x). We then use the ST-VAE module (both the decoder and the encoder modules) to generate a reconstruction of the layer i \u2212 Li."}, {"heading": "4 EVALUATION", "text": "In all our experiments, we use the same training settings as in Kingma & Welling (2014), i.e. we use Adagrad for optimization with minibatches of 100 with a learning rate of 0.01 and a weight degradation equivalent to a state of N (0, 1). We initialize the weights in our network using the heuristics of Glorot & Bengio (2010). However, for the pose recognition modules in the ST-VAE model, we found it useful to initialize distortions in a targeted way so that the poses are initially close to identity transformation (see Jaderberg et al. (2015)). We use vanilla VAE models as the base model, first against the ST-VAE model (single image plane), then against the more general CST-VAE model. In all our comparison, we set the training time for all models. We experiment with 20 to 50 dimensions for the latent content variables C and always use 6 dimensions for T."}, {"heading": "4.1 EVALUATING THE ST-VAE ON IMAGES OF SINGLE OBJECTS", "text": "In fact, the fact is that most of them will be able to move, to move and to move."}, {"heading": "4.2 EVALUATING THE CST-VAE ON IMAGES WITH MULTIPLE OVERLAPPING OBJECTS", "text": "This year it is so far that it will only be a matter of time before it is ready, until it is ready."}, {"heading": "5 CONCLUSION", "text": "We have shown how to combine an old idea - interpretive, generative, multi-layered image models - with modern deep learning techniques to tackle the challenging problem of integrating images in the presence of occlusion in a completely unattended way.3 We see this as follows: the random performance of this task is 0.018 (1.8% accuracy), as we require that the image correctly restores both digits within an image, which is a crucial stepping stone for future work on a deeper understanding of scenes that goes beyond simple prediction problems monitored by feedback forward.In the future, we would like to apply our approach to real images and possibly video, which will require an expansion of our methods for using revolutionary networks, and may also require weak oversight (e.g. in the form of observed object class labels associated with layers) or curricula to simplify the learning task."}, {"heading": "ACKNOWLEDGMENTS", "text": "We are grateful to Sergio Guadarrama and Rahul Sukthankar for reading a draft of this essay and providing feedback."}], "references": [], "referenceMentions": [], "year": 2016, "abstractText": "We present a generative model of images based on layering, in which image layers are individually generated, then composited from front to back. We are thus able to factor the appearance of an image into the appearance of individual objects within the image \u2014 and additionally for each individual object, we can factor content from pose. Unlike prior work on layered models, we learn a shape prior for each object/layer, allowing the model to tease out which object is in front by looking for a consistent shape, without needing access to motion cues or any labeled data. We show that ordinary stochastic gradient variational bayes (SGVB), which optimizes our fully differentiable lower-bound on the log-likelihood, is sufficient to learn an interpretable representation of images. Finally we present experiments demonstrating the effectiveness of the model for inferring foreground and background objects in images.", "creator": "LaTeX with hyperref package"}}}