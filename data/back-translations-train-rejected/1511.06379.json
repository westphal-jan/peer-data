{"id": "1511.06379", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "19-Nov-2015", "title": "Dynamic Adaptive Network Intelligence", "abstract": "Accurate representational learning of both the explicit and implicit relationships within data is critical to the ability of machines to perform more complex and abstract reasoning tasks. We describe the efficient weakly supervised learning of such inferences by our Dynamic Adaptive Network Intelligence (DANI) model. We report state-of-the-art results for DANI over question answering tasks in the bAbI dataset that have proved difficult for contemporary approaches to learning representation (Weston et al., 2015).", "histories": [["v1", "Thu, 19 Nov 2015 21:07:27 GMT  (237kb,D)", "http://arxiv.org/abs/1511.06379v1", "8 pages, 2 figures, 3 tables, ICLR 2016 conference paper submission"]], "COMMENTS": "8 pages, 2 figures, 3 tables, ICLR 2016 conference paper submission", "reviews": [], "SUBJECTS": "cs.CL cs.LG", "authors": ["richard searle", "megan bingham-walker"], "accepted": false, "id": "1511.06379"}, "pdf": {"name": "1511.06379.pdf", "metadata": {"source": "CRF", "title": null, "authors": ["Megan Bingham-Walker"], "emails": ["richard@eccentricdata.com", "megan@eccentricdata.com"], "sections": [{"heading": null, "text": "Accurate representative learning of both explicit and implicit relationships within data is critical for the ability of machines to perform more complex and abstract puzzles. We describe the efficient, weakly supervised learning of such conclusions through our Dynamic Adaptive Network Intelligence Model (DANI). We report on recent results for DANI on question-answering tasks in the bAbI dataset that have proved difficult for contemporary approaches to learning representation (Weston et al., 2015)."}, {"heading": "1 INTRODUCTION", "text": "The Facebook bAbI dataset was proposed by Weston et al. (2015) to demonstrate the efficiency of new algorithms for machine reading and understanding. Despite some success with adding a memory component to deep learning models to answer questions, tasks requiring inferences and reasoning remain difficult to solve without sufficient training data and strong monitoring (Weston et al., 2015; 2014; Sukhbaatar et al., 2015a; b; Kumar et al., 2015). We have set out to learn a new type of representation model, which we call Dynamic Adaptive Network Intelligence (DANI). This is a weakly monitored, network-based representation of data with efficient models limitations to allow scalability. We use DANI to perform complex considerations on the observed data to solve the specified bAbI tasks. Preliminary results reported in this paper show that simple system data are less representative of the situation of the previous DANI in order to achieve a more accurate domain graphical representation."}, {"heading": "2 METHODS", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "2.1 THE BABI QUESTION ANSWERING TASKS", "text": "The bAbI dataset consists of a set of 20 questions to answer, consisting of a unique training and test set for each task. Within each set, the data represents a set of text sets that describe a simple contextual area, which we call the \"bAbI world.\" Both the training and test sets are divided into separate sequences of statements that form a particular story. Each story is either finished or interspersed with questions relating to either the current state or historical state of the contemporary story. the bAbI dataset al. (2015) was presented in Weston et al. (b; Kumar et al., 2015) and formed the basis for a variety of recent attempts to develop systems that are capable of performing complex thought tasks (Weston et al., 2015; 2014; Sukhbaatar et al., 2015a; b; Kumar et al., 2015). The bAbI dataset with background research and the accompanying test method is available at Xiruebbaatar et al."}, {"heading": "2.2 DYNAMIC ADAPTIVE NETWORK INTELLIGENCE (DANI)", "text": "Dynamic Adaptive Network Intelligence (DANI) is a general method for learning a dynamic structural representation of the world based on the strength of the contextual connections perceived between discrete data over time. DANI is a graphical system that derives a learned representation of data using similarity distance measurements between individual components of the global model and class subdivisions. Together with Pujara et al. (2013), we find that Jaccard similarity distance measurement works well when scant, high-dimensional data is modeled, although a number of alternative distance measurements can also be successfully implemented within the same model structure (for a detailed survey, see Choi et al. (2010). The most important characteristics of DANI are network representation of unique data and continuous adjustment of model space and evaluation parameters in response to new data. DANI can be completely uncontrolled (with domain representation that is fully implemented during the monitoring process)."}, {"heading": "2.3 DATA REPRESENTATION", "text": "The primary measure of the strength of these relationships is a weighted similarity measurement in the form of a Jaccard coefficient (Jaccard, 1901). Each time a relationship is perceived, the similarity measurement between instances is updated. As new relationships between data instances are perceived, these contextual relationships become DANI. For the supervised learning of each bAbI task, we will construct an undirected graphic model of the type that has become popular through its application to problems in a variety of fields, including: communication technology, social network analysis, neuroscience, biology and geophysical studies (Denli et al, 2014). For each Story S within the bAbI training group, the bAbI domain model B includes a simple, undirected diagram defined by: (i) a non-empty series of links V (B)."}, {"heading": "2.4 TRAINING DANI", "text": "For the bAbI question to answer the tasks, we tested two versions of DANI to demonstrate comparative results: the fully monitored version (DANI-S) was initialized with logical primitives (e.g. the word \"die\" represents a bAbI world entity such as \"bathroom\") and a vocabulary of operative terms from the training data set. According to Weston et al. (2015), there were 150 words in the training data set. These were assigned to broad classes, such as \"name,\" \"item,\" \"form,\" \"place,\" etc. DANI-S used a series of logical primitives to decipher and display the statements contained in the training data. Decoding the semantic statements was undertaken with reference to a basic set of definitive grammatical terms; e.g. \"to,\" \"from\" to, \"\" \"to,\" and \"to.\" The second version of our system (DANI-WS) was weakly superimposed."}, {"heading": "2.5 INFORMATION RETRIEVAL", "text": "The bAbi tasks, which have proven to be the most difficult to perform in a weakly supervised manner in Weston et al. (2015; 2014); Sukhbaatar et al. (2015a; b); Kumar et al. (2015) seem to fall into two categories. Mapping the action data as an undirected Network B acts as a short-term memory for both training and testing purposes. In other applications, we have a gradual emergence and decomposition of entities within the domain space requiring consideration of implicit relationships in the database."}, {"heading": "3 RELATED WORK", "text": "Recent research by Berant et al. (2013; 2014) on graphical approaches to reading comprehension focused on methods of mapping question-and-answer pairs into a static knowledge graph (Kg) of facts using latent logical forms. Instead of attempting to map logical predicates using phrases, DANI tries to learn only those logical primitives required for character extraction from text input. Learned data representation provides a contextual representation independent of the observed data, which is why it is an approach that can be applied to several areas. Hixon et al. (2015) recently described a new system that learns a Kg of open, natural language dialogues using task-driven relationships. Together with the learning approach applied by Hixon et al. (2015), instead of trying to form a fixed Kg of a domain or a reference to prescribed rules (Pujara et al), morality education is learned."}, {"heading": "4 RESULTS", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.1 BASELINES", "text": "We compare our approach with the following recent research results: \u2022 Strengly supervised AM + NG + NL Memory Networks (MemNN) proposed in Weston et al. (2015). \u2022 A standard weakly supervised Long Term Memory (LSTM) model proposed in Weston et al. (2015). \u2022 Weakly supervised end-to-end Memory Networks (MemN2N) proposed in (Sukhbaatar et al., 2015a). \u2022 DANI-S: Fully supervised vocabulary-based model \u2022 DANI-WS: Weak supervised model with logical primitives."}, {"heading": "4.2 RESULTS", "text": "In this context, it should be noted that this project is a project which is, first and foremost, a project."}, {"heading": "5 CONCLUSIONS", "text": "In this paper we described a new approach to learning an adaptive network-based representation model of the bAbI dataset based on the contextual similarity between concepts. We tested the performance of this system against the question-answering tasks and reported on current results. Our preliminary results for the supervised version of the model suggest that this model architecture is an efficient approach to solving a variety of question-answering tasks with low computational effort. The preliminary results of the weakly monitored model, DANI-WS, suggest that this version of our system has the potential to generalize efficiently and with significantly less supervision or computational complexity than other contemporary approaches. Although we report the application of DANI as an independent framework for learning representation, we recognize that our system could be used to condition the input and intermediate layers of neural architectures described in comparable studies (Weston et al., 2015; Sukbaatar, 2015b)."}], "references": [{"title": "Semantic parsing on freebase from question-answer pairs", "author": ["Berant", "Jonathan", "Chou", "Andrew", "Frostig", "Roy", "Liang", "Percy"], "venue": "In EMNLP, pp", "citeRegEx": "Berant et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Berant et al\\.", "year": 2013}, {"title": "Modeling biological processes for reading comprehension", "author": ["Berant", "Jonathan", "Srikumar", "Vivek", "Chen", "Pei-Chun", "Huang", "Brad", "Manning", "Christopher D", "Vander Linden", "Abby", "Harding", "Brittany", "Clark", "Peter"], "venue": "In Proc. EMNLP,", "citeRegEx": "Berant et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Berant et al\\.", "year": 2014}, {"title": "A survey of binary similarity and distance measures", "author": ["Choi", "Seung-Seok", "Cha", "Sung-Hyuk", "Tappert", "Charles C"], "venue": "Journal of Systemics, Cybernetics and Informatics,", "citeRegEx": "Choi et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Choi et al\\.", "year": 2010}, {"title": "Multi-scale graphical models for spatio-temporal processes", "author": ["Denli", "Huseyin", "Subrahmanya", "Niranjan"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Denli et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Denli et al\\.", "year": 2014}, {"title": "Traversing knowledge graphs in vector space", "author": ["Gu", "Kelvin", "Miller", "John", "Liang", "Percy"], "venue": "arXiv preprint arXiv:1506.01094,", "citeRegEx": "Gu et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Gu et al\\.", "year": 2015}, {"title": "Etude comparative de la distribution florale dans une portion des Alpes et du Jura", "author": ["Jaccard", "Paul"], "venue": "Impr. Corbaz,", "citeRegEx": "Jaccard and Paul.,? \\Q1901\\E", "shortCiteRegEx": "Jaccard and Paul.", "year": 1901}, {"title": "Ask me anything: Dynamic memory networks for natural language processing", "author": ["Kumar", "Ankit", "Irsoy", "Ozan", "Su", "Jonathan", "Bradbury", "James", "English", "Robert", "Pierce", "Brian", "Ondruska", "Peter", "Gulrajani", "Ishaan", "Socher", "Richard"], "venue": "arXiv preprint arXiv:1506.07285,", "citeRegEx": "Kumar et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Kumar et al\\.", "year": 2015}, {"title": "Machine learning: a probabilistic perspective", "author": ["Murphy", "Kevin P"], "venue": "MIT press,", "citeRegEx": "Murphy and P.,? \\Q2012\\E", "shortCiteRegEx": "Murphy and P.", "year": 2012}, {"title": "Ontology-aware partitioning for knowledge graph identification", "author": ["Pujara", "Jay", "Miao", "Hui", "Getoor", "Lise", "Cohen", "William W"], "venue": "In Proceedings of the 2013 workshop on Automated knowledge base construction,", "citeRegEx": "Pujara et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Pujara et al\\.", "year": 2013}, {"title": "End-to-end memory networks", "author": ["Sukhbaatar", "Sainbayar", "Szlam", "Arthur", "Weston", "Jason", "Fergus", "Rob"], "venue": "arXiv preprint arXiv:1503.08895,", "citeRegEx": "Sukhbaatar et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Sukhbaatar et al\\.", "year": 2015}, {"title": "Weakly supervised memory networks", "author": ["Sukhbaatar", "Sainbayar", "Szlam", "Arthur", "Weston", "Jason", "Fergus", "Rob"], "venue": "arXiv preprint arXiv:1503.08895,", "citeRegEx": "Sukhbaatar et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Sukhbaatar et al\\.", "year": 2015}, {"title": "Similarity coefficients for binary data: properties of coefficients, coefficient matrices, multi-way metrics and multivariate coefficients", "author": ["Warrens", "Matthijs Joost"], "venue": "Psychometrics and Research", "citeRegEx": "Warrens and Joost,? \\Q2008\\E", "shortCiteRegEx": "Warrens and Joost", "year": 2008}, {"title": "Under review as a conference paper at ICLR", "author": ["Weston", "Jason", "Chopra", "Sumit", "Bordes", "Antoine"], "venue": null, "citeRegEx": "Weston et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Weston et al\\.", "year": 2016}, {"title": "Towards ai-complete question", "author": ["2014. Weston", "Jason", "Bordes", "Antoine", "Chopra", "Sumit", "Mikolov", "Tomas"], "venue": null, "citeRegEx": "Weston et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Weston et al\\.", "year": 2014}], "referenceMentions": [{"referenceID": 6, "context": "Despite some success with adding a memory component to deep learning models for question answering, tasks requiring inference and reasoning remain difficult to solve in the absence of sufficient training data and strong supervision (Weston et al., 2015; 2014; Sukhbaatar et al., 2015a;b; Kumar et al., 2015).", "startOffset": 232, "endOffset": 307}, {"referenceID": 9, "context": "The Facebook bAbI dataset was proposed by Weston et al. (2015) to demonstrate the efficiency of new algorithms for machine reading and comprehension.", "startOffset": 42, "endOffset": 63}, {"referenceID": 6, "context": "(2015) and has provided the basis for a variety of recent attempts to develop systems that are able to satisfy complex reasoning tasks (Weston et al., 2015; 2014; Sukhbaatar et al., 2015a;b; Kumar et al., 2015).", "startOffset": 135, "endOffset": 210}, {"referenceID": 9, "context": "The bAbI dataset was presented in Weston et al. (2015) and has provided the basis for a variety of recent attempts to develop systems that are able to satisfy complex reasoning tasks (Weston et al.", "startOffset": 34, "endOffset": 55}, {"referenceID": 7, "context": "In common with Pujara et al. (2013), we find that the Jaccard similarity distance measure works well when modelling sparse, high-dimensional data, although a range of alternative distance measures can also be implemented successfully within the same model structure (for a detailed survey see Choi et al.", "startOffset": 15, "endOffset": 36}, {"referenceID": 2, "context": "(2013), we find that the Jaccard similarity distance measure works well when modelling sparse, high-dimensional data, although a range of alternative distance measures can also be implemented successfully within the same model structure (for a detailed survey see Choi et al. (2010)).", "startOffset": 264, "endOffset": 283}, {"referenceID": 3, "context": "For supervised learning of each bAbI task; we construct an undirected graphical model of the type that has become popular through its application to problems in a diverse range of fields, including; communications engineering, social network analysis, neuroscience, biology, and geophysical studies (Denli et al., 2014).", "startOffset": 299, "endOffset": 319}, {"referenceID": 2, "context": "This approach is consistent with a variety of binary similarity coefficients (Warrens et al., 2008; Choi et al., 2010) that do not operationalise incidences of mutual absence (represented in Table 1 by quantity d).", "startOffset": 77, "endOffset": 118}, {"referenceID": 12, "context": "According to Weston et al. (2015), there were 150 words in the training set.", "startOffset": 13, "endOffset": 34}, {"referenceID": 6, "context": "(2015a;b); Kumar et al. (2015) appear to fall into two categories.", "startOffset": 11, "endOffset": 31}, {"referenceID": 0, "context": "Recent research by Berant et al. (2013; 2014) on graphical approaches to reading comprehension focused on methods to map question-answer pairs into a static knowledge graph (KG) of facts using latent logical forms. They then use the KG as a memory for information retrieval. Rather than seeking to map from phrases to logical predicates, DANI seeks to learn using only those logical primitives required for feature extraction from the text input. The learned data representation provides a contextual mapping that is independent of the data observed, which is why it is an approach that can be applied to multiple domains. Hixon et al.(2015) recently described a new system that learns a KG from open, natural language dialogs using task-driven relations.", "startOffset": 19, "endOffset": 642}, {"referenceID": 0, "context": "Recent research by Berant et al. (2013; 2014) on graphical approaches to reading comprehension focused on methods to map question-answer pairs into a static knowledge graph (KG) of facts using latent logical forms. They then use the KG as a memory for information retrieval. Rather than seeking to map from phrases to logical predicates, DANI seeks to learn using only those logical primitives required for feature extraction from the text input. The learned data representation provides a contextual mapping that is independent of the data observed, which is why it is an approach that can be applied to multiple domains. Hixon et al.(2015) recently described a new system that learns a KG from open, natural language dialogs using task-driven relations. In common with the approach to learning adopted by Hixon et al.(2015); rather than seeking to build a fixed KG of a domain or reference to prescribed ontolog-", "startOffset": 19, "endOffset": 826}, {"referenceID": 8, "context": "ical rules (Pujara et al., 2013), DANI learns a polymorphic KG continuously, using the similarity distance between the concepts that have been observed.", "startOffset": 11, "endOffset": 32}, {"referenceID": 4, "context": "Gu et al. (2015) recently demonstrated that performing path queries on a knowledge graph is an efficient means of inferring missing information for question answering tasks.", "startOffset": 0, "endOffset": 17}, {"referenceID": 10, "context": "\u2022 Strongly supervised AM+NG+NL Memory Networks (MemNN) proposed in Weston et al. (2015). \u2022 A standard weakly supervised Long Short Term Memory (LSTM) model as reported in Weston et al.", "startOffset": 67, "endOffset": 88}, {"referenceID": 10, "context": "\u2022 Strongly supervised AM+NG+NL Memory Networks (MemNN) proposed in Weston et al. (2015). \u2022 A standard weakly supervised Long Short Term Memory (LSTM) model as reported in Weston et al. (2015). \u2022 Weakly Supervised End-to-End Memory Networks (MemN2N) proposed in (Sukhbaatar et al.", "startOffset": 67, "endOffset": 192}, {"referenceID": 12, "context": "Compared with AM+NG+NL MemoryNetworks (MemNN) as presented in Weston et al. (2015).", "startOffset": 62, "endOffset": 83}, {"referenceID": 6, "context": "Although we report the application of DANI as an independent framework for learning representation, we recognize that our system could be employed to condition the input and intermediate layers of neural architectures of the type reported in comparable studies (Weston et al., 2015; 2014; Sukhbaatar et al., 2015a;b; Kumar et al., 2015).", "startOffset": 261, "endOffset": 336}], "year": 2015, "abstractText": "Accurate representational learning of both the explicit and implicit relationships within data is critical to the ability of machines to perform more complex and abstract reasoning tasks. We describe the efficient weakly supervised learning of such inferences by our Dynamic Adaptive Network Intelligence (DANI) model. We report state-of-the-art results for DANI over question answering tasks in the bAbI dataset that have proved difficult for contemporary approaches to learning representation (Weston et al., 2015).", "creator": "LaTeX with hyperref package"}}}