{"id": "1511.06381", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "19-Nov-2015", "title": "Manifold Regularized Deep Neural Networks using Adversarial Examples", "abstract": "Learning meaningful representations using deep neural networks involves designing efficient training schemes and well-structured networks. Currently, the method of stochastic gradient descent that has a momentum with dropout is one of the most popular training protocols. Based on that, more advanced methods (i.e., Maxout and Batch Normalization) have been proposed in recent years, but most still suffer from performance degradation caused by small perturbations, also known as adversarial examples. To address this issue, we propose manifold regularized networks (MRnet) that utilize a novel training objective function that minimizes the difference between multi-layer embedding results of samples and those adversarial. Our experimental results demonstrated that MRnet is more resilient to adversarial examples and helps us to generalize representations on manifolds. Furthermore, combining MRnet and dropout allowed us to obtain improvements over the best published results for three well-known benchmarks: MNIST, CIFAR-10, and SVHN.", "histories": [["v1", "Thu, 19 Nov 2015 21:10:29 GMT  (7007kb)", "http://arxiv.org/abs/1511.06381v1", "Submitted to ICLR 2016"], ["v2", "Thu, 14 Jan 2016 16:35:11 GMT  (2493kb,D)", "http://arxiv.org/abs/1511.06381v2", "Figure 2, 5, 7, and several descriptions revised"]], "COMMENTS": "Submitted to ICLR 2016", "reviews": [], "SUBJECTS": "cs.LG cs.CV", "authors": ["taehoon lee", "minsuk choi", "sungroh yoon"], "accepted": false, "id": "1511.06381"}, "pdf": {"name": "1511.06381.pdf", "metadata": {"source": "CRF", "title": null, "authors": ["Taehoon Lee", "Minsuk Choi"], "emails": ["taehoonlee@snu.ac.kr", "minsuk0523@snu.ac.kr", "sryoon@snu.ac.kr"], "sections": [{"heading": null, "text": "ar Xiv: 151 1,06 381v 1 [cs.L G] 19 Nov 2"}, {"heading": "1 INTRODUCTION", "text": "In fact, we see ourselves as being able to solve the problems mentioned, and we see ourselves as being able to solve them if we want to solve them, \"he said."}, {"heading": "2 RELATED WORK", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "2.1 DEEP MANIFOLD LEARNING", "text": "Despite the recent success of deep networks, their ultimate goal, the generalization of low-dimensional multiplicities, has not yet been achieved. Researchers believe that multi-layered neural networks could simultaneously learn multiple embedding and classification (Bengio et al., 2013). However, traditional loss concepts such as reconstruction or classification errors are not sufficient to capture local variations of multiplicity. To support the multi-layered hypothesis, we need to apply a different type of cost function that makes neighborhoods a similar representation. Therefore, there have been many attempts to combine both functional concepts of deep and multilayered learning (DIS)."}, {"heading": "2.2 ADVERSARIAL EXAMPLES", "text": "In our study, we include an explicit concept of loss in order to maintain neighborhood relationships in deep neural networks. As previously mentioned, neighborhood information based only on training samples can cause inappropriate embedding. As the samples are in most cases sufficiently densely populated, we generate conflicting examples that come sufficiently close to the original samples for diversity; an example of conflicting disturbance is shown in Figure 2 [retrieved from fellow Goodet al. (2015)]. We can easily obtain these conflicting disturbances by updating a sample instead of parameters, and while the parameter is being updated, the former results in the parameter being incorrect."}, {"heading": "3 MANIFOLD REGULARIZED NETWORKS", "text": "Faced with a data set X = {x1,., xn} with appropriate labels y = {y1,., yn}, we consider the following objective loss function: J (\u03b8; X, y) = L (\u03b8; X, y) + [2), where L (\u00b7) is a loss of classification and (\u00b7), is defined in practice as a maximizing function of a previous p (\u03b8). For the two terms L (\u00b7) and L (\u00b7), cross entropy and L2 weight loss are the most popular choices in a superordinate environment. L2 dekay works well in practice, however, these forms of neural networks have intrinsic blind spots due to the huge number of parameters and linear functions they use. Hence, we propose an additional manifold loss affecting the properties of blind spots.Suppose we have a neural network with L + 1 layers. The last (L + 1) layer is a soft layer."}, {"heading": "4 EXPERIMENTS", "text": "Optimization of the proposed method was performed using standard stochastic gradient descent. Mini-batch size and momentum were set to 100 and 0.9, respectively, and learning rate was annealed as described in Wan et al. (2013). For each section, we present an initial learning rate and three numbers of epochs, such as 0.001 (100-20-10). We trained models at the initial rate for the first number of epochs, then multiplied by 0.1 for the second epochs, followed by 0.5 again for the third epochs. For example, the curriculum denotes 0.001 (100-20-10) a learning rate of 0.001 for 20 epochs and 0.0005 for 10 epochs. We evaluated the proposed regulation using three benchmark datasets: MNIST (LeCun et al., 1998), CIFAR-10 (Krizhevsky & Hinton, 2009) and SVN (SVase, CN 4)."}, {"heading": "4.1 IMPROVED CLASSIFICATION PERFORMANCE", "text": "MNIST The MNIST (LeCun et al., 1998) Fellow is one of the most popular benchmarks and consists of a set of 28 x 28 grayscale images with corresponding labels 0-9. Because these images have a high contrast like binary images, they were typically tested without pre-processing. To obtain desirable representations in the diverse space, we tested two types of models. First, we trained models with two fully interconnected layers, each with 800 hidden units, followed by a Softmax layer. We used a learning curriculum of 0.1 (40-40-20) and achieved a test accuracy of 98.848 \u00b1 0.52%, which the published results show with only two hidden layers. DropConnect et al, 2013) and Dropout (Srivastava et al et al, 2014)."}, {"heading": "4.2 DISENTANGLEMENT AND GENERALIZATION", "text": "In other words, the difference in the activation of the last hidden layer between an original and its adverse sample is minimized. However, a hostile disturbance at the beginning of the training is meaningless. Therefore, we apply the difference in the formation of a neural network with multiple iterations, similar to the model phase of deep faith networks. Fig. 5 shows the variation of a (L) -a (L) -2 over the number of iterations. Iteration 0 indicates the point at which the visualization is applied. As illustrated in Fig. 5, MRnet minimizes the manifold distances more successfully than a vanilla network. The final value of a (L) -a (L) -2 was 3.57. In the network we tested, the number of hidden nodes in the last hidden layer is 1024, and the average difference of individual elements can be calculated as approximately 0.72."}, {"heading": "5 CONCLUSION", "text": "We have proposed a new method that combines deep learning and manifold learning, called Multiply Regulated Networks (MRnet). Traditional neural networks, even state-of-the-art deep networks, exhibit intrinsic blind spots due to a variety of parameters and linear functional components that use them. We tested the MRnet and confirmed its improved generalization performance, which is supported by the proposed concept of diversity on deep architectures. By exploiting the properties of blind spots, the proposed MRnet can be extended to the discovery of true representations of diversity in various learning tasks."}], "references": [{"title": "Representation learning: A review and new perspectives", "author": ["Bengio", "Yoshua", "Courville", "Aaron", "Vincent", "Pierre"], "venue": "IEEE TPAMI,", "citeRegEx": "Bengio et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Bengio et al\\.", "year": 2013}, {"title": "A fuzzy relative of the isodata process and its use in detecting compact wellseparated clusters", "author": ["Dunn", "Joseph C"], "venue": "Journal of cybernetics,", "citeRegEx": "Dunn and C.,? \\Q1973\\E", "shortCiteRegEx": "Dunn and C.", "year": 1973}, {"title": "Explaining and harnessing adversarial examples", "author": ["Goodfellow", "Ian", "Shlens", "Jonathon", "Szegedy", "Christian"], "venue": "In ICLR,", "citeRegEx": "Goodfellow et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Goodfellow et al\\.", "year": 2015}, {"title": "Reducing the dimensionality of data with neural networks", "author": ["Hinton", "Geoffrey E", "R.R. Salakhutdinov"], "venue": null, "citeRegEx": "Hinton et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Hinton et al\\.", "year": 2006}, {"title": "Improving neural networks by preventing co-adaptation of feature detectors", "author": ["Hinton", "Geoffrey E", "Srivastava", "Nitish", "Krizhevsky", "Alex", "Sutskever", "Ilya", "Salakhutdinov", "Ruslan R"], "venue": "arXiv preprint,", "citeRegEx": "Hinton et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Hinton et al\\.", "year": 2012}, {"title": "Batch normalization: Accelerating deep network training by reducing internal covariate shift", "author": ["Ioffe", "Sergey", "Szegedy", "Christian"], "venue": "In ICML,", "citeRegEx": "Ioffe et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Ioffe et al\\.", "year": 2015}, {"title": "What is the best multi-stage architecture for object recognition", "author": ["Jarrett", "Kevin", "Kavukcuoglu", "Koray", "Ranzato", "Marc\u2019Aurelio", "LeCun", "Yann"], "venue": "In ICCV,", "citeRegEx": "Jarrett et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Jarrett et al\\.", "year": 2009}, {"title": "Caffe: Convolutional architecture for fast feature embedding", "author": ["Jia", "Yangqing", "Shelhamer", "Evan", "Donahue", "Jeff", "Karayev", "Sergey", "Long", "Jonathan", "Girshick", "Ross", "Guadarrama", "Sergio", "Darrell", "Trevor"], "venue": "In Proceedings of the ACM International Conference on Multimedia,", "citeRegEx": "Jia et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Jia et al\\.", "year": 2014}, {"title": "Learning multiple layers of features from tiny images", "author": ["Krizhevsky", "Alex", "Hinton", "Geoffrey"], "venue": null, "citeRegEx": "Krizhevsky et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Krizhevsky et al\\.", "year": 2009}, {"title": "Imagenet classification with deep convolutional neural networks", "author": ["Krizhevsky", "Alex", "Sutskever", "Ilya", "Hinton", "Geoffrey E"], "venue": "In NIPS,", "citeRegEx": "Krizhevsky et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Krizhevsky et al\\.", "year": 2012}, {"title": "Gradient-based learning applied to document recognition", "author": ["LeCun", "Yann", "Bottou", "Leon", "Bengio", "Yoshua", "Haffner", "Patrick"], "venue": "Proceedings of the IEEE,", "citeRegEx": "LeCun et al\\.,? \\Q1998\\E", "shortCiteRegEx": "LeCun et al\\.", "year": 1998}, {"title": "Multi-manifold deep metric learning for image set classification", "author": ["Lu", "Jiwen", "Wang", "Gang", "Deng", "Weihong", "Moulin", "Pierre", "Zhou", "Jie"], "venue": "In CVPR,", "citeRegEx": "Lu et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Lu et al\\.", "year": 2015}, {"title": "Reading digits in natural images with unsupervised feature learning. In NIPS workshop on deep learning and unsupervised feature learning", "author": ["Netzer", "Yuval", "Wang", "Tao", "Coates", "Adam", "Bissacco", "Alessandro", "Wu", "Bo", "Ng", "Andrew Y"], "venue": null, "citeRegEx": "Netzer et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Netzer et al\\.", "year": 2011}, {"title": "Deep neural networks are easily fooled: High confidence predictions for unrecognizable images", "author": ["Nguyen", "Anh", "Yosinski", "Jason", "Clune", "Jeff"], "venue": "In CVPR,", "citeRegEx": "Nguyen et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Nguyen et al\\.", "year": 2015}, {"title": "Learning to disentangle factors of variation with manifold interaction", "author": ["Reed", "Scott", "Sohn", "Kihyuk", "Zhang", "Yuting", "Lee", "Honglak"], "venue": "In ICML,", "citeRegEx": "Reed et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Reed et al\\.", "year": 2014}, {"title": "Silhouettes: a graphical aid to the interpretation and validation of cluster analysis", "author": ["Rousseeuw", "Peter J"], "venue": "Journal of computational and applied mathematics,", "citeRegEx": "Rousseeuw and J.,? \\Q1987\\E", "shortCiteRegEx": "Rousseeuw and J.", "year": 1987}, {"title": "Nonlinear dimensionality reduction by locally linear embedding", "author": ["S.T. Roweis", "L.K. Saul"], "venue": null, "citeRegEx": "Roweis and Saul,? \\Q2000\\E", "shortCiteRegEx": "Roweis and Saul", "year": 2000}, {"title": "Dropout: A simple way to prevent neural networks from overfitting", "author": ["Srivastava", "Nitish", "Hinton", "Geoffrey", "Krizhevsky", "Alex", "Sutskever", "Ilya", "Salakhutdinov", "Ruslan"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Srivastava et al\\.,? \\Q1929\\E", "shortCiteRegEx": "Srivastava et al\\.", "year": 1929}, {"title": "Intriguing properties of neural networks", "author": ["Szegedy", "Christian", "Zaremba", "Wojciech", "Sutskever", "Ilya", "Bruna", "Joan", "Erhan", "Dumitru", "Goodfellow", "Ian", "Fergus", "Rob"], "venue": "In ICLR,", "citeRegEx": "Szegedy et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Szegedy et al\\.", "year": 2014}, {"title": "Going deeper with convolutions", "author": ["Szegedy", "Christian", "Liu", "Wei", "Jia", "Yangqing", "Sermanet", "Pierre", "Reed", "Scott", "Anguelov", "Dragomir", "Erhan", "Dumitru", "Vanhoucke", "Vincent", "Rabinovich", "Andrew"], "venue": "In CVPR,", "citeRegEx": "Szegedy et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Szegedy et al\\.", "year": 2015}, {"title": "A global geometric framework for nonlinear dimensionality reduction", "author": ["Tenenbaum", "Joshua B", "De Silva", "Vin", "Langford", "John C"], "venue": null, "citeRegEx": "Tenenbaum et al\\.,? \\Q2000\\E", "shortCiteRegEx": "Tenenbaum et al\\.", "year": 2000}, {"title": "Manifold regularized deep neural networks", "author": ["Tomar", "Vikrant Singh", "Rose", "Richard C"], "venue": "In Proceedings of InterSpeech, pp", "citeRegEx": "Tomar et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Tomar et al\\.", "year": 2014}, {"title": "Visualizing data using t-sne", "author": ["Van der Maaten", "Laurens", "Hinton", "Geoffrey"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Maaten et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Maaten et al\\.", "year": 2008}, {"title": "Regularization of neural networks using dropconnect", "author": ["Wan", "Li", "Zeiler", "Matthew D", "Zhang", "Sixin", "LeCun", "Yann", "Fergus", "Robert"], "venue": "In ICML,", "citeRegEx": "Wan et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Wan et al\\.", "year": 2013}, {"title": "Scene recognition by manifold regularized deep learning architecture", "author": ["Y. Yuan", "L. Mou", "X. Lu"], "venue": "IEEE TNNLS,", "citeRegEx": "Yuan et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Yuan et al\\.", "year": 2015}, {"title": "Stochastic pooling for regularization of deep convolutional neural networks", "author": ["Zeiler", "Matthew", "Fergus", "Robert"], "venue": "In ICLR,", "citeRegEx": "Zeiler et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Zeiler et al\\.", "year": 2013}], "referenceMentions": [{"referenceID": 9, "context": "Deep neural networks have been used successfully to learn meaningful representations on a variety of tasks (Hinton & Salakhutdinov, 2006; Krizhevsky et al., 2012).", "startOffset": 107, "endOffset": 162}, {"referenceID": 23, "context": ", 2014), DropConnect (Wan et al., 2013), and Batch Normalization (Ioffe & Szegedy, 2015)] and well-structured networks [i.", "startOffset": 21, "endOffset": 39}, {"referenceID": 19, "context": ", 2014), and Inception (Szegedy et al., 2015)].", "startOffset": 23, "endOffset": 45}, {"referenceID": 18, "context": "The perturbations, which are barely perceptible to humans but make neural networks easily less confident, are called adversarial examples (Szegedy et al., 2014; Goodfellow et al., 2015; Nguyen et al., 2015).", "startOffset": 138, "endOffset": 206}, {"referenceID": 2, "context": "The perturbations, which are barely perceptible to humans but make neural networks easily less confident, are called adversarial examples (Szegedy et al., 2014; Goodfellow et al., 2015; Nguyen et al., 2015).", "startOffset": 138, "endOffset": 206}, {"referenceID": 13, "context": "The perturbations, which are barely perceptible to humans but make neural networks easily less confident, are called adversarial examples (Szegedy et al., 2014; Goodfellow et al., 2015; Nguyen et al., 2015).", "startOffset": 138, "endOffset": 206}, {"referenceID": 2, "context": "This phenomenon occurs due to fewer training examples than parameters and the inner products of high-dimensional vectors (Goodfellow et al., 2015).", "startOffset": 121, "endOffset": 146}, {"referenceID": 2, "context": ", 2014; Goodfellow et al., 2015; Nguyen et al., 2015). This phenomenon occurs due to fewer training examples than parameters and the inner products of high-dimensional vectors (Goodfellow et al., 2015). In the case of fully connected layers, activation grows by \u01ebn in the worst-case, when n components of an inner product are changed by \u01eb. As shown in Fig. 1(a), decision boundaries constructed by deep networks are wiggly and sensitive to adversarial perturbations. In Goodfellow et al. (2015) in particular, adversarial training was proposed to minimize classification loss both on given samples and adversarial examples.", "startOffset": 8, "endOffset": 495}, {"referenceID": 20, "context": ", ISOMAP (Tenenbaum et al., 2000), Locally Linear Embedding (Roweis & Saul, 2000), and t-SNE (Van der Maaten & Hinton, 2008)] are well-formulated to find transformation preserving geodesic distances in high-dimensional space, they demonstrate limited performance in practice because of insufficient nearest training samples (Fig.", "startOffset": 9, "endOffset": 33}, {"referenceID": 14, "context": "There have been many attempts (Reed et al., 2014; Tomar & Rose, 2014; Yuan et al., 2015) to unify deep learning and manifold learning.", "startOffset": 30, "endOffset": 88}, {"referenceID": 24, "context": "There have been many attempts (Reed et al., 2014; Tomar & Rose, 2014; Yuan et al., 2015) to unify deep learning and manifold learning.", "startOffset": 30, "endOffset": 88}, {"referenceID": 0, "context": "Researchers anticipate that neural networks with multiple layers could learn a manifold embedding and classifier simultaneously (Bengio et al., 2013).", "startOffset": 128, "endOffset": 149}, {"referenceID": 24, "context": "For example, Locally Linear Embedding (Roweis & Saul, 2000) was used as a base unit for a deep architecture (Yuan et al., 2015).", "startOffset": 108, "endOffset": 127}, {"referenceID": 0, "context": "Researchers anticipate that neural networks with multiple layers could learn a manifold embedding and classifier simultaneously (Bengio et al., 2013). However, traditional loss terms, such as a reconstruction or classification error, are not sufficient to capture local variations on manifolds. To support the manifold hypothesis, we need to employ another type of cost function that makes neighborhoods have a similar representation. Hence, there have been many attempts to combine both functional concepts of deep and manifold learning. Attempts to unify deep and manifold learning can be divided into two categories: manifold learningbased unifying and deep learning-based unifying. The former finds a hierarchical manifold embedding with layer-wise manifold learning in the same way as pre-training in deep belief networks. For example, Locally Linear Embedding (Roweis & Saul, 2000) was used as a base unit for a deep architecture (Yuan et al., 2015). The latter incorporates an additional objective from the manifold learning perspective into a framework of deep learning. For instance, the following objective was proposed in Reed et al. (2014): \u2211", "startOffset": 129, "endOffset": 1152}, {"referenceID": 11, "context": ", Lu et al. (2015)] are also possible; however, variations still have limitations inherited from", "startOffset": 2, "endOffset": 19}, {"referenceID": 19, "context": "Figure 2: An example of adversarial perturbation applied to GoogLeNet (Szegedy et al., 2015) [retrieved from Goodfellow et al.", "startOffset": 70, "endOffset": 92}, {"referenceID": 2, "context": ", 2015) [retrieved from Goodfellow et al. (2015)].", "startOffset": 24, "endOffset": 49}, {"referenceID": 0, "context": "Training set neighborhood information may be problematic because most nearest samples have too little in common in high-dimensional Euclidean spaces (Bengio et al., 2013).", "startOffset": 149, "endOffset": 170}, {"referenceID": 9, "context": "The problem of adversarial perturbations arises in several learning models as well as state-of-the-art deep networks, such as AlexNet (Krizhevsky et al., 2012) or GoogLeNet (Szegedy et al.", "startOffset": 134, "endOffset": 159}, {"referenceID": 19, "context": ", 2012) or GoogLeNet (Szegedy et al., 2015).", "startOffset": 21, "endOffset": 43}, {"referenceID": 18, "context": "This suggests that adversarial examples expose fundamental blind spots in our objective functions (Szegedy et al., 2014).", "startOffset": 98, "endOffset": 120}, {"referenceID": 2, "context": "2 [retrieved from Goodfellow et al. (2015)].", "startOffset": 18, "endOffset": 43}, {"referenceID": 10, "context": "We evaluated the proposed regularization on three benchmark datasets: MNIST (LeCun et al., 1998), CIFAR-10 (Krizhevsky & Hinton, 2009), and SVHN (Netzer et al.", "startOffset": 76, "endOffset": 96}, {"referenceID": 12, "context": ", 1998), CIFAR-10 (Krizhevsky & Hinton, 2009), and SVHN (Netzer et al., 2011), as depicted in Fig.", "startOffset": 56, "endOffset": 77}, {"referenceID": 7, "context": "Our implementation was based on Caffe (Jia et al., 2014), which is one of the most popular deep learning frameworks.", "startOffset": 38, "endOffset": 56}, {"referenceID": 4, "context": "We added new forward-backward steps and customized four types of layers: convolutional, pooling, response normalization (Hinton et al., 2012), and fully-connected layers.", "startOffset": 120, "endOffset": 141}, {"referenceID": 17, "context": "The learning rate was annealed as described in Wan et al. (2013). For each subsection, we present an initial learning rate and three numbers of epochs, such as 0.", "startOffset": 47, "endOffset": 65}, {"referenceID": 2, "context": "In the case of the CIFAR and SVHN, we preprocessed the data using zero-phase component analysis (ZCA) whitening and local contrast normalization; the same techniques as Goodfellow et al. (2013) and Zeiler & Fergus (2013), respectively.", "startOffset": 169, "endOffset": 194}, {"referenceID": 2, "context": "In the case of the CIFAR and SVHN, we preprocessed the data using zero-phase component analysis (ZCA) whitening and local contrast normalization; the same techniques as Goodfellow et al. (2013) and Zeiler & Fergus (2013), respectively.", "startOffset": 169, "endOffset": 221}, {"referenceID": 23, "context": "DropConnect (Wan et al., 2013) 99.", "startOffset": 12, "endOffset": 30}, {"referenceID": 6, "context": "53 Lasso in F-layers (Jarrett et al., 2009) 99.", "startOffset": 21, "endOffset": 43}, {"referenceID": 23, "context": "DropConnect (Wan et al., 2013) 2C + 2L + 1F 90.", "startOffset": 12, "endOffset": 30}, {"referenceID": 10, "context": "MNIST The MNIST (LeCun et al., 1998) dataset is one of the most popular benchmarks and consists of a set of 28\u00d7 28 grayscale images with corresponding labels 0\u20139.", "startOffset": 16, "endOffset": 36}, {"referenceID": 23, "context": "DropConnect (Wan et al., 2013) and dropout (Srivastava et al.", "startOffset": 12, "endOffset": 30}, {"referenceID": 2, "context": "01) similar to Goodfellow et al. (2013). To be consistent with previous work, we evaluated our method with 24 \u00d7 24 random cropping and horizontal flipping augmentation.", "startOffset": 15, "endOffset": 40}, {"referenceID": 12, "context": "SVHN The SVHN (Netzer et al., 2011) dataset is composed of 604,388 training and 26,032 test images of 10 classes.", "startOffset": 14, "endOffset": 35}, {"referenceID": 12, "context": "SVHN The SVHN (Netzer et al., 2011) dataset is composed of 604,388 training and 26,032 test images of 10 classes. Following Zeiler & Fergus (2013), we conducted local contrast normalization with a 3 \u00d7 3 filter and 28 \u00d7 28 random cropping.", "startOffset": 15, "endOffset": 147}], "year": 2015, "abstractText": "Learning meaningful representations using deep neural networks involves designing efficient training schemes and well-structured networks. Currently, the method of stochastic gradient descent that has a momentum with dropout is one of the most popular training protocols. Based on that, more advanced methods (i.e., Maxout and Batch Normalization) have been proposed in recent years, but most still suffer from performance degradation caused by small perturbations, also known as adversarial examples. To address this issue, we propose manifold regularized networks (MRnet) that utilize a novel training objective function that minimizes the difference between multi-layer embedding results of samples and those adversarial. Our experimental results demonstrated that MRnet is more resilient to adversarial examples and helps us to generalize representations on manifolds. Furthermore, combining MRnet and dropout allowed us to obtain improvements over the best published results for three well-known benchmarks: MNIST, CIFAR-10, and SVHN.", "creator": "LaTeX with hyperref package"}}}