{"id": "1511.06388", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "19-Nov-2015", "title": "sense2vec - A Fast and Accurate Method for Word Sense Disambiguation In Neural Word Embeddings", "abstract": "Neural word representations have proven useful in Natural Language Processing (NLP) tasks due to their ability to efficiently model complex semantic and syntactic word relationships. However, most techniques model only one representation per word, despite the fact that a single word can have multiple meanings or \"senses\". Some techniques model words by using multiple vectors that are clustered based on context. However, recent neural approaches rarely focus on the application to a consuming NLP algorithm. Furthermore, the training process of recent word-sense models is expensive relative to single-sense embedding processes. This paper presents a novel approach which addresses these concerns by modeling multiple embeddings for each word based on supervised disambiguation, which provides a fast and accurate way for a consuming NLP model to select a sense-disambiguated embedding. We demonstrate that these embeddings can disambiguate both contrastive senses such as nominal and verbal senses as well as nuanced senses such as sarcasm. We further evaluate Part-of-Speech disambiguated embeddings on neural dependency parsing, yielding a greater than 8% average error reduction in unlabeled attachment scores across 6 languages.", "histories": [["v1", "Thu, 19 Nov 2015 21:22:42 GMT  (94kb,D)", "http://arxiv.org/abs/1511.06388v1", null]], "reviews": [], "SUBJECTS": "cs.CL cs.LG", "authors": ["andrew trask", "phil michalak", "john liu"], "accepted": false, "id": "1511.06388"}, "pdf": {"name": "1511.06388.pdf", "metadata": {"source": "CRF", "title": "A FAST AND ACCURATE METHOD FOR WORD SENSE DISAMBIGUATION IN NEURAL WORD EMBEDDINGS", "authors": ["Andrew Trask", "John Liu"], "emails": ["andrew.trask@digitalreasoning.com", "phil.michalak@digitalreasoning.com", "john.liu@digitalreasoning.com"], "sections": [{"heading": "1 INTRODUCTION", "text": "NLP systems attempt to automate the extraction of information from human language. A central challenge in this task is the complexity and scarcity in natural language, leading to a phenomenon known as the curse of dimensionality. To overcome this, the most recent work has experienced real appreciation, distributed representations of words using neural networks (G.E. Hinton, 1986; Bengio et al., 2003; Morin & Bengio, 2005; Mnih & Hinton, 2009). These \"neural language models\" embed a vocabulary in a smaller dimensional linear space that \"captures the probability function for word sequences expressed in relation to these representations.\" (Bengio et al., 2003) The result is a vector-space model (VSM) that represents word meanings with vectors that capture the semantic and syntactic information of words (Maas & Ng, 2010)."}, {"heading": "2 RELATED WORK", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "2.1 WORD2VEC", "text": "Mikolov et al. (2013a) proposed two simple methods for learning continuous word embedding using neural networks, based on Skip-gram or Continuous Bag-of-Word (CBOW) models, and called them word2vec. Word vectors built from these methods map words into points in space that effectively encode semantic and syntactic meanings while ignoring information about the word order. In addition, the word vectors exhibited certain algebraic relationships, as shown in the example: \"v [man] - v [king] + v [queen] \u2248 v [woman].\" Subsequent work using such neural word embedding has proven effective in a variety of tasks for modeling natural language (Al-Rfou et al., 2013; Al-Rfou et al., 2015; Chen & Manning, 2014)."}, {"heading": "2.2 WANG2VEC", "text": "Since word embedding in word2vec is insensitive to word order, it is suboptimal when used for syntactic tasks such as POS tagging or dependency sparsing. Ling et al. (2015) proposed changes to word2vec that incorporated word order. As these models consist of structured skip programs and continuous window methods, collectively referred to as wang2vec, they show significant capabilities for modeling syntactic representations. However, they come at the expense of computing speed. Furthermore, the method is unable to model polysemic words with multiple meanings, since words in wang2vec have a single vector representation. For example, the word \"work\" in the sentence \"We saw their work\" may be either a verb or a noun, depending on the broader context in which the sentence is enclosed. This technique encodes the coding statistics for each sense of a word into one or more fixed word embedding multiple dimensions, resulting from it."}, {"heading": "2.3 STATISTICAL MULTI-PROTOTYPE VECTOR-SPACE MODELS OF WORD MEANING", "text": "Reisinger & Mooney's approach (2010) may be a groundbreaking work on vector-space-sense-disambiguation, creating a vector-space model that encodes multiple meanings of words by first clustering the contexts in which a word appears. Once the contexts are bundled, multiple prototype vectors can be initialized by averaging the statistically generated vectors for each word in the cluster. This process of calculating clusters and creating embedding based on a vector for each cluster has become a canonical strategy for word-sense-disambiguity in vector spaces, but does not represent a strategy for context-specific selection of potentially many vectors that can be used in an NLP classifier."}, {"heading": "2.4 CLUSTERING WEIGHTED AVERAGE CONTEXT EMBEDDINGS", "text": "Our technique is inspired by the work of Huang et al. (2012), which uses a multi-prototype neural vector space model that summarizes contexts for generating prototypes. In contrast to Reisinger & Mooney (2010), context embedding is generated by a neural network in the following way: In a pre-trained word embedding model, each context embedding is generated by calculating a weighted sum of words in the context (weighted by tf-idf); then, for each term, the associated context embedding is clustered; the clusters are used to re-label each occurrence of each word in the corpus.After these terms are re-labeled with the number of clusters, a new word model is trained on the labeled embedding (each with a different vector), thereby generating the word-sense embedding. In addition to the selection problem and the cluster effort, which is also duplicated in the previous necessity described in the model."}, {"heading": "2.5 CLUSTERING CONVOLUTIONAL CONTEXT EMBEDDINGS", "text": "Contrary to previous approaches, Chen et al. (2015) selects the number of word clusters for each word based on the number of definitions for a word in WordNet Gloss (as opposed to other approaches, which usually select a fixed number of clusters). A variant of Neelakantan et al. (2015) \"s MSSG model, this work uses the WordNet glossary dataset and conventional embeddings to initialize the word prototypes. In addition to the selection problem, clustering, and the need to train neural embeddings several times, this higher-quality model is somewhat limited by the vocabulary of the English WordNet resource. Furthermore, the majority of WordNets relationships combine words from the same part of the language (POS)."}, {"heading": "3 THE SENSE2VEC MODEL", "text": "We extend the work of Huang et al. (2012) by using monitored NLP labels instead of unattended clusters to determine the meaning of a particular word instance, eliminating the need to train embedding multiple times, the need for a cluster step, and an efficient method by which a supervised classifier can consume the corresponding word embedding. If a marked corpus (either manually or by a model) is specified with one or more labels per word, the sense2vec model first counts the number of applications (where a unique word map is trained by one or more than 1https: / / wordnet.princeton.edu / labels / uses) of each word and generates a random \"embedding of meaning\" for each use. A model is then trained using the CBOW, Skip-gram, or Structured Skip-gram model configurations. Instead of predicting a token, the given model gives a surrounding token."}, {"heading": "3.1 SUBJECTIVE EVALUATION - SUBJECTIVE BASELINE", "text": "To subjectively evaluate these word embeddings, we trained models using multiple sets of data for comparison. First, we trained the application of the Word2vec Continuous Bag of Words 2 approach on the large, unlabeled corpus used for Google Word Analogy Task 3. Multiple word embeddings and their narrowest terms, measured by cosinal similarity, are displayed in Table 1 below. Note in this table that the \"bank\" column is similar to proper names (\"hsbc,\" \"citibank\"), verbs (\"lending,\" \"banking\"), and nouns (\"banks,\" \"\" lender \"), because the term\" bank \"is used in three different ways, as proper name, verb, and noun. This embedding for\" bank \"has modeled a mixture of these three meanings.\" apple, \"\" \"so,\" \"\" \"perfect,\" and \"may also have a mixture of meanings. In some cases,\" like \"apple,\" the apple \"is also not well represented.\""}, {"heading": "3.2 SUBJECTIVE EVALUATION - PART-OF-SPEECH DISAMBIGUATION", "text": "For Part-of-Speech Disambiguation, we designated the data set from Section 3.1 with Part-of-Speech tags using the Polyglot Universal Dependency Part-of-Speech Tagger by Al-Rfou et al. (2013) and trained sense2vec with parameters identical to Section 3.1. In Table 2, we see that this method has successfully decoded the difference between the noun \"apple,\" which refers to the fruit, and the proper word \"apple,\" which refers to the company. In Table 3, we see that all three uses of the word \"bank\" are distinct by their respective speech proportions, and in Table 4 also nuanced senses of the word \"so.\""}, {"heading": "3.3 SUBJECTIVE EVALUATION - SENTIMENT DISAMBIGUATION", "text": "To define the sensation, the IMDB-marked training corpus was labeled with part-of-speech tags using the Polyglot part-of-speech tagger from Al-Rfou et al. (2013). Adjectives were then labeled with the positive or negative mood associated with each comment. Subsequently, a CBOW sense2vec model was trained based on the resulting data set, which clearly distinguishes between part-of-speech and sentiment (for adjectives). Table 5 shows the difference between positive and negative vectors for the word \"bad.\" The negative vector most closely resembles the word that indicates the classical meaning of \"bad\" (including the negative version of \"good,\" e.g. \"good sorrow!\"). The positive \"bad\" vector indicates a tone of sarcasm that most closely relates to the positive sense of \"good\" (e.g. \"good work!\"). Table 6 shows the positive and negative senses of \"it perfectly.\""}, {"heading": "4 NAMED ENTITY RESOLUTION", "text": "To evaluate the embedding in the disambiguation of Named Entity Resolution (NER), we designated the standard word2vec dataset from Section 3.2 with Named Entity Labels. This showed how sense2vec can also distinguish between multiword text sequences and individual text sequences. In the following, we see that the word \"Washington\" is unique with both a PERSON and a GPE sense of the word. Furthermore, we see that Hillary Clinton is very similar to titles she held within the time span of the dataset."}, {"heading": "5 NEURAL DEPENDENCY PARSING", "text": "For the quantitative evaluation of unambiguous embedding of meaning relative to the current standard, we compared sense2vec embedding and wang2vec embedding on neural syntactic dependencies parsing tasks in six languages. First, we trained two sets of embedding using the Bulgarian, German, English, French, Italian and Swedish Wikipedia datasets from the Polyglot website4. Basic embedding was trained without language-specific part-of-speech disambiguation using the structured skip-gram approach of Ling et al. (2015). For each language, the sense2vec embedding was trained using unambiguous terms using the language-specific part-of-speech conference by Al-Rfou et al. (2013), and embedded in the same structured skip-gram approach. Both were trained with identical parametric risks of the language."}, {"heading": "6 CONCLUSION AND FUTURE WORK", "text": "In this paper, we have proposed a new model for word sense disambiguation that uses the use of a term through monitored NLP labeling to disambiguate between sense of words. Similar to previous models, it uses a form of context clustering to disambiguate the use of a term. However, rather than using uncontrolled cluster methods, our approach clusters use monitored labels that can analyze the context of a particular word and assign a label to it. This greatly reduces the computational effort of word sense modeling and provides a natural mechanism for other NLP tasks to select the appropriate embedding of meaning. Furthermore, we show that ambiguous embedding can increase the accuracy of syntactic dependence that parses in a variety of languages."}], "references": [{"title": "Polyglot: Distributed word representations for multilingual NLP", "author": ["Al-Rfou", "Rami", "Perozzi", "Bryan", "Skiena", "Steven"], "venue": "CoRR, abs/1307.1662,", "citeRegEx": "Al.Rfou et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Al.Rfou et al\\.", "year": 2013}, {"title": "Polyglot-NER: Massive multilingual named entity recognition", "author": ["Al-Rfou", "Rami", "Kulkarni", "Vivek", "Perozzi", "Bryan", "Skiena", "Steven"], "venue": "Proceedings of the 2015 SIAM International Conference on Data Mining,", "citeRegEx": "Al.Rfou et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Al.Rfou et al\\.", "year": 2015}, {"title": "A neural probabilistic language model", "author": ["Bengio", "Yoshua", "Ducharme", "R\u00e9jean", "Vincent", "Pascal", "Janvin", "Christian"], "venue": "J. Mach. Learn. Res.,", "citeRegEx": "Bengio et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Bengio et al\\.", "year": 2003}, {"title": "A fast and accurate dependency parser using neural networks", "author": ["Chen", "Danqi", "Manning", "Christopher"], "venue": "In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP),", "citeRegEx": "Chen et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Chen et al\\.", "year": 2014}, {"title": "Improving distributed representation of word sense via wordnet gloss composition and context clustering", "author": ["Chen", "Tao", "Xu", "Ruifeng", "He", "Yulan", "Wang", "Xuan"], "venue": "In Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Volume 2: Short Papers),", "citeRegEx": "Chen et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Chen et al\\.", "year": 2015}, {"title": "Distributed representations. Parallel dis-tributed processing: Explorations in the microstructure of cognition", "author": ["G.E. Hinton", "J.L. McClelland", "D.E. Rumelhart"], "venue": null, "citeRegEx": "Hinton et al\\.,? \\Q1986\\E", "shortCiteRegEx": "Hinton et al\\.", "year": 1986}, {"title": "Improving word representations via global context and multiple word prototypes. In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics: Long Papers - Volume 1, ACL \u201912", "author": ["Huang", "Eric H", "Socher", "Richard", "Manning", "Christopher D", "Ng", "Andrew Y"], "venue": null, "citeRegEx": "Huang et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Huang et al\\.", "year": 2012}, {"title": "Bringing machine learning and compositional semantics together", "author": ["P. Liang", "C. Potts"], "venue": "Annual Reviews of Linguistics,", "citeRegEx": "Liang and Potts,? \\Q2015\\E", "shortCiteRegEx": "Liang and Potts", "year": 2015}, {"title": "Two/too simple adaptations of word2vec for syntax problems", "author": ["Ling", "Wang", "Dyer", "Chris", "Black", "Alan W", "Trancoso", "Isabel"], "venue": "In Proceedings of the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies,", "citeRegEx": "Ling et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Ling et al\\.", "year": 2015}, {"title": "A probabilistic model for semantic word vectors", "author": ["Maas", "Andrew L", "Ng", "Andrew Y"], "venue": "In NIPS Workshop on Deep Learning and Unsupervised Feature Learning,", "citeRegEx": "Maas et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Maas et al\\.", "year": 2010}, {"title": "Efficient estimation of word representations in vector space", "author": ["Mikolov", "Tomas", "Chen", "Kai", "Corrado", "Greg", "Dean", "Jeffrey"], "venue": "CoRR, abs/1301.3781,", "citeRegEx": "Mikolov et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Exploiting similarities among languages for machine translation", "author": ["Mikolov", "Tomas", "Le", "Quoc V", "Sutskever", "Ilya"], "venue": "CoRR, abs/1309.4168,", "citeRegEx": "Mikolov et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Distributed representations of words and phrases and their compositionality", "author": ["Mikolov", "Tomas", "Sutskever", "Ilya", "Chen", "Kai", "Corrado", "Greg", "Dean", "Jeffrey"], "venue": "CoRR, abs/1310.4546,", "citeRegEx": "Mikolov et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "A scalable hierarchical distributed language model", "author": ["Mnih", "Andriy", "Hinton", "Geoffrey E"], "venue": "Advances in Neural Information Processing Systems", "citeRegEx": "Mnih et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Mnih et al\\.", "year": 2009}, {"title": "Hierarchical probabilistic neural network language model", "author": ["Morin", "Frederic", "Bengio", "Yoshua"], "venue": "In Proceedings of the international workshop on artificial intelligence and statistics,", "citeRegEx": "Morin et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Morin et al\\.", "year": 2005}, {"title": "Efficient nonparametric estimation of multiple embeddings per word in vector space", "author": ["Neelakantan", "Arvind", "Shankar", "Jeevan", "Passos", "Alexandre", "McCallum", "Andrew"], "venue": "CoRR, abs/1504.06654,", "citeRegEx": "Neelakantan et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Neelakantan et al\\.", "year": 2015}, {"title": "Multi-prototype vector-space models of word meaning", "author": ["Reisinger", "Joseph", "Mooney", "Raymond J"], "venue": "In Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics,", "citeRegEx": "Reisinger et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Reisinger et al\\.", "year": 2010}, {"title": "Under review as a conference paper at ICLR", "author": ["Trask", "Andrew", "Gilmore", "David", "Russell", "Matthew"], "venue": null, "citeRegEx": "Trask et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Trask et al\\.", "year": 2016}, {"title": "Character-level convolutional networks for text", "author": ["Zhang", "Xiang", "Zhao", "Junbo", "LeCun", "Yann"], "venue": "scale. CoRR,", "citeRegEx": "Zhang et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Zhang et al\\.", "year": 2015}], "referenceMentions": [{"referenceID": 2, "context": "To overcome this, recent work has learned real valued, distributed representations for words using neural networks (G.E. Hinton, 1986; Bengio et al., 2003; Morin & Bengio, 2005; Mnih & Hinton, 2009).", "startOffset": 115, "endOffset": 198}, {"referenceID": 2, "context": "These \u201dneural language models\u201d embed a vocabulary into a smaller dimensional linear space that models \u201dthe probability function for word sequences, expressed in terms of these representations\u201d (Bengio et al., 2003).", "startOffset": 193, "endOffset": 214}, {"referenceID": 0, "context": "Various forms of distributed representations have shown to be useful for a wide variety of NLP tasks including Part-of-Speech tagging, Named Entity Recognition, Analogy/Similarity Querying, Transliteration, and Dependency Parsing (Al-Rfou et al., 2013; Al-Rfou et al., 2015; Mikolov et al., 2013a;b; Chen & Manning, 2014).", "startOffset": 230, "endOffset": 321}, {"referenceID": 1, "context": "Various forms of distributed representations have shown to be useful for a wide variety of NLP tasks including Part-of-Speech tagging, Named Entity Recognition, Analogy/Similarity Querying, Transliteration, and Dependency Parsing (Al-Rfou et al., 2013; Al-Rfou et al., 2015; Mikolov et al., 2013a;b; Chen & Manning, 2014).", "startOffset": 230, "endOffset": 321}, {"referenceID": 8, "context": "Extensive research has been done to tune these embeddings to various tasks by incorporating features such as character (compositional) information, word order information, and multi-word (phrase) information (Ling et al., 2015; Mikolov et al., 2013c; Zhang et al., 2015; Trask et al., 2015).", "startOffset": 208, "endOffset": 290}, {"referenceID": 18, "context": "Extensive research has been done to tune these embeddings to various tasks by incorporating features such as character (compositional) information, word order information, and multi-word (phrase) information (Ling et al., 2015; Mikolov et al., 2013c; Zhang et al., 2015; Trask et al., 2015).", "startOffset": 208, "endOffset": 290}, {"referenceID": 6, "context": "Despite these advancements, most word embedding techniques share a common problem in that each word must encode all of its potential meanings into a single vector (Huang et al., 2012).", "startOffset": 163, "endOffset": 183}, {"referenceID": 0, "context": "Subsequent work leveraging such neural word embeddings has proven to be effective on a variety of natural language modeling tasks (Al-Rfou et al., 2013; Al-Rfou et al., 2015; Chen & Manning, 2014).", "startOffset": 130, "endOffset": 196}, {"referenceID": 1, "context": "Subsequent work leveraging such neural word embeddings has proven to be effective on a variety of natural language modeling tasks (Al-Rfou et al., 2013; Al-Rfou et al., 2015; Chen & Manning, 2014).", "startOffset": 130, "endOffset": 196}, {"referenceID": 8, "context": "Ling et al. (2015) proposed modifications to word2vec that incorporated word order.", "startOffset": 0, "endOffset": 19}, {"referenceID": 6, "context": "Our technique is inspired by the work of Huang et al. (2012), which uses a multi-prototype neural vector-space model that clusters contexts to generate prototypes.", "startOffset": 41, "endOffset": 61}, {"referenceID": 6, "context": "Our technique is inspired by the work of Huang et al. (2012), which uses a multi-prototype neural vector-space model that clusters contexts to generate prototypes. Unlike Reisinger & Mooney (2010), the context embeddings are generated by a neural network in the following way: given a pre-trained word embedding model, each context embedding is generated by computing a weighted sum of the words in the context (weighted by tf-idf).", "startOffset": 41, "endOffset": 197}, {"referenceID": 3, "context": "Unlike previous approaches, Chen et al. (2015) selects the number of word clusters for each word based on the number of definitions for a word in the WordNet Gloss (as opposed to other approaches that commonly pick a fixed number of clusters).", "startOffset": 28, "endOffset": 47}, {"referenceID": 3, "context": "Unlike previous approaches, Chen et al. (2015) selects the number of word clusters for each word based on the number of definitions for a word in the WordNet Gloss (as opposed to other approaches that commonly pick a fixed number of clusters). A variant on the MSSG model of Neelakantan et al. (2015), this work uses the WordNet Glosses dataset and convolutional embeddings to initialize the word prototypes.", "startOffset": 28, "endOffset": 301}, {"referenceID": 6, "context": "We expand on the work of Huang et al. (2012) by leveraging supervised NLP labels instead of unsupervised clusters to determine a particular word instance\u2019s sense.", "startOffset": 25, "endOffset": 45}, {"referenceID": 0, "context": "1 with Part-of-Speech tags using the Polyglot Universal Dependency Part-of-Speech tagger of Al-Rfou et al. (2013) and trained sense2vec with identical parameters as section 3.", "startOffset": 92, "endOffset": 114}, {"referenceID": 0, "context": "For Sentiment disambiguation, the IMDB labeled training corpus was labeled with Part-of-Speech tags using the Polyglot Part-of-Speech tagger from Al-Rfou et al. (2013). Adjectives were then labeled with the positive or negative sentiment associated with each comment.", "startOffset": 146, "endOffset": 168}, {"referenceID": 6, "context": "The baseline embeddings were trained without any Part-of-Speech disambiguation using the structured skip-gram approach of Ling et al. (2015). For each language, the sense2vec embeddings were trained by disambiguating terms using the language specific Polyglot Part-of-Speech tagger of Al-Rfou et al.", "startOffset": 122, "endOffset": 141}, {"referenceID": 0, "context": "For each language, the sense2vec embeddings were trained by disambiguating terms using the language specific Polyglot Part-of-Speech tagger of Al-Rfou et al. (2013), and embedded in the same structured skip-gram approach.", "startOffset": 143, "endOffset": 165}], "year": 2015, "abstractText": "Neural word representations have proven useful in Natural Language Processing (NLP) tasks due to their ability to efficiently model complex semantic and syntactic word relationships. However, most techniques model only one representation per word, despite the fact that a single word can have multiple meanings or \u201dsenses\u201d. Some techniques model words by using multiple vectors that are clustered based on context. However, recent neural approaches rarely focus on the application to a consuming NLP algorithm. Furthermore, the training process of recent word-sense models is expensive relative to single-sense embedding processes. This paper presents a novel approach which addresses these concerns by modeling multiple embeddings for each word based on supervised disambiguation, which provides a fast and accurate way for a consuming NLP model to select a sense-disambiguated embedding. We demonstrate that these embeddings can disambiguate both contrastive senses such as nominal and verbal senses as well as nuanced senses such as sarcasm. We further evaluate Part-of-Speech disambiguated embeddings on neural dependency parsing, yielding a greater than 8% average error reduction in unlabeled attachment scores across 6 languages.", "creator": "LaTeX with hyperref package"}}}