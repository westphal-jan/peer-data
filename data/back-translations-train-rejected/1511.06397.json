{"id": "1511.06397", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "19-Nov-2015", "title": "Compressing Word Embeddings", "abstract": "Recent methods for learning vector space representations of words have succeeded in capturing fine-grained semantic and syntactic regularities using vector arithmetic. However, these vector space representations (created through large-scale text analysis) are typically stored verbatim, since their internal structure is opaque. Using word-analogy tests to monitor the level of detail stored in compressed re-representations of the same vector space, the trade-offs between the reduction in memory usage and expressiveness are investigated. A simple scheme is outlined that can reduce the memory footprint of a state-of-the-art embedding by a factor of 10, with only minimal impact on performance. Then, using the same 'bit budget', a binary (approximate) factorisation of the same space is also explored, with the aim of creating an equivalent representation with better interpretability.", "histories": [["v1", "Thu, 19 Nov 2015 21:42:47 GMT  (562kb)", "http://arxiv.org/abs/1511.06397v1", "10 pages, 6 figures, ICLR-2016"], ["v2", "Mon, 16 May 2016 17:19:51 GMT  (15kb)", "http://arxiv.org/abs/1511.06397v2", "10 pages, 0 figures, submitted to ICONIP-2016. Previous experimental results were submitted to ICLR-2016, but the paper has been significantly updated, since a new experimental set-up worked much better"]], "COMMENTS": "10 pages, 6 figures, ICLR-2016", "reviews": [], "SUBJECTS": "cs.CL cs.LG", "authors": ["martin", "rews"], "accepted": false, "id": "1511.06397"}, "pdf": {"name": "1511.06397.pdf", "metadata": {"source": "CRF", "title": null, "authors": [], "emails": ["martin@redcatlabs.com"], "sections": [{"heading": null, "text": "ar Xiv: 151 1,06 397v 1 [cs.C L] 19 Nov 201 5"}, {"heading": "1 INTRODUCTION", "text": "Distributed representations of words have been shown to benefit from NLP tasks such as parsing, entity recognition and sentiment analysis, as well as from their use as raw material for other deep learning tasks. Surprisingly, these word vector embedding can be derived directly from raw, uncommented corpora. Once created, the vector embedding itself can be expressed simply as a list of vocabulary words (size V) and a matrix of size V \u00d7 d, where d is the dimensionality of the embedding space. In this paper, a 300-dimensional GloVe embedding (Pennington et al., 2014) was used as the starting point for a number of different lossy compression methods, each of which was investigated in terms of the trade-offs between compressed representation size and fidelity compared to the word analogy task by Mikolov et al. (2013)."}, {"heading": "2 APPROACHES TO COMPRESSION", "text": "The GloVe embeddings are made by minimizing the error in predicting the word co-occurrence matrix Xi, j from the word vectors associated with the words i and j 1: J = V \u2211 i, j = 1f (Xij) (w T i i w-j + bi + b-logXij) 2This formulation means that there is no arrangement of the components of the vectors. Furthermore, simple linear dependencies have already been eliminated (because if there were linear dependencies, the total error would be higher than if the terms independently had more characteristics of the target Xij). 1f () is a function that (i) disappears at zero to eliminate problems that Xij is zero in the log () term, and (ii) stands out as soon as Xij is \"large,\" so that frequent words do not dominate the error calculation."}, {"heading": "2.1 REAL-VALUED ENCODINGS", "text": "Since the embedding arrives as a vector of real numbers, it is natural to see whether they can be compressed into a representation with the same dimensionality, except quantified."}, {"heading": "2.1.1 COMPRESSION 1 : THROW AWAY DATA", "text": "The simplest approach is simply to set a fixed portion of the data in the embed to zero. A slightly more complex version is to select a threshold and set each value in a given vector representation below that value to zero. Compression is slightly offset here, as a \"is-zero\" mask needs to be stored along with the other values."}, {"heading": "2.1.2 COMPRESSION 2 : LINEAR QUANTISATION", "text": "A simple approach to quantizing embedding in n planes is to construct a linear scale of n steps between the maximum and minimum values of each element (columns in the embedding matrix); a simple variant used here by virtue of its symmetry is to first normalize the embedding column by column, so that each element is \u0445 N (0, 1), and then to encode the value using (i) 1 bit for the character, and (ii) an index representing which of the n / 2 steps should be used in the range [0, max | x |]. In order to approximately counteract the uneven distribution of values in the elements, a simple nonlinearity was also experimented with quantifying | x | \u03b1, where \u03b1 was an experimentally derived constant (a value of \u0445 0.4 was considered reasonably robust)."}, {"heading": "2.1.3 COMPRESSION 3 : ADAPTIVE LEVEL ENCODING", "text": "Taking into account the significant irregularity of the distributions of each element (see Figure 1), an adaptive quantization approach has been devised. For a certain number of quantization levels n and an element e \u0445 E. A series of quantization levels eq is developed, which are minimized: \u2211 e-Emin q (e \u2212 eq) 2This process can be carried out iteratively, starting with the eq, which is uniformly placed within the numerically sorted values of E. The descent of the gradients can then be executed on the error term, and updates of the eq are made until a stable configuration is created. Note that early iterations can lead to large runs in the eq (since the function to be minimized has many sharp discontinuities).The above optimization is performed independently for each element / column of the embedding, and a list of the respective levels eq is stored in addition to the index within each list for each element."}, {"heading": "2.2 BINARY ENCODINGS", "text": "Although the methods in the previous section can significantly reduce the number of bits required for embedding, they still impose an interdependence between the bits that represent a single element. In collaboration with the \"bit budget\" proposed by the previous encodings, \"real\" binary representations of the same embedding were sought, following the intuition that a good encoding should have a minimum predetermined structure."}, {"heading": "2.2.1 COMPRESSION 4 : QUASI AUTO-ENCODING", "text": "Assuming a binary representation B of the baseline embedding E can be found, a natural first step is to strive for a linear relationship: E = BW, for some weight matrix W. While it seems reasonable to want to optimize B and W together, the size of B (which will be a multiple of the size of E) makes this impractical. To make this easier to understand, a secondary mapping is set up where B \u0445 is generated from the respective lines of E by a traceable network, similar to autoencoding learning (see, for example, Harpur & Prager (1996)), except that the discovery of B (which is the purely binary embedding that B tends to work toward) is the final goal and not an intermediate step. Also, note that the structure or complexity of the network transformation NN in the generative B = NN (E) stage was selected as an important factor.After testing a number of different network configuration.1 The very 3."}, {"heading": "2.2.2 COMPRESSION 5 : GREEDY BINARY PCA", "text": "Starting from the original embedding E0 = E, an iterative search is made for a vector wn that minimizes the remaining total energy in En + 1 when the available options either subtract it from a specific embedding line or not. Each iteration of this creates a vector wn, a binary representation of whether this vector should be used for a specific word (which becomes the last column of Bn), and a new residual embedding En + 1. Ultimately, the aggregation of these representations and vectors in matrices results in a sequence of representations: BnWn \u2192 E."}, {"heading": "3 EXPERIMENTS", "text": "A 300-dimensional embedding (created from a 6 billion token corpus built from a combination of Wikipedia 2014 and Gigaword 5) was used as the GloVe basis for experiments.The word analogy task used here as a measure of compression performance was as described in Mikolov et al. (2013), and the data set provided contains 19,544 such questions, divided into semantic and syntactic subsets. The semantic questions are typically analogies to persons or places, such as \"Athens is for Greece like Berlin\"?, whereas the syntactic questions are typically analogies to verb times or forms of adjectives, such as \"Dance is dancing like a fly for?.\" In order to answer a question correctly, the model must clearly identify the missing term, counting only an exact match as a correct match. This match occurs on a cosmic / similarity basis, such as a cosmic answer for a word?"}, {"heading": "3.1 BASIC METHODS", "text": "Also shown in Figure 2 are the results of the method of \"discarding data,\" either by zeros of columns or by thresholds (circles or triangles). Furthermore, the hexagon characterizes the performance of a 100-dimensional embedding trained on the same body, so that it can be compared to a 300-dimensional embedding where 200 of these dimensions artificially have zeros."}, {"heading": "3.2 QUANTISATION ENCODINGS : LINEAR, NEAR-LINEAR AND ADAPTIVE", "text": "The results of the quantization methods are shown in Figure 3, where linear, simple nonlinear and adaptively selected quantization grids (down-triangle, up-triangle or circle) are used. Numbers associated with each label correspond to the number of layers used. It is noteworthy that the method \"ada.8\" achieves a performance that is only 0.58% lower than GloVe, while using only 3-bit resolution per element."}, {"heading": "3.3 BINARY ENCODINGS", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "3.3.1 QUASI AUTO-ENCODING TRAINING", "text": "Figure 4 shows the performance during the training of the \"quasi-auto-coding\" method. As can be seen, the L2 error is continuously lowered to a predetermined acceptable level, while the parameter \u03bb is slowly increased to increase the variance of B * (which is also visible, although it clearly hesitates to improve to the final target, which is close to 0.25 on the right axis)."}, {"heading": "3.3.2 GREEDY BINARY PCA TRAINING", "text": "Figure 5 shows the performance of greedy binary error reduction operations. The reduction in the overall error is clear, although the error that is removed at each step is very small. Interestingly, the number of \"on\" bits in each successive additional representation in Bn tends to have a very stable limit (see below)."}, {"heading": "3.3.3 BINARY ENCODING RESULTS", "text": "The results of the binary embedding methods are shown in Figure 6, where \"NN \u0445 3\" is a 1024-bit binary embedding (approximately 3.4 bits per original embedding measure), and \"pca.x\" is the embedding generated by the binary PCA method outlined above. The error (compared to the original GloVe embedding) of \"pca.3\" is 0.81%, which is directly comparable to the earlier \"ada.8\" method, which achieved 0.58% (i.e. the method of adaptive planes was superior). Furthermore, the \"pca.8\" method requires the additional transmission of a (900 x 300) matrix, while the \"ada.8\" method only requires a reference table of the size (8 x 300)."}, {"heading": "3.3.4 LEARNED BINARY REPRESENTATION USED \u2018RAW\u2019", "text": "Compared to the memory savings achieved by using a purely binary representation of the dictionary, the effort to obtain these binary embeddings (compared to quantization methods) is small. On the other hand, one of the intuitions for the desire for a binary representation is that it could offer a better interpretability of the embedding itself. Given the relative difficulty of obtaining these binary embeddings (compared to quantization methods), only limited experiments were possible. However, a simple test of interpretability is whether the binary representation B could be used as a vector embedding of the language on a stand-alone basis. Given the two binary embeddings generated by (i) quasi-automatic encoding and (ii) binary PCA, the same cosinoid similarity tests were performed as in the other experiments (simply using the values under \"1\" and are discussed in each case in the new \"Table 1\")."}, {"heading": "4 DISCUSSION", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.1 LEVEL QUANTISATION VS. MORE \u2018SOPHISTICATED\u2019 METHODS", "text": "The level quantization approaches to compression work very well and are relatively easy to implement, but do not achieve the goal of learning the underlying embedding by an efficient compression algorithm. In an ideal world, the binary representations would be easier to learn by gradient descent. However, a (wide) variety of \"tricks\" had to be implemented to prevent the network from engaging in solutions that avoid the goal of good binary representation. This is partly because the overall optimization is too low: a 300-element vector space is replaced by a 1024-dimensional one (although the larger space must ultimately be evaluated binarily). Part of this overpass was solved by the addition of an additional sound layer, which then leads to increasing training times."}, {"heading": "4.2 BINARY PCA OBSERVATIONS", "text": "The initial motivation for the search for optimal \"space-folding\" directions was to see if these directions would have a revealing linguistic interpretation. Disappointingly, it turned out that the process is simply to slice and dice the embedding into an ever-smaller sphere of noise around the origin. Recognizing space-folding as a mere way to compress distributions into \"itself\" also explains why the number of \"one\" bits in the binary PCA representation becomes so stable. Once the distribution of values in an embedding dimension has become stable, the proportion of distribution that is folded in each iteration becomes constant. There is a curious bump in energy improvement in Figure 5 at iteration \u0445 300. This is probably the first round of modifications to each vector element that are completed, and the next round of subdivision begins (complete with the first step of eliminating common offsets that can also be observed right at the beginning of the training)."}, {"heading": "4.3 BINARY ENCODINGS FOR INTERPRETABILITY", "text": "Although the binary PCA approach proves that binary linear encoding is possible, the PCA process has little incentive to capture the semantic or syntatic structure in the embedding. As the results in Table 2 show, the standard mantra that auto-encodings learn useful representations is confirmed. In fact, it is reasonably reassuring that the binary representation (which is only a by-product of adapting to an existing embedding) is itself a useful representation. Although the raw binary representation alone is not as useful as hoped, it is a small consolation that binary-rated encoding enables matrix multiplication only as a process of conditional additives."}, {"heading": "5 CONCLUSION", "text": "Ideally, the search for a good compression method would have resulted in a representation with a good degree of interpretation. By analogy, compare JPEG encoding (which is lossy in a way to which the brain is less sensitive) with Zip encoding (which is generally optimal without giving much insight into the nature of the compressed data).However, in terms of the goal of compressing a representation within a certain \"bit budget\" (especially 3 bits per vector dimension with < 1% error above the baseline), the adaptive quantization method is certainly effective here: it is relatively easy to implement, each element of the resulting representation remains independent, and it is possible to reconstruct the full embedding on-the-fly from the small search table that is required in addition to level index representation."}, {"heading": "ACKNOWLEDGMENTS", "text": "The author would like to thank DC Frontiers, a Singapore-based company that has developed the data-centric service \"Handshakes\" (http: / / www.handshakes.com.sg /), for their willingness to support this ongoing research. DC Frontiers receives a grant program for the commercialization of technology by companies from SPRING Singapore, where this work took place."}], "references": [{"title": "Estimating or propagating gradients through stochastic neurons for conditional computation", "author": ["Bengio", "Yoshua", "L\u00e9onard", "Nicholas", "Courville", "Aaron C"], "venue": "CoRR, abs/1308.3432,", "citeRegEx": "Bengio et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Bengio et al\\.", "year": 2013}, {"title": "Learning fast approximations of sparse coding", "author": ["Gregor", "Karol", "LeCun", "Yann"], "venue": "In Proceedings of the 27th International Conference on Machine Learning", "citeRegEx": "Gregor et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Gregor et al\\.", "year": 2010}, {"title": "Development of low entropy coding in a recurrent network. Network: computation", "author": ["Harpur", "George F", "Prager", "Richard W"], "venue": "in neural systems,", "citeRegEx": "Harpur et al\\.,? \\Q1996\\E", "shortCiteRegEx": "Harpur et al\\.", "year": 1996}, {"title": "Efficient estimation of word representations in vector space", "author": ["Mikolov", "Tomas", "Chen", "Kai", "Corrado", "Greg", "Dean", "Jeffrey"], "venue": "arXiv preprint arXiv:1301.3781,", "citeRegEx": "Mikolov et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Glove: Global vectors for word representation", "author": ["Pennington", "Jeffrey", "Socher", "Richard", "Manning", "Christopher D"], "venue": "Proceedings of the Empiricial Methods in Natural Language Processing (EMNLP", "citeRegEx": "Pennington et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Pennington et al\\.", "year": 2014}], "referenceMentions": [{"referenceID": 4, "context": "In this work, a 300 dimensional GloVe embedding (Pennington et al., 2014) was used as a baseline for a number of different lossy compression methods, each of which were explored in terms of the trade-offs between compressed representation size, and fidelity when compared using the word analogy task of Mikolov et al.", "startOffset": 48, "endOffset": 73}, {"referenceID": 3, "context": ", 2014) was used as a baseline for a number of different lossy compression methods, each of which were explored in terms of the trade-offs between compressed representation size, and fidelity when compared using the word analogy task of Mikolov et al. (2013). 2 APPROACHES TO COMPRESSION The GloVe embeddings are produced by minimising the error in predicting the word co-occurrence matrix Xi,j from the word vectors associated with words i and j 1 :", "startOffset": 237, "endOffset": 259}, {"referenceID": 0, "context": "Addition of noise : A constant, additive noise term was found to aid training greatly (see also Bengio et al. (2013)).", "startOffset": 96, "endOffset": 117}, {"referenceID": 3, "context": "The word analogy task used here as the compression performance measure was as described in Mikolov et al. (2013), and the provided dataset contains 19,544 such questions, divided into semantic and syntactic subsets.", "startOffset": 91, "endOffset": 113}, {"referenceID": 4, "context": "In Figure 2, the performance of a selection of 300-dimensional full-resolution embeddings is shown (as given in (Pennington et al., 2014)).", "startOffset": 112, "endOffset": 137}], "year": 2017, "abstractText": "Recent methods for learning vector space representations of words have succeeded in capturing fine-grained semantic and syntactic regularities using vector arithmetic. However, these vector space representations (created through large-scale text analysis) are typically stored verbatim, since their internal structure is opaque. Using word-analogy tests to monitor the level of detail stored in compressed rerepresentations of the same vector space, the trade-offs between the reduction in memory usage and expressiveness are investigated. A simple scheme is outlined that can reduce the memory footprint of a state-of-the-art embedding by a factor of 10, with only minimal impact on performance. Then, using the same \u2018bit budget\u2019, a binary (approximate) factorisation of the same space is also explored, with the aim of creating an equivalent representation with better interpretability.", "creator": "LaTeX with hyperref package"}}}