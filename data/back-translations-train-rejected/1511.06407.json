{"id": "1511.06407", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "19-Nov-2015", "title": "Recurrent Models for Auditory Attention in Multi-Microphone Distance Speech Recognition", "abstract": "Integration of multiple microphone data is one of the key ways to achieve robust speech recognition in noisy environments or when the speaker is located at some distance from the input device. Signal processing techniques such as beamforming are widely used to extract a speech signal of interest from background noise. These techniques, however, are highly dependent on prior spatial information about the microphones and the environment in which the system is being used. In this work, we present a neural attention network that directly combines multi-channel audio to generate phonetic states without requiring any prior knowledge of the microphone layout or any explicit signal preprocessing for speech enhancement. We embed an attention mechanism within a Recurrent Neural Network (RNN) based acoustic model to automatically tune its attention to a more reliable input source. Unlike traditional multi-channel preprocessing, our system can be optimized towards the desired output in one step. Although attention-based models have recently achieved impressive results on sequence-to-sequence learning, however, no attention mechanisms have been applied to learn multiple inputs which may be asynchronous and non-stationary. We evaluate our neural attention model on a subset of the CHiME-3 challenge task, and we show that the model achieves comparable performance to beamforming using a purely data-driven method.", "histories": [["v1", "Thu, 19 Nov 2015 21:56:53 GMT  (253kb,D)", "https://arxiv.org/abs/1511.06407v1", "Under review as a conference paper at ICLR 2016"], ["v2", "Thu, 7 Jan 2016 22:16:54 GMT  (259kb,D)", "http://arxiv.org/abs/1511.06407v2", "Under review as a conference paper at ICLR 2016"]], "COMMENTS": "Under review as a conference paper at ICLR 2016", "reviews": [], "SUBJECTS": "cs.LG cs.CL", "authors": ["suyoun kim", "ian lane"], "accepted": false, "id": "1511.06407"}, "pdf": {"name": "1511.06407.pdf", "metadata": {"source": "CRF", "title": null, "authors": ["SPEECH RECOGNITION", "Suyoun Kim"], "emails": ["suyoun@cmu.edu", "lane@cmu.edu"], "sections": [{"heading": "1 INTRODUCTION", "text": "In fact, most of them are able to move to another world, to move to another world, to move to another world, to move to another world, to move to another world, to move to another world, to move to another world, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move."}, {"heading": "2 MODEL", "text": "In this section, we describe our neural attention model, which enables neural networks to focus more on reliable input sources at different time points. We formulate the proposed framework with applications in multi-channel remote voice recognition. While there is some recent work on end-to-end neural speech recognition systems - from speech directly to transcripts (Graves et al., 2006; Graves & Jaitly, 2014; Hannun et al., 2014; Chorowski et al., 2014) - our model is based on typical hybrid DNN-HMM frameworks (Morgan & Bourlard, 1994; Hinton et al., 2012), with the acoustic model estimating the hidden Markov models (HMM) of state posteriors because we focus on dealing with the re-weighted input representation of misaligned multiple input sources (Morgan & Bourlard, 1994; Hinton et al, 2012)."}, {"heading": "2.1 ATTENTION MECHANISM FOR MULTIPLE SOURCES", "text": "This year it is more than ever before."}, {"heading": "2.2 LSTM ACOUSTIC MODEL", "text": "Our next subnetwork, LSTM-AM, serves as a typical RNN-based acoustic model, except that it accepts the reweighted input X-c instead of the original input Xc (Hochreiter & Schmidhuber, 1997), which has been successfully applied to speech recognition tasks due to its ability to handle long-term dependencies. LSTM contains special units called memory blocks in the recurring hidden layer, and each block has memory cells with special three gates (Input it, Output ot, and forget ft) to control the flow of information. In our work, we use a simplified version of an LSTM without peephole connections and distortion to reduce the computing costs of learning the standard LSTM models. Although LSTMs have many variations to improve their performance, such as BLSTM et al, 2013), LSTMP et al, 2014, Attention, and BLSTM, an improvement in the way of working."}, {"heading": "3 EXPERIMENTS", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "3.1 DATASET", "text": "The purpose of the CHiME-3 (Jon Barker, 2015) is to automatically detect speech for a tablet with multiple microphones in an everyday environment - a caf\u00e9, an intersection, public transportation, and a pedestrian street. There are two types of data sets: REAL and SIMU. The REAL data consists of 6-channel recordings. 12 speakers from the U.S. were asked to read the records from the WSJ0 corpus (Garofalo et al., 2007) while using the multi-microphone tablet. They were encouraged to adjust their reading positions so that the target distance kept changing over time. The simulated data from the SIMU were generated by mixing clean expressions from the WSJ0 corpus in background recordings. To verify our method in a real noisy environment, we initially opted not to use the simulated data sets, but only the REAL data sets, with microphones located about 5 cm apart from each of the microphones in each corner of the MEAL, the five of each corner we saw."}, {"heading": "3.2 SYSTEM TRAINING", "text": "The dataset was represented with 25ms frames of 40-dimensional log-filter-bank energy characteristics calculated every 10 ms. We produced 1,992 HMM state labels from a trained GMM-HMM system with near-field microphone data, and these state labels were used in all subsequent experiments. We used a layer of the LSTM architecture with 512 cells. Weights in all networks were initialized in the range (-0.03, 0.03) with a uniform distribution, and the initial attention weights were initialized to 1 / n in n dimensions. We set the configuration of the learning rate to 0.4, and after two epochs it decayed during training. All models resulted in a stable convergence from 1e-04 to 5e-04."}, {"heading": "3.3 RESULTS", "text": "In Tables 1 and 2, we summarize word error rates (WERs) obtained on the subset of the CHiME3 task. ALSTM is our proposed model, which has a multi-input attention mechanism as described in Section 2.1, and ALSTM (with phase processing data in addition to ALSTM). When we built three models on the REAL dataset, we used the same simple version of the LSTM architecture that we described in Section 2.2 with three different inputs. LSTM (Preprocessing 5 noisy-channel) was trained on the improved signal of 5 noisy channels. We obtained the improved signal from the beamforming toolkit provided by the CHiME3 organization (Jon Barker, 2010; Blandin et al., 2003)."}, {"heading": "4 CONCLUSIONS", "text": "We proposed an attention-based model (ALSTM) that uses asynchronous and non-stationary multi-channel inputs to generate outputs; for a remote speech recognition task, we embedded a novel attention mechanism into an RNN-based acoustic model to automatically adjust its attention to a more reliable input source; we presented our findings on the CHiME3 task and found that ALSTM showed a significant improvement in WER; our model achieved a performance comparable to beamforming without knowledge of microphone layout or any explicit pre-processing; the implications of this work are significant and far-reaching; our work points to a way to build a more efficient ASR system by circumventing pre-processing; and our results suggest that this approach is likely to perform well in tasks where unbalanced and non-stationary inputs from multiple sources need to be used, such as multimodal problems and sensory fusion."}, {"heading": "ACKNOWLEDGMENTS", "text": "The authors would like to thank Richard M. Stern and William Chan for their valuable and constructive proposals, which were supported by the LGE."}], "references": [], "referenceMentions": [], "year": 2016, "abstractText": "Integration of multiple microphone data is one of the key ways to achieve ro-<lb>bust speech recognition in noisy environments or when the speaker is located at<lb>some distance from the input device. Signal processing techniques such as beam-<lb>forming are widely used to extract a speech signal of interest from background<lb>noise. These techniques, however, are highly dependent on prior spatial informa-<lb>tion about the microphones and the environment in which the system is being used.<lb>In this work, we present a neural attention network that directly combines multi-<lb>channel audio to generate phonetic states without requiring any prior knowledge<lb>of the microphone layout or any explicit signal preprocessing for speech enhance-<lb>ment. We embed an attention mechanism within a Recurrent Neural Network<lb>(RNN) based acoustic model to automatically tune its attention to a more reli-<lb>able input source. Unlike traditional multi-channel preprocessing, our system can<lb>be optimized towards the desired output in one step. Although attention-based<lb>models have recently achieved impressive results on sequence-to-sequence learn-<lb>ing, no attention mechanisms have previously been applied to learn potentially<lb>asynchronous and non-stationary multiple inputs. We evaluate our neural atten-<lb>tion model on the CHiME-3 challenge task, and show that the model achieves<lb>comparable performance to beamforming using a purely data-driven method.", "creator": "LaTeX with hyperref package"}}}