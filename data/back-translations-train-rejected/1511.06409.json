{"id": "1511.06409", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "19-Nov-2015", "title": "Learning to Generate Images with Perceptual Similarity Metrics", "abstract": "Deep networks are increasingly being applied to problems involving image synthesis, e.g., generating images from textual descriptions, or generating reconstructions of an input image in an autoencoder architecture. Supervised training of image-synthesis networks typically uses a pixel-wise squared error (SE) loss to indicate the mismatch between a generated image and its corresponding target image. We propose to instead use a loss function that is better calibrated to human perceptual judgments of image quality: the structural-similarity (SSIM) score of Wang, Bovik, Sheikh, and Simoncelli (2004). Because the SSIM score is differentiable, it is easily incorporated into gradient-descent learning. We compare the consequences of using SSIM versus SE loss on representations formed in deep autoencoder and recurrent neural network architectures. SSIM-optimized representations yield a superior basis for image classification compared to SE-optimized representations. Further, human observers prefer images generated by the SSIM-optimized networks by nearly a 7:1 ratio. Just as computer vision has advanced through the use of convolutional architectures that mimic the structure of the mammalian visual system, we argue that significant additional advances can be made in modeling images through the use of training objectives that are well aligned to characteristics of human perception.", "histories": [["v1", "Thu, 19 Nov 2015 21:57:46 GMT  (405kb,D)", "http://arxiv.org/abs/1511.06409v1", "Submitted to ICLR 2016"], ["v2", "Thu, 17 Mar 2016 17:21:56 GMT  (652kb,D)", "http://arxiv.org/abs/1511.06409v2", null], ["v3", "Tue, 24 Jan 2017 02:03:41 GMT  (4999kb,D)", "http://arxiv.org/abs/1511.06409v3", null]], "COMMENTS": "Submitted to ICLR 2016", "reviews": [], "SUBJECTS": "cs.LG cs.CV", "authors": ["jake snell", "karl ridgeway", "renjie liao", "brett d roads", "michael c mozer", "richard s zemel"], "accepted": false, "id": "1511.06409"}, "pdf": {"name": "1511.06409.pdf", "metadata": {"source": "CRF", "title": "LEARNING TO GENERATE IMAGES WITH PERCEPTUAL SIMILARITY METRICS", "authors": ["Karl Ridgeway", "Jake Snell", "Brett D. Roads", "Richard S. Zemel", "Michael C. Mozer"], "emails": [], "sections": [{"heading": "1 INTRODUCTION", "text": "The reason for this increase is threefold: First, the problem of image generation spans a wide range of difficulties, from synthetic images to handwritten numerals and, of course, overloaded and high-dimensional scenes, the latter of which can also be transferred between tasks; third, image generation is fun and captures popular imagination, as efforts like Google's Inceptionism require new representations; one of the primary ways to learn generative images is the architecture of the machine, such as classification or clustering, and also the transfer between tasks; and third, image generation is fun and captures popular imagination."}, {"heading": "2 RELATED WORK", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "2.1 MODELS FOR GENERATING IMAGES", "text": "There are two main types of autoencoders: the first is deterministic, which maps input directly through hidden layers and produces a reconstruction of the original image. Typically, MSE is used to evaluate reconstruction; the second type is probabilistic, which deals with the unpredictability of inference in latent variables, such as Helmholtz machines (Dayan et al., 1995) and varying autoencoders (VAE) (Kingma & Welling, 2013); the encoder is used to approximate a posterior distribution; and the decoder is used to reconstruct data from latent variables. Gregor et al. (2015) introduced the Deep-Recurrent Attention Writer (DRAW), which expands the VAE approach by incorporating a novel differentiable attention mechanism."}, {"heading": "2.2 PERCEPTION-BASED ERROR METRICS", "text": "When the digitization of photos and videos was commonplace in the 1990s, the need for digital compression became evident. Lossy compression programs distorted image data, and it was important to quantify the loss of quality due to compression in order to optimize the compression scheme. As compressed digital artifacts are eventually used by humans, researchers attempted to develop complete image quality reference metrics that take into account features for which the human visual system is sensitive and on which it ignores insensitive features. Some of these metrics are based on complex models of the human visual system, such as the Sarnoff JND model (Lubin, 1998), the predictor of visual differences (Daly, 1992), the metric for the quality of moving images (Van den Branden Lambrecht & Verscheure, 1996), and the perception distortion metric (Winkler, 1998). Other metrics take into account a more technical and human-based approach to image perception and extraction rather than the specific characteristics of the image."}, {"heading": "2.2.1 STRUCTURAL SIMILARITY", "text": "In this paper, we train autoencoders with structural similarity (SSIM) and compare them with autoencoders trained with MSE. We chose the SSIM metric for our initial investigation because it is well accepted and widely used in the literature. Furthermore, its pixel-by-pixel gradient has a simple analytical form and is inexpensive to calculate. In this thesis, we focus on the original grayscale index SSIM, although there are interesting variations and improvements, including the color values SSIM (Hassan & Bhagvati, 2012) and the multi-scale SSIM (Wang et al., 2003). The original SSIM metric, as described in Wang et al. (2004), is a pixel-by-pixel measure that compares corresponding pixels in two images, designates x and y, with three comparison functions - luminosity (I), contrast (C), and structure (S) x x x - defined in Equation 1: (I, y) + 2 \u00b5xels + 2 pixels (we are equal to xels + 2 + 2 \u00b5m + 2 pixels) and 2x pixels (we are equal to xels)."}, {"heading": "3 METHODOLOGY", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "3.1 NETWORK ARCHITECTURES", "text": "Our primary experiments are based on two autoencoder architectures: a fully connected network and a winding network. Each architecture was constructed with a bottleneck layer - the middle layer of the deep autoencoder; we used 256 nodes for the fully connected network and 256 or 512 nodes for the winding network. Each architecture was trained with either MSE or SSIM-related losses. We refer to a particular model based on its architecture, the size of the bottleneck layer and the loss function it was trained with. The fully connected models are called FC-256 - {SSIM, MSE}, and the winding models are called Conv- {256,512} - {SSIM, MSE}. By using several model variants, we want to show that our results are robust against architectural details."}, {"heading": "3.1.1 FULLY CONNECTED ARCHITECTURE", "text": "We adopted the fully networked autoencoder architecture from Krizhevsky & Hinton (2011), which is described in detail in the left bar of Table 1. Each layer has linear activation capabilities, except for bottleneck and output layers with Tanh activation. Krizhevsky & Hinton (2011) trained their network by stacking limited Boltzmann machines and then fine-tuning with backpropagation. Instead, we trained our network from the ground up through backpropagation and stochastic gradient reduction."}, {"heading": "3.1.2 CONVOLUTIONAL ARCHITECTURE", "text": "Due to the popularity and performance of coil meshes for image processing, we have included a coil autoencoder in our experiments.The right panel of Table 1 shows the structure of our model, which uses coil layers to encode the input and then the coil layers to decipher the representation of the characteristics in the bottleneck layer.The coil layers are implemented as coil layers, preceded by an upsampling step, which, by repeating the values of the input layer, creates a layer with twice the dimensions of the input coating. To investigate the role of the coil layer capacity, we have built models with 256 and 512 node bottlenecks. The peculiarities of coil autocoders are described in Masci et al. (2011)."}, {"heading": "3.1.3 ACTIVATION QUANTIZATION", "text": "To force strong compression of the signal in our autoencoders, we force the activation of nodes in the bottleneck layer to be binary (\u2212 1 or + 1).According to Krizhevsky & Hinton (2011), we swell the activations in forward gear and use the original continuous value for the purpose of calculating gradients during backpropagation. We perform this quanization both during training and testing, and all results are based on the quantified bottleneck layer representations. Krizhevsky & Hinton (2011) quantified to obtain binary codes for indexing hash tables. We are interested in implementing a more categorical representation, we also experimented with models where the bottleneck layer was not quantified, and we found that the quantified representations were better at predicting image classifications (e.g. dog versus cat versus airplane)."}, {"heading": "3.2 DATA SETS AND TRAINING METHODOLOGY", "text": "The images in this dataset measure 32 x 32 pixels and consist of three RGB color channels. We mapped the three color channels to a single grayscale channel using the conversion function of the Python Cushion Library. Input pixels are scaled to the range [\u2212 1, 1] to adjust the Tanh activation function on all of our output layers. All tests and evaluations of our models used the CIFAR-10 dataset, which consists of 60,000 color images, each drawn from one of ten categories. We selected a diversified dataset for training to ensure that the auto-encoders learned general statistical properties of the images, not the specifics of the CIFAR-10 metrics. The CIFAR-10 color images were converted into a single dataset to ensure that the results of the search were split into a single dataset (MAR-10)."}, {"heading": "4 RESULTS", "text": "As expected, the MSE-optimized networks tend to achieve better MSE reconstruction values and the SSIM-optimized networks tend to achieve better SSIM reconstruction values. Figure 1 shows the relative performance of each network on the metric it is designed to optimize. Each bar indicates the proportion of images for which an architecture trained to optimize the performance metric X achieves better performance than an architecture trained to optimize the other performance metric when evaluated on the metric X. The fact that all bars are above 50% indicates that training on one metric or another has a significant impact on the resulting models. To further compare the models trained with the SSIM and MSE metrics, we use both subjective and objective characteristics of the model output. Subjective characteristics are determined by asking the person to evaluate the quality of the reconstruction. Objective characteristics are determined by examining the categories of cluster formation."}, {"heading": "4.1 JUDGMENTS OF IMAGE RECONSTRUCTION QUALITY", "text": "We collected judgments about the quality of perception in the region. Participants were confronted with a sequence of image triangles in the middle and half of the time that appeared on the right side. All reconstructions came from the region. Participants were instructed to choose which of the two reconstructions they preferred. Half of the time the reconstruction appeared on the left side. All reconstructions were performed on the right side."}, {"heading": "4.2 EVALUATION OF LEARNED REPRESENTATIONS", "text": "In this section, we will go further and claim that the SSIM target leads to the discovery of internal representations in the neural network that are more closely associated with the category associated with an image. We will examine the compressed representations of the image in the bottleneck layer, which we call image code. As explained in the Methodology section above, these codes are binary vectors of either 256 or 512 elements. If the SSIM target distorts learning in terms of discovering codes that convey good information about the object in an image, then we should see categorical clustering of codes. That is, the code associated with the image of a dog should be more similar to the codes for images of other dogs than the code for an image of a visually similar cat."}, {"heading": "4.3 QUALITATIVE EXPERIMENTS ON RECURRENT IMAGE GENERATION", "text": "To further explore the role of perceptual loss in learning models for image generation, we adapt the DRAW model by Gregor et al. (2015) to be equipped with an arbitrarily differentiated image similarity. (The DRAW models generate an image by selecting a latent zt-P (Zt) for each specified number of time frames. (The DRAW models are used to update the model) to update the accumulated representation of the output image (also known as the canvas) Due to the intractable posterior over the image x, the decoder RNN is simultaneously produced with an encoder RNN that produces a varying approximation Q (Zt k -M SE400600S SIM0.80.9T o p -10 Co-256-25SE 25SE 25SE 25SE 25SE 25SE 25SE 256-256-256)."}, {"heading": "5 DISCUSSION AND FUTURE WORK", "text": "Beyond this subjective metric, we have also shown that the compressed representations of SSIM-optimized autoencoders contain more information about object categories than those of MSE-optimized autoencoders. These key results apply to both fully networked and revolutionary architectures, and to various bottlenecks. In addition, we have shown that recurrent neural network architectures also benefit from training with SSIM, as they produce better quality reconstructions and generated samples than those derived from training with MSE. In terms of reciprocal image generation experiments, we plan to go beyond the qualitative assessment of the EL-DRAW model by collecting human judgments on reconstruction quality and epabilizing encouraging data."}, {"heading": "6 SUPPLEMENTARY MATERIAL", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "6.1 EXPERIMENTAL DETAILS OF EL-DRAW TRAINING", "text": "For the experiments with the EL-DRAW model in Section 4.3, we used the predefined test splits of CIFAR-10 to form a test set of 1,000 images with a class dog, and randomly selected 4,500 of the class dog training images as a training set. We used the remaining 500 images of the class dog for validation. As in the original DRAW model, we chose P (Z) as the default gauss with zero mean and unit variance for each latent dimension. We selected the logistic sigmoid function as the activation applied to the final screen, i.e. x-x-z = 11 + Exp (\u2212 cT). We used hyperparameters similar to those of the CIFAR-DRAW model trained by Gregor et al. (2015): 400 hidden units for the encoder and decoder LSTM, 200 dimensions for each latent and 5x5 read and write dimensions for operations."}, {"heading": "6.2 CHOICE OF C IN EL-DRAW OBJECTIVE", "text": "The value of C in the EL-DRAW lens (Eq.5) regulates the trade-off between the loss of the EL-DRAW and the reconstruction error. As the C-value increases, the model places greater emphasis on reconstructions. At the same time, the KL divergence of the previous one from the approximate posterior will increase, resulting in poorer samples. Selecting a value of C becomes more complicated due to the different scaling depending on the choice of image-specific loss. We trained MSE-optimized and SSIM-optimized EL-DRAW models with a value range of C from 1 to 1000. We then evaluated the KL component of LEL-DRAW using the validation set and tried to select a C setting for each loss that yielded comparable KL divergences. We opted for C = 10 for MSE, resulting in a validation loss of KL samples of 52.8535 and C = 500 for SSIM, which resulted in Figure 8: lower KL-DRAW reconstructions would be evaluated with a lower loss of 47W-1."}, {"heading": "6.3 DRAW AS A SPECIAL CASE OF EL-DRAW", "text": "As mentioned by Gregor et al. (2015), a natural choice of D for the DRAW model in the case of binary data is the Bernoulli distribution. We note that this setting of DRAW can be considered a special case of EL-DRAW, where C is set to 1 and \u0445 is considered a binary cross entropy. For the sake of completeness, we provide reconstructions and samples of EL-DRAW that have been trained with this setting of C and \u0445 in Figures 8 and 9 respectively."}], "references": [{"title": "Vsnr: A wavelet-based visual signal-to-noise ratio for natural images", "author": ["D.M. Chandler", "S.S. Hemami"], "venue": "IEEE Transactions on Image Processing,", "citeRegEx": "Chandler and Hemami,? \\Q2007\\E", "shortCiteRegEx": "Chandler and Hemami", "year": 2007}, {"title": "Visible differences predictor: an algorithm for the assessment of image fidelity", "author": ["S.J. Daly"], "venue": "SPIE/IS&T", "citeRegEx": "Daly,? \\Q1992\\E", "shortCiteRegEx": "Daly", "year": 1992}, {"title": "Deep Generative Image Models using a Laplacian Pyramid of Adversarial Networks", "author": ["E. Denton", "S. Chintala", "A. Szlam", "R. Fergus"], "venue": null, "citeRegEx": "Denton et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Denton et al\\.", "year": 2015}, {"title": "Generative adversarial networks. arXiv 1406.266v1 [stat.ML", "author": ["I.J. Goodfellow", "J. Pouget-Abadie", "M. Mirza"], "venue": null, "citeRegEx": "Goodfellow et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Goodfellow et al\\.", "year": 2014}, {"title": "DRAW: A recurrent neural network for image generation", "author": ["K. Gregor", "I. Danihelka", "A. Graves", "D. Wierstra"], "venue": "arXiv preprint arXiv:1502.04623,", "citeRegEx": "Gregor et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Gregor et al\\.", "year": 2015}, {"title": "Structural Similarity Measure for Color Images", "author": ["Hassan", "Mohammed", "Bhagvati", "Chakravarthy"], "venue": "International Journal of Computer Applications", "citeRegEx": "Hassan et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Hassan et al\\.", "year": 2012}, {"title": "Learning and relearning in boltzmann machines", "author": ["G.E. Hinton", "T.J. Sejnowski"], "venue": "Volume 1: Foundations,", "citeRegEx": "Hinton and Sejnowski,? \\Q1986\\E", "shortCiteRegEx": "Hinton and Sejnowski", "year": 1986}, {"title": "A fast learning algorithm for deep belief nets", "author": ["G.E. Hinton", "S. Osindero", "Y.W. Teh"], "venue": "Neural Computation,", "citeRegEx": "Hinton et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Hinton et al\\.", "year": 2006}, {"title": "Forward models: Supervised learning with a distal teacher", "author": ["M.I. Jordan", "D.E. Rumelhart"], "venue": "Cognitive Science,", "citeRegEx": "Jordan and Rumelhart,? \\Q1992\\E", "shortCiteRegEx": "Jordan and Rumelhart", "year": 1992}, {"title": "Adam: A method for stochastic optimization", "author": ["D.P. Kingma", "J. Ba"], "venue": "arXiv preprint arXiv:1412.6980,", "citeRegEx": "Kingma and Ba,? \\Q2014\\E", "shortCiteRegEx": "Kingma and Ba", "year": 2014}, {"title": "Auto-encoding variational bayes", "author": ["D.P. Kingma", "M. Welling"], "venue": "arXiv preprint arXiv:1312.6114,", "citeRegEx": "Kingma and Welling,? \\Q2013\\E", "shortCiteRegEx": "Kingma and Welling", "year": 2013}, {"title": "Using very deep autoencoders for content-based image retrieval", "author": ["Krizhevsky", "Alex", "Hinton", "Geoffrey E"], "venue": "In ESANN. Citeseer,", "citeRegEx": "Krizhevsky et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Krizhevsky et al\\.", "year": 2011}, {"title": "Generative Moment Matching Networks", "author": ["Li", "Yujia", "Swersky", "Kevin", "Zemel", "Rich"], "venue": "In Proceedings of The 32nd International Conference on Machine Learning,", "citeRegEx": "Li et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Li et al\\.", "year": 2015}, {"title": "A human vision system model for objective image fidelity and target detectability measurements", "author": ["Lubin", "Jeffrey"], "venue": "In Proc. EUSIPCO,", "citeRegEx": "Lubin and Jeffrey.,? \\Q1998\\E", "shortCiteRegEx": "Lubin and Jeffrey.", "year": 1998}, {"title": "Stacked convolutional autoencoders for hierarchical feature extraction", "author": ["Masci", "Jonathan", "Meier", "Ueli", "Cire\u015fan", "Dan", "Schmidhuber", "J\u00fcrgen"], "venue": "In Artificial Neural Networks and Machine Learning\u2013", "citeRegEx": "Masci et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Masci et al\\.", "year": 2011}, {"title": "Image information and visual quality", "author": ["Sheikh", "Hamid Rahim", "Bovik", "Alan C"], "venue": "IEEE Transactions on Image Processing,", "citeRegEx": "Sheikh et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Sheikh et al\\.", "year": 2006}, {"title": "Information processing in dynamical systems: Foundations of harmony theory", "author": ["P. Smolensky"], "venue": "Parallel Distributed Processing: Explorations in the Microstructure of Cognition,", "citeRegEx": "Smolensky,? \\Q1986\\E", "shortCiteRegEx": "Smolensky", "year": 1986}, {"title": "80 million tiny images: A large data set for nonparametric object and scene recognition", "author": ["Torralba", "Antonio", "Fergus", "Rob", "Freeman", "William T"], "venue": "Pattern Analysis and Machine Intelligence, IEEE Transactions on,", "citeRegEx": "Torralba et al\\.,? \\Q1958\\E", "shortCiteRegEx": "Torralba et al\\.", "year": 1958}, {"title": "Perceptual quality measure using a spatiotemporal model of the human visual system", "author": ["C.J. Van den Branden Lambrecht", "O. Verscheure"], "venue": "In Electronic Imaging: Science & Technology,", "citeRegEx": "Lambrecht and Verscheure,? \\Q1996\\E", "shortCiteRegEx": "Lambrecht and Verscheure", "year": 1996}, {"title": "Maximum differentiation (mad) competition: A methodology for comparing computational models of perceptual quantities", "author": ["Wang", "Zhou", "Simoncelli", "Eero P"], "venue": "Journal of Vision,", "citeRegEx": "Wang et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2008}, {"title": "Multi-scale structural similarity for image quality assessment", "author": ["Wang", "Zhou", "Simoncelli", "Eero P", "Bovik", "Alan C"], "venue": "IEEE Asilomar Conference on Signals, Systems and Computers,", "citeRegEx": "Wang et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2003}, {"title": "Image quality assessment: from error visibility to structural similarity", "author": ["Wang", "Zhou", "Bovik", "Alan Conrad", "Sheikh", "Hamid Rahim", "Simoncelli", "Eero P"], "venue": "IEEE Transactions on Image Processing,", "citeRegEx": "Wang et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2004}, {"title": "2015), a natural choice of D for the DRAW model in the case", "author": ["Gregor"], "venue": null, "citeRegEx": "Gregor,? \\Q2015\\E", "shortCiteRegEx": "Gregor", "year": 2015}], "referenceMentions": [{"referenceID": 16, "context": "A second class of generative models are variants of Boltzmann Machines (Smolensky, 1986; Hinton & Sejnowski, 1986) and Deep Belief Networks (Hinton et al.", "startOffset": 71, "endOffset": 114}, {"referenceID": 7, "context": "A second class of generative models are variants of Boltzmann Machines (Smolensky, 1986; Hinton & Sejnowski, 1986) and Deep Belief Networks (Hinton et al., 2006) While these models are very powerful, each iteration of training requires a computationally costly step of MCMC to approximate derivatives of an intractable partition function (normalization constant), making it difficult to scale them to large datasets.", "startOffset": 140, "endOffset": 161}, {"referenceID": 3, "context": "Generative Adversarial Networks (GANs) (Goodfellow et al., 2014) is a paradigm that involves training a discriminator that attempts to distinguish real from generated images, along with a generator that attempts to trick the discriminator.", "startOffset": 39, "endOffset": 64}, {"referenceID": 12, "context": "An alternative approach, moment-matching networks (Li et al., 2015), directly trains the generator to make the statistics of these two distributions match.", "startOffset": 50, "endOffset": 67}, {"referenceID": 2, "context": "Gregor et al. (2015) further introduced the Deep Recurrent Attention Writer (DRAW), extending the VAE approach by incorporating a novel differentiable attention mechanism.", "startOffset": 0, "endOffset": 21}, {"referenceID": 2, "context": "Recently, Denton et al. (2015) have scaled this approach by training conditional GANs at each level of a Laplacian pyramid of images.", "startOffset": 10, "endOffset": 31}, {"referenceID": 2, "context": "Recently, Denton et al. (2015) have scaled this approach by training conditional GANs at each level of a Laplacian pyramid of images. An alternative approach, moment-matching networks (Li et al., 2015), directly trains the generator to make the statistics of these two distributions match. Neither of these approaches, moment-matching or adversarial, directly train the network to reconstruct each training image, so they do not utilize any error measure on an image and its reconstruction. Autoencoders and deep belief nets have one advantage over models that directly generate images, such as GANs: they interpret images in addition to generating images. For example, Krizhevsky & Hinton (2011) used deep autoencoders to discover compact codes that were better for classifying images than using the raw image data.", "startOffset": 10, "endOffset": 697}, {"referenceID": 1, "context": "Some of these metrics are built on complex models of the human visual system, such as the Sarnoff JND model (Lubin, 1998), the visual differences predictor (Daly, 1992), the moving picture quality metric (Van den Branden Lambrecht & Verscheure, 1996), and the perceptual distortion metric (Winkler, 1998).", "startOffset": 156, "endOffset": 168}, {"referenceID": 21, "context": "The most popular of these metrics is the structural similarity metric (SSIM) (Wang et al., 2004), which aims to match", "startOffset": 77, "endOffset": 96}, {"referenceID": 20, "context": "In this work, we focus on the original grayscale SSIM, although there are interesting variations and improvements including color SSIM (Hassan & Bhagvati, 2012), and multiscale SSIM (Wang et al., 2003).", "startOffset": 182, "endOffset": 201}, {"referenceID": 19, "context": "In this work, we focus on the original grayscale SSIM, although there are interesting variations and improvements including color SSIM (Hassan & Bhagvati, 2012), and multiscale SSIM (Wang et al., 2003). The original SSIM metric, as described in Wang et al. (2004), is a pixelwise measure that compares corresponding pixels in two images, denoted x and y, with three comparison functions\u2014luminance (I), contrast (C), and structure (S)\u2014defined in Equation 1:", "startOffset": 183, "endOffset": 264}, {"referenceID": 19, "context": "Following Wang et al. (2004), we chose a square neighborhood of 5 pixels on either side of x or y, resulting in 11\u00d7 11 patches.", "startOffset": 10, "endOffset": 29}, {"referenceID": 14, "context": "The specifics of convolutional autoencoders are described in Masci et al. (2011).", "startOffset": 61, "endOffset": 81}, {"referenceID": 4, "context": "In order to further explore the role of perceptual losses in learning models for image generation, we adapt the DRAW model of Gregor et al. (2015) to be trained with an arbitrary differentiable image similarity metric.", "startOffset": 126, "endOffset": 147}], "year": 2017, "abstractText": "Deep networks are increasingly being applied to problems involving image synthesis, e.g., generating images from textual descriptions, or generating reconstructions of an input image in an autoencoder architecture. Supervised training of image-synthesis networks typically uses a pixel-wise squared error (SE) loss to indicate the mismatch between a generated image and its corresponding target image. We propose to instead use a loss function that is better calibrated to human perceptual judgments of image quality: the structural-similarity (SSIM) score of Wang, Bovik, Sheikh, and Simoncelli (2004). Because the SSIM score is differentiable, it is easily incorporated into gradient-descent learning. We compare the consequences of using SSIM versus SE loss on representations formed in deep autoencoder and recurrent neural network architectures. SSIM-optimized representations yield a superior basis for image classification compared to SE-optimized representations. Further, human observers prefer images generated by the SSIMoptimized networks by nearly a 7:1 ratio. Just as computer vision has advanced through the use of convolutional architectures that mimic the structure of the mammalian visual system, we argue that significant additional advances can be made in modeling images through the use of training objectives that are well aligned to characteristics of human perception.", "creator": "LaTeX with hyperref package"}}}