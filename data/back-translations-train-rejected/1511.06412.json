{"id": "1511.06412", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "19-Nov-2015", "title": "QBDC: Query by dropout committee for training deep supervised architecture", "abstract": "While the current trend is to increase the depth of neural networks to increase their performance, the size of their training database has to grow accordingly. We notice an emergence of tremendous databases, although providing labels to build a training set still remains a very expensive task. We tackle the problem of selecting the samples to be labelled in an online fashion. In this paper, we present an active learning strategy based on query by committee and dropout technique to train a Convolutional Neural Network (CNN). We derive a commmittee of partial CNNs resulting from batchwise dropout runs on the initial CNN. We evaluate our active learning strategy for CNN on MNIST benchmark, showing in particular that selecting less than 30 % from the annotated database is enough to get similar error rate as using the full training set on MNIST. We also studied the robustness of our method against adversarial examples.", "histories": [["v1", "Thu, 19 Nov 2015 22:03:14 GMT  (915kb,D)", "http://arxiv.org/abs/1511.06412v1", null], ["v2", "Thu, 26 Nov 2015 14:19:01 GMT  (915kb,D)", "http://arxiv.org/abs/1511.06412v2", "Submitted to ICLR2016"]], "reviews": [], "SUBJECTS": "cs.LG cs.CV", "authors": ["melanie ducoffe", "frederic precioso"], "accepted": false, "id": "1511.06412"}, "pdf": {"name": "1511.06412.pdf", "metadata": {"source": "CRF", "title": "QBDC: QUERY BY DROPOUT COMMITTEE FOR TRAINING DEEP SUPERVISED ARCHITECTURE", "authors": ["Melanie Ducoffe", "Frederic Precioso"], "emails": ["precioso}@i3s.unice.fr"], "sections": [{"heading": "1 INTRODUCTION", "text": "In recent years, it has been shown that most of them are people who are able to survive themselves, and that they are able to survive themselves. (...) Most of them, who are able to survive themselves, are able to survive themselves. (...) Most of them are able to survive themselves. (...) Most of them are able to survive themselves. (...) Most of them are able to survive themselves. (...) Most of them are able to survive themselves. (...) You have survived yourself. (...) You have survived yourself. (...) You have survived yourself. (...) You have outlived yourself. (...) You have outlived yourself. (...) You have outlived yourself. (...) You have outlived yourself. (...). (...) I have outlived yourself. (...) I have outlived myself. (...) I have outlived myself. (...) I have outlived myself. (...) I have outlived myself. (...) I have outlived myself. (...) I have outlived myself. (...) I have outlived myself. (...) I have outlived myself. (...) I have outlived myself."}, {"heading": "2 RELATED WORK", "text": "Several learning strategies have been proposed to address the problem of selecting the best training samples to optimize the calculation costs of the training phase: query-by-committee, semi-supervised methods, active learning strategies, and more recently active dropout deep architectures."}, {"heading": "2.1 QUERY BY COMMITTEE", "text": "The first algorithm, based on the Query By Committee (QBC) strategy, was proposed by Seung et al. Seung et al. (1992) and yielded two important results: First, the generalization error of a linear classifier for random training samples (referred to in the original article as g) behaves like the inverse power law, g (\u03b1) \u0445 1 / \u03b1, where \u03b1 = P / N with P is the number of training samples considered and N is the dimension of the input space; second, the generalization error of a linear classifier for training samples behaves like a query for rejection strategies, scales such as g (\u03b1) \u0445 e \u2212 \u03b1I with the constant decay are given by the information gain I. Freund et al. (1997) later proved that this property applies to a more general class of learning problems. In this paper we consider batch active learning based on the Query By Committee strategy."}, {"heading": "2.2 SEMI SUPERVISED LEARNING", "text": "Recently, some work has focused on combining unsupervised and supervised learning simultaneously from a limited annotated training set. This semi-supervised technique, which is suitable for deep architectures, reduces the size of the annotated database while competing with revolutionary networks trained on the complete annotated training set as in Rasmus et al. (2015) or Kingma et al. (2014). However, these methods are largely based on generative networks such as Kingma & Welling (2013), which are currently limited to inputs such as images (Kingma et al. (2014)) or texts (Graves (2013))."}, {"heading": "2.3 ACTIVE LEARNING FOR DEEP ARCHITECTURES", "text": "In Zhou et al. (2010), EZhou et al. (ADN) proposed an Active Deep Network to select the most relevant assessments that form the annotated training set for their semi-supervised tasks. First, they train a Restricted Boltzmann Machine and use active learning on top of that: they ask the labels of a randomly selected set of examples and select those who have the least trust on their label."}, {"heading": "2.4 ACTIVE DEEP DROPOUT", "text": "The simultaneous and independent work of Gammelsaeter (2015) also takes into account the query by a committee, using a standardized Multi-Layer Perceptron (MLP) to form a committee. To exceed the accuracy of the MLP, the first layers came from a pre-trained DBN, whose weights are reused every time the MLP is initialized after increasing the marked training set. Although their algorithm is similar to ours, it differs in three main aspects that make the difference in performance: \u2022 The author uses suspenders to form the committee, making it unusable for the evolutionary architecture. \u2022 The committee adds only one sample at a time, creating an unbalanced weight between the training samples."}, {"heading": "3 METHOD", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "3.1 SELECTION CRITERION", "text": "It consists in building a committee for deep architectures to select from a subset of unmarked samples, then ask for the most relevant ones to label, and add them to the training set. Between two selections of new samples, a deep network is formed on the labeled training set, which has its weights and distortions until early stoppage to minimize the error of prediction on an independent validation set. We first train the full network with a random, coming subset of training samples. It has been experimentally demonstrated that CNN initialized with random weights and distortions to minimize the error of prediction on an independent validation set."}, {"heading": "3.2 COMMITTEE DROPOUT WISE BUILT", "text": "The goal of the committee is to be representative of the version room in which the current training network is located. In fact, it shows what a network that correctly predicts the notes of the training set may not always be able to recognize from examples from an uncovered part of the input distribution. It shows that most examples will be informative and the variance of their results will be relatively low. We can now detail how to form the committee. In the early papers describing active learning by committee selection, we first proved that the Perceptron-like learning functions Seung et al. (1992) and later for more general classes of learning problems Freund et al. (1997)."}, {"heading": "4 EXPERIMENTS", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.1 PERFORMANCE ON BENCHMARK DATASETS", "text": "In this section, we analyze the performance of our active learning strategy on the benchmark MNIST. 1MNIST: We train a Convolutionary Neural Network with linear units for the complete NF network. The first two layers are Convolutionary layers with 20 and 40 filters of size (3.3) plus 2 pooling without overlapping. In addition, we have two fully connected layers with 100 neurons in each layer. We used RMSPProp Tieleman & Hinton (2012) with a learning rate of 0.001 and a decay rate of 0.9 with minibatches of size 200. The training set and validation are from 50,000 and 10,000 samples. Finally, when it comes to the hyperparameters of active learning, we have a size 3 committee and init to the upcoming DC training with 10 minibatches."}, {"heading": "4.2 DROPOUT SHORT NOTE", "text": "In order to improve initial performance in the formation of a committee, training the entire network with dropouts Srivastava et al. (2014) seems like an intuitive solution. In fact, when applying the natural dropout proposed by Hinton et al in Srivastava et al. (2014), any model of the dropout board is taken into account in training the entire network. However, dropouts are a technique for regulating large networks with a certain amount of data. Consequently, when confronted with dropouts with a smaller network or a limited training set as in MNIST, QBDC may not be able to learn the features correctly, which misleads the committee in selecting new queries. Figure 4.2, QDBC with dropouts using exactly the same state of hyperparameters and a dropout rate of 0.5 during the first periods, does not generalize, but only after 30 minibatches (when the training database is sufficient) the overall network begins with a better dropout of a similar number of samples and the dropout of Q2."}, {"heading": "4.3 ADVERSARY SENSITIVE", "text": "Like any other machine learning, deep networks, especially CNNs, are sensitive to hostile examples that statistically differ from the natural input distribution. Therefore, any model of the committee can still be deceived by a hostile example and select it to mark and add it to the training examples. Furthermore, as shown in Goodfellow et al. (2014), averaging or abandoning the model does not help the network to be more robust than hostile examples, so the decision-making strategy of the committee itself still does not prevent QBDC from considering and selecting conflicting examples. As we reduce the number of samples on which we train the entire network, a natural question arises: whether only a subset of examples can alter the adjustment to the input distribution, thus weakening the entire network against hostile attacks. To compare the robustness of the standard full classifier, we need to select the full value of the network (the full set of the DC was trained on the full set of the selected one)."}, {"heading": "5 DISCUSSION", "text": "Our QDBC method has shown that the use of an automatic selection of samples can drastically reduce the size of the training set while maintaining accuracy for a fully monitored task. Preliminary results from ongoing experiments underscore the usefulness of our method for more complicated distributions such as CIFAR10. However, one drawback of our method, which is shared by other active learning methods when applied to neural networks, is the need to retrain the entire network each time the committee adds a minibatch of new examples. We have checked whether the accuracy gain in retraining the network was low enough to forget this step and leisure time. Unfortunately, network training is obsolete in this way. As a first tendency, we have found that adding or retrieving some members of the committee does not alter accuracy on average up to an extreme case (committee size 1 or 2). As a future work, we would also like to analyze samples selected by a committee and check their relevance."}, {"heading": "6 CONCLUSION", "text": "In this article, we introduced our own Query-By-Dropout Committee (QBDC) for Deep Architectures. QBDC allows monitored Deep Networks to be trained using reduced annotated data sets by iteratively selecting the most relevant training samples. We have proven the effectiveness of our approach: with only 30% of MNIST's training set, we came close to the state of the art without greatly reducing the accuracy of confronting adverse examples. These promising results are being extended to other machine learning techniques such as semi-monitored classification (PCB) or regulation techniques such as batch normalization to deal with huge data sets such as ImageNet Challenge."}, {"heading": "ACKNOWLEDGMENTS", "text": "We would like to thank the developers of the frameworks Theano Bastien et al. (2012), Blocks and Fuel van Merrie \ufffd nboer et al. (2015), which we used for the experiments."}], "references": [{"title": "The dropout learning algorithm", "author": ["Baldi", "Pierre", "Sadowski", "Peter"], "venue": "Artificial intelligence,", "citeRegEx": "Baldi et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Baldi et al\\.", "year": 2014}, {"title": "Theano: new features and speed improvements", "author": ["Bastien", "Fr\u00e9d\u00e9ric", "Lamblin", "Pascal", "Pascanu", "Razvan", "Bergstra", "James", "Goodfellow", "Ian J", "Bergeron", "Arnaud", "Bouchard", "Nicolas", "Bengio", "Yoshua"], "venue": "Deep Learning and Unsupervised Feature Learning NIPS 2012 Workshop,", "citeRegEx": "Bastien et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Bastien et al\\.", "year": 2012}, {"title": "Representation learning: A review and new perspectives", "author": ["Y. Bengio", "A. Courville", "P. Vincent"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence,", "citeRegEx": "Bengio et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Bengio et al\\.", "year": 2013}, {"title": "On the expressive power of deep architectures", "author": ["Bengio", "Yoshua", "Delalleau", "Olivier"], "venue": "URL http://dx.doi.org/10.1007/978-3-642-24412-4_3", "citeRegEx": "Bengio et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Bengio et al\\.", "year": 2011}, {"title": "Greedy layer-wise training of deep networks", "author": ["Bengio", "Yoshua", "Lamblin", "Pascal", "Popovici", "Dan", "Larochelle", "Hugo"], "venue": "pp. 153\u2013160,", "citeRegEx": "Bengio et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Bengio et al\\.", "year": 2007}, {"title": "Selective sampling using the query by committee", "author": ["Freund", "Yoav", "Seung", "H. Sebastian", "Shamir", "Eli", "Tishby", "Naftali"], "venue": "ISSN 0885-6125. doi: 10.1023/A:1007330508534. URL http://dx.doi.org/10.1023/A:1007330508534", "citeRegEx": "Freund et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Freund et al\\.", "year": 1997}, {"title": "A committee of one. Master\u2019s thesis, NTNU", "author": ["Gammelsaeter", "Martin"], "venue": "Norwegian University of Science and Technology,", "citeRegEx": "Gammelsaeter and Martin.,? \\Q2015\\E", "shortCiteRegEx": "Gammelsaeter and Martin.", "year": 2015}, {"title": "Explaining and harnessing adversarial examples", "author": ["Goodfellow", "Ian J", "Shlens", "Jonathon", "Szegedy", "Christian"], "venue": "arXiv preprint arXiv:1412.6572,", "citeRegEx": "Goodfellow et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Goodfellow et al\\.", "year": 2014}, {"title": "Efficient batchwise dropout training using submatrices", "author": ["Graham", "Ben", "Reizenstein", "Jeremy", "Robinson", "Leigh"], "venue": "arXiv preprint arXiv:1502.02478,", "citeRegEx": "Graham et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Graham et al\\.", "year": 2015}, {"title": "Generating sequences with recurrent neural networks", "author": ["Graves", "Alex"], "venue": "arXiv preprint arXiv:1308.0850,", "citeRegEx": "Graves and Alex.,? \\Q2013\\E", "shortCiteRegEx": "Graves and Alex.", "year": 2013}, {"title": "Some improvements on deep convolutional neural network based image classification", "author": ["Howard", "Andrew G"], "venue": "CoRR, abs/1312.5402,", "citeRegEx": "Howard and G.,? \\Q2013\\E", "shortCiteRegEx": "Howard and G.", "year": 2013}, {"title": "Auto-encoding variational bayes", "author": ["Kingma", "Diederik P", "Welling", "Max"], "venue": "arXiv preprint arXiv:1312.6114,", "citeRegEx": "Kingma et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Kingma et al\\.", "year": 2013}, {"title": "Semi-supervised learning with deep generative models", "author": ["Kingma", "Diederik P", "Mohamed", "Shakir", "Rezende", "Danilo Jimenez", "Welling", "Max"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Kingma et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Kingma et al\\.", "year": 2014}, {"title": "Imagenet classification with deep convolutional neural networks", "author": ["Krizhevsky", "Alex", "Sutskever", "Ilya", "Hinton", "Geoffrey E"], "venue": "In Proceedings of Neural Information Processing Systems (NIPS),", "citeRegEx": "Krizhevsky et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Krizhevsky et al\\.", "year": 2012}, {"title": "Gradient-based learning applied to document recognition", "author": ["LeCun", "Yann", "Bottou", "L\u00e9on", "Bengio", "Yoshua", "Haffner", "Patrick"], "venue": "Proceedings of the IEEE,", "citeRegEx": "LeCun et al\\.,? \\Q1998\\E", "shortCiteRegEx": "LeCun et al\\.", "year": 1998}, {"title": "Diverse ensembles for active learning", "author": ["Melville", "Prem", "Mooney", "Raymond J"], "venue": "In Proceedings of 21st International Conference on Machine Learning (ICML-2004),", "citeRegEx": "Melville et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Melville et al\\.", "year": 2004}, {"title": "Semi-supervised learning with ladder network", "author": ["Rasmus", "Antti", "Valpola", "Harri", "Honkala", "Mikko", "Berglund", "Mathias", "Raiko", "Tapani"], "venue": "arXiv preprint arXiv:1507.02672,", "citeRegEx": "Rasmus et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Rasmus et al\\.", "year": 2015}, {"title": "On random weights and unsupervised feature learning", "author": ["Saxe", "Andrew", "Koh", "Pang W", "Chen", "Zhenghao", "Bhand", "Maneesh", "Suresh", "Bipin", "Ng", "Andrew Y"], "venue": "In Proceedings of the 28th International Conference on Machine Learning", "citeRegEx": "Saxe et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Saxe et al\\.", "year": 2011}, {"title": "Query by committee", "author": ["H.S. Seung", "M. Opper", "H. Sompolinsky"], "venue": "In Proceedings of the Fifth Annual Workshop on Computational Learning Theory,", "citeRegEx": "Seung et al\\.,? \\Q1992\\E", "shortCiteRegEx": "Seung et al\\.", "year": 1992}, {"title": "Very deep convolutional networks for large-scale image recognition", "author": ["Simonyan", "Karen", "Zisserman", "Andrew"], "venue": "CoRR, abs/1409.1556,", "citeRegEx": "Simonyan et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Simonyan et al\\.", "year": 2014}, {"title": "Dropout: A simple way to prevent neural networks from overfitting", "author": ["Srivastava", "Nitish", "Hinton", "Geoffrey", "Krizhevsky", "Alex", "Sutskever", "Ilya", "Salakhutdinov", "Ruslan"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "Srivastava et al\\.,? \\Q1929\\E", "shortCiteRegEx": "Srivastava et al\\.", "year": 1929}, {"title": "Intriguing properties of neural networks", "author": ["Szegedy", "Christian", "Zaremba", "Wojciech", "Sutskever", "Ilya", "Bruna", "Joan", "Erhan", "Dumitru", "Goodfellow", "Ian", "Fergus", "Rob"], "venue": "arXiv preprint arXiv:1312.6199,", "citeRegEx": "Szegedy et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Szegedy et al\\.", "year": 2013}, {"title": "Lecture 6.5-rmsprop: Divide the gradient by a running average of its recent magnitude", "author": ["Tieleman", "Tijmen", "Hinton", "Geoffrey"], "venue": "COURSERA: Neural Networks for Machine Learning,", "citeRegEx": "Tieleman et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Tieleman et al\\.", "year": 2012}, {"title": "Blocks and fuel: Frameworks for deep learning", "author": ["van Merri\u00ebnboer", "Bart", "Bahdanau", "Dzmitry", "Dumoulin", "Vincent", "Serdyuk", "Dmitriy", "Warde-Farley", "David", "Chorowski", "Jan", "Bengio", "Yoshua"], "venue": "CoRR, abs/1506.00619,", "citeRegEx": "Merri\u00ebnboer et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Merri\u00ebnboer et al\\.", "year": 2015}, {"title": "Active deep networks for semi-supervised sentiment classification", "author": ["Zhou", "Shusen", "Chen", "Qingcai", "Wang", "Xiaolong"], "venue": "In Proceedings of the 23rd International Conference on Computational Linguistics: Posters,", "citeRegEx": "Zhou et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Zhou et al\\.", "year": 2010}], "referenceMentions": [{"referenceID": 10, "context": "Indeed, in 2012, the winner was the SuperVision team Krizhevsky et al. (2012) using a deep convolutional neural network with 60 million parameters and making a real breakthrough in the image classification task.", "startOffset": 53, "endOffset": 78}, {"referenceID": 10, "context": "Indeed, in 2012, the winner was the SuperVision team Krizhevsky et al. (2012) using a deep convolutional neural network with 60 million parameters and making a real breakthrough in the image classification task. The huge step forward from SuperVision team has deeply impacted the following contributions to ILSVRC after 2012. In 2014, Simonyan et al. Simonyan & Zisserman (2014) proposed also to use a CNN architecture from 11 up to 19 layers with 133 up to 144 million of parameters.", "startOffset": 53, "endOffset": 379}, {"referenceID": 2, "context": "In their paper Bengio et al. (2007), Bengio et al.", "startOffset": 15, "endOffset": 36}, {"referenceID": 2, "context": "In their paper Bengio et al. (2007), Bengio et al. explain clearly that complex decisions can be seen as highly-varying functions and that the decision making algorithm which intends to comprehend all these variations must be composed of many non-linearities. This specificity of deep architectures also impacts the representation compactness of highly-varying functions. In this same paper, they illustrate how deep architectures outperform shallow ones in terms of number of computational units required and thus training samples, in order to represent a given function. Their examples highlight even the differences in representation compactness between deep architectures, with respect to the problem. Considering the huge amount of parameters of the aforementioned deep architectures to be learnt in order to address ImageNet Challenge, one can understand that the training set has to be huge too. Furthermore, in order to cover the dispersion of the input distribution, strategies to extend the training set have appeared. For instance, when deep networks have made an entrance since 2012 ImageNet challenge, they have been going along with image transformation pre-processing to enlarge the training set Krizhevsky et al. (2012). In 2013, for instance, Howard Howard (2013) started from the SuperVision team winning approach of 2012 and increase the accuracy by 20% with such techniques.", "startOffset": 15, "endOffset": 1236}, {"referenceID": 2, "context": "In their paper Bengio et al. (2007), Bengio et al. explain clearly that complex decisions can be seen as highly-varying functions and that the decision making algorithm which intends to comprehend all these variations must be composed of many non-linearities. This specificity of deep architectures also impacts the representation compactness of highly-varying functions. In this same paper, they illustrate how deep architectures outperform shallow ones in terms of number of computational units required and thus training samples, in order to represent a given function. Their examples highlight even the differences in representation compactness between deep architectures, with respect to the problem. Considering the huge amount of parameters of the aforementioned deep architectures to be learnt in order to address ImageNet Challenge, one can understand that the training set has to be huge too. Furthermore, in order to cover the dispersion of the input distribution, strategies to extend the training set have appeared. For instance, when deep networks have made an entrance since 2012 ImageNet challenge, they have been going along with image transformation pre-processing to enlarge the training set Krizhevsky et al. (2012). In 2013, for instance, Howard Howard (2013) started from the SuperVision team winning approach of 2012 and increase the accuracy by 20% with such techniques.", "startOffset": 15, "endOffset": 1281}, {"referenceID": 2, "context": "In their paper Bengio et al. (2007), Bengio et al. explain clearly that complex decisions can be seen as highly-varying functions and that the decision making algorithm which intends to comprehend all these variations must be composed of many non-linearities. This specificity of deep architectures also impacts the representation compactness of highly-varying functions. In this same paper, they illustrate how deep architectures outperform shallow ones in terms of number of computational units required and thus training samples, in order to represent a given function. Their examples highlight even the differences in representation compactness between deep architectures, with respect to the problem. Considering the huge amount of parameters of the aforementioned deep architectures to be learnt in order to address ImageNet Challenge, one can understand that the training set has to be huge too. Furthermore, in order to cover the dispersion of the input distribution, strategies to extend the training set have appeared. For instance, when deep networks have made an entrance since 2012 ImageNet challenge, they have been going along with image transformation pre-processing to enlarge the training set Krizhevsky et al. (2012). In 2013, for instance, Howard Howard (2013) started from the SuperVision team winning approach of 2012 and increase the accuracy by 20% with such techniques. Even on smaller datasets, such as MNIST, the accuracy was improved when methods have been including a training set extension process also based on image transformations LeCun et al. (1998). Despite these techniques provide a better sampling of the target manifold, despite the \u201cexpressive power of deep architectures\u201d Bengio & Delalleau (2011) and despite their generalization power Bengio et al.", "startOffset": 15, "endOffset": 1584}, {"referenceID": 2, "context": "In their paper Bengio et al. (2007), Bengio et al. explain clearly that complex decisions can be seen as highly-varying functions and that the decision making algorithm which intends to comprehend all these variations must be composed of many non-linearities. This specificity of deep architectures also impacts the representation compactness of highly-varying functions. In this same paper, they illustrate how deep architectures outperform shallow ones in terms of number of computational units required and thus training samples, in order to represent a given function. Their examples highlight even the differences in representation compactness between deep architectures, with respect to the problem. Considering the huge amount of parameters of the aforementioned deep architectures to be learnt in order to address ImageNet Challenge, one can understand that the training set has to be huge too. Furthermore, in order to cover the dispersion of the input distribution, strategies to extend the training set have appeared. For instance, when deep networks have made an entrance since 2012 ImageNet challenge, they have been going along with image transformation pre-processing to enlarge the training set Krizhevsky et al. (2012). In 2013, for instance, Howard Howard (2013) started from the SuperVision team winning approach of 2012 and increase the accuracy by 20% with such techniques. Even on smaller datasets, such as MNIST, the accuracy was improved when methods have been including a training set extension process also based on image transformations LeCun et al. (1998). Despite these techniques provide a better sampling of the target manifold, despite the \u201cexpressive power of deep architectures\u201d Bengio & Delalleau (2011) and despite their generalization power Bengio et al.", "startOffset": 15, "endOffset": 1739}, {"referenceID": 2, "context": "In their paper Bengio et al. (2007), Bengio et al. explain clearly that complex decisions can be seen as highly-varying functions and that the decision making algorithm which intends to comprehend all these variations must be composed of many non-linearities. This specificity of deep architectures also impacts the representation compactness of highly-varying functions. In this same paper, they illustrate how deep architectures outperform shallow ones in terms of number of computational units required and thus training samples, in order to represent a given function. Their examples highlight even the differences in representation compactness between deep architectures, with respect to the problem. Considering the huge amount of parameters of the aforementioned deep architectures to be learnt in order to address ImageNet Challenge, one can understand that the training set has to be huge too. Furthermore, in order to cover the dispersion of the input distribution, strategies to extend the training set have appeared. For instance, when deep networks have made an entrance since 2012 ImageNet challenge, they have been going along with image transformation pre-processing to enlarge the training set Krizhevsky et al. (2012). In 2013, for instance, Howard Howard (2013) started from the SuperVision team winning approach of 2012 and increase the accuracy by 20% with such techniques. Even on smaller datasets, such as MNIST, the accuracy was improved when methods have been including a training set extension process also based on image transformations LeCun et al. (1998). Despite these techniques provide a better sampling of the target manifold, despite the \u201cexpressive power of deep architectures\u201d Bengio & Delalleau (2011) and despite their generalization power Bengio et al. (2013), the work of Szegedy et al.", "startOffset": 15, "endOffset": 1799}, {"referenceID": 2, "context": "In their paper Bengio et al. (2007), Bengio et al. explain clearly that complex decisions can be seen as highly-varying functions and that the decision making algorithm which intends to comprehend all these variations must be composed of many non-linearities. This specificity of deep architectures also impacts the representation compactness of highly-varying functions. In this same paper, they illustrate how deep architectures outperform shallow ones in terms of number of computational units required and thus training samples, in order to represent a given function. Their examples highlight even the differences in representation compactness between deep architectures, with respect to the problem. Considering the huge amount of parameters of the aforementioned deep architectures to be learnt in order to address ImageNet Challenge, one can understand that the training set has to be huge too. Furthermore, in order to cover the dispersion of the input distribution, strategies to extend the training set have appeared. For instance, when deep networks have made an entrance since 2012 ImageNet challenge, they have been going along with image transformation pre-processing to enlarge the training set Krizhevsky et al. (2012). In 2013, for instance, Howard Howard (2013) started from the SuperVision team winning approach of 2012 and increase the accuracy by 20% with such techniques. Even on smaller datasets, such as MNIST, the accuracy was improved when methods have been including a training set extension process also based on image transformations LeCun et al. (1998). Despite these techniques provide a better sampling of the target manifold, despite the \u201cexpressive power of deep architectures\u201d Bengio & Delalleau (2011) and despite their generalization power Bengio et al. (2013), the work of Szegedy et al. Szegedy et al. (2013) demonstrated this was not enough.", "startOffset": 15, "endOffset": 1849}, {"referenceID": 17, "context": "The first algorithm based on Query By Committee (QBC) strategy has been proposed by Seung et al. Seung et al. (1992). They proved two important results: first, the generalization error (denoted g in the original article) of a linear classifier for random training samples behaves as the inverse power law, g(\u03b1) \u223c 1/\u03b1 where \u03b1 = P/N with P the number of training samples considered and N the dimension of the input space; second, the generalization error of a linear classifier for training samples selectect through a query by committee strategy, scales like g(\u03b1) \u223c e\u2212\u03b1I with the constant decay is given by the information gain I .", "startOffset": 84, "endOffset": 117}, {"referenceID": 5, "context": "Later Freund et al. Freund et al. (1997) proved that this property hold for a more general class of learning problems.", "startOffset": 6, "endOffset": 41}, {"referenceID": 14, "context": "Those semi supervised technique adapted for deep architectures are leaveraging the size of the annotated database while being competitive with convolutional networks trained on the full annotated training set like done in Rasmus et al. (2015) or Kingma et al.", "startOffset": 222, "endOffset": 243}, {"referenceID": 11, "context": "(2015) or Kingma et al. (2014). However those methods are mostly based on generative networks Kingma & Welling (2013), which are at the present time adapted to a restricted are of inputs like images (Kingma et al.", "startOffset": 10, "endOffset": 31}, {"referenceID": 11, "context": "(2015) or Kingma et al. (2014). However those methods are mostly based on generative networks Kingma & Welling (2013), which are at the present time adapted to a restricted are of inputs like images (Kingma et al.", "startOffset": 10, "endOffset": 118}, {"referenceID": 11, "context": "(2015) or Kingma et al. (2014). However those methods are mostly based on generative networks Kingma & Welling (2013), which are at the present time adapted to a restricted are of inputs like images (Kingma et al. (2014)) or texts( Graves (2013)).", "startOffset": 10, "endOffset": 221}, {"referenceID": 11, "context": "(2015) or Kingma et al. (2014). However those methods are mostly based on generative networks Kingma & Welling (2013), which are at the present time adapted to a restricted are of inputs like images (Kingma et al. (2014)) or texts( Graves (2013)).", "startOffset": 10, "endOffset": 246}, {"referenceID": 24, "context": "In Zhou et al. (2010), EZhou et al.", "startOffset": 3, "endOffset": 22}, {"referenceID": 17, "context": "It has experimentally been shown Saxe et al. (2011) that CNN initialized with random weights already hold some capability of discriminating classes (up to a certain accuracy of course) so that the initial annotated subset might be left empty, however we have chosen to start from an initial random subset of the training set to converge faster to relevant samples and benefit from the sample selection of our committee based approach from the first epochs.", "startOffset": 33, "endOffset": 52}, {"referenceID": 17, "context": "It has experimentally been shown Saxe et al. (2011) that CNN initialized with random weights already hold some capability of discriminating classes (up to a certain accuracy of course) so that the initial annotated subset might be left empty, however we have chosen to start from an initial random subset of the training set to converge faster to relevant samples and benefit from the sample selection of our committee based approach from the first epochs. After several epochs, the performance of this initial network does not increase anymore on the validation set. In order to improve the accuracy, we then inject a new minibatch of samples. Note that adding a minibatch of examples differ from Gammelsaeter (2015) in which samples are added to the training set one at a time.", "startOffset": 33, "endOffset": 718}, {"referenceID": 17, "context": "It has experimentally been shown Saxe et al. (2011) that CNN initialized with random weights already hold some capability of discriminating classes (up to a certain accuracy of course) so that the initial annotated subset might be left empty, however we have chosen to start from an initial random subset of the training set to converge faster to relevant samples and benefit from the sample selection of our committee based approach from the first epochs. After several epochs, the performance of this initial network does not increase anymore on the validation set. In order to improve the accuracy, we then inject a new minibatch of samples. Note that adding a minibatch of examples differ from Gammelsaeter (2015) in which samples are added to the training set one at a time. Furthermore, adding minibatches of samples fosters gpu parallelization for training deep architectures, plus balances every sample in the training set when applying mean gradient descent. We hence select a random subset of K samples from the rest of the training set. Let us notice than we are not requesting here the labels of the samples(Zhou et al. (2010)).", "startOffset": 33, "endOffset": 1139}, {"referenceID": 17, "context": "It has experimentally been shown Saxe et al. (2011) that CNN initialized with random weights already hold some capability of discriminating classes (up to a certain accuracy of course) so that the initial annotated subset might be left empty, however we have chosen to start from an initial random subset of the training set to converge faster to relevant samples and benefit from the sample selection of our committee based approach from the first epochs. After several epochs, the performance of this initial network does not increase anymore on the validation set. In order to improve the accuracy, we then inject a new minibatch of samples. Note that adding a minibatch of examples differ from Gammelsaeter (2015) in which samples are added to the training set one at a time. Furthermore, adding minibatches of samples fosters gpu parallelization for training deep architectures, plus balances every sample in the training set when applying mean gradient descent. We hence select a random subset of K samples from the rest of the training set. Let us notice than we are not requesting here the labels of the samples(Zhou et al. (2010)). A committee of deep networks assigns then a score to each sample from the selected minibatch, this score being related to the impact of the considered sample on the training of the deep architecture. If we denote by batch size the number of samples in a minibatch, we then request the labels of the batch size samples having the greatest score among the K samples. The score of a sample correspond to the number of models in the committee whose prediction differs from the majority prediction among the committee members. The quality of the score is related to both the size of the committee and the correlation between the models Melville & Mooney (2004).", "startOffset": 33, "endOffset": 1797}, {"referenceID": 17, "context": "In the early papers describing active learning through committee selection, convergence and better result against randomness have been proven, first for perceptron-like learning functions Seung et al. (1992) and later for more general classes of learning problems Freund et al.", "startOffset": 188, "endOffset": 208}, {"referenceID": 5, "context": "(1992) and later for more general classes of learning problems Freund et al. (1997). However, for those results to hold, each model of our committee has to lie in the current version space defined by the annotated training set (which is the set of networks built from the same \u2019architecture\u2019 and making no mistake on the current training set).", "startOffset": 63, "endOffset": 84}, {"referenceID": 8, "context": "To initiate a partial CNN while getting rid of the computation cost due to backpropagation, we apply batchwise dropout Graham et al. (2015) on our full network.", "startOffset": 119, "endOffset": 140}, {"referenceID": 8, "context": "To initiate a partial CNN while getting rid of the computation cost due to backpropagation, we apply batchwise dropout Graham et al. (2015) on our full network. The batchwise dropout Graham et al. (2015) is a version of dropout where we use a unique bernouilli mask to discard neurons for each sample in the minibatch.", "startOffset": 119, "endOffset": 204}, {"referenceID": 8, "context": "To initiate a partial CNN while getting rid of the computation cost due to backpropagation, we apply batchwise dropout Graham et al. (2015) on our full network. The batchwise dropout Graham et al. (2015) is a version of dropout where we use a unique bernouilli mask to discard neurons for each sample in the minibatch. Thus the batchwise dropout reduce quadratically in the percentage of kept neurons the number of parameters in the architecture. Moreover dropout, when applied on convolutional layers remove neurons independently given the spatial locations. Whereas batchwise dropout is spatially dependant, switching on or off filters so to discard neurons obtained through the same filter. It is thus preserving the consistency in a CNN architecture. This technique is required to extend QBDC on CNN architecture which may be one of the reasons for the difference in the accuracy with Gammelsaeter (2015) that solely applied the dropout committee on MLP architectures.", "startOffset": 119, "endOffset": 909}, {"referenceID": 8, "context": "To initiate a partial CNN while getting rid of the computation cost due to backpropagation, we apply batchwise dropout Graham et al. (2015) on our full network. The batchwise dropout Graham et al. (2015) is a version of dropout where we use a unique bernouilli mask to discard neurons for each sample in the minibatch. Thus the batchwise dropout reduce quadratically in the percentage of kept neurons the number of parameters in the architecture. Moreover dropout, when applied on convolutional layers remove neurons independently given the spatial locations. Whereas batchwise dropout is spatially dependant, switching on or off filters so to discard neurons obtained through the same filter. It is thus preserving the consistency in a CNN architecture. This technique is required to extend QBDC on CNN architecture which may be one of the reasons for the difference in the accuracy with Gammelsaeter (2015) that solely applied the dropout committee on MLP architectures. In Graham et al. (2015), a batchwise dropout committee has already been envisaged for future work in order to fasten the testing while averaging the prediction on the full network Baldi & Sadowski (2014).", "startOffset": 119, "endOffset": 997}, {"referenceID": 8, "context": "To initiate a partial CNN while getting rid of the computation cost due to backpropagation, we apply batchwise dropout Graham et al. (2015) on our full network. The batchwise dropout Graham et al. (2015) is a version of dropout where we use a unique bernouilli mask to discard neurons for each sample in the minibatch. Thus the batchwise dropout reduce quadratically in the percentage of kept neurons the number of parameters in the architecture. Moreover dropout, when applied on convolutional layers remove neurons independently given the spatial locations. Whereas batchwise dropout is spatially dependant, switching on or off filters so to discard neurons obtained through the same filter. It is thus preserving the consistency in a CNN architecture. This technique is required to extend QBDC on CNN architecture which may be one of the reasons for the difference in the accuracy with Gammelsaeter (2015) that solely applied the dropout committee on MLP architectures. In Graham et al. (2015), a batchwise dropout committee has already been envisaged for future work in order to fasten the testing while averaging the prediction on the full network Baldi & Sadowski (2014). The main advantage is to obtain a committee which the committee has the same architecture than the full network, with members having some zero constraints on several connections.", "startOffset": 119, "endOffset": 1177}, {"referenceID": 14, "context": "95 %, LeCun et al. (1998)), the error rate on the QBDC full architecture (a network trained using only the samples selected with QBDC) is competitive with the state of the art : we get an average error rate of 1.", "startOffset": 6, "endOffset": 26}, {"referenceID": 20, "context": "To improve the initial performance when instantiating a committee, training the full network with dropout Srivastava et al. (2014) appears like an intuitive solution.", "startOffset": 106, "endOffset": 131}, {"referenceID": 20, "context": "To improve the initial performance when instantiating a committee, training the full network with dropout Srivastava et al. (2014) appears like an intuitive solution. Indeed when applying natural dropout proposed by Hinton et al. in Srivastava et al. (2014), every possible model of the committee will be considered while training the full network.", "startOffset": 106, "endOffset": 258}, {"referenceID": 7, "context": "out in Goodfellow et al. (2014), model averaging or dropout are not helping the network to be more robust from adversarial examples so that the committee decision strategy itself is still not preserving QBDC from considering and selecting adversarial examples.", "startOffset": 7, "endOffset": 32}, {"referenceID": 7, "context": "To generate efficiently approximate adversary we used the fast gradient sign method proposed by Goodfellow et al. in (Goodfellow et al. (2014), see 3).", "startOffset": 96, "endOffset": 143}, {"referenceID": 7, "context": "Table 3: formula of the fast gradient sign method Goodfellow et al. (2014) \u2022 \u0398 the parameters of the model \u2022 x the input of the model \u2022 y the targets associated with x \u2022 J(\u0398, x, y) the cost used to train the network \u2022 \u03b7 the max norm constrained perturbation \u03b7 = sign(\u2206xJ(\u0398, x, y))", "startOffset": 50, "endOffset": 75}, {"referenceID": 1, "context": "We would like to thank the developpers of the frameworks Theano Bastien et al. (2012), Blocks and Fuel van Merri\u00ebnboer et al.", "startOffset": 64, "endOffset": 86}, {"referenceID": 1, "context": "We would like to thank the developpers of the frameworks Theano Bastien et al. (2012), Blocks and Fuel van Merri\u00ebnboer et al. (2015) which we used for the experi- ments.", "startOffset": 64, "endOffset": 133}], "year": 2017, "abstractText": "While the current trend is to increase the depth of neural networks to increase their performance, the size of their training database has to grow accordingly. We notice an emergence of tremendous databases, although providing labels to build a training set still remains a very expensive task. We tackle the problem of selecting the samples to be labelled in an online fashion. In this paper, we present an active learning strategy based on query by committee and dropout technique to train a Convolutional Neural Network (CNN). We derive a commmittee of partial CNNs resulting from batchwise dropout runs on the initial CNN. We evaluate our active learning strategy for CNN on MNIST benchmark, showing in particular that selecting less than 30 % from the annotated database is enough to get similar error rate as using the full training set on MNIST. We also studied the robustness of our method against adversarial examples.", "creator": "LaTeX with hyperref package"}}}