{"id": "1511.06418", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "19-Nov-2015", "title": "Binding via Reconstruction Clustering", "abstract": "Disentangled distributed representations of data are desirable for machine learning, since they are more expressive and can generalize from fewer examples. However, for complex data, the distributed representations of multiple objects present in the same input can interfere and lead to ambiguities, which is commonly referred to as the binding problem. We argue for the importance of the binding problem to the field of representation learning, and develop a probabilistic framework that explicitly models inputs as a composition of multiple objects. We propose an unsupervised algorithm that uses denoising autoencoders to dynamically bind features together in multi-object inputs through an Expectation-Maximization-like clustering process. The effectiveness of this method is demonstrated on artificially generated datasets of binary images, showing that it can even generalize to bind together new objects never seen by the autoencoder during training.", "histories": [["v1", "Thu, 19 Nov 2015 22:13:11 GMT  (1541kb,D)", "https://arxiv.org/abs/1511.06418v1", "11 pages, plus 9 pages Appendix"], ["v2", "Thu, 26 Nov 2015 23:35:10 GMT  (1537kb,D)", "http://arxiv.org/abs/1511.06418v2", "11 pages, plus 9 pages Appendix"], ["v3", "Thu, 7 Jan 2016 20:48:53 GMT  (1537kb,D)", "http://arxiv.org/abs/1511.06418v3", "12 pages, plus 9 pages Appendix"], ["v4", "Wed, 20 Jan 2016 19:31:17 GMT  (1811kb,D)", "http://arxiv.org/abs/1511.06418v4", "12 pages, plus 12 pages Appendix"]], "COMMENTS": "11 pages, plus 9 pages Appendix", "reviews": [], "SUBJECTS": "cs.LG cs.NE", "authors": ["klaus greff", "rupesh kumar srivastava", "j\\\"urgen schmidhuber"], "accepted": false, "id": "1511.06418"}, "pdf": {"name": "1511.06418.pdf", "metadata": {"source": "CRF", "title": null, "authors": ["RECONSTRUCTION CLUSTERING", "Klaus Greff", "Rupesh Kumar Srivastava"], "emails": ["klaus@idsia.ch", "rupesh@idsia.ch", "juergen@idsia.ch"], "sections": [{"heading": "1 THE BINDING PROBLEM", "text": "Two important characteristics of good representations are that they are distributed and untangled. Distributed representations (Hinton, 1984) are much more meaningful than local ones, requiring exponentially fewer characteristics to capture the same space. Unfortunately, this concept is closely linked to the invariance and facilitation of further processing, because many characteristics we might be interested in are invariant under a variety of transformations (Bengio et al., 2013a). Unfortunately, distributed representations can interfere with different independent characteristics and lead to ambiguities. The binding problem references to these ambiguities resulting from the superposition of multiple distributed representations."}, {"heading": "2 RECONSTRUCTION CLUSTERING", "text": "This section describes Reconstruction Clustering (RC), a formal framework for addressing the attachment problem as a cluster problem. For easier explanation, we refer to inputs as images and the individual dimensions of an input as pixels, although the framework is not limited to visual inputs. It is based on two insights: First, if the image were segmented into its components, there would be no attachment problem. Second, the intuitive notion of an object as a group of mutually predictive pixels can be formalized."}, {"heading": "2.1 IMAGES AS COMPOSITIONS", "text": "The first central idea behind RC is to model images in such a way that they are composed of several independent objects, each pixel belonging to one of them. In contrast to classical segmentation, where each pixel is assigned to a predefined class, the goal here is simply to separate different objects, thereby avoiding any ambiguities that might arise from overlapping their representations. Of course, the information about what objects are present and what pixels they consist of is unknown in practical applications. Therefore, the goal for each image is to derive both the object representations and the corresponding pixel assignments. Formally, we introduce a binary latent vector zi for each pixel xi, which indicates to which of the K objects it belongs. Therefore, zi = (zi1, zi2,., ziK) can derive both the object representations and the corresponding pixel assignments."}, {"heading": "2.2 OBJECTS", "text": "The second central idea of the RC is to concretize this idea by mutual predictability of pixels. Intuitively, knowing some of the pixel values that belong to one object helps to predict the others. An example is shown in Figure 1c, where the corrupt pixels in the lower left corner of the square could be reconstructed from knowledge of the rest of the square, but not from one of the triangles. Therefore, we define an object as a group of pixels that help each other predict, but no information about pixels outside of this group.The predictability we use here is derived from the structure of the underlying data distribution. Knowledge of this structure is also exactly what is needed to remove corruption from an image. Based on this knowledge, we propose to use a Denozing Autoencode (DAE) to measure predictability."}, {"heading": "2.3 DENOISING AUTOENCODER", "text": "Let f be the encoder and g be the decoder of a PCS, so \u03b8 = f (x) is the encoded representation of the input x and \u00b5 = g (\u03b8) is the decoded output. The PCS is trained to remove corruption from the images of individual objects, thus learning a local model of data generation distribution (Vincent et al., 2008; Bengio et al., 2013b). After training, the same PCS is used for each of the clusters to obtain predictions for the cluster k for pixel i, where the object in the cluster k is represented by phenomenon: P (x | phenomenon) = N \u0441i = 1 P (xi | phenomenon) = N \u0432i = 1 P (xi | \u00b5ik) (2) Here it is assumed that xi's are independent of the latent variables we obtain: P (x | Z) = N \u0141i = 1 K (xi | Mikik) zik (3)."}, {"heading": "2.4 CLUSTERING", "text": "We can now sketch a cluster algorithm that estimates the object identities and the associated pixel assignments. Formally, we strive to maximize the complete data log probability: logP (x, Z | \u00b5, \u03c0) = N \u2211 i = 1 K \u2211 k = 1 zik (logP (xi | \u00b5i) + log \u03c0k) (4) This can be done in an iterative procedure in which we first randomly initialize the latent cluster assignments Z and then alternate between the following two steps: 1. Apply the auto encoder to each of the K images assigned to the clusters to obtain a new estimate of the K object presentations. (R-Step) 2. Re-assign the pixels to the clusters according to their reconstruction accuracy. (E-Step)"}, {"heading": "2.4.1 RECONSTRUCTION STEP", "text": "The R-step uses the encoder to generate a new object representation from each of the K-frames associated with each cluster. We call this representation of a partial image because the encoder only sees as much of each pixel of the original image as the current cluster has been softly assigned to it. The PCS then denounces the \"corruption\" caused by the cluster assignment. Therefore, the R-step is given by the following formula, which denotes point-by-point multiplication: \u03b8k = f (\u03b3k x), (5) Unfortunately, this step cannot be guaranteed to increase the expected logging probability, as the PCS map only passes into regions with a lower probability of occurrence. Furthermore, this property applies only to the overall image and not to all subsets of pixels. Therefore, convergence cannot be proven and the RC is not an algorithm for maximizing expectations (Dempel et al, 1977). Nevertheless, empirical results reliably show convergence (4.2)."}, {"heading": "2.4.2 ESTIMATION STEP", "text": "In the E-step, for each pixel xi, the rear gics of Z are given, indicating the data and the predictions \u00b5i = {g (\u03b81) i,.., g (\u03b8K) i} of the auto encoders based on the object representations is\u03b3ik = P (zik = 1 | xi, \u00b5i, \u03c0) = P (xi | zik = 1, \u00b5i) P (zik = 1 | \u03c0) P (xi | \u00b5i, \u03c0). (6) In this paper, we assume that the pixels x are binary and that the predictions of the network \u00b5 correspond to the mean value of a binomial distribution. Subsequently, the soft mapping of the pixels to the K-clusters takes place: \u03b3ik = \u00b5xiik (1 \u2212 \u00b5ik) 1 \u2212 xik K = 1 \u00b5xiij (1 \u2212 \u00b5ij) 1 \u2212 xiij. (7)"}, {"heading": "3 EXPERIMENTS", "text": "We examined the RC using a series of artificially created datasets consisting of binary images of varying complexity. For each dataset, a PCS was trained to remove salt-and-pepper noise from images with individual objects. The autoencoders used were fully connected upstream neural networks with a single hidden layer and sigmoid output units. A random search was used to select appropriate hyperparameters (see Appendix for details). The best PCS obtained for each dataset was used to reconstruct clusters of 1000 test images containing multiple objects, and binding performance was assessed using grouptruth object identities.The entire code for this essay (including the creation of the datasets and images) is available online at GitHub.com / Qwlouse / Binding."}, {"heading": "3.1 DATASETS", "text": "Representative examples from the datasets are shown in Figure 3.Simple Superposition A collection of simple pixel patterns, two of which overlap; from Rao et al. (2008), a simple dataset with no translations but significant overlaps between patterns; Shapes Taken from Reichert & Serre (2013), which randomly places three shapes (, 4,5) in an image (possibly overlapping), testing the binding of shapes under translation invariance and varying overlaps; Bars Introduced by Fo \ufffd ldiak (1990) to demonstrate the unattended learning of independent components of an image; and Reichert & Serre (2013), which uses 6 horizontal and 6 vertical lines in random positions in the image.Corners This dataset consists of 8 key data placed in random orientations and positions."}, {"heading": "3.2 EVALUATION", "text": "As the data is generated, a ground truth segmentation is available for each image, and for the binding task, all pixels of the same object should be bundled together. We evaluated performance by measuring the Adjusted Mutual Information (AMI; Vinh et al., 2010) between the actual segmentation and the result of the binding we call the score, which measures how well two cluster assignments match and assumes a value of 1 if they are equivalent, and 0 if their match happens to be the expected one. Only pixels that uniquely belong to an object were counted, disregarding background pixels and regions where multiple objects overlap."}, {"heading": "4 RESULTS", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.1 SCORES", "text": "Figure 4a shows the mean values achieved for each dataset on average over 100 runs using RC. Values achieved with different decisions about the number of clusters K are consistent across runs, so the standard deviations are very small and barely visible. The optimal number of clusters is two for Simple Superposition and MNIST + Shape, three for Multi MNIST and Shapes, five for Corners and 12 for Bars. Values are higher than 0.5 for all datasets and higher than 0.8 for four of the six datasets demonstrating the RC's ability to successfully connect objects."}, {"heading": "4.2 CONVERGENCE", "text": "Figure 4b shows the convergence of the mean log probability via RC iterations on the form dataset. Convergence is fast, typically within 5-10 iterations, depending on the selected number of clusters K and the dataset (not shown). As expected, the final probability is highest if the number of clusters matches the number of objects in the form dataset (3) and matches the results from Figure 4a. Probability is much lower for k = 2 than for k = 3 and drops slightly again if we use k = 5. Probability for k = 12 is significantly lower. In some cases, the correct choice of k did not result in the highest probability, but in general this match seemed to hold. If the number of objects is unknown, this trend can be used to determine the correct number of clusters."}, {"heading": "4.3 QUALITATIVE ANALYSIS", "text": "Figure 5 shows some examples of RC passes on the shapes dataset for qualitative evaluation. The initial cluster assignment is random, so all observed structure is due to the cluster process. The final cluster assignment is well true even in cases with significant overlaps. Again, it is noteworthy that RC converges quickly (within 5 iterations)."}, {"heading": "4.4 LOSS VS SCORE", "text": "To this end, we trained 100 DAEs with the same architecture on each random learning rate and initialization dataset, and then performed an RC on each individual dataset. Figure 6a shows the relationship between the denozing loss and the binding score for each dataset. It is observed that a lower loss correlates positively with a higher score for all datasets, suggesting that denozing is a suitable surrogate target. We added a regression line to indicate this relationship for each dataset, even though it does not look nearly linear for MNIST + Shape and Multi MNIST. Instead, the individual points are roughly aligned on a curve, suggesting that there is a direct but complex interplay between the denoizing performance and the score."}, {"heading": "4.5 TRAINING ON MULTIPLE OBJECTS", "text": "So far, the DAEs have been trained on single object images, in order to then bind objects in multi-object images. In general, it is desirable not to need single object images for training and to be able to use each image directly without this limitation. This would eliminate the last bit of monitoring and make the RC a truly uncontrolled method. Why would this work at all? On the surface, it seems that the RC would rely on the PCS to prefer individual objects in order to work correctly. However, even if each cluster tries to reconstruct each object, there will be small asymmetries due to the differences in the inputs it sees. Since no object carries information about the shape and position of another object in our data sets, this results in differences in the prediction quality of the objects. The resulting difference in the reconstruction quality is then amplified by the RC and can still lead to a separation of the objects. To test this scenario, we performed a new random search to find the best way to combine the DAE with the hyperparameter of the small object in the case."}, {"heading": "4.6 GENERALIZATION TO A NEW DOMAIN", "text": "A key intuition behind our approach to binding is that the low structures learned by the model will generalize into new and invisible configurations, and the evaluation of invisible test sets has shown that this is true, but we can go a step further. We can test what happens when we confront our method with novel objects that the auto-encoders have not been trained on. We have also shown RC running on several images with non-digits, using a PCE that has been trained on the Multi-MNIST dataset. Figure 7 shows that RC connects letters and circles \"correctly.\" We also show images where the resulting binding deviates from our expectations. It seems that the network has mainly learned to bind due to spatial proximity, with a slight tendency to vertical proximity. This is to be expected, as it has so far seen only digits of roughly the same size, and because the autocode used is very limited to our future. Nevertheless, it is possible that the network is completely mutated by being very similar in terms to others, it is very spatially."}, {"heading": "5 RELATIONSHIP TO OTHER METHODS", "text": "The binding problem and its possible solutions are a long debate in the neuroscience literature (see e.g. Milner (1974); von der Malsburg (1981); Gray (1999); Treisman (1999); Di Lollo (2012). However, an important thread in the work on bonding was inspired by the temporal correlation theory (von der Malsburg, 1981), which relies on the use of synchronous oscillations to bind neuronal activities. Malsburg (1995) provides an overview of these ideas, which were implemented using complex, estimated activations in neural networks to jointly encode the shot rate and phase (Rao et al, 2008; Reichert & Serre, 2013)."}, {"heading": "6 CONCLUSION AND FUTURE WORK", "text": "Compared to many previous solutions to the binding problem, this framework is mathematically rigorous, integrates well with current learning methods of representation, and is effective for a wide range of binary image data. While a typical learning method of representation (such as a denoting autoencoder) learns a static binding of characteristics, Reconstruction Clustering uses it to perform iterative dynamic binding for each input example by introducing interactions between the statically bound characteristics extracted from the autoencoder. Specifically, this interaction enables dynamic binding of feature combinations that the autoencoder has never seen before. This paper lays the groundwork for many concrete lines of future exploration. Handling real bound characteristics is an important next step in extending the RC to natural data. The use of more powerful autoencoders will also play a key role."}, {"heading": "ACKNOWLEDGMENTS", "text": "We thank Jan Koutn\u0131k, Sjoerd van Steenkiste, Boyan Beronov and Julian Zilly for their helpful discussions and comments. This project was funded by the EU project NASCENCE (FP7-ICT-317662)."}, {"heading": "A RECONSTRUCTION CLUSTERING DERIVATION", "text": "This section contains a more detailed derivation of reconstruction clustering (RC) for binary inputs. It follows the notation and derivation of an expectation maximization (EM) algorithm wherever possible. Only for the M step, RC deviates from EM.Consider N random binary variables (one for each pixel), which are distributed according to a mixture of K Bernoulli distributions with means \u00b5i = (\u00b5i1, \u00b5i2,., \u00b5iK) and the mixing of coefficients \u03c0 = (one for each pixel), the sum K) yields a sum of K = 1. Within the framework of this model, the data probability is given in view of the parameters: P (xi | \u00b5i2,.,.)"}, {"heading": "B TRAINING DETAILS", "text": "The code for this work can be found on GitHub.B.1 TRAINING DENOISING AUTONCODERS \u2022 simple feed forward fully connected NNs \u2022 with sigmoid output layer \u2022 loss is Binomial Cross Entropy Error \u2022 trained with SGD \u2022 minibatch size 100 \u2022 salt & pepper noise \u2022 early stop when validation BinomialCEE does not decrease for more than 10 epochs.2 RANDOM SEARCHThere are several hyperparameters to choose for the denoising autoencoders."}, {"heading": "C MULTI OBJECT TRAINING", "text": "If you train the DAEs on images with multiple objects, it is less obvious why RC should result in a separation of the objects. It seems that the autoencoder should always try to reconstruct the entire image including all objects. And, when we run normal (soft) RC, we actually see that after a few iterations each pixel is represented equally by each cluster. By switching to hard cluster assignments, we eliminate this stable state and force the clusters to compete more for the pixels.Combined with the fact that objects in our datasets do not carry information about other objects, this results in a greater amplification of the initial differences in reconstruction quality. In Figure 10, this process can be seen on the shapes of the dataset. Note that the hard RC converts even faster, but generally leads to worse performance in our datasets. D ADDITIONAL FIGURES0.4 0.5 0.6 0.7 0.9 test pattern confice200 test samples by 10000 Sorted 10000 10000 Scores000 10000 10D mag 10D"}], "references": [{"title": "SLIC superpixels compared to state-of-the-art superpixel methods", "author": ["Achanta", "Radhakrishna", "Shaji", "Appu", "Smith", "Kevin", "Lucchi", "Aurelien", "Fua", "Pascal", "Susstrunk", "Sabine"], "venue": "Pattern Analysis and Machine Intelligence, IEEE Transactions on,", "citeRegEx": "Achanta et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Achanta et al\\.", "year": 2012}, {"title": "Neural machine translation by jointly learning to align and translate", "author": ["Bahdanau", "Dzmitry", "Cho", "Kyunghyun", "Bengio", "Yoshua"], "venue": "arXiv preprint arXiv:1409.0473,", "citeRegEx": "Bahdanau et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2014}, {"title": "Finding minimum entropy codes", "author": ["Barlow", "Horace B", "Kaushal", "Tej P", "Mitchison", "Graeme J"], "venue": "Neural Computation,", "citeRegEx": "Barlow et al\\.,? \\Q1989\\E", "shortCiteRegEx": "Barlow et al\\.", "year": 1989}, {"title": "Learning iterative image reconstruction in the Neural Abstraction Pyramid", "author": ["Behnke", "Sven"], "venue": "International Journal of Computational Intelligence and Applications,", "citeRegEx": "Behnke and Sven.,? \\Q2001\\E", "shortCiteRegEx": "Behnke and Sven.", "year": 2001}, {"title": "Scaling learning algorithms towards AI", "author": ["Bengio", "Yoshua", "LeCun", "Yann", "others"], "venue": "Large-scale kernel machines,", "citeRegEx": "Bengio et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Bengio et al\\.", "year": 2007}, {"title": "Representation learning: A review and new perspectives", "author": ["Bengio", "Yoshua", "Courville", "Aaron", "Vincent", "Pierre"], "venue": "Pattern Analysis and Machine Intelligence, IEEE Transactions on,", "citeRegEx": "Bengio et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Bengio et al\\.", "year": 2013}, {"title": "Generalized denoising auto-encoders as generative models", "author": ["Bengio", "Yoshua", "Yao", "Li", "Alain", "Guillaume", "Vincent", "Pascal"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Bengio et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Bengio et al\\.", "year": 2013}, {"title": "Maximum likelihood from incomplete data via the EM algorithm. Journal of the royal statistical society", "author": ["Dempster", "Arthur P", "Laird", "Nan M", "Rubin", "Donald B"], "venue": "Series B (methodological),", "citeRegEx": "Dempster et al\\.,? \\Q1977\\E", "shortCiteRegEx": "Dempster et al\\.", "year": 1977}, {"title": "The feature-binding problem is an ill-posed problem", "author": ["Di Lollo", "Vincent"], "venue": "Trends in Cognitive Sciences,", "citeRegEx": "Lollo and Vincent.,? \\Q2012\\E", "shortCiteRegEx": "Lollo and Vincent.", "year": 2012}, {"title": "Forming sparse representations by local anti-Hebbian learning", "author": ["F\u00f6ldiak", "Peter"], "venue": "Biological cybernetics,", "citeRegEx": "F\u00f6ldiak and Peter.,? \\Q1990\\E", "shortCiteRegEx": "F\u00f6ldiak and Peter.", "year": 1990}, {"title": "Neural network model for a mechanism of pattern recognition unaffected by shift in position - Neocognitron", "author": ["K. Fukushima"], "venue": "Trans. IECE,", "citeRegEx": "Fukushima,? \\Q1979\\E", "shortCiteRegEx": "Fukushima", "year": 1979}, {"title": "Becoming a \u201cGreeble\u201d expert: Exploring mechanisms for face recognition", "author": ["Gauthier", "Isabel", "Tarr", "Michael J"], "venue": "Vision Research,", "citeRegEx": "Gauthier et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Gauthier et al\\.", "year": 1997}, {"title": "The temporal correlation hypothesis of visual feature integration", "author": ["Gray", "Charles M"], "venue": "Still alive and well. Neuron,", "citeRegEx": "Gray and M.,? \\Q1999\\E", "shortCiteRegEx": "Gray and M.", "year": 1999}, {"title": "Handwritten digit recognition with a back-propagation network. In Advances in neural information processing systems", "author": ["Le Cun", "B. Boser", "Denker", "John S", "D. Henderson", "Howard", "Richard E", "W. Hubbard", "Jackel", "Lawrence D"], "venue": "URL http://citeseerx.ist.psu. edu/viewdoc/summary?doi=10.1.1.32.5076", "citeRegEx": "Cun et al\\.,? \\Q1990\\E", "shortCiteRegEx": "Cun et al\\.", "year": 1990}, {"title": "A model for visual shape recognition", "author": ["Milner", "Peter M"], "venue": "Psychological review,", "citeRegEx": "Milner and M.,? \\Q1974\\E", "shortCiteRegEx": "Milner and M.", "year": 1974}, {"title": "Recurrent models of visual attention", "author": ["Mnih", "Volodymyr", "Heess", "Nicolas", "Graves", "Alex"], "venue": "In Advances in Neural Information Processing Systems, pp. 2204\u20132212,", "citeRegEx": "Mnih et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Mnih et al\\.", "year": 2014}, {"title": "Generalizable relational binding from coarse-coded distributed representations", "author": ["O\u2019reilly", "Randall C", "Busby", "Richard S"], "venue": "Advances in neural information processing systems,", "citeRegEx": "O.reilly et al\\.,? \\Q2002\\E", "shortCiteRegEx": "O.reilly et al\\.", "year": 2002}, {"title": "Three forms of binding and their neural substrates: Alternatives to temporal synchrony. The unity of consciousness: Binding, integration, and dissociation", "author": ["O\u2019Reilly", "Randall C", "Busby", "Richard S", "Soto", "Rodolfo"], "venue": "pp. 168\u2013192,", "citeRegEx": "O.Reilly et al\\.,? \\Q2003\\E", "shortCiteRegEx": "O.Reilly et al\\.", "year": 2003}, {"title": "Unsupervised segmentation with dynamical units", "author": ["Rao", "Ravishankar A", "Cecchi", "Guillermo", "Peck", "Charles C", "Kozloski", "James R"], "venue": "Neural Networks, IEEE Transactions on,", "citeRegEx": "Rao et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Rao et al\\.", "year": 2008}, {"title": "Neuronal synchrony in Complex-Valued deep networks. arXiv:1312.6115 [cs, q-bio, stat", "author": ["Reichert", "David P", "Serre", "Thomas"], "venue": "URL http://arxiv.org/abs/1312", "citeRegEx": "Reichert et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Reichert et al\\.", "year": 2013}, {"title": "Hierarchical models of object recognition in cortex", "author": ["Riesenhuber", "Maximilian", "Poggio", "Tomaso"], "venue": "Nature neuroscience,", "citeRegEx": "Riesenhuber et al\\.,? \\Q1999\\E", "shortCiteRegEx": "Riesenhuber et al\\.", "year": 1999}, {"title": "The utility driven dynamic error propagation network", "author": ["A.J. Robinson", "F. Fallside"], "venue": "Technical Report CUED/F-INFENG/TR.1,", "citeRegEx": "Robinson and Fallside,? \\Q1987\\E", "shortCiteRegEx": "Robinson and Fallside", "year": 1987}, {"title": "Principles of neurodynamics. perceptrons and the theory of brain mechanisms", "author": ["Rosenblatt", "Frank"], "venue": "Technical report, DTIC Document,", "citeRegEx": "Rosenblatt and Frank.,? \\Q1961\\E", "shortCiteRegEx": "Rosenblatt and Frank.", "year": 1961}, {"title": "Learning to generate artificial fovea trajectories for target detection", "author": ["Schmidhuber", "Juergen", "Huber", "Rudolf"], "venue": "International Journal of Neural Systems,", "citeRegEx": "Schmidhuber et al\\.,? \\Q1991\\E", "shortCiteRegEx": "Schmidhuber et al\\.", "year": 1991}, {"title": "Learning factorial codes by predictability minimization", "author": ["Schmidhuber", "J\u00fcrgen"], "venue": "Neural Computation,", "citeRegEx": "Schmidhuber and J\u00fcrgen.,? \\Q1992\\E", "shortCiteRegEx": "Schmidhuber and J\u00fcrgen.", "year": 1992}, {"title": "Solutions to the binding problem: Progress through controversy and convergence", "author": ["Treisman", "Anne"], "venue": "ISSN 0896-6273. doi: 10.1016/S08966273(00)80826-0. URL http://www.sciencedirect.com/science/article/ pii/S0896627300808260", "citeRegEx": "Treisman and Anne.,? \\Q1999\\E", "shortCiteRegEx": "Treisman and Anne.", "year": 1999}, {"title": "Extracting and composing robust features with denoising autoencoders", "author": ["Vincent", "Pascal", "Larochelle", "Hugo", "Bengio", "Yoshua", "Manzagol", "Pierre-Antoine"], "venue": "In Proceedings of the 25th international conference on Machine learning,", "citeRegEx": "Vincent et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Vincent et al\\.", "year": 2008}, {"title": "Information theoretic measures for clusterings comparison: Variants, properties, normalization and correction for chance", "author": ["Vinh", "Nguyen Xuan", "Epps", "Julien", "Bailey", "James"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "Vinh et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Vinh et al\\.", "year": 2010}, {"title": "The correlation theory of brain function", "author": ["von der Malsburg", "Christoph"], "venue": null, "citeRegEx": "Malsburg and Christoph.,? \\Q1981\\E", "shortCiteRegEx": "Malsburg and Christoph.", "year": 1981}, {"title": "Binding in models of perception and brain function", "author": ["von der Malsburg", "Christoph"], "venue": "Current opinion in neurobiology,", "citeRegEx": "Malsburg and Christoph.,? \\Q1995\\E", "shortCiteRegEx": "Malsburg and Christoph.", "year": 1995}, {"title": "Learning lateral interactions for feature binding and sensory segmentation from prototypic basis interactions", "author": ["Weng", "Shijie", "Steil", "Jochen Jakob", "Ritter", "Helge"], "venue": "Neural Networks, IEEE Transactions on,", "citeRegEx": "Weng et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Weng et al\\.", "year": 2006}, {"title": "Generalization of backpropagation with application to a recurrent gas market model", "author": ["P.J. Werbos"], "venue": "Neural Networks,", "citeRegEx": "Werbos,? \\Q1988\\E", "shortCiteRegEx": "Werbos", "year": 1988}], "referenceMentions": [{"referenceID": 2, "context": "Complementary to that, disentangling (Barlow et al., 1989; Schmidhuber, 1992; Bengio et al., 2007) requires the factors of variation in the data to be separated into different independent features.", "startOffset": 37, "endOffset": 98}, {"referenceID": 4, "context": "Complementary to that, disentangling (Barlow et al., 1989; Schmidhuber, 1992; Bengio et al., 2007) requires the factors of variation in the data to be separated into different independent features.", "startOffset": 37, "endOffset": 98}, {"referenceID": 2, "context": "Complementary to that, disentangling (Barlow et al., 1989; Schmidhuber, 1992; Bengio et al., 2007) requires the factors of variation in the data to be separated into different independent features. This concept is closely related to invariance and eases further processing because many properties, that we might be interested in, are invariant under a wide variety of transformations (Bengio et al., 2013a). Unfortunately distributed representations can interfere and lead to ambiguities when multiple objects are to be represented at the same time. The binding problem refers to these ambiguities that can arise from the superposition of multiple distributed representations. This problem has been debated quite extensively in the neuroscience and psychology communities perhaps starting with Milner (1974) and von der Malsburg (1981), but its existence can be traced back at least to a description by Rosenblatt (1961).", "startOffset": 38, "endOffset": 808}, {"referenceID": 2, "context": "Complementary to that, disentangling (Barlow et al., 1989; Schmidhuber, 1992; Bengio et al., 2007) requires the factors of variation in the data to be separated into different independent features. This concept is closely related to invariance and eases further processing because many properties, that we might be interested in, are invariant under a wide variety of transformations (Bengio et al., 2013a). Unfortunately distributed representations can interfere and lead to ambiguities when multiple objects are to be represented at the same time. The binding problem refers to these ambiguities that can arise from the superposition of multiple distributed representations. This problem has been debated quite extensively in the neuroscience and psychology communities perhaps starting with Milner (1974) and von der Malsburg (1981), but its existence can be traced back at least to a description by Rosenblatt (1961).", "startOffset": 38, "endOffset": 836}, {"referenceID": 2, "context": "Complementary to that, disentangling (Barlow et al., 1989; Schmidhuber, 1992; Bengio et al., 2007) requires the factors of variation in the data to be separated into different independent features. This concept is closely related to invariance and eases further processing because many properties, that we might be interested in, are invariant under a wide variety of transformations (Bengio et al., 2013a). Unfortunately distributed representations can interfere and lead to ambiguities when multiple objects are to be represented at the same time. The binding problem refers to these ambiguities that can arise from the superposition of multiple distributed representations. This problem has been debated quite extensively in the neuroscience and psychology communities perhaps starting with Milner (1974) and von der Malsburg (1981), but its existence can be traced back at least to a description by Rosenblatt (1961). It is classically demonstrated with a system required to identify an input as either square( ) or triangle(4) and to decide whether it is at the top(\u2191) or at the bottom(\u2193).", "startOffset": 38, "endOffset": 921}, {"referenceID": 10, "context": "Convolutional Networks (Fukushima, 1979; Le Cun et al., 1990) use feature detectors with limited receptive fields (filters) replicated over the whole input to represent its inputs.", "startOffset": 23, "endOffset": 61}, {"referenceID": 26, "context": "This is achieved through a clustering process which utilizes a denoising autoencoder (DAE; Behnke, 2001; Vincent et al., 2008) to iteratively reconstruct an input.", "startOffset": 85, "endOffset": 126}, {"referenceID": 26, "context": "The DAE is trained to remove corruption from images of single objects and thus learns a local model of the data generating distribution (Vincent et al., 2008; Bengio et al., 2013b).", "startOffset": 136, "endOffset": 180}, {"referenceID": 7, "context": "Thus, convergence can\u2019t be proven and RC is not an Expectation Maximization algorithm (Dempster et al., 1977).", "startOffset": 86, "endOffset": 109}, {"referenceID": 18, "context": "Taken from Rao et al. (2008). This is a simple dataset with no translations, but significant overlap between patterns.", "startOffset": 11, "endOffset": 29}, {"referenceID": 18, "context": "Taken from Rao et al. (2008). This is a simple dataset with no translations, but significant overlap between patterns. Shapes Taken from Reichert & Serre (2013). Three shapes ( ,4,5) are randomly placed in an image (possibly with overlap).", "startOffset": 11, "endOffset": 161}, {"referenceID": 18, "context": "Taken from Rao et al. (2008). This is a simple dataset with no translations, but significant overlap between patterns. Shapes Taken from Reichert & Serre (2013). Three shapes ( ,4,5) are randomly placed in an image (possibly with overlap). This dataset tests binding of shapes under translation invariance and varying overlap. Bars Introduced by F\u00f6ldiak (1990) to demonstrate unsupervised learning of independent components of an image.", "startOffset": 11, "endOffset": 361}, {"referenceID": 18, "context": "Taken from Rao et al. (2008). This is a simple dataset with no translations, but significant overlap between patterns. Shapes Taken from Reichert & Serre (2013). Three shapes ( ,4,5) are randomly placed in an image (possibly with overlap). This dataset tests binding of shapes under translation invariance and varying overlap. Bars Introduced by F\u00f6ldiak (1990) to demonstrate unsupervised learning of independent components of an image. We use the variant from Reichert & Serre (2013) which employs 6 horizontal, and 6 vertical lines placed in random positions in the image.", "startOffset": 11, "endOffset": 485}, {"referenceID": 18, "context": "Taken from Rao et al. (2008). This is a simple dataset with no translations, but significant overlap between patterns. Shapes Taken from Reichert & Serre (2013). Three shapes ( ,4,5) are randomly placed in an image (possibly with overlap). This dataset tests binding of shapes under translation invariance and varying overlap. Bars Introduced by F\u00f6ldiak (1990) to demonstrate unsupervised learning of independent components of an image. We use the variant from Reichert & Serre (2013) which employs 6 horizontal, and 6 vertical lines placed in random positions in the image. Corners This dataset consists of 8 corner shapes placed in random orientations and positions, such that 4 of them align to form a square. It was introduced by Reichert & Serre (2013) to demonstrate that spatial connected-ness is not a requirement for binding.", "startOffset": 11, "endOffset": 758}, {"referenceID": 18, "context": "Taken from Rao et al. (2008). This is a simple dataset with no translations, but significant overlap between patterns. Shapes Taken from Reichert & Serre (2013). Three shapes ( ,4,5) are randomly placed in an image (possibly with overlap). This dataset tests binding of shapes under translation invariance and varying overlap. Bars Introduced by F\u00f6ldiak (1990) to demonstrate unsupervised learning of independent components of an image. We use the variant from Reichert & Serre (2013) which employs 6 horizontal, and 6 vertical lines placed in random positions in the image. Corners This dataset consists of 8 corner shapes placed in random orientations and positions, such that 4 of them align to form a square. It was introduced by Reichert & Serre (2013) to demonstrate that spatial connected-ness is not a requirement for binding. MNIST+Shape Another dataset from Reichert & Serre (2013), which combines a random shape from the shapes dataset with a single MNIST digit.", "startOffset": 11, "endOffset": 892}, {"referenceID": 27, "context": "We evaluated performance by measuring the Adjusted Mutual Information (AMI; Vinh et al., 2010) between the true segmentation and the result of the binding, to which we refer to as the score.", "startOffset": 70, "endOffset": 94}, {"referenceID": 18, "context": "Recently, these ideas were implemented using complex valued activations in neural networks to jointly encode firing rate and phase (Rao et al., 2008; Reichert & Serre, 2013).", "startOffset": 131, "endOffset": 173}, {"referenceID": 31, "context": "In principle, Recurrent Neural Networks (RNNs; e.g. Robinson & Fallside, 1987; Werbos, 1988) can solve the binding problem by learning a mechanism to avoid it.", "startOffset": 40, "endOffset": 92}, {"referenceID": 30, "context": "Psychologists (Di Lollo, 2012) and machine learning researchers (Weng et al., 2006) alike have suggested feedback as a mechanism to do binding.", "startOffset": 64, "endOffset": 83}, {"referenceID": 15, "context": "An RNN may utilize an implicit or explicit attention mechanism to selectively process different parts of the input (Schmidhuber & Huber, 1991; Mnih et al., 2014; Bahdanau et al., 2014).", "startOffset": 115, "endOffset": 184}, {"referenceID": 1, "context": "An RNN may utilize an implicit or explicit attention mechanism to selectively process different parts of the input (Schmidhuber & Huber, 1991; Mnih et al., 2014; Bahdanau et al., 2014).", "startOffset": 115, "endOffset": 184}, {"referenceID": 14, "context": "O\u2019Reilly et al., 2003). O\u2019reilly & Busby (2002) argued that the intuitive explanation of the binding problem from Figure 1b only applies if the distributed features themselves are local codes.", "startOffset": 0, "endOffset": 48}, {"referenceID": 14, "context": "O\u2019Reilly et al., 2003). O\u2019reilly & Busby (2002) argued that the intuitive explanation of the binding problem from Figure 1b only applies if the distributed features themselves are local codes. They suggested that neural networks can avoid the binding problem using coarse-coded representations. Various feature representation types including coarse-coding and their limitations were described by Hinton (1984). In principle, Recurrent Neural Networks (RNNs; e.", "startOffset": 0, "endOffset": 410}, {"referenceID": 0, "context": "Achanta et al. (2012) for an overview).", "startOffset": 0, "endOffset": 22}], "year": 2016, "abstractText": "Disentangled distributed representations of data are desirable for machine learning, since they are more expressive and can generalize from fewer examples. However, for complex data, the distributed representations of multiple objects present in the same input can interfere and lead to ambiguities, which is commonly referred to as the binding problem. We argue for the importance of the binding problem to the field of representation learning, and develop a probabilistic framework that explicitly models inputs as a composition of multiple objects. We propose an algorithm that uses a denoising autoencoder to dynamically bind features together in multi-object inputs through an Expectation-Maximization-like clustering process. The effectiveness of this method is demonstrated on artificially generated datasets of binary images, showing that it can even generalize to bind together new objects never seen by the autoencoder during training.", "creator": "LaTeX with hyperref package"}}}