{"id": "1511.06425", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "19-Nov-2015", "title": "First Step toward Model-Free, Anonymous Object Tracking with Recurrent Neural Networks", "abstract": "In this paper, we propose and study a novel visual object tracking approach based on convolutional networks and recurrent networks. The proposed approach is distinct from the existing approaches to visual object tracking, such as filtering-based ones and tracking-by-detection ones, in the sense that the tracking system is explicitly trained off-line to track anonymous objects in a noisy environment. The proposed visual tracking model is end-to-end trainable, minimizing any adversarial effect from mismatches in object representation and between the true underlying dynamics and learning dynamics. We empirically show that the proposed tracking approach works well in various scenarios by generating artificial video sequences with varying conditions; the number of objects, amount of noise and the match between the training shapes and test shapes.", "histories": [["v1", "Thu, 19 Nov 2015 22:24:15 GMT  (589kb,D)", "http://arxiv.org/abs/1511.06425v1", null], ["v2", "Wed, 25 Nov 2015 19:44:15 GMT  (660kb,D)", "http://arxiv.org/abs/1511.06425v2", null]], "reviews": [], "SUBJECTS": "cs.CV cs.LG", "authors": ["quan gan", "qipeng guo", "zheng zhang", "kyunghyun cho"], "accepted": false, "id": "1511.06425"}, "pdf": {"name": "1511.06425.pdf", "metadata": {"source": "CRF", "title": null, "authors": ["RECURRENT NEURAL NETWORKS", "Quan Gan", "Qipeng Guo", "Zheng Zhang", "Kyunghyun Cho"], "emails": ["zz@nyu.edu", "kyunghyun.cho@nyu.edu"], "sections": [{"heading": "1 INTRODUCTION", "text": "Visual object tracking is a problem of building a computer model that is able to predict the position of a particular object from a video clip that consists of many consecutive video frames. Using deep learning techniques, we tackle this problem with a model-free statistical approach. Our goal in this essay is to build a model that can track an anonymous object in an image sequence, a task that has immediate applications in important scenarios such as self-driving cars. Since safety is paramount, determining what class the object belongs to is much less critical than determining its whereabouts to avoid collision. It is also an important step toward dramatically improving the generalizability of a tracking system, as real-world objects far exceed the designated categories. Our model integrates a complex network with a recursive network and focuses attention on multiple layers of representation. The recursive network provides a ground-level prediction of the target from visual predictions, merging past predictions with their corresponding characteristics."}, {"heading": "2 BACKGROUND: VISUAL OBJECT TRACKING AND LIMITATIONS", "text": "A visual object tracking system often has two main components: object detection and object tracking. Object detection refers to the process by which a particular object is detected in each video image, while object tracking refers to the process of continuously predicting the position of the detected object. However, the objective of object detection is to extract a feature vector (x) of a detected object, which often encodes both the shape and location of the object, as each video image x. The specificity of the feature vector depends greatly on the choice of object representation. When an object is presented as a point, the feature vector of the detected object is a two-dimensional coordinate vector that encodes the object's center of gravity in each frame.There are many approaches suggested over a number of decades (see e.g. Yilmaz et al), and we are interested in this suggestion."}, {"heading": "3 VISUAL OBJECT TRACKING WITH DEEP NEURAL NETWORKS", "text": "We describe here a novel approach to visual anonymous object tracking using techniques from deep learning (LeCun et al., 2015). Our goal in introducing a novel approach is to build a system that simply circumvents the four limitations of conventional visual tracking systems that we discussed in the previous section. The proposed model is a deep recurrent network that takes raw video images of pixel intensities as input and returns the coordinates of a limiting box of an object that is tracked for each frame. Mathematically, this corresponds to the statement that the proposed model is the full tracking probability intop (z1, z2,.., zT, x1, x2,.., xT) = T, z < t, where zt and xt are the location of an object and an input frame, each in time. z < t is a history of all previous sites before time t, and xT."}, {"heading": "3.2 TRAINING", "text": "Unlike the existing approaches to visual object tracking described in Sec. 2, we adopt an offline training strategy that does not apply in practice. This means that the trained model is used as in the test phase. < p > p > p > p > p > p > p > p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \") p\" p \"p\" p \"p\" p \"p\" p \"p\" p"}, {"heading": "3.3 CHARACTERISTICS", "text": "There are three main features that set the proposed approach apart from previous work on visual object tracking. Firstly, the proposed model is trained end-to-end, including object representation, object recognition and object tracking. The model works directly on the raw pixel intensities of each image, which differs from traditional object tracking systems, where the modeling of the appearance is largely separated from the actual tracking system (see, for example, Li et al., 2013). This largely prevents potential performance impairments from a suboptimal, handcrafted object representation and detector.Secondly, the proposed model works constructively with anonymous objects. As we form a model with a large number of generically shaped objects offline, the model learns to detect a generic object that was originally shown, rather than to detect a specific, predefined set of objects.Since the proposed model works constructively with anonymous objects, it is a recurring neural network that maintains the objective line of the region it is part of the object tracking history."}, {"heading": "3.4 RELATED WORK", "text": "While we were preparing this paper, Kahou et al. (2015) independently proposed a similar visual object tracking approach (based on a recursive neural network), and let us describe the similarities and differences between their recursive attentive tracking model (RATM) and the proposed tracking approach. An essential common feature between these two approaches is that both use a recursive neural network as the main component.A major difference between the RATM and the proposed approach lies in education. Both the RATM and the model proposed in this paper use the intermediate positions of an object as an auxiliary target (see equivalent. (7). Kahou et al. (2015) report that this use of auxiliary costs stabilizes tracking quality, which is later confirmed by our experiments in this paper. A major difference is that Kahou et al uses an intermedial model. (2015) We suggest a classification error that is averaged across all frames."}, {"heading": "4 DATA GENERATION", "text": "In fact it is so that most of us will be in a position to be in a position to move, and specifically in the manner and manner in which they are able to move, in the manner and manner in which they are able to move, in the manner and manner in which they are able to move, in the manner and manner in which they are moving, in the manner and manner in which they are moving, in the manner and manner in which they are moving, in the manner and manner in which they are moving, in the manner and manner in which they are moving, in the manner and manner in which they are moving, in the manner and manner in which they are moving, in the manner in which they and in the manner in which they and in the manner in which they and in the manner in which they and in the manner in which they and in the manner in which they, in the manner in which they and in the manner in which they and in the manner in which they, in the manner in which they and in the manner in which they and in the manner in which they and in the manner in which they and in the manner in the manner in which they and in the manner in which they and in the manner in the manner in which they and in the manner in the manner in which they and in the manner in the manner in which they and in the manner in the manner in which they and in the manner in the manner in which they and in the manner in the manner in the manner in which they and in the manner in the manner in which they and in the manner in the manner in which they and in the manner in the manner in which they and in the manner in the manner in which they and in the manner in the manner in the manner in which they and in the manner in the manner in the manner in which they and in the manner in the manner in the manner in which they and in the manner in the manner in which they and in the manner in the manner in which they and in the manner in the manner in the manner in which they and in the manner in the manner in the manner in which they and in the manner in the manner in the manner in which they and in the manner in the manner in the manner in the manner in the manner in which they and they and in the manner in the manner in the manner in which they and in the manner in the manner in the manner in the manner in the manner in the manner in the manner in the manner in the manner"}, {"heading": "5 MODELS AND TRAINING", "text": "We test five models on each of the four cases, MNIST- {single, multi} - {seed, diff}.https: / / github.com / deepmind / mnist-clutteredRecurrent 0.5 Visual Object Tracker (RecTracker-X) The first model, RecTracker ID, is the proposed recurring visual object tracker (see Sec. 3.1.) As described above, it consists of a convolutionary network and a recursive network. In this case, we use an identity function to pre-process each input frame: m (xt, z, n, n, t, t, t, n, e, n, n, n, n, n, n, n, n, n, n, n, n, n, n, n, n, n, n, n, n, n, n, n, n, n, n, n, n, n, n, n, n, n, n, n, n, n, n, n, n, n, n, n, n, n, n, n, n, n, n, n, n, n, n, n, n, n, n, n, n, n, n, n, n, n, n, n, n, n, n, n, n, n, n, n, n, n, n, n, n, n, n, n, n, n, n, n, n, n, n, n, n, n, n, n, n, n, n, n, n, n, n, n, n, n, n, n, n."}, {"heading": "5.1 NETWORK ARCHITECTURES", "text": "We describe the architectures of the individual network-revolutionary and recurring networks - in Appendix A."}, {"heading": "5.2 TRAINING", "text": "In Appendix B.http: / / home.isr.uc.pt / \u02dc henriques / circulant / we describe the architectures of the individual network-revolutionary and recurring networks."}, {"heading": "6 RESULT AND ANALYSIS", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "6.1 EVALUATION METRIC", "text": "The IOU is defined as IOU (z-t, z-t) = EVP (0.05 in our experiments), where M-t and M-t are binary masks whose pixels within a boundary field (either ground truth or predicted) are 1 and otherwise 0. A higher IOU implies better tracking quality, and it is between 0 and 1. For each video sequence, we calculate the average IOU across all frames."}, {"heading": "6.2 QUANTITATIVE ANALYSIS", "text": "As a result, the number of working women in the US is many times higher than the number of working men in the US, compared to men in the US."}, {"heading": "6.3 VISUALIZATION OF TRACKING", "text": "We have video clips with tracking results of all the models tested at http: / / barclayiii. github.io / tracking-with-rnn. See Figure 3 for an example of this kind. Our visual inspection shows that 1. ConvTracker model fails in most test cases, which indicates that the model cannot track anonymous objects. 2. One possible reason is that the memory it has, either the memory to store positions or to store features, is too small. Such an inability requires a recurrent structure that has more storage capacity than normal feed networks. 2. In RecTracker ID models, there are cases where the model loses focus at the first image. Further frames showed that the model is still following the wrong object, showing the need for attention. 3. The RecTracker Att-1 model mainly stays on the object we want to track, but when another object is brighter, the model is distracted."}, {"heading": "7 CONCLUSION", "text": "Unlike traditional tracking approaches, the entire pipeline of visual tracking object representation, object extraction and location prediction is aligned to maximize tracking quality. The proposed tracking model combines many recent advances in deep learning and has proven to be well-suited for sophisticated, artificially created sequences. We view this work as a first step toward building a complete, fully traceable visual object tracking system. There are a number of issues that need to be explored and solved in the future. First, the proposed models need to be evaluated against natural scenes with real objects and their dynamics. Second, algorithms need to be researched to adapt a pre-tracked model online. Third, we need to find a network architecture that can track any number of objects without a predefined ceiling."}, {"heading": "A NETWORK ARCHITECTURES", "text": "Convolutional Network We use a single revolutionary layer with 32 10 \u00d7 10 filters. These filters are applied step by step 5 on the input frame. As it is important to obtain as much spatial information as possible to function well, we do not use pooling layer. This revolutionary layer is immediately followed by an elementary tango. In the case of ConvTracker, the revolutionary layer is followed by a fully connected layer with 200 tanh units. This fully connected layer also receives the predicted positions of the four preceding frames as input. Recurrent Network We use 200 gated recurrent units (GRU, Cho et al., 2014) to build a recursive network. At each step, the activation of the convolutional layer (see above) and the predicted object position z-t-1 in the previous frame are fed into the recursive network."}, {"heading": "B TRAINING", "text": "We train each model up to 50 epochs or until the training costs stop improving, using a training set of 3,200,000 randomly generated examples. We use RMSProp, which was implemented accordingly (Graves, 2013), with minibatches of 32 examples."}], "references": [{"title": "Multiple object recognition with visual attention", "author": ["Ba", "Jimmy", "Mnih", "Volodymyr", "Kavukcuoglu", "Koray"], "venue": "arXiv preprint arXiv:1412.7755,", "citeRegEx": "Ba et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Ba et al\\.", "year": 2014}, {"title": "Deep learning with non-medical training used for chest pathology identification", "author": ["Bar", "Yaniv", "Diamant", "Idit", "Wolf", "Lior", "Greenspan", "Hayit"], "venue": "In SPIE Medical Imaging, pp. 94140V\u201394140V. International Society for Optics and Photonics,", "citeRegEx": "Bar et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Bar et al\\.", "year": 2015}, {"title": "Estimation of object motion parameters from noisy images", "author": ["Broida", "Ted J", "Chellappa", "Rama"], "venue": "Pattern Analysis and Machine Intelligence, IEEE Transactions on,", "citeRegEx": "Broida et al\\.,? \\Q1986\\E", "shortCiteRegEx": "Broida et al\\.", "year": 1986}, {"title": "Learning phrase representations using rnn encoderdecoder for statistical machine translation", "author": ["Cho", "Kyunghyun", "Van Merri\u00ebnboer", "Bart", "Gulcehre", "Caglar", "Bahdanau", "Dzmitry", "Bougares", "Fethi", "Schwenk", "Holger", "Bengio", "Yoshua"], "venue": "arXiv preprint arXiv:1406.1078,", "citeRegEx": "Cho et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Cho et al\\.", "year": 2014}, {"title": "Learning where to attend with deep architectures for image tracking", "author": ["Denil", "Misha", "Bazzani", "Loris", "Larochelle", "Hugo", "de Freitas", "Nando"], "venue": "Neural computation,", "citeRegEx": "Denil et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Denil et al\\.", "year": 2012}, {"title": "Large-scale fpga-based convolutional networks", "author": ["Farabet", "Cl\u00e9ment", "LeCun", "Yann", "Kavukcuoglu", "Koray", "Culurciello", "Eugenio", "Martini", "Berin", "Akselrod", "Polina", "Talay", "Selcuk"], "venue": "Machine Learning on Very Large Data Sets,", "citeRegEx": "Farabet et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Farabet et al\\.", "year": 2011}, {"title": "Generating sequences with recurrent neural networks", "author": ["Graves", "Alex"], "venue": "arXiv preprint arXiv:1308.0850,", "citeRegEx": "Graves and Alex.,? \\Q2013\\E", "shortCiteRegEx": "Graves and Alex.", "year": 2013}, {"title": "Learning to transduce with unbounded memory", "author": ["Grefenstette", "Edward", "Hermann", "Karl Moritz", "Suleyman", "Mustafa", "Blunsom", "Phil"], "venue": "arXiv preprint arXiv:1506.02516,", "citeRegEx": "Grefenstette et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Grefenstette et al\\.", "year": 2015}, {"title": "Draw: A recurrent neural network for image generation", "author": ["Gregor", "Karol", "Danihelka", "Ivo", "Graves", "Alex", "Wierstra", "Daan"], "venue": "arXiv preprint arXiv:1502.04623,", "citeRegEx": "Gregor et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Gregor et al\\.", "year": 2015}, {"title": "High-speed tracking with kernelized correlation filters", "author": ["J.F. Henriques", "R. Caseiro", "P. Martins", "J. Batista"], "venue": "Pattern Analysis and Machine Intelligence, IEEE Transactions on,", "citeRegEx": "Henriques et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Henriques et al\\.", "year": 2015}, {"title": "Long short-term memory", "author": ["Hochreiter", "Sepp", "Schmidhuber", "J\u00fcrgen"], "venue": "Neural computation,", "citeRegEx": "Hochreiter et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Hochreiter et al\\.", "year": 1997}, {"title": "Inferring algorithmic patterns with stack-augmented recurrent nets", "author": ["Joulin", "Armand", "Mikolov", "Tomas"], "venue": "arXiv preprint arXiv:1503.01007,", "citeRegEx": "Joulin et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Joulin et al\\.", "year": 2015}, {"title": "Ratm: Recurrent attentive tracking model", "author": ["Kahou", "Samira Ebrahimi", "Michalski", "Vincent", "Memisevic", "Roland"], "venue": "arXiv preprint arXiv:1510.08660,", "citeRegEx": "Kahou et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Kahou et al\\.", "year": 2015}, {"title": "Imagenet classification with deep convolutional neural networks. In Advances in neural information processing", "author": ["Krizhevsky", "Alex", "Sutskever", "Ilya", "Hinton", "Geoffrey E"], "venue": null, "citeRegEx": "Krizhevsky et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Krizhevsky et al\\.", "year": 2012}, {"title": "Efficient backprop", "author": ["LeCun", "Yann A", "Bottou", "L\u00e9on", "Orr", "Genevieve B", "M\u00fcller", "Klaus-Robert"], "venue": "In Neural networks: Tricks of the trade,", "citeRegEx": "LeCun et al\\.,? \\Q1998\\E", "shortCiteRegEx": "LeCun et al\\.", "year": 1998}, {"title": "A survey of appearance models in visual object tracking", "author": ["Li", "Xi", "Hu", "Weiming", "Shen", "Chunhua", "Zhang", "Zhongfei", "Dick", "Anthony", "Hengel", "Anton Van Den"], "venue": "ACM transactions on Intelligent Systems and Technology (TIST),", "citeRegEx": "Li et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Li et al\\.", "year": 2013}, {"title": "Recurrent models of visual attention", "author": ["Mnih", "Volodymyr", "Heess", "Nicolas", "Graves", "Alex", "Kavukcuoglu", "Koray"], "venue": "Advances in Neural Information Processing Systems", "citeRegEx": "Mnih et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Mnih et al\\.", "year": 2014}, {"title": "How to construct deep recurrent neural networks", "author": ["Pascanu", "Razvan", "Gulcehre", "Caglar", "Cho", "Kyunghyun", "Bengio", "Yoshua"], "venue": "In International Conference on Learning Representation (ICLR),", "citeRegEx": "Pascanu et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Pascanu et al\\.", "year": 2014}, {"title": "Attention in early development: Themes and variations", "author": ["Ruff", "Holly Alliger", "Rothbart", "Mary Klevjord"], "venue": null, "citeRegEx": "Ruff et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Ruff et al\\.", "year": 2001}, {"title": "Overfeat: Integrated recognition, localization and detection using convolutional networks", "author": ["Sermanet", "Pierre", "Eigen", "David", "Zhang", "Xiang", "Mathieu", "Micha\u00ebl", "Fergus", "Rob", "LeCun", "Yann"], "venue": "arXiv preprint arXiv:1312.6229,", "citeRegEx": "Sermanet et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Sermanet et al\\.", "year": 2013}, {"title": "Information processing in dynamical systems: foundations of harmony theory", "author": ["P. Smolensky"], "venue": null, "citeRegEx": "Smolensky,? \\Q1986\\E", "shortCiteRegEx": "Smolensky", "year": 1986}, {"title": "Attentional neural network: Feature selection using cognitive feedback", "author": ["Wang", "Qian", "Zhang", "Jiaxing", "Song", "Sen", "Zheng"], "venue": "Advances in Neural Information Processing Systems", "citeRegEx": "Wang et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2014}, {"title": "Show, attend and tell: Neural image caption generation with visual attention", "author": ["Xu", "Kelvin", "Ba", "Jimmy", "Kiros", "Ryan", "Cho", "Kyunghyun", "Courville", "Aaron", "Salakhutdinov", "Ruslan", "Zemel", "Richard", "Bengio", "Yoshua"], "venue": "In International Conference on Machine Learning,", "citeRegEx": "Xu et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Xu et al\\.", "year": 2015}, {"title": "Describing videos by exploiting temporal structure", "author": ["Yao", "Li", "Torabi", "Atousa", "Cho", "Kyunghyun", "Ballas", "Nicolas", "Pal", "Christopher", "Larochelle", "Hugo", "Courville", "Aaron"], "venue": "In International Conference on Computer Vision,", "citeRegEx": "Yao et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Yao et al\\.", "year": 2015}, {"title": "Object tracking: A survey", "author": ["Yilmaz", "Alper", "Javed", "Omar", "Shah", "Mubarak"], "venue": "Acm computing surveys (CSUR),", "citeRegEx": "Yilmaz et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Yilmaz et al\\.", "year": 2006}], "referenceMentions": [{"referenceID": 24, "context": ", Yilmaz et al. (2006),) and we are interested in this proposal a statistical approach.", "startOffset": 2, "endOffset": 23}, {"referenceID": 15, "context": ", Li et al. (2013).) This approach is more holistic than the previous approach, because a single model is trained online to track an object in interest.", "startOffset": 2, "endOffset": 19}, {"referenceID": 13, "context": "At each time step t, an input frame xt is first processed by a convolutional network, which has recently become a de facto standard in handling visual input (see, e.g., LeCun et al., 1998; Krizhevsky et al., 2012).", "startOffset": 157, "endOffset": 213}, {"referenceID": 3, "context": "The recurrent neural network updates its internal memory vector ht based on the previous memory vector ht\u22121, previous location of an object z\u0303t\u22121 and the current frame \u03c6(xt): ht = rec\u03b8 r(ht\u22121, z\u0303t\u22121,\u03c6(xt)), (3) where rec\u03b8 r is a recurrent activation function such as gated recurrent units Cho et al. (2014), long short-term memory units Hochreiter & Schmidhuber (1997) or a simple logistic function, parametrized with the parameters \u03b8 r.", "startOffset": 289, "endOffset": 307}, {"referenceID": 3, "context": "The recurrent neural network updates its internal memory vector ht based on the previous memory vector ht\u22121, previous location of an object z\u0303t\u22121 and the current frame \u03c6(xt): ht = rec\u03b8 r(ht\u22121, z\u0303t\u22121,\u03c6(xt)), (3) where rec\u03b8 r is a recurrent activation function such as gated recurrent units Cho et al. (2014), long short-term memory units Hochreiter & Schmidhuber (1997) or a simple logistic function, parametrized with the parameters \u03b8 r.", "startOffset": 289, "endOffset": 369}, {"referenceID": 3, "context": "The recurrent neural network updates its internal memory vector ht based on the previous memory vector ht\u22121, previous location of an object z\u0303t\u22121 and the current frame \u03c6(xt): ht = rec\u03b8 r(ht\u22121, z\u0303t\u22121,\u03c6(xt)), (3) where rec\u03b8 r is a recurrent activation function such as gated recurrent units Cho et al. (2014), long short-term memory units Hochreiter & Schmidhuber (1997) or a simple logistic function, parametrized with the parameters \u03b8 r. This formulation lets the recurrent neural network to summarize the history of predicted locations z<t and input frames x\u2264t up to time step t. With the newly updated memory state ht , the recurrent neural network computes the predictive distribution over the object\u2019s location (see Eq. (1). This is done again by a deep neural network out\u03b8 o Pascanu et al. (2014): p(zt |z<t ,x\u2264t) = out\u03b8 o(ht), (4) where \u03b8 o is a set of parameters defining the output neural network.", "startOffset": 289, "endOffset": 802}, {"referenceID": 8, "context": "For instance, it is possible to adapt the weight mechanism from the selective attention model recently proposed by Gregor et al. (2015), which relies on both the current frame xt and the previous prediction z\u0303t\u22121.", "startOffset": 115, "endOffset": 136}, {"referenceID": 1, "context": "We were motivated from recent observations from many research groups that a deep convolutional network, pretrained on a large image dataset of generic, natural images, is useful for subsequent vision tasks which may not necessarily involve the same types of objects (see, e.g., Sermanet et al., 2013; Bar et al., 2015).", "startOffset": 266, "endOffset": 318}, {"referenceID": 5, "context": "As the model parameters are fixed during test time, it will be more straightforward to implement the trained model on a hardware, achieving a desirable level of power consumption and speed (Farabet et al., 2011).", "startOffset": 189, "endOffset": 211}, {"referenceID": 14, "context": ", Li et al., 2013). This largely prevents potential performance degradation from having suboptimal, hand-engineered object representation and detector. Second, the proposed model works with anonymous objects by design. As we train a model with a large set of generic-shaped objects offline, the model learns to detect a generic object that was pointed out initially rather than to detect a certain, predefined set of objects. As the proposed model is a recurrent neural network which can maintain the history of the object\u2019s trajectory, it implicitly learns to find a region in an input frame which has a similar activation pattern from the previous frames. In fact, human babies are known to be able to track objects even before having an ability to classify it into one of the object categories Ruff & Rothbart (2001). Lastly, training is done fully off-line.", "startOffset": 2, "endOffset": 820}, {"referenceID": 8, "context": "Furthermore, they use the selective attention mechanism from (Gregor et al., 2015), allowing the RATM only a small subregion of the whole canvas at each frame.", "startOffset": 61, "endOffset": 82}, {"referenceID": 20, "context": "They used a restricted Boltzmann machine (Smolensky, 1986) as an object detection model together in a filtering-based visual tracking (state-space model with particle filtering for inference.", "startOffset": 41, "endOffset": 58}, {"referenceID": 10, "context": "As we were preparing this work, Kahou et al. (2015) independently proposed a similar visual object tracker based on a recurrent neural network.", "startOffset": 32, "endOffset": 52}, {"referenceID": 10, "context": "As we were preparing this work, Kahou et al. (2015) independently proposed a similar visual object tracker based on a recurrent neural network. Here let us describe the similarities and differences between their recurrent attentive tracking model (RATM) with the proposed tracking approach. A major common feature between these two approaches is that both of these use a recurrent neural network as a main component. A major difference between the RATM and the proposed approach is in training. Both the RATM and the model proposed in this paper use the intermediate locations of an object as an auxiliary target (see Eq. (7).) Kahou et al. (2015) report that this use of auxiliary cost stabilized the tracking quality, which is further confirmed by our experiments presented later in this paper.", "startOffset": 32, "endOffset": 648}, {"referenceID": 10, "context": "As we were preparing this work, Kahou et al. (2015) independently proposed a similar visual object tracker based on a recurrent neural network. Here let us describe the similarities and differences between their recurrent attentive tracking model (RATM) with the proposed tracking approach. A major common feature between these two approaches is that both of these use a recurrent neural network as a main component. A major difference between the RATM and the proposed approach is in training. Both the RATM and the model proposed in this paper use the intermediate locations of an object as an auxiliary target (see Eq. (7).) Kahou et al. (2015) report that this use of auxiliary cost stabilized the tracking quality, which is further confirmed by our experiments presented later in this paper. A major difference is that Kahou et al. (2015) used a classification error, averaged over all the frames, as a final cost, while we propose to use the final localization error.", "startOffset": 32, "endOffset": 844}, {"referenceID": 4, "context": "Earlier, Denil et al. (2012) proposed a visual object tracking system based on deep neural networks.", "startOffset": 9, "endOffset": 29}, {"referenceID": 4, "context": "Earlier, Denil et al. (2012) proposed a visual object tracking system based on deep neural networks. Their model can be considered as an intermediate step away from the conventional tracking approaches toward the one proposed here and by Kahou et al. (2015). They used a restricted Boltzmann machine (Smolensky, 1986) as an object detection model together in a filtering-based visual tracking (state-space model with particle filtering for inference.", "startOffset": 9, "endOffset": 258}, {"referenceID": 15, "context": "Similarly, Mnih et al. (2014) and Ba et al.", "startOffset": 11, "endOffset": 30}, {"referenceID": 0, "context": "(2014) and Ba et al. (2014) showed that a recurrent network tracks an object if it were trained to classify an object, or multiple objects, in an image.", "startOffset": 11, "endOffset": 28}, {"referenceID": 21, "context": "More specifically, we test the models on two sets of sequences containing one or two MNIST-2 digits, where one MNIST-2 digit is created by randomly overlapping two randomly selected normal MNIST digits on top of each other (Wang et al., 2014).", "startOffset": 223, "endOffset": 242}, {"referenceID": 7, "context": "This is a necessary check for any model based on recurrent neural networks, as some recent findings suggest that on certain tasks recurrent neural networks fail to generalize to longer test sequences (Joulin & Mikolov, 2015; Grefenstette et al., 2015).", "startOffset": 200, "endOffset": 251}, {"referenceID": 8, "context": "We developed a variant of the weight scheme used by the selective attention model of Gregor et al. (2015) so as to make it more suitable for tracking.", "startOffset": 85, "endOffset": 106}, {"referenceID": 8, "context": "We developed a variant of the weight scheme used by the selective attention model of Gregor et al. (2015) so as to make it more suitable for tracking. A model employing this novel weighting scheme is is referred to by RecTracker-Att-N, where N denotes the size of grid G. We emphasize that we only weight the pixels of each frame and do not extract a patch, as was done by Gregor et al. (2015) and Kahou et al.", "startOffset": 85, "endOffset": 394}, {"referenceID": 8, "context": "We developed a variant of the weight scheme used by the selective attention model of Gregor et al. (2015) so as to make it more suitable for tracking. A model employing this novel weighting scheme is is referred to by RecTracker-Att-N, where N denotes the size of grid G. We emphasize that we only weight the pixels of each frame and do not extract a patch, as was done by Gregor et al. (2015) and Kahou et al. (2015), because this approach of ignoring an out-of-patch region may lose too much information needed for tracking.", "startOffset": 85, "endOffset": 418}, {"referenceID": 9, "context": "Kernelized Correlation Filters based Tracker (KerCorrTracker) Lastly, we use the kernelized correlation filters-based visual tracker proposed by Henriques et al. (2015). This is one of the stateof-the-art visual object tracking systems and follows a tracking-by-detection strategy (see Sec.", "startOffset": 145, "endOffset": 169}, {"referenceID": 9, "context": "one KerCorrTracker by Henriques et al. (2015). We observe that RecTracker-Att-1 outperforms the KerCorrTracker when there\u2019s only a single object in a test sequence.", "startOffset": 22, "endOffset": 46}], "year": 2017, "abstractText": "In this paper, we propose and study a novel visual object tracking approach based on convolutional networks and recurrent networks. The proposed approach is distinct from the existing approaches to visual object tracking, such as filtering-based ones and tracking-by-detection ones, in the sense that the tracking system is explicitly trained off-line to track anonymous objects in a noisy environment. The proposed visual tracking model is end-to-end trainable, minimizing any adversarial effect from mismatches in object representation and between the true underlying dynamics and learning dynamics. We empirically show that the proposed tracking approach works well in various scenarios by generating artificial video sequences with varying conditions; the number of objects, amount of noise and the match between the training shapes and test shapes.", "creator": "LaTeX with hyperref package"}}}