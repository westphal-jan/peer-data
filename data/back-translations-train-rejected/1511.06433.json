{"id": "1511.06433", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "19-Nov-2015", "title": "Blending LSTMs into CNNs", "abstract": "We show that a deep convolutional network with an architecture inspired by the models used in image recognition can yield accuracy similar to a long-short term memory (LSTM) network, which achieves the state-of-the-art performance on the standard Switchboard automatic speech recognition task. Moreover, we demonstrate that merging the knowledge in the CNN and LSTM models via model compression further improves the accuracy of the convolutional model.", "histories": [["v1", "Thu, 19 Nov 2015 22:48:59 GMT  (182kb,D)", "http://arxiv.org/abs/1511.06433v1", null], ["v2", "Fri, 4 Mar 2016 13:43:02 GMT  (163kb,D)", "http://arxiv.org/abs/1511.06433v2", null], ["v3", "Wed, 14 Sep 2016 14:36:53 GMT  (152kb,D)", "http://arxiv.org/abs/1511.06433v3", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["krzysztof j geras", "abdel-rahman mohamed", "rich caruana", "gregor urban", "shengjie wang", "ozlem aslan", "matthai philipose", "matthew richardson", "charles sutton"], "accepted": false, "id": "1511.06433"}, "pdf": {"name": "1511.06433.pdf", "metadata": {"source": "CRF", "title": null, "authors": ["COMPRESSING LSTMS", "INTO CNNS", "Krzysztof J. Geras", "Abdel-rahman Mohamed", "Rich Caruana", "Gregor Urban", "Shengjie Wang", "\u00d6zlem Aslan", "Matthai Philipose", "Matthew Richardson", "Charles Sutton"], "emails": [], "sections": [{"heading": "1 INTRODUCTION", "text": "There is evidence that neural networks equipped with the latest training algorithms use their large capacity inefficiently (Le Cun et al., 1990; Dauphin & Bengio, 2013; Ba & Caruana, 2014; Hinton et al., 2015) Although these overcapacities are necessary for precise learning and generalizations in education, the function that is often able to be presented in a much more compact way can often be presented as if profound network models are getting larger and more complex, their accuracy often increases, but also the difficulty of grasping them sometimes allows for the exact function that is learned from large, complex models that can be compressed."}, {"heading": "2 BACKGROUND", "text": "In fact, the fact is that most of them are able to move, to move and to move."}, {"heading": "2.1 BIDIRECTIONAL LSTM", "text": "A great example of a very powerful neural network architecture that provides the status-of-the-art is the hidden performance on a number of tasks that are expensive to perform in the test period. (1) There is only one hidden LSTM (Graves & Schmidhuber, 2005; Graves et al., 2013), the condensation of which is the focus of this work. (1) There is only one sequence of input vectors x = (x1,.), a standard RNN method that uses the hidden vector sequence h = (h1,.), the vector sequence h = (y1, yT) by iterating the following equations from 1 to T: ht = H (Wxhxt = H), yt = Whyht + Whyht +."}, {"heading": "3 VISION-STYLE CNNS FOR SPEECH RECOGNITION", "text": "Convolutionary neural networks (LeCun et al., 1998) have long been considered for language (LeCun & Bengio, 1998; Lee et al., 2009), although they have only recently become very successful (Abdel-Hamid et al., 2012; Sainath et al., 2013; Abdel-Hamid et al., 2014; Sainath et al., 2015) The most successful previous work on CNNs for language used only two or three revolutionary layers followed by fully interconnected layers. Also, folds and aggregations over one dimension, either time or frequency, seem wasteful."}, {"heading": "4 COMPRESSING BIDIRECTIONAL LSTMS INTO VISION-STYLE CNNS", "text": "We optimize the following training target, L (\u03bb) = \u03bb \u2212 \u2211 j \u2211 i p (i | xj) log q (i | xj) + (1 \u2212 \u03bb) \u2212 \u2211 j log q (yj | xj), (1) where p (i | xj) is the probability of class i for the training example xj estimated by the teacher, q (i | xj) is the probability of class i assigned to the training example xj by the student and yj is the correct class for the training example xj. The coefficient \u03bb [0, 1] controls the weight of errors on soft and hard labels in the object. \u03bb = 0 means that the network only learns using the hard labels, where the teacher ignores the teacher, while the networks only learn from the soft labels, ignoring the hard labels."}, {"heading": "5 EXPERIMENTS", "text": "In fact, most of them will be able to move to another world in which they will be able to move."}, {"heading": "5.1 BASELINE NETWORKS", "text": "We used Lasagne1, based on Theano (Bergstra et al., 2010; Bastien et al., 2012), to implement CNNs. We used the architecture described in Table 1. We used a batch size of 256 randomly drawn examples. Each epoch consisted of 2000 mini-batches. Hyper-parameters of the basic networks were: initial learning rate (1.7 \u00d7 10 \u2212 2), dynamic coefficient (0.9, we used Nesterov's dynamics) and learning rate decay coefficient (0.7). Since the data set we used is of enormous size, the only form of regulation we used was early1https: / / github.com / Lasagne / Stop in the following form. After each epoch we measured the loss (negative log probability of the correct class) at the validation level. If the loss did not improve the learning ability for five epochs, we multiplied the learning rate with one."}, {"heading": "5.2 NETWORKS TRAINED WITH MODEL COMPRESSION", "text": "We started our experiments with compressing the LSTMs in CNNs by optimizing the target in Equation 3 with \u03bb = 1.0, i.e. using only the soft labels from the LSTM. Doing just this yields FER of 35.43% and WER of 14.7%. Although the use of only the soft labels is not sufficient to match the FER of the teacher LSTM, a model trained in this way is remarkably better than the LSTM teacher in terms of WER.Our next step was the use of both soft labels provided by the LSTM and hard labels from the training data. This time, our best student mixing soft and hard labels in equal proportions in the training goal is better than both the baseline CNN and the teacher LSTM in terms of FER and WHO, reaching 34.19% and 14.0% respectively in terms of the hard labeling material from the training data. We assume that the other results for the LSTM 7 and LSTM 8 mapping results will be similar in this effect."}, {"heading": "6 RELATED WORK", "text": "Most similar to our work is the work of Chan et al. (2015), which compressed LSTMs into small networks without revolutionary layers. Using the soft labels of an LSTM, they were able to show an improvement in WHO over the hard-labeled baseline. Therefore, the main difference between this work and ours is that their student networks are non-convolutional and very tiny, which allows for a decent improvement over the baseline, but the student network is still much worse than the teacher. Therefore, this work does not answer the question we are dealing with, namely whether a network without a recursive structure can function as well or better than an LSTM when using soft labels provided by the LSTM. Model compression was also successfully applied in the speech recognition of Li et al. (2014), which can use deep neural networks without non-recursive labels both as teachers and as students. The architecture of the two networks was the same, except that the student had fewer hidden units in the STANG layer than Wang did in 2015, when Wang finally had the labels penetrated deep DNet."}, {"heading": "7 DISCUSSION AND FUTURE WORK", "text": "The main contribution of this paper is to introduce the technique of model compression in a previously unexplored environment, when both teachers and students perform well but have different inductive biases in mind. In fact, it is more appropriate to call it model compression. We showed that LSTM and CNN learn different kinds of knowledge from the data. We demonstrated how this observation can be utilized through simple ensembling or model blending using model compression. The success of the combination of the two models provides additional motivation for studying neural network architectures that utilize concepts from both recursive networks and evolutionary networks. Finally, we provided experimental evidence that CNN's appropriate vision architecture has the necessary capacity to handle large language sets, and provided a simple, practical recipe to further improve the performance of CNN-based speech recognition models at no cost."}], "references": [{"title": "Applying convolutional neural networks concepts to hybrid NN-HMM model for speech recognition", "author": ["Abdel-Hamid", "Ossama", "Mohamed", "Abdel-rahman", "Jiang", "Hui", "Penn", "Gerald"], "venue": "In ICASSP,", "citeRegEx": "Abdel.Hamid et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Abdel.Hamid et al\\.", "year": 2012}, {"title": "Convolutional neural networks for speech", "author": ["Abdel-Hamid", "Ossama", "Mohamed", "Abdel-rahman", "Jiang", "Hui", "Deng", "Li", "Penn", "Gerald", "Yu", "Dong"], "venue": "recognition. TASLP,", "citeRegEx": "Abdel.Hamid et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Abdel.Hamid et al\\.", "year": 2014}, {"title": "Do deep nets really need to be deep", "author": ["Ba", "Jimmy", "Caruana", "Rich"], "venue": "In NIPS,", "citeRegEx": "Ba et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Ba et al\\.", "year": 2014}, {"title": "Theano: new features and speed improvements", "author": ["Bastien", "Fr\u00e9d\u00e9ric", "Lamblin", "Pascal", "Pascanu", "Razvan", "Bergstra", "James", "Goodfellow", "Ian J", "Bergeron", "Arnaud", "Bouchard", "Nicolas", "Bengio", "Yoshua"], "venue": "Deep Learning and Unsupervised Feature Learning NIPS 2012 Workshop,", "citeRegEx": "Bastien et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Bastien et al\\.", "year": 2012}, {"title": "Theano: a CPU and GPU math expression compiler", "author": ["Bergstra", "James", "Breuleux", "Olivier", "Bastien", "Fr\u00e9d\u00e9ric", "Lamblin", "Pascal", "Pascanu", "Razvan", "Desjardins", "Guillaume", "Turian", "Joseph", "Warde-Farley", "David", "Bengio", "Yoshua"], "venue": "In SciPy,", "citeRegEx": "Bergstra et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Bergstra et al\\.", "year": 2010}, {"title": "Transferring knowledge from a RNN to a DNN", "author": ["Chan", "William", "Ke", "Nan Rosemary", "Laner", "Ian"], "venue": null, "citeRegEx": "Chan et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Chan et al\\.", "year": 2015}, {"title": "Big neural networks waste capacity", "author": ["Dauphin", "Yann", "Bengio", "Yoshua"], "venue": null, "citeRegEx": "Dauphin et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Dauphin et al\\.", "year": 2013}, {"title": "Predicting parameters in deep learning", "author": ["Denil", "Misha", "Shakibi", "Babak", "Dinh", "Laurent", "de Freitas", "Nando"], "venue": "In NIPS,", "citeRegEx": "Denil et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Denil et al\\.", "year": 2013}, {"title": "Learning precise timing with lstm recurrent networks", "author": ["Gers", "Felix A", "Schraudolph", "Nicol N", "Schmidhuber", "J\u00fcrgen"], "venue": null, "citeRegEx": "Gers et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Gers et al\\.", "year": 2003}, {"title": "Switchboard: telephone speech corpus for research and development", "author": ["J.J. Godfrey", "E.C. Holliman", "J. McDaniel"], "venue": "In Acoustics, Speech, and Signal Processing,", "citeRegEx": "Godfrey et al\\.,? \\Q1992\\E", "shortCiteRegEx": "Godfrey et al\\.", "year": 1992}, {"title": "Supervised sequence labelling with recurrent neural networks", "author": ["Graves", "Alex"], "venue": null, "citeRegEx": "Graves and Alex.,? \\Q2012\\E", "shortCiteRegEx": "Graves and Alex.", "year": 2012}, {"title": "Framewise phoneme classification with bidirectional LSTM and other neural network architectures", "author": ["Graves", "Alex", "Schmidhuber", "J\u00fcrgen"], "venue": "Neural Networks,", "citeRegEx": "Graves et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Graves et al\\.", "year": 2005}, {"title": "Speech recognition with deep recurrent neural networks", "author": ["Graves", "Alex", "Mohamed", "Abdel-rahman", "Hinton", "Geoffrey E"], "venue": "In ICASSP,", "citeRegEx": "Graves et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Graves et al\\.", "year": 2013}, {"title": "Distilling the knowledge in a neural network", "author": ["Hinton", "Geoffrey", "Vinyals", "Oriol", "Dean", "Jeff"], "venue": null, "citeRegEx": "Hinton et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Hinton et al\\.", "year": 2015}, {"title": "Long short-term memory", "author": ["Hochreiter", "Sepp", "Schmidhuber", "J\u00fcrgen"], "venue": "Neural computation,", "citeRegEx": "Hochreiter et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Hochreiter et al\\.", "year": 1997}, {"title": "Optimal brain damage", "author": ["Le Cun", "Yann", "Denker", "John S", "Solla", "Sara A"], "venue": "In NIPS,", "citeRegEx": "Cun et al\\.,? \\Q1990\\E", "shortCiteRegEx": "Cun et al\\.", "year": 1990}, {"title": "Convolutional networks for images, speech, and time series", "author": ["LeCun", "Yann", "Bengio", "Yoshua"], "venue": "In The Handbook of Brain Theory and Neural Networks", "citeRegEx": "LeCun et al\\.,? \\Q1998\\E", "shortCiteRegEx": "LeCun et al\\.", "year": 1998}, {"title": "Gradient-based learning applied to document recognition", "author": ["LeCun", "Yann", "Bottou", "L\u00e9on", "Bengio", "Yoshua", "Haffner", "Patrick"], "venue": "Proceedings of the IEEE,", "citeRegEx": "LeCun et al\\.,? \\Q1998\\E", "shortCiteRegEx": "LeCun et al\\.", "year": 1998}, {"title": "Unsupervised feature learning for audio classification using convolutional deep belief", "author": ["Lee", "Honglak", "Pham", "Peter", "Largman", "Yan", "Ng", "Andrew Y"], "venue": null, "citeRegEx": "Lee et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Lee et al\\.", "year": 2009}, {"title": "Learning small-size dnn with outputdistribution-based criteria", "author": ["Li", "Jinyu", "Zhao", "Rui", "Huang", "Jui-Ting", "Gong", "Yifan"], "venue": "In Interspeech,", "citeRegEx": "Li et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Li et al\\.", "year": 2014}, {"title": "Deep bidirectional recurrent networks over spectral windows", "author": ["Mohamed", "Abdel-rahman", "Seide", "Frank", "Yu", "Dong", "Droppo", "Jasha", "Stolcke", "Andreas", "Zweig", "Geoffrey", "Penn", "Gerald"], "venue": "In ASRU,", "citeRegEx": "Mohamed et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Mohamed et al\\.", "year": 2015}, {"title": "Fitnets: Hints for thin deep nets", "author": ["Romero", "Adriana", "Ballas", "Nicolas", "Kahou", "Samira Ebrahimi", "Chassang", "Antoine", "Gatta", "Carlo", "Bengio", "Yoshua"], "venue": "In ICLR,", "citeRegEx": "Romero et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Romero et al\\.", "year": 2014}, {"title": "Deep convolutional neural networks for lvcsr", "author": ["Sainath", "Tara", "Mohamed", "Abdel-rahman", "Kingsbury", "Brian", "Ramabhadran", "Bhuvana"], "venue": "In ICASSP,", "citeRegEx": "Sainath et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Sainath et al\\.", "year": 2013}, {"title": "Deep convolutional neural networks for large-scale speech tasks", "author": ["Sainath", "Tara", "Kingsbury", "Brian", "Saon", "George", "Soltau", "Hagen", "Mohamed", "Abdel-rahman", "Dahl", "Ramabhadran", "Bhuvana"], "venue": "Neural Networks,", "citeRegEx": "Sainath et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Sainath et al\\.", "year": 2015}, {"title": "Long short-term memory based recurrent neural network architectures for large vocabulary speech recognition", "author": ["Sak", "Hasim", "Senior", "Andrew W", "Beaufays", "Fran\u00e7oise"], "venue": null, "citeRegEx": "Sak et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Sak et al\\.", "year": 2014}, {"title": "Very deep convolutional networks for large-scale image recognition", "author": ["Simonyan", "Karen", "Zisserman", "Andrew"], "venue": "In ICLR,", "citeRegEx": "Simonyan et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Simonyan et al\\.", "year": 2014}, {"title": "Knowledge transfer pre-training", "author": ["Tang", "Zhiyuan", "Wang", "Dong", "Pan", "Yiqiao", "Zhang", "Zhiyong"], "venue": null, "citeRegEx": "Tang et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Tang et al\\.", "year": 2015}, {"title": "Recurrent neural network training with dark knowledge transfer", "author": ["Wang", "Dong", "Liu", "Chao", "Tang", "Zhiyuan", "Zhang", "Zhiyong", "Zhao", "Mengyuan"], "venue": null, "citeRegEx": "Wang et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2015}, {"title": "Tree-based state tying for high accuracy acoustic modelling", "author": ["S.J. Young", "J.J. Odell", "P.C. Woodland"], "venue": "In HLT,", "citeRegEx": "Young et al\\.,? \\Q1994\\E", "shortCiteRegEx": "Young et al\\.", "year": 1994}], "referenceMentions": [{"referenceID": 7, "context": "There is evidence that neural networks trained with the current training algorithms use their large capacity inefficiently (Le Cun et al., 1990; Denil et al., 2013; Dauphin & Bengio, 2013; Ba & Caruana, 2014; Hinton et al., 2015).", "startOffset": 123, "endOffset": 229}, {"referenceID": 13, "context": "There is evidence that neural networks trained with the current training algorithms use their large capacity inefficiently (Le Cun et al., 1990; Denil et al., 2013; Dauphin & Bengio, 2013; Ba & Caruana, 2014; Hinton et al., 2015).", "startOffset": 123, "endOffset": 229}, {"referenceID": 13, "context": "In the context of deep neural networks, this approach to model compression is also known as knowledge distillation (Hinton et al., 2015).", "startOffset": 115, "endOffset": 136}, {"referenceID": 13, "context": "In the context of deep neural networks, this approach to model compression is also known as knowledge distillation (Hinton et al., 2015). Additionally, the student can also be shown the original labels of the training data (we will refer to them as hard labels). The main advantage of training the student model using the model compression technique is the fact that the student trained with knowledge provided by the teacher gets a richer supervision signal than just the hard labels from the training data set, i.e. for each training example, it gets the information not only about the correct class but also about how similar the current training example is to other classes. Model compression can also be viewed as way to transfer inductive biases between models. For example, in the case of compressing deep models into shallow ones (Ba & Caruana, 2014), the student model is benefiting from hierarchical representation in the deep model, although it is not able to learn it on its own from hard labels. While model compression can be applied to arbitrary classifiers producing probabilistic prediction over target classes, with the recent success of deep neural networks fueled by new training techniques and dramatic increase in the computational power of modern computers, work on model compression focused on compressing large deep neural networks or ensembles thereof into smaller ones. That is, with less layers, less hidden units or less parameters. In the first paper pursuing that direction, Ba & Caruana (2014) showed that an ensemble of deep neural networks without convolutional layers can be compressed into a single layer network as accurate as a deep one.", "startOffset": 116, "endOffset": 1526}, {"referenceID": 13, "context": "In the context of deep neural networks, this approach to model compression is also known as knowledge distillation (Hinton et al., 2015). Additionally, the student can also be shown the original labels of the training data (we will refer to them as hard labels). The main advantage of training the student model using the model compression technique is the fact that the student trained with knowledge provided by the teacher gets a richer supervision signal than just the hard labels from the training data set, i.e. for each training example, it gets the information not only about the correct class but also about how similar the current training example is to other classes. Model compression can also be viewed as way to transfer inductive biases between models. For example, in the case of compressing deep models into shallow ones (Ba & Caruana, 2014), the student model is benefiting from hierarchical representation in the deep model, although it is not able to learn it on its own from hard labels. While model compression can be applied to arbitrary classifiers producing probabilistic prediction over target classes, with the recent success of deep neural networks fueled by new training techniques and dramatic increase in the computational power of modern computers, work on model compression focused on compressing large deep neural networks or ensembles thereof into smaller ones. That is, with less layers, less hidden units or less parameters. In the first paper pursuing that direction, Ba & Caruana (2014) showed that an ensemble of deep neural networks without convolutional layers can be compressed into a single layer network as accurate as a deep one. In a complementary work, Hinton et al. (2015) focused on compressing ensembles of deep networks into deep networks of the same architecture.", "startOffset": 116, "endOffset": 1722}, {"referenceID": 13, "context": "In the context of deep neural networks, this approach to model compression is also known as knowledge distillation (Hinton et al., 2015). Additionally, the student can also be shown the original labels of the training data (we will refer to them as hard labels). The main advantage of training the student model using the model compression technique is the fact that the student trained with knowledge provided by the teacher gets a richer supervision signal than just the hard labels from the training data set, i.e. for each training example, it gets the information not only about the correct class but also about how similar the current training example is to other classes. Model compression can also be viewed as way to transfer inductive biases between models. For example, in the case of compressing deep models into shallow ones (Ba & Caruana, 2014), the student model is benefiting from hierarchical representation in the deep model, although it is not able to learn it on its own from hard labels. While model compression can be applied to arbitrary classifiers producing probabilistic prediction over target classes, with the recent success of deep neural networks fueled by new training techniques and dramatic increase in the computational power of modern computers, work on model compression focused on compressing large deep neural networks or ensembles thereof into smaller ones. That is, with less layers, less hidden units or less parameters. In the first paper pursuing that direction, Ba & Caruana (2014) showed that an ensemble of deep neural networks without convolutional layers can be compressed into a single layer network as accurate as a deep one. In a complementary work, Hinton et al. (2015) focused on compressing ensembles of deep networks into deep networks of the same architecture. They also experimented with softening predictions of the teacher by dividing the logits by a constant T greater than one called temperature. Using the techniques developed in prior work and adding an extra mimic layer in the middle of the student network, Romero et al. (2014) demonstrated that a moderately deep and wide convolutional network can be compressed into a deeper and narrower convolutional network with much fewer parameters than the teacher network.", "startOffset": 116, "endOffset": 2094}, {"referenceID": 12, "context": "A great example of a very powerful neural network architecture yielding state-of-the-art performance on a range of tasks, yet expensive to run at the test time are the long-short memory network (LSTM) (Hochreiter & Schmidhuber, 1997) and the bidirectional LSTM (Graves & Schmidhuber, 2005; Graves et al., 2013), whose compression is the focus of this work.", "startOffset": 261, "endOffset": 310}, {"referenceID": 12, "context": "Prior work (Graves, 2012; Graves et al., 2013; Sak et al., 2014) has shown however that the LSTM architecture, which uses purposebuilt memory cells to store information, is better at finding and exploiting long range context.", "startOffset": 11, "endOffset": 64}, {"referenceID": 24, "context": "Prior work (Graves, 2012; Graves et al., 2013; Sak et al., 2014) has shown however that the LSTM architecture, which uses purposebuilt memory cells to store information, is better at finding and exploiting long range context.", "startOffset": 11, "endOffset": 64}, {"referenceID": 8, "context": "For the version of LSTM used in this paper (Gers et al., 2003)H is implemented by the following composite function:", "startOffset": 43, "endOffset": 62}, {"referenceID": 11, "context": "Figure from Graves et al. (2013). Figure 2: Bidirectional RNN.", "startOffset": 12, "endOffset": 33}, {"referenceID": 11, "context": "Figure from Graves et al. (2013). Figure 2: Bidirectional RNN. Figure from Graves et al. (2013).", "startOffset": 12, "endOffset": 96}, {"referenceID": 16, "context": "Convolutional neural networks (LeCun et al., 1998) were considered for speech for a long time (LeCun & Bengio, 1998; Lee et al.", "startOffset": 30, "endOffset": 50}, {"referenceID": 18, "context": ", 1998) were considered for speech for a long time (LeCun & Bengio, 1998; Lee et al., 2009), though only recently they have become very successful (Abdel-Hamid et al.", "startOffset": 51, "endOffset": 91}, {"referenceID": 0, "context": ", 2009), though only recently they have become very successful (Abdel-Hamid et al., 2012; Sainath et al., 2013; Abdel-Hamid et al., 2014; Sainath et al., 2015).", "startOffset": 63, "endOffset": 159}, {"referenceID": 22, "context": ", 2009), though only recently they have become very successful (Abdel-Hamid et al., 2012; Sainath et al., 2013; Abdel-Hamid et al., 2014; Sainath et al., 2015).", "startOffset": 63, "endOffset": 159}, {"referenceID": 1, "context": ", 2009), though only recently they have become very successful (Abdel-Hamid et al., 2012; Sainath et al., 2013; Abdel-Hamid et al., 2014; Sainath et al., 2015).", "startOffset": 63, "endOffset": 159}, {"referenceID": 23, "context": ", 2009), though only recently they have become very successful (Abdel-Hamid et al., 2012; Sainath et al., 2013; Abdel-Hamid et al., 2014; Sainath et al., 2015).", "startOffset": 63, "endOffset": 159}, {"referenceID": 0, "context": ", 2009), though only recently they have become very successful (Abdel-Hamid et al., 2012; Sainath et al., 2013; Abdel-Hamid et al., 2014; Sainath et al., 2015). The most successful of previous works on CNNs for speech used only two or three convolutional layers followed by fully connected layers. They also only did convolution and pooling over one dimension, either time or frequency. However, when looking at a spectrogram in Figure 5, it is obvious that similar patterns re-occur both across different points in time and across different frequencies. Using convolution across only one of these dimensions seems wasteful. We hypothesise that classification of windows of speech with CNNs is not that different from object recognition with CNNs. Looking at this problem through the lens of computer vision, we use a convolutional network architecture inspired by the work of Simonyan & Zisserman (2014). We only use small convolutional filters of size 3\u00d73, non-overlapping 2\u00d72 pooling regions and our network also has more layers than networks previously considered for the purpose of speech recognition.", "startOffset": 64, "endOffset": 905}, {"referenceID": 9, "context": "In the experiments we use the Switchboard data set (Godfrey et al., 1992).", "startOffset": 51, "endOffset": 73}, {"referenceID": 28, "context": "The 9000 output classes represent tied tri-phone states that are generated by an HMM/GMM system (Young et al., 1994).", "startOffset": 96, "endOffset": 116}, {"referenceID": 4, "context": "1 BASELINE NETWORKS We used Lasagne1, which is based on Theano (Bergstra et al., 2010; Bastien et al., 2012), for implementing CNNs.", "startOffset": 63, "endOffset": 108}, {"referenceID": 3, "context": "1 BASELINE NETWORKS We used Lasagne1, which is based on Theano (Bergstra et al., 2010; Bastien et al., 2012), for implementing CNNs.", "startOffset": 63, "endOffset": 108}, {"referenceID": 20, "context": "The bidirectional LSTM teacher network we used is very similar to the one in the work of Mohamed et al. (2015). It has four hidden layers, each with 512 hidden units for each direction.", "startOffset": 89, "endOffset": 111}, {"referenceID": 5, "context": "The most similar to our work is the work of Chan et al. (2015) who were compressing LSTMs into small networks without convolutional layers.", "startOffset": 44, "endOffset": 63}, {"referenceID": 5, "context": "The most similar to our work is the work of Chan et al. (2015) who were compressing LSTMs into small networks without convolutional layers. Using the soft labels from an LSTM, they were able to show an improvement in WER over the baseline trained with hard labels. The main difference between this work and ours is that their student networks are non-convolutional and very tiny. While this allows for a decent improvement over the baseline, the student network is still much worse than the teacher. Hence, this work does not answer the question we address, i.e. whether a network without recurrent structure can perform as well or better as an LSTM when using soft labels provided by the LSTM. Model compression was also successfully applied to speech recognition by Li et al. (2014) who used deep neural networks without convolutional layers both as a teacher and a student.", "startOffset": 44, "endOffset": 785}, {"referenceID": 5, "context": "The most similar to our work is the work of Chan et al. (2015) who were compressing LSTMs into small networks without convolutional layers. Using the soft labels from an LSTM, they were able to show an improvement in WER over the baseline trained with hard labels. The main difference between this work and ours is that their student networks are non-convolutional and very tiny. While this allows for a decent improvement over the baseline, the student network is still much worse than the teacher. Hence, this work does not answer the question we address, i.e. whether a network without recurrent structure can perform as well or better as an LSTM when using soft labels provided by the LSTM. Model compression was also successfully applied to speech recognition by Li et al. (2014) who used deep neural networks without convolutional layers both as a teacher and a student. The architecture of the two networks was the same except that the student had less hidden units in each layer. Finally, the work in the opposite direction was done by Wang et al. (2015) and Tang et al.", "startOffset": 44, "endOffset": 1063}, {"referenceID": 5, "context": "The most similar to our work is the work of Chan et al. (2015) who were compressing LSTMs into small networks without convolutional layers. Using the soft labels from an LSTM, they were able to show an improvement in WER over the baseline trained with hard labels. The main difference between this work and ours is that their student networks are non-convolutional and very tiny. While this allows for a decent improvement over the baseline, the student network is still much worse than the teacher. Hence, this work does not answer the question we address, i.e. whether a network without recurrent structure can perform as well or better as an LSTM when using soft labels provided by the LSTM. Model compression was also successfully applied to speech recognition by Li et al. (2014) who used deep neural networks without convolutional layers both as a teacher and a student. The architecture of the two networks was the same except that the student had less hidden units in each layer. Finally, the work in the opposite direction was done by Wang et al. (2015) and Tang et al. (2015). They demonstrated that, when using a small data set for which an LSTM is overfitting, a deep non-convolutional network can provide useful guidance for the LSTM.", "startOffset": 44, "endOffset": 1086}], "year": 2017, "abstractText": "We show that a deep convolutional network with an architecture inspired by the models used in image recognition can yield accuracy similar to a long-short term memory (LSTM) network, which achieves the state-of-the-art performance on the standard Switchboard automatic speech recognition task. Moreover, we demonstrate that merging the knowledge in the CNN and LSTM models via model compression further improves the accuracy of the convolutional model.", "creator": "LaTeX with hyperref package"}}}