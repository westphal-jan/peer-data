{"id": "1511.06435", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "19-Nov-2015", "title": "Comparative Study of Deep Learning Software Frameworks", "abstract": "Deep learning methods have resulted in significant performance improvements in several application domains and as such several software frameworks have been developed to facilitate their implementation. This paper presents a comparative study of four deep learning frameworks, namely Caffe, Neon, Theano, and Torch, on three aspects: extensibility, hardware utilization, and speed. The study is performed on several types of deep learning architectures and we evaluate the performance of the above frameworks when employed on a single machine for both (multi-threaded) CPU and GPU (Nvidia Titan X) settings. The speed performance metrics used here include the gradient computation time, which is important during the training phase of deep networks, and the forward time, which is important from the deployment perspective of trained networks. For convolutional networks, we also report how each of these frameworks support various convolutional algorithms and their corresponding performance. From our experiments, we observe that Theano and Torch are the most easily extensible frameworks. We observe that Torch is best suited for any deep architecture on CPU, followed by Theano. It also achieves the best performance on the GPU for large convolutional and fully connected networks, followed closely by Neon. Theano achieves the best performance on GPU for training and deployment of LSTM networks. Finally Caffe is the easiest for evaluating the performance of standard deep architectures.", "histories": [["v1", "Thu, 19 Nov 2015 22:51:38 GMT  (209kb)", "http://arxiv.org/abs/1511.06435v1", null], ["v2", "Mon, 4 Jan 2016 22:03:33 GMT  (210kb)", "http://arxiv.org/abs/1511.06435v2", null], ["v3", "Wed, 30 Mar 2016 00:54:34 GMT  (281kb)", "http://arxiv.org/abs/1511.06435v3", "Submitted to KDD 2016 with TensorFlow results added. At the time of submission to KDD, TensorFlow was available only with cuDNN v.2 and thus its performance is reported with that version"]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["soheil bahrampour", "naveen ramakrishnan", "lukas schott", "mohak shah"], "accepted": false, "id": "1511.06435"}, "pdf": {"name": "1511.06435.pdf", "metadata": {"source": "CRF", "title": null, "authors": ["Soheil Bahrampour", "Naveen Ramakrishnan", "Lukas Schott", "Mohak Shah"], "emails": ["fixed-term.Lukas.Schott@us.bosch.com", "Mohak.Shah@us.bosch.com"], "sections": [{"heading": null, "text": "ar Xiv: 151 1,06 435v 1 [cs.L G] 19 Nov 201 5"}, {"heading": "1 INTRODUCTION", "text": "This year is the highest in the history of the country."}, {"heading": "2 OVERVIEW OF THE DEEP LEARNING FRAMEWORKS", "text": "As deep learning methods have gained popularity in many application areas in recent years, there has been a lot of interest lately from many academic (e.g. the University of California at Berkeley, 1Note that most of these frameworks have very active community support, which helps to make some of our observations quickly obsolete in the near future.NYU) and industry groups (e.g. Google, Facebook) develop software frameworks (e.g. Theano, Caffe) that help to easily build and test various deep architectures. At the time, some of the widely used deep learning software frameworks were written: Caffe, Theano, Torch, Neon, Chainer, DeepLearning4J, deepmat, Eblearn, MXNet, etc. (for a more complete list of deep learning software see http: / deeplearning.net / software _ links /)."}, {"heading": "3 BENCHMARKING SETUP", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "3.1 EVALUATION METRICS", "text": "To get a holistic understanding of their performance under different system scenarios and application areas, we use two evaluation metrics to benchmark the four software frameworks. Specifically, we clock the speeds of these frameworks for the following operations: \u2022 Forward time: We measure the time it takes for an input batch of a pre-selected batch size, for a given dataset and network to flow through the entire network and produce the corresponding output. This is important because it indicates the latency of a deep network when it is used in the real world. \u2022 Gradient Computation Time: We also measure the time it takes to determine the gradient for each measurable parameter in the deep network for a given input batch. This is an important indicator of training time. Note that in most cases (e.g. Torch) this gradient computation time includes the computation time it takes to call the corresponding forward and backward functions to recalculate, as these two functions should be applied to each other."}, {"heading": "3.2 SYSTEM SETUP", "text": "All experiments are performed on a single machine running on Ubuntu 14.04 with Intel Xeon CPU E5-1650 v2 @ 3.50GHz 12; Nvidia GeForce GTX Titan X / PCIe / SSE2; 32 GiB DDR3 memory; and SSD hard disk. We used openCV 3.0.0, CUDA 7.5, cuDNN v3, and OpenBLAS 0.2.14 with commit ID 3684706. The caffe library used here with commit ID 8c8e832 is used. Neon uses version 1.0.0.rc1 (2015-09-08) with commit ID a6766ff."}, {"heading": "4 RESULTS AND DISCUSSIONS", "text": "Evaluations are performed by training stacked autoencoders and convolutionary networks based on the IMDB verification dataset (LeCun et al., 1998) and the ImageNet datasets (Deng et al., 2009), as well as training the LSTM network based on the IMDB verification dataset (Maas et al., 2011). Note that the evaluation metrics can2We only show the number of contributors in the main repository of the framework; the numbers do not include any other relevant repositories; they are drastically based on the CUDA package used in conjunction with native software. In Torch, you can perform conversion operations with the Nvidia cuDNN library or the cunn library (a CUDA backend for the nn package) or the fbcunn library (deep learning CUDA extensions from Facebook ADNN research, which is easy to perform with fast convolutions)."}, {"heading": "4.1 LENET", "text": "This year it is more than ever before."}, {"heading": "4.2 ALEXNET", "text": "In this section, we train the AlexNet (Krizhevsky et al., 2012) on the ImageNet Dataset, where the data is not read. Note that there are many newer, larger networks such as GoogleNet, OxfordNet, etc., but we stick to AlexNet, as it is the first network that significantly improves performance on ImageNet and is very popular. The network consists of five layers, three of which use groups that restrict the connectivity of filters, and two have local response normalization (LRN) layers. The network also has three layers, two fully connected layers with ReLU activation units and dropout, and a soft logistic loss. Each image is truncated to have the dimension of 224. Data augmentation with random cuts or transformations is not performed. For Caffe, the network is available in the caffe model repository and data Neon. We have taken the code from the database LDB."}, {"heading": "4.3 STACKED AUTOENCODERS", "text": "This year, it is at an all-time high in the history of the European Union."}, {"heading": "4.4 LSTM", "text": "In this section, we train an LSTM network (Graves et al., 2012) to perform sensitivity analysis on IMDB datasets. In this task, each set is treated as a sequence of words (of different lengths), and the network architecture is the same as in LISA Lab (2014), consisting of an embedding layer followed by an LSTM layer. Results from the LSTM layer are then averaged and transmitted to a linear, fully connected layer with softmax logistic regression for binary classification, the sequences within each batch are padded to be the same size as the largest sequence within the batch, and a masking array is used to ensure that the recursive compressions of the LSTM layer remain valid. Torch does not have libraries that support masked operations necessary to process different length input sequences."}, {"heading": "5 CONCLUSIONS", "text": "We evaluated four of the best deep learning frameworks, namely Caffe, Neon, Theano and Torch, for a variety of settings on a single machine. Here are our key observations: \u2022 Theano and Torch are the most extensible frameworks, both in terms of supporting different deep architectures and in terms of supported libraries. Symbolic differentiation is one of the most useful features Theano offers for implementing non-standard deep architectures. \u2022 For CPU-based training and deployment of any Deep Network Architecture tested, Torch is most suitable, followed by Theano, and Neon has the worst performance. \u2022 For GPU-based deployment of trained convolutional and fully connected networks, Torch is most suitable, followed by Theano. \u2022 For GPU-based training of revolutionary and fully connected networks, we found that Theano is fastest for small networks and Torch is fastest for larger networks, followed by STU for larger networks."}, {"heading": "6 APPENDIX", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "6.1 CAFFE", "text": "Caffe is a tool developed by the Berkeley Vision and Learning Center and community contributors and released under the BSD 2-Clause License (Jia et al., 2014). It was developed in C + + with respect to expression, speed, and modularity, using CUDA for GPU computation, and with command-line, python, and Matlab interfaces for training and deployment purposes. It separates the definition of network architecture from actual implementation, and allows convenient and fast exploration of different architectures and layers on CPU or GPU. Caffe can rely on an LMDB database that automatically and easily allocates memory on the host and device based on demand for efficient memory usage and high throughput. The LMDB database supports current readings. Several types of layers and loss functions are already implemented, which can be configured in the form of any graphical file."}, {"heading": "6.2 THEANO", "text": "Theano is a free Python symbol manipulation library under a BSD license that aims to improve execution time and development time for machine learning algorithms (Bergstra et al., 2011; Bastien et al., 2012). It was specifically used for gradient-based methods such as deep learning, which require repeated calculations of sort-based mathematical expressions. In Theano, such mathematical expressions can be quickly encoded with a high-grade description language similar to a functional language that can be compiled and executed on either a CPU or a GPU. Theano uses CUDA libraries to define arrays on an Nvidia GPU memory with Python bindings. Theano contains many Custolllrrm C and CUDA code generators tailored to different types, sizes and shapes of inputs that optimize the compilation of the Tensor calculations."}, {"heading": "6.3 TORCH", "text": "Torch is a scientific computational framework built with Lua (JIT) compilers (Collobert et al., 2011a). It has strong CUDA and CPU backends and includes well-developed, mature machine learning and optimization packages. The included tensor libraries have a very efficient CUDA backend and neural network (nn) libraries can be used to create arbitrary acyclic computation diagrams with automatic differentiation functions, i.e. Twitter has a: forward () function that calculates the output for a given input and lets the input flow through the network; and it has a: backward () function that differentiates every parameter in the network w.r.t., the gradient that is passed on. Torch also provides links to the latest version of Nvidia cuDNN, which provides state-of-the-art torch dups for revolutionary operations."}, {"heading": "6.4 NEON", "text": "Neon is a Python-based deep learning framework developed by Nervana, which was recently released under an open source Apache 2.0 license. Neon has custom CPU and GPU backends known as NervanaCPU and NervanaGPU backends, respectively. NervanaGPU backends consist of kernels written in MaxAs assembler and Python wrappers, and are highly optimized for Nvidia's Maxwell GPUs (e.g. Titan X). The NervanaCPU backend is based on the Python NumPy library. Neon supports popular models such as Convnets, MLPs, RNNNNs, and Autoencoders. Compared to the three frameworks above, Neon is a relatively young framework, so it has not yet been widely adopted in the deep learning community, and many of Neon's other features are already available in the LETS development suites."}], "references": [], "referenceMentions": [], "year": 2015, "abstractText": "Deep learning methods have resulted in significant performance improvements in<lb>several application domains and as such several software frameworks have been<lb>developed to facilitate their implementation. This paper presents a comparative<lb>study of four deep learning frameworks, namely Caffe, Neon, Theano, and Torch,<lb>on three aspects: extensibility, hardware utilization, and speed. The study is per-<lb>formed on several types of deep learning architectures and we evaluate the per-<lb>formance of the above frameworks when employed on a single machine for both<lb>(multi-threaded) CPU and GPU (Nvidia Titan X) settings. The speed performance<lb>metrics used here include the gradient computation time, which is important dur-<lb>ing the training phase of deep networks, and the forward time, which is important<lb>from the deployment perspective of trained networks. For convolutional networks,<lb>we also report how each of these frameworks support various convolutional algo-<lb>rithms and their corresponding performance. From our experiments, we observe<lb>that Theano and Torch are the most easily extensible frameworks. We observe<lb>that Torch is best suited for any deep architecture on CPU, followed by Theano. It<lb>also achieves the best performance on the GPU for large convolutional and fully<lb>connected networks, followed closely by Neon. Theano achieves the best perfor-<lb>mance on GPU for training and deployment of LSTM networks. Finally Caffe is<lb>the easiest for evaluating the performance of standard deep architectures.", "creator": "LaTeX with hyperref package"}}}