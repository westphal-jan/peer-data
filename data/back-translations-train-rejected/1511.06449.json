{"id": "1511.06449", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "19-Nov-2015", "title": "Learning to decompose for object detection and instance segmentation", "abstract": "Although deep convolutional neural networks(CNNs) have achieved remarkable results on object detection and segmentation, pre- and post-processing steps such as region proposals and non-maximum suppression(NMS), have been required. These steps result in high computational complexity and sensitivity to hyperparameters, e.g. thresholds for NMS. In this work, we propose a novel end-to-end trainable deep neural network architecture that generates the correct number of object instances and their bounding boxes (or segmentation masks) given an image, using only a single network evaluation without any pre- or post-processing steps. We have tested on detecting digits in multi-digit images synthesized using MNIST, automatically segmenting digits in these images, and detecting cars in the KITTI benchmark dataset. The proposed approach outperforms a strong CNN baseline on the synthesized digits datasets and shows promising results on KITTI car detection.", "histories": [["v1", "Thu, 19 Nov 2015 23:30:06 GMT  (1661kb,D)", "https://arxiv.org/abs/1511.06449v1", "ICLR 2016 submission"], ["v2", "Mon, 30 Nov 2015 06:07:28 GMT  (1678kb,D)", "http://arxiv.org/abs/1511.06449v2", "ICLR 2016 submission"], ["v3", "Wed, 11 May 2016 02:55:29 GMT  (1551kb,D)", "http://arxiv.org/abs/1511.06449v3", "ICLR 2016 Workshop"]], "COMMENTS": "ICLR 2016 submission", "reviews": [], "SUBJECTS": "cs.CV cs.LG", "authors": ["eunbyung park", "alexander c berg"], "accepted": false, "id": "1511.06449"}, "pdf": {"name": "1511.06449.pdf", "metadata": {"source": "CRF", "title": "LEARNING TO DECOMPOSE FOR OBJECT DETECTION AND INSTANCE SEGMENTATION", "authors": ["Eunbyung Park", "Alexander C. Berg"], "emails": ["eunbyung@cs.unc.edu", "aberg@cs.unc.edu"], "sections": [{"heading": "1 INTRODUCTION", "text": "The idea behind it is that it is a pure project, which is a project that is about putting oneself at the centre, and not a project that is about putting oneself at the centre, but a project that is about putting oneself at the centre. (...) The idea behind it is that it is a project, which is a project. (...) The idea behind it is that it is a project, which is a project, which is a project, which is a project, which is a project, which is a project, which is a project, which is a project. (...) The idea behind it is that it is a project, which is a project, which is a project, which is a project, which is a project, which is a project, which is a project, which is a project. (...) It is a project which is a project, which is a project, which is a project."}, {"heading": "2 DECOMPNET", "text": "In this section, we describe the proposed network architecture, decompNet. Basically, it differs from previous work on object recognition in two aspects. First, decompNet creates response maps for object categories and splits them into several individual instance maps. This is the opposite of previous work on object recognition. Most previous work on object recognition first finds object candidates and then determines the object category (Girshick et al., 2014; Erhan et al., 2014). This approach can require a lot of calculation since it has to go through a classification for each object candidate. Our decomposition is relatively light compared to the classification process. Second, decompNet's decomposition stages consider the entire response map and each stage of the recurring network to be a single instance. This is also very different from existing methods that divide input images into spatial grids and produce one or more predictions per cell by looking only at the local cells in the grid."}, {"heading": "2.1 CATEGORY DECOMPOSITION", "text": "The goal of the first part of the network is to create response cards for each object category. We maintain high-resolution response cards because they will try to give the second part of the network precise position and size of the objects. The proposed network, shown in Figure 1, is a pure Convolutionary Network inspired by Springenberg et al. (2015); Lin et al. (2013); Noh et al. (2015). We did not use fully interconnected layers, as they only increased the number of parameters with no appreciable improvement in performance. We used incremental convolution instead of max-pooling for downsampling and used deconvolution layers to restore resolution back to half or original size of the input image. In the last layers, we put a 1x1 conversion with the same number of filters as the number of categories. Thus, the output of the last Convolutionary Layer c, response cards for each category c."}, {"heading": "2.2 INSTANCE DECOMPOSITION", "text": "Once we have category response cards, the second part of the network shown in the upper part of Figure 1 requires a third and similar performance, takes response cards and produces response cards of the same size, each containing only one instantie.The category response card consists of several response blocks and each blob represents one instance of the category, so this network is responsible for splitting several response blocks into a single blob.The task of this network is relatively simple since classification and localization were carried out by the former network, so the calculation costs of the recursive network are not too expensive, although there are several stages to enable the detection of multiple instances. We used a recursive neural network to realize two important aspects of this job. The network should be able to produce variable length outputs and remember previous states so that it can generate individual response blocks that it has not yet produced.2 The second network architecture and many other alternatives require better results."}, {"heading": "2.3 BOUNDING BOX REGRESSION", "text": "We performed a Bounding Box regression, taking into account the respective instance response map. It predicts a Bounding Box as tuples best = (x, y, w, h), where x and y are the center coordinates of the box and w and h have width and height, respectively. x, y, w, h are each from 0 to 1. We could have written a simple program for drawing Bounding Boxes, but sometimes there is noise in the response map. So we applied a simple regression-based method with dropouts that gave us a robust Bounding Box prediction. We used a simple feed-forward neural network constructed by a hidden layer of 4K hidden units. We strictly followed the smooth L1 loss function in Girshick (2015).smoothL1 (x, y) = {0.5 (x \u2212 y) 2 if | x y < 1 | < otherwise, we recognized an estimated M-based object (x, y \u2212 1)."}, {"heading": "2.4 INSTANCE SEGMENTATION", "text": "Recently, the segmentation of object instances has attracted a lot of attention as people have accumulated extensive instance segmentation data sets (Liang et al., 2015; Hariharan et al., 2014; Lin et al., 2014). Unlike semantic segmentation, individual instances of an object category must be segmented separately; the good thing about decompNet and a mask-based loss function we have proposed is that they are directly applicable to instance segmentation without any modification. Our network generates activation values close to 1 in a safe place and close to 0 in a less safe place. Therefore, we defined a score as an average activation via an instance card (g) = \u2211 i gi1 [gi > \u03b4] / \u0445 i 1 [gi > \u03b4] (6), where g is an instance card that the network produces at each time step and serves as a threshold for ignoring noise activation values."}, {"heading": "3 EXPERIMENTS", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "3.1 SYNTHESIZED DATASET", "text": "In fact, most of us are able to outdo ourselves, both in terms of size and in terms of the way they move, as well as in terms of the way they move, and in terms of the way they do it, how they do it, how they do it, how they do it, how they do it, how they do it, how they do it, how they do it, how they do it, how they do it, how they do it, how they do it, how they do it, how they do it, how they do it, how they do it, how they do it, how they do it, how they do it, how they do it, how they do it, how they do it, how they do it, how they do it, how they do it, how they do it, how they do it, how they do it, they do, do it, do, do it, do, do it, do, do it, do it, do it, do, do it, do it, do it, do it, do, do it, do it, do, do it, do, do, do it, do, do, do it, do, do, do, do, do, do, do, do, do, do, do, do, do, do, do, do, do, do, do, do, do, do, do, do, do, do, do, do, do, do, do, do, do, do, do, do, do, do, do, do, do, do, do, do, do, do, do, do, do, do, do, do, do, do, do, do, do, do, do, do, do, do, do, do, do, do, do, do, do, do, do, do, do, do, do, do, do, do, do, do, do, do, do, do, do, do, do, do, do, do, do, do, do, do, do, do, do, do, do, do, do, do, do, do, do, do, do, do, do, do, do, do, do, do, do, do, do, do, do, do, do, do, do, it, it, it, it, it, it, it"}, {"heading": "4 RELATED WORK", "text": "Mnih et al. (2014); Ba et al. (2015) both proposed a new instance that subdivides multiple attention models based on multiple factual work. They designed the patch network, which is responsible for explicitly specifying the next location of an image to be edited. Then, the classification network processes only a small portion of the image associated with the position. However, we have shown that it is possible to train the network to implicitly view different locations at different time steps without an additional network. The trained network decided where to look at itself and produced an instance map at each time step. In terms of calculation costs, the attention model could save the computation because it does not have to process the entire image. However, if there are many objects in the entire image, it would ultimately process the entire area of the image. Furthermore, it may have to process the same regions multiple times if there is overlap between regional patches. Our network only needs a pillow evaluation and a competition repetition in the image."}, {"heading": "5 CONCLUSION AND FUTURE WORK", "text": "Our proposed network has shown that it is possible to extract high-resolution information for each object instance with a single evaluation. We have shown that this information can be used for important computer vision tasks such as object detection and instance segmentation. In this paper, we used a regression-based loss function with high-resolution masks. Although it worked reasonably well, it was not very robust for many learning factors such as initialization, learning rate, other parameters, and showed a slow convergence rate. We believe that this is mainly because it has too many degrees of freedom and we could replace it with a pixel-by-pixel classification loss function commonly used in segmentation tasks. Furthermore, we firmly believe that spatial regularity of instance maps such as CRF would be more effective to achieve better performance and faster learning speed. We could remove the category decomposition network and an instance replacement network would directly connect to the main motivation for CNN."}, {"heading": "ACKNOWLEDGMENTS", "text": "We thank NSF 1446631, 1452851 and NVIDIA for supporting the GPU hardware and thank Hadi Kiapour, Wei Liu and Phil Ammirato for helpful discussions."}, {"heading": "A. OBJECT DETECTION ON KITTI DATASET", "text": "To show that our proposed network would work for real and more complicated scenarios, we tested both smaller image patterns on the KITTI object detection datasets, especially for the auto category. KITTI has very high resolution images around 370x1224 and there are many object instances in each image. However, some images have more than half their size. The size of the objects varies from very small objects that are only 25 pixels in height to large objects with more than 300 pixels in height. In addition, many objects are overlaid with more than half their size. Figure 7 shows good examples of auto detection. An interesting thing we found is the specific arrangement during the instance composition process. we did not specify an arrangement of instances. However, the recursive network looks first in the center of the image. Then it goes to the left side of the image and then to the right."}], "references": [{"title": "Multiple object recognition with visual attention", "author": ["Jimmy Ba", "Volodymyr Mnih", "Koray Kavukcuoglu"], "venue": "In Proceedings of the International Conference on Learning Representations (ICLR),", "citeRegEx": "Ba et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Ba et al\\.", "year": 2015}, {"title": "Torch7: A matlab-like environment for machine learning", "author": ["Ronan Collobert", "Koray Kavukcuoglu", "Clement Farabet"], "venue": "In BigLearn, NIPS Workshop,", "citeRegEx": "Collobert et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Collobert et al\\.", "year": 2011}, {"title": "Scalable object detection using deep neural networks", "author": ["Dumitru Erhan", "Christian Szegedy", "Alexander Toshev", "Dragomir Anguelov"], "venue": null, "citeRegEx": "Erhan et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Erhan et al\\.", "year": 2014}, {"title": "Fast r-cnn", "author": ["Ross Girshick"], "venue": "In International Conference on Computer Vision (ICCV),", "citeRegEx": "Girshick.,? \\Q2015\\E", "shortCiteRegEx": "Girshick.", "year": 2015}, {"title": "Rich feature hierarchies for accurate object detection and semantic segmentation", "author": ["Ross Girshick", "Jeff Donahue", "Trevor Darrell", "Jitendra Malik"], "venue": "In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR),", "citeRegEx": "Girshick et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Girshick et al\\.", "year": 2014}, {"title": "Simultaneous detection and segmentation", "author": ["Bharath Hariharan", "Pablo Arbel\u00e1ez", "Ross Girshick", "Jitendra Malik"], "venue": "In European Conference on Computer Vision (ECCV),", "citeRegEx": "Hariharan et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Hariharan et al\\.", "year": 2014}, {"title": "Imagenet classification with deep convolutional neural networks", "author": ["Alex Krizhevsky", "Ilya Sutskever", "Geoffrey E. Hinton"], "venue": "In Advances in Neural Information Processing Systems (NIPS)", "citeRegEx": "Krizhevsky et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Krizhevsky et al\\.", "year": 2012}, {"title": "Proposal-free network for instance-level object segmentation", "author": ["Xiaodan Liang", "Yunchao Wei", "Xiaohui Shen", "Jianchao Yang", "Liang Lin", "Shuicheng Yan"], "venue": null, "citeRegEx": "Liang et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Liang et al\\.", "year": 2015}, {"title": "Fully convolutional networks for semantic segmentation", "author": ["Jonathan Long", "Evan Shelhamer", "Trevor Darrell"], "venue": "CVPR (to appear),", "citeRegEx": "Long et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Long et al\\.", "year": 2015}, {"title": "Recurrent models of visual attention", "author": ["Volodymyr Mnih", "Nicolas Heess", "Alex Graves", "koray kavukcuoglu"], "venue": "In Advances in Neural Information Processing Systems (NIPS)", "citeRegEx": "Mnih et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Mnih et al\\.", "year": 2014}, {"title": "Learning deconvolution network for semantic segmentation", "author": ["Hyeonwoo Noh", "Seunghoon Hong", "Bohyung Han"], "venue": "In 2015 IEEE International Conference on Computer Vision (ICCV),", "citeRegEx": "Noh et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Noh et al\\.", "year": 2015}, {"title": "You only look once: Unified, real-time object detection", "author": ["Joseph Redmon", "Santosh Kumar Divvala", "Ross B. Girshick", "Ali Farhadi"], "venue": null, "citeRegEx": "Redmon et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Redmon et al\\.", "year": 2015}, {"title": "Faster R-CNN: Towards real-time object detection with region proposal networks", "author": ["Shaoqing Ren", "Kaiming He", "Ross Girshick", "Jian Sun"], "venue": "In Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "Ren et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Ren et al\\.", "year": 2015}, {"title": "Very deep convolutional networks for large-scale image recognition", "author": ["Karen Simonyan", "Andrew Zisserman"], "venue": "In Proceedings of the International Conference on Learning Representations (ICLR),", "citeRegEx": "Simonyan and Zisserman.,? \\Q2015\\E", "shortCiteRegEx": "Simonyan and Zisserman.", "year": 2015}, {"title": "Deep inside convolutional networks: Visualising image classification models and saliency maps. arXiv:1312.6034", "author": ["Karen Simonyan", "Andrea Vedaldi", "Andrew Zisserman"], "venue": "ICLR workshop track,", "citeRegEx": "Simonyan et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Simonyan et al\\.", "year": 2014}, {"title": "Striving for simplicity: The all convolutional net. arXiv:1412.6806", "author": ["Jost Tobias Springenberg", "Alexey Dosovitskiy", "Thomas Brox", "Martin A. Riedmiller"], "venue": "ICLR workshop track,", "citeRegEx": "Springenberg et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Springenberg et al\\.", "year": 2015}, {"title": "End-to-end people detection in crowded scenes", "author": ["Russell Stewart", "Mykhaylo Andriluka"], "venue": null, "citeRegEx": "Stewart and Andriluka.,? \\Q2015\\E", "shortCiteRegEx": "Stewart and Andriluka.", "year": 2015}, {"title": "Deep neural networks for object detection", "author": ["Christian Szegedy", "Alexander Toshev", "Dumitru Erhan"], "venue": "In Advances in Neural Information Processing Systems (NIPS)", "citeRegEx": "Szegedy et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Szegedy et al\\.", "year": 2013}, {"title": "Segmentation as selective search for object recognition", "author": ["K.E.A. van de Sande", "J.R.R. Uijlings", "T. Gevers", "A.W.M. Smeulders"], "venue": "In IEEE International Conference on Computer Vision (ICCV),", "citeRegEx": "Sande et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Sande et al\\.", "year": 2011}, {"title": "End-to-end integration of a convolution network, deformable parts model and non-maximum suppression", "author": ["Li Wan", "David Eigen", "Rob Fergus"], "venue": "In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR),", "citeRegEx": "Wan et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Wan et al\\.", "year": 2015}, {"title": "Salient object subitizing", "author": ["Jianming Zhang", "Shugao Ma", "Mehrnoosh Sameki", "Stan Sclaroff", "Margrit Betke", "Zhe Lin", "Xiaohui Shen", "Brian Price", "Radomir Mech"], "venue": "In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR),", "citeRegEx": "Zhang et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Zhang et al\\.", "year": 2015}, {"title": "Object detectors emerge in deep scene cnns", "author": ["Bolei Zhou", "Aditya Khosla", "\u00c0gata Lapedriza", "Aude Oliva", "Antonio Torralba"], "venue": "In Proceedings of the International Conference on Learning Representations (ICLR),", "citeRegEx": "Zhou et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Zhou et al\\.", "year": 2015}, {"title": "conv and deconv has 5 parameters, which are the number of input and output channels, filter size, and stride. The number hidden units in fully connected recurrent layerrnn-fc was set to 2048. Batch normalization and rectifier linear unit were used after every layers. The stride 2 was used for down and upsampling", "author": ["Long"], "venue": null, "citeRegEx": "Long,? \\Q2015\\E", "shortCiteRegEx": "Long", "year": 2015}], "referenceMentions": [{"referenceID": 4, "context": "(2011), and is now part of the best performing systems, using deep CNNs as classifiers(Girshick et al., 2014; Girshick, 2015; Ren et al., 2015), sometimes using deep networks for proposing candidate reagions as well.", "startOffset": 86, "endOffset": 143}, {"referenceID": 3, "context": "(2011), and is now part of the best performing systems, using deep CNNs as classifiers(Girshick et al., 2014; Girshick, 2015; Ren et al., 2015), sometimes using deep networks for proposing candidate reagions as well.", "startOffset": 86, "endOffset": 143}, {"referenceID": 12, "context": "(2011), and is now part of the best performing systems, using deep CNNs as classifiers(Girshick et al., 2014; Girshick, 2015; Ren et al., 2015), sometimes using deep networks for proposing candidate reagions as well.", "startOffset": 86, "endOffset": 143}, {"referenceID": 19, "context": "Post-processing, such as non-maximal suppression, is performed to prune out many false positives and is known to be one of the critical factors that affect performance(Wan et al., 2015).", "startOffset": 167, "endOffset": 185}, {"referenceID": 12, "context": "This first became standard practice with work from van de Sande et al. (2011), and is now part of the best performing systems, using deep CNNs as classifiers(Girshick et al.", "startOffset": 58, "endOffset": 78}, {"referenceID": 3, "context": "(2011), and is now part of the best performing systems, using deep CNNs as classifiers(Girshick et al., 2014; Girshick, 2015; Ren et al., 2015), sometimes using deep networks for proposing candidate reagions as well. Post-processing, such as non-maximal suppression, is performed to prune out many false positives and is known to be one of the critical factors that affect performance(Wan et al., 2015). The tension between more object proposals in order to cover any possible object and the difficulty of later pruning out high-scoring but incorrect bounding boxes\u2014and the computational cost of evaluating all the candidate boxes\u2014is one of the challenges in current detector design. Some work, like Ren et al. (2015) tried to solve this problem by using a deep CNN to generate a relatively small number of high-quality candidates, but still require several hundreds of box evaluations.", "startOffset": 87, "endOffset": 718}, {"referenceID": 3, "context": "(2011), and is now part of the best performing systems, using deep CNNs as classifiers(Girshick et al., 2014; Girshick, 2015; Ren et al., 2015), sometimes using deep networks for proposing candidate reagions as well. Post-processing, such as non-maximal suppression, is performed to prune out many false positives and is known to be one of the critical factors that affect performance(Wan et al., 2015). The tension between more object proposals in order to cover any possible object and the difficulty of later pruning out high-scoring but incorrect bounding boxes\u2014and the computational cost of evaluating all the candidate boxes\u2014is one of the challenges in current detector design. Some work, like Ren et al. (2015) tried to solve this problem by using a deep CNN to generate a relatively small number of high-quality candidates, but still require several hundreds of box evaluations. Another tack, e.g., Redmon et al. (2015), removes the region proposal step and directly predicts multiple objects and their locations with a single CNN evaluation.", "startOffset": 87, "endOffset": 928}, {"referenceID": 3, "context": "(2011), and is now part of the best performing systems, using deep CNNs as classifiers(Girshick et al., 2014; Girshick, 2015; Ren et al., 2015), sometimes using deep networks for proposing candidate reagions as well. Post-processing, such as non-maximal suppression, is performed to prune out many false positives and is known to be one of the critical factors that affect performance(Wan et al., 2015). The tension between more object proposals in order to cover any possible object and the difficulty of later pruning out high-scoring but incorrect bounding boxes\u2014and the computational cost of evaluating all the candidate boxes\u2014is one of the challenges in current detector design. Some work, like Ren et al. (2015) tried to solve this problem by using a deep CNN to generate a relatively small number of high-quality candidates, but still require several hundreds of box evaluations. Another tack, e.g., Redmon et al. (2015), removes the region proposal step and directly predicts multiple objects and their locations with a single CNN evaluation. However that method is restricted to a predefined number of outputs (e.g. 49 in a 7x7 grid) even when there are fewer or more objects present, and its performance falls short compared to previous region-based detectors. Our aim is to produce a network that generates the correct number of object instances and their bounding boxes (or segmentation masks) given an image, using only a single network evaluation without any pre- and post-processing steps, such as region proposals and non-maximal suppression. Building these together into a single framework enables much more straightforward end-to-end training of multiple-class multiple-object detectors, potentially allowing easier and wider application, as well as better performance. Some recent results address sub-components of this problem and indicate that a solution may be possible. Predating the region proposal and deep-network-based classifier evaluation approaches, Krizhevsky et al. (2012) showed that their network applied to a whole image had enough information to predict a single object bounding box location per image.", "startOffset": 87, "endOffset": 2005}, {"referenceID": 19, "context": "(2014); Zhou et al. (2015) and others have shown the ability to localize objects in images.", "startOffset": 8, "endOffset": 27}, {"referenceID": 19, "context": "Other approaches try to directly count objects without explicitly addressing recognition or localization Zhang et al. (2015). Toward general computational savings, Ren et al.", "startOffset": 105, "endOffset": 125}, {"referenceID": 12, "context": "Toward general computational savings, Ren et al. (2015), showed deep CNN features can be shared for both object proposals and classification.", "startOffset": 38, "endOffset": 56}, {"referenceID": 4, "context": "Most previous work on object detection first finds object candidates, and then determines the object category(Girshick et al., 2014; Erhan et al., 2014).", "startOffset": 109, "endOffset": 152}, {"referenceID": 2, "context": "Most previous work on object detection first finds object candidates, and then determines the object category(Girshick et al., 2014; Erhan et al., 2014).", "startOffset": 109, "endOffset": 152}, {"referenceID": 21, "context": "These works might have difficulty recognizing larger objects since it is known that the empirical size of receptive fields are not large enough(Zhou et al., 2015).", "startOffset": 143, "endOffset": 162}, {"referenceID": 2, "context": ", 2014; Erhan et al., 2014). This approach can require a great deal of computation since it has to go through classification for every object candidate. Our decomposition is relatively light-weight compared to the classification process. Secondly, the decomposition stages of decompNet look at the entire response map and each stage of the recurrent network can return a single instance. This is also very different from existing methods that divide input images into spatial grids and produce one or multiple predictions per cell by only looking at the local cells in the grid. Region proposal network in Ren et al. (2015) produces k region proposals per each cell.", "startOffset": 8, "endOffset": 624}, {"referenceID": 2, "context": ", 2014; Erhan et al., 2014). This approach can require a great deal of computation since it has to go through classification for every object candidate. Our decomposition is relatively light-weight compared to the classification process. Secondly, the decomposition stages of decompNet look at the entire response map and each stage of the recurrent network can return a single instance. This is also very different from existing methods that divide input images into spatial grids and produce one or multiple predictions per cell by only looking at the local cells in the grid. Region proposal network in Ren et al. (2015) produces k region proposals per each cell. Redmon et al. (2015) produce one object per one location of a 7x7 output grid.", "startOffset": 8, "endOffset": 688}, {"referenceID": 2, "context": ", 2014; Erhan et al., 2014). This approach can require a great deal of computation since it has to go through classification for every object candidate. Our decomposition is relatively light-weight compared to the classification process. Secondly, the decomposition stages of decompNet look at the entire response map and each stage of the recurrent network can return a single instance. This is also very different from existing methods that divide input images into spatial grids and produce one or multiple predictions per cell by only looking at the local cells in the grid. Region proposal network in Ren et al. (2015) produces k region proposals per each cell. Redmon et al. (2015) produce one object per one location of a 7x7 output grid. Stewart & Andriluka (2015) produce multiple faces of people per cell of the last convolutional feature maps.", "startOffset": 8, "endOffset": 773}, {"referenceID": 14, "context": "The proposed network, depicted in figure 1, is an all convolutional network inspired by Springenberg et al. (2015); Lin et al.", "startOffset": 88, "endOffset": 115}, {"referenceID": 14, "context": "The proposed network, depicted in figure 1, is an all convolutional network inspired by Springenberg et al. (2015); Lin et al. (2013); Noh et al.", "startOffset": 88, "endOffset": 134}, {"referenceID": 10, "context": "(2013); Noh et al. (2015). We didn\u2019t use any fully connected layers since they only increased the number of parameters without notable performance improvement.", "startOffset": 8, "endOffset": 26}, {"referenceID": 17, "context": "loss function for penalizing activations at the location where there is no object corresponding to the category (Szegedy et al., 2013).", "startOffset": 112, "endOffset": 134}, {"referenceID": 17, "context": "As mentioned in Szegedy et al. (2013), \u221a Diag(y) + pI term played a key role in avoiding trivial solutions producing all zero outputs.", "startOffset": 16, "endOffset": 38}, {"referenceID": 3, "context": "We strictly followed the smooth L1 loss function in Girshick (2015).", "startOffset": 52, "endOffset": 68}, {"referenceID": 7, "context": "Recently, object instance segmentation has gained much attention since people have collected largescale instance segmentation datasets(Liang et al., 2015; Hariharan et al., 2014; Lin et al., 2014).", "startOffset": 134, "endOffset": 196}, {"referenceID": 5, "context": "Recently, object instance segmentation has gained much attention since people have collected largescale instance segmentation datasets(Liang et al., 2015; Hariharan et al., 2014; Lin et al., 2014).", "startOffset": 134, "endOffset": 196}, {"referenceID": 4, "context": "5 IoU overlap with ground truth boxes as positive examples (Girshick et al., 2014).", "startOffset": 59, "endOffset": 82}, {"referenceID": 4, "context": "3 (Girshick et al., 2014).", "startOffset": 2, "endOffset": 25}, {"referenceID": 7, "context": "For realistic scenarios, we added 30 noisy pen strokes per image in the same way as Mnih et al. (2014). Ground truth labels came for free when we generated the images.", "startOffset": 84, "endOffset": 103}, {"referenceID": 5, "context": "An instance map is correct if its segmentation overlaps with the segmentation of a ground truth instance by more that pre-defined threshold(Hariharan et al., 2014; Liang et al., 2015).", "startOffset": 139, "endOffset": 183}, {"referenceID": 7, "context": "An instance map is correct if its segmentation overlaps with the segmentation of a ground truth instance by more that pre-defined threshold(Hariharan et al., 2014; Liang et al., 2015).", "startOffset": 139, "endOffset": 183}, {"referenceID": 0, "context": "(2014); Ba et al. (2015) both proposed visual attention models.", "startOffset": 8, "endOffset": 25}, {"referenceID": 0, "context": "(2014); Ba et al. (2015) both proposed visual attention models. They designed the glimpse network that is responsible for explicitly indicating next location of an image to be processed. Then, the classification network processes only a small part of the image associated with the position. We have shown that it is possible to train the network to look at different places at different time steps implicitly without an extra network. The trained network decided where to look by itself and produced an instance map at each time steps. In terms of computational cost, the attention model could save computation since it doesn\u2019t have to process the entire image. However, if there are many objects across the image, it would end up processing the entire area of the image. Furthermore, it might have to process the same regions multiple times if there is overlap between region patches. Our network needs only one evaluation, and repetitive computations in instance decomposition stage are performed by smaller networks. Very recently, Liang et al. (2015) proposed a new instance segmentation method based on feed forward CNN.", "startOffset": 8, "endOffset": 1055}, {"referenceID": 0, "context": "(2014); Ba et al. (2015) both proposed visual attention models. They designed the glimpse network that is responsible for explicitly indicating next location of an image to be processed. Then, the classification network processes only a small part of the image associated with the position. We have shown that it is possible to train the network to look at different places at different time steps implicitly without an extra network. The trained network decided where to look by itself and produced an instance map at each time steps. In terms of computational cost, the attention model could save computation since it doesn\u2019t have to process the entire image. However, if there are many objects across the image, it would end up processing the entire area of the image. Furthermore, it might have to process the same regions multiple times if there is overlap between region patches. Our network needs only one evaluation, and repetitive computations in instance decomposition stage are performed by smaller networks. Very recently, Liang et al. (2015) proposed a new instance segmentation method based on feed forward CNN. It is related to our work in terms of the fact that it classified category first, and then divided it into multiple instances. It regresses the number of instance and instance location maps per category. With this information, they applied spectral clustering to separate multiple instances as a post-processing step. In contrast, our network implicitly predicted the number of instances and learned how to cluster response maps, and it is end-to-end trainable. Furthermore, their method is only applicable when there is a segmentation label. However, our method will work as object detection when there is no segmentation label. Stewart & Andriluka (2015) used LSTM to detect multiple people\u2019s faces.", "startOffset": 8, "endOffset": 1783}], "year": 2016, "abstractText": "Although deep convolutional neural networks(CNNs) have achieved remarkable results on object detection and segmentation, preand post-processing steps such as region proposals and non-maximum suppression(NMS), have been required. These steps result in high computational complexity and sensitivity to hyperparameters, e.g. thresholds for NMS. In this work, we propose a novel end-to-end trainable deep neural network architecture, which consists of convolutional and recurrent layers, that generates the correct number of object instances and their bounding boxes (or segmentation masks) given an image, using only a single network evaluation without any preor post-processing steps. We have tested on detecting digits in multi-digit images synthesized using MNIST, automatically segmenting digits in these images, and detecting cars in the KITTI benchmark dataset. The proposed approach outperforms a strong CNN baseline on the synthesized digits datasets and shows promising results on KITTI car detection.", "creator": "LaTeX with hyperref package"}}}