{"id": "1511.06456", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "19-Nov-2015", "title": "Task Loss Estimation for Sequence Prediction", "abstract": "Often, the performance on a supervised machine learning task is evaluated with a loss function that cannot be optimized directly. A common solution to this problem is to simply optimize a surrogate loss function in the hope that the task loss of interest will go down as well. The task loss function defines the cost for all possible answers and it is arguably the most important element of the problem setup. In this work, we argue, that it is beneficial to use a surrogate loss that is explicitly aware of the task loss of interest. We propose a generic method to define such loss-aware optimization criteria for a broad class of structured output problems. We are particularly interested in the end-to-end training scenario where the system is only allowed to produce one discrete answer, such as in speech recognition or machine translation. For this purpose, we propose to train an estimator of the task loss function itself and to make predictions by looking for the output with the lowest estimated loss. We show that the proposed method has certain theoretical guarantees and how it can be applied to the to sequence prediction problems with discrete output symbols. Finally, we validate the new method experimentally on a speech recognition task without extra text corpora and obtain a significant~13\\% relative gain in terms of Character Error Rate over cross-entropy training.", "histories": [["v1", "Thu, 19 Nov 2015 23:51:31 GMT  (54kb,D)", "https://arxiv.org/abs/1511.06456v1", "Submitted to ICLR 2016"], ["v2", "Fri, 27 Nov 2015 22:53:47 GMT  (80kb,D)", "http://arxiv.org/abs/1511.06456v2", "Submitted to ICLR 2016"], ["v3", "Fri, 8 Jan 2016 15:28:19 GMT  (89kb,D)", "http://arxiv.org/abs/1511.06456v3", "Submitted to ICLR 2016"], ["v4", "Tue, 19 Jan 2016 20:48:19 GMT  (90kb,D)", "http://arxiv.org/abs/1511.06456v4", "Submitted to ICLR 2016"]], "COMMENTS": "Submitted to ICLR 2016", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["dzmitry bahdanau", "dmitriy serdyuk", "phil\\'emon brakel", "nan rosemary ke", "jan chorowski", "aaron courville", "yoshua bengio"], "accepted": false, "id": "1511.06456"}, "pdf": {"name": "1511.06456.pdf", "metadata": {"source": "CRF", "title": "TASK LOSS ESTIMATION FOR SEQUENCE PREDICTION", "authors": ["Dzmitry Bahdanau", "Dmitriy Serdyuk", "Aaron Courville", "Yoshua Bengio"], "emails": [], "sections": [{"heading": "1 INTRODUCTION", "text": "In fact, it is so that most of us are able to put ourselves at the top of society, and not only at the top of society, but also at the top of the society in which it is located. (...) In fact, it is so that it is able to assert itself. (...) In fact, it is so that it is able to change the world. (...) It is so that it is able to change the world, to change the world. (...) It is as if it is able to change the world. (...) \"(...) It is as if it is able to change the world. (...)\" (...) \"(\") (\") ((\") (\") (((...) (\") (\") (((\") (() (() (() () (() (() (() () () (() () () () () () () () () () () ()) () () () () () () () () () ()) () () () () () () () () () () () () () () () ()) () () () () () ()) () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () (() () () () () () () () () () (() () () () () () (() () (() () () (() () () (() () () () (() ((() () () (() () (((() () () (() () (() (() () () () () () () (() () (((() () () (() () () (() (() (((() () (() () (() ((()"}, {"heading": "2 TASK LOSS ESTIMATION FOR SUPERVISED LEARNING", "text": "Basic definitions Consider the broad class of verified learning problems in which the trained learner may produce only a single answer y (x) (x). We assume that the task loss is not negative and that there is a unique basic truth answer y (x). (x) Thus, the learner's performance in relation to the task loss L (x) = 0.1 During the training, the learner is provided with training pairs (xi, yi) in which yi = g (xi), assuming that given the basic truth yi, the loss L (x) can be effective for each answer y. The training problem is then defined as follows: In the face of a family of parameterized mappings {h\u03b1} in which the students try to choose one that minimizes the risk (as much as possible)."}, {"heading": "3 TASK LOSS ESTIMATION FOR SEQUENCE PREDICTION", "text": "Sequence prediction problems are sequences over an alphabet C. We assume that the alphabet is not too big, more precisely that a loop over its elements is feasible. In addition, we extend alphabet C by a special sequence endmark $, resulting in the extended alphabet C = C = {$}. For convenience, we assume that all valid output sequences must end with this token. Now, we can formally define the output space as a set of all sequences ending with the sequence endmark Y = {y $: y% C \u0445}, where C * denotes a set of all finite sequences over the alphabet. We will now describe how the estimate of task losses can be applied to the sequence prediction for the following specific scenario: \u2022 The Score function is an encoder decoder model. \u2022 The prediction hmin\u03b1 is defined with a bar search or a tentative search."}, {"heading": "3.1 ENCODER-DECODER MODEL", "text": "A popular model for the sequence prediction is the encoder decoder model. In this approach, the decoder is trained to model the probability P (yj | z (x), y1... j \u2212 1) of the next token yj, giving a representation of the input z (x) generated by the encoder, and the previous tokens y1... j \u2212 1, where y = g (x) is the Ground Truth type type type type type type type type type type type type type type type type type type type type type type type type type type type type type type type type type type type type type type type type type type type type type type type type type type type type type type type type type type type type type type type type type type type type type type type type type type: FED1\u03b1 (x, y) = | y | jj jor q\u03b1 = 1 \u2212 log q\u03b1 (yj, q\u03b1, 1... q\u03b1, c, c, c, c, c, c, c, x, c, c, c, c, c, x, x, (c, c, c), c, c, c, x, x, x, x, (1), c, c, c, c, c, c, x, x, x, x, x, x, c, c, x, c, c, x, x, c, c, x, x, c, c, x, x, x, x, x, x, x, x, x, x, x, x, x."}, {"heading": "3.2 APPLYING TASK LOSS ESTIMATION TO ENCODER-DECODERS", "text": "We want to maintain the structure of the scoring function defined in Equation (8). However, the normalization performed in (9) is no longer necessary, so our new scoring function is simply the sum of the results that could be used for the training. However, there are two concerns that make this direct approach less attractive: \u2022 Intuitively, only the sum of the results can provide sufficient monitoring for the training."}, {"heading": "4 RELATED WORK", "text": "In an early attempt to minimize empirical risk for speech recognition models, the Word error rates were used to calculate a loss often referred to as Maximum Mutual Information (Povey & Woodland, 2002).For each sequence in the data, this goal requires summing up all possible sequences to calculate the expected error rate from the basic probability, something that is only possible for a limited class of models.A recent study (Er et al., 2008) explains and documents improvements in speech recognition that come from other speech recognition methods.In the context of encoder decoders for sequence generation, a curriculum generation is performed."}, {"heading": "5 EXPERIMENTAL SETUP AND RESULTS", "text": "For the experimental validation of the theory discussed in Sections 2 and 3, we use a character level recognition task similar to Bahdanau et al. (2015b) As in our previous paper, we used the Wall Street Journal (WSJ) for our experiments. Inputs to our models were sequences of feature vectors. Each feature vector contained the energy and 40 mel filter functions with their deltas and delta deltas functions, meaning that the dimensionality of the feature vector is 123. We use the standard trigram language model shipped with the WSJ dataset; in addition, we experiment with its extended version created by Kaldi WSJ s5 recipe."}, {"heading": "6 CONCLUSION AND DISCUSSION", "text": "The most important contributions of this paper are twofold: First, we have developed a method for constructing surrogate loss functions that provide guarantees for the loss of the task. Second, we have shown that such a surrogate loss for sequence prediction works better than cross-entropy surrogate loss in minimizing the error rate of the character for a speech recognition task. Our loss function is somewhat similar to that used in Structured SVM (Tsochantaridis et al., 2005). The main difference is that the structured SVM uses the loss of the task to define the difference between the energies assigned to the correct and false predictions, we use the loss of the task to directly define the desired score for all results. Therefore, the target value for the score of an output during the training does not change. We can also analyze our proposed loss from the perspective of score landscape formation (LeCet al, 2006)."}], "references": [{"title": "Neural machine translation by jointly learning to align and translate", "author": ["Bahdanau", "Dzmitry", "Cho", "Kyunghyun", "Bengio", "Yoshua"], "venue": "In Proceedings of the ICLR 2015,", "citeRegEx": "Bahdanau et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2015}, {"title": "End-to-end attention-based large vocabulary speech recognition", "author": ["Bahdanau", "Dzmitry", "Chorowski", "Jan", "Serdyuk", "Dmitriy", "Brakel", "Philemon", "Bengio", "Yoshua"], "venue": "CoRR, abs/1508.04395,", "citeRegEx": "Bahdanau et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2015}, {"title": "Theano: new features and speed improvements", "author": ["Bastien", "Fr\u00e9d\u00e9ric", "Lamblin", "Pascal", "Pascanu", "Razvan", "Bergstra", "James", "Goodfellow", "Ian J", "Bergeron", "Arnaud", "Bouchard", "Nicolas", "Bengio", "Yoshua"], "venue": "Deep Learning and Unsupervised Feature Learning NIPS Workshop,", "citeRegEx": "Bastien et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Bastien et al\\.", "year": 2012}, {"title": "Scheduled sampling for sequence prediction with recurrent neural networks", "author": ["Bengio", "Samy", "Vinyals", "Oriol", "Jaitly", "Navdeep", "Shazeer", "Noam"], "venue": "arXiv preprint arXiv:1506.03099,", "citeRegEx": "Bengio et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Bengio et al\\.", "year": 2015}, {"title": "Curriculum learning", "author": ["Bengio", "Yoshua", "Louradour", "J\u00e9r\u00f4me", "Collobert", "Ronan", "Weston", "Jason"], "venue": "In Proceedings of the 26th annual international conference on machine learning,", "citeRegEx": "Bengio et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Bengio et al\\.", "year": 2009}, {"title": "Pattern Recognition and Machine Learning", "author": ["Bishop", "Christopher"], "venue": null, "citeRegEx": "Bishop and Christopher.,? \\Q2006\\E", "shortCiteRegEx": "Bishop and Christopher.", "year": 2006}, {"title": "Attention-based models for speech recognition", "author": ["Chorowski", "Jan", "Bahdanau", "Dzmitry", "Serdyuk", "Dmitriy", "Cho", "KyungHyun", "Bengio", "Yoshua"], "venue": "CoRR, abs/1506.07503,", "citeRegEx": "Chorowski et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Chorowski et al\\.", "year": 2015}, {"title": "Generic methods for optimization-based modeling", "author": ["Domke", "Justin"], "venue": "In International Conference on Artificial Intelligence and Statistics,", "citeRegEx": "Domke and Justin.,? \\Q2012\\E", "shortCiteRegEx": "Domke and Justin.", "year": 2012}, {"title": "Training mrf-based phrase translation models using gradient ascent", "author": ["Gao", "Jianfeng", "He", "Xiaodong"], "venue": "In HLT-NAACL,", "citeRegEx": "Gao et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Gao et al\\.", "year": 2013}, {"title": "Towards end-to-end speech recognition with recurrent neural networks", "author": ["Graves", "Alex", "Jaitly", "Navdeep"], "venue": "In Proceedings of the 31st International Conference on Machine Learning", "citeRegEx": "Graves et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Graves et al\\.", "year": 2014}, {"title": "Deep speech: Scaling up end-to-end speech recognition", "author": ["Hannun", "Awni Y", "Case", "Carl", "Casper", "Jared", "Catanzaro", "Bryan C", "Diamos", "Greg", "Elsen", "Erich", "Prenger", "Ryan", "Satheesh", "Sanjeev", "Sengupta", "Shubho", "Coates", "Adam", "Ng", "Andrew Y"], "venue": "CoRR, abs/1412.5567,", "citeRegEx": "Hannun et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Hannun et al\\.", "year": 2014}, {"title": "First-pass large vocabulary continuous speech recognition using bi-directional recurrent dnns", "author": ["Hannun", "Awni Y", "Maas", "Andrew L", "Jurafsky", "Daniel", "Ng", "Andrew Y"], "venue": "arXiv preprint arXiv:1408.2873,", "citeRegEx": "Hannun et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Hannun et al\\.", "year": 2014}, {"title": "Direct loss minimization for structured prediction", "author": ["Hazan", "Tamir", "Keshet", "Joseph", "McAllester", "David A"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Hazan et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Hazan et al\\.", "year": 2010}, {"title": "Discriminative learning in sequential pattern recognition", "author": ["He", "Xiaodong", "Deng", "Li", "Chou", "Wu"], "venue": "Signal Processing Magazine, IEEE,", "citeRegEx": "He et al\\.,? \\Q2008\\E", "shortCiteRegEx": "He et al\\.", "year": 2008}, {"title": "Direct error rate minimization of hidden markov models", "author": ["Keshet", "Joseph", "Cheng", "Chih-Chieh", "Stoehr", "Mark", "McAllester", "David A", "Saul", "LK"], "venue": "In INTERSPEECH,", "citeRegEx": "Keshet et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Keshet et al\\.", "year": 2011}, {"title": "Imagenet classification with deep convolutional neural networks. In Advances in Neural Information Processing Systems", "author": ["Krizhevsky", "Alex", "Sutskever", "Ilya", "Hinton", "Geoffrey E"], "venue": "Annual Conference on Neural Information Processing Systems", "citeRegEx": "Krizhevsky et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Krizhevsky et al\\.", "year": 2012}, {"title": "Gradient-based learning applied to document recognition", "author": ["Y. LeCun", "L. Bottou", "Y. Bengio", "P. Haffner"], "venue": "Proceedings of the IEEE,", "citeRegEx": "LeCun et al\\.,? \\Q1998\\E", "shortCiteRegEx": "LeCun et al\\.", "year": 1998}, {"title": "Eesen: End-to-end speech recognition using deep rnn models and wfst-based decoding", "author": ["Miao", "Yajie", "Gowayyed", "Mohammad", "Metze", "Florian"], "venue": "arXiv preprint arXiv:1507.08240,", "citeRegEx": "Miao et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Miao et al\\.", "year": 2015}, {"title": "Minimum phone error and i-smoothing for improved discriminative training", "author": ["Povey", "Daniel", "Woodland", "Philip C"], "venue": "In Acoustics, Speech, and Signal Processing (ICASSP),", "citeRegEx": "Povey et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Povey et al\\.", "year": 2002}, {"title": "The kaldi speech recognition toolkit", "author": ["Povey", "Daniel", "Ghoshal", "Arnab", "Boulianne", "Gilles", "Burget", "Lukas", "Glembek", "Ondrej", "Goel", "Nagendra", "Hannemann", "Mirko", "Motlicek", "Petr", "Qian", "Yanmin", "Schwarz", "Silovsky", "Jan", "Stemmer", "Georg", "Vesely", "Karel"], "venue": "In IEEE 2011 Workshop on Automatic Speech Recognition and Understanding. IEEE Signal Processing Society,", "citeRegEx": "Povey et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Povey et al\\.", "year": 2011}, {"title": "Minimum risk annealing for training log-linear models", "author": ["Smith", "David A", "Eisner", "Jason"], "venue": "In Proceedings of the COLING/ACL on Main conference poster sessions,", "citeRegEx": "Smith et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Smith et al\\.", "year": 2006}, {"title": "Empirical risk minimization of graphical model parameters given approximate inference, decoding, and model structure", "author": ["Stoyanov", "Veselin", "Ropson", "Alexander", "Eisner", "Jason"], "venue": "In International Conference on Artificial Intelligence and Statistics,", "citeRegEx": "Stoyanov et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Stoyanov et al\\.", "year": 2011}, {"title": "Sequence to sequence learning with neural networks. In Advances in Neural Information Processing Systems", "author": ["Sutskever", "Ilya", "Vinyals", "Oriol", "Le", "Quoc V"], "venue": "Annual Conference on Neural Information Processing Systems", "citeRegEx": "Sutskever et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Sutskever et al\\.", "year": 2014}, {"title": "Large margin methods for structured and interdependent output variables", "author": ["Tsochantaridis", "Ioannis", "Joachims", "Thorsten", "Hofmann", "Thomas", "Altun", "Yasemin"], "venue": "In Journal of Machine Learning Research,", "citeRegEx": "Tsochantaridis et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Tsochantaridis et al\\.", "year": 2005}, {"title": "Blocks and fuel: Frameworks for deep learning", "author": ["van Merri\u00ebnboer", "Bart", "Bahdanau", "Dzmitry", "Dumoulin", "Vincent", "Serdyuk", "Dmitriy", "Warde-Farley", "David", "Chorowski", "Jan", "Bengio", "Yoshua"], "venue": null, "citeRegEx": "Merri\u00ebnboer et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Merri\u00ebnboer et al\\.", "year": 2015}, {"title": "Statistical learning theory, volume 1", "author": ["Vapnik", "Vladimir Naumovich"], "venue": "Wiley New York,", "citeRegEx": "Vapnik and Naumovich.,? \\Q1998\\E", "shortCiteRegEx": "Vapnik and Naumovich.", "year": 1998}, {"title": "Simple statistical gradient-following algorithms for connectionist reinforcement learning", "author": ["Williams", "Ronald J"], "venue": "Machine learning,", "citeRegEx": "Williams and J.,? \\Q1992\\E", "shortCiteRegEx": "Williams and J.", "year": 1992}, {"title": "How transferable are features in deep neural networks? In Advances in Neural Information Processing Systems", "author": ["Yosinski", "Jason", "Clune", "Jeff", "Bengio", "Yoshua", "Lipson", "Hod"], "venue": "Annual Conference on Neural Information Processing Systems", "citeRegEx": "Yosinski et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Yosinski et al\\.", "year": 2014}], "referenceMentions": [{"referenceID": 16, "context": "An early example of such a system is a highly successful convolutional network handwriting recognition pipeline (LeCun et al., 1998).", "startOffset": 112, "endOffset": 132}, {"referenceID": 15, "context": "More recent examples are deep convolutional networks designed for image recognition (Krizhevsky et al., 2012), neural translation systems (Sutskever et al.", "startOffset": 84, "endOffset": 109}, {"referenceID": 22, "context": ", 2012), neural translation systems (Sutskever et al., 2014; Bahdanau et al., 2015a), and speech recognizers (Graves & Jaitly, 2014; Hannun et al.", "startOffset": 36, "endOffset": 84}, {"referenceID": 6, "context": ", 2015a), and speech recognizers (Graves & Jaitly, 2014; Hannun et al., 2014a; Chorowski et al., 2015; Bahdanau et al., 2015b).", "startOffset": 33, "endOffset": 126}, {"referenceID": 27, "context": "Parts of end-to-end systems, such as image features extracted by convolutional networks, often successfully replace hand-designed ones (Yosinski et al., 2014).", "startOffset": 135, "endOffset": 158}, {"referenceID": 23, "context": "This target value does not depend on the score of other outputs, which is the key property of the proposed method and the key difference from other approaches to define consistent surrogate losses, such as the generalized hinge loss used in Structured Support Vector Machines (Tsochantaridis et al., 2005).", "startOffset": 276, "endOffset": 305}, {"referenceID": 23, "context": "\u2022 A generalized hinge loss used in Structured Support Vector Machines (Tsochantaridis et al., 2005): Lhinge(x, F\u03b1(x)) = max y (F\u03b1(x, g(x))\u2212 F\u03b1(x, y) + L(g(x), y), 0) .", "startOffset": 70, "endOffset": 99}, {"referenceID": 16, "context": "We refer the reader to LeCun et al. (2006) for a survey of surrogate loss functions (note that their definition of a loss function differs slightly from the one we use in this text).", "startOffset": 23, "endOffset": 43}, {"referenceID": 13, "context": "A recent survey (He et al., 2008) explains and documents improvements in speech recognition brought by other methods of discriminative training of speech recognition systems.", "startOffset": 16, "endOffset": 33}, {"referenceID": 4, "context": "In the context of Encoder-Decoders for sequence generation, a curriculum learning (Bengio et al., 2009) strategy has been proposed to address the discrepancy between the training and testing conditions of models trained with maximum likelihood (Bengio et al.", "startOffset": 82, "endOffset": 103}, {"referenceID": 3, "context": ", 2009) strategy has been proposed to address the discrepancy between the training and testing conditions of models trained with maximum likelihood (Bengio et al., 2015).", "startOffset": 148, "endOffset": 169}, {"referenceID": 21, "context": "Recently, methods for direct empirical risk minimization for structured prediction have been proposed that treat the model and the approximate inference procedure as a single black-box method for generating predictions (Stoyanov et al., 2011; Domke, 2012).", "startOffset": 219, "endOffset": 255}, {"referenceID": 14, "context": "This method has been extended to Hidden Markov Models and applied to phoneme recognition (Keshet et al., 2011).", "startOffset": 89, "endOffset": 110}, {"referenceID": 3, "context": "In the context of Encoder-Decoders for sequence generation, a curriculum learning (Bengio et al., 2009) strategy has been proposed to address the discrepancy between the training and testing conditions of models trained with maximum likelihood (Bengio et al., 2015). It was shown that the performance on several sequence prediction tasks can be improved by gradually transitioning from a fully guided training scheme to one where the model is increasingly conditioned on symbols it generated itself to make training more similar to the decoding stage in which the model will be conditioned on its own predictions as well. While this approach has an intuitive appeal and clearly works well in some situations, it doesn\u2019t take the task loss into account and to our knowledge no clear theoretical motivation for this method has been provided yet. Another issue is that one needs to decide how fast to transition between the two different types of training schemes. Recently, methods for direct empirical risk minimization for structured prediction have been proposed that treat the model and the approximate inference procedure as a single black-box method for generating predictions (Stoyanov et al., 2011; Domke, 2012). The gradient of the loss is backpropagated through the approximate inference procedure itself. While this approach is certainly more direct than the optimization of some auxiliary loss, it requires the loss to be differentiable. Hazan et al. (2010) propose a method for direct loss minimization that approximates the gradient of the task loss using a loss adjusted inference procedure.", "startOffset": 83, "endOffset": 1468}, {"referenceID": 19, "context": "We use the standard trigram language model shipped with the WSJ dataset; in addition we experiment with its extended version created by Kaldi WSJ s5 recipe (Povey et al., 2011).", "startOffset": 156, "endOffset": 176}, {"referenceID": 17, "context": "Another class of models for which results without the language model are sometimes reported are Connectionist Temporal Classification (CTC) models (Graves & Jaitly, 2014; Miao et al., 2015; Hannun et al., 2014b), and the best result we are aware of is 26.", "startOffset": 147, "endOffset": 211}, {"referenceID": 17, "context": "In our experiments with the language models we linearly interpolated the scores produced by the neural networks with the weights of the Finite State Transducer (FST), similarly to (Miao et al., 2015) and (Bahdanau et al.", "startOffset": 180, "endOffset": 199}, {"referenceID": 0, "context": "For experimental confirmation3 of the theory discussed in Sections 2 and 3, we use a characterlevel speech recognition task similar to Bahdanau et al. (2015b). Like in our previous work, we used the Wall Street Journal (WSJ) speech corpus for our experiments.", "startOffset": 135, "endOffset": 159}, {"referenceID": 0, "context": "For experimental confirmation3 of the theory discussed in Sections 2 and 3, we use a characterlevel speech recognition task similar to Bahdanau et al. (2015b). Like in our previous work, we used the Wall Street Journal (WSJ) speech corpus for our experiments. The model is trained on the full 81 hour \u2019train-si284\u2019 training set, we use the \u2019dev93\u2019 development set for validation and model selection, and we report the performance on the \u2019eval92\u2019 test set. The inputs to our models were sequences of feature vectors. Each feature vector contained the energy and 40 mel-filter bank features with their deltas and delta-deltas, which means that the dimensionality of the feature vector is 123. We use the standard trigram language model shipped with the WSJ dataset; in addition we experiment with its extended version created by Kaldi WSJ s5 recipe (Povey et al., 2011). Our main baseline is an Encoder-Decoder from our previous work on end-to-end speech recognition (Bahdanau et al., 2015b), trained with the cross-entropy surrogate loss. We trained a model with the same architecture but using the task loss estimation L greedy2 criterion, which involves greedy prediction of the candidate sequence \u0177 during training. Algorithm 1 formally describes our training procedure. Our main result is the 13% relative improvement of Character Error Rate that task loss estimation training brings compared to the baseline model when no external language model is used (see Table 1). This setup, being not typical for speech recognition research, is still an interesting benchmark for sequence prediction algorithms. We note, that the Word Error Rate of 18% we report here is the best in the literature. Another class of models for which results without the language model are sometimes reported are Connectionist Temporal Classification (CTC) models (Graves & Jaitly, 2014; Miao et al., 2015; Hannun et al., 2014b), and the best result we are aware of is 26.9% reported by Miao et al. (2015). In our experiments with the language models we linearly interpolated the scores produced by the neural networks with the weights of the Finite State Transducer (FST), similarly to (Miao et al.", "startOffset": 135, "endOffset": 1982}, {"referenceID": 17, "context": "The last section contains results from Graves & Jaitly (2014) and Miao et al. (2015). We found that increasing the beam size over 100 for the CE model does not give any improvement.", "startOffset": 66, "endOffset": 85}, {"referenceID": 23, "context": "Our loss function is somewhat similar to the one used in the Structured SVM (Tsochantaridis et al., 2005).", "startOffset": 76, "endOffset": 105}], "year": 2016, "abstractText": "Often, the performance on a supervised machine learning task is evaluated with a task loss function that cannot be optimized directly. Examples of such loss functions include the classification error, the edit distance and the BLEU score. A common workaround for this problem is to instead optimize a surrogate loss function, such as for instance cross-entropy or hinge loss. In order for this remedy to be effective, it is important to ensure that minimization of the surrogate loss results in minimization of the task loss, a condition that we call consistency with the task loss. In this work, we propose another method for deriving differentiable surrogate losses that provably meet this requirement. We focus on the broad class of models that define a score for every input-output pair. Our idea is that this score can be interpreted as an estimate of the task loss, and that the estimation error may be used as a consistent surrogate loss. A distinct feature of such an approach is that it defines the desirable value of the score for every input-output pair. We use this property to design specialized surrogate losses for Encoder-Decoder models often used for sequence prediction tasks. In our experiment, we benchmark on the task of speech recognition. Using a new surrogate loss instead of cross-entropy to train an Encoder-Decoder speech recognizer brings a significant 13% relative improvement in terms of Character Error Rate (CER) in the case when no extra corpora are used for language modeling.", "creator": "LaTeX with hyperref package"}}}