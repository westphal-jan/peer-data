{"id": "1511.06480", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "20-Nov-2015", "title": "On Binary Embedding using Circulant Matrices", "abstract": "Binary embeddings provide efficient and powerful ways to perform operations on large scale data. However binary embedding typically requires long codes in order to preserve the discriminative power of the input space. Thus binary coding methods traditionally suffer from high computation and storage costs in such a scenario. To address this problem, we propose Circulant Binary Embedding (CBE) which generates binary codes by projecting the data with a circulant matrix. The circulant structure allows us to use Fast Fourier Transform algorithms to speed up the computation. For obtaining $k$-bit binary codes from $d$-dimensional data, this improves the time complexity from $O(dk)$ to $O(d\\log{d})$, and the space complexity from $O(dk)$ to $O(d)$.", "histories": [["v1", "Fri, 20 Nov 2015 03:05:15 GMT  (120kb,D)", "https://arxiv.org/abs/1511.06480v1", "This is an extended version of a paper by the first, third, fourth and fifth authors that appeared in ICML 2014 [arXiv:1405.3162]"], ["v2", "Sat, 5 Dec 2015 02:36:10 GMT  (419kb,D)", "http://arxiv.org/abs/1511.06480v2", "This is an extended version of a paper by the first, third, fourth and fifth authors that appeared in ICML 2014 [arXiv:1405.3162]"]], "COMMENTS": "This is an extended version of a paper by the first, third, fourth and fifth authors that appeared in ICML 2014 [arXiv:1405.3162]", "reviews": [], "SUBJECTS": "cs.DS cs.LG", "authors": ["felix x yu", "aditya bhaskara", "sanjiv kumar", "yunchao gong", "shih-fu chang"], "accepted": false, "id": "1511.06480"}, "pdf": {"name": "1511.06480.pdf", "metadata": {"source": "CRF", "title": "On Binary Embedding using Circulant Matrices On Binary Embedding using Circulant Matrices", "authors": ["Felix X. Yu", "Aditya Bhaskara", "Sanjiv Kumar", "Yunchao Gong", "Shih-Fu Chang"], "emails": ["felixyu@google.com", "bhaskaraaditya@gmail.com", "sanjivk@google.com", "yunchao@cs.unc.edu", "sfchang@ee.columbia.edu"], "sections": [{"heading": null, "text": "We examine two settings that differ in the way we select the parameters of the circulation matrix: in the first, the parameters are randomly selected and in the second, the parameters are learned based on the data. For the randomized CBE, we give a theoretical analysis that compares it to binary embedding, using an unstructured random projection matrix. The challenge is to show that the dependencies in the entries of the circulation matrix do not lead to performance losses. In the second setting, we design a novel time-frequency optimization to learn data-dependent circulation predictions that alternately minimize the target in original and Fourier domains. In both settings, we show through extensive experiments that the CBE approach provides much better performance than the state-of-the-art approach when we determine runtime, and a much faster calculation with negligible performance impairment in the IC4th if we provide the number of the impairment in the IC4th."}, {"heading": "1. Introduction", "text": "This year, it is more than ever before in the history of the country in which it is a country, in which it is a country, in which it is a country, in which it is a country."}, {"heading": "1.1 Our results", "text": "In this paper, we propose a novel technique called Circulant Binary Embedding (CBE), which is even faster than bilinear coding. The main idea is to impose a circular (described in detail in Section 3) structure on the projection matrix A in (1).This special structure allows us to calculate the product Ax in time O (d log d) using Fast Fourier Transform (FFT), a tool of great importance in signal processing.The spatial complexity is also only O (d), which makes it efficient even for very high-dimensional data.Table 1 compares the time and spatial complexity for the various methods outlined above. Given the efficiency of the calculation of the CBE, two natural questions arise: How good is the embedding obtained for various information tasks? and how should we select the parameters of circular A? In Section 4, we examine the first question for random CBE, i.e.e."}, {"heading": "2. Background and related work", "text": "The problem of Johnson and Lindenstrauss [JL84] is a basic tool in the field of sketching and dimensioning. (The problem is that if we have a series of points in dimensional space, we can also be a matrix whose designs are drawn by N (0, 1). (The probability is that we will have at least 1 \u2212 2N2e \u2212 (2 \u2212 3)). (The probability is that there will be a decrease in the number of points that we will lead to a decrease in the number of points. (2 \u2212 3) The probability is that there will be a decrease in the number of points. (1 \u2212) The probability that we will lead to a decrease in the number of points. (2 \u2212) The probability that we will lead to a decrease in the number of points is very high. (4 +) The probability that we will come to a decrease in the number of points is very high. (3 \u2212 3) The probability that we will lead to a decrease in the number of points, the number of points \u2212 the number of a number of points."}, {"heading": "3. Circulant Binary Embedding", "text": "Let's start by describing our framework and creating the notation we use in the rest of the paper."}, {"heading": "3.1 The Framework", "text": "We will now describe our algorithm for generating k-bit binary codes from d-dimensional real vectors. Let's start with the discussion of the case k = d and come to the general case in Section 3.3. The key player is the circular matrix, which is represented by a real vector = (r0, r1, \u00b7 \u00b7, rd \u2212 1) T [Gra06].Cr: = r0 rd \u2212 1.. r2 r1 r0 rd \u2212 1 r2... r1 r0.... rd \u2212 2.... rd \u2212 1 rd \u2212 1 rd \u2212 2.. r1 r0. (5) Let D be a diagonal matrix with each diagonal input character \u03c3i, i = 0, \u00b7 \u00b7 \u00b7, d \u2212 1, with a wheel-maker variable (\u00b1 1 with the probability 1 / 2): D = 0 \u04451 \u04452."}, {"heading": "3.2 Computational Complexity", "text": "The main advantage of circular embedding is that it can be quickly calculated with the Fast Fourier Transformation (FFT). The following is a folkloristic result, the proof of which we append for completeness. Proposal 2. For a d-dimensional vector x and any r-bit CBE character (Cr (Dx)) we can calculate with O (d) space and O (d) time. Proof. The spatial complexity results only from the storage of the vector r and the characters D (the O (d). We never need to explicitly store the complete matrix Cr. The main characteristic of a circular matrix is that we can calculate for each vector y-Rd Cry in time O (d log d). This is becauseCr = F \u2212 1d diag (Fdr) Fd \u2212 where Fd is the matrix corresponding to the discrete time."}, {"heading": "3.3 Generalizing to k 6= d", "text": "The above calculation assumed that the number of bits (k) we produce is equal to the input dimension. Now, consider the general case.When k < d is used, we still use the circulation matrix R, Rd \u00b7 d with d parameters, but the output is set as the first k element in (7).This corresponds to the operation plan (x): = characters (Cr, kDx), (10) where Cr, k is the so-called sub-circular matrix shortened in k columns.We find that CBE is no more mathematically efficient with k < d than with k = d.If k > d, the use of a single r causes the repetition of bits, we suggest using Cr for multiple r and linking their outputs.This results in the computational complexity O (k log d) and the spatial complexity O (k)."}, {"heading": "3.4 Choosing the Parameters r", "text": "As mentioned in the introduction, we consider two solutions. In Section 4, we examine the randomized version, in which each element is sampled independently of a Gaussian unit, inspired by the popular approach of location-sensitive hash. Section 5 introduces an optimized version, in which the parameters are optimized based on training data and a distance-maintaining objective function."}, {"heading": "4. Randomized CBE \u2013 A Theoretical Analysis", "text": "We now analyze the properties of CBE when the circular matrix we use is generated by a random d-dimensional vector. Formally, we consider the partial circular matrix Cr, k, for both sides. (11) As before, D is a diagonal matrix of characters. (12) Embedding uses 2d independent \"units\" of randomness. Well, for all two vectors x, y, y, y, Rd, we have the thatE [12k] -like matrix of characters. (12) This implies that the random variable (1 / 2k) is variable (x)."}, {"heading": "4.1 Variance of the angle estimator", "text": "For a vector x and an index i = 1 = 1 = 2 = 2 = 2 = 2 = 2 = 2 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 ="}, {"heading": "4.2 The Johnson-Lindenstrauss Type Result", "text": "Next, we turn to the proof of Theorem 4, where we want to achieve a strong tail limit. < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < &"}, {"heading": "5. Optimized Binary Embedding", "text": "One problem with the randomized CBE method is that it does not use the underlying data distribution while generating the matrix R. In the next section, we propose to learn R in a data-dependent manner to minimize distortion due to circular projection and binarization. We propose data-dependent CBE (CBE-opt) by optimizing the projection matrix with a novel time-frequency-change optimization. We consider the following objective function in learning the d-bit CBE. Extending the Learning k < d-bits will result in Section 5.2.argmin B, r | B \u2212 XRT | | 2F + of the GRT \u2212 I | | 2F (31) s.t. R = circulating CBE (r), with the extension of the Learning k < d-bits in Section 5.2.argmin B | | | 2F + of the GRT frequency resulting in alternating optimizations."}, {"heading": "5.1 The Time-Frequency Alternating Optimization", "text": "In this section, we propose a new approach to efficiently find a local solution. (<) The idea is to optimize the target by specifying the target in the input domain (\"time\" as opposed to \"frequency\"), and B, respectively. (<) Therefore, we propose a new method by optimizing r in the input domain (\"time\" as opposed to \"frequency\"), which results in a very efficient procedure. The target is independent of any element of B. Denote Bij as an element of the i-th series and j-th column of B. It is easy to show that B is updatable: Bij = {1 if Rj-xi-xi 0 \u2212 1 if Rj-xi & ltF < 0, 32)."}, {"heading": "5.2 Learning with Dimensionality Reduction", "text": "In the case of k < d bits, we must solve the following optimization problem: argmin B, r | | BPk \u2212 XPTkRT | | 2F + \u03bb | | RPkPTkRT \u2212 I | | 2F (44) s.t. R = circ (r), where Pk = [Ik O Od \u2212 k], Ik is a k \u00b7 k identity matrix and Od \u2212 k is a (d \u2212 k) \u00b7 (d \u2212 k) all-zero matrix. To solve this problem, we propose a simple solution where Bij = 0, i = \u00b7 \u00b7, N \u2212 1, j = k, \u00b7 \u00b7 \u00b7, d \u2212 1 in (31)."}, {"heading": "6. Discussion", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "6.1 Limitations of the Theory for Long Codes", "text": "As in previous work [LSMK11, GKRL13, SP11] and as we see in our experiments (Section 7), long codes for high-dimensional data are necessary for all binary embedding methods, either randomized or optimized. However, if the code length is too large, our theoretical analysis is not optimal. Consider, for example, our limit of variance if k > \u221a d. Here, the \u03c1 term always dominates, because for each vector we have \u03c1 \u2265 1 / \u221a d (at least one entry of a unit vector is at least 1 / \u221a d).In numerical simulations we see that the variance decreases for a larger range of k by 1 / k, roughly to d. A similar behavior applies to theorem 4, where the condition \u0438 \u2264 \u0445216k log (k / \u03b4) can only apply if k < O (\u0438 d / log d).It is an interesting open question to analyze the variance and other concentration properties for larger k."}, {"heading": "6.2 Semi-supervised Extension", "text": "In some applications, you can access a few pairs of similar and unequal data points. Here, we show how the CBE formula can be expanded to integrate such information into learning by adding an additional objective term J (R).argmin B, r | | B \u2212 XRT | | 2F + \u03bb (RRT \u2212 I | | | 2F (R) (45) s.t. R = circ (r), J \u2212 M | | Rxi \u2212 Rxi \u2212 | | 22 \u2212 p (i, j \u2212 D (Rxi \u2212 Rxj | | 22). (46) Here, M \u2212 D are the sets of \"similar\" and \"unequal\" distances. Intuition is intended to maximize the distances between the unequal pairs and minimize the distances between the similar pairs. Such a term is commonly used in semi-supervised binary encoding methods [WKC10]."}, {"heading": "7. Experiments", "text": "To compare the performance of circular binary embedding techniques, we conduct experiments on three high-dimensional Q-48 datasets used by the current state-of-the-art method for generating long binary codes [GKRL13]. The Flickr 25600 dataset contains 100K images sampled from a noisy Internet image collection. Each image is represented by a 25.600 dimensional vector that is randomized. The third dataset (ImageNet-25600) is another random subset of ImageNet-51200 images that contain 100K images in 25.600 dimensional space. All vectors are normalized to be of the standard. We compare the performance of the randomized DSar versions (CBE-edge) and learned (CBE-opt) versions of our circular embedding with the current state of the art for high-dimensional data."}, {"heading": "7.1 Computational Time", "text": "When generating k-bit code for d-dimensional data, the full projection, the bilinear projection and the circular projection method have time complexity O (kd), O (\u221a kd) or O (d log d), respectively. We compare the computing time in Table 2 on a fixed hardware. Based on our implementation, the computing time of the above three methods can be roughly described as d2: d \u221a d: 5d log d. Note that faster implementation of FFT algorithms leads to better computing time for CBE by further reducing the constant factor. Due to the low memory requirements O (d) and the wide availability of highly optimized FFT libraries, CBE is also suitable for implementation on GPU. Our preliminary tests on a GPU basis show up to 20 times faster computing times compared to the CPU. In this paper, we use the same CBE-based implementation for all methods for a fair comparison."}, {"heading": "7.2 Retrieval", "text": "The second row compares the performance for different techniques with codes of the same length. In this case, the performance of CBE edge is much better than other methods. Even CBE edge exceeds LSH and bilinear code by a large margin. This is in line with our analysis in Section 4. In addition, the second row compares the performance for different techniques with codes of the same length. In this case, the performance of CBE edge is almost identical to LSH, although it is hundreds of times faster. This is in line with our analysis in Section 4. In addition, the CBE opt / bilinear edge is exceeded by bilinear opt / bilinear edge in addition to 2-3 times faster. There are several techniques that do not scale to a high-dimensional case. To compare our method with these, we conduct experiments with a fixed number of bits on a relatively low-dimensional dataset (Flickr-2048), although we have designed them."}, {"heading": "7.3 Classification", "text": "We follow the asymmetric setting of [SP11] by training linear SVM on binary code (Rx) and running tests on the original Rx. Empirically, this has been shown to provide better accuracy than the symmetric method. We use ImageNet-25600, with 100 randomly sampled images per category for training, 50 for validation, and 50 for testing. Code dimension is given as 25,600. As shown in Table 3, CBE, which has a much faster calculation, shows no performance deterioration compared to LSH or bilinear codes in the classification task."}, {"heading": "8. Conclusion", "text": "We proposed a method of binary embedding for high-dimensional data. At the heart of our framework is the use of a kind of highly structured matrix, the circular matrix, to perform linear projection. The proposed method has time complexity O (d log d) and space complexity O (d), but does not show any performance deterioration on real data compared to more expensive approaches (O (d2) or O (d1.5). The parameters of the method can be generated randomly, with interesting theoretical analyses being performed to show that the angle preservation quality can be as good as LSH. The parameters can also be learned on the basis of training data with an efficient optimization algorithm."}, {"heading": "Appendix A. Proofs of the Technical Lemmas", "text": "From our previous observation on independence, it is clear that we have this sign (rTu). (54) Because the LHS sign is the product of expectations, and the first term is 0. (1 \u2212 Character (rTa) Character (rTa) Character (rTa) Character (rTu) 2 \u2212 Character (rTu) Character (rTv) Character (rTv) Character (rTv) Character (rTv) Character (rTv) Character (rTv) Character (rTv) Character (rTv) Character) 2).Now, using the fact that E [XY] Character (X) Character (X) Character (we are equal), along with the observation that the sign (rTa) Character (rTb) Character (rTb) Character (rTb) Character (rTb) Character (rTb) Character (rTb)."}], "references": [{"title": "Approximate nearest neighbors and the fast Johnson-Lindenstrauss transform", "author": ["Nir Ailon", "Bernard Chazelle"], "venue": "In Proceedings of the ACM Symposium on Theory of Computing,", "citeRegEx": "Ailon and Chazelle.,? \\Q2006\\E", "shortCiteRegEx": "Ailon and Chazelle.", "year": 2006}, {"title": "Database-friendly random projections: JohnsonLindenstrauss with binary coins", "author": ["Dimitris Achlioptas"], "venue": "Journal of Computer and System Sciences,", "citeRegEx": "Achlioptas.,? \\Q2003\\E", "shortCiteRegEx": "Achlioptas.", "year": 2003}, {"title": "Similarity estimation techniques from rounding algorithms", "author": ["Moses S Charikar"], "venue": "In Proceedings of the ACM Symposium on Theory of Computing,", "citeRegEx": "Charikar.,? \\Q2002\\E", "shortCiteRegEx": "Charikar.", "year": 2002}, {"title": "Binary embeddings with structured hashed projections", "author": ["Anna Choromanska", "Choromanski Krzysztof", "Mariusz Bojarski", "Tony Jebara", "Sanjiv Kumar", "Yann LeCun"], "venue": "arXiv preprint arXiv:1511.05212v1,", "citeRegEx": "Choromanska et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Choromanska et al\\.", "year": 2015}, {"title": "An exploration of parameter redundancy in deep networks with circulant projections", "author": ["Yu Chen", "Felix Xinnan Yu", "Rogerio Feris", "Sanjiv Kumar", "S.-F. Choudhary", "Alok abd Chang"], "venue": "In Proceedings of the IEEE International Conference on Computer Vision,", "citeRegEx": "Chen et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Chen et al\\.", "year": 2015}, {"title": "Fast neural networks with circulant projections", "author": ["Y. Cheng", "Felix Xinnan Yu", "R.S Feris", "S. Kumar", "A. Choudhary", "S.F. Chang"], "venue": "arXiv preprint arXiv:1502.03436,", "citeRegEx": "Cheng et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Cheng et al\\.", "year": 2015}, {"title": "Imagenet: A large-scale hierarchical image database", "author": ["Jia Deng", "Wei Dong", "Richard Socher", "Li-Jia Li", "Kai Li", "Li Fei-Fei"], "venue": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,", "citeRegEx": "Deng et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Deng et al\\.", "year": 2009}, {"title": "Fast locality-sensitive hashing", "author": ["Anirban Dasgupta", "Ravi Kumar", "Tam\u00e1s Sarl\u00f3s"], "venue": "In Proceedings of the ACM SIGKDD Conference on Knowledge Discovery and Data Mining,", "citeRegEx": "Dasgupta et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Dasgupta et al\\.", "year": 2011}, {"title": "The johnson-lindenstrauss lemma and the sphericity of some graphs", "author": ["Peter Frankl", "Hiroshi Maehara"], "venue": "Journal of Combinatorial Theory, Series B,", "citeRegEx": "Frankl and Maehara.,? \\Q1988\\E", "shortCiteRegEx": "Frankl and Maehara.", "year": 1988}, {"title": "Learning binary codes for high-dimensional data using bilinear projections", "author": ["Yunchao Gong", "Sanjiv Kumar", "Henry A Rowley", "Svetlana Lazebnik"], "venue": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,", "citeRegEx": "Gong et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Gong et al\\.", "year": 2013}, {"title": "Angular quantization-based binary codes for fast similarity search", "author": ["Yunchao Gong", "Sanjiv Kumar", "Vishal Verma", "Svetlana Lazebnik"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Gong et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Gong et al\\.", "year": 2012}, {"title": "Iterative quantization: A procrustean approach to learning binary codes for large-scale image retrieval", "author": ["Y. Gong", "S. Lazebnik", "A. Gordo", "F. Perronnin"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence,", "citeRegEx": "Gong et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Gong et al\\.", "year": 2012}, {"title": "Asymmetric distances for binary embeddings", "author": ["Albert Gordo", "Florent Perronnin"], "venue": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,", "citeRegEx": "Gordo and Perronnin.,? \\Q2011\\E", "shortCiteRegEx": "Gordo and Perronnin.", "year": 2011}, {"title": "Toeplitz and circulant matrices: A review", "author": ["Robert M Gray"], "venue": "Now Pub,", "citeRegEx": "Gray.,? \\Q2006\\E", "shortCiteRegEx": "Gray.", "year": 2006}, {"title": "Vyb\u0301\u0131ral. Johnson-Lindenstrauss lemma for circulant matrices", "author": ["Aicke Hinrichs", "Jan"], "venue": "Random Structures & Algorithms,", "citeRegEx": "Hinrichs and Jan,? \\Q2011\\E", "shortCiteRegEx": "Hinrichs and Jan", "year": 2011}, {"title": "Approximate nearest neighbors: towards removing the curse of dimensionality", "author": ["Piotr Indyk", "Rajeev Motwani"], "venue": "In Proceedings of the ACM Symposium on Theory of Computing,", "citeRegEx": "Indyk and Motwani.,? \\Q1998\\E", "shortCiteRegEx": "Indyk and Motwani.", "year": 1998}, {"title": "Product quantization for nearest neighbor search", "author": ["Herve Jegou", "Matthijs Douze", "Cordelia Schmid"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence,", "citeRegEx": "Jegou et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Jegou et al\\.", "year": 2011}, {"title": "Extensions of lipschitz mappings into a hilbert space", "author": ["William B Johnson", "Joram Lindenstrauss"], "venue": "Contemporary Mathematics,", "citeRegEx": "Johnson and Lindenstrauss.,? \\Q1984\\E", "shortCiteRegEx": "Johnson and Lindenstrauss.", "year": 1984}, {"title": "Learning to hash with binary reconstructive embeddings", "author": ["Brian Kulis", "Trevor Darrell"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Kulis and Darrell.,? \\Q2009\\E", "shortCiteRegEx": "Kulis and Darrell.", "year": 2009}, {"title": "Dense fast random projections and lean walsh transforms. Approximation, Randomization and Combinatorial Optimization", "author": ["Edo Liberty", "Nir Ailon", "Amit Singer"], "venue": "Algorithms and Techniques,", "citeRegEx": "Liberty et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Liberty et al\\.", "year": 2008}, {"title": "Hashing algorithms for large-scale learning", "author": ["Ping Li", "Anshumali Shrivastava", "Joshua Moore", "Arnd Christian Konig"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Li et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Li et al\\.", "year": 2011}, {"title": "Fastfood \u2013 approximating kernel expansions in loglinear time", "author": ["Quoc Le", "Tam\u00e1s Sarl\u00f3s", "Alex Smola"], "venue": "In Proceedings of the International Conference on Machine Learning,", "citeRegEx": "Le et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Le et al\\.", "year": 2013}, {"title": "Hashing with graphs", "author": ["Wei Liu", "Jun Wang", "Sanjiv Kumar", "Shih-Fu Chang"], "venue": "In Proceedings of the International Conference on Machine Learning,", "citeRegEx": "Liu et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Liu et al\\.", "year": 2011}, {"title": "Minimal loss hashing for compact binary codes", "author": ["Mohammad Norouzi", "David Fleet"], "venue": "In Proceedings of the International Conference on Machine Learning,", "citeRegEx": "Norouzi and Fleet.,? \\Q2012\\E", "shortCiteRegEx": "Norouzi and Fleet.", "year": 2012}, {"title": "Hamming distance metric learning", "author": ["Mohammad Norouzi", "David Fleet", "Ruslan Salakhutdinov"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Norouzi et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Norouzi et al\\.", "year": 2012}, {"title": "Locality-sensitive binary codes from shift-invariant kernels", "author": ["Maxim Raginsky", "Svetlana Lazebnik"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Raginsky and Lazebnik.,? \\Q2009\\E", "shortCiteRegEx": "Raginsky and Lazebnik.", "year": 2009}, {"title": "Hanson-wright inequality and subgaussian concentration", "author": ["Mark Rudelson", "Roman Vershynin"], "venue": "Electron. Commun. Probab,", "citeRegEx": "Rudelson and Vershynin.,? \\Q2013\\E", "shortCiteRegEx": "Rudelson and Vershynin.", "year": 2013}, {"title": "High-dimensional signature compression for large-scale image classification", "author": ["Jorge S\u00e1nchez", "Florent Perronnin"], "venue": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,", "citeRegEx": "S\u00e1nchez and Perronnin.,? \\Q2011\\E", "shortCiteRegEx": "S\u00e1nchez and Perronnin.", "year": 2011}, {"title": "Sequential projection learning for hashing with compact codes", "author": ["Jun Wang", "Sanjiv Kumar", "Shih-Fu Chang"], "venue": "In Proceedings of the International Conference on Machine Learning,", "citeRegEx": "Wang et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2010}, {"title": "Spectral hashing", "author": ["Yair Weiss", "Antonio Torralba", "Rob Fergus"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Weiss et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Weiss et al\\.", "year": 2008}, {"title": "Binary embedding: Fundamental limits and fast algorithm", "author": ["Xinyang Yi", "Constantine Caramanis", "Eric Price"], "venue": "arXiv preprint arXiv:1502.05746,", "citeRegEx": "Yi et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Yi et al\\.", "year": 2015}, {"title": "Circulant binary embedding", "author": ["Felix Xinnan Yu", "S. Kumar", "Y. Gong", "S.-F. Chang"], "venue": "In Proceedings of the International Conference on Machine Learning,", "citeRegEx": "Yu et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Yu et al\\.", "year": 2014}, {"title": "Compact nonlinear maps and circulant extensions", "author": ["Felix Xinnan Yu", "Sanjiv Kumar", "Henry Rowley", "Shih-Fu Chang"], "venue": "arXiv preprint arXiv:1503.03893,", "citeRegEx": "Yu et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Yu et al\\.", "year": 2015}, {"title": "Deep fried convnets", "author": ["Zichao Yang", "Marcin Moczulski", "Misha Denil", "Nando de Freitas", "Alex Smola", "Le Song", "Ziyu Wang"], "venue": "arXiv preprint arXiv:1412.7149,", "citeRegEx": "Yang et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Yang et al\\.", "year": 2014}, {"title": "New bounds for circulant Johnson-Lindenstrauss embeddings", "author": ["Hui Zhang", "Lizhi Cheng"], "venue": "arXiv preprint arXiv:1308.6339,", "citeRegEx": "Zhang and Cheng.,? \\Q2013\\E", "shortCiteRegEx": "Zhang and Cheng.", "year": 2013}], "referenceMentions": [], "year": 2015, "abstractText": "Binary embeddings provide efficient and powerful ways to perform operations on large scale data. However binary embedding typically requires long codes in order to preserve the discriminative power of the input space. Thus binary coding methods traditionally suffer from high computation and storage costs in such a scenario. To address this problem, we propose Circulant Binary Embedding (CBE) which generates binary codes by projecting the data with a circulant matrix. The circulant structure allows us to use Fast Fourier Transform algorithms to speed up the computation. For obtaining k-bit binary codes from d-dimensional data, our method improves the time complexity from O(dk) to O(d log d), and the space complexity from O(dk) to O(d). We study two settings, which differ in the way we choose the parameters of the circulant matrix. In the first, the parameters are chosen randomly and in the second, the parameters are learned using the data. For randomized CBE, we give a theoretical analysis comparing it with binary embedding using an unstructured random projection matrix. The challenge here is to show that the dependencies in the entries of the circulant matrix do not lead to a loss in performance. In the second setting, we design a novel time-frequency alternating optimization to learn data-dependent circulant projections, which alternatively minimizes the objective in original and Fourier domains. In both the settings, we show by extensive experiments that the CBE approach gives much better performance than the state-ofthe-art approaches if we fix a running time, and provides much faster computation with negligible performance degradation if we fix the number of bits in the embedding.", "creator": "LaTeX with hyperref package"}}}