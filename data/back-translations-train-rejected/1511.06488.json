{"id": "1511.06488", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "20-Nov-2015", "title": "Resiliency of Deep Neural Networks under Quantization", "abstract": "The complexity of deep neural network algorithms for hardware implementation can be much lowered by optimizing the word-length of weights and signals. Direct quantization of floating-point weights, however, does not show good performance when the number of bits assigned is small. Retraining of quantized networks has been developed to relieve this problem. In this work, the effects of retraining are analyzed for a feedforward deep neural network (FFDNN) and a convolutional neural network (CNN). The network complexity is controlled to know their effects on the resiliency of quantized networks by retraining. The complexity of the FFDNN is controlled by varying the unit size in each hidden layer and the number of layers, while that of the CNN is done by modifying the feature map configuration. We find that the performance gap between the floating-point and the retrain-based ternary (+1, 0, -1) weight neural networks exists with a fair amount in 'complexity limited' networks, but the discrepancy almost vanishes in fully complex networks whose capability is limited by the training data, rather than by the number of connections. This research shows that highly complex DNNs have the capability of absorbing the effects of severe weight quantization through retraining, but connection limited networks are less resilient.", "histories": [["v1", "Fri, 20 Nov 2015 04:55:46 GMT  (699kb,D)", "http://arxiv.org/abs/1511.06488v1", null], ["v2", "Wed, 25 Nov 2015 12:59:38 GMT  (919kb,D)", "http://arxiv.org/abs/1511.06488v2", null], ["v3", "Thu, 7 Jan 2016 13:50:22 GMT  (591kb,D)", "http://arxiv.org/abs/1511.06488v3", null]], "reviews": [], "SUBJECTS": "cs.LG cs.NE", "authors": ["wonyong sung", "sungho shin", "kyuyeon hwang"], "accepted": false, "id": "1511.06488"}, "pdf": {"name": "1511.06488.pdf", "metadata": {"source": "CRF", "title": null, "authors": ["Wonyong Sung", "Sungho Shin", "Kyuyeon Hwang"], "emails": ["wysung@snu.ac.kr", "shshin@dsp.snu.ac.kr", "kyuyeon.hwang@gmail.com"], "sections": [{"heading": null, "text": "The direct quantization of floating-point weights, however, does not perform well when the number of bits allocated is low. To solve this problem, a quantified network retraining has been developed. As part of this work, the effects of retraining on a Feedback Deep Neural Network (FFDNN) and a Convolutionary Neural Network (CNN) are analyzed, the network complexity is controlled to know its impact on the resilience of quantified networks through retraining, the complexity of FFDNN is controlled by varying the unit size in each hidden layer and the number of layers, while that of CNN is controlled by modifying the configuration of the functional sketches. We note that the performance gap between the floating-point and the retraining capacity of ternals (+ 1, 0, -1) neural networks with an appropriate amount of complexity is limited."}, {"heading": "1 INTRODUCTION", "text": "Deep neural networks are beginning to find many real-time applications, such as speech recognition, autonomous driving, gesture recognition and robot control (Sak et al. (2015); Chen et al. (2015); Jalab et al. (2015); Corradini et al. (2015). Although most deep neural networks are nowadays implemented using GPUs (Graphics Processing Units), their implementation in hardware can offer many advantages in terms of power consumption and system size (Ovtcharov et al. (2015)). FPGA-based implementation examples from CNN show more than 10 times an advantage in power consumption (Ovtcharov et al. (2015). Neural network algorithms employ many multiply and add (MAC) operations that mimic the operations of biological neurons, suggesting that reconfigurable hardware arrays that contain fairly homogeneous hardware blocks, such as very efficient blocks."}, {"heading": "2 RELATED WORK", "text": "Some early work used statistical modeling of quantization noise for application to linear digital filters.The simulation-based word length optimization method used simulation tools to evaluate the fixed-point weight performance of a system that can optimize nonlinear algorithms (Sung & Kum (1995)).Ternary (+ 1, 0, -1) coefficient-based digital filters were used to eliminate multiplication at the expense of higher quantization noise.The implementation of adaptive filters with ternary weight values was developed, but it required over-sampling to eliminate quantization effects (Hussain et al. (2007).Fixed-point network design was also investigated with the same purpose of lowering hardware implementation costs (Moerland & Fiesler)."}, {"heading": "3 FIXED-POINT FFDNN AND CNN DESIGN", "text": "This section explains the design of FFDNN and CNN with different network complexity and fixed point optimization."}, {"heading": "3.1 FFDNN AND CNN DESIGN", "text": "Each layer k has a signal vector yk, which is extended to the next layer by multiplying the weight matrix Wk + 1. (1) One of the most popular activation functions is the rectified linear unit, which asRelu (x) = max. (0, x) is defined as follows: In this work, an FFDNN is used for phoneme detection. DNN has four hidden layers. Each of the hidden layers has Nh units; the value of Nh is changed to control the complexity of the network. We also reduce the number of hidden layers. We conduct experiments with the Nh size of 32, 64, 128, 256, 512 and 1024 layers. The layer of the network is reduced to an efficient one."}, {"heading": "3.2 FIXED-POINT OPTIMIZATION OF DNNS", "text": "Reducing the word length of weights brings several advantages in hardware-based implementation of neural networks. First, it lowers arithmetic precision, thereby reducing the number of gates needed for multipliers. Second, the memory size for storing weights is minimized, which would be a big advantage if they are held on a chip, rather than external DRAMs or NAND flash memory. Note: FFDNNs and recursive neural networks require a very large number of weights. Third, reduced arithmetic precision or minimization of off-chip memory accesses leads to low power consumption. However, we need to address the quantization effects that affect system performance in reducing word length. Direct quantization converts a floating point value to the next integrated number that is conventionally used in signal processing. However, direct quantization typically requires more than 8 when the number of bits is not good and does not show good performance."}, {"heading": "4 EXPERIMENTS", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.1 ANALYSIS OF DIRECT QUANTIZATION", "text": "In this analysis, quantity is applied to each weight group represented in Figures 1 and 2 in order to detect the effects of reducing the amount of words. In this section, we try to analyze the effects of direct quantification as follows: TIG = TIG = TIG = TIG = TIG = TIG = TIG = TIG = TIG = TIG = TIG = TIG = TIG = TIG = TIG = TIG = TIG = TIG = TIG = TIG = TIG = TIG = TIG = TIG = TIG = TIG = TIG = TIG = TIG = TIG = TIG = TIG = TIG = TIG = TIG = TIG = TIG = TIG = TIG = TIG = TIG = TIG = TIG = TIG = TIG = TIG = TIG = TIG = TIG = TIG = TIG = TIG = TIG = TIG = TIG = TIG = TIG = TIG = TIG = TIG = TIG = TIG = TIG = TIG = TIG = TIG = TIG = TIG = TIG = TIG = TIG = TIG = TIG = TIG = TIG = TIG = TIG = TIG = TIG = TIG = TIG = TIG = TIG = TIG = TIG = TIG = TIG = TIG = TIG = TIG = TIG = TIG = TIG = TIG = TIG = TIG = TIG = TIG = TIG = TIG = TIG = TIG = TIG = TIG = TIG = TIG = TIG = TIG = TIG = TIG = TIG = TIG = TIG = TIG = TIG = TIG = TIG = TIG = TIG = TIG = TIG = TIG = TIG = TIG = TIG = TIG = TIG = TIG = TIG = TIG = TIG = TIG = TIG = TIG = TIG = TIG = TIG = TIG = TIG = TIG = TIG = TIG = TIG = TIG = TIG = TIG = TIG = TIG = TIG = TIG = TIG = TIG = TIG ="}, {"heading": "4.2 EFFECTS OF RETRAINING ON QUANTIZED NETWORKS", "text": "To get the DNN to learn the effects of quantification, we perform retraining in the directly quantified networks using the same data for floating-point training; the fixed-point performance of the FFDNN is shown in Figure 6, when the number of hidden units in each layer varies; the performance of direct 2 bits (ternary levels), direct 3 bits (7 levels), retrain-based 2 bits and retrain-based 3 bits are compared to the floating-point simulation; we can find that the performance gap between the floating-point and retrain-based fixed-point networks converges very quickly as the network size grows; although the performance gap between the direct and floating-point networks also converges, the convergence speed of the floating-point network is significantly different."}, {"heading": "4.3 FIXED-POINT PERFORMANCES WHEN VARYING THE DEPTH", "text": "It is generally known that increasing the depth usually has a positive effect on the performance of a DNN (Yu et al. (2012a)). The network complexity of a DNN changes by increasing or decreasing the number of hidden layers or feature layers. The result of fixed point and floating point performance with variation in the number of hidden layers for the FFDNN is summarized in Table 1. This table shows that both the floating point and the fixed point performance of the FFDNN are varied by adding hidden layers from 0 to 4. As expected, the performance gap between floating point and fixed point networks shrinks with increasing number of layers. The network complexity of CNN is also varied by decreasing the level of feature maps as shown in Table 2. As expected, the performance of both floating point and retrain-based networks decreases due to low precision as the number of layers is reduced."}, {"heading": "5 DISCUSSION", "text": "In this study, we control the network size by changing the number of units in the hidden layers, the number of feature maps, or the number of layers. In any case, the reduced complexity reduces resistance to quantization. We are conducting similar experiments with relapsing neural networks, which are known to be more sensitive to quantization, and this work appears to be directly related to several methods of network optimization, such as pruning, error tolerance, and decomposition. In pruning, the retraining of weights after zeroing of smaller, more valuable weights is performed. The effects of pruning, fault tolerance, and network dismemberment efficiency would depend on the redundant rendering capability of DNNs. This study can be applied to hardware-efficient DNN design. For designs with limited hardware resources, if the size of the reference DNN is relatively small, it is recommended to apply very low computational accuracy and instead increase network capacity as much as is allowed."}, {"heading": "6 CONCLUSION", "text": "We analyze the performance of fixed neural networks, an FFDNN for phoneme detection, and a CNN for image classification, varying not only arithmetic precision, but also their network complexity; the low-precision networks for this analysis are determined by applying retrain-based quantization methods, and the network complexity is controlled by changing the configuration of the hidden layers or feature cards; the performance gap between the floating point and the neural fixed point networks with ternary weights (+ 1, 0, -1) almost disappears when the DNNs are in the power saturation region for the given training data; however, if the complexity of the DNNs is reduced by either reducing the number of units, feature maps, or hidden layers, the performance gap between them widens; in other words, a large network containing redundant imaging capabilities for the given training data may not harm the network by being more compact, but very precise."}, {"heading": "ACKNOWLEDGMENTS", "text": "This work was supported in part by the Brain Korea 21 Plus Project and the National Research Foundation of Korea (NRF), which is funded by the Ministry of Education, Science and Technology (MEST) of the Republic of Korea (No. 2012R1A2A2A06047297)."}], "references": [{"title": "Fixed point optimization of deep convolutional neural networks for object recognition", "author": ["Anwar", "Sajid", "Hwang", "Kyuyeon", "Sung", "Wonyong"], "venue": "In Acoustics, Speech and Signal Processing (ICASSP),", "citeRegEx": "Anwar et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Anwar et al\\.", "year": 2015}, {"title": "Deepdriving: Learning affordance for direct perception in autonomous driving", "author": ["Chen", "Chenyi", "Seff", "Ari", "Kornhauser", "Alain", "Xiao", "Jianxiong"], "venue": "arXiv preprint arXiv:1505.00256,", "citeRegEx": "Chen et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Chen et al\\.", "year": 2015}, {"title": "Robust control of robot arms via quasi sliding modes and neural networks", "author": ["Corradini", "Maria Letizia", "Giantomassi", "Andrea", "Ippoliti", "Gianluca", "Longhi", "Sauro", "Orlando", "Giuseppe"], "venue": "In Advances and Applications in Sliding Mode Control systems,", "citeRegEx": "Corradini et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Corradini et al\\.", "year": 2015}, {"title": "Binaryconnect: Training deep neural networks with binary weights during propagations", "author": ["Courbariaux", "Matthieu", "Bengio", "Yoshua", "David", "Jean-Pierre"], "venue": "arXiv preprint arXiv:1511.00363,", "citeRegEx": "Courbariaux et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Courbariaux et al\\.", "year": 2015}, {"title": "Weight discretization paradigm for optical neural networks", "author": ["Fiesler", "Emile", "Choudry", "Amar", "Caulfield", "H John"], "venue": "In The Hague\u201990,", "citeRegEx": "Fiesler et al\\.,? \\Q1990\\E", "shortCiteRegEx": "Fiesler et al\\.", "year": 1990}, {"title": "Deep compression: Compressing deep neural network with pruning, trained quantization and huffman coding", "author": ["Han", "Song", "Mao", "Huizi", "Dally", "William J"], "venue": null, "citeRegEx": "Han et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Han et al\\.", "year": 2015}, {"title": "Back propagation simulations using limited precision calculations", "author": ["Holt", "Jordan L", "Baker", "Thomas E"], "venue": "In Neural Networks,", "citeRegEx": "Holt et al\\.,? \\Q1991\\E", "shortCiteRegEx": "Holt et al\\.", "year": 1991}, {"title": "Short word-length lms filtering", "author": ["Hussain", "B Zahir M"], "venue": "In Signal Processing and Its Applications,", "citeRegEx": "Hussain and M,? \\Q2007\\E", "shortCiteRegEx": "Hussain and M", "year": 2007}, {"title": "Fixed-point feedforward deep neural network design using weights +1, 0, and -1", "author": ["Hwang", "Kyuyeon", "Sung", "Wonyong"], "venue": "In Signal Processing Systems (SiPS),", "citeRegEx": "Hwang et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Hwang et al\\.", "year": 2014}, {"title": "Human computer interface using hand gesture recognition based on neural network", "author": ["Jalab", "Hamid A", "Omer", "Herman"], "venue": "In Information Technology: Towards New Smart World (NSITNSW),", "citeRegEx": "Jalab et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Jalab et al\\.", "year": 2015}, {"title": "X1000 real-time phoneme recognition VLSI using feed-forward deep neural networks", "author": ["Kim", "Jonghong", "Hwang", "Kyuyeon", "Sung", "Wonyong"], "venue": "In Acoustics, Speech and Signal Processing (ICASSP),", "citeRegEx": "Kim et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Kim et al\\.", "year": 2014}, {"title": "Neural network adaptations to hardware implementations", "author": ["Moerland", "Perry", "Fiesler", "Emile"], "venue": "Technical report, IDIAP,", "citeRegEx": "Moerland et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Moerland et al\\.", "year": 1997}, {"title": "Accelerating deep convolutional neural networks using specialized hardware", "author": ["Ovtcharov", "Kalin", "Ruwase", "Olatunji", "Kim", "Joo-Young", "Fowers", "Jeremy", "Strauss", "Karin", "Chung", "Eric S"], "venue": "Microsoft Research Whitepaper,", "citeRegEx": "Ovtcharov et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Ovtcharov et al\\.", "year": 2015}, {"title": "Learning separable filters", "author": ["Rigamonti", "Roberto", "Sironi", "Amos", "Lepetit", "Vincent", "Fua", "Pascal"], "venue": "In Computer Vision and Pattern Recognition (CVPR),", "citeRegEx": "Rigamonti et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Rigamonti et al\\.", "year": 2013}, {"title": "Fast and accurate recurrent neural network acoustic models for speech recognition", "author": ["Sak", "Ha\u015fim", "Senior", "Andrew", "Rao", "Kanishka", "Beaufays", "Fran\u00e7oise"], "venue": "arXiv preprint arXiv:1507.06947,", "citeRegEx": "Sak et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Sak et al\\.", "year": 2015}, {"title": "Simulation-based word-length optimization method for fixed-point digital signal processing systems", "author": ["Sung", "Wonyong", "Kum", "Ki-II"], "venue": "Signal Processing, IEEE Transactions on,", "citeRegEx": "Sung et al\\.,? \\Q1995\\E", "shortCiteRegEx": "Sung et al\\.", "year": 1995}, {"title": "Lecture 6.5-rmsprop: Divide the gradient by a running average of its recent magnitude", "author": ["Tieleman", "Tijmen", "Hinton", "Geoffrey"], "venue": "COURSERA: Neural Networks for Machine Learning,", "citeRegEx": "Tieleman et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Tieleman et al\\.", "year": 2012}, {"title": "Restructuring of deep neural network acoustic models with singular value decomposition", "author": ["Xue", "Jian", "Li", "Jinyu", "Gong", "Yifan"], "venue": "In INTERSPEECH,", "citeRegEx": "Xue et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Xue et al\\.", "year": 2013}, {"title": "More data + deeper model = better accuracy", "author": ["Yu", "Dong", "Deng", "Alex Acero", "Dahl", "George", "Seide", "Frank", "Li", "Gang"], "venue": "In keynote at International Workshop on Statistical Machine Learning for Speech Processing,", "citeRegEx": "Yu et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Yu et al\\.", "year": 2012}, {"title": "Exploiting sparseness in deep neural networks for large vocabulary speech recognition", "author": ["Yu", "Dong", "Seide", "Frank", "Li", "Gang", "Deng"], "venue": "In Acoustics, Speech and Signal Processing (ICASSP),", "citeRegEx": "Yu et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Yu et al\\.", "year": 2012}], "referenceMentions": [{"referenceID": 9, "context": "Deep neural networks begin to find many real-time applications, such as speech recognition, autonomous driving, gesture recognition, and robotic control (Sak et al. (2015); Chen et al.", "startOffset": 154, "endOffset": 172}, {"referenceID": 0, "context": "(2015); Chen et al. (2015); Jalab et al.", "startOffset": 8, "endOffset": 27}, {"referenceID": 0, "context": "(2015); Chen et al. (2015); Jalab et al. (2015); Corradini et al.", "startOffset": 8, "endOffset": 48}, {"referenceID": 0, "context": "(2015); Chen et al. (2015); Jalab et al. (2015); Corradini et al. (2015)).", "startOffset": 8, "endOffset": 73}, {"referenceID": 0, "context": "(2015); Chen et al. (2015); Jalab et al. (2015); Corradini et al. (2015)). Although most of deep neural networks are implemented using GPUs (Graphics Processing Units) in these days, their implementation in hardware can give many benefits in terms of power consumption and system size (Ovtcharov et al. (2015)).", "startOffset": 8, "endOffset": 310}, {"referenceID": 0, "context": "(2015); Chen et al. (2015); Jalab et al. (2015); Corradini et al. (2015)). Although most of deep neural networks are implemented using GPUs (Graphics Processing Units) in these days, their implementation in hardware can give many benefits in terms of power consumption and system size (Ovtcharov et al. (2015)). FPGA based implementation examples of CNN show more than 10 times advantage in power consumption (Ovtcharov et al. (2015)).", "startOffset": 8, "endOffset": 434}, {"referenceID": 0, "context": "(2015); Chen et al. (2015); Jalab et al. (2015); Corradini et al. (2015)). Although most of deep neural networks are implemented using GPUs (Graphics Processing Units) in these days, their implementation in hardware can give many benefits in terms of power consumption and system size (Ovtcharov et al. (2015)). FPGA based implementation examples of CNN show more than 10 times advantage in power consumption (Ovtcharov et al. (2015)). Neural network algorithms employ many multiply and add (MAC) operations that mimic the operations of biological neurons, which suggests that reconfigurable hardware arrays that contain quite homogeneous hardware blocks, such as MAC units, can give very efficient solution to real-time neural network system design. Our previous works show that the precision required for implementing FFDNN or CNN needs not be very high, especially when the quantized networks are trained again to learn the effects of lowered precision. In the fixed-point optimization examples shown in Hwang & Sung (2014); Anwar et al.", "startOffset": 8, "endOffset": 1027}, {"referenceID": 0, "context": "In the fixed-point optimization examples shown in Hwang & Sung (2014); Anwar et al. (2015), neural networks with ternary weights showed quite good performance which is close to that of floating-point arithmetic, but not always.", "startOffset": 71, "endOffset": 91}, {"referenceID": 3, "context": "An implementation with ternary weights were reported for neural network design with optical fiber networks (Fiesler et al. (1990)).", "startOffset": 108, "endOffset": 130}, {"referenceID": 3, "context": "An implementation with ternary weights were reported for neural network design with optical fiber networks (Fiesler et al. (1990)). In this ternary network design, the authors employed retraining after direct quantization to improve the performance of a shallow network. Recently, fixed-point design of deep neural networks is revisited, and FFDNN and CNN with ternary weights show quite good performance that are very close to the floating-point results. The ternary weight based FFDNN and CNN are used for VLSI and FPGA based implementations, by which the algorithms can be operated with only on-chip memory consuming very low power (Kim et al. (2014)).", "startOffset": 108, "endOffset": 654}, {"referenceID": 3, "context": "Binary weight based deep neural network design is also studied (Courbariaux et al. (2015)).", "startOffset": 64, "endOffset": 90}, {"referenceID": 3, "context": "Binary weight based deep neural network design is also studied (Courbariaux et al. (2015)). Pruned floating-point weights are also utilized for efficient GPU based implementations, where small valued weights are forced to zero to reduce the number of arithmetic operations and the memory space for weight storage (Yu et al. (2012b); Han et al.", "startOffset": 64, "endOffset": 332}, {"referenceID": 3, "context": "Binary weight based deep neural network design is also studied (Courbariaux et al. (2015)). Pruned floating-point weights are also utilized for efficient GPU based implementations, where small valued weights are forced to zero to reduce the number of arithmetic operations and the memory space for weight storage (Yu et al. (2012b); Han et al. (2015)).", "startOffset": 64, "endOffset": 351}, {"referenceID": 3, "context": "Binary weight based deep neural network design is also studied (Courbariaux et al. (2015)). Pruned floating-point weights are also utilized for efficient GPU based implementations, where small valued weights are forced to zero to reduce the number of arithmetic operations and the memory space for weight storage (Yu et al. (2012b); Han et al. (2015)). A network restructuring technique using singular value decomposition technique is also studied (Xue et al. (2013); Rigamonti et al.", "startOffset": 64, "endOffset": 467}, {"referenceID": 3, "context": "Binary weight based deep neural network design is also studied (Courbariaux et al. (2015)). Pruned floating-point weights are also utilized for efficient GPU based implementations, where small valued weights are forced to zero to reduce the number of arithmetic operations and the memory space for weight storage (Yu et al. (2012b); Han et al. (2015)). A network restructuring technique using singular value decomposition technique is also studied (Xue et al. (2013); Rigamonti et al. (2013)).", "startOffset": 64, "endOffset": 492}, {"referenceID": 18, "context": "It is well known that increasing the depth usually results in positive effects on the performance of a DNN (Yu et al. (2012a)).", "startOffset": 108, "endOffset": 126}], "year": 2017, "abstractText": "The complexity of deep neural network algorithms for hardware implementation can be much lowered by optimizing the word-length of weights and signals. Direct quantization of floating-point weights, however, does not show good performance when the number of bits assigned is small. Retraining of quantized networks has been developed to relieve this problem. In this work, the effects of retraining are analyzed for a feedforward deep neural network (FFDNN) and a convolutional neural network (CNN). The network complexity is controlled to know their effects on the resiliency of quantized networks by retraining. The complexity of the FFDNN is controlled by varying the unit size in each hidden layer and the number of layers, while that of the CNN is done by modifying the feature map configuration. We find that the performance gap between the floating-point and the retrain-based ternary (+1, 0, -1) weight neural networks exists with a fair amount in complexity limited\u2019 networks, but the discrepancy almost vanishes in fully complex networks whose capability is limited by the training data, rather than by the number of connections. This research shows that highly complex DNNs have the capability of absorbing the effects of severe weight quantization through retraining, but connection limited networks are less resilient.", "creator": "LaTeX with hyperref package"}}}