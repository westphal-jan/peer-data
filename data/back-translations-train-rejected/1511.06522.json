{"id": "1511.06522", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "20-Nov-2015", "title": "Integrating Deep Features for Material Recognition", "abstract": "We propose a method for integration of features extracted using deep representations of Convolutional Neural Networks (CNNs) each of which is learned using a different image dataset of objects and materials for material recognition. Given a set of representations of multiple pre-trained CNNs, we first compute activations of features using the representations on the images to select a set of samples which are best represented by the features. Then, we measure the uncertainty of the features by computing the entropy of class distributions for each sample set. Finally, we compute the contribution of each feature to representation of classes for feature selection and integration. We examine the proposed method on three benchmark datasets for material recognition. Experimental results show that the proposed method achieves state-of-the-art performance by integrating deep features. Additionally, we introduce a new material dataset called EFMD by extending Flickr Material Database (FMD). By the employment of the EFMD with transfer learning for updating the learned CNN models, we achieve 84.0%+/-1.8% accuracy on the FMD dataset which is close to human performance that is 84.9%.", "histories": [["v1", "Fri, 20 Nov 2015 08:31:00 GMT  (905kb,D)", "https://arxiv.org/abs/1511.06522v1", "9 pages, 3 figures, Under review as a conference paper at ICLR 2016"], ["v2", "Sat, 28 Nov 2015 14:21:28 GMT  (905kb,D)", "http://arxiv.org/abs/1511.06522v2", "9 pages, 3 figures, Under review as a conference paper at ICLR 2016"], ["v3", "Sun, 13 Dec 2015 13:39:24 GMT  (905kb,D)", "http://arxiv.org/abs/1511.06522v3", "9 pages, 3 figures, Under review as a conference paper at ICLR 2016"], ["v4", "Mon, 22 Feb 2016 14:36:36 GMT  (905kb,D)", "http://arxiv.org/abs/1511.06522v4", "9 pages, 3 figures"], ["v5", "Tue, 5 Apr 2016 09:18:49 GMT  (905kb,D)", "http://arxiv.org/abs/1511.06522v5", "9 pages, 3 figures"], ["v6", "Thu, 21 Apr 2016 10:19:56 GMT  (2998kb,D)", "http://arxiv.org/abs/1511.06522v6", "6 pages"]], "COMMENTS": "9 pages, 3 figures, Under review as a conference paper at ICLR 2016", "reviews": [], "SUBJECTS": "cs.CV cs.LG", "authors": ["yan zhang", "mete ozay", "xing liu", "takayuki okatani"], "accepted": false, "id": "1511.06522"}, "pdf": {"name": "1511.06522.pdf", "metadata": {"source": "CRF", "title": "Integrating Deep Features for Material Recognition", "authors": ["Yan Zhang", "Mete Ozay", "Xing Liu", "Takayuki Okatani"], "emails": ["okatani}@vision.is.tohoku.ac.jp"], "sections": [{"heading": null, "text": "In this work, we look at the problem of material recognition, which consists in identifying material categories such as glass or the fabric of an object (i.e., the material from which the object is made) from its single RGB image. We are particularly interested in the link between material recognition and object recognition, aiming to develop material recognition features that can be efficiently transferred to different tasks / data sets. To this end, we use conventional neural networks (CNNs)."}, {"heading": "II. RELATED WORK", "text": "Surface properties of materials and the relationship between perceptions of material and object categories are analyzed in [3], [6], initially proposing a well-designed benchmark dataset called FMD. Subsequently, they designed descriptors to extract handcrafted features to represent various surface properties such as color, texture and shape for material recognition in [6]. In addition, they analyzed the relationship between object and material recognition for accurate and fast perception of materials in [3].In [7], a filter bank-based method was developed that uses CNNs for texture detection. The authors achieved a state-of-the-art performance on several benchmark datasets for texture detection and material detection, including 82.4% accuracy on the FMD datasets. In [8], a method was proposed to filter local material properties from crowd-sourced material distances to discover that shapes can be performed efficiently without, for example, relying on object numbers."}, {"heading": "III. FEATURE SELECTION AND INTEGRATION FOR DEEP REPRESENTATIONS", "text": "In this section, we propose a method to analyze and integrate several characteristics obtained from different CNNs trained for different tasks."}, {"heading": "A. Outline of the method", "text": "Specifically, we first extract features from material images by using multiple CNN models trained on different data sets / tasks. By a feature, we mean an activation value of a unit (neuron) at a selected level of CNN. If possible and necessary, the CNN models can be fine-tuned to the training samples of the target task of material recognition. Similar to the increase in [19], we then consider each individual feature as a weak classifier, although we do not explicitly train a weak classifier for each feature. Instead, we use class entropy as criteria to measure how discriminatory each feature is. As proposed in [20], [21], the images that maximize a feature are the most representative examples of the use of the feature. Therefore, we calculate the class entropy for each feature using an image set that maximizes the activation value of the feature, and then use a weighted sum of the class entropy over the image set."}, {"heading": "B. Details of the algorithm", "text": "Suppose that we create a new Dataset Dn = (I), Y (i)), Mni = 1, n = 1, 2,., N where I (i), Xn (i), Yn (i), Yn (i), Yi (i), Yi (i), Yi (i), Yi (i), Yi (i), Yi (i), Mni (i), Yi (i), Yi (i), Yi (i), Yi (i), Yi (i), Yi (i), Yi (i), Yi (i), Yi (i), Yi (i), Yi (i), Yi (i), Yi (i), Yi (i), Yi (i), Yi (i), Yi (i), Yi (i), Yi (i), Yi (i), Yi (i)."}, {"heading": "IV. EXPERIMENTAL ANALYSIS", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "A. Datasets", "text": "In the experiments, we used three benchmark datasets (see Table I). The MKS dataset [5] consists of 10 material categories, each of which contains 100 images. In the MKS dataset, the samples were manually selected by Flickr to cover different lighting conditions, compositions, colors, and textures. MINC is a large-area material recognition dataset [22]. It contains 3 million images from 23 categories. In addition, we are introducing a new dataset called EFMD, which is an extended version of MKS. EFMD consists of the same categories used in FMD, each of which contains 1,000 images. Therefore, the size of EFMD is ten times larger than the size of FMD. As we construct EFMD, we are trying to design it as similar as possible so that these images are similar in the context of visual perception and recognition."}, {"heading": "B. Details of experimental setups", "text": "In our experiments, we consider four different tasks, which we will call FMD, FMD-2, EFMD and MINC (val / test). In each of these tasks, we construct a material representation \u03a6n and an object representation \u0441n in different ways, which is described below. In all tasks, we start with two CNNs prepared for MINC and ILSVRC2012 [23], for which we use publicly available, pre-trained VGG-D Caffe [24] models consisting of 16 layers [2]. We then tune these CNNs to the tasks FMD, FMD-2 and EFMD using different samples, as shown in Table II. Fine-tuning is performed in such a way that the weights of the Conv1-Conv4 layers are fixed and those of the higher layers are processed on the basis of CASC. We have determined that this selection of firmer / updating layers is the best for FMD and EIND layers, which we have matched to the FIND layers."}, {"heading": "C. Performance analysis", "text": "iiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiii"}, {"heading": "D. Robustness analysis", "text": "We analyze how the number of selected top K samples and the number (T) of integrated characteristics affect the classification performance on the FMD dataset; see Fig. 2a and Fig. 2b. Fig. 2a sets the number of integrated characteristics at 3000. Note that only samples with positive activation are considered to be the most selected samples. For example, if K is selected to cover 100% of the samples, the ratio of characteristics selected by the algorithm may be less than 100%. This can be observed when most of the activation values of FMD are 0. To analyze the effect of T on performance, K is set to 10%. Although selecting T = 3000 characteristics provides the best performance, less number of integrated characteristics, such as 100-400, provides comparable accuracy. We also analyze the relationship between characteristics extracted using the representation of materials and objects that are FMD finalized."}, {"heading": "V. CONCLUSION", "text": "In this thesis, we propose a method for integrating deep characteristics obtained from multiple CNNs trained on images of materials and objects for material recognition. To this end, we first use a method for selecting and integrating deep characteristics by measuring their contribution to the representation of material categories. Then, the integrated characteristics are used for material recognition using classifiers. In the experimental results, we obtain state-of-the-art performance by using the features integrated with the proposed method on multiple benchmark data sets. In future work, we plan to examine the theoretical properties of the proposed methods for integrating deep representations using various deep learning algorithms such as auto-encoders to perform other tasks such as scene analysis, image classification and detection."}], "references": [{"title": "Imagenet classification with deep convolutional neural networks", "author": ["A. Krizhevsky", "I. Sutskever", "G.E. Hinton"], "venue": "Advances in Neural Information Processing Systems 25, 2012, pp. 1097\u20131105.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2012}, {"title": "Very deep convolutional networks for large-scale image recognition", "author": ["K. Simonyan", "A. Zisserman"], "venue": "CoRR, vol. abs/1409.1556, 2014.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2014}, {"title": "Accuracy and speed of material categorization in real-world images", "author": ["L. Sharan", "R. Rosenholtz", "E.H. Adelson"], "venue": "Journal of Vision, vol. 14, no. 10, 2014.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2014}, {"title": "Temporal properties of material categorization and material rating: visual vs non-visual material features", "author": ["T. Nagai", "T. Matsushima", "K. Koida", "Y. Tani", "M. Kitazaki", "S. Nakauchi"], "venue": "Vision Research, vol. 115, Part B, pp. 259 \u2013 270, 2015.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2015}, {"title": "Material perception: What can you see in a brief glance?", "author": ["L. Sharan", "R. Rosenholtz", "E.H. Adelson"], "venue": "Journal of Vision,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2014}, {"title": "Recognizing materials using perceptually inspired features", "author": ["L. Sharan", "C. Liu", "R. Rosenholtz", "E.H. Adelson"], "venue": "International Journal of Computer Vision, vol. 108, no. 3, pp. 348\u2013371, 2013.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2013}, {"title": "Deep filter banks for texture recognition and segmentation", "author": ["M. Cimpoi", "S. Maji", "A. Vedaldi"], "venue": "CVPR, June 2015, pp. 3828\u20133836.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2015}, {"title": "Automatically discovering local visual material attributes", "author": ["G. Schwartz", "K. Nishino"], "venue": "CVPR, June 2015.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2015}, {"title": "Wrappers for feature subset selection", "author": ["R. Kohavi", "G.H. John"], "venue": "Artificial Intelligence, vol. 97, no. 12, pp. 273 \u2013 324, 1997.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 1997}, {"title": "Irrelevant features and the subset selection problem", "author": ["G.H. John", "R. Kohavi", "K. Pfleger"], "venue": "Proc. 11th International Conference on Machine Learning. Morgan Kaufmann, 1994, pp. 121\u2013129.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 1994}, {"title": "Overfitting in making comparisons between variable selection methods", "author": ["J. Reunanen"], "venue": "J. Mach. Learn. Res., vol. 3, pp. 1371\u20131382, Mar. 2003.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2003}, {"title": "Selection of relevant features and examples in machine learning", "author": ["A.L. Blum", "P. Langley"], "venue": "Artificial Intelligence, vol. 97, no. 12, pp. 245 \u2013 271, 1997.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 1997}, {"title": "Use of the zero norm with linear models and kernel methods", "author": ["J. Weston", "A. Elisseeff", "B. Sch\u00f6lkopf", "M. Tipping"], "venue": "J. Mach. Learn. Res., vol. 3, pp. 1439\u20131461, Mar. 2003.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2003}, {"title": "An introduction to variable and feature selection", "author": ["I. Guyon", "A. Elisseeff"], "venue": "J. Mach. Learn. Res., vol. 3, pp. 1157\u20131182, Mar. 2003.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2003}, {"title": "Feature selection and feature extraction for text categorization", "author": ["Lewis", "D. David"], "venue": "Proceedings of the Workshop on Speech and Natural Language. Association for Computational Linguistics, 1992, pp. 212\u2013 217.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 1992}, {"title": "Feature selection based on mutual information criteria of max-dependency, max-relevance, and minredundancy", "author": ["H. Peng", "F. Long", "C. Ding"], "venue": "Pattern Analysis and Machine Intelligence, IEEE Transactions on, vol. 27, no. 8, pp. 1226\u20131238, Aug 2005.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2005}, {"title": "Using mutual information for selecting features in supervised neural net learning", "author": ["R. Battiti"], "venue": "IEEE Transactions on Neural Networks, vol. 5, no. 4, pp. 537\u2013550, Jul 1994.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 1994}, {"title": "Input feature selection for classification problems", "author": ["N. Kwak", "C.-H. Choi"], "venue": "IEEE Transactions on Neural Networks, vol. 13, no. 1, pp. 143\u2013159, Jan 2002.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2002}, {"title": "Rapid object detection using a boosted cascade of simple features", "author": ["P. Viola", "M. Jones"], "venue": "CVPR, vol. 1, 2001, pp. 511\u2013518.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2001}, {"title": "Rich feature hierarchies for accurate object detection and semantic segmentation", "author": ["R. Girshick", "J. Donahue", "T. Darrell", "J. Malik"], "venue": "Computer Vision and Pattern Recognition (CVPR), 2014 IEEE Conference on, June 2014, pp. 580\u2013587.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2014}, {"title": "Visualizing and understanding convolutional networks", "author": ["M. Zeiler", "R. Fergus"], "venue": "ECCV 2014, 2014, vol. 8689, pp. 818\u2013833.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2014}, {"title": "Material recognition in the wild with the materials in context database", "author": ["S. Bell", "P. Upchurch", "N. Snavely", "K. Bala"], "venue": "CVPR, June 2015, pp. 3479\u20133487.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2015}, {"title": "ImageNet Large Scale Visual Recognition Challenge", "author": ["O. Russakovsky", "J. Deng", "H. Su", "J. Krause", "S. Satheesh", "S. Ma", "Z. Huang", "A. Karpathy", "A. Khosla", "M. Bernstein", "A.C. Berg", "L. Fei-Fei"], "venue": "International Journal of Computer Vision (IJCV), pp. 1\u201342, April 2015.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2015}, {"title": "Caffe: Convolutional architecture for fast feature embedding", "author": ["Y. Jia", "E. Shelhamer", "J. Donahue", "S. Karayev", "J. Long", "R. Girshick", "S. Guadarrama", "T. Darrell"], "venue": "arXiv preprint arXiv:1408.5093, 2014.", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2014}, {"title": "Support-vector networks", "author": ["C. Cortes", "V. Vapnik"], "venue": "Machine Learning, vol. 20, no. 3, pp. 273\u2013297, 1995.", "citeRegEx": "25", "shortCiteRegEx": null, "year": 1995}, {"title": "Deep filter banks for texture recognition, description, and segmentation", "author": ["M. Cimpoi", "S. Maji", "I. Kokkinos", "A. Vedaldi"], "venue": "CoRR, vol. abs/1507.02620, 2015.", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2015}, {"title": "Measures of diversity in classifier ensembles and their relationship with the ensemble accuracy", "author": ["L.I. Kuncheva", "C.J. Whitaker"], "venue": "Mach. Learn., vol. 51, no. 2, pp. 181\u2013207, May 2003.", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2003}], "referenceMentions": [{"referenceID": 0, "context": "Toward this end, using convolutional neural networks (CNNs) [1], [2], we study how to select and integrate multiple features obtained by different models of CNNs trained on different tasks/datasets.", "startOffset": 60, "endOffset": 63}, {"referenceID": 1, "context": "Toward this end, using convolutional neural networks (CNNs) [1], [2], we study how to select and integrate multiple features obtained by different models of CNNs trained on different tasks/datasets.", "startOffset": 65, "endOffset": 68}, {"referenceID": 2, "context": "A few studies of psychophysics [3], [4] imply that material perception in human vision is interconnected with perception of object category.", "startOffset": 31, "endOffset": 34}, {"referenceID": 3, "context": "A few studies of psychophysics [3], [4] imply that material perception in human vision is interconnected with perception of object category.", "startOffset": 36, "endOffset": 39}, {"referenceID": 4, "context": "\u2022 We introduce an extended version of the benchmark material dataset (namely, FMD [5]), called EFMD.", "startOffset": 82, "endOffset": 85}, {"referenceID": 5, "context": "9%) [6].", "startOffset": 4, "endOffset": 7}, {"referenceID": 2, "context": "Surface properties of materials, and the relationship between perception of material and object categories are analyzed in [3], [6].", "startOffset": 123, "endOffset": 126}, {"referenceID": 5, "context": "Surface properties of materials, and the relationship between perception of material and object categories are analyzed in [3], [6].", "startOffset": 128, "endOffset": 131}, {"referenceID": 5, "context": "Then, they designed descriptors to extract hand-crafted features for representation of various surface properties such as color, texture and shape for material recognition in [6].", "startOffset": 175, "endOffset": 178}, {"referenceID": 2, "context": "Moreover, they analyzed the relationship between object and material recognition for accurate and fast perception of materials in [3].", "startOffset": 130, "endOffset": 133}, {"referenceID": 6, "context": "In [7], a filter bank based method was developed using CNNs for texture recognition.", "startOffset": 3, "endOffset": 6}, {"referenceID": 7, "context": "In [8], a method was proposed to discover local material attributes from crowdsourced perceptual material distances.", "startOffset": 3, "endOffset": 6}, {"referenceID": 8, "context": "Commonly used feature selection methods can be categorized into wrapper methods [9]\u2013[11] and filter methods [12]\u2013 [18].", "startOffset": 80, "endOffset": 83}, {"referenceID": 10, "context": "Commonly used feature selection methods can be categorized into wrapper methods [9]\u2013[11] and filter methods [12]\u2013 [18].", "startOffset": 84, "endOffset": 88}, {"referenceID": 11, "context": "Commonly used feature selection methods can be categorized into wrapper methods [9]\u2013[11] and filter methods [12]\u2013 [18].", "startOffset": 108, "endOffset": 112}, {"referenceID": 17, "context": "Commonly used feature selection methods can be categorized into wrapper methods [9]\u2013[11] and filter methods [12]\u2013 [18].", "startOffset": 114, "endOffset": 118}, {"referenceID": 12, "context": "[13] to measure importance of each individual feature.", "startOffset": 0, "endOffset": 4}, {"referenceID": 15, "context": "Mutual information is also a popular criterion used for ranking features [16]\u2013[18].", "startOffset": 73, "endOffset": 77}, {"referenceID": 17, "context": "Mutual information is also a popular criterion used for ranking features [16]\u2013[18].", "startOffset": 78, "endOffset": 82}, {"referenceID": 15, "context": "A max-relevance and min-redundancy criterion was proposed in [16].", "startOffset": 61, "endOffset": 65}, {"referenceID": 18, "context": "In [19], a feature selection method was proposed based on a variant of Adaboost.", "startOffset": 3, "endOffset": 7}, {"referenceID": 18, "context": "Similarly to boosting in [19], we then consider each individual feature as a weak classifier, although we do not explicitly train a weak classifier for each feature.", "startOffset": 25, "endOffset": 29}, {"referenceID": 19, "context": "As suggested in [20], [21], the images maximizing a feature are the most representative samples for utilization of the feature.", "startOffset": 16, "endOffset": 20}, {"referenceID": 20, "context": "As suggested in [20], [21], the images maximizing a feature are the most representative samples for utilization of the feature.", "startOffset": 22, "endOffset": 26}, {"referenceID": 1, "context": "1In our experiments, we used VGG-D-16 [2] and the fc7 layer.", "startOffset": 38, "endOffset": 41}, {"referenceID": 18, "context": "For this purpose, we consider each individual feature as a weak classifier as suggested in [19].", "startOffset": 91, "endOffset": 95}, {"referenceID": 0, "context": "In CNNs implemented using ReLU activation functions ( [1], [2]), the features that are represented in an individual neuron are visualized by analyzing and selecting the images on which the activation value of the neuron is maximized [20], [21].", "startOffset": 54, "endOffset": 57}, {"referenceID": 1, "context": "In CNNs implemented using ReLU activation functions ( [1], [2]), the features that are represented in an individual neuron are visualized by analyzing and selecting the images on which the activation value of the neuron is maximized [20], [21].", "startOffset": 59, "endOffset": 62}, {"referenceID": 19, "context": "In CNNs implemented using ReLU activation functions ( [1], [2]), the features that are represented in an individual neuron are visualized by analyzing and selecting the images on which the activation value of the neuron is maximized [20], [21].", "startOffset": 233, "endOffset": 237}, {"referenceID": 20, "context": "In CNNs implemented using ReLU activation functions ( [1], [2]), the features that are represented in an individual neuron are visualized by analyzing and selecting the images on which the activation value of the neuron is maximized [20], [21].", "startOffset": 239, "endOffset": 243}, {"referenceID": 4, "context": "FMD dataset [5] consists of 10 material categories each of which contains 100 images.", "startOffset": 12, "endOffset": 15}, {"referenceID": 21, "context": "MINC is a large scale material recognition dataset [22].", "startOffset": 51, "endOffset": 55}, {"referenceID": 22, "context": "In all the tasks, we start with two CNNs pre-trained on MINC and ILSVRC2012 [23], for which we use publicly available pre-trained Caffe [24] models of VGG-D consisting of 16 layers [2].", "startOffset": 76, "endOffset": 80}, {"referenceID": 23, "context": "In all the tasks, we start with two CNNs pre-trained on MINC and ILSVRC2012 [23], for which we use publicly available pre-trained Caffe [24] models of VGG-D consisting of 16 layers [2].", "startOffset": 136, "endOffset": 140}, {"referenceID": 1, "context": "In all the tasks, we start with two CNNs pre-trained on MINC and ILSVRC2012 [23], for which we use publicly available pre-trained Caffe [24] models of VGG-D consisting of 16 layers [2].", "startOffset": 181, "endOffset": 184}, {"referenceID": 21, "context": "45 [22] 68.", "startOffset": 3, "endOffset": 7}, {"referenceID": 21, "context": "19 [22] 68.", "startOffset": 3, "endOffset": 7}, {"referenceID": 24, "context": "The integrated features are fed into a support vector machines (SVM) with radial basis function (RBF) kernels [25] for training and testing of classifiers.", "startOffset": 110, "endOffset": 114}, {"referenceID": 25, "context": "A method proposed in [26] achieves 82.", "startOffset": 21, "endOffset": 25}, {"referenceID": 5, "context": "9%) [6].", "startOffset": 4, "endOffset": 7}, {"referenceID": 21, "context": "60% on MINC val/test also outperform the previous ones [22].", "startOffset": 55, "endOffset": 59}, {"referenceID": 22, "context": "Note that their CNN models are pre-trained on ILSVRC2012 [23].", "startOffset": 57, "endOffset": 61}, {"referenceID": 26, "context": "For this purpose, we employed five statistical measures [27], [28], namely i) inter-rater agreement (\u03ba) to measure the level of agreement of classifiers while correcting for chance, ii) Q statistics to measure statistical dependency of classifiers, iii) Kohavi-Wolpert variance to measure variance of agreement, iv) measurement of disagreement, v) generalized diversity to measure causal statistical diversity of classifiers.", "startOffset": 56, "endOffset": 60}], "year": 2016, "abstractText": "This paper considers the problem of material recognition. Motivated by an observation that there is close interconnection between material recognition and object recognition, we study how to select and integrate multiple features obtained by different models of Convolutional Neural Networks (CNNs) trained in a transfer learning setting. To be specific, we first compute activations of features using representations on images to select a set of samples which are best represented by the features. Then, we measure uncertainty of the features by computing entropy of class distributions for each sample set. Finally, we compute contribution of each feature to representation of classes for feature selection and integration. Experimental results show that the proposed method achieves state-of-the-art performance on two benchmark datasets for material recognition. Additionally, we introduce a new material dataset, named EFMD, which extends Flickr Material Database (FMD). By the employment of the EFMD for transfer learning, we achieve 84.0%\u00b1 1.8% accuracy on the FMD dataset, which is close to reported human performance 84.9%.", "creator": "LaTeX with hyperref package"}}}