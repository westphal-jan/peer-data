{"id": "1511.06653", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "20-Nov-2015", "title": "Recurrent Encoder Multi-Decoder Networks, Semi-supervised Classification and Constrained Adversarial Generation", "abstract": "Recent work on sequence to sequence translation using Recurrent Neural Networks (RNNs) based on Long Short Term Memory (LSTM) architectures has shown great potential for learning useful representations of sequential data. These architectures, using one recurrent neural network to encode sequences into fixed-length representations, and one or more network(s) to decode representations into new sequences have the advantages of being modular, while also allowing modules to be jointly trained. A one-to-many encoder-decoder(s) scheme allows for a single encoder to provide representations serving multiple purposes. In our case, we present an LSTM encoder network able to produce representations used by two decoders: one that reconstructs, and one that classifies if the training sequence has a labelling. This allows the network to learn representations that are useful for both discriminative and generative tasks at the same time. We show how this paradigm is very well suited for semi-supervised learning with sequences. We test our proposed approach on an action recognition task using motion capture (MOCAP) sequences and show that semi-supervised feature learning can improve movement classification.", "histories": [["v1", "Fri, 20 Nov 2015 15:47:55 GMT  (257kb,D)", "http://arxiv.org/abs/1511.06653v1", "Submitted for review for ICLR 2016"], ["v2", "Mon, 11 Apr 2016 01:03:26 GMT  (842kb,D)", "http://arxiv.org/abs/1511.06653v2", null], ["v3", "Mon, 25 Jul 2016 18:13:00 GMT  (1720kb,D)", "http://arxiv.org/abs/1511.06653v3", null], ["v4", "Fri, 26 May 2017 18:31:48 GMT  (2545kb,D)", "http://arxiv.org/abs/1511.06653v4", null], ["v5", "Mon, 5 Jun 2017 13:54:26 GMT  (2545kb,D)", "http://arxiv.org/abs/1511.06653v5", null], ["v6", "Tue, 6 Jun 2017 12:54:48 GMT  (2545kb,D)", "http://arxiv.org/abs/1511.06653v6", null]], "COMMENTS": "Submitted for review for ICLR 2016", "reviews": [], "SUBJECTS": "cs.CV cs.LG", "authors": ["f\\'elix g harvey", "christopher pal"], "accepted": false, "id": "1511.06653"}, "pdf": {"name": "1511.06653.pdf", "metadata": {"source": "CRF", "title": "SEMI-SUPERVISED LEARNING WITH ENCODER- DECODER RECURRENT NEURAL NETWORKS: EX-", "authors": ["PERIMENTS WITH", "MOTION CAPTURE SEQUENCES", "F\u00e9lix G. Harvey"], "emails": ["felix.gingras-harvey@polymtl.ca", "christopher.pal@polymtl.ca"], "sections": [{"heading": "1 INTRODUCTION", "text": "This year it is more than ever before."}, {"heading": "2 MOCAP", "text": "Motion capture (MOCAP) technologies make it possible to track and store the movements of an actor in a special suit with multiple markers, and the recorded positions of these markers at each step make it possible to apply these movements to virtual characters to simulate realistic movements. These technologies are used in various areas, such as video games, movies and health. As MOCAP is more widely used in these applications, searching through databases of sequences becomes desirable. While sequences could be labeled or annotated to facilitate searching, in practice, sequences are often not labeled or labeled on a rough semantic level. A key element for labeling MOCAP sequences is the recognition of human actions. This challenge can be seen as sequence classification when only one action is performed in the sequence, or as sequence translation when we want a more comprehensive description of the action or when multiple actions are performed."}, {"heading": "2.1 DATASETS", "text": "One of the biggest challenges in applying deep learning to MOCAP data is the lack of strongly labeled data. For this work, we used the two largest publicly available MOCAP datasets we know of: The first is the public dataset HDM05 (M\u00fcller et al., 2007), which contains over 270 weakly labeled takes or sequences of multiple actions for which the frames-to-action alignment is not specified; this dataset also contains 2329 strongly labeled sections, e.g. sequences of individual actions, so that the alignment is known, since all frames represent this single action; these sections are taken from full recordings; there are about 100 classes of movements that can be reduced to 65 if the number of repetitions or the side of the limbs that begin the action (left, right) are ignored; for this dataset, we only use these strongly labeled sections; the second dataset we use is the CMU Graphics MoeCapture 1."}, {"heading": "2.2 PREVIOUS WORK ON ACTION RECOGNITION", "text": "Cho & Chen (2013) tested several types of features using a fast technique they call Extreme Machine Learning to re-classify HDM05 sections, and the results were really good in both cases, with accuracies in excess of 95% and 92%, respectively, with 65 and 40 action classes, respectively. Their models were frame-trained, and sequence classification was done by majority voting. Other work by Du et al. (2015) addressed the simple sequences \"classification problem using the same action classes as Cho & Chen (2013) with hierarchical network handling in their first layers of the body (e.g., using arms and some of these layers in the classification of the body)."}, {"heading": "2.3 DEFINING A GOOD TEST SET", "text": "These techniques, based on their results, all seem to almost solve the problem of action detection in the HDM05 dataset. Nevertheless, there seems to be an underlying problem for these results (except for (Chaudhry et al., 2013), which is based on the definition of the test sets. All these experiments perform a 10-fold cross-validation with 10 balanced partitions of mixed sequences, which means that recordings or frames recorded with a particular actor can be found in both the training set and the test set. This might not be representative of reality when new settings are recorded with new actors. If an actor is asked to repeat the same motion five times in five different settings, then these will likely be very similar, and the frames or sequences will insert an unwanted bias into the test set. Chaudhry et al. (2013), on the other hand, isolate the set of tests based on the actors performing their actions on the HDM05 dataset."}, {"heading": "3 RECURRENT NEURAL NETWORKS", "text": "We will summarise here some key elements that form the basis of our approach."}, {"heading": "3.1 RECURRENT NEURAL NETWORKS", "text": "At its core, RNNs are artificial neural networks in which hidden layer units have connections to themselves over time. This means that hidden layers receive at each step the current outputs of the lower layers as well as their own outputs from the previous timeframe, allowing the network to use past context information to provide better outputs, which makes it very efficient when it comes to timing. Therefore, RNNs have often proved very powerful over the years for several successive problems, such as speech recognition (Graves et al., 2013b; Sak et al., 2014; Graves et al., 2013a), handwriting recognition (Graves et al., 2008), text generation (Sutskever et al., 2011; Graves, 2012) or in our case, MOCAP action recognition (Du et al., 2015). The forward process of an RNN is similar to that of an ordinary neural network sequence of the MOCAL, where it is a very plugged-in MOCAL."}, {"heading": "3.2 BI-DIRECTIONAL RECURRENT NETWORKS", "text": "In these cases, the use of bidirectional RNNs (BRNNs) can make the network more efficient. BRNNs contain layers with two units. One set treats the sequence in chronological order, while the other processes it in reverse order. Outputting such a layer is concatenating the hidden activations of both layers. Using such networks doubles the number of hidden units as well as the number of input dimensions for the output layer and all hidden layers, except the first. Outputting bidirectional recursive layers contains information about the entire sequence at each time step. Figure 1 shows an unfolded bidirectional layer."}, {"heading": "3.3 LONG-SHORT-TERM MEMORY", "text": "One of the most popular and effective ways to address this problem is to use Long-Short-Term Memory Networks (LSTMs), as presented by Hochreiter & Schmidhuber (1997). Instead of simple hidden units, these recurring networks have memory cells with input, output, and forget-not-gates that determine whether information is added, shared, and held in the cell at each time step. This allows the recurring network to store past context information internally for a long time, enabling it to model long-term time dependencies. A memory cell is shown in Figure 2. In our context, we do not use in-cell connections (also called peepholes) because they have not proven useful in recent experiments \u2212 bxcxt \u2212 bxcxt, 2015)."}, {"heading": "3.4 RECURRENT ENCODER-DECODERS", "text": "A major advantage and key attribute of recurrent encoder decoders is their ability to transform variable-length sequences into a fixed-size vector in the encoder and then decode that vector with one or more decoders for different purposes. Using an RNN as an encoder enables this representation of the entire input sequence to be obtained. Cho et al. (2014) and Sutskever et al. (2014) have used this approach for sequence-to-sequence translation, although there are some differences in the choice of hidden units and in the use of an additional summary vector (and weight set) in the case of Cho et al. (2014). Both approaches require an end-to-sequence symbol so that input and target sequences have different lengths. They are trained to maximize the conditional probability of the input sequence against the input sequence."}, {"heading": "4 OUR MODEL", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.1 THE ARCHITECTURE", "text": "Figure 3 shows an overview of the proposed LSTM-AEC architecture. The model consists of three main components: an encoder and two decoders, one of which is generative or, more accurately, reconstructive, while the other is discriminatory and classifies. The encoder and the reconstruction decoder are two separate LSTM networks with the same number of hidden units. This is necessary because the initial hidden state of the LSTM decoder is initialized with the last hidden state of the encoder. The classifier is a 1-hidden layer perceptron with a Softmax output layer. It uses the representation provided by the encoder at the time T as input. Some aspects of the architecture can be easily changed depending on the needs. For example, as explained by Srivastava et al. (2015), a recursive decoder is a recursive decoder in each timer, in addition to its last hidden state, referred to as a previous output."}, {"heading": "4.2 LOSS FUNCTION", "text": "The multi-purpose encoder decoder architecture described above naturally results in well-defined reconstructive and discriminatory loss functions for the associated decoders. For the loss of the classifier, we want to minimize the negative log probability (NNL) to maximize the conditional probability of a label y at a sequence X = x1,..., xT in the set of N sequences, and given the set of all network parameters. It is calculated using the mean over frames of the sum of square distances between the position of each marker and its reconstructed position: RE = 1 / T T T; t = 1 (xm, x-m, t); t (8 x functions), in which we can only reconstruct one square network and one reconstructed position at a time: RE = 1 / T T T T; t-x-m; t (8 x functions), in which we only represent one classifying network at a time."}, {"heading": "4.3 EMPIRICAL STUDY OF ARCHITECTURAL VARIANTS", "text": "Before conducting our key experiments, we examined the effects of some hyperparameters and variants of the architecture with a small network, on HDM05 data only to test the best possible configurations on a larger scale. Figure 2 summarizes these tests, which were all performed once, with the same weight initializations. Our baseline (BL) for these tests consists of two hidden layers LSTM networks for the encoder and reconstructive decoder components, each layer having 64 units. It uses cell values (c) and hidden outputs (h) in time T as the initial representation for the decoder. The baseline network has no hidden layer in its decoder decoder decoder (only one softmax layer with 64 units), and the reconstruction decoder is not conditional (as in Section 4.1)."}, {"heading": "5 EXPERIMENTS", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "5.1 DATA", "text": "The data in these experiments comes from the HDM05 and the MOCAP datasets of the CMU. Both are recorded at 120 frames per second (fps) and contain more than 30 marker positions. In our case, we use 23 common markers between the two datasets. We work with the C3D file format, which contains a series of positions for each marker. Our pre-processing of the data mainly consists of orienting, centering and scaling the point cloud of each image given by the files. The orientation process is a fundamental change of all 3D positions, so that the hips of the actor are always directed in the same direction. In this way, we facilitate the learning of movements, since the network does not have to learn to ignore orientations for classifying an action. This is especially useful given the small size of the thedasets. We then center the hips of the actor at the position (0.00) and scale so that each marker is always in the interval [1, \u2212 1]."}, {"heading": "5.2 NETWORK", "text": "As explained in Section 4.3, we combined all elements that had a positive impact on the classification task with the small network for the main experiment. Our larger network has two BDLSTM layers of size 512 in both the encoder and the reconstructive decoder. The hidden layer for the classifier decoder has a size of 2048. The reconstructive decoder is conditional, the minibatch size is 4 and the input dimension is 69. For initializing input weights, we consistently pull out [\u2212 \u221a 1 / fanin, \u221a 1 / fanin], while we use orthonormal initialization for recurring weight matrices. All distortions are initialized at 0, with the exception of LSTM oblivion gates initialized to 1, as proposed by Gers et al. (2000) and Jozefowicz et al. (2015)."}, {"heading": "5.3 RESULTS", "text": "Table 3 shows our results on the task of motion classification on our test kit within HDM05, which includes all sequences performed by actors identified by dg and tr in the files. We compare our results with our implementation of the technique by Cho & Chen (2013) on the same test kit. Again, we tested different classification cost ratios and confirmed that learning functions for reconstruction are useful for classification. Subsequently, we used the best classification cost ratio from networks that were trained only on HDM05 and applied it to train a network that uses both HDM05 and CMU datasets."}, {"heading": "5.3.1 ADDING UNLABELED DATA", "text": "Figure 4 shows the effect of adding unlabeled CMU data to the training set of a prepared HDM05 network only for 48 epochs. As we see, the more obvious change after epoch 48 (when adding CMU data) is the significant reduction in the gap between training and testing costs, which once again shows that a larger data set contributes to regulation, and the accuracy of classification is improved, demonstrating the added value of using unattended learning for a supervised task."}, {"heading": "6 CONCLUSION AND FUTURE WORK", "text": "We have shown that the use of the recurrent encoder decoder architecture with multiple decoders enables supervised and unsupervised tasks such as classification and reconstruction to be combined and a recursive network to be trained for both in a single phase. We have shown that the use of two decoders, one generative and one discriminatory, leads to better results in a motion classification task. Adding unlabelled data to the training set also helps with the supervised task. We have also defined a realistic test set on the HDM05, which we hope can serve as a benchmarking set in future work on MOCAP classification. Further tests will be carried out with more decoders (e.g. a future predictor decoder) to learn richer representations, and with different classification costs to find the best one. Other tests could be carried out with a summary vector such as Cho et al. (2014)."}, {"heading": "ACKNOWLEDGMENTS", "text": "We thank Ubisoft for their research and the authors of Theano (Bergstra et al., 2011)."}], "references": [{"title": "Learning long-term dependencies with gradient descent is difficult", "author": ["Bengio", "Yoshua", "Simard", "Patrice", "Frasconi", "Paolo"], "venue": "Neural Networks, IEEE Transactions on,", "citeRegEx": "Bengio et al\\.,? \\Q1994\\E", "shortCiteRegEx": "Bengio et al\\.", "year": 1994}, {"title": "Greedy layer-wise training of deep networks", "author": ["Bengio", "Yoshua", "Lamblin", "Pascal", "Popovici", "Dan", "Larochelle", "Hugo"], "venue": "Advances in neural information processing systems,", "citeRegEx": "Bengio et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Bengio et al\\.", "year": 2007}, {"title": "Theano: Deep learning on gpus with python", "author": ["Bergstra", "James", "Bastien", "Fr\u00e9d\u00e9ric", "Breuleux", "Olivier", "Lamblin", "Pascal", "Pascanu", "Razvan", "Delalleau", "Desjardins", "Guillaume", "Warde-Farley", "David", "Goodfellow", "Ian", "Bergeron", "Arnaud"], "venue": "In NIPS 2011,", "citeRegEx": "Bergstra et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Bergstra et al\\.", "year": 2011}, {"title": "Benchmarking of lstm networks", "author": ["Breuel", "Thomas M"], "venue": "arXiv preprint arXiv:1508.02774,", "citeRegEx": "Breuel and M.,? \\Q2015\\E", "shortCiteRegEx": "Breuel and M.", "year": 2015}, {"title": "Bio-inspired dynamic 3d discriminative skeletal features for human action recognition", "author": ["Chaudhry", "Rizwan", "Ofli", "Ferda", "Kurillo", "Gregorij", "Bajcsy", "Ruzena", "Vidal", "Ren\u00e9"], "venue": "In Computer Vision and Pattern Recognition Workshops (CVPRW),", "citeRegEx": "Chaudhry et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Chaudhry et al\\.", "year": 2013}, {"title": "Classification of rgb-d and motion capture sequences using extreme learning machine", "author": ["Chen", "Xi", "Koskela", "Markus"], "venue": "In Image Analysis,", "citeRegEx": "Chen et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Chen et al\\.", "year": 2013}, {"title": "Classifying and visualizing motion capture sequences using deep neural networks", "author": ["Cho", "Kyunghyun", "Chen", "Xi"], "venue": "arXiv preprint arXiv:1306.3874,", "citeRegEx": "Cho et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Cho et al\\.", "year": 2013}, {"title": "Learning phrase representations using rnn encoderdecoder for statistical machine translation", "author": ["Cho", "Kyunghyun", "Van Merri\u00ebnboer", "Bart", "Gulcehre", "Caglar", "Bahdanau", "Dzmitry", "Bougares", "Fethi", "Schwenk", "Holger", "Bengio", "Yoshua"], "venue": "arXiv preprint arXiv:1406.1078,", "citeRegEx": "Cho et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Cho et al\\.", "year": 2014}, {"title": "Hierarchical recurrent neural network for skeleton based action recognition", "author": ["Du", "Yong", "Wang", "Wei", "Liang"], "venue": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,", "citeRegEx": "Du et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Du et al\\.", "year": 2015}, {"title": "Why does unsupervised pre-training help deep learning", "author": ["Erhan", "Dumitru", "Bengio", "Yoshua", "Courville", "Aaron", "Manzagol", "Pierre-Antoine", "Vincent", "Pascal", "Samy"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "Erhan et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Erhan et al\\.", "year": 2010}, {"title": "Learning to forget: Continual prediction with lstm", "author": ["Gers", "Felix A", "Schmidhuber", "J\u00fcrgen", "Cummins", "Fred"], "venue": "Neural computation,", "citeRegEx": "Gers et al\\.,? \\Q2000\\E", "shortCiteRegEx": "Gers et al\\.", "year": 2000}, {"title": "Hybrid speech recognition with deep bidirectional lstm", "author": ["Graves", "Alan", "Jaitly", "Navdeep", "Mohamed", "Abdel-rahman"], "venue": "In Automatic Speech Recognition and Understanding (ASRU),", "citeRegEx": "Graves et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Graves et al\\.", "year": 2013}, {"title": "Speech recognition with deep recurrent neural networks", "author": ["Graves", "Alan", "Mohamed", "Abdel-rahman", "Hinton", "Geoffrey"], "venue": "In Acoustics, Speech and Signal Processing (ICASSP),", "citeRegEx": "Graves et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Graves et al\\.", "year": 2013}, {"title": "Supervised sequence labelling with recurrent neural networks, volume 385", "author": ["Graves", "Alex"], "venue": null, "citeRegEx": "Graves and Alex.,? \\Q2012\\E", "shortCiteRegEx": "Graves and Alex.", "year": 2012}, {"title": "Unconstrained on-line handwriting recognition with recurrent neural networks", "author": ["Graves", "Alex", "Liwicki", "Marcus", "Bunke", "Horst", "Schmidhuber", "J\u00fcrgen", "Fern\u00e1ndez", "Santiago"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Graves et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Graves et al\\.", "year": 2008}, {"title": "The vanishing gradient problem during learning recurrent neural nets and problem solutions", "author": ["Hochreiter", "Sepp"], "venue": "International Journal of Uncertainty, Fuzziness and Knowledge-Based Systems,", "citeRegEx": "Hochreiter and Sepp.,? \\Q1998\\E", "shortCiteRegEx": "Hochreiter and Sepp.", "year": 1998}, {"title": "Long short-term memory", "author": ["Hochreiter", "Sepp", "Schmidhuber", "J\u00fcrgen"], "venue": "Neural computation,", "citeRegEx": "Hochreiter et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Hochreiter et al\\.", "year": 1997}, {"title": "An empirical exploration of recurrent network architectures", "author": ["Jozefowicz", "Rafal", "Zaremba", "Wojciech", "Sutskever", "Ilya"], "venue": "In Proceedings of the 32nd International Conference on Machine Learning", "citeRegEx": "Jozefowicz et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Jozefowicz et al\\.", "year": 2015}, {"title": "Documentation mocap database hdm05", "author": ["M\u00fcller", "Meinard", "R\u00f6der", "Tido", "Clausen", "Michael", "Eberhardt", "Bernhard", "Kr\u00fcger", "Bj\u00f6rn", "Weber", "Andreas"], "venue": null, "citeRegEx": "M\u00fcller et al\\.,? \\Q2007\\E", "shortCiteRegEx": "M\u00fcller et al\\.", "year": 2007}, {"title": "Long short-term memory based recurrent neural network architectures for large vocabulary speech recognition", "author": ["Sak", "Ha\u015fim", "Senior", "Andrew", "Beaufays", "Fran\u00e7oise"], "venue": "arXiv preprint arXiv:1402.1128,", "citeRegEx": "Sak et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Sak et al\\.", "year": 2014}, {"title": "Unsupervised learning of video representations using lstms", "author": ["Srivastava", "Nitish", "Mansimov", "Elman", "Salakhutdinov", "Ruslan"], "venue": "arXiv preprint arXiv:1502.04681,", "citeRegEx": "Srivastava et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Srivastava et al\\.", "year": 2015}, {"title": "Generating text with recurrent neural networks", "author": ["Sutskever", "Ilya", "Martens", "James", "Hinton", "Geoffrey E"], "venue": "In Proceedings of the 28th International Conference on Machine Learning", "citeRegEx": "Sutskever et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Sutskever et al\\.", "year": 2011}, {"title": "Sequence to sequence learning with neural networks. In Advances in neural information processing", "author": ["Sutskever", "Ilya", "Vinyals", "Oriol", "Le", "Quoc VV"], "venue": null, "citeRegEx": "Sutskever et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Sutskever et al\\.", "year": 2014}, {"title": "Roles of pre-training and fine-tuning in context-dependent dbnhmms for real-world speech recognition", "author": ["Yu", "Dong", "Deng", "Li", "G. Dahl"], "venue": "In Proc. NIPS Workshop on Deep Learning and Unsupervised Feature Learning,", "citeRegEx": "Yu et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Yu et al\\.", "year": 2010}], "referenceMentions": [{"referenceID": 9, "context": "In these cases, semi-supervised learning may be preferred to supervised learning as it uses all the available data for training, and has good regularization properties (Erhan et al., 2010).", "startOffset": 168, "endOffset": 188}, {"referenceID": 1, "context": "A common technique for semi-supervised learning is to perform training in two phases: unsupervised pre-training, followed by supervised fine tuning (Bengio et al., 2007; Erhan et al., 2010; Yu et al., 2010).", "startOffset": 148, "endOffset": 206}, {"referenceID": 9, "context": "A common technique for semi-supervised learning is to perform training in two phases: unsupervised pre-training, followed by supervised fine tuning (Bengio et al., 2007; Erhan et al., 2010; Yu et al., 2010).", "startOffset": 148, "endOffset": 206}, {"referenceID": 23, "context": "A common technique for semi-supervised learning is to perform training in two phases: unsupervised pre-training, followed by supervised fine tuning (Bengio et al., 2007; Erhan et al., 2010; Yu et al., 2010).", "startOffset": 148, "endOffset": 206}, {"referenceID": 22, "context": "Recent advances in Recurrent Encoder-Decoder networks have afforded models the ability to perform both supervised learning (Sutskever et al., 2014; Cho et al., 2014) and unsupervised learning (Srivastava et al.", "startOffset": 123, "endOffset": 165}, {"referenceID": 7, "context": "Recent advances in Recurrent Encoder-Decoder networks have afforded models the ability to perform both supervised learning (Sutskever et al., 2014; Cho et al., 2014) and unsupervised learning (Srivastava et al.", "startOffset": 123, "endOffset": 165}, {"referenceID": 20, "context": ", 2014) and unsupervised learning (Srivastava et al., 2015).", "startOffset": 34, "endOffset": 59}, {"referenceID": 18, "context": "The first is the HDM05 public dataset (M\u00fcller et al., 2007).", "startOffset": 38, "endOffset": 59}, {"referenceID": 7, "context": "Other work by Du et al. (2015) treated the simple sequences\u2019 classification problem with the same action classes as Cho & Chen (2013) with a hierarchical network handling in its first layer parts of the body (e.", "startOffset": 14, "endOffset": 31}, {"referenceID": 7, "context": "Other work by Du et al. (2015) treated the simple sequences\u2019 classification problem with the same action classes as Cho & Chen (2013) with a hierarchical network handling in its first layer parts of the body (e.", "startOffset": 14, "endOffset": 134}, {"referenceID": 7, "context": "Other work by Du et al. (2015) treated the simple sequences\u2019 classification problem with the same action classes as Cho & Chen (2013) with a hierarchical network handling in its first layer parts of the body (e.g. torso, arms and legs), and concatenating some of these parts in each layer until the whole body is treated in the last hidden layer. They used recurrent neural networks to use context information, instead of concatenating features of some previous frames at each timestep like Cho & Chen (2013) and Chen & Koskela (2013).", "startOffset": 14, "endOffset": 509}, {"referenceID": 7, "context": "Other work by Du et al. (2015) treated the simple sequences\u2019 classification problem with the same action classes as Cho & Chen (2013) with a hierarchical network handling in its first layer parts of the body (e.g. torso, arms and legs), and concatenating some of these parts in each layer until the whole body is treated in the last hidden layer. They used recurrent neural networks to use context information, instead of concatenating features of some previous frames at each timestep like Cho & Chen (2013) and Chen & Koskela (2013). This led to better results, and their classification accuracy on simple sequences reached 96.", "startOffset": 14, "endOffset": 535}, {"referenceID": 4, "context": "Chaudhry et al. (2013) created bio-inspired features based on the neural encoding of shapes and, using neural nets with support vector machines, have obtained good results on simple sequences classification on 11 actions from the public HDM05 dataset.", "startOffset": 0, "endOffset": 23}, {"referenceID": 4, "context": "Nevertheless, it seems that there is an underlying problem for these results (except for (Chaudhry et al., 2013)), which relies in the definition of the test sets.", "startOffset": 89, "endOffset": 112}, {"referenceID": 4, "context": "Nevertheless, it seems that there is an underlying problem for these results (except for (Chaudhry et al., 2013)), which relies in the definition of the test sets. All those experiments perform 10-fold cross validation with 10 balanced partitions of shuffled sequences. This means that takes or frames recorded with a particular actor can be found in the training set as well as in the test set. This might not be representative of reality, when new takes are recorded with new actors. If an actor is asked to repeat five times the same movement in five different takes, then these will probably be very similar, and shuffling the frames or sequences will insert an undesired bias in the test set. Chaudhry et al. (2013), on the other hand, isolate their test set based on actors performing the actions.", "startOffset": 90, "endOffset": 721}, {"referenceID": 4, "context": "Nevertheless, it seems that there is an underlying problem for these results (except for (Chaudhry et al., 2013)), which relies in the definition of the test sets. All those experiments perform 10-fold cross validation with 10 balanced partitions of shuffled sequences. This means that takes or frames recorded with a particular actor can be found in the training set as well as in the test set. This might not be representative of reality, when new takes are recorded with new actors. If an actor is asked to repeat five times the same movement in five different takes, then these will probably be very similar, and shuffling the frames or sequences will insert an undesired bias in the test set. Chaudhry et al. (2013), on the other hand, isolate their test set based on actors performing the actions. This should give a test set more representative of reality. To show the difference it can make on the full dataset (65 classes), Table 1 presents our attempt at obtaining the results of Cho & Chen (2013) using their MLP+Auto-encoder hybrid with a random, balanced, 10% test set, as well as with a test set of sequences recorded with actors identified with dg and tr only.", "startOffset": 90, "endOffset": 1008}, {"referenceID": 4, "context": "Nevertheless, it seems that there is an underlying problem for these results (except for (Chaudhry et al., 2013)), which relies in the definition of the test sets. All those experiments perform 10-fold cross validation with 10 balanced partitions of shuffled sequences. This means that takes or frames recorded with a particular actor can be found in the training set as well as in the test set. This might not be representative of reality, when new takes are recorded with new actors. If an actor is asked to repeat five times the same movement in five different takes, then these will probably be very similar, and shuffling the frames or sequences will insert an undesired bias in the test set. Chaudhry et al. (2013), on the other hand, isolate their test set based on actors performing the actions. This should give a test set more representative of reality. To show the difference it can make on the full dataset (65 classes), Table 1 presents our attempt at obtaining the results of Cho & Chen (2013) using their MLP+Auto-encoder hybrid with a random, balanced, 10% test set, as well as with a test set of sequences recorded with actors identified with dg and tr only. Note that using two of the five actors of HDM05 to produce the test set leaves only roughly 60% of the data as training data, as opposed to 90% when using the 10% random test set. This is why, using the same technique, we show that even with 40% of the data in the test set, if partitions are random and balanced, results are considerably higher than when using a more realistic test set. Also, when using our preprocessing technique (PP) on the data, we get higher results on the newly defined test set. We explain this preprocessing method in section 5.1. The main difference with the one used by Cho & Chen (2013) is that we allow hips to rotate over an axis so that the actor may have a horizontal position (when lying down for example), while their method fixes the hips in a constant position and orientation.", "startOffset": 90, "endOffset": 1793}, {"referenceID": 19, "context": "This is why RNNs have often proven over the years to be very powerful on multiple sequential problems, such as speech recognition (Graves et al., 2013b; Sak et al., 2014; Graves et al., 2013a), handwriting recognition (Graves et al.", "startOffset": 130, "endOffset": 192}, {"referenceID": 14, "context": ", 2013a), handwriting recognition (Graves et al., 2008), text generation (Sutskever et al.", "startOffset": 34, "endOffset": 55}, {"referenceID": 21, "context": ", 2008), text generation (Sutskever et al., 2011; Graves, 2012), or in our case MOCAP action recognition (Du et al.", "startOffset": 25, "endOffset": 63}, {"referenceID": 8, "context": ", 2011; Graves, 2012), or in our case MOCAP action recognition (Du et al., 2015).", "startOffset": 63, "endOffset": 80}, {"referenceID": 0, "context": "One known problem with RNNs is that they can be very hard to train to model long-term dependencies because of the vanishing gradient problem (Bengio et al., 1994; Hochreiter, 1998).", "startOffset": 141, "endOffset": 180}, {"referenceID": 0, "context": "One known problem with RNNs is that they can be very hard to train to model long-term dependencies because of the vanishing gradient problem (Bengio et al., 1994; Hochreiter, 1998). One of the most popular and effective methods to counter this problem is the use of Long-Short-Term Memory networks (LSTMs), as presented by Hochreiter & Schmidhuber (1997). These recurrent networks", "startOffset": 142, "endOffset": 355}, {"referenceID": 11, "context": "Based on Figure 1 in Graves et al. (2013b).", "startOffset": 21, "endOffset": 43}, {"referenceID": 6, "context": "Cho et al. (2014) as well as Sutskever et al.", "startOffset": 0, "endOffset": 18}, {"referenceID": 6, "context": "Cho et al. (2014) as well as Sutskever et al. (2014) have used this approach for sequence-to-sequence translation, with some differences in the choice of hidden units and in the use of an additional summary vector (and set of weights) in the case of Cho et al.", "startOffset": 0, "endOffset": 53}, {"referenceID": 6, "context": "Cho et al. (2014) as well as Sutskever et al. (2014) have used this approach for sequence-to-sequence translation, with some differences in the choice of hidden units and in the use of an additional summary vector (and set of weights) in the case of Cho et al. (2014). Both these approaches need a symbol of end-of-sequence to allow input and target sequences to have different lengths.", "startOffset": 0, "endOffset": 268}, {"referenceID": 6, "context": "Cho et al. (2014) as well as Sutskever et al. (2014) have used this approach for sequence-to-sequence translation, with some differences in the choice of hidden units and in the use of an additional summary vector (and set of weights) in the case of Cho et al. (2014). Both these approaches need a symbol of end-of-sequence to allow input and target sequences to have different lengths. They are trained to maximize the conditional probability of the target sequence given the input sequence. Our approach is more closely related to the one used by Srivastava et al. (2015) in which they perform unsupervised learning, by either reconstructing the sequence, predicting the next frames, or both.", "startOffset": 0, "endOffset": 574}, {"referenceID": 20, "context": "For example, as explained by Srivastava et al. (2015), a recurrent decoder may use at each timestep, in addition to its last hidden state, its previous output as an additional input.", "startOffset": 29, "endOffset": 54}, {"referenceID": 10, "context": "All biases are initialized at 0, except for LSTM forget gates which are initialized to 1, as proposed by Gers et al. (2000) and Jozefowicz et al.", "startOffset": 105, "endOffset": 124}, {"referenceID": 10, "context": "All biases are initialized at 0, except for LSTM forget gates which are initialized to 1, as proposed by Gers et al. (2000) and Jozefowicz et al. (2015). The learning rate is initialized to 0.", "startOffset": 105, "endOffset": 153}, {"referenceID": 6, "context": "Other tests could be conducted using a summary vector as Cho et al. (2014) have done.", "startOffset": 57, "endOffset": 75}], "year": 2017, "abstractText": "Recent work on sequence to sequence translation using Recurrent Neural Networks (RNNs) based on Long Short Term Memory (LSTM) architectures has shown great potential for learning useful representations of sequential data. These architectures, using one recurrent neural network to encode sequences into fixedlength representations, and one or more network(s) to decode representations into new sequences have the advantages of being modular, while also allowing modules to be jointly trained. A one-to-many encoder-decoder(s) scheme allows for a single encoder to provide representations serving multiple purposes. In our case, we present an LSTM encoder network able to produce representations used by two decoders: one that reconstructs, and one that classifies if the training sequence has a labelling. This allows the network to learn representations that are useful for both discriminative and generative tasks at the same time. We show how this paradigm is very well suited for semi-supervised learning with sequences. We test our proposed approach on an action recognition task using motion capture (MOCAP) sequences and show that semi-supervised feature learning can improve movement classification.", "creator": "LaTeX with hyperref package"}}}