{"id": "1511.06674", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "20-Nov-2015", "title": "Stories in the Eye: Contextual Visual Interactions for Efficient Video to Language Translation", "abstract": "Integrating higher level visual and linguistic interpretations is at the heart of human intelligence. As automatic visual category recognition in images is approaching human performance, the high level understanding in the dynamic spatiotemporal domain of videos and its translation into natural language is still far from being solved. While most works on vision-to-text translations use pre-learned or pre-established computational linguistic models, in this paper we present an approach that uses vision alone to efficiently learn how to translate into language the video content. We discover, in simple form, the story played by main actors, while using only visual cues for representing objects and their interactions. Our method learns in a hierarchical manner higher level representations for recognizing subjects, actions and objects involved, their relevant contextual background and their interaction to one another over time. We have a three stage approach: first we take in consideration features of the individual entities at the local level of appearance, then we consider the relationship between these objects and actions and their video background, and third, we consider their spatiotemporal relations as inputs to classifiers at the highest level of interpretation. Thus, our approach finds a coherent linguistic description of videos in the form of a subject, verb and object based on their role played in the overall visual story learned directly from training data, without using a known language model. We test the efficiency of our approach on a large scale dataset containing YouTube clips taken in the wild and demonstrate state-of-the-art performance, often superior to current approaches that use more complex, pre-learned linguistic knowledge.", "histories": [["v1", "Fri, 20 Nov 2015 16:33:13 GMT  (2711kb,D)", "http://arxiv.org/abs/1511.06674v1", null]], "reviews": [], "SUBJECTS": "cs.CV cs.CL", "authors": ["anirudh goyal", "marius leordeanu"], "accepted": false, "id": "1511.06674"}, "pdf": {"name": "1511.06674.pdf", "metadata": {"source": "CRF", "title": null, "authors": ["Anirudh Goyal"], "emails": ["anirudh.goyal@students.iiit.ac.in", "leordeanu@gmail.com"], "sections": [{"heading": "1 INTRODUCTION", "text": "In fact, most of them will be able to move to another world, where they will be able to move to another world, where they will be able to move to another world, where they will be able to move, where they will be able to move, where they will be able to move."}, {"heading": "2 RELATION TO PRIOR WORK", "text": "The approaches we follow are the current works of (Guadarrama et al.), (Xu et al.), (Venugopalan et al.) (2014), (2015), (2015), (2015), (2015), (2015), (2015), (2015), (2015), (2015), (2015), (2015), (2015), (2015), (2015), (2015), (2015), (2015), (2015), (2015), (2016), (2017), (2017), (2017), (2017), (2017), (2017), (2017), (2017), (2017), (2017), (2017), (2017), (2017), (2017), (2017), (2017), (2017), (2017), (2017), (2017), (2017), (2017), (2017), (2017), (2017), (2017), (2017), (2017), (2017), (2017), (2017), (2017), (2017), (2017), (2017), (2017), (2017), (2017), (2017), (2017), (2017), (2017), (2017), (2017), (2017), (2017), (2017), (2017), (2017), (2017), (2017), (2017), (2017), (2017), (2017), (2017), (2017), (2017), (2017), (2017), (2017), (2017), (2017), (), (2017), (2017), (), (2017), (2017), (), (), (2017), (), (2017), (), (2017), (), ((), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), ()"}, {"heading": "3 OVERVIEW OF OUR APPROACH", "text": "We address the portrait of linguistic video interpretation by first creating classifiers to predict the different concepts, subjects, objects and verbs from pure visual information that we extract from deep AlexNet-like CNNs based on the large ILRSVC-2012 image classification dataset (Jia et al. (2014); Krizhevsky et al. (2011). Specifically, we use features from the last fully connected layer (fc7). As shown in our experiments, these features are often able to predict video content, such as the presence of certain subjects, objects and even verbs, without using explicit movement information. Correct prediction of verbs is due to the fact that higher-level concepts, such as words in a sentence, are strongly interconnected and the presence of one will affect the expectation of another. For example, the appearance of a boy could increase the expectation for the verb \"play.\""}, {"heading": "4 PRINCIPLES AND CONTRIBUTIONS OF THE VISUAL STORY APPROACH", "text": "In fact, we are able to go in search of a solution that meets the needs of the people."}, {"heading": "5 ALGORITHM IMPLEMENTATION", "text": "This year, it has come to the point that there will only be one such process in which there will be such a process."}, {"heading": "6 EXPERIMENTAL ANALYSIS", "text": "In fact, most of them will be able to play by the rules they have set themselves, and they will be able to play by the rules they have set."}, {"heading": "7 DISCUSSION AND CONCLUSIONS", "text": "We have presented an efficient method of video-language translation in the form of SVO triplets that takes only visual information into account by integrating contextual interactions between foreground and background at a higher level, the common occurrence of semantic classifiers, and the temporal arrangement of subjects, objects, and verbs. One of the most important innovations of our method is to show that visual information alone can perform better on a semantic level than more complex models that integrate both vision and semantics. Another important contribution of our work is to show how contextual relationships at different levels of visual interpretation can increase recognition performance, which automatically results in more coherent translations. We focus on the general idea of a visual history in which objects, scenes, and interactions are understood in a unified way. We define their basic principles and present an efficient and relatively simple implementation that significantly exceeds more complex systems, such as those based on long-short-term neural networks."}, {"heading": "8 ACKNOWLEDGMENTS", "text": "This work was partially supported by CNCS-UEFICSDI within the PNII PCE-2012-4-0581 project and the authors thank Dr. Rahul Sukthankar for numerous fruitful discussions and helpful feedback."}], "references": [{"title": "Salient object detection: A benchmark", "author": ["Borji", "Ali", "Sihite", "Dicky N", "Itti", "Laurent"], "venue": "In ECCV,", "citeRegEx": "Borji et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Borji et al\\.", "year": 2012}, {"title": "Action recognition by shape matching to key frames", "author": ["S. Carlsson", "J. Sullivan"], "venue": "In WMECV,", "citeRegEx": "Carlsson and Sullivan,? \\Q2001\\E", "shortCiteRegEx": "Carlsson and Sullivan", "year": 2001}, {"title": "Collecting highly parallel data for paraphrase evaluation", "author": ["Chen", "David L", "Dolan", "William B"], "venue": "In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies-Volume", "citeRegEx": "Chen et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Chen et al\\.", "year": 2011}, {"title": "Global contrast based salient region detection", "author": ["Cheng", "Ming", "Mitra", "Niloy J", "Huang", "Xumin", "Torr", "Philip HS", "Hu", "Song"], "venue": "Pattern Analysis and Machine Intelligence, IEEE Transactions on,", "citeRegEx": "Cheng et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Cheng et al\\.", "year": 2015}, {"title": "Stories of experience and narrative inquiry", "author": ["Connelly", "F Michael", "Clandinin", "D Jean"], "venue": "Educational researcher,", "citeRegEx": "Connelly et al\\.,? \\Q1990\\E", "shortCiteRegEx": "Connelly et al\\.", "year": 1990}, {"title": "A thousand frames in just a few words: Lingual description of videos through latent topics and sparse object stitching", "author": ["P. Das", "C. Xu", "R.F. Doell", "J.J. Corso"], "venue": "In CVPR,", "citeRegEx": "Das et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Das et al\\.", "year": 2013}, {"title": "Exploring the trade-off between accuracy and observational latency in action recognition", "author": ["C. Ellis", "S.Z. Masood", "M.F. Tappen", "Jr.", "J.J. LaViola", "R. Sukthankar"], "venue": null, "citeRegEx": "Ellis et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Ellis et al\\.", "year": 2012}, {"title": "Every picture tells a story: Generating sentences from images", "author": ["A. Farhadi", "M. Hejrati", "M.A. Sadeghi", "P. Young", "C. Rashtchian", "J. Hockenmaier", "D. Forsyth"], "venue": "In ECCV,", "citeRegEx": "Farhadi et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Farhadi et al\\.", "year": 2010}, {"title": "Object detection with discriminatively trained part-based models", "author": ["P.F. Felzenszwalb", "R.B. Girshick", "D. McAllester", "D. Ramanan"], "venue": null, "citeRegEx": "Felzenszwalb et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Felzenszwalb et al\\.", "year": 2010}, {"title": "Automatic caption generation for news", "author": ["Y. Feng", "M. Lapata"], "venue": "images. PAMI,", "citeRegEx": "Feng and Lapata,? \\Q2013\\E", "shortCiteRegEx": "Feng and Lapata", "year": 2013}, {"title": "Rich feature hierarchies for accurate object detection and semantic segmentation", "author": ["Girshick", "Ross", "Donahue", "Jeff", "Darrell", "Trevor", "Malik", "Jagannath"], "venue": "In CVPR,", "citeRegEx": "Girshick et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Girshick et al\\.", "year": 2014}, {"title": "Youtube2text: Recognizing and describing arbitrary activities using semantic hierarchies and zero-shot recognition", "author": ["S. Guadarrama", "N. Krishnamoorthy", "G. Malkarnenkar", "S. Venugopalan", "R. Mooney", "T. Darrell", "K. Saenko"], "venue": "In ICCV,", "citeRegEx": "Guadarrama et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Guadarrama et al\\.", "year": 2013}, {"title": "Saliency detection: A spectral residual approach", "author": ["Hou", "Xiaodi", "Zhang", "Liqing"], "venue": "In CVPR,", "citeRegEx": "Hou et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Hou et al\\.", "year": 2007}, {"title": "Caffe: Convolutional architecture for fast feature embedding", "author": ["Jia", "Yangqing", "Shelhamer", "Evan", "Donahue", "Jeff", "Karayev", "Sergey", "Long", "Jonathan", "Girshick", "Ross", "Guadarrama", "Sergio", "Darrell", "Trevor"], "venue": "In Proceedings of the ACM International Conference on Multimedia,", "citeRegEx": "Jia et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Jia et al\\.", "year": 2014}, {"title": "Imagenet classification with deep convolutional neural networks", "author": ["A. Krizhevsky", "I. Sutskever", "G.E. Hinton"], "venue": "In NIPS,", "citeRegEx": "Krizhevsky et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Krizhevsky et al\\.", "year": 2011}, {"title": "Baby talk: Understanding and generating image descriptions", "author": ["G. Kulkarni", "V. Premraj", "S. Dhar", "S. Li", "Y. Choi", "A.C. Berg", "T.L. Berg"], "venue": "In CVPR,", "citeRegEx": "Kulkarni et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Kulkarni et al\\.", "year": 2011}, {"title": "Composing simple image descriptions using web-scale n-grams", "author": ["S. Li", "G. Kulkarni", "T.L. Berg", "A.C. Berg", "Y. Choi"], "venue": "In Computational Natural Language Learning,", "citeRegEx": "Li et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Li et al\\.", "year": 2011}, {"title": "Microsoft coco: Common objects in context", "author": ["Lin", "Tsung-Yi", "Maire", "Michael", "Belongie", "Serge", "Hays", "James", "Perona", "Pietro", "Ramanan", "Deva", "Doll\u00e1r", "Piotr", "Zitnick", "C Lawrence"], "venue": "In Computer Vision\u2013ECCV", "citeRegEx": "Lin et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Lin et al\\.", "year": 2014}, {"title": "Single view human action recognition using key pose matching and viterbi path searching", "author": ["F. Lv", "F. Nevaita"], "venue": "In CVPR,", "citeRegEx": "Lv and Nevaita,? \\Q2007\\E", "shortCiteRegEx": "Lv and Nevaita", "year": 2007}, {"title": "The influence of scene context on object recognition is independent of attentional focus", "author": ["Munneke", "Jaap", "Brentari", "Valentina", "Peelen", "Marius V"], "venue": "Frontiers in psychology,", "citeRegEx": "Munneke et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Munneke et al\\.", "year": 2013}, {"title": "Modeling the shape of the scene: a holistic representation of the spatial envelope", "author": ["A. Oliva", "A. Torralba"], "venue": null, "citeRegEx": "Oliva and Torralba,? \\Q2001\\E", "shortCiteRegEx": "Oliva and Torralba", "year": 2001}, {"title": "Im2text: Describing images using 1 million captioned photographs", "author": ["V. Ordonez", "G. Kulkarni", "T.L. Berg"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Ordonez et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Ordonez et al\\.", "year": 2011}, {"title": "Artifactual literacies: Every object tells a story", "author": ["Pahl", "Kate", "Rowsell", "Jennifer"], "venue": "Teachers College Press New York,", "citeRegEx": "Pahl et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Pahl et al\\.", "year": 2010}, {"title": "Distributional clustering of english words", "author": ["F. Pereira", "N. Tishby", "L. Lee"], "venue": "In Proceedings of the 31st annual meeting on Association for Computational Linguistics,", "citeRegEx": "Pereira et al\\.,? \\Q1993\\E", "shortCiteRegEx": "Pereira et al\\.", "year": 1993}, {"title": "Translating video content to natural language descriptions", "author": ["M. Rohrbach", "W. Qiu", "I. Titov", "S. Thater", "M. Pinkal", "B. Schiele"], "venue": "In ICCV,", "citeRegEx": "Rohrbach et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Rohrbach et al\\.", "year": 2013}, {"title": "Imagenet large scale visual recognition challenge", "author": ["Russakovsky", "Olga", "Deng", "Jia", "Su", "Hao", "Krause", "Jonathan", "Satheesh", "Sanjeev", "Ma", "Sean", "Huang", "Zhiheng", "Karpathy", "Andrej", "Khosla", "Aditya", "Bernstein", "Michael"], "venue": "International Journal of Computer Vision,", "citeRegEx": "Russakovsky et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Russakovsky et al\\.", "year": 2014}, {"title": "Knowledge and memory: The real story. Knowledge and memory: The real story", "author": ["Schank", "Roger C", "Abelson", "Robert P"], "venue": "Advances in social cognition,", "citeRegEx": "Schank et al\\.,? \\Q1995\\E", "shortCiteRegEx": "Schank et al\\.", "year": 1995}, {"title": "Coherent multi-sentence video description with variable level of detail", "author": ["A. Senina", "M. Rohrbach", "W. Qiu", "A. Friedrich", "S. Amin", "M. Andriluka", "M. Pinkal", "B. Schiele"], "venue": "arXiv preprint arXiv:1403.6173,", "citeRegEx": "Senina et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Senina et al\\.", "year": 2014}, {"title": "Very deep convolutional networks for large-scale image recognition", "author": ["K. Simonyan", "A. Zisserman"], "venue": "arXiv preprint arXiv:1409.1556,", "citeRegEx": "Simonyan and Zisserman,? \\Q2014\\E", "shortCiteRegEx": "Simonyan and Zisserman", "year": 2014}, {"title": "Multiple frames matching for object discovery in video", "author": ["O. Stretcu", "M. Leordeanu"], "venue": "In British Machine Vision Conference,", "citeRegEx": "Stretcu and Leordeanu,? \\Q2015\\E", "shortCiteRegEx": "Stretcu and Leordeanu", "year": 2015}, {"title": "Integrating language and vision to generate natural language descriptions of videos in the wild", "author": ["J. Thomason", "S. Venugopalan", "S. Guadarrama", "K. Saenko", "R. Mooney"], "venue": "In International Conference on Computational Linguistics (COLING),", "citeRegEx": "Thomason et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Thomason et al\\.", "year": 2014}, {"title": "Translating videos to natural language using deep recurrent neural networks", "author": ["Venugopalan", "Subhashini", "Xu", "Huijuan", "Donahue", "Jeff", "Rohrbach", "Marcus", "Mooney", "Raymond", "Saenko", "Kate"], "venue": "arXiv preprint arXiv:1412.4729,", "citeRegEx": "Venugopalan et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Venugopalan et al\\.", "year": 2014}, {"title": "Jointly modeling deep video and compositional text to bridge vision and language in a unified framework", "author": ["R. Xu", "C. Xiong", "W. Chen", "J.J. Corso"], "venue": "In AAAI Conference on Artificial Intelligence,", "citeRegEx": "Xu et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Xu et al\\.", "year": 2015}, {"title": "I2t: Image parsing to text description", "author": ["B. Yao", "X. Yang", "L. Lin", "M.W. Lee", "S.C. Zhu"], "venue": "Proceedings of the IEEE,", "citeRegEx": "Yao et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Yao et al\\.", "year": 2010}, {"title": "Describing videos by exploiting temporal structure", "author": ["Yao", "Li", "Torabi", "Atousa", "Cho", "Kyunghyun", "Ballas", "Nicolas", "Pal", "Christopher", "Larochelle", "Hugo", "Courville", "Aaron"], "venue": null, "citeRegEx": "Yao et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Yao et al\\.", "year": 2015}, {"title": "From image descriptions to visual denotations: New similarity metrics for semantic inference over event descriptions", "author": ["Young", "Peter", "Lai", "Alice", "Hodosh", "Micah", "Hockenmaier", "Julia"], "venue": "Transactions of the Association for Computational Linguistics,", "citeRegEx": "Young et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Young et al\\.", "year": 2014}, {"title": "Grounded language learning from video described with sentences", "author": ["H. Yu", "J.M. Siskind"], "venue": "In ACL,", "citeRegEx": "Yu and Siskind,? \\Q2013\\E", "shortCiteRegEx": "Yu and Siskind", "year": 2013}, {"title": "The moving pose: An efficient 3d kinematics descriptor for low-latency action recognition and detection", "author": ["Zanfir", "Mihai", "Leordeanu", "Marius", "Sminchisescu", "Cristian"], "venue": "In Computer Vision (ICCV),", "citeRegEx": "Zanfir et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Zanfir et al\\.", "year": 2013}], "referenceMentions": [{"referenceID": 12, "context": "Most existing research that integrates vision and language has been studying the description of static images (Kulkarni et al. (2011); Li et al.", "startOffset": 111, "endOffset": 134}, {"referenceID": 12, "context": "Most existing research that integrates vision and language has been studying the description of static images (Kulkarni et al. (2011); Li et al. (2011); Farhadi et al.", "startOffset": 111, "endOffset": 152}, {"referenceID": 6, "context": "(2011); Farhadi et al. (2010); Yao et al.", "startOffset": 8, "endOffset": 30}, {"referenceID": 6, "context": "(2011); Farhadi et al. (2010); Yao et al. (2010); Feng & Lapata (2013); Ordonez et al.", "startOffset": 8, "endOffset": 49}, {"referenceID": 6, "context": "(2011); Farhadi et al. (2010); Yao et al. (2010); Feng & Lapata (2013); Ordonez et al.", "startOffset": 8, "endOffset": 71}, {"referenceID": 6, "context": "(2011); Farhadi et al. (2010); Yao et al. (2010); Feng & Lapata (2013); Ordonez et al. (2011)).", "startOffset": 8, "endOffset": 94}, {"referenceID": 6, "context": "(2011); Farhadi et al. (2010); Yao et al. (2010); Feng & Lapata (2013); Ordonez et al. (2011)). The problem is still far from being solved, with linguistic descriptions usually being limited to simple and rather strict descriptions. They usually have a single atomic action, one main actor acting upon a single object. This is also the case we study here, our method being focused on video to (subject(S), verb(V), object(O)) translations - a task tackled by a considerable number of recent works (Thomason et al. (2014); Guadarrama et al.", "startOffset": 8, "endOffset": 521}, {"referenceID": 6, "context": "(2011); Farhadi et al. (2010); Yao et al. (2010); Feng & Lapata (2013); Ordonez et al. (2011)). The problem is still far from being solved, with linguistic descriptions usually being limited to simple and rather strict descriptions. They usually have a single atomic action, one main actor acting upon a single object. This is also the case we study here, our method being focused on video to (subject(S), verb(V), object(O)) translations - a task tackled by a considerable number of recent works (Thomason et al. (2014); Guadarrama et al. (2013); Xu et al.", "startOffset": 8, "endOffset": 547}, {"referenceID": 6, "context": "(2011); Farhadi et al. (2010); Yao et al. (2010); Feng & Lapata (2013); Ordonez et al. (2011)). The problem is still far from being solved, with linguistic descriptions usually being limited to simple and rather strict descriptions. They usually have a single atomic action, one main actor acting upon a single object. This is also the case we study here, our method being focused on video to (subject(S), verb(V), object(O)) translations - a task tackled by a considerable number of recent works (Thomason et al. (2014); Guadarrama et al. (2013); Xu et al. (2015); Venugopalan et al.", "startOffset": 8, "endOffset": 565}, {"referenceID": 6, "context": "(2011); Farhadi et al. (2010); Yao et al. (2010); Feng & Lapata (2013); Ordonez et al. (2011)). The problem is still far from being solved, with linguistic descriptions usually being limited to simple and rather strict descriptions. They usually have a single atomic action, one main actor acting upon a single object. This is also the case we study here, our method being focused on video to (subject(S), verb(V), object(O)) translations - a task tackled by a considerable number of recent works (Thomason et al. (2014); Guadarrama et al. (2013); Xu et al. (2015); Venugopalan et al. (2014); Barbu et al.", "startOffset": 8, "endOffset": 592}, {"referenceID": 6, "context": "(2011); Farhadi et al. (2010); Yao et al. (2010); Feng & Lapata (2013); Ordonez et al. (2011)). The problem is still far from being solved, with linguistic descriptions usually being limited to simple and rather strict descriptions. They usually have a single atomic action, one main actor acting upon a single object. This is also the case we study here, our method being focused on video to (subject(S), verb(V), object(O)) translations - a task tackled by a considerable number of recent works (Thomason et al. (2014); Guadarrama et al. (2013); Xu et al. (2015); Venugopalan et al. (2014); Barbu et al. (2012); Das et al.", "startOffset": 8, "endOffset": 613}, {"referenceID": 5, "context": "(2012); Das et al. (2013); Rohrbach et al.", "startOffset": 8, "endOffset": 26}, {"referenceID": 5, "context": "(2012); Das et al. (2013); Rohrbach et al. (2013); Senina et al.", "startOffset": 8, "endOffset": 50}, {"referenceID": 5, "context": "(2012); Das et al. (2013); Rohrbach et al. (2013); Senina et al. (2014); Yu & Siskind (2013)).", "startOffset": 8, "endOffset": 72}, {"referenceID": 5, "context": "(2012); Das et al. (2013); Rohrbach et al. (2013); Senina et al. (2014); Yu & Siskind (2013)).", "startOffset": 8, "endOffset": 93}, {"referenceID": 8, "context": "However, recent advances with hierarchical recognition models, such as the highly successful Deformable Part Model (Felzenszwalb et al. (2010)) and the deep learning approaches with neural networks (Krizhevsky et al.", "startOffset": 116, "endOffset": 143}, {"referenceID": 8, "context": "However, recent advances with hierarchical recognition models, such as the highly successful Deformable Part Model (Felzenszwalb et al. (2010)) and the deep learning approaches with neural networks (Krizhevsky et al. (2011); Simonyan & Zisserman (2014); Szegedy et al.", "startOffset": 116, "endOffset": 224}, {"referenceID": 8, "context": "However, recent advances with hierarchical recognition models, such as the highly successful Deformable Part Model (Felzenszwalb et al. (2010)) and the deep learning approaches with neural networks (Krizhevsky et al. (2011); Simonyan & Zisserman (2014); Szegedy et al.", "startOffset": 116, "endOffset": 253}, {"referenceID": 8, "context": "However, recent advances with hierarchical recognition models, such as the highly successful Deformable Part Model (Felzenszwalb et al. (2010)) and the deep learning approaches with neural networks (Krizhevsky et al. (2011); Simonyan & Zisserman (2014); Szegedy et al. (2014)) show great promise towards solving the object categorization and detection part, fast approaching the human performance.", "startOffset": 116, "endOffset": 276}, {"referenceID": 8, "context": "However, recent advances with hierarchical recognition models, such as the highly successful Deformable Part Model (Felzenszwalb et al. (2010)) and the deep learning approaches with neural networks (Krizhevsky et al. (2011); Simonyan & Zisserman (2014); Szegedy et al. (2014)) show great promise towards solving the object categorization and detection part, fast approaching the human performance. 2) While image recognition is enjoying a great success, recognition in video is different and it is not clear yet how the spatiotemporal consistency that is naturally present in a video could be best used for improved recognition. Moreover, the larger data available in a video contains important and relevant information about the main objects of interest as well as about the contextual scene in which the events take place. How could we best take advantage of the contextual relationships between objects, and between objects and the scene for superior classification. In video, the foreground and background can be better separated automatically (Stretcu & Leordeanu (2015)), and considering both their contrasting properties and their interdependence could benefit recognition.", "startOffset": 116, "endOffset": 1076}, {"referenceID": 11, "context": "The approaches that are most similar to our method are the recent works of (Guadarrama et al. (2013); Xu et al.", "startOffset": 76, "endOffset": 101}, {"referenceID": 11, "context": "The approaches that are most similar to our method are the recent works of (Guadarrama et al. (2013); Xu et al. (2015); Venugopalan et al.", "startOffset": 76, "endOffset": 119}, {"referenceID": 11, "context": "The approaches that are most similar to our method are the recent works of (Guadarrama et al. (2013); Xu et al. (2015); Venugopalan et al. (2014); Thomason et al.", "startOffset": 76, "endOffset": 146}, {"referenceID": 11, "context": "The approaches that are most similar to our method are the recent works of (Guadarrama et al. (2013); Xu et al. (2015); Venugopalan et al. (2014); Thomason et al. (2014)), which are able to translate videos into SVO triplets with a relatively numerous vocabulary of concepts learned from a large set of training videos taken in the wild.", "startOffset": 76, "endOffset": 170}, {"referenceID": 11, "context": "The approaches that are most similar to our method are the recent works of (Guadarrama et al. (2013); Xu et al. (2015); Venugopalan et al. (2014); Thomason et al. (2014)), which are able to translate videos into SVO triplets with a relatively numerous vocabulary of concepts learned from a large set of training videos taken in the wild. Guadarrama et al. (2013) propose a method based on semantic hierarchies.", "startOffset": 76, "endOffset": 363}, {"referenceID": 11, "context": "The approaches that are most similar to our method are the recent works of (Guadarrama et al. (2013); Xu et al. (2015); Venugopalan et al. (2014); Thomason et al. (2014)), which are able to translate videos into SVO triplets with a relatively numerous vocabulary of concepts learned from a large set of training videos taken in the wild. Guadarrama et al. (2013) propose a method based on semantic hierarchies. They take a language driven approach in which semantic relationships are learned between different labels by mining large corpus of text and then clustering semantically related concepts based on their correlations and co-occurrences in human annotated text descriptions (Pereira et al. (1993)).", "startOffset": 76, "endOffset": 705}, {"referenceID": 11, "context": "The approaches that are most similar to our method are the recent works of (Guadarrama et al. (2013); Xu et al. (2015); Venugopalan et al. (2014); Thomason et al. (2014)), which are able to translate videos into SVO triplets with a relatively numerous vocabulary of concepts learned from a large set of training videos taken in the wild. Guadarrama et al. (2013) propose a method based on semantic hierarchies. They take a language driven approach in which semantic relationships are learned between different labels by mining large corpus of text and then clustering semantically related concepts based on their correlations and co-occurrences in human annotated text descriptions (Pereira et al. (1993)). Thomason et al. (2014) use a factor graph model that combines state-of-theart visual recognition systems with probabilistic knowledge mined from large text corpora that are independent of the video dataset.", "startOffset": 76, "endOffset": 730}, {"referenceID": 11, "context": "The approaches that are most similar to our method are the recent works of (Guadarrama et al. (2013); Xu et al. (2015); Venugopalan et al. (2014); Thomason et al. (2014)), which are able to translate videos into SVO triplets with a relatively numerous vocabulary of concepts learned from a large set of training videos taken in the wild. Guadarrama et al. (2013) propose a method based on semantic hierarchies. They take a language driven approach in which semantic relationships are learned between different labels by mining large corpus of text and then clustering semantically related concepts based on their correlations and co-occurrences in human annotated text descriptions (Pereira et al. (1993)). Thomason et al. (2014) use a factor graph model that combines state-of-theart visual recognition systems with probabilistic knowledge mined from large text corpora that are independent of the video dataset. Following a similar direction, Venugopalan et al. (2014) employ a two layer LSTM model on top of visual features learned with the Convolutional Neural Network model available in Caffe (Jia et al.", "startOffset": 76, "endOffset": 971}, {"referenceID": 11, "context": "The approaches that are most similar to our method are the recent works of (Guadarrama et al. (2013); Xu et al. (2015); Venugopalan et al. (2014); Thomason et al. (2014)), which are able to translate videos into SVO triplets with a relatively numerous vocabulary of concepts learned from a large set of training videos taken in the wild. Guadarrama et al. (2013) propose a method based on semantic hierarchies. They take a language driven approach in which semantic relationships are learned between different labels by mining large corpus of text and then clustering semantically related concepts based on their correlations and co-occurrences in human annotated text descriptions (Pereira et al. (1993)). Thomason et al. (2014) use a factor graph model that combines state-of-theart visual recognition systems with probabilistic knowledge mined from large text corpora that are independent of the video dataset. Following a similar direction, Venugopalan et al. (2014) employ a two layer LSTM model on top of visual features learned with the Convolutional Neural Network model available in Caffe (Jia et al. (2014)), a minor variant of AlexNet (Krizhevsky et al.", "startOffset": 76, "endOffset": 1117}, {"referenceID": 11, "context": "The approaches that are most similar to our method are the recent works of (Guadarrama et al. (2013); Xu et al. (2015); Venugopalan et al. (2014); Thomason et al. (2014)), which are able to translate videos into SVO triplets with a relatively numerous vocabulary of concepts learned from a large set of training videos taken in the wild. Guadarrama et al. (2013) propose a method based on semantic hierarchies. They take a language driven approach in which semantic relationships are learned between different labels by mining large corpus of text and then clustering semantically related concepts based on their correlations and co-occurrences in human annotated text descriptions (Pereira et al. (1993)). Thomason et al. (2014) use a factor graph model that combines state-of-theart visual recognition systems with probabilistic knowledge mined from large text corpora that are independent of the video dataset. Following a similar direction, Venugopalan et al. (2014) employ a two layer LSTM model on top of visual features learned with the Convolutional Neural Network model available in Caffe (Jia et al. (2014)), a minor variant of AlexNet (Krizhevsky et al. (2011))", "startOffset": 76, "endOffset": 1172}, {"referenceID": 24, "context": "and trained on ILSVRC-2012 classification subset of ImageNet (Russakovsky et al. (2014)).", "startOffset": 62, "endOffset": 88}, {"referenceID": 24, "context": "and trained on ILSVRC-2012 classification subset of ImageNet (Russakovsky et al. (2014)). The Long Short Term Memory model uses the CNN features as inputs and it is, for best performance, pre-trained on other two large image datasets Flickr30k (Young et al. (2014)) and Microsoft COCO 2014 (Lin et al.", "startOffset": 62, "endOffset": 265}, {"referenceID": 17, "context": "(2014)) and Microsoft COCO 2014 (Lin et al. (2014)), then fine-tuned on the video dataset used in experiments.", "startOffset": 33, "endOffset": 51}, {"referenceID": 17, "context": "(2014)) and Microsoft COCO 2014 (Lin et al. (2014)), then fine-tuned on the video dataset used in experiments. One limitation of this approach (Venugopalan et al. (2014)) is that its video representation exploits only weakly temporal information, as it accumulates CNN feature responses over time by mean pooling.", "startOffset": 33, "endOffset": 170}, {"referenceID": 17, "context": "(2014)) and Microsoft COCO 2014 (Lin et al. (2014)), then fine-tuned on the video dataset used in experiments. One limitation of this approach (Venugopalan et al. (2014)) is that its video representation exploits only weakly temporal information, as it accumulates CNN feature responses over time by mean pooling. While the previously mentioned works explore language modeling on a fixed visual model, the work of Xu et al. (2015) goes a step further by combining a compositional semantics language model with a deep video model into a single unified framework.", "startOffset": 33, "endOffset": 431}, {"referenceID": 13, "context": "The visual cues we start from are pre-trained CNN features from the superior, second to last fully connected (fc7) layer of (Jia et al. (2014)).", "startOffset": 125, "endOffset": 143}, {"referenceID": 32, "context": "In different work, Yao et al. (2015) addresses the limitations of Venugopalan et al.", "startOffset": 19, "endOffset": 37}, {"referenceID": 31, "context": "(2015) addresses the limitations of Venugopalan et al. (2014) by employing a 3-D convnet model (which is pre-trained on different video datatsets of action recognition) in order to incorporate spatiotemporal motion features.", "startOffset": 36, "endOffset": 62}, {"referenceID": 31, "context": "(2015) addresses the limitations of Venugopalan et al. (2014) by employing a 3-D convnet model (which is pre-trained on different video datatsets of action recognition) in order to incorporate spatiotemporal motion features. For capturing changes and movement, they extract dense trajectory features over the video. They also include an attention mechanism that learns to weight the features non-uniformly over frames, rather than blandly averaging them. We propose a simpler approach for exploiting temporal consistency in a video. Instead of using a complex LSTM model, we take advantage of co-firing of different visual classifiers, at different relative temporal sections in the video. Different from Yao et al. (2015) we do not use features that ar specific for motion, such as cues computed from optical flow.", "startOffset": 36, "endOffset": 723}, {"referenceID": 31, "context": "(2015) addresses the limitations of Venugopalan et al. (2014) by employing a 3-D convnet model (which is pre-trained on different video datatsets of action recognition) in order to incorporate spatiotemporal motion features. For capturing changes and movement, they extract dense trajectory features over the video. They also include an attention mechanism that learns to weight the features non-uniformly over frames, rather than blandly averaging them. We propose a simpler approach for exploiting temporal consistency in a video. Instead of using a complex LSTM model, we take advantage of co-firing of different visual classifiers, at different relative temporal sections in the video. Different from Yao et al. (2015) we do not use features that ar specific for motion, such as cues computed from optical flow. All our features are trained on a per static image basis, such that accurate verb predictions are the result of the visual context from the scene and from the other classes present. Our relatively simpler model, proves its effectiveness and generally outperforms our competitors by an important margin on a large video dataset (Chen & Dolan (2011)).", "startOffset": 36, "endOffset": 1164}, {"referenceID": 13, "context": "We address the porblem of linguistic video interpretation by first creating classifiers to predict the different concepts, subject, object and verb from pure visual information that we extract from deep AlexNet-like CNNs pre-trained on the large ILRSVC-2012 image classification dataset (Jia et al. (2014); Krizhevsky et al.", "startOffset": 288, "endOffset": 306}, {"referenceID": 13, "context": "We address the porblem of linguistic video interpretation by first creating classifiers to predict the different concepts, subject, object and verb from pure visual information that we extract from deep AlexNet-like CNNs pre-trained on the large ILRSVC-2012 image classification dataset (Jia et al. (2014); Krizhevsky et al. (2011)).", "startOffset": 288, "endOffset": 332}, {"referenceID": 0, "context": "For detecting foreground regions in images related ideas can be found in the works of (Borji et al. (2012); Cheng", "startOffset": 87, "endOffset": 107}, {"referenceID": 6, "context": "(2015); Hou & Zhang (2007)), whereas in video the supervised selection of key frames is known to be very effective for action and activity classification (Ellis et al. (August 2012); Lv & Nevaita (2007); Carlsson & Sullivan (2001); Zanfir et al.", "startOffset": 155, "endOffset": 203}, {"referenceID": 6, "context": "(2015); Hou & Zhang (2007)), whereas in video the supervised selection of key frames is known to be very effective for action and activity classification (Ellis et al. (August 2012); Lv & Nevaita (2007); Carlsson & Sullivan (2001); Zanfir et al.", "startOffset": 155, "endOffset": 231}, {"referenceID": 6, "context": "(2015); Hou & Zhang (2007)), whereas in video the supervised selection of key frames is known to be very effective for action and activity classification (Ellis et al. (August 2012); Lv & Nevaita (2007); Carlsson & Sullivan (2001); Zanfir et al. (2013)).", "startOffset": 155, "endOffset": 253}, {"referenceID": 6, "context": "(2015); Hou & Zhang (2007)), whereas in video the supervised selection of key frames is known to be very effective for action and activity classification (Ellis et al. (August 2012); Lv & Nevaita (2007); Carlsson & Sullivan (2001); Zanfir et al. (2013)). The method proposed here is unsupervised and more related to the VideoPCA approach for discovery of foreground regions in video (Stretcu & Leordeanu (2015)).", "startOffset": 155, "endOffset": 411}, {"referenceID": 19, "context": "The importance of global scene context was recognized in computer vision for a long time (Oliva & Torralba (2001)), while recent findings in psychology suggest that contextual scene information is crucial for recognition, independent of the attentional focus Munneke et al. (2013). This suggests that scene recognition happens before the detection of individual objects.", "startOffset": 259, "endOffset": 281}, {"referenceID": 10, "context": "Selecting in an unsupervised manner potentially interesting regions and features is also related to approaches in object recognition, in which segmentation cues are used in order to guide the process of attention (Girshick et al. (2014)).", "startOffset": 214, "endOffset": 237}, {"referenceID": 11, "context": "We use the same training and testing split as in previous works: there are 1299 training videos and 671 testing videos, as in (Guadarrama et al. 2013).", "startOffset": 126, "endOffset": 150}], "year": 2015, "abstractText": "Integrating higher level visual and linguistic interpretations is at the heart of human intelligence. As automatic visual category recognition in images is approaching human performance, the high level understanding in the dynamic spatiotemporal domain of videos and its translation into natural language is still far from being solved. While most works on vision-to-text translations use pre-learned or pre-established computational linguistic models, in this paper we present an approach that uses vision alone to efficiently learn how to translate into language the video content. We discover, in simple form, the story played by main actors, while using only visual cues for representing objects and their interactions. Our method learns in a hierarchical manner higher level representations for recognizing subjects, actions and objects involved, their relevant contextual background and their interaction to one another over time. We have a three stage approach: first we take in consideration features of the individual entities at the local level of appearance, then we consider the relationship between these objects and actions and their video background, and third, we consider their spatiotemporal relations as inputs to classifiers at the highest level of interpretation. Thus, our approach finds a coherent linguistic description of videos in the form of a subject, verb and object based on their role played in the overall visual story learned directly from training data, without using a known language model. We test the efficiency of our approach on a large scale dataset containing YouTube clips taken in the wild and demonstrate state-of-the-art performance, often superior to current approaches that use more complex, pre-learned linguistic knowledge.", "creator": "LaTeX with hyperref package"}}}