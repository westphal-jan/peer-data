{"id": "1511.06811", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "21-Nov-2015", "title": "Learning visual groups from co-occurrences in space and time", "abstract": "We propose a self-supervised framework that learns to group visual entities based on their rate of co-occurrence in space and time. To model statistical dependencies between the entities, we set up a simple binary classification problem in which the goal is to predict if two visual primitives occur in the same spatial or temporal context. We apply this framework to three domains: learning patch affinities from spatial adjacency in images, learning frame affinities from temporal adjacency in videos, and learning photo affinities from geospatial proximity in image collections. We demonstrate that in each case the learned affinities uncover meaningful semantic groupings. From patch affinities we generate object proposals that are competitive with state-of-the-art supervised methods. From frame affinities we generate movie scene segmentations that correlate well with DVD chapter structure. Finally, from geospatial affinities we learn groups that relate well to semantic place categories.", "histories": [["v1", "Sat, 21 Nov 2015 01:33:12 GMT  (2322kb,D)", "http://arxiv.org/abs/1511.06811v1", null]], "reviews": [], "SUBJECTS": "cs.LG cs.CV", "authors": ["phillip isola", "daniel zoran", "dilip krishnan", "edward h adelson"], "accepted": false, "id": "1511.06811"}, "pdf": {"name": "1511.06811.pdf", "metadata": {"source": "CRF", "title": null, "authors": ["Phillip Isola", "Dilip Krishnan"], "emails": ["phillipi@berkeley.edu", "danielz@mit.edu", "dilipkay@google.com", "adelson@mit.edu"], "sections": [{"heading": null, "text": "To model statistical dependencies between entities, we set up a simple problem of binary classification, in which the goal is to predict whether two visual primitives occur in the same spatial or temporal context. We apply this framework to three areas: learning patch affinities from spatial neighborhood in images, learning frame affinities from temporal neighborhood in videos, and learning photo affinities from spatial neighborhood in image collections. We show that the learned affinities always reveal significant semantic groupings. From patch affinities, we generate object suggestions that compete with the most advanced supervised methods. From frame affinities, we generate film scene segmentations that correlate well with the chapter structure of DVDs. Finally, we learn from spatial affinities groups that relate well to semantic categories."}, {"heading": "1 INTRODUCTION", "text": "When we look at the world around us, we constantly notice which things are related to which. These associations allow us to segment and organize the world into coherent visual representations.This paper addresses the question of how representations such as \"objects\" and \"scenes\" can be learned from the natural visual experience. However, in the absence of knowledgeable comments, it remains unclear how we can even uncover these representations. Do \"objects\" come directly from the statistics of the environment, or are they a more subjective, human construct? Here we examine the earlier hypothesis. As the physical world is highly structured, it remains unclear how we can even uncover these representations."}, {"heading": "2 RELATED WORK", "text": "The idea that perception groups reflect statistical structures in the environment has deep roots in perception and cognitive literature (Barlow (1985); Wilkin & Tenenbaum (1985); Tenenbaum & Witkin (1983); Lowe (2012); Malinick (1983))).Barlow postulated that the brain is constantly on the lookout for events that occur much more frequently than randomly, and uses these \"suspicious coincidences\" to discover the underlying structure in the world (Barlow (1985).Subsequent researchers argued that children learn from phonemer co-occurrence (Saffran et al al al al), and that humans can also pick up visual patterns from co-occurrence statistics (Fiser & Aslin (2001); Schapiro et al al al al al al al al. (2013).Visual grouping is also a central problem in computer vision that is reflected in the tasks of Edge / Contour Detection 1986 (Fiser & Aslin); Schapiro et al al al al al al al (2011)."}, {"heading": "3 MODELING VISUAL AFFINITIES BY PREDICTING CO-OCCURRENCE", "text": "We want to group the visual primitives, A and B, based on the probability that they belong to the same semantic entity. (For example, A and B can be two image fields in which we want to group them if they belong to the same visual object. (2013) During this paper, we use the property that A and B share the same semantic labels. (0, 1} where Q = 1 iff A and B are on the same object. (2013) During this paper, we use Q to indicate the property that A and B share the same semantic labels. (A and B may need the training data for Q.) Instead, we will explore an alternative strategy to predict the Q directly."}, {"heading": "3.1 PREDICTING CO-OCCURRENCES WITH A CNN", "text": "To model w (A, B), we use a revolutionary neural network (CNN) with a Siamese architecture (Figure 2, Chopra et al. (2005)), which we implement in Caffe (Jia et al. (2014). The network has two revolutionary branches, one for processing A and the other for processing B. These branches can be considered as trait extractors, which are then concatenated and fed to a series of fully interconnected layers that compare the characteristics and try to predict C. We use a logistic loss to C and train all models with stochastic gradient lineage. Our goal can be expressed as E (A, B, C; \u03b8) = \u2212 1 N \u00b2 1 Ci log (f (Ai, Bi; \u03b8; 1 Ci) + (1 \u2212 Ci) log (1 \u2212 Ci) log (1 \u2212 Qi) log (1 \u2212 Qi) and 1 (f (Af, Bi) GOP)."}, {"heading": "3.2 PATCH AFFINITIES FROM CO-OCCURRENCE IN IMAGES", "text": "First, we examine whether predicting patch co-occurrence in images leads to an effective affinity measurement Q = Q for object-level grouping. We set A and B to 17 x 17 pixel fields (with circular masks). Context function C is spatial neighbourhood. Positive examples (C = 1) are pairs of adjacent patches (without overlapping between them) and negative examples (C = 0) are pairs sampled from random locations within the dataset. We try training patches from the Pascal VOC 2012 training set.We train a CNN (two contiguous layers, two completely connected layers; Figure 2) to model P (C = 1, B). Figure 3 shows a t-SNE visualization of learned affinities (van der Maaten & Hinton1 In each of our applications we trained 50% positive (C = 1) and 50% negative (C = 0) examples."}, {"heading": "3.3 FRAME AFFINITIES FROM CO-OCCURRENCE IN MOVIES", "text": "To test this, we put A and B on frames cut to 33 x 33 pixels and scanned downwards from a series of 96 films scanned from the 100 best rated movies on IMDB2. In this setting, C indicates the temporal neighborhood - specifically 2http: / / www.imdb.com / two frames are assigned to C = 1 if they are at least 3 seconds apart and no more than 10 seconds apart. Otherwise, C = 0. Again, we train a CNN to model P (C = 1 | A, B) (three revolutionary layers, two fully connected layers). To evaluate the prediction of C, we train half of the movies and test the remaining half. Our method can learn to predict C fairly effectively by achieving an average precision of 0.95 on the test set. How do the learned temporal relationships relate to semantic visual scenes? To test this, we compare it with the comments on the DVD, we can easily put the 67 \"Q-Q pairs on DVD."}, {"heading": "3.4 PHOTO AFFINITIES FROM GEOSPATIAL CO-OCCURRENCE", "text": "Just as an object is a collection of associated patches, and a movie scene is a collection of associated frames, a visual place is a collection of associated photos. Here, we specify A and B to be geotagged photos cut to 33 x 33 pixels and sampled down, and C shows whether A and B are taken within 11 meters of each other or not (we exclude exact duplicate locations), using the same CNN architecture as for the film frame network, we learn P (C = 1 | A, B) again, but for this new setting of variables. We train on five cities selected from the MIT City Database Zhou et al. (2014) and test C's prediction on a set of three other cities from that dataset. We also test how well the network predicts semantics, defining Q as \"these two photos belong to the same location category.\""}, {"heading": "3.5 WHICH CUES DID THE NETWORKS LEARN TO USE?", "text": "Similar to how a psychophysicist might experiment with human perception, we show our networks custom-made stimuli. For each test, we feed the networks with many pairs {A, B}, which are scanned from places like C = 1. We leave A unchanged but modify B in a controlled way. This allows us to test what types of transformations of B will change the network's prediction of whether or not C = 1 (cf. Lenc & Vedaldi (2014)). We consider the following changes: rotation by 90 \u043c, vertical and horizontal reflection, removal of color (by replacing each pixel with its mean via color channels), and a darkening transformation in which we multiply each color channel by 0.5. The results of these tests are shown in Table 2. Each number is the mean output of the network for a given case."}, {"heading": "4 FROM PREDICTING CO-OCCURRENCE TO FORMING VISUAL GROUPS", "text": "We apply the following general approach to learning visual groups: 1. Define A, B, and C based on the domain. 2. Learn P (C | A, B) with a CNN. 3. Put a graph in which ANi = 1 and B are N i = 1 nodes and edge weights are given by w (Ai, Bi) (Equation 1)."}, {"heading": "4.1 FINDING OBJECTS", "text": "We focus on the specific problem of \"object suggestions\" (Zitnick & Dolla \u0301 r (2014); Krahnenbuhl & Koltun (2015)), with the goal of localizing all objects in an image. We use the patch associations P (C = 1 | A, B) defined in Section 4, trained on the Pascal VOC 2012 training kit. Following previous benchmarks, we test using the validation set. In the face of a test image, we sample all 17 x 17 patches with a spacing of 8 pixels. We construct a graph in which each patch is a node and node connected by an edge."}, {"heading": "4.2 SEGMENTING MOVIES", "text": "Just as objects are composed of associated areas, scenes in a film are composed of associated images. Here, we show how our learned frame affinities can be used to split a film into coherent scenes, a problem to which some attention has previously been paid (Chen et al. (2008); Zhai & Shah (2006)). To segment a film, we create a graph in which each frame is a node and all frames are connected by an edge within ten seconds of each other. Then, we weigh the edges using the fractions P (C = 1 | A, B) (section 4) and divide the graph using spectral clustering. To evaluate this, we use annotations from the DVD chapter as a basic truth. Following a standardized evaluation procedure for recognizing image boundaries (Arbelaez et al. (2011)), we measure performance on the retrieval task of finding all boundaries of truth."}, {"heading": "4.3 DISCOVERING PLACE CATEGORIES", "text": "Taking into account the geoassociation model from Section 4, we group photos into coherent types of locations. Here, we create a fully connected graph between all photos of a particular collection, weighting the edges with P (C = 1 | A, B), and then applying spectral clustering to divide the collection. We test the purity of the clusters on LabelMe Outdoors dataset (Liu et al. (2009)). Cluster purity versus number of clusters k is given in Figure 7 (right), which shows that our method is effective in detecting semantic space categories. As in our film segmentation experiments, we select the optimal \u03b1 to scale the affinity of our method as well as each baseline. Figure 7 (left) shows random sample images of each cluster by cluster in 8 categories. This cluster formation has a purity of 59%."}, {"heading": "5 CONCLUSION", "text": "We have presented a simple and general approach to learning visual groupings that does not require predefined labels. Instead, our framework uses the juxtaposition of space and time as a control signal, allowing us to learn different cluster mechanisms for a variety of tasks. Our approach achieves competitive results in creating object suggestions, even compared to methods trained on the basis of marked data. Furthermore, we have shown that the same method can be used to segment movies into scenes and uncover semantic place categories. The principles underlying the framework are fairly general and can apply to data in other areas when natural coexistence signals and groups.3http: / / moviebarcode.tumblr.com /"}, {"heading": "ACKNOWLEDGMENTS", "text": "We thank William T. Freeman, Joshua B. Tenenbaum and Alexei A. Efros for helpful feedback and discussion. Thanks to Andrew Owens for his help in collecting the film record. This work is supported by NSF Prize 1161731, Understanding Translucency and Shell Research."}], "references": [{"title": "Learning to see by moving", "author": ["Agrawal", "Pulkit", "Carreira", "Joao", "Malik", "Jitendra"], "venue": "arXiv preprint arXiv:1505.01596,", "citeRegEx": "Agrawal et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Agrawal et al\\.", "year": 2015}, {"title": "Measuring the objectness of image windows", "author": ["Alexe", "Bogdan", "Deselaers", "Thomas", "Ferrari", "Vittorio"], "venue": null, "citeRegEx": "Alexe et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Alexe et al\\.", "year": 2012}, {"title": "Contour detection and hierarchical image segmentation", "author": ["Arbelaez", "Pablo", "Maire", "Michael", "Fowlkes", "Charless", "Malik", "Jitendra"], "venue": null, "citeRegEx": "Arbelaez et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Arbelaez et al\\.", "year": 2011}, {"title": "Cerebral cortex as model builder", "author": ["Barlow", "Horace"], "venue": "Models of the visual cortex, pp", "citeRegEx": "Barlow and Horace.,? \\Q1985\\E", "shortCiteRegEx": "Barlow and Horace.", "year": 1985}, {"title": "A computational approach to edge detection", "author": ["Canny", "John"], "venue": "PAMI, (6):679\u2013698,", "citeRegEx": "Canny and John.,? \\Q1986\\E", "shortCiteRegEx": "Canny and John.", "year": 1986}, {"title": "Movie scene segmentation using background information", "author": ["Chen", "Liang-Hua", "Lai", "Yu-Chun", "Liao", "Hong-Yuan Mark"], "venue": "Pattern Recognition,", "citeRegEx": "Chen et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Chen et al\\.", "year": 2008}, {"title": "Bing: Binarized normed gradients for objectness estimation at 300fps", "author": ["Cheng", "Ming-Ming", "Zhang", "Ziming", "Lin", "Wen-Yan", "Torr", "Philip"], "venue": "In CVPR,", "citeRegEx": "Cheng et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Cheng et al\\.", "year": 2014}, {"title": "Learning a similarity metric discriminatively, with application to face verification", "author": ["Chopra", "Sumit", "Hadsell", "Raia", "LeCun", "Yann"], "venue": "In Computer Vision and Pattern Recognition,", "citeRegEx": "Chopra et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Chopra et al\\.", "year": 2005}, {"title": "Word association norms, mutual information, and lexicography", "author": ["Church", "Kenneth Ward", "Hanks", "Patrick"], "venue": "Computational linguistics,", "citeRegEx": "Church et al\\.,? \\Q1990\\E", "shortCiteRegEx": "Church et al\\.", "year": 1990}, {"title": "Histograms of oriented gradients for human detection", "author": ["Dalal", "Navneet", "Triggs", "Bill"], "venue": "In CVPR,", "citeRegEx": "Dalal et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Dalal et al\\.", "year": 2005}, {"title": "Unsupervised visual representation learning by context prediction", "author": ["Doersch", "Carl", "Gupta", "Abhinav", "Efros", "Alexei A"], "venue": "CoRR, abs/1505.05192,", "citeRegEx": "Doersch et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Doersch et al\\.", "year": 2015}, {"title": "Structured forests for fast edge detection", "author": ["Doll\u00e1r", "Piotr", "Zitnick", "C Lawrence"], "venue": "In ICCV,", "citeRegEx": "Doll\u00e1r et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Doll\u00e1r et al\\.", "year": 2013}, {"title": "Clustering by composition\u2013unsupervised discovery of image categories", "author": ["Faktor", "Alon", "Irani", "Michal"], "venue": "In ECCV", "citeRegEx": "Faktor et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Faktor et al\\.", "year": 2012}, {"title": "Co-segmentation by composition", "author": ["Faktor", "Alon", "Irani", "Michal"], "venue": "In ICCV,", "citeRegEx": "Faktor et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Faktor et al\\.", "year": 2013}, {"title": "Unsupervised statistical learning of higher-order spatial structures from visual scenes", "author": ["Fiser", "J\u00f3zsef", "Aslin", "Richard N"], "venue": "Psychological science,", "citeRegEx": "Fiser et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Fiser et al\\.", "year": 2001}, {"title": "Crisp boundary detection using pointwise mutual information", "author": ["Isola", "Phillip", "Zoran", "Daniel", "Krishnan", "Dilip", "Adelson", "Edward H"], "venue": "In ECCV,", "citeRegEx": "Isola et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Isola et al\\.", "year": 2014}, {"title": "Learning image representations equivariant to ego-motion", "author": ["Jayaraman", "Dinesh", "Grauman", "Kristen"], "venue": "arXiv preprint arXiv:1505.02206,", "citeRegEx": "Jayaraman et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Jayaraman et al\\.", "year": 2015}, {"title": "Caffe: Convolutional architecture for fast feature embedding", "author": ["Jia", "Yangqing", "Shelhamer", "Evan", "Donahue", "Jeff", "Karayev", "Sergey", "Long", "Jonathan", "Girshick", "Ross", "Guadarrama", "Sergio", "Darrell", "Trevor"], "venue": "In Proceedings of the ACM International Conference on Multimedia,", "citeRegEx": "Jia et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Jia et al\\.", "year": 2014}, {"title": "Extracting slow subspaces from natural videos leads to complex cells", "author": ["Kayser", "Christoph", "Einh\u00e4user", "Wolfgang", "D\u00fcmmer", "Olaf", "K\u00f6nig", "Peter", "K\u00f6rding", "Konrad"], "venue": "In Artificial Neural Networks?ICANN", "citeRegEx": "Kayser et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Kayser et al\\.", "year": 2001}, {"title": "Logistic regression in rare events data", "author": ["King", "Gary", "Zeng", "Langche"], "venue": "Political analysis,", "citeRegEx": "King et al\\.,? \\Q2001\\E", "shortCiteRegEx": "King et al\\.", "year": 2001}, {"title": "Geodesic object proposals", "author": ["Krahenbuhl", "Philipp", "Koltun", "Vladlen"], "venue": "In ECCV", "citeRegEx": "Krahenbuhl et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Krahenbuhl et al\\.", "year": 2014}, {"title": "Learning to propose objects", "author": ["Krahnenbuhl", "Phillip", "Koltun", "Vladlen"], "venue": "In CVPR,", "citeRegEx": "Krahnenbuhl et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Krahnenbuhl et al\\.", "year": 2015}, {"title": "Understanding image representations by measuring their equivariance and equivalence", "author": ["Lenc", "Karel", "Vedaldi", "Andrea"], "venue": "arXiv preprint arXiv:1411.5908,", "citeRegEx": "Lenc et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Lenc et al\\.", "year": 2014}, {"title": "Linguistic regularities in sparse and explicit word representations", "author": ["Levy", "Omer", "Goldberg", "Yoav", "Ramat-Gan", "Israel"], "venue": null, "citeRegEx": "Levy et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Levy et al\\.", "year": 2014}, {"title": "Nonparametric scene parsing: Label transfer via dense scene alignment", "author": ["Liu", "Ce", "Yuen", "Jenny", "Torralba", "Antonio"], "venue": "In CVPR,", "citeRegEx": "Liu et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Liu et al\\.", "year": 2009}, {"title": "Perceptual organization and visual recognition, volume", "author": ["Lowe", "David"], "venue": "Science & Business Media,", "citeRegEx": "Lowe and David.,? \\Q2012\\E", "shortCiteRegEx": "Lowe and David.", "year": 2012}, {"title": "Improving spatial support for objects via multiple segmentations", "author": ["Malisiewicz", "Tomasz", "Efros", "Alexei A"], "venue": "In BMVC,", "citeRegEx": "Malisiewicz et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Malisiewicz et al\\.", "year": 2007}, {"title": "Prime object proposals with randomized prim\u2019s algorithm", "author": ["Manen", "Santiago", "Guillaumin", "Matthieu", "Gool", "Luc Van"], "venue": "In ICCV,", "citeRegEx": "Manen et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Manen et al\\.", "year": 2013}, {"title": "Deep learning from temporal coherence in video", "author": ["Mobahi", "Hossein", "Collobert", "Ronan", "Weston", "Jason"], "venue": null, "citeRegEx": "Mobahi et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Mobahi et al\\.", "year": 2009}, {"title": "The logic of perception", "author": ["Rock", "Irvin"], "venue": null, "citeRegEx": "Rock and Irvin.,? \\Q1983\\E", "shortCiteRegEx": "Rock and Irvin.", "year": 1983}, {"title": "Statistical learning by 8-month-old infants", "author": ["Saffran", "Jenny R", "Aslin", "Richard N", "Newport", "Elissa L"], "venue": null, "citeRegEx": "Saffran et al\\.,? \\Q1926\\E", "shortCiteRegEx": "Saffran et al\\.", "year": 1926}, {"title": "Neural representations of events arise from temporal community structure", "author": ["Schapiro", "Anna C", "Rogers", "Timothy T", "Cordova", "Natalia I", "Turk-Browne", "Nicholas B", "Botvinick", "Matthew M"], "venue": "Nature Neuroscience,", "citeRegEx": "Schapiro et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Schapiro et al\\.", "year": 2013}, {"title": "Normalized cuts and image", "author": ["Shi", "Jianbo", "Malik", "Jitendra"], "venue": "segmentation. PAMI,", "citeRegEx": "Shi et al\\.,? \\Q2000\\E", "shortCiteRegEx": "Shi et al\\.", "year": 2000}, {"title": "Discovering objects and their location in images", "author": ["Sivic", "Josef", "Russell", "Bryan C", "Efros", "Alexei", "Zisserman", "Andrew", "Freeman", "William T"], "venue": "In Computer Vision,", "citeRegEx": "Sivic et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Sivic et al\\.", "year": 2005}, {"title": "Unsupervised learning of video representations using lstms", "author": ["Srivastava", "Nitish", "Mansimov", "Elman", "Salakhutdinov", "Ruslan"], "venue": "arXiv preprint arXiv:1502.04681,", "citeRegEx": "Srivastava et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Srivastava et al\\.", "year": 2015}, {"title": "On the role of structure in vision", "author": ["Tenenbaum", "Jay M", "Witkin", "AP"], "venue": "Human and machine vision,", "citeRegEx": "Tenenbaum et al\\.,? \\Q1983\\E", "shortCiteRegEx": "Tenenbaum et al\\.", "year": 1983}, {"title": "Selective search for object recognition", "author": ["Uijlings", "Jasper RR", "van de Sande", "Koen EA", "Gevers", "Theo", "Smeulders", "Arnold WM"], "venue": null, "citeRegEx": "Uijlings et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Uijlings et al\\.", "year": 2013}, {"title": "Visualizing high-dimensional data using t-sne", "author": ["L.J.P. van der Maaten", "G.E. Hinton"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Maaten and Hinton,? \\Q2009\\E", "shortCiteRegEx": "Maaten and Hinton", "year": 2009}, {"title": "Unsupervised learning of visual representations using videos", "author": ["Wang", "Xiaolong", "Gupta", "Abhinav"], "venue": "arXiv preprint arXiv:1505.00687,", "citeRegEx": "Wang et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2015}, {"title": "What is perceptual organization for", "author": ["Wilkin", "Andrew P", "Tenenbaum", "Jay M"], "venue": "From Pixels to Predicates,", "citeRegEx": "Wilkin et al\\.,? \\Q1985\\E", "shortCiteRegEx": "Wilkin et al\\.", "year": 1985}, {"title": "Slow feature analysis: Unsupervised learning of invariances", "author": ["Wiskott", "Laurenz", "Sejnowski", "Terrence J"], "venue": "Neural computation,", "citeRegEx": "Wiskott et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Wiskott et al\\.", "year": 2002}, {"title": "Video scene segmentation using markov chain monte carlo", "author": ["Zhai", "Yun", "Shah", "Mubarak"], "venue": "Multimedia, IEEE Transactions on,", "citeRegEx": "Zhai et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Zhai et al\\.", "year": 2006}, {"title": "Recognizing City Identity via Attribute Analysis of Geo-tagged Images", "author": ["B. Zhou", "Liu", "A. Oliva", "A. Torralba"], "venue": null, "citeRegEx": "Zhou et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Zhou et al\\.", "year": 2014}, {"title": "Edge boxes: Locating object proposals from edges", "author": ["Zitnick", "C Lawrence", "Doll\u00e1r", "Piotr"], "venue": "In ECCV", "citeRegEx": "Zitnick et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Zitnick et al\\.", "year": 2014}], "referenceMentions": [{"referenceID": 25, "context": "Subsequent researchers have argued that infants learn about linguistic groups from phoneme co-occurrence (Saffran et al. (1996)), and that humans may also pick up on visual patterns from co-occurrence statistics alone (Fiser & Aslin (2001); Schapiro et al.", "startOffset": 106, "endOffset": 128}, {"referenceID": 25, "context": "Subsequent researchers have argued that infants learn about linguistic groups from phoneme co-occurrence (Saffran et al. (1996)), and that humans may also pick up on visual patterns from co-occurrence statistics alone (Fiser & Aslin (2001); Schapiro et al.", "startOffset": 106, "endOffset": 240}, {"referenceID": 25, "context": "Subsequent researchers have argued that infants learn about linguistic groups from phoneme co-occurrence (Saffran et al. (1996)), and that humans may also pick up on visual patterns from co-occurrence statistics alone (Fiser & Aslin (2001); Schapiro et al. (2013)).", "startOffset": 106, "endOffset": 264}, {"referenceID": 25, "context": "Subsequent researchers have argued that infants learn about linguistic groups from phoneme co-occurrence (Saffran et al. (1996)), and that humans may also pick up on visual patterns from co-occurrence statistics alone (Fiser & Aslin (2001); Schapiro et al. (2013)). Visual grouping is also a central problem in computer vision, showing up in the tasks of edge/contour detection Canny (1986); Arbelaez et al.", "startOffset": 106, "endOffset": 391}, {"referenceID": 0, "context": "Visual grouping is also a central problem in computer vision, showing up in the tasks of edge/contour detection Canny (1986); Arbelaez et al. (2011); Isola et al.", "startOffset": 126, "endOffset": 149}, {"referenceID": 0, "context": "Visual grouping is also a central problem in computer vision, showing up in the tasks of edge/contour detection Canny (1986); Arbelaez et al. (2011); Isola et al. (2014) , (semantic) segmentation Shi & Malik (2000); Malisiewicz & Efros (2007), and object proposals Alexe et al.", "startOffset": 126, "endOffset": 170}, {"referenceID": 0, "context": "Visual grouping is also a central problem in computer vision, showing up in the tasks of edge/contour detection Canny (1986); Arbelaez et al. (2011); Isola et al. (2014) , (semantic) segmentation Shi & Malik (2000); Malisiewicz & Efros (2007), and object proposals Alexe et al.", "startOffset": 126, "endOffset": 215}, {"referenceID": 0, "context": "Visual grouping is also a central problem in computer vision, showing up in the tasks of edge/contour detection Canny (1986); Arbelaez et al. (2011); Isola et al. (2014) , (semantic) segmentation Shi & Malik (2000); Malisiewicz & Efros (2007), and object proposals Alexe et al.", "startOffset": 126, "endOffset": 243}, {"referenceID": 0, "context": "(2014) , (semantic) segmentation Shi & Malik (2000); Malisiewicz & Efros (2007), and object proposals Alexe et al. (2012); Zitnick & Doll\u00e1r (2014); Krahnenbuhl & Koltun (2015), among others.", "startOffset": 102, "endOffset": 122}, {"referenceID": 0, "context": "(2014) , (semantic) segmentation Shi & Malik (2000); Malisiewicz & Efros (2007), and object proposals Alexe et al. (2012); Zitnick & Doll\u00e1r (2014); Krahnenbuhl & Koltun (2015), among others.", "startOffset": 102, "endOffset": 147}, {"referenceID": 0, "context": "(2014) , (semantic) segmentation Shi & Malik (2000); Malisiewicz & Efros (2007), and object proposals Alexe et al. (2012); Zitnick & Doll\u00e1r (2014); Krahnenbuhl & Koltun (2015), among others.", "startOffset": 102, "endOffset": 176}, {"referenceID": 0, "context": "(2014) , (semantic) segmentation Shi & Malik (2000); Malisiewicz & Efros (2007), and object proposals Alexe et al. (2012); Zitnick & Doll\u00e1r (2014); Krahnenbuhl & Koltun (2015), among others. Many papers in this field take the approach of first modeling the affinity between visual elements, then grouping elements with high affinity (e.g., Shi & Malik (2000)).", "startOffset": 102, "endOffset": 359}, {"referenceID": 0, "context": "(2014) , (semantic) segmentation Shi & Malik (2000); Malisiewicz & Efros (2007), and object proposals Alexe et al. (2012); Zitnick & Doll\u00e1r (2014); Krahnenbuhl & Koltun (2015), among others. Many papers in this field take the approach of first modeling the affinity between visual elements, then grouping elements with high affinity (e.g., Shi & Malik (2000)). Our work follows this approach. However, rather than using a hand-engineered grouping cue Shi & Malik (2000); Zitnick & Doll\u00e1r (2014), or learning to group with direct supervision Doll\u00e1r & Zitnick (2013); Krahnenbuhl & Koltun (2015), we use a measure of spatial and temporal dependence as the affinity.", "startOffset": 102, "endOffset": 470}, {"referenceID": 0, "context": "(2014) , (semantic) segmentation Shi & Malik (2000); Malisiewicz & Efros (2007), and object proposals Alexe et al. (2012); Zitnick & Doll\u00e1r (2014); Krahnenbuhl & Koltun (2015), among others. Many papers in this field take the approach of first modeling the affinity between visual elements, then grouping elements with high affinity (e.g., Shi & Malik (2000)). Our work follows this approach. However, rather than using a hand-engineered grouping cue Shi & Malik (2000); Zitnick & Doll\u00e1r (2014), or learning to group with direct supervision Doll\u00e1r & Zitnick (2013); Krahnenbuhl & Koltun (2015), we use a measure of spatial and temporal dependence as the affinity.", "startOffset": 102, "endOffset": 495}, {"referenceID": 0, "context": "(2014) , (semantic) segmentation Shi & Malik (2000); Malisiewicz & Efros (2007), and object proposals Alexe et al. (2012); Zitnick & Doll\u00e1r (2014); Krahnenbuhl & Koltun (2015), among others. Many papers in this field take the approach of first modeling the affinity between visual elements, then grouping elements with high affinity (e.g., Shi & Malik (2000)). Our work follows this approach. However, rather than using a hand-engineered grouping cue Shi & Malik (2000); Zitnick & Doll\u00e1r (2014), or learning to group with direct supervision Doll\u00e1r & Zitnick (2013); Krahnenbuhl & Koltun (2015), we use a measure of spatial and temporal dependence as the affinity.", "startOffset": 102, "endOffset": 565}, {"referenceID": 0, "context": "(2014) , (semantic) segmentation Shi & Malik (2000); Malisiewicz & Efros (2007), and object proposals Alexe et al. (2012); Zitnick & Doll\u00e1r (2014); Krahnenbuhl & Koltun (2015), among others. Many papers in this field take the approach of first modeling the affinity between visual elements, then grouping elements with high affinity (e.g., Shi & Malik (2000)). Our work follows this approach. However, rather than using a hand-engineered grouping cue Shi & Malik (2000); Zitnick & Doll\u00e1r (2014), or learning to group with direct supervision Doll\u00e1r & Zitnick (2013); Krahnenbuhl & Koltun (2015), we use a measure of spatial and temporal dependence as the affinity.", "startOffset": 102, "endOffset": 594}, {"referenceID": 0, "context": "(2014) , (semantic) segmentation Shi & Malik (2000); Malisiewicz & Efros (2007), and object proposals Alexe et al. (2012); Zitnick & Doll\u00e1r (2014); Krahnenbuhl & Koltun (2015), among others. Many papers in this field take the approach of first modeling the affinity between visual elements, then grouping elements with high affinity (e.g., Shi & Malik (2000)). Our work follows this approach. However, rather than using a hand-engineered grouping cue Shi & Malik (2000); Zitnick & Doll\u00e1r (2014), or learning to group with direct supervision Doll\u00e1r & Zitnick (2013); Krahnenbuhl & Koltun (2015), we use a measure of spatial and temporal dependence as the affinity. Grouping based on co-occurrence has received some prior attention in computer vision (Sivic et al. (2005); Faktor & Irani (2012; 2013); Isola et al.", "startOffset": 102, "endOffset": 770}, {"referenceID": 0, "context": "(2014) , (semantic) segmentation Shi & Malik (2000); Malisiewicz & Efros (2007), and object proposals Alexe et al. (2012); Zitnick & Doll\u00e1r (2014); Krahnenbuhl & Koltun (2015), among others. Many papers in this field take the approach of first modeling the affinity between visual elements, then grouping elements with high affinity (e.g., Shi & Malik (2000)). Our work follows this approach. However, rather than using a hand-engineered grouping cue Shi & Malik (2000); Zitnick & Doll\u00e1r (2014), or learning to group with direct supervision Doll\u00e1r & Zitnick (2013); Krahnenbuhl & Koltun (2015), we use a measure of spatial and temporal dependence as the affinity. Grouping based on co-occurrence has received some prior attention in computer vision (Sivic et al. (2005); Faktor & Irani (2012; 2013); Isola et al. (2014)).", "startOffset": 102, "endOffset": 820}, {"referenceID": 0, "context": "(2014) , (semantic) segmentation Shi & Malik (2000); Malisiewicz & Efros (2007), and object proposals Alexe et al. (2012); Zitnick & Doll\u00e1r (2014); Krahnenbuhl & Koltun (2015), among others. Many papers in this field take the approach of first modeling the affinity between visual elements, then grouping elements with high affinity (e.g., Shi & Malik (2000)). Our work follows this approach. However, rather than using a hand-engineered grouping cue Shi & Malik (2000); Zitnick & Doll\u00e1r (2014), or learning to group with direct supervision Doll\u00e1r & Zitnick (2013); Krahnenbuhl & Koltun (2015), we use a measure of spatial and temporal dependence as the affinity. Grouping based on co-occurrence has received some prior attention in computer vision (Sivic et al. (2005); Faktor & Irani (2012; 2013); Isola et al. (2014)). Sivic et al. (2005) demonstrated that object categories can be discovered and roughly localized using an unsupervised generative model.", "startOffset": 102, "endOffset": 842}, {"referenceID": 0, "context": "(2014) , (semantic) segmentation Shi & Malik (2000); Malisiewicz & Efros (2007), and object proposals Alexe et al. (2012); Zitnick & Doll\u00e1r (2014); Krahnenbuhl & Koltun (2015), among others. Many papers in this field take the approach of first modeling the affinity between visual elements, then grouping elements with high affinity (e.g., Shi & Malik (2000)). Our work follows this approach. However, rather than using a hand-engineered grouping cue Shi & Malik (2000); Zitnick & Doll\u00e1r (2014), or learning to group with direct supervision Doll\u00e1r & Zitnick (2013); Krahnenbuhl & Koltun (2015), we use a measure of spatial and temporal dependence as the affinity. Grouping based on co-occurrence has received some prior attention in computer vision (Sivic et al. (2005); Faktor & Irani (2012; 2013); Isola et al. (2014)). Sivic et al. (2005) demonstrated that object categories can be discovered and roughly localized using an unsupervised generative model. Isola et al. (2014) showed that statistical dependences between adjacent pixel colors, measured by pointwise mutual information (PMI) can be very effective at localizing object boundaries.", "startOffset": 102, "endOffset": 978}, {"referenceID": 0, "context": "(2014) , (semantic) segmentation Shi & Malik (2000); Malisiewicz & Efros (2007), and object proposals Alexe et al. (2012); Zitnick & Doll\u00e1r (2014); Krahnenbuhl & Koltun (2015), among others. Many papers in this field take the approach of first modeling the affinity between visual elements, then grouping elements with high affinity (e.g., Shi & Malik (2000)). Our work follows this approach. However, rather than using a hand-engineered grouping cue Shi & Malik (2000); Zitnick & Doll\u00e1r (2014), or learning to group with direct supervision Doll\u00e1r & Zitnick (2013); Krahnenbuhl & Koltun (2015), we use a measure of spatial and temporal dependence as the affinity. Grouping based on co-occurrence has received some prior attention in computer vision (Sivic et al. (2005); Faktor & Irani (2012; 2013); Isola et al. (2014)). Sivic et al. (2005) demonstrated that object categories can be discovered and roughly localized using an unsupervised generative model. Isola et al. (2014) showed that statistical dependences between adjacent pixel colors, measured by pointwise mutual information (PMI) can be very effective at localizing object boundaries. Both these methods require modeling generative probability distributions, which restricts their ability to scale to high-dimensional data. Our model, on the other hand, is discriminative and can be easily scaled. A recent line of work in representation learning has taken a similar tack, training discriminative models to predict one aspect of raw sensory data from another. This work may be termed selfsupervised learning and has a number of flavors. The common theme is exploiting spatial and/or temporal structure as supervisory signals. Mobahi et al. (2009) learn a feature embedding such that features adjacent in time are similar and features far apart in time are dissimilar.", "startOffset": 102, "endOffset": 1709}, {"referenceID": 0, "context": "(2014) , (semantic) segmentation Shi & Malik (2000); Malisiewicz & Efros (2007), and object proposals Alexe et al. (2012); Zitnick & Doll\u00e1r (2014); Krahnenbuhl & Koltun (2015), among others. Many papers in this field take the approach of first modeling the affinity between visual elements, then grouping elements with high affinity (e.g., Shi & Malik (2000)). Our work follows this approach. However, rather than using a hand-engineered grouping cue Shi & Malik (2000); Zitnick & Doll\u00e1r (2014), or learning to group with direct supervision Doll\u00e1r & Zitnick (2013); Krahnenbuhl & Koltun (2015), we use a measure of spatial and temporal dependence as the affinity. Grouping based on co-occurrence has received some prior attention in computer vision (Sivic et al. (2005); Faktor & Irani (2012; 2013); Isola et al. (2014)). Sivic et al. (2005) demonstrated that object categories can be discovered and roughly localized using an unsupervised generative model. Isola et al. (2014) showed that statistical dependences between adjacent pixel colors, measured by pointwise mutual information (PMI) can be very effective at localizing object boundaries. Both these methods require modeling generative probability distributions, which restricts their ability to scale to high-dimensional data. Our model, on the other hand, is discriminative and can be easily scaled. A recent line of work in representation learning has taken a similar tack, training discriminative models to predict one aspect of raw sensory data from another. This work may be termed selfsupervised learning and has a number of flavors. The common theme is exploiting spatial and/or temporal structure as supervisory signals. Mobahi et al. (2009) learn a feature embedding such that features adjacent in time are similar and features far apart in time are dissimilar. Srivastava et al. (2015) predict future frames in a video, and rely on strict temporal ordering; extension to spatial or unordered data is unclear.", "startOffset": 102, "endOffset": 1855}, {"referenceID": 0, "context": "(2014) , (semantic) segmentation Shi & Malik (2000); Malisiewicz & Efros (2007), and object proposals Alexe et al. (2012); Zitnick & Doll\u00e1r (2014); Krahnenbuhl & Koltun (2015), among others. Many papers in this field take the approach of first modeling the affinity between visual elements, then grouping elements with high affinity (e.g., Shi & Malik (2000)). Our work follows this approach. However, rather than using a hand-engineered grouping cue Shi & Malik (2000); Zitnick & Doll\u00e1r (2014), or learning to group with direct supervision Doll\u00e1r & Zitnick (2013); Krahnenbuhl & Koltun (2015), we use a measure of spatial and temporal dependence as the affinity. Grouping based on co-occurrence has received some prior attention in computer vision (Sivic et al. (2005); Faktor & Irani (2012; 2013); Isola et al. (2014)). Sivic et al. (2005) demonstrated that object categories can be discovered and roughly localized using an unsupervised generative model. Isola et al. (2014) showed that statistical dependences between adjacent pixel colors, measured by pointwise mutual information (PMI) can be very effective at localizing object boundaries. Both these methods require modeling generative probability distributions, which restricts their ability to scale to high-dimensional data. Our model, on the other hand, is discriminative and can be easily scaled. A recent line of work in representation learning has taken a similar tack, training discriminative models to predict one aspect of raw sensory data from another. This work may be termed selfsupervised learning and has a number of flavors. The common theme is exploiting spatial and/or temporal structure as supervisory signals. Mobahi et al. (2009) learn a feature embedding such that features adjacent in time are similar and features far apart in time are dissimilar. Srivastava et al. (2015) predict future frames in a video, and rely on strict temporal ordering; extension to spatial or unordered data is unclear. Wang & Gupta (2015) use a siamese triplet loss to learn a representation that can track patches through a video.", "startOffset": 102, "endOffset": 1998}, {"referenceID": 0, "context": "Agrawal et al. (2015) as well as Jayaraman & Grauman (2015) regress on egomotion sig-", "startOffset": 0, "endOffset": 22}, {"referenceID": 0, "context": "Agrawal et al. (2015) as well as Jayaraman & Grauman (2015) regress on egomotion sig-", "startOffset": 0, "endOffset": 60}, {"referenceID": 10, "context": "Finally, Doersch et al. (2015) learn features by predicting the relative orientation between patches within an image.", "startOffset": 9, "endOffset": 31}, {"referenceID": 26, "context": "Given object labels, a straightforward approach to this problem would be to train a supervised classifier to predict indicator variable Q \u2208 {0, 1}, where Q = 1 iff A and B lie on the same object Manen et al. (2013). Throughout this paper, we use Q to indicate the property that A and B share the same semantic label.", "startOffset": 195, "endOffset": 215}, {"referenceID": 18, "context": "Kayser et al. (2001); Wiskott & Sejnowski (2002)).", "startOffset": 0, "endOffset": 21}, {"referenceID": 18, "context": "Kayser et al. (2001); Wiskott & Sejnowski (2002)).", "startOffset": 0, "endOffset": 49}, {"referenceID": 15, "context": "Previous work on visual grouping Isola et al. (2014) and word embeddings (Church & Hanks (1990); Levy et al.", "startOffset": 33, "endOffset": 53}, {"referenceID": 15, "context": "Previous work on visual grouping Isola et al. (2014) and word embeddings (Church & Hanks (1990); Levy et al.", "startOffset": 33, "endOffset": 96}, {"referenceID": 15, "context": "Previous work on visual grouping Isola et al. (2014) and word embeddings (Church & Hanks (1990); Levy et al. (2014)) has found PMI to be an effective measure for these tasks.", "startOffset": 33, "endOffset": 116}, {"referenceID": 7, "context": "To model w(A,B) we use a Convolutional Neural Net (CNN) with a Siamese-style architecture (Figure 2, Chopra et al. (2005)), which we implement in Caffe (Jia et al.", "startOffset": 101, "endOffset": 122}, {"referenceID": 7, "context": "To model w(A,B) we use a Convolutional Neural Net (CNN) with a Siamese-style architecture (Figure 2, Chopra et al. (2005)), which we implement in Caffe (Jia et al. (2014)).", "startOffset": 101, "endOffset": 171}, {"referenceID": 2, "context": "Similarity measures like these are commonly used in visual grouping algorithms Arbelaez et al. (2011); Faktor & Irani (2012).", "startOffset": 79, "endOffset": 102}, {"referenceID": 2, "context": "Similarity measures like these are commonly used in visual grouping algorithms Arbelaez et al. (2011); Faktor & Irani (2012). Full results of this analysis are given in Table 1.", "startOffset": 79, "endOffset": 125}, {"referenceID": 41, "context": "We train on five cities selected from the MIT City Database Zhou et al. (2014) and test predicting C on a held out set of three more cities from that dataset.", "startOffset": 60, "endOffset": 79}, {"referenceID": 24, "context": "For this, we define Q as \u201cdo these two photos belong to the same place category?\u201d We test this task on the LabelMe Outdoors dataset Liu et al. (2009) for which each photo was assigned to one of eight place categories (e.", "startOffset": 132, "endOffset": 150}, {"referenceID": 24, "context": "For this, we define Q as \u201cdo these two photos belong to the same place category?\u201d We test this task on the LabelMe Outdoors dataset Liu et al. (2009) for which each photo was assigned to one of eight place categories (e.g., \u201ccoast\u201d, \u201chighway\u201d, \u201ctall building\u201d). Our network shows promising performance on this task, reaching 0.79 AP on predicting Q. HOG similarity reaches the same performance, which corroborates past findings that HOG is effective at grouping related photos (Dalal & Triggs (2005)).", "startOffset": 132, "endOffset": 500}, {"referenceID": 5, "context": "The papers compared to are: BING (Cheng et al. (2014)), EdgeBoxes Zitnick & Doll\u00e1r (2014), LPO (Krahnenbuhl & Koltun (2015)), Objectness (Alexe et al.", "startOffset": 34, "endOffset": 54}, {"referenceID": 5, "context": "The papers compared to are: BING (Cheng et al. (2014)), EdgeBoxes Zitnick & Doll\u00e1r (2014), LPO (Krahnenbuhl & Koltun (2015)), Objectness (Alexe et al.", "startOffset": 34, "endOffset": 90}, {"referenceID": 5, "context": "The papers compared to are: BING (Cheng et al. (2014)), EdgeBoxes Zitnick & Doll\u00e1r (2014), LPO (Krahnenbuhl & Koltun (2015)), Objectness (Alexe et al.", "startOffset": 34, "endOffset": 124}, {"referenceID": 1, "context": "(2014)), EdgeBoxes Zitnick & Doll\u00e1r (2014), LPO (Krahnenbuhl & Koltun (2015)), Objectness (Alexe et al. (2012)), GOP (Krahenbuhl & Koltun (2014)), Randomized Prim (Manen et al.", "startOffset": 91, "endOffset": 111}, {"referenceID": 1, "context": "(2014)), EdgeBoxes Zitnick & Doll\u00e1r (2014), LPO (Krahnenbuhl & Koltun (2015)), Objectness (Alexe et al. (2012)), GOP (Krahenbuhl & Koltun (2014)), Randomized Prim (Manen et al.", "startOffset": 91, "endOffset": 145}, {"referenceID": 1, "context": "(2014)), EdgeBoxes Zitnick & Doll\u00e1r (2014), LPO (Krahnenbuhl & Koltun (2015)), Objectness (Alexe et al. (2012)), GOP (Krahenbuhl & Koltun (2014)), Randomized Prim (Manen et al. (2013)), Sel.", "startOffset": 91, "endOffset": 184}, {"referenceID": 1, "context": "(2014)), EdgeBoxes Zitnick & Doll\u00e1r (2014), LPO (Krahnenbuhl & Koltun (2015)), Objectness (Alexe et al. (2012)), GOP (Krahenbuhl & Koltun (2014)), Randomized Prim (Manen et al. (2013)), Sel. Search (Uijlings et al. (2013)).", "startOffset": 91, "endOffset": 222}, {"referenceID": 5, "context": "Here we show how our learned frame affinities can be used to break a movie into coherent scenes, a problem that has received some prior attention (Chen et al. (2008); Zhai & Shah (2006)).", "startOffset": 147, "endOffset": 166}, {"referenceID": 5, "context": "Here we show how our learned frame affinities can be used to break a movie into coherent scenes, a problem that has received some prior attention (Chen et al. (2008); Zhai & Shah (2006)).", "startOffset": 147, "endOffset": 186}, {"referenceID": 24, "context": "Figure 7: Left: Clustering the LabelMe Outdoor dataset (Liu et al. (2009)) into 8 groups using our learned affinities.", "startOffset": 56, "endOffset": 74}, {"referenceID": 24, "context": "Figure 7: Left: Clustering the LabelMe Outdoor dataset (Liu et al. (2009)) into 8 groups using our learned affinities. Random sample images are shown from each group. Right: Photo cluster purity versus number of clusters k. Note that we trained our model on an independent dataset, the MIT City dataset (Zhou et al. (2014)).", "startOffset": 56, "endOffset": 323}, {"referenceID": 2, "context": "Following a standard evaluation procedure in image boundary detection (Arbelaez et al. (2011)), we measure performance on the retrieval task of finding all ground truth boundaries.", "startOffset": 71, "endOffset": 94}, {"referenceID": 24, "context": "We test the purity of the clusters on LabelMe Outdoors dataset (Liu et al. (2009)).", "startOffset": 64, "endOffset": 82}], "year": 2015, "abstractText": "We propose a self-supervised framework that learns to group visual entities based on their rate of co-occurrence in space and time. To model statistical dependencies between the entities, we set up a simple binary classification problem in which the goal is to predict if two visual primitives occur in the same spatial or temporal context. We apply this framework to three domains: learning patch affinities from spatial adjacency in images, learning frame affinities from temporal adjacency in videos, and learning photo affinities from geospatial proximity in image collections. We demonstrate that in each case the learned affinities uncover meaningful semantic groupings. From patch affinities we generate object proposals that are competitive with state-of-the-art supervised methods. From frame affinities we generate movie scene segmentations that correlate well with DVD chapter structure. Finally, from geospatial affinities we learn groups that relate well to semantic place categories.", "creator": "LaTeX with hyperref package"}}}