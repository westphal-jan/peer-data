{"id": "1511.06827", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "21-Nov-2015", "title": "GradNets: Dynamic Interpolation Between Neural Architectures", "abstract": "In machine learning, there is a fundamental trade-off between ease of optimization and expressive power. Neural Networks, in particular, have enormous expressive power and yet are notoriously challenging to train. The nature of that optimization challenge changes over the course of learning. Traditionally in deep learning, one makes a static trade-off between the needs of early and late optimization. In this paper, we investigate a novel framework, GradNets, for dynamically adapting architectures during training to get the benefits of both. For example, we can gradually transition from linear to non-linear networks, deterministic to stochastic computation, shallow to deep architectures, or even simple downsampling to fully differentiable attention mechanisms. Benefits include increased accuracy, easier convergence with more complex architectures, solutions to test-time execution of batch normalization, and the ability to train networks of up to 200 layers.", "histories": [["v1", "Sat, 21 Nov 2015 03:50:49 GMT  (106kb,D)", "http://arxiv.org/abs/1511.06827v1", null]], "reviews": [], "SUBJECTS": "cs.LG cs.NE", "authors": ["diogo almeida", "nate sauder"], "accepted": false, "id": "1511.06827"}, "pdf": {"name": "1511.06827.pdf", "metadata": {"source": "CRF", "title": "GRADNETS: DYNAMIC INTERPOLATION BETWEEN NEURAL ARCHITECTURES", "authors": ["Diogo Almeida", "Nate Sauder"], "emails": ["diogo@enlitic.com", "nate@enlitic.com"], "sections": [{"heading": "1 INTRODUCTION", "text": "Many of the benefits of deep learning stem from its ability to automatically learn abstract representations from raw input functions, its scalability from large data sets, and its exponential growth in expressive power with depth. As a result, deep learning can suffer from slow convergence, overmatch, poor optimism, difficulties optimizing, and acute sensitivity to initializations and hyperparameters. Deep learning practitioners often optimize for the best possible generalization. Dropout and Reflected Linear Units (ReLUs) are two of the more important algorithmic breakthroughs of the past 5 years. In fact, both were crucial to the ground-breaking results of Krizhevsky et al al al al al al al al al al al al (2012), which reduced the error rate in the 2012 ImageNet Challenge by nearly 40%. Both techniques facilitated the optimization of airflow systems with a view to significantly increase the expressiveness of 27."}, {"heading": "2 METHOD DESCRIPTION", "text": "A GradNet architectural component is exceptionally simple: a weighted mean value of two architectural components where the weight g is annealed from 0 to 1 during training. For example, a gradual ReLU anneals from an identity layer to a ReLU layer (see Figure 1). In this thesis, we use a linear plan with a single hyperparameter \u03c4 as the epoch when the annealing is complete: g = min (t / \u03c4, 1), where t is the number of epochs. The GradNet framework is exceptionally flexible as it can be applied to any pair of network components that result in a tensor of the same shape. Our examples can be found in Table 1."}, {"heading": "3 EXPERIMENTS", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "3.1 BASELINE ARCHITECTURE", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "3.1.1 CIFAR-10", "text": "We use the network topology of the Scalable Bayesian Optimization Using Deep Neural Networks (Snoek et al., 2015). If no step-by-step dropouts were used, activations were aborted with a probability of 0.3 after each revolutionary shift. By using batch normalization, mean and variance were achieved at test time by determining an exponential moving mean of the train time averages and deviations. Adam optimized the network with a stack size of 256 and stopped and initialized it early with orthogonal initialization."}, {"heading": "3.1.2 MNIST", "text": "Experiments in Section 3.8 were performed at MNIST. A multi-layer perceptron with 512 units per hidden layer was used, optimized with Adam with a batch size of 500 and stopped and initialized early by orthogonal initialization."}, {"heading": "3.1.3 CLUTTERED MNIST", "text": "Experiments in Section 3.9 were conducted on Cluttered MNIST (Mnih et al., 2014), which used an architecture with 2 convolutionary layers, each 3x3 with 32 filters, followed by a maximum pooling layer and 2 fully connected layers with 256 and 10 units, respectively, to classify 20x20 pixel images. Input 60x60 pixel images were scanned either with 3x3 center pooling or with an affinity spatial transformer network (Jaderberg et al., 2015), whose localization network had an architecture of 2 revolutionary layers, each 5x5 with 20 filters, followed by a maximum pooling layer and 2 fully connected layers, each with 50 and 10 units per hidden layer, respectively. Models were optimized with Adam with a stack size of 500 and stopped and initialized early by orthogonal initialization."}, {"heading": "3.2 GRADUAL RELUS (GRELUS)", "text": "ReLUs are the predominant form of nonlinearity in modern deep neural networks, but do not allow gradients to flow well, leading to several solutions: leaky and very leaky ReLUs or ReLUs with parameterized slope for the negative part (He et al., 2015). On the other hand, linear networks are easier to optimize due to their convexity, but do not allow learning abstract hierarchical feature detectors. By starting with fully linear networks and interpolating towards completely leaky ReLU networks, we gain the ease of optimizing linear networks and also the expressiveness of ReLU networks. We call them Gradual ReLUs. Instead of interpolating between two separate outputs, it would be equivalent to interpolating the slope of the negative part of leaky ReLUs, resulting in a technique without overhead over conventional ReLU networks."}, {"heading": "3.3 GRADUAL DROPOUT", "text": "Stochastic dropouts are an excellent regulator, but affect the rate of convergence so much that some state-of-the-art architectures even remove them (Ioffe & Szegedy, 2015). Furthermore, networks do not form at all at sufficiently high failure rates. Thus, there is a balance between regulating networks that are slightly overfit and the ability to optimize a network. Gradual dropouts mean that starting training with zero failures and annealing to a higher level (even at 0.9) means that early training of randomly initiated weights does not suffer from excessive noise, while the final networks may be sufficiently large, but are regulated to generalize in other areas as well. Table 3 shows that, similar to GReLUs, applying step-by-step dropouts to the same CNN architecture shows that not only does it perform better when using a static amount of dropouts to achieve better performance."}, {"heading": "3.4 GRADUAL POOLING", "text": "Max pooling is the most commonly used subsampling approach for state-of-the-art networks. However, low-level features at the beginning of training are not informative and therefore Max pooling impedes the gradient flow to most of these early units. To solve this problem, we use the same effect of glow from linear to nonlinear networks. In particular, we capture this intuition by glow from medium pooling units to maximum pooling units. As shown in Table 4, this combined approach is better than average or maximum pooling as well as the determination of an average, the learning of a weighted average or the learning of an attention mechanism (Lee et al., 2015)."}, {"heading": "3.5 GRADUAL BATCH NORMALIZATION", "text": "Batch Normalization (BN) is a state-of-the-art architectural component that greatly promotes convergence and accuracy, but it cannot simply be executed at test time, especially when evaluating individual data points. There are several clever ways to calculate means and deviations to circumvent this fundamental problem, but ideally we want to take advantage of the convergence and accuracy of BN while avoiding problems at validation time. Using the GradNet framework to glow from BN layers to identity layers, we create a network that is trivial to evaluate at validation time, while taking full advantage of batch normalization."}, {"heading": "3.6 GRADUAL CONVOLUTIONS", "text": "Flatter networks are easier to train, but less powerful than deep networks. Accordingly, it is intuitively appealing to take advantage of the simpler convergence of flatter networks while maintaining the performance of very deep networks. Two proposed solutions, Highway Networks (Srivastava et al., 2015) and Network in Network (NiN et al., 2013), are emblematic of this problem: NiN is very deep but difficult to train, while motorway networks require much greater depths than normal to adapt to base performance, resulting in greatly increased computing costs. In terms of the depth used, motorway networks perform significantly worse than the baseline network, while the interpolation of identity layers in convective layers (and thus gradually increasing the depth) causes almost no performance loss. On the other hand, the interpolation of convective layers in NiN layers results in significant performance improvements."}, {"heading": "3.7 COMBINING GRADNETS", "text": "Table 7 shows the composition of GradNets."}, {"heading": "3.8 GRELU DEPTH EXPERIMENTS", "text": "The trend in neural networks is towards ever deeper architectures, which is in line with the theoretical results, which point to an exponentially higher significance. However, deeper networks exacerbate the challenges we have pointed out, for example the poor flow of information in ReLU networks. As shown in Figure 2, the use of the GradNet framework not only improves performance for slightly deeper networks, but also enables the successful training of significantly deeper networks that are not successfully trained by standard networks."}, {"heading": "3.9 GRADUAL SPATIAL TRANSFORMER NETWORKS", "text": "Spatial transformer networks (Jaderberg et al., 2015) are a powerful new type of attention model that makes performing an affinity glance differentiating, making it much easier to train learning methods than reinforcement methods, but still struggling with convergence. In particular, when randomly initialized, the localization network can transform or zoom in excessively on the original image from the screen. Since the localization network depends on the signal of the classification network, it makes sense to train the classification network first before tuning the localization network. The flexibility of the GradNet framework allows for a computational implementation of this intuition. In particular, we glow gradually from a simple downsample to an affinity spatial transformer network. As shown in Table 8, the regular spatial transformer often differs from each other. However, the GradNet implementation does not differ from each other and nevertheless achieves the same results at the highest level."}, {"heading": "4 RELATED WORK", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.1 PRECONDITIONING", "text": "Hinton & Salakhutdinov (2006) achieved ground-breaking results by training deeper multi-layer networks by pre-conditioning early layers by greedily training each layer one after the other. More recently, Yosinski et al. (2014) initiated from known successful weights, immediately dropping the network to a lower point of the loss area and avoiding getting stuck in the many local optima that arise in deeper and larger networks. Distillation (Hinton et al., 2015) is another technique that uses existing cumbersome models to extract structure from the data before transferring it to a less cumbersome model. GradNets embody this philosophy by combining easy-to-optimize and powerful training levels with the added benefit of being able to perform the training together in a single training phase."}, {"heading": "4.2 RECTIFIED LINEAR UNITS (RELU) ALTERNATIVES", "text": "In essence, these experiments have shown that networks are robust for the negative part. While ReLUs saturate all gradients on the negative side to zero, leaky ReLUs allow the flow of this information. Accordingly, very leaky ReLUs have been highly successful in deep learning competitions. PReLUs (He et al., 2015) instead allow the slope to be trained at the negative edge of the ReLU, allowing the network to train how linear its nonlinearities should be. In randomized ReLUs (Xu et al., 2015), tracks are randomly selected within a certain range before being frozen during the test. The subsequent stochastic regulation proved successful in the Kaggle Data Science Bowl competition. Instead of establishing a static compromise between ease of optimization and expressiveness, GradNets allow us to dynamically alter this trade during training."}, {"heading": "4.3 VERY DEEP NETWORKS", "text": "Motorway networks (Srivastava et al., 2015) are compositions of layers in which a gating tensor controls the interpolation between an identity layer and a regular, fully connected layer. Smooth variation enables learning of much deeper networks, with the calculation of the gating factor on each layer requiring twice as much computation as with conventional networks. Saxe et al. (2013) demonstrated the advantages of using deep linear networks as a medium to study the learning dynamics of deep neural networks, and in particular found that precise initializations can achieve deep-independent learning times for deep linear networks. GradNets uses insights from these two results: borrow the theoretically justified initializations for linear networks, while obtaining the optimization benefits of interpolation operations in motorway networks without incurring additional computing costs.These results support GradNet's ability to exercise the theoretical insights of simpler architectures and very complex architectures at the same time."}, {"heading": "4.4 ATTENTIONAL MODELS", "text": "Interpolation between architectures is very similar to the soft attention used to interpolate between differentiable data structures (Joulin & Mikolov (2015), Grefenstette et al. (2015)), which use back propagation to learn the weights for interpolation."}, {"heading": "5 CONCLUSION", "text": "A difficult decision in neural network design is to address the fundamental trade-off between simple optimization and expressive power. GradNets are a framework to facilitate this choice by leveraging the ease of optimization early in training and expressive power late in training. Our experiments show that this not only enables greater amounts of stochastic regularization and successful training of extremely deep networks, but also consistently improves performance without high computing costs. Finally, the results on spatial transformer networks support the ability of GradNets to dramatically facilitate the training of complex architectural components and enable the use of new classes of components in neural networks. Future work includes interpolation between simpler, easier-to-carry architectural components and a more complex, more expressive component that can be generalized more easily. Based on the results in deeper networks, GradNets can provide tremendous benefits for recurring architectures. In addition, the framework itself can be expanded by adapting the interpolarity policy through more intelligent ways such as learning."}, {"heading": "ACKNOWLEDGMENTS", "text": "We thank NVIDIA for their generosity in making part of their cluster available to support Enlitic's mission and our research."}], "references": [{"title": "Rich feature hierarchies for accurate object detection and semantic segmentation", "author": ["Girshick", "Ross", "Donahue", "Jeff", "Darrell", "Trevor", "Malik", "Jagannath"], "venue": "In Computer Vision and Pattern Recognition (CVPR),", "citeRegEx": "Girshick et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Girshick et al\\.", "year": 2014}, {"title": "Learning to transduce with unbounded memory", "author": ["Grefenstette", "Edward", "Hermann", "Karl Moritz", "Suleyman", "Mustafa", "Blunsom", "Phil"], "venue": "arXiv preprint arXiv:1506.02516,", "citeRegEx": "Grefenstette et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Grefenstette et al\\.", "year": 2015}, {"title": "Delving deep into rectifiers: Surpassing human-level performance on imagenet classification", "author": ["He", "Kaiming", "Zhang", "Xiangyu", "Ren", "Shaoqing", "Sun", "Jian"], "venue": "arXiv preprint arXiv:1502.01852,", "citeRegEx": "He et al\\.,? \\Q2015\\E", "shortCiteRegEx": "He et al\\.", "year": 2015}, {"title": "Distilling the knowledge in a neural network", "author": ["Hinton", "Geoffrey", "Vinyals", "Oriol", "Dean", "Jeff"], "venue": "arXiv preprint arXiv:1503.02531,", "citeRegEx": "Hinton et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Hinton et al\\.", "year": 2015}, {"title": "Reducing the dimensionality of data with neural networks", "author": ["Hinton", "Geoffrey E", "Salakhutdinov", "Ruslan R"], "venue": null, "citeRegEx": "Hinton et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Hinton et al\\.", "year": 2006}, {"title": "Improving neural networks by preventing co-adaptation of feature detectors", "author": ["Hinton", "Geoffrey E", "Srivastava", "Nitish", "Krizhevsky", "Alex", "Sutskever", "Ilya", "Salakhutdinov", "Ruslan R"], "venue": "arXiv preprint arXiv:1207.0580,", "citeRegEx": "Hinton et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Hinton et al\\.", "year": 2012}, {"title": "Batch normalization: Accelerating deep network training by reducing internal covariate shift", "author": ["Ioffe", "Sergey", "Szegedy", "Christian"], "venue": "arXiv preprint arXiv:1502.03167,", "citeRegEx": "Ioffe et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Ioffe et al\\.", "year": 2015}, {"title": "Spatial transformer networks", "author": ["Jaderberg", "Max", "Simonyan", "Karen", "Zisserman", "Andrew", "Kavukcuoglu", "Koray"], "venue": "arXiv preprint arXiv:1506.02025,", "citeRegEx": "Jaderberg et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Jaderberg et al\\.", "year": 2015}, {"title": "Inferring algorithmic patterns with stack-augmented recurrent nets", "author": ["Joulin", "Armand", "Mikolov", "Tomas"], "venue": "arXiv preprint arXiv:1503.01007,", "citeRegEx": "Joulin et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Joulin et al\\.", "year": 2015}, {"title": "Imagenet classification with deep convolutional neural networks. In Advances in neural information processing", "author": ["Krizhevsky", "Alex", "Sutskever", "Ilya", "Hinton", "Geoffrey E"], "venue": null, "citeRegEx": "Krizhevsky et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Krizhevsky et al\\.", "year": 2012}, {"title": "Generalizing pooling functions in convolutional neural networks: Mixed", "author": ["Lee", "Chen-Yu", "Gallagher", "Patrick W", "Tu", "Zhuowen"], "venue": "gated, and tree. arXiv preprint arXiv:1509.08985,", "citeRegEx": "Lee et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Lee et al\\.", "year": 2015}, {"title": "Rectifier nonlinearities improve neural network acoustic models", "author": ["Maas", "Andrew L", "Hannun", "Awni Y", "Ng", "Andrew Y"], "venue": "In Proc. ICML,", "citeRegEx": "Maas et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Maas et al\\.", "year": 2013}, {"title": "Recurrent models of visual attention", "author": ["Mnih", "Volodymyr", "Heess", "Nicolas", "Graves", "Alex"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Mnih et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Mnih et al\\.", "year": 2014}, {"title": "Rectified linear units improve restricted boltzmann machines", "author": ["Nair", "Vinod", "Hinton", "Geoffrey E"], "venue": "In Proceedings of the 27th International Conference on Machine Learning", "citeRegEx": "Nair et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Nair et al\\.", "year": 2010}, {"title": "Exact solutions to the nonlinear dynamics of learning in deep linear neural networks", "author": ["Saxe", "Andrew M", "McClelland", "James L", "Ganguli", "Surya"], "venue": "arXiv preprint arXiv:1312.6120,", "citeRegEx": "Saxe et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Saxe et al\\.", "year": 2013}, {"title": "Scalable bayesian optimization using deep neural networks", "author": ["Snoek", "Jasper", "Rippel", "Oren", "Swersky", "Kevin", "Kiros", "Ryan", "Satish", "Nadathur", "Sundaram", "Narayanan", "Patwary", "Md", "Ali", "Mostofa", "Adams", "Ryan P"], "venue": "arXiv preprint arXiv:1502.05700,", "citeRegEx": "Snoek et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Snoek et al\\.", "year": 2015}, {"title": "Empirical evaluation of rectified activations in convolutional network", "author": ["Xu", "Bing", "Wang", "Naiyan", "Chen", "Tianqi", "Li", "Mu"], "venue": "arXiv preprint arXiv:1505.00853,", "citeRegEx": "Xu et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Xu et al\\.", "year": 2015}, {"title": "How transferable are features in deep neural networks", "author": ["Yosinski", "Jason", "Clune", "Jeff", "Bengio", "Yoshua", "Lipson", "Hod"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Yosinski et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Yosinski et al\\.", "year": 2014}], "referenceMentions": [{"referenceID": 5, "context": "Dropout's addition of stochastic noise improves performance by enabling larger networks and preventing coadaptation of weights albeit at a significant cost of convergence speed (Hinton et al., 2012).", "startOffset": 177, "endOffset": 198}, {"referenceID": 14, "context": "Despite being easier to optimize than sigmoids or hyperbolic tangents, ReLUs (Nair & Hinton, 2010) increase the expressive power of networks while being more difficult to initialize and optimize than linear models (Saxe et al., 2013) or leaky ReLUs (Maas et al.", "startOffset": 214, "endOffset": 233}, {"referenceID": 11, "context": ", 2013) or leaky ReLUs (Maas et al., 2013).", "startOffset": 23, "endOffset": 42}, {"referenceID": 5, "context": "Deep learning has become the state of the art system for many machine learning challenges (Krizhevsky et al. (2012), Girshick et al.", "startOffset": 91, "endOffset": 116}, {"referenceID": 0, "context": "(2012), Girshick et al. (2014)).", "startOffset": 8, "endOffset": 31}, {"referenceID": 0, "context": "(2012), Girshick et al. (2014)). Many of deep learning's benefits come from its ability to automatically learn abstract representations from raw input features, its scalability to large datasets, and its exponential growth in expressive power with depth. As a consequence of this great expressive power, deep learning can suffer from slow convergence, over-fitting, poor optima, difficulty of optimization, and acute sensitivity to initialization and hyper-parameters. Deep learning practitioners often optimize for the best possible generalization. Dropout and rectified linear units (ReLUs) are two of the more important algorithmic breakthroughs of the last 5 years. Indeed, both were essential to the breakthrough results of Krizhevsky et al. (2012) that reduced the error rate on the ImageNet Challenge of 2012 by almost 40%.", "startOffset": 8, "endOffset": 754}, {"referenceID": 15, "context": "We employ the network topology of Scalable Bayesian Optimization Using Deep Neural Networks (Snoek et al., 2015).", "startOffset": 92, "endOffset": 112}, {"referenceID": 12, "context": "9 were performed on Cluttered MNIST(Mnih et al., 2014).", "startOffset": 35, "endOffset": 54}, {"referenceID": 7, "context": "The input 60x60 pixel images were downsampled either with 3x3 mean pooling or an affine Spatial Transformer Network (Jaderberg et al., 2015), whose localization network had an architecture of 2 convolutional layers, each 5x5 with 20 filters followed by a max pooling layer, and 2 fully connected layers, with 50 and 10 units respectively with 512 units per hidden layer, Models were optimized with Adam with a batch size of 500 and early stopping and initialized using orthogonal initialization.", "startOffset": 116, "endOffset": 140}, {"referenceID": 2, "context": "ReLUs are the dominant form of nonlinearity in modern deep neural networks, however they do not allow gradients to flow well leading to several proposed solutions: leaky and very leaky ReLUs or ReLUs with parameterized slope for the negative part (He et al., 2015).", "startOffset": 247, "endOffset": 264}, {"referenceID": 10, "context": "As seen in Table 4, this combined approach does better than either mean or max pooling as well as taking an average, learning a weighted average, or learning an attention mechanism (Lee et al., 2015).", "startOffset": 181, "endOffset": 199}, {"referenceID": 7, "context": "Spatial transformer networks (Jaderberg et al., 2015) are a powerful new type of attentional model that makes performing an affine glimpse differentiable.", "startOffset": 29, "endOffset": 53}, {"referenceID": 3, "context": "Distillation (Hinton et al., 2015) is another technique that uses existing cumbersome models to extract structure from the data before transferring it to a less cumbersome model.", "startOffset": 13, "endOffset": 34}, {"referenceID": 14, "context": "More recently, Yosinski et al. (2014) initialized from known successful weights thereby dropping the network immediately into a lower point of the loss surface and avoiding getting stuck in the many local optima that arise with deeper and larger networks.", "startOffset": 15, "endOffset": 38}, {"referenceID": 2, "context": "PReLUs (He et al., 2015) instead allow for training the slope on the negative edge of the ReLU thus allowing the network to train how linear its nonlinearities should be.", "startOffset": 7, "endOffset": 24}, {"referenceID": 16, "context": "In randomized ReLUs (Xu et al., 2015), slopes are randomly chosen within a specific range before being frozen during testing.", "startOffset": 20, "endOffset": 37}, {"referenceID": 14, "context": "Saxe et al. (2013) showed the benefits of using deep linear networks as a medium for studying the learning dynamics of deep neural networks.", "startOffset": 0, "endOffset": 19}, {"referenceID": 1, "context": "4 ATTENTIONAL MODELS Interpolation between architectures is very similar to the soft attention used to interpolate between differentiable data structures (Joulin & Mikolov (2015), Grefenstette et al. (2015)).", "startOffset": 180, "endOffset": 207}], "year": 2015, "abstractText": "In machine learning, there is a fundamental trade-off between ease of optimization and expressive power. Neural Networks, in particular, have enormous expressive power and yet are notoriously challenging to train. The nature of that optimization challenge changes over the course of learning. Traditionally in deep learning, one makes a static trade-off between the needs of early and late optimization. In this paper, we investigate a novel framework, GradNets, for dynamically adapting architectures during training to get the benefits of both. For example, we can gradually transition from linear to non-linear networks, deterministic to stochastic computation, shallow to deep architectures, or even simple downsampling to fully differentiable attention mechanisms. Benefits include increased accuracy, easier convergence with more complex architectures, solutions to test-time execution of batch normalization, and the ability to train networks of up to 200 layers.", "creator": "LaTeX with hyperref package"}}}