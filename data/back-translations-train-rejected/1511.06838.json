{"id": "1511.06838", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "21-Nov-2015", "title": "Mapping Images to Sentiment Adjective Noun Pairs with Factorized Neural Nets", "abstract": "We consider the visual sentiment task of mapping an image to an adjective noun pair (ANP) such as \"cute baby\". To capture the two-factor structure of our ANP semantics as well as to overcome annotation noise and ambiguity, we propose a novel factorized CNN model which learns separate representations for adjectives and nouns but optimizes the classification performance over their product. Our experiments on the publicly available SentiBank dataset show that our model significantly outperforms not only independent ANP classifiers on unseen ANPs and on retrieving images of novel ANPs, but also image captioning models which capture word semantics from co-occurrence of natural text; the latter turn out to be surprisingly poor at capturing the sentiment evoked by pure visual experience. That is, our factorized ANP CNN not only trains better from noisy labels, generalizes better to new images, but can also expands the ANP vocabulary on its own.", "histories": [["v1", "Sat, 21 Nov 2015 04:58:46 GMT  (17350kb,D)", "http://arxiv.org/abs/1511.06838v1", null]], "reviews": [], "SUBJECTS": "cs.CV cs.CL", "authors": ["takuya narihira", "damian borth", "stella x yu", "karl ni", "trevor darrell"], "accepted": false, "id": "1511.06838"}, "pdf": {"name": "1511.06838.pdf", "metadata": {"source": "CRF", "title": "Mapping Images to Sentiment Adjective Noun Pairs with Factorized Neural Nets", "authors": ["Takuya Narihira", "Damian Borth", "Stella X. Yu"], "emails": ["takuya.narihira@jp.sony.com", "damian.borth@dfki.de", "stellayu@berkeley.edu", "kni@iqt.org", "trevor@berkeley.edu"], "sections": [{"heading": "1. Introduction", "text": "An essential element in achieving this goal is the use of Adjective Noun Pair (ANP), the concepts that are both applied at the middle level of visual content. We consider the task of marking user-generated images with ANPs that visually convey a plausible feeling, e.g. the representation of women in public. This task can be subjective and holistic, e.g. a beautiful landscape [1], compared to object categorization [9], or purely visual attributes that represent an analysis [7, 15, 23]. It also has a simpler focus than image editing, which aims to describe an image as possible [21, 13].ANP labeling is related to broader and more abstract image analysis for aesthetics [6, 20], interest or emotions [11]."}, {"heading": "2. Sentiment ANP CNN Classifiers", "text": "The first is a simple classification model that treats each ANP as an independent class, while the other two models have an explicit classification layer for A and N, which is then jointly multiplied for the representation of the ANP class. ANP-Net: Basic ANP CNN Classifier. Fig. 4a shows the base model that treats the ANP prediction as a simple classification problem. We use VGG 16 layer as the base model and replace the final, fully connected layer \"fc8\" from the prediction of 1,000 ImageNet categories to the prediction of 1,523 sentiment ANP classes."}, {"heading": "3. Experiments and Results", "text": "This year, it is so far that it will only take a few days to achieve an outcome in which everyone else will participate."}], "references": [{"title": "What makes a beautiful landscape beautiful: Adjective noun pairs attention by eyetracking and gaze analysis", "author": ["Mohammad Al-Naser", "Seyyed Saleh Mozafari Chanijani", "Syed Saqib Bukhari", "Damian Borth", "Andreas Dengel"], "venue": "In Proceedings of the 1st International Workshop on Affect & Sentiment in Multimedia,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2015}, {"title": "Automatic Attribute Discovery and Characterization from Noisy Web Data", "author": ["T. Berg", "A. Berg", "J. Shih"], "venue": "ECCV, pages 663\u2013676. Springer, September", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2010}, {"title": "Large-scale Visual Sentiment Ontology and Detectors Using Adjective Noun Pairs", "author": ["D. Borth", "R. Ji", "T. Chen", "T. Breuel", "S.-F. Chang"], "venue": "ACM MM, pages 223\u2013232, October", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2013}, {"title": "DeepSentiBank: Visual Sentiment Concept Classification with Deep Convolutional Neural Networks", "author": ["T. Chen", "D. Borth", "T. Darrell", "S.-F. Chang"], "venue": "arXiv:1410.8586, October", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2014}, {"title": "Object- Based Visual Sentiment Concept Analysis and Application", "author": ["T. Chen", "F. Yu", "J. Chen", "Y. Cui", "Y.-Y. Chen", "S.-F. Chang"], "venue": "ACM MM, pages 367\u2013376, Novenber", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2014}, {"title": "Studying Aesthetics in Photographic Images using a Computational Approach", "author": ["R. Datta", "D. Joshi", "J. Li", "J. Wang"], "venue": "ECCV, pages 288\u2013301. Springer, May", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2006}, {"title": "Describing Objects by their Attributes", "author": ["A. Farhadi", "I. Endres", "D. Hoiem", "D. Forsyth"], "venue": "CVPR, pages 1778\u20131785, June", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2009}, {"title": "Learning bilinear models for two-factor problems in vision", "author": ["William T. Freeman", "J.B. Tenenbaum"], "venue": "In CVPR,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 1997}, {"title": "Multi-scale orderless pooling of deep convolutional activation features", "author": ["Yunchao Gong", "Liwei Wang", "Ruiqi Guo", "Svetlana Lazebnik"], "venue": "In ECCV,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2014}, {"title": "Delving deep into rectifiers: Surpassing human-level performance on imagenet classification", "author": ["Kaiming He", "Xiangyu Zhang", "Shaoqing Ren", "Jian Sun"], "venue": "CoRR, abs/1502.01852,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2015}, {"title": "What Makes an Image Memorable", "author": ["P. Isola", "J. Xiao", "A. Torralba", "A. Oliva"], "venue": "In CVPR, July", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2011}, {"title": "Caffe: Convolutional architecture for fast feature embedding", "author": ["Yangqing Jia", "Evan Shelhamer", "Jeff Donahue", "Sergey Karayev", "Jonathan Long", "Ross Girshick", "Sergio Guadarrama", "Trevor Darrell"], "venue": null, "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2014}, {"title": "Deep visual-semantic alignments for generating image descriptions", "author": ["Andrej Karpathy", "Li Fei-Fei"], "venue": "In CVPR,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2015}, {"title": "ImageNet Classification with Deep Convolutional Neural Networks", "author": ["A. Krizhevsky", "I. Sutskever", "G. Hinton"], "venue": "NIPS, pages 1106\u2013 1114, December", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2012}, {"title": "Learning to Detect Unseen Object Classes by Between-class Attribute Transfer", "author": ["C. Lampert", "H. Nickisch", "S. Harmeling"], "venue": "CVPR, pages 951\u2013958, June", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2009}, {"title": "Object Bank: A High-Level Image Representation for Scene Classification & Semantic Feature Sparsification", "author": ["L.-J. Li", "H. Su", "L. Fei-Fei", "E. Xing"], "venue": "NIPS, pages 1378\u20131386, December", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2010}, {"title": "Bilinear cnn models for fine-grained visual recognition", "author": ["Tsung-Yu Lin", "Aruni Roy Chowdhury", "Subhransu Maji"], "venue": "In Arxiv,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2015}, {"title": "Affective Image Classification using Features Inspired by Psychology and Art Theory", "author": ["J. Machajdik", "A. Hanbury"], "venue": "ACM MM, pages 83\u201392, October", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2010}, {"title": "Assessing the Aesthetic Quality of Photographs using Generic Image Descriptors", "author": ["L. Marchesotti", "F. Perronnin", "D. Larlus", "G. Csurka"], "venue": "ICCV, November", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2011}, {"title": "Im2text: Describing images using 1 million captioned photographs", "author": ["Vicente Ordonez", "Girish Kulkarni", "Tamara L Berg"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2011}, {"title": "Bilinear classifiers for visual recognition", "author": ["H. Pirsiavash", "D.Ramanan", "C.C. Fowlkes"], "venue": "In NIPS,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2009}, {"title": "Attribute Learning in Large-scale Datasets", "author": ["O. Russakovsky", "L. Fei-Fei"], "venue": "Trends and Topics in Computer Vision, pages 1\u201314. Springer,", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2012}, {"title": "Imagenet large scale visual recognition challenge", "author": ["Olga Russakovsky", "Jia Deng", "Hao Su", "Jonathan Krause", "Sanjeev Satheesh", "Sean Ma", "Zhiheng Huang", "Andrej Karpathy", "Aditya Khosla", "Michael Bernstein"], "venue": null, "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2014}, {"title": "Very Deep Convolutional Networks for Large-Scale Image Recognition", "author": ["K. Simonyan", "A. Zisserman"], "venue": "arXiv:1409.1556, September", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2014}, {"title": "Visual sentiment prediction with deep convolutional neural networks", "author": ["Can Xu", "Suleyman Cetintas", "Kuang-Chih Lee", "Li-Jia Li"], "venue": null, "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2014}, {"title": "In the Eye of the Beholder: Employing Statistical Analysis and Eye Tracking for Analyzing Abstract Paintings", "author": ["V. Yanulevskaya", "J. Uijlings", "E. Bruni", "A. Sartori", "E. Zamboni", "F. Bacci", "D. Melcher", "N. Sebe"], "venue": "ACM MM, pages 349\u2013.358, October", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2012}, {"title": "Emotional Valence Categorization using Holistic Image Features", "author": ["V. Yanulevskaya", "J. van Gemert", "K. Roth", "A. Herbold", "N. Sebe", "J.M. Geusebroek"], "venue": "In ICIP,", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2008}, {"title": "Robust image sentiment analysis using progressively trained and domain transferred deep networks", "author": ["Quanzeng You", "Jiebo Luo", "Hailin Jin", "Jianchao Yang"], "venue": "In AAAI,", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2015}], "referenceMentions": [{"referenceID": 2, "context": "Automatic assessment of sentiment from visual content has gained considerable attention [3, 4, 5, 26, 29].", "startOffset": 88, "endOffset": 105}, {"referenceID": 3, "context": "Automatic assessment of sentiment from visual content has gained considerable attention [3, 4, 5, 26, 29].", "startOffset": 88, "endOffset": 105}, {"referenceID": 4, "context": "Automatic assessment of sentiment from visual content has gained considerable attention [3, 4, 5, 26, 29].", "startOffset": 88, "endOffset": 105}, {"referenceID": 24, "context": "Automatic assessment of sentiment from visual content has gained considerable attention [3, 4, 5, 26, 29].", "startOffset": 88, "endOffset": 105}, {"referenceID": 27, "context": "Automatic assessment of sentiment from visual content has gained considerable attention [3, 4, 5, 26, 29].", "startOffset": 88, "endOffset": 105}, {"referenceID": 0, "context": "beautiful landscape [1], as compared to object detection [16, 24], scene categorization [9], or pure visual attribute analysis [7, 15, 2, 23].", "startOffset": 20, "endOffset": 23}, {"referenceID": 15, "context": "beautiful landscape [1], as compared to object detection [16, 24], scene categorization [9], or pure visual attribute analysis [7, 15, 2, 23].", "startOffset": 57, "endOffset": 65}, {"referenceID": 22, "context": "beautiful landscape [1], as compared to object detection [16, 24], scene categorization [9], or pure visual attribute analysis [7, 15, 2, 23].", "startOffset": 57, "endOffset": 65}, {"referenceID": 8, "context": "beautiful landscape [1], as compared to object detection [16, 24], scene categorization [9], or pure visual attribute analysis [7, 15, 2, 23].", "startOffset": 88, "endOffset": 91}, {"referenceID": 6, "context": "beautiful landscape [1], as compared to object detection [16, 24], scene categorization [9], or pure visual attribute analysis [7, 15, 2, 23].", "startOffset": 127, "endOffset": 141}, {"referenceID": 14, "context": "beautiful landscape [1], as compared to object detection [16, 24], scene categorization [9], or pure visual attribute analysis [7, 15, 2, 23].", "startOffset": 127, "endOffset": 141}, {"referenceID": 1, "context": "beautiful landscape [1], as compared to object detection [16, 24], scene categorization [9], or pure visual attribute analysis [7, 15, 2, 23].", "startOffset": 127, "endOffset": 141}, {"referenceID": 21, "context": "beautiful landscape [1], as compared to object detection [16, 24], scene categorization [9], or pure visual attribute analysis [7, 15, 2, 23].", "startOffset": 127, "endOffset": 141}, {"referenceID": 19, "context": "It also has a simpler focus than image captioning which aims to describe an image as completely and objectively as possible [21, 13].", "startOffset": 124, "endOffset": 132}, {"referenceID": 12, "context": "It also has a simpler focus than image captioning which aims to describe an image as completely and objectively as possible [21, 13].", "startOffset": 124, "endOffset": 132}, {"referenceID": 5, "context": "ANP labeling is related to broader and more abstract image analysis for aesthetics [6, 20], interestingness [11], affect or emotions [19, 28, 27].", "startOffset": 83, "endOffset": 90}, {"referenceID": 18, "context": "ANP labeling is related to broader and more abstract image analysis for aesthetics [6, 20], interestingness [11], affect or emotions [19, 28, 27].", "startOffset": 83, "endOffset": 90}, {"referenceID": 10, "context": "ANP labeling is related to broader and more abstract image analysis for aesthetics [6, 20], interestingness [11], affect or emotions [19, 28, 27].", "startOffset": 108, "endOffset": 112}, {"referenceID": 17, "context": "ANP labeling is related to broader and more abstract image analysis for aesthetics [6, 20], interestingness [11], affect or emotions [19, 28, 27].", "startOffset": 133, "endOffset": 145}, {"referenceID": 26, "context": "ANP labeling is related to broader and more abstract image analysis for aesthetics [6, 20], interestingness [11], affect or emotions [19, 28, 27].", "startOffset": 133, "endOffset": 145}, {"referenceID": 25, "context": "ANP labeling is related to broader and more abstract image analysis for aesthetics [6, 20], interestingness [11], affect or emotions [19, 28, 27].", "startOffset": 133, "endOffset": 145}, {"referenceID": 2, "context": "[3] uses a bank Adjective Noun girls baby face eyes", "startOffset": 0, "endOffset": 3}, {"referenceID": 3, "context": "of linear SVMs (SentiBank), and [4] uses deep CNNs.", "startOffset": 32, "endOffset": 35}, {"referenceID": 24, "context": "Deep CNNs have also been used for sentiment prediction [26, 29], but they are unable to model sentiment prediction by a mid-level representation such as ANPs.", "startOffset": 55, "endOffset": 63}, {"referenceID": 27, "context": "Deep CNNs have also been used for sentiment prediction [26, 29], but they are unable to model sentiment prediction by a mid-level representation such as ANPs.", "startOffset": 55, "endOffset": 63}, {"referenceID": 2, "context": "Our goal is to map an image onto embedding derived from the visual sentiment ontology [3] that is built completely from visual data and respects visual correlations along adjective (A) and noun (N) semantics.", "startOffset": 86, "endOffset": 89}, {"referenceID": 2, "context": "For example, not every ANP is popular on platforms like Flickr: adorable eyes and attractive baby are not frequent enough to have associated images in the visual sentiment dataset [3], suggesting that adorable is reserved more for overall impressions, whereas attractive is more for sexual appeal.", "startOffset": 180, "endOffset": 183}, {"referenceID": 7, "context": "Unlike the classical bilinear factorization model [8] which decomposes an image into style and content variations in a generative process, our model is discriminative and nonlinear.", "startOffset": 50, "endOffset": 53}, {"referenceID": 20, "context": "Compared to the bilinear SVM classifiers [22] which represents the classifier as a product of two low-rank matrices, our model learns both the feature and the classifier in a deep neural network achitecture.", "startOffset": 41, "endOffset": 45}, {"referenceID": 16, "context": "We emphasize that our factorized ANP CNN is only seemingly similar to the recent bilinear CNN model [18]; we differ completely on the problem, the architecture, and the technical approach.", "startOffset": 100, "endOffset": 104}, {"referenceID": 16, "context": "1) The bilinear CNN model [18] is a feature extractor; it takes the particular form of CNN products and the two CNNs have no particular meaning.", "startOffset": 26, "endOffset": 30}, {"referenceID": 2, "context": "Experimental results on the publicly available dataset [3] demonstrate that our model significantly outperforms independent ANP classification on unseen ANPs, and on retrieving images of new ANP vocabulary.", "startOffset": 55, "endOffset": 58}, {"referenceID": 23, "context": "We use VGG 16layer [25] as a base model and replace the final fully connected layer \u201cfc8\u201d from predicting 1,000 ImageNet categories to predicting 1,523 sentiment ANP classes.", "startOffset": 19, "endOffset": 23}, {"referenceID": 2, "context": "VSO was created by mining online platforms such as Flickr and Youtube by the 24 emotions from Plutchnik\u2019s Wheel of Emotions [3].", "startOffset": 124, "endOffset": 127}, {"referenceID": 2, "context": "We use the publicly available dataset of Flickr images introduced in [3] with SentiBank 1.", "startOffset": 69, "endOffset": 72}, {"referenceID": 9, "context": "The fc7-a and fc7-n layers are followed by a parametric ReLU (PReLU) layer for better convergence [10].", "startOffset": 98, "endOffset": 102}, {"referenceID": 11, "context": "Our models are implemented using our modified branch of CAFFE [12].", "startOffset": 62, "endOffset": 66}, {"referenceID": 12, "context": "For the two recurrent models, we expand and modify Andrej Karpathy\u2019s \u201dneuraltalk\u201d Github branch [13].", "startOffset": 96, "endOffset": 100}, {"referenceID": 2, "context": "Table 1: Visual Sentiment Ontology ANP statistics [3].", "startOffset": 50, "endOffset": 53}, {"referenceID": 3, "context": "DeepSentiBank [4] 9.", "startOffset": 14, "endOffset": 17}, {"referenceID": 3, "context": "DeepSentiBank [4] - ANP-Net - Fork-Net 0.", "startOffset": 14, "endOffset": 17}, {"referenceID": 3, "context": "We take the DeepSentiBank model [4] as a baseline, which already outperforms the initial SentiBank 1.", "startOffset": 32, "endOffset": 35}, {"referenceID": 2, "context": "model [3].", "startOffset": 6, "endOffset": 9}, {"referenceID": 13, "context": "It uses the AlexNet architecture [14] but fine-tuned to the ANP classes.", "startOffset": 33, "endOffset": 37}], "year": 2015, "abstractText": "We consider the visual sentiment task of mapping an image to an adjective noun pair (ANP) such as \u201dcute baby\u201d. To capture the two-factor structure of our ANP semantics as well as to overcome annotation noise and ambiguity, we propose a novel factorized CNN model which learns separate representations for adjectives and nouns but optimizes the classification performance over their product. Our experiments on the publicly available SentiBank dataset show that our model significantly outperforms not only independent ANP classifiers on unseen ANPs and on retrieving images of novel ANPs, but also image captioning models which capture word semantics from co-occurrence of natural text; the latter turn out to be surprisingly poor at capturing the sentiment evoked by pure visual experience. That is, our factorized ANP CNN not only trains better from noisy labels, generalizes better to new images, but can also expands the ANP vocabulary on its own.", "creator": "LaTeX with hyperref package"}}}