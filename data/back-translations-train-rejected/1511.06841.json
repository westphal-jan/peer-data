{"id": "1511.06841", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "21-Nov-2015", "title": "Online Sequence Training of Recurrent Neural Networks with Connectionist Temporal Classification", "abstract": "Connectionist temporal classification (CTC) based supervised sequence training of recurrent neural networks (RNNs) has shown great success in many machine learning areas including end-to-end speech and handwritten character recognition. For the CTC training, however, it is required to unroll the RNN by the length of an input sequence. This unrolling requires a lot of memory and hinders a small footprint implementation of online learning or adaptation. Furthermore, the length of training sequences is usually not uniform, which makes parallel training with multiple sequences inefficient on shared memory models such as graphics processing units (GPUs). In this work, we introduce an expectation-maximization (EM) based online CTC algorithm that enables unidirectional RNNs to learn sequences that are longer than the amount of unrolling. The RNNs can also be trained to process an infinitely long input sequence without pre-segmentation or external reset. Moreover, the proposed approach allows efficient parallel training on GPUs. For evaluation, end-to-end speech recognition examples are presented on the Wall Street Journal (WSJ) corpus.", "histories": [["v1", "Sat, 21 Nov 2015 05:22:37 GMT  (510kb)", "https://arxiv.org/abs/1511.06841v1", null], ["v2", "Mon, 30 Nov 2015 19:10:36 GMT  (394kb)", "http://arxiv.org/abs/1511.06841v2", "Submitted to ICLR 2016"], ["v3", "Tue, 1 Dec 2015 12:09:14 GMT  (394kb)", "http://arxiv.org/abs/1511.06841v3", "Submitted to ICLR 2016"], ["v4", "Thu, 7 Jan 2016 20:52:42 GMT  (395kb)", "http://arxiv.org/abs/1511.06841v4", "Submitted to ICLR 2016"], ["v5", "Thu, 2 Feb 2017 13:42:49 GMT  (395kb)", "http://arxiv.org/abs/1511.06841v5", "Final version: Kyuyeon Hwang and Wonyong Sung, \"Sequence to Sequence Training of CTC-RNNs with Partial Windowing,\" Proceedings of The 33rd International Conference on Machine Learning, pp. 2178-2187, 2016. URL:this http URL"]], "reviews": [], "SUBJECTS": "cs.LG cs.NE", "authors": ["kyuyeon hwang", "wonyong sung"], "accepted": false, "id": "1511.06841"}, "pdf": {"name": "1511.06841.pdf", "metadata": {"source": "CRF", "title": null, "authors": [], "emails": ["kyuyeon.hwang@gmail.com", "wysung@snu.ac.kr"], "sections": [{"heading": null, "text": "ar Xiv: 151 1.06 841v 5 [cs.L G] 2F eb"}, {"heading": "1 INTRODUCTION", "text": "This year is the highest in the history of the country."}, {"heading": "2 CONNECTIONIST TEMPORAL CLASSIFICATION (CTC)", "text": "1Further experiments are performed on TIMIT (Garofolo et al., 1993) in Appendix B.The CTC algorithm (Graves et al., 2006; 2012) takes into account the sequence of output labels of RNNs with ignoring alignments or timings by introducing an additional blank label, b. For the set of input labels, L is and its extended sentence with the additional CTC blank label, L \u2032 = L \u2032 b \u00b2, the path is defined as a sequence via L \u2032, i.e., the length of the input label is L \u2032 T, where T is the length of the input label, x. Then, the output label z \u00b2 L \u2264 T is represented by z = F (\u03c0) with the sequence mapping functionF. F maps each path label with the length T into the shorter sequence of the label, z, by merging the successive labels, and then merging the labels into a sequence of the labels, e.g."}, {"heading": "3 ONLINE SEQUENCE TRAINING", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "3.1 PROBLEM DEFINITION", "text": "Throughout the work, the problem of online sequence training is defined as follows: \u2022 The training set S consists of pairs of input sequence x and the corresponding target sequence z, i.e. (x, z) \u0445 S. \u2022 The estimation model M learns the general mapping z = G (x), with the training sequences (x, z) and the entire target sequence z continuously given. \u2022 The length of the input sequence, | x |, is unknown unless t = | x |. \u2022 The parameters of the estimation model M are updated in the way of online learning, i.e. they can be updated frequently even before the entire input sequence x. This online learning problem usually occurs when a person learns a language from texts and becomes the appropriate example for the audio sources when access to the films is incomplete (x)."}, {"heading": "3.2 OVERVIEW OF THE PROPOSED APPROACH", "text": "We propose an online CTC training with BPTT (h; h \") algorithm where the RNN can learn the sequences for longer than the rolling quantity, h. The algorithm is based on the shortened backflow through time (BPTT) algorithm (Werbos, 1990) with the forward error magnitude of h\" and the rolling quantity of h, \"which is referred to as BPTT (h; h\"), as proposed in Williams & Peng (1990). Algorithm 1 describes the BPTT (h; h \") algorithm combined with the CTC loss, where T is the length of the training sequence, xAlgorithm 1 Online CTC training with BPTT (h; h\") for a single sequence BPTT (h; h \") algorithm combined with the CTC loss, where T is the length of the training sequence, xAlgorithm 1 Online CTC training with BPTT (h) with the single sequence BTh: (h)."}, {"heading": "3.3 CTC-TR: STANDARD CTC WITH TRUNCATION", "text": "With the standard CTC algorithm, it is not possible to calculate the backward variables at \u03c4n < T, because the future information beyond \u03c4n cannot be accessed. Therefore, we calculate the CTC errors only at the last iteration, with \u03c4n = T as in Algorithm 2. In this case, however, the gradients are only available in the roll-off area. Since the backward propagation is truncated at the beginning of the roll-off area, we designate the CTC algorithm in this area as truncated CTC or CTC-TR. In addition, we designate the area covered by the CTC-TR algorithm as CTC-TR coverage. The RNN can only be trained with CTC-TR if there are sufficient labels within the CTCTR coverage."}, {"heading": "3.4 CTC-EM: ONLINE CTC WITH EXPECTATION-MAXIMIZATION (EM)", "text": "Suppose that only a fraction of the input sequence, x1: \u03c4, is available. Then, as shown in Figure 3, there are | z | + 1 possible partial markers. (6) One of the simplest approaches to forming the network under this condition is to select the most likely partial alignment of Z and calculate the standard CTC error by treating partial alignment as a truth label. For example, we can select z1: m \u00b2 where m \u00b2 = argmaxm \u03b1 (m) 2Although z1: m is not possible by the standard CTC formulation, we can still say that z1: m is a possible label with a probability of zero without loss of ality.da: m \u00b2 a problem with a probability of 1."}, {"heading": "4 TRAINING CONTINUOUSLY RUNNING RNNS", "text": "In this section, the online CTC algorithm for training proposed in Section 3 is extended by infinitely long currents. Of course, the training current can be very long with the target sequence boundaries or can be generated by concatenating training sequences in a particular order. If it is trained in this training stream without external resetting of the RNN at the sequence boundaries, the resulting RNN can also continuously process infinitely long input streams without pre-segmentation or external reset. This feature is useful for real-time speech recognition or keyword spotting systems as we can remove the front-end voice activity detector (Sohn et al., 1999) for detecting and pre-sequencing expressions. The CTC (h; h \u2032) algorithm can be applied directly to the infinitely long training streams, as shown in Figure 4. If the sequence boundaries of the pre-sequence marker are reached during forward marking, we should perform CTC-TR correction with two, the next forward variation and the next one without any problems."}, {"heading": "5 PARALLEL TRAINING", "text": "In a massively parallel shared memory model such as a GPU, efficient parallel training is achieved by using the memory hierarchy; for example, the common computation of multiple frames reduces the number of read operations of network parameters from slow off-chip memory by temporarily storing them on the on-chip cache memory and reusing them multiple times; to train RNNs on a GPU, this parallelism can be explored by using multiple training sequences simultaneously (Hwang & Sung, 2015); the continuous CTC (h; h \u2032) algorithm in Section 4 can be directly expanded for parallel training with multiple streams; since the forward step size and roll-off quantity of TC are specified, the RNN can be synchronized forward, backward, gradient calculation and weight update steps across multiple training streams; thus, the GPU-based parallelization approach can be further applied in Training Cwang & Sang (2015), although the RNN can be applied relatively quickly in the RPU calculations."}, {"heading": "6 EXPERIMENTS", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "6.1 END-TO-END SPEECH RECOGNITION WITH RNNS", "text": "To evaluate the proposed approach, we present examples of character-level speech recognition with continuously trained RNNs without external speech models. RNN speech recognition is similar to that used in Graves & Jaitly (2014), except that our model uses a long-term unidirectional short-term memory (LSTM) network (Hochreiter & Schmidhuber, 1997) trained with the online CTC algorithm on the continuous stream of speech, instead of the bi-directional LSTM network with sentence-based CTC training. Specifically, experiments are being conducted with the deep unidirectional LSTM network with 3 LSTM layers in which each layer has 768 LSTM cells. The output layer is a 31-dimensional Softmax layer. Each unit of the Softmax layer represents one of the rear probabilities of 26 alphabet characters, two special characters (. and '), a whitespace character, the end of sentence, the EOS layer, and the energy layer in front of the 123 alphabet characters."}, {"heading": "6.2 WALL STREET JOURNAL (WSJ) CORPUS", "text": "The experiments are conducted on the corpus of the Wall Street Journal (WSJ) (Paul & Baker, 1992), using the subset of the SI-284 set of the WSJ, containing only utterances with non-verbalized punctuations (NVPs), resulting in approximately 71 hours of utterances.The histogram of the length of the sequences in the training set is shown in Figure 5. Note that the average length of the sequences is 772.5 frames. If we unroll the network using N-frames, the sequences that are longer than N-frames will not be fully analyzed by CTC-TR.In Figure 6, the CTC-TR coverage is further analyzed in terms of the length of the sequence and the rolling amount. If the sequence stream is trained using the Continuos-CTC algorithm, the CTC-TR coverage is calculated as a function of the framesets of the CTC (h; h ') as the maximum coverage, so that the average coverage of the CTC is determined by the CTC; the assumption that the CTC-4h coverage is double; the average coverage of the assumption that the CTC-4h is; the assumption that the CTC-4h will be distributed as the maximum coverage."}, {"heading": "6.3 TRAINING PROCEDURE", "text": "The networks are trained on a GPU as in Section 5 with memory usage limitation. To maintain memory usage to change the same amount of rollouts, we have set the total amount of rollouts over multiple training streams at 16,384. For example, the number of parallel streams 8 with the rolling amount of 2,048 and 32 with 512 rollings is set to be about 9.5 GiB in our implementation based on Hwang & Sung (2015). The network performance evaluation is performed on each 10,485,760 training framework (i.e. N continuous training streams with the length of 10, 485, 760 / N each) in terms of the word error rate (WHO), of which the length is 16.384 each. For this mean evaluation, the best path decoding (Graves et al., 2006) is used for quick calculation."}, {"heading": "6.4 EVALUATION", "text": "The convergence curves of the CTC-TR decrease only when the rolling quantity decreases. This is because the percentage of effective training frames decreases due to reduced CTC-TR coverage. It can also be observed that the performance of the CTC-TR only trained network with 512 times the rolling quantity corresponds to the worse WHO than that of the other networks due to the reduced size of the effective training set. On the other hand, the convergence curves of the CTC-TR and the CTC-EM combined training with the rolling quantities of 256 and 512 are similar to that of the CTC-TR only for training with 2,048 times the rolling speed. Considering that the average sequence duration of the training is x, the results are quite encouraging."}, {"heading": "7 CONCLUDING REMARKS", "text": "The proposed algorithm consists of CTC-TR and CTC-EM. CTC-TR is the standard CTC with circumcision algorithm, and CTC-EM is the general EM-based algorithm that covers the training frames to which CTC-TR cannot be applied. The proposed algorithm requires less than the length of the training sequence and is suitable for online learning systems with a small footprint or massive parallel implementation on a common memory model such as a GPU. In addition, the online CTC algorithm is extended for training that continuously executes RNNNs without external reset and evaluates them in the TIMIT experiments with a continuous input language. On the WSJ corpus, the experimental results indicate that with limited storage capacity, the proposed approach achieves significant acceleration on a GPU, without using the performance of the resulting from the training, if no further performance steps are taken."}, {"heading": "ACKNOWLEDGMENTS", "text": "This work was partially supported by the Brain Korea 21 Plus Project and the Korean Government-funded National Research Foundation of Korea (NRF) (No. 2015R1A2A1A10056051)."}, {"heading": "A DERIVATION OF THE CTC-EM EQUATIONS", "text": "In the maximization step, the goal is to derive from k = K = K = K = W (W = W = W = W = W (W): W (W): W (W): W (W): W (W): W (W): W (W): W (W): W (W): W (W): W (W): W (W): W (W): W (W): W (Z): W (Z): W (Z): W (Z): W (Z): W (Z): W (Z): W (Z): W (Z): W (Z): W (Z): W (Z): W (Z): W (Z): W (Z): W (Z): W (Z): W (Z): W (Z): W (Z): W (W): (W): (W): W (Z): W (Z): W (Z): W (Z): W (Z): W (Z): W (Z): W (Z): W (Z): W (Z): W (Z): W (Z): W (Z): W (Z): W (Z): W (Z): W (Z): W (Z): W (Z): W (Z): W (Z): W (Z): W (Z: W (Z): W (Z): W (Z: W (Z): W (Z: W (Z): W (Z: W): W (Z: (Z): W: (W): (W): (W: (W): (W): (W: (W): (W): (W: (W): (W): (W: (W): (W): (W): (W): (W): (W: (W): (W): (W): (W: (W): (W): (W): (W: (W): (W): (W): (W)."}, {"heading": "B PHONEME RECOGNITION ON TIMIT", "text": "In fact, it is the case that most people are able to determine for themselves what they want and what they want. (...) In fact, it is the case that most people are able to determine for themselves what they want. (...) It is the case that most people are able to determine for themselves. (...) It is the case that most people are able to determine for themselves. (...) It is the case that they are able to determine for themselves. (...) It is the case that they are able to determine for themselves. (...) It is the case that they are able to determine for themselves what they want. (...) It is the case that they want to, as if they want to, as if they want to, as if they want to, as if they want to, as if they want to, as if they want to, as if they want to or not. \"(...) It is as if they want to.\" (...) It is as if they want to. (...) It is as if they want to."}], "references": [{"title": "Neural machine translation by jointly learning to align and translate", "author": ["Bahdanau", "Dzmitry", "Cho", "Kyunghyun", "Bengio", "Yoshua"], "venue": "arXiv preprint arXiv:1409.0473,", "citeRegEx": "Bahdanau et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2014}, {"title": "End-to-end attention-based large vocabulary speech recognition", "author": ["Bahdanau", "Dzmitry", "Chorowski", "Jan", "Serdyuk", "Dmitriy", "Brakel", "Philemon", "Bengio", "Yoshua"], "venue": "arXiv preprint arXiv:1508.04395,", "citeRegEx": "Bahdanau et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2015}, {"title": "Advances in optimizing recurrent networks", "author": ["Bengio", "Yoshua", "Boulanger-Lewandowski", "Nicolas", "Pascanu", "Razvan"], "venue": "In Acoustics, Speech and Signal Processing (ICASSP),", "citeRegEx": "Bengio et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Bengio et al\\.", "year": 2013}, {"title": "Probabilistic interpretation of feedforward classification network outputs, with relationships to statistical pattern recognition", "author": ["Bridle", "John S"], "venue": "In Neurocomputing,", "citeRegEx": "Bridle and S.,? \\Q1990\\E", "shortCiteRegEx": "Bridle and S.", "year": 1990}, {"title": "Listen, attend and spell", "author": ["Chan", "William", "Jaitly", "Navdeep", "Le", "Quoc V", "Vinyals", "Oriol"], "venue": "arXiv preprint arXiv:1508.01211,", "citeRegEx": "Chan et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Chan et al\\.", "year": 2015}, {"title": "Efficient GPU-based training of recurrent neural network language models using spliced sentence bunch", "author": ["Chen", "Xie", "Wang", "Yongqiang", "Liu", "Xunying", "Gales", "Mark JF", "Woodland", "Philip C"], "venue": "In Proc. Interspeech,", "citeRegEx": "Chen et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Chen et al\\.", "year": 2014}, {"title": "Learning phrase representations using RNN encoderdecoder for statistical machine translation", "author": ["Cho", "Kyunghyun", "Van Merri\u00ebnboer", "Bart", "Gulcehre", "Caglar", "Bahdanau", "Dzmitry", "Bougares", "Fethi", "Schwenk", "Holger", "Bengio", "Yoshua"], "venue": "arXiv preprint arXiv:1406.1078,", "citeRegEx": "Cho et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Cho et al\\.", "year": 2014}, {"title": "Attention-based models for speech recognition", "author": ["Chorowski", "Jan", "Bahdanau", "Dzmitry", "Serdyuk", "Dmitriy", "Cho", "Kyunghyun", "Bengio", "Yoshua"], "venue": "arXiv preprint arXiv:1506.07503,", "citeRegEx": "Chorowski et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Chorowski et al\\.", "year": 2015}, {"title": "Incremental speech recognition for multimodal interfaces", "author": ["Fink", "Gernot", "Schillo", "Christoph", "Kummert", "Franz", "Sagerer", "Gerhard"], "venue": "In Industrial Electronics Society,", "citeRegEx": "Fink et al\\.,? \\Q1998\\E", "shortCiteRegEx": "Fink et al\\.", "year": 1998}, {"title": "A novel word spotting method based on recurrent neural networks. Pattern Analysis and Machine Intelligence", "author": ["Frinken", "Volkmar", "Fischer", "Andreas", "R Manmatha", "Bunke", "Horst"], "venue": "IEEE Transactions on,", "citeRegEx": "Frinken et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Frinken et al\\.", "year": 2012}, {"title": "DARPA TIMIT acoustic-phonetic continous speech corpus CD-ROM. NIST speech disc 1-1.1", "author": ["Garofolo", "John S", "Lamel", "Lori F", "Fisher", "William M", "Fiscus", "Jonathon G", "Pallett", "David S"], "venue": "NASA STI/Recon Technical Report N,", "citeRegEx": "Garofolo et al\\.,? \\Q1993\\E", "shortCiteRegEx": "Garofolo et al\\.", "year": 1993}, {"title": "Towards end-to-end speech recognition with recurrent neural networks", "author": ["Graves", "Alex", "Jaitly", "Navdeep"], "venue": "In Proceedings of the 31st International Conference on Machine Learning", "citeRegEx": "Graves et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Graves et al\\.", "year": 2014}, {"title": "Framewise phoneme classification with bidirectional LSTM and other neural network architectures", "author": ["Graves", "Alex", "Schmidhuber", "J\u00fcrgen"], "venue": "Neural Networks,", "citeRegEx": "Graves et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Graves et al\\.", "year": 2005}, {"title": "Connectionist temporal classification: labelling unsegmented sequence data with recurrent neural networks", "author": ["Graves", "Alex", "Fern\u00e1ndez", "Santiago", "Gomez", "Faustino", "Schmidhuber", "J\u00fcrgen"], "venue": "In Proceedings of the 23rd international conference on Machine learning,", "citeRegEx": "Graves et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Graves et al\\.", "year": 2006}, {"title": "Unconstrained on-line handwriting recognition with recurrent neural networks", "author": ["Graves", "Alex", "Liwicki", "Marcus", "Bunke", "Horst", "Schmidhuber", "J\u00fcrgen", "Fern\u00e1ndez", "Santiago"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Graves et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Graves et al\\.", "year": 2008}, {"title": "Speech recognition with deep recurrent neural networks", "author": ["Graves", "Alex", "Mohamed", "Abdel-rahman", "Hinton", "Geoffrey"], "venue": "In Acoustics, Speech and Signal Processing (ICASSP),", "citeRegEx": "Graves et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Graves et al\\.", "year": 2013}, {"title": "Supervised sequence labelling with recurrent neural networks, volume 385", "author": ["Graves", "Alex"], "venue": null, "citeRegEx": "Graves and Alex,? \\Q2012\\E", "shortCiteRegEx": "Graves and Alex", "year": 2012}, {"title": "Deepspeech: Scaling up endto-end speech recognition", "author": ["Hannun", "Awni", "Case", "Carl", "Casper", "Jared", "Catanzaro", "Bryan", "Diamos", "Greg", "Elsen", "Erich", "Prenger", "Ryan", "Satheesh", "Sanjeev", "Sengupta", "Shubho", "Coates", "Adam"], "venue": "arXiv preprint arXiv:1412.5567,", "citeRegEx": "Hannun et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Hannun et al\\.", "year": 2014}, {"title": "Improving neural networks by preventing co-adaptation of feature detectors", "author": ["Hinton", "Geoffrey E", "Srivastava", "Nitish", "Krizhevsky", "Alex", "Sutskever", "Ilya", "Salakhutdinov", "Ruslan R"], "venue": "arXiv preprint arXiv:1207.0580,", "citeRegEx": "Hinton et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Hinton et al\\.", "year": 2012}, {"title": "Long short-term memory", "author": ["Hochreiter", "Sepp", "Schmidhuber", "J\u00fcrgen"], "venue": "Neural computation,", "citeRegEx": "Hochreiter et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Hochreiter et al\\.", "year": 1997}, {"title": "Single stream parallelization of generalized LSTM-like RNNs on a GPU", "author": ["Hwang", "Kyuyeon", "Sung", "Wonyong"], "venue": "In Acoustics, Speech and Signal Processing (ICASSP),", "citeRegEx": "Hwang et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Hwang et al\\.", "year": 2015}, {"title": "Speaker-independent phone recognition using hidden Markov models", "author": ["Lee", "Kai-Fu", "Hon", "Hsiao-Wuen"], "venue": "Acoustics, Speech and Signal Processing, IEEE Transactions on,", "citeRegEx": "Lee et al\\.,? \\Q1989\\E", "shortCiteRegEx": "Lee et al\\.", "year": 1989}, {"title": "A method for unconstrained convex minimization problem with the rate of convergence O (1/k2)", "author": ["Nesterov", "Yurii"], "venue": "In Doklady AN SSSR,", "citeRegEx": "Nesterov and Yurii.,? \\Q1983\\E", "shortCiteRegEx": "Nesterov and Yurii.", "year": 1983}, {"title": "The design for the Wall Street Journal-based CSR corpus", "author": ["Paul", "Douglas B", "Baker", "Janet M"], "venue": "In Proceedings of the workshop on Speech and Natural Language,", "citeRegEx": "Paul et al\\.,? \\Q1992\\E", "shortCiteRegEx": "Paul et al\\.", "year": 1992}, {"title": "A statistical model-based voice activity detection", "author": ["Sohn", "Jongseo", "Kim", "Nam Soo", "Sung", "Wonyong"], "venue": "Signal Processing Letters, IEEE,", "citeRegEx": "Sohn et al\\.,? \\Q1999\\E", "shortCiteRegEx": "Sohn et al\\.", "year": 1999}, {"title": "Sequence to sequence learning with neural networks. In Advances in neural information processing", "author": ["Sutskever", "Ilya", "Vinyals", "Oriol", "Le", "Quoc VV"], "venue": null, "citeRegEx": "Sutskever et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Sutskever et al\\.", "year": 2014}, {"title": "Backpropagation through time: what it does and how to do it", "author": ["Werbos", "Paul J"], "venue": "Proceedings of the IEEE,", "citeRegEx": "Werbos and J.,? \\Q1990\\E", "shortCiteRegEx": "Werbos and J.", "year": 1990}, {"title": "An efficient gradient-based algorithm for on-line training of recurrent network trajectories", "author": ["Williams", "Ronald J", "Peng", "Jing"], "venue": "Neural Computation,", "citeRegEx": "Williams et al\\.,? \\Q1990\\E", "shortCiteRegEx": "Williams et al\\.", "year": 1990}, {"title": "Recurrent neural network regularization", "author": ["Zaremba", "Wojciech", "Sutskever", "Ilya", "Vinyals", "Oriol"], "venue": "arXiv preprint arXiv:1409.2329,", "citeRegEx": "Zaremba et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Zaremba et al\\.", "year": 2014}, {"title": "ADADELTA: An adaptive learning rate method", "author": ["Zeiler", "Matthew D"], "venue": "arXiv preprint arXiv:1212.5701,", "citeRegEx": "Zeiler and D.,? \\Q2012\\E", "shortCiteRegEx": "Zeiler and D.", "year": 2012}], "referenceMentions": [{"referenceID": 0, "context": "When combined with recurrent neural networks (RNNs), supervised sequence learning has shown great success in many applications including machine translation (Bahdanau et al., 2014; Sutskever et al., 2014; Cho et al., 2014), speech recognition (Graves et al.", "startOffset": 157, "endOffset": 222}, {"referenceID": 25, "context": "When combined with recurrent neural networks (RNNs), supervised sequence learning has shown great success in many applications including machine translation (Bahdanau et al., 2014; Sutskever et al., 2014; Cho et al., 2014), speech recognition (Graves et al.", "startOffset": 157, "endOffset": 222}, {"referenceID": 6, "context": "When combined with recurrent neural networks (RNNs), supervised sequence learning has shown great success in many applications including machine translation (Bahdanau et al., 2014; Sutskever et al., 2014; Cho et al., 2014), speech recognition (Graves et al.", "startOffset": 157, "endOffset": 222}, {"referenceID": 15, "context": ", 2014), speech recognition (Graves et al., 2013; Graves & Jaitly, 2014; Hannun et al., 2014; Bahdanau et al., 2015; Chorowski et al., 2015; Chan et al., 2015), and handwritten character recognition (Graves et al.", "startOffset": 28, "endOffset": 159}, {"referenceID": 17, "context": ", 2014), speech recognition (Graves et al., 2013; Graves & Jaitly, 2014; Hannun et al., 2014; Bahdanau et al., 2015; Chorowski et al., 2015; Chan et al., 2015), and handwritten character recognition (Graves et al.", "startOffset": 28, "endOffset": 159}, {"referenceID": 1, "context": ", 2014), speech recognition (Graves et al., 2013; Graves & Jaitly, 2014; Hannun et al., 2014; Bahdanau et al., 2015; Chorowski et al., 2015; Chan et al., 2015), and handwritten character recognition (Graves et al.", "startOffset": 28, "endOffset": 159}, {"referenceID": 7, "context": ", 2014), speech recognition (Graves et al., 2013; Graves & Jaitly, 2014; Hannun et al., 2014; Bahdanau et al., 2015; Chorowski et al., 2015; Chan et al., 2015), and handwritten character recognition (Graves et al.", "startOffset": 28, "endOffset": 159}, {"referenceID": 4, "context": ", 2014), speech recognition (Graves et al., 2013; Graves & Jaitly, 2014; Hannun et al., 2014; Bahdanau et al., 2015; Chorowski et al., 2015; Chan et al., 2015), and handwritten character recognition (Graves et al.", "startOffset": 28, "endOffset": 159}, {"referenceID": 14, "context": ", 2015), and handwritten character recognition (Graves et al., 2008; Frinken et al., 2012).", "startOffset": 47, "endOffset": 90}, {"referenceID": 9, "context": ", 2015), and handwritten character recognition (Graves et al., 2008; Frinken et al., 2012).", "startOffset": 47, "endOffset": 90}, {"referenceID": 13, "context": "Although several attention-based models have been introduced recently, connectionist temporal classification (CTC) (Graves et al., 2006) is still one of the most successful techniques in practice, especially for end-to-end speech and character recognition tasks (Graves & Jaitly, 2014; Hannun et al.", "startOffset": 115, "endOffset": 136}, {"referenceID": 17, "context": ", 2006) is still one of the most successful techniques in practice, especially for end-to-end speech and character recognition tasks (Graves & Jaitly, 2014; Hannun et al., 2014; Graves et al., 2008; Frinken et al., 2012).", "startOffset": 133, "endOffset": 220}, {"referenceID": 14, "context": ", 2006) is still one of the most successful techniques in practice, especially for end-to-end speech and character recognition tasks (Graves & Jaitly, 2014; Hannun et al., 2014; Graves et al., 2008; Frinken et al., 2012).", "startOffset": 133, "endOffset": 220}, {"referenceID": 9, "context": ", 2006) is still one of the most successful techniques in practice, especially for end-to-end speech and character recognition tasks (Graves & Jaitly, 2014; Hannun et al., 2014; Graves et al., 2008; Frinken et al., 2012).", "startOffset": 133, "endOffset": 220}, {"referenceID": 8, "context": "Therefore, the bidirectional RNNs are not suitable for realtime applications such as incremental speech recognition (Fink et al., 1998), that require low-latency output from the RNN.", "startOffset": 116, "endOffset": 135}, {"referenceID": 5, "context": "For unidirectional RNNs, this problem can be addressed by concatenating sequences to make a very long stream of sequences, and training the RNNs with synchronized fixed-length unroll-windows over multiple training streams (Chen et al., 2014; Hwang & Sung, 2015).", "startOffset": 222, "endOffset": 261}, {"referenceID": 10, "context": "It is shown by Graves et al. (2012) that CTC can also be employed for sequence training of unidirectional RNNs on a phoneme recognition task.", "startOffset": 15, "endOffset": 36}, {"referenceID": 5, "context": "For unidirectional RNNs, this problem can be addressed by concatenating sequences to make a very long stream of sequences, and training the RNNs with synchronized fixed-length unroll-windows over multiple training streams (Chen et al., 2014; Hwang & Sung, 2015). However, it is not straightforward to apply this approach to the CTC training, since the standard CTC algorithm requires full unrolling for the backward variable propagation, which starts from the end of the sequence. In this paper, we propose an expectation-maximization (EM) based online CTC algorithm for sequence training of unidirectional RNNs. The algorithm allows training sequences to be longer than the amount of the network unroll. Moreover, it can be applied to infinitely long input streams with roughly segmented target sequences (e.g. only with the utterance boundaries and the corresponding transcriptions for training an end-to-end speech recognition RNN). Then, the resulting RNN can run continuously without pre-segmentation or external reset. Due to the fixed unroll amount, the proposed algorithm is suitable for online learning or adaptation systems with constrained hardware resource. Furthermore, the approach can directly be combined with the GPU based parallel RNN training algorithm described in Hwang & Sung (2015). For evaluation, we present examples of end-to-end speech recognition on the Wall Street Journal (WSJ) corpus (Paul & Baker, 1992) with continuously running RNNs.", "startOffset": 223, "endOffset": 1305}, {"referenceID": 10, "context": "Further experiments are performed on TIMIT (Garofolo et al., 1993) in Appendix B.", "startOffset": 43, "endOffset": 66}, {"referenceID": 13, "context": "The CTC algorithm (Graves et al., 2006; 2012) considers the order of the output labels of RNNs with ignoring the alignments or timings by introducing an additional blank label, b.", "startOffset": 18, "endOffset": 45}, {"referenceID": 24, "context": "This property is useful for realtime speech recognition or keyword spotting (spoken term detection) systems since we can remove the frontend voice activity detector (Sohn et al., 1999) for detecting and pre-segmenting utterances.", "startOffset": 165, "endOffset": 184}, {"referenceID": 13, "context": "For this intermediate evaluation, best path decoding (Graves et al., 2006) is employed for fast computation.", "startOffset": 53, "endOffset": 74}, {"referenceID": 2, "context": "9 (Nesterov, 1983; Bengio et al., 2013).", "startOffset": 2, "endOffset": 39}, {"referenceID": 10, "context": "See Appendix B for further experiments on TIMIT (Garofolo et al., 1993).", "startOffset": 48, "endOffset": 71}], "year": 2017, "abstractText": "Connectionist temporal classification (CTC) based supervised sequence training of recurrent neural networks (RNNs) has shown great success in many machine learning areas including end-to-end speech and handwritten character recognition. For the CTC training, however, it is required to unroll (or unfold) the RNN by the length of an input sequence. This unrolling requires a lot of memory and hinders a small footprint implementation of online learning or adaptation. Furthermore, the length of training sequences is usually not uniform, which makes parallel training with multiple sequences inefficient on shared memory models such as graphics processing units (GPUs). In this work, we introduce an expectation-maximization (EM) based online CTC algorithm that enables unidirectional RNNs to learn sequences that are longer than the amount of unrolling. The RNNs can also be trained to process an infinitely long input sequence without pre-segmentation or external reset. Moreover, the proposed approach allows efficient parallel training on GPUs. For evaluation, phoneme recognition and end-to-end speech recognition examples are presented on the TIMIT and Wall Street Journal (WSJ) corpora, respectively. Our online model achieves 20.7% phoneme error rate (PER) on the very long input sequence that is generated by concatenating all 192 utterances in the TIMIT core test set. On WSJ, a network can be trained with only 64 times of unrolling while sacrificing 4.5% relative word error rate (WER).", "creator": "LaTeX with hyperref package"}}}