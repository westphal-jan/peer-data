{"id": "1511.06855", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "21-Nov-2015", "title": "Unsupervised learning of object semantic parts from internal states of CNNs by population encoding", "abstract": "In this paper, we provide a method for understanding the internal representations of Convolutional Neural Networks (CNNs) trained on objects. We hypothesize that the information is distributed across multiple neuronal responses and propose a simple clustering technique to extract this information, which we call \\emph{population encoding}. The population encoding technique looks into the entrails of an object-CNN at multiple layers of the network and shows the implicit presence of mid-level object part semantics distributed in the neuronal responses. Our qualitative visualizations show that population encoding can extract mid-level image patches that are visually tighter than the patches that produce high single-filter activations. Moreover, our comprehensive quantitative experiments using the object key point annotations from the PASCAL3D+ dataset corroborate the visualizations by demonstrating the superiority of population encoding over single-filter detectors, in the task of object-part detection. We also perform some preliminary experiments where we uncover the compositional relations between the adjacent layers using the parts detected by population encoding clusters. Finally, based on the insights gained from this work, we point to various new directions which will enable us to have a better understanding of the CNN's internal representations.", "histories": [["v1", "Sat, 21 Nov 2015 09:02:21 GMT  (4295kb,D)", "http://arxiv.org/abs/1511.06855v1", "This is the submission for ICLR 2016"], ["v2", "Thu, 7 Jan 2016 22:10:52 GMT  (5168kb,D)", "http://arxiv.org/abs/1511.06855v2", "This is the submission for ICLR 2016"], ["v3", "Sat, 12 Nov 2016 13:37:07 GMT  (8201kb,D)", "http://arxiv.org/abs/1511.06855v3", null]], "COMMENTS": "This is the submission for ICLR 2016", "reviews": [], "SUBJECTS": "cs.LG cs.CV", "authors": ["jianyu wang", "zhishuai zhang", "cihang xie", "vittal premachandran", "alan yuille"], "accepted": false, "id": "1511.06855"}, "pdf": {"name": "1511.06855.pdf", "metadata": {"source": "CRF", "title": "DISCOVERING INTERNAL REPRESENTATIONS FROM OBJECT-CNNS USING POPULATION ENCODING", "authors": ["Jianyu Wang", "Zhishuai Zhang", "Vittal Premachandran", "Alan Yuille"], "emails": [], "sections": [{"heading": "1 INTRODUCTION", "text": "In fact, it is so that most people who stand up for the rights of women and men are able to determine for themselves what they want and what they do not. (...) It is not so that they have the same rights. (...) It is as if they have the same rights. (...) It is not as if they have the same rights. (...) It is as if they have the same rights. (...) It is as if they have the same rights. (...) It is so. (...) It is so. (...). (...). (...). \"(...).\" It. (...). \"(...).\" (...). \"(...).\" (...). \"(...).\" (...). (...). (). \"(...). (). (...). (). (). (.). (.). (.). (.).\" (. \"(.). (.).\" (...). \"(.\" (.). (.). (.). (.). (.). (.). (.). (.). (.). (.). (.). (.). (.). (.).). (.). (.). (.). (.). (.). (.). (.). (.). (.). (.). (.). (.). (. (.).). (. (.). (.). (.). (.). (. (.).). (.). (. (.). (.). (. (.). (.).). (.). (). (). (. (.). (). (). (). (). (). (). (). (). (). (). (). (). (). (). (). (). (). (). (). (). (). (). (). (). (). (). (). (). (). (). (). (). ("}, {"heading": "2 RELATED WORK", "text": "Erhan et al. (2009) visualized the models by finding images that maximized neuronal activity by means of gradient parentage in the image space. Zeiler & Fergus (2014) proposed a DeConvNet-based visualization strategy that brings intermedia activations back into the image space by using the deconvolution networks of Zeiler et al.1 We use the terms neurons and filters that are interactive in this paper. (2011) Similar to Erhan et al. (2009), Simonyan et al. (2014) proposed a gradient-based visualization technique to generate an image that maximizes an object class score; this allowed the notion of a class to be captured. Building on the idea of a gradient-based image technology, Yinosal."}, {"heading": "3 POPULATION ENCODING", "text": "In our experiments, we limit ourselves to object CNNs and show that object parts can be discovered from the bowels of an object-CNN. Our method is generic enough to be used for other scenarios (e.g. scenes). First, we extract characteristics generated by all filters from a specific interlayer of CNN, based on object images of a specific category, and then group these characteristics using K-means + + (Arthur & Vassilvitskii (2007) to obtain a dictionary of middle-level elements. These dictionary elements, when visualized, correspond to the semantic substructures of the objects."}, {"heading": "3.1 DICTIONARY LEARNING BY CLUSTERING", "text": "1x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x"}, {"heading": "4 VISUALIZATION", "text": "In Figure 1, we show a visualization of some middle-level examples that we learned from the Pool3 Pool5 layer for different parent object categories. Each row corresponds to a specific layer, and we show three typical clusters for each layer with four sample fields. Note that population coding is able to extract the representative middle parts of the parent object category. We now offer visual comparisons of the concepts learned through population coding, and compare them to a single grandmother-cell-like representation. Figure 2 provides sample visualizations from Pool3 Pool5. We do not visualize the Pool1 and Pool2 clusters because their receptive fields of coding are too small. The top two rows in each subfigure show patches that are assigned to two clusters from the population coding, and the bottom two rows show the highest activations in two individual filters."}, {"heading": "5 QUANTITATIVE EVALUATION STRATEGY", "text": "The visualizations in the previous sections show that clusters of population responses can capture the semantics of mid-level objects. In this section, we explain the evaluation strategy and present the results in the next section. Since the clusters correspond to representative object parts, we aim to test their ability to act as a detector of object parts. We use the precision / retrieval method and calculate the average precision (AP) of each cluster across multiple detection thresholds. We test each cluster against each key point and assign the best AP as the detector of that key point. We use a simple approach to compress the distance between cluster centers and test patches in the characteristic domain of the corresponding layer. We use this distance as the detection value for the corresponding cluster."}, {"heading": "6 EXPERIMENTAL RESULTS", "text": "In fact, the fact is that most of them are able to move to another world, where they move to another world, where they do not find themselves."}, {"heading": "6.1 DIAGNOSTIC EXPERIMENTS", "text": "In this subsection, we present the results of some diagnostic experiments that we are conducting to allow a better understanding of the clusters that have emerged from population coding. Currently, our diagnostic experiments are only performed on cars. Experiments on other object categories are considered to be future work. Effect of Intermediate Layer: In Table 1, we present the results with the dictionary elements that we have learned from the Pool4 layer. We choose the Pool4 dictionaries because they correspond well to the semantic level of the commented key points in the PASCAL3D + Dataset. In Table 2, we present the results we have obtained with the dictionaries of Pool3 and Pool5 layers. Looking at the numbers of Pool3, it is clear that its dictionaries are not able to capture the semantic level of partial annotations. This is because the receptive field of Pool3 is too small to capture the whole part."}, {"heading": "7 FURTHER STUDIES", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "7.1 TIGHT CLUSTER WITH LOW AVERAGE PRECISION", "text": "When we examine the clusters with a low AP value, we find that many of them are narrow clusters, which means that these clusters capture visual patterns very well, but do not match semantic key points. Figure 4 illustrates this face. On the left is an image of a car. The two Bounding Boxes are detections of two different clusters, which are visualized with six patches on the right. We see that the two clusters capture horizontal structures or textures, but do not match the key point annotations in the dataset, and therefore have low AP values. This is due to the limitation of the current key point annotations. To fully evaluate the clusters of different layers, we should have key point annotations with a high degree of coverage and at different semantic levels."}, {"heading": "7.2 COMPOSITIONAL RELATION BETWEEN ADJACENT LAYERS", "text": "Once we have obtained the middle parts of the population coding method, we can continue to find out the compositional relationships between adjacent layers (e.g. pool 3 and pool 4). If a cluster fires from a higher layer (is detected), we can calculate which clusters in the lower layer also fire within the receiver field of higher layer clusters. In other words, we want to calculate the conditional probability that clusters from the lower layer fire spatially nearby when a higher layer cluster fires. Figure 5 shows two examples of such compositional relationships between pool 4 and pool 3. We can see that the selected top pool 3 clusters capture parts of the patterns that the pool 4 cluster represents. Note that we can apply this compositional analysis to adjacent layer pairs."}, {"heading": "8 CONCLUSION AND FUTURE WORK", "text": "In this paper, we hypothesized that visual information is distributed across multiple CNN filters and proposed a simple clustering technique, namely population coding, to extract it. We demonstrated the superiority of population coding strategy over single-filter coding both qualitatively and quantitatively. Our experiments provide new insights into the internal representations of CNNs and pave the way for many interesting future exploration directions. While this work focused on object CNNs, further studies in understanding the inventory characteristics of deep networks help to identify CNN's strengths and weaknesses. Furthermore, we found that the performance of clusters varies across different layers, suggesting that different layers of CNN capture different levels of semantic parts."}, {"heading": "A. L2-NORMALIZATION", "text": "In this section, we compare the use of L2 normalization as a pre-processing step and without L2 normalization. Table 4 shows that L2 normalization is critical for clusters from the population coding strategy. We then assume that it is the direction of the population response that captures visual information, not the order of magnitude of the population response. In the case of individual neurons, we have also attempted a similar L2 normalization across filters for each spatial position, using the normalized response as a recognizable value."}, {"heading": "B. RESULTS ON PASCAL VOC", "text": "In this section, we provide the evaluation results of both population coding and individual filters for cars and airplanes from the Pascal VOC dataset. Because we only consider non-occluded Bounding Box objects, there is a limited number of non-occlusive objects in Pascal for cars and airplanes. Therefore, we use all Pascal data for testing purposes, while we use the dictionaries trained on Imagenet for the corresponding category. We use keypoint annotations from Bourdev et al. (2010) as they provide much richer key points. AP values are shown in Table 5 and PR curves are shown in Figure 6. We can see that methods that use population coding are better than single-neuron coding, while the strong monitoring method always achieves the best AP value."}], "references": [{"title": "Analyzing the performance of multilayer neural networks for object recognition", "author": ["Agrawal", "Pulkit", "Girshick", "Ross B", "Malik", "Jitendra"], "venue": "In ECCV,", "citeRegEx": "Agrawal et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Agrawal et al\\.", "year": 2014}, {"title": "k-means++: The advantages of careful seeding", "author": ["Arthur", "David", "Vassilvitskii", "Sergei"], "venue": "In Proceedings of the eighteenth annual ACM-SIAM symposium on Discrete algorithms,", "citeRegEx": "Arthur et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Arthur et al\\.", "year": 2007}, {"title": "Single units and sensation: A neuron doctrine for perceptual psychology? Perception, 1:371\u2013394", "author": ["H Barlow"], "venue": null, "citeRegEx": "Barlow,? \\Q1972\\E", "shortCiteRegEx": "Barlow", "year": 1972}, {"title": "Detecting people using mutually consistent poselet activations", "author": ["Bourdev", "Lubomir D", "Maji", "Subhransu", "Brox", "Thomas", "Malik", "Jitendra"], "venue": "In ECCV, pp", "citeRegEx": "Bourdev et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Bourdev et al\\.", "year": 2010}, {"title": "Semantic image segmentation with deep convolutional nets and fully connected crfs", "author": ["Chen", "Liang-Chieh", "Papandreou", "George", "Kokkinos", "Iasonas", "Murphy", "Kevin", "Yuille", "Alan L"], "venue": "In ICLR,", "citeRegEx": "Chen et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Chen et al\\.", "year": 2015}, {"title": "Visualizing higher-layer features of a deep network", "author": ["Erhan", "Dumitru", "Bengio", "Yoshua", "Courville", "Aaron", "Vincent", "Pascal"], "venue": "In Technical Report", "citeRegEx": "Erhan et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Erhan et al\\.", "year": 2009}, {"title": "Neuronal population coding of movement", "author": ["Georgopoulos", "Apostolos P", "Schwartz", "Andrew B", "Kettner", "Ronald E"], "venue": "direction. Science,", "citeRegEx": "Georgopoulos et al\\.,? \\Q1986\\E", "shortCiteRegEx": "Georgopoulos et al\\.", "year": 1986}, {"title": "Rich feature hierarchies for accurate object detection and semantic segmentation", "author": ["Girshick", "Ross B", "Donahue", "Jeff", "Darrell", "Trevor", "Malik", "Jitendra"], "venue": "In CVPR, pp", "citeRegEx": "Girshick et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Girshick et al\\.", "year": 2014}, {"title": "Learning distributed representations of concepts", "author": ["Hinton", "Geoffrey E"], "venue": "In Proceedings of the Eights Annual Conference of the Cognitive Science Society,", "citeRegEx": "Hinton and E.,? \\Q1986\\E", "shortCiteRegEx": "Hinton and E.", "year": 1986}, {"title": "Caffe: Convolutional architecture for fast feature embedding", "author": ["Jia", "Yangqing", "Shelhamer", "Evan", "Donahue", "Jeff", "Karayev", "Sergey", "Long", "Jonathan", "Girshick", "Ross", "Guadarrama", "Sergio", "Darrell", "Trevor"], "venue": "arXiv preprint arXiv:1408.5093,", "citeRegEx": "Jia et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Jia et al\\.", "year": 2014}, {"title": "Blocks that shout: Distinctive parts for scene classification", "author": ["Juneja", "Mamta", "Vedaldi", "Andrea", "CV Jawahar", "Zisserman", "Andrew"], "venue": "In CVPR,", "citeRegEx": "Juneja et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Juneja et al\\.", "year": 2013}, {"title": "Imagenet classification with deep convolutional neural networks", "author": ["Krizhevsky", "Alex", "Sutskever", "Ilya", "Hinton", "Geoffrey E"], "venue": "In NIPS, pp", "citeRegEx": "Krizhevsky et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Krizhevsky et al\\.", "year": 2012}, {"title": "Mid-level deep pattern mining", "author": ["Li", "Yao", "Liu", "Lingqiao", "Shen", "Chunhua", "van den Hengel", "Anton"], "venue": "In CVPR, pp", "citeRegEx": "Li et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Li et al\\.", "year": 2015}, {"title": "Fully convolutional networks for semantic segmentation", "author": ["Long", "Jonathan", "Shelhamer", "Evan", "Darrell", "Trevor"], "venue": "In CVPR,", "citeRegEx": "Long et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Long et al\\.", "year": 2015}, {"title": "Understanding deep image representations by inverting them", "author": ["Mahendran", "Aravindh", "Vedaldi", "Andrea"], "venue": "In CVPR, pp", "citeRegEx": "Mahendran et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Mahendran et al\\.", "year": 2015}, {"title": "Parallel distributed processing: Psychological and biological models, volume 2", "author": ["McClelland", "James L", "Rumelhart", "David E", "Group", "PDP Research"], "venue": null, "citeRegEx": "McClelland et al\\.,? \\Q1987\\E", "shortCiteRegEx": "McClelland et al\\.", "year": 1987}, {"title": "Deep neural networks are easily fooled: High confidence predictions for unrecognizable images", "author": ["Nguyen", "Anh Mai", "Yosinski", "Jason", "Clune", "Jeff"], "venue": "In CVPR, pp", "citeRegEx": "Nguyen et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Nguyen et al\\.", "year": 2015}, {"title": "Parallel distributed processing: Explorations in the microstructure of cognition, volume 1", "author": ["Rumelhart", "David E", "McClelland", "James L", "Group", "PDP Research"], "venue": null, "citeRegEx": "Rumelhart et al\\.,? \\Q1986\\E", "shortCiteRegEx": "Rumelhart et al\\.", "year": 1986}, {"title": "Neural activation constellations: Unsupervised part model discovery with convolutional networks", "author": ["Simon", "Marcel", "Rodner", "Erik"], "venue": "CoRR, abs/1504.08289,", "citeRegEx": "Simon et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Simon et al\\.", "year": 2015}, {"title": "Very deep convolutional networks for large-scale image recognition", "author": ["Simonyan", "Karen", "Zisserman", "Andrew"], "venue": "In ICLR,", "citeRegEx": "Simonyan et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Simonyan et al\\.", "year": 2015}, {"title": "Deep inside convolutional networks: Visualising image classification models and saliency maps", "author": ["Simonyan", "Karen", "Vedaldi", "Andrea", "Zisserman", "Andrew"], "venue": "In ICLR Workshop,", "citeRegEx": "Simonyan et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Simonyan et al\\.", "year": 2014}, {"title": "Unsupervised discovery of mid-level discriminative patches", "author": ["Singh", "Saurabh", "Gupta", "Abhinav", "Efros", "Alexei"], "venue": "In ECCV,", "citeRegEx": "Singh et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Singh et al\\.", "year": 2012}, {"title": "Intriguing properties of neural networks", "author": ["Szegedy", "Christian", "Zaremba", "Wojciech", "Sutskever", "Ilya", "Bruna", "Joan", "Erhan", "Dumitru", "Goodfellow", "Ian J", "Fergus", "Rob"], "venue": "CoRR, abs/1312.6199,", "citeRegEx": "Szegedy et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Szegedy et al\\.", "year": 2013}, {"title": "Beyond pascal: A benchmark for 3d object detection in the wild", "author": ["Xiang", "Yu", "Mottaghi", "Roozbeh", "Savarese", "Silvio"], "venue": "In WACV, pp", "citeRegEx": "Xiang et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Xiang et al\\.", "year": 2014}, {"title": "The application of two-level attention models in deep convolutional neural network for fine-grained image classification", "author": ["Xiao", "Tianjun", "Xu", "Yichong", "Yang", "Kuiyuan", "Zhang", "Jiaxing", "Peng", "Yuxin", "Zheng"], "venue": "In CVPR, pp", "citeRegEx": "Xiao et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Xiao et al\\.", "year": 2015}, {"title": "Understanding neural networks through deep visualization", "author": ["Yosinski", "Jason", "Clune", "Jeff", "Nguyen", "Anh Mai", "Fuchs", "Thomas", "Lipson", "Hod"], "venue": "In ICML Deep Learning Workshop,", "citeRegEx": "Yosinski et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Yosinski et al\\.", "year": 2015}, {"title": "Visualizing and understanding convolutional networks", "author": ["Zeiler", "Matthew D", "Fergus", "Rob"], "venue": "In ECCV, pp", "citeRegEx": "Zeiler et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Zeiler et al\\.", "year": 2014}, {"title": "Adaptive deconvolutional networks for mid and high level feature learning", "author": ["Zeiler", "Matthew D", "Taylor", "Graham W", "Fergus", "Rob"], "venue": "In ICCV,", "citeRegEx": "Zeiler et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Zeiler et al\\.", "year": 2011}, {"title": "Object detectors emerge in deep scene cnns", "author": ["Zhou", "Bolei", "Khosla", "Aditya", "Lapedriza", "\u00c0gata", "Oliva", "Aude", "Torralba", "Antonio"], "venue": "In ICLR,", "citeRegEx": "Zhou et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Zhou et al\\.", "year": 2015}], "referenceMentions": [{"referenceID": 8, "context": "CNNs have been especially successful at many difficult computer vision tasks such as image classification, as showcased by Krizhevsky et al. (2012) and Simonyan & Zisserman (2015), at object detection as shown by Girshick et al.", "startOffset": 123, "endOffset": 148}, {"referenceID": 8, "context": "CNNs have been especially successful at many difficult computer vision tasks such as image classification, as showcased by Krizhevsky et al. (2012) and Simonyan & Zisserman (2015), at object detection as shown by Girshick et al.", "startOffset": 123, "endOffset": 180}, {"referenceID": 5, "context": "(2012) and Simonyan & Zisserman (2015), at object detection as shown by Girshick et al. (2014), and, at semantic segmentation as demonstrated by Long et al.", "startOffset": 72, "endOffset": 95}, {"referenceID": 5, "context": "(2012) and Simonyan & Zisserman (2015), at object detection as shown by Girshick et al. (2014), and, at semantic segmentation as demonstrated by Long et al. (2015) and Chen et al.", "startOffset": 72, "endOffset": 164}, {"referenceID": 3, "context": "(2015) and Chen et al. (2015), among other tasks.", "startOffset": 11, "endOffset": 30}, {"referenceID": 3, "context": "(2015) and Chen et al. (2015), among other tasks. Deep Networks have enabled end-to-end learning from raw data due to a handful of well-performing recipes such as convolutions, Dropout, and Rectified Linear Units (ReLUs). In spite of the fact that we have developed these robust engineering techniques to facilitate end-to-end learning of deep networks, leading to better performance on various applications, our understanding of why CNNs work so well and how CNNs represent visual patterns is still at an early stage. One of the important questions that remains unanswered is how does a CNN represent data internally? When it sees an object, does it understand the concept of parts of an object? If so, does it understand the rules of compositionality of these parts? Such questions are extremely important to ask since the answers to these questions will allow us to build more powerful AI systems that can reason about their tasks. There have been several works on trying to understand how CNNs work. The thrust of these works have been in visualizing the internal neuronal activations independently as done by Zeiler & Fergus (2014), Yosinski et al.", "startOffset": 11, "endOffset": 1137}, {"referenceID": 3, "context": "(2015) and Chen et al. (2015), among other tasks. Deep Networks have enabled end-to-end learning from raw data due to a handful of well-performing recipes such as convolutions, Dropout, and Rectified Linear Units (ReLUs). In spite of the fact that we have developed these robust engineering techniques to facilitate end-to-end learning of deep networks, leading to better performance on various applications, our understanding of why CNNs work so well and how CNNs represent visual patterns is still at an early stage. One of the important questions that remains unanswered is how does a CNN represent data internally? When it sees an object, does it understand the concept of parts of an object? If so, does it understand the rules of compositionality of these parts? Such questions are extremely important to ask since the answers to these questions will allow us to build more powerful AI systems that can reason about their tasks. There have been several works on trying to understand how CNNs work. The thrust of these works have been in visualizing the internal neuronal activations independently as done by Zeiler & Fergus (2014), Yosinski et al. (2015) and Zhou et al.", "startOffset": 11, "endOffset": 1161}, {"referenceID": 3, "context": "(2015) and Chen et al. (2015), among other tasks. Deep Networks have enabled end-to-end learning from raw data due to a handful of well-performing recipes such as convolutions, Dropout, and Rectified Linear Units (ReLUs). In spite of the fact that we have developed these robust engineering techniques to facilitate end-to-end learning of deep networks, leading to better performance on various applications, our understanding of why CNNs work so well and how CNNs represent visual patterns is still at an early stage. One of the important questions that remains unanswered is how does a CNN represent data internally? When it sees an object, does it understand the concept of parts of an object? If so, does it understand the rules of compositionality of these parts? Such questions are extremely important to ask since the answers to these questions will allow us to build more powerful AI systems that can reason about their tasks. There have been several works on trying to understand how CNNs work. The thrust of these works have been in visualizing the internal neuronal activations independently as done by Zeiler & Fergus (2014), Yosinski et al. (2015) and Zhou et al. (2015). The idea that neurons can independently represent concepts is an old idea from Neuroscience.", "startOffset": 11, "endOffset": 1184}, {"referenceID": 2, "context": "Barlow (1972) advocates extreme localist theories in which each concept is represented by a single neuronal unit.", "startOffset": 0, "endOffset": 14}, {"referenceID": 6, "context": "The distributed representation argument claims that concepts are encoded by a distributed pattern of activities spread across multiple neurons (Georgopoulos et al. (1986)).", "startOffset": 144, "endOffset": 171}, {"referenceID": 6, "context": "The distributed representation argument claims that concepts are encoded by a distributed pattern of activities spread across multiple neurons (Georgopoulos et al. (1986)). One advantage of representing concepts by a distributed representation is that it leads to automatic generalization. As mentioned by Hinton (1986), when the weights in the network are updated to incorporate new concepts, the changes affect the knowledge associated with other concepts that are represented by similar neuronal activity patterns.", "startOffset": 144, "endOffset": 320}, {"referenceID": 6, "context": "The distributed representation argument claims that concepts are encoded by a distributed pattern of activities spread across multiple neurons (Georgopoulos et al. (1986)). One advantage of representing concepts by a distributed representation is that it leads to automatic generalization. As mentioned by Hinton (1986), when the weights in the network are updated to incorporate new concepts, the changes affect the knowledge associated with other concepts that are represented by similar neuronal activity patterns. Other advantages of the distributed representation lies in the fact that it is more robust to rely on the activity of a population of neurons to encode information than relying on a single perishable neuronal cell. A more detailed debate between these two types of representations can be found in Rumelhart et al. (1986) and McClelland et al.", "startOffset": 144, "endOffset": 839}, {"referenceID": 6, "context": "The distributed representation argument claims that concepts are encoded by a distributed pattern of activities spread across multiple neurons (Georgopoulos et al. (1986)). One advantage of representing concepts by a distributed representation is that it leads to automatic generalization. As mentioned by Hinton (1986), when the weights in the network are updated to incorporate new concepts, the changes affect the knowledge associated with other concepts that are represented by similar neuronal activity patterns. Other advantages of the distributed representation lies in the fact that it is more robust to rely on the activity of a population of neurons to encode information than relying on a single perishable neuronal cell. A more detailed debate between these two types of representations can be found in Rumelhart et al. (1986) and McClelland et al. (1987). In this work, we lean towards the distributed representation argument and propose a simple clustering method to extract the information encoded by a population of neurons.", "startOffset": 144, "endOffset": 868}, {"referenceID": 5, "context": "Erhan et al. (2009) visualized the models by finding images that maximized the neuron activities using gradient descent in the image space.", "startOffset": 0, "endOffset": 20}, {"referenceID": 5, "context": "Erhan et al. (2009) visualized the models by finding images that maximized the neuron activities using gradient descent in the image space. Zeiler & Fergus (2014) proposed DeConvNet-based visualization strategy that mapped intermediate features activations back to the image space using the Deconvolution Networks of Zeiler et al.", "startOffset": 0, "endOffset": 163}, {"referenceID": 4, "context": "Similar to Erhan et al. (2009), Simonyan et al.", "startOffset": 11, "endOffset": 31}, {"referenceID": 4, "context": "Similar to Erhan et al. (2009), Simonyan et al. (2014) proposed a gradient-based visualization technique to generate an image, which maximized an object class score; this enabled them to visualize the notion of a class as captured by a CNN.", "startOffset": 11, "endOffset": 55}, {"referenceID": 4, "context": "Similar to Erhan et al. (2009), Simonyan et al. (2014) proposed a gradient-based visualization technique to generate an image, which maximized an object class score; this enabled them to visualize the notion of a class as captured by a CNN. Building upon the idea of gradient-based image generation, Yosinski et al. (2015) incorporated stronger priors on the optimization procedure to generate \u201cbetter-looking\u201d images.", "startOffset": 11, "endOffset": 323}, {"referenceID": 4, "context": "Similar to Erhan et al. (2009), Simonyan et al. (2014) proposed a gradient-based visualization technique to generate an image, which maximized an object class score; this enabled them to visualize the notion of a class as captured by a CNN. Building upon the idea of gradient-based image generation, Yosinski et al. (2015) incorporated stronger priors on the optimization procedure to generate \u201cbetter-looking\u201d images. They also release a toolbox that helps visualize what any individual neuron at any intermediate layer likes to see. Mahendran & Vedaldi (2015) showed that incorporating strong natural image priors to the optimization procedure produces images that look more like real images.", "startOffset": 11, "endOffset": 562}, {"referenceID": 4, "context": "Similar to Erhan et al. (2009), Simonyan et al. (2014) proposed a gradient-based visualization technique to generate an image, which maximized an object class score; this enabled them to visualize the notion of a class as captured by a CNN. Building upon the idea of gradient-based image generation, Yosinski et al. (2015) incorporated stronger priors on the optimization procedure to generate \u201cbetter-looking\u201d images. They also release a toolbox that helps visualize what any individual neuron at any intermediate layer likes to see. Mahendran & Vedaldi (2015) showed that incorporating strong natural image priors to the optimization procedure produces images that look more like real images. Moreover, Mahendran & Vedaldi (2015)\u2019s optimization procedure differs from others in that they do not aim to produce an image that maximizes the activation of an individual neuron, but aim to produce natural-looking images whose bank of filter responses in a particular layer are similar to the bank of filter responses from a target image.", "startOffset": 11, "endOffset": 732}, {"referenceID": 4, "context": "Similar to Erhan et al. (2009), Simonyan et al. (2014) proposed a gradient-based visualization technique to generate an image, which maximized an object class score; this enabled them to visualize the notion of a class as captured by a CNN. Building upon the idea of gradient-based image generation, Yosinski et al. (2015) incorporated stronger priors on the optimization procedure to generate \u201cbetter-looking\u201d images. They also release a toolbox that helps visualize what any individual neuron at any intermediate layer likes to see. Mahendran & Vedaldi (2015) showed that incorporating strong natural image priors to the optimization procedure produces images that look more like real images. Moreover, Mahendran & Vedaldi (2015)\u2019s optimization procedure differs from others in that they do not aim to produce an image that maximizes the activation of an individual neuron, but aim to produce natural-looking images whose bank of filter responses in a particular layer are similar to the bank of filter responses from a target image. Although they use a population of filter responses, their only aim is in producing better visualizations. We propose a method to extract the knowledge that is distributed over a population of neurons. The following works probe the CNN using other interesting approaches. Szegedy et al. (2013) brought out the brittleness in CNNs by showing that one can change the CNN\u2019s prediction by adding an imperceptible amount of adversarial noise to the input image.", "startOffset": 11, "endOffset": 1329}, {"referenceID": 4, "context": "Similar to Erhan et al. (2009), Simonyan et al. (2014) proposed a gradient-based visualization technique to generate an image, which maximized an object class score; this enabled them to visualize the notion of a class as captured by a CNN. Building upon the idea of gradient-based image generation, Yosinski et al. (2015) incorporated stronger priors on the optimization procedure to generate \u201cbetter-looking\u201d images. They also release a toolbox that helps visualize what any individual neuron at any intermediate layer likes to see. Mahendran & Vedaldi (2015) showed that incorporating strong natural image priors to the optimization procedure produces images that look more like real images. Moreover, Mahendran & Vedaldi (2015)\u2019s optimization procedure differs from others in that they do not aim to produce an image that maximizes the activation of an individual neuron, but aim to produce natural-looking images whose bank of filter responses in a particular layer are similar to the bank of filter responses from a target image. Although they use a population of filter responses, their only aim is in producing better visualizations. We propose a method to extract the knowledge that is distributed over a population of neurons. The following works probe the CNN using other interesting approaches. Szegedy et al. (2013) brought out the brittleness in CNNs by showing that one can change the CNN\u2019s prediction by adding an imperceptible amount of adversarial noise to the input image. Moreover, they suggested that it might be the space of the features, and not the individual units, that contains the semantic information in the higher layers of the network. Similarly, Nguyen et al. (2015) generated images using evolutionary algorithms that are unrecognizable to humans but can easily fool CNNs.", "startOffset": 11, "endOffset": 1699}, {"referenceID": 0, "context": "One work that makes a preliminary attempt at quantifying the distributedness of the information in CNNs is that of Agrawal et al. (2014). They question the presence of grandmother cells by computing the average precision when using individual filters as object detectors.", "startOffset": 115, "endOffset": 137}, {"referenceID": 0, "context": "One work that makes a preliminary attempt at quantifying the distributedness of the information in CNNs is that of Agrawal et al. (2014). They question the presence of grandmother cells by computing the average precision when using individual filters as object detectors. They claim that grandmother cell-like filters might exist only for a few object classes. And, to test whether the information is distributed, they train linear SVMs on features extracted from a bank of filters after spatial pooling. Their results show that substantially more number of filters are needed for achieving good performance on a number of classes. While we also lean towards the presence of a distributed code, our work differs from their work in many ways. i) Agrawal et al. (2014) employ a supervised technique and train SVMs on banks of filter responses after spatial pooling, while we exploit the spatial information and extract the distributed representation using population encoding in a completely unsupervised fashion.", "startOffset": 115, "endOffset": 767}, {"referenceID": 0, "context": "One work that makes a preliminary attempt at quantifying the distributedness of the information in CNNs is that of Agrawal et al. (2014). They question the presence of grandmother cells by computing the average precision when using individual filters as object detectors. They claim that grandmother cell-like filters might exist only for a few object classes. And, to test whether the information is distributed, they train linear SVMs on features extracted from a bank of filters after spatial pooling. Their results show that substantially more number of filters are needed for achieving good performance on a number of classes. While we also lean towards the presence of a distributed code, our work differs from their work in many ways. i) Agrawal et al. (2014) employ a supervised technique and train SVMs on banks of filter responses after spatial pooling, while we exploit the spatial information and extract the distributed representation using population encoding in a completely unsupervised fashion. ii) Agrawal et al. (2014) restrict themselves to extracting object-level representations, while we go over multiple layers of the CNN and identify the mid-level object parts/sub-parts, which is clearly more challenging.", "startOffset": 115, "endOffset": 1038}, {"referenceID": 0, "context": "One work that makes a preliminary attempt at quantifying the distributedness of the information in CNNs is that of Agrawal et al. (2014). They question the presence of grandmother cells by computing the average precision when using individual filters as object detectors. They claim that grandmother cell-like filters might exist only for a few object classes. And, to test whether the information is distributed, they train linear SVMs on features extracted from a bank of filters after spatial pooling. Their results show that substantially more number of filters are needed for achieving good performance on a number of classes. While we also lean towards the presence of a distributed code, our work differs from their work in many ways. i) Agrawal et al. (2014) employ a supervised technique and train SVMs on banks of filter responses after spatial pooling, while we exploit the spatial information and extract the distributed representation using population encoding in a completely unsupervised fashion. ii) Agrawal et al. (2014) restrict themselves to extracting object-level representations, while we go over multiple layers of the CNN and identify the mid-level object parts/sub-parts, which is clearly more challenging. Discovering mid-level patches has been studied even before the power of CNNs was realized (e.g. Singh et al. (2012) and Juneja et al.", "startOffset": 115, "endOffset": 1348}, {"referenceID": 0, "context": "One work that makes a preliminary attempt at quantifying the distributedness of the information in CNNs is that of Agrawal et al. (2014). They question the presence of grandmother cells by computing the average precision when using individual filters as object detectors. They claim that grandmother cell-like filters might exist only for a few object classes. And, to test whether the information is distributed, they train linear SVMs on features extracted from a bank of filters after spatial pooling. Their results show that substantially more number of filters are needed for achieving good performance on a number of classes. While we also lean towards the presence of a distributed code, our work differs from their work in many ways. i) Agrawal et al. (2014) employ a supervised technique and train SVMs on banks of filter responses after spatial pooling, while we exploit the spatial information and extract the distributed representation using population encoding in a completely unsupervised fashion. ii) Agrawal et al. (2014) restrict themselves to extracting object-level representations, while we go over multiple layers of the CNN and identify the mid-level object parts/sub-parts, which is clearly more challenging. Discovering mid-level patches has been studied even before the power of CNNs was realized (e.g. Singh et al. (2012) and Juneja et al. (2013)).", "startOffset": 115, "endOffset": 1373}, {"referenceID": 0, "context": "One work that makes a preliminary attempt at quantifying the distributedness of the information in CNNs is that of Agrawal et al. (2014). They question the presence of grandmother cells by computing the average precision when using individual filters as object detectors. They claim that grandmother cell-like filters might exist only for a few object classes. And, to test whether the information is distributed, they train linear SVMs on features extracted from a bank of filters after spatial pooling. Their results show that substantially more number of filters are needed for achieving good performance on a number of classes. While we also lean towards the presence of a distributed code, our work differs from their work in many ways. i) Agrawal et al. (2014) employ a supervised technique and train SVMs on banks of filter responses after spatial pooling, while we exploit the spatial information and extract the distributed representation using population encoding in a completely unsupervised fashion. ii) Agrawal et al. (2014) restrict themselves to extracting object-level representations, while we go over multiple layers of the CNN and identify the mid-level object parts/sub-parts, which is clearly more challenging. Discovering mid-level patches has been studied even before the power of CNNs was realized (e.g. Singh et al. (2012) and Juneja et al. (2013)). More recently, Li et al. (2015) used CNN features with association rule algorithm to discover mid-level visual elements, and evaluated them on object/scene classification task.", "startOffset": 115, "endOffset": 1407}, {"referenceID": 0, "context": "One work that makes a preliminary attempt at quantifying the distributedness of the information in CNNs is that of Agrawal et al. (2014). They question the presence of grandmother cells by computing the average precision when using individual filters as object detectors. They claim that grandmother cell-like filters might exist only for a few object classes. And, to test whether the information is distributed, they train linear SVMs on features extracted from a bank of filters after spatial pooling. Their results show that substantially more number of filters are needed for achieving good performance on a number of classes. While we also lean towards the presence of a distributed code, our work differs from their work in many ways. i) Agrawal et al. (2014) employ a supervised technique and train SVMs on banks of filter responses after spatial pooling, while we exploit the spatial information and extract the distributed representation using population encoding in a completely unsupervised fashion. ii) Agrawal et al. (2014) restrict themselves to extracting object-level representations, while we go over multiple layers of the CNN and identify the mid-level object parts/sub-parts, which is clearly more challenging. Discovering mid-level patches has been studied even before the power of CNNs was realized (e.g. Singh et al. (2012) and Juneja et al. (2013)). More recently, Li et al. (2015) used CNN features with association rule algorithm to discover mid-level visual elements, and evaluated them on object/scene classification task. Their method uses a CNN only as a feature extractor without exploring how the intermediate layer CNN filters capture the mid-level visual elements. Xiao et al. (2015) and Simon & Rodner (2015) used the activations from the intermediate layers of the CNN to find object parts, which they then use in the task of fine-grained object classification.", "startOffset": 115, "endOffset": 1719}, {"referenceID": 0, "context": "One work that makes a preliminary attempt at quantifying the distributedness of the information in CNNs is that of Agrawal et al. (2014). They question the presence of grandmother cells by computing the average precision when using individual filters as object detectors. They claim that grandmother cell-like filters might exist only for a few object classes. And, to test whether the information is distributed, they train linear SVMs on features extracted from a bank of filters after spatial pooling. Their results show that substantially more number of filters are needed for achieving good performance on a number of classes. While we also lean towards the presence of a distributed code, our work differs from their work in many ways. i) Agrawal et al. (2014) employ a supervised technique and train SVMs on banks of filter responses after spatial pooling, while we exploit the spatial information and extract the distributed representation using population encoding in a completely unsupervised fashion. ii) Agrawal et al. (2014) restrict themselves to extracting object-level representations, while we go over multiple layers of the CNN and identify the mid-level object parts/sub-parts, which is clearly more challenging. Discovering mid-level patches has been studied even before the power of CNNs was realized (e.g. Singh et al. (2012) and Juneja et al. (2013)). More recently, Li et al. (2015) used CNN features with association rule algorithm to discover mid-level visual elements, and evaluated them on object/scene classification task. Their method uses a CNN only as a feature extractor without exploring how the intermediate layer CNN filters capture the mid-level visual elements. Xiao et al. (2015) and Simon & Rodner (2015) used the activations from the intermediate layers of the CNN to find object parts, which they then use in the task of fine-grained object classification.", "startOffset": 115, "endOffset": 1745}, {"referenceID": 0, "context": "One work that makes a preliminary attempt at quantifying the distributedness of the information in CNNs is that of Agrawal et al. (2014). They question the presence of grandmother cells by computing the average precision when using individual filters as object detectors. They claim that grandmother cell-like filters might exist only for a few object classes. And, to test whether the information is distributed, they train linear SVMs on features extracted from a bank of filters after spatial pooling. Their results show that substantially more number of filters are needed for achieving good performance on a number of classes. While we also lean towards the presence of a distributed code, our work differs from their work in many ways. i) Agrawal et al. (2014) employ a supervised technique and train SVMs on banks of filter responses after spatial pooling, while we exploit the spatial information and extract the distributed representation using population encoding in a completely unsupervised fashion. ii) Agrawal et al. (2014) restrict themselves to extracting object-level representations, while we go over multiple layers of the CNN and identify the mid-level object parts/sub-parts, which is clearly more challenging. Discovering mid-level patches has been studied even before the power of CNNs was realized (e.g. Singh et al. (2012) and Juneja et al. (2013)). More recently, Li et al. (2015) used CNN features with association rule algorithm to discover mid-level visual elements, and evaluated them on object/scene classification task. Their method uses a CNN only as a feature extractor without exploring how the intermediate layer CNN filters capture the mid-level visual elements. Xiao et al. (2015) and Simon & Rodner (2015) used the activations from the intermediate layers of the CNN to find object parts, which they then use in the task of fine-grained object classification. This method was based on the assumption that a single filter from an intermediate layer can detect parts. Zhou et al. (2015), looked at the hierarchy between object and scenes and claimed that object detectors emerge from training CNNs on scenes.", "startOffset": 115, "endOffset": 2024}, {"referenceID": 28, "context": "Similar idea is in Zhou et al. (2015) using a different method.", "startOffset": 19, "endOffset": 38}, {"referenceID": 23, "context": "Therefore, we use annotations from PASCAL3D+ dataset (Xiang et al. (2014)) for our evaluation.", "startOffset": 54, "endOffset": 74}, {"referenceID": 9, "context": "Implementation Details: In the experiments, we use the Caffe toolbox (Jia et al. (2014)) and VGG16 network Simonyan & Zisserman (2015) pre-trained on ImageNet objects as our object-CNN.", "startOffset": 70, "endOffset": 88}, {"referenceID": 9, "context": "Implementation Details: In the experiments, we use the Caffe toolbox (Jia et al. (2014)) and VGG16 network Simonyan & Zisserman (2015) pre-trained on ImageNet objects as our object-CNN.", "startOffset": 70, "endOffset": 135}], "year": 2016, "abstractText": "In this paper, we provide a method for understanding the internal representations of Convolutional Neural Networks (CNNs) trained on objects. We hypothesize that the information is distributed across multiple neuronal responses and propose a simple clustering technique to extract this information, which we call population encoding. The population encoding technique looks into the entrails of an object-CNN at multiple layers of the network and shows the implicit presence of mid-level object part semantics distributed in the neuronal responses. Our qualitative visualizations show that population encoding can extract mid-level image patches that are visually tighter than the patches that produce high single-filter activations. Moreover, our comprehensive quantitative experiments using the object key point annotations from the PASCAL3D+ dataset corroborate the visualizations by demonstrating the superiority of population encoding over single-filter detectors, in the task of object-part detection. We also perform some preliminary experiments where we uncover the compositional relations between the adjacent layers using the parts detected by population encoding clusters. Finally, based on the insights gained from this work, we point to various new directions which will enable us to have a better understanding of the CNN\u2019s internal representations.", "creator": "LaTeX with hyperref package"}}}