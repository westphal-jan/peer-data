{"id": "1511.06910", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "21-Nov-2015", "title": "ICU Patient Deterioration prediction: a Data-Mining Approach", "abstract": "A huge amount of medical data is generated every day, which presents a challenge in analysing these data. The obvious solution to this challenge is to reduce the amount of data without information loss. Dimension reduction is considered the most popular approach for reducing data size and also to reduce noise and redundancies in data. In this paper, we investigate the effect of feature selection in improving the prediction of patient deterioration in ICUs. We consider lab tests as features. Thus, choosing a subset of features would mean choosing the most important lab tests to perform. If the number of tests can be reduced by identifying the most important tests, then we could also identify the redundant tests. By omitting the redundant tests, observation time could be reduced and early treatment could be provided to avoid the risk. Additionally, unnecessary monetary cost would be avoided. Our approach uses state-ofthe- art feature selection for predicting ICU patient deterioration using the medical lab results. We apply our technique on the publicly available MIMIC-II database and show the effectiveness of the feature selection. We also provide a detailed analysis of the best features identified by our approach.", "histories": [["v1", "Sat, 21 Nov 2015 17:50:20 GMT  (498kb)", "http://arxiv.org/abs/1511.06910v1", "16 pages, 3 figures, 10 tables, confeence"]], "COMMENTS": "16 pages, 3 figures, 10 tables, confeence", "reviews": [], "SUBJECTS": "cs.CY cs.LG", "authors": ["noura alnuaimi", "mohammad m masud", "farhan mohammed"], "accepted": false, "id": "1511.06910"}, "pdf": {"name": "1511.06910.pdf", "metadata": {"source": "CRF", "title": "ICU PATIENT DETERIORATION PREDICTION: A DATA-MINING APPROACH", "authors": ["Noura AlNuaimi", "Mohammad M Masud"], "emails": ["200835338}@uaeu.ac.ae"], "sections": [{"heading": null, "text": "The obvious solution to this challenge is to reduce the amount of data without losing information. Reducing dimensions is considered the most popular approach to reducing the size of the data and also to reducing noise and redundancy in the data. In this paper, we examine the impact of feature selection in improving the prediction of patient deterioration in ICU. We consider laboratory tests to be features. Therefore, selecting a subset of features would mean selecting the most important laboratory tests to perform. If the number of tests can be reduced by identifying the most important tests, we could also identify the redundant tests. By eliminating redundant tests, observation time could be shortened and early treatment provided to avoid risk. In addition, unnecessary monetary costs would be avoided. Our approach uses state-of-the-art feature selection to predict the deterioration of intensive patients using the results of the medical laboratory. We apply our technology to the publicly available MIMIC data base and provide the most detailed selection of characteristics; We also provide the YMIC data base for selecting the most effective features;"}, {"heading": "1. INTRODUCTION", "text": "Healthcare is transforming from traditional medical practice to modern evidence-based healthcare, based primarily on patient data collected from various sources such as electronic medical records, monitoring devices and sensors. [1] A concrete example of these technological advances is the observation and monitoring technologies for ICU patients. Currently, the data generated in the ICU process is huge, complex and unstructured. Such data can be called big data because of its complexity, size and difficulty to process in real time [2]. However, this data could be used with intelligent systems such as Big Data Analytics and Decision Support Systems to determine which patients are at increased risk of death, which could support the right decision to improve the efficiency, accuracy and timeliness of clinical decision-making in ICU."}, {"heading": "2. LITERATURE REVIEW", "text": "This section examines work on predicting death in intensive care units or the deterioration of intensive care units. We highlight some similarities and differences between some of the related work and the proposed work. In [3], the authors developed an integrated data mining approach to provide early deterioration warnings for patients under real-time ICU monitoring and real-time data acquisition (RDS). They synthesized a wide range of functionality that included first- and second-tier time series characteristics, detrenched fluctuation analysis (DFA), spectral analysis, approximate entropy and cross-signal characteristics. They then systematically applied a number of established data mining methods, including prediction of characteristics, linear and nonlinear classification algorithms, and exploratory sampling of class imbalances. In our work, we use the same data sets. However, we only use the laboratory tests to examine the medical results."}, {"heading": "3. BACKGROUND ON DATA MINING AND BIG DATA ANALYTICS", "text": "Healthcare, like other sectors, faces the need to analyze large amounts of information known as big data, which has become a major driver of innovation and success. Big data has the potential to support a wide range of medical and health functions, including supporting clinical decision-making [2]. Data mining is the analytical step of knowledge discovery. It involves \"extracting interesting (non-trivial, implicit, previously unknown, and potentially useful) patterns or knowledge from vast amounts of data [10].\" When mining huge data sets, two of the most common, important, and immediate problems are sampling and feature selection. Appropriate sampling and feature selection help reduce the size of the data set while achieving satisfactory results in modeling [4]."}, {"heading": "3.1. Feature Selection", "text": "In machine learning, the selection of characteristics or attributes is the process of selecting a subset of relevant characteristics (variables, predictors) for use in model building. Feature selection techniques are used (a) to avoid overfitting and improving model performance, i.e. to predict performance in the case of supervised classification and better cluster detection in the case of clustering, (b) to provide faster and more cost-effective models, and (c) to gain deeper insight into the underlying processes that generated the data. In the context of classification, feature selection techniques can be organized into three categories, depending on how they perform the feature selection search to create the classification model: filter methods, wrapper methods, and embedded methods, which are sorted in Table 1 [6] [7]: 1) Filter methods based on the application of a statistical measure to assign a score to each characteristic, and then removed from data sets or sorted by characteristics and scores."}, {"heading": "3.2. Data Classification Techniques", "text": "Classification is a pattern recognition task that has applications in a wide range of fields. It requires the construction of a model that approximates the relationship between input characteristics and output categories. [8] Some of the most popular techniques are briefly discussed here, all of which are used in our thesis. [1] The Na\u00efve Bayes classifier is based on the application of Bayes' theorem with strong assumptions of independence between characteristics. As one of its main features, the Na\u00efve Bayes classifier is easy to implement because it requires a small amount of training data to estimate the parameters, and good results can be found in most cases. However, it has class-related independence, which means that it causes loss of accuracy and dependency [9]. [2] Sequential Minimal Optimization (SMO) is an algorithm for efficiently solving the optimization problem that arises during the formation of support vector machines. [10] The amount of memory required for SMO is very large, which SMO can be converted into erosion size."}, {"heading": "4. PROPOSED APPROACH", "text": "In this section, we present our approach to Big Data Mining to predict ICU deterioration. Figure 1 shows the architecture of the proposed technology; the data is collected from the ICU patient database (Step 1); the data is then integrated, purified and relevant features extracted (Step 2); then the patient's data is collected incrementally and the data dimension reduced (Step 3); then the prediction model is learned using a machine learning approach (Step 4); when a new patient is admitted to the CPU, the data is collected incrementally (Step 5); patient data is evaluated using the prediction model (Step 6) to predict the possibility of patient deterioration and warnings are generated accordingly. Each of these steps is summarized here, and further details of the data sets are recorded in the database."}, {"heading": "5. MIMIC II DATABASE", "text": "The MIMIC-II database is part of the Multi-Parameter Intelligent Monitoring in Intensive Care project funded by the National Institute of Biomedical Imaging and Bioengineering at the Laboratory of Computational Physiology at MIT and collected from 2001 to 2008, representing 26,870 adult hospital admissions. In our work, we use MIMIC-II version 2.6 because it is more stable than the newer version 3, which is still in beta and requires further purification, optimization and testing. MIMIC-II consists of two main components: clinical data and physiological waveforms. The MIMIC dataset has three main features: (1) it is public; (2) it has a diverse and very large population of intensive care patients; and (3) it contains high-resolution data, including laboratory results, electronic documentation and physiological waveforms."}, {"heading": "6. EXPERIMENTS", "text": "We carried out four experiments to fulfil the various approaches in order to achieve our goal of predicting the deterioration of the situation of intensive care patients through the results of laboratory tests. In each experiment, a different dataset was obtained from the pre-processing of the MIMIC II v2.6 database."}, {"heading": "6.1. Experiment 1: Building a Baseline of the Medical Lab Tests Average", "text": "1) Experimental Objective: The aim of this experiment was to examine the effects of laboratory tests on predicting the deterioration of the patient's situation. Normally, health professionals compare the result of the laboratory test with a reference range [17]. If the value is not within this range, the patient may be faced with fatal consequences. As we deal with real cases and the test is repeated for a certain period of time, we investigated the average value of the same repeated test and, more specifically, how the average value of laboratory results could assist health professionals in assessing patient status. Since the only way to evaluate the quality and characteristics of a data mineral model was the final status of the patient, i.e. whether the patient survived or not. Therefore, our evaluation criterion was to accurately predict whether the patient died or not. 2) The construction of the data set was based on the use of Avataset: The data set was constructed by taking the average test result of each patient."}, {"heading": "6.2. Experiment 2: Average Medical Lab Tests Feature Selection", "text": "This year it has come to the point that it has never come as far as this year."}, {"heading": "6.4. Experiment 4: Feature Selection for Total Number of Medical Lab Tests", "text": "1) Experimental Objective: The aim of this experiment was to study the relationship between feature selection and classification accuracy. Feature selection is one of the dimensionality reduction techniques used to reduce the feature space of a feature set. Specifically, it is measured how many features should be sufficient to give moderate accuracy. 2) Structure of the dataset: In this experiment we used a count set. 3) Pre-processing: In pre-processing, we built ten datasets depending on the number of features selected. The first dataset contained only 10% of the total attributes. Then we increased the total feature selections by 10% with each new dataset. For example, dataset 1 contained 10% of the total attributes, dataset 2 contained 20% of the total attributes, dataset 3 contained 30% of the total attributes, and so on until dataset 10 contained all the 100% of the total attributes we monitored."}, {"heading": "7. DISCUSSION", "text": "Unless it is an attempt that is able to trump itself, in the way that it has taken place in recent years. (...) It is not as if it is able to trump itself. (...) It is not as if it is able to trump itself. (...) It is as if it is able to trump itself. (...) It is as if it feels it is trumping itself. (...) It is as if it is trumping itself. (...) It is as if it is trumping itself. (...) It is as if it is trumping itself. (...) It is as if it is trumping itself. (...) It is as if it is trumping itself. (...) It is as if it is trumping itself. (...) It is as if it is trumping itself. (...) It is as if it is trumping itself. (...)"}, {"heading": "8. CONCLUSION AND FUTURE WORK", "text": "In this paper, we presented our proposed approach to reducing observation time in the ICU by predicting the deterioration of the patient situation in its early stages. In our work, we presented Experiments 1 and 3 to build a model to predict the deterioration of the patient situation. Experiments 2 and 4 identified the most important medical laboratory tests and then highlighted the joint tests between the two datasets. These four experiments would help physicians make better decisions in a very short time.For future work, the authors plan to conduct more experiments with larger data. Big data analysis would bring potential benefits to making the right decision to improve the efficiency, accuracy and timeliness of clinical decision-making in ICU."}], "references": [{"title": "Big data analytics in healthcare: promise and potential", "author": ["Raghupathi", "Wullianallur", "Raghupathi", "Viju"], "venue": "Health Inf. Sci. Syst., vol. 2, no. 1, p. 3, 2014.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2014}, {"title": "An integrated data mining approach to real-time clinical monitoring and deterioration warning", "author": ["Yi Mao", "Wenlin Chen", "Yixin Chen", "Chenyang Lu", "Marin Kollef", "Thomas Bailey"], "venue": "Knowledge discovery and data mining, 2012, pp. 1140\u20131148.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2012}, {"title": "Mining the Big Data: The Critical Feature Dimension Problem", "author": ["Q. Liu", "Sung", "Andrew H", "Ribeiro", "Bernardete", "Suryakumar", "Divya"], "venue": "Adv. Appl. Inform. IIAIAAI 2014 IIAI 3rd Int. Conf. On, pp. 499\u2013504, 2014.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2014}, {"title": "Reducing unnecessary lab testing in the ICU with artificial intelligence", "author": ["Federico Cismondi", "Leo A. Celi", "Andr\u00e9 S. Fialho", "Susana M. Vieira", "Shane R. Reti", "Joao MC Sousa", "Stan N. Finkelstein"], "venue": "Int. J. Med. Inf., vol. 82, no. 5, pp. 345\u2013358, 2013.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2013}, {"title": "A review of feature selection techniques in bioinformatics", "author": ["Yvan Saeys", "I\u00f1aki Inza", "Pedro Larra\u00f1aga"], "venue": "bioinformatics, vol. 23, no. 19, pp. 2507\u20132517, 2007.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2007}, {"title": "Ant Colony Optimization Algorithm for Interpretable Bayesian Classifiers Combination: Application to Medical Predictions", "author": ["S. Bouktif"], "venue": "PLoS ONE, vol. 9, no. 2, 2014.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2014}, {"title": "Top 10 algorithms in data mining", "author": ["X. Wu"], "venue": "Knowl. Inf. Syst., vol. 14, no. 1, pp. 1\u201337, 2008.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2008}, {"title": "Evaluation of Different Classification Techniques for WEB Data", "author": ["Chitra Nasa", "Suman"], "venue": "Int. J. Comput. Appl., vol. 52, no. 9, 2012.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2012}, {"title": "Sequential Minimal Optimization: A Fast Algorithm for Training Support Vector Machines", "author": ["John C. Platt"], "venue": "Adv. Kernel Methods\u2014support Vector Learn., vol. 3, 1999.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 1999}, {"title": "Random Forests", "author": ["Leo Breiman"], "venue": "Mach. Learn., vol. 45, no. 1, pp. 5\u201332, 2001.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2001}, {"title": "Customized prediction of short length of stay following elective cardiac surgery in elderly patients using a genetic algorithm", "author": ["J Lee", "S Govindan", "L Celi", "K Khabbaz", "B Subramaniam"], "venue": "World J Cardiovasc Surg, vol. 3, no. 5, pp. 163\u2013170, Sep. 2013.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2013}, {"title": "Methods of blood pressure measurement in the ICU", "author": ["LH Lehman", "M Saeed", "D Talmor", "R Mark", "A Malhotra"], "venue": "Crit Care Med, vol. 41, no. 1, pp. 34\u201340, 2013.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2013}, {"title": "Latent topic discovery of clinical concepts from hospital discharge summaries of a heterogeneous patient cohort", "author": ["L Lehman", "W Long", "M Saeed", "R Mark"], "venue": "Proceedings of the 36th International Conference of the IEEE Engineering in Medicine and Biology Society, 2014.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2014}], "referenceMentions": [{"referenceID": 0, "context": "Such data can be called big data due to their complexity, large size and difficulty to process in real-time [2].", "startOffset": 108, "endOffset": 111}, {"referenceID": 1, "context": "In [3], the authors developed an integrated data-mining approach to give early deterioration warnings for patients under real-time monitoring in the ICU and real-time data sensing (RDS).", "startOffset": 3, "endOffset": 6}, {"referenceID": 2, "context": "A health-data search engine was developed in [4] that supported predictions based on the summarised clusters patient types which claimed that it was better than predictions based on the non-summarised original data.", "startOffset": 45, "endOffset": 48}, {"referenceID": 2, "context": "[4] investigated the critical feature size dimension.", "startOffset": 0, "endOffset": 3}, {"referenceID": 4, "context": "A survey of feature selection is presented in [6].", "startOffset": 46, "endOffset": 49}, {"referenceID": 3, "context": "[5] proposed reducing unnecessary lab testing in the ICU.", "startOffset": 0, "endOffset": 3}, {"referenceID": 0, "context": "Big data has potential to support a wide range of medical and healthcare functions, including clinical decision support [2].", "startOffset": 120, "endOffset": 123}, {"referenceID": 7, "context": "It is about the \u2018extraction of interesting (non-trivial, implicit, previously unknown, and potentially useful) patterns or knowledge from huge amount of data [10]\u2019.", "startOffset": 158, "endOffset": 162}, {"referenceID": 2, "context": "Appropriate sampling and feature selection contribute to reducing the size of the dataset while obtaining satisfactory results in model building [4].", "startOffset": 145, "endOffset": 148}, {"referenceID": 4, "context": "In the context of classification, feature selection techniques can be organized into three categories, depending on how they perform the feature selection search to build the classification model: filter methods, wrapper methods and embedded methods, presented in table 1 [6] [7]:", "startOffset": 272, "endOffset": 275}, {"referenceID": 5, "context": "It requires the construction of a model that approximates the relationship between input features and output categories [8].", "startOffset": 120, "endOffset": 123}, {"referenceID": 6, "context": "However, it has class conditional independence, meaning it causes losses of accuracy and dependency [9].", "startOffset": 100, "endOffset": 103}, {"referenceID": 7, "context": "2) Sequential minimal optimization (SMO) is an algorithm for efficiently solving the optimization problem which arises during the training of support vector machines [10].", "startOffset": 166, "endOffset": 170}, {"referenceID": 8, "context": "The amount of memory required for SMO is linear in the training set size, which allows SMO to handle very large training sets [11].", "startOffset": 126, "endOffset": 130}, {"referenceID": 7, "context": "Although there is no predictability power in ZeroR, it is useful for determining a baseline performance as a benchmark for other classification methods [10].", "startOffset": 152, "endOffset": 156}, {"referenceID": 7, "context": "The decision tree partitions the input space of a data set into mutually exclusive regions, each of which is assigned a label, a value or an action to characterize its data points [10].", "startOffset": 180, "endOffset": 184}, {"referenceID": 9, "context": "5) A RandomForest is a combination of tree predictors such that each tree depends on the values of a random vector sampled independently and with the same distribution for all trees in the forest [12].", "startOffset": 196, "endOffset": 200}, {"referenceID": 10, "context": "Several works have used the MIMIC dataset, such as [14], [15] and [16].", "startOffset": 51, "endOffset": 55}, {"referenceID": 11, "context": "Several works have used the MIMIC dataset, such as [14], [15] and [16].", "startOffset": 57, "endOffset": 61}, {"referenceID": 12, "context": "Several works have used the MIMIC dataset, such as [14], [15] and [16].", "startOffset": 66, "endOffset": 70}, {"referenceID": 9, "context": "The reason for the best performance by RandomForest is that it works relatively well when used with high-dimensional data with a redundant/noisy set of features [12].", "startOffset": 161, "endOffset": 165}, {"referenceID": 8, "context": "The reason for this higher accuracy is that the amount of memory required for SMO is linear in the training set size, which allows SMO to handle very large training sets [11].", "startOffset": 170, "endOffset": 174}], "year": 2015, "abstractText": "A huge amount of medical data is generated every day, which presents a challenge in analysing these data. The obvious solution to this challenge is to reduce the amount of data without information loss. Dimension reduction is considered the most popular approach for reducing data size and also to reduce noise and redundancies in data. In this paper, we investigate the effect of feature selection in improving the prediction of patient deterioration in ICUs. We consider lab tests as features. Thus, choosing a subset of features would mean choosing the most important lab tests to perform. If the number of tests can be reduced by identifying the most important tests, then we could also identify the redundant tests. By omitting the redundant tests, observation time could be reduced and early treatment could be provided to avoid the risk. Additionally, unnecessary monetary cost would be avoided. Our approach uses state-ofthe-art feature selection for predicting ICU patient deterioration using the medical lab results. We apply our technique on the publicly available MIMIC-II database and show the effectiveness of the feature selection. We also provide a detailed analysis of the best features identified by our approach.", "creator": "Microsoft\u00ae Word 2013"}}}