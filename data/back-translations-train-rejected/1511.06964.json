{"id": "1511.06964", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "22-Nov-2015", "title": "Online Semi-Supervised Learning with Deep Hybrid Boltzmann Machines and Denoising Autoencoders", "abstract": "Two novel deep hybrid architectures, the Deep Hybrid Boltzmann Machine and the Deep Hybrid Denoising Auto-encoder, are proposed for handling semi-supervised learning problems. The models combine experts that model relevant distributions at different levels of abstraction to improve overall predictive performance on discriminative tasks. Theoretical motivations and algorithms for joint learning for each are presented. We apply the new models to the domain of data-streams in work towards life-long learning. The proposed architectures show improved performance compared to a pseudo-labeled, drop-out rectifier network.", "histories": [["v1", "Sun, 22 Nov 2015 04:53:43 GMT  (107kb,D)", "https://arxiv.org/abs/1511.06964v1", null], ["v2", "Tue, 24 Nov 2015 22:12:51 GMT  (79kb,D)", "http://arxiv.org/abs/1511.06964v2", null], ["v3", "Fri, 27 Nov 2015 05:06:36 GMT  (79kb,D)", "http://arxiv.org/abs/1511.06964v3", null], ["v4", "Sun, 13 Dec 2015 07:24:34 GMT  (79kb,D)", "http://arxiv.org/abs/1511.06964v4", null], ["v5", "Thu, 7 Jan 2016 19:44:42 GMT  (134kb,D)", "http://arxiv.org/abs/1511.06964v5", null], ["v6", "Fri, 8 Jan 2016 06:19:07 GMT  (160kb,D)", "http://arxiv.org/abs/1511.06964v6", null], ["v7", "Mon, 18 Jan 2016 18:06:01 GMT  (160kb,D)", "http://arxiv.org/abs/1511.06964v7", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["alexander g ororbia ii", "c lee giles", "david reitter"], "accepted": false, "id": "1511.06964"}, "pdf": {"name": "1511.06964.pdf", "metadata": {"source": "CRF", "title": "ONLINE SEMI-SUPERVISED LEARNING WITH DEEP HYBRID BOLTZMANN MACHINES AND DENOISING AUTOENCODERS", "authors": ["Alexander G. Ororbia"], "emails": ["ago109@psu.edu", "giles@psu.edu", "reitter@psu.edu"], "sections": [{"heading": null, "text": "Two novel deep hybrid architectures, the Deep Hybrid Boltzmann Machine and the Deep Hybrid Denoising Auto-Encoder, are proposed for dealing with semi-supervised learning problems, combining experts who model relevant distributions at different levels of abstraction to improve overall predictiveness for discriminatory tasks. Theoretical motivations and shared learning algorithms are presented for each of these models. We apply the new models to the area of data streams in lifelong learning, and the proposed architectures show improved performance compared to a pseudo-tagged fail-safe rectifier network."}, {"heading": "1 INTRODUCTION", "text": "This year it has come to the point that it has never come as far as this year."}, {"heading": "2 THE MULTI-LEVEL SEMI-SUPERVISED HYPOTHESIS", "text": "Our motivation for developing hybrid models stems from the semi-supervised learning method (previously) hypothesis of Rifai et al. (2011a), in which the learning aspects of p (x) improve the conditional p (y) x-th distribution of the model. It is hoped that as long as there is a relationship between p (x) and p (y), a learner might be able to use the unlabeled samples obtained from cheaply obtained samples. (2015b, a) shows that a hybrid neural architecture, L-layers deep, could combine this hypothesis with the expressivity granted from depth. (or the block model) could be used to p p (y) for l = [0, L] and vertically aggregated to achieve improved predictive performance."}, {"heading": "3 DEEP HYBRID MODEL ARCHITECTURES", "text": "In the following, we present the design of two candidates for the construction of uniform, deep hybrid architectures, namely the Deep Hybrid Boltzmann Machine (DHBM) and the Deep Hybrid Denoising Autoencoder (DHDA)."}, {"heading": "3.1 THE DEEP HYBRID BOLTZMANN MACHINE", "text": "Just as the Deep Boltzmann Machine (DBM) is applied to each level at its respective level of general predictive power, so the DHBM is to be regarded as a more complex version of an SBEN model. The primary advantage of a DHBM over the DBM, similar to the SBN, is that the hybrid architecture can be learned with the ultimate intention of a continuous classification, resulting in the tracking model of performance being led to a discriminatory loss through classification errors or another way, and the need for costly, non-trivial methods as annexed meaning sampling (AIS, Neal (2001)) to estimate partition functions for approximate purposes (Hinton, 2002). Instead of a single model, a DHBM is another way of simply viewing a composition of tightly integrated hybrid restricted Boltzmann machines (HRBMs)."}, {"heading": "3.2 THE DEEP HYBRID DENOISING AUTOENCODER", "text": "After a similar way to the previous section, but starting from the HSDA base architecture, we also suggest the autoencoder variant of the DHBM (DHDA = DHDA = DHDA = DHDA = DHDA = DHDA = DHDA = DHDA = DHDA = DHDA = DHDA = DHDA = DHDA = DHDA = DHDA = DHDA DHDA = DHDA DHDA DHDA DHDA = DHDA DHDA DHDA = DHDA DHDA DHDA = DHDA DHDA DHDA = DHDA DHDA DHDA = DHDA DHDA DHDA = DHDA DHDA = DHDA DHDA = DHDA DHDA = DHDA DHDA = DHDA DHDA DHDA = DHDA DHDA = DHDA DHDA = DHDA DHDA = DHDA DHDA DHDA = DHDA DHDA = DHDA DHDA DHDA = DHDA DHDA DHDA = DHDA DHDA DHDA DHDA = DHDA DHDA DHDA DHDA = DHDA DHDA DHDA = DHDA DHDA DHDA = DHDA DHDA = DHDA DHDA = DHDA DHDA DHDA = DHDA DHDA = DHDA DHDA DHDA = DHDA DHDA DHDA = DHDA DHDA DHDA = DHDA DHDA DHDA = DHDA DHDA DHDA = DHDA DHDA DHDA = DHDA DHDA DHDA = DHDA DHDA DHDA = DHDA DHDA DHDA DHDA = DHDA DHDA DHDA DHDA = DHDA DHDA DHDA = DHDA DHDA DHDA = DHDA DHDA DHDA DHDA DHDA = DHDA DHDA DHDA DHDA DHDA = DHDA DHDA DHDA DHDA = DHDA DHDA DHDA DHDA DHDA = DHDA DHDA = DHDA DHDA DHDA = DHDA DHDA DHDA = DHDA DHDA DHDA DHDA DHDA DHDA = DHDA DHDA DHDA = DHDA DHDA DHDA = DHDA DHDA DHDA DHDA = DHDA DHDA DHDA DHDA = DHDA DHDA DHDA = DHDA DHDA DHDA = DHDA DHDA DHDA = DHDA DHDA DHDA = DHDA = DHDA DHDA DHDA = DHDA DHDA DHDA = DHDA = DHDA DHDA DHDA DHDA = DHDA = DHDA DHDA = DH"}, {"heading": "3.3 JOINT PARAMETER LEARNING METHODS", "text": "In the following, we present the general learning framework for common training parameters of a deep hybrid architecture, such as the previously described DHBM and the DHDA. Since all parameters are modified together, the problem of displacement of representations is no longer a problem that coordinates all levels of the hybrid architecture globally during learning. In this context, a variety of estimators can be used to calculate parameter gradients. In particular, we will describe the two used in this study, namely stochastic maximum probability (for the DHBM) and backpropagation of errors (for the DHDA). However, before proceeding, we will first define the hybrid objective functions that both architectures attempt to minimize the loss.The DHDA is designed to minimize the following hybrid loss function: L (Dlab, Dunlab) = DDDDDlab | Dxt = 1 log p = 1 log p (y | xt) + LCE (Dlab) = explicit."}, {"heading": "3.3.1 THE JOINT HYBRID LEARNING FRAMEWORK", "text": "The general learning framework can be broken down into 3 key steps: 1) collecting midfield statistics using approximate variational conclusions, 2) calculating gradients to fit the detection network when used to initialize the center field, and 3) calculating the gradients for the common hybrid model using a relevant, architecture-specific algorithm, which works for either labeled or unlabeled samples, in the latter case using either the current estimate of the model's class probabilities or a pseudo-label (Lee, 2013) to create a proxy label. The complete, general procedure is presented in Algorithm 1. In offline learning environments, we find that, as in Salakhutdinov & Larochelle (2010), greedy, layered pre-training could be used to initiate a DHBM or DHDA much in the same way that learning a BDM model can be used as a predictive DN."}, {"heading": "3.3.2 MEAN-FIELD CONTRASTIVE DIVERGENCE & BACK-PROPAGATION", "text": "A simple estimator for calculating the gradients of a hybrid architecture uses only two sets of multi-level statistics that come from the detection network and execute the field equations in a single step, resulting in a series of \"positive phase statistics\" derived from the data vectors clamped at the input level of the model, and \"negative phase statistics\" derived from a single step of the freewheeling mode of the model. These two statistics can then be used to calculate parameter gradients to move the hybrid model toward a more desirable optimum."}, {"heading": "3.3.3 STOCHASTIC APPROXIMATION PROCEDURE (SAP)", "text": "Another estimator of the gradients of a hybrid architecture could take advantage of better mixing rates (i.e., minimum number of steps before the distribution of the Markov chain is close to its stationary distribution in relation to the total variation distance), which is made possible by an approximate maximum probability of learning. The key idea behind this method is to maintain a set of multiple persistent Gibbs sample chains (y | xt) using the ensemble backpropagation algorithm proposed in Ororbia II et al. (2015a), which could further improve performance, but we found no need in this paper."}, {"heading": "4 RELATED WORK", "text": "In fact, most of them will be able to abide by the rules that they have applied in practice."}, {"heading": "5 EXPERIMENTAL RESULTS", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "5.1 ONLINE LEARNING RESULTS", "text": "We use the Massive Online Analysis Framework (MOA, Bifet et al. (2010)) to simulate the process of online learning, where the learner does not have access to a static, limited dataset of i.i.d. - and thus does not have access to static, limited data, but rather is presented with mini-batches of samples (we chose N = 20 samples at the time) generated from a continuous non-stationary flow of non-i.i.d data. In order to learn effectively, the learner must make the best use of these described samples while using himself / herself from unlabeled samples. Specifically, the LED and Waveform streams were used, especially the versions that characterized the customizable feature concept to make learning more difficult over time."}, {"heading": "5.2 FINITE DATA-SET RESULTS", "text": "We evaluate our proposed architectures based on the MNIST dataset to assess their viability for offline semi-supervised learning. Details of the experimental setup can be found in Appendix C.1. In Table 1 6, we note that the 3-DHBM competes with several state-of-the-art models, in particular the EMBEDNN and DROPNN + PL. Although DHBM is not able to surpass the pre-training method proposed in Section 3.3.1, we note that this state-of-the-art method uses pre-training to achieve its final performance enhancement. Further work could combine our hybrid architectures with the discriminatory pre-training approach also proposed in Section 3.3.1. Alternatively, the DHBM could benefit from an additional weighted discriminatory gradient such as the one in the top-down algorithm by Orbia II et al. (2015a)."}, {"heading": "6 CONCLUSIONS", "text": "In contrast to their predecessors, these unified hybrid models circumvent the problem of displacement of representations because the different layers of these models are optimized from a global perspective. Furthermore, prediction uses classification information that is available at all abstraction levels of the model6 For purely monitored models, NN stands for a single-hidden-layer neural network, SVM stands for support sector machine, CNN for Convolutionary Neural Network. For semi-monitored models, TSVM is a transductive support vector machine, EMBEDNN is a deep neural network formed with an uncontrolled embedding criterion, CAE is a contractive auto-encoder, MTC is the manifold tangent classifier, and DROPNN stands for a drop-out deep neural network (where PL can be derived from the abelabelabelabelabelabelabelabelabelabelabelabelabelabelabelabelabelabelabelabelabelabelabelabelabelabelabelabelabelabelabelabelabelabelabelabelabelabelabelabelabelabelabelabelabelabelabelabelabelabelabelabelabelabelabelabelabelabelabelabelabelabelabelabelabelabelabelabelabelabelabelabelabelabelabelabelabelabelabelabelabelabelabelabelabelabelabelabelabelabelabelabelabelabelabelabelabelabelabelabelabelabelabelabelabelabelabelabelabelabelabelabelabelabelabelabelabelabelabelabelabelabelabelabelabelabelabelabelabelabelabelabelabelabelabelabelabelabelabelabelabelabelabelabelabelabelabelabelabelabelabelabelabelabelabelabelabelabelab"}, {"heading": "ACKNOWLEDGMENTS", "text": "We would like to thank Hugo Larochelle for the useful conversations that have contributed to this work. However, the shortcomings of this paper lie solely with us. This work was supported by NSF grant 1528409 to DR."}, {"heading": "A MEAN-FIELD CONTRASTIVE DIVERGENCE & BACK-PROPAGATION DETAILS", "text": "If a DHBM method is used to estimate this gradient, this is an effective combination with a DHBM method based on a DBM system (DHBM-DHBM-DBM-DBM-DBM-DBM-DBM-DBM-DBM-DBM-hybrid architect (MF-CD, Welling & Hinton (2002))). (DHBM-DBM-DBM-DBM-DBM-DBM-DBM-DBM-DBM-DBM-DBL-DBL-DBM-DBM-DBM-DBM-DBL-DBL-DBL-DBL-DBL-DBL-BL-BL-BDL-BL-BL-BL-BL-BDL-BL-BL-BDL-BL-BDL-BL-BL-BDL-BL-BL-BDL-BL-BL-BL-BL-BDL-BL-BL-BL-BDL-BL-DBL-DBL-DL-DL-DBL-DBL-DL-DBL-DBL-DBL-DBL-DBL-DBL-DBL-DBL-DBL-DBL-DBL-DBL-DBL-DBL-DBL-DBM-DBM-DBM-DBM-DBM-DBM-DBM-DBM-DM-DBM-DBM-DM-DBM-DM-DBM-DBM-DBM-DM-DM-DBM-DBM-DM-DBM-DBM-DBM-DM-DBM-DBM-DBM-DM-DBM-DM-DM-DM-DM-DM-DBL-DBL-DBL-DM-D"}, {"heading": "B STOCHASTIC APPROXIMATION PROCEDURE DETAILS", "text": "The stochastic approximation procedure specifically requires maintaining a set of persistent Markov chains (randomly initialized) or a set of M fantasy particles Xt = {xt, 1,..., xt, M} from which we calculate a mean. From an implementation perspective, each time we update the parameters of the hybrid model, we use a new state xt + 1, which is given via a transitional operator Tt (xt + 1 \u2190 xt), from which, like the DBM in Salakhutdinov & Larochelle (2010), we use Gibbs sampling. Maintaining a set of persistent Gibbs chains facilitates better mixing during the MCMC learning process and a better exploration of the energy landscape of the model. More importantly, we take a gradient step to obtain a better estimate for the final gradient by means of a point estimate of the intractable expectation of the model on the sample + 1."}, {"heading": "C EXPERIMENTAL SET-UP DETAILS", "text": "QC.1 MNISTs (QALCPARGRADENTS) (QM), QM (QM), QM), QMF (QM), 3), QMF (QM), 3), QM (QM), 3), QM (QM), QMF (QM), 3), QMF (QM), 3), QM (QM), QM (QM), QM (QM), QM (QM), QM (QM), QM (QM), QM (QM), QM (QM), QM (QM), QM (QM), QM (QM), QM (QM), QM (QM), QM (QM), QM (QM), QM (QM), QM (QM), QM (QM), QM (QM), QM (QM), QM (QM), QM (QM), QM (QM), QM (QM), QM (QM), QM (QM), QM (QM), QM (QM (QM), QM (QM), QM (QM (QM), QM (QM), QM (QM), QM (QM (QM), QM (QM), QM (QM (QM), QM (QM), QM (QM (QM), QM (QM), QM (QM (QM), QM (QM), QM (QM (QM), QM (QM (QM), QM (QM), QM (QM), QM (QM (QM), QM (QM), QM (QM (QM), QM (QM), QM (QM (QM), QM (QM), QM (QM (QM), QM (QM), QM (QM), QM (QM (QM"}], "references": [{"title": "Layer-wise learning of deep generative models", "author": ["Arnold", "Ludovic", "Ollivier", "Yann"], "venue": null, "citeRegEx": "Arnold et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Arnold et al\\.", "year": 2012}, {"title": "How auto-encoders could provide credit assignment in deep networks via target propagation", "author": ["Bengio", "Yoshua"], "venue": null, "citeRegEx": "Bengio and Yoshua.,? \\Q2014\\E", "shortCiteRegEx": "Bengio and Yoshua.", "year": 2014}, {"title": "Greedy Layer-wise Training of Deep Networks", "author": ["Bengio", "Yoshua", "Lamblin", "Pascal", "Popovici", "Dan", "Larochelle", "Hugo"], "venue": "Advances in Neural Information Processing Systems,", "citeRegEx": "Bengio et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Bengio et al\\.", "year": 2007}, {"title": "MOA: Massive online analysis, a framework for stream classification and clustering", "author": ["Bifet", "Albert", "Holmes", "Geoffrey", "Pfahringer", "Bernhard", "Kranen", "Philipp", "Kremer", "Hardy", "Jansen", "Timm", "Seidl", "Thomas"], "venue": null, "citeRegEx": "Bifet et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Bifet et al\\.", "year": 2010}, {"title": "Learning Deep Belief Networks from Non-stationary Streams", "author": ["Calandra", "Roberto", "Raiko", "Tapani", "Deisenroth", "Marc Peter", "Pouzols", "Federico Montesino"], "venue": "Artificial Neural Networks and Machine Learning \u2013 ICANN 2012,", "citeRegEx": "Calandra et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Calandra et al\\.", "year": 2012}, {"title": "Rules and representations", "author": ["Chomsky", "Noam"], "venue": "Behavioral and brain sciences,", "citeRegEx": "Chomsky and Noam.,? \\Q1980\\E", "shortCiteRegEx": "Chomsky and Noam.", "year": 1980}, {"title": "Why does unsupervised pre-training help deep learning", "author": ["Erhan", "Dumitru", "Bengio", "Yoshua", "Courville", "Aaron", "Manzagol", "Pierre-Antoine", "Vincent", "Pascal", "Samy"], "venue": null, "citeRegEx": "Erhan et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Erhan et al\\.", "year": 2010}, {"title": "On evaluating stream learning algorithms", "author": ["Gama", "Jo\u00e3o", "Sebasti\u00e3o", "Raquel", "Rodrigues", "Pedro Pereira"], "venue": "Machine Learning,", "citeRegEx": "Gama et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Gama et al\\.", "year": 2012}, {"title": "Multi-prediction deep boltzmann machines", "author": ["Goodfellow", "Ian", "Mirza", "Mehdi", "Courville", "Aaron", "Bengio", "Yoshua"], "venue": "Advances in Neural Information Processing Systems", "citeRegEx": "Goodfellow et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Goodfellow et al\\.", "year": 2013}, {"title": "Training Products of Experts by Minimizing Contrastive Divergence", "author": ["Hinton", "Geoffrey E"], "venue": "Neural Computation,", "citeRegEx": "Hinton and E.,? \\Q2002\\E", "shortCiteRegEx": "Hinton and E.", "year": 2002}, {"title": "wake-sleep\u201d algorithm for unsupervised neural networks", "author": ["Hinton", "Geoffrey E", "Dayan", "Peter", "Frey", "Brendan J", "Neal", "Radford M. The"], "venue": null, "citeRegEx": "Hinton et al\\.,? \\Q1995\\E", "shortCiteRegEx": "Hinton et al\\.", "year": 1995}, {"title": "Improving neural networks by preventing co-adaptation of feature detectors", "author": ["Hinton", "Geoffrey E", "Srivastava", "Nitish", "Krizhevsky", "Alex", "Sutskever", "Ilya", "Salakhutdinov", "Ruslan R"], "venue": null, "citeRegEx": "Hinton et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Hinton et al\\.", "year": 2012}, {"title": "Semisupervised learning with deep generative models", "author": ["Kingma", "Diederik P", "Mohamed", "Shakir", "Rezende", "Danilo Jimenez", "Welling", "Max"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Kingma et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Kingma et al\\.", "year": 2014}, {"title": "Classification using Discriminative Restricted Boltzmann Machines", "author": ["Larochelle", "Hugo", "Bengio", "Yoshua"], "venue": "In Proceedings of the 25th International Conference on Machine learning,", "citeRegEx": "Larochelle et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Larochelle et al\\.", "year": 2008}, {"title": "Learning Algorithms for the Classification Restricted Boltzmann Machine", "author": ["Larochelle", "Hugo", "Mandel", "Michael", "Pascanu", "Razvan", "Bengio", "Yoshua"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "Larochelle et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Larochelle et al\\.", "year": 2012}, {"title": "Principled Hybrids of Generative and Discriminative Models", "author": ["Lasserre", "Julia A", "Bishop", "Christopher M", "Minka", "Thomas P"], "venue": "In Proceedings of the 2006 IEEE Computer Society Conference on Computer Vision and Pattern Recognition - Volume 1,", "citeRegEx": "Lasserre et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Lasserre et al\\.", "year": 2006}, {"title": "Pseudo-label: The Simple and Efficient Semi-supervised Learning Method for Deep Neural Networks", "author": ["Lee", "Dong-Hyun"], "venue": "In Workshop on Challenges in Representation Learning,", "citeRegEx": "Lee and Dong.Hyun.,? \\Q2013\\E", "shortCiteRegEx": "Lee and Dong.Hyun.", "year": 2013}, {"title": "Difference target propagation", "author": ["Lee", "Dong-Hyun", "Zhang", "Saizheng", "Fischer", "Asja", "Bengio", "Yoshua"], "venue": null, "citeRegEx": "Lee et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Lee et al\\.", "year": 2015}, {"title": "Annealed importance sampling", "author": ["Neal", "Radford M"], "venue": null, "citeRegEx": "Neal and M.,? \\Q2001\\E", "shortCiteRegEx": "Neal and M.", "year": 2001}, {"title": "Learning a deep hybrid model for semisupervised text classification", "author": ["Ororbia II", "Alexander G", "Giles", "C. Lee", "Reitter", "David"], "venue": "In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing (EMNLP),", "citeRegEx": "II et al\\.,? \\Q2015\\E", "shortCiteRegEx": "II et al\\.", "year": 2015}, {"title": "Online learning of deep hybrid architectures for semi-supervised categorization. In Machine Learning and Knowledge Discovery in Databases (Proceedings", "author": ["Ororbia II", "Alexander G", "Reitter", "David", "Wu", "Jian", "Giles", "C. Lee"], "venue": "ECML PKDD 2015),", "citeRegEx": "II et al\\.,? \\Q2015\\E", "shortCiteRegEx": "II et al\\.", "year": 2015}, {"title": "Sum-product networks: A new deep architecture", "author": ["Poon", "Hoifung", "Domingos", "Pedro"], "venue": "In Computer Vision Workshops (ICCV Workshops),", "citeRegEx": "Poon et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Poon et al\\.", "year": 2011}, {"title": "Semi-supervised learning of compact document representations with deep networks", "author": ["Ranzato", "Marc\u2019 Aurelio", "Szummer", "Martin"], "venue": "In Proceedings of the 25th International Conference on Machine Learning,", "citeRegEx": "Ranzato et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Ranzato et al\\.", "year": 2008}, {"title": "The manifold tangent classifier", "author": ["Rifai", "Salah", "Dauphin", "Yann N", "Vincent", "Pascal", "Bengio", "Yoshua", "Muller", "Xavier"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Rifai et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Rifai et al\\.", "year": 2011}, {"title": "Contractive autoencoders: Explicit invariance during feature extraction", "author": ["Rifai", "Salah", "Vincent", "Pascal", "Muller", "Xavier", "Glorot", "Bengio", "Yoshua"], "venue": "In Proceedings of the 28th International Conference on Machine Learning", "citeRegEx": "Rifai et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Rifai et al\\.", "year": 2011}, {"title": "Deep boltzmann machines", "author": ["Salakhutdinov", "Ruslan", "Hinton", "Geoffrey E"], "venue": "In International Conference on Artificial Intelligence and Statistics, pp", "citeRegEx": "Salakhutdinov et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Salakhutdinov et al\\.", "year": 2009}, {"title": "Efficient learning of deep boltzmann machines", "author": ["Salakhutdinov", "Ruslan", "Larochelle", "Hugo"], "venue": "In International Conference on Artificial Intelligence and Statistics,", "citeRegEx": "Salakhutdinov et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Salakhutdinov et al\\.", "year": 2010}, {"title": "Semi-supervised recursive autoencoders for predicting sentiment distributions", "author": ["Socher", "Richard", "Pennington", "Jeffrey", "Huang", "Eric H", "Ng", "Andrew Y", "Manning", "Christopher D"], "venue": "In Proceedings of the Conference on Empirical Methods in Natural Language Processing,", "citeRegEx": "Socher et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Socher et al\\.", "year": 2011}, {"title": "A new learning algorithm for mean field boltzmann machines", "author": ["Welling", "Max", "Hinton", "Geoffrey E"], "venue": "Artificial Neural Networks \u2014 ICANN 2002,", "citeRegEx": "Welling et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Welling et al\\.", "year": 2002}, {"title": "Deep learning via semisupervised embedding", "author": ["Weston", "Jason", "Ratle", "Fr\u00e9d\u00e9ric", "Mobahi", "Hossein", "Collobert", "Ronan"], "venue": "In Neural Networks: Tricks of the Trade,", "citeRegEx": "Weston et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Weston et al\\.", "year": 2012}, {"title": "Supervised Deep Learning with Auxiliary Networks", "author": ["Zhang", "Junbo", "Tian", "Guangjian", "Mu", "Yadong", "Fan", "Wei"], "venue": "In Proceedings of the 20th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD", "citeRegEx": "Zhang et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Zhang et al\\.", "year": 2014}], "referenceMentions": [{"referenceID": 6, "context": "Unsupervised pre-training can help construct an architecture composed of many layers of feature detectors (Erhan et al., 2010).", "startOffset": 106, "endOffset": 126}, {"referenceID": 2, "context": "Even though the ultimate task is discriminative, a generative architecture, such as the Deep Belief Network (DBN) or Stacked Denoising Autoencoder (SDA), may be first used to initialize the parameters of a multi-layer perceptron (MLP) which then is fine-tuned to the supervised learning task (Bengio et al., 2007).", "startOffset": 292, "endOffset": 313}, {"referenceID": 14, "context": "However, learning the parameters of the unsupervised architecture is quite difficult, often with little to no grasp of the final influence the generative parameters will have on the final discriminative model (Larochelle et al., 2012; Goodfellow et al., 2013).", "startOffset": 209, "endOffset": 259}, {"referenceID": 8, "context": "However, learning the parameters of the unsupervised architecture is quite difficult, often with little to no grasp of the final influence the generative parameters will have on the final discriminative model (Larochelle et al., 2012; Goodfellow et al., 2013).", "startOffset": 209, "endOffset": 259}, {"referenceID": 10, "context": "While greedily built models can be later tuned jointly to construct architectures such as the Deep Boltzmann Machine (DBM, Salakhutdinov & Hinton (2009)) or via the Wake-Sleep algorithm (Hinton et al., 1995) (and improved variants thereof (Bornschein & Bengio)) the original training difficulty remains.", "startOffset": 186, "endOffset": 207}, {"referenceID": 2, "context": "Even though the ultimate task is discriminative, a generative architecture, such as the Deep Belief Network (DBN) or Stacked Denoising Autoencoder (SDA), may be first used to initialize the parameters of a multi-layer perceptron (MLP) which then is fine-tuned to the supervised learning task (Bengio et al., 2007). However, learning the parameters of the unsupervised architecture is quite difficult, often with little to no grasp of the final influence the generative parameters will have on the final discriminative model (Larochelle et al., 2012; Goodfellow et al., 2013). These architectures often feature many hyper-parameters that affect generalization performance, quickly creating a challenging tuning problem for human users. Furthermore, though efficient, the generative models used for pre-training learnt greedily carry the potential disadvantage of not providing \u201cglobal coordination between the different levels\u201d (Bengio, 2014), the sub-optimality of which was empirically shown in Arnold & Ollivier (2012). This issue was further discussed as the problem of \u201cshifting representations\u201d in Ororbia II et al.", "startOffset": 293, "endOffset": 1021}, {"referenceID": 2, "context": "Even though the ultimate task is discriminative, a generative architecture, such as the Deep Belief Network (DBN) or Stacked Denoising Autoencoder (SDA), may be first used to initialize the parameters of a multi-layer perceptron (MLP) which then is fine-tuned to the supervised learning task (Bengio et al., 2007). However, learning the parameters of the unsupervised architecture is quite difficult, often with little to no grasp of the final influence the generative parameters will have on the final discriminative model (Larochelle et al., 2012; Goodfellow et al., 2013). These architectures often feature many hyper-parameters that affect generalization performance, quickly creating a challenging tuning problem for human users. Furthermore, though efficient, the generative models used for pre-training learnt greedily carry the potential disadvantage of not providing \u201cglobal coordination between the different levels\u201d (Bengio, 2014), the sub-optimality of which was empirically shown in Arnold & Ollivier (2012). This issue was further discussed as the problem of \u201cshifting representations\u201d in Ororbia II et al. (2015b), where upper layers of a multi-level model are updated using immature latent representations from layers below potentially leading to unstable learning behavior or worsened generalization performance.", "startOffset": 293, "endOffset": 1129}, {"referenceID": 2, "context": "Even though the ultimate task is discriminative, a generative architecture, such as the Deep Belief Network (DBN) or Stacked Denoising Autoencoder (SDA), may be first used to initialize the parameters of a multi-layer perceptron (MLP) which then is fine-tuned to the supervised learning task (Bengio et al., 2007). However, learning the parameters of the unsupervised architecture is quite difficult, often with little to no grasp of the final influence the generative parameters will have on the final discriminative model (Larochelle et al., 2012; Goodfellow et al., 2013). These architectures often feature many hyper-parameters that affect generalization performance, quickly creating a challenging tuning problem for human users. Furthermore, though efficient, the generative models used for pre-training learnt greedily carry the potential disadvantage of not providing \u201cglobal coordination between the different levels\u201d (Bengio, 2014), the sub-optimality of which was empirically shown in Arnold & Ollivier (2012). This issue was further discussed as the problem of \u201cshifting representations\u201d in Ororbia II et al. (2015b), where upper layers of a multi-level model are updated using immature latent representations from layers below potentially leading to unstable learning behavior or worsened generalization performance. While greedily built models can be later tuned jointly to construct architectures such as the Deep Boltzmann Machine (DBM, Salakhutdinov & Hinton (2009)) or via the Wake-Sleep algorithm (Hinton et al.", "startOffset": 293, "endOffset": 1483}, {"referenceID": 2, "context": "Even though the ultimate task is discriminative, a generative architecture, such as the Deep Belief Network (DBN) or Stacked Denoising Autoencoder (SDA), may be first used to initialize the parameters of a multi-layer perceptron (MLP) which then is fine-tuned to the supervised learning task (Bengio et al., 2007). However, learning the parameters of the unsupervised architecture is quite difficult, often with little to no grasp of the final influence the generative parameters will have on the final discriminative model (Larochelle et al., 2012; Goodfellow et al., 2013). These architectures often feature many hyper-parameters that affect generalization performance, quickly creating a challenging tuning problem for human users. Furthermore, though efficient, the generative models used for pre-training learnt greedily carry the potential disadvantage of not providing \u201cglobal coordination between the different levels\u201d (Bengio, 2014), the sub-optimality of which was empirically shown in Arnold & Ollivier (2012). This issue was further discussed as the problem of \u201cshifting representations\u201d in Ororbia II et al. (2015b), where upper layers of a multi-level model are updated using immature latent representations from layers below potentially leading to unstable learning behavior or worsened generalization performance. While greedily built models can be later tuned jointly to construct architectures such as the Deep Boltzmann Machine (DBM, Salakhutdinov & Hinton (2009)) or via the Wake-Sleep algorithm (Hinton et al., 1995) (and improved variants thereof (Bornschein & Bengio)) the original training difficulty remains. One way to exploit the power of representation learning without the difficulties of pre-training is to instead solve the hybrid learning problem: force a model to balance multiple supervised and unsupervised learning objectives in a principled manner. Many recent examples demonstrate the power and flexibility of this approach (Larochelle & Bengio, 2008; Ranzato & Szummer, 2008; Socher et al., 2011; Larochelle et al., 2012; Ororbia II et al., 2015b,a). The Stacked Boltzmann Expert Network (SBEN) and its autoencoder variant, the Hybrid Stacked Denoising Autoencoder (HSDA, Ororbia II et al. (2015b)) were proposed as semi-supervised deep architectures that combined the expressiveness afforded by a multi-layer composition of non-linearities with a more practical approach to model construction.", "startOffset": 293, "endOffset": 2237}, {"referenceID": 21, "context": "Our motivation for developing hybrid models comes from the semi-supervised learning (prior) hypothesis of Rifai et al. (2011a), where learning aspects of p(x) improves the model\u2019s conditional p(y|x).", "startOffset": 106, "endOffset": 127}, {"referenceID": 18, "context": "In the work of Ororbia II et al. (2015b), a 3-layer SBEN outperformed a semi-supervised deep rectifier network (among other base-lines) by nearly 14% in image classification tasks.", "startOffset": 23, "endOffset": 41}, {"referenceID": 18, "context": "In the work of Ororbia II et al. (2015b), a 3-layer SBEN outperformed a semi-supervised deep rectifier network (among other base-lines) by nearly 14% in image classification tasks. In Ororbia II et al. (2015a) a 3-layer SBEN (trained via the bottom-up-top-down algorithm) was shown to outperform a semi-supervised rectifier network and the original SBEN of Ororbia II et al.", "startOffset": 23, "endOffset": 210}, {"referenceID": 18, "context": "In the work of Ororbia II et al. (2015b), a 3-layer SBEN outperformed a semi-supervised deep rectifier network (among other base-lines) by nearly 14% in image classification tasks. In Ororbia II et al. (2015a) a 3-layer SBEN (trained via the bottom-up-top-down algorithm) was shown to outperform a semi-supervised rectifier network and the original SBEN of Ororbia II et al. (2015b) (and other competitive text classifiers) consistently in text categorization experiments by as much as 36%.", "startOffset": 23, "endOffset": 383}, {"referenceID": 4, "context": "The works of Calandra et al. (2012); Zhou et al.", "startOffset": 13, "endOffset": 36}, {"referenceID": 4, "context": "The works of Calandra et al. (2012); Zhou et al. (2012), which are special cases of the HSDA, are also further experimental evidence that support the weak multi-level semi-supervised hypothesis.", "startOffset": 13, "endOffset": 56}, {"referenceID": 19, "context": "Ororbia II et al. (2015a) introduced a degree of joint learning of SBEN parameters through the Bottom-Up-Top-Down algorithm to improve performance, however, its bottom-up generative gradient step was still layer-wise in nature.", "startOffset": 8, "endOffset": 26}, {"referenceID": 19, "context": "We note that the DHBM\u2019s hybrid loss function could be augmented with a pair of direct discriminative gradients that additionally minimize \u2212 \u2211|D| t=1 log p(y|xt) for both Dlab and Dunlab using the ensemble backpropagation algorithm proposed in Ororbia II et al. (2015a). This could further improve performance but we found not necessary in this paper\u2019s experiments.", "startOffset": 251, "endOffset": 269}, {"referenceID": 2, "context": "Some leverage auxiliary models to encourage learning of discriminative information earlier at various stages of the learning process (Bengio et al., 2007; Zhang et al., 2014; Lee et al., 2014).", "startOffset": 133, "endOffset": 192}, {"referenceID": 30, "context": "Some leverage auxiliary models to encourage learning of discriminative information earlier at various stages of the learning process (Bengio et al., 2007; Zhang et al., 2014; Lee et al., 2014).", "startOffset": 133, "endOffset": 192}, {"referenceID": 29, "context": ", 2011a) or through a special regularizer (Weston et al., 2012).", "startOffset": 42, "endOffset": 63}, {"referenceID": 15, "context": "Other hybrid learning approaches follow similar ideas of (Lasserre et al., 2006), including the training schemes proposed in Ororbia II et al.", "startOffset": 57, "endOffset": 80}, {"referenceID": 12, "context": "Other hybrid learning approaches follow similar ideas of (Lasserre et al., 2006), including the training schemes proposed in Ororbia II et al. (2015b,a), which built on the initial ideas of Larochelle & Bengio (2008); Larochelle et al.", "startOffset": 58, "endOffset": 217}, {"referenceID": 12, "context": "(2015b,a), which built on the initial ideas of Larochelle & Bengio (2008); Larochelle et al. (2012), and of which the schemes of Calandra et al.", "startOffset": 75, "endOffset": 100}, {"referenceID": 4, "context": "(2012), and of which the schemes of Calandra et al. (2012); Zhou et al.", "startOffset": 36, "endOffset": 59}, {"referenceID": 4, "context": "(2012), and of which the schemes of Calandra et al. (2012); Zhou et al. (2012) were shown to be special cases.", "startOffset": 36, "endOffset": 79}, {"referenceID": 8, "context": "The DHBM shares some similarites to that of the \u201cstitched-together\u201d DBM of Salakhutdinov & Hinton (2009) and the MP-DBM (Goodfellow et al., 2013), which was originally proposed as another way to circumvent the greedy pre-training of DBM\u2019s using a back-propagation-based approach on an unfolded inference graph.", "startOffset": 120, "endOffset": 145}, {"referenceID": 8, "context": "The DHBM shares some similarites to that of the \u201cstitched-together\u201d DBM of Salakhutdinov & Hinton (2009) and the MP-DBM (Goodfellow et al., 2013), which was originally proposed as another way to circumvent the greedy pre-training of DBM\u2019s using a back-propagation-based approach on an unfolded inference graph. The key advantage of the MP-DBM is that it is capable of working on a variety of variational inference tasks beyond classification (i.e., input completion, classification with missing inputs, etc.). We note that combining our hybrid learning framework with a more advanced Boltzmann architecture like the MP-DBM could yield even more powerful semi-supervised models. The deep generative models proposed Kingma et al. (2014) also share similarity to our work especially in their use of a recognition network to make inference of latent states in such models fast and tractable.", "startOffset": 121, "endOffset": 735}, {"referenceID": 17, "context": "model that is capable of learning either discriminatively or generatively may be employed, such as Sum-Product Networks (Poon & Domingos, 2011) or recently proposed back-propagation-free models like the Difference-Target Propagation network (Lee et al., 2015).", "startOffset": 241, "endOffset": 259}, {"referenceID": 10, "context": "For simplicity, we only chose to incorporate a drop-out scheme Hinton et al. (2012) to our learning procedure since our comparative base-line (the psuedo-labeled MLP) also employed one.", "startOffset": 63, "endOffset": 84}, {"referenceID": 10, "context": "For simplicity, we only chose to incorporate a drop-out scheme Hinton et al. (2012) to our learning procedure since our comparative base-line (the psuedo-labeled MLP) also employed one. During learning, all this entails is applying random binary masks to the hidden layer statistics calculated from the recognition network and once again to calculated mean-field statistics (which will set various hidden units to 0 under a given probability). For the DHDA, the same binary masks used during calculation of layer-wise statistics are applied to the error deltas computed during back-propagation (as in Hinton et al. (2012)).", "startOffset": 63, "endOffset": 622}, {"referenceID": 3, "context": "We make use of the Massive Online Analysis framework (MOA, Bifet et al. (2010)) to simulate the process of online learning, where the learner does not have access to a static, finite data-set of i.", "startOffset": 59, "endOffset": 79}, {"referenceID": 7, "context": "995 (following the discussion of admissible values in Gama et al. (2012)).", "startOffset": 54, "endOffset": 73}, {"referenceID": 7, "context": "(Gama et al., 2012) showed that metrics with forgetting mechanisms were more appropriate for evolving data stream settings (as compared to simple predictive sequential error).", "startOffset": 0, "endOffset": 19}, {"referenceID": 29, "context": "81 EMBEDNN (Weston et al., 2012) 16.", "startOffset": 11, "endOffset": 32}, {"referenceID": 19, "context": "Alternatively, the DHBM could benefit from an additional weighted discriminative gradient like that used in the Top-DownBottom-Up algorithm of Ororbia II et al. (2015a). We also note that our DHDA model does not perform so well on this benchmark, only beating out the architectures trained on the supervised subset.", "startOffset": 151, "endOffset": 169}], "year": 2016, "abstractText": "Two novel deep hybrid architectures, the Deep Hybrid Boltzmann Machine and the Deep Hybrid Denoising Auto-encoder, are proposed for handling semisupervised learning problems. The models combine experts that model relevant distributions at different levels of abstraction to improve overall predictive performance on discriminative tasks. Theoretical motivations and algorithms for joint learning for each are presented. We apply the new models to the domain of datastreams in work towards life-long learning. The proposed architectures show improved performance compared to a pseudo-labeled, drop-out rectifier network.", "creator": "LaTeX with hyperref package"}}}