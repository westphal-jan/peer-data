{"id": "1511.06995", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "22-Nov-2015", "title": "Non-Sentential Utterances in Dialogue: Experiments in Classification and Interpretation", "abstract": "Non-sentential utterances (NSUs) are utterances that lack a complete sentential form but whose meaning can be inferred from the dialogue context, such as \"OK\", \"where?\", \"probably at his apartment\". The interpretation of non-sentential utterances is an important problem in computational linguistics since they constitute a frequent phenomena in dialogue and they are intrinsically context-dependent. The interpretation of NSUs is the task of retrieving their full semantic content from their form and the dialogue context. The first half of this thesis is devoted to the NSU classification task. Our work builds upon Fern\\'andez et al. (2007) which present a series of machine-learning experiments on the classification of NSUs. We extended their approach with a combination of new features and semi-supervised learning techniques. The empirical results presented in this thesis show a modest but significant improvement over the state-of-the-art classification performance. The consecutive, yet independent, problem is how to infer an appropriate semantic representation of such NSUs on the basis of the dialogue context. Fern\\'andez (2006) formalizes this task in terms of \"resolution rules\" built on top of the Type Theory with Records (TTR). Our work is focused on the reimplementation of the resolution rules from Fern\\'andez (2006) with a probabilistic account of the dialogue state. The probabilistic rules formalism Lison (2014) is particularly suited for this task because, similarly to the framework developed by Ginzburg (2012) and Fern\\'andez (2006), it involves the specification of update rules on the variables of the dialogue state to capture the dynamics of the conversation. However, the probabilistic rules can also encode probabilistic knowledge, thereby providing a principled account of ambiguities in the NSU resolution process.", "histories": [["v1", "Sun, 22 Nov 2015 11:28:26 GMT  (460kb,D)", "http://arxiv.org/abs/1511.06995v1", "Master thesis, 98 pages, ISBN: 9788887096057"]], "COMMENTS": "Master thesis, 98 pages, ISBN: 9788887096057", "reviews": [], "SUBJECTS": "cs.CL cs.AI", "authors": ["paolo dragone"], "accepted": false, "id": "1511.06995"}, "pdf": {"name": "1511.06995.pdf", "metadata": {"source": "CRF", "title": "Non-Sentential Utterances in Dialogue: Experiments in Classification and Interpretation", "authors": ["Paolo Dragone", "Pierre Lison"], "emails": ["dragone.paolo@gmail.com"], "sections": [{"heading": null, "text": "Sentential Utterances in Dialogue: Experiments in Classification and InterpretationDepartment of Computer, Control and Management Engineering Master of Science in Engineering in Computer ScienceCandidate Paolo Dragone 1370640Thesis Advisor Prof. Roberto NavigliCo-Advisor Dr. Pierre Lison (University of Oslo) Academic Year 2014 / 2015ar Xiv: 151 1.06 995v 1 [cs.C L] 22 Nov 201 5Non-Sentential Utterances in Dialogue: Experiments in Classification and Interpretation Master thesis. Sapienza - University of Rome \u00a9 2015 Paolo Dragone. All rights reservedThis thesis was developed by LATEX and the Sapthesis class.Version: 14 October 2015 Author's email: dragone.paolo @ gmail.com"}, {"heading": "Abstract", "text": "The interpretation of non-entential expressions is an important problem in computational linguistics, as they are a common phenomenon in dialogue and are intrinsically context-dependent. Interpretation of NSUs is the task of retrieving their complete semantic content from their form and context. NSUs, however, also come in a variety of forms and functions, and classification into the correct category is a prerequisite for their interpretation. The first half of this thesis is devoted to the task of NSU classification. Our work builds on Ferna'ndez et al. (2007), which presents a series of machine-learning experiments based on NSUs (formal theories). We have expanded their approach to include a combination of new features and semi-supervised learning techniques."}, {"heading": "Acknowledgments", "text": "I have studied hard, explored the world, been homesick and experienced many wonderful moments. It has been an exciting time and now, at the finish line, I am coming to the end with a smile on my face. I am grateful to my alma mater Sapienza for these opportunities, who, despite all the difficulties, has also brought me many positive memories. However, this thesis is the product of the time I spent at the University of Oslo under the supervision of Dr. Pierre Lison. Pierre was not only a great mentor, he was also my mentor, he taught me an immense number of things. He introduced me to the research on dialogue and suggested that I put the topic of non-judgmental utterances first place. My collaboration with Pierre also led to my very first scientific publication at Semdial 2015 in Go-teg."}, {"heading": "Contents", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "1 Introduction 1", "text": "1.1 Motivation.................................................................................."}, {"heading": "2 Background 5", "text": "......................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................"}, {"heading": "3 Classification of Non-Sentential Utterances 21", "text": ".)......................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................."}, {"heading": "4 Resolution of Non-Sentential Utterances 47", "text": "The number of unemployed in Germany increased slightly compared to the previous year, while the number of unemployed in Germany rose slightly compared to the previous year."}, {"heading": "5 Conclusion 73", "text": "5.1 Contributions.......................................... 73 5.2 Future developments.......................... 75 Annexes 79"}, {"heading": "A Context update rules 79", "text": ""}, {"heading": "List of acronyms", "text": "AL Active LearningBNC British National CorpusDGB Dialogue GameboardFEC Focus-Establishing ConstitutionMaxQUD Maximum QUD elementNLG Natural Language GenerationNLU Natural Language Understanding NLU Natural Language Vector MachineTSVM Transductive Support Vector MachineTTR Type Theory with RecordsxiChapter 1"}, {"heading": "Introduction", "text": "In dialogue, utterances do not always take the form of complete sentences. Utterances can sometimes miss some components - subject, verb or complement - because they can be understood from previous utterances or other contextual information. [BNC: JK8 168-169] 1 (1,2) a: They would not do it, no.b: Why? [BNC: H5H 202-203] (1,3) a: So the tape will last the entire two hours? b: Yes, apparently. [BNC: J9A 76-77] (1,4) a: Right disk number four? b: Three. [BNC: HDH 377-378] We can understand the meaning of NSUs in the short topless dialogues, even if they cannot easily influence our sentences."}, {"heading": "1.1 Motivation", "text": "According to Fernandez and Ginzburg (2002) and related work, the frequency of NSUs in the dialogue transcripts of the British National Corpus amounts to about 10% of the total number of utterances, but this number can vary greatly when taking into account a greater variety of phenomena or different dialog areas, e.g. Snakes (2003), which estimates the frequency of NSUs at 20% of the total number of utterances. Despite their ubiquity, the semantic content of NSUs is often difficult to extract automatically. Indeed, non-sentences per se depend on the dialogue context, and it is impossible to understand them without accessing the surrounding context. Their high context dependence makes their interpretation a difficult problem, both from a theoretical and a computational point of view. NSUs constitute a wide range of linguistic phenomena that must be considered in dialogue."}, {"heading": "1.2 Contribution", "text": "Our work follows two parallel paths. On the one hand, we address the problem of classification of NSUs by extending the work of Ferna \u0301 ndez et al. (2007). On the other hand, we propose a novel approach to resolution of NSUs by using probabilistic rules (Lison, 2015). The classification task is necessary to select the solution method, but it is nonetheless an independent problem, and it can occur in many different situations. Our contribution to this problem is a small but significant improvement on the accuracy of previous work, as well as exploring a way to address the scarcity of the data referred to. 2Our work on resolution of NSUs is inspired by Ferna \u0301 ndez and Ginzburg (2012), which provide the theoretical background for our study. However, its framework is purely logical-based, so it may have some drawbacks in dealing with raw, conspiratorial data that often contain hidden or partially observable variables."}, {"heading": "1.3 Outline", "text": "Chapter 2This chapter covers the background knowledge required for the development of the following chapters. In particular, the chapter describes the concept of non-sentential enunciation and the task of interpreting NSUs with an emphasis on previous work. Second, the chapter provides an overview of the formal representation of the dialogue context from the Ginzburg Theory (2012). We briefly discuss type theory with records, the semantic representation of statements, and the updated rules for the dialogue context. Finally, we introduce the probabilistic approach to defining the dialogue context from Lisbon (2014). We discuss the basics of Bayesian networks (dialogue context representation) and probabilistic rule formalization. 3Chapter 3This chapter describes the task of classifying non-sentential statements. It provides details of our approach, starting with the replication of the work from Ferna \u2012 ndez et al. (2007), which we use as a basis."}, {"heading": "Background", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "2.1 Non-Sentential Utterances", "text": "From a linguistic perspective, the term \"NSU\" is historically a collective term for many elliptical phenomena that often occur in dialogue. To give a definition of non-sentence utopias, we must first cite the definition of \"non-sentence utopias\" (2006), which is indeed a very general definition, whereas the term \"non-sentence utopia\" (2012) of \"non-sentence utopia\" (2012) is in most cases a purely formal definition."}, {"heading": "2.1.1 A taxonomy of NSUs", "text": "As we briefly mentioned in Chapter 1, there are non-sentential statements in a wide variety of forms. We can categorize NSUs by their form and intended meaning. NSUs can, for example, provide affirmative or negative answers to polar questions, requests for clarification or corrections.To classify NSUs, we use a taxonomy defined by Ferna \u0301 ndez and Ginzburg (2002), which is a far-reaching taxonomy resulting from a corpus study of part of the British National Corpus (Burnard, 2000). Table 2.1 contains a summary of taxonomy with an additional categorization of classes by function, as defined by Ferna \u0301 ndez (2006) and refined by Ginzburg (2012). Other NSUs taxonomies are available from previous work by e.g. Snakes (2003), but we have opted for the taxonomy used by Ferna \u0301 ndez and Ginzburg (2002)."}, {"heading": "Plain Acknowledgment", "text": "Receipts are used to signal understanding or acceptance of the previous statement, usually with words or sounds such as yeah, right, mhm. (2.2) a: I will receive a copy of this ribbon.b: Right. [BNC: J42 71-72] 6"}, {"heading": "Repeated Acknowledgment", "text": "This is another kind of confirmation that uses the repetition or reformulation of a component of the predecessor to show understanding. (2.3) a: Oh so, if you press Enter, it will come down one row.b: Enter. [BNC: G4K 102-103]"}, {"heading": "Clarification Ellipsis", "text": "These are NSUs used to demand clarification of an aspect of the predecessor that was not fully understood. (2.4) a: I would press F ten.b: Just press F ten? [BNC: G4K 72-73]"}, {"heading": "Check Question", "text": "Checking questions are used to ask for explicit feedback of understanding or acceptance, which is usually expressed by the same speaker as the previous one. 7 (2.5) a: So (pause) may I record you.Okay? b: Yes. [BNC: KSR 5-6]"}, {"heading": "Sluice", "text": "(2.6) a: They would not do it, b: Why? [BNC: H5H 202-203]"}, {"heading": "Filler", "text": "(2.7) a: [...] would include satellites such as ermb: Northallerton. [BNC: H5D 78-79]"}, {"heading": "Short Answer", "text": "The NSUs, which are typically answers to wh questions. (2,8) a: What is plus three times plus three? b: Nine. [BNC: J91 172-173]"}, {"heading": "Plain Affirmative Answer and Plain Rejection", "text": "(2: 9) a: Have you settled in? b: Yes, thank you. [BNC: JSN 36-37] (2: 10) a: (pause) Right, are we ready? b: No, not yet. [BNC: JK8 137-138] 8"}, {"heading": "Repeated Affirmative Answer", "text": "NSUs used to give an affirmative answer by repeating or reformulating part of the query. (2.11) a: You were the first blind person to serve on the district council? b: On the district council, yes. [BNC: HDM 19-20]"}, {"heading": "Helpful Rejection", "text": "(2.12) a: Right disk number four? b: Three. [BNC: H61 10-11]"}, {"heading": "Propositional and Factual Modifiers", "text": "(2.13) a: Oh could you hear it? b: Occasionally yes. [BNC: J8D 14-15] (2.14) a: You would be there at six o'clock. b: Wonderful. [BNC: J40 164-165]"}, {"heading": "Bare Modifier Phrase", "text": "(2.15) a: [...] then opposite from there to this. b: From side to side. [BNC: HDH 377-378]"}, {"heading": "Conjunct", "text": "A conjunction is a modifier that extends an earlier utterance by a conjunction. (2.16) a: I am writing a letter to Chrisb: And other people. [BNC: G4K 19-20] 9"}, {"heading": "NSU Class Total %", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "2.1.2 The NSU corpus", "text": "The taxonomy presented in the previous section is the result of a corpus study of part of the dialog protocols used in the Corpus study, which include both two-party and multi-party conversations; the transcripts cover a variety of dialog areas, including free conversation, interviews, seminars, and more. Ferna \u0301 ndez (2006) also describes the annotation process and a reliability test; the reliability test was performed on a subset of commented instances, in which the manual annotation of three NSotators was compared; the test showed good agreement between the annotators with a kappa score of 0.76; this test also shows that humans can reliably distinguish between the annotators \"annotations in the taxonomy. Ferna \u0301 ndez (2006) provides more details on the complete analysis of the corpus.A total of 14,000 sentences were examined by us, 1 of whom 99 judgments were defined in the taxonomy."}, {"heading": "2.1.3 Interpretation of NSUs", "text": "The first step in the interpretation of an NSU is its classification, i.e. the determination of its class according to the taxonomy described in Section 2.1.1. As shown in Ferna \u0301 ndez et al. (2007), we can derive the class of an NSU by machine learning, i.e. we can train a classifier on the basis of the body detailed in Section 2.1.2 and use it to classify invisible NSU instances. NSU type is used to determine the correct solution method. Dissolving an NSU is the task of extracting the complete meaning of a dialogue from its incomplete form."}, {"heading": "2.2 A formal model of dialogue", "text": "As a theoretical basis for our work, we rely on the theory of the dialogue context raised by Ginzburg (2012), which represents a grammatical framework explicitly developed for dialogue. Ginzburg's claim (2012) is that the rules encoding the dynamics of dialogue must be built into the grammar itself; the grammatical framework is formulated using the type theory with notes (Cooper, 2005); type theory with notes (TTR) is a logical formalism developed to cope with the semantics of natural language. TTR is used to build a semantic ontology of abstract units and events, as well as to formalize the dialogue ameboard, i.e. a formal representation of the dialogue context and its rules. The development of the conversation is formalized by updated rules of the dialogue context. Ginzburg (2012) also takes NSUs into account and offers a set of dedicated rules."}, {"heading": "2.2.1 Type Theory with Records", "text": "We will now briefly introduce the basic concepts of type theory with records (TTR), where the statement x: T is a typical judgment indicating that the object x is of type T. If x is of type T, x should be a witness of T. Types can be either fundamental (atomic) like IND1 or complex, i.e. dependent on other objects or types like drive (x, y). Types also include constructs like lists, records, and so on. Other useful constructs are records and record types. A record contains a series of mappings between labels and values, whereas a record type contains a series of judgments between labels and types: r: l1 = v1 l2 = vn: l2.. ln: Tn: T2."}, {"heading": "Utterance representation", "text": "At the base of the grammatical framework of Ginzburg (2012) lies the term proposition = clearly entities are used to represent facts, events and situations as well as to characterize the communicative process. In TTR Propositions, records of the type are: Prop = sit: Record sit-type: Record A simple example of proposition can be the following: Paul drives a car. sit = r1 sit-type = x: IND p1: named (x, Paul) y: IND: drive (x, y) 1The type IND stands for a generic \"individual.\" 12On the other hand, questions are translated as propositional abstracts i.e. functions from the question domain q to propositions. Following the definition of Fernandez (2006): Question = q \u2192 Propaganda-Domain-q is the question-domain-domain-q."}, {"heading": "2.2.2 The dialogue context", "text": "In Ginzburg (2012), the dialogue context - also known as the Dialoggameboard (DGB) - is a formal presentation that describes the current state of the dialogue. It includes a wide range of variables necessary to handle various aspects of the dialogue. However, we focused on the most basic: \u2022 facts, a number of known facts; \u2022 LatestMove, the latest step in the dialogue; \u2022 QUD, a partially ordered catalogue of questions that is currently being debated.The DGB can be presented in TTR as a protocol as follows: Facts: Set (Prop) LatestMove: IllocPropQUD: poset (Question) The elements in the DGB represent the commonalities of the conversation that are shared between all participants. In this presentation, we have outlined some details that would be contained in the actual DGB presented by Ginzburg (2012), how to track the fields, the current time, etc. We will now go into the basic variables of the DGB."}, {"heading": "Facts", "text": "Facts are a set of known facts that are shared by all participants in the conversation. Elements of facts are theses that are believed to be sufficient to encode the participants \"knowledge in the context of the dialogue. Facts encode all records that are accepted by all participants, i.e. facts that will not raise any problems in the future development of the conversation. A supplementary problem that we address marginally is the understanding - or grounding - of a sentence. Ginzburg (2012) develops a comprehensive theory of grounding, but we do not include it in our work."}, {"heading": "LatestMove", "text": "Dialogue statements consist of coherent responses to previous statements, so it is important to keep an eye on the history of the dialogue. In a two-party dialogue, it is usually the case that the current statement is a response to previous statements, instead in a multi-party dialogue it can be useful to keep an eye on a larger window of dialogue history. Ginzburg (2012) tracks the history of dialogue within the Moves variables, while a reference to the most recent (illocutionary) proposal is recorded in the LatestMove.14 field."}, {"heading": "QUD", "text": "QUD is a set of questions that will be discussed. In a general sense, a \"question that will be discussed\" is a question that will be raised in the conversation and that will drive the future discussion. Despite the name, QUDs can arise from both questions and motions. Ginzburg (2012) defines QUD as a partially ordered set (poset), the order of which determines the priority of the questions to be solved. Of particular importance is the first element in the set according to the defined order that will be taken as the topic of discussion of subsequent statements until their dissolution. Such an element is referred to as MaxQUD. Formalizing the order is a fairly complex matter in a general theory of context that must take into account the beliefs of the participants, and it is particularly problematic when it comes to multi-party dialogues. In our case, the use of QUD is of particular importance because the MaxQUD is used as a precursor in the interpretation of NSUs."}, {"heading": "2.2.3 Update rules", "text": "The green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green."}, {"heading": "2.3 Probabilistic modeling of dialogue", "text": "Another possible approach to dialog modeling is based on probabilistic models for coding the variables and dynamics of the dialog context. This approach may well be considered more robust than the intrinsic randomness present in the dialog, which is partly why we have examined this strategy and other advantages described in Chapter 4.We base our work on the probabilistic rule formalism developed by Lison (2012). This formalism is particularly suited for our purpose because of its similarities with the updating rules described in Chapter 2.2.3. Probabilistic rule formalism is based on the representation of the dialog state as a Bajesian network. In this section, we briefly describe how Bajesian networks are structured, and then we detail the probabilistic rule formalism we apply in Chapter 4 to model the dissolution of NSUs."}, {"heading": "2.3.1 Bayesian Networks", "text": "Bayesian networks are probabilistic graphical models6, which represent a set of random variables (nodes) and their conditional dependencies (edges).A Bayesian network is a directed acyclic graph, i.e. a direct graph that does not contain cycles (two random variables cannot be dependent on each other).Given the random variables X1,..., Xn in a Bayesian network, we are interested in the common probability distribution P (X1,., Xn) of these variables. Generally, the size of the common distribution is exponential in the number n of variables, so it is difficult to estimate when n such variables will grow. In the case of Bayesian networks, we can use conditional independence to reduce the complexity of the common distribution. Given three random variables X, Y and Z, X and Y, X and Y, X and Y, Xyesian variables are given conditionally independently."}, {"heading": "2.3.2 Probabilistic rules", "text": "This year, it has reached the stage where it will be able to take the lead."}, {"heading": "2.4 Summary", "text": "In this chapter, we discussed the background knowledge needed to describe our work on non-sentence statements. First, we described the concept of non-sentence statements and the problem of their interpretation. We showed how these statements can be categorized using a taxonomy from Ferna \u0301 ndez and Ginzburg (2002). In Chapter 3, we described how to approach the interpretation of non-sentence statements by first classifying them according to the aforementioned taxonomy and then applying a kind of \"solution procedure\" to extract their meaning from the context of the dialogue. In Chapter 3, we deal with the problem of NSU classification based on the experiments from Ferna \u0301 ndez et al. (2007). Chapter 4, instead, we deal with the task of the NSU resolution. Ferna \u0301 ndez (2006) describes a series of solution rules rooted in a TTR representation of the dialogue context. Section 2.2 briefly describes the TTR terms we use and the dialogue theory based on the GTTR context (2012)."}, {"heading": "Classification of Non-Sentential", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "Utterances", "text": "As described in Section 2.1, NSUs come in a variety of forms that need to be treated differently from each other. To this end, the most basic (and perhaps most useful) task is to classify them. In our work, we use the taxonomy and corpus described in Sections 2.1.1 and 2.1.2. As Ferna \u0301 ndez et al. (2007) has shown, we can use machine learning techniques to automatically classify a given NSU using the annotated corpus as training data. Ferna \u0301 ndez et al. (2007) is our most important theoretical reference and (to our knowledge) state of the art in performance for the task of classifying NSUs. First, we replicated the approach of the above work and used it as a benchmark for improving the results, which we then apply to extending the data sets by using the secondary data described."}, {"heading": "3.1 The data", "text": "The corpus from Ferna \u0301 ndez et al. (2007) contains 1,283 commented NSU cases, each identified by the name of the contained BNC file and its record number, a sequential number for the unambiguous identification of a record in a dialogue transcript. Furthermore, the instances are provided with the record number of their predecessor, which forms the context for the classification, and the raw statements can be retrieved from the BNC using this information. For the classification task, we make the same simplistic limitation of the corpus that Ferna \u0301 ndez et al. (2007) made, namely that only the NSUs whose precursor is their previous sentence are taken into account. This assumption facilitates the feature extraction process without significantly reducing the size of the record (approximately 12% of the total). The resulting distribution of NSUs after the restriction is shown in Table 3.1.21."}, {"heading": "NSU class Total", "text": "As can be seen from Table 3.1, the distribution of instances is quite skewed, largely in favor of some classes over others. In addition, very common classes are generally the easiest to classify, with the most difficult with a few instances serving as examples for the classifiers. Although the scarcity of training materials and the imbalance of classes are the two main problems for the classification task, we propose a number of methods to address them, as described in the following sections."}, {"heading": "The British National Corpus", "text": "The British National Corpus (Burnard, 2000) - BNC for short - is a collection of spoken and written material containing approximately 100 million words of (British) English texts from a wide variety of sources, including a vast selection of dialogue transcripts covering a wide range of areas. Each dialogue transcript in the BNC is contained in an XML file, along with many details of the dialogue settings. The dialogues are structured according to the CLAWS tagging system (Garside, 1993), which segments the utterances at both word and sentence level. Word units contain both the raw text, the associated lemmas (keyword), and the POS tag according to the C5 tagset (Leech et al., 1994). Each sentence is identified by a unique ID number within the file. Sentences may also contain information about the pauses and the ambiguities. Sentries are sorted in order of appearance and contain additional information about the alignment of the lines in the case."}, {"heading": "3.2 Machine learning algorithms", "text": "We use two different monitored learning algorithms: Decision Trees and Support Vector Machines. The former serve mainly as a comparison to Ferna \u0301 ndez et al. (2007), who also use this algorithm. We implemented a coordinate ascension algorithm for the tuning of parameters. As a framework for our experiments we use the Weka toolkit (Hall et al., 2009), a Java library that includes the implementation of many machine learning algorithms as well as a universal API for machine learning."}, {"heading": "3.2.1 Classification: Decision Trees", "text": "We use the C4.5 agglorithm (Quinlan, 1993) for learning decision trees. Weka includes an implementation of this algorithm called J48. The goal of learning decision trees is to create a predictive model from the training data. The construction of the decision tree is done by dividing the training set into subsets according to the values of an attribute. This process is then repeated recursively on each subset. The construction algorithm is usually an informed search using some kind of heuristics to advance the selection of the slit attribute. In the case of C4.5, the metric used to select the attribute is the expected information gain. The information gain is based on the concept of entropy (Shannon, 1948) is the expected value of the information transmitted by a message (or an event in general). It is also a measure of the \"unpredictability\" of an event. The more unforeseeable an event is, the more leisurely the information (if an event occurs)."}, {"heading": "3.2.2 Classification: Support Vector Machines", "text": "The SVM is a binary classifier that uses a representation of the instances as points in a m-dimensional space, where m is the number of attributes. Assuming that the instances of the two classes are linearly separable.1 The aim of the SVMs is to find a hyperplane that separates the classes with the maximum margin. The task of finding the best hyperplane that separates the classes is defined as an optimization problem. the SVMs can also be formulated to have \"soft margins,\" i.e. that some points of a class are located on the opposite side of the hyperplane to find a better solution.The SVM algorithm we use regulates the model by a single parameter C.The SVMs can also use non-linear (i.e. non-linearly separable) data that can be separated using the so-called kernel method.The core function maps the points from the space into the popular spatial classification by a single parameter C.The SVMs can also be separated by means of the SVOs.The SVOs can only be separated using the SVOs."}, {"heading": "3.2.3 Optimization: Coordinate ascent", "text": "The Tuning parameter of all our experiments is automatically performed by a simple coordinate ascent2 optimization algorithm; coordinate ascent is based on the idea of maximizing a multivariable function f (X) along one direction at a time, such as gradient descent following the direction given by the gradient of the function. Our implementation detects the ascent direction by looking up the function value. Algorithm 1 contains a method to maximize a function f along the direction, while algorithm 2 performs the coordinate ascent. Step size values decay at a rate given by the coefficient alpha. Minimal step sizes determine the stop conditions for the maximization function, instead, the coordinateAscent algorithm stops as soon as the found values do not change between two iterations, the latter algorithm can be easily modified to take into account a state given by a maximum number of iterations."}, {"heading": "3.3 The baseline feature set", "text": "Our starting point should be the replicated approach of the classification experiments by Ferna \u0301 ndez et al. (2007), which are our reference work for our study. It contains two experiments, one with a limited number of classes (disregarding acknowledgments and check questions) and a second taking into account all classes. The latter is of interest to us, although the former is useful for understanding the problem and analyzing the results of our classifier. The paper above also includes an analysis of the results and the feature contribution that proved useful in replicating the experiments. To our starting point, we use only the features they describe. The feature set consists of 9 features that exploit a series of syntactical and lexical properties of the NSUs and their predecessors. The features can be categorized as: NSU Features, Antecedent Features, Similarity Features. Table 3.2 provides an overview of the feature set of the NSUs and their predecessors."}, {"heading": "NSU features", "text": "The following is a group of characteristics that exploit their syntactical and lexical characteristics. nsu cont Denotes the \"content\" of the NSU, i.e. whether it is a question or a sentence. This is useful to distinguish between questions that denote classes, such as clarification ellipsis and locks, and the rest.wh nsu Denotes whether the NSU contains a wh word, namely: what, who, where, when, how. This can help, for example, to distinguish distances between locks and clarification lips, the former being wh questions, while the latter do not exist.aff neg Denotes the presence of a yes word, a no word or a file word in the NSU. Yes words are for example: yes, yes, aye; no words are for example: no, not, nay; ack words are: correct, aha, mhm. This is particularly necessary in order to distinguish between the affirals."}, {"heading": "Antecedent features", "text": "As far as NSUs are concerned, precursors also have different syntactical and lexical characteristics that can be used as characteristics for the classification task. This is a group of characteristics that exploit these characteristics. Antenna tuning As defined by Rodr'\u0131guez and Schlangen (2004), this characteristic should be distinguished between declarative and non-declarative precursor sentences. This characteristic is useful to indicate the presence of an answer NSU when the precursor is a question or a modification when the precursor is not a question. wh ant As a corresponding characteristic of NSU, this indicates the presence of a Wh word in the precursor. Usually, short answers are answers to Wh questions, while affirmative answers and rejections are answers to polar questions, i.e. yes / no questions without a Wh interrogation."}, {"heading": "Similarity features", "text": "As described in Section 2.1, some classes exhibit a kind of parallelism between the NSU and its predecessor, and the parallelism of certain classes may be partially determined by similarity measures. In the following, a group of characteristics is described that encode the similarity at the word and POS level between the NSUs and their ancestors.repeat This feature counts the substantive words that the NSU and the predecessor have in common (a maximum value of 3 is taken as simplification).A value greater than 0 is usually a sign of repeated confirmation or affirmative replies. In parallel, this feature encodes whether there is a common sequence of POS tags between the NSU and the predecessor and indicates its length. This feature can help classify repeated confirmations, repeated affirmative replies and helpful rejections."}, {"heading": "3.4 Feature engineering", "text": "The first and simplest method we use to solve the classification problem is to find more features to describe the NSU cases. Here, we present the combination of features we use as our definitive approach. The extended feature set consists of all the basic features plus 23 new linguistic features and adds up to a total of 32 features. Our features can be grouped into five groups: features at POS level, features at phrase level, dependency features, turn-taking features and similarity features. Table 3.3 shows an overview of the additional features we use in the extended feature set."}, {"heading": "POS-level features", "text": "Superficial syntactical properties of NSUs that use the information already available in the BNC, such as POS tags and other markings.pos {1,2,3,4} A feature for each of the first four POS tags in the NSU. If an NSU is shorter than four words, no value is assigned to each missing POS tag. Many NSU classes share (shallow) syntactical patterns among their instances, especially at the beginning of the NSU sentence. These features aim to capture these patterns in a flat way through the POS tag."}, {"heading": "Phrase-level features", "text": "These features were extracted from the utterances using the Stanford PCFG parser (Klein and Manning, 2003). See Marcus et al. (1993) for more information on the tag used for English grammar. {sq, sbarq, sinv} These features indicate the presence of the syntactic tags SQ, SBARQ and SINV in the prehistory. These tags refer to a question formulated in different ways, even if there is no explicit question mark at the end. Useful, for example, to detect short reports. nsu marks the first sentence of the first sentence (S, SQ, SBAR,...) in the NSU.nsu first phrase marks the first sentence of the phrase level (NP, VP, ADJP,...) the first sentence of the sentence (S, SBAR,...)."}, {"heading": "Dependency features", "text": "These features were extracted by using the Stanford Dependency Parser (Chen and Manning, 2014) on the pronouncements. For more details on the dependency relationship, see De Marneffe et al. (2014).ant neg Signals the presence of a neg dependency relationship in the precursor. The neg dependency results from an adverbial negation in the sentence (not, don't, never,...) This feature helps to grasp situations like: (3.2) a: You don't get any funny fits from it at all, June? b: Er no. [BNC: H4P 36-37] 30Since the question in the precursor is negative, the NSU in (3.2) is actually an affirmative answer (although it contains a negative word, June? b: Er no. [BNC: H4P 36-37]."}, {"heading": "Turn-taking features", "text": "Characteristics that indicate certain patterns in the course of the dialogue, the same one that indicates whether the NSU and the predecessor were uttered by the same speaker. Sometimes, dialogues do not provide the speaker with information, so that in these cases an added value is created. This characteristic is particularly important in order to capture check questions that are almost always uttered by the same speaker."}, {"heading": "Similarity features", "text": "Additional numerical features and measures of similarity between the NSU and its antecedents. Last Repeat This measures the number of words common between the NSU and the last part of the antecedent.abs len The total number of words in the NSU.cont len The number of content words in the NSU.local all A feature that indicates the local character-level alignment between the NSU and its antecedents, calculated using the Smith-Waterman algorithm (Smith and Waterman, 1981).31lcs A feature that expresses the longest common subsequence at the word level between the NSU and its antecedents, calculated using a modified version of the Needleman-Wish algorithm (Needleman and Wish, 1970), tailored to words instead of anteceds. lcs pos The longest common subsequence at the POS level between the NSU and the same POS algorithm, but calculated with its antecedents."}, {"heading": "3.5 Semi-Supervised Learning", "text": "Although the quality of the data is good enough, it is still difficult for a classifier to learn patterns from 20 instances or less for some classes (see Table 3.1). However, there is a large amount of unlabeled data available in the BNC. There are many classification tasks, such as ours, where it is difficult or costly to label a large number of instances, while it is relatively cheap to extract unlabeled data. Empirically, the use of unlabeled data is useful to improve classification performance. Semi-supervised learning techniques deal with this problem, using the combination of a small amount of labeled data and a large amount of unlabeled data to improve classification accuracy. Although it is still a young field of research, semi-supervised learning has already found many applications (Liang, 2005; Bergsma, 2010)."}, {"heading": "3.5.1 Unlabeled data extraction", "text": "With the use of some locusts, it is possible to remove good quality instances from the BNC. We use a set of rules to determine whether a statement in a BNC transcript has a probable cause. \u2022 The number of words in the indictment must be higher than a certain threshold. \u2022 The indictment must contain not only pauses, but also unclear passages and interpretations."}, {"heading": "3.5.2 Semi-supervised learning techniques", "text": "As already mentioned, semi-supervised learning techniques are used when marked data is scarce and unlabelled data is abundant. Each technique attempts to integrate the information obtained through the unmarked instances into a learning model based on the available marked data. In this section, we give a brief and high-level description of the semi-supervised learning techniques we have used, namely: Self-training, Transductive SVM and Active Learning. 3A Repeated affirmative response, but the additional content after the conjunction makes the NSU much longer. It is still a valid NSU as it does not have a complete clauses structure. 4The NSU is a repeated recognition. If the words are repeated in the precursor, it introduces a verb. It is still considered NSU according to the definition of Ferna \u0301 ndez (2006). 33"}, {"heading": "Self Training", "text": "The easiest way to exploit unmarked data is to automatically predict some unmarked instances by a classifier, which is built from the available marked data, and then add them to the training data for the next step. It is an iterative process in which at each step one or more newly marked instances are added to the training set, then the classifier is retrained and more unmarked instances are predicted. At each step, different strategies can be used: \u2022 Add one or more (random) instances at the same time; \u2022 Add some particularly safe instances; \u2022 Add all the first time, correct the wrong predictions next time. The last strategy and other variants can be considered a problem of maximizing expectations, especially when using a probable learning model."}, {"heading": "Transductive SVM", "text": "As already described in Section 3.2.2, Support Vector Machines are one of the best studied and most reliable families of classification algorithms. Transductive SVM (TSVM) is a variant of the standard SVM algorithm that uses unmarked data to adjust the SVM model. The basic assumption under TSVM is that unmarked instances are separated from different classes at great distances. Therefore, as with the standard SVM, TSVM tries to find the hyperplane that maximizes the unmarked data span, i.e. considers blank points as marked points. To decide whether an unmarked point should be considered by one class or another, cluster techniques are used, e.g. k-next neighbors (the class of the majority of neighbors or another variant). We will not go into mathematical details, so we recommend the interested reader Vapnik (1998), Collobert et al. (2006)."}, {"heading": "Active Learning", "text": "This technique has the advantage that it reduces the cost of manual annotation of the instances by making informative guesses about the instances to the redundant ones. This type of technique is typically applied to cope with the scarcity of the labeled data. In our case, the lack of sufficient training data is particularly problematic due to the strong class imbalance between the NSU classes. 34The Active Learning (AL) scheme, which is a specific case of semi-supervised learning, the model then trains the user on the available labeled data for the label of one (or a few more) instances."}, {"heading": "3.6 Evaluation", "text": "In this section, we discuss the evaluation of our experiments and their empirical results. We first discuss the evaluation metrics for the classification task we have used, and then present the evaluation results for each setting."}, {"heading": "3.6.1 Metrics", "text": "In view of the data set with a total of N instances, the key figures are based on the amount of true positives (TP), real negatives (TN), false positives (FP) and false negatives (FN)."}, {"heading": "Accuracy", "text": "The ratio of correctly classified instances to totalAcc = \u2211 c \u0441C TPc + TNcNwhere C is the set of classes and TPc and TNc are the true positives or the true negatives of class c \u0441C."}, {"heading": "Precision", "text": "In a context with several classes (more than two) like ours, the precision per class must be calculated, where the positive instances are those classified with the current class, while the negative instances are the others. Precision per class is calculated as follows: Precc = TPcTPc + FPcTo have a summary value for all classes, we can calculate the weighted average precision: Precavg = \u2211 c \u0192C Nc \u00b7 PreccN36"}, {"heading": "Recall", "text": "The callback is the ratio between the true positives and the total instances that are actually positive. As for precision, we can calculate the value per class callback: Recc = TPcTPc + FNcAnd the weighted average callback: Recavg = \u2211 c-C Nc \u00b7 ReccNF1-ScoreThe F1-score is the harmonious mean of precision and callback. As for the other two measurements, we calculate the value per class callback: F1, c = 2 \u00b7 Recc Precc + Recc = 2 \u00b7 TPc2 \u00b7 TPc + FPc + FNcThen the weighted average of F1-Score: F1, avg = \u2211 c-C Nc F1, cNE-Score's t-testEmpirical results alone cannot judge whether one classifier performs better than another. To judge that the performances of a classifier are higher than zero-score data, cNE-Score's t-testEmpirical results alone cannot judge whether one classifier performs better than another. To judge that the performance of a classifier is manipulated by a difference between zero-score, the statistically the score is not due to the randomness, but rather the fact that the classifiability is manipulated."}, {"heading": "3.6.2 Empirical results", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "Baseline", "text": "As in Ferna \u0301 ndez et al. (2007), we evaluate our system for 10-fold cross-validation. Weka's J48 algorithm served as a comparative classifier, and by analyzing the resulting trees, we were able to mimic the behavior of their system fairly accurately, achieving a very narrow overall performance. Although we use the same functionality and algorithm, the performance parameters turn out to be slightly lower than those claimed in Ferna \u0301 ndez et al. (2007), for a variety of reasons, such as how the features were extracted or how the parameters were tuned. Nevertheless, the overall performance and many of the patterns in the scores are matched. Table 3.4 shows the comparison between the performance parameters of the reference classification (Ferna \u0301 ndez et al., 2007) and the values of the same parameters achieved through our implementation."}, {"heading": "Self-training and TSVM", "text": "Both techniques did not perform particularly well and sometimes even impaired classification accuracy. Self-training has been implemented and tested in many variants, but none has been successful. One possible explanation is that the labeled data that is added to the training data at each step is always distorted by the labeled data available in the first training set, which can cause redundant data to be added that is not really useful to improve classification performance. On the other hand, TSVM has been unsuccessful mainly due to the processing power of the implementation and other technical difficulties."}, {"heading": "Active Learning", "text": "Our active learning experiment was performed with the JCLAL library. For the active learning process, we divided the data set into three parts: training set (50%), development set (25%) and test set (25%). At each iteration, the JCLAL library builds a classifier on the training set and evaluates it over the development set. The same classifier is then used to select an instance from the blank data as described in Section 3.5.2. The user is then asked to comment on the selected instance. In this way, the process iterates until the stop criteria are met, i.e. when the goal of 100 newly described instances is reached. Table 3.5 shows the distribution of the instances commented with active learning. From Table 3.5, we can see that the AL algorithm that uses entropy prefers the instances that belong to the classes that are the most difficult to classify, and in particular the instances that are not unique."}, {"heading": "NSU Class Instances", "text": "In fact, it is the case that most people who are able are able to determine for themselves what they want and what they do not want."}, {"heading": "3.7 Summary", "text": "This task is formulated as a problem of machine learning, and we follow and extend the work of Ferna \u0301 ndez et al. (2007). We use its corpus as the gold standard and a replica of its approach as the baseline. The data, the algorithm of machine learning used and the scope of the baseline were discussed in Section 3.1, 3.2, 3.3 respectively. The two main problems we faced in our work were the scarcity of the data referred to and the imbalance in the distribution of classes. In order to address these problems, we expanded the baseline approach in two ways: the use of a larger range of functions (detailed in Section 3.4) and the use of semi-supervised learning techniques to exploit the abundance of undescribed data. In Section 3.5, we described the semi-supervised learning techniques we used, namely: Self Training, Transductive SVM and Active Learning. Section 3.6 shows the empirical results that we have not significantly improved through the combination of 45 years alone."}, {"heading": "Resolution of Non-Sentential", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "Utterances", "text": "As introduced in Chapter 2, the task of resolving an NSU is to reconstruct its meaning from the context of dialogue (Lisburg). Fernandez (2006) proposes a set of rules for solving NSUs on the basis of TTR, the logical framework of Cooper (2004), which was then further developed in Ginzburg (2012). However, one limitation of logical frameworks such as TTR is their inability to directly represent (and reason for) uncertain knowledge. Furthermore, many dialogue domains contain variables that are only partially observable and, as Ginzburg argues (2012), we must take into account a certain degree of stochastic behavior when modelling the dialogue, since we still have an imperfect understanding of its dynamics. The stochastic component is particularly important in dealing with NSUs, as it does not have a precise meaning for itself and, as Ginzburg argues (2012), they are in principle highly ambiguous. Therefore, we propose a new approach to resolving NSUs that finds a probable consideration of the involved and applied variables."}, {"heading": "4.1 The resolution task", "text": "The resolution of an NSU is the task of extracting its meaning from the context of the dialog. Specifically, let, inter alia, and nsua represent the sequence of words that make up the NSU, and its type according to the taxonomy presented in Section 2.1.1. We also assume that MaxQUD is a high-level semantic representation of the predecessor, as mentioned in Section 2.2.2. By means of a resolution method, we want to extract aa, i.e. the high-level semantic representation of the NSU. The correct resolution method is selected on the basis of the type of NSU. In our case, the value of nsua is determined using the classifier developed in Chapter 3, which takes the raw NSU and the precursor as input. Figure 4.1 shows a scheme of the task just defined. In fact, this is the easiest way to define the task. The resolution method may also depend on other variables in the state of the dialog, such as the facts. In principle, the resolution task is defined independently of the task actually defined."}, {"heading": "4.2 Theoretical foundation", "text": "As already mentioned, we rely on Ferna \u0301 ndez (2006) and Ginzburg (2012) for the theoretical concepts needed to represent the state of dialogue and to develop the rules of resolution for the NSU. In Section 2.2, we explain the basic concepts of the TTR, the statement and the rules of updating for the state of dialogue. In this section, we describe the concepts needed to dissolve the NSUs. In particular, we describe how to exploit the parallelism between the NSU and its predecessors, which we have mentioned in Section 2.1. Here, we discuss the concepts defined by Ginzburg (2012) to address the dissolution of the NSUs, and describe in the next section how to adapt these concepts to our needs."}, {"heading": "4.2.1 Partial Parallelism", "text": "Instances of NSU classes such as Acknowledgment and Affirmative Answers are related to their ancestor as a whole, i.e. they understand their meaning in such a way that they do not consider a specific aspect of the predecessor, but the entire sentence. On the other hand, there are NSU classes such as Short Answers and Sluices, which exhibit a finer-grained parallelism between their instances and their ancestors, i.e. they can refer in particular to certain aspects of the predecessor. In the theory of Ginzburg (2012), this concept is called Partial Parallelism2. Partial Parallelism is a way to categorize NSU classes according to the relationship to their ancestors. NSU classes are categorized as + / -ParPar in order to find the right way to treat them. An NSU class categorized as + ParPar involves access to one or more partial explanations from their ancestor ancestors. On the contrary, Par- Par classes of NSU."}, {"heading": "4.2.2 Propositional lexemes", "text": "-ParPar NSU classes are (mainly) realized by statement terms, i.e. words that stand alone and can form a sentence with full contextual meaning. These classes of NSU include simple affirmative answers, simple rejections, and statement modifiers, each of which is realized by the words yes, no, and adverbial as likely and possible. These classes of NSU arise from polar questions such as (4.1). (4.1) a: Will you go to the party on Saturday? b: yes. / no. / probably.The semantic content of these distinct lexemes can be modeled as a function R of the content of the previous polar question. For affirmative answers, R is the identity (Id) relationship, i.e. the function that the argument itself returns. This means that the positive answer \"yes\" to a polar question is equivalent to the assertion of a statement with the same content as the polarity."}, {"heading": "4.2.3 Focus Establishing Constituents", "text": "In order to explain the partial parallelism between the NSUs and their predecessors resulting from the instances of the + ParPar group classes, we must keep in mind the focal sub-expressions of the precursors, i.e. the elements of the QUD. Therefore, we use the concept of voter identification (FEC) from the theory of Ginzburg (2012) 3. The FECs are relevant components in the elements of the QUD that can be used to dissolve NSUs. Consider the following example: (4.3) a: A friend comes to the partie.b: Who? 3The concept was previously used by Ferna? ndez (2006) as a current constitution.50The noun phrase \"A friend\" in the first sentence of (4.3) is the one to which the following sluice refers: (the sluice) can be solved in a rough way: \"Who is your friend coming to the party?\" It is clear that the following sluice (the sluice) is available."}, {"heading": "4.2.4 Understanding and acceptance", "text": "The Plain Acknowledgments and Check Questions classes are used to handle understanding and acceptance in conversation. Plain Acknowledgments are used to send a direct feedback of understanding or acceptance of previous utterances. Understanding involves successfully grasping the content of a utterance, while acceptance is a sign of common faith, which therefore updates the facts with the accepted utterance and removes the corresponding problem from the QUD. As argued in Ferna'ndez (2006), understanding does not always imply acceptance, and Plain Ackledgments are ambiguous in this distinction. Despite this difference, we assume that Plain Acknowledgments are used to show acceptance, so the use of a Plain Ackledgments also diminishes QUD. On the other hand, understanding is assumed to be shown by any utterance that is not a clarification of ellipses. Check Questions are used in conversation to ask for an understanding of the prior feedback / acceptance of the expression."}, {"heading": "4.2.5 Sluicing", "text": "To formalize the meaning of locks, Ferna \u0301 ndez et al. (2007) distinguish four types of locks that convey different meanings: direct locks, recapitulation locks, clarification locks, whanaphores. The paper above describes a machine learning experiment to automatically classify locks by these types. Ginzburg (2012) describes several different treatments for each group of locks. In our work, we do not differentiate between these types of locks, but limit ourselves to the simplicity of managing locks only. Direct locks, such as those in (4.7), are used to ask the other speaker for additional information on one aspect of the history. (4.7) a: Can I please nudge something? b: What kind? [BNC: KCH 104-105]"}, {"heading": "4.3 Dialogue context design", "text": "The values of these random variables can represent practically anything, from crude utterances to their semantic representation. The variables in the dialogue context are inspired by Ginzburg (2012). To make the transition from Fern\u00e1ndez (2006) rules to probabilistic rules as direct as possible, we imitate the fundamental dynamics of the DGB, which is described in Section 2.2. For our semantics, we do not use TTR because it would make our formalization unnecessarily complex. In this section, we first describe the semantics we adopt, and then discuss the random variables that compose the dialogue context."}, {"heading": "4.3.1 Semantics", "text": "The semantic content of the utterance is represented by logical predicates, individuals and variables, such as IND for generic individuals or E for events. Variables are capitalized with X. Individuals and variables are uniquely identified by a numerical subscription. Predicates represent the high-level semantic meaning of the constituent parts of utterances. Intuitively, predicates without variables can represent statements such as (4,8) as an argument. As discussed in Section 2.2.1, polar questions and Wh questions can be considered functions of / for recording types. Polar questions take the empty recording type as an argument. 52 Following this scheme, in our formalism polar questions are referred to by predicates without variables, while wh questions are represented by predicates containing one or more variables as an example of the empty recording types."}, {"heading": "4.3.2 Dialogue acts", "text": "To represent the \"purpose\" of a statement, we must, as described in Section 2.2.1, use an illocutionary relationship, also known as the act of dialogue; the series of acts of dialogue that we use in our context of dialogue is a small subset of those defined by Ginzburg (2012): \u2022 Asserting what signifies the act of asserting a sentence; \u2022 Asking what signifies the act of asking a question; \u2022 Earthing, which signifies the act of understanding what was said before; \u2022 Accepting what signifies the act of accepting what was said before. Assertions are applied to statements and are implicitly considered truthful unless they violate some predicates in the facts. Asking questions is the act of asking questions, and they are stacked up in the QUD until they are resolved by an answer. The act of answering a question corresponds, in the case of a question, to the predicate of the question."}, {"heading": "4.3.3 Variables of the dialogue context", "text": "In fact, the fact is that most of them will be able to be in a position to be in what they are in."}, {"heading": "4.4 NSU resolution rules", "text": "For each rule, we present an example of use. As they are (almost) direct translations of the deterministic rules from Ferna \u0301 ndez (2006), most of them have deterministic effects (i.e., a single effect with probability 1). Nevertheless, the updates are probabilistically handled by the probabilistic rulebook by probabilistic conclusions via the Bayesian network that represents the dialog state. An example of probabilistic updates is shown in Section 4.4.1, which applies to every other resolution rule."}, {"heading": "4.4.1 Acknowledgments", "text": "As explained in Section 4.2.4, we assume that explicit recognition is a sign of acceptance of the last problem discussed. In order for repeated recognition, Ferna \u0301 ndez (2006) must have a co-referentiality between the repeated constituent in NSU and the relative constituent in MaxQUD's FEC. We have decided to drop this requirement, assuming that the co-reference is always present when the classifier assigns the class RepAck to the current NSU. This assumption does not affect the system, since the effect on state variables is the same for both Acks and RepAcks. The rule for acknowledgement is the following 5: ack: if (nsua = Ack, nsua = RepAck, nsua) the probability is that the variables are with the same probability."}, {"heading": "4.4.2 Affirmative Answers", "text": "The context for an affirmative answer contains a polar question q (y) as MaxQUD. As for the receipts, the requirement for co-referencing between the repeated component of the RepAffAns and the same component in the FECs of the MaxQUD element is omitted. The actual distribution would not necessarily assign any as an alternative value, since other rules can be triggered by the other values of nsua.56An affirmative answer to a polar question corresponds to the assertion of the same semantic content (predicate) of the question. Following, the rule for affirmative answers 7,8: affAns: \u0439q, y if ((nsua = AffAns = RepAffAns), qud = qud [max-qud].q = q = q (y) thenP aa \u2190 Assert (q (y)), new-fec qud qud qud [max-qud].c = an example of the application of the rule (xmad) (x11)."}, {"heading": "4.4.3 Rejections", "text": "As for the affirmative answer, the context of rejection is a polar question q (y), but, as explained in Section 4.2.2, we must distinguish the cases in which q is positive or negative. We will define the following function Neg, which indicates the negation of a sentence p (or equivalent a question): Neg (p) = p, if p is positive, if p is negative, where p is negative of p. As an extension of the above notation, we point to an explicitly negative statement as p. 7As convention, quantified variables and quantified individuals are each given as x and y in the rule definitions. Vectors of variables or individuals are each given as x and y.8As in this case, a probable effect can contain multiple assignments."}, {"heading": "4.4.4 Propositional Modifiers", "text": "In fact, it is a reactionary, reactionary, reactionary and reactionary party that is able to position itself against the reactionary, reactionary, reactionary and reactionary left."}, {"heading": "4.4.5 Check Questions", "text": "As defined in Section 4.2.4, check questions are used to ask for understanding / acceptance of the last raised problem. In practice, this means asking the last asserted suggestion as a polar question. The following rule applies to dealing with this type of NSUs: checkQu: p, y if (nsua = CheckQu-qud [max-qud].q = p (y) thenP aa \u2190 Ask (p (y), new-fec \u2190 qud [max-qud].fec = 1 An application example for the previous rule is: (4.14) a: I'm going to the partie.a: OK? The dialog context of (4.14) is: max-qud = 1qud [max-qud].q = goingToParty (IND1) qud [max-qud].fec = {} nsua = CheckQua = After applying the rule: aa = Asgok (ToingParty {Dc) {1)."}, {"heading": "4.4.6 Short Answers", "text": "As already mentioned, in this work we limit ourselves to simple wh questions, i.e. questions with only an unknown variable x. short answers are solved by applying them to the MaxQUD whinterrogative and then asserting the resulting sentence. Applying the Short Answer is by replacing all occurrences of the variable x with ua (or equivalent a high-level representation of it).The following is the rule for the Short Answers: shortAns: q, x, y, pi, yi if (nsua = ShortAns, qud [max-qud].q = q (x, y)."}, {"heading": "4.4.7 Sluices", "text": "As explained in Section 4.2.5, we limit ourselves to dealing with direct sluices. Even for this type of sluices, the resolution rules are many, as they must take into account the lexical meaning of each wh word. Furthermore, the meaning of wh words would be modified in many ways, e.g. \"how many,\" \"how long,\" \"who else,\" \"how.\" This would require detailed treatment for this type of NSUs, which we are not trying to elaborate. 61 Nevertheless, we will show some rules to deal with direct sluices like those mentioned in (4.16) and (4.16) b: A friend comes to the party. (4.17) b: Paul throws a party.a: When is the context of sluices a MaxQUD with at least one variable (raised by a wh question or a proposition with some undefined reference)."}, {"heading": "4.4.8 Clarification Ellipsis", "text": "In order to solve requests for clarification and their elliptical variants, Ginzburg (2012) contains a general theory of grounding and clarification requests. This theory would add a non-trivial degree of complexity to our formalization, so that we assume in our formalization that the last utterance is always grounded, unless a request for clarification occurs afterwards. Therefore, we resolve requests for clarification ellipsis to the MaxQUD element without adding any other structure to the dialogue state. We also consider the clarification ellipsis as just a small confirmation reading, apart from its other possible readings that would require a more detailed approach (further details in Ginzburg (2012). The small confirmation lecture = E = lecture = E = lecture party E = E = lecture party E = E = lecture party E = E = E = lecture party E = E = E = lecture party E = E = E lecture party E = E = E lecture party E = E."}, {"heading": "4.5 Implementation and use case example", "text": "In this section, we illustrate some possible applications of the rules for a conversation in the real world. It shows some examples of the behavior of the rules towards a selected transcript from the Communicator dataset. However, we would like to point out that this section does not require an empirical evaluation of the rules, which is anything but trivial, since such an evaluation (at least) requires the availability of a fully commented dataset of transcripts with the dialogue files and the context updates at each step. The Communicator dataset (Walker et al., 2001) is a set of transcripts of interactions between a dialogue system and human testers. The Communicator dataset contains transcripts of conversations for booking airline tickets. The interactions are mainly \"automated,\" meaning that the system drives the conversation, it asks questions, and the user answers only to these questions. In this scenario, it is possible to find many answers to NSUs, such as short answers, affirmative and affirmative responses."}, {"heading": "4.5.1 Dialogue system architecture", "text": "As defined in Lison (2014), \"a spoken dialogue system is a computational tool that can communicate with humans through the everyday spoken language.\" These systems have a complex structure that consists of many different parts, but is usually composed of the following main components: \u2022 Natural language understanding (NLU), maps the textual utterances into a high-level semantic representation; \u2022 Dialog management, updates the dialogue state and plans the actions to be performed; \u2022 Natural language generation (NLU), generates the linguistic implementation of the planned actions or dialog actions; The resolution of NSUs is closely related to the NLU task and dialogue management in the presence of such utterances. For their proper operation, our implementation does indeed include flat NLU and NLG modules as well as a very simple selection process. Figure 4.2 shows the workflow of the system. The system takes as input a generalized expression of the user's utterance."}, {"heading": "4.5.2 Example", "text": "In this section, we present an extended example of using some of the rules on the sample transcript (4,20) from the Communicator Dataset. The transcript (4,20) encrypts a conversation between a dialogue system and a user for the purpose of booking a plane ticket. (4,20) Communicator 1693: 041. m: What are your travel plans? 2. u: I'd like to travel from Columbus Ohio to Phoenix Arizona onThursday October 5th.3. m: What time would you like to leave Columbus? 4. u: Before 6 p.m. 5. m: Do you have a preferred airline? 6. u: No.7. m: I have you go from Columbus Ohio to Phoenix onOctober 5th. Is that correct? 9. m: Will you return to Columbus? 10. u: Is Phoenix your final destination? 12. u: Yes.13. m: Hold on while I check availability."}, {"heading": "4.6 Summary", "text": "In this chapter, we have explained how to solve the semantic meaning of NSUs. Resolution is a task aimed at extracting the meaning of an NSU in the context of dialogue. To do this, we relied on the previous work by Ferna \u0301 ndez (2006), which presented a set of rules to remove the meaning of NSUs from a TTR-encoded dialogue context. As argued during this work, the use of a purely logical formalism, such as TTR, has some disadvantages in dealing with partially observable inputs and stochastic events compared to a probabilistic approach. We have shown how to reformulate the rules from Ferna \u0301 ndez (2006) using probabilistic rule formalism (Lison, 2014) to include a probabilistic representation of the state of dialogue. We have used part of the dialogue context theory from Ginzburg (2012) to code the basic elements of the state of dialogue, see the section 2.Us required for the resolution."}, {"heading": "Conclusion", "text": "This chapter concludes the present thesis by summarizing the contributions of our work and indicating some possible developments that can be followed in future work."}, {"heading": "5.1 Contributions", "text": "In this thesis, we have described our research work in relation to undecided statements. NSUs are statements that do not have a complete sentence form but have a full meaning. However, they require that they be \"interpreted,\" i.e. their meaning must be taken out of the context of the dialogue. Our experiments concern two different aspects of the interpretation of NSUs, namely: the classification of NSUs according to their context; the dissolution of the semantic content of NSUs from the context of the dialogue; in Chapter 2, we presented the background knowledge necessary for the development of our work. In Section 2.1, we discussed the concept of undecided utterance referring to Ferna, which is referred to as our theoretical basis of Ferna."}, {"heading": "5.2 Future developments", "text": "In this section, we list a number of ideas for possible future work that arise directly from our findings and the assumptions we make."}, {"heading": "Improve the NSU classification performances", "text": "In our work on the classification of NSUs we experimented with many different approaches in the search for an improvement in classification accuracy. However, there are many other ways we do not explore or aspects in our approaches that can be improved. Here we discuss a few of the possible extensions of our work. There are classes of NSU that are inherently difficult to predict, such as Helpful Rejections and other pairs of classes that are difficult to distinguish, such as repeated comments and repeated affirmative responses. A common problem in trying to predict these classes is that parallelism with their ancestors is almost entirely on the semantic level. This requires a deeper understanding of the phenomena and the use of functions that exploit semantic relationships in the NSU instances. We have not used semantic features as they have added a non-trivial amount of complexity to our functions."}, {"heading": "Extend our NSU resolution approach", "text": "Our study on the resolution of NSUs is a pioneer of a rules-based approach that includes a probable dynamics of the state of dialogue. There are still many issues that need to be resolved: \u2022 Extending the scope of the rules to all classes not covered by our work: factual modifiers, helpful rejections, etc. \u2022 Developing an appropriate rule adjustment mechanism in the presence of lexical modifiers, e.g. for locks like \"For how long?\" \u2022 Consider grammatical and lexical resources to extract more complex meanings. A simple short answer that would not be covered by our rules is: (5.1) a: Who is coming tomorrow? b: Nobody. \u2022 Correct evaluation of the rules for checking data from different areas of dialogue. The last point, perhaps most important, would require the development of a corpus of dialogue transcripts that would be commented on with each semantic movement and state update."}, {"heading": "Combine different NSU resolution approaches", "text": "For our work on the NSU resolution, we are developing a rules-based approach based on a probabilistic representation of the dialogue state. It shows similarities with statistical approaches to the resolution such as Raghu et al. (2015). Their work focuses on NSU follow-up questions such as: (5.2) a: How much for this model? b:.. a: For this other? Their approach is based on the combination of key words from the follow-up question and the original one. From the combination of key words, they build up possible meaningful \"additions\" to NSU, e.g. an addition to the NSU in the third line in (5.2) would76be: \"How much for this other model?\" After generating every possible completion, they rank them according to any score and select the best ones. Unlike our approach, their approach does not use a high-level semantic representation of the utterances."}, {"heading": "Appendix A", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "Context update rules", "text": "In this appendix, we give an overview of the probabilistic rules used to update the context implemented in the dialog system for checking the resolution rules. As described in Section 4.5, the dialog system that we have implemented is focused on human-machine type conversations, so we will use the notations on and around to refer to each of the system's dialog acts and corresponding crude utterances. Following the above-mentioned interaction model, the dialog context represents only the pieces of information known by the system. Contextualization rules are necessary for the dialog context to evolve with the actions of the user and the relative system reactions. In particular, we need the rules for updating the QUD and the Facts variables. These rules are inspired by Ginzburg (2012), but not limited to them. Section 2.2 provides the background knowledge for the rules from Ginzburg (2012). The rules set out in this appendix are not intended to provide a comprehensive view of the system architecture, but rather to provide an overview of the system architecture."}, {"heading": "QUD increment", "text": "x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x"}, {"heading": "QUD downdate", "text": "The system responds with an accept act that removes MaxQUD from the QUD.qud downdate: if (am = Accept (p)) then P qud [max-qud].q \u2190 None, qud [max-qud].utt \u2190 None, qud [max-qud].fec \u2190 None, qud.size \u2190 qud.size \u2212 1 = 180"}, {"heading": "Max-qud update", "text": "As soon as the QUD array is updated, the MaxQUD is also updated. As explained in Section 4.3.3, the max-qud variable is defined as the MaxQUD index within the QUD array, and its stack-like behavior is determined by an exponentially decreasing probability with the maximum on the last element inserted."}, {"heading": "Facts increment", "text": "As mentioned above, the dialog context encodes the knowledge of the system and thus also the facts. The variable Facts contains only predicates accepted by the system. In the rule below, we integrate the predicates shown in Section 4.4.4 for statement modifiers and the predicates for dealing with rejections and affirmative answers. Again, we point out that the probabilities are handmade for the sake of simplicity, but can actually be learned from data.facts-increment: p, y if (am = accept (p) (y)) then {P (facts (p) (y))) then {P (facts (facts) = 0.25. Otherwise (am = accept (Neg (p) (y)) then {P (facts (Neg (y)) then {Neg (p (y)} (new) then {fec = 18y (otherwise) (new))."}], "references": [{"title": "Large-scale semi-supervised learning for natural language processing", "author": ["S. Bergsma"], "venue": "University of Alberta. Boser, B. E., I. M. Guyon, and V. N. Vapnik (1992). \u201cA training algorithm for optimal margin classifiers\u201d. In: Proceedings of the fifth annual workshop on Computational learning theory. ACM, pp. 144\u2013152.", "citeRegEx": "Bergsma,? 2010", "shortCiteRegEx": "Bergsma", "year": 2010}, {"title": "Reference guide for the British National Corpus (world edition)", "author": ["L. Burnard"], "venue": "Chawla, N. V. et al. (2002). \u201cSMOTE: synthetic minority over-sampling technique\u201d. In: Journal of artificial intelligence research, pp. 321\u2013357. Chen, D. and C. D. Manning (2014). \u201cA fast and accurate dependency parser using neural networks\u201d. In: Proceedings of the 2014 Conference on Empirical Methods in Natural", "citeRegEx": "Burnard,? 2000", "shortCiteRegEx": "Burnard", "year": 2000}, {"title": "Large scale transductive SVMs", "author": ["R Collobert"], "venue": "Language Processing (EMNLP)", "citeRegEx": "Collobert,? \\Q2006\\E", "shortCiteRegEx": "Collobert", "year": 2006}, {"title": "Records and record types in semantic theory", "author": ["R. Cooper"], "venue": "In: Journal of Logic and Computation 15.2, pp. 99\u2013112. De Marneffe, M.-C. et al. (2014). \u201cUniversal Stanford Dependencies: A cross-linguistic typology\u201d. In: Proceedings of LREC, pp. 4585\u20134592. Dragone, P. and P. Lison (2015a). \u201cAn Active Learning Approach to the Classification of", "citeRegEx": "Cooper,? 2005", "shortCiteRegEx": "Cooper", "year": 2005}, {"title": "Non-Sentential Utterances", "author": ["P. Dragone", "P. Lison"], "venue": "Proceedings of the second Italian Conference on Computational Linguistics. Trento, Italy,", "citeRegEx": "Dragone and Lison,? \\Q2015\\E", "shortCiteRegEx": "Dragone and Lison", "year": 2015}, {"title": "Which is the best multiclass SVM method? An empirical study", "author": ["Duan", "K.-B.", "S.S. Keerthi"], "venue": "In: Multiple Classifier Systems, pp. 278\u2013285.", "citeRegEx": "Duan et al\\.,? 2005", "shortCiteRegEx": "Duan et al\\.", "year": 2005}, {"title": "Non-sentential utterances in dialogue: A corpus", "author": ["R. Fern\u00e1ndez", "J. Ginzburg"], "venue": null, "citeRegEx": "Fern\u00e1ndez and Ginzburg,? \\Q2002\\E", "shortCiteRegEx": "Fern\u00e1ndez and Ginzburg", "year": 2002}, {"title": "Non-sentential utterances in dialogue: Classification, resolution", "author": ["R.R. Fern\u00e1ndez"], "venue": null, "citeRegEx": "Fern\u00e1ndez,? \\Q2006\\E", "shortCiteRegEx": "Fern\u00e1ndez", "year": 2006}, {"title": "The large-scale production of syntactically analysed corpora", "author": ["R. Garside"], "venue": "In:", "citeRegEx": "Garside,? 1993", "shortCiteRegEx": "Garside", "year": 1993}, {"title": "The interactive stance", "author": ["J. Ginzburg"], "venue": "Oxford University Press.", "citeRegEx": "Ginzburg,? 2012", "shortCiteRegEx": "Ginzburg", "year": 2012}, {"title": "Clarification, ellipsis, and the nature of contextual", "author": ["J. Ginzburg", "R. Cooper"], "venue": null, "citeRegEx": "Ginzburg and Cooper,? \\Q2004\\E", "shortCiteRegEx": "Ginzburg and Cooper", "year": 2004}, {"title": "Interrogative investigations", "author": ["J. Ginzburg", "I. Sag"], "venue": "Stanford: CSLI publications.", "citeRegEx": "Ginzburg and Sag,? 2000", "shortCiteRegEx": "Ginzburg and Sag", "year": 2000}, {"title": "SHARDS: Fragment resolution in dialogue", "author": ["J Ginzburg"], "venue": "In: Computing", "citeRegEx": "Ginzburg,? 2007", "shortCiteRegEx": "Ginzburg", "year": 2007}, {"title": "The WEKA data mining software: an update", "author": ["M Hall"], "venue": "In: ACM SIGKDD", "citeRegEx": "Hall,? 2009", "shortCiteRegEx": "Hall", "year": 2009}, {"title": "Accurate unlexicalized parsing", "author": ["D. Klein", "C.D. Manning"], "venue": "In: Proceedings of", "citeRegEx": "Klein and Manning,? 2003", "shortCiteRegEx": "Klein and Manning", "year": 2003}, {"title": "GoDiS: an accommodating dialogue system", "author": ["S Larsson"], "venue": "In: Proceedings of", "citeRegEx": "Larsson,? 2000", "shortCiteRegEx": "Larsson", "year": 2000}, {"title": "CLAWS4: the tagging of the British Na", "author": ["G. Leech", "R. Garside", "M. Bryant"], "venue": null, "citeRegEx": "Leech et al\\.,? \\Q1994\\E", "shortCiteRegEx": "Leech et al\\.", "year": 1994}, {"title": "Heterogeneous uncertainty sampling for supervised", "author": ["D.D. Lewis", "J. Catlett"], "venue": null, "citeRegEx": "Lewis and Catlett,? \\Q1994\\E", "shortCiteRegEx": "Lewis and Catlett", "year": 1994}, {"title": "Semi-supervised learning for natural language", "author": ["P. Liang"], "venue": "PhD thesis. Mas-", "citeRegEx": "Liang,? 2005", "shortCiteRegEx": "Liang", "year": 2005}, {"title": "A hybrid approach to dialogue management based on", "author": ["P. Lison"], "venue": null, "citeRegEx": "Lison,? \\Q2015\\E", "shortCiteRegEx": "Lison", "year": 2015}, {"title": "Declarative Design of Spoken Dialogue Systems with Probabilistic", "author": ["P. Lison"], "venue": null, "citeRegEx": "Lison,? \\Q2012\\E", "shortCiteRegEx": "Lison", "year": 2012}, {"title": "Structured Probabilistic Modelling for Dialogue Management", "author": ["P. Lison"], "venue": "PhD", "citeRegEx": "Lison,? 2014", "shortCiteRegEx": "Lison", "year": 2014}, {"title": "Developing Spoken Dialogue Systems with the Open", "author": ["P. Lison", "C. Kennington"], "venue": null, "citeRegEx": "Lison and Kennington,? \\Q2015\\E", "shortCiteRegEx": "Lison and Kennington", "year": 2015}, {"title": "Anaphora resolution", "author": ["R. Mitkov"], "venue": "Routledge.", "citeRegEx": "Mitkov,? 2014", "shortCiteRegEx": "Mitkov", "year": 2014}, {"title": "A survey of named entity recognition and classification", "author": ["D. Nadeau", "S. Sekine"], "venue": null, "citeRegEx": "Nadeau and Sekine,? \\Q2007\\E", "shortCiteRegEx": "Nadeau and Sekine", "year": 2007}, {"title": "A general method applicable to the search", "author": ["S.B. Needleman", "C.D. Wunsch"], "venue": null, "citeRegEx": "Needleman and Wunsch,? \\Q1970\\E", "shortCiteRegEx": "Needleman and Wunsch", "year": 1970}, {"title": "Fast training of support vector machines using sequential minimal", "author": ["J Platt"], "venue": null, "citeRegEx": "Platt,? \\Q1999\\E", "shortCiteRegEx": "Platt", "year": 1999}, {"title": "CLARIE: Handling Clarification Requests in Dialogue System", "author": ["M. Purver"], "venue": "In:", "citeRegEx": "Purver,? 2006", "shortCiteRegEx": "Purver", "year": 2006}, {"title": "R: A Language and Environment for Statistical Computing", "author": ["R Core Team"], "venue": "R Foun-", "citeRegEx": "Team,? 2015", "shortCiteRegEx": "Team", "year": 2015}, {"title": "A Statistical Approach for Non-Sentential Utterance Resolution", "author": ["D Raghu"], "venue": null, "citeRegEx": "Raghu,? \\Q2015\\E", "shortCiteRegEx": "Raghu", "year": 2015}, {"title": "Form, intonation and function of clarification", "author": ["K.J. Rod\u0155\u0131guez", "D. Schlangen"], "venue": null, "citeRegEx": "Rod\u0155\u0131guez and Schlangen,? \\Q2004\\E", "shortCiteRegEx": "Rod\u0155\u0131guez and Schlangen", "year": 2004}, {"title": "A coherence-based approach to the interpretation of non-sentential", "author": ["D. Schlangen"], "venue": null, "citeRegEx": "Schlangen,? \\Q2003\\E", "shortCiteRegEx": "Schlangen", "year": 2003}, {"title": "Towards finding and fixing fragments: Using ML to identify nonsentential utterances and their antecedents in multi-party dialogue", "author": ["D. Schlangen"], "venue": "In: Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics. Association for Computational Linguistics, pp. 247\u2013254. Settles, B. (2010). \u201cActive learning literature survey\u201d. In: University of Wisconsin, Madi-", "citeRegEx": "Schlangen,? 2005", "shortCiteRegEx": "Schlangen", "year": 2005}, {"title": "A mathematical theory of communication", "author": ["C. Shannon"], "venue": "Bell System Technical Journal,", "citeRegEx": "Shannon,? \\Q1948\\E", "shortCiteRegEx": "Shannon", "year": 1948}, {"title": "Statistical Learning Theory", "author": ["V.N. Vapnik"], "venue": "Wiley-Interscience. Walker, M. et al. (2001). \u201cDARPA Communicator dialog travel planning systems: The June 2000 data collection\u201d. In: EUROSPEECH, pp. 1371\u20131374. Zhang, N. L. and D. Poole (1996). \u201cExploiting causal independence in Bayesian network inference\u201d. In: Journal of Artificial Intelligence Research, pp. 301\u2013328.", "citeRegEx": "Vapnik,? 1998", "shortCiteRegEx": "Vapnik", "year": 1998}], "referenceMentions": [{"referenceID": 9, "context": "Fern\u00e1ndez (2006) formalizes this task in terms of \u201cresolution rules\u201d built on top of the Type Theory with Records (TTR), a theoretical framework for dialogue context modeling (Ginzburg, 2012).", "startOffset": 175, "endOffset": 191}, {"referenceID": 21, "context": "The probabilistic rules formalism (Lison, 2014) is particularly suited for this task because, similarly to the framework developed by Ginzburg (2012) and Fern\u00e1ndez (2006), it involves the specification of update rules on the variables of the dialogue state to capture the dynamics of the conversation.", "startOffset": 34, "endOffset": 47}, {"referenceID": 7, "context": "Our work builds upon Fern\u00e1ndez et al. (2007) which present a series of machine-learning experiments on the classification of NSUs.", "startOffset": 21, "endOffset": 45}, {"referenceID": 7, "context": "Our work builds upon Fern\u00e1ndez et al. (2007) which present a series of machine-learning experiments on the classification of NSUs. We extended their approach with a combination of new features and semi-supervised learning techniques. The empirical results presented in this thesis show a modest but significant improvement over the state-of-the-art classification performance. The consecutive, yet independent, problem is how to infer an appropriate semantic representation of such NSUs on the basis of the dialogue context. Fern\u00e1ndez (2006) formalizes this task in terms of \u201cresolution rules\u201d built on top of the Type Theory with Records (TTR), a theoretical framework for dialogue context modeling (Ginzburg, 2012).", "startOffset": 21, "endOffset": 542}, {"referenceID": 7, "context": "Our work builds upon Fern\u00e1ndez et al. (2007) which present a series of machine-learning experiments on the classification of NSUs. We extended their approach with a combination of new features and semi-supervised learning techniques. The empirical results presented in this thesis show a modest but significant improvement over the state-of-the-art classification performance. The consecutive, yet independent, problem is how to infer an appropriate semantic representation of such NSUs on the basis of the dialogue context. Fern\u00e1ndez (2006) formalizes this task in terms of \u201cresolution rules\u201d built on top of the Type Theory with Records (TTR), a theoretical framework for dialogue context modeling (Ginzburg, 2012). We argue that logic-based formalisms, such as TTR, have a number of shortcomings when dealing with conversational data, which often include partially observable knowledge and nondeterministic phenomena. An alternative to address these issues is to rely on probabilistic modeling of the dialogue context. Our work is focused on the reimplementation of the resolution rules from Fern\u00e1ndez (2006) with a probabilistic account of the dialogue state.", "startOffset": 21, "endOffset": 1112}, {"referenceID": 7, "context": "Our work builds upon Fern\u00e1ndez et al. (2007) which present a series of machine-learning experiments on the classification of NSUs. We extended their approach with a combination of new features and semi-supervised learning techniques. The empirical results presented in this thesis show a modest but significant improvement over the state-of-the-art classification performance. The consecutive, yet independent, problem is how to infer an appropriate semantic representation of such NSUs on the basis of the dialogue context. Fern\u00e1ndez (2006) formalizes this task in terms of \u201cresolution rules\u201d built on top of the Type Theory with Records (TTR), a theoretical framework for dialogue context modeling (Ginzburg, 2012). We argue that logic-based formalisms, such as TTR, have a number of shortcomings when dealing with conversational data, which often include partially observable knowledge and nondeterministic phenomena. An alternative to address these issues is to rely on probabilistic modeling of the dialogue context. Our work is focused on the reimplementation of the resolution rules from Fern\u00e1ndez (2006) with a probabilistic account of the dialogue state. The probabilistic rules formalism (Lison, 2014) is particularly suited for this task because, similarly to the framework developed by Ginzburg (2012) and Fern\u00e1ndez (2006), it involves the specification of update rules on the variables of the dialogue state to capture the dynamics of the conversation.", "startOffset": 21, "endOffset": 1314}, {"referenceID": 7, "context": "Our work builds upon Fern\u00e1ndez et al. (2007) which present a series of machine-learning experiments on the classification of NSUs. We extended their approach with a combination of new features and semi-supervised learning techniques. The empirical results presented in this thesis show a modest but significant improvement over the state-of-the-art classification performance. The consecutive, yet independent, problem is how to infer an appropriate semantic representation of such NSUs on the basis of the dialogue context. Fern\u00e1ndez (2006) formalizes this task in terms of \u201cresolution rules\u201d built on top of the Type Theory with Records (TTR), a theoretical framework for dialogue context modeling (Ginzburg, 2012). We argue that logic-based formalisms, such as TTR, have a number of shortcomings when dealing with conversational data, which often include partially observable knowledge and nondeterministic phenomena. An alternative to address these issues is to rely on probabilistic modeling of the dialogue context. Our work is focused on the reimplementation of the resolution rules from Fern\u00e1ndez (2006) with a probabilistic account of the dialogue state. The probabilistic rules formalism (Lison, 2014) is particularly suited for this task because, similarly to the framework developed by Ginzburg (2012) and Fern\u00e1ndez (2006), it involves the specification of update rules on the variables of the dialogue state to capture the dynamics of the conversation.", "startOffset": 21, "endOffset": 1335}, {"referenceID": 6, "context": "Fern\u00e1ndez and Ginzburg (2002) identify 15 different types of NSUs.", "startOffset": 0, "endOffset": 30}, {"referenceID": 6, "context": "Fern\u00e1ndez and Ginzburg (2002) identify 15 different types of NSUs. One of the problems that must be addressed to make sense of NSUs is determining their type. One possible way is to classify NSUs using machine learning, as previously experimented by Fern\u00e1ndez et al. (2007). To interpret a given NSU, one also has to resolve its meaning i.", "startOffset": 0, "endOffset": 274}, {"referenceID": 6, "context": "According to Fern\u00e1ndez and Ginzburg (2002) and related works, the frequency of NSUs in the dialogue transcripts of the British National Corpus is about 10% of the total number of utterances.", "startOffset": 13, "endOffset": 43}, {"referenceID": 6, "context": "According to Fern\u00e1ndez and Ginzburg (2002) and related works, the frequency of NSUs in the dialogue transcripts of the British National Corpus is about 10% of the total number of utterances. However, this number may vary greatly if one takes into account a larger variety of phenomena or different dialogue domains e.g. Schlangen (2003) estimates the frequency of NSUs to be 20% of the total number of utterances.", "startOffset": 13, "endOffset": 337}, {"referenceID": 19, "context": "On the other hand we propose a novel approach to the resolution of NSUs using probabilistic rules (Lison, 2015).", "startOffset": 98, "endOffset": 111}, {"referenceID": 7, "context": "On one hand we address the problem of the classification of NSUs by extending the work of Fern\u00e1ndez et al. (2007). On the other hand we propose a novel approach to the resolution of NSUs using probabilistic rules (Lison, 2015).", "startOffset": 90, "endOffset": 114}, {"referenceID": 22, "context": "Nevertheless we detail a large set of NSU resolution rules based on the probabilistic rules formalism and provide an actual implementation of a dialogue system for NSU resolution using the OpenDial toolkit (Lison and Kennington, 2015), which can be the baseline reference for future developments.", "startOffset": 206, "endOffset": 234}, {"referenceID": 7, "context": "Our work on the resolution of NSUs takes inspiration from Fern\u00e1ndez (2006) and Ginzburg (2012) which provide the theoretical background for our study.", "startOffset": 58, "endOffset": 75}, {"referenceID": 7, "context": "Our work on the resolution of NSUs takes inspiration from Fern\u00e1ndez (2006) and Ginzburg (2012) which provide the theoretical background for our study.", "startOffset": 58, "endOffset": 95}, {"referenceID": 7, "context": "Our work on the resolution of NSUs takes inspiration from Fern\u00e1ndez (2006) and Ginzburg (2012) which provide the theoretical background for our study. Their framework is however purely logic-based therefore it can have some drawbacks in dealing with raw conversational data which often contains hidden or partially observable variables. To this end a probabilistic account of the dialogue state is preferable. In our work we implemented a new approach to NSU resolution based on the probabilistic rules formalism of Lison (2015). Probabilistic rules are similar, in some way, to the rules formalized by Ginzburg (2012), as both express updates on the dialogue state given a set of conditions.", "startOffset": 58, "endOffset": 529}, {"referenceID": 7, "context": "Our work on the resolution of NSUs takes inspiration from Fern\u00e1ndez (2006) and Ginzburg (2012) which provide the theoretical background for our study. Their framework is however purely logic-based therefore it can have some drawbacks in dealing with raw conversational data which often contains hidden or partially observable variables. To this end a probabilistic account of the dialogue state is preferable. In our work we implemented a new approach to NSU resolution based on the probabilistic rules formalism of Lison (2015). Probabilistic rules are similar, in some way, to the rules formalized by Ginzburg (2012), as both express updates on the dialogue state given a set of conditions.", "startOffset": 58, "endOffset": 619}, {"referenceID": 9, "context": "Secondly the chapter contains an overview on the formal representation of the dialogue context from the theory of Ginzburg (2012). We discuss briefly the Type Theory with Records, the semantic representation of utterances and the update rules on the dialogue context.", "startOffset": 114, "endOffset": 130}, {"referenceID": 9, "context": "Secondly the chapter contains an overview on the formal representation of the dialogue context from the theory of Ginzburg (2012). We discuss briefly the Type Theory with Records, the semantic representation of utterances and the update rules on the dialogue context. Finally, we introduce the probabilistic approach to the definition of the dialogue context from Lison (2014). We discuss the basics of Bayesian Networks (the dialogue context representation) and the probabilistic rules formalism.", "startOffset": 114, "endOffset": 377}, {"referenceID": 7, "context": "It provides details on our approach, starting from the replication of the work from Fern\u00e1ndez et al. (2007) which we use as baseline.", "startOffset": 84, "endOffset": 108}, {"referenceID": 7, "context": "In order to give a definition of Non-Sentential Utterances ourselves, we shall start by quoting the definition given by Fern\u00e1ndez (2006):", "startOffset": 120, "endOffset": 137}, {"referenceID": 9, "context": "This is indeed a very general definition, whereas a perhaps simpler approach is taken by Ginzburg (2012) which defines NSUs as \u201cutterances without an overt predicate\u201d.", "startOffset": 89, "endOffset": 105}, {"referenceID": 9, "context": "As described in Ginzburg (2012), the parallelism between an NSU and its antecedent can be of syntactic, semantic or phonological nature.", "startOffset": 16, "endOffset": 32}, {"referenceID": 1, "context": "This is a wide-coverage taxonomy resulting from a corpus study on a portion of the British National Corpus (Burnard, 2000).", "startOffset": 107, "endOffset": 122}, {"referenceID": 5, "context": "In order to classify the NSUs, we use a taxonomy defined by Fern\u00e1ndez and Ginzburg (2002). This is a wide-coverage taxonomy resulting from a corpus study on a portion of the British National Corpus (Burnard, 2000).", "startOffset": 60, "endOffset": 90}, {"referenceID": 1, "context": "This is a wide-coverage taxonomy resulting from a corpus study on a portion of the British National Corpus (Burnard, 2000). Table 2.1 contains a summary of the taxonomy with an additional categorization of the classes by their function, as defined by Fern\u00e1ndez (2006) then refined by Ginzburg (2012).", "startOffset": 108, "endOffset": 268}, {"referenceID": 1, "context": "This is a wide-coverage taxonomy resulting from a corpus study on a portion of the British National Corpus (Burnard, 2000). Table 2.1 contains a summary of the taxonomy with an additional categorization of the classes by their function, as defined by Fern\u00e1ndez (2006) then refined by Ginzburg (2012). Other taxonomies of NSUs are available from previous works by e.", "startOffset": 108, "endOffset": 300}, {"referenceID": 1, "context": "This is a wide-coverage taxonomy resulting from a corpus study on a portion of the British National Corpus (Burnard, 2000). Table 2.1 contains a summary of the taxonomy with an additional categorization of the classes by their function, as defined by Fern\u00e1ndez (2006) then refined by Ginzburg (2012). Other taxonomies of NSUs are available from previous works by e.g. Schlangen (2003), but we opted for the one from Fern\u00e1ndez and Ginzburg (2002) because it has been used in an extensive machine learning experiment by Fern\u00e1ndez et al.", "startOffset": 108, "endOffset": 385}, {"referenceID": 1, "context": "This is a wide-coverage taxonomy resulting from a corpus study on a portion of the British National Corpus (Burnard, 2000). Table 2.1 contains a summary of the taxonomy with an additional categorization of the classes by their function, as defined by Fern\u00e1ndez (2006) then refined by Ginzburg (2012). Other taxonomies of NSUs are available from previous works by e.g. Schlangen (2003), but we opted for the one from Fern\u00e1ndez and Ginzburg (2002) because it has been used in an extensive machine learning experiment by Fern\u00e1ndez et al.", "startOffset": 108, "endOffset": 446}, {"referenceID": 1, "context": "This is a wide-coverage taxonomy resulting from a corpus study on a portion of the British National Corpus (Burnard, 2000). Table 2.1 contains a summary of the taxonomy with an additional categorization of the classes by their function, as defined by Fern\u00e1ndez (2006) then refined by Ginzburg (2012). Other taxonomies of NSUs are available from previous works by e.g. Schlangen (2003), but we opted for the one from Fern\u00e1ndez and Ginzburg (2002) because it has been used in an extensive machine learning experiment by Fern\u00e1ndez et al. (2007) and it is also used in the theory of Ginzburg (2012), which is our reference for the resolution part of our investigation.", "startOffset": 108, "endOffset": 542}, {"referenceID": 1, "context": "This is a wide-coverage taxonomy resulting from a corpus study on a portion of the British National Corpus (Burnard, 2000). Table 2.1 contains a summary of the taxonomy with an additional categorization of the classes by their function, as defined by Fern\u00e1ndez (2006) then refined by Ginzburg (2012). Other taxonomies of NSUs are available from previous works by e.g. Schlangen (2003), but we opted for the one from Fern\u00e1ndez and Ginzburg (2002) because it has been used in an extensive machine learning experiment by Fern\u00e1ndez et al. (2007) and it is also used in the theory of Ginzburg (2012), which is our reference for the resolution part of our investigation.", "startOffset": 108, "endOffset": 595}, {"referenceID": 1, "context": "This is a wide-coverage taxonomy resulting from a corpus study on a portion of the British National Corpus (Burnard, 2000). Table 2.1 contains a summary of the taxonomy with an additional categorization of the classes by their function, as defined by Fern\u00e1ndez (2006) then refined by Ginzburg (2012). Other taxonomies of NSUs are available from previous works by e.g. Schlangen (2003), but we opted for the one from Fern\u00e1ndez and Ginzburg (2002) because it has been used in an extensive machine learning experiment by Fern\u00e1ndez et al. (2007) and it is also used in the theory of Ginzburg (2012), which is our reference for the resolution part of our investigation. A detailed comparison of this taxonomy and other ones is given by Fern\u00e1ndez (2006), which also details the corpus study on the BNC that led to the definition of this taxonomy.", "startOffset": 108, "endOffset": 748}, {"referenceID": 1, "context": "This is a wide-coverage taxonomy resulting from a corpus study on a portion of the British National Corpus (Burnard, 2000). Table 2.1 contains a summary of the taxonomy with an additional categorization of the classes by their function, as defined by Fern\u00e1ndez (2006) then refined by Ginzburg (2012). Other taxonomies of NSUs are available from previous works by e.g. Schlangen (2003), but we opted for the one from Fern\u00e1ndez and Ginzburg (2002) because it has been used in an extensive machine learning experiment by Fern\u00e1ndez et al. (2007) and it is also used in the theory of Ginzburg (2012), which is our reference for the resolution part of our investigation. A detailed comparison of this taxonomy and other ones is given by Fern\u00e1ndez (2006), which also details the corpus study on the BNC that led to the definition of this taxonomy. Follows a brief description of all the classes with some examples. Fern\u00e1ndez (2006) provides more details about the rationale of each class.", "startOffset": 108, "endOffset": 925}, {"referenceID": 6, "context": "The taxonomy presented in the previous section is the result of a corpus study on a portion of the dialogue transcripts in the British National Corpus, first started by Fern\u00e1ndez and Ginzburg (2002), then refined by Fern\u00e1ndez (2006).", "startOffset": 169, "endOffset": 199}, {"referenceID": 6, "context": "The taxonomy presented in the previous section is the result of a corpus study on a portion of the dialogue transcripts in the British National Corpus, first started by Fern\u00e1ndez and Ginzburg (2002), then refined by Fern\u00e1ndez (2006). The dialogue transcripts used in the corpus study contain both two-party and multi-party conversations.", "startOffset": 169, "endOffset": 233}, {"referenceID": 6, "context": "The taxonomy presented in the previous section is the result of a corpus study on a portion of the dialogue transcripts in the British National Corpus, first started by Fern\u00e1ndez and Ginzburg (2002), then refined by Fern\u00e1ndez (2006). The dialogue transcripts used in the corpus study contain both two-party and multi-party conversations. The transcripts cover a wide variety of dialogue domains including free conversation, interviews, seminars and more. Fern\u00e1ndez (2006) also describes the annotation procedure and a reliability test.", "startOffset": 169, "endOffset": 472}, {"referenceID": 6, "context": "The taxonomy presented in the previous section is the result of a corpus study on a portion of the dialogue transcripts in the British National Corpus, first started by Fern\u00e1ndez and Ginzburg (2002), then refined by Fern\u00e1ndez (2006). The dialogue transcripts used in the corpus study contain both two-party and multi-party conversations. The transcripts cover a wide variety of dialogue domains including free conversation, interviews, seminars and more. Fern\u00e1ndez (2006) also describes the annotation procedure and a reliability test. The reliability test was carried out on a subset of the annotated instances comparing the manual annotation of three annotators. The test showed a good agreement between the annotators with a kappa-score of 0.76. From this test it is also clear that humans can reliably distinguish between the NSU classes in the taxonomy. Fern\u00e1ndez (2006) provides more details about the complete analysis of the corpus.", "startOffset": 169, "endOffset": 876}, {"referenceID": 7, "context": "Fern\u00e1ndez (2006) describes a study of the distance between NSUs and their antecedents, with a comparison between two-party and multi-party dialogues.", "startOffset": 0, "endOffset": 17}, {"referenceID": 7, "context": "One way to interpret NSUs is developed by Fern\u00e1ndez (2006), in turn based on Schlangen (2003), and it is formed by to consecutive steps, namely the classification and the resolution of the NSUs.", "startOffset": 42, "endOffset": 59}, {"referenceID": 7, "context": "One way to interpret NSUs is developed by Fern\u00e1ndez (2006), in turn based on Schlangen (2003), and it is formed by to consecutive steps, namely the classification and the resolution of the NSUs.", "startOffset": 42, "endOffset": 94}, {"referenceID": 7, "context": "One way to interpret NSUs is developed by Fern\u00e1ndez (2006), in turn based on Schlangen (2003), and it is formed by to consecutive steps, namely the classification and the resolution of the NSUs. The first step for the interpretation of an NSU is its classification i.e. finding its class according to the taxonomy described in Section 2.1.1. As demonstrated in Fern\u00e1ndez et al. (2007), we can infer the class of an NSU using machine learning, i.", "startOffset": 42, "endOffset": 385}, {"referenceID": 7, "context": "One way to interpret NSUs is developed by Fern\u00e1ndez (2006), in turn based on Schlangen (2003), and it is formed by to consecutive steps, namely the classification and the resolution of the NSUs. The first step for the interpretation of an NSU is its classification i.e. finding its class according to the taxonomy described in Section 2.1.1. As demonstrated in Fern\u00e1ndez et al. (2007), we can infer the class of an NSU using machine learning, i.e. we can train a classifier on the corpus detailed in Section 2.1.2 and use it to classify unseen NSU instances. The type of an NSU is used to determine the right resolution procedure to use. The resolution of an NSU is the task of recovering the full clausal meaning from their incomplete form on the basis of contextual information. Fern\u00e1ndez (2006) describes a resolution procedure in terms of rules that, given some preconditions on the antecedent and other elements of the dialogue states, builds the semantic representation of the NSU.", "startOffset": 42, "endOffset": 798}, {"referenceID": 7, "context": "One way to interpret NSUs is developed by Fern\u00e1ndez (2006), in turn based on Schlangen (2003), and it is formed by to consecutive steps, namely the classification and the resolution of the NSUs. The first step for the interpretation of an NSU is its classification i.e. finding its class according to the taxonomy described in Section 2.1.1. As demonstrated in Fern\u00e1ndez et al. (2007), we can infer the class of an NSU using machine learning, i.e. we can train a classifier on the corpus detailed in Section 2.1.2 and use it to classify unseen NSU instances. The type of an NSU is used to determine the right resolution procedure to use. The resolution of an NSU is the task of recovering the full clausal meaning from their incomplete form on the basis of contextual information. Fern\u00e1ndez (2006) describes a resolution procedure in terms of rules that, given some preconditions on the antecedent and other elements of the dialogue states, builds the semantic representation of the NSU. This approach to the resolution of NSUs has been the basis of several implementations of dialogue systems handling the resolution of NSUs such as Ginzburg et al. (2007) and Purver (2006).", "startOffset": 42, "endOffset": 1157}, {"referenceID": 7, "context": "One way to interpret NSUs is developed by Fern\u00e1ndez (2006), in turn based on Schlangen (2003), and it is formed by to consecutive steps, namely the classification and the resolution of the NSUs. The first step for the interpretation of an NSU is its classification i.e. finding its class according to the taxonomy described in Section 2.1.1. As demonstrated in Fern\u00e1ndez et al. (2007), we can infer the class of an NSU using machine learning, i.e. we can train a classifier on the corpus detailed in Section 2.1.2 and use it to classify unseen NSU instances. The type of an NSU is used to determine the right resolution procedure to use. The resolution of an NSU is the task of recovering the full clausal meaning from their incomplete form on the basis of contextual information. Fern\u00e1ndez (2006) describes a resolution procedure in terms of rules that, given some preconditions on the antecedent and other elements of the dialogue states, builds the semantic representation of the NSU. This approach to the resolution of NSUs has been the basis of several implementations of dialogue systems handling the resolution of NSUs such as Ginzburg et al. (2007) and Purver (2006). Extending the interpretation problem to raw conversational data we need also a way to \u201cdetect\u201d an NSU i.", "startOffset": 42, "endOffset": 1175}, {"referenceID": 3, "context": "The grammatical framework is formulated using Type Theory with Records (Cooper, 2005).", "startOffset": 71, "endOffset": 85}, {"referenceID": 8, "context": "As theoretical base of our work we rely on the theory of dialogue context brought up by Ginzburg (2012), which presents a grammatical framework expressly developed for dialogue.", "startOffset": 88, "endOffset": 104}, {"referenceID": 8, "context": "As theoretical base of our work we rely on the theory of dialogue context brought up by Ginzburg (2012), which presents a grammatical framework expressly developed for dialogue. The claim of Ginzburg (2012) is that the rules that encode the dynamics of the dialogue have to be built into the grammar itself.", "startOffset": 88, "endOffset": 207}, {"referenceID": 3, "context": "The grammatical framework is formulated using Type Theory with Records (Cooper, 2005). Type Theory with Records (TTR) is a logical formalism developed to cope with semantics of natural language. TTR is used to build a semantic ontology of abstract entities and events as well as to formalize the dialogue gameboard i.e. a formal representation of the dialogue context and its rules. The evolution of the conversation is formalized by means of update rules on the dialogue context. Ginzburg (2012) also accounts for NSUs and provides a set of dedicated rules.", "startOffset": 72, "endOffset": 497}, {"referenceID": 9, "context": "We will now briefly introduce the basic notions of the Type Theory with Records (TTR), with just enough detail needed by to understand the following sections, referring to Ginzburg (2012) for a complete description.", "startOffset": 172, "endOffset": 188}, {"referenceID": 9, "context": "At the basis of the grammatical framework of Ginzburg (2012) lies the notion of proposition.", "startOffset": 45, "endOffset": 61}, {"referenceID": 7, "context": "Following the definition of Fern\u00e1ndez (2006): Question = \u2206q \u2192 Prop The question domain \u2206q is a record type containing the wh-restrictors of the question q.", "startOffset": 28, "endOffset": 45}, {"referenceID": 9, "context": "In Ginzburg (2012), the dialogue context \u2013 also known as the Dialogue Gameboard (DGB) \u2013 is a formal representation that describes the current state of the dialogue.", "startOffset": 3, "endOffset": 19}, {"referenceID": 9, "context": "In this representation we abstracted away several details that would be included in the actual DGB presented by Ginzburg (2012) such as the fields to track who is holding the turn, the current time and so on.", "startOffset": 112, "endOffset": 128}, {"referenceID": 9, "context": "Ginzburg (2012) develops a comprehensive theory of grounding but we do not include it in our work.", "startOffset": 0, "endOffset": 16}, {"referenceID": 9, "context": "Ginzburg (2012) keeps track of the history of the dialogue within the variable Moves while a reference to the latest (illocutionary) proposition is recorded in the field LatestMove.", "startOffset": 0, "endOffset": 16}, {"referenceID": 9, "context": "In the previous section we detailed a logic-based model of dialogue from Ginzburg (2012). Another possible approach to dialogue modeling relies on probabilistic models to encode the variables and the dynamics of the dialogue context.", "startOffset": 73, "endOffset": 89}, {"referenceID": 9, "context": "In the previous section we detailed a logic-based model of dialogue from Ginzburg (2012). Another possible approach to dialogue modeling relies on probabilistic models to encode the variables and the dynamics of the dialogue context. Arguably this approach can be considered more robust to the intrinsic randomness present in dialogue. This is partially the reason why we explored this strategy as well as other advantages that will be discussed in Chapter 4. We based our work on the probabilistic rules formalism developed by Lison (2012). This formalism is particularly suited for our purpose because of their commonalities with the update rules described in Section 2.", "startOffset": 73, "endOffset": 541}, {"referenceID": 19, "context": "Lison (2014) details the rules and update procedure.", "startOffset": 0, "endOffset": 13}, {"referenceID": 22, "context": "The probabilistic rules formalism has also been implemented into a framework called OpenDial (Lison and Kennington, 2015).", "startOffset": 93, "endOffset": 121}, {"referenceID": 6, "context": "We showed how those utterances can be categorized with a taxonomy from Fern\u00e1ndez and Ginzburg (2002). We described how the interpretation of non-sentential utterances can be addressed by first classifying them using the aforementioned taxonomy and then applying some kind of \u201cresolution\u201d procedure to extract their meaning from the dialogue context.", "startOffset": 71, "endOffset": 101}, {"referenceID": 6, "context": "We showed how those utterances can be categorized with a taxonomy from Fern\u00e1ndez and Ginzburg (2002). We described how the interpretation of non-sentential utterances can be addressed by first classifying them using the aforementioned taxonomy and then applying some kind of \u201cresolution\u201d procedure to extract their meaning from the dialogue context. In Chapter 3 we will address the NSU classification problem on the basis of the experiments from Fern\u00e1ndez et al. (2007). In Chapter 4 instead we will address the NSU resolution task.", "startOffset": 71, "endOffset": 471}, {"referenceID": 6, "context": "We showed how those utterances can be categorized with a taxonomy from Fern\u00e1ndez and Ginzburg (2002). We described how the interpretation of non-sentential utterances can be addressed by first classifying them using the aforementioned taxonomy and then applying some kind of \u201cresolution\u201d procedure to extract their meaning from the dialogue context. In Chapter 3 we will address the NSU classification problem on the basis of the experiments from Fern\u00e1ndez et al. (2007). In Chapter 4 instead we will address the NSU resolution task. Fern\u00e1ndez (2006) describes a set of NSU resolution rules rooted in a TTR representation of the dialogue context.", "startOffset": 71, "endOffset": 551}, {"referenceID": 6, "context": "We showed how those utterances can be categorized with a taxonomy from Fern\u00e1ndez and Ginzburg (2002). We described how the interpretation of non-sentential utterances can be addressed by first classifying them using the aforementioned taxonomy and then applying some kind of \u201cresolution\u201d procedure to extract their meaning from the dialogue context. In Chapter 3 we will address the NSU classification problem on the basis of the experiments from Fern\u00e1ndez et al. (2007). In Chapter 4 instead we will address the NSU resolution task. Fern\u00e1ndez (2006) describes a set of NSU resolution rules rooted in a TTR representation of the dialogue context. Section 2.2 briefly described the TTR notions we employed as well as the dialogue context theory based on TTR from Ginzburg (2012). At last we described the probabilistic modeling of dialogue from Lison (2014) based on the probabilistic rules formalism.", "startOffset": 71, "endOffset": 778}, {"referenceID": 6, "context": "We showed how those utterances can be categorized with a taxonomy from Fern\u00e1ndez and Ginzburg (2002). We described how the interpretation of non-sentential utterances can be addressed by first classifying them using the aforementioned taxonomy and then applying some kind of \u201cresolution\u201d procedure to extract their meaning from the dialogue context. In Chapter 3 we will address the NSU classification problem on the basis of the experiments from Fern\u00e1ndez et al. (2007). In Chapter 4 instead we will address the NSU resolution task. Fern\u00e1ndez (2006) describes a set of NSU resolution rules rooted in a TTR representation of the dialogue context. Section 2.2 briefly described the TTR notions we employed as well as the dialogue context theory based on TTR from Ginzburg (2012). At last we described the probabilistic modeling of dialogue from Lison (2014) based on the probabilistic rules formalism.", "startOffset": 71, "endOffset": 857}, {"referenceID": 6, "context": "We showed how those utterances can be categorized with a taxonomy from Fern\u00e1ndez and Ginzburg (2002). We described how the interpretation of non-sentential utterances can be addressed by first classifying them using the aforementioned taxonomy and then applying some kind of \u201cresolution\u201d procedure to extract their meaning from the dialogue context. In Chapter 3 we will address the NSU classification problem on the basis of the experiments from Fern\u00e1ndez et al. (2007). In Chapter 4 instead we will address the NSU resolution task. Fern\u00e1ndez (2006) describes a set of NSU resolution rules rooted in a TTR representation of the dialogue context. Section 2.2 briefly described the TTR notions we employed as well as the dialogue context theory based on TTR from Ginzburg (2012). At last we described the probabilistic modeling of dialogue from Lison (2014) based on the probabilistic rules formalism. As we mentioned in Chapter 1 this formalism is the framework for our formulation of the NSU resolution rules on the basis of the one developed by Fern\u00e1ndez (2006). We described in Section 2.", "startOffset": 71, "endOffset": 1064}, {"referenceID": 7, "context": "As demonstrated by Fern\u00e1ndez et al. (2007), we can use machine learning techniques to automatically classify a given NSU, using the annotated corpus as training data.", "startOffset": 19, "endOffset": 43}, {"referenceID": 7, "context": "As demonstrated by Fern\u00e1ndez et al. (2007), we can use machine learning techniques to automatically classify a given NSU, using the annotated corpus as training data. Fern\u00e1ndez et al. (2007) is our main theoretical reference and (to our knowledge) the state-of-the-art in performance for the task of classification of NSUs.", "startOffset": 19, "endOffset": 191}, {"referenceID": 7, "context": "The corpus from Fern\u00e1ndez et al. (2007) contains 1 283 annotated NSU instances, each one identified by the name of the containing BNC file and their sentence number, a sequential number to uniquely identify a sentence in a dialogue transcript.", "startOffset": 16, "endOffset": 40}, {"referenceID": 7, "context": "The corpus from Fern\u00e1ndez et al. (2007) contains 1 283 annotated NSU instances, each one identified by the name of the containing BNC file and their sentence number, a sequential number to uniquely identify a sentence in a dialogue transcript. The instances are also tagged with the sentence number of their antecedent which makes up the context for the classification. The raw utterances can be retrieved from the BNC using this information. For the classification task, we make the same simplifying restriction on the corpus made by Fern\u00e1ndez et al. (2007), that is to consider only the NSUs whose antecedent is their preceding sentence.", "startOffset": 16, "endOffset": 559}, {"referenceID": 1, "context": "The British National Corpus (Burnard, 2000) \u2013 BNC for short \u2013 is a collection of spoken and written material, containing about 100 million words of (British) English texts from a large variety of sources.", "startOffset": 28, "endOffset": 43}, {"referenceID": 8, "context": "The dialogues are structured following the CLAWS tagging system (Garside, 1993) which segmented the utterances both at word and sentence level.", "startOffset": 64, "endOffset": 79}, {"referenceID": 16, "context": "The word units contains both the raw text, the corresponding lemma (headword) and the POS-tag according to the C5 tagset (Leech et al., 1994).", "startOffset": 121, "endOffset": 141}, {"referenceID": 7, "context": "The former are used mainly as a comparison with Fern\u00e1ndez et al. (2007) which employ this algorithm as well.", "startOffset": 48, "endOffset": 72}, {"referenceID": 33, "context": "In information theory, the entropy (Shannon, 1948) is the expected value of information carried by a message (or an event in general).", "startOffset": 35, "endOffset": 50}, {"referenceID": 7, "context": "Our baseline is set to be the replicated approach of the classification experiments carried out by Fern\u00e1ndez et al. (2007), which is our reference work for our study.", "startOffset": 99, "endOffset": 123}, {"referenceID": 30, "context": "ant mood As defined by Rod\u0155\u0131guez and Schlangen (2004), this feature was though to distinguish between declarative and non-declarative antecedent sentences.", "startOffset": 23, "endOffset": 54}, {"referenceID": 14, "context": "These features were extracted through the use of the Stanford PCFG parser (Klein and Manning, 2003) on the utterances.", "startOffset": 74, "endOffset": 99}, {"referenceID": 14, "context": "These features were extracted through the use of the Stanford PCFG parser (Klein and Manning, 2003) on the utterances. Refer to Marcus et al. (1993) for more information about the tag set used for the English grammar.", "startOffset": 75, "endOffset": 149}, {"referenceID": 25, "context": "lcs A feature to express the longest common subsequence at the word-level between the NSU and its antecedent, computed using a modified version of the Needleman\u2013Wunsch algorithm (Needleman and Wunsch, 1970), tailored to account for words instead of characters.", "startOffset": 178, "endOffset": 206}, {"referenceID": 18, "context": "Even though it is still a young research field, semi-supervised learning has already found many fields of application (Liang, 2005; Bergsma, 2010).", "startOffset": 118, "endOffset": 146}, {"referenceID": 0, "context": "Even though it is still a young research field, semi-supervised learning has already found many fields of application (Liang, 2005; Bergsma, 2010).", "startOffset": 118, "endOffset": 146}, {"referenceID": 32, "context": "While there has been some previous work towards using machine learning techniques for the detection of the antecedent of NSUs in multi-party dialogue (Schlangen, 2005), we consider sufficient the amount of unlabeled data we can extract following the previous rule.", "startOffset": 150, "endOffset": 167}, {"referenceID": 7, "context": "Nevertheless, as proved in the corpus study of Fern\u00e1ndez (2006), the percentage of the utterances whose antecedent is not the preceding utterance is rather low.", "startOffset": 47, "endOffset": 64}, {"referenceID": 7, "context": "It is still considered an NSU according to the definition of Fern\u00e1ndez (2006).", "startOffset": 61, "endOffset": 78}, {"referenceID": 33, "context": "We will not go into mathematical details so we recommend the interested reader to Vapnik (1998), Collobert et al.", "startOffset": 82, "endOffset": 96}, {"referenceID": 2, "context": "We will not go into mathematical details so we recommend the interested reader to Vapnik (1998), Collobert et al. (2006).", "startOffset": 97, "endOffset": 121}, {"referenceID": 17, "context": "The particular active learning algorithm we employed in our experiments is a pool-based method5 with uncertainty sampling (Lewis and Catlett, 1994).", "startOffset": 122, "endOffset": 147}, {"referenceID": 7, "context": "As in Fern\u00e1ndez et al. (2007), we evaluate our system in a 10-fold cross-validation fashion.", "startOffset": 6, "endOffset": 30}, {"referenceID": 7, "context": "As in Fern\u00e1ndez et al. (2007), we evaluate our system in a 10-fold cross-validation fashion. Weka\u2019s J48 algorithm was used as a comparing classifier. Thanks to the analysis of the resulting trees, we managed to imitate quite closely the behavior of their system as well as reaching a very close performance overall. Even though we use the same feature set and the same algorithm the performance parameters turn out to be slightly lower than the ones claimed in Fern\u00e1ndez et al. (2007). That might be for a variety of reasons, for instance the way feature were extracted or how the parameters were tuned.", "startOffset": 6, "endOffset": 485}, {"referenceID": 7, "context": "4: Performances comparison between Fern\u00e1ndez et al. (2007) and our replica.", "startOffset": 35, "endOffset": 59}, {"referenceID": 7, "context": "This task is formulated as a machine learning problem and we follow and extend the work of Fern\u00e1ndez et al. (2007). We use their corpus as a goldstandard and a replica of their approach as a baseline.", "startOffset": 91, "endOffset": 115}, {"referenceID": 6, "context": "Fern\u00e1ndez (2006) proposes a set of rules to resolve NSUs based on TTR, the logical framework from Cooper (2004) further developed then in Ginzburg (2012).", "startOffset": 0, "endOffset": 17}, {"referenceID": 3, "context": "Fern\u00e1ndez (2006) proposes a set of rules to resolve NSUs based on TTR, the logical framework from Cooper (2004) further developed then in Ginzburg (2012).", "startOffset": 98, "endOffset": 112}, {"referenceID": 3, "context": "Fern\u00e1ndez (2006) proposes a set of rules to resolve NSUs based on TTR, the logical framework from Cooper (2004) further developed then in Ginzburg (2012). One limitation of logical frameworks such as TTR is their inability to directly represent (and reason over) uncertain knowledge.", "startOffset": 98, "endOffset": 154}, {"referenceID": 3, "context": "Fern\u00e1ndez (2006) proposes a set of rules to resolve NSUs based on TTR, the logical framework from Cooper (2004) further developed then in Ginzburg (2012). One limitation of logical frameworks such as TTR is their inability to directly represent (and reason over) uncertain knowledge. Moreover, many dialogue domains contain variables that are only partially observable. We have to take into account a certain degree of stochastic behavior when modeling dialogue since we still have an imperfect understanding of its dynamics. The stochastic component is especially important in dealing with NSUs since they do not have a precise meaning by themselves and, as argued in Ginzburg (2012), they are in principle highly ambiguous.", "startOffset": 98, "endOffset": 685}, {"referenceID": 3, "context": "Fern\u00e1ndez (2006) proposes a set of rules to resolve NSUs based on TTR, the logical framework from Cooper (2004) further developed then in Ginzburg (2012). One limitation of logical frameworks such as TTR is their inability to directly represent (and reason over) uncertain knowledge. Moreover, many dialogue domains contain variables that are only partially observable. We have to take into account a certain degree of stochastic behavior when modeling dialogue since we still have an imperfect understanding of its dynamics. The stochastic component is especially important in dealing with NSUs since they do not have a precise meaning by themselves and, as argued in Ginzburg (2012), they are in principle highly ambiguous. For this reason we propose a new approach to the resolution of NSUs that takes probabilistic account of the variables involved and the procedures used. We employ the probabilistic rules formalism of Lison (2015) (detailed in Section 2.", "startOffset": 98, "endOffset": 938}, {"referenceID": 3, "context": "Fern\u00e1ndez (2006) proposes a set of rules to resolve NSUs based on TTR, the logical framework from Cooper (2004) further developed then in Ginzburg (2012). One limitation of logical frameworks such as TTR is their inability to directly represent (and reason over) uncertain knowledge. Moreover, many dialogue domains contain variables that are only partially observable. We have to take into account a certain degree of stochastic behavior when modeling dialogue since we still have an imperfect understanding of its dynamics. The stochastic component is especially important in dealing with NSUs since they do not have a precise meaning by themselves and, as argued in Ginzburg (2012), they are in principle highly ambiguous. For this reason we propose a new approach to the resolution of NSUs that takes probabilistic account of the variables involved and the procedures used. We employ the probabilistic rules formalism of Lison (2015) (detailed in Section 2.3) to encode the NSU resolution procedures as probabilistic rules. Probabilistic rules are similar, to a certain extent, to the update rules developed by Ginzburg (2012) (described in Section 2.", "startOffset": 98, "endOffset": 1131}, {"referenceID": 3, "context": "Fern\u00e1ndez (2006) proposes a set of rules to resolve NSUs based on TTR, the logical framework from Cooper (2004) further developed then in Ginzburg (2012). One limitation of logical frameworks such as TTR is their inability to directly represent (and reason over) uncertain knowledge. Moreover, many dialogue domains contain variables that are only partially observable. We have to take into account a certain degree of stochastic behavior when modeling dialogue since we still have an imperfect understanding of its dynamics. The stochastic component is especially important in dealing with NSUs since they do not have a precise meaning by themselves and, as argued in Ginzburg (2012), they are in principle highly ambiguous. For this reason we propose a new approach to the resolution of NSUs that takes probabilistic account of the variables involved and the procedures used. We employ the probabilistic rules formalism of Lison (2015) (detailed in Section 2.3) to encode the NSU resolution procedures as probabilistic rules. Probabilistic rules are similar, to a certain extent, to the update rules developed by Ginzburg (2012) (described in Section 2.2). For this reason probabilistic rules are particularly suited for our purpose since, in this way, we could reuse many theoretical aspects from Ginzburg (2012) and Fern\u00e1ndez (2006).", "startOffset": 98, "endOffset": 1316}, {"referenceID": 3, "context": "Fern\u00e1ndez (2006) proposes a set of rules to resolve NSUs based on TTR, the logical framework from Cooper (2004) further developed then in Ginzburg (2012). One limitation of logical frameworks such as TTR is their inability to directly represent (and reason over) uncertain knowledge. Moreover, many dialogue domains contain variables that are only partially observable. We have to take into account a certain degree of stochastic behavior when modeling dialogue since we still have an imperfect understanding of its dynamics. The stochastic component is especially important in dealing with NSUs since they do not have a precise meaning by themselves and, as argued in Ginzburg (2012), they are in principle highly ambiguous. For this reason we propose a new approach to the resolution of NSUs that takes probabilistic account of the variables involved and the procedures used. We employ the probabilistic rules formalism of Lison (2015) (detailed in Section 2.3) to encode the NSU resolution procedures as probabilistic rules. Probabilistic rules are similar, to a certain extent, to the update rules developed by Ginzburg (2012) (described in Section 2.2). For this reason probabilistic rules are particularly suited for our purpose since, in this way, we could reuse many theoretical aspects from Ginzburg (2012) and Fern\u00e1ndez (2006). We reinterpreted the variables in the dialogue state as random variables and straightforwardly \u201cconverted\u201d the resolution rules into probabilistic rules.", "startOffset": 98, "endOffset": 1337}, {"referenceID": 3, "context": "Fern\u00e1ndez (2006) proposes a set of rules to resolve NSUs based on TTR, the logical framework from Cooper (2004) further developed then in Ginzburg (2012). One limitation of logical frameworks such as TTR is their inability to directly represent (and reason over) uncertain knowledge. Moreover, many dialogue domains contain variables that are only partially observable. We have to take into account a certain degree of stochastic behavior when modeling dialogue since we still have an imperfect understanding of its dynamics. The stochastic component is especially important in dealing with NSUs since they do not have a precise meaning by themselves and, as argued in Ginzburg (2012), they are in principle highly ambiguous. For this reason we propose a new approach to the resolution of NSUs that takes probabilistic account of the variables involved and the procedures used. We employ the probabilistic rules formalism of Lison (2015) (detailed in Section 2.3) to encode the NSU resolution procedures as probabilistic rules. Probabilistic rules are similar, to a certain extent, to the update rules developed by Ginzburg (2012) (described in Section 2.2). For this reason probabilistic rules are particularly suited for our purpose since, in this way, we could reuse many theoretical aspects from Ginzburg (2012) and Fern\u00e1ndez (2006). We reinterpreted the variables in the dialogue state as random variables and straightforwardly \u201cconverted\u201d the resolution rules into probabilistic rules. In the next sections we explain how we represented the variables in the dialogue state and how we translated the rules from Fern\u00e1ndez (2006) into probabilistic rules.", "startOffset": 98, "endOffset": 1633}, {"referenceID": 3, "context": "Fern\u00e1ndez (2006) proposes a set of rules to resolve NSUs based on TTR, the logical framework from Cooper (2004) further developed then in Ginzburg (2012). One limitation of logical frameworks such as TTR is their inability to directly represent (and reason over) uncertain knowledge. Moreover, many dialogue domains contain variables that are only partially observable. We have to take into account a certain degree of stochastic behavior when modeling dialogue since we still have an imperfect understanding of its dynamics. The stochastic component is especially important in dealing with NSUs since they do not have a precise meaning by themselves and, as argued in Ginzburg (2012), they are in principle highly ambiguous. For this reason we propose a new approach to the resolution of NSUs that takes probabilistic account of the variables involved and the procedures used. We employ the probabilistic rules formalism of Lison (2015) (detailed in Section 2.3) to encode the NSU resolution procedures as probabilistic rules. Probabilistic rules are similar, to a certain extent, to the update rules developed by Ginzburg (2012) (described in Section 2.2). For this reason probabilistic rules are particularly suited for our purpose since, in this way, we could reuse many theoretical aspects from Ginzburg (2012) and Fern\u00e1ndez (2006). We reinterpreted the variables in the dialogue state as random variables and straightforwardly \u201cconverted\u201d the resolution rules into probabilistic rules. In the next sections we explain how we represented the variables in the dialogue state and how we translated the rules from Fern\u00e1ndez (2006) into probabilistic rules. First we describe the theoretical aspects from Ginzburg (2012) we employ in our approach.", "startOffset": 98, "endOffset": 1722}, {"referenceID": 3, "context": "Fern\u00e1ndez (2006) proposes a set of rules to resolve NSUs based on TTR, the logical framework from Cooper (2004) further developed then in Ginzburg (2012). One limitation of logical frameworks such as TTR is their inability to directly represent (and reason over) uncertain knowledge. Moreover, many dialogue domains contain variables that are only partially observable. We have to take into account a certain degree of stochastic behavior when modeling dialogue since we still have an imperfect understanding of its dynamics. The stochastic component is especially important in dealing with NSUs since they do not have a precise meaning by themselves and, as argued in Ginzburg (2012), they are in principle highly ambiguous. For this reason we propose a new approach to the resolution of NSUs that takes probabilistic account of the variables involved and the procedures used. We employ the probabilistic rules formalism of Lison (2015) (detailed in Section 2.3) to encode the NSU resolution procedures as probabilistic rules. Probabilistic rules are similar, to a certain extent, to the update rules developed by Ginzburg (2012) (described in Section 2.2). For this reason probabilistic rules are particularly suited for our purpose since, in this way, we could reuse many theoretical aspects from Ginzburg (2012) and Fern\u00e1ndez (2006). We reinterpreted the variables in the dialogue state as random variables and straightforwardly \u201cconverted\u201d the resolution rules into probabilistic rules. In the next sections we explain how we represented the variables in the dialogue state and how we translated the rules from Fern\u00e1ndez (2006) into probabilistic rules. First we describe the theoretical aspects from Ginzburg (2012) we employ in our approach. We present then the design of our dialogue context and the rules to resolve the NSUs. We surely take a much simpler approach than Ginzburg (2012) in the modeling of the dialogue state, abstracting intentionally from many details that would add complexity to the modeling.", "startOffset": 98, "endOffset": 1895}, {"referenceID": 9, "context": "We developed a dialogue system able to update the dialogue state probabilistically with update rules similar to the ones from Ginzburg (2012). The system is also able to resolve toy examples in an interactive way.", "startOffset": 126, "endOffset": 142}, {"referenceID": 7, "context": "As previously stated, we rely on Fern\u00e1ndez (2006) and Ginzburg (2012) for the theoretical notions needed to represent the dialogue state and to develop the NSU resolution rules.", "startOffset": 33, "endOffset": 50}, {"referenceID": 7, "context": "As previously stated, we rely on Fern\u00e1ndez (2006) and Ginzburg (2012) for the theoretical notions needed to represent the dialogue state and to develop the NSU resolution rules.", "startOffset": 33, "endOffset": 70}, {"referenceID": 7, "context": "As previously stated, we rely on Fern\u00e1ndez (2006) and Ginzburg (2012) for the theoretical notions needed to represent the dialogue state and to develop the NSU resolution rules. In Section 2.2 we detailed the basic concepts of TTR, the utterance representation and the update rules for the dialogue state. In this section we describe the notions needed for the resolution of the NSUs. In particular we describe how we can exploit the parallelism between the NSU and its antecedent that we mentioned in Section 2.1. We discuss here the concepts that Ginzburg (2012) defines to address the resolution of NSUs then we will describe how we adapt those concepts to our needs in the next section.", "startOffset": 33, "endOffset": 565}, {"referenceID": 9, "context": "In the theory of Ginzburg (2012), this concept is named Partial Parallelism2.", "startOffset": 17, "endOffset": 33}, {"referenceID": 9, "context": "2) (rephrased from Ginzburg (2012)).", "startOffset": 19, "endOffset": 35}, {"referenceID": 8, "context": "For this reason we employ the notion of focus establishing constituents (FEC) from the theory of Ginzburg (2012)3.", "startOffset": 97, "endOffset": 113}, {"referenceID": 7, "context": "b: Who? The concept was previously formalized by Fern\u00e1ndez (2006) as topical constituents.", "startOffset": 49, "endOffset": 66}, {"referenceID": 9, "context": "In this we follow Ginzburg (2012), who defines a set of rules to follow to make FECs contextually available.", "startOffset": 18, "endOffset": 34}, {"referenceID": 7, "context": "As argued in Fern\u00e1ndez (2006), understanding does not always imply acceptance, and Plain Acknowledgments are ambiguous in this distinction.", "startOffset": 13, "endOffset": 30}, {"referenceID": 8, "context": "As in Ginzburg (2012) we consider only unary wh-interrogatives.", "startOffset": 6, "endOffset": 22}, {"referenceID": 7, "context": "Refer to Fern\u00e1ndez (2006) for an account of utterances with multiple wh-interrogatives", "startOffset": 9, "endOffset": 26}, {"referenceID": 7, "context": "To formalize the meaning of Sluices, Fern\u00e1ndez et al. (2007) distinguish four types of Sluices that convey different meanings: Direct Sluices, Reprise Sluices, Clarification Sluices, Whanaphor.", "startOffset": 37, "endOffset": 61}, {"referenceID": 7, "context": "To formalize the meaning of Sluices, Fern\u00e1ndez et al. (2007) distinguish four types of Sluices that convey different meanings: Direct Sluices, Reprise Sluices, Clarification Sluices, Whanaphor. The aforementioned paper describes a machine learning experiment to automatically classify Sluices according to these types. Ginzburg (2012) describes several different treatments for every group of Sluices.", "startOffset": 37, "endOffset": 335}, {"referenceID": 8, "context": "The variables in the dialogue context are inspired by Ginzburg (2012). In order to make the transition from the rules of Fern\u00e1ndez (2006) to probabilistic rules as direct as possible, we mimic the basic dynamic of the DGB detailed in Section 2.", "startOffset": 54, "endOffset": 70}, {"referenceID": 7, "context": "In order to make the transition from the rules of Fern\u00e1ndez (2006) to probabilistic rules as direct as possible, we mimic the basic dynamic of the DGB detailed in Section 2.", "startOffset": 50, "endOffset": 67}, {"referenceID": 9, "context": "The set of dialogue acts we employ in our dialogue context is a small subset of the ones defined by Ginzburg (2012): \u2022 Assert, denoting the act of asserting a proposition; \u2022 Ask, denoting the act of posing a question; \u2022 Ground, denoting the act of understanding what being previously said; \u2022 Accept, denoting the act of accepting what being previously said.", "startOffset": 100, "endOffset": 116}, {"referenceID": 9, "context": "Despite being represented as the maximal element of QUD in Ginzburg (2012), here max-qud actually denotes the index of such element which therefore is retrieved in this way: qud[max-qud].", "startOffset": 59, "endOffset": 75}, {"referenceID": 9, "context": "Despite being represented as the maximal element of QUD in Ginzburg (2012), here max-qud actually denotes the index of such element which therefore is retrieved in this way: qud[max-qud]. In Ginzburg (2012), MaxQUD is given from the partial ordering imposed on QUD.", "startOffset": 59, "endOffset": 207}, {"referenceID": 7, "context": "Since they are a (almost) direct translation of the deterministic rules from Fern\u00e1ndez (2006), most of them have deterministic effects (i.", "startOffset": 77, "endOffset": 94}, {"referenceID": 7, "context": "For Repeated Acknowledgments, Fern\u00e1ndez (2006) requires to have co-referentiality between the repeated constituent in the NSU and the relative constituent in the FEC of MaxQUD.", "startOffset": 30, "endOffset": 47}, {"referenceID": 9, "context": "As argued in Ginzburg (2012), the antecedent of a Sluice can contain more than one potential FEC as exemplified by the following transcript.", "startOffset": 13, "endOffset": 29}, {"referenceID": 9, "context": "To resolve clarification requests and their elliptical variants, Ginzburg (2012) includes a general theory of grounding and clarification requests.", "startOffset": 65, "endOffset": 81}, {"referenceID": 9, "context": "To resolve clarification requests and their elliptical variants, Ginzburg (2012) includes a general theory of grounding and clarification requests. This theory would add a non-trivial amount of complexity to our formalization so we shall assume in our formalization that the latest utterance always grounded unless a clarification request comes afterwards. Therefore we resolve Clarification Ellipsis on the MaxQUD element without adding any other structure to the dialogue state. We also consider the Clarification Ellipsis to have only a clausal confirmation reading, leaving aside their other possible readings which would require a more elaborate approach (more details in Ginzburg (2012)).", "startOffset": 65, "endOffset": 693}, {"referenceID": 19, "context": "As defined in Lison (2014), \u201ca spoken dialogue system is a computational agent that can converse with humans through everyday spoken language\u201d.", "startOffset": 14, "endOffset": 27}, {"referenceID": 23, "context": "Anaphora Resolution (Mitkov, 2014) and Named Entity Recognition (Nadeau and Sekine, 2007) are just a few of the problems concerning the correct interpretation of NSUs.", "startOffset": 20, "endOffset": 34}, {"referenceID": 24, "context": "Anaphora Resolution (Mitkov, 2014) and Named Entity Recognition (Nadeau and Sekine, 2007) are just a few of the problems concerning the correct interpretation of NSUs.", "startOffset": 64, "endOffset": 89}, {"referenceID": 21, "context": "We showed how to reformulate the rules from Fern\u00e1ndez (2006) using the probabilistic rules formalism (Lison, 2014) in order to include a probabilistic account of the dialogue state.", "startOffset": 101, "endOffset": 114}, {"referenceID": 22, "context": "The framework presented in this chapter has also been implemented and tested with OpenDial (Lison and Kennington, 2015).", "startOffset": 91, "endOffset": 119}, {"referenceID": 7, "context": "To do so, we relied on the previous work of Fern\u00e1ndez (2006) which presented a series of rules to resolve the meaning of the NSUs from a TTR-encoded dialogue context.", "startOffset": 44, "endOffset": 61}, {"referenceID": 7, "context": "To do so, we relied on the previous work of Fern\u00e1ndez (2006) which presented a series of rules to resolve the meaning of the NSUs from a TTR-encoded dialogue context. As previously argued throughout this thesis, the use of a purely logic-based formalism, such as TTR, has some disadvantages in dealing with partially observable inputs and stochastic events when compared to a probabilistic approach. We showed how to reformulate the rules from Fern\u00e1ndez (2006) using the probabilistic rules formalism (Lison, 2014) in order to include a probabilistic account of the dialogue state.", "startOffset": 44, "endOffset": 461}, {"referenceID": 7, "context": "To do so, we relied on the previous work of Fern\u00e1ndez (2006) which presented a series of rules to resolve the meaning of the NSUs from a TTR-encoded dialogue context. As previously argued throughout this thesis, the use of a purely logic-based formalism, such as TTR, has some disadvantages in dealing with partially observable inputs and stochastic events when compared to a probabilistic approach. We showed how to reformulate the rules from Fern\u00e1ndez (2006) using the probabilistic rules formalism (Lison, 2014) in order to include a probabilistic account of the dialogue state. We made use of a portion of the dialogue context theory from Ginzburg (2012) to encode the basic elements of the dialogue state needed for the resolution of the NSUs (see Section 2.", "startOffset": 44, "endOffset": 659}, {"referenceID": 7, "context": "1 the concept of non-sentential utterance, referring to Fern\u00e1ndez (2006) as our theoretical basis.", "startOffset": 56, "endOffset": 73}, {"referenceID": 7, "context": "1 the concept of non-sentential utterance, referring to Fern\u00e1ndez (2006) as our theoretical basis. From the aforementioned work we employ the same taxonomy and corpus of NSUs. We then explain our methodology for the interpretation of NSUs, also based on the theory from Fern\u00e1ndez (2006). To interpret an NSU, we first classify it according to the aforementioned taxonomy using machine learning then we \u201cresolve\u201d its meaning from the dialogue context through a resolution procedure dependent on its type.", "startOffset": 56, "endOffset": 287}, {"referenceID": 9, "context": "Fern\u00e1ndez (2006) develops a set of resolution procedures based on Type Theory with Records (Cooper, 2004; Ginzburg, 2012).", "startOffset": 91, "endOffset": 121}, {"referenceID": 3, "context": "Fern\u00e1ndez (2006) develops a set of resolution procedures based on Type Theory with Records (Cooper, 2004; Ginzburg, 2012). In Section 2.2 we briefly describe the aspects of TTR and the theory of dialogue context from Ginzburg (2012) that we needed in our work.", "startOffset": 92, "endOffset": 233}, {"referenceID": 19, "context": "To this end we employ the probabilistic rules formalism and the theory from Lison (2014) as probabilistic representation of the dialogue state and the NSU resolution procedures.", "startOffset": 76, "endOffset": 89}, {"referenceID": 19, "context": "To this end we employ the probabilistic rules formalism and the theory from Lison (2014) as probabilistic representation of the dialogue state and the NSU resolution procedures. We detail the basic aspects of the theory from Lison (2014) in Section 2.", "startOffset": 76, "endOffset": 238}, {"referenceID": 19, "context": "To this end we employ the probabilistic rules formalism and the theory from Lison (2014) as probabilistic representation of the dialogue state and the NSU resolution procedures. We detail the basic aspects of the theory from Lison (2014) in Section 2.3. In Lison (2014) the dialogue state is represented as a Bayesian network and its dynamics are described by probabilistic rules.", "startOffset": 76, "endOffset": 270}, {"referenceID": 6, "context": "Our work on the classification of NSUs is based on Fern\u00e1ndez et al. (2007). We replicated their approach and set it as our baseline, as explained in Section 3.", "startOffset": 51, "endOffset": 75}, {"referenceID": 4, "context": "In Dragone and Lison (2015a) we present our findings in the classification of NSUs using Active Learning.", "startOffset": 3, "endOffset": 29}, {"referenceID": 9, "context": "2 we describe the theoretical concepts we need from Ginzburg (2012). The description of our theory starts in Section 4.", "startOffset": 52, "endOffset": 68}, {"referenceID": 9, "context": "2 we describe the theoretical concepts we need from Ginzburg (2012). The description of our theory starts in Section 4.3 where we explained the design of the dialogue context. To model our dialogue context we take inspiration from Ginzburg (2012), however we reinterpret its constructs as random variables.", "startOffset": 52, "endOffset": 247}, {"referenceID": 6, "context": "We showed how we could reuse many concepts from the theory of Fern\u00e1ndez (2006) and Ginzburg (2012) and \u201ctranslate\u201d the resolution rules based on TTR into probabilistic rules.", "startOffset": 62, "endOffset": 79}, {"referenceID": 6, "context": "We showed how we could reuse many concepts from the theory of Fern\u00e1ndez (2006) and Ginzburg (2012) and \u201ctranslate\u201d the resolution rules based on TTR into probabilistic rules.", "startOffset": 62, "endOffset": 99}, {"referenceID": 4, "context": "The works on classification and interpretation carried out in this thesis were also presented in Dragone and Lison (2015b).", "startOffset": 97, "endOffset": 123}, {"referenceID": 6, "context": "Additional data for the gold standard should be composed of high-quality, manually annotated instances extracted within a corpus study that closely follows the original one from Fern\u00e1ndez and Ginzburg (2002).", "startOffset": 178, "endOffset": 208}, {"referenceID": 6, "context": "The corpus study from Fern\u00e1ndez and Ginzburg (2002) is focused on data extracted from the British National Corpus therefore confined to only certain kind of dialogue domains.", "startOffset": 22, "endOffset": 52}, {"referenceID": 6, "context": "The corpus study from Fern\u00e1ndez and Ginzburg (2002) is focused on data extracted from the British National Corpus therefore confined to only certain kind of dialogue domains. As argued by Raghu et al. (2015), there are many elliptical phenomena that do not fit well in the taxonomy from Fern\u00e1ndez and Ginzburg (2002).", "startOffset": 22, "endOffset": 208}, {"referenceID": 6, "context": "The corpus study from Fern\u00e1ndez and Ginzburg (2002) is focused on data extracted from the British National Corpus therefore confined to only certain kind of dialogue domains. As argued by Raghu et al. (2015), there are many elliptical phenomena that do not fit well in the taxonomy from Fern\u00e1ndez and Ginzburg (2002). An interesting follow up work on non-sentential utterances might try to find new elliptical phenomena in different dialogue Synthetic Minority Over-sampling Technique.", "startOffset": 22, "endOffset": 317}, {"referenceID": 29, "context": "It bares similarities with statistical approaches for the resolution such as Raghu et al. (2015). Their work is concentrated on follow-up NSU questions such as: (5.", "startOffset": 77, "endOffset": 97}, {"referenceID": 27, "context": ", 2007) and one of its extensions CLARIE (Purver, 2006).", "startOffset": 41, "endOffset": 55}, {"referenceID": 6, "context": "Another useful comparison, and perhaps integration, should be made with the systems originally developed on the theory of Fern\u00e1ndez (2006), namely SHARDS (Ginzburg et al.", "startOffset": 122, "endOffset": 139}, {"referenceID": 3, "context": "The latter is a dialogue system developed to deal with clarification requests and, among them, Clarification Ellipsis, implementing the theory of Ginzburg and Cooper (2004) on top of the GoDiS dialogue system (Larsson et al.", "startOffset": 159, "endOffset": 173}, {"referenceID": 3, "context": "The latter is a dialogue system developed to deal with clarification requests and, among them, Clarification Ellipsis, implementing the theory of Ginzburg and Cooper (2004) on top of the GoDiS dialogue system (Larsson et al., 2000). Both are based on the HPSG2 framework from Ginzburg and Sag (2000), which is substantially different from our current design.", "startOffset": 159, "endOffset": 300}, {"referenceID": 9, "context": "These rules are inspired by, but not limited to, Ginzburg (2012). Section 2.", "startOffset": 49, "endOffset": 65}, {"referenceID": 9, "context": "These rules are inspired by, but not limited to, Ginzburg (2012). Section 2.2 gives the background knowledge for the rules from Ginzburg (2012).", "startOffset": 49, "endOffset": 144}], "year": 2015, "abstractText": "Non-sentential utterances (NSUs) are utterances that lack a complete sentential form but whose meaning can be inferred from the dialogue context, such as \u201cOK\u201d, \u201cwhere?\u201d, \u201cprobably at his apartment\u201d. The interpretation of non-sentential utterances is an important problem in computational linguistics since they constitute a frequent phenomena in dialogue and they are intrinsically context-dependent. The interpretation of NSUs is the task of retrieving their full semantic content from their form and the dialogue context. NSUs also come in a wide variety of forms and functions and classifying them in the right category is a prerequisite to their interpretation. The first half of this thesis is devoted to the NSU classification task. Our work builds upon Fern\u00e1ndez et al. (2007) which present a series of machine-learning experiments on the classification of NSUs. We extended their approach with a combination of new features and semi-supervised learning techniques. The empirical results presented in this thesis show a modest but significant improvement over the state-of-the-art classification performance. The consecutive, yet independent, problem is how to infer an appropriate semantic representation of such NSUs on the basis of the dialogue context. Fern\u00e1ndez (2006) formalizes this task in terms of \u201cresolution rules\u201d built on top of the Type Theory with Records (TTR), a theoretical framework for dialogue context modeling (Ginzburg, 2012). We argue that logic-based formalisms, such as TTR, have a number of shortcomings when dealing with conversational data, which often include partially observable knowledge and nondeterministic phenomena. An alternative to address these issues is to rely on probabilistic modeling of the dialogue context. Our work is focused on the reimplementation of the resolution rules from Fern\u00e1ndez (2006) with a probabilistic account of the dialogue state. The probabilistic rules formalism (Lison, 2014) is particularly suited for this task because, similarly to the framework developed by Ginzburg (2012) and Fern\u00e1ndez (2006), it involves the specification of update rules on the variables of the dialogue state to capture the dynamics of the conversation. However, the probabilistic rules can also encode probabilistic knowledge, thereby providing a principled account of ambiguities in the NSU resolution process. In the second part of this thesis, we present our proof-of-concept framework for NSU resolution using probabilistic rules.", "creator": "LaTeX with hyperref package"}}}